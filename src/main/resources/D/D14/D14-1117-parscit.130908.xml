<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.9976025">
Knowledge Graph and Corpus Driven Segmentation and
Answer Inference for Telegraphic Entity-seeking Queries
</title>
<author confidence="0.994082">
Mandar Joshi ∗ Uma Sawant Soumen Chakrabarti
</author>
<affiliation confidence="0.989299">
IBM Research IIT Bombay, Yahoo Labs IIT Bombay
</affiliation>
<email confidence="0.987583">
mandarj90@in.ibm.com uma@cse.iitb.ac.in soumen@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.993608" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.987672324324325">
Much recent work focuses on formal in-
terpretation of natural question utterances,
with the goal of executing the resulting
structured queries on knowledge graphs
(KGs) such as Freebase. Here we address
two limitations of this approach when ap-
plied to open-domain, entity-oriented Web
queries. First, Web queries are rarely well-
formed questions. They are “telegraphic”,
with missing verbs, prepositions, clauses,
case and phrase clues. Second, the KG is
always incomplete, unable to directly an-
swer many queries. We propose a novel
technique to segment a telegraphic query
and assign a coarse-grained purpose to
each segment: a base entity e1, a rela-
tion type r, a target entity type t2, and
contextual words s. The query seeks en-
tity e2 ∈ t2 where r(e1, e2) holds, fur-
ther evidenced by schema-agnostic words
s. Query segmentation is integrated with
the KG and an unstructured corpus where
mentions of entities have been linked to
the KG. We do not trust the best or any
specific query segmentation. Instead, evi-
dence in favor of candidate e2s are aggre-
gated across several segmentations. Ex-
tensive experiments on the ClueWeb cor-
pus and parts of Freebase as our KG, us-
ing over a thousand telegraphic queries
adapted from TREC, INEX, and Web-
Questions, show the efficacy of our ap-
proach. For one benchmark, MAP im-
proves from 0.2–0.29 (competitive base-
lines) to 0.42 (our system). NDCG@10
improves from 0.29–0.36 to 0.54.
∗Work done as Masters student at IIT Bombay
</bodyText>
<sectionHeader confidence="0.998013" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999905">
A majority of Web queries mention an entity or
type (Lin et al., 2012), as users increasingly ex-
plore the Web of objects using Web search. To
better support entity-oriented queries, commercial
Web search engines are rapidly building up large
catalogs of types, entities and relations, popu-
larly called a “knowledge graph” (KG) (Gallagher,
2012). Despite these advances, robust, Web-scale,
open-domain, entity-oriented search faces many
challenges. Here, we focus on two.
</bodyText>
<subsectionHeader confidence="0.995776">
1.1 “Telegraphic” queries
</subsectionHeader>
<bodyText confidence="0.999931142857143">
First, the surface utterances of entity-oriented Web
queries are dramatically different from TREC-
or Watson-style factoid question answering (QA),
where questions are grammatically well-formed.
Web queries are usually “telegraphic”: they are
short, rarely use function words, punctuations
or clausal structure, and use relatively flexible
word orders. E.g., the natural utterance “on the
bank of which river is the Hermitage Museum lo-
cated” may be translated to the telegraphic Web
query hermitage museum river bank. Even
on well-formed question utterances, 50% of in-
terpretation failures are contributed by parsing or
structural matching failures (Kwiatkowski et al.,
2013). Telegraphic utterances will generally be
even more challenging.
Consequently, whereas TREC-QA/NLP-style
research has focused on parsing and precise in-
terpretation of a well-formed query sentence to
a strongly structured (typically graph-oriented)
query language (Kasneci et al., 2008; Pound et
al., 2012; Yahya et al., 2012; Berant et al., 2013;
Kwiatkowski et al., 2013), the Web search and in-
formation retrieval (IR) community has focused
on telegraphic queries (Guo et al., 2009; Sarkas
et al., 2010; Li et al., 2011; Pantel et al., 2012; Lin
et al., 2012; Sawant and Chakrabarti, 2013). In
terms of target schema richness, these efforts may
</bodyText>
<page confidence="0.976745">
1104
</page>
<note confidence="0.9005795">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1104–1114,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9996908">
appear more modest. The act of query ‘interpre-
tation’ is mainly a segmentation of query tokens
by purpose. In the example above, one may re-
port segments “Hermitage Museum” (a located ar-
tifact or named entity), and “river bank” (the target
type). This is reminiscent of record segmentation
in information extraction (IE). Over well-formed
utterances, IE baselines are quite competitive (Yao
and Van Durme, 2014). But here, we are interested
exclusively in telegraphic queries.
</bodyText>
<subsectionHeader confidence="0.899046">
1.2 Incomplete knowledge graph
</subsectionHeader>
<bodyText confidence="0.999993034482759">
The second problem is that the KG is always
work in progress (Pereira, 2013), and connec-
tions found within nodes of the KG, between the
KG and the query, or the KG and unstructured
text, are often incomplete or erroneous. E.g.,
Wikipedia is considered tiny, and Freebase rather
small, compared to what is needed to answer all
but the “head” queries. Google’s Freebase an-
notations (Gabrilovich et al., 2013) on ClueWeb
(ClueWeb09, 2009) number fewer than 15 per
page to ensure precision. Fewer than 2% are to
entities in Freebase but not in Wikipedia.
It may also be difficult to harness the KG for
answering certain queries. E.g., answering the
query fastest odi century batsman, the intent of
which is to find the batsman holding the record for
the fastest century in One Day International (ODI)
cricket, may be too difficult for most KG-only sys-
tems, but may be answered quite effectively by a
system that also utilizes evidence from unstruc-
tured text.
There is a clear need for a “pay-as-you-go” ar-
chitecture that involves both the corpus and KG. A
query easily served by a curated KG should give
accurate results, but it is desirable to have a grace-
ful interpolation supported by the corpus: e.g., if
the relation r(e1, e2) is not directly evidenced in
the KG, but strongly hinted in the corpus, we still
want to use this for ranking.
</bodyText>
<subsectionHeader confidence="0.978705">
1.3 Our contributions
</subsectionHeader>
<bodyText confidence="0.9995118">
Here, we make progress beyond the above frontier
of prior work in the following significant ways.
We present a new architecture for structural in-
terpretation of a telegraphic query into these seg-
ments (some may be empty):
</bodyText>
<listItem confidence="0.9917154">
• Mention/s ei of an entity e1,
• Mention r of a relation type r,
• Mention t2 of a target type t2, and
• Other contextual matching words s (some-
times called selectors),
</listItem>
<bodyText confidence="0.999795303030303">
with the simultaneous intent of finding and rank-
ing entities e2 E t2, such that r(e1, e2) is likely
to hold, evidenced near the matching words in un-
structured text.
Given the short, telegraphic query utterances,
we limit our scope to at most one relation mention,
unlike the complex mapping of clauses in well-
formed questions to twig and join style queries
(e.g., “find an actor whose spouse was an Italian
bookwriter”). On the other hand, we need to deal
with the unhelpful input, as well as consolidate
the KG with the corpus for ranking candidate e2s.
Despite the modest specification, our query tem-
plate is quite expressive, covering a wide range of
entity-oriented queries (Yih et al., 2014).
We present a novel discriminative graphical
model to capture the entity ranking inference task,
with query segmentation as a by-product. Ex-
tensive experiments with over a thousand entity-
seeking telegraphic queries using the ClueWeb09
corpus and a subset of Freebase show that we can
accurately predict the segmentation and intent of
telegraphic relational queries, and simultaneously
rank candidate responses with high accuracy. We
also present evidence that the KG and corpus have
synergistic salutary effects on accuracy.
§2 explores related work in more detail. §3
gives some examples fitting our query template,
explains why interpreting some of them is nontriv-
ial, and sets up notation. §4 presents our core tech-
nical contributions. §5 presents experiments. Data
can be accessed at http://bit.ly/Spva49
and http://bit.ly/WSpxvr.
</bodyText>
<sectionHeader confidence="0.999621" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999943">
The NLP/QA community has traditionally as-
sumed that question utterances are grammatically
well-formed, from which precise clause structure,
ground constants, variables, and connective rela-
tions can be inferred via semantic parsing (Kas-
neci et al., 2008; Pound et al., 2012; Yahya et
al., 2012; Berant et al., 2013; Kwiatkowski et
al., 2013) and translated to lambda expressions
(Liang, 2013) or SPARQL style queries (Kasneci
et al., 2008), with elaborate schema knowledge.
Such approaches are often correlated with the as-
sumption that all usable knowledge has been cu-
rated into a KG. The query is first translated to a
structured form and then “executed” on the KG. A
</bodyText>
<page confidence="0.899445">
1105
</page>
<bodyText confidence="0.998736923076923">
Telegraphic query el r t2 s
first african american nobel prize winner nobel prize winner african american first
nobel prize - winner first african american
- - winner first african american nobel prize
dave navarro first band dave navarro band - first
dave navarro band band first
merril lynch headquarters merril lynch headquarters - -
merril lynch - headquarters -
spanish poet died in civil war spanish died in poet civil war
civil war died - spanish poet
spanish in poet died civil war
first american in space - - - first american in space
- - american first, in space
</bodyText>
<figureCaption confidence="0.998914">
Figure 1: Example queries and some potential segmentations.
</figureCaption>
<bodyText confidence="0.999992588235294">
large corpus may be used to build relation expres-
sion models (Yao and Van Durme, 2014), but not
as supporting evidence for target entities.
In contrast, the Web and IR community gener-
ally assumes a free-form query that is often tele-
graphic (Guo et al., 2009; Sarkas et al., 2010; Li
et al., 2011). Queries being far more noisy, the
goal of structure discovery is more modest, and of-
ten takes the form of a segmentation of the query
regarded as a token sequence, assigning a broad
purpose (Pantel et al., 2012; Lin et al., 2012) to
each segment, mapping them probabilistically to
a relatively loose schema, and ranking responses
in conjunction with segmentations (Sawant and
Chakrabarti, 2013). To maintain quality in the face
of noisy input, these approaches often additionally
exploit clicks (Li et al., 2011) or a corpus that has
been annotated with entity mentions (Cheng and
Chang, 2010; Li et al., 2010). The corpus provides
contextual snippets for queries where the KG fails,
preventing the systems from falling off the “struc-
ture cliff” (Pereira, 2013).
Our work advances the capabilities of the lat-
ter class of approaches, bringing them closer to
the depth of the former, while handling telegraphic
queries and retaining the advantage of corpus evi-
dence over and above the KG. Very recently, (Yao
et al., 2014) have concluded that for current bench-
marks, deep parsing and shallow information ex-
traction give comparable interpretation accuracy.
The very recent work of (Yih et al., 2014) is simi-
lar in spirit to ours, but they do not unify segmen-
tation and answer inference, along with corpus ev-
idence, like we do.
</bodyText>
<sectionHeader confidence="0.962671" genericHeader="method">
3 Notation and examples
</sectionHeader>
<bodyText confidence="0.993324967741935">
We use e1, r, t2, e2 to represent abstract nodes and
edges (MIDs in case of Freebase) from the KG,
and 61, r, t2 to represent their textual mentions or
hints, if any, in the query. s is a set of uninterpreted
textual tokens in the query that are used to match
and collect corpus contexts that lend evidence to
candidate entities.
Figure 1 shows some telegraphic queries
with possible segmentation into the above
parts. Consider another example: dave
navarro first band. ‘Band’ is a hint for
type /music/musical group, so it com-
prises t2. Dave Navarro is an entity, with men-
tion words ‘dave navarro’ comprising 61. r is
made up of ‘band’, and represents the relation
/music/group member/membership. Fi-
nally, the word first cannot be mapped to any sim-
ple KG artifact, so are relegated to s (which makes
the corpus a critical part of answer inference). We
use s and s interchangeably.
Generally, there will be enough noise and uncer-
tainty that the search system should try out several
of the most promising segmentations as shown in
Figure 1. The accuracy of any specific segmenta-
tion is expected to be low in such adversarial set-
tings. Therefore, support for an answer entity is
aggregated over several segmentations. The ex-
pectation is that by considering multiple interpre-
tations, the system will choose the entity with best
supporting evidence from corpus and knowledge
base.
</bodyText>
<sectionHeader confidence="0.983879" genericHeader="method">
4 Our Approach
</sectionHeader>
<bodyText confidence="0.999947">
Telegraphic queries are usually short, so we enu-
merate query token spans (with some restrictions,
similar to beam search) to propose segmentations
(§4.1). Candidate response entities are lined up
for each interpretation, and then scored in a global
model along with query segmentations (§4.2).
§4.3 describes how model parameters are trained.
</bodyText>
<page confidence="0.949725">
1106
</page>
<listItem confidence="0.990410583333333">
1: input: query token sequence q
2: initialize segmentations Z = ∅
3: £1 = (entity, mention) pairs from linker
4: for all (e1, be1) E £1 do
5: assign label E1 to mention tokens be1
6: for all contiguous span v C q \ be1 do
7: label each word w E v as T2R
8: label other words w E q \ be1 \ v as S
9: add segments (E1, T2R, S) to Z
10: end for
11: end for
12: return candidate segmentations Z
</listItem>
<figureCaption confidence="0.9561745">
Figure 2: Generating candidate query segmenta-
tions.
</figureCaption>
<subsectionHeader confidence="0.8591285">
4.1 Generating candidate query
segmentations
</subsectionHeader>
<bodyText confidence="0.999930777777778">
Each query token can have four labels,
E1, T2, R, S, corresponding to the mentions
of the base entity, target type, connecting relation,
and context words. We found that segments
hinting at T2 and R frequently overlapped
(e.g., ‘author’ in the query zhivago author).
In our implementation, we simplified to three
labels, E1, T2R,S, where tokens labeled T2R
are involved with both t2 and r, the proposed
structured target type and connecting relation.
Another reasonable assumption was that the base
entity mention and type/relation mentions are
contiguous token spans, whereas context words
can be scattered in multiple segments.
Figure 2 shows how candidate segmentations
are generated. For step 3, we use TagMe (Ferrag-
ina and Scaiella, 2010), an entity linker backed by
an entity gazette derived from our KG.
</bodyText>
<subsectionHeader confidence="0.974347">
4.2 Graphical model
</subsectionHeader>
<bodyText confidence="0.993682">
Based on the previous discussion, we assume that
an entity-seeking query q is a sequence of tokens
q1, q2, . . ., and this can be partitioned into different
kinds of subsequences, corresponding to e1, r, t2
and s, and denoted by a structured (vector) label-
ing z = z1, z2,.. .. Given sequences q and z, we
can separate out (possibly empty) token segments
be1(q, z), bt2(q, z), br(q, z), and bs(q, z).
A query segmentation z becomes plausible in
conjunction with proposals for e1, r, t2 and e2
from the KG. The probability Pr(z, e1, r, t2, e2|q)
is modeled as proportional to the product of sev-
eral potentials (Koller and Friedman, 2009) in a
graphical model. In subsequent subsections, we
will present the design of specific potentials.
</bodyText>
<listItem confidence="0.9669319">
• &amp;R(q, z, r) denotes the compatibility be-
tween the relation hint segment br(q, z) and
a proposed relation type r in the KG (§4.2.1).
• &amp;T2(q, z, t2) denotes the compatibility be-
tween the type hint segment bt2(q, z) and a
proposed target entity type t2 in the KG
(§4.2.2).
• &amp;E1,R,E2,S(q, z, e1, r, e2) is a novel corpus-
based evidence potential that measures how
strongly e1 and e2 appear in corpus snippets
in the proximity of words in bs(q, z), and ap-
parently related by relation type r (§4.2.3).
• &amp;E1(q, z, e1) denotes the compatibility be-
tween the query segment be1(q, z) and entity
e1 that it purportedly mentions (§4.2.4).
• &amp;S(q, z) denotes selector compatibility. Se-
lectors are a fallback label, so this is pinned
arbitrarily to 1; other potentials are balanced
against this base value.
• &amp;E1,R,E2(e1, r, e2) is A if the relation
r(e1, e2) exists in the KG, and is B &gt; 0 oth-
erwise, for tuned/learnt constants A &gt; B &gt;
0. Note that this is a soft constraint (B &gt; 0);
if the KG is incomplete, the corpus may be
able to supplement the required information.
• &amp;E2,T2(e2, t2) is 1 if e2 belongs to t2 and
zero otherwise. In other words, candidate e2s
must be proposed to be instances of the pro-
posed t2 — this is a hard constraint, but can
be softened if desired, like &amp;E1,R,E2.
</listItem>
<bodyText confidence="0.941229625">
Figure 3 shows the relevant variable states as
circled nodes, and the potentials as square factor
nodes. To rank candidate entities e2, we pin the
node E2 to each entity in turn. With E2 pinned,
we perform a MAP inference over all other hidden
variables and note the score of e2 as the product of
the above potentials maximized over choices of all
other variables: score(e2) =
</bodyText>
<equation confidence="0.9901105">
Maxx,t2,r,e1&amp;T2(q, z, t2)&amp;R(q, z, r)
&amp;E1(q, z, e1)&amp;S(q, z)
&amp;E2,T2(e2, t2)&amp;E1,R,E2(e1, r, e2)
&amp;E1,R,E2,S(q, z, e1, r, e2). (1)
</equation>
<bodyText confidence="0.9999066">
We rank candidate e2s by decreasing score, which
is estimated by max-product message-passing
(Koller and Friedman, 2009).
As noted earlier, any of the relation/type, or
query entity partitions may be empty. To handle
</bodyText>
<page confidence="0.994782">
1107
</page>
<figureCaption confidence="0.971810666666667">
Figure 3: Graphical model for query segmentation and entity scoring. Factors/potentials are shown as
squares. A candidate e2 is observed and scored using equation (1). Query q is also observed but not
shown to reduce clutter; most potentials depend on it.
</figureCaption>
<figure confidence="0.996529714285714">
Type
language
model
Segmentation
Entity
language
model
Target
type
Query Selectors
entity
Connecting
relation
Corpus-assisted
entity-relation
evidence potential
Relation
language
model
Candidate
entity
</figure>
<bodyText confidence="0.999370875">
this case, we allow each of the entity, relation or
target type nodes in the graphical to take the value
⊥ or ‘null’. To support this, the value of the fac-
tor between the query segmentation node Z and
ΨE1(q, z, e1), ΨT2(q, z, t2), and ΨR(q, z, r)) are
set to suitable low values.
Next, we will describe the detailed design of
some of the key potentials introduced above.
</bodyText>
<subsubsectionHeader confidence="0.710905">
4.2.1 Relation language model for ΨR
</subsubsectionHeader>
<bodyText confidence="0.999748658536586">
Potential ΨR(q, z, r) captures the compatibility
between r(q, z) and the proposed relation r.
E.g., if the query is steve jobs death rea-
son, and r is (correctly chosen as) death rea-
son, then the correct candidate r is /people/
deceased_person/cause _aof _ddeath. An
incorrect r is /people/decese person/
place_of_death. An incorrect z may lead to
r(q, z) being jobs death.
Using corpus: Considerable variation may exist
in how r is represented textually in a query. The
relation language model needs to build a bridge
between the formal r and the textual r, so that
(un)likely r’s have (small) large potential. Many
approaches (Berant et al., 2013; Berant and Liang,
2014; Kwiatkowski et al., 2013; Yih et al., 2014)
to this problem have been intensely studied re-
cently. Given our need to process billions of Web
pages efficiently, we chose a pattern-based ap-
proach (Nakashole et al., 2012): with each r, dis-
cover the most strongly associated phrase patterns
from a reference corpus, then mark these patterns
into much larger payload corpus.
We started with the 2000 (out of approximately
14000) most frequent relation types in Freebase,
and the ClueWeb09 corpus annotated with Free-
base entities (Gabrilovich et al., 2013). For each
triple instance of each relation type, we located all
corpus sentences that mentioned both participat-
ing entities. We made the crude assumption that
if r(e1,e2) holds and e1, e2 co-occur in a sen-
tence then this sentence is evidence of the rela-
tionship. Each such sentence is parsed to obtain
a dependency graph using the Malt Parser (Hall
et al., 2014). Words in the path connecting the
entities are joined together and added to a candi-
date phrase dictionary, provided the path is at most
three hops. (Inspection suggested that longer de-
pendency paths mostly arise out of noisy sentences
or botched parses.) 30% of the sentences were
thus retained. Finally, we defined
</bodyText>
<equation confidence="0.992263333333333">
n(r, �r(q, z))
ΨR(q, z, r) = (2)
Ep, n(r, p&apos;),
</equation>
<bodyText confidence="0.998504777777778">
where p&apos; ranges over all phrases that are known to
hint at r, and n(r, p) denotes the number of sen-
tences where the phrase p occurred in the depen-
dency path between the entities participating in re-
lation r.
Assuming entity co-occurrence implies evi-
dence is admittedly simplistic. However, the pri-
mary function of the relation model is to retrieve
top-k relations that are compatible with the type/s
</bodyText>
<page confidence="0.977416">
1108
</page>
<bodyText confidence="0.999869548387097">
of e1 and the given relation hint. Moreover, the
remaining noise is further mitigated by the collec-
tive scoring in the graphical model. While we may
miss relations if they are expressed in the query
through obscure hints, allowing the relation to be
⊥ acts as a safety net.
Using Freebase relation names: As mentioned
earlier, queries may express relations differently as
compared to the corpus. A relation model based
solely on corpus annotations may not be able to
bridge that gap effectively, particularly so, because
of sparsity of corpus annotations or the rarity of
Freebase triples in ClueWeb. E.g., for the Freebase
relation /people/person/profession, we
found very few annotated sentences. One way
to address this problem is to utilize relation type
names in Freebase to map hints to relation types.
Thus, in addition to the corpus-derived relation
model, we also built a language model that used
Freebase relation type names as lemmas. E.g., the
word ‘profession’ would contribute to the relation
type /people/person/profession.
Our relation models are admittedly simple. This
is mainly because telegraphic queries may ex-
press relations very differently from natural lan-
guage text. As it is difficult to ensure precision of
query interpretation stage, our models are geared
towards recall. The system generates a large num-
ber of interpretations and relies on signals from
the corpus and KG to bring forth correct interpre-
tations.
</bodyText>
<subsubsectionHeader confidence="0.85839">
4.2.2 Type language model for ΨT2
</subsubsectionHeader>
<bodyText confidence="0.999926361702128">
Similar to the relation language model, we need
a type language model to measure compatibil-
ity between t2 and t2(q, z). Estimating the tar-
get entity type, without over-generalizing or over-
specifying it, has always been important for QA.
E.g., when t2 is ‘city’, a good type language model
should prefer t2 as /location/citytown
over /location/location while avoiding
/location/es_autonomous_city.
A catalog like Freebase suggests a straight-
forward method to collect a type language model.
Each type is described by one or more phrases
through the link /common/topic/alias. We
can collect these into a micro-‘document’ and
use a standard Dirichlet-smoothed language model
from IR (Zhai, 2008). In Freebase, an entity
node (e.g., Einstein, /m/0jcx) may be linked
to a type node (e.g. /base/scientist/
physicist) using an edge with label /type/
object/type.
But relation types provide additional clues to
types of the endpoint entities. Freebase relation
types have the form /x/y/z, where x is the
domain of the relation, and y and z are string
representations of the type of the entities partic-
ipating in the relation. E.g., the (directed) re-
lation type /location/country/capital
connects from from /location/country to
/location/citytown. Therefore, “capital”
can be added to the set of descriptive phrases of
entity type /location/citytown.
It is important to note that while we use Free-
base link nomenclature for relation and type lan-
guage models, our models are not incompati-
ble with other catalogs. Indeed, most catalogs
have established ways of deriving language mod-
els that describe their various structures. For ex-
ample, most YAGO types are derived from Word-
Net synsets with associated phrasal descriptions
(lemmas). YAGO relations also have readable
names such as actedIn, isMarriedTo, etc. which
can be used to estimate language models. DB-
Pedia relations are mostly derived from (mean-
ingfully) named attributes taken from infoboxes,
hence they can be used directly. Furthermore, oth-
ers (Wu and Weld, 2007) have shown how to asso-
ciate language models with such relations.
</bodyText>
<subsectionHeader confidence="0.52029">
4.2.3 Snippet scoring
</subsectionHeader>
<bodyText confidence="0.99990865">
The factor ΨE1,R,E2,S(q, z, e1, r, e2) should be
large if many snippets contain a mention of e1 and
e2, relation r, and many high-signal words from s.
Recall that we begin with a corpus annotated with
entity mentions. Our corpus is not directly anno-
tated with relation mentions. Therefore, we get
from relations to documents via high-confidence
phrases. Snippets are retrieved using a combined
entity + word index, and scored for a given e1, r,
e2, and selectors s(q, z).
Given that relation phrases may be noisy and
that their occurrence in the snippet may not nec-
essarily mean that the given relation is being ex-
pressed, we need a scoring function that is cog-
nizant of the roles of relation phrases and enti-
ties occurring in the snippets. In a basic ver-
sion, e1, p, e2, &apos;g are used to probe a combined en-
tity+word index to collect high scoring snippets,
with the score being adapted from BM25. The sec-
ond, refined scoring function used a RankSVM-
</bodyText>
<page confidence="0.952351">
1109
</page>
<bodyText confidence="0.52363">
style (Joachims, 2002) optimization.
</bodyText>
<equation confidence="0.99328825">
min I IλI I2 + C Ee+,e− ξe+,e− s.t.
λ,ξ
be+, e− : λ · f(q, De+, e+) + ξe+,e− (3)
&gt; λ · f(q, De−, e−) + 1.
</equation>
<bodyText confidence="0.9688446">
where e+ and e− are positive and negative enti-
ties for the query q and f(q, De, e) represents the
feature map for the set of snippets De belonging
to entity e. The assumption here is that all snip-
pets containing e+ are “positive” snippets for the
query. f consolidates various signals like the num-
ber of snippets where e occurs near query entity
e1 and a relation phrase, or the number of snippets
with high proportion of query IDF, hinting that e
is a positive entity for the given query. A partial
</bodyText>
<figureCaption confidence="0.5541965">
list of features used for snippet scoring is given in
Figure 4.
</figureCaption>
<figure confidence="0.812534857142857">
Number of snippets with distance(e2, el) &lt; k1 (k1 = 5, 10)
Number of snippets with distance(e2, relation phrase) &lt; k2
(k2 = 3, 6)
Number of snippets with relation r = ⊥
Number of snippets with relation phrases as prepositions
Number of snippets covering fraction of query IDF &gt; k3
(k3 = 0.2, 0.4, 0.6, 0.8)
</figure>
<figureCaption confidence="0.9794115">
Figure 4: Sample features used for learning
weights λ to score snippets.
</figureCaption>
<subsubsectionHeader confidence="0.428888">
4.2.4 Query entity model
</subsubsectionHeader>
<bodyText confidence="0.998525666666667">
Potential ΨE1(q, z, e1) captures the compatibil-
ity between el(q, z) (i.e., the words that mention
e1) and the claimed entity e1 mentioned in the
query. We used the TagMe entity linker (Fer-
ragina and Scaiella, 2010) for annotating enti-
ties in queries. TagMe annotates the query with
Wikipedia entities, which we map to Freebase, and
use the annotation confidence scores as the poten-
tial ΨE1(q, z, e1).
</bodyText>
<subsectionHeader confidence="0.9956145">
4.3 Discriminative parameter training with
latent variables
</subsectionHeader>
<bodyText confidence="0.99987325">
We first set the potentials in (1) as explained in
§4.2 (henceforth called ‘Unoptimized’), and got
encouraging accuracy. Then we rewrote each po-
tential as
</bodyText>
<equation confidence="0.997384">
Ψ•(···) = exp(w• · φ•(· · · )� (4)
or log H• Ψ•(···) = E• w• · φ•(· · · ),
</equation>
<bodyText confidence="0.938095909090909">
with w• being a weight vector for a specific poten-
tial •, and φ• being a corresponding feature vector.
During inference, we seek to maximize
max w · φ(q, z, e1, t2, r, e2), (5)
q,z,e1,t2,r
for a fixed w, to find the score of each candidate
entity e2. Here all w• and φ• have been collected
into unified weight and feature vectors w, φ. Dur-
ing training of w, we are given pairs of correct and
incorrect answer entities e+2 , e−2 , and we wish to
satisfy constraints of the form
</bodyText>
<equation confidence="0.99854">
w · φ(q, z, e1, t2, r, e+2 ) + ξ (6)
w · φ(q, z, e1, t2, r, e−2 ),
</equation>
<bodyText confidence="0.999868">
because collecting e+2 , e−2 pairs is less work than
supervising with values of z, e1, t2, r, e2 for each
query. Similar distant supervision problems were
posed via bundle method by (Bergeron et al.,
2008), and (Yu and Joachims, 2009), who used
CCCP (Yuille and Rangarajan, 2006). These are
equivalent in our setting. We use the CCCP style,
and augment the objective with an additional en-
tropy term as in (Sawant and Chakrabarti, 2013).
We call this LVDT (latent variable discriminative
training) in §5.
</bodyText>
<sectionHeader confidence="0.999271" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.893726">
5.1 Testbed
</subsectionHeader>
<bodyText confidence="0.999851142857143">
Corpus and knowledge graph: We used the
ClueWeb09B (ClueWeb09, 2009) corpus contain-
ing 50 million Web documents. This corpus
was annotated by Google with Freebase enti-
ties (Gabrilovich et al., 2013). The average page
contains 15 entity annotations from Freebase. We
used the Freebase KG and its links to Wikipedia.
Queries: We report on two sets of entity-seeking
queries. A sample of about 800 well-formed
queries from WebQuestions (Berant et al., 2013)
were converted to telegraphic utterances (such as
would be typed into commercial search engines)
by volunteers familiar with Web search. We call
this WQT (WebQuestions, telegraphic). Queries
are accompanied by ground truth entities. The
second data set, TREC-INEX, from (Sawant and
Chakrabarti, 2013) has about 700 queries sam-
pled from TREC and INEX, available at http:
//bit.ly/WSpxvr. These come with well-
formed and telegraphic utterances, as well as
ground truth entities.
</bodyText>
<equation confidence="0.56974825">
max
q,z,e1,t2,r
&gt; 1 + max
q,z,e1,t2,r
</equation>
<page confidence="0.938925">
1110
</page>
<bodyText confidence="0.9999863">
There are some notable differences between
these query sets. For WQT, queries were gener-
ated by using Google’s query suggestions inter-
face. Volunteers were asked to find answers using
single Freebase pages. Therefore, by construction,
queries retained can be answered using the Free-
base KG alone, with a simple r(e1, ?) form. In
contrast, TREC-INEX queries provide a balanced
mix of t2 and r hints in the queries, and direct an-
swers from triples is relatively less available.
</bodyText>
<subsectionHeader confidence="0.999634">
5.2 Implementation details
</subsectionHeader>
<bodyText confidence="0.99994955">
On an average, the pseudocode in Figure 2
generated 13 segmentations per query, with
longer queries generating more segmentations
than shorter ones.
We used an MG4J (Boldi and Vigna, 2005)
based query processor, written in Java, over en-
tity and word indices on ClueWeb09B. The in-
dex supplies snippets with a specified maximum
width, containing a mention of some entity and
satisfying a WAND (Broder et al., 2003) predi-
cate over words in s. In case of phrases in the
query, the WAND threshold was computed by
adding the IDF of constituent words. The index
returned about 330,000 snippets on average for
WAND threshold of 0.6.
We retained the top 200 candidate entities from
the corpus; increasing this horizon did not give
benefits. We also considered as candidates for e2
those entities that are adjacent to e1 in the KG
via top-scoring r candidates. In order to gener-
ate supporting snippets for an interpretation con-
taining entity annotation e, we need to match e
with Google’s corpus annotations. However, re-
lying solely on corpus annotations fails to retrieve
many potential evidence snippets, because entity
annotations are sparse. Therefore we probed the
token index with the textual mention of e1 in the
query; this improved recall.
We also investigated the feasibility of our pro-
posals for interactive search. There are three major
processes involved in answering a query - gener-
ating potential interpretations, collecting/scoring
snippets, and inference (MAP for Unoptimized
and wo(·) for LVDT). For the WQT dataset, av-
erage time per query for each stage was approx-
imately - 0.2, 16.6 and 1.3 seconds respectively.
Our (Java) code did not optimize the bottleneck
at all; only 10 hosts and no clever load balancing
were used. We believe commercial search engines
can cut this down to less than a second.
</bodyText>
<subsectionHeader confidence="0.998176">
5.3 Research questions
</subsectionHeader>
<bodyText confidence="0.993564">
In the rest of this section we will address these
questions:
</bodyText>
<listItem confidence="0.985799">
• For telegraphic queries, is our entity-relation-
type-selector segmentation better than the
type-selector segmentation of (Sawant and
Chakrabarti, 2013)?
• When semantic parsers (Berant et al., 2013;
Kwiatkowski et al., 2013) are subjected to
telegraphic queries, how do they perform
compared to our proposal?
• Are the KG and corpus really complementary
as regards their support of accurate ranking of
candidate entities?
• Is the prediction of r and t2 from our ap-
proach better than a greedy assignment based
on local language models?
</listItem>
<bodyText confidence="0.8498395">
We also discuss anecdotes of successes and fail-
ures of various systems.
</bodyText>
<subsectionHeader confidence="0.999145">
5.4 Benefits of relation in addition to type
</subsectionHeader>
<bodyText confidence="0.999945333333333">
Figure 5 shows entity-ranking MAP, MRR, and
NDCG@10 (n@10) for two data sets and vari-
ous systems. “No interpretation” is an IR baseline
without any KG. Type+selector is our implemen-
tation of (Sawant and Chakrabarti, 2013). Unopti-
mized and LVDT both beat “no interpretation” and
“type+selector” by wide margins. (Boldface im-
plies best performing formulation.) There are two
notable differences between S&amp;C and our work.
First, S&amp;C do not use the knowledge graph (KG)
and rely on a noisy corpus. This means S&amp;C fails
to answer queries whose answers are found only
in KG. This can be seen from WQT results; they
perform only slightly better than the baseline. Sec-
ond, even for queries that can be answered through
the corpus alone, S&amp;C miss out on two important
signals that the query may provide - namely the
query entity and the relation. Our framework not
only provides a way to use a curated and high pre-
cision knowledge graph but also attempts to pro-
vide more reachability to corpus by the use of re-
lational phrases.
In case of TREC-INEX, LVDT improves upon
the unoptimized graphical model, where for WQT,
it does not. Preliminary inspection suggests this is
because WQT has noisy and incomplete ground
truth, and LVDT trains to the noise; a non-convex
</bodyText>
<page confidence="0.972908">
1111
</page>
<table confidence="0.999677888888889">
Dataset Formulation map mrr n@10
No interpretation .205 .215 .292
TREC Type+selector .292 .306 .356
-INEX Unoptimized .409 .419 .502
LVDT .419 .436 .541
No interpretation .080 .095 .131
WQT Type+selector .116 .152 .201
Unoptimized .377 .401 .474
LVDT .295 .323 .406
</table>
<figureCaption confidence="0.978093">
Figure 5: ‘Entity-relation-type-selector’ segmen-
tation yields better accuracy than ‘type-selector’
segmentation.
</figureCaption>
<bodyText confidence="0.6319865">
objective makes matters worse. The bias in our
unoptimized model circumvents training noise.
</bodyText>
<subsectionHeader confidence="0.999333">
5.5 Comparison with semantic parsers
</subsectionHeader>
<bodyText confidence="0.999630133333333">
For TREC-INEX, both unoptimized and LVDT
beat SEMPRE (Berant et al., 2013) convinc-
ingly, whether it is trained with Free917 or Web-
Questions (Figure 6).
SEMPRE’s relatively poor performance, in this
case, is explained by its complete reliance on the
knowledge graph. As discussed previously, the
TREC-INEX dataset contains a sizable proportion
of queries that may be difficult to answer using
a KG alone. When SEMPRE is compared with
our systems with a telegraphic sample of Web-
Questions (WQT), results are mixed. Our Unop-
timized model still compares favorably to SEM-
PRE, but with slimmer gains. As before, LVDT
falls behind.
</bodyText>
<table confidence="0.9996334">
Dataset Formulation map mrr n@10
SEMPRE(Free917) .154 .159 .186
TREC SEMPRE(WQ) .197 .208 .247
-INEX Unoptimized .409 .419 .502
LVDT .419 .436 .541
SEMPRE(Free917) .229 .255 .285
WQT SEMPRE(WQ) .374 .406 .449
Unoptimized .377 .401 .474
Jacana .239 .256 .329
LVDT .295 .323 .406
</table>
<figureCaption confidence="0.992755">
Figure 6: Comparison with semantic parsers.
</figureCaption>
<bodyText confidence="0.999892058823529">
Our smaller gains over SEMPRE in case of
WebQuestions is explained by how WebQuestions
was assembled (Berant et al., 2013). Although
Google’s query suggestions gave an eclectic pool,
only those queries survived that could be answered
using a single Freebase page, which effectively re-
duced the role of a corpus. In fact, a large frac-
tion of WQT queries cannot be answered well us-
ing the corpus alone, because FACC1 annotations
are too sparse and rarely cover common nouns and
phrases such as ‘democracy’ or ‘drug overdose’
which are needed for some WQT queries.
For WQT, our system also compares favorably
with Jacana (Yao and Van Durme, 2014). Given
that they subject their input to natural langauge
parsing, their relatively poor performance is not
unsurprsing.
</bodyText>
<subsectionHeader confidence="0.975486">
5.6 Complementary benefits of KG &amp; corpus
</subsectionHeader>
<bodyText confidence="0.9163275">
Figure 7 shows the synergy between the corpus
and the KG. In all cases and for all metrics, using
the corpus and KG together gives superior perfor-
mance to using any of them alone. However, it
is instructive that in case of TREC-INEX, corpus-
only is better than KG-only, whereas this is re-
versed for WQT, which also supports the above
argument.
</bodyText>
<table confidence="0.999695153846154">
Data Formulation map mrr n@10
TREC-INEX Unoptimized (KG) .201 .209 .241
Unoptimized (Corpus) .381 .388 .471
Unoptimized (Both) .409 .419 .502
LVDT (KG only) .255 .264 .293
LVDT (Corpus) .267 .272 .315
LVDT (Both) .419 .436 .541
WQT Unoptimized (KG) .329 .343 .394
Unoptimized (Corpus) .188 .228 .291
Unoptimized (Both) .377 .401 .474
LVDT (KG only) .257 .281 .345
LVDT (Corpus only) .170 .210 .280
LVDT (Both) .295 .323 .406
</table>
<figureCaption confidence="0.972626">
Figure 7: Synergy between KB and corpus.
</figureCaption>
<subsectionHeader confidence="0.95979">
5.7 Collective vs. greedy segmentation
</subsectionHeader>
<bodyText confidence="0.9696118">
To judge the quality of interpretations, we asked
paid volunteers to annotate queries with an appro-
priate relation and type, and compared them with
the interpretations associated with top-ranked en-
tities. Results in Figure 8 indicate that in spite
of noisy relation and type language models, our
formulations produce high quality interpretations
through collective inference.
Figure 9 demonstrates the benefit of collective
inference over greedy segmentation followed by
</bodyText>
<page confidence="0.959395">
1112
</page>
<table confidence="0.906145">
Formulation Type Relation Type/Rel
Unoptimized (top 1) 23 49 60
Unoptimized (top 5) 29 57 68
LVDT (top 1) 25 52 61
LVDT (top 5) 33 61 69
</table>
<figureCaption confidence="0.8830995">
Figure 8: Fraction of queries (%) with correct in-
terpretations of t2, r, and t2 or r, on TREC-INEX.
</figureCaption>
<bodyText confidence="0.3943665">
evaluation. Collective inference boosts absolute
MAP by as much as 0.2.
</bodyText>
<table confidence="0.999546555555555">
Dataset Formulation map mrr n@10
Unoptimized (greedy) .343 .347 .432
TREC Unoptimized .409 .419 .502
-INEX LVDT (greedy) 205 .214 .259
LVDT .419 .436 .541
Unoptimized (greedy) .246 .271 .335
Unoptimized .377 .401 .474
WQT LVDT (greedy) .212 .246 .317
LVDT .295 .323 .406
</table>
<figureCaption confidence="0.995332">
Figure 9: Collective vs. greedy segmentation
</figureCaption>
<subsectionHeader confidence="0.686002">
5.8 Discussion
</subsectionHeader>
<bodyText confidence="0.999508923076923">
Closer scrutiny revealed that collective infer-
ence often overcame errors in earlier stages
to produce a correct ranking over answer en-
tities. E.g., for the query automobile com-
pany makes spider the entity disambiguation
stage fails to identify the car Alfa Romeo Spi-
der (/m/08ys39). However, the interpretation
stage recovers from the error and segments the
query with Automobile (/m/0k4j) as the query
entity e1, /organization/organization
and /business/industry/companies as
target type t2 and relation r respectively (from the
relation/type hint ‘company’), and spider as se-
</bodyText>
<figureCaption confidence="0.993879">
Figure 10: Comparison of various approaches for
NDCG at rank 1 to 10, TREC-INEX dataset
Figure 11: Comparison of various approaches for
NDCG at rank 1 to 10, WQT dataset
</figureCaption>
<bodyText confidence="0.999348666666667">
lector to arrive at the correct answer Alfa Romeo
(/m/09c50). The corpus features also play a cru-
cial role for queries which may not be accurately
represented with an appropriate logical formula.
For the query meg ryan bookstore movie, the
textual patterns for the relation ActedTn in con-
junction with the selector word ‘bookstore’ cor-
rectly identifies the answer entity You’ve Got Mail
(/m/014zwb).
We also analyzed samples of queries where
our system did not perform particularly well.
We observed that one of the recurring themes
of these queries was that their answer enti-
ties had very little corpus support, and the
type/relation hint mapped to too many or no
candidate type/relations. For example, in the
query south africa political system, the rel-
evant type/relation hint ‘political system’ could
not be mapped to /government/form_of_
government and /location/country/
form_of_government respectively.
</bodyText>
<sectionHeader confidence="0.990236" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999955071428572">
We presented a technique to partition telegraphic
entity-seeking queries into functional segments
and to rank answer entities accordingly. While
our results are favorable compared to strong prior
art, further improvements may result from relax-
ing our model to recognize multiple e1s and rs. It
may also help to deploy more sophisticated para-
phrasing models (Berant and Liang, 2014) or word
embeddings (Yih et al., 2014) for relation hints.
It would also be interesting to supplement entity-
linked corpora and curated KGs with extracted
triples (Fader et al., 2014). Another possibility is
to apply the ideas presented here to well-formed
questions.
</bodyText>
<page confidence="0.985719">
1113
</page>
<sectionHeader confidence="0.995842" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999757504132232">
Jonathan Berant and Percy Liang. 2014. Semantic
parsing via paraphrasing. In ACL Conference.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Empirical Methods in
Natural Language Processing (EMNLP).
Charles Bergeron, Jed Zaretzki, Curt Breneman, and
Kristin P. Bennett. 2008. Multiple instance ranking.
In ICML, pages 48–55. ACM.
Paolo Boldi and Sebastiano Vigna. 2005. MG4J at
TREC 2005. In Ellen M. Voorhees and Lori P. Buck-
land, editors, TREC, number SP 500-266 in Special
Publications. NIST.
Andrei Z. Broder, David Carmel, Michael Herscovici,
Aya Soffer, and Jason Zien. 2003. Efficient query
evaluation using a two-level retrieval process. In
CIKM, pages 426–434. ACM.
Tao Cheng and Kevin Chen-Chuan Chang. 2010. Be-
yond pages: supporting efficient, scalable entity
search with dual-inversion index. In EDBT. ACM.
ClueWeb09. 2009. http://www.
lemurproject.org/clueweb09.php/.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2014. Open question answering over curated and
extracted knowledge bases. In SIGKDD Confer-
ence.
Paolo Ferragina and Ugo Scaiella. 2010. TAGME:
on-the-fly annotation of short text fragments (by
wikipedia entities). CoRR/arXiv, abs/1006.3498.
http://arxiv.org/abs/1006.3498.
Evgeniy Gabrilovich, Michael Ringgaard, and Amar-
nag Subramanya. 2013. FACC1: Free-
base annotation of ClueWeb corpora. http://
lemurproject.org/clueweb12/, June. Ver-
sion 1 (Release date 2013-06-26, Format version 1,
Correction level 0).
Sean Gallagher. 2012. How Google and Microsoft
taught search to ‘understand’ the Web. ArsTechnica
article. http://goo.gl/NWs0zT.
Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009.
Named entity recognition in query. In SIGIR Con-
ference, pages 267–274. ACM.
Johan Hall, Jens Nilsson, and Joakim Nivre. 2014.
Maltparser. http://www.maltparser.org/.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In SIGKDD Conference,
pages 133–142. ACM.
Gjergji Kasneci, Fabian M. Suchanek, Georgiana Ifrim,
Maya Ramanath, and Gerhard Weikum. 2008.
NAGA: Searching and ranking knowledge. In
ICDE. IEEE.
Daphne Koller and Nir Friedman. 2009. Probabilistic
Graphical Models: Principles and Techniques. MIT
Press.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and
Luke S. Zettlemoyer. 2013. Scaling seman-
tic parsers with on-the-fly ontology matching. In
EMNLP Conference, pages 1545–1556.
Xiaonan Li, Chengkai Li, and Cong Yu. 2010. Enti-
tyEngine: Answering entity-relationship queries us-
ing shallow semantics. In CIKM, October. (demo).
Yanen Li, Bo-Jun Paul Hsu, ChengXiang Zhai, and
Kuansan Wang. 2011. Unsupervised query segmen-
tation using clickthrough for information retrieval.
In SIGIR Conference, pages 285–294. ACM.
Percy Liang. 2013. Lambda dependency-
based compositional semantics. Technical Report
arXiv:1309.4408, Stanford University. http://
arxiv.org/abs/1309.4408.
Thomas Lin, Patrick Pantel, Michael Gamon, Anitha
Kannan, and Ariel Fuxman. 2012. Active objects:
Actions for entity-centric search. In WWW Confer-
ence, pages 589–598. ACM.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. PATTY: A taxonomy of relational
patterns with semantic types. In EMNLP Confer-
ence, EMNLP-CoNLL ’12, pages 1135–1145. ACL.
Patrick Pantel, Thomas Lin, and Michael Gamon.
2012. Mining entity types from query logs via user
intent modeling. In ACL Conference, pages 563–
571, Jeju Island, Korea, July.
Fernando Pereira. 2013. Meaning in the
wild. Invited talk at EMNLP Conference.
http://hum.csse.unimelb.edu.au/
emnlp2013/invited-talks.html.
Jeffrey Pound, Alexander K. Hudek, Ihab F. Ilyas, and
Grant Weddell. 2012. Interpreting keyword queries
over Web knowledge bases. In CIKM.
Nikos Sarkas, Stelios Paparizos, and Panayiotis
Tsaparas. 2010. Structured annotations of Web
queries. In SIGMOD Conference.
Uma Sawant and Soumen Chakrabarti. 2013. Learn-
ing joint query interpretation and response ranking.
In WWW Conference, Brazil.
Fei Wu and Daniel S Weld. 2007. Automatically se-
mantifying Wikipedia. In CIKM, pages 41–50.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural language questions for the
Web of data. In EMNLP Conference, pages 379–
390, Jeju Island, Korea, July.
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with Freebase. In ACL Conference. ACL.
Xuchen Yao, Jonathan Berant, and Benjamin Van
Durme. 2014. Freebase QA: Information extrac-
tion or semantic parsing? In ACL 2014 Workshop
on Semantic Parsing (SP14).
Wen-tau Yih, Xiaodong He, and Christopher Meek.
2014. Semantic parsing for single-relation question
answering. In ACL Conference. ACL.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables. In
ICML, pages 1169–1176. ACM.
A. L. Yuille and Anand Rangarajan. 2006. The
concave-convex procedure. Neural Computation,
15(4):915–936.
ChengXiang Zhai. 2008. Statistical language models
for information retrieval: A critical review. Founda-
tions and Trends in Information Retrieval, 2(3):137–
213, March.
</reference>
<page confidence="0.996226">
1114
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.219075">
<title confidence="0.995235">Knowledge Graph and Corpus Driven Segmentation and Answer Inference for Telegraphic Entity-seeking Queries</title>
<author confidence="0.998607">Joshi Sawant Soumen Chakrabarti</author>
<affiliation confidence="0.997316">IBM Research IIT Bombay, Yahoo Labs IIT</affiliation>
<email confidence="0.358527">mandarj90@in.ibm.comuma@cse.iitb.ac.insoumen@cse.iitb.ac.in</email>
<abstract confidence="0.98390872972973">Much recent work focuses on formal interpretation of natural question utterances, with the goal of executing the resulting structured queries on knowledge graphs (KGs) such as Freebase. Here we address two limitations of this approach when applied to open-domain, entity-oriented Web queries. First, Web queries are rarely wellformed questions. They are “telegraphic”, with missing verbs, prepositions, clauses, case and phrase clues. Second, the KG is always incomplete, unable to directly answer many queries. We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to segment: a base entity a relatype a target entity type and words The query seeks enfurther evidenced by schema-agnostic words Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG. We do not trust the best or any specific query segmentation. Instead, eviin favor of candidate are aggregated across several segmentations. Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG, using over a thousand telegraphic queries adapted from TREC, INEX, and Web- Questions, show the efficacy of our approach. For one benchmark, MAP improves from 0.2–0.29 (competitive baselines) to 0.42 (our system). NDCG@10 improves from 0.29–0.36 to 0.54.</abstract>
<intro confidence="0.671187">done as Masters student at IIT Bombay</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing via paraphrasing. In</title>
<date>2014</date>
<booktitle>ACL Conference.</booktitle>
<contexts>
<context position="17864" citStr="Berant and Liang, 2014" startWordPosition="2957" endWordPosition="2960">between r(q, z) and the proposed relation r. E.g., if the query is steve jobs death reason, and r is (correctly chosen as) death reason, then the correct candidate r is /people/ deceased_person/cause _aof _ddeath. An incorrect r is /people/decese person/ place_of_death. An incorrect z may lead to r(q, z) being jobs death. Using corpus: Considerable variation may exist in how r is represented textually in a query. The relation language model needs to build a bridge between the formal r and the textual r, so that (un)likely r’s have (small) large potential. Many approaches (Berant et al., 2013; Berant and Liang, 2014; Kwiatkowski et al., 2013; Yih et al., 2014) to this problem have been intensely studied recently. Given our need to process billions of Web pages efficiently, we chose a pattern-based approach (Nakashole et al., 2012): with each r, discover the most strongly associated phrase patterns from a reference corpus, then mark these patterns into much larger payload corpus. We started with the 2000 (out of approximately 14000) most frequent relation types in Freebase, and the ClueWeb09 corpus annotated with Freebase entities (Gabrilovich et al., 2013). For each triple instance of each relation type,</context>
</contexts>
<marker>Berant, Liang, 2014</marker>
<rawString>Jonathan Berant and Percy Liang. 2014. Semantic parsing via paraphrasing. In ACL Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Andrew Chou</author>
<author>Roy Frostig</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing on Freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="3263" citStr="Berant et al., 2013" startWordPosition="492" endWordPosition="495"> river is the Hermitage Museum located” may be translated to the telegraphic Web query hermitage museum river bank. Even on well-formed question utterances, 50% of interpretation failures are contributed by parsing or structural matching failures (Kwiatkowski et al., 2013). Telegraphic utterances will generally be even more challenging. Consequently, whereas TREC-QA/NLP-style research has focused on parsing and precise interpretation of a well-formed query sentence to a strongly structured (typically graph-oriented) query language (Kasneci et al., 2008; Pound et al., 2012; Yahya et al., 2012; Berant et al., 2013; Kwiatkowski et al., 2013), the Web search and information retrieval (IR) community has focused on telegraphic queries (Guo et al., 2009; Sarkas et al., 2010; Li et al., 2011; Pantel et al., 2012; Lin et al., 2012; Sawant and Chakrabarti, 2013). In terms of target schema richness, these efforts may 1104 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1104–1114, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics appear more modest. The act of query ‘interpretation’ is mainly a segmentation of query tokens by </context>
<context position="7885" citStr="Berant et al., 2013" startWordPosition="1247" endWordPosition="1250"> work in more detail. §3 gives some examples fitting our query template, explains why interpreting some of them is nontrivial, and sets up notation. §4 presents our core technical contributions. §5 presents experiments. Data can be accessed at http://bit.ly/Spva49 and http://bit.ly/WSpxvr. 2 Related work The NLP/QA community has traditionally assumed that question utterances are grammatically well-formed, from which precise clause structure, ground constants, variables, and connective relations can be inferred via semantic parsing (Kasneci et al., 2008; Pound et al., 2012; Yahya et al., 2012; Berant et al., 2013; Kwiatkowski et al., 2013) and translated to lambda expressions (Liang, 2013) or SPARQL style queries (Kasneci et al., 2008), with elaborate schema knowledge. Such approaches are often correlated with the assumption that all usable knowledge has been curated into a KG. The query is first translated to a structured form and then “executed” on the KG. A 1105 Telegraphic query el r t2 s first african american nobel prize winner nobel prize winner african american first nobel prize - winner first african american - - winner first african american nobel prize dave navarro first band dave navarro b</context>
<context position="17840" citStr="Berant et al., 2013" startWordPosition="2953" endWordPosition="2956">es the compatibility between r(q, z) and the proposed relation r. E.g., if the query is steve jobs death reason, and r is (correctly chosen as) death reason, then the correct candidate r is /people/ deceased_person/cause _aof _ddeath. An incorrect r is /people/decese person/ place_of_death. An incorrect z may lead to r(q, z) being jobs death. Using corpus: Considerable variation may exist in how r is represented textually in a query. The relation language model needs to build a bridge between the formal r and the textual r, so that (un)likely r’s have (small) large potential. Many approaches (Berant et al., 2013; Berant and Liang, 2014; Kwiatkowski et al., 2013; Yih et al., 2014) to this problem have been intensely studied recently. Given our need to process billions of Web pages efficiently, we chose a pattern-based approach (Nakashole et al., 2012): with each r, discover the most strongly associated phrase patterns from a reference corpus, then mark these patterns into much larger payload corpus. We started with the 2000 (out of approximately 14000) most frequent relation types in Freebase, and the ClueWeb09 corpus annotated with Freebase entities (Gabrilovich et al., 2013). For each triple instanc</context>
<context position="27404" citStr="Berant et al., 2013" startWordPosition="4578" endWordPosition="4581">ve with an additional entropy term as in (Sawant and Chakrabarti, 2013). We call this LVDT (latent variable discriminative training) in §5. 5 Experiments 5.1 Testbed Corpus and knowledge graph: We used the ClueWeb09B (ClueWeb09, 2009) corpus containing 50 million Web documents. This corpus was annotated by Google with Freebase entities (Gabrilovich et al., 2013). The average page contains 15 entity annotations from Freebase. We used the Freebase KG and its links to Wikipedia. Queries: We report on two sets of entity-seeking queries. A sample of about 800 well-formed queries from WebQuestions (Berant et al., 2013) were converted to telegraphic utterances (such as would be typed into commercial search engines) by volunteers familiar with Web search. We call this WQT (WebQuestions, telegraphic). Queries are accompanied by ground truth entities. The second data set, TREC-INEX, from (Sawant and Chakrabarti, 2013) has about 700 queries sampled from TREC and INEX, available at http: //bit.ly/WSpxvr. These come with wellformed and telegraphic utterances, as well as ground truth entities. max q,z,e1,t2,r &gt; 1 + max q,z,e1,t2,r 1110 There are some notable differences between these query sets. For WQT, queries we</context>
<context position="30520" citStr="Berant et al., 2013" startWordPosition="5078" endWordPosition="5081">ptimized and wo(·) for LVDT). For the WQT dataset, average time per query for each stage was approximately - 0.2, 16.6 and 1.3 seconds respectively. Our (Java) code did not optimize the bottleneck at all; only 10 hosts and no clever load balancing were used. We believe commercial search engines can cut this down to less than a second. 5.3 Research questions In the rest of this section we will address these questions: • For telegraphic queries, is our entity-relationtype-selector segmentation better than the type-selector segmentation of (Sawant and Chakrabarti, 2013)? • When semantic parsers (Berant et al., 2013; Kwiatkowski et al., 2013) are subjected to telegraphic queries, how do they perform compared to our proposal? • Are the KG and corpus really complementary as regards their support of accurate ranking of candidate entities? • Is the prediction of r and t2 from our approach better than a greedy assignment based on local language models? We also discuss anecdotes of successes and failures of various systems. 5.4 Benefits of relation in addition to type Figure 5 shows entity-ranking MAP, MRR, and NDCG@10 (n@10) for two data sets and various systems. “No interpretation” is an IR baseline without </context>
<context position="32819" citStr="Berant et al., 2013" startWordPosition="5453" endWordPosition="5456">ains to the noise; a non-convex 1111 Dataset Formulation map mrr n@10 No interpretation .205 .215 .292 TREC Type+selector .292 .306 .356 -INEX Unoptimized .409 .419 .502 LVDT .419 .436 .541 No interpretation .080 .095 .131 WQT Type+selector .116 .152 .201 Unoptimized .377 .401 .474 LVDT .295 .323 .406 Figure 5: ‘Entity-relation-type-selector’ segmentation yields better accuracy than ‘type-selector’ segmentation. objective makes matters worse. The bias in our unoptimized model circumvents training noise. 5.5 Comparison with semantic parsers For TREC-INEX, both unoptimized and LVDT beat SEMPRE (Berant et al., 2013) convincingly, whether it is trained with Free917 or WebQuestions (Figure 6). SEMPRE’s relatively poor performance, in this case, is explained by its complete reliance on the knowledge graph. As discussed previously, the TREC-INEX dataset contains a sizable proportion of queries that may be difficult to answer using a KG alone. When SEMPRE is compared with our systems with a telegraphic sample of WebQuestions (WQT), results are mixed. Our Unoptimized model still compares favorably to SEMPRE, but with slimmer gains. As before, LVDT falls behind. Dataset Formulation map mrr n@10 SEMPRE(Free917) </context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Bergeron</author>
<author>Jed Zaretzki</author>
<author>Curt Breneman</author>
<author>Kristin P Bennett</author>
</authors>
<title>Multiple instance ranking.</title>
<date>2008</date>
<booktitle>In ICML,</booktitle>
<pages>48--55</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="26625" citStr="Bergeron et al., 2008" startWordPosition="4453" endWordPosition="4456">k to maximize max w · φ(q, z, e1, t2, r, e2), (5) q,z,e1,t2,r for a fixed w, to find the score of each candidate entity e2. Here all w• and φ• have been collected into unified weight and feature vectors w, φ. During training of w, we are given pairs of correct and incorrect answer entities e+2 , e−2 , and we wish to satisfy constraints of the form w · φ(q, z, e1, t2, r, e+2 ) + ξ (6) w · φ(q, z, e1, t2, r, e−2 ), because collecting e+2 , e−2 pairs is less work than supervising with values of z, e1, t2, r, e2 for each query. Similar distant supervision problems were posed via bundle method by (Bergeron et al., 2008), and (Yu and Joachims, 2009), who used CCCP (Yuille and Rangarajan, 2006). These are equivalent in our setting. We use the CCCP style, and augment the objective with an additional entropy term as in (Sawant and Chakrabarti, 2013). We call this LVDT (latent variable discriminative training) in §5. 5 Experiments 5.1 Testbed Corpus and knowledge graph: We used the ClueWeb09B (ClueWeb09, 2009) corpus containing 50 million Web documents. This corpus was annotated by Google with Freebase entities (Gabrilovich et al., 2013). The average page contains 15 entity annotations from Freebase. We used the </context>
</contexts>
<marker>Bergeron, Zaretzki, Breneman, Bennett, 2008</marker>
<rawString>Charles Bergeron, Jed Zaretzki, Curt Breneman, and Kristin P. Bennett. 2008. Multiple instance ranking. In ICML, pages 48–55. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paolo Boldi</author>
<author>Sebastiano Vigna</author>
</authors>
<title>MG4J at TREC</title>
<date>2005</date>
<booktitle>SP 500-266 in Special</booktitle>
<editor>In Ellen M. Voorhees and Lori P. Buckland, editors, TREC, number</editor>
<publisher>Publications. NIST.</publisher>
<contexts>
<context position="28615" citStr="Boldi and Vigna, 2005" startWordPosition="4768" endWordPosition="4771"> queries were generated by using Google’s query suggestions interface. Volunteers were asked to find answers using single Freebase pages. Therefore, by construction, queries retained can be answered using the Freebase KG alone, with a simple r(e1, ?) form. In contrast, TREC-INEX queries provide a balanced mix of t2 and r hints in the queries, and direct answers from triples is relatively less available. 5.2 Implementation details On an average, the pseudocode in Figure 2 generated 13 segmentations per query, with longer queries generating more segmentations than shorter ones. We used an MG4J (Boldi and Vigna, 2005) based query processor, written in Java, over entity and word indices on ClueWeb09B. The index supplies snippets with a specified maximum width, containing a mention of some entity and satisfying a WAND (Broder et al., 2003) predicate over words in s. In case of phrases in the query, the WAND threshold was computed by adding the IDF of constituent words. The index returned about 330,000 snippets on average for WAND threshold of 0.6. We retained the top 200 candidate entities from the corpus; increasing this horizon did not give benefits. We also considered as candidates for e2 those entities t</context>
</contexts>
<marker>Boldi, Vigna, 2005</marker>
<rawString>Paolo Boldi and Sebastiano Vigna. 2005. MG4J at TREC 2005. In Ellen M. Voorhees and Lori P. Buckland, editors, TREC, number SP 500-266 in Special Publications. NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Z Broder</author>
<author>David Carmel</author>
<author>Michael Herscovici</author>
<author>Aya Soffer</author>
<author>Jason Zien</author>
</authors>
<title>Efficient query evaluation using a two-level retrieval process.</title>
<date>2003</date>
<booktitle>In CIKM,</booktitle>
<pages>426--434</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="28839" citStr="Broder et al., 2003" startWordPosition="4806" endWordPosition="4809">, with a simple r(e1, ?) form. In contrast, TREC-INEX queries provide a balanced mix of t2 and r hints in the queries, and direct answers from triples is relatively less available. 5.2 Implementation details On an average, the pseudocode in Figure 2 generated 13 segmentations per query, with longer queries generating more segmentations than shorter ones. We used an MG4J (Boldi and Vigna, 2005) based query processor, written in Java, over entity and word indices on ClueWeb09B. The index supplies snippets with a specified maximum width, containing a mention of some entity and satisfying a WAND (Broder et al., 2003) predicate over words in s. In case of phrases in the query, the WAND threshold was computed by adding the IDF of constituent words. The index returned about 330,000 snippets on average for WAND threshold of 0.6. We retained the top 200 candidate entities from the corpus; increasing this horizon did not give benefits. We also considered as candidates for e2 those entities that are adjacent to e1 in the KG via top-scoring r candidates. In order to generate supporting snippets for an interpretation containing entity annotation e, we need to match e with Google’s corpus annotations. However, rely</context>
</contexts>
<marker>Broder, Carmel, Herscovici, Soffer, Zien, 2003</marker>
<rawString>Andrei Z. Broder, David Carmel, Michael Herscovici, Aya Soffer, and Jason Zien. 2003. Efficient query evaluation using a two-level retrieval process. In CIKM, pages 426–434. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Cheng</author>
<author>Kevin Chen-Chuan Chang</author>
</authors>
<title>Beyond pages: supporting efficient, scalable entity search with dual-inversion index.</title>
<date>2010</date>
<booktitle>In EDBT.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="9768" citStr="Cheng and Chang, 2010" startWordPosition="1567" endWordPosition="1570">al., 2010; Li et al., 2011). Queries being far more noisy, the goal of structure discovery is more modest, and often takes the form of a segmentation of the query regarded as a token sequence, assigning a broad purpose (Pantel et al., 2012; Lin et al., 2012) to each segment, mapping them probabilistically to a relatively loose schema, and ranking responses in conjunction with segmentations (Sawant and Chakrabarti, 2013). To maintain quality in the face of noisy input, these approaches often additionally exploit clicks (Li et al., 2011) or a corpus that has been annotated with entity mentions (Cheng and Chang, 2010; Li et al., 2010). The corpus provides contextual snippets for queries where the KG fails, preventing the systems from falling off the “structure cliff” (Pereira, 2013). Our work advances the capabilities of the latter class of approaches, bringing them closer to the depth of the former, while handling telegraphic queries and retaining the advantage of corpus evidence over and above the KG. Very recently, (Yao et al., 2014) have concluded that for current benchmarks, deep parsing and shallow information extraction give comparable interpretation accuracy. The very recent work of (Yih et al., 2</context>
</contexts>
<marker>Cheng, Chang, 2010</marker>
<rawString>Tao Cheng and Kevin Chen-Chuan Chang. 2010. Beyond pages: supporting efficient, scalable entity search with dual-inversion index. In EDBT. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ClueWeb09</author>
</authors>
<date>2009</date>
<note>http://www. lemurproject.org/clueweb09.php/.</note>
<marker>ClueWeb09, 2009</marker>
<rawString>ClueWeb09. 2009. http://www. lemurproject.org/clueweb09.php/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Luke Zettlemoyer</author>
<author>Oren Etzioni</author>
</authors>
<title>Open question answering over curated and extracted knowledge bases.</title>
<date>2014</date>
<booktitle>In SIGKDD Conference.</booktitle>
<marker>Fader, Zettlemoyer, Etzioni, 2014</marker>
<rawString>Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2014. Open question answering over curated and extracted knowledge bases. In SIGKDD Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paolo Ferragina</author>
<author>Ugo Scaiella</author>
</authors>
<title>TAGME: on-the-fly annotation of short text fragments (by wikipedia entities).</title>
<date>2010</date>
<journal>CoRR/arXiv,</journal>
<volume>1006</volume>
<pages>1006--3498</pages>
<contexts>
<context position="13496" citStr="Ferragina and Scaiella, 2010" startWordPosition="2200" endWordPosition="2204">, connecting relation, and context words. We found that segments hinting at T2 and R frequently overlapped (e.g., ‘author’ in the query zhivago author). In our implementation, we simplified to three labels, E1, T2R,S, where tokens labeled T2R are involved with both t2 and r, the proposed structured target type and connecting relation. Another reasonable assumption was that the base entity mention and type/relation mentions are contiguous token spans, whereas context words can be scattered in multiple segments. Figure 2 shows how candidate segmentations are generated. For step 3, we use TagMe (Ferragina and Scaiella, 2010), an entity linker backed by an entity gazette derived from our KG. 4.2 Graphical model Based on the previous discussion, we assume that an entity-seeking query q is a sequence of tokens q1, q2, . . ., and this can be partitioned into different kinds of subsequences, corresponding to e1, r, t2 and s, and denoted by a structured (vector) labeling z = z1, z2,.. .. Given sequences q and z, we can separate out (possibly empty) token segments be1(q, z), bt2(q, z), br(q, z), and bs(q, z). A query segmentation z becomes plausible in conjunction with proposals for e1, r, t2 and e2 from the KG. The pro</context>
<context position="25399" citStr="Ferragina and Scaiella, 2010" startWordPosition="4218" endWordPosition="4222">Figure 4. Number of snippets with distance(e2, el) &lt; k1 (k1 = 5, 10) Number of snippets with distance(e2, relation phrase) &lt; k2 (k2 = 3, 6) Number of snippets with relation r = ⊥ Number of snippets with relation phrases as prepositions Number of snippets covering fraction of query IDF &gt; k3 (k3 = 0.2, 0.4, 0.6, 0.8) Figure 4: Sample features used for learning weights λ to score snippets. 4.2.4 Query entity model Potential ΨE1(q, z, e1) captures the compatibility between el(q, z) (i.e., the words that mention e1) and the claimed entity e1 mentioned in the query. We used the TagMe entity linker (Ferragina and Scaiella, 2010) for annotating entities in queries. TagMe annotates the query with Wikipedia entities, which we map to Freebase, and use the annotation confidence scores as the potential ΨE1(q, z, e1). 4.3 Discriminative parameter training with latent variables We first set the potentials in (1) as explained in §4.2 (henceforth called ‘Unoptimized’), and got encouraging accuracy. Then we rewrote each potential as Ψ•(···) = exp(w• · φ•(· · · )� (4) or log H• Ψ•(···) = E• w• · φ•(· · · ), with w• being a weight vector for a specific potential •, and φ• being a corresponding feature vector. During inference, we</context>
</contexts>
<marker>Ferragina, Scaiella, 2010</marker>
<rawString>Paolo Ferragina and Ugo Scaiella. 2010. TAGME: on-the-fly annotation of short text fragments (by wikipedia entities). CoRR/arXiv, abs/1006.3498. http://arxiv.org/abs/1006.3498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Michael Ringgaard</author>
<author>Amarnag Subramanya</author>
</authors>
<title>FACC1: Freebase annotation of ClueWeb corpora. http://</title>
<date>2013</date>
<booktitle>lemurproject.org/clueweb12/,</booktitle>
<contexts>
<context position="4678" citStr="Gabrilovich et al., 2013" startWordPosition="716" endWordPosition="719"> in information extraction (IE). Over well-formed utterances, IE baselines are quite competitive (Yao and Van Durme, 2014). But here, we are interested exclusively in telegraphic queries. 1.2 Incomplete knowledge graph The second problem is that the KG is always work in progress (Pereira, 2013), and connections found within nodes of the KG, between the KG and the query, or the KG and unstructured text, are often incomplete or erroneous. E.g., Wikipedia is considered tiny, and Freebase rather small, compared to what is needed to answer all but the “head” queries. Google’s Freebase annotations (Gabrilovich et al., 2013) on ClueWeb (ClueWeb09, 2009) number fewer than 15 per page to ensure precision. Fewer than 2% are to entities in Freebase but not in Wikipedia. It may also be difficult to harness the KG for answering certain queries. E.g., answering the query fastest odi century batsman, the intent of which is to find the batsman holding the record for the fastest century in One Day International (ODI) cricket, may be too difficult for most KG-only systems, but may be answered quite effectively by a system that also utilizes evidence from unstructured text. There is a clear need for a “pay-as-you-go” archite</context>
<context position="18415" citStr="Gabrilovich et al., 2013" startWordPosition="3046" endWordPosition="3049">ge potential. Many approaches (Berant et al., 2013; Berant and Liang, 2014; Kwiatkowski et al., 2013; Yih et al., 2014) to this problem have been intensely studied recently. Given our need to process billions of Web pages efficiently, we chose a pattern-based approach (Nakashole et al., 2012): with each r, discover the most strongly associated phrase patterns from a reference corpus, then mark these patterns into much larger payload corpus. We started with the 2000 (out of approximately 14000) most frequent relation types in Freebase, and the ClueWeb09 corpus annotated with Freebase entities (Gabrilovich et al., 2013). For each triple instance of each relation type, we located all corpus sentences that mentioned both participating entities. We made the crude assumption that if r(e1,e2) holds and e1, e2 co-occur in a sentence then this sentence is evidence of the relationship. Each such sentence is parsed to obtain a dependency graph using the Malt Parser (Hall et al., 2014). Words in the path connecting the entities are joined together and added to a candidate phrase dictionary, provided the path is at most three hops. (Inspection suggested that longer dependency paths mostly arise out of noisy sentences o</context>
<context position="27148" citStr="Gabrilovich et al., 2013" startWordPosition="4537" endWordPosition="4540">ch query. Similar distant supervision problems were posed via bundle method by (Bergeron et al., 2008), and (Yu and Joachims, 2009), who used CCCP (Yuille and Rangarajan, 2006). These are equivalent in our setting. We use the CCCP style, and augment the objective with an additional entropy term as in (Sawant and Chakrabarti, 2013). We call this LVDT (latent variable discriminative training) in §5. 5 Experiments 5.1 Testbed Corpus and knowledge graph: We used the ClueWeb09B (ClueWeb09, 2009) corpus containing 50 million Web documents. This corpus was annotated by Google with Freebase entities (Gabrilovich et al., 2013). The average page contains 15 entity annotations from Freebase. We used the Freebase KG and its links to Wikipedia. Queries: We report on two sets of entity-seeking queries. A sample of about 800 well-formed queries from WebQuestions (Berant et al., 2013) were converted to telegraphic utterances (such as would be typed into commercial search engines) by volunteers familiar with Web search. We call this WQT (WebQuestions, telegraphic). Queries are accompanied by ground truth entities. The second data set, TREC-INEX, from (Sawant and Chakrabarti, 2013) has about 700 queries sampled from TREC an</context>
</contexts>
<marker>Gabrilovich, Ringgaard, Subramanya, 2013</marker>
<rawString>Evgeniy Gabrilovich, Michael Ringgaard, and Amarnag Subramanya. 2013. FACC1: Freebase annotation of ClueWeb corpora. http:// lemurproject.org/clueweb12/, June. Version 1 (Release date 2013-06-26, Format version 1, Correction level 0).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean Gallagher</author>
</authors>
<title>How Google and Microsoft taught search to ‘understand’ the Web. ArsTechnica article.</title>
<date>2012</date>
<note>http://goo.gl/NWs0zT.</note>
<contexts>
<context position="2091" citStr="Gallagher, 2012" startWordPosition="330" endWordPosition="331">s adapted from TREC, INEX, and WebQuestions, show the efficacy of our approach. For one benchmark, MAP improves from 0.2–0.29 (competitive baselines) to 0.42 (our system). NDCG@10 improves from 0.29–0.36 to 0.54. ∗Work done as Masters student at IIT Bombay 1 Introduction A majority of Web queries mention an entity or type (Lin et al., 2012), as users increasingly explore the Web of objects using Web search. To better support entity-oriented queries, commercial Web search engines are rapidly building up large catalogs of types, entities and relations, popularly called a “knowledge graph” (KG) (Gallagher, 2012). Despite these advances, robust, Web-scale, open-domain, entity-oriented search faces many challenges. Here, we focus on two. 1.1 “Telegraphic” queries First, the surface utterances of entity-oriented Web queries are dramatically different from TRECor Watson-style factoid question answering (QA), where questions are grammatically well-formed. Web queries are usually “telegraphic”: they are short, rarely use function words, punctuations or clausal structure, and use relatively flexible word orders. E.g., the natural utterance “on the bank of which river is the Hermitage Museum located” may be </context>
</contexts>
<marker>Gallagher, 2012</marker>
<rawString>Sean Gallagher. 2012. How Google and Microsoft taught search to ‘understand’ the Web. ArsTechnica article. http://goo.gl/NWs0zT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiafeng Guo</author>
<author>Gu Xu</author>
<author>Xueqi Cheng</author>
<author>Hang Li</author>
</authors>
<title>Named entity recognition in query.</title>
<date>2009</date>
<booktitle>In SIGIR Conference,</booktitle>
<pages>267--274</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3400" citStr="Guo et al., 2009" startWordPosition="514" endWordPosition="517">tion utterances, 50% of interpretation failures are contributed by parsing or structural matching failures (Kwiatkowski et al., 2013). Telegraphic utterances will generally be even more challenging. Consequently, whereas TREC-QA/NLP-style research has focused on parsing and precise interpretation of a well-formed query sentence to a strongly structured (typically graph-oriented) query language (Kasneci et al., 2008; Pound et al., 2012; Yahya et al., 2012; Berant et al., 2013; Kwiatkowski et al., 2013), the Web search and information retrieval (IR) community has focused on telegraphic queries (Guo et al., 2009; Sarkas et al., 2010; Li et al., 2011; Pantel et al., 2012; Lin et al., 2012; Sawant and Chakrabarti, 2013). In terms of target schema richness, these efforts may 1104 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1104–1114, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics appear more modest. The act of query ‘interpretation’ is mainly a segmentation of query tokens by purpose. In the example above, one may report segments “Hermitage Museum” (a located artifact or named entity), and “river bank” (the tar</context>
<context position="9135" citStr="Guo et al., 2009" startWordPosition="1462" endWordPosition="1465"> first merril lynch headquarters merril lynch headquarters - - merril lynch - headquarters - spanish poet died in civil war spanish died in poet civil war civil war died - spanish poet spanish in poet died civil war first american in space - - - first american in space - - american first, in space Figure 1: Example queries and some potential segmentations. large corpus may be used to build relation expression models (Yao and Van Durme, 2014), but not as supporting evidence for target entities. In contrast, the Web and IR community generally assumes a free-form query that is often telegraphic (Guo et al., 2009; Sarkas et al., 2010; Li et al., 2011). Queries being far more noisy, the goal of structure discovery is more modest, and often takes the form of a segmentation of the query regarded as a token sequence, assigning a broad purpose (Pantel et al., 2012; Lin et al., 2012) to each segment, mapping them probabilistically to a relatively loose schema, and ranking responses in conjunction with segmentations (Sawant and Chakrabarti, 2013). To maintain quality in the face of noisy input, these approaches often additionally exploit clicks (Li et al., 2011) or a corpus that has been annotated with entit</context>
</contexts>
<marker>Guo, Xu, Cheng, Li, 2009</marker>
<rawString>Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009. Named entity recognition in query. In SIGIR Conference, pages 267–274. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>Joakim Nivre</author>
</authors>
<date>2014</date>
<note>Maltparser. http://www.maltparser.org/.</note>
<contexts>
<context position="18778" citStr="Hall et al., 2014" startWordPosition="3109" endWordPosition="3112">eference corpus, then mark these patterns into much larger payload corpus. We started with the 2000 (out of approximately 14000) most frequent relation types in Freebase, and the ClueWeb09 corpus annotated with Freebase entities (Gabrilovich et al., 2013). For each triple instance of each relation type, we located all corpus sentences that mentioned both participating entities. We made the crude assumption that if r(e1,e2) holds and e1, e2 co-occur in a sentence then this sentence is evidence of the relationship. Each such sentence is parsed to obtain a dependency graph using the Malt Parser (Hall et al., 2014). Words in the path connecting the entities are joined together and added to a candidate phrase dictionary, provided the path is at most three hops. (Inspection suggested that longer dependency paths mostly arise out of noisy sentences or botched parses.) 30% of the sentences were thus retained. Finally, we defined n(r, �r(q, z)) ΨR(q, z, r) = (2) Ep, n(r, p&apos;), where p&apos; ranges over all phrases that are known to hint at r, and n(r, p) denotes the number of sentences where the phrase p occurred in the dependency path between the entities participating in relation r. Assuming entity co-occurrence</context>
</contexts>
<marker>Hall, Nilsson, Nivre, 2014</marker>
<rawString>Johan Hall, Jens Nilsson, and Joakim Nivre. 2014. Maltparser. http://www.maltparser.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In SIGKDD Conference,</booktitle>
<pages>133--142</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="24095" citStr="Joachims, 2002" startWordPosition="3976" endWordPosition="3977">ets are retrieved using a combined entity + word index, and scored for a given e1, r, e2, and selectors s(q, z). Given that relation phrases may be noisy and that their occurrence in the snippet may not necessarily mean that the given relation is being expressed, we need a scoring function that is cognizant of the roles of relation phrases and entities occurring in the snippets. In a basic version, e1, p, e2, &apos;g are used to probe a combined entity+word index to collect high scoring snippets, with the score being adapted from BM25. The second, refined scoring function used a RankSVM1109 style (Joachims, 2002) optimization. min I IλI I2 + C Ee+,e− ξe+,e− s.t. λ,ξ be+, e− : λ · f(q, De+, e+) + ξe+,e− (3) &gt; λ · f(q, De−, e−) + 1. where e+ and e− are positive and negative entities for the query q and f(q, De, e) represents the feature map for the set of snippets De belonging to entity e. The assumption here is that all snippets containing e+ are “positive” snippets for the query. f consolidates various signals like the number of snippets where e occurs near query entity e1 and a relation phrase, or the number of snippets with high proportion of query IDF, hinting that e is a positive entity for the gi</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In SIGKDD Conference, pages 133–142. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gjergji Kasneci</author>
<author>Fabian M Suchanek</author>
<author>Georgiana Ifrim</author>
<author>Maya Ramanath</author>
<author>Gerhard Weikum</author>
</authors>
<title>NAGA: Searching and ranking knowledge.</title>
<date>2008</date>
<booktitle>In ICDE.</booktitle>
<publisher>IEEE.</publisher>
<contexts>
<context position="3202" citStr="Kasneci et al., 2008" startWordPosition="480" endWordPosition="483">word orders. E.g., the natural utterance “on the bank of which river is the Hermitage Museum located” may be translated to the telegraphic Web query hermitage museum river bank. Even on well-formed question utterances, 50% of interpretation failures are contributed by parsing or structural matching failures (Kwiatkowski et al., 2013). Telegraphic utterances will generally be even more challenging. Consequently, whereas TREC-QA/NLP-style research has focused on parsing and precise interpretation of a well-formed query sentence to a strongly structured (typically graph-oriented) query language (Kasneci et al., 2008; Pound et al., 2012; Yahya et al., 2012; Berant et al., 2013; Kwiatkowski et al., 2013), the Web search and information retrieval (IR) community has focused on telegraphic queries (Guo et al., 2009; Sarkas et al., 2010; Li et al., 2011; Pantel et al., 2012; Lin et al., 2012; Sawant and Chakrabarti, 2013). In terms of target schema richness, these efforts may 1104 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1104–1114, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics appear more modest. The act of query </context>
<context position="7824" citStr="Kasneci et al., 2008" startWordPosition="1234" endWordPosition="1238"> synergistic salutary effects on accuracy. §2 explores related work in more detail. §3 gives some examples fitting our query template, explains why interpreting some of them is nontrivial, and sets up notation. §4 presents our core technical contributions. §5 presents experiments. Data can be accessed at http://bit.ly/Spva49 and http://bit.ly/WSpxvr. 2 Related work The NLP/QA community has traditionally assumed that question utterances are grammatically well-formed, from which precise clause structure, ground constants, variables, and connective relations can be inferred via semantic parsing (Kasneci et al., 2008; Pound et al., 2012; Yahya et al., 2012; Berant et al., 2013; Kwiatkowski et al., 2013) and translated to lambda expressions (Liang, 2013) or SPARQL style queries (Kasneci et al., 2008), with elaborate schema knowledge. Such approaches are often correlated with the assumption that all usable knowledge has been curated into a KG. The query is first translated to a structured form and then “executed” on the KG. A 1105 Telegraphic query el r t2 s first african american nobel prize winner nobel prize winner african american first nobel prize - winner first african american - - winner first africa</context>
</contexts>
<marker>Kasneci, Suchanek, Ifrim, Ramanath, Weikum, 2008</marker>
<rawString>Gjergji Kasneci, Fabian M. Suchanek, Georgiana Ifrim, Maya Ramanath, and Gerhard Weikum. 2008. NAGA: Searching and ranking knowledge. In ICDE. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daphne Koller</author>
<author>Nir Friedman</author>
</authors>
<title>Probabilistic Graphical Models: Principles and Techniques.</title>
<date>2009</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="14219" citStr="Koller and Friedman, 2009" startWordPosition="2331" endWordPosition="2334">the previous discussion, we assume that an entity-seeking query q is a sequence of tokens q1, q2, . . ., and this can be partitioned into different kinds of subsequences, corresponding to e1, r, t2 and s, and denoted by a structured (vector) labeling z = z1, z2,.. .. Given sequences q and z, we can separate out (possibly empty) token segments be1(q, z), bt2(q, z), br(q, z), and bs(q, z). A query segmentation z becomes plausible in conjunction with proposals for e1, r, t2 and e2 from the KG. The probability Pr(z, e1, r, t2, e2|q) is modeled as proportional to the product of several potentials (Koller and Friedman, 2009) in a graphical model. In subsequent subsections, we will present the design of specific potentials. • &amp;R(q, z, r) denotes the compatibility between the relation hint segment br(q, z) and a proposed relation type r in the KG (§4.2.1). • &amp;T2(q, z, t2) denotes the compatibility between the type hint segment bt2(q, z) and a proposed target entity type t2 in the KG (§4.2.2). • &amp;E1,R,E2,S(q, z, e1, r, e2) is a novel corpusbased evidence potential that measures how strongly e1 and e2 appear in corpus snippets in the proximity of words in bs(q, z), and apparently related by relation type r (§4.2.3). </context>
<context position="16225" citStr="Koller and Friedman, 2009" startWordPosition="2688" endWordPosition="2691">2. Figure 3 shows the relevant variable states as circled nodes, and the potentials as square factor nodes. To rank candidate entities e2, we pin the node E2 to each entity in turn. With E2 pinned, we perform a MAP inference over all other hidden variables and note the score of e2 as the product of the above potentials maximized over choices of all other variables: score(e2) = Maxx,t2,r,e1&amp;T2(q, z, t2)&amp;R(q, z, r) &amp;E1(q, z, e1)&amp;S(q, z) &amp;E2,T2(e2, t2)&amp;E1,R,E2(e1, r, e2) &amp;E1,R,E2,S(q, z, e1, r, e2). (1) We rank candidate e2s by decreasing score, which is estimated by max-product message-passing (Koller and Friedman, 2009). As noted earlier, any of the relation/type, or query entity partitions may be empty. To handle 1107 Figure 3: Graphical model for query segmentation and entity scoring. Factors/potentials are shown as squares. A candidate e2 is observed and scored using equation (1). Query q is also observed but not shown to reduce clutter; most potentials depend on it. Type language model Segmentation Entity language model Target type Query Selectors entity Connecting relation Corpus-assisted entity-relation evidence potential Relation language model Candidate entity this case, we allow each of the entity, </context>
</contexts>
<marker>Koller, Friedman, 2009</marker>
<rawString>Daphne Koller and Nir Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Eunsol Choi</author>
<author>Yoav Artzi</author>
<author>Luke S Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In EMNLP Conference,</booktitle>
<pages>1545--1556</pages>
<contexts>
<context position="2917" citStr="Kwiatkowski et al., 2013" startWordPosition="443" endWordPosition="446"> Web queries are dramatically different from TRECor Watson-style factoid question answering (QA), where questions are grammatically well-formed. Web queries are usually “telegraphic”: they are short, rarely use function words, punctuations or clausal structure, and use relatively flexible word orders. E.g., the natural utterance “on the bank of which river is the Hermitage Museum located” may be translated to the telegraphic Web query hermitage museum river bank. Even on well-formed question utterances, 50% of interpretation failures are contributed by parsing or structural matching failures (Kwiatkowski et al., 2013). Telegraphic utterances will generally be even more challenging. Consequently, whereas TREC-QA/NLP-style research has focused on parsing and precise interpretation of a well-formed query sentence to a strongly structured (typically graph-oriented) query language (Kasneci et al., 2008; Pound et al., 2012; Yahya et al., 2012; Berant et al., 2013; Kwiatkowski et al., 2013), the Web search and information retrieval (IR) community has focused on telegraphic queries (Guo et al., 2009; Sarkas et al., 2010; Li et al., 2011; Pantel et al., 2012; Lin et al., 2012; Sawant and Chakrabarti, 2013). In term</context>
<context position="7912" citStr="Kwiatkowski et al., 2013" startWordPosition="1251" endWordPosition="1254"> §3 gives some examples fitting our query template, explains why interpreting some of them is nontrivial, and sets up notation. §4 presents our core technical contributions. §5 presents experiments. Data can be accessed at http://bit.ly/Spva49 and http://bit.ly/WSpxvr. 2 Related work The NLP/QA community has traditionally assumed that question utterances are grammatically well-formed, from which precise clause structure, ground constants, variables, and connective relations can be inferred via semantic parsing (Kasneci et al., 2008; Pound et al., 2012; Yahya et al., 2012; Berant et al., 2013; Kwiatkowski et al., 2013) and translated to lambda expressions (Liang, 2013) or SPARQL style queries (Kasneci et al., 2008), with elaborate schema knowledge. Such approaches are often correlated with the assumption that all usable knowledge has been curated into a KG. The query is first translated to a structured form and then “executed” on the KG. A 1105 Telegraphic query el r t2 s first african american nobel prize winner nobel prize winner african american first nobel prize - winner first african american - - winner first african american nobel prize dave navarro first band dave navarro band - first dave navarro ba</context>
<context position="17890" citStr="Kwiatkowski et al., 2013" startWordPosition="2961" endWordPosition="2964">proposed relation r. E.g., if the query is steve jobs death reason, and r is (correctly chosen as) death reason, then the correct candidate r is /people/ deceased_person/cause _aof _ddeath. An incorrect r is /people/decese person/ place_of_death. An incorrect z may lead to r(q, z) being jobs death. Using corpus: Considerable variation may exist in how r is represented textually in a query. The relation language model needs to build a bridge between the formal r and the textual r, so that (un)likely r’s have (small) large potential. Many approaches (Berant et al., 2013; Berant and Liang, 2014; Kwiatkowski et al., 2013; Yih et al., 2014) to this problem have been intensely studied recently. Given our need to process billions of Web pages efficiently, we chose a pattern-based approach (Nakashole et al., 2012): with each r, discover the most strongly associated phrase patterns from a reference corpus, then mark these patterns into much larger payload corpus. We started with the 2000 (out of approximately 14000) most frequent relation types in Freebase, and the ClueWeb09 corpus annotated with Freebase entities (Gabrilovich et al., 2013). For each triple instance of each relation type, we located all corpus sen</context>
<context position="30547" citStr="Kwiatkowski et al., 2013" startWordPosition="5082" endWordPosition="5085">r LVDT). For the WQT dataset, average time per query for each stage was approximately - 0.2, 16.6 and 1.3 seconds respectively. Our (Java) code did not optimize the bottleneck at all; only 10 hosts and no clever load balancing were used. We believe commercial search engines can cut this down to less than a second. 5.3 Research questions In the rest of this section we will address these questions: • For telegraphic queries, is our entity-relationtype-selector segmentation better than the type-selector segmentation of (Sawant and Chakrabarti, 2013)? • When semantic parsers (Berant et al., 2013; Kwiatkowski et al., 2013) are subjected to telegraphic queries, how do they perform compared to our proposal? • Are the KG and corpus really complementary as regards their support of accurate ranking of candidate entities? • Is the prediction of r and t2 from our approach better than a greedy assignment based on local language models? We also discuss anecdotes of successes and failures of various systems. 5.4 Benefits of relation in addition to type Figure 5 shows entity-ranking MAP, MRR, and NDCG@10 (n@10) for two data sets and various systems. “No interpretation” is an IR baseline without any KG. Type+selector is ou</context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke S. Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In EMNLP Conference, pages 1545–1556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaonan Li</author>
<author>Chengkai Li</author>
<author>Cong Yu</author>
</authors>
<title>EntityEngine: Answering entity-relationship queries using shallow semantics.</title>
<date>2010</date>
<booktitle>In CIKM,</booktitle>
<contexts>
<context position="9786" citStr="Li et al., 2010" startWordPosition="1571" endWordPosition="1574">011). Queries being far more noisy, the goal of structure discovery is more modest, and often takes the form of a segmentation of the query regarded as a token sequence, assigning a broad purpose (Pantel et al., 2012; Lin et al., 2012) to each segment, mapping them probabilistically to a relatively loose schema, and ranking responses in conjunction with segmentations (Sawant and Chakrabarti, 2013). To maintain quality in the face of noisy input, these approaches often additionally exploit clicks (Li et al., 2011) or a corpus that has been annotated with entity mentions (Cheng and Chang, 2010; Li et al., 2010). The corpus provides contextual snippets for queries where the KG fails, preventing the systems from falling off the “structure cliff” (Pereira, 2013). Our work advances the capabilities of the latter class of approaches, bringing them closer to the depth of the former, while handling telegraphic queries and retaining the advantage of corpus evidence over and above the KG. Very recently, (Yao et al., 2014) have concluded that for current benchmarks, deep parsing and shallow information extraction give comparable interpretation accuracy. The very recent work of (Yih et al., 2014) is similar in</context>
</contexts>
<marker>Li, Li, Yu, 2010</marker>
<rawString>Xiaonan Li, Chengkai Li, and Cong Yu. 2010. EntityEngine: Answering entity-relationship queries using shallow semantics. In CIKM, October. (demo).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanen Li</author>
<author>Bo-Jun Paul Hsu</author>
<author>ChengXiang Zhai</author>
<author>Kuansan Wang</author>
</authors>
<title>Unsupervised query segmentation using clickthrough for information retrieval.</title>
<date>2011</date>
<booktitle>In SIGIR Conference,</booktitle>
<pages>285--294</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3438" citStr="Li et al., 2011" startWordPosition="522" endWordPosition="525">failures are contributed by parsing or structural matching failures (Kwiatkowski et al., 2013). Telegraphic utterances will generally be even more challenging. Consequently, whereas TREC-QA/NLP-style research has focused on parsing and precise interpretation of a well-formed query sentence to a strongly structured (typically graph-oriented) query language (Kasneci et al., 2008; Pound et al., 2012; Yahya et al., 2012; Berant et al., 2013; Kwiatkowski et al., 2013), the Web search and information retrieval (IR) community has focused on telegraphic queries (Guo et al., 2009; Sarkas et al., 2010; Li et al., 2011; Pantel et al., 2012; Lin et al., 2012; Sawant and Chakrabarti, 2013). In terms of target schema richness, these efforts may 1104 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1104–1114, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics appear more modest. The act of query ‘interpretation’ is mainly a segmentation of query tokens by purpose. In the example above, one may report segments “Hermitage Museum” (a located artifact or named entity), and “river bank” (the target type). This is reminiscent of reco</context>
<context position="9174" citStr="Li et al., 2011" startWordPosition="1470" endWordPosition="1473"> lynch headquarters - - merril lynch - headquarters - spanish poet died in civil war spanish died in poet civil war civil war died - spanish poet spanish in poet died civil war first american in space - - - first american in space - - american first, in space Figure 1: Example queries and some potential segmentations. large corpus may be used to build relation expression models (Yao and Van Durme, 2014), but not as supporting evidence for target entities. In contrast, the Web and IR community generally assumes a free-form query that is often telegraphic (Guo et al., 2009; Sarkas et al., 2010; Li et al., 2011). Queries being far more noisy, the goal of structure discovery is more modest, and often takes the form of a segmentation of the query regarded as a token sequence, assigning a broad purpose (Pantel et al., 2012; Lin et al., 2012) to each segment, mapping them probabilistically to a relatively loose schema, and ranking responses in conjunction with segmentations (Sawant and Chakrabarti, 2013). To maintain quality in the face of noisy input, these approaches often additionally exploit clicks (Li et al., 2011) or a corpus that has been annotated with entity mentions (Cheng and Chang, 2010; Li e</context>
</contexts>
<marker>Li, Hsu, Zhai, Wang, 2011</marker>
<rawString>Yanen Li, Bo-Jun Paul Hsu, ChengXiang Zhai, and Kuansan Wang. 2011. Unsupervised query segmentation using clickthrough for information retrieval. In SIGIR Conference, pages 285–294. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Lambda dependencybased compositional semantics.</title>
<date>2013</date>
<tech>Technical Report arXiv:1309.4408,</tech>
<institution>Stanford University.</institution>
<note>http:// arxiv.org/abs/1309.4408.</note>
<contexts>
<context position="7963" citStr="Liang, 2013" startWordPosition="1260" endWordPosition="1261"> interpreting some of them is nontrivial, and sets up notation. §4 presents our core technical contributions. §5 presents experiments. Data can be accessed at http://bit.ly/Spva49 and http://bit.ly/WSpxvr. 2 Related work The NLP/QA community has traditionally assumed that question utterances are grammatically well-formed, from which precise clause structure, ground constants, variables, and connective relations can be inferred via semantic parsing (Kasneci et al., 2008; Pound et al., 2012; Yahya et al., 2012; Berant et al., 2013; Kwiatkowski et al., 2013) and translated to lambda expressions (Liang, 2013) or SPARQL style queries (Kasneci et al., 2008), with elaborate schema knowledge. Such approaches are often correlated with the assumption that all usable knowledge has been curated into a KG. The query is first translated to a structured form and then “executed” on the KG. A 1105 Telegraphic query el r t2 s first african american nobel prize winner nobel prize winner african american first nobel prize - winner first african american - - winner first african american nobel prize dave navarro first band dave navarro band - first dave navarro band band first merril lynch headquarters merril lync</context>
</contexts>
<marker>Liang, 2013</marker>
<rawString>Percy Liang. 2013. Lambda dependencybased compositional semantics. Technical Report arXiv:1309.4408, Stanford University. http:// arxiv.org/abs/1309.4408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lin</author>
<author>Patrick Pantel</author>
<author>Michael Gamon</author>
<author>Anitha Kannan</author>
<author>Ariel Fuxman</author>
</authors>
<title>Active objects: Actions for entity-centric search.</title>
<date>2012</date>
<booktitle>In WWW Conference,</booktitle>
<pages>589--598</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1817" citStr="Lin et al., 2012" startWordPosition="287" endWordPosition="290">the KG. We do not trust the best or any specific query segmentation. Instead, evidence in favor of candidate e2s are aggregated across several segmentations. Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG, using over a thousand telegraphic queries adapted from TREC, INEX, and WebQuestions, show the efficacy of our approach. For one benchmark, MAP improves from 0.2–0.29 (competitive baselines) to 0.42 (our system). NDCG@10 improves from 0.29–0.36 to 0.54. ∗Work done as Masters student at IIT Bombay 1 Introduction A majority of Web queries mention an entity or type (Lin et al., 2012), as users increasingly explore the Web of objects using Web search. To better support entity-oriented queries, commercial Web search engines are rapidly building up large catalogs of types, entities and relations, popularly called a “knowledge graph” (KG) (Gallagher, 2012). Despite these advances, robust, Web-scale, open-domain, entity-oriented search faces many challenges. Here, we focus on two. 1.1 “Telegraphic” queries First, the surface utterances of entity-oriented Web queries are dramatically different from TRECor Watson-style factoid question answering (QA), where questions are grammat</context>
<context position="3477" citStr="Lin et al., 2012" startWordPosition="530" endWordPosition="533"> structural matching failures (Kwiatkowski et al., 2013). Telegraphic utterances will generally be even more challenging. Consequently, whereas TREC-QA/NLP-style research has focused on parsing and precise interpretation of a well-formed query sentence to a strongly structured (typically graph-oriented) query language (Kasneci et al., 2008; Pound et al., 2012; Yahya et al., 2012; Berant et al., 2013; Kwiatkowski et al., 2013), the Web search and information retrieval (IR) community has focused on telegraphic queries (Guo et al., 2009; Sarkas et al., 2010; Li et al., 2011; Pantel et al., 2012; Lin et al., 2012; Sawant and Chakrabarti, 2013). In terms of target schema richness, these efforts may 1104 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1104–1114, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics appear more modest. The act of query ‘interpretation’ is mainly a segmentation of query tokens by purpose. In the example above, one may report segments “Hermitage Museum” (a located artifact or named entity), and “river bank” (the target type). This is reminiscent of record segmentation in information extracti</context>
<context position="9405" citStr="Lin et al., 2012" startWordPosition="1512" endWordPosition="1515"> - - american first, in space Figure 1: Example queries and some potential segmentations. large corpus may be used to build relation expression models (Yao and Van Durme, 2014), but not as supporting evidence for target entities. In contrast, the Web and IR community generally assumes a free-form query that is often telegraphic (Guo et al., 2009; Sarkas et al., 2010; Li et al., 2011). Queries being far more noisy, the goal of structure discovery is more modest, and often takes the form of a segmentation of the query regarded as a token sequence, assigning a broad purpose (Pantel et al., 2012; Lin et al., 2012) to each segment, mapping them probabilistically to a relatively loose schema, and ranking responses in conjunction with segmentations (Sawant and Chakrabarti, 2013). To maintain quality in the face of noisy input, these approaches often additionally exploit clicks (Li et al., 2011) or a corpus that has been annotated with entity mentions (Cheng and Chang, 2010; Li et al., 2010). The corpus provides contextual snippets for queries where the KG fails, preventing the systems from falling off the “structure cliff” (Pereira, 2013). Our work advances the capabilities of the latter class of approach</context>
</contexts>
<marker>Lin, Pantel, Gamon, Kannan, Fuxman, 2012</marker>
<rawString>Thomas Lin, Patrick Pantel, Michael Gamon, Anitha Kannan, and Ariel Fuxman. 2012. Active objects: Actions for entity-centric search. In WWW Conference, pages 589–598. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ndapandula Nakashole</author>
<author>Gerhard Weikum</author>
<author>Fabian Suchanek</author>
</authors>
<title>PATTY: A taxonomy of relational patterns with semantic types.</title>
<date>2012</date>
<booktitle>In EMNLP Conference, EMNLP-CoNLL ’12,</booktitle>
<pages>1135--1145</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="18083" citStr="Nakashole et al., 2012" startWordPosition="2994" endWordPosition="2997">ncorrect r is /people/decese person/ place_of_death. An incorrect z may lead to r(q, z) being jobs death. Using corpus: Considerable variation may exist in how r is represented textually in a query. The relation language model needs to build a bridge between the formal r and the textual r, so that (un)likely r’s have (small) large potential. Many approaches (Berant et al., 2013; Berant and Liang, 2014; Kwiatkowski et al., 2013; Yih et al., 2014) to this problem have been intensely studied recently. Given our need to process billions of Web pages efficiently, we chose a pattern-based approach (Nakashole et al., 2012): with each r, discover the most strongly associated phrase patterns from a reference corpus, then mark these patterns into much larger payload corpus. We started with the 2000 (out of approximately 14000) most frequent relation types in Freebase, and the ClueWeb09 corpus annotated with Freebase entities (Gabrilovich et al., 2013). For each triple instance of each relation type, we located all corpus sentences that mentioned both participating entities. We made the crude assumption that if r(e1,e2) holds and e1, e2 co-occur in a sentence then this sentence is evidence of the relationship. Each</context>
</contexts>
<marker>Nakashole, Weikum, Suchanek, 2012</marker>
<rawString>Ndapandula Nakashole, Gerhard Weikum, and Fabian Suchanek. 2012. PATTY: A taxonomy of relational patterns with semantic types. In EMNLP Conference, EMNLP-CoNLL ’12, pages 1135–1145. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Thomas Lin</author>
<author>Michael Gamon</author>
</authors>
<title>Mining entity types from query logs via user intent modeling.</title>
<date>2012</date>
<booktitle>In ACL Conference,</booktitle>
<pages>563--571</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="3459" citStr="Pantel et al., 2012" startWordPosition="526" endWordPosition="529">ributed by parsing or structural matching failures (Kwiatkowski et al., 2013). Telegraphic utterances will generally be even more challenging. Consequently, whereas TREC-QA/NLP-style research has focused on parsing and precise interpretation of a well-formed query sentence to a strongly structured (typically graph-oriented) query language (Kasneci et al., 2008; Pound et al., 2012; Yahya et al., 2012; Berant et al., 2013; Kwiatkowski et al., 2013), the Web search and information retrieval (IR) community has focused on telegraphic queries (Guo et al., 2009; Sarkas et al., 2010; Li et al., 2011; Pantel et al., 2012; Lin et al., 2012; Sawant and Chakrabarti, 2013). In terms of target schema richness, these efforts may 1104 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1104–1114, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics appear more modest. The act of query ‘interpretation’ is mainly a segmentation of query tokens by purpose. In the example above, one may report segments “Hermitage Museum” (a located artifact or named entity), and “river bank” (the target type). This is reminiscent of record segmentation in in</context>
<context position="9386" citStr="Pantel et al., 2012" startWordPosition="1508" endWordPosition="1511">rst american in space - - american first, in space Figure 1: Example queries and some potential segmentations. large corpus may be used to build relation expression models (Yao and Van Durme, 2014), but not as supporting evidence for target entities. In contrast, the Web and IR community generally assumes a free-form query that is often telegraphic (Guo et al., 2009; Sarkas et al., 2010; Li et al., 2011). Queries being far more noisy, the goal of structure discovery is more modest, and often takes the form of a segmentation of the query regarded as a token sequence, assigning a broad purpose (Pantel et al., 2012; Lin et al., 2012) to each segment, mapping them probabilistically to a relatively loose schema, and ranking responses in conjunction with segmentations (Sawant and Chakrabarti, 2013). To maintain quality in the face of noisy input, these approaches often additionally exploit clicks (Li et al., 2011) or a corpus that has been annotated with entity mentions (Cheng and Chang, 2010; Li et al., 2010). The corpus provides contextual snippets for queries where the KG fails, preventing the systems from falling off the “structure cliff” (Pereira, 2013). Our work advances the capabilities of the latte</context>
</contexts>
<marker>Pantel, Lin, Gamon, 2012</marker>
<rawString>Patrick Pantel, Thomas Lin, and Michael Gamon. 2012. Mining entity types from query logs via user intent modeling. In ACL Conference, pages 563– 571, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
</authors>
<title>Meaning in the wild. Invited talk at EMNLP Conference.</title>
<date>2013</date>
<note>http://hum.csse.unimelb.edu.au/ emnlp2013/invited-talks.html.</note>
<contexts>
<context position="4348" citStr="Pereira, 2013" startWordPosition="663" endWordPosition="664"> for Computational Linguistics appear more modest. The act of query ‘interpretation’ is mainly a segmentation of query tokens by purpose. In the example above, one may report segments “Hermitage Museum” (a located artifact or named entity), and “river bank” (the target type). This is reminiscent of record segmentation in information extraction (IE). Over well-formed utterances, IE baselines are quite competitive (Yao and Van Durme, 2014). But here, we are interested exclusively in telegraphic queries. 1.2 Incomplete knowledge graph The second problem is that the KG is always work in progress (Pereira, 2013), and connections found within nodes of the KG, between the KG and the query, or the KG and unstructured text, are often incomplete or erroneous. E.g., Wikipedia is considered tiny, and Freebase rather small, compared to what is needed to answer all but the “head” queries. Google’s Freebase annotations (Gabrilovich et al., 2013) on ClueWeb (ClueWeb09, 2009) number fewer than 15 per page to ensure precision. Fewer than 2% are to entities in Freebase but not in Wikipedia. It may also be difficult to harness the KG for answering certain queries. E.g., answering the query fastest odi century batsm</context>
<context position="9937" citStr="Pereira, 2013" startWordPosition="1596" endWordPosition="1597"> token sequence, assigning a broad purpose (Pantel et al., 2012; Lin et al., 2012) to each segment, mapping them probabilistically to a relatively loose schema, and ranking responses in conjunction with segmentations (Sawant and Chakrabarti, 2013). To maintain quality in the face of noisy input, these approaches often additionally exploit clicks (Li et al., 2011) or a corpus that has been annotated with entity mentions (Cheng and Chang, 2010; Li et al., 2010). The corpus provides contextual snippets for queries where the KG fails, preventing the systems from falling off the “structure cliff” (Pereira, 2013). Our work advances the capabilities of the latter class of approaches, bringing them closer to the depth of the former, while handling telegraphic queries and retaining the advantage of corpus evidence over and above the KG. Very recently, (Yao et al., 2014) have concluded that for current benchmarks, deep parsing and shallow information extraction give comparable interpretation accuracy. The very recent work of (Yih et al., 2014) is similar in spirit to ours, but they do not unify segmentation and answer inference, along with corpus evidence, like we do. 3 Notation and examples We use e1, r,</context>
</contexts>
<marker>Pereira, 2013</marker>
<rawString>Fernando Pereira. 2013. Meaning in the wild. Invited talk at EMNLP Conference. http://hum.csse.unimelb.edu.au/ emnlp2013/invited-talks.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pound</author>
<author>Alexander K Hudek</author>
<author>Ihab F Ilyas</author>
<author>Grant Weddell</author>
</authors>
<title>Interpreting keyword queries over Web knowledge bases.</title>
<date>2012</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="3222" citStr="Pound et al., 2012" startWordPosition="484" endWordPosition="487"> natural utterance “on the bank of which river is the Hermitage Museum located” may be translated to the telegraphic Web query hermitage museum river bank. Even on well-formed question utterances, 50% of interpretation failures are contributed by parsing or structural matching failures (Kwiatkowski et al., 2013). Telegraphic utterances will generally be even more challenging. Consequently, whereas TREC-QA/NLP-style research has focused on parsing and precise interpretation of a well-formed query sentence to a strongly structured (typically graph-oriented) query language (Kasneci et al., 2008; Pound et al., 2012; Yahya et al., 2012; Berant et al., 2013; Kwiatkowski et al., 2013), the Web search and information retrieval (IR) community has focused on telegraphic queries (Guo et al., 2009; Sarkas et al., 2010; Li et al., 2011; Pantel et al., 2012; Lin et al., 2012; Sawant and Chakrabarti, 2013). In terms of target schema richness, these efforts may 1104 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1104–1114, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics appear more modest. The act of query ‘interpretation’ is </context>
<context position="7844" citStr="Pound et al., 2012" startWordPosition="1239" endWordPosition="1242">effects on accuracy. §2 explores related work in more detail. §3 gives some examples fitting our query template, explains why interpreting some of them is nontrivial, and sets up notation. §4 presents our core technical contributions. §5 presents experiments. Data can be accessed at http://bit.ly/Spva49 and http://bit.ly/WSpxvr. 2 Related work The NLP/QA community has traditionally assumed that question utterances are grammatically well-formed, from which precise clause structure, ground constants, variables, and connective relations can be inferred via semantic parsing (Kasneci et al., 2008; Pound et al., 2012; Yahya et al., 2012; Berant et al., 2013; Kwiatkowski et al., 2013) and translated to lambda expressions (Liang, 2013) or SPARQL style queries (Kasneci et al., 2008), with elaborate schema knowledge. Such approaches are often correlated with the assumption that all usable knowledge has been curated into a KG. The query is first translated to a structured form and then “executed” on the KG. A 1105 Telegraphic query el r t2 s first african american nobel prize winner nobel prize winner african american first nobel prize - winner first african american - - winner first african american nobel pri</context>
</contexts>
<marker>Pound, Hudek, Ilyas, Weddell, 2012</marker>
<rawString>Jeffrey Pound, Alexander K. Hudek, Ihab F. Ilyas, and Grant Weddell. 2012. Interpreting keyword queries over Web knowledge bases. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikos Sarkas</author>
</authors>
<title>Stelios Paparizos, and Panayiotis Tsaparas.</title>
<date>2010</date>
<booktitle>In SIGMOD Conference.</booktitle>
<marker>Sarkas, 2010</marker>
<rawString>Nikos Sarkas, Stelios Paparizos, and Panayiotis Tsaparas. 2010. Structured annotations of Web queries. In SIGMOD Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uma Sawant</author>
<author>Soumen Chakrabarti</author>
</authors>
<title>Learning joint query interpretation and response ranking.</title>
<date>2013</date>
<booktitle>In WWW Conference,</booktitle>
<contexts>
<context position="3508" citStr="Sawant and Chakrabarti, 2013" startWordPosition="534" endWordPosition="537">ng failures (Kwiatkowski et al., 2013). Telegraphic utterances will generally be even more challenging. Consequently, whereas TREC-QA/NLP-style research has focused on parsing and precise interpretation of a well-formed query sentence to a strongly structured (typically graph-oriented) query language (Kasneci et al., 2008; Pound et al., 2012; Yahya et al., 2012; Berant et al., 2013; Kwiatkowski et al., 2013), the Web search and information retrieval (IR) community has focused on telegraphic queries (Guo et al., 2009; Sarkas et al., 2010; Li et al., 2011; Pantel et al., 2012; Lin et al., 2012; Sawant and Chakrabarti, 2013). In terms of target schema richness, these efforts may 1104 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1104–1114, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics appear more modest. The act of query ‘interpretation’ is mainly a segmentation of query tokens by purpose. In the example above, one may report segments “Hermitage Museum” (a located artifact or named entity), and “river bank” (the target type). This is reminiscent of record segmentation in information extraction (IE). Over well-formed utter</context>
<context position="9570" citStr="Sawant and Chakrabarti, 2013" startWordPosition="1534" endWordPosition="1537">ao and Van Durme, 2014), but not as supporting evidence for target entities. In contrast, the Web and IR community generally assumes a free-form query that is often telegraphic (Guo et al., 2009; Sarkas et al., 2010; Li et al., 2011). Queries being far more noisy, the goal of structure discovery is more modest, and often takes the form of a segmentation of the query regarded as a token sequence, assigning a broad purpose (Pantel et al., 2012; Lin et al., 2012) to each segment, mapping them probabilistically to a relatively loose schema, and ranking responses in conjunction with segmentations (Sawant and Chakrabarti, 2013). To maintain quality in the face of noisy input, these approaches often additionally exploit clicks (Li et al., 2011) or a corpus that has been annotated with entity mentions (Cheng and Chang, 2010; Li et al., 2010). The corpus provides contextual snippets for queries where the KG fails, preventing the systems from falling off the “structure cliff” (Pereira, 2013). Our work advances the capabilities of the latter class of approaches, bringing them closer to the depth of the former, while handling telegraphic queries and retaining the advantage of corpus evidence over and above the KG. Very re</context>
<context position="26855" citStr="Sawant and Chakrabarti, 2013" startWordPosition="4492" endWordPosition="4495">g of w, we are given pairs of correct and incorrect answer entities e+2 , e−2 , and we wish to satisfy constraints of the form w · φ(q, z, e1, t2, r, e+2 ) + ξ (6) w · φ(q, z, e1, t2, r, e−2 ), because collecting e+2 , e−2 pairs is less work than supervising with values of z, e1, t2, r, e2 for each query. Similar distant supervision problems were posed via bundle method by (Bergeron et al., 2008), and (Yu and Joachims, 2009), who used CCCP (Yuille and Rangarajan, 2006). These are equivalent in our setting. We use the CCCP style, and augment the objective with an additional entropy term as in (Sawant and Chakrabarti, 2013). We call this LVDT (latent variable discriminative training) in §5. 5 Experiments 5.1 Testbed Corpus and knowledge graph: We used the ClueWeb09B (ClueWeb09, 2009) corpus containing 50 million Web documents. This corpus was annotated by Google with Freebase entities (Gabrilovich et al., 2013). The average page contains 15 entity annotations from Freebase. We used the Freebase KG and its links to Wikipedia. Queries: We report on two sets of entity-seeking queries. A sample of about 800 well-formed queries from WebQuestions (Berant et al., 2013) were converted to telegraphic utterances (such as </context>
<context position="30474" citStr="Sawant and Chakrabarti, 2013" startWordPosition="5070" endWordPosition="5073"> collecting/scoring snippets, and inference (MAP for Unoptimized and wo(·) for LVDT). For the WQT dataset, average time per query for each stage was approximately - 0.2, 16.6 and 1.3 seconds respectively. Our (Java) code did not optimize the bottleneck at all; only 10 hosts and no clever load balancing were used. We believe commercial search engines can cut this down to less than a second. 5.3 Research questions In the rest of this section we will address these questions: • For telegraphic queries, is our entity-relationtype-selector segmentation better than the type-selector segmentation of (Sawant and Chakrabarti, 2013)? • When semantic parsers (Berant et al., 2013; Kwiatkowski et al., 2013) are subjected to telegraphic queries, how do they perform compared to our proposal? • Are the KG and corpus really complementary as regards their support of accurate ranking of candidate entities? • Is the prediction of r and t2 from our approach better than a greedy assignment based on local language models? We also discuss anecdotes of successes and failures of various systems. 5.4 Benefits of relation in addition to type Figure 5 shows entity-ranking MAP, MRR, and NDCG@10 (n@10) for two data sets and various systems. </context>
</contexts>
<marker>Sawant, Chakrabarti, 2013</marker>
<rawString>Uma Sawant and Soumen Chakrabarti. 2013. Learning joint query interpretation and response ranking. In WWW Conference, Brazil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Automatically semantifying Wikipedia. In</title>
<date>2007</date>
<booktitle>CIKM,</booktitle>
<pages>41--50</pages>
<contexts>
<context position="23033" citStr="Wu and Weld, 2007" startWordPosition="3787" endWordPosition="3790">k nomenclature for relation and type language models, our models are not incompatible with other catalogs. Indeed, most catalogs have established ways of deriving language models that describe their various structures. For example, most YAGO types are derived from WordNet synsets with associated phrasal descriptions (lemmas). YAGO relations also have readable names such as actedIn, isMarriedTo, etc. which can be used to estimate language models. DBPedia relations are mostly derived from (meaningfully) named attributes taken from infoboxes, hence they can be used directly. Furthermore, others (Wu and Weld, 2007) have shown how to associate language models with such relations. 4.2.3 Snippet scoring The factor ΨE1,R,E2,S(q, z, e1, r, e2) should be large if many snippets contain a mention of e1 and e2, relation r, and many high-signal words from s. Recall that we begin with a corpus annotated with entity mentions. Our corpus is not directly annotated with relation mentions. Therefore, we get from relations to documents via high-confidence phrases. Snippets are retrieved using a combined entity + word index, and scored for a given e1, r, e2, and selectors s(q, z). Given that relation phrases may be noisy</context>
</contexts>
<marker>Wu, Weld, 2007</marker>
<rawString>Fei Wu and Daniel S Weld. 2007. Automatically semantifying Wikipedia. In CIKM, pages 41–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Yahya</author>
<author>Klaus Berberich</author>
<author>Shady Elbassuoni</author>
<author>Maya Ramanath</author>
<author>Volker Tresp</author>
<author>Gerhard Weikum</author>
</authors>
<title>Natural language questions for the Web of data.</title>
<date>2012</date>
<booktitle>In EMNLP Conference,</booktitle>
<pages>379--390</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="3242" citStr="Yahya et al., 2012" startWordPosition="488" endWordPosition="491">on the bank of which river is the Hermitage Museum located” may be translated to the telegraphic Web query hermitage museum river bank. Even on well-formed question utterances, 50% of interpretation failures are contributed by parsing or structural matching failures (Kwiatkowski et al., 2013). Telegraphic utterances will generally be even more challenging. Consequently, whereas TREC-QA/NLP-style research has focused on parsing and precise interpretation of a well-formed query sentence to a strongly structured (typically graph-oriented) query language (Kasneci et al., 2008; Pound et al., 2012; Yahya et al., 2012; Berant et al., 2013; Kwiatkowski et al., 2013), the Web search and information retrieval (IR) community has focused on telegraphic queries (Guo et al., 2009; Sarkas et al., 2010; Li et al., 2011; Pantel et al., 2012; Lin et al., 2012; Sawant and Chakrabarti, 2013). In terms of target schema richness, these efforts may 1104 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1104–1114, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics appear more modest. The act of query ‘interpretation’ is mainly a segmentatio</context>
<context position="7864" citStr="Yahya et al., 2012" startWordPosition="1243" endWordPosition="1246"> §2 explores related work in more detail. §3 gives some examples fitting our query template, explains why interpreting some of them is nontrivial, and sets up notation. §4 presents our core technical contributions. §5 presents experiments. Data can be accessed at http://bit.ly/Spva49 and http://bit.ly/WSpxvr. 2 Related work The NLP/QA community has traditionally assumed that question utterances are grammatically well-formed, from which precise clause structure, ground constants, variables, and connective relations can be inferred via semantic parsing (Kasneci et al., 2008; Pound et al., 2012; Yahya et al., 2012; Berant et al., 2013; Kwiatkowski et al., 2013) and translated to lambda expressions (Liang, 2013) or SPARQL style queries (Kasneci et al., 2008), with elaborate schema knowledge. Such approaches are often correlated with the assumption that all usable knowledge has been curated into a KG. The query is first translated to a structured form and then “executed” on the KG. A 1105 Telegraphic query el r t2 s first african american nobel prize winner nobel prize winner african american first nobel prize - winner first african american - - winner first african american nobel prize dave navarro firs</context>
</contexts>
<marker>Yahya, Berberich, Elbassuoni, Ramanath, Tresp, Weikum, 2012</marker>
<rawString>Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, Maya Ramanath, Volker Tresp, and Gerhard Weikum. 2012. Natural language questions for the Web of data. In EMNLP Conference, pages 379– 390, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Information extraction over structured data: Question answering with Freebase.</title>
<date>2014</date>
<booktitle>In ACL Conference. ACL.</booktitle>
<marker>Yao, Van Durme, 2014</marker>
<rawString>Xuchen Yao and Benjamin Van Durme. 2014. Information extraction over structured data: Question answering with Freebase. In ACL Conference. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Jonathan Berant</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Freebase QA: Information extraction or semantic parsing?</title>
<date>2014</date>
<booktitle>In ACL 2014 Workshop on Semantic Parsing (SP14).</booktitle>
<marker>Yao, Berant, Van Durme, 2014</marker>
<rawString>Xuchen Yao, Jonathan Berant, and Benjamin Van Durme. 2014. Freebase QA: Information extraction or semantic parsing? In ACL 2014 Workshop on Semantic Parsing (SP14).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Xiaodong He</author>
<author>Christopher Meek</author>
</authors>
<title>Semantic parsing for single-relation question answering.</title>
<date>2014</date>
<booktitle>In ACL Conference. ACL.</booktitle>
<contexts>
<context position="6721" citStr="Yih et al., 2014" startWordPosition="1073" endWordPosition="1076"> r(e1, e2) is likely to hold, evidenced near the matching words in unstructured text. Given the short, telegraphic query utterances, we limit our scope to at most one relation mention, unlike the complex mapping of clauses in wellformed questions to twig and join style queries (e.g., “find an actor whose spouse was an Italian bookwriter”). On the other hand, we need to deal with the unhelpful input, as well as consolidate the KG with the corpus for ranking candidate e2s. Despite the modest specification, our query template is quite expressive, covering a wide range of entity-oriented queries (Yih et al., 2014). We present a novel discriminative graphical model to capture the entity ranking inference task, with query segmentation as a by-product. Extensive experiments with over a thousand entityseeking telegraphic queries using the ClueWeb09 corpus and a subset of Freebase show that we can accurately predict the segmentation and intent of telegraphic relational queries, and simultaneously rank candidate responses with high accuracy. We also present evidence that the KG and corpus have synergistic salutary effects on accuracy. §2 explores related work in more detail. §3 gives some examples fitting ou</context>
<context position="10372" citStr="Yih et al., 2014" startWordPosition="1665" endWordPosition="1668">d Chang, 2010; Li et al., 2010). The corpus provides contextual snippets for queries where the KG fails, preventing the systems from falling off the “structure cliff” (Pereira, 2013). Our work advances the capabilities of the latter class of approaches, bringing them closer to the depth of the former, while handling telegraphic queries and retaining the advantage of corpus evidence over and above the KG. Very recently, (Yao et al., 2014) have concluded that for current benchmarks, deep parsing and shallow information extraction give comparable interpretation accuracy. The very recent work of (Yih et al., 2014) is similar in spirit to ours, but they do not unify segmentation and answer inference, along with corpus evidence, like we do. 3 Notation and examples We use e1, r, t2, e2 to represent abstract nodes and edges (MIDs in case of Freebase) from the KG, and 61, r, t2 to represent their textual mentions or hints, if any, in the query. s is a set of uninterpreted textual tokens in the query that are used to match and collect corpus contexts that lend evidence to candidate entities. Figure 1 shows some telegraphic queries with possible segmentation into the above parts. Consider another example: dav</context>
<context position="17909" citStr="Yih et al., 2014" startWordPosition="2965" endWordPosition="2968"> if the query is steve jobs death reason, and r is (correctly chosen as) death reason, then the correct candidate r is /people/ deceased_person/cause _aof _ddeath. An incorrect r is /people/decese person/ place_of_death. An incorrect z may lead to r(q, z) being jobs death. Using corpus: Considerable variation may exist in how r is represented textually in a query. The relation language model needs to build a bridge between the formal r and the textual r, so that (un)likely r’s have (small) large potential. Many approaches (Berant et al., 2013; Berant and Liang, 2014; Kwiatkowski et al., 2013; Yih et al., 2014) to this problem have been intensely studied recently. Given our need to process billions of Web pages efficiently, we chose a pattern-based approach (Nakashole et al., 2012): with each r, discover the most strongly associated phrase patterns from a reference corpus, then mark these patterns into much larger payload corpus. We started with the 2000 (out of approximately 14000) most frequent relation types in Freebase, and the ClueWeb09 corpus annotated with Freebase entities (Gabrilovich et al., 2013). For each triple instance of each relation type, we located all corpus sentences that mention</context>
</contexts>
<marker>Yih, He, Meek, 2014</marker>
<rawString>Wen-tau Yih, Xiaodong He, and Christopher Meek. 2014. Semantic parsing for single-relation question answering. In ACL Conference. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chun-Nam John Yu</author>
<author>Thorsten Joachims</author>
</authors>
<title>Learning structural SVMs with latent variables.</title>
<date>2009</date>
<booktitle>In ICML,</booktitle>
<pages>1169--1176</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="26654" citStr="Yu and Joachims, 2009" startWordPosition="4458" endWordPosition="4461"> e1, t2, r, e2), (5) q,z,e1,t2,r for a fixed w, to find the score of each candidate entity e2. Here all w• and φ• have been collected into unified weight and feature vectors w, φ. During training of w, we are given pairs of correct and incorrect answer entities e+2 , e−2 , and we wish to satisfy constraints of the form w · φ(q, z, e1, t2, r, e+2 ) + ξ (6) w · φ(q, z, e1, t2, r, e−2 ), because collecting e+2 , e−2 pairs is less work than supervising with values of z, e1, t2, r, e2 for each query. Similar distant supervision problems were posed via bundle method by (Bergeron et al., 2008), and (Yu and Joachims, 2009), who used CCCP (Yuille and Rangarajan, 2006). These are equivalent in our setting. We use the CCCP style, and augment the objective with an additional entropy term as in (Sawant and Chakrabarti, 2013). We call this LVDT (latent variable discriminative training) in §5. 5 Experiments 5.1 Testbed Corpus and knowledge graph: We used the ClueWeb09B (ClueWeb09, 2009) corpus containing 50 million Web documents. This corpus was annotated by Google with Freebase entities (Gabrilovich et al., 2013). The average page contains 15 entity annotations from Freebase. We used the Freebase KG and its links to </context>
</contexts>
<marker>Yu, Joachims, 2009</marker>
<rawString>Chun-Nam John Yu and Thorsten Joachims. 2009. Learning structural SVMs with latent variables. In ICML, pages 1169–1176. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Yuille</author>
<author>Anand Rangarajan</author>
</authors>
<title>The concave-convex procedure.</title>
<date>2006</date>
<journal>Neural Computation,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="26699" citStr="Yuille and Rangarajan, 2006" startWordPosition="4465" endWordPosition="4468">fixed w, to find the score of each candidate entity e2. Here all w• and φ• have been collected into unified weight and feature vectors w, φ. During training of w, we are given pairs of correct and incorrect answer entities e+2 , e−2 , and we wish to satisfy constraints of the form w · φ(q, z, e1, t2, r, e+2 ) + ξ (6) w · φ(q, z, e1, t2, r, e−2 ), because collecting e+2 , e−2 pairs is less work than supervising with values of z, e1, t2, r, e2 for each query. Similar distant supervision problems were posed via bundle method by (Bergeron et al., 2008), and (Yu and Joachims, 2009), who used CCCP (Yuille and Rangarajan, 2006). These are equivalent in our setting. We use the CCCP style, and augment the objective with an additional entropy term as in (Sawant and Chakrabarti, 2013). We call this LVDT (latent variable discriminative training) in §5. 5 Experiments 5.1 Testbed Corpus and knowledge graph: We used the ClueWeb09B (ClueWeb09, 2009) corpus containing 50 million Web documents. This corpus was annotated by Google with Freebase entities (Gabrilovich et al., 2013). The average page contains 15 entity annotations from Freebase. We used the Freebase KG and its links to Wikipedia. Queries: We report on two sets of </context>
</contexts>
<marker>Yuille, Rangarajan, 2006</marker>
<rawString>A. L. Yuille and Anand Rangarajan. 2006. The concave-convex procedure. Neural Computation, 15(4):915–936.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ChengXiang Zhai</author>
</authors>
<title>Statistical language models for information retrieval: A critical review.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<volume>2</volume>
<issue>3</issue>
<pages>213</pages>
<contexts>
<context position="21711" citStr="Zhai, 2008" startWordPosition="3583" endWordPosition="3584"> compatibility between t2 and t2(q, z). Estimating the target entity type, without over-generalizing or overspecifying it, has always been important for QA. E.g., when t2 is ‘city’, a good type language model should prefer t2 as /location/citytown over /location/location while avoiding /location/es_autonomous_city. A catalog like Freebase suggests a straightforward method to collect a type language model. Each type is described by one or more phrases through the link /common/topic/alias. We can collect these into a micro-‘document’ and use a standard Dirichlet-smoothed language model from IR (Zhai, 2008). In Freebase, an entity node (e.g., Einstein, /m/0jcx) may be linked to a type node (e.g. /base/scientist/ physicist) using an edge with label /type/ object/type. But relation types provide additional clues to types of the endpoint entities. Freebase relation types have the form /x/y/z, where x is the domain of the relation, and y and z are string representations of the type of the entities participating in the relation. E.g., the (directed) relation type /location/country/capital connects from from /location/country to /location/citytown. Therefore, “capital” can be added to the set of descr</context>
</contexts>
<marker>Zhai, 2008</marker>
<rawString>ChengXiang Zhai. 2008. Statistical language models for information retrieval: A critical review. Foundations and Trends in Information Retrieval, 2(3):137– 213, March.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.962056">
Refining Word Segmentation Using a Manually Aligned Corpus
for Statistical Machine Translation
</title>
<author confidence="0.994972">
Xiaolin Wang Masao Utiyama Andrew Finch Eiichiro Sumita
</author>
<affiliation confidence="0.994536">
National Institute of Information and Communications Technology
</affiliation>
<email confidence="0.993313">
{xiaolin.wang,mutiyama,andrew.finch,eiichiro.sumita}@nict.go.jp
</email>
<sectionHeader confidence="0.994672" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999939111111111">
Languages that have no explicit word de-
limiters often have to be segmented for sta-
tistical machine translation (SMT). This is
commonly performed by automated seg-
menters trained on manually annotated
corpora. However, the word segmentation
(WS) schemes of these annotated corpora
are handcrafted for general usage, and
may not be suitable for SMT. An analysis
was performed to test this hypothesis us-
ing a manually annotated word alignment
(WA) corpus for Chinese-English SMT.
An analysis revealed that 74.60% of the
sentences in the WA corpus if segmented
using an automated segmenter trained on
the Penn Chinese Treebank (CTB) will
contain conflicts with the gold WA an-
notations. We formulated an approach
based on word splitting with reference to
the annotated WA to alleviate these con-
flicts. Experimental results show that the
refined WS reduced word alignment error
rate by 6.82% and achieved the highest
BLEU improvement (0.63 on average) on
the Chinese-English open machine trans-
lation (OpenMT) corpora compared to re-
lated work.
</bodyText>
<sectionHeader confidence="0.998879" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999619574468085">
Word segmentation is a prerequisite for many
natural language processing (NLP) applications
on those languages that have no explicit space
between words, such as Arabic, Chinese and
Japanese. As the first processing step, WS affects
all successive steps, thus it has a large potential
impact on the final performance. For SMT, the
unsupervised WA, building translation models and
reordering models, and decoding are all based on
segmented words.
Automated word segmenters built through
supervised-learning methods, after decades of in-
tensive research, have emerged as effective so-
lutions to WS tasks and become widely used in
many NLP applications. For example, the Stan-
ford word segmenter (Xue et al., 2002)1 which is
based on conditional random field (CRF) is em-
ployed to prepare the official corpus for NTCIR-
9 Chinese-English patent translation task (Goto et
al., 2011).
However, one problem with applying these
supervised-learning word segmenters to SMT is
that the WS scheme of annotating the training cor-
pus may not be optimal for SMT. (Chang et al.,
2008) noticed that the words in CTB are often too
long for SMT. For example, a full Chinese per-
sonal name which consists of a family name and a
given name is always taken as a single word, but
its counterpart in English is usually two words.
Manually WA corpora are precious resources
for SMT research, but they used to be only avail-
able in small volumes due to the production cost.
For example, (Och and Ney, 2000) initially an-
notated 447 English-French sentence pairs, which
later became the test data set in ACL 2003 shared
task on word alignment (Mihalcea and Pedersen,
2003), and was used frequently thereafter (Liang
et al., 2006; DeNero and Klein, 2007; Haghighi et
al., 2009)
For Chinese and English, the shortage of man-
ually WA corpora has recently been relieved
by the linguistic data consortium (LDC) 2
GALE Chinese-English word alignment and tag-
ging training corpus (the GALE WA corpus)3.
The corpus is considerably large, containing 4,735
documents, 18,507 sentence pairs, 620,189 Chi-
nese tokens, 518,137 English words, and 421,763
</bodyText>
<footnote confidence="0.9498834">
1http://nlp.stanford.edu/software/
segmenter.shtml
2http://catalog.ldc.upenn.edu
3Catalog numbers: LDC2012T16, LDC2012T20,
LDC2012T24 and LDC2013T05.
</footnote>
<page confidence="0.885177">
1654
</page>
<note confidence="0.9101565">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1654–1664,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999170785714286">
alignment annotations. The corpus carries no Chi-
nese WS annotation, and the WA annotation was
performed between Chinese characters and En-
glish words. The alignment identifies minimum
translation units and relations 4, referred as atomic
blocks and atomic edges, respectively, in this pa-
per. Figure 1 shows an example that contains six
atomic edges.
Visual inspection of the segmentation of an au-
tomatic segmenter with reference to a WA cor-
pus revealed a number of inconsistencies. For ex-
ample, consider the word “bao fa” in Figure 1.
Empirically we observed that this word is seg-
mented as a single token by an automatic seg-
menter trained on the CTB, however, this segmen-
tation differs with the alignment in the WA cor-
pus, since its two components are aligned to two
different English words. Our hypothesis was that
the removal of these inconsistencies would benefit
machine translation performance (this is explained
further in Section 2.3), and we explored this idea
in this work.
This paper focuses on optimizing Chinese WS
for Chinese-English SMT, but both the research
method and the proposed solution are language-
independent. They can be applied to other lan-
guage pairs.
The major contributions of this paper include,
</bodyText>
<listItem confidence="0.995330625">
• analyze the CTB WS scheme for Chinese-
English SMT;
• propose a lexical word splitter to refine the
WS;
• achieve a BLEU improvement over a baseline
Stanford word segmenter, and a state-of-the-
art extension, on Chinese-English OpenMT
corpora.
</listItem>
<bodyText confidence="0.993140125">
The rest of this paper is organized as follows:
first, Section 2 analyzes WS using a WA corpus;
next, Section 3 proposes a lexical word splitter
to refine WS; then, Section 4 evaluates the pro-
posed method on end-to-end SMT as well as word
segmentation and alignment; after that, Section 5
compares this work to related work; finally, Sec-
tion 6 concludes this paper.
</bodyText>
<footnote confidence="0.9838065">
4Guidelines for Chinese-English Word Align-
ment(Version 4.0)
</footnote>
<subsectionHeader confidence="0.7138335">
2 Analysis of a General-purpose
Automatic Word Segmenter
</subsectionHeader>
<bodyText confidence="0.999942666666667">
This section first briefly describes the GALE WA
corpus, then presents an analysis of the WS arising
from a CTB-standard word segmenter with refer-
ence to the segmentation of the atomic blocks in
the GALE WA corpus, finally the impact of the
findings on SMT is discussed.
</bodyText>
<subsectionHeader confidence="0.992813">
2.1 GALE WA corpus
</subsectionHeader>
<bodyText confidence="0.99996635">
The GALE WA corpus was developed by the
LDC, and was used as training data in the DARPA
GALE global autonomous language exploitation
program 5. The corpus incorporates linguistic
knowledge into word aligned text to help improve
automatic WA and translation quality. It em-
ploys two annotation schemes: alignment and tag-
ging (Li et al., 2010). Alignment identifies min-
imum translation units and translation relations;
tagging adds contextual, syntactic and language-
specific features to the alignment annotation. For
example, the sample shown in Figure 1 carries tags
on both alignment edges and tokens.
The GALE WA corpus contains 18,057 man-
ually word aligned Chinese and English parallel
sentences which are extracted from newswire and
web blogs. Table 1 presents the statistics on the
corpus. One third of the sentences are approxi-
mately newswire text, and the remainder consists
of web blogs.
</bodyText>
<subsectionHeader confidence="0.999923">
2.2 Analysis of WS
</subsectionHeader>
<bodyText confidence="0.998910333333333">
In order to produce a Chinese word segmenta-
tion consistent with the CTB standard we used the
Stanford Chinese word segmenter with a model
trained on the CTB corpus. We will refer to this
as the ‘CTB segmenter’ in the rest of this paper.
The Chinese sentences in the GALE WA cor-
pus were first segmented by the CTB segmenter,
and the predicted words were compared against
the atomic blocks with respect to the granularity of
segmentation. The analysis falls into the following
three categories, two of which may be potentially
harmful to SMT:
</bodyText>
<listItem confidence="0.867728333333333">
• Fully consistent: the word locates within the
block of one atomic alignment edge. For ex-
ample, in Figure 2(a), the Chinese text has
</listItem>
<footnote confidence="0.9886315">
5https://catalog.ldc.upenn.edu/
LDC2012T16
</footnote>
<page confidence="0.989585">
1655
</page>
<figure confidence="0.7053295">
fei de le yi kao fa qiu wan chen bao fa
ToI DET
</figure>
<figureCaption confidence="0.979968333333333">
Figure 1: Example from the GALE WA corpus. Each line arrow represents an atomic edge, and each box
represents an atomic block. SEM (semantic), GIS (grammatically inferred semantic) and FUN (function)
are tags of edges. INC (not translated), TOI (to-infinitive) and DET (determiner) are tags of tokens.
</figureCaption>
<table confidence="0.9965975">
Genre # Files # Sentences† # CN tokens # EN tokens # Alignment edges
Newswire 2,175 6,218 246,371 205,281 164,033
Web blog 2,560 11,839 373,818 312,856 257,730
Total 4,735 18,057 620,189 518,137 421,763
</table>
<tableCaption confidence="0.999774">
Table 1: GALE WA corpus. † Sentences rejected by the annotators are excluded.
</tableCaption>
<figure confidence="0.9959825">
Federer
� � � � � � � � � � �
SEM SEM GIS GIS GIS FUN
relied on
his service
INC INC
to keep the serve .
0
</figure>
<bodyText confidence="0.507778">
four atomic blocks; the CTB segmenter pro-
duces five words which all locate within the
blocks, so they are all small enough.
</bodyText>
<listItem confidence="0.937904461538462">
• Alignment inconsistent: the word aligns to
more than one atomic block, but the target
expression is contiguous, allowing for cor-
rect phrase pair extraction (Zens et al., 2002).
For example, in Figure 2(b), the characters in
the word “shuang fang”, which is produced
by the CTB segmenter, contains two atomic
blocks, but the span of the target “to both
side” is continuous, therefore the phrase pair
tracted.
• Alignment inconsistent and extraction hin-
dered: the word aligned to more than one
atomic block, and the target expression is not
</listItem>
<bodyText confidence="0.959107769230769">
contiguous, which hinders correct phrase pair
extractions. For example, in Figure 2(c), the
word “zeng chan” has to be split in order to
match the target language.
Table 2 shows the statistics of the three cat-
egories of CTB WS on the GALE WA corpus.
90.74% of the words are fully consistent, while the
remaining 9.26% of the words have inconsistent
alignments. 74.60% of the sentences contain this
problem. The category with inconsistent align-
ment and extraction hindered only accounts for
0.46% of the words, affecting 9.06% of the sen-
tences.
</bodyText>
<subsectionHeader confidence="0.999374">
2.3 Impact of WS on SMT
</subsectionHeader>
<bodyText confidence="0.9986145">
The word alignment has a direct impact on the na-
ture of both the translation model, and lexical re-
ordering model in a phrase-base SMT system. The
words in last two categories are all longer than an
atomic block, which might lead to problems in the
word alignment in two ways:
</bodyText>
<listItem confidence="0.9995654">
• First, longer words tend to be more sparse in
the training corpus, thus the estimated distri-
bution of their target phrases are less accu-
rate.
• Second, the alignment from them to target
</listItem>
<bodyText confidence="0.8574844">
sides are one-to-many, which is much more
complicated and requires fertilized alignment
models such as IBM model 4 – 6 (Och and
Ney, 2000).
The words in the category of “fully consistent”
can be aligned using simple models, because the
alignment from them to the target side are one-to-
one or many-to-one, and simple alignment models
such as IBM model 1, IBM model 2 and HMM
model are sufficient (Och and Ney, 2000).
</bodyText>
<sectionHeader confidence="0.977388" genericHeader="method">
3 Refining the Word Segmentation
</sectionHeader>
<bodyText confidence="0.999978857142857">
In the last subsection, it was shown that 74.60% of
parallel sentences were affected by issues related
to under-segmentation of the corpus. Our hypoth-
esis is that if these words are split into pieces that
match English words, the accuracy of the unsuper-
vised WA as well as the translation quality will be
improved. To achieve this, we adopt a splitting
</bodyText>
<page confidence="0.957266">
1656
</page>
<bodyText confidence="0.81917">
shu gian ming tie lu giang xiu ren yuan
he zuo shuang fang shi hu hui de
</bodyText>
<figure confidence="0.980333083333333">
~�
~
~~
����
thousands of railway
emergency personel
(a)
(b)
cooperation is beneficial to both sides
a4 &apos;F3(T Al
-q� - M
wei zeng chan de shi you zhao dao mai jia
</figure>
<figureCaption confidence="0.9224048">
Figure 2: Examples of automated WS on manually WA corpus: (a) Fully consistent; (b) Alignment
inconsistent; (c) Alignment inconsistent and extraction hindered. The Chinese words separated by white
space are the output of the CTB segmenter. Arrows represent the alignment of atomic blocks. Note that
“shuang fang” and “zeng chan” are words produced by the CTB segmenter, but consist of two atomic
blocks.
</figureCaption>
<table confidence="0.9883544">
Category Count Word Ratio Sentence Ratio
Fully consistent 355,702 90.74% 25.40%†
Alignment inconsistent 34,464 8.81% 65.54%
Alignment inconsistent &amp; extraction hindered 1,830 0.46% 9.06%
Sum of conflict ‡ 36,294 9.26% 74.60%
</table>
<tableCaption confidence="0.8284685">
Table 2: CTB WS on GALE WA corpus: † All words are fully consistent; ‡ Alignment inconsistent plus
alignment inconsistent &amp; extraction hindered
</tableCaption>
<figure confidence="0.99111725">
(c)
find buyers
�7 �
for the additional oil we
�
M �FA
RA 3Q*
produce
</figure>
<bodyText confidence="0.997930666666667">
strategy, based on a supervised learning approach,
to re-segment the corpus. This subsection first for-
malizes the task, and then presents the approach.
</bodyText>
<subsectionHeader confidence="0.999609">
3.1 Word splitting task
</subsectionHeader>
<bodyText confidence="0.9419396">
The word splitting task is formalized as a sequence
labeling task as follows: each word (represented
by a sequence of characters x = x1 ... xT where
T is the length of sample) produced by the CTB
segmenter is a sample, and a corresponding se-
quence of binary boundary labels y = y1 ... yT
is the learning target,
yt = { 1 if there is a split point (1)
between ct and ct_1;
0 otherwise.
The sequence of boundary labels is derived
from the gold WA annotation as follows: for a
sequence of two atomic blocks, where the first
character of the second block is xt, then the la-
Input Target y
</bodyText>
<equation confidence="0.93802125">
�� �� 0 0
3()� 3X_)� 0 1
��� ��_� 0 0 1
�J� �_� 0 1
</equation>
<figureCaption confidence="0.996743">
Figure 3: Samples of word splitting task
</figureCaption>
<bodyText confidence="0.999749222222222">
bel yt = 1. Figure 3 presents several samples ex-
tracted from the examples in Figure 2.
Each word sample may have no split point, one
split point or multiple split points, depending on
the gold WA annotation. Table 3 shows the statis-
tics of the word splitting data set which is built
from the GALE manual WA corpus and the CTB
segmenter’s output, where 2000 randomly sam-
pled sentences are taken as a held-out test set.
</bodyText>
<page confidence="0.971867">
1657
</page>
<table confidence="0.962666666666667">
Set # Sentences # Samples # Split points # Split points per sample
Train. 16,057 348,086 32,337 0.0929
Test 2,000 43,910 3,929 0.0895
</table>
<tableCaption confidence="0.999449">
Table 3: Data set for learning the word splitting
</tableCaption>
<subsectionHeader confidence="0.999285">
3.2 CRF approach
</subsectionHeader>
<bodyText confidence="0.9999055">
This paper employs a condition random field
(CRF) to solve this sequence labeling task (Laf-
ferty et al., 2001). A linear-chain CRF defines the
conditional probability of y given x as,
</bodyText>
<equation confidence="0.9978978">
T
1
PA(y|x) = (
ZX
t=1
</equation>
<bodyText confidence="0.990994666666667">
(2)
where A = {λ1, ...} are parameters, ZX is a per-
input normalization that makes the probability of
all state sequences sum to one; fk(yt−1, yt, x, t) is
a feature function which is often a binary-valued
sparse feature. The training of CRF model is to
maximize the likelihood of training data together
with a regularization penalty to avoid over-fitting
as (Peng et al., 2004; Peng and McCallum, 2006),
</bodyText>
<equation confidence="0.827369">
�A* = argmax(
A i
(3)
</equation>
<bodyText confidence="0.998992333333333">
where (x,y) are training samples; the hyperparam-
eter δk can be understood as the variance of the
prior distribution of λk. When predicting the la-
bels of test samples, the CRF decoder searches for
the optimal label sequence y* that maximizes the
conditional probability,
</bodyText>
<equation confidence="0.996161">
y* = argmax PA(y|x). (4)
Y
</equation>
<bodyText confidence="0.999894833333333">
In (Chang et al., 2008) a method is proposed to
select an appropriate level of segmentation gran-
ularity (in practical terms, to encourage smaller
segments). We call their method “length tuner”.
The following artificial feature is introduced into
the learned CRF model:
</bodyText>
<equation confidence="0.9716535">
_� 1 if yt = +1
f0(x, yt−1, yt,1) 0 otherwise (5)
</equation>
<bodyText confidence="0.999985846153846">
The weight λ0 of this feature is set by hand to
bias the output of CRF model. By way of expla-
nation, a very large positive λ0 will cause every
character to be segmented, or conversely a very
large negative λ0 will inhibit the output of segmen-
tation boundaries. In their experiments, λ0 = 2
was used to force a CRF segmenter to adopt an in-
termediate granularity between character and the
CTB WS scheme. Compared to the length tuner,
our proposed method exploits lexical knowledge
about word splitting, and we will therefore refer to
it as the “lexical word splitter” or “lexical splitter”
for short.
</bodyText>
<subsectionHeader confidence="0.995053">
3.3 Feature Set
</subsectionHeader>
<bodyText confidence="0.999990771428571">
The features fk(yt−1, yt, x, t) we used include the
WS features from the Chinese Stanford word seg-
menter and a set of extended features described
below. The WS features are included because the
target split points may share some common char-
acteristics with the boundaries in the CTB WS
scheme.
The extended features consists of four types –
named entities, word frequency, word length and
character-level unsupervised WA. For each type of
the feature, the value and value concatenated with
previous or current character are taken as sparse
features (see Table 4 for details). The real val-
ues of word frequency, word length and character-
level unsupervised WA are converted into sparse
features due to the routine of CRF model.
The character-level unsupervised alignment
feature is inspired by the related works of unsu-
pervised bilingual WS (Xu et al., 2008; Chung and
Gildea, 2009; Nguyen et al., 2010; Michael et al.,
2011). The idea is that the character-level WA can
approximately capture the counterpart English ex-
pression of each Chinese token, and source tokens
aligned to different target expressions should be
split into different words (see Figure 4 for an illus-
tration).
The values of the character-level alignment fea-
tures are obtained through building a dictionary.
First, unsupervised WA is performed on the SMT
training corpus where the Chinese sentences are
treated as sequences of characters; then, the Chi-
nese sentences are segmented by CTB segmenter
and a dictionary of segmented words are built; fi-
nally, for each word in the dictionary, the relative
frequency of being split at a certain position is cal-
</bodyText>
<equation confidence="0.917362375">
� λkfk(yt−1, yt, x, t)),
k
�
logPA(yi|xi) −
k
λ2k
),
2δ2 k
</equation>
<page confidence="0.865652">
1658
</page>
<table confidence="0.824210714285714">
Feature Definition Example
NE NE tag of current word Geography:NE
NE-C_1 NE concatenated with previous character Geo.-ding:NE-C_1
NE-C0 NE concatenated with current character Geo.-mei:NE-C0
Frequency Nearest integer of negative logarithm of word frequency 5t:Freq
Freq.-C_1 Frequency concatenated with previous character 5-ding:Freq-C_1
Freq.-C0 Frequency concatenated with current character 5-mei:Freq-C0
Length Length of current word (1,2,3,4,5,6,7 or &gt;,7) 4:Len
Len.-Position Length concatenated with the position 4-2:Len-Pos
Len.-C_1 Length concatenated with previous character 4-ding:Len-C_1
Len.-C0 Length concatenated with current character 4-mei:Len-C0
Char. Align. Five-level relative frequency of being split 0.4t:CA
C.A.-C_1 C.A. concatenated with previous character 0.4-ding:CA-C_1
C.A.-C0 C.A. concatenated with current character 0.4-mei:CA-C0
</table>
<tableCaption confidence="0.976228333333333">
Table 4: Extended features used in the CRF model for word splitting. The example shows the features
used in the decision whether to split the Chinese word “la ding mei zhou” (Latin America, the first
four Chinese characters in Figure 4) after the second Chinese character. t Round(-log10(0.00019)); t
</tableCaption>
<figure confidence="0.820538">
Round(0.43 x 5 ) / 5
Latin America should improve schooling
la ding mei zhou ying gai gai shan jiao yu
R AT X *I i A&amp; � t Ift
</figure>
<figureCaption confidence="0.997451">
Figure 4: Illustration of character-level unsuper-
vised alignment features. The dotted lines are
word boundaries suggested by the alignment.
</figureCaption>
<equation confidence="0.836645">
culated as,
nz
fCA(w, i) =
nw
</equation>
<bodyText confidence="0.999910142857143">
where w is a word, i is a splitting position (from
1 to the length of w minus 1); nz is the number of
times the words as split at position i according to
the character-level alignment, that is, the character
before and after i are aligned to different English
expressions; nw is occurrence count of word w in
the training corpus.
</bodyText>
<sectionHeader confidence="0.999671" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99997625">
In the last section we found that 9.26% of words
produced by the CTB segmenter have the poten-
tial to cause problems for SMT, and propose a
lexical word splitter to address this issue through
segmentation refinement. This section contains
experiments designed to empirically evaluate the
proposed lexical word splitter in three aspects:
first, whether the WS accuracy is improved; sec-
ond, whether the accuracy of the unsupervised WA
during training SMT systems is improved; third,
whether the end-to-end translation quality is im-
proved.
This section first describes the experimental
methodology, then presents the experimental re-
sults, and finally illustrates the operation of our
proposed method using a real example.
</bodyText>
<subsectionHeader confidence="0.754547">
4.1 Experimental Methodology
4.1.1 Experimental Corpora
</subsectionHeader>
<bodyText confidence="0.999971882352941">
The GALE manual WA corpus and the Chinese to
English corpus from the shared task of the NIST
open machine translation (OpenMT) 2006 evalua-
tion 6 were employed as the experimental corpus
(Table 5).
The experimental corpus for WS was con-
structed by first segmenting 2000 held out sen-
tences from the GALE manual WA corpus with
the Stanford segmenter, and then refining the seg-
mentation with the gold alignment annotation. For
example, the gold segmentation for the examples
in Figure 2 is presented in Figure 5. Note that
this test corpus is intended to represent an oracle
segmentation for our proposed method, and serves
primarily to gauge the improvement of our method
over the baseline Stanford segmenter, relative to
an upper bound.
</bodyText>
<footnote confidence="0.905223">
6http://www.itl.nist.gov/iad/mig/
tests/mt/2006/
</footnote>
<equation confidence="0.910113166666667">
(6)
1659
�� � � � �� � �� � ��
�� � � � � � � � �� � �
�� � �� � � � ��
� � � � � � � � �� � �� � �
</equation>
<figureCaption confidence="0.997476">
Figure 5: Examples of gold WS for evaluation
</figureCaption>
<table confidence="0.998260428571429">
Set # sent. pairs # CN tokens # EN tokens
Train. 442,967 19,755,573 13,444,927
Eval02 878† 38,204 105,944
Eval03 919† 40,900 113,970
Eval04 1,597† 71,890 207,279
Eval05 1,082† 50,461 138,952
Eval06 1,664† 62,422 189,059
</table>
<tableCaption confidence="0.9652445">
Table 5: NIST Open machine translation 2006
Corpora. † Number of sentence samples which
contain one Chinese sentence and four English ref-
erence sentences.
</tableCaption>
<bodyText confidence="0.99993125">
The experimental corpus for unsupervised WA
was the union set of the NIST OpenMT training
set and the 2000 test sentence pairs from GALE
WA corpus. We removed the United Nations cor-
pus from the NIST OpenMT constraint training re-
sources because it is out of domain.
The main result of this paper is the evaluation
of the end-to-end performance of an SMT sys-
tem. The experimental corpus for this task was
the NIST OpenMT corpus. The data set of the
NIST evaluation 2002 was used as a development
set for MERT tuning (Och, 2003), and the remain-
ing data sets of the NIST evaluation from 2003 to
2006 were used as test sets. The English sentences
were tokenized by Stanford toolkit 7 and converted
to lowercase.
</bodyText>
<subsectionHeader confidence="0.533612">
4.1.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.99633725">
The performance of WS was measured by pre-
cision, recall and F1 of gold words (Sproat and
Emerson, 2003),
The performance of unsupervised WA in the
SMT training procedure was measured through
alignment error rate (AER)(Och and Ney, 2000;
Liang et al., 2006). Sure alignment edges and
possible alignment edges were not distinguished
in this paper as no such tags are found in GALE
manual WA corpus.
The performance of SMT was measured using
BLEU (Papineni et al., 2002).
</bodyText>
<footnote confidence="0.884863">
7http://nlp.stanford.edu/software/
corenlp.shtml
</footnote>
<sectionHeader confidence="0.629202" genericHeader="method">
4.1.3 Baseline Methods
</sectionHeader>
<bodyText confidence="0.999745333333333">
Two Chinese WS methods were taken as the base-
line methods in this paper. One method was the
CTB segmenter, that is, Stanford Chinese word
segmenter with the model trained on CTB corpus.
The other method was the length tuner in (Chang
et al., 2008), which added a constant into the con-
fidence scores of a trained CRF word segmenter to
encourage it to output more word boundaries (see
Section 3.2 for details).
</bodyText>
<subsubsectionHeader confidence="0.469661">
4.1.4 Implementation and Parameter settings
</subsubsectionHeader>
<bodyText confidence="0.99998572">
The proposed lexical word splitter was imple-
mented on the CRF model toolkit released with
the Stanford segmenter (Tseng et al., 2005). The
regularity parameters Sk are set to be 3, the same
as the Stanford segmenter, because no significant
performance improvements were observed by tun-
ing that parameter.
To extract features for the word splitter, the
Stanford named entity recognizer (Finkel et al.,
2005)8 was employed to obtain the tags of named
entities. Word frequencies were caculated from
the source side of SMT training corpus. The
character-level unsupervised alignment was con-
ducted using GIZA++ (Och and Ney, 2003)9.
The length tuner reused the CRF model of CTB
segmenter. The parameter A0 was tuned through
the grid search in (Chang et al., 2008), that is, ob-
serving the BLEU score on the SMT development
set varing from A0 = 0 to A0 = 32. The grid
search showed that A0 = 2 was optimal, agreeing
with the value in (Chang et al., 2008).
Moses (Koehn et al., 2007)10, a state-of-the-art
phrase-based SMT system, was employed to per-
form end-to-end SMT experiments. GIZA++ was
employed to perform unsupervised WA.
</bodyText>
<subsectionHeader confidence="0.993689">
4.2 Experimental Results
4.2.1 Word Segmentation
</subsectionHeader>
<bodyText confidence="0.999783857142857">
The WS performance of CTB segmenter, length
tuner and the proposed lexical splitter are pre-
sented in Table 6. The proposed method achieves
the highest scores on all the criterion of F1, preci-
sion and recall. The length tuner outperforms the
CTB segmenter in terms of recall, but with lower
precision.
</bodyText>
<footnote confidence="0.997423">
8http://nlp.stanford.edu/software/
CRF-NER.shtml
9http://www.statmt.org/moses/giza/
GIZA++.html
10http://www.statmt.org/moses/
</footnote>
<page confidence="0.917871">
1660
</page>
<table confidence="0.99959425">
WS F1 Prec. Recall
CTB segmenter 0.878 0.917 0.842
Length tuner 0.873 0.894 0.852
Lexical splitter 0.915 0.922 0.908
</table>
<tableCaption confidence="0.863357">
Table 6: Performance of WS
</tableCaption>
<table confidence="0.99995575">
WS AER Prec. Recall
CTB segmenter 0.425 0.622 0.534
Length tuner 0.417 0.642 0.535
Lexical splitter 0.396 0.674 0.547
</table>
<tableCaption confidence="0.9811635">
Table 7: Performance of unsupervised WA using
different WS strategies
</tableCaption>
<subsubsectionHeader confidence="0.731191">
4.2.2 Word Alignment
</subsubsectionHeader>
<bodyText confidence="0.9999789">
The WA performance of the CTB segmenter,
length tuner and the proposed lexical spliter is pre-
sented in Table 7. Both lexical splitter and length
tuner outperform the CTB segmenter, indicating
the splitting words into smaller pieces can improve
the accuracy of unsupervised WA. This result sup-
ports the finding in (Chang et al., 2008) that the
segment size from CTB WS is too large for SMT.
In addition, the proposed lexical splitter signifi-
cantly outperforms the length tuner.
</bodyText>
<subsectionHeader confidence="0.727983">
4.2.3 Machine Translation
</subsectionHeader>
<bodyText confidence="0.9999496">
The end-to-end SMT performance of CTB seg-
menter, length tuner and the proposed lexical
spliter are presented in Table 8. Each experiment
was performed three times, and the average BLEU
and standard derivation were calculated, because
there is randomness in the results from MERT.
The proposed lexical splitter outperformed the two
baselines on all the test sets, and achieves an
average improvement of 0.63 BLEU percentage
points, indicating that the proposed method can
effectively improve the translation quality. The
length tuner also outperforms the CTB segmenter,
but the average improvement is 0.15 BLEU per-
centage points, much less than the proposed meth-
ods.
</bodyText>
<subsectionHeader confidence="0.99947">
4.3 Analysis
</subsectionHeader>
<bodyText confidence="0.999966166666667">
Figure 6 presents an example from the test cor-
pus, which demonstrates how the proposed lexical
splitter splits words more accurately than the base-
line length tuner method. Two words in the seg-
mentation result of the CTB segmenter are wor-
thy of attention. The first one is “yang nian”(the
year of goat), the lexical splitter split this word and
got the right translation, while the length tuner did
not split it. The second is “rong jing”(booming or
prosperity), the length tuner split this word, which
resulted in wrong translations, while the lexical
splitter avoided this mistake.
</bodyText>
<sectionHeader confidence="0.839411" genericHeader="method">
5 Comparison to Related Work
</sectionHeader>
<bodyText confidence="0.999947340909091">
The most similar work in the literature to the pro-
posed method is the the length tuner method pro-
posed by (Chang et al., 2008). This method also
encourages the generation of more words during
segmentation by using a single parameter that can
be use to control segment length. Our method dif-
fers from theirs in that it is able to acquire vocabu-
lary knowledge from word alignments that can be
used to more accurately split words into segments
suitable for machine translation.
There is large volume of research using bilin-
gual unsupervised and semi-supervised WS to ad-
dress the problem of optimizing WS for SMT (Xu
et al., 2008; Chung and Gildea, 2009; Nguyen et
al., 2010; Michael et al., 2011). The main differ-
ence with our approach is that they use automatic
WA results, most often obtained using the same
tools as are used in training SMT systems. One of
the main problems of using unsupervised WA is
that it is noisy, and therefore, employing iterative
optimization methods to refine the results of unsu-
pervised WA is a key issue in their research, for
example boosting (Ma and Way, 2009; Michael et
al., 2011), expectation maximization (Chung and
Gildea, 2009), Bayesian sampling (Xu et al., 2008;
Nguyen et al., 2010), or heuristic search (Zhao et
al., 2013). Nevertheless, noisy WA makes both
analyzing WS and improving SMT quality quite
hard. In contrast, by using manual WA, we can
clearly analyze the segmentation problems (Sec-
tion 2), and train supervised models to solve the
problem (Section 3).
As far as we are aware, among related work
on WS, our method achieves the highest BLEU
improvement relative to the start-of-the-art WS –
the Stanford Chinese word segmenter – on the
Chinese-English OpenMT corpora. The meth-
ods proposed in (Ma and Way, 2009; Chung
and Gildea, 2009) fail to outperform the Stan-
ford Chinese word segmenter on Chinese-English
OpenMT corpora. The length tuner method pro-
posed in (Chang et al., 2008) is less effective to
ours according to the experimental results in this
paper.
</bodyText>
<page confidence="0.950416">
1661
</page>
<table confidence="0.99951625">
WS eval03 eval04 eval05 eval06 improve
CTB segmenter 31.89 f 0.09 32.73 f 0.19 31.03 f 0.16 31.38 f 0.23
Length tuner 32.06 f 0.07 32.74 f 0.10 31.34 f 0.11 31.50 f 0.11 0.15 f 0.12
Lexical splitter 32.55 f 0.18 32.94 f 0.11 31.87 f 0.15 32.17 f 0.35 0.63 f 0.29
</table>
<tableCaption confidence="0.999185">
Table 8: Performance (BLEU) of SMT
</tableCaption>
<figure confidence="0.558783">
ya zhou hua ren huan xi ying yang nian pan wang xin nian jing ji xiang rong jing
gVii * A V-4ATT� �V_ ,Vr * C&apos;_- VF �IFIV
</figure>
<figureCaption confidence="0.597387">
Figure 6: Example of SMT from test sets. (a) source; (b) CTB segmenter; (c) length tuner; (d) lexical
splitter. The four gold references are: “ethnic chinese in asia celebrate year of goat and hope for economic
prosperity in new year”, “ asian chinese celebrate the arrival of the year of sheep and wish a prosperous
new year”, “ asian chinese happily welcome the year of goat , expecting economic prosperity in new
year”,“asian chinese happily welcomed year of the goat, praying for prosperity in the new year”
</figureCaption>
<figure confidence="0.999159258064516">
M H *A
A14 �
T_* *V_
fir* LVF
fh g
A
asian chinese
through to
in view of new
economy
&lt;unk&gt; lookforwardto
the present rong,
M H *A
fh g�:
A14 �
&lt;unk&gt;
now
asian to welcome the chinese
newyear through
T-* *V_ fir* L=VF
booming economic
M H *A
fh g�:
A14 AI
� � �� � � ��
booming
if the asian
to celebrate the
economic now
chinese newyear
yearofthe goat
</figure>
<sectionHeader confidence="0.965703" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999965">
This paper is concerned with the role of word
segmentation in Chinese-to-English SMT. We ex-
plored the use of a manually annotated word align-
ment corpus to refine word segmentation for ma-
chine translation. Based on an initial finding that
74.60% of running sentences in the WA corpus
have segmentation inconsistent with a gold WA
annotation, we proposed a supervised lexical re-
segmentation model to modify the WS in order to
relieve these issues.
Our main experimental results show that the
proposed approach is capable of improving both
alignment quality and end-to-end translation qual-
ity. The proposed method achieved the highest
BLEU score relative to a number of respectable
baseline systems that included the Stanford word
segmenter, and an improved Stanford word seg-
menter that could be tuned for segment length. No
language-specific techniques other than a manu-
ally aligned corpus were employed in this paper,
thus the approach can applied to other SMT lan-
guage pairs that require WS.
In the future, we plan to explore combining
multiple source words which are aligned to the
same target words. This is the symmetric topic
of the post word splitting which is studied in this
paper. The effect of this word combination oper-
</bodyText>
<page confidence="0.982511">
1662
</page>
<bodyText confidence="0.908086333333333">
ation on SMT is non-trivial. On one hand, it can
reduce the ambiguity in the source side. On the
other hand, it may cause sparseness problems.
</bodyText>
<note confidence="0.845572571428571">
Acknowledgements
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning, pages 282–
289. Association for Computing Machinery.
</note>
<bodyText confidence="0.9998625">
We thank the three reviewers for their valuable
comments. We also thank the Stanford natural lan-
guage processing group for releasing the source
codes of their word segmenter.
</bodyText>
<sectionHeader confidence="0.997763" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998235237410072">
Pi-Chuan Chang, Michel Galley, and Christopher D
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the 3rd Workshop on Statistical Machine
Translation, pages 224–232. Association for Com-
putational Linguistics.
Tagyoung Chung and Daniel Gildea. 2009. Unsu-
pervised tokenization for machine translation. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
2-Volume 2, pages 718–726. Association for Com-
putational Linguistics.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL,
volume 45, page 17.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363–370. Association for Computational Lin-
guistics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings ofNTCIR, volume 9, pages 559–578.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with su-
pervised itg models. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 2-
Volume 2, pages 923–931. Association for Compu-
tational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177–180. Association for Computational Lin-
guistics.
Xuansong Li, Niyu Ge, Stephen Grimes, Stephanie
Strassel, and Kazuaki Maeda. 2010. Enriching
word alignment with linguistic tags. In LREC, pages
2189–2195.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 104–
111. Association for Computational Linguistics.
Yanjun Ma and Andy Way. 2009. Bilingually moti-
vated domain-adapted word segmentation for statis-
tical machine translation. In Proceedings of the 12th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 549–557.
Association for Computational Linguistics.
Paul Michael, Andrew Finch, and Eiichiro Sumita.
2011. Integration of multiple bilingually-trained
segmentation schemes into statistical machine trans-
lation. IEICE transactions on information and sys-
tems, 94(3):690–697.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation
exercise for word alignment. In Rada Mihalcea and
Ted Pedersen, editors, HLT-NAACL 2003 Workshop:
Building and Using Parallel Texts: Data Driven Ma-
chine Translation and Beyond, pages 1–10, Edmon-
ton, Alberta, Canada, May 31. Association for Com-
putational Linguistics.
ThuyLinh Nguyen, Stephan Vogel, and Noah A Smith.
2010. Nonparametric word segmentation for ma-
chine translation. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 815–823. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2000. A com-
parison of alignment models for statistical machine
translation. In Proceedings of the 18th confer-
ence on Computational linguistics-Volume 2, pages
1086–1090. Association for Computational Linguis-
tics.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160–167. As-
sociation for Computational Linguistics.
1663
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311–318. Association
for Computational Linguistics.
Fuchun Peng and Andrew McCallum. 2006. Infor-
mation extraction from research papers using con-
ditional random fields. Information Processing &amp;
Management, 42(4):963–979.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. Computer Sci-
ence Department Faculty Publication Series.
Richard Sproat and Thomas Emerson. 2003. The
first international chinese word segmentation bake-
off. In Proceedings of the second SIGHAN work-
shop on Chinese language processing-Volume 17,
pages 133–143. Association for Computational Lin-
guistics.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter for SIGHAN
Bakeoff 2005. In Proceedings of the 4th SIGHAN
Workshop on Chinese Language Processing, volume
171. Jeju Island, Korea.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised
Chinese word segmentation for statistical machine
translation. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics-
Volume 1, pages 1017–1024. Association for Com-
putational Linguistics.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese cor-
pus. In Proceedings of the 19th international confer-
ence on Computational linguistics-Volume 1, pages
1–8. Association for Computational Linguistics.
Richard Zens, Franz Josef Och, and Hermann Ney.
2002. Phrase-based statistical machine translation.
In Advances in Artificial Intelligence, pages 18–32.
Springer.
Hai Zhao, Masao Utiyama, Eiichro Sumita, and Bao-
Liang Lu. 2013. An empirical study on word seg-
mentation for chinese machine translation. A. Gel-
bukh (Ed.): CICLing 2013, Part II, LNCS 7817,
pages 248–263.
</reference>
<page confidence="0.994953">
1664
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.696521">
<title confidence="0.9996405">Refining Word Segmentation Using a Manually Aligned for Statistical Machine Translation</title>
<author confidence="0.783776">Xiaolin Wang Masao Utiyama Andrew Finch Eiichiro Sumita</author>
<affiliation confidence="0.787598">National Institute of Information and Communications</affiliation>
<abstract confidence="0.996493928571428">Languages that have no explicit word delimiters often have to be segmented for statistical machine translation (SMT). This is commonly performed by automated segmenters trained on manually annotated corpora. However, the word segmentation (WS) schemes of these annotated corpora are handcrafted for general usage, and may not be suitable for SMT. An analysis was performed to test this hypothesis using a manually annotated word alignment (WA) corpus for Chinese-English SMT. An analysis revealed that 74.60% of the sentences in the WA corpus if segmented using an automated segmenter trained on the Penn Chinese Treebank (CTB) will contain conflicts with the gold WA annotations. We formulated an approach based on word splitting with reference to the annotated WA to alleviate these conflicts. Experimental results show that the refined WS reduced word alignment error rate by 6.82% and achieved the highest BLEU improvement (0.63 on average) on the Chinese-English open machine translation (OpenMT) corpora compared to related work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd Workshop on Statistical Machine Translation,</booktitle>
<pages>224--232</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2401" citStr="Chang et al., 2008" startWordPosition="359" endWordPosition="362">s. Automated word segmenters built through supervised-learning methods, after decades of intensive research, have emerged as effective solutions to WS tasks and become widely used in many NLP applications. For example, the Stanford word segmenter (Xue et al., 2002)1 which is based on conditional random field (CRF) is employed to prepare the official corpus for NTCIR9 Chinese-English patent translation task (Goto et al., 2011). However, one problem with applying these supervised-learning word segmenters to SMT is that the WS scheme of annotating the training corpus may not be optimal for SMT. (Chang et al., 2008) noticed that the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently </context>
<context position="14416" citStr="Chang et al., 2008" startWordPosition="2405" endWordPosition="2408"> one; fk(yt−1, yt, x, t) is a feature function which is often a binary-valued sparse feature. The training of CRF model is to maximize the likelihood of training data together with a regularization penalty to avoid over-fitting as (Peng et al., 2004; Peng and McCallum, 2006), �A* = argmax( A i (3) where (x,y) are training samples; the hyperparameter δk can be understood as the variance of the prior distribution of λk. When predicting the labels of test samples, the CRF decoder searches for the optimal label sequence y* that maximizes the conditional probability, y* = argmax PA(y|x). (4) Y In (Chang et al., 2008) a method is proposed to select an appropriate level of segmentation granularity (in practical terms, to encourage smaller segments). We call their method “length tuner”. The following artificial feature is introduced into the learned CRF model: _� 1 if yt = +1 f0(x, yt−1, yt,1) 0 otherwise (5) The weight λ0 of this feature is set by hand to bias the output of CRF model. By way of explanation, a very large positive λ0 will cause every character to be segmented, or conversely a very large negative λ0 will inhibit the output of segmentation boundaries. In their experiments, λ0 = 2 was used to fo</context>
<context position="22401" citStr="Chang et al., 2008" startWordPosition="3720" endWordPosition="3723"> was measured through alignment error rate (AER)(Och and Ney, 2000; Liang et al., 2006). Sure alignment edges and possible alignment edges were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The performance of SMT was measured using BLEU (Papineni et al., 2002). 7http://nlp.stanford.edu/software/ corenlp.shtml 4.1.3 Baseline Methods Two Chinese WS methods were taken as the baseline methods in this paper. One method was the CTB segmenter, that is, Stanford Chinese word segmenter with the model trained on CTB corpus. The other method was the length tuner in (Chang et al., 2008), which added a constant into the confidence scores of a trained CRF word segmenter to encourage it to output more word boundaries (see Section 3.2 for details). 4.1.4 Implementation and Parameter settings The proposed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter (Tseng et al., 2005). The regularity parameters Sk are set to be 3, the same as the Stanford segmenter, because no significant performance improvements were observed by tuning that parameter. To extract features for the word splitter, the Stanford named entity recognizer (Finkel e</context>
<context position="24898" citStr="Chang et al., 2008" startWordPosition="4115" endWordPosition="4118">73 0.894 0.852 Lexical splitter 0.915 0.922 0.908 Table 6: Performance of WS WS AER Prec. Recall CTB segmenter 0.425 0.622 0.534 Length tuner 0.417 0.642 0.535 Lexical splitter 0.396 0.674 0.547 Table 7: Performance of unsupervised WA using different WS strategies 4.2.2 Word Alignment The WA performance of the CTB segmenter, length tuner and the proposed lexical spliter is presented in Table 7. Both lexical splitter and length tuner outperform the CTB segmenter, indicating the splitting words into smaller pieces can improve the accuracy of unsupervised WA. This result supports the finding in (Chang et al., 2008) that the segment size from CTB WS is too large for SMT. In addition, the proposed lexical splitter significantly outperforms the length tuner. 4.2.3 Machine Translation The end-to-end SMT performance of CTB segmenter, length tuner and the proposed lexical spliter are presented in Table 8. Each experiment was performed three times, and the average BLEU and standard derivation were calculated, because there is randomness in the results from MERT. The proposed lexical splitter outperformed the two baselines on all the test sets, and achieves an average improvement of 0.63 BLEU percentage points,</context>
<context position="26485" citStr="Chang et al., 2008" startWordPosition="4374" endWordPosition="4377">re accurately than the baseline length tuner method. Two words in the segmentation result of the CTB segmenter are worthy of attention. The first one is “yang nian”(the year of goat), the lexical splitter split this word and got the right translation, while the length tuner did not split it. The second is “rong jing”(booming or prosperity), the length tuner split this word, which resulted in wrong translations, while the lexical splitter avoided this mistake. 5 Comparison to Related Work The most similar work in the literature to the proposed method is the the length tuner method proposed by (Chang et al., 2008). This method also encourages the generation of more words during segmentation by using a single parameter that can be use to control segment length. Our method differs from theirs in that it is able to acquire vocabulary knowledge from word alignments that can be used to more accurately split words into segments suitable for machine translation. There is large volume of research using bilingual unsupervised and semi-supervised WS to address the problem of optimizing WS for SMT (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Michael et al., 2011). The main difference with our ap</context>
<context position="28293" citStr="Chang et al., 2008" startWordPosition="4679" endWordPosition="4682"> improving SMT quality quite hard. In contrast, by using manual WA, we can clearly analyze the segmentation problems (Section 2), and train supervised models to solve the problem (Section 3). As far as we are aware, among related work on WS, our method achieves the highest BLEU improvement relative to the start-of-the-art WS – the Stanford Chinese word segmenter – on the Chinese-English OpenMT corpora. The methods proposed in (Ma and Way, 2009; Chung and Gildea, 2009) fail to outperform the Stanford Chinese word segmenter on Chinese-English OpenMT corpora. The length tuner method proposed in (Chang et al., 2008) is less effective to ours according to the experimental results in this paper. 1661 WS eval03 eval04 eval05 eval06 improve CTB segmenter 31.89 f 0.09 32.73 f 0.19 31.03 f 0.16 31.38 f 0.23 Length tuner 32.06 f 0.07 32.74 f 0.10 31.34 f 0.11 31.50 f 0.11 0.15 f 0.12 Lexical splitter 32.55 f 0.18 32.94 f 0.11 31.87 f 0.15 32.17 f 0.35 0.63 f 0.29 Table 8: Performance (BLEU) of SMT ya zhou hua ren huan xi ying yang nian pan wang xin nian jing ji xiang rong jing gVii * A V-4ATT� �V_ ,Vr * C&apos;_- VF �IFIV Figure 6: Example of SMT from test sets. (a) source; (b) CTB segmenter; (c) length tuner; (d) l</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Christopher D Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proceedings of the 3rd Workshop on Statistical Machine Translation, pages 224–232. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Daniel Gildea</author>
</authors>
<title>Unsupervised tokenization for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>718--726</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16206" citStr="Chung and Gildea, 2009" startWordPosition="2708" endWordPosition="2711">the boundaries in the CTB WS scheme. The extended features consists of four types – named entities, word frequency, word length and character-level unsupervised WA. For each type of the feature, the value and value concatenated with previous or current character are taken as sparse features (see Table 4 for details). The real values of word frequency, word length and characterlevel unsupervised WA are converted into sparse features due to the routine of CRF model. The character-level unsupervised alignment feature is inspired by the related works of unsupervised bilingual WS (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Michael et al., 2011). The idea is that the character-level WA can approximately capture the counterpart English expression of each Chinese token, and source tokens aligned to different target expressions should be split into different words (see Figure 4 for an illustration). The values of the character-level alignment features are obtained through building a dictionary. First, unsupervised WA is performed on the SMT training corpus where the Chinese sentences are treated as sequences of characters; then, the Chinese sentences are segmented by CTB segmenter and a dictio</context>
<context position="27008" citStr="Chung and Gildea, 2009" startWordPosition="4463" endWordPosition="4466">e literature to the proposed method is the the length tuner method proposed by (Chang et al., 2008). This method also encourages the generation of more words during segmentation by using a single parameter that can be use to control segment length. Our method differs from theirs in that it is able to acquire vocabulary knowledge from word alignments that can be used to more accurately split words into segments suitable for machine translation. There is large volume of research using bilingual unsupervised and semi-supervised WS to address the problem of optimizing WS for SMT (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Michael et al., 2011). The main difference with our approach is that they use automatic WA results, most often obtained using the same tools as are used in training SMT systems. One of the main problems of using unsupervised WA is that it is noisy, and therefore, employing iterative optimization methods to refine the results of unsupervised WA is a key issue in their research, for example boosting (Ma and Way, 2009; Michael et al., 2011), expectation maximization (Chung and Gildea, 2009), Bayesian sampling (Xu et al., 2008; Nguyen et al., 2010), or heuristic search (Zhao</context>
</contexts>
<marker>Chung, Gildea, 2009</marker>
<rawString>Tagyoung Chung and Daniel Gildea. 2009. Unsupervised tokenization for machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 718–726. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Tailoring word alignments to syntactic machine translation.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<volume>45</volume>
<pages>17</pages>
<contexts>
<context position="3055" citStr="DeNero and Klein, 2007" startWordPosition="475" endWordPosition="478">re often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3. The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,137 English words, and 421,763 1http://nlp.stanford.edu/software/ segmenter.shtml 2http://catalog.ldc.upenn.edu 3Catalog numbers: LDC2012T16, LDC2012T20, LDC2012T24 and LDC2013T05. 1654 Proceedings of the 2014 Conference on Empirical Methods in </context>
</contexts>
<marker>DeNero, Klein, 2007</marker>
<rawString>John DeNero and Dan Klein. 2007. Tailoring word alignments to syntactic machine translation. In ACL, volume 45, page 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="23013" citStr="Finkel et al., 2005" startWordPosition="3818" endWordPosition="3821">., 2008), which added a constant into the confidence scores of a trained CRF word segmenter to encourage it to output more word boundaries (see Section 3.2 for details). 4.1.4 Implementation and Parameter settings The proposed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter (Tseng et al., 2005). The regularity parameters Sk are set to be 3, the same as the Stanford segmenter, because no significant performance improvements were observed by tuning that parameter. To extract features for the word splitter, the Stanford named entity recognizer (Finkel et al., 2005)8 was employed to obtain the tags of named entities. Word frequencies were caculated from the source side of SMT training corpus. The character-level unsupervised alignment was conducted using GIZA++ (Och and Ney, 2003)9. The length tuner reused the CRF model of CTB segmenter. The parameter A0 was tuned through the grid search in (Chang et al., 2008), that is, observing the BLEU score on the SMT development set varing from A0 = 0 to A0 = 32. The grid search showed that A0 = 2 was optimal, agreeing with the value in (Chang et al., 2008). Moses (Koehn et al., 2007)10, a state-of-the-art phrase-b</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Bin Lu</author>
<author>Ka Po Chow</author>
<author>Eiichiro Sumita</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Overview of the patent machine translation task at the NTCIR-9 workshop.</title>
<date>2011</date>
<booktitle>In Proceedings ofNTCIR,</booktitle>
<volume>9</volume>
<pages>559--578</pages>
<contexts>
<context position="2211" citStr="Goto et al., 2011" startWordPosition="327" endWordPosition="330"> thus it has a large potential impact on the final performance. For SMT, the unsupervised WA, building translation models and reordering models, and decoding are all based on segmented words. Automated word segmenters built through supervised-learning methods, after decades of intensive research, have emerged as effective solutions to WS tasks and become widely used in many NLP applications. For example, the Stanford word segmenter (Xue et al., 2002)1 which is based on conditional random field (CRF) is employed to prepare the official corpus for NTCIR9 Chinese-English patent translation task (Goto et al., 2011). However, one problem with applying these supervised-learning word segmenters to SMT is that the WS scheme of annotating the training corpus may not be optimal for SMT. (Chang et al., 2008) noticed that the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000)</context>
</contexts>
<marker>Goto, Lu, Chow, Sumita, Tsou, 2011</marker>
<rawString>Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and Benjamin K Tsou. 2011. Overview of the patent machine translation task at the NTCIR-9 workshop. In Proceedings ofNTCIR, volume 9, pages 559–578.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John Blitzer</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Better word alignments with supervised itg models.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>923--931</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3079" citStr="Haghighi et al., 2009" startWordPosition="479" endWordPosition="482">T. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3. The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,137 English words, and 421,763 1http://nlp.stanford.edu/software/ segmenter.shtml 2http://catalog.ldc.upenn.edu 3Catalog numbers: LDC2012T16, LDC2012T20, LDC2012T24 and LDC2013T05. 1654 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Process</context>
</contexts>
<marker>Haghighi, Blitzer, DeNero, Klein, 2009</marker>
<rawString>Aria Haghighi, John Blitzer, John DeNero, and Dan Klein. 2009. Better word alignments with supervised itg models. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 923–931. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="23582" citStr="Koehn et al., 2007" startWordPosition="3921" endWordPosition="3924">ford named entity recognizer (Finkel et al., 2005)8 was employed to obtain the tags of named entities. Word frequencies were caculated from the source side of SMT training corpus. The character-level unsupervised alignment was conducted using GIZA++ (Och and Ney, 2003)9. The length tuner reused the CRF model of CTB segmenter. The parameter A0 was tuned through the grid search in (Chang et al., 2008), that is, observing the BLEU score on the SMT development set varing from A0 = 0 to A0 = 32. The grid search showed that A0 = 2 was optimal, agreeing with the value in (Chang et al., 2008). Moses (Koehn et al., 2007)10, a state-of-the-art phrase-based SMT system, was employed to perform end-to-end SMT experiments. GIZA++ was employed to perform unsupervised WA. 4.2 Experimental Results 4.2.1 Word Segmentation The WS performance of CTB segmenter, length tuner and the proposed lexical splitter are presented in Table 6. The proposed method achieves the highest scores on all the criterion of F1, precision and recall. The length tuner outperforms the CTB segmenter in terms of recall, but with lower precision. 8http://nlp.stanford.edu/software/ CRF-NER.shtml 9http://www.statmt.org/moses/giza/ GIZA++.html 10http</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuansong Li</author>
<author>Niyu Ge</author>
<author>Stephen Grimes</author>
<author>Stephanie Strassel</author>
<author>Kazuaki Maeda</author>
</authors>
<title>Enriching word alignment with linguistic tags.</title>
<date>2010</date>
<booktitle>In LREC,</booktitle>
<pages>2189--2195</pages>
<contexts>
<context position="6370" citStr="Li et al., 2010" startWordPosition="996" endWordPosition="999">irst briefly describes the GALE WA corpus, then presents an analysis of the WS arising from a CTB-standard word segmenter with reference to the segmentation of the atomic blocks in the GALE WA corpus, finally the impact of the findings on SMT is discussed. 2.1 GALE WA corpus The GALE WA corpus was developed by the LDC, and was used as training data in the DARPA GALE global autonomous language exploitation program 5. The corpus incorporates linguistic knowledge into word aligned text to help improve automatic WA and translation quality. It employs two annotation schemes: alignment and tagging (Li et al., 2010). Alignment identifies minimum translation units and translation relations; tagging adds contextual, syntactic and languagespecific features to the alignment annotation. For example, the sample shown in Figure 1 carries tags on both alignment edges and tokens. The GALE WA corpus contains 18,057 manually word aligned Chinese and English parallel sentences which are extracted from newswire and web blogs. Table 1 presents the statistics on the corpus. One third of the sentences are approximately newswire text, and the remainder consists of web blogs. 2.2 Analysis of WS In order to produce a Chine</context>
</contexts>
<marker>Li, Ge, Grimes, Strassel, Maeda, 2010</marker>
<rawString>Xuansong Li, Niyu Ge, Stephen Grimes, Stephanie Strassel, and Kazuaki Maeda. 2010. Enriching word alignment with linguistic tags. In LREC, pages 2189–2195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3031" citStr="Liang et al., 2006" startWordPosition="471" endWordPosition="474">t the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3. The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,137 English words, and 421,763 1http://nlp.stanford.edu/software/ segmenter.shtml 2http://catalog.ldc.upenn.edu 3Catalog numbers: LDC2012T16, LDC2012T20, LDC2012T24 and LDC2013T05. 1654 Proceedings of the 2014 Conference </context>
<context position="21869" citStr="Liang et al., 2006" startWordPosition="3634" endWordPosition="3637">The experimental corpus for this task was the NIST OpenMT corpus. The data set of the NIST evaluation 2002 was used as a development set for MERT tuning (Och, 2003), and the remaining data sets of the NIST evaluation from 2003 to 2006 were used as test sets. The English sentences were tokenized by Stanford toolkit 7 and converted to lowercase. 4.1.2 Evaluation The performance of WS was measured by precision, recall and F1 of gold words (Sproat and Emerson, 2003), The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)(Och and Ney, 2000; Liang et al., 2006). Sure alignment edges and possible alignment edges were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The performance of SMT was measured using BLEU (Papineni et al., 2002). 7http://nlp.stanford.edu/software/ corenlp.shtml 4.1.3 Baseline Methods Two Chinese WS methods were taken as the baseline methods in this paper. One method was the CTB segmenter, that is, Stanford Chinese word segmenter with the model trained on CTB corpus. The other method was the length tuner in (Chang et al., 2008), which added a constant into the confidence scores of a trained CRF</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 104– 111. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanjun Ma</author>
<author>Andy Way</author>
</authors>
<title>Bilingually motivated domain-adapted word segmentation for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>549--557</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="27449" citStr="Ma and Way, 2009" startWordPosition="4541" endWordPosition="4544"> There is large volume of research using bilingual unsupervised and semi-supervised WS to address the problem of optimizing WS for SMT (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Michael et al., 2011). The main difference with our approach is that they use automatic WA results, most often obtained using the same tools as are used in training SMT systems. One of the main problems of using unsupervised WA is that it is noisy, and therefore, employing iterative optimization methods to refine the results of unsupervised WA is a key issue in their research, for example boosting (Ma and Way, 2009; Michael et al., 2011), expectation maximization (Chung and Gildea, 2009), Bayesian sampling (Xu et al., 2008; Nguyen et al., 2010), or heuristic search (Zhao et al., 2013). Nevertheless, noisy WA makes both analyzing WS and improving SMT quality quite hard. In contrast, by using manual WA, we can clearly analyze the segmentation problems (Section 2), and train supervised models to solve the problem (Section 3). As far as we are aware, among related work on WS, our method achieves the highest BLEU improvement relative to the start-of-the-art WS – the Stanford Chinese word segmenter – on the C</context>
</contexts>
<marker>Ma, Way, 2009</marker>
<rawString>Yanjun Ma and Andy Way. 2009. Bilingually motivated domain-adapted word segmentation for statistical machine translation. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 549–557. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Michael</author>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Integration of multiple bilingually-trained segmentation schemes into statistical machine translation. IEICE transactions on information and systems,</title>
<date>2011</date>
<pages>94--3</pages>
<contexts>
<context position="16250" citStr="Michael et al., 2011" startWordPosition="2716" endWordPosition="2719">nded features consists of four types – named entities, word frequency, word length and character-level unsupervised WA. For each type of the feature, the value and value concatenated with previous or current character are taken as sparse features (see Table 4 for details). The real values of word frequency, word length and characterlevel unsupervised WA are converted into sparse features due to the routine of CRF model. The character-level unsupervised alignment feature is inspired by the related works of unsupervised bilingual WS (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Michael et al., 2011). The idea is that the character-level WA can approximately capture the counterpart English expression of each Chinese token, and source tokens aligned to different target expressions should be split into different words (see Figure 4 for an illustration). The values of the character-level alignment features are obtained through building a dictionary. First, unsupervised WA is performed on the SMT training corpus where the Chinese sentences are treated as sequences of characters; then, the Chinese sentences are segmented by CTB segmenter and a dictionary of segmented words are built; finally, </context>
<context position="27052" citStr="Michael et al., 2011" startWordPosition="4471" endWordPosition="4474">e length tuner method proposed by (Chang et al., 2008). This method also encourages the generation of more words during segmentation by using a single parameter that can be use to control segment length. Our method differs from theirs in that it is able to acquire vocabulary knowledge from word alignments that can be used to more accurately split words into segments suitable for machine translation. There is large volume of research using bilingual unsupervised and semi-supervised WS to address the problem of optimizing WS for SMT (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Michael et al., 2011). The main difference with our approach is that they use automatic WA results, most often obtained using the same tools as are used in training SMT systems. One of the main problems of using unsupervised WA is that it is noisy, and therefore, employing iterative optimization methods to refine the results of unsupervised WA is a key issue in their research, for example boosting (Ma and Way, 2009; Michael et al., 2011), expectation maximization (Chung and Gildea, 2009), Bayesian sampling (Xu et al., 2008; Nguyen et al., 2010), or heuristic search (Zhao et al., 2013). Nevertheless, noisy WA makes</context>
</contexts>
<marker>Michael, Finch, Sumita, 2011</marker>
<rawString>Paul Michael, Andrew Finch, and Eiichiro Sumita. 2011. Integration of multiple bilingually-trained segmentation schemes into statistical machine translation. IEICE transactions on information and systems, 94(3):690–697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Ted Pedersen</author>
</authors>
<title>An evaluation exercise for word alignment.</title>
<date>2003</date>
<booktitle>HLT-NAACL 2003 Workshop: Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,</booktitle>
<pages>1--10</pages>
<editor>In Rada Mihalcea and Ted Pedersen, editors,</editor>
<location>Edmonton, Alberta, Canada,</location>
<contexts>
<context position="2975" citStr="Mihalcea and Pedersen, 2003" startWordPosition="462" endWordPosition="465">orpus may not be optimal for SMT. (Chang et al., 2008) noticed that the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3. The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,137 English words, and 421,763 1http://nlp.stanford.edu/software/ segmenter.shtml 2http://catalog.ldc.upenn.edu 3Catalog numbers: LDC2012T16, LDC2012T20, LDC2012T24 </context>
</contexts>
<marker>Mihalcea, Pedersen, 2003</marker>
<rawString>Rada Mihalcea and Ted Pedersen. 2003. An evaluation exercise for word alignment. In Rada Mihalcea and Ted Pedersen, editors, HLT-NAACL 2003 Workshop: Building and Using Parallel Texts: Data Driven Machine Translation and Beyond, pages 1–10, Edmonton, Alberta, Canada, May 31. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ThuyLinh Nguyen</author>
<author>Stephan Vogel</author>
<author>Noah A Smith</author>
</authors>
<title>Nonparametric word segmentation for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>815--823</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16227" citStr="Nguyen et al., 2010" startWordPosition="2712" endWordPosition="2715">B WS scheme. The extended features consists of four types – named entities, word frequency, word length and character-level unsupervised WA. For each type of the feature, the value and value concatenated with previous or current character are taken as sparse features (see Table 4 for details). The real values of word frequency, word length and characterlevel unsupervised WA are converted into sparse features due to the routine of CRF model. The character-level unsupervised alignment feature is inspired by the related works of unsupervised bilingual WS (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Michael et al., 2011). The idea is that the character-level WA can approximately capture the counterpart English expression of each Chinese token, and source tokens aligned to different target expressions should be split into different words (see Figure 4 for an illustration). The values of the character-level alignment features are obtained through building a dictionary. First, unsupervised WA is performed on the SMT training corpus where the Chinese sentences are treated as sequences of characters; then, the Chinese sentences are segmented by CTB segmenter and a dictionary of segmented wor</context>
<context position="27029" citStr="Nguyen et al., 2010" startWordPosition="4467" endWordPosition="4470">osed method is the the length tuner method proposed by (Chang et al., 2008). This method also encourages the generation of more words during segmentation by using a single parameter that can be use to control segment length. Our method differs from theirs in that it is able to acquire vocabulary knowledge from word alignments that can be used to more accurately split words into segments suitable for machine translation. There is large volume of research using bilingual unsupervised and semi-supervised WS to address the problem of optimizing WS for SMT (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Michael et al., 2011). The main difference with our approach is that they use automatic WA results, most often obtained using the same tools as are used in training SMT systems. One of the main problems of using unsupervised WA is that it is noisy, and therefore, employing iterative optimization methods to refine the results of unsupervised WA is a key issue in their research, for example boosting (Ma and Way, 2009; Michael et al., 2011), expectation maximization (Chung and Gildea, 2009), Bayesian sampling (Xu et al., 2008; Nguyen et al., 2010), or heuristic search (Zhao et al., 2013). Never</context>
</contexts>
<marker>Nguyen, Vogel, Smith, 2010</marker>
<rawString>ThuyLinh Nguyen, Stephan Vogel, and Noah A Smith. 2010. Nonparametric word segmentation for machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 815–823. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A comparison of alignment models for statistical machine translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics-Volume 2,</booktitle>
<pages>1086--1090</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2811" citStr="Och and Ney, 2000" startWordPosition="436" endWordPosition="439">Goto et al., 2011). However, one problem with applying these supervised-learning word segmenters to SMT is that the WS scheme of annotating the training corpus may not be optimal for SMT. (Chang et al., 2008) noticed that the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3. The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,1</context>
<context position="10238" citStr="Och and Ney, 2000" startWordPosition="1660" endWordPosition="1663">ct of WS on SMT The word alignment has a direct impact on the nature of both the translation model, and lexical reordering model in a phrase-base SMT system. The words in last two categories are all longer than an atomic block, which might lead to problems in the word alignment in two ways: • First, longer words tend to be more sparse in the training corpus, thus the estimated distribution of their target phrases are less accurate. • Second, the alignment from them to target sides are one-to-many, which is much more complicated and requires fertilized alignment models such as IBM model 4 – 6 (Och and Ney, 2000). The words in the category of “fully consistent” can be aligned using simple models, because the alignment from them to the target side are one-toone or many-to-one, and simple alignment models such as IBM model 1, IBM model 2 and HMM model are sufficient (Och and Ney, 2000). 3 Refining the Word Segmentation In the last subsection, it was shown that 74.60% of parallel sentences were affected by issues related to under-segmentation of the corpus. Our hypothesis is that if these words are split into pieces that match English words, the accuracy of the unsupervised WA as well as the translation </context>
<context position="21848" citStr="Och and Ney, 2000" startWordPosition="3630" endWordPosition="3633"> of an SMT system. The experimental corpus for this task was the NIST OpenMT corpus. The data set of the NIST evaluation 2002 was used as a development set for MERT tuning (Och, 2003), and the remaining data sets of the NIST evaluation from 2003 to 2006 were used as test sets. The English sentences were tokenized by Stanford toolkit 7 and converted to lowercase. 4.1.2 Evaluation The performance of WS was measured by precision, recall and F1 of gold words (Sproat and Emerson, 2003), The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)(Och and Ney, 2000; Liang et al., 2006). Sure alignment edges and possible alignment edges were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The performance of SMT was measured using BLEU (Papineni et al., 2002). 7http://nlp.stanford.edu/software/ corenlp.shtml 4.1.3 Baseline Methods Two Chinese WS methods were taken as the baseline methods in this paper. One method was the CTB segmenter, that is, Stanford Chinese word segmenter with the model trained on CTB corpus. The other method was the length tuner in (Chang et al., 2008), which added a constant into the confidence sc</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. A comparison of alignment models for statistical machine translation. In Proceedings of the 18th conference on Computational linguistics-Volume 2, pages 1086–1090. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="23232" citStr="Och and Ney, 2003" startWordPosition="3852" endWordPosition="3855">posed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter (Tseng et al., 2005). The regularity parameters Sk are set to be 3, the same as the Stanford segmenter, because no significant performance improvements were observed by tuning that parameter. To extract features for the word splitter, the Stanford named entity recognizer (Finkel et al., 2005)8 was employed to obtain the tags of named entities. Word frequencies were caculated from the source side of SMT training corpus. The character-level unsupervised alignment was conducted using GIZA++ (Och and Ney, 2003)9. The length tuner reused the CRF model of CTB segmenter. The parameter A0 was tuned through the grid search in (Chang et al., 2008), that is, observing the BLEU score on the SMT development set varing from A0 = 0 to A0 = 32. The grid search showed that A0 = 2 was optimal, agreeing with the value in (Chang et al., 2008). Moses (Koehn et al., 2007)10, a state-of-the-art phrase-based SMT system, was employed to perform end-to-end SMT experiments. GIZA++ was employed to perform unsupervised WA. 4.2 Experimental Results 4.2.1 Word Segmentation The WS performance of CTB segmenter, length tuner and</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="21414" citStr="Och, 2003" startWordPosition="3559" endWordPosition="3560">nce samples which contain one Chinese sentence and four English reference sentences. The experimental corpus for unsupervised WA was the union set of the NIST OpenMT training set and the 2000 test sentence pairs from GALE WA corpus. We removed the United Nations corpus from the NIST OpenMT constraint training resources because it is out of domain. The main result of this paper is the evaluation of the end-to-end performance of an SMT system. The experimental corpus for this task was the NIST OpenMT corpus. The data set of the NIST evaluation 2002 was used as a development set for MERT tuning (Och, 2003), and the remaining data sets of the NIST evaluation from 2003 to 2006 were used as test sets. The English sentences were tokenized by Stanford toolkit 7 and converted to lowercase. 4.1.2 Evaluation The performance of WS was measured by precision, recall and F1 of gold words (Sproat and Emerson, 2003), The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)(Och and Ney, 2000; Liang et al., 2006). Sure alignment edges and possible alignment edges were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160–167. Association for Computational Linguistics. 1663</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="22080" citStr="Papineni et al., 2002" startWordPosition="3670" endWordPosition="3673">aluation from 2003 to 2006 were used as test sets. The English sentences were tokenized by Stanford toolkit 7 and converted to lowercase. 4.1.2 Evaluation The performance of WS was measured by precision, recall and F1 of gold words (Sproat and Emerson, 2003), The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)(Och and Ney, 2000; Liang et al., 2006). Sure alignment edges and possible alignment edges were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The performance of SMT was measured using BLEU (Papineni et al., 2002). 7http://nlp.stanford.edu/software/ corenlp.shtml 4.1.3 Baseline Methods Two Chinese WS methods were taken as the baseline methods in this paper. One method was the CTB segmenter, that is, Stanford Chinese word segmenter with the model trained on CTB corpus. The other method was the length tuner in (Chang et al., 2008), which added a constant into the confidence scores of a trained CRF word segmenter to encourage it to output more word boundaries (see Section 3.2 for details). 4.1.4 Implementation and Parameter settings The proposed lexical word splitter was implemented on the CRF model toolk</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Andrew McCallum</author>
</authors>
<title>Information extraction from research papers using conditional random fields.</title>
<date>2006</date>
<journal>Information Processing &amp; Management,</journal>
<volume>42</volume>
<issue>4</issue>
<contexts>
<context position="14072" citStr="Peng and McCallum, 2006" startWordPosition="2344" endWordPosition="2347">2 CRF approach This paper employs a condition random field (CRF) to solve this sequence labeling task (Lafferty et al., 2001). A linear-chain CRF defines the conditional probability of y given x as, T 1 PA(y|x) = ( ZX t=1 (2) where A = {λ1, ...} are parameters, ZX is a perinput normalization that makes the probability of all state sequences sum to one; fk(yt−1, yt, x, t) is a feature function which is often a binary-valued sparse feature. The training of CRF model is to maximize the likelihood of training data together with a regularization penalty to avoid over-fitting as (Peng et al., 2004; Peng and McCallum, 2006), �A* = argmax( A i (3) where (x,y) are training samples; the hyperparameter δk can be understood as the variance of the prior distribution of λk. When predicting the labels of test samples, the CRF decoder searches for the optimal label sequence y* that maximizes the conditional probability, y* = argmax PA(y|x). (4) Y In (Chang et al., 2008) a method is proposed to select an appropriate level of segmentation granularity (in practical terms, to encourage smaller segments). We call their method “length tuner”. The following artificial feature is introduced into the learned CRF model: _� 1 if yt</context>
</contexts>
<marker>Peng, McCallum, 2006</marker>
<rawString>Fuchun Peng and Andrew McCallum. 2006. Information extraction from research papers using conditional random fields. Information Processing &amp; Management, 42(4):963–979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<institution>Computer Science Department Faculty Publication Series.</institution>
<contexts>
<context position="14046" citStr="Peng et al., 2004" startWordPosition="2340" endWordPosition="2343">e word splitting 3.2 CRF approach This paper employs a condition random field (CRF) to solve this sequence labeling task (Lafferty et al., 2001). A linear-chain CRF defines the conditional probability of y given x as, T 1 PA(y|x) = ( ZX t=1 (2) where A = {λ1, ...} are parameters, ZX is a perinput normalization that makes the probability of all state sequences sum to one; fk(yt−1, yt, x, t) is a feature function which is often a binary-valued sparse feature. The training of CRF model is to maximize the likelihood of training data together with a regularization penalty to avoid over-fitting as (Peng et al., 2004; Peng and McCallum, 2006), �A* = argmax( A i (3) where (x,y) are training samples; the hyperparameter δk can be understood as the variance of the prior distribution of λk. When predicting the labels of test samples, the CRF decoder searches for the optimal label sequence y* that maximizes the conditional probability, y* = argmax PA(y|x). (4) Y In (Chang et al., 2008) a method is proposed to select an appropriate level of segmentation granularity (in practical terms, to encourage smaller segments). We call their method “length tuner”. The following artificial feature is introduced into the lea</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. Computer Science Department Faculty Publication Series.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Thomas Emerson</author>
</authors>
<title>The first international chinese word segmentation bakeoff.</title>
<date>2003</date>
<booktitle>In Proceedings of the second SIGHAN workshop on Chinese language processing-Volume 17,</booktitle>
<pages>133--143</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21716" citStr="Sproat and Emerson, 2003" startWordPosition="3610" endWordPosition="3613">nMT constraint training resources because it is out of domain. The main result of this paper is the evaluation of the end-to-end performance of an SMT system. The experimental corpus for this task was the NIST OpenMT corpus. The data set of the NIST evaluation 2002 was used as a development set for MERT tuning (Och, 2003), and the remaining data sets of the NIST evaluation from 2003 to 2006 were used as test sets. The English sentences were tokenized by Stanford toolkit 7 and converted to lowercase. 4.1.2 Evaluation The performance of WS was measured by precision, recall and F1 of gold words (Sproat and Emerson, 2003), The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)(Och and Ney, 2000; Liang et al., 2006). Sure alignment edges and possible alignment edges were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The performance of SMT was measured using BLEU (Papineni et al., 2002). 7http://nlp.stanford.edu/software/ corenlp.shtml 4.1.3 Baseline Methods Two Chinese WS methods were taken as the baseline methods in this paper. One method was the CTB segmenter, that is, Stanford Chinese word segmenter with the model</context>
</contexts>
<marker>Sproat, Emerson, 2003</marker>
<rawString>Richard Sproat and Thomas Emerson. 2003. The first international chinese word segmentation bakeoff. In Proceedings of the second SIGHAN workshop on Chinese language processing-Volume 17, pages 133–143. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter for SIGHAN Bakeoff</title>
<date>2005</date>
<booktitle>In Proceedings of the 4th SIGHAN Workshop on Chinese Language Processing,</booktitle>
<volume>171</volume>
<location>Island,</location>
<contexts>
<context position="22740" citStr="Tseng et al., 2005" startWordPosition="3775" endWordPosition="3778">nlp.shtml 4.1.3 Baseline Methods Two Chinese WS methods were taken as the baseline methods in this paper. One method was the CTB segmenter, that is, Stanford Chinese word segmenter with the model trained on CTB corpus. The other method was the length tuner in (Chang et al., 2008), which added a constant into the confidence scores of a trained CRF word segmenter to encourage it to output more word boundaries (see Section 3.2 for details). 4.1.4 Implementation and Parameter settings The proposed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter (Tseng et al., 2005). The regularity parameters Sk are set to be 3, the same as the Stanford segmenter, because no significant performance improvements were observed by tuning that parameter. To extract features for the word splitter, the Stanford named entity recognizer (Finkel et al., 2005)8 was employed to obtain the tags of named entities. Word frequencies were caculated from the source side of SMT training corpus. The character-level unsupervised alignment was conducted using GIZA++ (Och and Ney, 2003)9. The length tuner reused the CRF model of CTB segmenter. The parameter A0 was tuned through the grid searc</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter for SIGHAN Bakeoff 2005. In Proceedings of the 4th SIGHAN Workshop on Chinese Language Processing, volume 171. Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Jianfeng Gao</author>
<author>Kristina Toutanova</author>
<author>Hermann Ney</author>
</authors>
<title>Bayesian semi-supervised Chinese word segmentation for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational LinguisticsVolume 1,</booktitle>
<pages>1017--1024</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16182" citStr="Xu et al., 2008" startWordPosition="2704" endWordPosition="2707">acteristics with the boundaries in the CTB WS scheme. The extended features consists of four types – named entities, word frequency, word length and character-level unsupervised WA. For each type of the feature, the value and value concatenated with previous or current character are taken as sparse features (see Table 4 for details). The real values of word frequency, word length and characterlevel unsupervised WA are converted into sparse features due to the routine of CRF model. The character-level unsupervised alignment feature is inspired by the related works of unsupervised bilingual WS (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Michael et al., 2011). The idea is that the character-level WA can approximately capture the counterpart English expression of each Chinese token, and source tokens aligned to different target expressions should be split into different words (see Figure 4 for an illustration). The values of the character-level alignment features are obtained through building a dictionary. First, unsupervised WA is performed on the SMT training corpus where the Chinese sentences are treated as sequences of characters; then, the Chinese sentences are segmented by CT</context>
<context position="26984" citStr="Xu et al., 2008" startWordPosition="4459" endWordPosition="4462">imilar work in the literature to the proposed method is the the length tuner method proposed by (Chang et al., 2008). This method also encourages the generation of more words during segmentation by using a single parameter that can be use to control segment length. Our method differs from theirs in that it is able to acquire vocabulary knowledge from word alignments that can be used to more accurately split words into segments suitable for machine translation. There is large volume of research using bilingual unsupervised and semi-supervised WS to address the problem of optimizing WS for SMT (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Michael et al., 2011). The main difference with our approach is that they use automatic WA results, most often obtained using the same tools as are used in training SMT systems. One of the main problems of using unsupervised WA is that it is noisy, and therefore, employing iterative optimization methods to refine the results of unsupervised WA is a key issue in their research, for example boosting (Ma and Way, 2009; Michael et al., 2011), expectation maximization (Chung and Gildea, 2009), Bayesian sampling (Xu et al., 2008; Nguyen et al., 2010), o</context>
</contexts>
<marker>Xu, Gao, Toutanova, Ney, 2008</marker>
<rawString>Jia Xu, Jianfeng Gao, Kristina Toutanova, and Hermann Ney. 2008. Bayesian semi-supervised Chinese word segmentation for statistical machine translation. In Proceedings of the 22nd International Conference on Computational LinguisticsVolume 1, pages 1017–1024. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>Building a large-scale annotated chinese corpus.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics-Volume 1,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2047" citStr="Xue et al., 2002" startWordPosition="300" endWordPosition="303"> on those languages that have no explicit space between words, such as Arabic, Chinese and Japanese. As the first processing step, WS affects all successive steps, thus it has a large potential impact on the final performance. For SMT, the unsupervised WA, building translation models and reordering models, and decoding are all based on segmented words. Automated word segmenters built through supervised-learning methods, after decades of intensive research, have emerged as effective solutions to WS tasks and become widely used in many NLP applications. For example, the Stanford word segmenter (Xue et al., 2002)1 which is based on conditional random field (CRF) is employed to prepare the official corpus for NTCIR9 Chinese-English patent translation task (Goto et al., 2011). However, one problem with applying these supervised-learning word segmenters to SMT is that the WS scheme of annotating the training corpus may not be optimal for SMT. (Chang et al., 2008) noticed that the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually </context>
</contexts>
<marker>Xue, Chiou, Palmer, 2002</marker>
<rawString>Nianwen Xue, Fu-Dong Chiou, and Martha Palmer. 2002. Building a large-scale annotated chinese corpus. In Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Phrase-based statistical machine translation.</title>
<date>2002</date>
<booktitle>In Advances in Artificial Intelligence,</booktitle>
<pages>18--32</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="8704" citStr="Zens et al., 2002" startWordPosition="1393" endWordPosition="1396">ewswire 2,175 6,218 246,371 205,281 164,033 Web blog 2,560 11,839 373,818 312,856 257,730 Total 4,735 18,057 620,189 518,137 421,763 Table 1: GALE WA corpus. † Sentences rejected by the annotators are excluded. Federer � � � � � � � � � � � SEM SEM GIS GIS GIS FUN relied on his service INC INC to keep the serve . 0 four atomic blocks; the CTB segmenter produces five words which all locate within the blocks, so they are all small enough. • Alignment inconsistent: the word aligns to more than one atomic block, but the target expression is contiguous, allowing for correct phrase pair extraction (Zens et al., 2002). For example, in Figure 2(b), the characters in the word “shuang fang”, which is produced by the CTB segmenter, contains two atomic blocks, but the span of the target “to both side” is continuous, therefore the phrase pair tracted. • Alignment inconsistent and extraction hindered: the word aligned to more than one atomic block, and the target expression is not contiguous, which hinders correct phrase pair extractions. For example, in Figure 2(c), the word “zeng chan” has to be split in order to match the target language. Table 2 shows the statistics of the three categories of CTB WS on the GA</context>
</contexts>
<marker>Zens, Och, Ney, 2002</marker>
<rawString>Richard Zens, Franz Josef Och, and Hermann Ney. 2002. Phrase-based statistical machine translation. In Advances in Artificial Intelligence, pages 18–32. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Masao Utiyama</author>
<author>Eiichro Sumita</author>
<author>BaoLiang Lu</author>
</authors>
<title>An empirical study on word segmentation for chinese machine translation. A. Gelbukh (Ed.): CICLing 2013, Part II,</title>
<date>2013</date>
<tech>LNCS 7817,</tech>
<contexts>
<context position="27622" citStr="Zhao et al., 2013" startWordPosition="4568" endWordPosition="4571">2009; Nguyen et al., 2010; Michael et al., 2011). The main difference with our approach is that they use automatic WA results, most often obtained using the same tools as are used in training SMT systems. One of the main problems of using unsupervised WA is that it is noisy, and therefore, employing iterative optimization methods to refine the results of unsupervised WA is a key issue in their research, for example boosting (Ma and Way, 2009; Michael et al., 2011), expectation maximization (Chung and Gildea, 2009), Bayesian sampling (Xu et al., 2008; Nguyen et al., 2010), or heuristic search (Zhao et al., 2013). Nevertheless, noisy WA makes both analyzing WS and improving SMT quality quite hard. In contrast, by using manual WA, we can clearly analyze the segmentation problems (Section 2), and train supervised models to solve the problem (Section 3). As far as we are aware, among related work on WS, our method achieves the highest BLEU improvement relative to the start-of-the-art WS – the Stanford Chinese word segmenter – on the Chinese-English OpenMT corpora. The methods proposed in (Ma and Way, 2009; Chung and Gildea, 2009) fail to outperform the Stanford Chinese word segmenter on Chinese-English O</context>
</contexts>
<marker>Zhao, Utiyama, Sumita, Lu, 2013</marker>
<rawString>Hai Zhao, Masao Utiyama, Eiichro Sumita, and BaoLiang Lu. 2013. An empirical study on word segmentation for chinese machine translation. A. Gelbukh (Ed.): CICLing 2013, Part II, LNCS 7817,</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000172">
<title confidence="0.786456">
Go Climb a Dependency Tree and Correct the Grammatical Errors
</title>
<author confidence="0.995258">
Longkai Zhang Houfeng Wang
</author>
<affiliation confidence="0.9850465">
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education, China
</affiliation>
<email confidence="0.986312">
zhlongk@qq.com, wanghf@pku.edu.cn
</email>
<sectionHeader confidence="0.997271" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972263157895">
State-of-art systems for grammar error
correction often correct errors based on
word sequences or phrases. In this paper,
we describe a grammar error correction
system which corrects grammatical errors
at tree level directly. We cluster all error
into two groups and divide our system into
two modules correspondingly: the general
module and the special module. In the
general module, we propose a TreeNode
Language Model to correct errors related
to verbs and nouns. The TreeNode Lan-
guage Model is easy to train and the de-
coding is efficient. In the special module,
two extra classification models are trained
to correct errors related to determiners and
prepositions. Experiments show that our
system outperforms the state-of-art sys-
tems and improves the F1 score.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976508474577">
The task of grammar error correction is difficult
yet important. An automatic grammar error cor-
rection system can help second language (L2)
learners improve the quality of their writing. In re-
cent years, there are various competitions devoted
to grammar error correction, such as the HOO-
2011(Dale and Kilgarriff, 2011), HOO-2012(Dale
et al., 2012) and the CoNLL-2013 shared task (Ng
et al., 2013). There has been a lot of work ad-
dressing errors made by L2 learners. A significant
proportion of the systems for grammar error cor-
rection train individual statistical models to cor-
rect each special kind of error word by word and
ignore error interactions. These methods assume
no interactions between different kinds of gram-
matical errors. In real problem settings errors are
correlated, which makes grammar error correction
much more difficult.
Recent research begins to focus on the error
interaction problem. For example, Wu and Ng
(2013) decodes a global optimized result based
on the individual correction confidence of each
kind of errors. The individual correction confi-
dence is still based on the noisy context. Ro-
zovskaya and Roth (2013) uses a joint modeling
approach, which considers corrections in phrase
structures instead of words. For dependencies that
are not covered by the joint learning model, Ro-
zovskaya and Roth (2013) uses the results of Illi-
nois system in the joint inference. These results
are still at word level and are based on the noisy
context. These systems can consider error inter-
actions, however, the systems are complex and
inefficient. In both Wu and Ng (2013) and Ro-
zovskaya and Roth (2013), Integer Linear Pro-
gramming (ILP) is used for decoding a global op-
timized result. In the worst case, the time com-
plexity of ILP can be exponent.
In contrast, we think a better grammar error cor-
rection system should correct grammatical errors
at sentence level directly and efficiently. The sys-
tem should correct as many kinds of errors as pos-
sible in a generalized framework, while allowing
special models for some kinds of errors that we
need to take special care. We cluster all error into
two groups and correspondingly divide our sys-
tem into two modules: the general module and the
special module. In the general module, our sys-
tem views each parsed sentence as a dependency
tree. The system generates correction candidates
for each node on the dependency tree. The cor-
rection can be made on the dependency tree glob-
ally. In this module, nearly all replacement errors
related to verb form, noun form and subject-verb
agreement errors can be considered. In the spe-
cial module, two extra classification models are
used to correct the determiner errors and preposi-
tion errors . The classifiers are also trained at tree
node level. We take special care of these two kinds
</bodyText>
<page confidence="0.9736">
266
</page>
<note confidence="0.910308">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 266–277,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999978944444444">
of errors because these errors not only include re-
placement errors, but also include insertion and
deletion errors. A classification model is more
suitable for handling insertion and deletion errors.
Besides, they are the most common errors made
by English as a Second Language (ESL) learners
and are much easier to be incorporated into a clas-
sification framework.
We propose a TreeNode Language Model
(TNLM) to efficiently measure the correctness of
selecting a correction candidate of a node in the
general module. Similar to the existing statistical
language models which assign a probability to a
linear chain of words, our TNLM assigns correct-
ness scores directly on each node on the depen-
dency tree. We select candidates for each node
to maximize the global correctness score and use
these candidates to form the corrected sentence.
The global optimized inference can be tackled ef-
ficiently using dynamic programming. Because
the decoding is based on the whole sentence, error
interactions can be considered. Our TNLM only
needs to use context words related to each node
on the dependency tree. Training a TreeNode lan-
guage model costs no more than training ordinary
language models on the same corpus. Experiments
show that our system can outperform the state-of-
art systems.
The paper is structured as follows. Section 1
gives the introduction. In section 2 we describe the
task and give an overview of the system. In section
3 we describe the general module and in section 4
we describe the special module. Experiments are
described in section 5. In section 6 related works
are introduced, and the paper is concluded in the
last section.
</bodyText>
<sectionHeader confidence="0.946653" genericHeader="introduction">
2 Task and System Overview
</sectionHeader>
<subsectionHeader confidence="0.90617">
2.1 Task Description
</subsectionHeader>
<bodyText confidence="0.9997854375">
The task of grammar error correction aims to cor-
rect grammatical errors in sentences. There are
various competitions devoted to the grammar er-
ror correction task for L2 learners. The CoNLL-
2013 shared task is one of the most famous, which
focuses on correcting five types of errors that
are commonly made by non-native speakers of
English, including determiner, preposition, noun
number, subject-verb agreement and verb form er-
rors. The training data released by the task orga-
nizers come from the NUCLE corpus(Dahlmeier
et al., 2013). This corpus contains essays writ-
ten by ESL learners, which are then corrected by
English teachers. The test data are 50 student es-
says. Details of the corpus are described in Ng
et al. (2013).
</bodyText>
<subsectionHeader confidence="0.999495">
2.2 System Architecture
</subsectionHeader>
<bodyText confidence="0.999218">
In our system, lists of correction candidates are
first generated for each word. We generate can-
didates for nouns based on their plurality. We gen-
erate candidates for verbs based on their tenses.
Then we select the correction candidates that max-
imize the overall correctness score. An example
process of correcting figure 1(a) is shown in table
1.
Correcting grammatical errors using local sta-
tistical models on word sequence is insufficient.
The local models can only consider the contexts
in a fixed window. In the example of figure 1(a),
the context of the verb “is” is “that boy is on the”,
which sounds reasonable at first glance but is in-
correct when considering the whole sentence. The
limitation of local classifiers is that long distance
syntax information cannot be incorporated within
the local context. In order to effectively use the
syntax information to get a more accurate correct-
ing result, we think a better way is to tackle the
problem directly at tree level to view the sentence
as a whole. From figure 1(a) we can see that the
node “is” has two children on the dependency tree:
“books” and “on”. When we consider the node
“is”, its context is “books is on”, which sounds in-
correct. Therefore, we can make better corrections
using such context information on nodes.
Therefore, our system corrects grammatical er-
rors on dependency trees directly. Because the
correlated of words are more linked on trees than
in a word sequence, the errors are more easier to
be corrected on the trees and the agreement of dif-
ferent error types is guaranteed by the edges. We
follow the strategy of treating different kinds of
errors differently, which is used by lots of gram-
mar error correction systems. We cluster the five
types of errors considered in CoNLL-2013 into
two groups and divide our system into two mod-
ules correspondingly.
• The general module, which is responsible
for the verb form errors, noun number errors
and subject-verb agreement errors. These er-
rors are all replacement errors, which can
be corrected by replacing the wrongly used
word with a reasonable candidate word.
</bodyText>
<page confidence="0.997458">
267
</page>
<figureCaption confidence="0.9565925">
Figure 1: Dependency parsing results of (a) the original sentence “The books of that boy is on the desk
.” (b) the corrected sentence.
</figureCaption>
<table confidence="0.4836725">
Position Original Correction Candidates Corrected
1 The The The
</table>
<listItem confidence="0.850177888888889">
2 books books,book books
3 of of of
4 that that that
5 boy boy, boys boy
6 is is,are,am,was,were,be,being,been are
7 on on on
8 the the the
9 desk desk, desks desk
10 . . .
</listItem>
<tableCaption confidence="0.982844">
Table 1: An example of the “correction candidate generation and candidate selection” framework.
</tableCaption>
<bodyText confidence="0.995545111111111">
• The special module, where two classifica-
tion models are used to correct the determiner
errors and preposition errors at tree level. We
take special care of these two kinds of errors
because these errors include both replace-
ment errors and insertion/deletion errors. Be-
sides, they are the most common errors made
by ESL learners and is much easier to be in-
corporated into a classification framework.
We should make it clear that we are not the first
to use tree level correction models on ungrammat-
ical sentences. Yoshimoto et al. (2013) uses a
Treelet Language model (Pauls and Klein, 2012)
to correct agreement errors. However, the perfor-
mance of Treelet language model is not that good
compared with the top-ranked system in CoNLL-
2013. The reason is that the production rules in the
Treelet language model are based on complex con-
texts, which will exacerbate the data sparseness
problem. The “context” in Treelet language model
also include words ahead of treelets, which are
sometimes unrelated to the current node. In con-
trast, our TreeNode Language model only needs to
consider useful context words related to each node
on the dependency tree. To train a TreeNode lan-
guage model costs no more than training ordinary
language models on the same corpus.
</bodyText>
<subsectionHeader confidence="0.997681">
2.3 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999868277777778">
Our system corrects grammatical errors on de-
pendency trees directly, therefore the sentences
in training and testing data should have been
parsed before being corrected. In our system, we
use the Stanford parser1 to parse the New York
Times source of the Gigaword corpus2, and use the
parsed sentences as our training data. We use the
original training data provided by CoNLL-2013 as
the develop set to tune all parameters.
Some sentences in the news texts use a differ-
ent writing style against the sentences written by
ESL learners. For example, sentences written by
ESL learners seldom include dialogues between
people, while very often news texts include para-
graphs such as “‘I am frightened!’ cried Tom”. We
use heuristic rules to eliminate the sentences in the
Gigaword corpus that are less likely to appear in
the ESL writing. The heuristic rules include delet-
</bodyText>
<footnote confidence="0.9998325">
1http://nlp.stanford.edu/software/lex-parser.shtml
2https://catalog.ldc.upenn.edu/LDC2003T05
</footnote>
<page confidence="0.995743">
268
</page>
<bodyText confidence="0.999757">
ing sentences that are too short or too long3, delet-
ing sentences that contains certain punctuations
such as quotation marks, or deleting sentences that
are not ended with a period.
In total we select and parse 5 million sen-
tences of the New York Times source of English
newswire in the Gigaword corpus. We build the
system and experiment based on these sentences.
</bodyText>
<sectionHeader confidence="0.981902" genericHeader="method">
3 The General Module
</sectionHeader>
<subsectionHeader confidence="0.988945">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.999797763157894">
The general module aims to correct verb form er-
rors, noun number errors and subject-verb agree-
ment errors. Other replacement errors such as
spelling errors can also be incorporated into the
general module. Here we focus on verb form er-
rors, noun number errors and subject-verb agree-
ment errors only. Our general module views each
sentence as a dependency tree. All words in the
sentence form the nodes of the tree. Nodes are
linked through directed edges, annotated with the
dependency relations.
Before correcting the grammatical errors, the
general module should generate correction candi-
dates for each node first. For each node we use
the word itself as its first candidate. Because the
general module considers errors related to verbs
and nouns, we generate extra correction candi-
dates only for verbs and nouns. For verbs we use
all its verb forms as its extra candidates. For ex-
ample when considering the word “speaks”, we
use itself and {speak, spoke, spoken, speaking}
as its correction candidates. For nouns we use
its singular form and plural form as its extra cor-
rection candidates. For example when consider-
ing the word “dog”, we use itself and “dogs” as
its correction candidate. If the system selects the
original word as the final correction, the sentence
remains unchanged. But for convenience we still
call the newly generated sentence “the corrected
sentence”.
In a dependency tree, the whole sentence
s can be formulized as a list of production
rules r1, ..., rL of the form: [r = head —*
modifier1, modifier2...]. An example of all
production rules of figure 1(a) is shown in table
2. Because the production rules are made up of
words, selecting a different correction candidate
for only one node will result in a list of different
</bodyText>
<footnote confidence="0.8050245">
3In our experiment, no less than 5 words and no more than
30 words.
</footnote>
<bodyText confidence="0.97804992">
production rules. For example, figure 1(b) selects
the correction candidate “is” to replace the origi-
nal “are”. Therefore the production rules of figure
1(b) include [are —* books, on], instead of [is —*
books, on] in figure 1(a).
books —* The, of
of —* boy
boy —* that
is —* books, on
on —* desk
desk — *the
Table 2: All the production rules in the example of
figure 1(a)
The overall correctness score of s, which
is score(s), can be further decomposed into
r1Li=0 score(ri). A reasonable score function
should score the correct candidate higher than the
incorrect one. Consider the node “is” in Figure
1(a), the production rule with head “is” is [is —*
books, on]. Because the correction of “is” is “are”,
a reasonable scorer should have score([is —*
books, on]) &lt; score([are —* books, on]).
Given the formulation of sentence s =
[r1, ..., rL] and the candidates for each node, we
are faced with two problems:
</bodyText>
<listItem confidence="0.837847">
1. Score Function. Given a fixed selection of
candidate for each node, how to compute
the overall score of the dependency tree, i.e.,
score(s). Because score(s) is decomposed
into r1Li=0 score(ri), the problem becomes
finding a score function to measure the cor-
rectness of each r given a fixed selection of
candidates.
2. Decoding. Given each node a list of correc-
</listItem>
<bodyText confidence="0.963409642857143">
tion candidates and a reasonable score func-
tion score(r) for the production rules, how to
find the selection of candidates that maximize
the overall score of the dependency tree.
For the first problem, we propose a TreeNode
Language Model as the correctness measure of a
fixed candidate selection. For the decoding prob-
lem, we use a dynamic programming method to
efficiently find the correction candidates that max-
imize the overall score. We will describe the de-
tails in the following sections.
One concern is whether the automatically
parsed trees are reliable for grammar error cor-
rection. We define “reliable” as follows. If we
</bodyText>
<page confidence="0.98844">
269
</page>
<bodyText confidence="0.999958176470588">
change some words in original sentence into their
reasonable correction candidates (e.g. change “is”
to “are”) but the structure of the dependency tree
does not change (except the replaced word and
its corresponding POS tag, which are definitely
changed), then we say the dependency tree is reli-
able for this sentence. To verify this we randomly
selected 1000 sentences parsed by the Stanford
Parser. We randomly select the verbs and nouns
and replace them with a wrong form. We parsed
the modified sentences again and asked 2 annota-
tors to examine whether the dependency trees are
reliable for grammar error correction. We find that
99% of the dependency trees are reliable. There-
fore we can see that the dependency tree can be
used as the structure for grammar error correction
directly.
</bodyText>
<subsectionHeader confidence="0.997327">
3.2 TreeNode Language Model
</subsectionHeader>
<bodyText confidence="0.999954977272727">
In our system we use the score of TreeNode Lan-
guage Model (TNLM) as the scoring function.
Consider a node n on a dependency tree and as-
sume n has K modifiers C1, ..., CK as its child
nodes. We define 5eq(n) = [C1,..., n,..., CK]
as an ordered sub-sequence of nodes that includes
the node n itself and all its child nodes. The or-
der of the sub-sequence in 5eq(n) is sorted based
on their position in the sentence. In this formula-
tion, we can score the correctness of a production
rule r by scoring the correctness of 5eq(n). Be-
cause 5eq(n) is a word sequence, we can use a
language model to measure its correctness. The
sub-sequences are not identical to the original
text. Therefore instead of using ordinary language
models, we should train special language models
using the sub-sequences to measure the correct-
ness of a production rule.
Take the sentence in figure 2 as an example.
When considering the node “is” in the word se-
quence, it is likely to be corrected into “are” be-
cause it appear directly after the plural noun “par-
ents”. However, by the definition above, the sub-
sequence corresponding to the node “damaged” is
“car is damaged by ”. In such context, the word
“is” is less likely to be changed to “are”. From
the example we can see that the sub-sequence is
suitable to be used to measure the correctness of
a production rule. From this example we can also
find that the sub-sequences are different with or-
dinary sentences, because ordinary sentences are
less likely to end with “by”.
Table 3 shows all the sub-sequences in the ex-
ample of figure 2. If we collect all the sub-
sequences in the corpus to form a new sub-
sequence corpus, we can train a language model
based on the new sub-sequence corpus. This is
our TreeNode Language Model. One advantage
of TLM is that once we have generated the sub-
sequences, we can train the TLM in the same
way as we train ordinary language models. Be-
sides, the TLM is not limited to a fixed smoothing
method. Any smoothing methods for ordinary lan-
guage models are applicable for TLM.
</bodyText>
<table confidence="0.999078818181818">
Node Sub-sentence
The The
car The car of
of of parents
my my
parents my parents
is is
damaged car is damaged by
by by storm
the the
storm the storm
</table>
<tableCaption confidence="0.8207715">
Table 3: All the sub-sentences in the example of
figure 2
</tableCaption>
<bodyText confidence="0.9863905">
In our system we train the TLM using the same
way as training tri-gram language model. For a
sub-sequence 5 = w0...wL, we calculate P(5) =
HL P w I w w ) The smoothingmethod
</bodyText>
<equation confidence="0.641154">
Z=o ( Z Z—i Z-2 g
</equation>
<bodyText confidence="0.703014666666667">
we use is interpolation, which assumes the final
P0(wz|wz−1wz−2) of the tri-gram language model
follows the following decomposition:
</bodyText>
<equation confidence="0.999637333333333">
P0(wz|wz−1wz−2) =A1P(wz|wz−1wz−2)
+A2P(wz|wz−1) (1)
+A3P(wz)
</equation>
<bodyText confidence="0.999529">
where A1, A2 and A3 are parameters sum to
1. The parameters A1, A2 and A3 are estimated
through EM algorithm(Baum et al., 1970; Demp-
ster et al., 1977; Jelinek, 1980).
</bodyText>
<subsectionHeader confidence="0.993821">
3.3 Decoding
</subsectionHeader>
<bodyText confidence="0.999955333333333">
The decoding problem is to select one correction
candidate for each node that maximizes the over-
all score of the corrected sentence. When the sen-
tence is long and contains many verbs and nouns,
enumerating all possible candidate selections is
time-consuming. We use a bottom-up dynamic
</bodyText>
<page confidence="0.991154">
270
</page>
<figureCaption confidence="0.999691">
Figure 2: A illustrative sentence for TreeNode Language Model.
</figureCaption>
<bodyText confidence="0.998837875">
programming approach to find the maximized cor-
rections within polynomial time complexity.
For a node n with L correction candidates
n1, ...nL and K child nodes C1,..., CK, we define
n.scores[i] as the maximum score if we choose
the ith candidate ni for n. Because we decode
from leaves to the root, C1.scores, ..., CK.scores
have already been calculated before we calculate
n.scores.
We assume the sub-sequence Seq(ni) =
[C1, ..., CM, ni, CM+1, ..., CK] without loss of
generality, where C1, .., CM are the nodes before
ni and CM+1, ..., CK are the nodes after ni.
We define ci,j as the jth correction can-
didate of child node Ci. Given a se-
lection of candidates for each child node
</bodyText>
<equation confidence="0.9982664">
seq = [c1,j1, ..., cM,jM, ni, cM+1,jM+1, ..., cK,jK],
we can calculate score(seq) as:
K
score(seq) = TNLM(seq) Ci.scores[ji]
i=1
</equation>
<bodyText confidence="0.9876115">
(2)
where TNLM(seq) is the TreeNode Language
Model score of seq. Then, n.scores[i] is calcu-
lated as:
</bodyText>
<equation confidence="0.969677">
n.scores[i] = max score(seq) (3)
∀seq
</equation>
<bodyText confidence="0.940308352941176">
Because seq is a word sequence, the maxi-
mization can be efficiently calculated using Viterbi
algorithm (Forney Jr, 1973). To be specific,
the Viterbi algorithm uses the transition scores
and emission scores as its input. The transition
scores in our model are the tri-gram probabilities
from our tri-gram TNLM. The emission scores
in our model are the candidate scores of each
child: C1.scores, ..., CK.scores, which have al-
ready been calculated.
After the bottom-up calculation, we only need
to look into the “ROOT” node to find the maxi-
mum score of the whole tree. Similar to the Viterbi
algorithm, back pointers should be kept to find
which candidate is selected for the final corrected
sentence. Detailed decoding algorithm is shown in
table 4.
</bodyText>
<table confidence="0.897057846153846">
Function decode(Node n)
if n is leaf
set n.scores uniformly
return
for each child c of n
decode(c)
calculating n.scores using Viterbi
End Function
BEGIN
decode(ROOT)
find the maximum score for the tree and back-
track all candidates
END
</table>
<tableCaption confidence="0.987108">
Table 4: The Decoding algorithm
</tableCaption>
<bodyText confidence="0.999906222222222">
In the real world implementations, we add a
controlling parameter for the confidence of the
correctness of the inputs. We multiply A on
P (w0|w−2w−1) of the tri-gram TNLM if the cor-
recting candidate w0 is the same word in the orig-
inal input. A is larger than 1 to “emphasis” the
confidence of the original word because the most
of the words in the inputs are correct. The value of
A can be set using the development data.
</bodyText>
<subsectionHeader confidence="0.991321">
3.4 The Special Module
</subsectionHeader>
<bodyText confidence="0.999980125">
The special module is designed for determiner er-
rors and preposition errors. We take special care
of these two kinds of errors because these errors
include insertion and deletion errors, which can-
not be corrected in the general module. Because
there is a fixed number of prepositions and deter-
miners, these two kinds of errors are much easier
to be incorporated into a classification framework.
Besides, they are the most common errors made by
ESL learners and there are lots of previous works
that leave valuable guidance for us to follow.
Similar to many previous state-of-art systems,
we treat the correction of determiner errors and
preposition errors as a classification problem. Al-
though some previous works (e.g. Rozovskaya
et al. (2013)) use NPs and the head of NPs as
</bodyText>
<page confidence="0.993756">
271
</page>
<bodyText confidence="0.999959896103896">
features, they are basically local classifiers mak-
ing predictions on word sequences. Difference to
the local classifier approaches, we make predic-
tions on the nodes of the dependency tree directly.
In our system we correct determiner errors and
preposition errors separately.
For the determiner errors, we consider the in-
sertion, deletion and replacement of articles (i.e.
‘a’, ‘an’ and ‘the’). Because the articles are used
to modify nouns in the dependency trees, we can
classify based on noun nodes. We give each noun
node (node whose POS tag is noun) a label to in-
dicate which article it should take. We use left po-
sition (LP) and right position (RP) to specify the
position of the article. The article therefore lo-
cates between LP and RP. If a noun node already
has an article as its modifier, then LP will be the
position directly ahead of the article. In this case,
RP = LP + 2. If an insertion is needed, the RP
is the position of the first child node of the noun
node. In this case LP = RP − 1. With this no-
tation, detailed feature templates we use to correct
determiner errors are listed in table 5. In our model
we use 3 labels: ‘a’, ‘the’ and ‘0’. We use ‘a’, ‘the’
to represent a noun node should be modified with
‘a’ or ‘’the’ correspondingly. We use ‘’0’ to in-
dicate that no article is needed for the noun node.
We use rule-based method to distinguish between
“a” and “an” as a post-process.
For the preposition errors, we only consider
deletion and replacement of an existing preposi-
tion. The classification framework is similar to
determiner errors. We consider classification on
preposition nodes (nodes whose POS tag is prepo-
sition). We use prepositions as labels to indicate
which preposition should be used. and use “0”
to denote that the preposition should be deleted.
We use the same definition of LP and RP as the
correction of determiner errors. Detailed feature
templates we use to correct preposition errors are
listed in table 6. Similar to the previous work(Xing
et al., 2013), we find that adding more preposi-
tions will not improve the performance in our ex-
periments. Thus we only consider a fixed set of
prepositions: {in, for, to, of, on}.
Previous works such as Rozovskaya et al.
(2013) show that Naive Bayes model and averaged
perceptron model show better results than other
classification models. These classifiers can give a
reasonably good performance when there are lim-
ited amount of training data. In our system, we use
large amount of automatically generated training
data based on the parsed Gigaword corpus instead
of the limited training data provided by CoNLL-
2013.
Take generating training data for determiner er-
rors as an example. We generate training data
based on the parsed Gigaword corpus C described
in section 2. Each sentence S in C is a depen-
dency tree T. We use each noun node N on T as
one training instance. If N is modified by “the”,
its label will be “THE”. If N is modified by “a”
or “an”, its label will be “A”. Otherwise its label
will be “NULL”. Then we just omit the determiner
modifier and generate features based on table 5.
Generating training data for preposition errors is
the same, except we use preposition nodes instead
of noun nodes.
By generating training instances in this way, we
can get large amount of training data. Therefore
we think it is a good time to try different classifi-
cation models with enough training data. We ex-
periment on Naive Bayes, Averaged Perceptron,
SVM and Maximum Entropy models (ME) in a 5-
fold cross validation on the training data. We find
ME achieves the highest accuracy. Therefore we
use ME as the classification model in our system.
</bodyText>
<sectionHeader confidence="0.999846" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.999489">
4.1 Experiment Settings
</subsectionHeader>
<bodyText confidence="0.999975454545455">
In the experiments, we use our parsed Gigaword
corpus as the training data, use the training data
provided by CoNLL-2013 as the develop data, and
use the test data of CoNLL-2013 as test data di-
rectly. In the general module, the training data
is used for the training of TreeNode Language
Model. In the special module, the training data is
used for training individual classification models.
We use the M2 scorer (Dahlmeier and Ng,
2012b) provided by the organizer of CoNLL-2013
for the evaluation of our system. The M2 scorer
is widely used as a standard scorer in previous
systems. Because we make comparison with the
state-of-art systems on the CoNLL-2013 corpus,
we use the same evaluation metric F1 score of M2
scorer as the evaluation metric.
In reality, some sentences may have more than
one kind of possible correction. As the example in
“The books of that boy is on the desk.”, the cor-
responding correction can be either “The books of
that boy are on the desk.” or “The book of that boy
is on the desk.”. The gold test data can only con-
</bodyText>
<page confidence="0.993527">
272
</page>
<table confidence="0.999496">
Word Features wLP , wLP−1, wLP−2, wRP, wRP+1, wRP+2, wLP−2wLP−1, wLP−1wLP,
wLPwRP, wRPwRP+1, wRP+1wRP+2, wLP−2wLP−1wLP, wLP−1wLPwRP,
wLPwRPwRP+1, wRPwRP+1wRP+2
Noun Node NN, wLPNN, wLP−1wLPNN, wLP−2wLP−1wLPNN
Features
Father/Child Fa, wRPFa, wRPwRP+1Fa, wRPwRP+1wRP+2Fa, Fa&amp;Ch
Node Features
</table>
<tableCaption confidence="0.979738">
Table 5: Feature templates for the determiner errors. wz is the word at the ith position. NN is the current
noun node. Fa is the father node of the current noun node. Ch is a child node of the current noun node.
</tableCaption>
<table confidence="0.9996136">
Word Features wLP , wLP−1, wLP−2, wRP, wRP+1, wRP+2, wLP−2wLP−1, wLP−1wLP,
wLPwRP, wRPwRP+1, wRP+1wRP+2, wLP−2wLP−1wLP, wLP−1wLPwRP,
wLPwRPwRP+1, wRPwRP+1wRP+2
Father/Child Fa, wRPFa, wRPwRP+1Fa, wRPwRP+1wRP+2Fa, Fa&amp;Ch
Node Features
</table>
<tableCaption confidence="0.983808">
Table 6: Feature templates for preposition errors. wz is the word at the ith position. Fa is the father node
</tableCaption>
<bodyText confidence="0.975286769230769">
of the current preposition node. Ch is a child node of the current preposition node.
sider a small portion of possible answers. To re-
lieve this, the CoNLL-2013 shared task allows all
participating teams to provide alterative answers
if they believe their system outputs are also cor-
rect. These alterative answers form the “Revised
Data” in the shared task, which indeed help evalu-
ate the outputs of the participating systems. How-
ever, the revised data only include alterative cor-
rections from the participating teams. Therefore
the evaluation is not that fair for future systems. In
our experiment we only use the original test data
as the evaluation dataset.
</bodyText>
<subsectionHeader confidence="0.997128">
4.2 Experiment Results
</subsectionHeader>
<bodyText confidence="0.999376">
We first show the performance of each stage of our
system. In our system, the general module and
the special module correct grammar errors conse-
quently. Therefore in table 7 we show the perfor-
mance when each component is added to the sys-
tem.
</bodyText>
<table confidence="0.999282">
Method P R F1 score
TNLM 33.96% 17.71% 23.28%
+Det 32.83% 38.28% 35.35%
+Prep 32.64% 39.20% 35.62%
</table>
<tableCaption confidence="0.993439">
Table 7: Results of each stage in our system.
</tableCaption>
<bodyText confidence="0.994014">
TNLM is the general module. “+Det” is the sys-
tem containing the general module and determiner
part of special module.“+Prep” is the final system
We evaluate the effect of using TreeNode lan-
guage model for the general module. We compare
the TNLM with ordinary tri-gram language model.
We use the same amount of training data and the
same smoothing strategy (i.e. interpolation) for
both of them. Table 8 shows the comparison. The
TNLM can improve the F1 by +2.1%.
</bodyText>
<table confidence="0.999551333333333">
Method P R F1 score
Ordinary LM 29.27% 16.68% 21.27%
Our TNLM 33.96% 17.71% 23.28%
</table>
<tableCaption confidence="0.98651">
Table 8: Comparison for the general module
</tableCaption>
<bodyText confidence="0.953814523809524">
between TNLM and ordinary tri-gram language
model on the test data.
Based on the result of the general module using
TNLM, we compare our tree level special mod-
ule against the local classification approach. The
special module of our system makes predictions
on the dependency tree directly, while local clas-
sification approaches make predictions on linear
chain of words and decide the article of a noun
Phrase or the preposition of a preposition phrase.
We use the same word level features for the two
approaches except for the local classifiers we do
not add tree level features. Table 9 shows the com-
parison.
When using the parsed Gigaword texts as train-
ing data, the quality of the sentences we select
will influence the result. For comparison, we ran-
domly select the same amount of sentences from
the same source of Gigaword and parse them as
a alterative training set. Table 10 shows the com-
parison between random chosen training data and
</bodyText>
<page confidence="0.996033">
273
</page>
<table confidence="0.999816666666667">
Method P R F1 score
Local Classifier 26.38% 39.14% 31.51%
Our Tree-based 32.64% 39.20% 35.62%
</table>
<tableCaption confidence="0.973558">
Table 9: Comparison for the special module on the
</tableCaption>
<bodyText confidence="0.959293166666667">
test data. The input of the special module is the
sentences corrected by the TNLM in the general
module.
the selected training data of our system. We can
see that the data selection (cleaning) procedure is
important for the improvement of system F1.
</bodyText>
<table confidence="0.999519">
Method P R F1 score
Random 31.89% 35.85% 33.75%
Selected 32.64% 39.20% 35.62%
</table>
<tableCaption confidence="0.9980435">
Table 10: Comparison of training using random
chosen sentences and selected sentences.
</tableCaption>
<table confidence="0.998627666666667">
Method F1 score
Rozovskaya et al. (2013) 31.20%
Kao et al. (2013) 25.01%
Yoshimoto et al. (2013) 22.17%
Rozovskaya and Roth (2013) 35.20%
Our method 35.62%
</table>
<tableCaption confidence="0.993801">
Table 11: Comparison of F1 of different systems
on the test data.
</tableCaption>
<subsectionHeader confidence="0.999638">
4.3 Comparison With Other Systems
</subsectionHeader>
<bodyText confidence="0.9999776">
We also compare our system with the state-of-
art systems. The first two are the top-2 systems
at CoNLL-2013 shared task : Rozovskaya et al.
(2013) and Kao et al. (2013). The third one is
the Treelet Language Model in Yoshimoto et al.
(2013). The fourth one is Rozovskaya and Roth
(2013), which until now shows the best perfor-
mance. The comparison on the test data is shown
in table 11.
In CoNLL-2013 only 5 kinds of errors are con-
sidered. Our system can be slightly modified to
handle the case where other errors such as spelling
errors should be considered. In that case, we can
modify the candidate generation of the general
module. We only need to let the generate cor-
rection candidates be any possible words that are
similar to the original word, and run the same de-
coding algorithm to get the corrected sentence. As
a comparison, the ILP systems should add extra
scoring system to score extra kind of errors.
</bodyText>
<sectionHeader confidence="0.999411" genericHeader="method">
5 Related Works
</sectionHeader>
<bodyText confidence="0.99997505">
Early grammatical error correction systems use
the knowledge engineering approach (Murata and
Nagao, 1994; Bond et al., 1996; Bond and Ikehara,
1996; Heine, 1998). However, manually designed
rules usually have exceptions. Therefore, the ma-
chine learning approach has become the dominant
approach recently. Previous machine learning ap-
proaches typically formulates the task as a clas-
sification problem. Of all the errors, determiner
and preposition errors are the two main research
topics (Knight and Chander, 1994; AEHAN et al.,
2006; Tetreault and Chodorow, 2008; Dahlmeier
and Ng, 2011). Features used in the classifica-
tion models include the context words, POS tags,
language model scores (Gamon, 2010), and tree
level features (Tetreault et al., 2010). Models used
include maximum entropy (AEHAN et al., 2006;
Tetreault and Chodorow, 2008), averaged percep-
tron, Naive Bayes (Rozovskaya and Roth, 2011),
etc. Other errors such as verb form and noun num-
ber errors also attract some attention recently (Liu
et al., 2010; Tajiri et al., 2012).
Recent research efforts have started to deal with
correcting different errors jointly (Gamon, 2011;
Park and Levy, 2011; Dahlmeier and Ng, 2012a;
Wu and Ng, 2013; Rozovskaya and Roth, 2013).
Gamon (2011) uses a high-order sequential label-
ing model to detect various errors. Park and Levy
(2011) models grammatical error correction using
a noisy channel model. Dahlmeier and Ng (2012a)
uses a beam search decoder, which iteratively cor-
rects to produce the best corrected output. Wu and
Ng (2013) and Rozovskaya and Roth (2013) use
ILP to decode a global optimized result. The joint
learning and joint inference are still at word/phrase
level and are based on the noisy context. In the
worst case, the time complexity of ILP can reach
exponent. In contrast, our system corrects gram-
mar errors at tree level directly, and the decoding
is finished with polynomial time complexity.
</bodyText>
<sectionHeader confidence="0.998684" genericHeader="conclusions">
6 Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.998676875">
In this paper we describe our grammar error cor-
rection system which corrects errors at tree level
directly. We propose a TreeNode Language Model
and use it in the general module to correct errors
related to verbs and nouns. The TNLM is easy to
train and the decoding of corrected sentence is ef-
ficient. In the special module, two extra classifica-
tion models are trained to correct errors related to
</bodyText>
<page confidence="0.991439">
274
</page>
<bodyText confidence="0.999922857142857">
determiners and prepositions at tree level directly.
Because our current method depends on an auto-
matically parsed corpus, future work may include
applying some additional filtering (e.g. Mejer and
Crammer (2012)) of the extended training set ac-
cording to some confidence measure of parsing ac-
curacy.
</bodyText>
<sectionHeader confidence="0.998091" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998225">
This research was partly supported by Na-
tional Natural Science Foundation of China
(No.61370117,61333018), Major National Social
Science Fund of China (No.12&amp;ZD227) and Na-
tional High Technology Research and Devel-
opment Program of China (863 Program) (No.
2012AA011101). The contact author of this pa-
per, according to the meaning given to this role
by Key Laboratory of Computational Linguistics,
Ministry of Education, School of Electronics En-
gineering and Computer Science, Peking Univer-
sity, is Houfeng Wang. We thank Longyue Wang
and the reviewers for their comments and sugges-
tions.
</bodyText>
<sectionHeader confidence="0.977046" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.809361506849315">
AEHAN, N., Chodorow, M., and LEACOCK,
C. L. (2006). Detecting errors in english arti-
cle usage by non-native speakers.
Baum, L. E., Petrie, T., Soules, G., and Weiss, N.
(1970). A maximization technique occurring in
the statistical analysis of probabilistic functions
of markov chains. The annals of mathematical
statistics, pages 164–171.
Bond, F. and Ikehara, S. (1996). When and how to
disambiguate?—countability in machine trans-
lation—. In International Seminar on Multi-
modal Interactive Disambiguation: MIDDIM-
96, pages 29–40. Citeseer.
Bond, F., Ogura, K., and Kawaoka, T. (1996).
Noun phrase reference in japanese-to-english
machine translation. arXiv preprint cmp-
lg/9601008.
Dahlmeier, D. and Ng, H. T. (2011). Grammatical
error correction with alternating structure opti-
mization. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies-
Volume 1, pages 915–923. Association for
Computational Linguistics.
Dahlmeier, D. and Ng, H. T. (2012a). A beam-
search decoder for grammatical error correc-
tion. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural
Language Learning, pages 568–578. Associa-
tion for Computational Linguistics.
Dahlmeier, D. and Ng, H. T. (2012b). Better eval-
uation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North
American Chapter of the Association for Com-
putational Linguistics: Human Language Tech-
nologies, pages 568–572. Association for Com-
putational Linguistics.
Dahlmeier, D., Ng, H. T., and Wu, S. M. (2013).
Building a large annotated corpus of learner en-
glish: The nus corpus of learner english. In Pro-
ceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applica-
tions, pages 22–31.
Dale, R., Anisimoff, I., and Narroway, G. (2012).
Hoo 2012: A report on the preposition and de-
terminer error correction shared task. In Pro-
ceedings of the Seventh Workshop on Build-
ing Educational Applications Using NLP, pages
54–62. Association for Computational Linguis-
tics.
Dale, R. and Kilgarriff, A. (2011). Helping our
own: The hoo 2011 pilot shared task. In Pro-
ceedings of the 13th European Workshop on
Natural Language Generation, pages 242–249.
Association for Computational Linguistics.
Dempster, A. P., Laird, N. M., Rubin, D. B., et al.
(1977). Maximum likelihood from incomplete
data via the em algorithm. Journal of the Royal
statistical Society, 39(1):1–38.
Forney Jr, G. D. (1973). The viterbi algorithm.
Proceedings of the IEEE, 61(3):268–278.
Gamon, M. (2010). Using mostly native data
to correct errors in learners’ writing: a meta-
classifier approach. In Human Language Tech-
nologies: The 2010 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics, pages 163–171. As-
sociation for Computational Linguistics.
Gamon, M. (2011). High-order sequence model-
ing for language learner error detection. In Pro-
ceedings of the 6th Workshop on Innovative Use
of NLP for Building Educational Applications,
</bodyText>
<page confidence="0.993878">
275
</page>
<reference confidence="0.996446979591837">
pages 180–189. Association for Computational
Linguistics.
Heine, J. E. (1998). Definiteness predictions
for japanese noun phrases. In Proceedings
of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th
International Conference on Computational
Linguistics-Volume 1, pages 519–525. Associ-
ation for Computational Linguistics.
Jelinek, F. (1980). Interpolated estimation of
markov source parameters from sparse data.
Pattern recognition in practice.
Kao, T.-h., Chang, Y.-w., Chiu, H.-w., Yen, T.-H.,
Boisson, J., Wu, J.-c., and Chang, J. S. (2013).
Conll-2013 shared task: Grammatical error cor-
rection nthu system description. In Proceed-
ings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared
Task, pages 20–25, Sofia, Bulgaria. Association
for Computational Linguistics.
Knight, K. and Chander, I. (1994). Automated
postediting of documents. In AAAI, volume 94,
pages 779–784.
Liu, X., Han, B., Li, K., Stiller, S. H., and Zhou,
M. (2010). Srl-based verb selection for esl.
In Proceedings of the 2010 conference on em-
pirical methods in natural language process-
ing, pages 1068–1076. Association for Compu-
tational Linguistics.
Mejer, A. and Crammer, K. (2012). Are you
sure? confidence in prediction of dependency
tree edges. In Proceedings of the 2012 Con-
ference of the North American Chapter of the
Association for Computational Linguistics: Hu-
man Language Technologies, pages 573–576,
Montr´eal, Canada. Association for Computa-
tional Linguistics.
Murata, M. and Nagao, M. (1994). Determination
of referential property and number of nouns in
japanese sentences for machine translation into
english. arXiv preprint cmp-lg/9405019.
Ng, H. T., Wu, S. M., Wu, Y., Hadiwinoto, C., and
Tetreault, J. (2013). The conll-2013 shared task
on grammatical error correction. In Proceed-
ings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared
Task, pages 1–12, Sofia, Bulgaria. Association
for Computational Linguistics.
Park, Y. A. and Levy, R. (2011). Automated
whole sentence grammar correction using a
noisy channel model. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies-Volume 1, pages 934–944. Asso-
ciation for Computational Linguistics.
Pauls, A. and Klein, D. (2012). Large-scale syn-
tactic language modeling with treelets. In Pro-
ceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics: Long
Papers-Volume 1, pages 959–968. Association
for Computational Linguistics.
Rozovskaya, A., Chang, K.-W., Sammons, M.,
and Roth, D. (2013). The university of illi-
nois system in the conll-2013 shared task.
In Proceedings of the Seventeenth Conference
on Computational Natural Language Learning:
Shared Task, pages 13–19, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Rozovskaya, A. and Roth, D. (2011). Algorithm
selection and model adaptation for esl correc-
tion tasks. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies-
Volume 1, pages 924–933. Association for
Computational Linguistics.
Rozovskaya, A. and Roth, D. (2013). Joint learn-
ing and inference for grammatical error correc-
tion. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Pro-
cessing, pages 791–802, Seattle, Washington,
USA. Association for Computational Linguis-
tics.
Tajiri, T., Komachi, M., and Matsumoto, Y.
(2012). Tense and aspect error correction for
esl learners using global context. In Proceed-
ings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Short
Papers-Volume 2, pages 198–202. Association
for Computational Linguistics.
Tetreault, J., Foster, J., and Chodorow, M. (2010).
Using parse features for preposition selection
and error detection. In Proceedings of the acl
2010 conference short papers, pages 353–358.
Association for Computational Linguistics.
Tetreault, J. R. and Chodorow, M. (2008). The
ups and downs of preposition error detec-
tion in esl writing. In Proceedings of the
22nd International Conference on Computa-
</reference>
<page confidence="0.976014">
276
</page>
<reference confidence="0.999732">
tional Linguistics-Volume 1, pages 865–872.
Association for Computational Linguistics.
Wu, Y. and Ng, H. T. (2013). Grammatical error
correction using integer linear programming. In
Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1456–1465, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Xing, J., Wang, L., Wong, D. F., Chao, L. S., and
Zeng, X. (2013). Um-checker: A hybrid sys-
tem for english grammatical error correction.
In Proceedings of the Seventeenth Conference
on Computational Natural Language Learning:
Shared Task, pages 34–42, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Yoshimoto, I., Kose, T., Mitsuzawa, K., Sak-
aguchi, K., Mizumoto, T., Hayashibe, Y., Ko-
machi, M., and Matsumoto, Y. (2013). Naist at
2013 conll grammatical error correction shared
task. CoNLL-2013, 26.
</reference>
<page confidence="0.997269">
277
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.603072">
<title confidence="0.999037">Go Climb a Dependency Tree and Correct the Grammatical Errors</title>
<author confidence="0.981601">Longkai Zhang Houfeng</author>
<affiliation confidence="0.9420655">Key Laboratory of Computational Linguistics (Peking Ministry of Education,</affiliation>
<email confidence="0.864391">zhlongk@qq.com,wanghf@pku.edu.cn</email>
<abstract confidence="0.98957245">State-of-art systems for grammar error correction often correct errors based on word sequences or phrases. In this paper, we describe a grammar error correction system which corrects grammatical errors at tree level directly. We cluster all error into two groups and divide our system into two modules correspondingly: the general module and the special module. In the general module, we propose a TreeNode Language Model to correct errors related to verbs and nouns. The TreeNode Language Model is easy to train and the decoding is efficient. In the special module, two extra classification models are trained to correct errors related to determiners and prepositions. Experiments show that our system outperforms the state-of-art sysand improves the</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<pages>180--189</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<marker></marker>
<rawString>pages 180–189. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Heine</author>
</authors>
<title>Definiteness predictions for japanese noun phrases.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>519--525</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="32637" citStr="Heine, 1998" startWordPosition="5509" endWordPosition="5510">ndle the case where other errors such as spelling errors should be considered. In that case, we can modify the candidate generation of the general module. We only need to let the generate correction candidates be any possible words that are similar to the original word, and run the same decoding algorithm to get the corrected sentence. As a comparison, the ILP systems should add extra scoring system to score extra kind of errors. 5 Related Works Early grammatical error correction systems use the knowledge engineering approach (Murata and Nagao, 1994; Bond et al., 1996; Bond and Ikehara, 1996; Heine, 1998). However, manually designed rules usually have exceptions. Therefore, the machine learning approach has become the dominant approach recently. Previous machine learning approaches typically formulates the task as a classification problem. Of all the errors, determiner and preposition errors are the two main research topics (Knight and Chander, 1994; AEHAN et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in the classification models include the context words, POS tags, language model scores (Gamon, 2010), and tree level features (Tetreault et al., 2010). Model</context>
</contexts>
<marker>Heine, 1998</marker>
<rawString>Heine, J. E. (1998). Definiteness predictions for japanese noun phrases. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics-Volume 1, pages 519–525. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Interpolated estimation of markov source parameters from sparse data. Pattern recognition in practice.</title>
<date>1980</date>
<contexts>
<context position="18993" citStr="Jelinek, 1980" startWordPosition="3186" endWordPosition="3187">torm Table 3: All the sub-sentences in the example of figure 2 In our system we train the TLM using the same way as training tri-gram language model. For a sub-sequence 5 = w0...wL, we calculate P(5) = HL P w I w w ) The smoothingmethod Z=o ( Z Z—i Z-2 g we use is interpolation, which assumes the final P0(wz|wz−1wz−2) of the tri-gram language model follows the following decomposition: P0(wz|wz−1wz−2) =A1P(wz|wz−1wz−2) +A2P(wz|wz−1) (1) +A3P(wz) where A1, A2 and A3 are parameters sum to 1. The parameters A1, A2 and A3 are estimated through EM algorithm(Baum et al., 1970; Dempster et al., 1977; Jelinek, 1980). 3.3 Decoding The decoding problem is to select one correction candidate for each node that maximizes the overall score of the corrected sentence. When the sentence is long and contains many verbs and nouns, enumerating all possible candidate selections is time-consuming. We use a bottom-up dynamic 270 Figure 2: A illustrative sentence for TreeNode Language Model. programming approach to find the maximized corrections within polynomial time complexity. For a node n with L correction candidates n1, ...nL and K child nodes C1,..., CK, we define n.scores[i] as the maximum score if we choose the </context>
</contexts>
<marker>Jelinek, 1980</marker>
<rawString>Jelinek, F. (1980). Interpolated estimation of markov source parameters from sparse data. Pattern recognition in practice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T-h Kao</author>
<author>Y-w Chang</author>
<author>H-w Chiu</author>
<author>T-H Yen</author>
<author>J Boisson</author>
<author>J-c Wu</author>
<author>J S Chang</author>
</authors>
<title>Conll-2013 shared task: Grammatical error correction nthu system description.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>pages</pages>
<institution>Sofia, Bulgaria. Association for Computational Linguistics.</institution>
<contexts>
<context position="31355" citStr="Kao et al. (2013)" startWordPosition="5285" endWordPosition="5288">P R F1 score Local Classifier 26.38% 39.14% 31.51% Our Tree-based 32.64% 39.20% 35.62% Table 9: Comparison for the special module on the test data. The input of the special module is the sentences corrected by the TNLM in the general module. the selected training data of our system. We can see that the data selection (cleaning) procedure is important for the improvement of system F1. Method P R F1 score Random 31.89% 35.85% 33.75% Selected 32.64% 39.20% 35.62% Table 10: Comparison of training using random chosen sentences and selected sentences. Method F1 score Rozovskaya et al. (2013) 31.20% Kao et al. (2013) 25.01% Yoshimoto et al. (2013) 22.17% Rozovskaya and Roth (2013) 35.20% Our method 35.62% Table 11: Comparison of F1 of different systems on the test data. 4.3 Comparison With Other Systems We also compare our system with the state-ofart systems. The first two are the top-2 systems at CoNLL-2013 shared task : Rozovskaya et al. (2013) and Kao et al. (2013). The third one is the Treelet Language Model in Yoshimoto et al. (2013). The fourth one is Rozovskaya and Roth (2013), which until now shows the best performance. The comparison on the test data is shown in table 11. In CoNLL-2013 only 5 kin</context>
</contexts>
<marker>Kao, Chang, Chiu, Yen, Boisson, Wu, Chang, 2013</marker>
<rawString>Kao, T.-h., Chang, Y.-w., Chiu, H.-w., Yen, T.-H., Boisson, J., Wu, J.-c., and Chang, J. S. (2013). Conll-2013 shared task: Grammatical error correction nthu system description. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 20–25, Sofia, Bulgaria. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>I Chander</author>
</authors>
<title>Automated postediting of documents.</title>
<date>1994</date>
<booktitle>In AAAI,</booktitle>
<volume>94</volume>
<pages>779--784</pages>
<contexts>
<context position="32988" citStr="Knight and Chander, 1994" startWordPosition="5558" endWordPosition="5561">s a comparison, the ILP systems should add extra scoring system to score extra kind of errors. 5 Related Works Early grammatical error correction systems use the knowledge engineering approach (Murata and Nagao, 1994; Bond et al., 1996; Bond and Ikehara, 1996; Heine, 1998). However, manually designed rules usually have exceptions. Therefore, the machine learning approach has become the dominant approach recently. Previous machine learning approaches typically formulates the task as a classification problem. Of all the errors, determiner and preposition errors are the two main research topics (Knight and Chander, 1994; AEHAN et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in the classification models include the context words, POS tags, language model scores (Gamon, 2010), and tree level features (Tetreault et al., 2010). Models used include maximum entropy (AEHAN et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, Naive Bayes (Rozovskaya and Roth, 2011), etc. Other errors such as verb form and noun number errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Recent research efforts have started to deal with correcting differen</context>
</contexts>
<marker>Knight, Chander, 1994</marker>
<rawString>Knight, K. and Chander, I. (1994). Automated postediting of documents. In AAAI, volume 94, pages 779–784.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Liu</author>
<author>B Han</author>
<author>K Li</author>
<author>S H Stiller</author>
<author>M Zhou</author>
</authors>
<title>Srl-based verb selection for esl.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 conference on empirical methods in natural language processing,</booktitle>
<pages>1068--1076</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="33495" citStr="Liu et al., 2010" startWordPosition="5639" endWordPosition="5642"> all the errors, determiner and preposition errors are the two main research topics (Knight and Chander, 1994; AEHAN et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in the classification models include the context words, POS tags, language model scores (Gamon, 2010), and tree level features (Tetreault et al., 2010). Models used include maximum entropy (AEHAN et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, Naive Bayes (Rozovskaya and Roth, 2011), etc. Other errors such as verb form and noun number errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Recent research efforts have started to deal with correcting different errors jointly (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a; Wu and Ng, 2013; Rozovskaya and Roth, 2013). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model. Dahlmeier and Ng (2012a) uses a beam search decoder, which iteratively corrects to produce the best corrected output. Wu and Ng (2013) and Rozovskaya and Roth (2013) use ILP to decode a global optimized result. The j</context>
</contexts>
<marker>Liu, Han, Li, Stiller, Zhou, 2010</marker>
<rawString>Liu, X., Han, B., Li, K., Stiller, S. H., and Zhou, M. (2010). Srl-based verb selection for esl. In Proceedings of the 2010 conference on empirical methods in natural language processing, pages 1068–1076. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mejer</author>
<author>K Crammer</author>
</authors>
<title>Are you sure? confidence in prediction of dependency tree edges.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>573--576</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="35040" citStr="Mejer and Crammer (2012)" startWordPosition="5893" endWordPosition="5896">ure work In this paper we describe our grammar error correction system which corrects errors at tree level directly. We propose a TreeNode Language Model and use it in the general module to correct errors related to verbs and nouns. The TNLM is easy to train and the decoding of corrected sentence is efficient. In the special module, two extra classification models are trained to correct errors related to 274 determiners and prepositions at tree level directly. Because our current method depends on an automatically parsed corpus, future work may include applying some additional filtering (e.g. Mejer and Crammer (2012)) of the extended training set according to some confidence measure of parsing accuracy. Acknowledgments This research was partly supported by National Natural Science Foundation of China (No.61370117,61333018), Major National Social Science Fund of China (No.12&amp;ZD227) and National High Technology Research and Development Program of China (863 Program) (No. 2012AA011101). The contact author of this paper, according to the meaning given to this role by Key Laboratory of Computational Linguistics, Ministry of Education, School of Electronics Engineering and Computer Science, Peking University, i</context>
</contexts>
<marker>Mejer, Crammer, 2012</marker>
<rawString>Mejer, A. and Crammer, K. (2012). Are you sure? confidence in prediction of dependency tree edges. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 573–576, Montr´eal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Murata</author>
<author>M Nagao</author>
</authors>
<title>Determination of referential property and number of nouns in japanese sentences for machine translation into english. arXiv preprint cmp-lg/9405019.</title>
<date>1994</date>
<contexts>
<context position="32580" citStr="Murata and Nagao, 1994" startWordPosition="5497" endWordPosition="5500">of errors are considered. Our system can be slightly modified to handle the case where other errors such as spelling errors should be considered. In that case, we can modify the candidate generation of the general module. We only need to let the generate correction candidates be any possible words that are similar to the original word, and run the same decoding algorithm to get the corrected sentence. As a comparison, the ILP systems should add extra scoring system to score extra kind of errors. 5 Related Works Early grammatical error correction systems use the knowledge engineering approach (Murata and Nagao, 1994; Bond et al., 1996; Bond and Ikehara, 1996; Heine, 1998). However, manually designed rules usually have exceptions. Therefore, the machine learning approach has become the dominant approach recently. Previous machine learning approaches typically formulates the task as a classification problem. Of all the errors, determiner and preposition errors are the two main research topics (Knight and Chander, 1994; AEHAN et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in the classification models include the context words, POS tags, language model scores (Gamon, 2010)</context>
</contexts>
<marker>Murata, Nagao, 1994</marker>
<rawString>Murata, M. and Nagao, M. (1994). Determination of referential property and number of nouns in japanese sentences for machine translation into english. arXiv preprint cmp-lg/9405019.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>S M Wu</author>
<author>Y Wu</author>
<author>C Hadiwinoto</author>
<author>J Tetreault</author>
</authors>
<title>The conll-2013 shared task on grammatical error correction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--12</pages>
<institution>Sofia, Bulgaria. Association for Computational Linguistics.</institution>
<contexts>
<context position="1403" citStr="Ng et al., 2013" startWordPosition="210" endWordPosition="213">ule, two extra classification models are trained to correct errors related to determiners and prepositions. Experiments show that our system outperforms the state-of-art systems and improves the F1 score. 1 Introduction The task of grammar error correction is difficult yet important. An automatic grammar error correction system can help second language (L2) learners improve the quality of their writing. In recent years, there are various competitions devoted to grammar error correction, such as the HOO2011(Dale and Kilgarriff, 2011), HOO-2012(Dale et al., 2012) and the CoNLL-2013 shared task (Ng et al., 2013). There has been a lot of work addressing errors made by L2 learners. A significant proportion of the systems for grammar error correction train individual statistical models to correct each special kind of error word by word and ignore error interactions. These methods assume no interactions between different kinds of grammatical errors. In real problem settings errors are correlated, which makes grammar error correction much more difficult. Recent research begins to focus on the error interaction problem. For example, Wu and Ng (2013) decodes a global optimized result based on the individual</context>
<context position="6420" citStr="Ng et al. (2013)" startWordPosition="1037" endWordPosition="1040">titions devoted to the grammar error correction task for L2 learners. The CoNLL2013 shared task is one of the most famous, which focuses on correcting five types of errors that are commonly made by non-native speakers of English, including determiner, preposition, noun number, subject-verb agreement and verb form errors. The training data released by the task organizers come from the NUCLE corpus(Dahlmeier et al., 2013). This corpus contains essays written by ESL learners, which are then corrected by English teachers. The test data are 50 student essays. Details of the corpus are described in Ng et al. (2013). 2.2 System Architecture In our system, lists of correction candidates are first generated for each word. We generate candidates for nouns based on their plurality. We generate candidates for verbs based on their tenses. Then we select the correction candidates that maximize the overall correctness score. An example process of correcting figure 1(a) is shown in table 1. Correcting grammatical errors using local statistical models on word sequence is insufficient. The local models can only consider the contexts in a fixed window. In the example of figure 1(a), the context of the verb “is” is “</context>
</contexts>
<marker>Ng, Wu, Wu, Hadiwinoto, Tetreault, 2013</marker>
<rawString>Ng, H. T., Wu, S. M., Wu, Y., Hadiwinoto, C., and Tetreault, J. (2013). The conll-2013 shared task on grammatical error correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, Sofia, Bulgaria. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y A Park</author>
<author>R Levy</author>
</authors>
<title>Automated whole sentence grammar correction using a noisy channel model.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>934--944</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="33638" citStr="Park and Levy, 2011" startWordPosition="5661" endWordPosition="5664"> and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in the classification models include the context words, POS tags, language model scores (Gamon, 2010), and tree level features (Tetreault et al., 2010). Models used include maximum entropy (AEHAN et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, Naive Bayes (Rozovskaya and Roth, 2011), etc. Other errors such as verb form and noun number errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Recent research efforts have started to deal with correcting different errors jointly (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a; Wu and Ng, 2013; Rozovskaya and Roth, 2013). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model. Dahlmeier and Ng (2012a) uses a beam search decoder, which iteratively corrects to produce the best corrected output. Wu and Ng (2013) and Rozovskaya and Roth (2013) use ILP to decode a global optimized result. The joint learning and joint inference are still at word/phrase level and are based on the noisy context. In the worst case, the time complexity of </context>
</contexts>
<marker>Park, Levy, 2011</marker>
<rawString>Park, Y. A. and Levy, R. (2011). Automated whole sentence grammar correction using a noisy channel model. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 934–944. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pauls</author>
<author>D Klein</author>
</authors>
<title>Large-scale syntactic language modeling with treelets.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>959--968</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9607" citStr="Pauls and Klein, 2012" startWordPosition="1580" endWordPosition="1583">ndidate selection” framework. • The special module, where two classification models are used to correct the determiner errors and preposition errors at tree level. We take special care of these two kinds of errors because these errors include both replacement errors and insertion/deletion errors. Besides, they are the most common errors made by ESL learners and is much easier to be incorporated into a classification framework. We should make it clear that we are not the first to use tree level correction models on ungrammatical sentences. Yoshimoto et al. (2013) uses a Treelet Language model (Pauls and Klein, 2012) to correct agreement errors. However, the performance of Treelet language model is not that good compared with the top-ranked system in CoNLL2013. The reason is that the production rules in the Treelet language model are based on complex contexts, which will exacerbate the data sparseness problem. The “context” in Treelet language model also include words ahead of treelets, which are sometimes unrelated to the current node. In contrast, our TreeNode Language model only needs to consider useful context words related to each node on the dependency tree. To train a TreeNode language model costs </context>
</contexts>
<marker>Pauls, Klein, 2012</marker>
<rawString>Pauls, A. and Klein, D. (2012). Large-scale syntactic language modeling with treelets. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 959–968. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>K-W Chang</author>
<author>M Sammons</author>
<author>D Roth</author>
</authors>
<title>The university of illinois system in the conll-2013 shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>13--19</pages>
<institution>Sofia, Bulgaria. Association for Computational Linguistics.</institution>
<contexts>
<context position="22516" citStr="Rozovskaya et al. (2013)" startWordPosition="3777" endWordPosition="3780">errors because these errors include insertion and deletion errors, which cannot be corrected in the general module. Because there is a fixed number of prepositions and determiners, these two kinds of errors are much easier to be incorporated into a classification framework. Besides, they are the most common errors made by ESL learners and there are lots of previous works that leave valuable guidance for us to follow. Similar to many previous state-of-art systems, we treat the correction of determiner errors and preposition errors as a classification problem. Although some previous works (e.g. Rozovskaya et al. (2013)) use NPs and the head of NPs as 271 features, they are basically local classifiers making predictions on word sequences. Difference to the local classifier approaches, we make predictions on the nodes of the dependency tree directly. In our system we correct determiner errors and preposition errors separately. For the determiner errors, we consider the insertion, deletion and replacement of articles (i.e. ‘a’, ‘an’ and ‘the’). Because the articles are used to modify nouns in the dependency trees, we can classify based on noun nodes. We give each noun node (node whose POS tag is noun) a label </context>
<context position="24772" citStr="Rozovskaya et al. (2013)" startWordPosition="4177" endWordPosition="4180">n preposition nodes (nodes whose POS tag is preposition). We use prepositions as labels to indicate which preposition should be used. and use “0” to denote that the preposition should be deleted. We use the same definition of LP and RP as the correction of determiner errors. Detailed feature templates we use to correct preposition errors are listed in table 6. Similar to the previous work(Xing et al., 2013), we find that adding more prepositions will not improve the performance in our experiments. Thus we only consider a fixed set of prepositions: {in, for, to, of, on}. Previous works such as Rozovskaya et al. (2013) show that Naive Bayes model and averaged perceptron model show better results than other classification models. These classifiers can give a reasonably good performance when there are limited amount of training data. In our system, we use large amount of automatically generated training data based on the parsed Gigaword corpus instead of the limited training data provided by CoNLL2013. Take generating training data for determiner errors as an example. We generate training data based on the parsed Gigaword corpus C described in section 2. Each sentence S in C is a dependency tree T. We use eac</context>
<context position="31330" citStr="Rozovskaya et al. (2013)" startWordPosition="5280" endWordPosition="5283">en training data and 273 Method P R F1 score Local Classifier 26.38% 39.14% 31.51% Our Tree-based 32.64% 39.20% 35.62% Table 9: Comparison for the special module on the test data. The input of the special module is the sentences corrected by the TNLM in the general module. the selected training data of our system. We can see that the data selection (cleaning) procedure is important for the improvement of system F1. Method P R F1 score Random 31.89% 35.85% 33.75% Selected 32.64% 39.20% 35.62% Table 10: Comparison of training using random chosen sentences and selected sentences. Method F1 score Rozovskaya et al. (2013) 31.20% Kao et al. (2013) 25.01% Yoshimoto et al. (2013) 22.17% Rozovskaya and Roth (2013) 35.20% Our method 35.62% Table 11: Comparison of F1 of different systems on the test data. 4.3 Comparison With Other Systems We also compare our system with the state-ofart systems. The first two are the top-2 systems at CoNLL-2013 shared task : Rozovskaya et al. (2013) and Kao et al. (2013). The third one is the Treelet Language Model in Yoshimoto et al. (2013). The fourth one is Rozovskaya and Roth (2013), which until now shows the best performance. The comparison on the test data is shown in table 11.</context>
</contexts>
<marker>Rozovskaya, Chang, Sammons, Roth, 2013</marker>
<rawString>Rozovskaya, A., Chang, K.-W., Sammons, M., and Roth, D. (2013). The university of illinois system in the conll-2013 shared task. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 13–19, Sofia, Bulgaria. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Algorithm selection and model adaptation for esl correction tasks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesVolume 1,</booktitle>
<pages>924--933</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="33380" citStr="Rozovskaya and Roth, 2011" startWordPosition="5618" endWordPosition="5621">ominant approach recently. Previous machine learning approaches typically formulates the task as a classification problem. Of all the errors, determiner and preposition errors are the two main research topics (Knight and Chander, 1994; AEHAN et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in the classification models include the context words, POS tags, language model scores (Gamon, 2010), and tree level features (Tetreault et al., 2010). Models used include maximum entropy (AEHAN et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, Naive Bayes (Rozovskaya and Roth, 2011), etc. Other errors such as verb form and noun number errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Recent research efforts have started to deal with correcting different errors jointly (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a; Wu and Ng, 2013; Rozovskaya and Roth, 2013). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model. Dahlmeier and Ng (2012a) uses a beam search decoder, which iteratively corrects to produce the best c</context>
</contexts>
<marker>Rozovskaya, Roth, 2011</marker>
<rawString>Rozovskaya, A. and Roth, D. (2011). Algorithm selection and model adaptation for esl correction tasks. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesVolume 1, pages 924–933. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Joint learning and inference for grammatical error correction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>791--802</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="2150" citStr="Rozovskaya and Roth (2013)" startWordPosition="329" endWordPosition="333"> error correction train individual statistical models to correct each special kind of error word by word and ignore error interactions. These methods assume no interactions between different kinds of grammatical errors. In real problem settings errors are correlated, which makes grammar error correction much more difficult. Recent research begins to focus on the error interaction problem. For example, Wu and Ng (2013) decodes a global optimized result based on the individual correction confidence of each kind of errors. The individual correction confidence is still based on the noisy context. Rozovskaya and Roth (2013) uses a joint modeling approach, which considers corrections in phrase structures instead of words. For dependencies that are not covered by the joint learning model, Rozovskaya and Roth (2013) uses the results of Illinois system in the joint inference. These results are still at word level and are based on the noisy context. These systems can consider error interactions, however, the systems are complex and inefficient. In both Wu and Ng (2013) and Rozovskaya and Roth (2013), Integer Linear Programming (ILP) is used for decoding a global optimized result. In the worst case, the time complexit</context>
<context position="31420" citStr="Rozovskaya and Roth (2013)" startWordPosition="5295" endWordPosition="5298">ree-based 32.64% 39.20% 35.62% Table 9: Comparison for the special module on the test data. The input of the special module is the sentences corrected by the TNLM in the general module. the selected training data of our system. We can see that the data selection (cleaning) procedure is important for the improvement of system F1. Method P R F1 score Random 31.89% 35.85% 33.75% Selected 32.64% 39.20% 35.62% Table 10: Comparison of training using random chosen sentences and selected sentences. Method F1 score Rozovskaya et al. (2013) 31.20% Kao et al. (2013) 25.01% Yoshimoto et al. (2013) 22.17% Rozovskaya and Roth (2013) 35.20% Our method 35.62% Table 11: Comparison of F1 of different systems on the test data. 4.3 Comparison With Other Systems We also compare our system with the state-ofart systems. The first two are the top-2 systems at CoNLL-2013 shared task : Rozovskaya et al. (2013) and Kao et al. (2013). The third one is the Treelet Language Model in Yoshimoto et al. (2013). The fourth one is Rozovskaya and Roth (2013), which until now shows the best performance. The comparison on the test data is shown in table 11. In CoNLL-2013 only 5 kinds of errors are considered. Our system can be slightly modified </context>
<context position="33708" citStr="Rozovskaya and Roth, 2013" startWordPosition="5673" endWordPosition="5676">the classification models include the context words, POS tags, language model scores (Gamon, 2010), and tree level features (Tetreault et al., 2010). Models used include maximum entropy (AEHAN et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, Naive Bayes (Rozovskaya and Roth, 2011), etc. Other errors such as verb form and noun number errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Recent research efforts have started to deal with correcting different errors jointly (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a; Wu and Ng, 2013; Rozovskaya and Roth, 2013). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model. Dahlmeier and Ng (2012a) uses a beam search decoder, which iteratively corrects to produce the best corrected output. Wu and Ng (2013) and Rozovskaya and Roth (2013) use ILP to decode a global optimized result. The joint learning and joint inference are still at word/phrase level and are based on the noisy context. In the worst case, the time complexity of ILP can reach exponent. In contrast, our system corrects grammar error</context>
</contexts>
<marker>Rozovskaya, Roth, 2013</marker>
<rawString>Rozovskaya, A. and Roth, D. (2013). Joint learning and inference for grammatical error correction. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 791–802, Seattle, Washington, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Tajiri</author>
<author>M Komachi</author>
<author>Y Matsumoto</author>
</authors>
<title>Tense and aspect error correction for esl learners using global context.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2,</booktitle>
<pages>198--202</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="33517" citStr="Tajiri et al., 2012" startWordPosition="5643" endWordPosition="5646">eterminer and preposition errors are the two main research topics (Knight and Chander, 1994; AEHAN et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in the classification models include the context words, POS tags, language model scores (Gamon, 2010), and tree level features (Tetreault et al., 2010). Models used include maximum entropy (AEHAN et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, Naive Bayes (Rozovskaya and Roth, 2011), etc. Other errors such as verb form and noun number errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Recent research efforts have started to deal with correcting different errors jointly (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a; Wu and Ng, 2013; Rozovskaya and Roth, 2013). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model. Dahlmeier and Ng (2012a) uses a beam search decoder, which iteratively corrects to produce the best corrected output. Wu and Ng (2013) and Rozovskaya and Roth (2013) use ILP to decode a global optimized result. The joint learning and join</context>
</contexts>
<marker>Tajiri, Komachi, Matsumoto, 2012</marker>
<rawString>Tajiri, T., Komachi, M., and Matsumoto, Y. (2012). Tense and aspect error correction for esl learners using global context. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 198–202. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>J Foster</author>
<author>M Chodorow</author>
</authors>
<title>Using parse features for preposition selection and error detection.</title>
<date>2010</date>
<booktitle>In Proceedings of the acl</booktitle>
<pages>353--358</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="33230" citStr="Tetreault et al., 2010" startWordPosition="5596" endWordPosition="5599">nd Ikehara, 1996; Heine, 1998). However, manually designed rules usually have exceptions. Therefore, the machine learning approach has become the dominant approach recently. Previous machine learning approaches typically formulates the task as a classification problem. Of all the errors, determiner and preposition errors are the two main research topics (Knight and Chander, 1994; AEHAN et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in the classification models include the context words, POS tags, language model scores (Gamon, 2010), and tree level features (Tetreault et al., 2010). Models used include maximum entropy (AEHAN et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, Naive Bayes (Rozovskaya and Roth, 2011), etc. Other errors such as verb form and noun number errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Recent research efforts have started to deal with correcting different errors jointly (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a; Wu and Ng, 2013; Rozovskaya and Roth, 2013). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatic</context>
</contexts>
<marker>Tetreault, Foster, Chodorow, 2010</marker>
<rawString>Tetreault, J., Foster, J., and Chodorow, M. (2010). Using parse features for preposition selection and error detection. In Proceedings of the acl 2010 conference short papers, pages 353–358. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Tetreault</author>
<author>M Chodorow</author>
</authors>
<title>The ups and downs of preposition error detection in esl writing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>865--872</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="33038" citStr="Tetreault and Chodorow, 2008" startWordPosition="5566" endWordPosition="5569">ra scoring system to score extra kind of errors. 5 Related Works Early grammatical error correction systems use the knowledge engineering approach (Murata and Nagao, 1994; Bond et al., 1996; Bond and Ikehara, 1996; Heine, 1998). However, manually designed rules usually have exceptions. Therefore, the machine learning approach has become the dominant approach recently. Previous machine learning approaches typically formulates the task as a classification problem. Of all the errors, determiner and preposition errors are the two main research topics (Knight and Chander, 1994; AEHAN et al., 2006; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011). Features used in the classification models include the context words, POS tags, language model scores (Gamon, 2010), and tree level features (Tetreault et al., 2010). Models used include maximum entropy (AEHAN et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, Naive Bayes (Rozovskaya and Roth, 2011), etc. Other errors such as verb form and noun number errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Recent research efforts have started to deal with correcting different errors jointly (Gamon, 2011; Park and Levy, 2011</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>Tetreault, J. R. and Chodorow, M. (2008). The ups and downs of preposition error detection in esl writing. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 865–872. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wu</author>
<author>H T Ng</author>
</authors>
<title>Grammatical error correction using integer linear programming.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1456--1465</pages>
<institution>Sofia, Bulgaria. Association for Computational Linguistics.</institution>
<contexts>
<context position="1945" citStr="Wu and Ng (2013)" startWordPosition="297" endWordPosition="300">O-2012(Dale et al., 2012) and the CoNLL-2013 shared task (Ng et al., 2013). There has been a lot of work addressing errors made by L2 learners. A significant proportion of the systems for grammar error correction train individual statistical models to correct each special kind of error word by word and ignore error interactions. These methods assume no interactions between different kinds of grammatical errors. In real problem settings errors are correlated, which makes grammar error correction much more difficult. Recent research begins to focus on the error interaction problem. For example, Wu and Ng (2013) decodes a global optimized result based on the individual correction confidence of each kind of errors. The individual correction confidence is still based on the noisy context. Rozovskaya and Roth (2013) uses a joint modeling approach, which considers corrections in phrase structures instead of words. For dependencies that are not covered by the joint learning model, Rozovskaya and Roth (2013) uses the results of Illinois system in the joint inference. These results are still at word level and are based on the noisy context. These systems can consider error interactions, however, the systems</context>
<context position="33680" citStr="Wu and Ng, 2013" startWordPosition="5669" endWordPosition="5672">Features used in the classification models include the context words, POS tags, language model scores (Gamon, 2010), and tree level features (Tetreault et al., 2010). Models used include maximum entropy (AEHAN et al., 2006; Tetreault and Chodorow, 2008), averaged perceptron, Naive Bayes (Rozovskaya and Roth, 2011), etc. Other errors such as verb form and noun number errors also attract some attention recently (Liu et al., 2010; Tajiri et al., 2012). Recent research efforts have started to deal with correcting different errors jointly (Gamon, 2011; Park and Levy, 2011; Dahlmeier and Ng, 2012a; Wu and Ng, 2013; Rozovskaya and Roth, 2013). Gamon (2011) uses a high-order sequential labeling model to detect various errors. Park and Levy (2011) models grammatical error correction using a noisy channel model. Dahlmeier and Ng (2012a) uses a beam search decoder, which iteratively corrects to produce the best corrected output. Wu and Ng (2013) and Rozovskaya and Roth (2013) use ILP to decode a global optimized result. The joint learning and joint inference are still at word/phrase level and are based on the noisy context. In the worst case, the time complexity of ILP can reach exponent. In contrast, our s</context>
</contexts>
<marker>Wu, Ng, 2013</marker>
<rawString>Wu, Y. and Ng, H. T. (2013). Grammatical error correction using integer linear programming. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1456–1465, Sofia, Bulgaria. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xing</author>
<author>L Wang</author>
<author>D F Wong</author>
<author>L S Chao</author>
<author>X Zeng</author>
</authors>
<title>Um-checker: A hybrid system for english grammatical error correction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>34--42</pages>
<institution>Sofia, Bulgaria. Association for Computational Linguistics.</institution>
<contexts>
<context position="24558" citStr="Xing et al., 2013" startWordPosition="4139" endWordPosition="4142"> as a post-process. For the preposition errors, we only consider deletion and replacement of an existing preposition. The classification framework is similar to determiner errors. We consider classification on preposition nodes (nodes whose POS tag is preposition). We use prepositions as labels to indicate which preposition should be used. and use “0” to denote that the preposition should be deleted. We use the same definition of LP and RP as the correction of determiner errors. Detailed feature templates we use to correct preposition errors are listed in table 6. Similar to the previous work(Xing et al., 2013), we find that adding more prepositions will not improve the performance in our experiments. Thus we only consider a fixed set of prepositions: {in, for, to, of, on}. Previous works such as Rozovskaya et al. (2013) show that Naive Bayes model and averaged perceptron model show better results than other classification models. These classifiers can give a reasonably good performance when there are limited amount of training data. In our system, we use large amount of automatically generated training data based on the parsed Gigaword corpus instead of the limited training data provided by CoNLL20</context>
</contexts>
<marker>Xing, Wang, Wong, Chao, Zeng, 2013</marker>
<rawString>Xing, J., Wang, L., Wong, D. F., Chao, L. S., and Zeng, X. (2013). Um-checker: A hybrid system for english grammatical error correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 34–42, Sofia, Bulgaria. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Yoshimoto</author>
<author>T Kose</author>
<author>K Mitsuzawa</author>
<author>K Sakaguchi</author>
<author>T Mizumoto</author>
<author>Y Hayashibe</author>
<author>M Komachi</author>
<author>Y Matsumoto</author>
</authors>
<title>Naist at 2013 conll grammatical error correction shared task.</title>
<date>2013</date>
<booktitle>CoNLL-2013,</booktitle>
<pages>26</pages>
<contexts>
<context position="9553" citStr="Yoshimoto et al. (2013)" startWordPosition="1571" endWordPosition="1574"> example of the “correction candidate generation and candidate selection” framework. • The special module, where two classification models are used to correct the determiner errors and preposition errors at tree level. We take special care of these two kinds of errors because these errors include both replacement errors and insertion/deletion errors. Besides, they are the most common errors made by ESL learners and is much easier to be incorporated into a classification framework. We should make it clear that we are not the first to use tree level correction models on ungrammatical sentences. Yoshimoto et al. (2013) uses a Treelet Language model (Pauls and Klein, 2012) to correct agreement errors. However, the performance of Treelet language model is not that good compared with the top-ranked system in CoNLL2013. The reason is that the production rules in the Treelet language model are based on complex contexts, which will exacerbate the data sparseness problem. The “context” in Treelet language model also include words ahead of treelets, which are sometimes unrelated to the current node. In contrast, our TreeNode Language model only needs to consider useful context words related to each node on the depe</context>
<context position="31386" citStr="Yoshimoto et al. (2013)" startWordPosition="5290" endWordPosition="5293">fier 26.38% 39.14% 31.51% Our Tree-based 32.64% 39.20% 35.62% Table 9: Comparison for the special module on the test data. The input of the special module is the sentences corrected by the TNLM in the general module. the selected training data of our system. We can see that the data selection (cleaning) procedure is important for the improvement of system F1. Method P R F1 score Random 31.89% 35.85% 33.75% Selected 32.64% 39.20% 35.62% Table 10: Comparison of training using random chosen sentences and selected sentences. Method F1 score Rozovskaya et al. (2013) 31.20% Kao et al. (2013) 25.01% Yoshimoto et al. (2013) 22.17% Rozovskaya and Roth (2013) 35.20% Our method 35.62% Table 11: Comparison of F1 of different systems on the test data. 4.3 Comparison With Other Systems We also compare our system with the state-ofart systems. The first two are the top-2 systems at CoNLL-2013 shared task : Rozovskaya et al. (2013) and Kao et al. (2013). The third one is the Treelet Language Model in Yoshimoto et al. (2013). The fourth one is Rozovskaya and Roth (2013), which until now shows the best performance. The comparison on the test data is shown in table 11. In CoNLL-2013 only 5 kinds of errors are considered. Ou</context>
</contexts>
<marker>Yoshimoto, Kose, Mitsuzawa, Sakaguchi, Mizumoto, Hayashibe, Komachi, Matsumoto, 2013</marker>
<rawString>Yoshimoto, I., Kose, T., Mitsuzawa, K., Sakaguchi, K., Mizumoto, T., Hayashibe, Y., Komachi, M., and Matsumoto, Y. (2013). Naist at 2013 conll grammatical error correction shared task. CoNLL-2013, 26.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
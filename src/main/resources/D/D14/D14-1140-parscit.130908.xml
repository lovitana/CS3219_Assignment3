<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000023">
<title confidence="0.9989845">
Don’t Until the Final Verb Wait:
Reinforcement Learning for Simultaneous Machine Translation
</title>
<author confidence="0.934855">
Alvin C. Grissom II
and Jordan Boyd-Graber
</author>
<affiliation confidence="0.995232">
Computer Science
University of Colorado
</affiliation>
<address confidence="0.708029">
Boulder, CO
</address>
<email confidence="0.990387">
Alvin.Grissom@colorado.edu
Jordan.Boyd.Graber@colorado.edu
</email>
<sectionHeader confidence="0.995498" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999132">
We introduce a reinforcement learning-
based approach to simultaneous ma-
chine translation—producing a trans-
lation while receiving input words—
between languages with drastically dif-
ferent word orders: from verb-final lan-
guages (e.g., German) to verb-medial
languages (English). In traditional ma-
chine translation, a translator must
“wait” for source material to appear be-
fore translation begins. We remove this
bottleneck by predicting the final verb
in advance. We use reinforcement learn-
ing to learn when to trust predictions
about unseen, future portions of the
sentence. We also introduce an evalua-
tion metric to measure expeditiousness
and quality. We show that our new
translation model outperforms batch
and monotone translation strategies.
</bodyText>
<sectionHeader confidence="0.998737" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971">
We introduce a simultaneous machine transla-
tion (MT) system that predicts unseen verbs
and uses reinforcement learning to learn when
to trust these predictions and when to wait for
more input.
Simultaneous translation is producing a par-
tial translation of a sentence before the input
sentence is complete, and is often used in im-
portant diplomatic settings. One of the first
noted uses of human simultaneous interpreta-
tion was the Nuremberg trials after the Sec-
ond World War. Siegfried Ramler (2009), the
Austrian-American who organized the transla-
tion teams, describes the linguistic predictions
</bodyText>
<note confidence="0.8569295">
He He, John Morgan,
and Hal Daum´e III
</note>
<affiliation confidence="0.788621">
Computer Science and UMIACS
University of Maryland
</affiliation>
<address confidence="0.790686">
College Park, MD
</address>
<email confidence="0.997949">
{hhe,jjm,hal}@cs.umd.edu
</email>
<bodyText confidence="0.997704894736842">
and circumlocutions that translators would use
to achieve a tradeoff between translation la-
tency and accuracy. The audio recording tech-
nology used by those interpreters sowed the
seeds of technology-assisted interpretation at
the United Nations (Gaiba, 1998).
Performing real-time translation is especially
difficult when information that comes early in
the target language (the language you’re trans-
lating to) comes late in the source language (the
language you’re translating from). A common
example is when translating from a verb-final
(sov) language (e.g., German or Japanese) to
a verb-medial (svo) language, (e.g., English).
In the example in Figure 1, for instance, the
main verb of the sentence (in bold) appears
at the end of the German sentence. An of-
fline (or “batch”) translation system waits until
the end of the sentence before translating any-
thing. While this is a reasonable approach, it
has obvious limitations. Real-time, interactive
scenarios—such as online multilingual video
conferences or diplomatic meetings—require
comprehensible partial interpretations before
a sentence ends. Thus, a significant goal in
interpretation is to make the translation as
expeditious as possible.
We present three components for an sov-to-
svo simultaneous MT system: a reinforcement
learning framework that uses predictions to
create expeditious translations (Section 2), a
system to predict how a sentence will end (e.g.,
predicting the main verb; Section 4), and a met-
ric that balances quality and expeditiousness
(Section 3). We combine these components in
a framework that learns when to begin trans-
lating sections of a sentence (Section 5).
Section 6 combines this framework with a
</bodyText>
<page confidence="0.944441">
1342
</page>
<note confidence="0.9446095">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1342–1352,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.682624">
ich bin mit dem Zug nach Ulm gefahren
I am with the train to Ulm traveled
I ( waiting ) traveled by train to Ulm
</figure>
<figureCaption confidence="0.955752">
Figure 1: An example of translating from a
</figureCaption>
<bodyText confidence="0.997355363636364">
verb-final language to English. The verb, in
bold, appears at the end of the sentence, pre-
venting coherent translations until the final
source word is revealed.
translation system that produces simultaneous
translations. We show that our data-driven
system can successfully predict unseen parts
of the sentence, learn when to trust them, and
outperform strong baselines (Section 7).
While some prior research has approached
the problem of simultaneous translation—we re-
view these systems in more detail in Section 8—
no current model learns when to definitively
begin translating chunks of an incomplete sen-
tence. Finally, in Section 9, we discuss the
limitations of our system: it only uses the most
frequent source language verbs, it only applies
to sentences with a single main verb, and it
uses an idealized translation system. However,
these limitations are not insurmountable; we
describe how a more robust system can be as-
sembled from these components.
</bodyText>
<sectionHeader confidence="0.946848" genericHeader="introduction">
2 Decision Process for
Simultaneous Translation
</sectionHeader>
<bodyText confidence="0.999757">
Human interpreters learn strategies for their
profession with experience and practice. As
words in the source language are observed, a
translator—human or machine—must decide
whether and how to translate, while, for cer-
tain language pairs, simultaneously predicting
future words. We would like our system to do
the same. To this end, we model simultaneous
MT as a Markov decision process (MDP) and
use reinforcement learning to effectively com-
bine predicting, waiting, and translating into
a coherent strategy.
</bodyText>
<subsectionHeader confidence="0.685745">
2.1 States: What is, what is to come
</subsectionHeader>
<bodyText confidence="0.9998892">
The state st represents the current view of
the world given that we have seen t words of
a source language sentence.1 The state con-
tains information both about what is known
and what is predicted based on what is known.
</bodyText>
<footnote confidence="0.635134">
1We use t to evoke a discrete version of time. We
only allow actions after observing a complete source
word.
</footnote>
<bodyText confidence="0.999857615384615">
To compare the system to a human transla-
tor in a decision-making process, the state is
akin to the translator’s cognitive state. At any
given time, we have knowledge (observations)
and beliefs (predictions) with varying degrees
of certainty: that is, the state contains the re-
vealed words x1:t of a sentence; the state also
contains predictions about the remainder of
the sentence: we predict the next word in the
sentence and the final verb.
More formally, we have a prediction at time
t of the next source language word that will
appear, nt+1, and for the final verb, v(t). For
</bodyText>
<equation confidence="0.582176">
(t)
</equation>
<bodyText confidence="0.999965142857143">
example, given the partial observation “ich
bin mit dem”, the state might contain a pre-
diction that the next word, n(t)
t+1, will be “Zug”
and that the final verb v(t) will be “gefahren”.
We discuss the mechanics of next-word and
verb prediction further in Section 4; for now,
consider these black boxes which, after observ-
ing every new source word xt, make predictions
of future words in the source language. This
representation of the state allows for a richer set
of actions, described below, permitting simul-
taneous translations that outpace the source
language input2 by predicting the future.
</bodyText>
<subsectionHeader confidence="0.989778">
2.2 Actions: What our system can do
</subsectionHeader>
<bodyText confidence="0.996353714285714">
Given observed and hypothesized input, our
simultaneous translation system must decide
when to translate them. This is expressed
in the form of four actions: our system can
commit to a partial translation, predict the
next word and use it to update the transla-
tion, predict the verb and use it to update the
translation, or wait for more words.
We discuss each of these actions in turn be-
fore describing how they come together to in-
crementally translate an entire sentence:
Wait Waiting is the simplest action. It pro-
duces no output and allows the system to re-
ceive more input, biding its time, so that when
it does choose to translate, the translation is
based on more information.
Commit Committing produces translation
output: given the observed source sentence,
produce the best translation possible.
2Throughout, “input” refers to source language in-
put, and “output” refers to target language translation.
</bodyText>
<page confidence="0.991701">
1343
</page>
<figureCaption confidence="0.9818048">
Figure 2: A simultaneous translation from source (German) to target (English). The agent
chooses to wait until after (3). At this point, it is sufficiently confident to predict the final verb
of the sentence (4). Given this additional information, it can now begin translating the sentence
into English, constraining future translations (5). As the rest of the sentence is revealed, the
system can translate the remainder of the sentence.
</figureCaption>
<figure confidence="0.988696833333334">
state
STOP
Observation
6. Mit dem Zug bin ich
nach Ulm gefahren.
Commit
Observation
1. Mit dem Zug
S
Output: I traveled
by train
to Ulm.
with the
train
by train
Wait
Output: I traveled
by train
to Ulm
S
Commit
Observation
2. Mit dem Zug bin
ich
I am
5. Mit dem Zug bin ich
nach Ulm
... gefahren ...
Observation (prediction)
Fixed
output
with the
train
by train
Wait
to Ulm
Observation
3. Mit dem Zug bin
ich nach
Wait
S
I traveled
Predict
Output: I traveled
by train
with the
train
by train
4. Mit dem Zug bin
ich nach ... gefahren ...
Observation (prediction)
Commit
to
Next Word The next word action takes
</figure>
<bodyText confidence="0.973006210526316">
a prediction of the next source word and pro-
duces an updated translation based on that
prediction, i.e., appending the predicted word
to the source sentence and translating the new
sentence.
Verb Our system can also predict the source
sentence’s final verb (the last word in the sen-
tence). When our system takes the verb ac-
tion, it uses its verb prediction to update the
translation using the prediction, by placing it
at the end of the source sentence.
We can recreate a traditional batch trans-
lation system (interpreted temporally) by a
sequence of wait actions until all input is ob-
served, followed by a commit to the complete
translation. Our system can commit to par-
tial translations if it is confident, but producing
a good translation early in the sentence often
depends on missing information.
</bodyText>
<subsectionHeader confidence="0.978153">
2.3 Translation Process
</subsectionHeader>
<bodyText confidence="0.99998465">
Having described the state, its components,
and the possible actions at a state, we present
the process in its entirety. In Figure 2, after
each German word is received, the system ar-
rives at a new state, which consists of the source
input, target translation so far, and predictions
of the unseen words. The translation system
must then take an action given information
about the current state. The action will result
in receiving and translating more source words,
transitioning the system to the next state. In
the example, for the first few source-language
words, the translator lacks the confidence to
produce any output due to insufficient informa-
tion at the state. However, after State 3, the
state shows high confidence in the predicted
verb “gefahren”. Combined with the German
input it has observed, the system is sufficiently
confident to act on that prediction to produce
English translation.
</bodyText>
<subsectionHeader confidence="0.997199">
2.4 Consensus Translations
</subsectionHeader>
<bodyText confidence="0.9998462">
Three straightforward actions—commit, next
word, and verb—all produce translations.
These rely black box access to a translation
(discussed in detail in Section 6): that is, given
a source language sentence fragment, the trans-
lation system produces a target language sen-
tence fragment.
Because these actions can happen more than
once in a sentence, we must form a single con-
sensus translation from all of the translations
that we might have seen. If we have only one
translation or if translations are identical, form-
ing the consensus translation is trivial. But
how should we resolve conflicting translations?
Any time our system chooses an action that
</bodyText>
<page confidence="0.970761">
1344
</page>
<bodyText confidence="0.999980647058824">
produces output, the observed input (plus extra
predictions in the case of next-word or verb),
is passed into the translation system. That
system then produces a complete translation
of its input fragment.
Any new words—i.e., words whose target
index is greater than the length of any previ-
ous translation—are appended to the previous
translation.3 Table 1 shows an example of
forming these consensus translations.
Now that we have defined how states evolve
based on our system’s actions, we need to know
how to select which actions to take. Eventu-
ally, we will formalize this as a learned policy
(Section 5) that maps from states to actions.
First, however, we need to define a reward that
measures how “good” an action is.
</bodyText>
<sectionHeader confidence="0.5553225" genericHeader="method">
3 Objective: What is a good
simultaneous translation?
</sectionHeader>
<bodyText confidence="0.99435465060241">
Good simultaneous translations must optimize
two objectives that are often at odds, i.e., pro-
ducing translations that are, in the end, accu-
rate, and producing them in pieces that are
presented expeditiously. While there are exist-
ing automated metrics for assessing translation
quality (Papineni et al., 2002; Banerjee and
Lavie, 2005; Snover et al., 2006), these must
be modified to find the necessary compromise
between translation quality and expeditious-
ness. That is, a good metric for simultaneous
translation must achieve a balance between
translating chunks early and translating accu-
rately. All else being equal, maximizing either
goal in isolation is trivial: for accurate transla-
tions, use a batch system and wait until the
sentence is complete, translating it all at once;
for a maximally expeditious translation, cre-
ate monotone translations, translating each
word as it appears, as in Tillmann et al. (1997)
and Pytlik and Yarowsky (2006). The former
is not simultaneous at all; the latter is mere
word-for-word replacement and results in awk-
ward, often unintelligible translations of distant
language pairs.
Once we have predictions, we have an ex-
panded array of possibilities, however. On one
extreme, we can imagine a psychic translator—
3Using constrained decoding to enforce consistent
translation prefixes would complicate our method but
is an appealing extension.
one that can completely translate an imagined
sentence after one word is uttered—as an un-
obtainable system. On the other extreme is a
standard batch translator, which waits until
it has access to the utterer’s complete sentence
before translating anything.
Again, we argue that a system can improve
on this by predicting unseen parts of the sen-
tence to find a better tradeoff between these
conflicting goals. However, to evaluate and op-
timize such a system, we must measure where
a system falls on the continuum of accuracy
versus expeditiousness.
Consider partial translations in a two-
dimensional space, with time (quantized by
the number of source words seen) increasing
from left to right on the x axis and the BLEU
score (including brevity penalty against the
reference length) on the y axis. At each point
in time, the system may add to the consensus
translation, changing the precision (Figure 3).
Like an Roc curve, a good system will be high
and to the left, optimizing the area under the
curve: the ideal system would produce points
as high as possible immediately. A translation
which is, in the end, accurate, but which is less
expeditious, would accrue its score more slowly
but outperform a similarly expeditious system
which nevertheless translates poorly.
An idealized psychic system achieves this,
claiming all of the area under the curve, as it
would have a perfect translation instantly, hav-
ing no need of even waiting for future input.4
A batch system has only a narrow (but tall)
sliver to the right, since it translates nothing
until all of the words are observed.
Formally, let Q be the score function for a
partial translation, x the sequentially revealed
source words x1, x2, ... , xT from time step 1 to
T, and y the partial translations y1, y2,.. . , yT,
where T is the length of the source language
input. Each incremental translation yt has a
BLEU-n score with respect to a reference r. We
apply the usual BLEU brevity penalty to all the
incremental translations (initially empty) to
4One could reasonably argue that this is not ideal:
a fluid conversation requires the prosody and timing
between source and target to match exactly. Thus, a
psychic system would provide too much information
too quickly, making information exchange unnatural.
However, we take the information-centric approach:
more information faster is better.
</bodyText>
<page confidence="0.974883">
1345
</page>
<table confidence="0.845408266666667">
Consensus
He1
He1 wase designed3
He1 wase designed3
He1 wase designed3
yesterday¢
Pos Input Intermediate
1
2 Er He1
3 Er wurde It1 wase designed3
gestaltet
4 It1 wase designed3
5 Er wurde It1 wase renovated3
gestern yesterday¢
renoviert
</table>
<tableCaption confidence="0.994158">
Table 1: How intermediate translations are combined into a consensus translation. Incorrect
</tableCaption>
<bodyText confidence="0.878484">
translations (e.g., “he” for an inanimate object in position 3) and incorrect predictions (e.g.,
incorrectly predicting the verb gestaltet in position 5) are kept in the consensus translation.
When no translation is made, the consensus translation remains static.
</bodyText>
<subsectionHeader confidence="0.964853">
Source Sentence
</subsectionHeader>
<figureCaption confidence="0.558452">
Figure 3: Comparison of LBLEU (the area under
</figureCaption>
<bodyText confidence="0.991940857142857">
the curve given by Equation 1) for an impossi-
ble psychic system, a traditional batch system,
a monotone (German word order) system, and
our prediction-based system. By correctly pre-
dicting the verb “gegangen” (to go), we achieve
a better overall translation more quickly.
obtain latency-bleu (LBLEU),
</bodyText>
<equation confidence="0.9881668">
�
Q(x, y) = 1
T
t
+ T · BLEU(yT, r)
</equation>
<bodyText confidence="0.999626076923077">
The LBLEU score is a word-by-word inte-
gral across the input source sentence. As each
source word is observed, the system receives a
reward based on the BLEU score of the partial
translation. LBLEU, then, represents the sum of
these T rewards at each point in the sentence.
The score of a simultaneous translation is the
sum of the scores of all individual segments
that contribute to the overall translation.
We multiply the final BLEU score by T to en-
sure good final translations in learned systems
to compensate for the implicit bias toward low
latency.5
</bodyText>
<sectionHeader confidence="0.997506" genericHeader="method">
4 Predicting Verbs and Next
Words
</sectionHeader>
<bodyText confidence="0.999737535714286">
The next and verb actions depend on predic-
tions of the sentence’s next word and final verb;
this section describes our process for predict-
ing verbs and next words given a partial source
language sentence.
The prediction of the next word in the source
language sentence is modeled with a left-to-
right language model. This is (naively) anal-
ogous to how a human translator might use
his own “language model” to guess upcoming
words to gain some speed by completing, for
example, collocations before they are uttered.
We use a simple bigram language model for
next-word prediction. We use Heafield et al.
(2013).
For verb prediction, we use a generative
model that combines the prior probability of
a particular verb v, p(v), with the likelihood
of the source context at time t given that
verb (namely, p(x1:t  |v)), as estimated by a
smoothed Kneser-Ney language model (Kneser
and Ney, 1995). We use Pauls and Klein
(2011). The prior probability p(v) is estimated
by simple relative frequency estimation. The
context, x1:t, consists of all words observed.
We model p(x1:t  |v) with verb-specific n-gram
language models. The predicted verb vet) at
time t is then:
</bodyText>
<equation confidence="0.983697">
p(xi  |v,xi−n+1:i−1) (2)
</equation>
<footnote confidence="0.964205">
5One could replace T with a parameter, β, to bias
towards different kinds of simultaneous translations. As
β → ∞, we recover batch translation.
</footnote>
<figure confidence="0.98874952">
Er ist zum Laden gegangen
Psychic
β
He went to
the store
Monotone
He
He to the
He to the store
He to the
store went
Batch
He went
to the
store
Policy He
Prediction
He went to
He went He went
to the the store
BLEU(yt, r) (1)
arg max p(v)
v
t
i=1
</figure>
<page confidence="0.973647">
1346
</page>
<bodyText confidence="0.999911666666667">
where xi_n+1:i_1 is the n −1-gram context. To
narrow the search space, we consider only the
100 most frequent final verbs, where a “final
verb” is defined as the sentence-final sequence
of verbs and particles as detected by a German
part-of-speech tagger (Toutanova et al., 2003).6
</bodyText>
<sectionHeader confidence="0.806374" genericHeader="method">
5 Learning a Policy
</sectionHeader>
<bodyText confidence="0.999971210526316">
We have a framework (states and actions) for
simultaneous machine translation and a metric
for assessing simultaneous translations. We
now describe the use of reinforcement learning
to learn a policy, a mapping from states to
actions, to maximize LBLEU reward.
We use imitation learning (Abbeel and Ng,
2004; Syed et al., 2008): given an optimal se-
quence of actions, learn a generalized policy
that maps states to actions. This can be viewed
as a cost-sensitive classification (Langford and
Zadrozny, 2005): a state is represented as a fea-
ture vector, the loss corresponds to the quality
of the action, and the output of the classifier is
the action that should be taken in that state.
In this section, we explain each of these com-
ponents: generating an optimal policy, repre-
senting states through features, and learning a
policy that can generalize to new sentences.
</bodyText>
<subsectionHeader confidence="0.932615">
5.1 Optimal Policies
</subsectionHeader>
<bodyText confidence="0.9877558">
Because we will eventually learn policies via
a classifier, we must provide training exam-
ples to our classifier. These training exam-
ples come from an oracle policy 7r* that
demonstrates the optimal sequence—i.e., with
maximal LBLEU score—of actions for each se-
quence. Using dynamic programming, we can
determine such actions for a fixed translation
model.7 From this oracle policy, we generate
training examples for a supervised classifier.
State st is represented as a tuple of the ob-
served words x1:t, predicted verb v(t), and the
predicted word n(t)
t+1. We represent the state to
a classifier as a feature vector φ(x1:t, n(t)
</bodyText>
<footnote confidence="0.934185375">
t+1, v(t)).
6This has the obvious disadvantage of ignoring mor-
phology and occasionally creating duplicates of common
verbs that have may be associated with multiple parti-
cles; nevertheless, it provides a straightforward verb to
predict.
7This is possible for the limited class of consensus
translation schemes discussed in Section 2.4.
</footnote>
<subsectionHeader confidence="0.968401">
5.2 Feature Representation
</subsectionHeader>
<bodyText confidence="0.999978230769231">
We want a feature representation that will al-
low a classifier to generalize beyond the specific
examples on which it is trained. We use sev-
eral general classes of features: features that
describe the input, features that describe the
possible translations, and features that describe
the quality of the predictions.
Input We include both a bag of words rep-
resentation of the input sentence as well as
the most recent word and bigram to model
word-specific effects. We also use a feature
that encodes the length of the source sentence.
Prediction We include the identity of the
predicted verb and next word as well as their re-
spective probabilities under the language mod-
els that generate the predictions. If the model
is confident in the prediction, the classifier can
learn to more so trust the predictions.
Translation In addition to the state, we in-
clude features derived from the possible actions
the system might take. This includes a bag of
words representation of the target sentence, the
score of the translation (decreasing the score is
undesirable), the score of the current consen-
sus translation, and the difference between the
current and potential translation scores.
</bodyText>
<subsectionHeader confidence="0.990339">
5.3 Policy Learning
</subsectionHeader>
<bodyText confidence="0.999994846153846">
Our goal is to learn a classifier that can accu-
rately mimic the oracle’s choices on previously
unseen data. However, at test time, when we
run the learned policy classifier, the learned
policy’s state distribution may deviate from
the optimal policy’s state distribution due to
imperfect imitation, arriving in states not on
the oracle’s path. To address this, we use
SEARN (Daum´e III et al., 2009), an iterative
imitation learning algorithm. We learn from
the optimal policy in the first iteration, as in
standard supervised learning; in the following
iterations, we run an interpolated policy
</bodyText>
<equation confidence="0.998987">
7rk+1 = c7rk + (1 − c)7r*, (3)
</equation>
<bodyText confidence="0.9996874">
with k as the iteration number and c the mixing
probability. We collect examples by asking
the policy to label states on its path. The
interpolated policy will execute the optimal
action with probability 1 − c and the learned
</bodyText>
<page confidence="0.987538">
1347
</page>
<bodyText confidence="0.999502083333333">
policy’s action with probability c. In the first
iteration, we have π0 = π*.
Mixing in the learned policy allows the
learned policy to slowly change from the oracle
policy. As it trains on these no-longer-perfect
state trajectories, the state distribution at test
time will be more consistent with the states
used in training.
sEARN learns the policy by training a cost-
sensitive classifier. Besides providing the opti-
mal action, the oracle must also assign a cost
to an action
</bodyText>
<equation confidence="0.981894">
C(at, x) = Q(x, π*(xt)) — Q(x, at(xt)), (4)
</equation>
<bodyText confidence="0.999923666666667">
where at(xt) represents the translation outcome
of taking action at. The cost is the regret of
not taking the optimal action.
</bodyText>
<sectionHeader confidence="0.98776" genericHeader="method">
6 Translation System
</sectionHeader>
<bodyText confidence="0.99991628125">
The focus of this work is to show that given an
effective batch translation system and predic-
tions, we can learn a policy that will turn this
into a simultaneous translation system. Thus,
to separate translation errors from policy er-
rors, we perform experiments with a nearly
optimal translation system we call an omni-
scient translator.
More realistic translation systems will nat-
urally lower the objective function, often in
ways that make it difficult to show that we can
effectively predict the verbs in verb-final source
languages. For instance, German to English
translation systems often drop the verb; thus,
predicting a verb that will be ignored by the
translation system will not be effective.
The omniscient translator translates a source
sentence correctly once it has been fed the ap-
propriate source words as input. There are
two edge cases: empty input yields an empty
output, while a complete, correct source sen-
tence returns the correct, complete translation.
Intermediate cases—where the input is either
incomplete or incorrect—require using an align-
ment. The omniscient translator assumes as
input a reference translation r, a partial source
language input x1:t and a corresponding partial
output y. In addition, the omniscient transla-
tor assumes access to an alignment between r
and x. In practice, we use the xMM aligner (Vo-
gel et al., 1996; Och and Ney, 2003).
We first consider incomplete but correct in-
puts. Let y = τ(x1:t) be the translator’s output
given a partial source input x1:t with transla-
tion y. Then, τ(x1:t) produces all target words
yj if there is a source word xi in the input
aligned to those words—i.e., (i, j) E ax,y—and
all preceding target words can be translated.
(That translations must be contiguous is a nat-
ural requirement for human recipients of trans-
lations). In the case where yj is unaligned, the
closest aligned target word to yj that has a
corresponding alignment entry is aligned to xi;
then, if xi is present in the input, yj appears in
the output. Thus, our omniscient translation
system will always produce the correct output
given the correct input.
However, our learned policy can make wrong
predictions, which can produce partial trans-
lations y that do not match the reference.
In this event, an incorrect source word ˜xi
produces incorrect target words ˜yj, for all
j: (i, j) E ax,y. These ˜yj are sampled from
the IBM Model 1 lexical probability table mul-
tiplied by the source language model ˜yj �
Mult(θ˜xi)pLM(˜x).8 Thus, even if we predict
the correct verb using a next word action, it
will be in the wrong position and thus gener-
ate a translation from the lexical probabilities.
Since translations based on Model 1 probabil-
ities are generally inaccurate, the omniscient
translator will do very well when given correct
input but will produce very poor translations
otherwise.
</bodyText>
<sectionHeader confidence="0.998814" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999906625">
In this section, we describe our experimental
framework and results from our experiments.
From aligned data, we derive an omniscient
translator. We use monolingual data in the
source language to train the verb predictor and
the next word predictor. From these features,
we compute an optimal policy from which we
train a learned policy.
</bodyText>
<subsectionHeader confidence="0.996689">
7.1 Data sets
</subsectionHeader>
<bodyText confidence="0.99993325">
For translation model and policy training, we
use data from the German-English Parallel “de-
news” corpus of radio broadcast news (Koehn,
2000), which we lower-cased and stripped of
</bodyText>
<footnote confidence="0.908768">
8If a policy chooses an incorrect unaligned word, it
has no effect on the output. Alignments are position-
specific, so “wrong” refers to position and type.
</footnote>
<page confidence="0.9936">
1348
</page>
<bodyText confidence="0.9999655">
punctuation. A total of 48,601 sentence pairs
are randomly selected for building our system.
Of these, we use 70% (34, 528 pairs) for training
word alignments.
For training the translation policy, we re-
strict ourselves to sentences that end with one
of the 100 most frequent verbs (see Section 4).
This results in a data set of 4401 training sen-
tences and 1832 test sentences from the de-news
data. We did this to narrow the search space
(from thousands of possible, but mostly very
infrequent, verbs).
We used 1 million words of news text from
the Leipzig Wortschatz (Quasthoff et al., 2006)
German corpus to train 5-gram language mod-
els to predict a verb from the 100 most frequent
verbs.
For next-word prediction, we use the 18,345
most frequent German bigrams from the train-
ing set to provide a set of candidates in a lan-
guage model trained on the same set. We use
frequent bigrams to reduce the computational
cost of finding the completion probability of
the next word.
</bodyText>
<subsectionHeader confidence="0.997385">
7.2 Training Policies
</subsectionHeader>
<bodyText confidence="0.9999904">
In each iteration of SEARN, we learn a
multi-class classifier to implement the pol-
icy. The specific learning algorithm we use
is AROw (Crammer et al., 2013). In the com-
plete version of SEARN, the cost of each action
is calculated as the highest expected reward
starting at the current state minus the actual
roll-out reward. However, computing the full
roll-out reward is computationally very expen-
sive. We thus use a surrogate binary cost: if
the predicted action is the same as the opti-
mal action, the cost is 0; otherwise, the cost
is 1. We then run SEARN for five iterations.
Results on the development data indicate that
continuing for more iterations yields no benefit.
</bodyText>
<subsectionHeader confidence="0.991793">
7.3 Policy Rewards on Test Set
</subsectionHeader>
<bodyText confidence="0.999914444444444">
In Figure 4, we show performance of the opti-
mal policy vis-`a-vis the learned policy, as well
as the two baseline policies: the batch policy
and the monotone policy. The x-axis is the
percentage of the source sentence seen by the
model, and the y-axis is a smoothed average of
the reward as a function of the percentage of
the sentence revealed. The monotone policy’s
performance is close to the optimal policy for
</bodyText>
<figure confidence="0.657451">
COMMIT WAIT NEXTWORD VERB
Action
</figure>
<figureCaption confidence="0.974072">
Figure 5: Histogram of actions taken by the
policies.
</figureCaption>
<bodyText confidence="0.999069916666667">
the first half of the sentence, as German and
English have similar word order, though they
diverge toward the end. Our learned policy
outperforms the monotone policy toward the
end and of course outperforms the batch policy
throughout the sentence.
Figure 5 shows counts of actions taken by
each policy. The batch policy always commits
at the end. The monotone policy commits at
each position. Our learned policy has an ac-
tion distribution similar to that of the optimal
policy, but is slightly more cautious.
</bodyText>
<subsectionHeader confidence="0.982745">
7.4 What Policies Do
</subsectionHeader>
<bodyText confidence="0.998567666666667">
Figure 6 shows a policy that, predicting incor-
rectly, still produces sensible output. The pol-
icy correctly intuits that the person discussed
</bodyText>
<figure confidence="0.947546333333333">
0.00 0.25 0.50 0.75 1.00
% of Sentence
● Batch Monotone Optimal Searn
</figure>
<figureCaption confidence="0.995031666666667">
Figure 4: The final reward of policies on Ger-
man data. Our policy outperforms all baselines
by the end of the sentence.
</figureCaption>
<figure confidence="0.988326564102564">
1.25
1.00
Smoothed Average
●
0.75
0.50
0.25
●●●●●●●●●● ● ● ● ● ●
●
●
●
Policy Actions
10000
7500
Batch
5000
2500
0
10000
Monotone
7500
Action Count
5000
2500
0
10000
Optimal
7500
5000
2500
0
10000
7500
Searn
5000
2500
0
1349
INPUT OUTPUT
</figure>
<figureCaption confidence="0.98519525">
Figure 6: An imperfect execution of a learned
policy. Despite choosing the wrong verb
“gezeigt” (showed) instead of “vorgestellt” (pre-
sented), the translation retains the meaning.
</figureCaption>
<bodyText confidence="0.9986816">
is Angela Merkel, who was the environmen-
tal minister at the time, but the policy uses
an incorrectly predicted verb. Because of our
poor translation model (Section 6), it renders
this word as “shown”, which is a poor transla-
tion. However, it is still comprehensible, and
the overall policy is similar to what a human
would do: intuit the subject of the sentence
from early clues and use a more general verb
to stand in for a more specific one.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999973016393443">
Just as MT was revolutionized by statistical
learning, we suspect that simultaneous MT will
similarly benefit from this paradigm, both from
a systematic system for simultaneous transla-
tion and from a framework for learning how to
incorporate predictions.
Simultaneous translation has been
dominated by rule and parse-based ap-
proaches (Mima et al., 1998a; Ryu et al., 2006).
In contrast, although Verbmobil (Wahlster,
2000) performs incremental translation using a
statistical MT module, its incremental decision-
making module is rule-based. Other recent
approaches in speech-based systems focus on
waiting until a pause to translate (Sakamoto
et al., 2013) or using word alignments (Ryu
et al., 2012) between languages to determine
optimal translation units.
Unlike our work, which focuses on predic-
tion and learning, previous strategies for deal-
ing with sov-to-svo translation use rule-based
methods (Mima et al., 1998b) (for instance,
passivization) to buy time for the translator to
hear more information in a spoken context—or
use phrase table and reordering probabilities to
decide where to translate with less delay (Fu-
jita et al., 2013). Oda et al. (2014) is the
most similar to our work on the translation
side. They frame word segmentation as an
optimization problem, using a greedy search
and dynamic programming to find segmenta-
tion strategies that maximize an evaluation
measure. However, unlike our work, the direc-
tion of translation was from from svo to svo,
obviating the need for verb prediction. Simul-
taneous translation is more straightforward for
languages with compatible word orders, such
as English and Spanish (F¨ugen, 2008).
To our knowledge, the only attempt to
specifically predict verbs or any late-occurring
terms (Matsubara et al., 2000) uses pattern
matching on what would today be considered
a small data set to predict English verbs for
Japanese to English simultaneous MT.
Incorporating verb predictions into the trans-
lation process is a significant component of
our framework, though n-gram models strongly
prefer highly frequent verbs. Verb prediction
might be improved by applying the insights
from psycholinguistics. Ferreira (2000) argues
that verb lemmas are required early in sentence
production—prior to the first noun phrase
argument—and that multiple possible syntac-
tic hypotheses are maintained in parallel as the
sentence is produced. Schriefers et al. (1998)
argues that, in simple German sentences, non-
initial verbs do not need lemma planning at
all. Momma et al. (2014), investigating these
prior claims, argues that the abstract relation-
ship between the internal arguments and verbs
triggers selective verb planning.
</bodyText>
<sectionHeader confidence="0.975916" genericHeader="conclusions">
9 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999790090909091">
Creating an effective simultaneous translation
system for sov to svo languages requires not
only translating partial sentences, but also ef-
fectively predicting a sentence’s verb. Both
elements of the system require substantial re-
finement before they are usable in a real-world
system.
Replacing our idealized translation system
is the most challenging and most important
next step. Supporting multiple translation hy-
potheses and incremental decoding (Sankaran
</bodyText>
<figure confidence="0.969997555555555">
bundesumweltministerin
Merkel
bundesumweltministerin
merkel hat den entwurf
gezeigt
bundesumweltministerin
merkel hat den entwurf
eines umweltpolitischen
programms vorgestellt
</figure>
<bodyText confidence="0.847208333333333">
federal minister of the
environment angela merkel
federal minister of the
environment angela merkel
shown the draft
federal minister of the
environment angela merkel
shown the draft of an
ecopolitical program
</bodyText>
<page confidence="0.96828">
1350
</page>
<bodyText confidence="0.999968318181818">
et al., 2010) would improve both the efficiency
and effectiveness of our system. Using data
from human translators (Shimizu et al., 2014)
could also add richer strategies for simultane-
ous translation: passive constructions, reorder-
ing, etc.
Verb prediction also can be substantially im-
proved both in its scope in the system and
how we predict verbs. Verb-final languages
also often place verbs at the end of clauses,
and also predicting these verbs would improve
simultaneous translation, enabling its effective
application to a wider range of sentences. In-
stead predicting an exact verb early (which is
very difficult), predicting a semantically close
or a more general verb might yield interpretable
translations.
A natural next step is expanding this work
to other languages, such as Japanese, which not
only has sov word order but also requires tok-
enization and morphological analysis, perhaps
requiring sub-word prediction.
</bodyText>
<sectionHeader confidence="0.996166" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999972727272727">
We thank the anonymous reviewers, as well as
Yusuke Miyao, Naho Orita, Doug Oard, and
Sudha Rao for their insightful comments. This
work was supported by NSF Grant IIS-1320538.
Boyd-Graber is also partially supported by
NSF Grant CCF-1018625. Daum´e III and He
are also partially supported by NSF Grant IIS-
0964681. Any opinions, findings, conclusions,
or recommendations expressed here are those
of the authors and do not necessarily reflect
the view of the sponsor.
</bodyText>
<sectionHeader confidence="0.996556" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998875208955224">
Pieter Abbeel and Andrew Y. Ng. 2004. Appren-
ticeship learning via inverse reinforcement learn-
ing. In Proceedings of the International Confer-
ence of Machine Learning.
Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: An automatic metric for MT evalua-
tion with improved correlation with human judg-
ments. In Proceedings of the Association for
Computational Linguistics. Association for Com-
putational Linguistics.
Koby Crammer, Alex Kulesza, and Mark Dredze.
2013. Adaptive regularization of weight vectors.
Machine Learning, 91(2):155–187.
Hal Daum´e III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Ma-
chine Learning Journal (MLJ).
Fernanda Ferreira. 2000. Syntax in language
production: An approach using tree-adjoining
grammars. Aspects of language production,
pages 291–330.
Christian F¨ugen. 2008. A system for simultane-
ous translation of lectures and speeches. Ph.D.
thesis, KIT-Bibliothek.
Tomoki Fujita, Graham Neubig, Sakriani Sakti,
Tomoki Toda, and Satoshi Nakamura. 2013.
Simple, lexicalized choice of translation timing
for simultaneous speech translation. INTER-
SPEECH.
Francesca Gaiba. 1998. The origins of simultane-
ous interpretation: The Nuremberg Trial. Uni-
versity of Ottawa Press.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable
modified Kneser-Ney language model estima-
tion. In Proceedings of the Association for Com-
putational Linguistics.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for n-gram language model-
ing. In Acoustics, Speech, and Signal Processing,
1995. ICASSP-95., 1995 International Confer-
ence on. IEEE.
Philipp Koehn. 2000. German-english parallel cor-
pus “de-news”.
John Langford and Bianca Zadrozny. 2005. Relat-
ing reinforcement learning performance to clas-
sification performance. In Proceedings of the In-
ternational Conference of Machine Learning.
Shigeki Matsubara, Keiichi Iwashima, Nobuo
Kawaguchi, Katsuhiko Toyama, and Yasuyoshi
Inagaki. 2000. Simultaneous Japanese-English
interpretation based on early predition of En-
glish verb. In Symposium on Natural Language
Processing.
Hideki Mima, Hitoshi Iida, and Osamu Furuse.
1998a. Simultaneous interpretation utilizing
example-based incremental transfer. In Pro-
ceedings of the 17th international conference on
Computational linguistics-Volume 2, pages 855–
861. Association for Computational Linguistics.
Hideki Mima, Hitoshi Iida, and Osamu Furuse.
1998b. Simultaneous interpretation utilizing
example-based incremental transfer. In Proceed-
ings of the Association for Computational Lin-
guistics.
Shota Momma, Robert Slevc, and Colin Phillips.
2014. The timing of verb selection in english
active and passive sentences.
</reference>
<page confidence="0.815853">
1351
</page>
<reference confidence="0.999828931818182">
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical
alignment models. Computational Linguistics,
29(1):19–51.
Yusuke Oda, Graham Neubig, Sakriani Sakti,
Tomoki Toda, and Satoshi Nakamura. 2014. Op-
timizing segmentation strategies for simultane-
ous speech translation. In Proceedings of the As-
sociation for Computational Linguistics, June.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the Association for Computational
Linguistics.
Adam Pauls and Dan Klein. 2011. Faster and
smaller n-gram language models. In Proceed-
ings of the Association for Computational Lin-
guistics.
Brock Pytlik and David Yarowsky. 2006. Machine
translation for languages lacking bitext via mul-
tilingual gloss transduction. In 5th Conference
of the Association for Machine Translation in
the Americas (AMTA), August.
Uwe Quasthoff, Matthias Richter, and Christian
Biemann. 2006. Corpus portal for search in
monolingual corpora. In International Language
Resources and Evaluation, pages 1799–1802.
Siegfried Ramler and Paul Berry. 2009. Nuremberg
and Beyond: The Memoirs of Siegfried Ramler
from 20th Century Europe to Hawai’i. Booklines
Hawaii Limited.
Koichiro Ryu, Shigeki Matsubara, and Yasuyoshi
Inagaki. 2006. Simultaneous english-japanese
spoken language translation based on incremen-
tal dependency parsing and transfer. In Proceed-
ings of the Association for Computational Lin-
guistics.
Koichiro Ryu, Shigeki Matsubara, and Yasuyoshi
Inagaki. 2012. Alignment-based translation
unit for simultaneous japanese-english spoken di-
alogue translation. In Innovations in Intelligent
Machines–2, pages 33–44. Springer.
Akiko Sakamoto, Nayuko Watanabe, Satoshi Ka-
matani, and Kazuo Sumita. 2013. Development
of a simultaneous interpretation system for face-
to-face services and its evaluation experiment in
real situation.
Baskaran Sankaran, Ajeet Grewal, and Anoop
Sarkar. 2010. Incremental decoding for phrase-
based statistical machine translation. In Pro-
ceedings of the Joint Fifth Workshop on Statis-
tical Machine Translation.
H Schriefers, E Teruel, and RM Meinshausen.
1998. Producing simple sentences: Results from
picture–word interference experiments. Journal
of Memory and Language, 39(4):609–632.
Hiroaki Shimizu, Graham Neubig, Sakriani Sakti,
Tomoki Toda, and Satoshi Nakamura. 2014.
Collection of a simultaneous translation corpus
for comparative analysis. In International Lan-
guage Resources and Evaluation.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted hu-
man annotation. In In Proceedings of Associa-
tion for Machine Translation in the Americas.
Umar Syed, Michael Bowling, and Robert E.
Schapire. 2008. Apprenticeship learning using
linear programming. In Proceedings of the Inter-
national Conference of Machine Learning.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
and Alex Zubiaga. 1997. A dp-based search us-
ing monotone alignments in statistical transla-
tion. In Proceedings of the Association for Com-
putational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D
Manning, and Yoram Singer. 2003. Feature-rich
part-of-speech tagging with a cyclic dependency
network. In Conference of the North American
Chapter of the Association for Computational
Linguistics, pages 173–180.
Stephan Vogel, Hermann Ney, and Christoph Till-
mann. 1996. HMM-based word alignment in
statistical translation. In Proceedings of the 16th
International Conference on Computational Lin-
guistics (COLING).
Wolfgang Wahlster. 2000. Verbmobil: foundations
of speech-to-speech translation. Springer.
</reference>
<page confidence="0.994357">
1352
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.617011">
<title confidence="0.99828">Don’t Until the Final Verb Reinforcement Learning for Simultaneous Machine Translation</title>
<author confidence="0.999953">Alvin C Grissom</author>
<affiliation confidence="0.873140666666667">Computer University of Boulder,</affiliation>
<email confidence="0.996843">Jordan.Boyd.Graber@colorado.edu</email>
<abstract confidence="0.999904571428572">We introduce a reinforcement learningbased approach to simultaneous machine translation—producing a translation while receiving input words— between languages with drastically different word orders: from verb-final languages (e.g., German) to verb-medial languages (English). In traditional machine translation, a translator must “wait” for source material to appear before translation begins. We remove this bottleneck by predicting the final verb in advance. We use reinforcement learning to learn when to trust predictions about unseen, future portions of the sentence. We also introduce an evaluation metric to measure expeditiousness and quality. We show that our new translation model outperforms batch and monotone translation strategies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Pieter Abbeel</author>
<author>Andrew Y Ng</author>
</authors>
<title>Apprenticeship learning via inverse reinforcement learning.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference of Machine Learning.</booktitle>
<contexts>
<context position="19569" citStr="Abbeel and Ng, 2004" startWordPosition="3167" endWordPosition="3170"> i=1 1346 where xi_n+1:i_1 is the n −1-gram context. To narrow the search space, we consider only the 100 most frequent final verbs, where a “final verb” is defined as the sentence-final sequence of verbs and particles as detected by a German part-of-speech tagger (Toutanova et al., 2003).6 5 Learning a Policy We have a framework (states and actions) for simultaneous machine translation and a metric for assessing simultaneous translations. We now describe the use of reinforcement learning to learn a policy, a mapping from states to actions, to maximize LBLEU reward. We use imitation learning (Abbeel and Ng, 2004; Syed et al., 2008): given an optimal sequence of actions, learn a generalized policy that maps states to actions. This can be viewed as a cost-sensitive classification (Langford and Zadrozny, 2005): a state is represented as a feature vector, the loss corresponds to the quality of the action, and the output of the classifier is the action that should be taken in that state. In this section, we explain each of these components: generating an optimal policy, representing states through features, and learning a policy that can generalize to new sentences. 5.1 Optimal Policies Because we will ev</context>
</contexts>
<marker>Abbeel, Ng, 2004</marker>
<rawString>Pieter Abbeel and Andrew Y. Ng. 2004. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="12389" citStr="Banerjee and Lavie, 2005" startWordPosition="1980" endWordPosition="1983"> we need to know how to select which actions to take. Eventually, we will formalize this as a learned policy (Section 5) that maps from states to actions. First, however, we need to define a reward that measures how “good” an action is. 3 Objective: What is a good simultaneous translation? Good simultaneous translations must optimize two objectives that are often at odds, i.e., producing translations that are, in the end, accurate, and producing them in pieces that are presented expeditiously. While there are existing automated metrics for assessing translation quality (Papineni et al., 2002; Banerjee and Lavie, 2005; Snover et al., 2006), these must be modified to find the necessary compromise between translation quality and expeditiousness. That is, a good metric for simultaneous translation must achieve a balance between translating chunks early and translating accurately. All else being equal, maximizing either goal in isolation is trivial: for accurate translations, use a batch system and wait until the sentence is complete, translating it all at once; for a maximally expeditious translation, create monotone translations, translating each word as it appears, as in Tillmann et al. (1997) and Pytlik an</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Alex Kulesza</author>
<author>Mark Dredze</author>
</authors>
<title>Adaptive regularization of weight vectors.</title>
<date>2013</date>
<booktitle>Machine Learning,</booktitle>
<volume>91</volume>
<issue>2</issue>
<contexts>
<context position="28568" citStr="Crammer et al., 2013" startWordPosition="4659" endWordPosition="4662">ws text from the Leipzig Wortschatz (Quasthoff et al., 2006) German corpus to train 5-gram language models to predict a verb from the 100 most frequent verbs. For next-word prediction, we use the 18,345 most frequent German bigrams from the training set to provide a set of candidates in a language model trained on the same set. We use frequent bigrams to reduce the computational cost of finding the completion probability of the next word. 7.2 Training Policies In each iteration of SEARN, we learn a multi-class classifier to implement the policy. The specific learning algorithm we use is AROw (Crammer et al., 2013). In the complete version of SEARN, the cost of each action is calculated as the highest expected reward starting at the current state minus the actual roll-out reward. However, computing the full roll-out reward is computationally very expensive. We thus use a surrogate binary cost: if the predicted action is the same as the optimal action, the cost is 0; otherwise, the cost is 1. We then run SEARN for five iterations. Results on the development data indicate that continuing for more iterations yields no benefit. 7.3 Policy Rewards on Test Set In Figure 4, we show performance of the optimal p</context>
</contexts>
<marker>Crammer, Kulesza, Dredze, 2013</marker>
<rawString>Koby Crammer, Alex Kulesza, and Mark Dredze. 2013. Adaptive regularization of weight vectors. Machine Learning, 91(2):155–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e John Langford</author>
<author>Daniel Marcu</author>
</authors>
<title>Search-based structured prediction.</title>
<date>2009</date>
<journal>Machine Learning Journal (MLJ).</journal>
<marker>Langford, Marcu, 2009</marker>
<rawString>Hal Daum´e III, John Langford, and Daniel Marcu. 2009. Search-based structured prediction. Machine Learning Journal (MLJ).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernanda Ferreira</author>
</authors>
<title>Syntax in language production: An approach using tree-adjoining grammars. Aspects of language production,</title>
<date>2000</date>
<pages>291--330</pages>
<contexts>
<context position="33528" citStr="Ferreira (2000)" startWordPosition="5475" endWordPosition="5476">forward for languages with compatible word orders, such as English and Spanish (F¨ugen, 2008). To our knowledge, the only attempt to specifically predict verbs or any late-occurring terms (Matsubara et al., 2000) uses pattern matching on what would today be considered a small data set to predict English verbs for Japanese to English simultaneous MT. Incorporating verb predictions into the translation process is a significant component of our framework, though n-gram models strongly prefer highly frequent verbs. Verb prediction might be improved by applying the insights from psycholinguistics. Ferreira (2000) argues that verb lemmas are required early in sentence production—prior to the first noun phrase argument—and that multiple possible syntactic hypotheses are maintained in parallel as the sentence is produced. Schriefers et al. (1998) argues that, in simple German sentences, noninitial verbs do not need lemma planning at all. Momma et al. (2014), investigating these prior claims, argues that the abstract relationship between the internal arguments and verbs triggers selective verb planning. 9 Conclusion and Future Work Creating an effective simultaneous translation system for sov to svo langu</context>
</contexts>
<marker>Ferreira, 2000</marker>
<rawString>Fernanda Ferreira. 2000. Syntax in language production: An approach using tree-adjoining grammars. Aspects of language production, pages 291–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian F¨ugen</author>
</authors>
<title>A system for simultaneous translation of lectures and speeches.</title>
<date>2008</date>
<tech>Ph.D. thesis, KIT-Bibliothek.</tech>
<marker>F¨ugen, 2008</marker>
<rawString>Christian F¨ugen. 2008. A system for simultaneous translation of lectures and speeches. Ph.D. thesis, KIT-Bibliothek.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoki Fujita</author>
<author>Graham Neubig</author>
<author>Sakriani Sakti</author>
<author>Tomoki Toda</author>
<author>Satoshi Nakamura</author>
</authors>
<title>Simple, lexicalized choice of translation timing for simultaneous speech translation.</title>
<date>2013</date>
<publisher>INTERSPEECH.</publisher>
<contexts>
<context position="32502" citStr="Fujita et al., 2013" startWordPosition="5315" endWordPosition="5319">le is rule-based. Other recent approaches in speech-based systems focus on waiting until a pause to translate (Sakamoto et al., 2013) or using word alignments (Ryu et al., 2012) between languages to determine optimal translation units. Unlike our work, which focuses on prediction and learning, previous strategies for dealing with sov-to-svo translation use rule-based methods (Mima et al., 1998b) (for instance, passivization) to buy time for the translator to hear more information in a spoken context—or use phrase table and reordering probabilities to decide where to translate with less delay (Fujita et al., 2013). Oda et al. (2014) is the most similar to our work on the translation side. They frame word segmentation as an optimization problem, using a greedy search and dynamic programming to find segmentation strategies that maximize an evaluation measure. However, unlike our work, the direction of translation was from from svo to svo, obviating the need for verb prediction. Simultaneous translation is more straightforward for languages with compatible word orders, such as English and Spanish (F¨ugen, 2008). To our knowledge, the only attempt to specifically predict verbs or any late-occurring terms (</context>
</contexts>
<marker>Fujita, Neubig, Sakti, Toda, Nakamura, 2013</marker>
<rawString>Tomoki Fujita, Graham Neubig, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2013. Simple, lexicalized choice of translation timing for simultaneous speech translation. INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesca Gaiba</author>
</authors>
<title>The origins of simultaneous interpretation: The Nuremberg Trial.</title>
<date>1998</date>
<publisher>University of Ottawa Press.</publisher>
<contexts>
<context position="2004" citStr="Gaiba, 1998" startWordPosition="287" endWordPosition="288">noted uses of human simultaneous interpretation was the Nuremberg trials after the Second World War. Siegfried Ramler (2009), the Austrian-American who organized the translation teams, describes the linguistic predictions He He, John Morgan, and Hal Daum´e III Computer Science and UMIACS University of Maryland College Park, MD {hhe,jjm,hal}@cs.umd.edu and circumlocutions that translators would use to achieve a tradeoff between translation latency and accuracy. The audio recording technology used by those interpreters sowed the seeds of technology-assisted interpretation at the United Nations (Gaiba, 1998). Performing real-time translation is especially difficult when information that comes early in the target language (the language you’re translating to) comes late in the source language (the language you’re translating from). A common example is when translating from a verb-final (sov) language (e.g., German or Japanese) to a verb-medial (svo) language, (e.g., English). In the example in Figure 1, for instance, the main verb of the sentence (in bold) appears at the end of the German sentence. An offline (or “batch”) translation system waits until the end of the sentence before translating any</context>
</contexts>
<marker>Gaiba, 1998</marker>
<rawString>Francesca Gaiba. 1998. The origins of simultaneous interpretation: The Nuremberg Trial. University of Ottawa Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Ivan Pouzyrevsky</author>
<author>Jonathan H Clark</author>
<author>Philipp Koehn</author>
</authors>
<title>Scalable modified Kneser-Ney language model estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="17991" citStr="Heafield et al. (2013)" startWordPosition="2896" endWordPosition="2899">and Next Words The next and verb actions depend on predictions of the sentence’s next word and final verb; this section describes our process for predicting verbs and next words given a partial source language sentence. The prediction of the next word in the source language sentence is modeled with a left-toright language model. This is (naively) analogous to how a human translator might use his own “language model” to guess upcoming words to gain some speed by completing, for example, collocations before they are uttered. We use a simple bigram language model for next-word prediction. We use Heafield et al. (2013). For verb prediction, we use a generative model that combines the prior probability of a particular verb v, p(v), with the likelihood of the source context at time t given that verb (namely, p(x1:t |v)), as estimated by a smoothed Kneser-Ney language model (Kneser and Ney, 1995). We use Pauls and Klein (2011). The prior probability p(v) is estimated by simple relative frequency estimation. The context, x1:t, consists of all words observed. We model p(x1:t |v) with verb-specific n-gram language models. The predicted verb vet) at time t is then: p(xi |v,xi−n+1:i−1) (2) 5One could replace T with</context>
</contexts>
<marker>Heafield, Pouzyrevsky, Clark, Koehn, 2013</marker>
<rawString>Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable modified Kneser-Ney language model estimation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for n-gram language modeling.</title>
<date>1995</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<contexts>
<context position="18271" citStr="Kneser and Ney, 1995" startWordPosition="2943" endWordPosition="2946">ce is modeled with a left-toright language model. This is (naively) analogous to how a human translator might use his own “language model” to guess upcoming words to gain some speed by completing, for example, collocations before they are uttered. We use a simple bigram language model for next-word prediction. We use Heafield et al. (2013). For verb prediction, we use a generative model that combines the prior probability of a particular verb v, p(v), with the likelihood of the source context at time t given that verb (namely, p(x1:t |v)), as estimated by a smoothed Kneser-Ney language model (Kneser and Ney, 1995). We use Pauls and Klein (2011). The prior probability p(v) is estimated by simple relative frequency estimation. The context, x1:t, consists of all words observed. We model p(x1:t |v) with verb-specific n-gram language models. The predicted verb vet) at time t is then: p(xi |v,xi−n+1:i−1) (2) 5One could replace T with a parameter, β, to bias towards different kinds of simultaneous translations. As β → ∞, we recover batch translation. Er ist zum Laden gegangen Psychic β He went to the store Monotone He He to the He to the store He to the store went Batch He went to the store Policy He Predicti</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for n-gram language modeling. In Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<date>2000</date>
<note>German-english parallel corpus “de-news”.</note>
<contexts>
<context position="27216" citStr="Koehn, 2000" startWordPosition="4428" endWordPosition="4429"> translator will do very well when given correct input but will produce very poor translations otherwise. 7 Experiments In this section, we describe our experimental framework and results from our experiments. From aligned data, we derive an omniscient translator. We use monolingual data in the source language to train the verb predictor and the next word predictor. From these features, we compute an optimal policy from which we train a learned policy. 7.1 Data sets For translation model and policy training, we use data from the German-English Parallel “denews” corpus of radio broadcast news (Koehn, 2000), which we lower-cased and stripped of 8If a policy chooses an incorrect unaligned word, it has no effect on the output. Alignments are positionspecific, so “wrong” refers to position and type. 1348 punctuation. A total of 48,601 sentence pairs are randomly selected for building our system. Of these, we use 70% (34, 528 pairs) for training word alignments. For training the translation policy, we restrict ourselves to sentences that end with one of the 100 most frequent verbs (see Section 4). This results in a data set of 4401 training sentences and 1832 test sentences from the de-news data. We</context>
</contexts>
<marker>Koehn, 2000</marker>
<rawString>Philipp Koehn. 2000. German-english parallel corpus “de-news”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Langford</author>
<author>Bianca Zadrozny</author>
</authors>
<title>Relating reinforcement learning performance to classification performance.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference of Machine Learning.</booktitle>
<contexts>
<context position="19768" citStr="Langford and Zadrozny, 2005" startWordPosition="3199" endWordPosition="3202">uence of verbs and particles as detected by a German part-of-speech tagger (Toutanova et al., 2003).6 5 Learning a Policy We have a framework (states and actions) for simultaneous machine translation and a metric for assessing simultaneous translations. We now describe the use of reinforcement learning to learn a policy, a mapping from states to actions, to maximize LBLEU reward. We use imitation learning (Abbeel and Ng, 2004; Syed et al., 2008): given an optimal sequence of actions, learn a generalized policy that maps states to actions. This can be viewed as a cost-sensitive classification (Langford and Zadrozny, 2005): a state is represented as a feature vector, the loss corresponds to the quality of the action, and the output of the classifier is the action that should be taken in that state. In this section, we explain each of these components: generating an optimal policy, representing states through features, and learning a policy that can generalize to new sentences. 5.1 Optimal Policies Because we will eventually learn policies via a classifier, we must provide training examples to our classifier. These training examples come from an oracle policy 7r* that demonstrates the optimal sequence—i.e., with</context>
</contexts>
<marker>Langford, Zadrozny, 2005</marker>
<rawString>John Langford and Bianca Zadrozny. 2005. Relating reinforcement learning performance to classification performance. In Proceedings of the International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shigeki Matsubara</author>
<author>Keiichi Iwashima</author>
<author>Nobuo Kawaguchi</author>
<author>Katsuhiko Toyama</author>
<author>Yasuyoshi Inagaki</author>
</authors>
<title>Simultaneous Japanese-English interpretation based on early predition of English verb. In</title>
<date>2000</date>
<booktitle>Symposium on Natural Language Processing.</booktitle>
<contexts>
<context position="33125" citStr="Matsubara et al., 2000" startWordPosition="5414" endWordPosition="5417">. Oda et al. (2014) is the most similar to our work on the translation side. They frame word segmentation as an optimization problem, using a greedy search and dynamic programming to find segmentation strategies that maximize an evaluation measure. However, unlike our work, the direction of translation was from from svo to svo, obviating the need for verb prediction. Simultaneous translation is more straightforward for languages with compatible word orders, such as English and Spanish (F¨ugen, 2008). To our knowledge, the only attempt to specifically predict verbs or any late-occurring terms (Matsubara et al., 2000) uses pattern matching on what would today be considered a small data set to predict English verbs for Japanese to English simultaneous MT. Incorporating verb predictions into the translation process is a significant component of our framework, though n-gram models strongly prefer highly frequent verbs. Verb prediction might be improved by applying the insights from psycholinguistics. Ferreira (2000) argues that verb lemmas are required early in sentence production—prior to the first noun phrase argument—and that multiple possible syntactic hypotheses are maintained in parallel as the sentence</context>
</contexts>
<marker>Matsubara, Iwashima, Kawaguchi, Toyama, Inagaki, 2000</marker>
<rawString>Shigeki Matsubara, Keiichi Iwashima, Nobuo Kawaguchi, Katsuhiko Toyama, and Yasuyoshi Inagaki. 2000. Simultaneous Japanese-English interpretation based on early predition of English verb. In Symposium on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Mima</author>
<author>Hitoshi Iida</author>
<author>Osamu Furuse</author>
</authors>
<title>Simultaneous interpretation utilizing example-based incremental transfer.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics-Volume 2,</booktitle>
<pages>855--861</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="31712" citStr="Mima et al., 1998" startWordPosition="5197" endWordPosition="5200">hown”, which is a poor translation. However, it is still comprehensible, and the overall policy is similar to what a human would do: intuit the subject of the sentence from early clues and use a more general verb to stand in for a more specific one. 8 Related Work Just as MT was revolutionized by statistical learning, we suspect that simultaneous MT will similarly benefit from this paradigm, both from a systematic system for simultaneous translation and from a framework for learning how to incorporate predictions. Simultaneous translation has been dominated by rule and parse-based approaches (Mima et al., 1998a; Ryu et al., 2006). In contrast, although Verbmobil (Wahlster, 2000) performs incremental translation using a statistical MT module, its incremental decisionmaking module is rule-based. Other recent approaches in speech-based systems focus on waiting until a pause to translate (Sakamoto et al., 2013) or using word alignments (Ryu et al., 2012) between languages to determine optimal translation units. Unlike our work, which focuses on prediction and learning, previous strategies for dealing with sov-to-svo translation use rule-based methods (Mima et al., 1998b) (for instance, passivization) t</context>
</contexts>
<marker>Mima, Iida, Furuse, 1998</marker>
<rawString>Hideki Mima, Hitoshi Iida, and Osamu Furuse. 1998a. Simultaneous interpretation utilizing example-based incremental transfer. In Proceedings of the 17th international conference on Computational linguistics-Volume 2, pages 855– 861. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Mima</author>
<author>Hitoshi Iida</author>
<author>Osamu Furuse</author>
</authors>
<title>Simultaneous interpretation utilizing example-based incremental transfer.</title>
<date>1998</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="31712" citStr="Mima et al., 1998" startWordPosition="5197" endWordPosition="5200">hown”, which is a poor translation. However, it is still comprehensible, and the overall policy is similar to what a human would do: intuit the subject of the sentence from early clues and use a more general verb to stand in for a more specific one. 8 Related Work Just as MT was revolutionized by statistical learning, we suspect that simultaneous MT will similarly benefit from this paradigm, both from a systematic system for simultaneous translation and from a framework for learning how to incorporate predictions. Simultaneous translation has been dominated by rule and parse-based approaches (Mima et al., 1998a; Ryu et al., 2006). In contrast, although Verbmobil (Wahlster, 2000) performs incremental translation using a statistical MT module, its incremental decisionmaking module is rule-based. Other recent approaches in speech-based systems focus on waiting until a pause to translate (Sakamoto et al., 2013) or using word alignments (Ryu et al., 2012) between languages to determine optimal translation units. Unlike our work, which focuses on prediction and learning, previous strategies for dealing with sov-to-svo translation use rule-based methods (Mima et al., 1998b) (for instance, passivization) t</context>
</contexts>
<marker>Mima, Iida, Furuse, 1998</marker>
<rawString>Hideki Mima, Hitoshi Iida, and Osamu Furuse. 1998b. Simultaneous interpretation utilizing example-based incremental transfer. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shota Momma</author>
<author>Robert Slevc</author>
<author>Colin Phillips</author>
</authors>
<title>The timing of verb selection in english active and passive sentences.</title>
<date>2014</date>
<contexts>
<context position="33876" citStr="Momma et al. (2014)" startWordPosition="5528" endWordPosition="5531">ous MT. Incorporating verb predictions into the translation process is a significant component of our framework, though n-gram models strongly prefer highly frequent verbs. Verb prediction might be improved by applying the insights from psycholinguistics. Ferreira (2000) argues that verb lemmas are required early in sentence production—prior to the first noun phrase argument—and that multiple possible syntactic hypotheses are maintained in parallel as the sentence is produced. Schriefers et al. (1998) argues that, in simple German sentences, noninitial verbs do not need lemma planning at all. Momma et al. (2014), investigating these prior claims, argues that the abstract relationship between the internal arguments and verbs triggers selective verb planning. 9 Conclusion and Future Work Creating an effective simultaneous translation system for sov to svo languages requires not only translating partial sentences, but also effectively predicting a sentence’s verb. Both elements of the system require substantial refinement before they are usable in a real-world system. Replacing our idealized translation system is the most challenging and most important next step. Supporting multiple translation hypothes</context>
</contexts>
<marker>Momma, Slevc, Phillips, 2014</marker>
<rawString>Shota Momma, Robert Slevc, and Colin Phillips. 2014. The timing of verb selection in english active and passive sentences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="25250" citStr="Och and Ney, 2003" startWordPosition="4097" endWordPosition="4100">rectly once it has been fed the appropriate source words as input. There are two edge cases: empty input yields an empty output, while a complete, correct source sentence returns the correct, complete translation. Intermediate cases—where the input is either incomplete or incorrect—require using an alignment. The omniscient translator assumes as input a reference translation r, a partial source language input x1:t and a corresponding partial output y. In addition, the omniscient translator assumes access to an alignment between r and x. In practice, we use the xMM aligner (Vogel et al., 1996; Och and Ney, 2003). We first consider incomplete but correct inputs. Let y = τ(x1:t) be the translator’s output given a partial source input x1:t with translation y. Then, τ(x1:t) produces all target words yj if there is a source word xi in the input aligned to those words—i.e., (i, j) E ax,y—and all preceding target words can be translated. (That translations must be contiguous is a natural requirement for human recipients of translations). In the case where yj is unaligned, the closest aligned target word to yj that has a corresponding alignment entry is aligned to xi; then, if xi is present in the input, yj </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Oda</author>
<author>Graham Neubig</author>
<author>Sakriani Sakti</author>
<author>Tomoki Toda</author>
<author>Satoshi Nakamura</author>
</authors>
<title>Optimizing segmentation strategies for simultaneous speech translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="32521" citStr="Oda et al. (2014)" startWordPosition="5320" endWordPosition="5323">r recent approaches in speech-based systems focus on waiting until a pause to translate (Sakamoto et al., 2013) or using word alignments (Ryu et al., 2012) between languages to determine optimal translation units. Unlike our work, which focuses on prediction and learning, previous strategies for dealing with sov-to-svo translation use rule-based methods (Mima et al., 1998b) (for instance, passivization) to buy time for the translator to hear more information in a spoken context—or use phrase table and reordering probabilities to decide where to translate with less delay (Fujita et al., 2013). Oda et al. (2014) is the most similar to our work on the translation side. They frame word segmentation as an optimization problem, using a greedy search and dynamic programming to find segmentation strategies that maximize an evaluation measure. However, unlike our work, the direction of translation was from from svo to svo, obviating the need for verb prediction. Simultaneous translation is more straightforward for languages with compatible word orders, such as English and Spanish (F¨ugen, 2008). To our knowledge, the only attempt to specifically predict verbs or any late-occurring terms (Matsubara et al., 2</context>
</contexts>
<marker>Oda, Neubig, Sakti, Toda, Nakamura, 2014</marker>
<rawString>Yusuke Oda, Graham Neubig, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2014. Optimizing segmentation strategies for simultaneous speech translation. In Proceedings of the Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="12363" citStr="Papineni et al., 2002" startWordPosition="1976" endWordPosition="1979">n our system’s actions, we need to know how to select which actions to take. Eventually, we will formalize this as a learned policy (Section 5) that maps from states to actions. First, however, we need to define a reward that measures how “good” an action is. 3 Objective: What is a good simultaneous translation? Good simultaneous translations must optimize two objectives that are often at odds, i.e., producing translations that are, in the end, accurate, and producing them in pieces that are presented expeditiously. While there are existing automated metrics for assessing translation quality (Papineni et al., 2002; Banerjee and Lavie, 2005; Snover et al., 2006), these must be modified to find the necessary compromise between translation quality and expeditiousness. That is, a good metric for simultaneous translation must achieve a balance between translating chunks early and translating accurately. All else being equal, maximizing either goal in isolation is trivial: for accurate translations, use a batch system and wait until the sentence is complete, translating it all at once; for a maximally expeditious translation, create monotone translations, translating each word as it appears, as in Tillmann e</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Faster and smaller n-gram language models.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="18302" citStr="Pauls and Klein (2011)" startWordPosition="2949" endWordPosition="2952">ght language model. This is (naively) analogous to how a human translator might use his own “language model” to guess upcoming words to gain some speed by completing, for example, collocations before they are uttered. We use a simple bigram language model for next-word prediction. We use Heafield et al. (2013). For verb prediction, we use a generative model that combines the prior probability of a particular verb v, p(v), with the likelihood of the source context at time t given that verb (namely, p(x1:t |v)), as estimated by a smoothed Kneser-Ney language model (Kneser and Ney, 1995). We use Pauls and Klein (2011). The prior probability p(v) is estimated by simple relative frequency estimation. The context, x1:t, consists of all words observed. We model p(x1:t |v) with verb-specific n-gram language models. The predicted verb vet) at time t is then: p(xi |v,xi−n+1:i−1) (2) 5One could replace T with a parameter, β, to bias towards different kinds of simultaneous translations. As β → ∞, we recover batch translation. Er ist zum Laden gegangen Psychic β He went to the store Monotone He He to the He to the store He to the store went Batch He went to the store Policy He Prediction He went to He went He went t</context>
</contexts>
<marker>Pauls, Klein, 2011</marker>
<rawString>Adam Pauls and Dan Klein. 2011. Faster and smaller n-gram language models. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brock Pytlik</author>
<author>David Yarowsky</author>
</authors>
<title>Machine translation for languages lacking bitext via multilingual gloss transduction.</title>
<date>2006</date>
<booktitle>In 5th Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<contexts>
<context position="13006" citStr="Pytlik and Yarowsky (2006)" startWordPosition="2076" endWordPosition="2079">vie, 2005; Snover et al., 2006), these must be modified to find the necessary compromise between translation quality and expeditiousness. That is, a good metric for simultaneous translation must achieve a balance between translating chunks early and translating accurately. All else being equal, maximizing either goal in isolation is trivial: for accurate translations, use a batch system and wait until the sentence is complete, translating it all at once; for a maximally expeditious translation, create monotone translations, translating each word as it appears, as in Tillmann et al. (1997) and Pytlik and Yarowsky (2006). The former is not simultaneous at all; the latter is mere word-for-word replacement and results in awkward, often unintelligible translations of distant language pairs. Once we have predictions, we have an expanded array of possibilities, however. On one extreme, we can imagine a psychic translator— 3Using constrained decoding to enforce consistent translation prefixes would complicate our method but is an appealing extension. one that can completely translate an imagined sentence after one word is uttered—as an unobtainable system. On the other extreme is a standard batch translator, which </context>
</contexts>
<marker>Pytlik, Yarowsky, 2006</marker>
<rawString>Brock Pytlik and David Yarowsky. 2006. Machine translation for languages lacking bitext via multilingual gloss transduction. In 5th Conference of the Association for Machine Translation in the Americas (AMTA), August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uwe Quasthoff</author>
<author>Matthias Richter</author>
<author>Christian Biemann</author>
</authors>
<title>Corpus portal for search in monolingual corpora.</title>
<date>2006</date>
<booktitle>In International Language Resources and Evaluation,</booktitle>
<pages>1799--1802</pages>
<contexts>
<context position="28007" citStr="Quasthoff et al., 2006" startWordPosition="4561" endWordPosition="4564">ers to position and type. 1348 punctuation. A total of 48,601 sentence pairs are randomly selected for building our system. Of these, we use 70% (34, 528 pairs) for training word alignments. For training the translation policy, we restrict ourselves to sentences that end with one of the 100 most frequent verbs (see Section 4). This results in a data set of 4401 training sentences and 1832 test sentences from the de-news data. We did this to narrow the search space (from thousands of possible, but mostly very infrequent, verbs). We used 1 million words of news text from the Leipzig Wortschatz (Quasthoff et al., 2006) German corpus to train 5-gram language models to predict a verb from the 100 most frequent verbs. For next-word prediction, we use the 18,345 most frequent German bigrams from the training set to provide a set of candidates in a language model trained on the same set. We use frequent bigrams to reduce the computational cost of finding the completion probability of the next word. 7.2 Training Policies In each iteration of SEARN, we learn a multi-class classifier to implement the policy. The specific learning algorithm we use is AROw (Crammer et al., 2013). In the complete version of SEARN, the</context>
</contexts>
<marker>Quasthoff, Richter, Biemann, 2006</marker>
<rawString>Uwe Quasthoff, Matthias Richter, and Christian Biemann. 2006. Corpus portal for search in monolingual corpora. In International Language Resources and Evaluation, pages 1799–1802.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siegfried Ramler</author>
<author>Paul Berry</author>
</authors>
<date>2009</date>
<booktitle>Nuremberg and Beyond: The Memoirs of Siegfried Ramler from 20th Century Europe to</booktitle>
<institution>Hawai’i. Booklines Hawaii Limited.</institution>
<marker>Ramler, Berry, 2009</marker>
<rawString>Siegfried Ramler and Paul Berry. 2009. Nuremberg and Beyond: The Memoirs of Siegfried Ramler from 20th Century Europe to Hawai’i. Booklines Hawaii Limited.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koichiro Ryu</author>
<author>Shigeki Matsubara</author>
<author>Yasuyoshi Inagaki</author>
</authors>
<title>Simultaneous english-japanese spoken language translation based on incremental dependency parsing and transfer.</title>
<date>2006</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="31732" citStr="Ryu et al., 2006" startWordPosition="5201" endWordPosition="5204">or translation. However, it is still comprehensible, and the overall policy is similar to what a human would do: intuit the subject of the sentence from early clues and use a more general verb to stand in for a more specific one. 8 Related Work Just as MT was revolutionized by statistical learning, we suspect that simultaneous MT will similarly benefit from this paradigm, both from a systematic system for simultaneous translation and from a framework for learning how to incorporate predictions. Simultaneous translation has been dominated by rule and parse-based approaches (Mima et al., 1998a; Ryu et al., 2006). In contrast, although Verbmobil (Wahlster, 2000) performs incremental translation using a statistical MT module, its incremental decisionmaking module is rule-based. Other recent approaches in speech-based systems focus on waiting until a pause to translate (Sakamoto et al., 2013) or using word alignments (Ryu et al., 2012) between languages to determine optimal translation units. Unlike our work, which focuses on prediction and learning, previous strategies for dealing with sov-to-svo translation use rule-based methods (Mima et al., 1998b) (for instance, passivization) to buy time for the t</context>
</contexts>
<marker>Ryu, Matsubara, Inagaki, 2006</marker>
<rawString>Koichiro Ryu, Shigeki Matsubara, and Yasuyoshi Inagaki. 2006. Simultaneous english-japanese spoken language translation based on incremental dependency parsing and transfer. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koichiro Ryu</author>
<author>Shigeki Matsubara</author>
<author>Yasuyoshi Inagaki</author>
</authors>
<title>Alignment-based translation unit for simultaneous japanese-english spoken dialogue translation.</title>
<date>2012</date>
<booktitle>In Innovations in Intelligent Machines–2,</booktitle>
<pages>33--44</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="32059" citStr="Ryu et al., 2012" startWordPosition="5248" endWordPosition="5251">s MT will similarly benefit from this paradigm, both from a systematic system for simultaneous translation and from a framework for learning how to incorporate predictions. Simultaneous translation has been dominated by rule and parse-based approaches (Mima et al., 1998a; Ryu et al., 2006). In contrast, although Verbmobil (Wahlster, 2000) performs incremental translation using a statistical MT module, its incremental decisionmaking module is rule-based. Other recent approaches in speech-based systems focus on waiting until a pause to translate (Sakamoto et al., 2013) or using word alignments (Ryu et al., 2012) between languages to determine optimal translation units. Unlike our work, which focuses on prediction and learning, previous strategies for dealing with sov-to-svo translation use rule-based methods (Mima et al., 1998b) (for instance, passivization) to buy time for the translator to hear more information in a spoken context—or use phrase table and reordering probabilities to decide where to translate with less delay (Fujita et al., 2013). Oda et al. (2014) is the most similar to our work on the translation side. They frame word segmentation as an optimization problem, using a greedy search a</context>
</contexts>
<marker>Ryu, Matsubara, Inagaki, 2012</marker>
<rawString>Koichiro Ryu, Shigeki Matsubara, and Yasuyoshi Inagaki. 2012. Alignment-based translation unit for simultaneous japanese-english spoken dialogue translation. In Innovations in Intelligent Machines–2, pages 33–44. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akiko Sakamoto</author>
<author>Nayuko Watanabe</author>
<author>Satoshi Kamatani</author>
<author>Kazuo Sumita</author>
</authors>
<title>Development of a simultaneous interpretation system for faceto-face services and its evaluation experiment in real situation.</title>
<date>2013</date>
<contexts>
<context position="32015" citStr="Sakamoto et al., 2013" startWordPosition="5240" endWordPosition="5243">statistical learning, we suspect that simultaneous MT will similarly benefit from this paradigm, both from a systematic system for simultaneous translation and from a framework for learning how to incorporate predictions. Simultaneous translation has been dominated by rule and parse-based approaches (Mima et al., 1998a; Ryu et al., 2006). In contrast, although Verbmobil (Wahlster, 2000) performs incremental translation using a statistical MT module, its incremental decisionmaking module is rule-based. Other recent approaches in speech-based systems focus on waiting until a pause to translate (Sakamoto et al., 2013) or using word alignments (Ryu et al., 2012) between languages to determine optimal translation units. Unlike our work, which focuses on prediction and learning, previous strategies for dealing with sov-to-svo translation use rule-based methods (Mima et al., 1998b) (for instance, passivization) to buy time for the translator to hear more information in a spoken context—or use phrase table and reordering probabilities to decide where to translate with less delay (Fujita et al., 2013). Oda et al. (2014) is the most similar to our work on the translation side. They frame word segmentation as an o</context>
</contexts>
<marker>Sakamoto, Watanabe, Kamatani, Sumita, 2013</marker>
<rawString>Akiko Sakamoto, Nayuko Watanabe, Satoshi Kamatani, and Kazuo Sumita. 2013. Development of a simultaneous interpretation system for faceto-face services and its evaluation experiment in real situation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baskaran Sankaran</author>
<author>Ajeet Grewal</author>
<author>Anoop Sarkar</author>
</authors>
<title>Incremental decoding for phrasebased statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation.</booktitle>
<marker>Sankaran, Grewal, Sarkar, 2010</marker>
<rawString>Baskaran Sankaran, Ajeet Grewal, and Anoop Sarkar. 2010. Incremental decoding for phrasebased statistical machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schriefers</author>
<author>E Teruel</author>
<author>RM Meinshausen</author>
</authors>
<title>Producing simple sentences: Results from picture–word interference experiments.</title>
<date>1998</date>
<journal>Journal of Memory and Language,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="33763" citStr="Schriefers et al. (1998)" startWordPosition="5508" endWordPosition="5511">matching on what would today be considered a small data set to predict English verbs for Japanese to English simultaneous MT. Incorporating verb predictions into the translation process is a significant component of our framework, though n-gram models strongly prefer highly frequent verbs. Verb prediction might be improved by applying the insights from psycholinguistics. Ferreira (2000) argues that verb lemmas are required early in sentence production—prior to the first noun phrase argument—and that multiple possible syntactic hypotheses are maintained in parallel as the sentence is produced. Schriefers et al. (1998) argues that, in simple German sentences, noninitial verbs do not need lemma planning at all. Momma et al. (2014), investigating these prior claims, argues that the abstract relationship between the internal arguments and verbs triggers selective verb planning. 9 Conclusion and Future Work Creating an effective simultaneous translation system for sov to svo languages requires not only translating partial sentences, but also effectively predicting a sentence’s verb. Both elements of the system require substantial refinement before they are usable in a real-world system. Replacing our idealized </context>
</contexts>
<marker>Schriefers, Teruel, Meinshausen, 1998</marker>
<rawString>H Schriefers, E Teruel, and RM Meinshausen. 1998. Producing simple sentences: Results from picture–word interference experiments. Journal of Memory and Language, 39(4):609–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroaki Shimizu</author>
<author>Graham Neubig</author>
<author>Sakriani Sakti</author>
<author>Tomoki Toda</author>
<author>Satoshi Nakamura</author>
</authors>
<title>Collection of a simultaneous translation corpus for comparative analysis.</title>
<date>2014</date>
<booktitle>In International Language Resources and Evaluation.</booktitle>
<contexts>
<context position="35041" citStr="Shimizu et al., 2014" startWordPosition="5689" endWordPosition="5692">tant next step. Supporting multiple translation hypotheses and incremental decoding (Sankaran bundesumweltministerin Merkel bundesumweltministerin merkel hat den entwurf gezeigt bundesumweltministerin merkel hat den entwurf eines umweltpolitischen programms vorgestellt federal minister of the environment angela merkel federal minister of the environment angela merkel shown the draft federal minister of the environment angela merkel shown the draft of an ecopolitical program 1350 et al., 2010) would improve both the efficiency and effectiveness of our system. Using data from human translators (Shimizu et al., 2014) could also add richer strategies for simultaneous translation: passive constructions, reordering, etc. Verb prediction also can be substantially improved both in its scope in the system and how we predict verbs. Verb-final languages also often place verbs at the end of clauses, and also predicting these verbs would improve simultaneous translation, enabling its effective application to a wider range of sentences. Instead predicting an exact verb early (which is very difficult), predicting a semantically close or a more general verb might yield interpretable translations. A natural next step i</context>
</contexts>
<marker>Shimizu, Neubig, Sakti, Toda, Nakamura, 2014</marker>
<rawString>Hiroaki Shimizu, Graham Neubig, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2014. Collection of a simultaneous translation corpus for comparative analysis. In International Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation. In</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="12411" citStr="Snover et al., 2006" startWordPosition="1984" endWordPosition="1987">lect which actions to take. Eventually, we will formalize this as a learned policy (Section 5) that maps from states to actions. First, however, we need to define a reward that measures how “good” an action is. 3 Objective: What is a good simultaneous translation? Good simultaneous translations must optimize two objectives that are often at odds, i.e., producing translations that are, in the end, accurate, and producing them in pieces that are presented expeditiously. While there are existing automated metrics for assessing translation quality (Papineni et al., 2002; Banerjee and Lavie, 2005; Snover et al., 2006), these must be modified to find the necessary compromise between translation quality and expeditiousness. That is, a good metric for simultaneous translation must achieve a balance between translating chunks early and translating accurately. All else being equal, maximizing either goal in isolation is trivial: for accurate translations, use a batch system and wait until the sentence is complete, translating it all at once; for a maximally expeditious translation, create monotone translations, translating each word as it appears, as in Tillmann et al. (1997) and Pytlik and Yarowsky (2006). The</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In In Proceedings of Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Umar Syed</author>
<author>Michael Bowling</author>
<author>Robert E Schapire</author>
</authors>
<title>Apprenticeship learning using linear programming.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference of Machine Learning.</booktitle>
<contexts>
<context position="19589" citStr="Syed et al., 2008" startWordPosition="3171" endWordPosition="3174">1:i_1 is the n −1-gram context. To narrow the search space, we consider only the 100 most frequent final verbs, where a “final verb” is defined as the sentence-final sequence of verbs and particles as detected by a German part-of-speech tagger (Toutanova et al., 2003).6 5 Learning a Policy We have a framework (states and actions) for simultaneous machine translation and a metric for assessing simultaneous translations. We now describe the use of reinforcement learning to learn a policy, a mapping from states to actions, to maximize LBLEU reward. We use imitation learning (Abbeel and Ng, 2004; Syed et al., 2008): given an optimal sequence of actions, learn a generalized policy that maps states to actions. This can be viewed as a cost-sensitive classification (Langford and Zadrozny, 2005): a state is represented as a feature vector, the loss corresponds to the quality of the action, and the output of the classifier is the action that should be taken in that state. In this section, we explain each of these components: generating an optimal policy, representing states through features, and learning a policy that can generalize to new sentences. 5.1 Optimal Policies Because we will eventually learn polic</context>
</contexts>
<marker>Syed, Bowling, Schapire, 2008</marker>
<rawString>Umar Syed, Michael Bowling, and Robert E. Schapire. 2008. Apprenticeship learning using linear programming. In Proceedings of the International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Alex Zubiaga</author>
</authors>
<title>A dp-based search using monotone alignments in statistical translation.</title>
<date>1997</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="12975" citStr="Tillmann et al. (1997)" startWordPosition="2071" endWordPosition="2074"> al., 2002; Banerjee and Lavie, 2005; Snover et al., 2006), these must be modified to find the necessary compromise between translation quality and expeditiousness. That is, a good metric for simultaneous translation must achieve a balance between translating chunks early and translating accurately. All else being equal, maximizing either goal in isolation is trivial: for accurate translations, use a batch system and wait until the sentence is complete, translating it all at once; for a maximally expeditious translation, create monotone translations, translating each word as it appears, as in Tillmann et al. (1997) and Pytlik and Yarowsky (2006). The former is not simultaneous at all; the latter is mere word-for-word replacement and results in awkward, often unintelligible translations of distant language pairs. Once we have predictions, we have an expanded array of possibilities, however. On one extreme, we can imagine a psychic translator— 3Using constrained decoding to enforce consistent translation prefixes would complicate our method but is an appealing extension. one that can completely translate an imagined sentence after one word is uttered—as an unobtainable system. On the other extreme is a st</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, 1997</marker>
<rawString>Christoph Tillmann, Stephan Vogel, Hermann Ney, and Alex Zubiaga. 1997. A dp-based search using monotone alignments in statistical translation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="19239" citStr="Toutanova et al., 2003" startWordPosition="3115" endWordPosition="3118">towards different kinds of simultaneous translations. As β → ∞, we recover batch translation. Er ist zum Laden gegangen Psychic β He went to the store Monotone He He to the He to the store He to the store went Batch He went to the store Policy He Prediction He went to He went He went to the the store BLEU(yt, r) (1) arg max p(v) v t i=1 1346 where xi_n+1:i_1 is the n −1-gram context. To narrow the search space, we consider only the 100 most frequent final verbs, where a “final verb” is defined as the sentence-final sequence of verbs and particles as detected by a German part-of-speech tagger (Toutanova et al., 2003).6 5 Learning a Policy We have a framework (states and actions) for simultaneous machine translation and a metric for assessing simultaneous translations. We now describe the use of reinforcement learning to learn a policy, a mapping from states to actions, to maximize LBLEU reward. We use imitation learning (Abbeel and Ng, 2004; Syed et al., 2008): given an optimal sequence of actions, learn a generalized policy that maps states to actions. This can be viewed as a cost-sensitive classification (Langford and Zadrozny, 2005): a state is represented as a feature vector, the loss corresponds to t</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Conference of the North American Chapter of the Association for Computational Linguistics, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="25230" citStr="Vogel et al., 1996" startWordPosition="4092" endWordPosition="4096"> source sentence correctly once it has been fed the appropriate source words as input. There are two edge cases: empty input yields an empty output, while a complete, correct source sentence returns the correct, complete translation. Intermediate cases—where the input is either incomplete or incorrect—require using an alignment. The omniscient translator assumes as input a reference translation r, a partial source language input x1:t and a corresponding partial output y. In addition, the omniscient translator assumes access to an alignment between r and x. In practice, we use the xMM aligner (Vogel et al., 1996; Och and Ney, 2003). We first consider incomplete but correct inputs. Let y = τ(x1:t) be the translator’s output given a partial source input x1:t with translation y. Then, τ(x1:t) produces all target words yj if there is a source word xi in the input aligned to those words—i.e., (i, j) E ax,y—and all preceding target words can be translated. (That translations must be contiguous is a natural requirement for human recipients of translations). In the case where yj is unaligned, the closest aligned target word to yj that has a corresponding alignment entry is aligned to xi; then, if xi is prese</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of the 16th International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Wahlster</author>
</authors>
<title>Verbmobil: foundations of speech-to-speech translation.</title>
<date>2000</date>
<publisher>Springer.</publisher>
<contexts>
<context position="31782" citStr="Wahlster, 2000" startWordPosition="5209" endWordPosition="5210"> and the overall policy is similar to what a human would do: intuit the subject of the sentence from early clues and use a more general verb to stand in for a more specific one. 8 Related Work Just as MT was revolutionized by statistical learning, we suspect that simultaneous MT will similarly benefit from this paradigm, both from a systematic system for simultaneous translation and from a framework for learning how to incorporate predictions. Simultaneous translation has been dominated by rule and parse-based approaches (Mima et al., 1998a; Ryu et al., 2006). In contrast, although Verbmobil (Wahlster, 2000) performs incremental translation using a statistical MT module, its incremental decisionmaking module is rule-based. Other recent approaches in speech-based systems focus on waiting until a pause to translate (Sakamoto et al., 2013) or using word alignments (Ryu et al., 2012) between languages to determine optimal translation units. Unlike our work, which focuses on prediction and learning, previous strategies for dealing with sov-to-svo translation use rule-based methods (Mima et al., 1998b) (for instance, passivization) to buy time for the translator to hear more information in a spoken con</context>
</contexts>
<marker>Wahlster, 2000</marker>
<rawString>Wolfgang Wahlster. 2000. Verbmobil: foundations of speech-to-speech translation. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
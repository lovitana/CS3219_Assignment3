<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.965646">
Confidence-based Rewriting of Machine Translation Output
</title>
<author confidence="0.882309">
Benjamin Marie Aur´elien Max
</author>
<note confidence="0.4724995">
LIMSI-CNRS, Orsay, France LIMSI-CNRS, Orsay, France
Lingua et Machina, Le Chesnay, France Univ. Paris Sud, Orsay, France
</note>
<email confidence="0.96906">
benjamin.marie@limsi.fr aurelien.max@limsi.fr
</email>
<sectionHeader confidence="0.993102" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99978756">
Numerous works in Statistical Machine
Translation (SMT) have attempted to iden-
tify better translation hypotheses obtained
by an initial decoding using an improved,
but more costly scoring function. In this
work, we introduce an approach that takes
the hypotheses produced by a state-of-
the-art, reranked phrase-based SMT sys-
tem, and explores new parts of the search
space by applying rewriting rules se-
lected on the basis of posterior phrase-
level confidence. In the medical do-
main, we obtain a 1.9 BLEU improve-
ment over a reranked baseline exploiting
the same scoring function, corresponding
to a 5.4 BLEU improvement over the orig-
inal Moses baseline. We show that if an
indication of which phrases require rewrit-
ing is provided, our automatic rewriting
procedure yields an additional improve-
ment of 1.5 BLEU. Various analyses, in-
cluding a manual error analysis, further il-
lustrate the good performance and poten-
tial for improvement of our approach in
spite of its simplicity.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999681377358491">
The standard configuration of modern phrase-
based Statistical Machine Translation (SMT)
(Koehn et al., 2003) systems can produce very ac-
ceptable results on some tasks. However, early
integration of better features to guide the search
for the best hypothesis can result in significant im-
provements, an expression of the complexity of
modeling translation quality. For instance, im-
provements have been obtained by integrating fea-
tures into decoding that better model semantic co-
herence at the sentence level (Hasan and Ney,
2009) or syntactic well-formedness (Schwartz et
al., 2011). However, early use of such complex
features typically comes at a high computational
cost. Moreover, some informative features require
or are better computed when complete translation
hypotheses are available. This is addressed in nu-
merous works on reranking of the highest scored
sub-space of hypotheses, on so-called n-best lists
(Och et al., 2004; Zhang et al., 2006; Carter and
Monz, 2011) or output lattices (Schwenk et al.,
2006; Blackwood et al., 2010), where many works
specifically target the inclusion of better language
modelling capabilities, a well-known weakness of
current automatic generation approaches (Knight,
2007).
Another way to improve translation a posteriori
can be done by rewriting initial hypotheses, for in-
stance in a greedy fashion by including new mod-
els (Langlais et al., 2007; Hardmeier et al., 2012),
or by specifically modeling a task of automatic
post-editing targeting a specific system (Simard et
al., 2007; Dugast et al., 2007). While such auto-
matic post-editing may seem to be too limited, no-
tably because of the limited initial diversity con-
sidered and the fact that it may be in some in-
stances agnostic to the internals of the initial sys-
tem, it has been shown to potentially improve ac-
curacy of the new translation hypotheses (Parton
et al., 2012) and to offer very high oracle perfor-
mance (Marie and Max, 2013).
However, an important issue for such ap-
proaches is their capacity to only rewrite incor-
rect parts of the translation hypotheses and to use
appropriate replacement candidates. Many works
have tackled the issue of word to n-gram confi-
dence estimation in SMT output (Zens and Ney,
2006; Ueffing and Ney, 2007; Bach et al., 2011;
de Gispert et al., 2013), and some attempts have
been made to exploit confidence estimates for lat-
tice rescoring (Blackwood et al., 2010) or n-best
reranking (Bach et al., 2011; Luong et al., 2014b).
In this work, we present an approach in which
</bodyText>
<page confidence="0.918101">
1261
</page>
<note confidence="0.89662">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1261–1272,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999966205128205">
new complete hypotheses are produced by rewrit-
ing existing hypotheses, and are scored using com-
plex models that could not be used during the ini-
tial decoding. We will use as competitive baselines
systems that rerank the output of an initial decoder
using the complete set of available features, and
will show that we manage to improve their trans-
lation. The difference between our approach and
the reranking baseline lies in the manner in which
we expand our training data, as well as in our use
of high-confidence rewritings to obtain new trans-
lation hypotheses. Importantly, this work will only
exploit simple confidence estimates corresponding
to phrase-based posteriors, which do not require
that large sets of human-annotated data be avail-
able as in other works (Bach et al., 2011; Luong et
al., 2014b).
The remainder of this paper is organized as fol-
lows. Section 2 is devoted to the description of
our approach, with details on our rewriting ap-
proach (2.1), additional features (2.2), rewriting
phrase table (2.3), and training examples (2.4).
Section 3 presents experiments. We first describe
our experimental setup (3.1) and our baseline sys-
tems (3.2). We then report results when naive
rewriting is performed and then with confidence-
based rewriting (3.3). We next devote a significant
part of the paper in section 4 to report further re-
sults and analyses: an analysis of the performance
of our system depending on the quality of initial
hypotheses (4.1); a semi-oracle experiment where
correct phrases are known (4.2); an oracle exper-
iment where only correct rewriting decisions are
made (4.3); a manual error analysis of the main
configurations studied in this work (4.4); and, fi-
nally, a study of the performance of our approach
on a more difficult translation task (4.5). Related
work is discussed in section 5 and we conclude
and introduce our future work in section 6.
</bodyText>
<sectionHeader confidence="0.890125" genericHeader="method">
2 Description of the approach
</sectionHeader>
<subsectionHeader confidence="0.999505">
2.1 Rewriting of translation hypotheses
</subsectionHeader>
<bodyText confidence="0.999927714285714">
Langlais et al (2007) proposed a greedy search
procedure to improve translations by reusing the
same translation table and scoring function that
were used during an initial phrase-based decoding.
In our approach, we rewrite hypotheses by using
the same greedy search algorithm, adding more
complex models and using the most-confident bi-
phrases according to the initial decoder’s search
space. To select the hypothesis to rewrite for
each sentence, we produce a n-best list of the ini-
tial decoder and rerank this list with a new, bet-
ter informed scoring function (see section 2.2).
The one-best hypothesis obtained after rerank-
ing is then rewritten by our system (denoted as
rewriter). In this way, we ensure that the hy-
pothesis that was rewritten had been so far the
best one according to the initial decoding best sub-
space and the new models used.
At each iteration, new hypotheses are obtained
from a current hypothesis by applying one rewrit-
ing operation on bi-phrases. The set of all new hy-
potheses is called the neighborhood of the current
hypothesis. Focusing in this work on local rewrit-
ing, we used the following set of operations (N de-
notes the number of bi-phrases, T the maximum
number of entries per source phrase in a rewriting
phrase table (see 2.3), and S the average number
of tokens per source phrase)1:
</bodyText>
<listItem confidence="0.9922104">
1. replace (O(N.T)): replaces the transla-
tion of a source phrase with another transla-
tion from the rewriting phrase table;
2. split (O(N.S.T2)): splits a source phrase
into all possible sets of two (contiguous)
phrases, and uses replace on each of the
resulting phrases;
3. merge (O(T.N)): merges two contiguous
source phrases and uses replace on the re-
sulting new phrase.
</listItem>
<bodyText confidence="0.9858945">
This rewriting algorithm is described in pseudo-
code in Algorithm 1.
</bodyText>
<equation confidence="0.7029122">
Algorithm 1 rewriter Algorithm
Require: source a sentence to translate
nbestList +— TRANSLATE(source)
oneBest +— RERANK(nbestList)
sCurrent +— GET SCORE(oneBest)
loop
hypothesesSet +— NEIGHBORHOOD(oneBest)
newOneBest +— RANK(hypothesesSet)
s +— GET SCORE(newOneBest)
if s &lt; sCurrent then
return oneBest
else
oneBest +— newOneBest
sCurrent +— s
end if
</equation>
<footnote confidence="0.87052075">
end loop
1Complexity is expressed in terms of the maximum num-
ber of hypotheses that will be considered given some hypoth-
esis to rewrite.
</footnote>
<page confidence="0.99496">
1262
</page>
<bodyText confidence="0.999974357142857">
The produced hypotheses are then ranked ac-
cording to a new, better informed scoring function
(see 2.2). At the next iteration, the hypothesis now
ranked at the top of the list is rewritten, and search
terminates when no better hypothesis is found.
Such a greedy search has several obvious lim-
itations, in particular it can only perform a lim-
ited exploration of the search space, a situation
that can be improved by using a beam (see Sec-
tion 3.3). However, associated with a small and
precise rewriting phrase table, this approach only
visits small numbers of more-confident hypothe-
ses, which is a critical property given the cost of
computing the new scoring function used.
</bodyText>
<subsectionHeader confidence="0.998969">
2.2 Reranking and features
</subsectionHeader>
<bodyText confidence="0.999971">
The rerankings of the hypotheses sets de-
scribe in this work are all performed with
kb-mira (Cherry and Foster, 2012) using the ini-
tial features set of the decoder in conjunction with
the following additional features:2
</bodyText>
<listItem confidence="0.863219269230769">
• SOUL models: SOUL models are structured
output layer neural network language mod-
els (LMs) which have been shown to be use-
ful in reranking tasks, for instance for WMT
evaluations (Allauzen et al., 2013; P´echeux et
al., 2014). SOUL scoring being too costly to
be integrated during decoding, it fits perfectly
the reranker scenario, which furthermore
enables to use larger contexts for n-grams.
We used both monolingual (Le et al., 2011)
and bilingual (Le et al., 2012) SOUL 10-gram
models, which were trained on the WMT’12
data.
• POS language model: part-of-speech (POS)
LMs have been shown to yield improvements
in n-best list reranking (Carter and Monz,
2011). In this work, we trained a 6-gram POS
LM using Witten-Bell smoothing.
• IBM1 : the IBM1 scores (p(e|f) and p(f|e))
of the complete hypothesis (Och et al., 2004).
• phrase-based confidence score: bi-phrases
are associated to a posterior probability, in-
spired from n-gram posterior probability esti-
mation as defined in (de Gispert et al., 2013).
Let E be the set of all hypotheses in the
space of translation hypotheses defined by
</listItem>
<bodyText confidence="0.9700365">
2Note that we did not try to explore the independant con-
tribution of each feature in this work.
the n-best list used for source sentence f, and
Eα be the subset of E such that word align-
ments in sentence pairs (e&apos;, f), be&apos; E Eα,
allow us to extract bi-phrase α. Let also
H(e, f) be the score assigned by a base-
line decoder (denoted as 1-pass Moses
henceforth) to sentence pair (e, f). We use
the following posterior probability for α:
</bodyText>
<equation confidence="0.989990666666667">
P(α|F)
�e1EE exp(H(e&apos;, f))
=
</equation>
<bodyText confidence="0.999175333333333">
Then, the logarithms of each phrase’s con-
fidence score are summed to use as a confi-
dence score for the complete hypothesis.
</bodyText>
<subsectionHeader confidence="0.999149">
2.3 Rewriting phrase table
</subsectionHeader>
<bodyText confidence="0.999444714285714">
Taking the whole translation table of the decoder
as a rewriting phrase table to perform the greedy
search produces very large neighborhoods that
rewriter cannot handle due to the cost of the
models that have to be computed. We tried two
different approaches to extract a rewriting phrase
table from the translation table of the system.
We first tried a naive approach where the rewrit-
ing phrase table of rewriter for the test set
uses the phrase table of 1-pass Moses, filtered
to keep the k best entries according to the direct
translation model. We denote such a configuration
rptkpef.
Our second approach consists in extracting the
rewriting phrase table containing bi-phrases that
were the most probable according to the set of all
models used in 1-pass Moses. Selection of bi-
phrases for each sentence is done in a binary fash-
ion, depending on their presence in k-best lists of
1-pass Moses for a given value of k. This con-
figuration will be denoted confk.
</bodyText>
<subsectionHeader confidence="0.998597">
2.4 Training examples
</subsectionHeader>
<bodyText confidence="0.999995230769231">
We tried several sets of examples to train the
ranker of rewriter. We used the 1,000-best
list of the development set produced by 1-pass
Moses during its tuning. In other configurations
we mixed a) the neighborhood of the reranker
n-best hypotheses computed by our system on the
development set using a rewriting phrase table
containing the bi-phrases found in the k-best list
produced by 1-pass Moses; and b) the neigh-
borhood of the one-best hypotheses of reranker
using a rewriting phrase table containing the 10-
best translations from the 1-pass Moses trans-
lation table according to the direct translation
</bodyText>
<equation confidence="0.9966115">
« (1)
Ee11EE exp(H(e&apos;&apos;, f ))
</equation>
<page confidence="0.616765">
1263
</page>
<bodyText confidence="0.999456625">
model. Both neighborhoods are produced by a
single iteration of rewriter. We denote re-
spectively these sets of hypotheses n-bestNeigh
and 10PefNeigh. Our intuition behind the consti-
tution of these training sets is that the ranker of
rewriter needs, in order to perform well, train-
ing examples that will be similar to hypotheses
that it actually generates.
</bodyText>
<sectionHeader confidence="0.999823" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999782">
3.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.95543862962963">
We used two datasets from two different domains:
the data provided for the WMT’14 medical trans-
lation task3 (Medical) and a smaller task using
the TED talks4 (TED Talks) data of the IWSLT
evaluation campaigns. For the Medical task we
used only the English to French translation di-
rection, and both translation directions, English
to French and French to English, for the TED
Talks task. In this work, the main part of our ex-
periments uses Medical, and TED Talks will
be used at a later stage to study a lower-quality
situation (cf. 4.5). For the Medical task, initial
decodings were produced using a LM trained on
all WMT’14 monolingual and bilingual medical
data, while for the TED Talks task we used a
much larger LM trained on all the data provided
for WMT’135. Both are 4-gram LMs estimated
with Kneser-Ney smoothing (Chen and Goodman,
1998). For the 6-gram POS LMs used (see 2.2),
we used the same data as used for the token-based
LM for Medical, and the concatenation of the
News Commentaries and Europarl sub-parts of the
WMT’13 data for TED Talks. Table 1 provides
relevant statistics about the data used.
Tasks Corpus Sentences Tokens(en-fr)
train 4.9M 78M - 91M
Medical dev 500 10k - 12k
</bodyText>
<equation confidence="0.9261895">
test 1,000 21k - 26k
LM - 146M
train 107 758 2M - 2.2M
TED Talks dev 934 20k - 20k
test 1,664 31k - 34k
LM 6B - 2.5B
</equation>
<tableCaption confidence="0.998808">
Table 1: Corpora used in this work.
</tableCaption>
<footnote confidence="0.9971542">
3http://www.statmt.org/wmt14/
medical-task/
4https://wit3.fbk.eu/mt.php?release=
2013-01
5http://www.statmt.org/wmt13
</footnote>
<bodyText confidence="0.999987904761905">
We first built a state-of-the-art phrase-based
SMT system using Moses (Koehn et al., 2003)
with standard settings. We tuned its parameters to-
wards BLEU (Papineni et al., 2002) on the tuning
dataset using the kb-mira implementation avail-
able in Moses with default parameters.
Our results will be compared using BLEU
and TER (Snover et al., 2006) to a) the initial
best translation produced by the Moses decoder
(1-pass Moses) and b) the best translation ob-
tained by reranking the 1,000-best list of 1-pass
Moses (reranker). Since reranker imple-
ments a well-documented approach and uses types
of features commonly used in reranking tasks we
will consider it as our main baseline. It was trained
using kb-mira on the 1,000-best of the develop-
ment data decoded by 1-pass Moses.
In our experiments, rewriter rewrites the
one-best hypothesis6 produced by reranker
using the operators Replace, Split and
Merge as described in section 2.1.
</bodyText>
<subsectionHeader confidence="0.999637">
3.2 Baseline results
</subsectionHeader>
<bodyText confidence="0.999895238095238">
Table 2 gives the results of the 1-pass Moses
decoding for the Medical task and the rerank-
ing results of reranker applied to the 1-pass
Moses 1,000-best list.
1-pass Moses obtains a score of38.2BLEU
on the test set, which can be considered as
a good baseline system.7 reranker outper-
forms 1-pass Moses by 3.5 BLEU, indicating
a strong performance of the features used on this
task. In particular, SOUL is known to be a use-
ful feature for reranking n-best lists on highly-
inflected languages such as French. Note also that
the SOUL models we used were trained on the
WMT’12 monolingual and bilingual data and so
were better informed than the models used dur-
ing the 1-pass Moses decoding.8 Moreover,
as can be seen on Figure 1, the 1,000-best ora-
cle reveals a large potential for improvement over
the one-best (+12.4 BLEU). We further observe
that the reranked list of reranker shows a much
faster potential for translation improvement.
</bodyText>
<footnote confidence="0.68934475">
6Note that we will also provide results where a beam of
k-best hypotheses are rewritten.
7Distribution of error types on a sub-part of the test set
will be provided in section 4.4.
8However, SOUL considers only a small sample of the
training data for training. For instance, the training of the
French monolingual model used roughly only 1% (895K sen-
tences) of all the WMT’12 data.
</footnote>
<page confidence="0.991052">
1264
</page>
<figureCaption confidence="0.9255245">
Figure 1: n-best list oracle for 1-pass Moses
and reranker
</figureCaption>
<subsectionHeader confidence="0.998068">
3.3 rewriter results
</subsectionHeader>
<bodyText confidence="0.999829720930233">
Results for the different rewriting phrase tables
and training examples are given in Table 2. First,
concerning the rewriting phrase table, for the
k=5 (rpt5pef) and k=10 (rpt10pef) con-
figurations9 a decrease of 0.7-0.8 BLEU over
reranker is obtained. This illustrates that naive
rewritings applied on the test set cannot be used
with our training regime to improve translation
quality.
In the next experiments, we used a confk
rewriting table. Table 210 shows the results of
rewriter when rewriting the one-best hypothe-
sis from reranker for various values of k to de-
fine the k-best list from which the rewriting table
is built. Various training sets are also considered
in the table.
The 1-pass Moses 1,000-best configuration
reused the same set of hypotheses used to train
reranker. For this configuration, rewriter
loses 2.6 BLEU over reranker on the test set
with conf10k. Of course, this training data set
is of a quite different nature compared to the hy-
potheses built by rewriter.
In the 10pefNeigh training, the ranker is trained
with the neighborhoods produced by the first itera-
tion of rewriter on the development set with a
rewriting phrase table containing only the k-best
translations for each source phrase according to
the direct translation model. This configuration
9We did not experiment with higher values of k because of
the computationnal cost of the features used by reranker.
Indeed, adding more phrase translations increases the size of
the neighborhoods corresponding to many additional n-grams
to score by SOUL, the most expensive model.
10In Table 2 the number of unique bi-phrases for the
rpt rewriting phrase tables is computed by considering only
source phrases appearing in the test set, for the n-best Neigh-
borhood configurations we merged the phrase tables of each
sentence into one and count just as one unique entry bi-
phrases appearing several times.
improves over the previous one by 1.7 BLEU, but
is still 0.9 BLEU below reranker. Adding the
neighborhoods of the reranker n-best hypothe-
ses produced with a conf10k rewriting phrase
table to the training data does not improve over
the previous situation for n = 10, but increasing n
to 30 and then 50 produces strong improvements
on the test set (resp. +1.4 and +1.6 BLEU). Con-
sidering a larger neighborhood obtained by rewrit-
ing the best n = 90 hypotheses does not yield
further gains. We denote from now on opti
our best configuration thus far, considering the
performance on the development set and having
the largest confidence-based rewriting phrase ta-
ble considered.
Letting rewriter perform a beam search on
the 10-best hypotheses of the test set, further gains
are obtained, corresponding now to an improve-
ment of +1.9 BLEU over our reranker base-
line, or +5.4 BLEU over 1-pass Moses.11 Fur-
thermore, although taking the bi-phrases from the
10,000-best is our best configuration, it is inter-
esting to note that taking bi-phrases from the 10-
best only already yields a moderate improvement
of +0.6 BLEU over reranker. Figure 2a shows
that up to k = 10, 000 higher value of k to ex-
tract the rewriting phrase table increase the BLEU
score on the test set. 12 We did not experiment with
higher values of k, but plan to use the output lat-
tice produced by 1-pass Moses to compute ef-
ficiently posteriors for larger sets of bi-phrases (de
Gispert et al., 2013).
As illustrated on Figure 2b, rewriter mostly
improves the BLEU score during the three first
iterations and then converges at the ninth iteration.
However, it is important to note that not all
sentences are actually improved by our system.
As illustrated on Figure 3a, opti improves
40.8% of the sentences of the test set but degrades
29.2% of them according to sentence-BLEU (Lin
and Och, 2004). It is certainly the case that
more informative confidence features may help
idenfity more precisely which fragments of the
translations should really undergo rewriting. We
will investigate the exploitation of an oracle
phrase-based confidence measure in Section 4.2.
</bodyText>
<footnote confidence="0.735912714285714">
11Using a beam becomes quickly prohibitive: using
12 threads, 25 mn vs. 3h were needed for the test set for
the configurations of size 1 and 10, respectively.
12Note that even for k = 10, 000 the computed neighbor-
hoods are still quite small with an average of 116 hypotheses
for each hypothesis to rewrite per iteration, against an average
of 788 hypotheses for the rpt10pef configuration.
</footnote>
<page confidence="0.966756">
1265
</page>
<figure confidence="0.976852">
(a) Results of rewriter with rpt5pef, rpt10pef and dif- (b) Iterations of rewriter on test with opti and two beam
ferent values of k for confk sizes : 1 and 10.
</figure>
<figureCaption confidence="0.9969255">
Figure 2: Performance of rewriter depending on the type of the rewriting phrase table and the number
of iterations and beam sizes.
</figureCaption>
<table confidence="0.999867611111111">
baseline dev test
BLEU BLEU TER GOS BLEU
1-pass Moses 40.9 38.3 44.6
reranker 44.1 41.8 41.6
training data rewriting unique beam
phrase table bi-phrases size
1-pass Moses 1000-best conf10k 38 455 1 44.1 39.2(_2.6) 43.8(+2.2) 58.7
10pefNeigh conf10k 38 455 1 43.9 40.9(_0.9) 41.2(_0.4) 58.7
10-bestNeigh + 10pefNeigh conf10k 38 455 1 43.8 40.9(_0.9) 41.2(_0.4) 58.7
30-bestNeigh + 10pefNeigh conf10k 38 455 1 44.2 43.2(+1.4) 40.6(_1.0) 58.7
50-bestNeigh+ 10pefNeigh rpt5pef 85 530 1 44.5 41.0(_0.8) 42.0(+0.4) 50.6
= rpt10pef 149 887 1 44.5 41.1(_0.7) 42.1(+0.5) 54.5
= conf10 21 398 1 44.5 42.4(+0.6) 41.0(_0.6) 45.9
= conf100 28 730 1 44.5 42.9(+1.1) 40.8(_0.8) 50.2
= conf1k 33 929 1 44.5 43.0(+1.2) 40.6(_1.0) 53.3
=(opti) conf10k 38 455 1 44.5 43.4(+1.8) 40.4(_1.a) 58.7
= conf10k 38 455 10 44.5 43.7(+1.$) 40.1(_1.$) 59.6
90-bestNeigh + 10pefNeigh conf10k 38 455 1 44.4 43.4(+1.6) 40.4(_1.2) 58.7
</table>
<tableCaption confidence="0.7602985">
Table 2: Results on Medical for different training configurations, rewriting phrase tables and beam
sizes. opti denotes our optimal configuration for rewriter.
</tableCaption>
<sectionHeader confidence="0.887766" genericHeader="method">
4 Analysis of confidence-based rewriting
</sectionHeader>
<subsectionHeader confidence="0.9680835">
4.1 Performance of rewriter depending on
the quality of initial hypotheses
</subsectionHeader>
<bodyText confidence="0.999986833333333">
The first question we address in our analysis of
rewriter is whether its performance depends
on the difficulty of each individual sentence. As
a proxy of sentence difficulty we used sentence-
BLEU of 1-pass Moses, and used it to di-
vide the sentences of the test set into quartiles.
Figure 4 shows that reranker improves more
over 1-pass Moses and that at the same time
rewriter improves more over reranker as
the sentences are more difficult. In particular,
rewriter obtains a 8.6 BLEU improvement
over 1-pass Moses on the more difficult quar-
tile, but only a 1.3 BLEU improvement on the least
difficult quartile. We hypothesize that better per-
formance may be achieved if adapting the training
and rewriting of rewriter to sentences of vary-
ing quality, which may, for instance, be estimated
with off-the-shelf estimators (Specia et al., 2013).
</bodyText>
<subsectionHeader confidence="0.9676135">
4.2 Semi-oracle experiments: rewriting only
incorrect fragments
</subsectionHeader>
<bodyText confidence="0.9999753">
We observed in section 3.3 that our opti con-
figuration, which obtains strong improvements in
translation quality (as given by corpus-BLEU),
in fact degrades (as given by sentence-BLEU)
a significant proportion of sentences. To fur-
ther analyze these results, we simulate a situa-
tion where oracle confidence information is avail-
able at the phrase-level: in particular, rewriter
is prevented from rewriting bi-phrases whose tar-
get phrase appears exactly in the reference transla-
</bodyText>
<page confidence="0.980295">
1266
</page>
<figure confidence="0.995188">
(a) automatic (b) semi-oracle
</figure>
<figureCaption confidence="0.913192">
Figure 3: sBLEU delta, for each sentence, between the reranker one-best to rewrite and its auto-
matic (3a) or semi-oracle (3b) rewriting computed by rewriter with the opti configuration.
Figure 4: Source sentences were divided into
quartiles according to sBLEU of the 1-pass
Moses system. For each quartile we reported the
performance of 1-pass Moses, reranker,
rewriter, GOS.
</figureCaption>
<bodyText confidence="0.9986935">
the first iteration, rewriter “froze” approx-
imatively 65.6% of the bi-phrases, and 70.5%
at the last iteration, demonstrating the ability of
rewriter to find good rewritings that match the
reference translation. Looking at Figure 3b, we
now find that, as expected, only a limited num-
ber of sentences are now degraded by rewriter.
The large improvements obtained clearly under-
lines the important role that better confidence esti-
mates could play in our framework.
</bodyText>
<table confidence="0.995158166666667">
System test TER
BLEU
reranker 41.8 41.6
opti 43.4 40.4
semi-oracle, beam 1 44.9(+1.5) 39.2(−1.2)
semi-oracle, beam 10 44.9(+1.5) 39.0(−1.4)
</table>
<tableCaption confidence="0.998924">
Table 3: Results for the semi-oracle using opti.
</tableCaption>
<bodyText confidence="0.997349538461538">
tion.13 Furthermore, this “freezing” of bi-phrases
can be repeated after each iteration of rewriter.
Thus, we now have an oracle situation for
choosing which source phrases may be rewrit-
ten, but the rest of the rewriting procedure is
still fully automatic. Moreover, we purposefully
did not adapt the training procedure to this new
configuration, and reused opti as is. Results,
reported in Table 3, indicate that an additional
1.5 BLEU is obtained from opti, or 3.1 BLEU
from reranker and 6.6 BLEU from 1-pass
Moses. The use of a larger beam of size 10
did not improve those results any further. At
</bodyText>
<footnote confidence="0.826391">
13This is obviously not an optimal solution.
</footnote>
<subsectionHeader confidence="0.953536">
4.3 Oracle experiments: making only the
correct decisions
</subsectionHeader>
<bodyText confidence="0.999817583333333">
We now turn to the situation where only rewrit-
ings that actually improve translation performance
would be made. In practice, we use a sim-
ple solution: we resort to greedy oracle search
(GOS) (Marie and Max, 2013), where sentence-
BLEU is maximized using rewritings from the
opti phrase table. At each iteration the rewrit-
ing in the neighborhood that maximizes sentence-
BLEU is selected until convergence.
Results for this greedy search oracle appear in
the last column of Table 2 and allow us to put
in perspective the individual potential of the var-
</bodyText>
<page confidence="0.985324">
1267
</page>
<bodyText confidence="0.99998375">
ious tested configurations. We can first notice
that the rpt5pef phrase table allows the ora-
cle to reach 50.6 BLEU, 8.1 BLEU below the
oracle value obtained with conf10k, although
rpt5pef contains twice as many bi-phrases. The
same conclusion can be made about rpt10pef,
which is 3.9 BLEU higher than rpt5pef but con-
tains nearly twice as many bi-phrases. Finally, al-
though conf10k contains approximatively four
times fewer bi-phrases than rpt10pef, its ora-
cle value is 4.2 BLEU higher. This points out the
fact that conf10k is a lot more precise rewrit-
ing phrase table for the translations to rewrite, as
well as the fact that rpt5pef and rpt10pef
are much noisier and consequently difficult to use
efficiently by our automatic rewriting procedure.
</bodyText>
<subsectionHeader confidence="0.995798">
4.4 Manual error analysis
</subsectionHeader>
<bodyText confidence="0.999482760869565">
In the previous sections, we have shown that our
automatic rewriting procedure can improve trans-
lation quality over both an initial Moses baseline,
and a reranked baseline using the same features
as our procedure. We have further shown in sec-
tion 4.3 that much larger improvements could be
obtained by using an oracle procedure.
We now focus on the four following con-
figurations: 1-pass Moses, reranker,
rewriter and GOS. Although this four configu-
rations are well separated both in terms of BLEU
and TER scores, it is informative to look more
precisely into what makes their results different.
We performed a small-scale manual error analysis
of these four configurations. A French native
speaker annotated 70 translation hypotheses using
an error typology adapted from (Vilar et al.,
2006).
Results of the manual error analysis are re-
ported in Table 4. The most significant results
are for the disamb(iguation) and form error types,
the former being more related to translation accu-
racy, and the later to fluency. In both cases, we
first observe a strong reduction of errors between
1-pass Moses and reranker, which demon-
strates the positive impact of the features used
on these levels. Then, another, similar reduction
is obtained between reranker and rewriter,
demonstrating that our reranking procedure man-
ages to identify more precise and fluent hypothe-
ses. Finally, a further reduction is found between
rewriter and GOS, indicating that our proposed
local, greedy rewriting can still be improved, no-
tably by using more informative features and bet-
ter confidence estimates.
The other types of error categories are less in-
formative. We find no clear differences in er-
ror types attributable to style issues, which seem
to be irrecoverable even for GOS. reranker
and rewriter both improve on order-related er-
rors over 1-pass Moses, but our local rewrit-
ing unsurprisingly did not fix any of these errors.
Finally, reranker and rewriter decreased
slightly the number of extra words from 1-pass
Moses, while GOS sometimes artificially intro-
duces extra words.
</bodyText>
<subsectionHeader confidence="0.996992">
4.5 Lower-quality SMT experiments
</subsectionHeader>
<bodyText confidence="0.999897285714286">
We now turn to the question of how our rewrit-
ing system fares on a more difficult task, and used
TED Talks, 6 BLEU below Medical for the
English to French direction, for this purpose. In
the same way as we did for Medical, we first
tried to find the best training configuration for the
ranker of the rewriting system. For this task, mix-
ing the n-best neighborhood and 10pefNeigh with
n=10 seemed to be sufficient to have no more im-
provement on the development set by increasing n
for both language directions, so we used this train-
ing configuration. As for the rewriting phrase table
used on the test set, we simply selected conf10k
as in the Medical task. Results are reported
in Table 5 for French to English and English to
French.
We first observe that reranker performed
similarly for the two translation directions, by
improving 1-pass Moses by 0.5 BLEU. The
smaller improvements may be partly attributed to
the better LM used in 1-pass Moses, implying
a better early modeling of grammaticality, but also
by the fact that models such as SOUL and POS
LMs rely on accurate contexts and are therefore
more apt to help in choosing translations among
generally better candidates.
Finally, rewriter obtains smaller but consis-
tent improvements over reranker: +0.4 BLEU
for translation into English, and +0.9 BLEU for
translation into French. The smaller improvement
in the former situation may be attributed to the na-
ture of the target language which has a simpler
agreement system. Consequently, the form-related
errors discussed in Section 4.4 are possibly less
subject to improvement here.
</bodyText>
<page confidence="0.982714">
1268
</page>
<table confidence="0.978878333333333">
extra missing incorrect unknown
word word disamb form style order word all
1-pass Moses 11 1 57 91 13 31 10 214
reranker 5 3 47 73 11 19 10 168
rewriter 4 4 40 55 12 19 10 144
rewriter oracle 19 2 26 44 14 22 10 137
</table>
<tableCaption confidence="0.933843">
Table 4: Results for manual error analysis for the first 70 test sentences.
</tableCaption>
<table confidence="0.999555">
System fr-en en-fr
BLEU TER BLEU TER
1-pass Moses 32.5 47.7 32.3 49.9
reranker 33.0 47.3 32.8 49.4
rewriter 33.4(+0.4) 47.4(+0.1) 33.7(+0.9) 49.3(−0.1)
semi-oracle 34.1(+1.1) 46.6(−0.7) 34.2(+1.4) 48.6(−0.8)
</table>
<tableCaption confidence="0.992655">
Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks.
</tableCaption>
<sectionHeader confidence="0.999334" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.984938881355933">
Reranking of translation hypotheses n-best
list reranking was extensively studied in (Och et
al., 2004), using features not used in the initial
decoder such as IBM1 scores (which also proved
useful for word-level confidence estimation (Blatz
et al., 2004)) and generative syntactic models.
While the experiments in (Och et al., 2004) did
not show any clear contribution of syntactic in-
formation used in this manner, the later work by
Carter and Monz (2011) managed to successfully
exploit syntactic features using discriminative lan-
guage modeling for n-best reranking. Gimpel et
al. (2013) outperformed n-best reranking by gen-
erating, with an expensive but simple method, di-
verse hypotheses used as training data. Recently,
Luong et al. (2014b) reranked n-best lists using
confidence scores at the hypothesis level com-
puted from word-level confidence measures learnt
from roughly 10,000 SMT system outputs anno-
tated by humans.
Rewriting of translation hypotheses Langlais
et al. (2007) described a greedy search decoder,
first introduced in (Germann et al., 2001), able to
improve translations produced by a dynamic pro-
gramming decoder using the same scoring func-
tion and translation table. However, the more re-
cent work by Arun et al. (2010) using a Gibbs
sampler for approximating maximum translation
decoding showed the adequacy of the approxima-
tions made by state-of-the-art decoders for finding
the best translation in their search space. Other
works were more directly targeted at automatic
post-editing of SMT output, and approached the
problem as one of second-pass translation be-
tween automatic predictions and correct transla-
tions (Simard et al., 2007; Dugast et al., 2007).
The recent work of Zhu et al. (2013) attempts to
repair translations by exploiting confidence esti-
mates for examples derived from the similarity
between source words in the input text and in
training examples. Luong et al. (2014a) obtained
improvements by computing word confidence es-
timation, trained on human annotated data, and
large sets of lexical, syntactic and semantic fea-
tures, for the words in the n-best list produced
during a first-pass decoding, and performing a
second-pass decoding exploiting these new scores.
Confidence estimation of Machine Translation
The Word Posterior Probability (WPP) proposed
by Ueffing and Ney (2007), derived from informa-
tion from the n-best list produced by a decoder,
proved to be useful for estimating word-level con-
fidence. Bach et al. (2011) worked on the issue
of predicting sentence-level and word-level MT
errors by using WPP and other features derived
from the source context, the source-target align-
ment, and dependency structures, but relied on a
significantly large manually annotated corpus of
MT errors. De Gispert et al. (2013) calculate k-
</bodyText>
<page confidence="0.989032">
1269
</page>
<bodyText confidence="0.999862833333333">
gram posterior probabilities from n-best lists or
word lattices, and demonstrated that they were rea-
sonably accurate indications of whether specific k-
grams would be found or not in human reference
translations. Finally, the work of Blackwood et
al. (2010) proposed to segment translation lattices
according to confidence measures over the maxi-
mum likelihood translation hypothesis to focus on
regions with potential translation errors. Hypothe-
sis space constraints based on monolingual cover-
age are then applied to the low confidence regions
to improve translation fluency.
</bodyText>
<sectionHeader confidence="0.993547" genericHeader="conclusions">
6 Conclusions and perspectives
</sectionHeader>
<bodyText confidence="0.999517351351351">
In this paper, we have described an approach
that improves translations a posteriori by applying
simple local rewritings. We have shown that the
quality of phrase-level confidence estimates has
a direct impact of the amplitude of the improve-
ments that can be obtained, as well as the initial
quality of the rewritten hypotheses. We have used
a very simple definition for confidence estimates
under the form of phrase posteriors estimated from
n-best lists from an initial decoder, which obtained
good empirical performance, in spite of not requir-
ing large human-annotated datasets as in other ap-
proaches (Bach et al., 2011; Luong et al., 2014b).
Our work could be extended in several direc-
tions. First, we could use a larger set of rewrit-
ing operations (Langlais et al., 2007), including
the rewrite (sic) operation introduced in (Marie
and Max, 2013) that paraphrases source phrases
and then translates them.
We could also possibly consider any phrase seg-
mentation compatible with a specific word align-
ment rather than rely on specific phrase segmenta-
tions. This would allow us to attain faster some
rewritings that could otherwise require several
rewriting iterations and may never be attained by
the greedy procedure.
More features could also be used, for instance
to model more fine-grained syntax (Post, 2011)
or document-level lexical coherence (Hardmeier
et al., 2012). However, anticipating that some
features might be very expensive to compute, we
could adapt our procedure to work in several
passes: initial passes would tend to restrict the
search space more and more using an initial set
of features, before a more expensive pass would
concentrate on a limited number of hypotheses.
Figure 1 indeed already showed a much faster or-
acle improvement between 1-pass Moses and
reranker for n-best list of small sizes.
Another avenue for improvement lies in the pos-
sibility to perform the training of our rewriter
by providing it with more reference translations.
As these are typically not readily available, we
could resort to targeted paraphrasing (Madnani
and Dorr, 2013) to rewrite reference translations
into acceptable paraphrases that reuse n-grams
from the best hypotheses of the system so far.
Contrarily to (Madnani and Dorr, 2013), we could
bias the paraphrasing table so that it only con-
tains paraphrases that correspond to target phrases
of high confidence values, which would add new
n-grams likely of being produced by rewriter.
It is furthermore worth noticing that our work
proposes a potential answer to an original ques-
tion: contrarily to typical works on sub-sentencial
MT confidence estimation, which predict whether
a word or phrase is correct or not, our rewriter
system could be used to determine automatically
whether a rewriting system could (if asked to) at-
tempt to improve locally a translation, or whether
a human post-editor should already tackle work-
ing on improving it. As we showed in our manual
error analysis in section 4.4, there are in fact many
instances of errors that could not be recovered by
our approach, be it because of its local rewriting
strategy or of the bilingual resources or models
used, so that some knowledge would have to be
provided as hard constraints by a human transla-
tor, as hinted in (Crego et al., 2010). We could
then finally have our rewriter system work in a
turn-based fashion in collaboration with a human
translator, fixing errors or making improvements
that are being made possible by the last edits from
the translator.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997731">
The authors would like to thank the anonymous re-
viewers and Guillaume Wisniewski for their use-
ful remarks. Additional thanks go to Hai Son Le
for “anticipating” the need for a large and effi-
cient cache in his SOUL implementation, Quoc
Khanh Do for his assistance on using SOUL, and
Li Gong and Nicolas P´echeux for providing the
authors with data used in the experiments. The
work of the first author is supported by a CIFRE
grant from French ANRT.
</bodyText>
<page confidence="0.986349">
1270
</page>
<sectionHeader confidence="0.989946" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999870317307692">
Alexandre Allauzen, Nicolas P´echeux, Quoc Khanh
Do, Marco Dinarelli, Thomas Lavergne, Aur´elien
Max, Hai-son Le, and Franc¸ois Yvon. 2013.
LIMSI @ WMT13. In Proceedings of WMT, Sofia,
Bulgaria.
Abhishek Arun, Phil Blunsom, Chris Dyer, Adam
Lopez, Barry Haddow, and Philipp Koehn. 2010.
Monte Carlo inference and maximization for phrase-
based translation. In Proceedings of CoNLL, Boul-
der, USA.
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011. Goodness: A Method for Measuring Ma-
chine Translation Confidence. In Proceedings of
ACL, Portland, USA.
Graeme Blackwood, Adri`a de Gispert, and William
Byrne. 2010. Fluency Constraints for Minimum
Bayes-Risk Decoding of Statistical Machine Trans-
lation Lattices. In Proceedings of COLING, Beijing,
China.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence Es-
timation for Machine Translation. In Proceedings of
COLING, Geneva, Switzerland.
Simon Carter and Christof Monz. 2011. Syntactic
Discriminative Language Model Rerankers for Sta-
tistical Machine Translation. Machine Translation,
25(4):317–339.
Stanley F. Chen and Joshua T. Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University.
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proceedings of NAACL, Montr´eal, Canada.
Josep M. Crego, Aur´elien Max, and Franc¸ois Yvon.
2010. Local lexical adaptation in Machine Trans-
lation through triangulation: SMT helping SMT. In
Proceedings of COLING, Beijing, China.
Adri`a de Gispert, Graeme Blackwood, Gonzalo Igle-
sias, and William Byrne. 2013. N-gram poste-
rior probability confidence measures for statistical
machine translation: an empirical study. Machine
Translation, 27(2):85–114.
Loic Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical Post-Editing on SYSTRANs Rule-Based
Translation System. In Proceedings of WMT,
Prague, Czech Republic.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast Decoding
and Optimal Decoding for Machine Translation. In
Proceedings of ACL, Toulouse, France.
Kevin Gimpel, Dhruv Batra, Chris Dyer, Gregory
Shakhnarovich, and Virginia Tech. 2013. A Sys-
tematic Exploration of Diversity in Machine Trans-
lation. In Proceedings of EMNLP, Seatlle, USA.
Christian Hardmeier, Joakim Nivre, and Jorg Tiede-
man. 2012. Document-Wide Decoding for Phrase-
Based Statistical Machine Translation. In Proceed-
ings of EMNLP, Jeju Island, Korea.
Saˇsa Hasan and Hermann Ney. 2009. Comparison of
Extended Lexicon Models in Search and Rescoring
for SMT. In Proceedings of NAACL, short papers,
Boulder, USA.
Kevin Knight. 2007. Automatic Language Translation
Generation Help Needs Badly. In MT Summit (in-
vited talk), Copenhagen, Denmark.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of NAACL, Edmonton, Canada.
Philippe Langlais, Alexandre Patry, and Fabrizio Gotti.
2007. A Greedy Decoder for Phrase-Based Statisti-
cal Machine Translation. In Proceedings of Confer-
ence on Theoretical and Methodological Issues in
Machine Translation (TMI), Skovde, Sweden.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc¸ois Yvon. 2011. Structured
Output Layer Neural Network Language Model. In
Proceedings of ICASSP, Prague, Czech Republic.
Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon.
2012. Continuous Space Translation Models with
Neural Networks. In Proceedings of NAACL,
Montr´eal, Canada.
Chin Y. Lin and Franz J. Och. 2004. ORANGE: a
method for evaluating automatic evaluation metrics
for machine translation. In Proceedings of COL-
ING, Geneva, Switzerland.
Ngoc-Quang Luong, Laurent Besacier, and Benjamin
Lecouteux. 2014a. An Efficient Two-Pass Decoder
for SMT Using Word Confidence Estimation. In
Proceedings of EAMT, Dubrovnik, Croatia.
Ngoc-Quang Luong, Laurent Besacier, and Benjamin
Lecouteux. 2014b. Word Confidence Estimation
for SMT N -best List Re-ranking. In Proceedings
of the Workshop on Humans and Computer-assisted
Translation (HaCaT), Gothenburg, Sweden.
Nitin Madnani and Bonnie J. Dorr. 2013. Generat-
ing Targeted Paraphrases for Improved Translation.
ACM Transactions on Intelligent Systems and Tech-
nology, special issue on Paraphrasing, 4(3).
Benjamin Marie and Aur´elien Max. 2013. A Study
in Greedy Oracle Improvement of Translation Hy-
potheses. In Proceedings of IWSLT, Heidelberg,
Germany.
</reference>
<page confidence="0.779288">
1271
</page>
<reference confidence="0.999934901639345">
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004.
A Smorgasbord of Features for Statistical Machine
Translation. In Proceedings of NAACL, Boston,
USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of ACL, Philadelphia, USA.
Kristen Parton, Nizar Habash, Kathleen R. McKeown,
Gonzalo Iglesias, and Adri`a de Gispert. 2012. Can
Automatic Post-editing Make MT more Meaning-
ful? In Proceedings of EAMT, Trento, Italy.
Nicolas P´echeux, Li Gong, Quoc Khanh Do, Ben-
jamin Marie, Yulia Ivanishcheva, Alexander Al-
lauzen, Thomas Lavergne, Jan Niehues, Aur´elien
Max, and Franc¸ois Yvon. 2014. LIMSI @ WMT’14
Medical Translation Task. In Proceedings of WMT,
Baltimore, USA.
Matt Post. 2011. Judging Grammaticality with Tree
Substitution Grammar Derivations. In Proceedings
of ACL, short papers, Portland, USA.
Lane Schwartz, Chris Callison-Burch, William
Schuler, and Stephen Wu. 2011. Incremental
Syntactic Language Models for Phrase-based
Translation. In Proceedings ofACL, Portland, USA.
Holger Schwenk, Daniel D´echelotte, and Jean-Luc
Gauvain. 2006. Continuous Space Language Mod-
els for Statistical Machine Translation. In Proceed-
ings of COLING-ACL, Sydney, Australia.
Michel Simard, Cyril Goutte, and Pierre Isabelle.
2007. Statistical Phrase-based Post-editing. In Pro-
ceedings of NAACL, Rochester, USA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, , and John Makhoul. 2006. A
Study of Translation Edit Rate with Targeted Human
Annotation. In Proceedings of AMTA, Cambridge,
USA.
Lucia Specia, Kashif Shah, Jose G.C. de Souza, and
Trevor Cohn. 2013. QuEst - A Translation Qual-
ity Estimation Framework. In Proceedings of ACL,
System Demonstrations, Sofia, Bulgaria.
Nicola Ueffing and Hermann Ney. 2007. Word-
Level Confidence Estimation for Machine Transla-
tion. Computational Linguistics.
David Vilar, Jia Xu, Luis Fernando D’Haro, and Her-
mann Ney. 2006. Error Analysis of Statistical Ma-
chine Translation Output. In Proceedings of LREC,
Genoa, Italy.
Richard Zens and Hermann Ney. 2006. N -Gram Pos-
terior Probabilities for Statistical Machine Transla-
tion. In Proceedings of WMT, New York, USA.
Ying Zhang, Almut Silja Hildebrand, and Stephan Vo-
gel. 2006. Distributed Language Modeling for N-
best List Re-ranking. In Proceedings of EMNLP,
Sydney, Australia.
Junguo Zhu, Muyun Yang, Sheng Li, and Tiejun Zhao.
2013. Repairing Incorrect Translation with Exam-
ples. In Proceedings of IJCNLP, Nagoya, Japan.
</reference>
<page confidence="0.992659">
1272
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.513498">
<title confidence="0.998951">Confidence-based Rewriting of Machine Translation Output</title>
<author confidence="0.994764">Benjamin Marie Aur´elien Max</author>
<affiliation confidence="0.740238">LIMSI-CNRS, Orsay, France LIMSI-CNRS, Orsay, France</affiliation>
<address confidence="0.584239">Lingua et Machina, Le Chesnay, France Univ. Paris Sud, Orsay, France</address>
<email confidence="0.862322">benjamin.marie@limsi.fraurelien.max@limsi.fr</email>
<abstract confidence="0.999572692307692">Numerous works in Statistical Machine Translation (SMT) have attempted to identify better translation hypotheses obtained by an initial decoding using an improved, but more costly scoring function. In this work, we introduce an approach that takes the hypotheses produced by a state-ofthe-art, reranked phrase-based SMT system, and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phraselevel confidence. In the medical domain, we obtain a 1.9 BLEU improvement over a reranked baseline exploiting the same scoring function, corresponding to a 5.4 BLEU improvement over the orig- We show that if an indication of which phrases require rewriting is provided, our automatic rewriting procedure yields an additional improvement of 1.5 BLEU. Various analyses, including a manual error analysis, further illustrate the good performance and potential for improvement of our approach in spite of its simplicity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandre Allauzen</author>
<author>Nicolas P´echeux</author>
<author>Quoc Khanh Do</author>
<author>Marco Dinarelli</author>
<author>Thomas Lavergne</author>
<author>Aur´elien Max</author>
<author>Hai-son Le</author>
<author>Franc¸ois Yvon</author>
</authors>
<date>2013</date>
<booktitle>LIMSI @ WMT13. In Proceedings of WMT,</booktitle>
<location>Sofia, Bulgaria.</location>
<marker>Allauzen, P´echeux, Do, Dinarelli, Lavergne, Max, Le, Yvon, 2013</marker>
<rawString>Alexandre Allauzen, Nicolas P´echeux, Quoc Khanh Do, Marco Dinarelli, Thomas Lavergne, Aur´elien Max, Hai-son Le, and Franc¸ois Yvon. 2013. LIMSI @ WMT13. In Proceedings of WMT, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhishek Arun</author>
<author>Phil Blunsom</author>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
</authors>
<title>Monte Carlo inference and maximization for phrasebased translation.</title>
<date>2010</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<location>Boulder, USA.</location>
<contexts>
<context position="32535" citStr="Arun et al. (2010)" startWordPosition="5325" endWordPosition="5328">enerating, with an expensive but simple method, diverse hypotheses used as training data. Recently, Luong et al. (2014b) reranked n-best lists using confidence scores at the hypothesis level computed from word-level confidence measures learnt from roughly 10,000 SMT system outputs annotated by humans. Rewriting of translation hypotheses Langlais et al. (2007) described a greedy search decoder, first introduced in (Germann et al., 2001), able to improve translations produced by a dynamic programming decoder using the same scoring function and translation table. However, the more recent work by Arun et al. (2010) using a Gibbs sampler for approximating maximum translation decoding showed the adequacy of the approximations made by state-of-the-art decoders for finding the best translation in their search space. Other works were more directly targeted at automatic post-editing of SMT output, and approached the problem as one of second-pass translation between automatic predictions and correct translations (Simard et al., 2007; Dugast et al., 2007). The recent work of Zhu et al. (2013) attempts to repair translations by exploiting confidence estimates for examples derived from the similarity between sour</context>
</contexts>
<marker>Arun, Blunsom, Dyer, Lopez, Haddow, Koehn, 2010</marker>
<rawString>Abhishek Arun, Phil Blunsom, Chris Dyer, Adam Lopez, Barry Haddow, and Philipp Koehn. 2010. Monte Carlo inference and maximization for phrasebased translation. In Proceedings of CoNLL, Boulder, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nguyen Bach</author>
<author>Fei Huang</author>
<author>Yaser Al-Onaizan</author>
</authors>
<title>Goodness: A Method for Measuring Machine Translation Confidence.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Portland, USA.</location>
<contexts>
<context position="3513" citStr="Bach et al., 2011" startWordPosition="548" endWordPosition="551">itial diversity considered and the fact that it may be in some instances agnostic to the internals of the initial system, it has been shown to potentially improve accuracy of the new translation hypotheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is their capacity to only rewrite incorrect parts of the translation hypotheses and to use appropriate replacement candidates. Many works have tackled the issue of word to n-gram confidence estimation in SMT output (Zens and Ney, 2006; Ueffing and Ney, 2007; Bach et al., 2011; de Gispert et al., 2013), and some attempts have been made to exploit confidence estimates for lattice rescoring (Blackwood et al., 2010) or n-best reranking (Bach et al., 2011; Luong et al., 2014b). In this work, we present an approach in which 1261 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1261–1272, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics new complete hypotheses are produced by rewriting existing hypotheses, and are scored using complex models that could not be used during the initial de</context>
<context position="4747" citStr="Bach et al., 2011" startWordPosition="746" endWordPosition="749">se as competitive baselines systems that rerank the output of an initial decoder using the complete set of available features, and will show that we manage to improve their translation. The difference between our approach and the reranking baseline lies in the manner in which we expand our training data, as well as in our use of high-confidence rewritings to obtain new translation hypotheses. Importantly, this work will only exploit simple confidence estimates corresponding to phrase-based posteriors, which do not require that large sets of human-annotated data be available as in other works (Bach et al., 2011; Luong et al., 2014b). The remainder of this paper is organized as follows. Section 2 is devoted to the description of our approach, with details on our rewriting approach (2.1), additional features (2.2), rewriting phrase table (2.3), and training examples (2.4). Section 3 presents experiments. We first describe our experimental setup (3.1) and our baseline systems (3.2). We then report results when naive rewriting is performed and then with confidencebased rewriting (3.3). We next devote a significant part of the paper in section 4 to report further results and analyses: an analysis of the </context>
<context position="33767" citStr="Bach et al. (2011)" startWordPosition="5514" endWordPosition="5517">e input text and in training examples. Luong et al. (2014a) obtained improvements by computing word confidence estimation, trained on human annotated data, and large sets of lexical, syntactic and semantic features, for the words in the n-best list produced during a first-pass decoding, and performing a second-pass decoding exploiting these new scores. Confidence estimation of Machine Translation The Word Posterior Probability (WPP) proposed by Ueffing and Ney (2007), derived from information from the n-best list produced by a decoder, proved to be useful for estimating word-level confidence. Bach et al. (2011) worked on the issue of predicting sentence-level and word-level MT errors by using WPP and other features derived from the source context, the source-target alignment, and dependency structures, but relied on a significantly large manually annotated corpus of MT errors. De Gispert et al. (2013) calculate k1269 gram posterior probabilities from n-best lists or word lattices, and demonstrated that they were reasonably accurate indications of whether specific kgrams would be found or not in human reference translations. Finally, the work of Blackwood et al. (2010) proposed to segment translation</context>
<context position="35307" citStr="Bach et al., 2011" startWordPosition="5752" endWordPosition="5755">is paper, we have described an approach that improves translations a posteriori by applying simple local rewritings. We have shown that the quality of phrase-level confidence estimates has a direct impact of the amplitude of the improvements that can be obtained, as well as the initial quality of the rewritten hypotheses. We have used a very simple definition for confidence estimates under the form of phrase posteriors estimated from n-best lists from an initial decoder, which obtained good empirical performance, in spite of not requiring large human-annotated datasets as in other approaches (Bach et al., 2011; Luong et al., 2014b). Our work could be extended in several directions. First, we could use a larger set of rewriting operations (Langlais et al., 2007), including the rewrite (sic) operation introduced in (Marie and Max, 2013) that paraphrases source phrases and then translates them. We could also possibly consider any phrase segmentation compatible with a specific word alignment rather than rely on specific phrase segmentations. This would allow us to attain faster some rewritings that could otherwise require several rewriting iterations and may never be attained by the greedy procedure. M</context>
</contexts>
<marker>Bach, Huang, Al-Onaizan, 2011</marker>
<rawString>Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. 2011. Goodness: A Method for Measuring Machine Translation Confidence. In Proceedings of ACL, Portland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Blackwood</author>
<author>Adri`a de Gispert</author>
<author>William Byrne</author>
</authors>
<title>Fluency Constraints for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Beijing, China.</location>
<marker>Blackwood, de Gispert, Byrne, 2010</marker>
<rawString>Graeme Blackwood, Adri`a de Gispert, and William Byrne. 2010. Fluency Constraints for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices. In Proceedings of COLING, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence Estimation for Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="31548" citStr="Blatz et al., 2004" startWordPosition="5171" endWordPosition="5174"> analysis for the first 70 test sentences. System fr-en en-fr BLEU TER BLEU TER 1-pass Moses 32.5 47.7 32.3 49.9 reranker 33.0 47.3 32.8 49.4 rewriter 33.4(+0.4) 47.4(+0.1) 33.7(+0.9) 49.3(−0.1) semi-oracle 34.1(+1.1) 46.6(−0.7) 34.2(+1.4) 48.6(−0.8) Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks. 5 Related work Reranking of translation hypotheses n-best list reranking was extensively studied in (Och et al., 2004), using features not used in the initial decoder such as IBM1 scores (which also proved useful for word-level confidence estimation (Blatz et al., 2004)) and generative syntactic models. While the experiments in (Och et al., 2004) did not show any clear contribution of syntactic information used in this manner, the later work by Carter and Monz (2011) managed to successfully exploit syntactic features using discriminative language modeling for n-best reranking. Gimpel et al. (2013) outperformed n-best reranking by generating, with an expensive but simple method, diverse hypotheses used as training data. Recently, Luong et al. (2014b) reranked n-best lists using confidence scores at the hypothesis level computed from word-level confidence meas</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2004</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2004. Confidence Estimation for Machine Translation. In Proceedings of COLING, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Carter</author>
<author>Christof Monz</author>
</authors>
<title>Syntactic Discriminative Language Model Rerankers for Statistical Machine Translation.</title>
<date>2011</date>
<journal>Machine Translation,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="2226" citStr="Carter and Monz, 2011" startWordPosition="334" endWordPosition="337">n quality. For instance, improvements have been obtained by integrating features into decoding that better model semantic coherence at the sentence level (Hasan and Ney, 2009) or syntactic well-formedness (Schwartz et al., 2011). However, early use of such complex features typically comes at a high computational cost. Moreover, some informative features require or are better computed when complete translation hypotheses are available. This is addressed in numerous works on reranking of the highest scored sub-space of hypotheses, on so-called n-best lists (Och et al., 2004; Zhang et al., 2006; Carter and Monz, 2011) or output lattices (Schwenk et al., 2006; Blackwood et al., 2010), where many works specifically target the inclusion of better language modelling capabilities, a well-known weakness of current automatic generation approaches (Knight, 2007). Another way to improve translation a posteriori can be done by rewriting initial hypotheses, for instance in a greedy fashion by including new models (Langlais et al., 2007; Hardmeier et al., 2012), or by specifically modeling a task of automatic post-editing targeting a specific system (Simard et al., 2007; Dugast et al., 2007). While such automatic post</context>
<context position="9754" citStr="Carter and Monz, 2011" startWordPosition="1567" endWordPosition="1570"> output layer neural network language models (LMs) which have been shown to be useful in reranking tasks, for instance for WMT evaluations (Allauzen et al., 2013; P´echeux et al., 2014). SOUL scoring being too costly to be integrated during decoding, it fits perfectly the reranker scenario, which furthermore enables to use larger contexts for n-grams. We used both monolingual (Le et al., 2011) and bilingual (Le et al., 2012) SOUL 10-gram models, which were trained on the WMT’12 data. • POS language model: part-of-speech (POS) LMs have been shown to yield improvements in n-best list reranking (Carter and Monz, 2011). In this work, we trained a 6-gram POS LM using Witten-Bell smoothing. • IBM1 : the IBM1 scores (p(e|f) and p(f|e)) of the complete hypothesis (Och et al., 2004). • phrase-based confidence score: bi-phrases are associated to a posterior probability, inspired from n-gram posterior probability estimation as defined in (de Gispert et al., 2013). Let E be the set of all hypotheses in the space of translation hypotheses defined by 2Note that we did not try to explore the independant contribution of each feature in this work. the n-best list used for source sentence f, and Eα be the subset of E suc</context>
<context position="31749" citStr="Carter and Monz (2011)" startWordPosition="5205" endWordPosition="5208">i-oracle 34.1(+1.1) 46.6(−0.7) 34.2(+1.4) 48.6(−0.8) Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks. 5 Related work Reranking of translation hypotheses n-best list reranking was extensively studied in (Och et al., 2004), using features not used in the initial decoder such as IBM1 scores (which also proved useful for word-level confidence estimation (Blatz et al., 2004)) and generative syntactic models. While the experiments in (Och et al., 2004) did not show any clear contribution of syntactic information used in this manner, the later work by Carter and Monz (2011) managed to successfully exploit syntactic features using discriminative language modeling for n-best reranking. Gimpel et al. (2013) outperformed n-best reranking by generating, with an expensive but simple method, diverse hypotheses used as training data. Recently, Luong et al. (2014b) reranked n-best lists using confidence scores at the hypothesis level computed from word-level confidence measures learnt from roughly 10,000 SMT system outputs annotated by humans. Rewriting of translation hypotheses Langlais et al. (2007) described a greedy search decoder, first introduced in (Germann et al.</context>
</contexts>
<marker>Carter, Monz, 2011</marker>
<rawString>Simon Carter and Christof Monz. 2011. Syntactic Discriminative Language Model Rerankers for Statistical Machine Translation. Machine Translation, 25(4):317–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua T Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Computer Science Group, Harvard University.</institution>
<contexts>
<context position="13666" citStr="Chen and Goodman, 1998" startWordPosition="2235" endWordPosition="2238">cal task we used only the English to French translation direction, and both translation directions, English to French and French to English, for the TED Talks task. In this work, the main part of our experiments uses Medical, and TED Talks will be used at a later stage to study a lower-quality situation (cf. 4.5). For the Medical task, initial decodings were produced using a LM trained on all WMT’14 monolingual and bilingual medical data, while for the TED Talks task we used a much larger LM trained on all the data provided for WMT’135. Both are 4-gram LMs estimated with Kneser-Ney smoothing (Chen and Goodman, 1998). For the 6-gram POS LMs used (see 2.2), we used the same data as used for the token-based LM for Medical, and the concatenation of the News Commentaries and Europarl sub-parts of the WMT’13 data for TED Talks. Table 1 provides relevant statistics about the data used. Tasks Corpus Sentences Tokens(en-fr) train 4.9M 78M - 91M Medical dev 500 10k - 12k test 1,000 21k - 26k LM - 146M train 107 758 2M - 2.2M TED Talks dev 934 20k - 20k test 1,664 31k - 34k LM 6B - 2.5B Table 1: Corpora used in this work. 3http://www.statmt.org/wmt14/ medical-task/ 4https://wit3.fbk.eu/mt.php?release= 2013-01 5http</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua T. Goodman. 1998. An Empirical Study of Smoothing Techniques for Language Modeling. Technical Report TR-10-98, Computer Science Group, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch Tuning Strategies for Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="8988" citStr="Cherry and Foster, 2012" startWordPosition="1442" endWordPosition="1445"> search terminates when no better hypothesis is found. Such a greedy search has several obvious limitations, in particular it can only perform a limited exploration of the search space, a situation that can be improved by using a beam (see Section 3.3). However, associated with a small and precise rewriting phrase table, this approach only visits small numbers of more-confident hypotheses, which is a critical property given the cost of computing the new scoring function used. 2.2 Reranking and features The rerankings of the hypotheses sets describe in this work are all performed with kb-mira (Cherry and Foster, 2012) using the initial features set of the decoder in conjunction with the following additional features:2 • SOUL models: SOUL models are structured output layer neural network language models (LMs) which have been shown to be useful in reranking tasks, for instance for WMT evaluations (Allauzen et al., 2013; P´echeux et al., 2014). SOUL scoring being too costly to be integrated during decoding, it fits perfectly the reranker scenario, which furthermore enables to use larger contexts for n-grams. We used both monolingual (Le et al., 2011) and bilingual (Le et al., 2012) SOUL 10-gram models, which </context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch Tuning Strategies for Statistical Machine Translation. In Proceedings of NAACL, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Aur´elien Max</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Local lexical adaptation in Machine Translation through triangulation: SMT helping SMT.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="37957" citStr="Crego et al., 2010" startWordPosition="6184" endWordPosition="6187">r phrase is correct or not, our rewriter system could be used to determine automatically whether a rewriting system could (if asked to) attempt to improve locally a translation, or whether a human post-editor should already tackle working on improving it. As we showed in our manual error analysis in section 4.4, there are in fact many instances of errors that could not be recovered by our approach, be it because of its local rewriting strategy or of the bilingual resources or models used, so that some knowledge would have to be provided as hard constraints by a human translator, as hinted in (Crego et al., 2010). We could then finally have our rewriter system work in a turn-based fashion in collaboration with a human translator, fixing errors or making improvements that are being made possible by the last edits from the translator. Acknowledgments The authors would like to thank the anonymous reviewers and Guillaume Wisniewski for their useful remarks. Additional thanks go to Hai Son Le for “anticipating” the need for a large and efficient cache in his SOUL implementation, Quoc Khanh Do for his assistance on using SOUL, and Li Gong and Nicolas P´echeux for providing the authors with data used in the </context>
</contexts>
<marker>Crego, Max, Yvon, 2010</marker>
<rawString>Josep M. Crego, Aur´elien Max, and Franc¸ois Yvon. 2010. Local lexical adaptation in Machine Translation through triangulation: SMT helping SMT. In Proceedings of COLING, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a de Gispert</author>
<author>Graeme Blackwood</author>
<author>Gonzalo Iglesias</author>
<author>William Byrne</author>
</authors>
<title>N-gram posterior probability confidence measures for statistical machine translation: an empirical study.</title>
<date>2013</date>
<journal>Machine Translation,</journal>
<volume>27</volume>
<issue>2</issue>
<marker>de Gispert, Blackwood, Iglesias, Byrne, 2013</marker>
<rawString>Adri`a de Gispert, Graeme Blackwood, Gonzalo Iglesias, and William Byrne. 2013. N-gram posterior probability confidence measures for statistical machine translation: an empirical study. Machine Translation, 27(2):85–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Loic Dugast</author>
<author>Jean Senellart</author>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Post-Editing on SYSTRANs Rule-Based Translation System. In</title>
<date>2007</date>
<booktitle>Proceedings of WMT,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2799" citStr="Dugast et al., 2007" startWordPosition="422" endWordPosition="425">004; Zhang et al., 2006; Carter and Monz, 2011) or output lattices (Schwenk et al., 2006; Blackwood et al., 2010), where many works specifically target the inclusion of better language modelling capabilities, a well-known weakness of current automatic generation approaches (Knight, 2007). Another way to improve translation a posteriori can be done by rewriting initial hypotheses, for instance in a greedy fashion by including new models (Langlais et al., 2007; Hardmeier et al., 2012), or by specifically modeling a task of automatic post-editing targeting a specific system (Simard et al., 2007; Dugast et al., 2007). While such automatic post-editing may seem to be too limited, notably because of the limited initial diversity considered and the fact that it may be in some instances agnostic to the internals of the initial system, it has been shown to potentially improve accuracy of the new translation hypotheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is their capacity to only rewrite incorrect parts of the translation hypotheses and to use appropriate replacement candidates. Many works have tackled the issue o</context>
<context position="32976" citStr="Dugast et al., 2007" startWordPosition="5391" endWordPosition="5394">, able to improve translations produced by a dynamic programming decoder using the same scoring function and translation table. However, the more recent work by Arun et al. (2010) using a Gibbs sampler for approximating maximum translation decoding showed the adequacy of the approximations made by state-of-the-art decoders for finding the best translation in their search space. Other works were more directly targeted at automatic post-editing of SMT output, and approached the problem as one of second-pass translation between automatic predictions and correct translations (Simard et al., 2007; Dugast et al., 2007). The recent work of Zhu et al. (2013) attempts to repair translations by exploiting confidence estimates for examples derived from the similarity between source words in the input text and in training examples. Luong et al. (2014a) obtained improvements by computing word confidence estimation, trained on human annotated data, and large sets of lexical, syntactic and semantic features, for the words in the n-best list produced during a first-pass decoding, and performing a second-pass decoding exploiting these new scores. Confidence estimation of Machine Translation The Word Posterior Probabil</context>
</contexts>
<marker>Dugast, Senellart, Koehn, 2007</marker>
<rawString>Loic Dugast, Jean Senellart, and Philipp Koehn. 2007. Statistical Post-Editing on SYSTRANs Rule-Based Translation System. In Proceedings of WMT, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast Decoding and Optimal Decoding for Machine Translation.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="32356" citStr="Germann et al., 2001" startWordPosition="5294" endWordPosition="5297">nd Monz (2011) managed to successfully exploit syntactic features using discriminative language modeling for n-best reranking. Gimpel et al. (2013) outperformed n-best reranking by generating, with an expensive but simple method, diverse hypotheses used as training data. Recently, Luong et al. (2014b) reranked n-best lists using confidence scores at the hypothesis level computed from word-level confidence measures learnt from roughly 10,000 SMT system outputs annotated by humans. Rewriting of translation hypotheses Langlais et al. (2007) described a greedy search decoder, first introduced in (Germann et al., 2001), able to improve translations produced by a dynamic programming decoder using the same scoring function and translation table. However, the more recent work by Arun et al. (2010) using a Gibbs sampler for approximating maximum translation decoding showed the adequacy of the approximations made by state-of-the-art decoders for finding the best translation in their search space. Other works were more directly targeted at automatic post-editing of SMT output, and approached the problem as one of second-pass translation between automatic predictions and correct translations (Simard et al., 2007; </context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast Decoding and Optimal Decoding for Machine Translation. In Proceedings of ACL, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Dhruv Batra</author>
<author>Chris Dyer</author>
<author>Gregory Shakhnarovich</author>
<author>Virginia Tech</author>
</authors>
<title>A Systematic Exploration of Diversity in Machine Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Seatlle, USA.</location>
<contexts>
<context position="31882" citStr="Gimpel et al. (2013)" startWordPosition="5223" endWordPosition="5226"> the TED Talks. 5 Related work Reranking of translation hypotheses n-best list reranking was extensively studied in (Och et al., 2004), using features not used in the initial decoder such as IBM1 scores (which also proved useful for word-level confidence estimation (Blatz et al., 2004)) and generative syntactic models. While the experiments in (Och et al., 2004) did not show any clear contribution of syntactic information used in this manner, the later work by Carter and Monz (2011) managed to successfully exploit syntactic features using discriminative language modeling for n-best reranking. Gimpel et al. (2013) outperformed n-best reranking by generating, with an expensive but simple method, diverse hypotheses used as training data. Recently, Luong et al. (2014b) reranked n-best lists using confidence scores at the hypothesis level computed from word-level confidence measures learnt from roughly 10,000 SMT system outputs annotated by humans. Rewriting of translation hypotheses Langlais et al. (2007) described a greedy search decoder, first introduced in (Germann et al., 2001), able to improve translations produced by a dynamic programming decoder using the same scoring function and translation table</context>
</contexts>
<marker>Gimpel, Batra, Dyer, Shakhnarovich, Tech, 2013</marker>
<rawString>Kevin Gimpel, Dhruv Batra, Chris Dyer, Gregory Shakhnarovich, and Virginia Tech. 2013. A Systematic Exploration of Diversity in Machine Translation. In Proceedings of EMNLP, Seatlle, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Hardmeier</author>
<author>Joakim Nivre</author>
<author>Jorg Tiedeman</author>
</authors>
<title>Document-Wide Decoding for PhraseBased Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP, Jeju Island,</booktitle>
<contexts>
<context position="2666" citStr="Hardmeier et al., 2012" startWordPosition="401" endWordPosition="404">This is addressed in numerous works on reranking of the highest scored sub-space of hypotheses, on so-called n-best lists (Och et al., 2004; Zhang et al., 2006; Carter and Monz, 2011) or output lattices (Schwenk et al., 2006; Blackwood et al., 2010), where many works specifically target the inclusion of better language modelling capabilities, a well-known weakness of current automatic generation approaches (Knight, 2007). Another way to improve translation a posteriori can be done by rewriting initial hypotheses, for instance in a greedy fashion by including new models (Langlais et al., 2007; Hardmeier et al., 2012), or by specifically modeling a task of automatic post-editing targeting a specific system (Simard et al., 2007; Dugast et al., 2007). While such automatic post-editing may seem to be too limited, notably because of the limited initial diversity considered and the fact that it may be in some instances agnostic to the internals of the initial system, it has been shown to potentially improve accuracy of the new translation hypotheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is their capacity to only rew</context>
<context position="36060" citStr="Hardmeier et al., 2012" startWordPosition="5871" endWordPosition="5874"> (Langlais et al., 2007), including the rewrite (sic) operation introduced in (Marie and Max, 2013) that paraphrases source phrases and then translates them. We could also possibly consider any phrase segmentation compatible with a specific word alignment rather than rely on specific phrase segmentations. This would allow us to attain faster some rewritings that could otherwise require several rewriting iterations and may never be attained by the greedy procedure. More features could also be used, for instance to model more fine-grained syntax (Post, 2011) or document-level lexical coherence (Hardmeier et al., 2012). However, anticipating that some features might be very expensive to compute, we could adapt our procedure to work in several passes: initial passes would tend to restrict the search space more and more using an initial set of features, before a more expensive pass would concentrate on a limited number of hypotheses. Figure 1 indeed already showed a much faster oracle improvement between 1-pass Moses and reranker for n-best list of small sizes. Another avenue for improvement lies in the possibility to perform the training of our rewriter by providing it with more reference translations. As th</context>
</contexts>
<marker>Hardmeier, Nivre, Tiedeman, 2012</marker>
<rawString>Christian Hardmeier, Joakim Nivre, and Jorg Tiedeman. 2012. Document-Wide Decoding for PhraseBased Statistical Machine Translation. In Proceedings of EMNLP, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saˇsa Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>Comparison of Extended Lexicon Models in Search and Rescoring for SMT.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL, short papers,</booktitle>
<location>Boulder, USA.</location>
<contexts>
<context position="1779" citStr="Hasan and Ney, 2009" startWordPosition="266" endWordPosition="269">ance and potential for improvement of our approach in spite of its simplicity. 1 Introduction The standard configuration of modern phrasebased Statistical Machine Translation (SMT) (Koehn et al., 2003) systems can produce very acceptable results on some tasks. However, early integration of better features to guide the search for the best hypothesis can result in significant improvements, an expression of the complexity of modeling translation quality. For instance, improvements have been obtained by integrating features into decoding that better model semantic coherence at the sentence level (Hasan and Ney, 2009) or syntactic well-formedness (Schwartz et al., 2011). However, early use of such complex features typically comes at a high computational cost. Moreover, some informative features require or are better computed when complete translation hypotheses are available. This is addressed in numerous works on reranking of the highest scored sub-space of hypotheses, on so-called n-best lists (Och et al., 2004; Zhang et al., 2006; Carter and Monz, 2011) or output lattices (Schwenk et al., 2006; Blackwood et al., 2010), where many works specifically target the inclusion of better language modelling capab</context>
</contexts>
<marker>Hasan, Ney, 2009</marker>
<rawString>Saˇsa Hasan and Hermann Ney. 2009. Comparison of Extended Lexicon Models in Search and Rescoring for SMT. In Proceedings of NAACL, short papers, Boulder, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Automatic Language Translation Generation Help Needs Badly.</title>
<date>2007</date>
<booktitle>In MT Summit (invited talk),</booktitle>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="2467" citStr="Knight, 2007" startWordPosition="369" endWordPosition="370">f such complex features typically comes at a high computational cost. Moreover, some informative features require or are better computed when complete translation hypotheses are available. This is addressed in numerous works on reranking of the highest scored sub-space of hypotheses, on so-called n-best lists (Och et al., 2004; Zhang et al., 2006; Carter and Monz, 2011) or output lattices (Schwenk et al., 2006; Blackwood et al., 2010), where many works specifically target the inclusion of better language modelling capabilities, a well-known weakness of current automatic generation approaches (Knight, 2007). Another way to improve translation a posteriori can be done by rewriting initial hypotheses, for instance in a greedy fashion by including new models (Langlais et al., 2007; Hardmeier et al., 2012), or by specifically modeling a task of automatic post-editing targeting a specific system (Simard et al., 2007; Dugast et al., 2007). While such automatic post-editing may seem to be too limited, notably because of the limited initial diversity considered and the fact that it may be in some instances agnostic to the internals of the initial system, it has been shown to potentially improve accuracy</context>
</contexts>
<marker>Knight, 2007</marker>
<rawString>Kevin Knight. 2007. Automatic Language Translation Generation Help Needs Badly. In MT Summit (invited talk), Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1360" citStr="Koehn et al., 2003" startWordPosition="200" endWordPosition="203">, we obtain a 1.9 BLEU improvement over a reranked baseline exploiting the same scoring function, corresponding to a 5.4 BLEU improvement over the original Moses baseline. We show that if an indication of which phrases require rewriting is provided, our automatic rewriting procedure yields an additional improvement of 1.5 BLEU. Various analyses, including a manual error analysis, further illustrate the good performance and potential for improvement of our approach in spite of its simplicity. 1 Introduction The standard configuration of modern phrasebased Statistical Machine Translation (SMT) (Koehn et al., 2003) systems can produce very acceptable results on some tasks. However, early integration of better features to guide the search for the best hypothesis can result in significant improvements, an expression of the complexity of modeling translation quality. For instance, improvements have been obtained by integrating features into decoding that better model semantic coherence at the sentence level (Hasan and Ney, 2009) or syntactic well-formedness (Schwartz et al., 2011). However, early use of such complex features typically comes at a high computational cost. Moreover, some informative features </context>
<context position="14380" citStr="Koehn et al., 2003" startWordPosition="2353" endWordPosition="2356">or Medical, and the concatenation of the News Commentaries and Europarl sub-parts of the WMT’13 data for TED Talks. Table 1 provides relevant statistics about the data used. Tasks Corpus Sentences Tokens(en-fr) train 4.9M 78M - 91M Medical dev 500 10k - 12k test 1,000 21k - 26k LM - 146M train 107 758 2M - 2.2M TED Talks dev 934 20k - 20k test 1,664 31k - 34k LM 6B - 2.5B Table 1: Corpora used in this work. 3http://www.statmt.org/wmt14/ medical-task/ 4https://wit3.fbk.eu/mt.php?release= 2013-01 5http://www.statmt.org/wmt13 We first built a state-of-the-art phrase-based SMT system using Moses (Koehn et al., 2003) with standard settings. We tuned its parameters towards BLEU (Papineni et al., 2002) on the tuning dataset using the kb-mira implementation available in Moses with default parameters. Our results will be compared using BLEU and TER (Snover et al., 2006) to a) the initial best translation produced by the Moses decoder (1-pass Moses) and b) the best translation obtained by reranking the 1,000-best list of 1-pass Moses (reranker). Since reranker implements a well-documented approach and uses types of features commonly used in reranking tasks we will consider it as our main baseline. It was train</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of NAACL, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Langlais</author>
<author>Alexandre Patry</author>
<author>Fabrizio Gotti</author>
</authors>
<title>A Greedy Decoder for Phrase-Based Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of Conference on Theoretical and Methodological Issues in Machine Translation (TMI), Skovde,</booktitle>
<contexts>
<context position="2641" citStr="Langlais et al., 2007" startWordPosition="397" endWordPosition="400">otheses are available. This is addressed in numerous works on reranking of the highest scored sub-space of hypotheses, on so-called n-best lists (Och et al., 2004; Zhang et al., 2006; Carter and Monz, 2011) or output lattices (Schwenk et al., 2006; Blackwood et al., 2010), where many works specifically target the inclusion of better language modelling capabilities, a well-known weakness of current automatic generation approaches (Knight, 2007). Another way to improve translation a posteriori can be done by rewriting initial hypotheses, for instance in a greedy fashion by including new models (Langlais et al., 2007; Hardmeier et al., 2012), or by specifically modeling a task of automatic post-editing targeting a specific system (Simard et al., 2007; Dugast et al., 2007). While such automatic post-editing may seem to be too limited, notably because of the limited initial diversity considered and the fact that it may be in some instances agnostic to the internals of the initial system, it has been shown to potentially improve accuracy of the new translation hypotheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is t</context>
<context position="5937" citStr="Langlais et al (2007)" startWordPosition="941" endWordPosition="944">d analyses: an analysis of the performance of our system depending on the quality of initial hypotheses (4.1); a semi-oracle experiment where correct phrases are known (4.2); an oracle experiment where only correct rewriting decisions are made (4.3); a manual error analysis of the main configurations studied in this work (4.4); and, finally, a study of the performance of our approach on a more difficult translation task (4.5). Related work is discussed in section 5 and we conclude and introduce our future work in section 6. 2 Description of the approach 2.1 Rewriting of translation hypotheses Langlais et al (2007) proposed a greedy search procedure to improve translations by reusing the same translation table and scoring function that were used during an initial phrase-based decoding. In our approach, we rewrite hypotheses by using the same greedy search algorithm, adding more complex models and using the most-confident biphrases according to the initial decoder’s search space. To select the hypothesis to rewrite for each sentence, we produce a n-best list of the initial decoder and rerank this list with a new, better informed scoring function (see section 2.2). The one-best hypothesis obtained after r</context>
<context position="32278" citStr="Langlais et al. (2007)" startWordPosition="5282" endWordPosition="5285">bution of syntactic information used in this manner, the later work by Carter and Monz (2011) managed to successfully exploit syntactic features using discriminative language modeling for n-best reranking. Gimpel et al. (2013) outperformed n-best reranking by generating, with an expensive but simple method, diverse hypotheses used as training data. Recently, Luong et al. (2014b) reranked n-best lists using confidence scores at the hypothesis level computed from word-level confidence measures learnt from roughly 10,000 SMT system outputs annotated by humans. Rewriting of translation hypotheses Langlais et al. (2007) described a greedy search decoder, first introduced in (Germann et al., 2001), able to improve translations produced by a dynamic programming decoder using the same scoring function and translation table. However, the more recent work by Arun et al. (2010) using a Gibbs sampler for approximating maximum translation decoding showed the adequacy of the approximations made by state-of-the-art decoders for finding the best translation in their search space. Other works were more directly targeted at automatic post-editing of SMT output, and approached the problem as one of second-pass translation</context>
<context position="35461" citStr="Langlais et al., 2007" startWordPosition="5780" endWordPosition="5783">f phrase-level confidence estimates has a direct impact of the amplitude of the improvements that can be obtained, as well as the initial quality of the rewritten hypotheses. We have used a very simple definition for confidence estimates under the form of phrase posteriors estimated from n-best lists from an initial decoder, which obtained good empirical performance, in spite of not requiring large human-annotated datasets as in other approaches (Bach et al., 2011; Luong et al., 2014b). Our work could be extended in several directions. First, we could use a larger set of rewriting operations (Langlais et al., 2007), including the rewrite (sic) operation introduced in (Marie and Max, 2013) that paraphrases source phrases and then translates them. We could also possibly consider any phrase segmentation compatible with a specific word alignment rather than rely on specific phrase segmentations. This would allow us to attain faster some rewritings that could otherwise require several rewriting iterations and may never be attained by the greedy procedure. More features could also be used, for instance to model more fine-grained syntax (Post, 2011) or document-level lexical coherence (Hardmeier et al., 2012).</context>
</contexts>
<marker>Langlais, Patry, Gotti, 2007</marker>
<rawString>Philippe Langlais, Alexandre Patry, and Fabrizio Gotti. 2007. A Greedy Decoder for Phrase-Based Statistical Machine Translation. In Proceedings of Conference on Theoretical and Methodological Issues in Machine Translation (TMI), Skovde, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Ilya Oparin</author>
<author>Alexandre Allauzen</author>
<author>JeanLuc Gauvain</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Structured Output Layer Neural Network Language Model.</title>
<date>2011</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="9528" citStr="Le et al., 2011" startWordPosition="1530" endWordPosition="1533">scribe in this work are all performed with kb-mira (Cherry and Foster, 2012) using the initial features set of the decoder in conjunction with the following additional features:2 • SOUL models: SOUL models are structured output layer neural network language models (LMs) which have been shown to be useful in reranking tasks, for instance for WMT evaluations (Allauzen et al., 2013; P´echeux et al., 2014). SOUL scoring being too costly to be integrated during decoding, it fits perfectly the reranker scenario, which furthermore enables to use larger contexts for n-grams. We used both monolingual (Le et al., 2011) and bilingual (Le et al., 2012) SOUL 10-gram models, which were trained on the WMT’12 data. • POS language model: part-of-speech (POS) LMs have been shown to yield improvements in n-best list reranking (Carter and Monz, 2011). In this work, we trained a 6-gram POS LM using Witten-Bell smoothing. • IBM1 : the IBM1 scores (p(e|f) and p(f|e)) of the complete hypothesis (Och et al., 2004). • phrase-based confidence score: bi-phrases are associated to a posterior probability, inspired from n-gram posterior probability estimation as defined in (de Gispert et al., 2013). Let E be the set of all hypo</context>
</contexts>
<marker>Le, Oparin, Allauzen, Gauvain, Yvon, 2011</marker>
<rawString>Hai-Son Le, Ilya Oparin, Alexandre Allauzen, JeanLuc Gauvain, and Franc¸ois Yvon. 2011. Structured Output Layer Neural Network Language Model. In Proceedings of ICASSP, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Continuous Space Translation Models with Neural Networks.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="9560" citStr="Le et al., 2012" startWordPosition="1536" endWordPosition="1539">ormed with kb-mira (Cherry and Foster, 2012) using the initial features set of the decoder in conjunction with the following additional features:2 • SOUL models: SOUL models are structured output layer neural network language models (LMs) which have been shown to be useful in reranking tasks, for instance for WMT evaluations (Allauzen et al., 2013; P´echeux et al., 2014). SOUL scoring being too costly to be integrated during decoding, it fits perfectly the reranker scenario, which furthermore enables to use larger contexts for n-grams. We used both monolingual (Le et al., 2011) and bilingual (Le et al., 2012) SOUL 10-gram models, which were trained on the WMT’12 data. • POS language model: part-of-speech (POS) LMs have been shown to yield improvements in n-best list reranking (Carter and Monz, 2011). In this work, we trained a 6-gram POS LM using Witten-Bell smoothing. • IBM1 : the IBM1 scores (p(e|f) and p(f|e)) of the complete hypothesis (Och et al., 2004). • phrase-based confidence score: bi-phrases are associated to a posterior probability, inspired from n-gram posterior probability estimation as defined in (de Gispert et al., 2013). Let E be the set of all hypotheses in the space of translati</context>
</contexts>
<marker>Le, Allauzen, Yvon, 2012</marker>
<rawString>Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon. 2012. Continuous Space Translation Models with Neural Networks. In Proceedings of NAACL, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin Y Lin</author>
<author>Franz J Och</author>
</authors>
<title>ORANGE: a method for evaluating automatic evaluation metrics for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="20385" citStr="Lin and Och, 2004" startWordPosition="3358" endWordPosition="3361">score on the test set. 12 We did not experiment with higher values of k, but plan to use the output lattice produced by 1-pass Moses to compute efficiently posteriors for larger sets of bi-phrases (de Gispert et al., 2013). As illustrated on Figure 2b, rewriter mostly improves the BLEU score during the three first iterations and then converges at the ninth iteration. However, it is important to note that not all sentences are actually improved by our system. As illustrated on Figure 3a, opti improves 40.8% of the sentences of the test set but degrades 29.2% of them according to sentence-BLEU (Lin and Och, 2004). It is certainly the case that more informative confidence features may help idenfity more precisely which fragments of the translations should really undergo rewriting. We will investigate the exploitation of an oracle phrase-based confidence measure in Section 4.2. 11Using a beam becomes quickly prohibitive: using 12 threads, 25 mn vs. 3h were needed for the test set for the configurations of size 1 and 10, respectively. 12Note that even for k = 10, 000 the computed neighborhoods are still quite small with an average of 116 hypotheses for each hypothesis to rewrite per iteration, against an</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin Y. Lin and Franz J. Och. 2004. ORANGE: a method for evaluating automatic evaluation metrics for machine translation. In Proceedings of COLING, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ngoc-Quang Luong</author>
<author>Laurent Besacier</author>
<author>Benjamin Lecouteux</author>
</authors>
<title>An Efficient Two-Pass Decoder for SMT Using Word Confidence Estimation.</title>
<date>2014</date>
<booktitle>In Proceedings of EAMT,</booktitle>
<location>Dubrovnik, Croatia.</location>
<contexts>
<context position="3711" citStr="Luong et al., 2014" startWordPosition="582" endWordPosition="585">potheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is their capacity to only rewrite incorrect parts of the translation hypotheses and to use appropriate replacement candidates. Many works have tackled the issue of word to n-gram confidence estimation in SMT output (Zens and Ney, 2006; Ueffing and Ney, 2007; Bach et al., 2011; de Gispert et al., 2013), and some attempts have been made to exploit confidence estimates for lattice rescoring (Blackwood et al., 2010) or n-best reranking (Bach et al., 2011; Luong et al., 2014b). In this work, we present an approach in which 1261 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1261–1272, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics new complete hypotheses are produced by rewriting existing hypotheses, and are scored using complex models that could not be used during the initial decoding. We will use as competitive baselines systems that rerank the output of an initial decoder using the complete set of available features, and will show that we manage to improve their translat</context>
<context position="32035" citStr="Luong et al. (2014" startWordPosition="5247" endWordPosition="5250">used in the initial decoder such as IBM1 scores (which also proved useful for word-level confidence estimation (Blatz et al., 2004)) and generative syntactic models. While the experiments in (Och et al., 2004) did not show any clear contribution of syntactic information used in this manner, the later work by Carter and Monz (2011) managed to successfully exploit syntactic features using discriminative language modeling for n-best reranking. Gimpel et al. (2013) outperformed n-best reranking by generating, with an expensive but simple method, diverse hypotheses used as training data. Recently, Luong et al. (2014b) reranked n-best lists using confidence scores at the hypothesis level computed from word-level confidence measures learnt from roughly 10,000 SMT system outputs annotated by humans. Rewriting of translation hypotheses Langlais et al. (2007) described a greedy search decoder, first introduced in (Germann et al., 2001), able to improve translations produced by a dynamic programming decoder using the same scoring function and translation table. However, the more recent work by Arun et al. (2010) using a Gibbs sampler for approximating maximum translation decoding showed the adequacy of the app</context>
<context position="35327" citStr="Luong et al., 2014" startWordPosition="5756" endWordPosition="5759">escribed an approach that improves translations a posteriori by applying simple local rewritings. We have shown that the quality of phrase-level confidence estimates has a direct impact of the amplitude of the improvements that can be obtained, as well as the initial quality of the rewritten hypotheses. We have used a very simple definition for confidence estimates under the form of phrase posteriors estimated from n-best lists from an initial decoder, which obtained good empirical performance, in spite of not requiring large human-annotated datasets as in other approaches (Bach et al., 2011; Luong et al., 2014b). Our work could be extended in several directions. First, we could use a larger set of rewriting operations (Langlais et al., 2007), including the rewrite (sic) operation introduced in (Marie and Max, 2013) that paraphrases source phrases and then translates them. We could also possibly consider any phrase segmentation compatible with a specific word alignment rather than rely on specific phrase segmentations. This would allow us to attain faster some rewritings that could otherwise require several rewriting iterations and may never be attained by the greedy procedure. More features could a</context>
</contexts>
<marker>Luong, Besacier, Lecouteux, 2014</marker>
<rawString>Ngoc-Quang Luong, Laurent Besacier, and Benjamin Lecouteux. 2014a. An Efficient Two-Pass Decoder for SMT Using Word Confidence Estimation. In Proceedings of EAMT, Dubrovnik, Croatia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ngoc-Quang Luong</author>
<author>Laurent Besacier</author>
<author>Benjamin Lecouteux</author>
</authors>
<title>Word Confidence Estimation for SMT N -best List Re-ranking.</title>
<date>2014</date>
<booktitle>In Proceedings of the Workshop on Humans and Computer-assisted Translation (HaCaT),</booktitle>
<location>Gothenburg,</location>
<contexts>
<context position="3711" citStr="Luong et al., 2014" startWordPosition="582" endWordPosition="585">potheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is their capacity to only rewrite incorrect parts of the translation hypotheses and to use appropriate replacement candidates. Many works have tackled the issue of word to n-gram confidence estimation in SMT output (Zens and Ney, 2006; Ueffing and Ney, 2007; Bach et al., 2011; de Gispert et al., 2013), and some attempts have been made to exploit confidence estimates for lattice rescoring (Blackwood et al., 2010) or n-best reranking (Bach et al., 2011; Luong et al., 2014b). In this work, we present an approach in which 1261 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1261–1272, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics new complete hypotheses are produced by rewriting existing hypotheses, and are scored using complex models that could not be used during the initial decoding. We will use as competitive baselines systems that rerank the output of an initial decoder using the complete set of available features, and will show that we manage to improve their translat</context>
<context position="32035" citStr="Luong et al. (2014" startWordPosition="5247" endWordPosition="5250">used in the initial decoder such as IBM1 scores (which also proved useful for word-level confidence estimation (Blatz et al., 2004)) and generative syntactic models. While the experiments in (Och et al., 2004) did not show any clear contribution of syntactic information used in this manner, the later work by Carter and Monz (2011) managed to successfully exploit syntactic features using discriminative language modeling for n-best reranking. Gimpel et al. (2013) outperformed n-best reranking by generating, with an expensive but simple method, diverse hypotheses used as training data. Recently, Luong et al. (2014b) reranked n-best lists using confidence scores at the hypothesis level computed from word-level confidence measures learnt from roughly 10,000 SMT system outputs annotated by humans. Rewriting of translation hypotheses Langlais et al. (2007) described a greedy search decoder, first introduced in (Germann et al., 2001), able to improve translations produced by a dynamic programming decoder using the same scoring function and translation table. However, the more recent work by Arun et al. (2010) using a Gibbs sampler for approximating maximum translation decoding showed the adequacy of the app</context>
<context position="35327" citStr="Luong et al., 2014" startWordPosition="5756" endWordPosition="5759">escribed an approach that improves translations a posteriori by applying simple local rewritings. We have shown that the quality of phrase-level confidence estimates has a direct impact of the amplitude of the improvements that can be obtained, as well as the initial quality of the rewritten hypotheses. We have used a very simple definition for confidence estimates under the form of phrase posteriors estimated from n-best lists from an initial decoder, which obtained good empirical performance, in spite of not requiring large human-annotated datasets as in other approaches (Bach et al., 2011; Luong et al., 2014b). Our work could be extended in several directions. First, we could use a larger set of rewriting operations (Langlais et al., 2007), including the rewrite (sic) operation introduced in (Marie and Max, 2013) that paraphrases source phrases and then translates them. We could also possibly consider any phrase segmentation compatible with a specific word alignment rather than rely on specific phrase segmentations. This would allow us to attain faster some rewritings that could otherwise require several rewriting iterations and may never be attained by the greedy procedure. More features could a</context>
</contexts>
<marker>Luong, Besacier, Lecouteux, 2014</marker>
<rawString>Ngoc-Quang Luong, Laurent Besacier, and Benjamin Lecouteux. 2014b. Word Confidence Estimation for SMT N -best List Re-ranking. In Proceedings of the Workshop on Humans and Computer-assisted Translation (HaCaT), Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Generating Targeted Paraphrases for Improved Translation.</title>
<date>2013</date>
<journal>ACM Transactions on Intelligent Systems and Technology, special issue on Paraphrasing,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="36766" citStr="Madnani and Dorr, 2013" startWordPosition="5985" endWordPosition="5988">ould adapt our procedure to work in several passes: initial passes would tend to restrict the search space more and more using an initial set of features, before a more expensive pass would concentrate on a limited number of hypotheses. Figure 1 indeed already showed a much faster oracle improvement between 1-pass Moses and reranker for n-best list of small sizes. Another avenue for improvement lies in the possibility to perform the training of our rewriter by providing it with more reference translations. As these are typically not readily available, we could resort to targeted paraphrasing (Madnani and Dorr, 2013) to rewrite reference translations into acceptable paraphrases that reuse n-grams from the best hypotheses of the system so far. Contrarily to (Madnani and Dorr, 2013), we could bias the paraphrasing table so that it only contains paraphrases that correspond to target phrases of high confidence values, which would add new n-grams likely of being produced by rewriter. It is furthermore worth noticing that our work proposes a potential answer to an original question: contrarily to typical works on sub-sentencial MT confidence estimation, which predict whether a word or phrase is correct or not, </context>
</contexts>
<marker>Madnani, Dorr, 2013</marker>
<rawString>Nitin Madnani and Bonnie J. Dorr. 2013. Generating Targeted Paraphrases for Improved Translation. ACM Transactions on Intelligent Systems and Technology, special issue on Paraphrasing, 4(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Marie</author>
<author>Aur´elien Max</author>
</authors>
<title>A Study in Greedy Oracle Improvement of Translation Hypotheses.</title>
<date>2013</date>
<booktitle>In Proceedings of IWSLT,</booktitle>
<location>Heidelberg, Germany.</location>
<contexts>
<context position="3187" citStr="Marie and Max, 2013" startWordPosition="493" endWordPosition="496">nstance in a greedy fashion by including new models (Langlais et al., 2007; Hardmeier et al., 2012), or by specifically modeling a task of automatic post-editing targeting a specific system (Simard et al., 2007; Dugast et al., 2007). While such automatic post-editing may seem to be too limited, notably because of the limited initial diversity considered and the fact that it may be in some instances agnostic to the internals of the initial system, it has been shown to potentially improve accuracy of the new translation hypotheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is their capacity to only rewrite incorrect parts of the translation hypotheses and to use appropriate replacement candidates. Many works have tackled the issue of word to n-gram confidence estimation in SMT output (Zens and Ney, 2006; Ueffing and Ney, 2007; Bach et al., 2011; de Gispert et al., 2013), and some attempts have been made to exploit confidence estimates for lattice rescoring (Blackwood et al., 2010) or n-best reranking (Bach et al., 2011; Luong et al., 2014b). In this work, we present an approach in which 1261 Proceedings of the 20</context>
<context position="25876" citStr="Marie and Max, 2013" startWordPosition="4235" endWordPosition="4238">t the training procedure to this new configuration, and reused opti as is. Results, reported in Table 3, indicate that an additional 1.5 BLEU is obtained from opti, or 3.1 BLEU from reranker and 6.6 BLEU from 1-pass Moses. The use of a larger beam of size 10 did not improve those results any further. At 13This is obviously not an optimal solution. 4.3 Oracle experiments: making only the correct decisions We now turn to the situation where only rewritings that actually improve translation performance would be made. In practice, we use a simple solution: we resort to greedy oracle search (GOS) (Marie and Max, 2013), where sentenceBLEU is maximized using rewritings from the opti phrase table. At each iteration the rewriting in the neighborhood that maximizes sentenceBLEU is selected until convergence. Results for this greedy search oracle appear in the last column of Table 2 and allow us to put in perspective the individual potential of the var1267 ious tested configurations. We can first notice that the rpt5pef phrase table allows the oracle to reach 50.6 BLEU, 8.1 BLEU below the oracle value obtained with conf10k, although rpt5pef contains twice as many bi-phrases. The same conclusion can be made about</context>
<context position="35536" citStr="Marie and Max, 2013" startWordPosition="5791" endWordPosition="5794">he improvements that can be obtained, as well as the initial quality of the rewritten hypotheses. We have used a very simple definition for confidence estimates under the form of phrase posteriors estimated from n-best lists from an initial decoder, which obtained good empirical performance, in spite of not requiring large human-annotated datasets as in other approaches (Bach et al., 2011; Luong et al., 2014b). Our work could be extended in several directions. First, we could use a larger set of rewriting operations (Langlais et al., 2007), including the rewrite (sic) operation introduced in (Marie and Max, 2013) that paraphrases source phrases and then translates them. We could also possibly consider any phrase segmentation compatible with a specific word alignment rather than rely on specific phrase segmentations. This would allow us to attain faster some rewritings that could otherwise require several rewriting iterations and may never be attained by the greedy procedure. More features could also be used, for instance to model more fine-grained syntax (Post, 2011) or document-level lexical coherence (Hardmeier et al., 2012). However, anticipating that some features might be very expensive to comput</context>
</contexts>
<marker>Marie, Max, 2013</marker>
<rawString>Benjamin Marie and Aur´elien Max. 2013. A Study in Greedy Oracle Improvement of Translation Hypotheses. In Proceedings of IWSLT, Heidelberg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen</location>
<marker>Och, Gildea, 2004</marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A Smorgasbord of Features for Statistical Machine Translation. In Proceedings of NAACL, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Weijing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Philadelphia, USA.</location>
<contexts>
<context position="14465" citStr="Papineni et al., 2002" startWordPosition="2367" endWordPosition="2370">of the WMT’13 data for TED Talks. Table 1 provides relevant statistics about the data used. Tasks Corpus Sentences Tokens(en-fr) train 4.9M 78M - 91M Medical dev 500 10k - 12k test 1,000 21k - 26k LM - 146M train 107 758 2M - 2.2M TED Talks dev 934 20k - 20k test 1,664 31k - 34k LM 6B - 2.5B Table 1: Corpora used in this work. 3http://www.statmt.org/wmt14/ medical-task/ 4https://wit3.fbk.eu/mt.php?release= 2013-01 5http://www.statmt.org/wmt13 We first built a state-of-the-art phrase-based SMT system using Moses (Koehn et al., 2003) with standard settings. We tuned its parameters towards BLEU (Papineni et al., 2002) on the tuning dataset using the kb-mira implementation available in Moses with default parameters. Our results will be compared using BLEU and TER (Snover et al., 2006) to a) the initial best translation produced by the Moses decoder (1-pass Moses) and b) the best translation obtained by reranking the 1,000-best list of 1-pass Moses (reranker). Since reranker implements a well-documented approach and uses types of features commonly used in reranking tasks we will consider it as our main baseline. It was trained using kb-mira on the 1,000-best of the development data decoded by 1-pass Moses. I</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of ACL, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristen Parton</author>
<author>Nizar Habash</author>
<author>Kathleen R McKeown</author>
<author>Gonzalo Iglesias</author>
<author>Adri`a de Gispert</author>
</authors>
<title>Can Automatic Post-editing Make MT more Meaningful?</title>
<date>2012</date>
<booktitle>In Proceedings of EAMT,</booktitle>
<location>Trento, Italy.</location>
<marker>Parton, Habash, McKeown, Iglesias, de Gispert, 2012</marker>
<rawString>Kristen Parton, Nizar Habash, Kathleen R. McKeown, Gonzalo Iglesias, and Adri`a de Gispert. 2012. Can Automatic Post-editing Make MT more Meaningful? In Proceedings of EAMT, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolas P´echeux</author>
<author>Li Gong</author>
<author>Quoc Khanh Do</author>
<author>Benjamin Marie</author>
</authors>
<title>Yulia Ivanishcheva, Alexander Allauzen,</title>
<date></date>
<booktitle>LIMSI @ WMT’14 Medical Translation Task. In Proceedings of WMT,</booktitle>
<location>Thomas Lavergne,</location>
<marker>P´echeux, Gong, Do, Marie, </marker>
<rawString>Nicolas P´echeux, Li Gong, Quoc Khanh Do, Benjamin Marie, Yulia Ivanishcheva, Alexander Allauzen, Thomas Lavergne, Jan Niehues, Aur´elien Max, and Franc¸ois Yvon. 2014. LIMSI @ WMT’14 Medical Translation Task. In Proceedings of WMT, Baltimore, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
</authors>
<title>Judging Grammaticality with Tree Substitution Grammar Derivations.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL, short papers,</booktitle>
<location>Portland, USA.</location>
<contexts>
<context position="35999" citStr="Post, 2011" startWordPosition="5865" endWordPosition="5866">we could use a larger set of rewriting operations (Langlais et al., 2007), including the rewrite (sic) operation introduced in (Marie and Max, 2013) that paraphrases source phrases and then translates them. We could also possibly consider any phrase segmentation compatible with a specific word alignment rather than rely on specific phrase segmentations. This would allow us to attain faster some rewritings that could otherwise require several rewriting iterations and may never be attained by the greedy procedure. More features could also be used, for instance to model more fine-grained syntax (Post, 2011) or document-level lexical coherence (Hardmeier et al., 2012). However, anticipating that some features might be very expensive to compute, we could adapt our procedure to work in several passes: initial passes would tend to restrict the search space more and more using an initial set of features, before a more expensive pass would concentrate on a limited number of hypotheses. Figure 1 indeed already showed a much faster oracle improvement between 1-pass Moses and reranker for n-best list of small sizes. Another avenue for improvement lies in the possibility to perform the training of our rew</context>
</contexts>
<marker>Post, 2011</marker>
<rawString>Matt Post. 2011. Judging Grammaticality with Tree Substitution Grammar Derivations. In Proceedings of ACL, short papers, Portland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lane Schwartz</author>
<author>Chris Callison-Burch</author>
<author>William Schuler</author>
<author>Stephen Wu</author>
</authors>
<title>Incremental Syntactic Language Models for Phrase-based Translation.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL,</booktitle>
<location>Portland, USA.</location>
<contexts>
<context position="1832" citStr="Schwartz et al., 2011" startWordPosition="273" endWordPosition="276">in spite of its simplicity. 1 Introduction The standard configuration of modern phrasebased Statistical Machine Translation (SMT) (Koehn et al., 2003) systems can produce very acceptable results on some tasks. However, early integration of better features to guide the search for the best hypothesis can result in significant improvements, an expression of the complexity of modeling translation quality. For instance, improvements have been obtained by integrating features into decoding that better model semantic coherence at the sentence level (Hasan and Ney, 2009) or syntactic well-formedness (Schwartz et al., 2011). However, early use of such complex features typically comes at a high computational cost. Moreover, some informative features require or are better computed when complete translation hypotheses are available. This is addressed in numerous works on reranking of the highest scored sub-space of hypotheses, on so-called n-best lists (Och et al., 2004; Zhang et al., 2006; Carter and Monz, 2011) or output lattices (Schwenk et al., 2006; Blackwood et al., 2010), where many works specifically target the inclusion of better language modelling capabilities, a well-known weakness of current automatic g</context>
</contexts>
<marker>Schwartz, Callison-Burch, Schuler, Wu, 2011</marker>
<rawString>Lane Schwartz, Chris Callison-Burch, William Schuler, and Stephen Wu. 2011. Incremental Syntactic Language Models for Phrase-based Translation. In Proceedings ofACL, Portland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Daniel D´echelotte</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Continuous Space Language Models for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<location>Sydney, Australia.</location>
<marker>Schwenk, D´echelotte, Gauvain, 2006</marker>
<rawString>Holger Schwenk, Daniel D´echelotte, and Jean-Luc Gauvain. 2006. Continuous Space Language Models for Statistical Machine Translation. In Proceedings of COLING-ACL, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>Cyril Goutte</author>
<author>Pierre Isabelle</author>
</authors>
<title>Statistical Phrase-based Post-editing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>Rochester, USA.</location>
<contexts>
<context position="2777" citStr="Simard et al., 2007" startWordPosition="418" endWordPosition="421"> lists (Och et al., 2004; Zhang et al., 2006; Carter and Monz, 2011) or output lattices (Schwenk et al., 2006; Blackwood et al., 2010), where many works specifically target the inclusion of better language modelling capabilities, a well-known weakness of current automatic generation approaches (Knight, 2007). Another way to improve translation a posteriori can be done by rewriting initial hypotheses, for instance in a greedy fashion by including new models (Langlais et al., 2007; Hardmeier et al., 2012), or by specifically modeling a task of automatic post-editing targeting a specific system (Simard et al., 2007; Dugast et al., 2007). While such automatic post-editing may seem to be too limited, notably because of the limited initial diversity considered and the fact that it may be in some instances agnostic to the internals of the initial system, it has been shown to potentially improve accuracy of the new translation hypotheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is their capacity to only rewrite incorrect parts of the translation hypotheses and to use appropriate replacement candidates. Many works ha</context>
<context position="32954" citStr="Simard et al., 2007" startWordPosition="5387" endWordPosition="5390">Germann et al., 2001), able to improve translations produced by a dynamic programming decoder using the same scoring function and translation table. However, the more recent work by Arun et al. (2010) using a Gibbs sampler for approximating maximum translation decoding showed the adequacy of the approximations made by state-of-the-art decoders for finding the best translation in their search space. Other works were more directly targeted at automatic post-editing of SMT output, and approached the problem as one of second-pass translation between automatic predictions and correct translations (Simard et al., 2007; Dugast et al., 2007). The recent work of Zhu et al. (2013) attempts to repair translations by exploiting confidence estimates for examples derived from the similarity between source words in the input text and in training examples. Luong et al. (2014a) obtained improvements by computing word confidence estimation, trained on human annotated data, and large sets of lexical, syntactic and semantic features, for the words in the n-best list produced during a first-pass decoding, and performing a second-pass decoding exploiting these new scores. Confidence estimation of Machine Translation The W</context>
</contexts>
<marker>Simard, Goutte, Isabelle, 2007</marker>
<rawString>Michel Simard, Cyril Goutte, and Pierre Isabelle. 2007. Statistical Phrase-based Post-editing. In Proceedings of NAACL, Rochester, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA,</booktitle>
<location>Cambridge, USA.</location>
<contexts>
<context position="14634" citStr="Snover et al., 2006" startWordPosition="2395" endWordPosition="2398"> 12k test 1,000 21k - 26k LM - 146M train 107 758 2M - 2.2M TED Talks dev 934 20k - 20k test 1,664 31k - 34k LM 6B - 2.5B Table 1: Corpora used in this work. 3http://www.statmt.org/wmt14/ medical-task/ 4https://wit3.fbk.eu/mt.php?release= 2013-01 5http://www.statmt.org/wmt13 We first built a state-of-the-art phrase-based SMT system using Moses (Koehn et al., 2003) with standard settings. We tuned its parameters towards BLEU (Papineni et al., 2002) on the tuning dataset using the kb-mira implementation available in Moses with default parameters. Our results will be compared using BLEU and TER (Snover et al., 2006) to a) the initial best translation produced by the Moses decoder (1-pass Moses) and b) the best translation obtained by reranking the 1,000-best list of 1-pass Moses (reranker). Since reranker implements a well-documented approach and uses types of features commonly used in reranking tasks we will consider it as our main baseline. It was trained using kb-mira on the 1,000-best of the development data decoded by 1-pass Moses. In our experiments, rewriter rewrites the one-best hypothesis6 produced by reranker using the operators Replace, Split and Merge as described in section 2.1. 3.2 Baseline</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, , and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of AMTA, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Kashif Shah</author>
<author>Jose G C de Souza</author>
<author>Trevor Cohn</author>
</authors>
<title>QuEst - A Translation Quality Estimation Framework.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL, System Demonstrations,</booktitle>
<location>Sofia, Bulgaria.</location>
<marker>Specia, Shah, de Souza, Cohn, 2013</marker>
<rawString>Lucia Specia, Kashif Shah, Jose G.C. de Souza, and Trevor Cohn. 2013. QuEst - A Translation Quality Estimation Framework. In Proceedings of ACL, System Demonstrations, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>WordLevel Confidence Estimation for Machine Translation. Computational Linguistics.</title>
<date>2007</date>
<contexts>
<context position="3494" citStr="Ueffing and Ney, 2007" startWordPosition="544" endWordPosition="547">cause of the limited initial diversity considered and the fact that it may be in some instances agnostic to the internals of the initial system, it has been shown to potentially improve accuracy of the new translation hypotheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is their capacity to only rewrite incorrect parts of the translation hypotheses and to use appropriate replacement candidates. Many works have tackled the issue of word to n-gram confidence estimation in SMT output (Zens and Ney, 2006; Ueffing and Ney, 2007; Bach et al., 2011; de Gispert et al., 2013), and some attempts have been made to exploit confidence estimates for lattice rescoring (Blackwood et al., 2010) or n-best reranking (Bach et al., 2011; Luong et al., 2014b). In this work, we present an approach in which 1261 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1261–1272, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics new complete hypotheses are produced by rewriting existing hypotheses, and are scored using complex models that could not be used du</context>
<context position="33620" citStr="Ueffing and Ney (2007)" startWordPosition="5489" endWordPosition="5492">hu et al. (2013) attempts to repair translations by exploiting confidence estimates for examples derived from the similarity between source words in the input text and in training examples. Luong et al. (2014a) obtained improvements by computing word confidence estimation, trained on human annotated data, and large sets of lexical, syntactic and semantic features, for the words in the n-best list produced during a first-pass decoding, and performing a second-pass decoding exploiting these new scores. Confidence estimation of Machine Translation The Word Posterior Probability (WPP) proposed by Ueffing and Ney (2007), derived from information from the n-best list produced by a decoder, proved to be useful for estimating word-level confidence. Bach et al. (2011) worked on the issue of predicting sentence-level and word-level MT errors by using WPP and other features derived from the source context, the source-target alignment, and dependency structures, but relied on a significantly large manually annotated corpus of MT errors. De Gispert et al. (2013) calculate k1269 gram posterior probabilities from n-best lists or word lattices, and demonstrated that they were reasonably accurate indications of whether </context>
</contexts>
<marker>Ueffing, Ney, 2007</marker>
<rawString>Nicola Ueffing and Hermann Ney. 2007. WordLevel Confidence Estimation for Machine Translation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Jia Xu</author>
<author>Luis Fernando D’Haro</author>
<author>Hermann Ney</author>
</authors>
<title>Error Analysis of Statistical Machine Translation Output.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<location>Genoa, Italy.</location>
<marker>Vilar, Xu, D’Haro, Ney, 2006</marker>
<rawString>David Vilar, Jia Xu, Luis Fernando D’Haro, and Hermann Ney. 2006. Error Analysis of Statistical Machine Translation Output. In Proceedings of LREC, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>N -Gram Posterior Probabilities for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of WMT,</booktitle>
<location>New York, USA.</location>
<contexts>
<context position="3471" citStr="Zens and Ney, 2006" startWordPosition="540" endWordPosition="543"> limited, notably because of the limited initial diversity considered and the fact that it may be in some instances agnostic to the internals of the initial system, it has been shown to potentially improve accuracy of the new translation hypotheses (Parton et al., 2012) and to offer very high oracle performance (Marie and Max, 2013). However, an important issue for such approaches is their capacity to only rewrite incorrect parts of the translation hypotheses and to use appropriate replacement candidates. Many works have tackled the issue of word to n-gram confidence estimation in SMT output (Zens and Ney, 2006; Ueffing and Ney, 2007; Bach et al., 2011; de Gispert et al., 2013), and some attempts have been made to exploit confidence estimates for lattice rescoring (Blackwood et al., 2010) or n-best reranking (Bach et al., 2011; Luong et al., 2014b). In this work, we present an approach in which 1261 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1261–1272, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics new complete hypotheses are produced by rewriting existing hypotheses, and are scored using complex models th</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>Richard Zens and Hermann Ney. 2006. N -Gram Posterior Probabilities for Statistical Machine Translation. In Proceedings of WMT, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Almut Silja Hildebrand</author>
<author>Stephan Vogel</author>
</authors>
<title>Distributed Language Modeling for Nbest List Re-ranking.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2202" citStr="Zhang et al., 2006" startWordPosition="330" endWordPosition="333"> modeling translation quality. For instance, improvements have been obtained by integrating features into decoding that better model semantic coherence at the sentence level (Hasan and Ney, 2009) or syntactic well-formedness (Schwartz et al., 2011). However, early use of such complex features typically comes at a high computational cost. Moreover, some informative features require or are better computed when complete translation hypotheses are available. This is addressed in numerous works on reranking of the highest scored sub-space of hypotheses, on so-called n-best lists (Och et al., 2004; Zhang et al., 2006; Carter and Monz, 2011) or output lattices (Schwenk et al., 2006; Blackwood et al., 2010), where many works specifically target the inclusion of better language modelling capabilities, a well-known weakness of current automatic generation approaches (Knight, 2007). Another way to improve translation a posteriori can be done by rewriting initial hypotheses, for instance in a greedy fashion by including new models (Langlais et al., 2007; Hardmeier et al., 2012), or by specifically modeling a task of automatic post-editing targeting a specific system (Simard et al., 2007; Dugast et al., 2007). W</context>
</contexts>
<marker>Zhang, Hildebrand, Vogel, 2006</marker>
<rawString>Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel. 2006. Distributed Language Modeling for Nbest List Re-ranking. In Proceedings of EMNLP, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junguo Zhu</author>
<author>Muyun Yang</author>
<author>Sheng Li</author>
<author>Tiejun Zhao</author>
</authors>
<title>Repairing Incorrect Translation with Examples.</title>
<date>2013</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<location>Nagoya, Japan.</location>
<contexts>
<context position="33014" citStr="Zhu et al. (2013)" startWordPosition="5399" endWordPosition="5402">y a dynamic programming decoder using the same scoring function and translation table. However, the more recent work by Arun et al. (2010) using a Gibbs sampler for approximating maximum translation decoding showed the adequacy of the approximations made by state-of-the-art decoders for finding the best translation in their search space. Other works were more directly targeted at automatic post-editing of SMT output, and approached the problem as one of second-pass translation between automatic predictions and correct translations (Simard et al., 2007; Dugast et al., 2007). The recent work of Zhu et al. (2013) attempts to repair translations by exploiting confidence estimates for examples derived from the similarity between source words in the input text and in training examples. Luong et al. (2014a) obtained improvements by computing word confidence estimation, trained on human annotated data, and large sets of lexical, syntactic and semantic features, for the words in the n-best list produced during a first-pass decoding, and performing a second-pass decoding exploiting these new scores. Confidence estimation of Machine Translation The Word Posterior Probability (WPP) proposed by Ueffing and Ney </context>
</contexts>
<marker>Zhu, Yang, Li, Zhao, 2013</marker>
<rawString>Junguo Zhu, Muyun Yang, Sheng Li, and Tiejun Zhao. 2013. Repairing Incorrect Translation with Examples. In Proceedings of IJCNLP, Nagoya, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000038">
<title confidence="0.997751">
Relieving the Computational Bottleneck: Joint Inference for
Event Extraction with High-Dimensional Features
</title>
<author confidence="0.995197">
Deepak Venugopal and Chen Chen and Vibhav Gogate and Vincent Ng
</author>
<affiliation confidence="0.9985705">
Department of Computer Science and Human Language Technology Research Institute
University of Texas at Dallas
</affiliation>
<address confidence="0.870398">
Richardson, TX 75083-0688
</address>
<email confidence="0.996377">
dxv021000@utdallas.edu, {yzcchen,vgogate,vince}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.994684" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999700766666667">
Several state-of-the-art event extraction sys-
tems employ models based on Support Vec-
tor Machines (SVMs) in a pipeline architec-
ture, which fails to exploit the joint depen-
dencies that typically exist among events
and arguments. While there have been at-
tempts to overcome this limitation using
Markov Logic Networks (MLNs), it re-
mains challenging to perform joint infer-
ence in MLNs when the model encodes
many high-dimensional sophisticated fea-
tures such as those essential for event ex-
traction. In this paper, we propose a new
model for event extraction that combines
the power of MLNs and SVMs, dwarfing
their limitations. The key idea is to reli-
ably learn and process high-dimensional
features using SVMs; encode the output
of SVMs as low-dimensional, soft formu-
las in MLNs; and use the superior joint in-
ferencing power of MLNs to enforce joint
consistency constraints over the soft for-
mulas. We evaluate our approach for the
task of extracting biomedical events on
the BioNLP 2013, 2011 and 2009 Genia
shared task datasets. Our approach yields
the best F1 score to date on the BioNLP’13
(53.61) and BioNLP’11 (58.07) datasets
and the second-best F1 score to date on the
BioNLP’09 dataset (58.16).
</bodyText>
<sectionHeader confidence="0.999154" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999903204081632">
Event extraction is the task of extracting and la-
beling all instances in a text document that corre-
spond to a pre-defined event type. This task is quite
challenging for a multitude of reasons: events are
often nested, recursive and have several arguments;
there is no clear distinction between arguments and
events; etc. For instance, consider the BioNLP Ge-
nia event extraction shared task (N´edellec et al.,
2013). In this task, participants are asked to extract
instances of a pre-defined set of biomedical events
from text. An event is identified by a keyword
called the trigger and can have an arbitrary number
of arguments that correspond to pre-defined argu-
ment types. The task is complicated by the fact
that an event may serve as an argument of another
event (nested events). An example of the task is
shown in Figure 1. As we can see, event E13 takes
as arguments two events, E14 and E12, which in
turn has E11 as one of its arguments.
A standard method that has been frequently em-
ployed to perform this shared task uses a pipeline
architecture with three steps: (1) detect if a token
is a trigger and assign a trigger type label to it; (2)
for every detected trigger, determine all its argu-
ments and assign types to each detected argument;
and (3) combine the extracted triggers and argu-
ments to obtain events. Though adopted by the
top-performing systems such as the highest scoring
system on the BioNLP’13 Genia shared task (Kim
et al., 2013), this approach is problematic for at
least two reasons. First, as is typical in pipeline
architectures, errors may propagate from one stage
to the next. Second, since each event/argument is
identified and assigned a type independently of the
others, it fails to capture the relationship between
a trigger and its neighboring triggers, an argument
and its neighboring arguments, etc.
More recently, researchers have investigated
joint inference techniques for event extraction us-
ing Markov Logic Networks (MLNs) (e.g., Poon
and Domingos (2007), Poon and Vanderwende
(2010), Riedel and McCallum (2011a)), a statis-
tical relational model that enables us to model the
dependencies between different instances of a data
sample. However, it is extremely challenging to
make joint inference using MLNs work well in
practice (Poon and Domingos, 2007). One reason
is that it is generally difficult to model sophisti-
cated linguistic features using MLNs. The diffi-
</bodyText>
<page confidence="0.97352">
831
</page>
<note confidence="0.9444125">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 831–843,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<footnote confidence="0.362903">
... demonstrated that HOIL-1L interacting protein (HOIP), a ubiquitin ligase that can catalyze the assembly of linear
polyubiquitin chains, is recruited to DC40 in a TRAF2-dependent manner following engagement of CD40 ...
</footnote>
<note confidence="0.8902035">
(a) Sentence fragment
ID Event Type Trigger Arguments
E11 Binding recruited Theme=1HOIL-1L interacting protein,CD401
E12 Regulation dependent Theme=E11, Cause=TRAF2
E13 +ve Regulation following Theme=E12, Cause=E14
E14 Binding engagement Theme=CD40
</note>
<figure confidence="0.991339">
(b) Events
</figure>
<figureCaption confidence="0.9878865">
Figure 1: Example of event extraction in the BioNLP Genia task. The table in (b) shows all the events
extracted from sentence (a). Note that successful extraction of E13 depends on E12 and E14.
</figureCaption>
<bodyText confidence="0.999868712121212">
culty stems from the fact that some of these fea-
tures are extremely high dimensional (e.g., Chen
and Ng (2012), Huang and Riloff (2012b), Li et al.
(2012), Li et al. (2013b), Li et al. (2013c)), and to
reliably learn weights of formulas that encode such
features, one would require an enormous number
of data samples. Moreover, even the complexity of
approximate inference on such models is quite high,
often prohibitively so. For example, a trigram can
be encoded as an MLN formula, Word(w1, p−1) ∧
Word(w2, p) ∧ Word(w3, p + 1) ==&gt;- Type(p, T).
For any given position (p), this formula has W3
groundings, where W is the number of possible
words, making it too large for learning/inference.
Therefore, current MLN-based systems tend to in-
clude a highly simplified model ignoring powerful
linguistic features. This is problematic because
such features are essential for event extraction.
Our contributions in this paper are two-fold.
First, we propose a novel model for biomedical
event extraction based on MLNs that addresses the
aforementioned limitations by leveraging the power
of Support Vector Machines (SVMs) (Vapnik,
1995; Joachims, 1999) to handle high-dimensional
features. Specifically, we (1) learn SVM models us-
ing rich linguistic features for trigger and argument
detection and type labeling; (2) design an MLN
composed of soft formulas (each of which encodes
a soft constraint whose associated weight indicates
how important it is to satisfy the constraint) and
hard formulas (constraints that always need to be
satisfied, thus having a weight of oc) to capture
the relational dependencies between triggers and
arguments; and (3) encode the SVM output as prior
knowledge in the MLN in the form of soft formulas,
whose weights are computed using the confidence
values generated by the SVMs. This formulation
naturally allows SVMs and MLNs to complement
each other’s strengths and weaknesses: learning
in a large and sparse feature space is much easier
with SVMs than with MLNs, whereas modeling
relational dependencies is much easier with MLNs
than with SVMs.
Our second contribution concerns making infer-
ence with this MLN feasible. Recall that inference
involves detecting and assigning the type label to
all the triggers and arguments. We show that exist-
ing Maximum-a-posteriori (MAP) inference meth-
ods, even the most advanced approximate ones
(e.g., Selman et al. (1996), Marinescu and Dechter
(2009), Sontag and Globerson (2011) ), are infea-
sible on our proposed MLN because of their high
memory cost. Consequently, we identify decompo-
sitions of the MLN into disconnected components
and solve each independently, thereby drastically
reducing the memory requirements.
We evaluate our approach on the BioNLP 2009,
2011 and 2013 Genia shared task datasets. On
the BioNLP’13 dataset, our model significantly
outperforms state-of-the-art pipeline approaches
and achieves the best F1 score to date. On the
BioNLP’11 and BioNLP’09 datasets, our scores
are slightly better and slightly worse respectively
than the best reported results. However, they
are significantly better than state-of-the-art MLN-
based systems.
</bodyText>
<sectionHeader confidence="0.9513605" genericHeader="introduction">
2 Background
2.1 Related Work
</sectionHeader>
<bodyText confidence="0.999342666666667">
As a core task in information extraction, event ex-
traction has received significant attention in the nat-
ural language processing (NLP) community. The
development and evaluation of large-scale learning-
based event extraction systems was propelled in
part by the availability of annotated corpora pro-
duced as part of the Message Understanding Con-
ferences (MUCs), the Automatic Content Extrac-
tion (ACE) evaluations, and the BioNLP shared
</bodyText>
<page confidence="0.995682">
832
</page>
<bodyText confidence="0.999951983606557">
tasks on event extraction. Previous work on event
extraction can be broadly divided into two cate-
gories, one focusing on the development of fea-
tures (henceforth feature-based approaches) and
the other focusing on the development of models
(henceforth model-based approaches).
Feature-based approaches. Early work on
feature-based approaches has primarily focused
on designing local sentence-level features such as
token and syntactic features (Grishman et al., 2005;
Ahn, 2006). Later, it was realized that local features
were insufficient to reliably and accurately perform
event extraction in complex domains and therefore
several researchers proposed using high-level fea-
tures. For instance, Ji and Grishman (2008) used
global information from related documents; Gupta
and Ji (2009) extracted implicit time information;
Patwardhan and Riloff (2009) used broader sen-
tential context; Liao and Grishman (2010; 2011)
leveraged document-level cross-event information
and topic-based features; and Huang and Riloff
(2012b) explored discourse properties.
Model-based approaches. The model-based ap-
proaches developed to date have focused on mod-
eling global properties and seldom use rich, high-
dimensional features. To capture global event struc-
ture properties, McClosky et al. (2011a) proposed
a dependency parsing model. To extract event ar-
guments, Li et al. (2013b) proposed an Integer
Linear Programming (ILP) model to encode the
relationship between event mentions. To overcome
the error propagation problem associated with the
pipeline architecture, several joint models have
been proposed, including those that are based on
MLNs (e.g., Poon and Domingos (2007), Riedel et
al. (2009), Poon and Vanderwende (2010)), struc-
tured perceptrons (e.g., Li et al. (2013c)), and dual
decomposition with minimal domain adaptation
(e.g., Riedel and McCallum (2011a; 2011b)).
In light of the high annotation cost required by
supervised learning-based event extraction systems,
several semi-supervised, unsupervised, and rule-
based systems have been proposed. For instance,
Huang and Riloff (2012a) proposed a bootstrap-
ping method to extract event arguments using only
a small amount of annotated data; Lu and Roth
(2012) developed a novel unsupervised sequence
labeling model; Bui et al. (2013) implemented a
rule-based approach to extract biomedical events;
and Ritter et al. (2012) used unsupervised learning
to extract events from Twitter data.
Our work extends prior work by developing a
rich framework that leverages sophisticated feature-
based approaches as well as joint inference using
MLNs. This combination gives us the best of both
worlds because on one hand, it is challenging to
model sophisticated linguistic features using MLNs
while on the other hand, feature-based approaches
employing sophisticated high-dimensional features
suffer from error propagation as the model is gen-
erally not rich enough for joint inference.
</bodyText>
<subsectionHeader confidence="0.999453">
2.2 The Genia Event Extraction Task
</subsectionHeader>
<bodyText confidence="0.999291666666666">
The BioNLP Shared Task (BioNLP-ST) series
(Kim et al. (2009), Kim et al. (2011a) and N´edellec
et al. (2013)) is designed to tackle the problem of
extracting structured information from the biomedi-
cal literature. The Genia Event Extraction task is ar-
guably the most important of all the tasks proposed
in BioNLP-ST and is also the only task organized
in all three events in the series.
The 2009 edition of the Genia task (Kim et
al., 2009) was conducted on the Genia event
corpus (Kim et al., 2008), which only contains
abstracts of the articles that represent domain
knowledge around NFκB proteins. The 2011 edi-
tion (Kim et al., 2011b) augmented the dataset to
include full text articles, resulting in two collec-
tions, the abstract collection and the full text col-
lection. The 2013 edition (Kim et al., 2013) further
augmented the dataset with recent full text articles
but removed the abstract collection entirely.
The targeted event types have also changed
slightly over the years. Both the 2009 and 2011
editions are concerned with nine fine-grained event
sub-types that can be categorized into three main
types, namely simple, binding and regulation
events. These three main event types can be dis-
tinguished by the kinds of arguments they take. A
simple event can take exactly one protein as its
Theme argument. A binding event can take one
or more proteins as its Theme arguments, and is
therefore slightly more difficult to extract than a
simple event. A regulation event takes exactly one
protein or event as its Theme argument and option-
ally one protein or event as its Cause argument. If
a regulation event takes another event as its Theme
or Cause argument, it will lead to a nested event.
Regulation events are considered the most difficult-
to-extract among the three event types owing in part
to the presence of an optional Cause argument and
their recursive structure. The 2013 edition intro-
</bodyText>
<page confidence="0.997992">
833
</page>
<bodyText confidence="0.999801">
duced a new event type, protein-mod, and its three
sub-types. Theoretically, a protein-mod event takes
exactly one protein as its Theme argument and
optionally one protein or event as its Cause argu-
ment. In practice, however, it rarely occurs: there
are only six protein-mod events having Cause ar-
guments in the training data for the 2013 edition.
Consequently, our model makes the simplifying
assumption that a protein-mod event can only take
one Theme argument, meaning that we are effec-
tively processing protein-mod events in the same
way as simple events.
</bodyText>
<subsectionHeader confidence="0.999458">
2.3 Markov Logic Networks
</subsectionHeader>
<bodyText confidence="0.999991">
Statistical relational learning (SRL) (Getoor and
Taskar, 2007) is an emerging field that seeks to
unify logic and probability, and since most NLP
techniques are grounded either in logic or proba-
bility or both, NLP serves as an ideal application
domain for SRL. In this paper, we will employ a
popular SRL approach called Markov logic net-
works (MLNs) (Domingos and Lowd, 2009). At a
high level, an MLN is a set of weighted first-order
logic formulas (fi, wi), where wi is the weight
associated with formula fi. Given a set of con-
stants that model objects in the domain, it defines a
Markov network or a log-linear model (Koller and
Friedman, 2009) in which we have one node per
ground first-order atom and a propositional feature
corresponding to each grounding of each first-order
formula. The weight of the feature is the weight of
the corresponding first-order formula.
Formally, the probability of a world ω, which
represents an assignment of values to all ground
atoms in the Markov network, is given by:
</bodyText>
<equation confidence="0.992369">
1 X Pr(ω) = Z exp
i
</equation>
<bodyText confidence="0.985208076923077">
where N(fi, ω) is the number of groundings of fi
that evaluate to True in ω and Z is a normalization
constant called the partition function.
The key inference tasks over MLNs are com-
puting the partition function (Z) and the most-
probable explanation given evidence (the MAP
task). Most queries, including those required by
event extraction, can be reduced to these inference
tasks. Formally, the partition function and the MAP
tasks are given by:
arg max XP(ω) = arg max wiN(fi, ω) (2)
w w
i
</bodyText>
<sectionHeader confidence="0.995878" genericHeader="method">
3 Pipeline Model
</sectionHeader>
<bodyText confidence="0.999991470588235">
We implement a pipeline event extraction system
using SVMs. This pipeline model serves two im-
portant functions: (1) providing a baseline for eval-
uation and (2) producing prior knowledge for the
joint model.
Our pipeline model consists of two steps: trig-
ger labeling and argument labeling. In the trigger
labeling step, we determine whether a candidate
trigger is a true trigger and label each true trigger
with its trigger type. Then, in the argument label-
ing step, we identify the arguments for each true
trigger discovered in the trigger labeling step and
assign a role to each argument.
We recast each of the two steps as a classification
task and employ SVMmulticlass (Tsochantaridis
et al., 2004) to train the two classifiers. We describe
each step in detail below.
</bodyText>
<subsectionHeader confidence="0.999513">
3.1 Trigger Labeling
</subsectionHeader>
<bodyText confidence="0.99998188">
A preliminary study of the BioNLP’13 training
data suggests that 98.7% of the true triggers’ head
words1 are either verbs, nouns or adjectives. There-
fore, we consider only those words whose part-of-
speech tags belong to the above three categories
as candidate triggers. To train the trigger classifier,
we create one training instance for each candidate
trigger in the training data. If the candidate trigger
is not a trigger, the class label of the corresponding
instance is None; otherwise, the label is the type
of the trigger. Thus, the number of class labels
equals the number of trigger types plus one. Each
training instance is represented by the features de-
scribed in Table 1(a). These features closely mirror
those used in state-of-the-art trigger labeling sys-
tems such as Miwa et al. (2010b) and Bj¨orne and
Salakoski (2013).
After training, we apply the resulting trigger clas-
sifier to classify the test instances, which are cre-
ated in the same way as the training instances. If a
test instance is predicted as None by the classifier,
the corresponding candidate trigger is labeled as
a non-trigger; otherwise, the corresponding candi-
date trigger is posited as a true trigger whose type
is the class value assigned by the classifier.
</bodyText>
<equation confidence="0.9971648">
!wiN(fi, ω)
XZ = Xexp
w i
!
wiN(fi,ω) (1)
</equation>
<footnote confidence="0.863779">
1Head words are found using Collins’ (1999) rules.
</footnote>
<page confidence="0.993752">
834
</page>
<figure confidence="0.477808">
(a) Features for trigger labeling
</figure>
<bodyText confidence="0.965137619047619">
Token features The basic token features (see Table 1(c)) computed from (1) the candidate trigger word and (2) the
surrounding tokens in a window of two; character bigrams and trigrams of the candidate trigger word;
word n-grams (n=1,2,3) of the candidate trigger word and its context words in a window of three; whether
the candidate trigger word contains a digit; whether the candidate trigger word contains an upper case
letter; whether the candidate trigger word contains a symbol.
Dependency The basic dependency path features (see Table 1(c)) computed using the shortest paths from the candidate
features trigger to (1) the nearest protein word, (2) the nearest protein word to its left, and (3) the nearest protein
word to its right.
Other The distances from the candidate trigger word to (1) the nearest protein word, (2) the nearest protein
features word to its left, and (3) the nearest protein word to its right; the number of protein words in the sentence.
(b) Features for argument labeling
Token features Word n-grams (n=1,2,3) of (1) the candidate trigger word and its context in a window of three and (2) the
candidate argument word and its context in a window of three; the basic token features (see Table 1(c))
computed from (1) the candidate trigger word and (2) the candidate argument word; the trigger type of
the candidate trigger word.
Dependency The basic dependency features (see Table 1(c)) computed using the shortest path from the candidate
features trigger word to the candidate argument word.
Other The distance between the candidate trigger word and the candidate argument word; the number of
features proteins between the candidate trigger word and the candidate argument word; the concatenation of the
candidate trigger word and the candidate argument word; the concatenation of the candidate trigger type
and the candidate argument word.
</bodyText>
<listItem confidence="0.49638">
(c) Basic token and dependency features
</listItem>
<figureCaption confidence="0.543304142857143">
Basic token fea- Six features are computed given a token t, including: (a) the lexical string of t, (b) the lemma of t, (c) the
tures stem of t obtained using the Porter stemmer (Porter, 1980), (d) the part-of-speech tag of t, (e) whether t
appears as a true trigger in the training data, and (f) whether t is a protein name.
Basic Six features are computed given a dependency path p, including: (a) the vertex walk in p, (b) the edge
dependency walk in p, (c) the n-grams (n=2,3,4) of the (stemmed) words associated with the vertices in p, (d) the
features n-grams (n=2,3,4) of the part-of-speech tags of the words associated with the vertices in p, (e) the
n-grams (n=2,3,4) of the dependency types associated with the edges in p, and (f) the length of p.
</figureCaption>
<tableCaption confidence="0.937108">
Table 1: Features for trigger labeling and argument labeling.
</tableCaption>
<subsectionHeader confidence="0.999942">
3.2 Argument Labeling
</subsectionHeader>
<bodyText confidence="0.991614461538462">
The argument classifier is trained as follows. Each
training instance corresponds to a candidate trigger
and one of its candidate arguments.2 A candidate
argument for a candidate trigger ct is either a pro-
tein or a candidate trigger that appears in the same
sentence as ct. If ct is not a true trigger, the label of
the associated instance is set to None. On the other
hand, if ct is a true trigger, we check whether the
candidate argument in the associated instance is in-
deed one of ct’s arguments. If so, the class label of
the instance is the argument’s role; otherwise, the
class label is None. The features used for repre-
senting each training instance, which are modeled
after those used in Miwa et al. (2010b) and Bj¨orne
and Salakoski (2013), are shown in Table 1(b).
After training, we can apply the resulting clas-
sifier to classify the test instances, which are cre-
ated in the same way as the training instances. If
a test instance is assigned the class None by the
classifier, the corresponding candidate argument is
classified as not an argument of the trigger. Other-
2Following the definition of the GENIA event extraction
task, the protein names are provided as part of the input.
wise, the candidate argument is a true argument of
the trigger whose role is the class value assigned
by the classifier.
</bodyText>
<sectionHeader confidence="0.998127" genericHeader="method">
4 Joint Model
</sectionHeader>
<bodyText confidence="0.999988">
In this section, we describe our Markov logic model
that encodes the relational dependencies in the
shared task and uses the output of the pipeline
model as prior knowledge (soft evidence). We be-
gin by describing the structure of our Markov logic
model, and then describe the parameter learning
and inference algorithms for it.
</bodyText>
<subsectionHeader confidence="0.998953">
4.1 MLN Structure
</subsectionHeader>
<bodyText confidence="0.9997158">
Figure 2 shows our proposed MLN for BioNLP
event extraction, which we refer to as BioMLN.
The MLN contains six predicates.
The query predicates in Figure 2(a) are those
whose assignments are not given during infer-
ence and thus need to be predicted. Predicate
TriggerType(sid,tid,ttype!) is true when the
token located in sentence sid at position tid has
type ttype. Δttype, which denotes the set of con-
stants (or objects) that the logical variable ttype
</bodyText>
<page confidence="0.983501">
835
</page>
<figure confidence="0.846884">
TriggerType(sid,tid,ttype!) Simple(sid,tid) Word(sid,tid,word)
ArgumentRole(sid,aid,tid,arole!) Regulation(sid,tid) DepType(sid,aid,tid,dtype)
(a) Query (b) Hidden (c) Evidence
</figure>
<listItem confidence="0.993986">
1. Elt TriggerType(i,j,t).
2. Ela ArgumentRole(i,k,j,a).
3. -TriggerType(i,j,None) ==&gt; Elk ArgumentRole(i,k,j,Theme).
4. Simple(i,j) ==&gt; - Elk ArgumentRole(i,k,j,Cause).
5. TriggerType(i,j,None) #s ArgumentRole(i,k,j,None).
6. -ArgumentRole(i,k,j,None) n -TriggerType(i,k,None) ==&gt; Regulation(i,j).
7. Simple(i,j) #s TriggerType(i,j,Simplel) V ... V TriggerType(i,j,Binding).
8. Regulation(i,j) #s TriggerType(i,j,Reg) V TriggerType(i,j,PosReg)
V TriggerType(i,j,NegReg).
9. Word(i,j,+w) n TriggerType(i,j,+t) n DepType(i,k,j,+d) n ArgumentRole(i,k,j,+a)
(d) Joint Formulas
</listItem>
<figureCaption confidence="0.99904">
Figure 2: The BioMLN structure.
</figureCaption>
<bodyText confidence="0.996788666666667">
can be instantiated to, includes all possible trigger
types in the dataset plus None (which indicates
that the token is not a trigger). The “!” symbol mod-
els commonsense knowledge that only one of the
types in the domain Δttype of ttype is true for every
unique combination of sid and tid. Similarly, pred-
icate ArgumentRole(sid,aid,tid,arole!) as-
serts that a token in sentence sid at position aid
plays exactly one argument role, denoted by arole,
with respect to the token at position tid. Δarole
includes the two argument types, namely, Theme
and Cause plus the additional None that indicates
that the token is not an argument.
The hidden predicates in Figure 2(b) are “clus-
ters” of trigger types. Predicate Simple(sid,tid)
is true when the token in sentence sid at posi-
tion tid corresponds to one of the Simple event
trigger types (BioNLP’13 has 9 simple events,
BioNLP’09/’11 have 5) or a binding event trig-
ger type. Similarly, Regulation(sid,tid) asserts
that the token in sentence sid at position tid corre-
sponds to any of the three regulation event trigger
types.
The evidence predicates in Figure 2(c) are those
that are always assumed to be known during in-
ference. We define two evidence predicates based
on dependency structures. Word(sid,tid,word) is
true when the word in sentence sid at position tid
is equal to word. DepType(sid,aid,tid,dtype)
asserts that dtype is the dependency type in the de-
pendency parse tree that connects the token at posi-
tion tid to the token at position aid in sentence sid.
If the word at tid and the word at aid are directly
connected in the dependency tree, then dtype is the
label of dependency edge with direction; otherwise
dtype is None.
The MLN formulas, expressing commonsense,
prior knowledge in the domain (Poon and Van-
derwende, 2010; Riedel and McCallum, 2011a),
are shown in Fig. 2(d). All formulas, except For-
mula (9), are hard formulas, meaning that they have
infinite weights. Note that during weight learning,
we only learn the weights of soft formulas.
Formulas (1) and (2) along with the “!” con-
straint in the predicate definition ensure that the
token types are mutually exclusive and exhaustive.
Formula (3) asserts that every trigger should have
an argument of type Theme, since a Theme argu-
ment is mandatory for any event. Formula (4) mod-
els the constraint that a Simple or Binding trigger
has no arguments of type Cause since only regu-
lation events have a Cause. Formula (5) asserts
that non-triggers have no arguments and vice-versa.
Formula (6) models the constraint that if a token
is both an argument of t and a trigger by itself,
then t must belong to one of the three regulation
trigger types. This formula captures the recursive
relationship between triggers. Formulas (7) and
(8) connect the hidden predicates with the query
predicates. Formula (9) is a soft formula encoding
</bodyText>
<page confidence="0.99339">
836
</page>
<bodyText confidence="0.999816375">
the relationship between triggers and arguments in
a dependency parse tree. It joins a word and the
dependency type label that connects the word token
to the argument token in the dependency parse tree
with the trigger types and argument types of the
two tokens. The “+” symbol indicates that each
grounding of Formula (9) may have a different
weight.
</bodyText>
<subsectionHeader confidence="0.994447">
4.2 Weight Learning
</subsectionHeader>
<bodyText confidence="0.999735416666667">
We can learn BioMLN from data either discrimina-
tively or generatively. Since discriminative learning
is much faster than generative learning, we use the
former. In discriminative training, we maximize
the conditional log-likelihood (CLL) of the query
and the hidden variables given an assignment to
the evidence variables. In principle, we can use the
standard gradient descent algorithm for maximiz-
ing the CLL. In each iteration of gradient descent,
we update the weights using the following equation
(cf. Singla and Domingos (2005) and Domingos
and Lowd (2009)):
</bodyText>
<equation confidence="0.99539">
wt+1
j = wtj − α(Ew(nj) − nj) (3)
</equation>
<bodyText confidence="0.9999375">
where wt j represents the weight of the jth formula
in the tth iteration, nj is the number of groundings
in which the jth formula is satisfied in the training
data, Ew(nj) is the expected number of ground-
ings in which the jth formula is satisfied given the
current weight vector w, and α is the learning rate.
As such, the update rule given in Equation (3)
is likely to yield poor accuracy because the num-
ber of training examples of some types (e.g.,
None) far outnumber other types. To rectify this
ill-conditioning problem (Singla and Domingos,
2005; Lowd and Domingos, 2007), we divide the
gradient with the number of true groundings in
the data, namely, we compute the gradient using
</bodyText>
<equation confidence="0.8134165">
(Ew(nj)−nj)
nj .
</equation>
<bodyText confidence="0.9999702">
Another key issue with using Equation (3) is that
computing Ew(nj) requires performing inference
over the MLN. This step is intractable, #P-complete
in the worst case. To circumvent this problem and
for fast, scalable training, we instead propose to
use the voted perceptron algorithm (Collins, 2002;
Singla and Domingos, 2005). This algorithm ap-
proximates Ew(nj) by counting the number of
satisfied groundings of each formula in the MAP
assignment. Computing the MAP assignment is
much easier (although still NP-hard in the worst
case) than computing Ew(nj), and as a result the
voted perceptron algorithm is more scalable than
the standard gradient descent algorithm. In addi-
tion, it converges much faster.
</bodyText>
<subsectionHeader confidence="0.998396">
4.3 Testing
</subsectionHeader>
<bodyText confidence="0.999808526315789">
In the testing phase, we combine BioMLN with the
output of the pipeline model (see Section 3) to ob-
tain a new MLN, which we refer to as BioMLN+.
For every candidate trigger, the SVM trigger clas-
sifier outputs a vector of signed confidence val-
ues (which is proportional to the distance from
the separating hyperplane) of dimension Dttype
with one entry for each trigger type. Similarly,
for every candidate argument, the SVM argu-
ment classifier outputs a vector of signed confi-
dence values of dimension Darole with one en-
try for each argument role. In BioMLN+, we
model the SVM output as soft evidence, using
two soft unit clauses, TriggerType(i,+j,+t) and
ArgumentRole(i,+k,+j,+a). We use the con-
fidence values to determine the weights of these
clauses. Intuitively, higher (smaller) the confidence,
higher (smaller) the weight.
Specifically, the weights of the soft unit clauses
are set as follows. If the SVM trigger classifier
determines that the trigger in sentence i at po-
sition j belongs to type t with confidence Ci,j,
then we attach a weight of Ci,j
αni to the clause
TriggerType(i,j,t). Here, ni denotes the num-
ber of trigger candidates in sentence i. Similarly,
if the SVM argument classifier determines that the
token at position k in sentence i belongs to the ar-
gument role a with respect to the token at position
j, with confidence C&apos;i,k,j, then we attach a weight
of
Cik,j to the clause ArgumentRole(i, k,
β ni
j,a). Here, mij denotes the number of argument
candidates for the jth trigger candidate in sentence
i. α and Q act as scale parameters for the confi-
dence values ensuring that the weights don’t get
too large (or too small).
</bodyText>
<subsectionHeader confidence="0.761551">
4.4 Inference
</subsectionHeader>
<bodyText confidence="0.999942333333333">
As we need to perform MAP inference, both at
training time and at test time, in this subsection we
will describe how to do it efficiently by exploiting
unique properties of our proposed BioMLN.
Naively, we can perform MAP inference by
grounding BioMLN to a Markov network and
then reducing the Markov network by removing
from it all (grounded propositional) formulas that
are inconsistent with the evidence. On the re-
</bodyText>
<page confidence="0.994247">
837
</page>
<bodyText confidence="0.998995304347826">
duced Markov network, we can then compute the
MAP solution using standard MAP solvers such as
MaxWalkSAT (a state-of-the-art local search based
MAP solver) (Selman et al., 1996) and Gurobi3 (a
state-of-the-art, parallelized ILP solver).
The problem with the above approach is that
grounding the MLN is infeasible in practice; even
the reduced Markov network is just too large. For
example, assuming a total of |Asid |sentences and
a maximum of N tokens in a sentence, Formula (3)
alone has O(|Asid|N3) groundings. Concretely, at
training time, assuming 1000 sentences with 10
tokens per sentence, Formula (3) itself yields one
million groundings. Clearly, this approach is not
scalable. It turns out, however, that the (ground)
Markov network can be decomposed into several
disconnected components, each of which can be
solved independently. This greatly reduces the
memory requirement of the inference step. Specif-
ically, for every grounding of sid, we get a set of
nodes in the Markov network that are disconnected
from the rest of the Markov network and therefore
independent of the rest of the network. Formally,
</bodyText>
<equation confidence="0.9239525">
Proposition 1. For any world w of the BioMLN,
PM(w) = PMi(wi)PM\Mi(w \ wi) (4)
</equation>
<bodyText confidence="0.9292702">
where wi is the world w projected on the ground-
ings of sentence i and Mi is BioMLN grounded
only using sentence i.
Using Equation (4), it is easy to see that the MLN
M can be decomposed into |Asid |disjoint MLNs,
</bodyText>
<equation confidence="0.9158896">
{Mk}|Δsid|
k=1 . The MAP assignment to M can be
|Δsid|
computed using, U C wiarg max PMi (wi)� . This
i=1
</equation>
<bodyText confidence="0.841752666666667">
result ensures that to approximate the expected
counts Ew(nj), it is sufficient to keep exactly one
sentence’s groundings in memory. Specifically,
</bodyText>
<equation confidence="0.992116">
Ew(nj) can be written as E|Δsid|
k=1 Ew(nkj ), where
</equation>
<bodyText confidence="0.9998502">
Ew(nkj) indicates the expected number of satisfied
groundings of the jth formula in the kth sentence.
Since the MAP computation is decomposable, we
can estimate Ew(nkj) using MAP inference on just
the kth sentence.
</bodyText>
<sectionHeader confidence="0.999711" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.996453">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9993725">
We evaluate our system on the BioNLP’13 (Kim
et al., 2013), ’11 (Kim et al., 2011a) and ’09 (Kim
</bodyText>
<footnote confidence="0.927586">
3http://www.gurobi.com/
</footnote>
<table confidence="0.9988395">
Dataset #Papers #Abstracts #TT #Events
BioNLP’13 (10,10,14) (0,0,0) 13 (2817,3199,3348)
BioNLP’11 (5,5,4) (800,150,260) 9 (10310,4690,5301)
BioNLP’09 (0,0,0) (800,150,260) 9 (8597,1809,3182)
</table>
<tableCaption confidence="0.993078">
Table 2: Statistics on the BioNLP datasets, which
</tableCaption>
<bodyText confidence="0.986929711111111">
consist of annotated papers/abstracts from PubMed.
(x, y, z): x in training, y in development and z
in test. #TT indicates the total number of trigger
types. The total number of argument types is 2.
et al., 2009) Genia datasets for the main event ex-
traction shared task. Note that this task is the most
important one for Genia and therefore has the most
active participation. Statistics on the datasets are
shown in Table 2. All our evaluations use the on-
line tool provided by the shared task organizers.
We report scores obtained using the approximate
span, recursive evaluation.
To generate features, we employ the support-
ing resources provided by the organizers. Specif-
ically, sentence split and tokenization are done
using the GENIA tools, while part-of-speech in-
formation is provided by the BLLIP parser that
uses the self-trained biomedical model (McClosky,
2010). Also, we create dependency features from
the parse trees provided by two dependency parsers,
the Enju parser (Miyao and Tsujii, 2008) and the
aforementioned BLLIP parser that uses the self-
trained biomedical model, which results in two sets
of dependency features.
For MAP inference, we use Gurobi, a par-
allelized ILP solver. After inference, a post-
processing step is required to generate biomedi-
cal events from the extracted triggers and argu-
ments. Specifically, for binding events, we em-
ploy a learning-based method similar to Bj¨orne and
Salakoski (2011), while for the other events, we
employ a rule-based approach similar to Bj¨orne et
al. (2009). Both the SVM baseline system and the
combined MLN+SVM system employ the same
post-processing strategy.
During weight learning, in order to combat the
problem of different initializations yielding radi-
cally different parameter estimates, we start at sev-
eral different initialization points and average the
weights obtained after 100 iterations of gradient
descent. However, we noticed that if we simply
choose random initialization points, the variance of
the weights was quite high and some initialization
points were much worse than others. To counter
this, we use the following method to systematically
</bodyText>
<page confidence="0.997925">
838
</page>
<note confidence="0.889852428571429">
System Rec. Prec. F1
Our System 48.95 59.24 53.61
EVEX (Hakala et al., 2013) 45.44 58.03 50.97
TEES-2.1 (Bj¨orne and Salakoski, 2013) 46.17 56.32 50.74
BIOSEM (Bui et al., 2013) 42.47 62.83 50.68
NCBI (Liu et al., 2013) 40.53 61.72 48.93
DLUTNLP (Li et al., 2013a) 40.81 57.00 47.56
</note>
<tableCaption confidence="0.7976885">
Table 3: Recall (Rec.), Precision (Prec.) and F1
score on the BioNLP’13 test data.
</tableCaption>
<bodyText confidence="0.945632133333333">
initialize the weights. Let ni be the number of sat-
isfied groundings of formula fi in the training data
and mi be the total number of possible groundings
of fi. We use a threshold -y to determine whether
we wish to make the initial weight positive or neg-
ative. If ni G -y, then we choose the initial weight
mi
uniformly at random from the range [−0.1, 0]. Oth-
erwise, we chose it from the range [0, 0.1]. These
steps ensure that the weights generated from dif-
ferent initialization points have smaller variance.
Also, in the testing phase, we set the scale parame-
ters for the soft evidence as α = Q = max |c|, where
c∈C
C is the set of SVM confidence values.
</bodyText>
<subsectionHeader confidence="0.957287">
5.2 Results on the BioNLP’13 Dataset
</subsectionHeader>
<bodyText confidence="0.999952703703704">
Among the three datasets, the BioNLP’13 dataset
is most “realistic” one because it is the only one
that contains full papers and no abstracts. As a re-
sult, it is also the most challenging dataset among
the three. Table 3 shows the results of our system
along with the results of other top systems pub-
lished in the official evaluation of BioNLP’13. Our
system achieves the best F1-score (an improvement
of 2.64 points over the top-performing system) and
has a much higher recall (mainly because our sys-
tem detects more regulation events which outnum-
ber other event types in the dataset) and a slightly
higher precision than the winning system. Of the
top five teams, NCBI is the only other joint infer-
ence system, which adopts joint pattern matching
to predict triggers and arguments at the same time.
These results illustrate the challenge in using joint
inference effectively. NCBI performed much worse
than the SVM-based pipeline systems, EVEX and
TEES2.1. It was also worse than BIOSEM, a rule-
based system that uses considerable domain exper-
tise. Nevertheless, it was better than DLUTNLP,
another SVM-based system.
Figure 3 compares our baseline pipeline model
with our combined model. We can clearly see that
the combined model has a significantly better F1
score than the pipeline model on most event types.
</bodyText>
<note confidence="0.953854166666667">
System Rec. Prec. F1
Our System 53.42 63.61 58.07
Miwa12 (Miwa et al., 2012) 53.35 63.48 57.98
Riedel11 (Riedel et al., 2011) − − 56
UTurku (Bj¨orne and Salakoski, 2011) 49.56 57.65 53.30
MSR-NLP (Quirk et al., 2011) 48.64 54.71 51.50
</note>
<tableCaption confidence="0.997442">
Table 4: Results on the BioNLP’11 test data.
</tableCaption>
<bodyText confidence="0.999707888888889">
The regulation events are considered the most com-
plex events to detect because they have a recursive
structure. At the same time, this structure yields a
large number of joint dependencies. The advantage
of using a rich model such as MLNs can be clearly
seen in this case; the combined model yields a 10
point and 6 point increase in F1-score on the test
data and development data respectively compared
to the pipeline model.
</bodyText>
<subsectionHeader confidence="0.9578">
5.3 Results on the BioNLP’11 Dataset
</subsectionHeader>
<bodyText confidence="0.999939333333333">
Table 4 shows the results on the BioNLP’11 dataset.
We can see that our system is marginally better than
Miwa12, which is a pipeline-based system. It is
also more than two points better than Riedel11,
a state-of-the-art structured prediction-based joint
inference system. Reidel11 incorporates the Stan-
ford predictions (McClosky et al., 2011b) as fea-
tures in the model. On the two hardest, most
complex tasks, detecting regulation events (which
have recursive structures and more joint dependen-
cies than other event types) and detecting bind-
ing events (which may have multiple arguments),
our system performs better than both Miwa12 and
Riedel11.4 Specifically, our system’s F1 score for
regulation events is 46.84, while those of Miwa12
and Riedel11 are 45.46 and 44.94 respectively. Our
system’s F1 score for the binding event is 58.79,
while those of Miwa12 and Riedel11 are 56.64 and
48.49 respectively. These results clearly demon-
strate the effectiveness of enforcing joint dependen-
cies along with high-dimensional features.
</bodyText>
<subsectionHeader confidence="0.958203">
5.4 Results on the BioNLP’09 Dataset
</subsectionHeader>
<bodyText confidence="0.997347">
Table 5 shows the results on the BioNLP’09 dataset.
Our system has a marginally lower score (by 0.11
points) than Miwa12, which is the best performing
system on this dataset. Specifically, our system
achieves a higher recall but a lower precision than
Miwa12. However, note that Miwa12 used co-
reference features while we are able to achieve
</bodyText>
<footnote confidence="0.9854155">
4Detailed results are not shown for any of these three
datasets due to space limitations.
</footnote>
<page confidence="0.991869">
839
</page>
<table confidence="0.995431933333333">
SVM MLN+SVM
Type Rec. Prec. F1 Rec. Prec. F1
Simple 64.47 87.89 74.38 73.11 78.99 75.94
Protein-Mod 66.49 79.87 72.57 72.25 69.70 70.95
Binding 39.04 50.00 43.84 48.05 43.84 45.85
Regulation 23.51 56.21 33.15 36.47 50.86 42.48
Overall 37.90 67.88 48.64 48.95 59.24 53.61
(a) Test
SVM MLN+SVM
Type Rec. Prec. F1 Rec. Prec. F1
Simple 55.79 81.63 66.28 63.21 75.10 68.64
Protein-Mod 64.47 87.89 74.38 71.14 85.63 77.72
Binding 31.90 48.77 38.57 47.99 50.00 48.97
Regulation 20.13 52.46 29.10 28.57 43.41 34.46
Overall 34.42 66.14 45.28 43.50 57.45 49.51
</table>
<figure confidence="0.900347">
(b) Development
</figure>
<figureCaption confidence="0.9681375">
Figure 3: Comparison of the combined model (MLN+SVM) with the pipeline model on the BioNLP’13
test and development data.
</figureCaption>
<table confidence="0.999437">
System Rec. Prec. F1
Miwa12 (Miwa et al., 2012) 52.67 65.19 58.27
Our System 53.96 63.08 58.16
Riedel11 (Riedel et al., 2011) − − 57.4
Miwa10 (Miwa et al., 2010a) 50.13 64.16 56.28
Bjorne (Bj¨orne et al., 2009) 46.73 58.48 51.95
PoonMLN (Poon&amp;Vanderwende,2010) 43.7 58.6 50.0
RiedelMLN (Riedel et al., 2009) 36.9 55.6 44.4
</table>
<tableCaption confidence="0.995283">
Table 5: Results on the BioNLP’09 test data. “−”
</tableCaption>
<bodyText confidence="0.980358555555556">
indicates that the corresponding values are not
known.
similar accuracy without the use of co-reference
data. The F1 score of Miwa10, which does not
use co-reference features, is nearly 2 points lower
than that of our system. Our system also has a
higher F1 score than Reidel11, which is the best
joint inference-based system for this task.
On the regulation events, our system (47.55) out-
performs both Miwa12 (45.99) and Riedel11 (46.9),
while on the binding event, our system (59.88) is
marginally worse than Miwa12 (59.91) and signifi-
cantly better than Riedel11 (52.6). As mentioned
earlier, these are the hardest events to extract. Also,
existing MLN-based joint inference systems such
as RiedelMLN and PoonMLN do not achieve state-
of-the-art results because they do not leverage com-
plex, high-dimensional features.
</bodyText>
<sectionHeader confidence="0.972476" genericHeader="conclusions">
6 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.999962810810811">
Markov logic networks (MLNs) are a powerful
representation that can compactly encode rich rela-
tional structures and ambiguities (uncertainty). As
a result, they are an ideal representation for com-
plex NLP tasks that require joint inference, such
as event extraction. Unfortunately, the superior
representational power greatly complicates infer-
ence and learning over MLN models. Even the
most advanced methods for inference and learning
in MLNs (Gogate and Domingos, 2011) are un-
able to handle complex, high-dimensional features,
and therefore existing MLN systems primarily use
low-dimensional features. This limitation severely
affects the accuracy of MLN-based NLP systems,
and as a result, in some cases their performance
is inferior to pipeline methods that do not employ
joint inference.
In this paper, we presented a general approach
for exploiting the power of high-dimensional lin-
guistic features in MLNs. Our approach involves
reliably processing and learning high-dimensional
features using SVMs and encoding their output as
low-dimensional features in MLNs. We showed
that we could achieve scalable learning and in-
ference in our proposed MLN model by exploit-
ing decomposition. Our results on the BioNLP
shared tasks from ’13, ’11, and ’09 clearly show
that our proposed combination is extremely effec-
tive, achieving the best or second best score on all
three datasets.
In future work, we plan to (1) improve our joint
model by incorporating co-reference information
and developing model ensembles; (2) transfer the
results of this investigation to other complex NLP
tasks that can potentially benefit from joint infer-
ence; and (3) develop scalable inference and learn-
ing algorithms (Ahmadi et al., 2013).
</bodyText>
<sectionHeader confidence="0.99698" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999892545454546">
This work was supported in part by the AFRL un-
der contract number FA8750-14-C-0021, by the
ARO MURI grant W911NF-08-1-0242, and by the
DARPA Probabilistic Programming for Advanced-
Machine Learning Program under AFRL prime
contract number FA8750-14-C-0005. Any opin-
ions, findings, conclusions, or recommendations
expressed in this paper are those of the authors
and do not necessarily reflect the views or official
policies, either expressed or implied, of DARPA,
AFRL, ARO or the US government.
</bodyText>
<page confidence="0.993124">
840
</page>
<sectionHeader confidence="0.982036" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99902779245283">
Babak Ahmadi, Kristian Kersting, Martin Mladenov,
and Sriraam Natarajan. 2013. Exploiting symme-
tries for scaling loopy belief propagation and rela-
tional training. Machine Learning, 92(1):91–132.
David Ahn. 2006. The stages of event extraction.
In Proceedings of the Workshop on Annotating and
Reasoning About Time and Events, pages 1–8.
Jari Bj¨orne and Tapio Salakoski. 2011. Generaliz-
ing biomedical event extraction. In Proceedings of
the BioNLP Shared Task 2011 Workshop, pages 183–
191.
Jari Bj¨orne and Tapio Salakoski. 2013. TEES 2.1: Au-
tomated annotation scheme learning in the bionlp
2013 shared task. In Proceedings of the BioNLP
Shared Task 2013 Workshop, pages 16–25.
Jari Bj¨orne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 10–18.
Quoc-Chinh Bui, David Campos, Erik van Mulligen,
and Jan Kors. 2013. A fast rule-based approach
for biomedical event extraction. In Proceedings of
the BioNLP Shared Task 2013 Workshop, pages 104–
108.
Chen Chen and Vincent Ng. 2012. Joint modeling for
Chinese event extraction with rich linguistic features.
In Proceedings of the 24th International Conference
on Computational Linguistics, pages 529–544.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, Uni-
versity of Pennsylvania, Philadelphia, PA.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1–8.
Pedro Domingos and Daniel Lowd. 2009. Markov
Logic: An Interface Layer forArtificial Intelligence.
Morgan &amp; Claypool, San Rafael, CA.
Lise Getoor and Ben Taskar, editors. 2007. Introduc-
tion to Statistical Relational Learning. MIT Press.
Vibhav Gogate and Pedro Domingos. 2011. Proba-
bilistic theorem proving. In Proceedings of the 27th
Conference on Uncertainty in Artificial Intelligence,
pages 256–265.
Ralph Grishman, David Westbrook, and Adam Meyers.
2005. NYU’s English ACE 2005 system description.
In Proceedings of the ACE 2005 Evaluation Work-
shop. Washington.
Prashant Gupta and Heng Ji. 2009. Predicting un-
known time arguments based on cross-event prop-
agation. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 369–372.
Kai Hakala, Sofie Van Landeghem, Tapio Salakoski,
Yves Van de Peer, and Filip Ginter. 2013. EVEX
in ST’13: Application of a large-scale text mining
resource to event extraction and network construc-
tion. In Proceedings of the BioNLP Shared Task
2013 Workshop, pages 26–34.
Ruihong Huang and Ellen Riloff. 2012a. Bootstrapped
training of event extraction classifiers. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 286–295.
Ruihong Huang and Ellen Riloff. 2012b. Modeling
textual cohesion for event extraction. In Proceed-
ings of the 26th AAAI Conference on Artificial Intel-
ligence.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 254–262.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Schlkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press, Cambridge,
MA, USA.
Jin-Dong Kim, Tomoko Ohta, and Jun’ichi Tsujii.
2008. Corpus annotation for mining biomedi-
cal events from literature. BMC bioinformatics,
9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun’ichi Tsujii. 2009. Overview of
BioNLP’09 shared task on event extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1–9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun’ichi Tsujii. 2011a.
Overview of BioNLP shared task 2011. In Pro-
ceedings of the BioNLP Shared Task 2011 Workshop,
pages 1–6.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of Genia event
task in BioNLP shared task 2011. In Proceedings
of the BioNLP Shared Task 2011 Workshop, pages
7–15.
Jin-Dong Kim, Yue Wang, and Yamamoto Yasunori.
2013. The Genia event extraction shared task, 2013
edition - overview. In Proceedings of the BioNLP
Shared Task 2013 Workshop, pages 8–15.
Daphne Koller and Nir Friedman. 2009. Probabilistic
Graphical Models: Principles and Techniques. MIT
Press.
</reference>
<page confidence="0.99134">
841
</page>
<reference confidence="0.999091268518519">
Peifeng Li, Guodong Zhou, Qiaoming Zhu, and Libin
Hou. 2012. Employing compositional semantics
and discourse consistency in Chinese event extrac-
tion. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 1006–1016.
Lishuang Li, Yiwen Wang, and Degen Huang. 2013a.
Improving feature-based biomedical event extrac-
tion system by integrating argument information. In
Proceedings of the BioNLP Shared Task 2013 Work-
shop, pages 109–115.
Peifeng Li, Qiaoming Zhu, and Guodong Zhou. 2013b.
Argument inference from relevant event mentions in
Chinese argument extraction. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 1477–1487.
Qi Li, Heng Ji, and Liang Huang. 2013c. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
pages 73–82.
Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event
extraction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 789–797.
Shasha Liao and Ralph Grishman. 2011. Acquiring
topic features to improve event extraction: in pre-
selected and balanced collections. In Proceedings
of the International Conference Recent Advances in
Natural Language Processing 2011, pages 9–16.
Haibin Liu, Karin Verspoor, Donald C. Comeau, An-
drew MacKinlay, and W John Wilbur. 2013. Gen-
eralizing an approximate subgraph matching-based
system to extract events in molecular biology and
cancer genetics. In Proceedings of the BioNLP
Shared Task 2013 Workshop, pages 76–85.
Daniel Lowd and Pedro Domingos. 2007. Efficient
weight learning for markov logic networks. In
Proceedings of the 11th European Conference on
Principles and Practice of Knowledge Discovery in
Databases, pages 200–211.
Wei Lu and Dan Roth. 2012. Automatic event extrac-
tion with structured preference modeling. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 835–844.
Radu Marinescu and Rina Dechter. 2009. AND/OR
branch-and-bound search for combinatorial opti-
mization in graphical models. Artificial Intelligence,
173(16-17):1457–1491.
David McClosky, Mihai Surdeanu, and Chris Manning.
2011a. Event extraction as dependency parsing. In
Proceedings of the Association for Computational
Linguistics: Human Language Technologies, pages
1626–1635.
David McClosky, Mihai Surdeanu, and Christopher
Manning. 2011b. Event extraction as dependency
parsing for BioNLP 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 41–45.
David McClosky. 2010. Any domain parsing: Auto-
matic domain adaptation for natural language pars-
ing. Ph.D. thesis, Ph.D. thesis, Brown University,
Providence, RI.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun’ichi Tsujii. 2010a. Evaluating dependency rep-
resentation for event extraction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 779–787.
Makoto Miwa, Rune Sætre, Jin-Dong Kim, and
Jun’ichi Tsujii. 2010b. Event extraction with com-
plex event classification using rich features. Jour-
nal of Bioinformatics and Computational Biology,
8(01):131–146.
Makoto Miwa, Paul Thompson, and Sophia Ananiadou.
2012. Boosting automatic event extraction from the
literature using domain adaptation and coreference
resolution. Bioinformatics, 28(13):1759–1765.
Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35–80.
Claire N´edellec, Robert Bossy, Jin-Dong Kim, Jung-
Jae Kim, Tomoko Ohta, Sampo Pyysalo, and Pierre
Zweigenbaum. 2013. Overview of BioNLP shared
task 2013. In Proceedings of the BioNLP Shared
Task 2013 Workshop, pages 1–7.
Siddharth Patwardhan and Ellen Riloff. 2009. A uni-
fied model of phrasal and sentential evidence for in-
formation extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–160.
Hoifung Poon and Pedro Domingos. 2007. Joint in-
ference in information extraction. In Proceedings
of the 22nd National Conference on Artificial Intelli-
gence, pages 913–918.
Hoifung Poon and Lucy Vanderwende. 2010. Joint
inference for knowledge extraction from biomedi-
cal literature. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 813–821.
Martin F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3):130–137.
Chris Quirk, Pallavi Choudhury, Michael Gamon, and
Lucy Vanderwende. 2011. MSR-NLP entry in
BioNLP shared task 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 155–
163.
</reference>
<page confidence="0.977541">
842
</page>
<reference confidence="0.999613934782609">
Sebastian Riedel and Andrew McCallum. 2011a. Fast
and robust joint models for biomedical event extrac-
tion. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1–12.
Sebastian Riedel and Andrew McCallum. 2011b. Ro-
bust biomedical event extraction with dual decom-
position and minimal domain adaptation. In Pro-
ceedings of the BioNLP Shared Task 2011 Workshop,
pages 46–50.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun’ichi Tsujii. 2009. A Markov logic approach
to bio-molecular event extraction. In Proceedings of
the BioNLP 2009 Workshop Companion Volume for
Shared Task, pages 41–49.
Sebastian Riedel, David McClosky, Mihai Surdeanu,
Andrew McCallum, and Christopher D. Manning.
2011. Model combination for event extraction in
bionlp 2011. In Proceedings of the BioNLP Shared
Task 2011 Workshop, pages 51–55.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from Twit-
ter. In Proceedings of the 18th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining, pages 1104–1112.
Bart Selman, Henry Kautz, and Bram Cohen. 1996.
Local Search Strategies for Satisfiability Testing. In
D. S. Johnson and M. A. Trick, editors, Cliques,
Coloring, and Satisfiability: Second DIMACS Im-
plementation Challenge, pages 521–532. American
Mathematical Society, Washington, DC.
Parag Singla and Pedro Domingos. 2005. Discrimina-
tive training of Markov logic networks. In Proceed-
ings of the 20th National Conference on Artificial
Intelligence, pages 868–873.
David Sontag and Amir Globerson. 2011. Introduction
to Dual Decomposition for Inference. Optimization
for Machine Learning.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vector
machine learning for interdependent and structured
output spaces. In Proceedings of the 21st Interna-
tional Conference on Machine Learning, pages 104–
112.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer, New York, NY.
</reference>
<page confidence="0.999184">
843
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.233827">
<title confidence="0.9927555">Relieving the Computational Bottleneck: Joint Inference Event Extraction with High-Dimensional Features</title>
<author confidence="0.99954">Venugopal Chen Gogate</author>
<affiliation confidence="0.9969735">Department of Computer Science and Human Language Technology Research University of Texas at</affiliation>
<address confidence="0.549154">Richardson, TX</address>
<abstract confidence="0.961764161290323">Several state-of-the-art event extraction systems employ models based on Support Vector Machines (SVMs) in a pipeline architecture, which fails to exploit the joint dependencies that typically exist among events and arguments. While there have been attempts to overcome this limitation using Markov Logic Networks (MLNs), it remains challenging to perform joint inference in MLNs when the model encodes many high-dimensional sophisticated features such as those essential for event extraction. In this paper, we propose a new model for event extraction that combines the power of MLNs and SVMs, dwarfing their limitations. The key idea is to reliably learn and process high-dimensional features using SVMs; encode the output of SVMs as low-dimensional, soft formulas in MLNs; and use the superior joint inferencing power of MLNs to enforce joint consistency constraints over the soft formulas. We evaluate our approach for the task of extracting biomedical events on the BioNLP 2013, 2011 and 2009 Genia shared task datasets. Our approach yields the best F1 score to date on the BioNLP’13 (53.61) and BioNLP’11 (58.07) datasets and the second-best F1 score to date on the BioNLP’09 dataset (58.16).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Babak Ahmadi</author>
<author>Kristian Kersting</author>
<author>Martin Mladenov</author>
<author>Sriraam Natarajan</author>
</authors>
<title>Exploiting symmetries for scaling loopy belief propagation and relational training.</title>
<date>2013</date>
<booktitle>Machine Learning,</booktitle>
<volume>92</volume>
<issue>1</issue>
<marker>Ahmadi, Kersting, Mladenov, Natarajan, 2013</marker>
<rawString>Babak Ahmadi, Kristian Kersting, Martin Mladenov, and Sriraam Natarajan. 2013. Exploiting symmetries for scaling loopy belief propagation and relational training. Machine Learning, 92(1):91–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ahn</author>
</authors>
<title>The stages of event extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Annotating and Reasoning About Time and Events,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="8949" citStr="Ahn, 2006" startWordPosition="1387" endWordPosition="1388"> part of the Message Understanding Conferences (MUCs), the Automatic Content Extraction (ACE) evaluations, and the BioNLP shared 832 tasks on event extraction. Previous work on event extraction can be broadly divided into two categories, one focusing on the development of features (henceforth feature-based approaches) and the other focusing on the development of models (henceforth model-based approaches). Feature-based approaches. Early work on feature-based approaches has primarily focused on designing local sentence-level features such as token and syntactic features (Grishman et al., 2005; Ahn, 2006). Later, it was realized that local features were insufficient to reliably and accurately perform event extraction in complex domains and therefore several researchers proposed using high-level features. For instance, Ji and Grishman (2008) used global information from related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based approaches. Th</context>
</contexts>
<marker>Ahn, 2006</marker>
<rawString>David Ahn. 2006. The stages of event extraction. In Proceedings of the Workshop on Annotating and Reasoning About Time and Events, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jari Bj¨orne</author>
<author>Tapio Salakoski</author>
</authors>
<title>Generalizing biomedical event extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the BioNLP Shared Task 2011 Workshop,</booktitle>
<pages>183--191</pages>
<marker>Bj¨orne, Salakoski, 2011</marker>
<rawString>Jari Bj¨orne and Tapio Salakoski. 2011. Generalizing biomedical event extraction. In Proceedings of the BioNLP Shared Task 2011 Workshop, pages 183– 191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jari Bj¨orne</author>
<author>Tapio Salakoski</author>
</authors>
<title>TEES 2.1: Automated annotation scheme learning in the bionlp 2013 shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the BioNLP Shared Task 2013 Workshop,</booktitle>
<pages>16--25</pages>
<marker>Bj¨orne, Salakoski, 2013</marker>
<rawString>Jari Bj¨orne and Tapio Salakoski. 2013. TEES 2.1: Automated annotation scheme learning in the bionlp 2013 shared task. In Proceedings of the BioNLP Shared Task 2013 Workshop, pages 16–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jari Bj¨orne</author>
<author>Juho Heimonen</author>
<author>Filip Ginter</author>
<author>Antti Airola</author>
<author>Tapio Pahikkala</author>
<author>Tapio Salakoski</author>
</authors>
<title>Extracting complex biological events with rich graphbased feature sets.</title>
<date>2009</date>
<booktitle>In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task,</booktitle>
<pages>10--18</pages>
<marker>Bj¨orne, Heimonen, Ginter, Airola, Pahikkala, Salakoski, 2009</marker>
<rawString>Jari Bj¨orne, Juho Heimonen, Filip Ginter, Antti Airola, Tapio Pahikkala, and Tapio Salakoski. 2009. Extracting complex biological events with rich graphbased feature sets. In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc-Chinh Bui</author>
<author>David Campos</author>
<author>Erik van Mulligen</author>
<author>Jan Kors</author>
</authors>
<title>A fast rule-based approach for biomedical event extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the BioNLP Shared Task 2013 Workshop,</booktitle>
<pages>104--108</pages>
<marker>Bui, Campos, van Mulligen, Kors, 2013</marker>
<rawString>Quoc-Chinh Bui, David Campos, Erik van Mulligen, and Jan Kors. 2013. A fast rule-based approach for biomedical event extraction. In Proceedings of the BioNLP Shared Task 2013 Workshop, pages 104– 108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Chen</author>
<author>Vincent Ng</author>
</authors>
<title>Joint modeling for Chinese event extraction with rich linguistic features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics,</booktitle>
<pages>529--544</pages>
<contexts>
<context position="4987" citStr="Chen and Ng (2012)" startWordPosition="780" endWordPosition="783">AF2-dependent manner following engagement of CD40 ... (a) Sentence fragment ID Event Type Trigger Arguments E11 Binding recruited Theme=1HOIL-1L interacting protein,CD401 E12 Regulation dependent Theme=E11, Cause=TRAF2 E13 +ve Regulation following Theme=E12, Cause=E14 E14 Binding engagement Theme=CD40 (b) Events Figure 1: Example of event extraction in the BioNLP Genia task. The table in (b) shows all the events extracted from sentence (a). Note that successful extraction of E13 depends on E12 and E14. culty stems from the fact that some of these features are extremely high dimensional (e.g., Chen and Ng (2012), Huang and Riloff (2012b), Li et al. (2012), Li et al. (2013b), Li et al. (2013c)), and to reliably learn weights of formulas that encode such features, one would require an enormous number of data samples. Moreover, even the complexity of approximate inference on such models is quite high, often prohibitively so. For example, a trigram can be encoded as an MLN formula, Word(w1, p−1) ∧ Word(w2, p) ∧ Word(w3, p + 1) ==&gt;- Type(p, T). For any given position (p), this formula has W3 groundings, where W is the number of possible words, making it too large for learning/inference. Therefore, current</context>
</contexts>
<marker>Chen, Ng, 2012</marker>
<rawString>Chen Chen and Vincent Ng. 2012. Joint modeling for Chinese event extraction with rich linguistic features. In Proceedings of the 24th International Conference on Computational Linguistics, pages 529–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="28123" citStr="Collins, 2002" startWordPosition="4494" endWordPosition="4495">ecause the number of training examples of some types (e.g., None) far outnumber other types. To rectify this ill-conditioning problem (Singla and Domingos, 2005; Lowd and Domingos, 2007), we divide the gradient with the number of true groundings in the data, namely, we compute the gradient using (Ew(nj)−nj) nj . Another key issue with using Equation (3) is that computing Ew(nj) requires performing inference over the MLN. This step is intractable, #P-complete in the worst case. To circumvent this problem and for fast, scalable training, we instead propose to use the voted perceptron algorithm (Collins, 2002; Singla and Domingos, 2005). This algorithm approximates Ew(nj) by counting the number of satisfied groundings of each formula in the MAP assignment. Computing the MAP assignment is much easier (although still NP-hard in the worst case) than computing Ew(nj), and as a result the voted perceptron algorithm is more scalable than the standard gradient descent algorithm. In addition, it converges much faster. 4.3 Testing In the testing phase, we combine BioMLN with the output of the pipeline model (see Section 3) to obtain a new MLN, which we refer to as BioMLN+. For every candidate trigger, the </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Domingos</author>
<author>Daniel Lowd</author>
</authors>
<title>Markov Logic: An Interface Layer forArtificial Intelligence.</title>
<date>2009</date>
<publisher>Morgan &amp; Claypool,</publisher>
<location>San Rafael, CA.</location>
<contexts>
<context position="14299" citStr="Domingos and Lowd, 2009" startWordPosition="2224" endWordPosition="2227"> edition. Consequently, our model makes the simplifying assumption that a protein-mod event can only take one Theme argument, meaning that we are effectively processing protein-mod events in the same way as simple events. 2.3 Markov Logic Networks Statistical relational learning (SRL) (Getoor and Taskar, 2007) is an emerging field that seeks to unify logic and probability, and since most NLP techniques are grounded either in logic or probability or both, NLP serves as an ideal application domain for SRL. In this paper, we will employ a popular SRL approach called Markov logic networks (MLNs) (Domingos and Lowd, 2009). At a high level, an MLN is a set of weighted first-order logic formulas (fi, wi), where wi is the weight associated with formula fi. Given a set of constants that model objects in the domain, it defines a Markov network or a log-linear model (Koller and Friedman, 2009) in which we have one node per ground first-order atom and a propositional feature corresponding to each grounding of each first-order formula. The weight of the feature is the weight of the corresponding first-order formula. Formally, the probability of a world ω, which represents an assignment of values to all ground atoms in</context>
<context position="27082" citStr="Domingos and Lowd (2009)" startWordPosition="4313" endWordPosition="4316"> Formula (9) may have a different weight. 4.2 Weight Learning We can learn BioMLN from data either discriminatively or generatively. Since discriminative learning is much faster than generative learning, we use the former. In discriminative training, we maximize the conditional log-likelihood (CLL) of the query and the hidden variables given an assignment to the evidence variables. In principle, we can use the standard gradient descent algorithm for maximizing the CLL. In each iteration of gradient descent, we update the weights using the following equation (cf. Singla and Domingos (2005) and Domingos and Lowd (2009)): wt+1 j = wtj − α(Ew(nj) − nj) (3) where wt j represents the weight of the jth formula in the tth iteration, nj is the number of groundings in which the jth formula is satisfied in the training data, Ew(nj) is the expected number of groundings in which the jth formula is satisfied given the current weight vector w, and α is the learning rate. As such, the update rule given in Equation (3) is likely to yield poor accuracy because the number of training examples of some types (e.g., None) far outnumber other types. To rectify this ill-conditioning problem (Singla and Domingos, 2005; Lowd and D</context>
</contexts>
<marker>Domingos, Lowd, 2009</marker>
<rawString>Pedro Domingos and Daniel Lowd. 2009. Markov Logic: An Interface Layer forArtificial Intelligence. Morgan &amp; Claypool, San Rafael, CA.</rawString>
</citation>
<citation valid="true">
<title>Introduction to Statistical Relational Learning.</title>
<date>2007</date>
<editor>Lise Getoor and Ben Taskar, editors.</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3593" citStr="(2007)" startWordPosition="576" endWordPosition="576">ng system on the BioNLP’13 Genia shared task (Kim et al., 2013), this approach is problematic for at least two reasons. First, as is typical in pipeline architectures, errors may propagate from one stage to the next. Second, since each event/argument is identified and assigned a type independently of the others, it fails to capture the relationship between a trigger and its neighboring triggers, an argument and its neighboring arguments, etc. More recently, researchers have investigated joint inference techniques for event extraction using Markov Logic Networks (MLNs) (e.g., Poon and Domingos (2007), Poon and Vanderwende (2010), Riedel and McCallum (2011a)), a statistical relational model that enables us to model the dependencies between different instances of a data sample. However, it is extremely challenging to make joint inference using MLNs work well in practice (Poon and Domingos, 2007). One reason is that it is generally difficult to model sophisticated linguistic features using MLNs. The diffi831 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 831–843, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Ling</context>
<context position="10133" citStr="(2007)" startWordPosition="1556" endWordPosition="1556">l-based approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between event mentions. To overcome the error propagation problem associated with the pipeline architecture, several joint models have been proposed, including those that are based on MLNs (e.g., Poon and Domingos (2007), Riedel et al. (2009), Poon and Vanderwende (2010)), structured perceptrons (e.g., Li et al. (2013c)), and dual decomposition with minimal domain adaptation (e.g., Riedel and McCallum (2011a; 2011b)). In light of the high annotation cost required by supervised learning-based event extraction systems, several semi-supervised, unsupervised, and rulebased systems have been proposed. For instance, Huang and Riloff (2012a) proposed a bootstrapping method to extract event arguments using only a small amount of annotated data; Lu and Roth (2012) developed a novel unsupervised sequence labeling model</context>
</contexts>
<marker>2007</marker>
<rawString>Lise Getoor and Ben Taskar, editors. 2007. Introduction to Statistical Relational Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vibhav Gogate</author>
<author>Pedro Domingos</author>
</authors>
<title>Probabilistic theorem proving.</title>
<date>2011</date>
<booktitle>In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>256--265</pages>
<contexts>
<context position="42082" citStr="Gogate and Domingos, 2011" startWordPosition="6779" endWordPosition="6782">as RiedelMLN and PoonMLN do not achieve stateof-the-art results because they do not leverage complex, high-dimensional features. 6 Summary and Future Work Markov logic networks (MLNs) are a powerful representation that can compactly encode rich relational structures and ambiguities (uncertainty). As a result, they are an ideal representation for complex NLP tasks that require joint inference, such as event extraction. Unfortunately, the superior representational power greatly complicates inference and learning over MLN models. Even the most advanced methods for inference and learning in MLNs (Gogate and Domingos, 2011) are unable to handle complex, high-dimensional features, and therefore existing MLN systems primarily use low-dimensional features. This limitation severely affects the accuracy of MLN-based NLP systems, and as a result, in some cases their performance is inferior to pipeline methods that do not employ joint inference. In this paper, we presented a general approach for exploiting the power of high-dimensional linguistic features in MLNs. Our approach involves reliably processing and learning high-dimensional features using SVMs and encoding their output as low-dimensional features in MLNs. We</context>
</contexts>
<marker>Gogate, Domingos, 2011</marker>
<rawString>Vibhav Gogate and Pedro Domingos. 2011. Probabilistic theorem proving. In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence, pages 256–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>David Westbrook</author>
<author>Adam Meyers</author>
</authors>
<title>NYU’s English ACE</title>
<date>2005</date>
<booktitle>In Proceedings of the ACE 2005 Evaluation Workshop.</booktitle>
<location>Washington.</location>
<contexts>
<context position="8937" citStr="Grishman et al., 2005" startWordPosition="1383" endWordPosition="1386">ted corpora produced as part of the Message Understanding Conferences (MUCs), the Automatic Content Extraction (ACE) evaluations, and the BioNLP shared 832 tasks on event extraction. Previous work on event extraction can be broadly divided into two categories, one focusing on the development of features (henceforth feature-based approaches) and the other focusing on the development of models (henceforth model-based approaches). Feature-based approaches. Early work on feature-based approaches has primarily focused on designing local sentence-level features such as token and syntactic features (Grishman et al., 2005; Ahn, 2006). Later, it was realized that local features were insufficient to reliably and accurately perform event extraction in complex domains and therefore several researchers proposed using high-level features. For instance, Ji and Grishman (2008) used global information from related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based ap</context>
</contexts>
<marker>Grishman, Westbrook, Meyers, 2005</marker>
<rawString>Ralph Grishman, David Westbrook, and Adam Meyers. 2005. NYU’s English ACE 2005 system description. In Proceedings of the ACE 2005 Evaluation Workshop. Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prashant Gupta</author>
<author>Heng Ji</author>
</authors>
<title>Predicting unknown time arguments based on cross-event propagation.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>369--372</pages>
<contexts>
<context position="9257" citStr="Gupta and Ji (2009)" startWordPosition="1429" endWordPosition="1432">re-based approaches) and the other focusing on the development of models (henceforth model-based approaches). Feature-based approaches. Early work on feature-based approaches has primarily focused on designing local sentence-level features such as token and syntactic features (Grishman et al., 2005; Ahn, 2006). Later, it was realized that local features were insufficient to reliably and accurately perform event extraction in complex domains and therefore several researchers proposed using high-level features. For instance, Ji and Grishman (2008) used global information from related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Li</context>
</contexts>
<marker>Gupta, Ji, 2009</marker>
<rawString>Prashant Gupta and Heng Ji. 2009. Predicting unknown time arguments based on cross-event propagation. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 369–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Hakala</author>
<author>Sofie Van Landeghem</author>
<author>Tapio Salakoski</author>
<author>Yves Van de Peer</author>
<author>Filip Ginter</author>
</authors>
<title>EVEX in ST’13: Application of a large-scale text mining resource to event extraction and network construction.</title>
<date>2013</date>
<booktitle>In Proceedings of the BioNLP Shared Task 2013 Workshop,</booktitle>
<pages>26--34</pages>
<marker>Hakala, Van Landeghem, Salakoski, Van de Peer, Ginter, 2013</marker>
<rawString>Kai Hakala, Sofie Van Landeghem, Tapio Salakoski, Yves Van de Peer, and Filip Ginter. 2013. EVEX in ST’13: Application of a large-scale text mining resource to event extraction and network construction. In Proceedings of the BioNLP Shared Task 2013 Workshop, pages 26–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruihong Huang</author>
<author>Ellen Riloff</author>
</authors>
<title>Bootstrapped training of event extraction classifiers.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>286--295</pages>
<contexts>
<context position="5011" citStr="Huang and Riloff (2012" startWordPosition="784" endWordPosition="787"> following engagement of CD40 ... (a) Sentence fragment ID Event Type Trigger Arguments E11 Binding recruited Theme=1HOIL-1L interacting protein,CD401 E12 Regulation dependent Theme=E11, Cause=TRAF2 E13 +ve Regulation following Theme=E12, Cause=E14 E14 Binding engagement Theme=CD40 (b) Events Figure 1: Example of event extraction in the BioNLP Genia task. The table in (b) shows all the events extracted from sentence (a). Note that successful extraction of E13 depends on E12 and E14. culty stems from the fact that some of these features are extremely high dimensional (e.g., Chen and Ng (2012), Huang and Riloff (2012b), Li et al. (2012), Li et al. (2013b), Li et al. (2013c)), and to reliably learn weights of formulas that encode such features, one would require an enormous number of data samples. Moreover, even the complexity of approximate inference on such models is quite high, often prohibitively so. For example, a trigram can be encoded as an MLN formula, Word(w1, p−1) ∧ Word(w2, p) ∧ Word(w3, p + 1) ==&gt;- Type(p, T). For any given position (p), this formula has W3 groundings, where W is the number of possible words, making it too large for learning/inference. Therefore, current MLN-based systems tend </context>
<context position="9489" citStr="Huang and Riloff (2012" startWordPosition="1459" endWordPosition="1462">l features such as token and syntactic features (Grishman et al., 2005; Ahn, 2006). Later, it was realized that local features were insufficient to reliably and accurately perform event extraction in complex domains and therefore several researchers proposed using high-level features. For instance, Ji and Grishman (2008) used global information from related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between event mentions. To overcome the error propagation problem associated with the pipeline architecture, several joint models have been proposed, including those that are b</context>
</contexts>
<marker>Huang, Riloff, 2012</marker>
<rawString>Ruihong Huang and Ellen Riloff. 2012a. Bootstrapped training of event extraction classifiers. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 286–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruihong Huang</author>
<author>Ellen Riloff</author>
</authors>
<title>Modeling textual cohesion for event extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 26th AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="5011" citStr="Huang and Riloff (2012" startWordPosition="784" endWordPosition="787"> following engagement of CD40 ... (a) Sentence fragment ID Event Type Trigger Arguments E11 Binding recruited Theme=1HOIL-1L interacting protein,CD401 E12 Regulation dependent Theme=E11, Cause=TRAF2 E13 +ve Regulation following Theme=E12, Cause=E14 E14 Binding engagement Theme=CD40 (b) Events Figure 1: Example of event extraction in the BioNLP Genia task. The table in (b) shows all the events extracted from sentence (a). Note that successful extraction of E13 depends on E12 and E14. culty stems from the fact that some of these features are extremely high dimensional (e.g., Chen and Ng (2012), Huang and Riloff (2012b), Li et al. (2012), Li et al. (2013b), Li et al. (2013c)), and to reliably learn weights of formulas that encode such features, one would require an enormous number of data samples. Moreover, even the complexity of approximate inference on such models is quite high, often prohibitively so. For example, a trigram can be encoded as an MLN formula, Word(w1, p−1) ∧ Word(w2, p) ∧ Word(w3, p + 1) ==&gt;- Type(p, T). For any given position (p), this formula has W3 groundings, where W is the number of possible words, making it too large for learning/inference. Therefore, current MLN-based systems tend </context>
<context position="9489" citStr="Huang and Riloff (2012" startWordPosition="1459" endWordPosition="1462">l features such as token and syntactic features (Grishman et al., 2005; Ahn, 2006). Later, it was realized that local features were insufficient to reliably and accurately perform event extraction in complex domains and therefore several researchers proposed using high-level features. For instance, Ji and Grishman (2008) used global information from related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between event mentions. To overcome the error propagation problem associated with the pipeline architecture, several joint models have been proposed, including those that are b</context>
</contexts>
<marker>Huang, Riloff, 2012</marker>
<rawString>Ruihong Huang and Ellen Riloff. 2012b. Modeling textual cohesion for event extraction. In Proceedings of the 26th AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Refining event extraction through cross-document inference.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>254--262</pages>
<contexts>
<context position="9189" citStr="Ji and Grishman (2008)" startWordPosition="1419" endWordPosition="1422">tegories, one focusing on the development of features (henceforth feature-based approaches) and the other focusing on the development of models (henceforth model-based approaches). Feature-based approaches. Early work on feature-based approaches has primarily focused on designing local sentence-level features such as token and syntactic features (Grishman et al., 2005; Ahn, 2006). Later, it was realized that local features were insufficient to reliably and accurately perform event extraction in complex domains and therefore several researchers proposed using high-level features. For instance, Ji and Grishman (2008) used global information from related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. </context>
</contexts>
<marker>Ji, Grishman, 2008</marker>
<rawString>Heng Ji and Ralph Grishman. 2008. Refining event extraction through cross-document inference. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 254–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods -Support Vector Learning.</booktitle>
<editor>In B. Schlkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="6023" citStr="Joachims, 1999" startWordPosition="947" endWordPosition="948">=&gt;- Type(p, T). For any given position (p), this formula has W3 groundings, where W is the number of possible words, making it too large for learning/inference. Therefore, current MLN-based systems tend to include a highly simplified model ignoring powerful linguistic features. This is problematic because such features are essential for event extraction. Our contributions in this paper are two-fold. First, we propose a novel model for biomedical event extraction based on MLNs that addresses the aforementioned limitations by leveraging the power of Support Vector Machines (SVMs) (Vapnik, 1995; Joachims, 1999) to handle high-dimensional features. Specifically, we (1) learn SVM models using rich linguistic features for trigger and argument detection and type labeling; (2) design an MLN composed of soft formulas (each of which encodes a soft constraint whose associated weight indicates how important it is to satisfy the constraint) and hard formulas (constraints that always need to be satisfied, thus having a weight of oc) to capture the relational dependencies between triggers and arguments; and (3) encode the SVM output as prior knowledge in the MLN in the form of soft formulas, whose weights are c</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. In B. Schlkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods -Support Vector Learning. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Corpus annotation for mining biomedical events from literature.</title>
<date>2008</date>
<journal>BMC bioinformatics,</journal>
<pages>9--1</pages>
<contexts>
<context position="11927" citStr="Kim et al., 2008" startWordPosition="1833" endWordPosition="1836">m error propagation as the model is generally not rich enough for joint inference. 2.2 The Genia Event Extraction Task The BioNLP Shared Task (BioNLP-ST) series (Kim et al. (2009), Kim et al. (2011a) and N´edellec et al. (2013)) is designed to tackle the problem of extracting structured information from the biomedical literature. The Genia Event Extraction task is arguably the most important of all the tasks proposed in BioNLP-ST and is also the only task organized in all three events in the series. The 2009 edition of the Genia task (Kim et al., 2009) was conducted on the Genia event corpus (Kim et al., 2008), which only contains abstracts of the articles that represent domain knowledge around NFκB proteins. The 2011 edition (Kim et al., 2011b) augmented the dataset to include full text articles, resulting in two collections, the abstract collection and the full text collection. The 2013 edition (Kim et al., 2013) further augmented the dataset with recent full text articles but removed the abstract collection entirely. The targeted event types have also changed slightly over the years. Both the 2009 and 2011 editions are concerned with nine fine-grained event sub-types that can be categorized into</context>
</contexts>
<marker>Kim, Ohta, Tsujii, 2008</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, and Jun’ichi Tsujii. 2008. Corpus annotation for mining biomedical events from literature. BMC bioinformatics, 9(1):10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Sampo Pyysalo</author>
<author>Yoshinobu Kano</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Overview of BioNLP’09 shared task on event extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="11489" citStr="Kim et al. (2009)" startWordPosition="1755" endWordPosition="1758">extract events from Twitter data. Our work extends prior work by developing a rich framework that leverages sophisticated featurebased approaches as well as joint inference using MLNs. This combination gives us the best of both worlds because on one hand, it is challenging to model sophisticated linguistic features using MLNs while on the other hand, feature-based approaches employing sophisticated high-dimensional features suffer from error propagation as the model is generally not rich enough for joint inference. 2.2 The Genia Event Extraction Task The BioNLP Shared Task (BioNLP-ST) series (Kim et al. (2009), Kim et al. (2011a) and N´edellec et al. (2013)) is designed to tackle the problem of extracting structured information from the biomedical literature. The Genia Event Extraction task is arguably the most important of all the tasks proposed in BioNLP-ST and is also the only task organized in all three events in the series. The 2009 edition of the Genia task (Kim et al., 2009) was conducted on the Genia event corpus (Kim et al., 2008), which only contains abstracts of the articles that represent domain knowledge around NFκB proteins. The 2011 edition (Kim et al., 2011b) augmented the dataset t</context>
</contexts>
<marker>Kim, Ohta, Pyysalo, Kano, Tsujii, 2009</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Jun’ichi Tsujii. 2009. Overview of BioNLP’09 shared task on event extraction. In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Sampo Pyysalo</author>
<author>Tomoko Ohta</author>
<author>Robert Bossy</author>
<author>Ngan Nguyen</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Overview of BioNLP shared task</title>
<date>2011</date>
<booktitle>In Proceedings of the BioNLP Shared Task 2011 Workshop,</booktitle>
<pages>1--6</pages>
<contexts>
<context position="11507" citStr="Kim et al. (2011" startWordPosition="1759" endWordPosition="1762"> Twitter data. Our work extends prior work by developing a rich framework that leverages sophisticated featurebased approaches as well as joint inference using MLNs. This combination gives us the best of both worlds because on one hand, it is challenging to model sophisticated linguistic features using MLNs while on the other hand, feature-based approaches employing sophisticated high-dimensional features suffer from error propagation as the model is generally not rich enough for joint inference. 2.2 The Genia Event Extraction Task The BioNLP Shared Task (BioNLP-ST) series (Kim et al. (2009), Kim et al. (2011a) and N´edellec et al. (2013)) is designed to tackle the problem of extracting structured information from the biomedical literature. The Genia Event Extraction task is arguably the most important of all the tasks proposed in BioNLP-ST and is also the only task organized in all three events in the series. The 2009 edition of the Genia task (Kim et al., 2009) was conducted on the Genia event corpus (Kim et al., 2008), which only contains abstracts of the articles that represent domain knowledge around NFκB proteins. The 2011 edition (Kim et al., 2011b) augmented the dataset to include full tex</context>
<context position="32675" citStr="Kim et al., 2011" startWordPosition="5255" endWordPosition="5258">AP assignment to M can be |Δsid| computed using, U C wiarg max PMi (wi)� . This i=1 result ensures that to approximate the expected counts Ew(nj), it is sufficient to keep exactly one sentence’s groundings in memory. Specifically, Ew(nj) can be written as E|Δsid| k=1 Ew(nkj ), where Ew(nkj) indicates the expected number of satisfied groundings of the jth formula in the kth sentence. Since the MAP computation is decomposable, we can estimate Ew(nkj) using MAP inference on just the kth sentence. 5 Evaluation 5.1 Experimental Setup We evaluate our system on the BioNLP’13 (Kim et al., 2013), ’11 (Kim et al., 2011a) and ’09 (Kim 3http://www.gurobi.com/ Dataset #Papers #Abstracts #TT #Events BioNLP’13 (10,10,14) (0,0,0) 13 (2817,3199,3348) BioNLP’11 (5,5,4) (800,150,260) 9 (10310,4690,5301) BioNLP’09 (0,0,0) (800,150,260) 9 (8597,1809,3182) Table 2: Statistics on the BioNLP datasets, which consist of annotated papers/abstracts from PubMed. (x, y, z): x in training, y in development and z in test. #TT indicates the total number of trigger types. The total number of argument types is 2. et al., 2009) Genia datasets for the main event extraction shared task. Note that this task is the most important one fo</context>
</contexts>
<marker>Kim, Pyysalo, Ohta, Bossy, Nguyen, Tsujii, 2011</marker>
<rawString>Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert Bossy, Ngan Nguyen, and Jun’ichi Tsujii. 2011a. Overview of BioNLP shared task 2011. In Proceedings of the BioNLP Shared Task 2011 Workshop, pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Yue Wang</author>
</authors>
<title>Toshihisa Takagi, and Akinori Yonezawa. 2011b. Overview of Genia event task in BioNLP shared task</title>
<date>2011</date>
<booktitle>In Proceedings of the BioNLP Shared Task 2011 Workshop,</booktitle>
<pages>7--15</pages>
<marker>Kim, Wang, 2011</marker>
<rawString>Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Akinori Yonezawa. 2011b. Overview of Genia event task in BioNLP shared task 2011. In Proceedings of the BioNLP Shared Task 2011 Workshop, pages 7–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Yue Wang</author>
<author>Yamamoto Yasunori</author>
</authors>
<title>The Genia event extraction shared task,</title>
<date>2013</date>
<booktitle>In Proceedings of the BioNLP Shared Task 2013 Workshop,</booktitle>
<pages>8--15</pages>
<contexts>
<context position="3050" citStr="Kim et al., 2013" startWordPosition="492" endWordPosition="495">n see, event E13 takes as arguments two events, E14 and E12, which in turn has E11 as one of its arguments. A standard method that has been frequently employed to perform this shared task uses a pipeline architecture with three steps: (1) detect if a token is a trigger and assign a trigger type label to it; (2) for every detected trigger, determine all its arguments and assign types to each detected argument; and (3) combine the extracted triggers and arguments to obtain events. Though adopted by the top-performing systems such as the highest scoring system on the BioNLP’13 Genia shared task (Kim et al., 2013), this approach is problematic for at least two reasons. First, as is typical in pipeline architectures, errors may propagate from one stage to the next. Second, since each event/argument is identified and assigned a type independently of the others, it fails to capture the relationship between a trigger and its neighboring triggers, an argument and its neighboring arguments, etc. More recently, researchers have investigated joint inference techniques for event extraction using Markov Logic Networks (MLNs) (e.g., Poon and Domingos (2007), Poon and Vanderwende (2010), Riedel and McCallum (2011a</context>
<context position="12238" citStr="Kim et al., 2013" startWordPosition="1884" endWordPosition="1887">omedical literature. The Genia Event Extraction task is arguably the most important of all the tasks proposed in BioNLP-ST and is also the only task organized in all three events in the series. The 2009 edition of the Genia task (Kim et al., 2009) was conducted on the Genia event corpus (Kim et al., 2008), which only contains abstracts of the articles that represent domain knowledge around NFκB proteins. The 2011 edition (Kim et al., 2011b) augmented the dataset to include full text articles, resulting in two collections, the abstract collection and the full text collection. The 2013 edition (Kim et al., 2013) further augmented the dataset with recent full text articles but removed the abstract collection entirely. The targeted event types have also changed slightly over the years. Both the 2009 and 2011 editions are concerned with nine fine-grained event sub-types that can be categorized into three main types, namely simple, binding and regulation events. These three main event types can be distinguished by the kinds of arguments they take. A simple event can take exactly one protein as its Theme argument. A binding event can take one or more proteins as its Theme arguments, and is therefore sligh</context>
<context position="32652" citStr="Kim et al., 2013" startWordPosition="5250" endWordPosition="5253">, {Mk}|Δsid| k=1 . The MAP assignment to M can be |Δsid| computed using, U C wiarg max PMi (wi)� . This i=1 result ensures that to approximate the expected counts Ew(nj), it is sufficient to keep exactly one sentence’s groundings in memory. Specifically, Ew(nj) can be written as E|Δsid| k=1 Ew(nkj ), where Ew(nkj) indicates the expected number of satisfied groundings of the jth formula in the kth sentence. Since the MAP computation is decomposable, we can estimate Ew(nkj) using MAP inference on just the kth sentence. 5 Evaluation 5.1 Experimental Setup We evaluate our system on the BioNLP’13 (Kim et al., 2013), ’11 (Kim et al., 2011a) and ’09 (Kim 3http://www.gurobi.com/ Dataset #Papers #Abstracts #TT #Events BioNLP’13 (10,10,14) (0,0,0) 13 (2817,3199,3348) BioNLP’11 (5,5,4) (800,150,260) 9 (10310,4690,5301) BioNLP’09 (0,0,0) (800,150,260) 9 (8597,1809,3182) Table 2: Statistics on the BioNLP datasets, which consist of annotated papers/abstracts from PubMed. (x, y, z): x in training, y in development and z in test. #TT indicates the total number of trigger types. The total number of argument types is 2. et al., 2009) Genia datasets for the main event extraction shared task. Note that this task is th</context>
</contexts>
<marker>Kim, Wang, Yasunori, 2013</marker>
<rawString>Jin-Dong Kim, Yue Wang, and Yamamoto Yasunori. 2013. The Genia event extraction shared task, 2013 edition - overview. In Proceedings of the BioNLP Shared Task 2013 Workshop, pages 8–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daphne Koller</author>
<author>Nir Friedman</author>
</authors>
<title>Probabilistic Graphical Models: Principles and Techniques.</title>
<date>2009</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="14570" citStr="Koller and Friedman, 2009" startWordPosition="2274" endWordPosition="2277">l learning (SRL) (Getoor and Taskar, 2007) is an emerging field that seeks to unify logic and probability, and since most NLP techniques are grounded either in logic or probability or both, NLP serves as an ideal application domain for SRL. In this paper, we will employ a popular SRL approach called Markov logic networks (MLNs) (Domingos and Lowd, 2009). At a high level, an MLN is a set of weighted first-order logic formulas (fi, wi), where wi is the weight associated with formula fi. Given a set of constants that model objects in the domain, it defines a Markov network or a log-linear model (Koller and Friedman, 2009) in which we have one node per ground first-order atom and a propositional feature corresponding to each grounding of each first-order formula. The weight of the feature is the weight of the corresponding first-order formula. Formally, the probability of a world ω, which represents an assignment of values to all ground atoms in the Markov network, is given by: 1 X Pr(ω) = Z exp i where N(fi, ω) is the number of groundings of fi that evaluate to True in ω and Z is a normalization constant called the partition function. The key inference tasks over MLNs are computing the partition function (Z) a</context>
</contexts>
<marker>Koller, Friedman, 2009</marker>
<rawString>Daphne Koller and Nir Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peifeng Li</author>
<author>Guodong Zhou</author>
<author>Qiaoming Zhu</author>
<author>Libin Hou</author>
</authors>
<title>Employing compositional semantics and discourse consistency in Chinese event extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1006--1016</pages>
<contexts>
<context position="5031" citStr="Li et al. (2012)" startWordPosition="788" endWordPosition="791">D40 ... (a) Sentence fragment ID Event Type Trigger Arguments E11 Binding recruited Theme=1HOIL-1L interacting protein,CD401 E12 Regulation dependent Theme=E11, Cause=TRAF2 E13 +ve Regulation following Theme=E12, Cause=E14 E14 Binding engagement Theme=CD40 (b) Events Figure 1: Example of event extraction in the BioNLP Genia task. The table in (b) shows all the events extracted from sentence (a). Note that successful extraction of E13 depends on E12 and E14. culty stems from the fact that some of these features are extremely high dimensional (e.g., Chen and Ng (2012), Huang and Riloff (2012b), Li et al. (2012), Li et al. (2013b), Li et al. (2013c)), and to reliably learn weights of formulas that encode such features, one would require an enormous number of data samples. Moreover, even the complexity of approximate inference on such models is quite high, often prohibitively so. For example, a trigram can be encoded as an MLN formula, Word(w1, p−1) ∧ Word(w2, p) ∧ Word(w3, p + 1) ==&gt;- Type(p, T). For any given position (p), this formula has W3 groundings, where W is the number of possible words, making it too large for learning/inference. Therefore, current MLN-based systems tend to include a highly </context>
</contexts>
<marker>Li, Zhou, Zhu, Hou, 2012</marker>
<rawString>Peifeng Li, Guodong Zhou, Qiaoming Zhu, and Libin Hou. 2012. Employing compositional semantics and discourse consistency in Chinese event extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1006–1016.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lishuang Li</author>
<author>Yiwen Wang</author>
<author>Degen Huang</author>
</authors>
<title>Improving feature-based biomedical event extraction system by integrating argument information.</title>
<date>2013</date>
<booktitle>In Proceedings of the BioNLP Shared Task 2013 Workshop,</booktitle>
<pages>109--115</pages>
<contexts>
<context position="5048" citStr="Li et al. (2013" startWordPosition="792" endWordPosition="795">ce fragment ID Event Type Trigger Arguments E11 Binding recruited Theme=1HOIL-1L interacting protein,CD401 E12 Regulation dependent Theme=E11, Cause=TRAF2 E13 +ve Regulation following Theme=E12, Cause=E14 E14 Binding engagement Theme=CD40 (b) Events Figure 1: Example of event extraction in the BioNLP Genia task. The table in (b) shows all the events extracted from sentence (a). Note that successful extraction of E13 depends on E12 and E14. culty stems from the fact that some of these features are extremely high dimensional (e.g., Chen and Ng (2012), Huang and Riloff (2012b), Li et al. (2012), Li et al. (2013b), Li et al. (2013c)), and to reliably learn weights of formulas that encode such features, one would require an enormous number of data samples. Moreover, even the complexity of approximate inference on such models is quite high, often prohibitively so. For example, a trigram can be encoded as an MLN formula, Word(w1, p−1) ∧ Word(w2, p) ∧ Word(w3, p + 1) ==&gt;- Type(p, T). For any given position (p), this formula has W3 groundings, where W is the number of possible words, making it too large for learning/inference. Therefore, current MLN-based systems tend to include a highly simplified model </context>
<context position="9832" citStr="Li et al. (2013" startWordPosition="1510" endWordPosition="1513">m related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between event mentions. To overcome the error propagation problem associated with the pipeline architecture, several joint models have been proposed, including those that are based on MLNs (e.g., Poon and Domingos (2007), Riedel et al. (2009), Poon and Vanderwende (2010)), structured perceptrons (e.g., Li et al. (2013c)), and dual decomposition with minimal domain adaptation (e.g., Riedel and McCallum (2011a; 2011b)). In light of the high annotation cost required by supervised learning-based event extraction syste</context>
<context position="35350" citStr="Li et al., 2013" startWordPosition="5670" endWordPosition="5673"> different initialization points and average the weights obtained after 100 iterations of gradient descent. However, we noticed that if we simply choose random initialization points, the variance of the weights was quite high and some initialization points were much worse than others. To counter this, we use the following method to systematically 838 System Rec. Prec. F1 Our System 48.95 59.24 53.61 EVEX (Hakala et al., 2013) 45.44 58.03 50.97 TEES-2.1 (Bj¨orne and Salakoski, 2013) 46.17 56.32 50.74 BIOSEM (Bui et al., 2013) 42.47 62.83 50.68 NCBI (Liu et al., 2013) 40.53 61.72 48.93 DLUTNLP (Li et al., 2013a) 40.81 57.00 47.56 Table 3: Recall (Rec.), Precision (Prec.) and F1 score on the BioNLP’13 test data. initialize the weights. Let ni be the number of satisfied groundings of formula fi in the training data and mi be the total number of possible groundings of fi. We use a threshold -y to determine whether we wish to make the initial weight positive or negative. If ni G -y, then we choose the initial weight mi uniformly at random from the range [−0.1, 0]. Otherwise, we chose it from the range [0, 0.1]. These steps ensure that the weights generated from different initialization points have smal</context>
</contexts>
<marker>Li, Wang, Huang, 2013</marker>
<rawString>Lishuang Li, Yiwen Wang, and Degen Huang. 2013a. Improving feature-based biomedical event extraction system by integrating argument information. In Proceedings of the BioNLP Shared Task 2013 Workshop, pages 109–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peifeng Li</author>
<author>Qiaoming Zhu</author>
<author>Guodong Zhou</author>
</authors>
<title>Argument inference from relevant event mentions in Chinese argument extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1477--1487</pages>
<contexts>
<context position="5048" citStr="Li et al. (2013" startWordPosition="792" endWordPosition="795">ce fragment ID Event Type Trigger Arguments E11 Binding recruited Theme=1HOIL-1L interacting protein,CD401 E12 Regulation dependent Theme=E11, Cause=TRAF2 E13 +ve Regulation following Theme=E12, Cause=E14 E14 Binding engagement Theme=CD40 (b) Events Figure 1: Example of event extraction in the BioNLP Genia task. The table in (b) shows all the events extracted from sentence (a). Note that successful extraction of E13 depends on E12 and E14. culty stems from the fact that some of these features are extremely high dimensional (e.g., Chen and Ng (2012), Huang and Riloff (2012b), Li et al. (2012), Li et al. (2013b), Li et al. (2013c)), and to reliably learn weights of formulas that encode such features, one would require an enormous number of data samples. Moreover, even the complexity of approximate inference on such models is quite high, often prohibitively so. For example, a trigram can be encoded as an MLN formula, Word(w1, p−1) ∧ Word(w2, p) ∧ Word(w3, p + 1) ==&gt;- Type(p, T). For any given position (p), this formula has W3 groundings, where W is the number of possible words, making it too large for learning/inference. Therefore, current MLN-based systems tend to include a highly simplified model </context>
<context position="9832" citStr="Li et al. (2013" startWordPosition="1510" endWordPosition="1513">m related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between event mentions. To overcome the error propagation problem associated with the pipeline architecture, several joint models have been proposed, including those that are based on MLNs (e.g., Poon and Domingos (2007), Riedel et al. (2009), Poon and Vanderwende (2010)), structured perceptrons (e.g., Li et al. (2013c)), and dual decomposition with minimal domain adaptation (e.g., Riedel and McCallum (2011a; 2011b)). In light of the high annotation cost required by supervised learning-based event extraction syste</context>
<context position="35350" citStr="Li et al., 2013" startWordPosition="5670" endWordPosition="5673"> different initialization points and average the weights obtained after 100 iterations of gradient descent. However, we noticed that if we simply choose random initialization points, the variance of the weights was quite high and some initialization points were much worse than others. To counter this, we use the following method to systematically 838 System Rec. Prec. F1 Our System 48.95 59.24 53.61 EVEX (Hakala et al., 2013) 45.44 58.03 50.97 TEES-2.1 (Bj¨orne and Salakoski, 2013) 46.17 56.32 50.74 BIOSEM (Bui et al., 2013) 42.47 62.83 50.68 NCBI (Liu et al., 2013) 40.53 61.72 48.93 DLUTNLP (Li et al., 2013a) 40.81 57.00 47.56 Table 3: Recall (Rec.), Precision (Prec.) and F1 score on the BioNLP’13 test data. initialize the weights. Let ni be the number of satisfied groundings of formula fi in the training data and mi be the total number of possible groundings of fi. We use a threshold -y to determine whether we wish to make the initial weight positive or negative. If ni G -y, then we choose the initial weight mi uniformly at random from the range [−0.1, 0]. Otherwise, we chose it from the range [0, 0.1]. These steps ensure that the weights generated from different initialization points have smal</context>
</contexts>
<marker>Li, Zhu, Zhou, 2013</marker>
<rawString>Peifeng Li, Qiaoming Zhu, and Guodong Zhou. 2013b. Argument inference from relevant event mentions in Chinese argument extraction. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1477–1487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Heng Ji</author>
<author>Liang Huang</author>
</authors>
<title>Joint event extraction via structured prediction with global features.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>73--82</pages>
<contexts>
<context position="5048" citStr="Li et al. (2013" startWordPosition="792" endWordPosition="795">ce fragment ID Event Type Trigger Arguments E11 Binding recruited Theme=1HOIL-1L interacting protein,CD401 E12 Regulation dependent Theme=E11, Cause=TRAF2 E13 +ve Regulation following Theme=E12, Cause=E14 E14 Binding engagement Theme=CD40 (b) Events Figure 1: Example of event extraction in the BioNLP Genia task. The table in (b) shows all the events extracted from sentence (a). Note that successful extraction of E13 depends on E12 and E14. culty stems from the fact that some of these features are extremely high dimensional (e.g., Chen and Ng (2012), Huang and Riloff (2012b), Li et al. (2012), Li et al. (2013b), Li et al. (2013c)), and to reliably learn weights of formulas that encode such features, one would require an enormous number of data samples. Moreover, even the complexity of approximate inference on such models is quite high, often prohibitively so. For example, a trigram can be encoded as an MLN formula, Word(w1, p−1) ∧ Word(w2, p) ∧ Word(w3, p + 1) ==&gt;- Type(p, T). For any given position (p), this formula has W3 groundings, where W is the number of possible words, making it too large for learning/inference. Therefore, current MLN-based systems tend to include a highly simplified model </context>
<context position="9832" citStr="Li et al. (2013" startWordPosition="1510" endWordPosition="1513">m related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between event mentions. To overcome the error propagation problem associated with the pipeline architecture, several joint models have been proposed, including those that are based on MLNs (e.g., Poon and Domingos (2007), Riedel et al. (2009), Poon and Vanderwende (2010)), structured perceptrons (e.g., Li et al. (2013c)), and dual decomposition with minimal domain adaptation (e.g., Riedel and McCallum (2011a; 2011b)). In light of the high annotation cost required by supervised learning-based event extraction syste</context>
<context position="35350" citStr="Li et al., 2013" startWordPosition="5670" endWordPosition="5673"> different initialization points and average the weights obtained after 100 iterations of gradient descent. However, we noticed that if we simply choose random initialization points, the variance of the weights was quite high and some initialization points were much worse than others. To counter this, we use the following method to systematically 838 System Rec. Prec. F1 Our System 48.95 59.24 53.61 EVEX (Hakala et al., 2013) 45.44 58.03 50.97 TEES-2.1 (Bj¨orne and Salakoski, 2013) 46.17 56.32 50.74 BIOSEM (Bui et al., 2013) 42.47 62.83 50.68 NCBI (Liu et al., 2013) 40.53 61.72 48.93 DLUTNLP (Li et al., 2013a) 40.81 57.00 47.56 Table 3: Recall (Rec.), Precision (Prec.) and F1 score on the BioNLP’13 test data. initialize the weights. Let ni be the number of satisfied groundings of formula fi in the training data and mi be the total number of possible groundings of fi. We use a threshold -y to determine whether we wish to make the initial weight positive or negative. If ni G -y, then we choose the initial weight mi uniformly at random from the range [−0.1, 0]. Otherwise, we chose it from the range [0, 0.1]. These steps ensure that the weights generated from different initialization points have smal</context>
</contexts>
<marker>Li, Ji, Huang, 2013</marker>
<rawString>Qi Li, Heng Ji, and Liang Huang. 2013c. Joint event extraction via structured prediction with global features. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 73–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Liao</author>
<author>Ralph Grishman</author>
</authors>
<title>Using document level cross-event inference to improve event extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>789--797</pages>
<contexts>
<context position="9380" citStr="Liao and Grishman (2010" startWordPosition="1446" endWordPosition="1449">ased approaches. Early work on feature-based approaches has primarily focused on designing local sentence-level features such as token and syntactic features (Grishman et al., 2005; Ahn, 2006). Later, it was realized that local features were insufficient to reliably and accurately perform event extraction in complex domains and therefore several researchers proposed using high-level features. For instance, Ji and Grishman (2008) used global information from related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between event mentions. To overcome the error propagation problem a</context>
</contexts>
<marker>Liao, Grishman, 2010</marker>
<rawString>Shasha Liao and Ralph Grishman. 2010. Using document level cross-event inference to improve event extraction. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 789–797.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Liao</author>
<author>Ralph Grishman</author>
</authors>
<title>Acquiring topic features to improve event extraction: in preselected and balanced collections.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Conference Recent Advances in Natural Language Processing</booktitle>
<pages>9--16</pages>
<marker>Liao, Grishman, 2011</marker>
<rawString>Shasha Liao and Ralph Grishman. 2011. Acquiring topic features to improve event extraction: in preselected and balanced collections. In Proceedings of the International Conference Recent Advances in Natural Language Processing 2011, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haibin Liu</author>
<author>Karin Verspoor</author>
<author>Donald C Comeau</author>
<author>Andrew MacKinlay</author>
<author>W John Wilbur</author>
</authors>
<title>Generalizing an approximate subgraph matching-based system to extract events in molecular biology and cancer genetics.</title>
<date>2013</date>
<booktitle>In Proceedings of the BioNLP Shared Task 2013 Workshop,</booktitle>
<pages>76--85</pages>
<contexts>
<context position="35307" citStr="Liu et al., 2013" startWordPosition="5662" endWordPosition="5665">rent parameter estimates, we start at several different initialization points and average the weights obtained after 100 iterations of gradient descent. However, we noticed that if we simply choose random initialization points, the variance of the weights was quite high and some initialization points were much worse than others. To counter this, we use the following method to systematically 838 System Rec. Prec. F1 Our System 48.95 59.24 53.61 EVEX (Hakala et al., 2013) 45.44 58.03 50.97 TEES-2.1 (Bj¨orne and Salakoski, 2013) 46.17 56.32 50.74 BIOSEM (Bui et al., 2013) 42.47 62.83 50.68 NCBI (Liu et al., 2013) 40.53 61.72 48.93 DLUTNLP (Li et al., 2013a) 40.81 57.00 47.56 Table 3: Recall (Rec.), Precision (Prec.) and F1 score on the BioNLP’13 test data. initialize the weights. Let ni be the number of satisfied groundings of formula fi in the training data and mi be the total number of possible groundings of fi. We use a threshold -y to determine whether we wish to make the initial weight positive or negative. If ni G -y, then we choose the initial weight mi uniformly at random from the range [−0.1, 0]. Otherwise, we chose it from the range [0, 0.1]. These steps ensure that the weights generated fro</context>
</contexts>
<marker>Liu, Verspoor, Comeau, MacKinlay, Wilbur, 2013</marker>
<rawString>Haibin Liu, Karin Verspoor, Donald C. Comeau, Andrew MacKinlay, and W John Wilbur. 2013. Generalizing an approximate subgraph matching-based system to extract events in molecular biology and cancer genetics. In Proceedings of the BioNLP Shared Task 2013 Workshop, pages 76–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Lowd</author>
<author>Pedro Domingos</author>
</authors>
<title>Efficient weight learning for markov logic networks.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th European Conference on Principles and Practice of Knowledge Discovery in Databases,</booktitle>
<pages>200--211</pages>
<contexts>
<context position="27696" citStr="Lowd and Domingos, 2007" startWordPosition="4424" endWordPosition="4427">owd (2009)): wt+1 j = wtj − α(Ew(nj) − nj) (3) where wt j represents the weight of the jth formula in the tth iteration, nj is the number of groundings in which the jth formula is satisfied in the training data, Ew(nj) is the expected number of groundings in which the jth formula is satisfied given the current weight vector w, and α is the learning rate. As such, the update rule given in Equation (3) is likely to yield poor accuracy because the number of training examples of some types (e.g., None) far outnumber other types. To rectify this ill-conditioning problem (Singla and Domingos, 2005; Lowd and Domingos, 2007), we divide the gradient with the number of true groundings in the data, namely, we compute the gradient using (Ew(nj)−nj) nj . Another key issue with using Equation (3) is that computing Ew(nj) requires performing inference over the MLN. This step is intractable, #P-complete in the worst case. To circumvent this problem and for fast, scalable training, we instead propose to use the voted perceptron algorithm (Collins, 2002; Singla and Domingos, 2005). This algorithm approximates Ew(nj) by counting the number of satisfied groundings of each formula in the MAP assignment. Computing the MAP assi</context>
</contexts>
<marker>Lowd, Domingos, 2007</marker>
<rawString>Daniel Lowd and Pedro Domingos. 2007. Efficient weight learning for markov logic networks. In Proceedings of the 11th European Conference on Principles and Practice of Knowledge Discovery in Databases, pages 200–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Dan Roth</author>
</authors>
<title>Automatic event extraction with structured preference modeling.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>835--844</pages>
<contexts>
<context position="10678" citStr="Lu and Roth (2012)" startWordPosition="1633" endWordPosition="1636">d, including those that are based on MLNs (e.g., Poon and Domingos (2007), Riedel et al. (2009), Poon and Vanderwende (2010)), structured perceptrons (e.g., Li et al. (2013c)), and dual decomposition with minimal domain adaptation (e.g., Riedel and McCallum (2011a; 2011b)). In light of the high annotation cost required by supervised learning-based event extraction systems, several semi-supervised, unsupervised, and rulebased systems have been proposed. For instance, Huang and Riloff (2012a) proposed a bootstrapping method to extract event arguments using only a small amount of annotated data; Lu and Roth (2012) developed a novel unsupervised sequence labeling model; Bui et al. (2013) implemented a rule-based approach to extract biomedical events; and Ritter et al. (2012) used unsupervised learning to extract events from Twitter data. Our work extends prior work by developing a rich framework that leverages sophisticated featurebased approaches as well as joint inference using MLNs. This combination gives us the best of both worlds because on one hand, it is challenging to model sophisticated linguistic features using MLNs while on the other hand, feature-based approaches employing sophisticated high</context>
</contexts>
<marker>Lu, Roth, 2012</marker>
<rawString>Wei Lu and Dan Roth. 2012. Automatic event extraction with structured preference modeling. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 835–844.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Marinescu</author>
<author>Rina Dechter</author>
</authors>
<title>AND/OR branch-and-bound search for combinatorial optimization in graphical models.</title>
<date>2009</date>
<journal>Artificial Intelligence,</journal>
<pages>173--16</pages>
<contexts>
<context position="7295" citStr="Marinescu and Dechter (2009)" startWordPosition="1144" endWordPosition="1147">d by the SVMs. This formulation naturally allows SVMs and MLNs to complement each other’s strengths and weaknesses: learning in a large and sparse feature space is much easier with SVMs than with MLNs, whereas modeling relational dependencies is much easier with MLNs than with SVMs. Our second contribution concerns making inference with this MLN feasible. Recall that inference involves detecting and assigning the type label to all the triggers and arguments. We show that existing Maximum-a-posteriori (MAP) inference methods, even the most advanced approximate ones (e.g., Selman et al. (1996), Marinescu and Dechter (2009), Sontag and Globerson (2011) ), are infeasible on our proposed MLN because of their high memory cost. Consequently, we identify decompositions of the MLN into disconnected components and solve each independently, thereby drastically reducing the memory requirements. We evaluate our approach on the BioNLP 2009, 2011 and 2013 Genia shared task datasets. On the BioNLP’13 dataset, our model significantly outperforms state-of-the-art pipeline approaches and achieves the best F1 score to date. On the BioNLP’11 and BioNLP’09 datasets, our scores are slightly better and slightly worse respectively th</context>
</contexts>
<marker>Marinescu, Dechter, 2009</marker>
<rawString>Radu Marinescu and Rina Dechter. 2009. AND/OR branch-and-bound search for combinatorial optimization in graphical models. Artificial Intelligence, 173(16-17):1457–1491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Mihai Surdeanu</author>
<author>Chris Manning</author>
</authors>
<title>Event extraction as dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1626--1635</pages>
<contexts>
<context position="9749" citStr="McClosky et al. (2011" startWordPosition="1496" endWordPosition="1499">ing high-level features. For instance, Ji and Grishman (2008) used global information from related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between event mentions. To overcome the error propagation problem associated with the pipeline architecture, several joint models have been proposed, including those that are based on MLNs (e.g., Poon and Domingos (2007), Riedel et al. (2009), Poon and Vanderwende (2010)), structured perceptrons (e.g., Li et al. (2013c)), and dual decomposition with minimal domain adaptation (e.g., Riedel and McCallum (2011a; 2011b)). In light of th</context>
<context position="38545" citStr="McClosky et al., 2011" startWordPosition="6217" endWordPosition="6220">ncies. The advantage of using a rich model such as MLNs can be clearly seen in this case; the combined model yields a 10 point and 6 point increase in F1-score on the test data and development data respectively compared to the pipeline model. 5.3 Results on the BioNLP’11 Dataset Table 4 shows the results on the BioNLP’11 dataset. We can see that our system is marginally better than Miwa12, which is a pipeline-based system. It is also more than two points better than Riedel11, a state-of-the-art structured prediction-based joint inference system. Reidel11 incorporates the Stanford predictions (McClosky et al., 2011b) as features in the model. On the two hardest, most complex tasks, detecting regulation events (which have recursive structures and more joint dependencies than other event types) and detecting binding events (which may have multiple arguments), our system performs better than both Miwa12 and Riedel11.4 Specifically, our system’s F1 score for regulation events is 46.84, while those of Miwa12 and Riedel11 are 45.46 and 44.94 respectively. Our system’s F1 score for the binding event is 58.79, while those of Miwa12 and Riedel11 are 56.64 and 48.49 respectively. These results clearly demonstrate</context>
</contexts>
<marker>McClosky, Surdeanu, Manning, 2011</marker>
<rawString>David McClosky, Mihai Surdeanu, and Chris Manning. 2011a. Event extraction as dependency parsing. In Proceedings of the Association for Computational Linguistics: Human Language Technologies, pages 1626–1635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Mihai Surdeanu</author>
<author>Christopher Manning</author>
</authors>
<title>Event extraction as dependency parsing for BioNLP</title>
<date>2011</date>
<booktitle>In Proceedings of the BioNLP Shared Task 2011 Workshop,</booktitle>
<pages>41--45</pages>
<contexts>
<context position="9749" citStr="McClosky et al. (2011" startWordPosition="1496" endWordPosition="1499">ing high-level features. For instance, Ji and Grishman (2008) used global information from related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between event mentions. To overcome the error propagation problem associated with the pipeline architecture, several joint models have been proposed, including those that are based on MLNs (e.g., Poon and Domingos (2007), Riedel et al. (2009), Poon and Vanderwende (2010)), structured perceptrons (e.g., Li et al. (2013c)), and dual decomposition with minimal domain adaptation (e.g., Riedel and McCallum (2011a; 2011b)). In light of th</context>
<context position="38545" citStr="McClosky et al., 2011" startWordPosition="6217" endWordPosition="6220">ncies. The advantage of using a rich model such as MLNs can be clearly seen in this case; the combined model yields a 10 point and 6 point increase in F1-score on the test data and development data respectively compared to the pipeline model. 5.3 Results on the BioNLP’11 Dataset Table 4 shows the results on the BioNLP’11 dataset. We can see that our system is marginally better than Miwa12, which is a pipeline-based system. It is also more than two points better than Riedel11, a state-of-the-art structured prediction-based joint inference system. Reidel11 incorporates the Stanford predictions (McClosky et al., 2011b) as features in the model. On the two hardest, most complex tasks, detecting regulation events (which have recursive structures and more joint dependencies than other event types) and detecting binding events (which may have multiple arguments), our system performs better than both Miwa12 and Riedel11.4 Specifically, our system’s F1 score for regulation events is 46.84, while those of Miwa12 and Riedel11 are 45.46 and 44.94 respectively. Our system’s F1 score for the binding event is 58.79, while those of Miwa12 and Riedel11 are 56.64 and 48.49 respectively. These results clearly demonstrate</context>
</contexts>
<marker>McClosky, Surdeanu, Manning, 2011</marker>
<rawString>David McClosky, Mihai Surdeanu, and Christopher Manning. 2011b. Event extraction as dependency parsing for BioNLP 2011. In Proceedings of the BioNLP Shared Task 2011 Workshop, pages 41–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
</authors>
<title>Any domain parsing: Automatic domain adaptation for natural language parsing.</title>
<date>2010</date>
<booktitle>Ph.D. thesis, Ph.D. thesis,</booktitle>
<institution>Brown University,</institution>
<location>Providence, RI.</location>
<contexts>
<context position="33825" citStr="McClosky, 2010" startWordPosition="5431" endWordPosition="5432">ion shared task. Note that this task is the most important one for Genia and therefore has the most active participation. Statistics on the datasets are shown in Table 2. All our evaluations use the online tool provided by the shared task organizers. We report scores obtained using the approximate span, recursive evaluation. To generate features, we employ the supporting resources provided by the organizers. Specifically, sentence split and tokenization are done using the GENIA tools, while part-of-speech information is provided by the BLLIP parser that uses the self-trained biomedical model (McClosky, 2010). Also, we create dependency features from the parse trees provided by two dependency parsers, the Enju parser (Miyao and Tsujii, 2008) and the aforementioned BLLIP parser that uses the selftrained biomedical model, which results in two sets of dependency features. For MAP inference, we use Gurobi, a parallelized ILP solver. After inference, a postprocessing step is required to generate biomedical events from the extracted triggers and arguments. Specifically, for binding events, we employ a learning-based method similar to Bj¨orne and Salakoski (2011), while for the other events, we employ a </context>
</contexts>
<marker>McClosky, 2010</marker>
<rawString>David McClosky. 2010. Any domain parsing: Automatic domain adaptation for natural language parsing. Ph.D. thesis, Ph.D. thesis, Brown University, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Sampo Pyysalo</author>
<author>Tadayoshi Hara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Evaluating dependency representation for event extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>779--787</pages>
<contexts>
<context position="17051" citStr="Miwa et al. (2010" startWordPosition="2693" endWordPosition="2696">ds whose part-ofspeech tags belong to the above three categories as candidate triggers. To train the trigger classifier, we create one training instance for each candidate trigger in the training data. If the candidate trigger is not a trigger, the class label of the corresponding instance is None; otherwise, the label is the type of the trigger. Thus, the number of class labels equals the number of trigger types plus one. Each training instance is represented by the features described in Table 1(a). These features closely mirror those used in state-of-the-art trigger labeling systems such as Miwa et al. (2010b) and Bj¨orne and Salakoski (2013). After training, we apply the resulting trigger classifier to classify the test instances, which are created in the same way as the training instances. If a test instance is predicted as None by the classifier, the corresponding candidate trigger is labeled as a non-trigger; otherwise, the corresponding candidate trigger is posited as a true trigger whose type is the class value assigned by the classifier. !wiN(fi, ω) XZ = Xexp w i ! wiN(fi,ω) (1) 1Head words are found using Collins’ (1999) rules. 834 (a) Features for trigger labeling Token features The basi</context>
<context position="21094" citStr="Miwa et al. (2010" startWordPosition="3376" endWordPosition="3379"> trigger and one of its candidate arguments.2 A candidate argument for a candidate trigger ct is either a protein or a candidate trigger that appears in the same sentence as ct. If ct is not a true trigger, the label of the associated instance is set to None. On the other hand, if ct is a true trigger, we check whether the candidate argument in the associated instance is indeed one of ct’s arguments. If so, the class label of the instance is the argument’s role; otherwise, the class label is None. The features used for representing each training instance, which are modeled after those used in Miwa et al. (2010b) and Bj¨orne and Salakoski (2013), are shown in Table 1(b). After training, we can apply the resulting classifier to classify the test instances, which are created in the same way as the training instances. If a test instance is assigned the class None by the classifier, the corresponding candidate argument is classified as not an argument of the trigger. Other2Following the definition of the GENIA event extraction task, the protein names are provided as part of the input. wise, the candidate argument is a true argument of the trigger whose role is the class value assigned by the classifier.</context>
<context position="40554" citStr="Miwa et al., 2010" startWordPosition="6543" endWordPosition="6546">all 37.90 67.88 48.64 48.95 59.24 53.61 (a) Test SVM MLN+SVM Type Rec. Prec. F1 Rec. Prec. F1 Simple 55.79 81.63 66.28 63.21 75.10 68.64 Protein-Mod 64.47 87.89 74.38 71.14 85.63 77.72 Binding 31.90 48.77 38.57 47.99 50.00 48.97 Regulation 20.13 52.46 29.10 28.57 43.41 34.46 Overall 34.42 66.14 45.28 43.50 57.45 49.51 (b) Development Figure 3: Comparison of the combined model (MLN+SVM) with the pipeline model on the BioNLP’13 test and development data. System Rec. Prec. F1 Miwa12 (Miwa et al., 2012) 52.67 65.19 58.27 Our System 53.96 63.08 58.16 Riedel11 (Riedel et al., 2011) − − 57.4 Miwa10 (Miwa et al., 2010a) 50.13 64.16 56.28 Bjorne (Bj¨orne et al., 2009) 46.73 58.48 51.95 PoonMLN (Poon&amp;Vanderwende,2010) 43.7 58.6 50.0 RiedelMLN (Riedel et al., 2009) 36.9 55.6 44.4 Table 5: Results on the BioNLP’09 test data. “−” indicates that the corresponding values are not known. similar accuracy without the use of co-reference data. The F1 score of Miwa10, which does not use co-reference features, is nearly 2 points lower than that of our system. Our system also has a higher F1 score than Reidel11, which is the best joint inference-based system for this task. On the regulation events, our system (47.55) ou</context>
</contexts>
<marker>Miwa, Pyysalo, Hara, Tsujii, 2010</marker>
<rawString>Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and Jun’ichi Tsujii. 2010a. Evaluating dependency representation for event extraction. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 779–787.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Rune Sætre</author>
<author>Jin-Dong Kim</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Event extraction with complex event classification using rich features.</title>
<date>2010</date>
<journal>Journal of Bioinformatics and Computational Biology,</journal>
<volume>8</volume>
<issue>01</issue>
<contexts>
<context position="17051" citStr="Miwa et al. (2010" startWordPosition="2693" endWordPosition="2696">ds whose part-ofspeech tags belong to the above three categories as candidate triggers. To train the trigger classifier, we create one training instance for each candidate trigger in the training data. If the candidate trigger is not a trigger, the class label of the corresponding instance is None; otherwise, the label is the type of the trigger. Thus, the number of class labels equals the number of trigger types plus one. Each training instance is represented by the features described in Table 1(a). These features closely mirror those used in state-of-the-art trigger labeling systems such as Miwa et al. (2010b) and Bj¨orne and Salakoski (2013). After training, we apply the resulting trigger classifier to classify the test instances, which are created in the same way as the training instances. If a test instance is predicted as None by the classifier, the corresponding candidate trigger is labeled as a non-trigger; otherwise, the corresponding candidate trigger is posited as a true trigger whose type is the class value assigned by the classifier. !wiN(fi, ω) XZ = Xexp w i ! wiN(fi,ω) (1) 1Head words are found using Collins’ (1999) rules. 834 (a) Features for trigger labeling Token features The basi</context>
<context position="21094" citStr="Miwa et al. (2010" startWordPosition="3376" endWordPosition="3379"> trigger and one of its candidate arguments.2 A candidate argument for a candidate trigger ct is either a protein or a candidate trigger that appears in the same sentence as ct. If ct is not a true trigger, the label of the associated instance is set to None. On the other hand, if ct is a true trigger, we check whether the candidate argument in the associated instance is indeed one of ct’s arguments. If so, the class label of the instance is the argument’s role; otherwise, the class label is None. The features used for representing each training instance, which are modeled after those used in Miwa et al. (2010b) and Bj¨orne and Salakoski (2013), are shown in Table 1(b). After training, we can apply the resulting classifier to classify the test instances, which are created in the same way as the training instances. If a test instance is assigned the class None by the classifier, the corresponding candidate argument is classified as not an argument of the trigger. Other2Following the definition of the GENIA event extraction task, the protein names are provided as part of the input. wise, the candidate argument is a true argument of the trigger whose role is the class value assigned by the classifier.</context>
<context position="40554" citStr="Miwa et al., 2010" startWordPosition="6543" endWordPosition="6546">all 37.90 67.88 48.64 48.95 59.24 53.61 (a) Test SVM MLN+SVM Type Rec. Prec. F1 Rec. Prec. F1 Simple 55.79 81.63 66.28 63.21 75.10 68.64 Protein-Mod 64.47 87.89 74.38 71.14 85.63 77.72 Binding 31.90 48.77 38.57 47.99 50.00 48.97 Regulation 20.13 52.46 29.10 28.57 43.41 34.46 Overall 34.42 66.14 45.28 43.50 57.45 49.51 (b) Development Figure 3: Comparison of the combined model (MLN+SVM) with the pipeline model on the BioNLP’13 test and development data. System Rec. Prec. F1 Miwa12 (Miwa et al., 2012) 52.67 65.19 58.27 Our System 53.96 63.08 58.16 Riedel11 (Riedel et al., 2011) − − 57.4 Miwa10 (Miwa et al., 2010a) 50.13 64.16 56.28 Bjorne (Bj¨orne et al., 2009) 46.73 58.48 51.95 PoonMLN (Poon&amp;Vanderwende,2010) 43.7 58.6 50.0 RiedelMLN (Riedel et al., 2009) 36.9 55.6 44.4 Table 5: Results on the BioNLP’09 test data. “−” indicates that the corresponding values are not known. similar accuracy without the use of co-reference data. The F1 score of Miwa10, which does not use co-reference features, is nearly 2 points lower than that of our system. Our system also has a higher F1 score than Reidel11, which is the best joint inference-based system for this task. On the regulation events, our system (47.55) ou</context>
</contexts>
<marker>Miwa, Sætre, Kim, Tsujii, 2010</marker>
<rawString>Makoto Miwa, Rune Sætre, Jin-Dong Kim, and Jun’ichi Tsujii. 2010b. Event extraction with complex event classification using rich features. Journal of Bioinformatics and Computational Biology, 8(01):131–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Paul Thompson</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Boosting automatic event extraction from the literature using domain adaptation and coreference resolution.</title>
<date>2012</date>
<journal>Bioinformatics,</journal>
<volume>28</volume>
<issue>13</issue>
<contexts>
<context position="37537" citStr="Miwa et al., 2012" startWordPosition="6050" endWordPosition="6053">d arguments at the same time. These results illustrate the challenge in using joint inference effectively. NCBI performed much worse than the SVM-based pipeline systems, EVEX and TEES2.1. It was also worse than BIOSEM, a rulebased system that uses considerable domain expertise. Nevertheless, it was better than DLUTNLP, another SVM-based system. Figure 3 compares our baseline pipeline model with our combined model. We can clearly see that the combined model has a significantly better F1 score than the pipeline model on most event types. System Rec. Prec. F1 Our System 53.42 63.61 58.07 Miwa12 (Miwa et al., 2012) 53.35 63.48 57.98 Riedel11 (Riedel et al., 2011) − − 56 UTurku (Bj¨orne and Salakoski, 2011) 49.56 57.65 53.30 MSR-NLP (Quirk et al., 2011) 48.64 54.71 51.50 Table 4: Results on the BioNLP’11 test data. The regulation events are considered the most complex events to detect because they have a recursive structure. At the same time, this structure yields a large number of joint dependencies. The advantage of using a rich model such as MLNs can be clearly seen in this case; the combined model yields a 10 point and 6 point increase in F1-score on the test data and development data respectively co</context>
<context position="40441" citStr="Miwa et al., 2012" startWordPosition="6522" endWordPosition="6525"> 72.25 69.70 70.95 Binding 39.04 50.00 43.84 48.05 43.84 45.85 Regulation 23.51 56.21 33.15 36.47 50.86 42.48 Overall 37.90 67.88 48.64 48.95 59.24 53.61 (a) Test SVM MLN+SVM Type Rec. Prec. F1 Rec. Prec. F1 Simple 55.79 81.63 66.28 63.21 75.10 68.64 Protein-Mod 64.47 87.89 74.38 71.14 85.63 77.72 Binding 31.90 48.77 38.57 47.99 50.00 48.97 Regulation 20.13 52.46 29.10 28.57 43.41 34.46 Overall 34.42 66.14 45.28 43.50 57.45 49.51 (b) Development Figure 3: Comparison of the combined model (MLN+SVM) with the pipeline model on the BioNLP’13 test and development data. System Rec. Prec. F1 Miwa12 (Miwa et al., 2012) 52.67 65.19 58.27 Our System 53.96 63.08 58.16 Riedel11 (Riedel et al., 2011) − − 57.4 Miwa10 (Miwa et al., 2010a) 50.13 64.16 56.28 Bjorne (Bj¨orne et al., 2009) 46.73 58.48 51.95 PoonMLN (Poon&amp;Vanderwende,2010) 43.7 58.6 50.0 RiedelMLN (Riedel et al., 2009) 36.9 55.6 44.4 Table 5: Results on the BioNLP’09 test data. “−” indicates that the corresponding values are not known. similar accuracy without the use of co-reference data. The F1 score of Miwa10, which does not use co-reference features, is nearly 2 points lower than that of our system. Our system also has a higher F1 score than Reidel</context>
</contexts>
<marker>Miwa, Thompson, Ananiadou, 2012</marker>
<rawString>Makoto Miwa, Paul Thompson, and Sophia Ananiadou. 2012. Boosting automatic event extraction from the literature using domain adaptation and coreference resolution. Bioinformatics, 28(13):1759–1765.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Feature forest models for probabilistic HPSG parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="33960" citStr="Miyao and Tsujii, 2008" startWordPosition="5450" endWordPosition="5453">stics on the datasets are shown in Table 2. All our evaluations use the online tool provided by the shared task organizers. We report scores obtained using the approximate span, recursive evaluation. To generate features, we employ the supporting resources provided by the organizers. Specifically, sentence split and tokenization are done using the GENIA tools, while part-of-speech information is provided by the BLLIP parser that uses the self-trained biomedical model (McClosky, 2010). Also, we create dependency features from the parse trees provided by two dependency parsers, the Enju parser (Miyao and Tsujii, 2008) and the aforementioned BLLIP parser that uses the selftrained biomedical model, which results in two sets of dependency features. For MAP inference, we use Gurobi, a parallelized ILP solver. After inference, a postprocessing step is required to generate biomedical events from the extracted triggers and arguments. Specifically, for binding events, we employ a learning-based method similar to Bj¨orne and Salakoski (2011), while for the other events, we employ a rule-based approach similar to Bj¨orne et al. (2009). Both the SVM baseline system and the combined MLN+SVM system employ the same post</context>
</contexts>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature forest models for probabilistic HPSG parsing. Computational Linguistics, 34(1):35–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire N´edellec</author>
<author>Robert Bossy</author>
<author>Jin-Dong Kim</author>
<author>JungJae Kim</author>
<author>Tomoko Ohta</author>
<author>Sampo Pyysalo</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Overview of BioNLP shared task</title>
<date>2013</date>
<booktitle>In Proceedings of the BioNLP Shared Task 2013 Workshop,</booktitle>
<pages>1--7</pages>
<marker>N´edellec, Bossy, Kim, Kim, Ohta, Pyysalo, Zweigenbaum, 2013</marker>
<rawString>Claire N´edellec, Robert Bossy, Jin-Dong Kim, JungJae Kim, Tomoko Ohta, Sampo Pyysalo, and Pierre Zweigenbaum. 2013. Overview of BioNLP shared task 2013. In Proceedings of the BioNLP Shared Task 2013 Workshop, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ellen Riloff</author>
</authors>
<title>A unified model of phrasal and sentential evidence for information extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--160</pages>
<contexts>
<context position="9323" citStr="Patwardhan and Riloff (2009)" startWordPosition="1437" endWordPosition="1440">pment of models (henceforth model-based approaches). Feature-based approaches. Early work on feature-based approaches has primarily focused on designing local sentence-level features such as token and syntactic features (Grishman et al., 2005; Ahn, 2006). Later, it was realized that local features were insufficient to reliably and accurately perform event extraction in complex domains and therefore several researchers proposed using high-level features. For instance, Ji and Grishman (2008) used global information from related documents; Gupta and Ji (2009) extracted implicit time information; Patwardhan and Riloff (2009) used broader sentential context; Liao and Grishman (2010; 2011) leveraged document-level cross-event information and topic-based features; and Huang and Riloff (2012b) explored discourse properties. Model-based approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between ev</context>
</contexts>
<marker>Patwardhan, Riloff, 2009</marker>
<rawString>Siddharth Patwardhan and Ellen Riloff. 2009. A unified model of phrasal and sentential evidence for information extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 151–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint inference in information extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd National Conference on Artificial Intelligence,</booktitle>
<pages>913--918</pages>
<contexts>
<context position="3593" citStr="Poon and Domingos (2007)" startWordPosition="573" endWordPosition="576"> the highest scoring system on the BioNLP’13 Genia shared task (Kim et al., 2013), this approach is problematic for at least two reasons. First, as is typical in pipeline architectures, errors may propagate from one stage to the next. Second, since each event/argument is identified and assigned a type independently of the others, it fails to capture the relationship between a trigger and its neighboring triggers, an argument and its neighboring arguments, etc. More recently, researchers have investigated joint inference techniques for event extraction using Markov Logic Networks (MLNs) (e.g., Poon and Domingos (2007), Poon and Vanderwende (2010), Riedel and McCallum (2011a)), a statistical relational model that enables us to model the dependencies between different instances of a data sample. However, it is extremely challenging to make joint inference using MLNs work well in practice (Poon and Domingos, 2007). One reason is that it is generally difficult to model sophisticated linguistic features using MLNs. The diffi831 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 831–843, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Ling</context>
<context position="10133" citStr="Poon and Domingos (2007)" startWordPosition="1553" endWordPosition="1556">e properties. Model-based approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between event mentions. To overcome the error propagation problem associated with the pipeline architecture, several joint models have been proposed, including those that are based on MLNs (e.g., Poon and Domingos (2007), Riedel et al. (2009), Poon and Vanderwende (2010)), structured perceptrons (e.g., Li et al. (2013c)), and dual decomposition with minimal domain adaptation (e.g., Riedel and McCallum (2011a; 2011b)). In light of the high annotation cost required by supervised learning-based event extraction systems, several semi-supervised, unsupervised, and rulebased systems have been proposed. For instance, Huang and Riloff (2012a) proposed a bootstrapping method to extract event arguments using only a small amount of annotated data; Lu and Roth (2012) developed a novel unsupervised sequence labeling model</context>
</contexts>
<marker>Poon, Domingos, 2007</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2007. Joint inference in information extraction. In Proceedings of the 22nd National Conference on Artificial Intelligence, pages 913–918.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Joint inference for knowledge extraction from biomedical literature.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>813--821</pages>
<contexts>
<context position="3622" citStr="Poon and Vanderwende (2010)" startWordPosition="577" endWordPosition="580">m on the BioNLP’13 Genia shared task (Kim et al., 2013), this approach is problematic for at least two reasons. First, as is typical in pipeline architectures, errors may propagate from one stage to the next. Second, since each event/argument is identified and assigned a type independently of the others, it fails to capture the relationship between a trigger and its neighboring triggers, an argument and its neighboring arguments, etc. More recently, researchers have investigated joint inference techniques for event extraction using Markov Logic Networks (MLNs) (e.g., Poon and Domingos (2007), Poon and Vanderwende (2010), Riedel and McCallum (2011a)), a statistical relational model that enables us to model the dependencies between different instances of a data sample. However, it is extremely challenging to make joint inference using MLNs work well in practice (Poon and Domingos, 2007). One reason is that it is generally difficult to model sophisticated linguistic features using MLNs. The diffi831 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 831–843, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics ... demonstrated that</context>
<context position="10184" citStr="Poon and Vanderwende (2010)" startWordPosition="1561" endWordPosition="1564">based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between event mentions. To overcome the error propagation problem associated with the pipeline architecture, several joint models have been proposed, including those that are based on MLNs (e.g., Poon and Domingos (2007), Riedel et al. (2009), Poon and Vanderwende (2010)), structured perceptrons (e.g., Li et al. (2013c)), and dual decomposition with minimal domain adaptation (e.g., Riedel and McCallum (2011a; 2011b)). In light of the high annotation cost required by supervised learning-based event extraction systems, several semi-supervised, unsupervised, and rulebased systems have been proposed. For instance, Huang and Riloff (2012a) proposed a bootstrapping method to extract event arguments using only a small amount of annotated data; Lu and Roth (2012) developed a novel unsupervised sequence labeling model; Bui et al. (2013) implemented a rule-based approa</context>
<context position="25080" citStr="Poon and Vanderwende, 2010" startWordPosition="3983" endWordPosition="3987">ring inference. We define two evidence predicates based on dependency structures. Word(sid,tid,word) is true when the word in sentence sid at position tid is equal to word. DepType(sid,aid,tid,dtype) asserts that dtype is the dependency type in the dependency parse tree that connects the token at position tid to the token at position aid in sentence sid. If the word at tid and the word at aid are directly connected in the dependency tree, then dtype is the label of dependency edge with direction; otherwise dtype is None. The MLN formulas, expressing commonsense, prior knowledge in the domain (Poon and Vanderwende, 2010; Riedel and McCallum, 2011a), are shown in Fig. 2(d). All formulas, except Formula (9), are hard formulas, meaning that they have infinite weights. Note that during weight learning, we only learn the weights of soft formulas. Formulas (1) and (2) along with the “!” constraint in the predicate definition ensure that the token types are mutually exclusive and exhaustive. Formula (3) asserts that every trigger should have an argument of type Theme, since a Theme argument is mandatory for any event. Formula (4) models the constraint that a Simple or Binding trigger has no arguments of type Cause </context>
</contexts>
<marker>Poon, Vanderwende, 2010</marker>
<rawString>Hoifung Poon and Lucy Vanderwende. 2010. Joint inference for knowledge extraction from biomedical literature. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 813–821.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="19731" citStr="Porter, 1980" startWordPosition="3136" endWordPosition="3137">to the candidate argument word. Other The distance between the candidate trigger word and the candidate argument word; the number of features proteins between the candidate trigger word and the candidate argument word; the concatenation of the candidate trigger word and the candidate argument word; the concatenation of the candidate trigger type and the candidate argument word. (c) Basic token and dependency features Basic token fea- Six features are computed given a token t, including: (a) the lexical string of t, (b) the lemma of t, (c) the tures stem of t obtained using the Porter stemmer (Porter, 1980), (d) the part-of-speech tag of t, (e) whether t appears as a true trigger in the training data, and (f) whether t is a protein name. Basic Six features are computed given a dependency path p, including: (a) the vertex walk in p, (b) the edge dependency walk in p, (c) the n-grams (n=2,3,4) of the (stemmed) words associated with the vertices in p, (d) the features n-grams (n=2,3,4) of the part-of-speech tags of the words associated with the vertices in p, (e) the n-grams (n=2,3,4) of the dependency types associated with the edges in p, and (f) the length of p. Table 1: Features for trigger labe</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Pallavi Choudhury</author>
<author>Michael Gamon</author>
<author>Lucy Vanderwende</author>
</authors>
<title>MSR-NLP entry in BioNLP shared task</title>
<date>2011</date>
<booktitle>In Proceedings of the BioNLP Shared Task 2011 Workshop,</booktitle>
<pages>155--163</pages>
<contexts>
<context position="37677" citStr="Quirk et al., 2011" startWordPosition="6074" endWordPosition="6077">he SVM-based pipeline systems, EVEX and TEES2.1. It was also worse than BIOSEM, a rulebased system that uses considerable domain expertise. Nevertheless, it was better than DLUTNLP, another SVM-based system. Figure 3 compares our baseline pipeline model with our combined model. We can clearly see that the combined model has a significantly better F1 score than the pipeline model on most event types. System Rec. Prec. F1 Our System 53.42 63.61 58.07 Miwa12 (Miwa et al., 2012) 53.35 63.48 57.98 Riedel11 (Riedel et al., 2011) − − 56 UTurku (Bj¨orne and Salakoski, 2011) 49.56 57.65 53.30 MSR-NLP (Quirk et al., 2011) 48.64 54.71 51.50 Table 4: Results on the BioNLP’11 test data. The regulation events are considered the most complex events to detect because they have a recursive structure. At the same time, this structure yields a large number of joint dependencies. The advantage of using a rich model such as MLNs can be clearly seen in this case; the combined model yields a 10 point and 6 point increase in F1-score on the test data and development data respectively compared to the pipeline model. 5.3 Results on the BioNLP’11 Dataset Table 4 shows the results on the BioNLP’11 dataset. We can see that our s</context>
</contexts>
<marker>Quirk, Choudhury, Gamon, Vanderwende, 2011</marker>
<rawString>Chris Quirk, Pallavi Choudhury, Michael Gamon, and Lucy Vanderwende. 2011. MSR-NLP entry in BioNLP shared task 2011. In Proceedings of the BioNLP Shared Task 2011 Workshop, pages 155– 163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Fast and robust joint models for biomedical event extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--12</pages>
<contexts>
<context position="3649" citStr="Riedel and McCallum (2011" startWordPosition="581" endWordPosition="584">ed task (Kim et al., 2013), this approach is problematic for at least two reasons. First, as is typical in pipeline architectures, errors may propagate from one stage to the next. Second, since each event/argument is identified and assigned a type independently of the others, it fails to capture the relationship between a trigger and its neighboring triggers, an argument and its neighboring arguments, etc. More recently, researchers have investigated joint inference techniques for event extraction using Markov Logic Networks (MLNs) (e.g., Poon and Domingos (2007), Poon and Vanderwende (2010), Riedel and McCallum (2011a)), a statistical relational model that enables us to model the dependencies between different instances of a data sample. However, it is extremely challenging to make joint inference using MLNs work well in practice (Poon and Domingos, 2007). One reason is that it is generally difficult to model sophisticated linguistic features using MLNs. The diffi831 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 831–843, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics ... demonstrated that HOIL-1L interacting protei</context>
<context position="10323" citStr="Riedel and McCallum (2011" startWordPosition="1581" endWordPosition="1584">l event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between event mentions. To overcome the error propagation problem associated with the pipeline architecture, several joint models have been proposed, including those that are based on MLNs (e.g., Poon and Domingos (2007), Riedel et al. (2009), Poon and Vanderwende (2010)), structured perceptrons (e.g., Li et al. (2013c)), and dual decomposition with minimal domain adaptation (e.g., Riedel and McCallum (2011a; 2011b)). In light of the high annotation cost required by supervised learning-based event extraction systems, several semi-supervised, unsupervised, and rulebased systems have been proposed. For instance, Huang and Riloff (2012a) proposed a bootstrapping method to extract event arguments using only a small amount of annotated data; Lu and Roth (2012) developed a novel unsupervised sequence labeling model; Bui et al. (2013) implemented a rule-based approach to extract biomedical events; and Ritter et al. (2012) used unsupervised learning to extract events from Twitter data. Our work extends </context>
<context position="25107" citStr="Riedel and McCallum, 2011" startWordPosition="3988" endWordPosition="3991">o evidence predicates based on dependency structures. Word(sid,tid,word) is true when the word in sentence sid at position tid is equal to word. DepType(sid,aid,tid,dtype) asserts that dtype is the dependency type in the dependency parse tree that connects the token at position tid to the token at position aid in sentence sid. If the word at tid and the word at aid are directly connected in the dependency tree, then dtype is the label of dependency edge with direction; otherwise dtype is None. The MLN formulas, expressing commonsense, prior knowledge in the domain (Poon and Vanderwende, 2010; Riedel and McCallum, 2011a), are shown in Fig. 2(d). All formulas, except Formula (9), are hard formulas, meaning that they have infinite weights. Note that during weight learning, we only learn the weights of soft formulas. Formulas (1) and (2) along with the “!” constraint in the predicate definition ensure that the token types are mutually exclusive and exhaustive. Formula (3) asserts that every trigger should have an argument of type Theme, since a Theme argument is mandatory for any event. Formula (4) models the constraint that a Simple or Binding trigger has no arguments of type Cause since only regulation event</context>
</contexts>
<marker>Riedel, McCallum, 2011</marker>
<rawString>Sebastian Riedel and Andrew McCallum. 2011a. Fast and robust joint models for biomedical event extraction. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Robust biomedical event extraction with dual decomposition and minimal domain adaptation.</title>
<date>2011</date>
<booktitle>In Proceedings of the BioNLP Shared Task 2011 Workshop,</booktitle>
<pages>46--50</pages>
<contexts>
<context position="3649" citStr="Riedel and McCallum (2011" startWordPosition="581" endWordPosition="584">ed task (Kim et al., 2013), this approach is problematic for at least two reasons. First, as is typical in pipeline architectures, errors may propagate from one stage to the next. Second, since each event/argument is identified and assigned a type independently of the others, it fails to capture the relationship between a trigger and its neighboring triggers, an argument and its neighboring arguments, etc. More recently, researchers have investigated joint inference techniques for event extraction using Markov Logic Networks (MLNs) (e.g., Poon and Domingos (2007), Poon and Vanderwende (2010), Riedel and McCallum (2011a)), a statistical relational model that enables us to model the dependencies between different instances of a data sample. However, it is extremely challenging to make joint inference using MLNs work well in practice (Poon and Domingos, 2007). One reason is that it is generally difficult to model sophisticated linguistic features using MLNs. The diffi831 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 831–843, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics ... demonstrated that HOIL-1L interacting protei</context>
<context position="10323" citStr="Riedel and McCallum (2011" startWordPosition="1581" endWordPosition="1584">l event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between event mentions. To overcome the error propagation problem associated with the pipeline architecture, several joint models have been proposed, including those that are based on MLNs (e.g., Poon and Domingos (2007), Riedel et al. (2009), Poon and Vanderwende (2010)), structured perceptrons (e.g., Li et al. (2013c)), and dual decomposition with minimal domain adaptation (e.g., Riedel and McCallum (2011a; 2011b)). In light of the high annotation cost required by supervised learning-based event extraction systems, several semi-supervised, unsupervised, and rulebased systems have been proposed. For instance, Huang and Riloff (2012a) proposed a bootstrapping method to extract event arguments using only a small amount of annotated data; Lu and Roth (2012) developed a novel unsupervised sequence labeling model; Bui et al. (2013) implemented a rule-based approach to extract biomedical events; and Ritter et al. (2012) used unsupervised learning to extract events from Twitter data. Our work extends </context>
<context position="25107" citStr="Riedel and McCallum, 2011" startWordPosition="3988" endWordPosition="3991">o evidence predicates based on dependency structures. Word(sid,tid,word) is true when the word in sentence sid at position tid is equal to word. DepType(sid,aid,tid,dtype) asserts that dtype is the dependency type in the dependency parse tree that connects the token at position tid to the token at position aid in sentence sid. If the word at tid and the word at aid are directly connected in the dependency tree, then dtype is the label of dependency edge with direction; otherwise dtype is None. The MLN formulas, expressing commonsense, prior knowledge in the domain (Poon and Vanderwende, 2010; Riedel and McCallum, 2011a), are shown in Fig. 2(d). All formulas, except Formula (9), are hard formulas, meaning that they have infinite weights. Note that during weight learning, we only learn the weights of soft formulas. Formulas (1) and (2) along with the “!” constraint in the predicate definition ensure that the token types are mutually exclusive and exhaustive. Formula (3) asserts that every trigger should have an argument of type Theme, since a Theme argument is mandatory for any event. Formula (4) models the constraint that a Simple or Binding trigger has no arguments of type Cause since only regulation event</context>
</contexts>
<marker>Riedel, McCallum, 2011</marker>
<rawString>Sebastian Riedel and Andrew McCallum. 2011b. Robust biomedical event extraction with dual decomposition and minimal domain adaptation. In Proceedings of the BioNLP Shared Task 2011 Workshop, pages 46–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Hong-Woo Chun</author>
<author>Toshihisa Takagi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A Markov logic approach to bio-molecular event extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task,</booktitle>
<pages>41--49</pages>
<contexts>
<context position="10155" citStr="Riedel et al. (2009)" startWordPosition="1557" endWordPosition="1560">approaches. The model-based approaches developed to date have focused on modeling global properties and seldom use rich, highdimensional features. To capture global event structure properties, McClosky et al. (2011a) proposed a dependency parsing model. To extract event arguments, Li et al. (2013b) proposed an Integer Linear Programming (ILP) model to encode the relationship between event mentions. To overcome the error propagation problem associated with the pipeline architecture, several joint models have been proposed, including those that are based on MLNs (e.g., Poon and Domingos (2007), Riedel et al. (2009), Poon and Vanderwende (2010)), structured perceptrons (e.g., Li et al. (2013c)), and dual decomposition with minimal domain adaptation (e.g., Riedel and McCallum (2011a; 2011b)). In light of the high annotation cost required by supervised learning-based event extraction systems, several semi-supervised, unsupervised, and rulebased systems have been proposed. For instance, Huang and Riloff (2012a) proposed a bootstrapping method to extract event arguments using only a small amount of annotated data; Lu and Roth (2012) developed a novel unsupervised sequence labeling model; Bui et al. (2013) im</context>
<context position="40701" citStr="Riedel et al., 2009" startWordPosition="6564" endWordPosition="6567">-Mod 64.47 87.89 74.38 71.14 85.63 77.72 Binding 31.90 48.77 38.57 47.99 50.00 48.97 Regulation 20.13 52.46 29.10 28.57 43.41 34.46 Overall 34.42 66.14 45.28 43.50 57.45 49.51 (b) Development Figure 3: Comparison of the combined model (MLN+SVM) with the pipeline model on the BioNLP’13 test and development data. System Rec. Prec. F1 Miwa12 (Miwa et al., 2012) 52.67 65.19 58.27 Our System 53.96 63.08 58.16 Riedel11 (Riedel et al., 2011) − − 57.4 Miwa10 (Miwa et al., 2010a) 50.13 64.16 56.28 Bjorne (Bj¨orne et al., 2009) 46.73 58.48 51.95 PoonMLN (Poon&amp;Vanderwende,2010) 43.7 58.6 50.0 RiedelMLN (Riedel et al., 2009) 36.9 55.6 44.4 Table 5: Results on the BioNLP’09 test data. “−” indicates that the corresponding values are not known. similar accuracy without the use of co-reference data. The F1 score of Miwa10, which does not use co-reference features, is nearly 2 points lower than that of our system. Our system also has a higher F1 score than Reidel11, which is the best joint inference-based system for this task. On the regulation events, our system (47.55) outperforms both Miwa12 (45.99) and Riedel11 (46.9), while on the binding event, our system (59.88) is marginally worse than Miwa12 (59.91) and signi</context>
</contexts>
<marker>Riedel, Chun, Takagi, Tsujii, 2009</marker>
<rawString>Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi, and Jun’ichi Tsujii. 2009. A Markov logic approach to bio-molecular event extraction. In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task, pages 41–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>David McClosky</author>
<author>Mihai Surdeanu</author>
<author>Andrew McCallum</author>
<author>Christopher D Manning</author>
</authors>
<title>Model combination for event extraction in bionlp 2011.</title>
<date>2011</date>
<booktitle>In Proceedings of the BioNLP Shared Task 2011 Workshop,</booktitle>
<pages>51--55</pages>
<contexts>
<context position="37586" citStr="Riedel et al., 2011" startWordPosition="6058" endWordPosition="6061">ustrate the challenge in using joint inference effectively. NCBI performed much worse than the SVM-based pipeline systems, EVEX and TEES2.1. It was also worse than BIOSEM, a rulebased system that uses considerable domain expertise. Nevertheless, it was better than DLUTNLP, another SVM-based system. Figure 3 compares our baseline pipeline model with our combined model. We can clearly see that the combined model has a significantly better F1 score than the pipeline model on most event types. System Rec. Prec. F1 Our System 53.42 63.61 58.07 Miwa12 (Miwa et al., 2012) 53.35 63.48 57.98 Riedel11 (Riedel et al., 2011) − − 56 UTurku (Bj¨orne and Salakoski, 2011) 49.56 57.65 53.30 MSR-NLP (Quirk et al., 2011) 48.64 54.71 51.50 Table 4: Results on the BioNLP’11 test data. The regulation events are considered the most complex events to detect because they have a recursive structure. At the same time, this structure yields a large number of joint dependencies. The advantage of using a rich model such as MLNs can be clearly seen in this case; the combined model yields a 10 point and 6 point increase in F1-score on the test data and development data respectively compared to the pipeline model. 5.3 Results on the </context>
<context position="40519" citStr="Riedel et al., 2011" startWordPosition="6535" endWordPosition="6538">.51 56.21 33.15 36.47 50.86 42.48 Overall 37.90 67.88 48.64 48.95 59.24 53.61 (a) Test SVM MLN+SVM Type Rec. Prec. F1 Rec. Prec. F1 Simple 55.79 81.63 66.28 63.21 75.10 68.64 Protein-Mod 64.47 87.89 74.38 71.14 85.63 77.72 Binding 31.90 48.77 38.57 47.99 50.00 48.97 Regulation 20.13 52.46 29.10 28.57 43.41 34.46 Overall 34.42 66.14 45.28 43.50 57.45 49.51 (b) Development Figure 3: Comparison of the combined model (MLN+SVM) with the pipeline model on the BioNLP’13 test and development data. System Rec. Prec. F1 Miwa12 (Miwa et al., 2012) 52.67 65.19 58.27 Our System 53.96 63.08 58.16 Riedel11 (Riedel et al., 2011) − − 57.4 Miwa10 (Miwa et al., 2010a) 50.13 64.16 56.28 Bjorne (Bj¨orne et al., 2009) 46.73 58.48 51.95 PoonMLN (Poon&amp;Vanderwende,2010) 43.7 58.6 50.0 RiedelMLN (Riedel et al., 2009) 36.9 55.6 44.4 Table 5: Results on the BioNLP’09 test data. “−” indicates that the corresponding values are not known. similar accuracy without the use of co-reference data. The F1 score of Miwa10, which does not use co-reference features, is nearly 2 points lower than that of our system. Our system also has a higher F1 score than Reidel11, which is the best joint inference-based system for this task. On the regul</context>
</contexts>
<marker>Riedel, McClosky, Surdeanu, McCallum, Manning, 2011</marker>
<rawString>Sebastian Riedel, David McClosky, Mihai Surdeanu, Andrew McCallum, and Christopher D. Manning. 2011. Model combination for event extraction in bionlp 2011. In Proceedings of the BioNLP Shared Task 2011 Workshop, pages 51–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Oren Etzioni Mausam</author>
<author>Sam Clark</author>
</authors>
<title>Open domain event extraction from Twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>1104--1112</pages>
<contexts>
<context position="10841" citStr="Ritter et al. (2012)" startWordPosition="1657" endWordPosition="1660">t al. (2013c)), and dual decomposition with minimal domain adaptation (e.g., Riedel and McCallum (2011a; 2011b)). In light of the high annotation cost required by supervised learning-based event extraction systems, several semi-supervised, unsupervised, and rulebased systems have been proposed. For instance, Huang and Riloff (2012a) proposed a bootstrapping method to extract event arguments using only a small amount of annotated data; Lu and Roth (2012) developed a novel unsupervised sequence labeling model; Bui et al. (2013) implemented a rule-based approach to extract biomedical events; and Ritter et al. (2012) used unsupervised learning to extract events from Twitter data. Our work extends prior work by developing a rich framework that leverages sophisticated featurebased approaches as well as joint inference using MLNs. This combination gives us the best of both worlds because on one hand, it is challenging to model sophisticated linguistic features using MLNs while on the other hand, feature-based approaches employing sophisticated high-dimensional features suffer from error propagation as the model is generally not rich enough for joint inference. 2.2 The Genia Event Extraction Task The BioNLP S</context>
</contexts>
<marker>Ritter, Mausam, Clark, 2012</marker>
<rawString>Alan Ritter, Mausam, Oren Etzioni, and Sam Clark. 2012. Open domain event extraction from Twitter. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1104–1112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Selman</author>
<author>Henry Kautz</author>
<author>Bram Cohen</author>
</authors>
<title>Local Search Strategies for Satisfiability Testing.</title>
<date>1996</date>
<booktitle>Cliques, Coloring, and Satisfiability: Second DIMACS Implementation Challenge,</booktitle>
<pages>521--532</pages>
<editor>In D. S. Johnson and M. A. Trick, editors,</editor>
<publisher>American Mathematical Society,</publisher>
<location>Washington, DC.</location>
<contexts>
<context position="7265" citStr="Selman et al. (1996)" startWordPosition="1140" endWordPosition="1143">idence values generated by the SVMs. This formulation naturally allows SVMs and MLNs to complement each other’s strengths and weaknesses: learning in a large and sparse feature space is much easier with SVMs than with MLNs, whereas modeling relational dependencies is much easier with MLNs than with SVMs. Our second contribution concerns making inference with this MLN feasible. Recall that inference involves detecting and assigning the type label to all the triggers and arguments. We show that existing Maximum-a-posteriori (MAP) inference methods, even the most advanced approximate ones (e.g., Selman et al. (1996), Marinescu and Dechter (2009), Sontag and Globerson (2011) ), are infeasible on our proposed MLN because of their high memory cost. Consequently, we identify decompositions of the MLN into disconnected components and solve each independently, thereby drastically reducing the memory requirements. We evaluate our approach on the BioNLP 2009, 2011 and 2013 Genia shared task datasets. On the BioNLP’13 dataset, our model significantly outperforms state-of-the-art pipeline approaches and achieves the best F1 score to date. On the BioNLP’11 and BioNLP’09 datasets, our scores are slightly better and </context>
<context position="30805" citStr="Selman et al., 1996" startWordPosition="4941" endWordPosition="4944">small). 4.4 Inference As we need to perform MAP inference, both at training time and at test time, in this subsection we will describe how to do it efficiently by exploiting unique properties of our proposed BioMLN. Naively, we can perform MAP inference by grounding BioMLN to a Markov network and then reducing the Markov network by removing from it all (grounded propositional) formulas that are inconsistent with the evidence. On the re837 duced Markov network, we can then compute the MAP solution using standard MAP solvers such as MaxWalkSAT (a state-of-the-art local search based MAP solver) (Selman et al., 1996) and Gurobi3 (a state-of-the-art, parallelized ILP solver). The problem with the above approach is that grounding the MLN is infeasible in practice; even the reduced Markov network is just too large. For example, assuming a total of |Asid |sentences and a maximum of N tokens in a sentence, Formula (3) alone has O(|Asid|N3) groundings. Concretely, at training time, assuming 1000 sentences with 10 tokens per sentence, Formula (3) itself yields one million groundings. Clearly, this approach is not scalable. It turns out, however, that the (ground) Markov network can be decomposed into several dis</context>
</contexts>
<marker>Selman, Kautz, Cohen, 1996</marker>
<rawString>Bart Selman, Henry Kautz, and Bram Cohen. 1996. Local Search Strategies for Satisfiability Testing. In D. S. Johnson and M. A. Trick, editors, Cliques, Coloring, and Satisfiability: Second DIMACS Implementation Challenge, pages 521–532. American Mathematical Society, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parag Singla</author>
<author>Pedro Domingos</author>
</authors>
<title>Discriminative training of Markov logic networks.</title>
<date>2005</date>
<booktitle>In Proceedings of the 20th National Conference on Artificial Intelligence,</booktitle>
<pages>868--873</pages>
<contexts>
<context position="27053" citStr="Singla and Domingos (2005)" startWordPosition="4308" endWordPosition="4311">ndicates that each grounding of Formula (9) may have a different weight. 4.2 Weight Learning We can learn BioMLN from data either discriminatively or generatively. Since discriminative learning is much faster than generative learning, we use the former. In discriminative training, we maximize the conditional log-likelihood (CLL) of the query and the hidden variables given an assignment to the evidence variables. In principle, we can use the standard gradient descent algorithm for maximizing the CLL. In each iteration of gradient descent, we update the weights using the following equation (cf. Singla and Domingos (2005) and Domingos and Lowd (2009)): wt+1 j = wtj − α(Ew(nj) − nj) (3) where wt j represents the weight of the jth formula in the tth iteration, nj is the number of groundings in which the jth formula is satisfied in the training data, Ew(nj) is the expected number of groundings in which the jth formula is satisfied given the current weight vector w, and α is the learning rate. As such, the update rule given in Equation (3) is likely to yield poor accuracy because the number of training examples of some types (e.g., None) far outnumber other types. To rectify this ill-conditioning problem (Singla a</context>
</contexts>
<marker>Singla, Domingos, 2005</marker>
<rawString>Parag Singla and Pedro Domingos. 2005. Discriminative training of Markov logic networks. In Proceedings of the 20th National Conference on Artificial Intelligence, pages 868–873.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Sontag</author>
<author>Amir Globerson</author>
</authors>
<title>Introduction to Dual Decomposition for Inference. Optimization for Machine Learning.</title>
<date>2011</date>
<contexts>
<context position="7324" citStr="Sontag and Globerson (2011)" startWordPosition="1148" endWordPosition="1151">n naturally allows SVMs and MLNs to complement each other’s strengths and weaknesses: learning in a large and sparse feature space is much easier with SVMs than with MLNs, whereas modeling relational dependencies is much easier with MLNs than with SVMs. Our second contribution concerns making inference with this MLN feasible. Recall that inference involves detecting and assigning the type label to all the triggers and arguments. We show that existing Maximum-a-posteriori (MAP) inference methods, even the most advanced approximate ones (e.g., Selman et al. (1996), Marinescu and Dechter (2009), Sontag and Globerson (2011) ), are infeasible on our proposed MLN because of their high memory cost. Consequently, we identify decompositions of the MLN into disconnected components and solve each independently, thereby drastically reducing the memory requirements. We evaluate our approach on the BioNLP 2009, 2011 and 2013 Genia shared task datasets. On the BioNLP’13 dataset, our model significantly outperforms state-of-the-art pipeline approaches and achieves the best F1 score to date. On the BioNLP’11 and BioNLP’09 datasets, our scores are slightly better and slightly worse respectively than the best reported results.</context>
</contexts>
<marker>Sontag, Globerson, 2011</marker>
<rawString>David Sontag and Amir Globerson. 2011. Introduction to Dual Decomposition for Inference. Optimization for Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
<author>Thorsten Joachims</author>
<author>Yasemin Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the 21st International Conference on Machine Learning,</booktitle>
<pages>104--112</pages>
<contexts>
<context position="16162" citStr="Tsochantaridis et al., 2004" startWordPosition="2546" endWordPosition="2549"> model serves two important functions: (1) providing a baseline for evaluation and (2) producing prior knowledge for the joint model. Our pipeline model consists of two steps: trigger labeling and argument labeling. In the trigger labeling step, we determine whether a candidate trigger is a true trigger and label each true trigger with its trigger type. Then, in the argument labeling step, we identify the arguments for each true trigger discovered in the trigger labeling step and assign a role to each argument. We recast each of the two steps as a classification task and employ SVMmulticlass (Tsochantaridis et al., 2004) to train the two classifiers. We describe each step in detail below. 3.1 Trigger Labeling A preliminary study of the BioNLP’13 training data suggests that 98.7% of the true triggers’ head words1 are either verbs, nouns or adjectives. Therefore, we consider only those words whose part-ofspeech tags belong to the above three categories as candidate triggers. To train the trigger classifier, we create one training instance for each candidate trigger in the training data. If the candidate trigger is not a trigger, the class label of the corresponding instance is None; otherwise, the label is the </context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the 21st International Conference on Machine Learning, pages 104– 112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="6006" citStr="Vapnik, 1995" startWordPosition="945" endWordPosition="946">d(w3, p + 1) ==&gt;- Type(p, T). For any given position (p), this formula has W3 groundings, where W is the number of possible words, making it too large for learning/inference. Therefore, current MLN-based systems tend to include a highly simplified model ignoring powerful linguistic features. This is problematic because such features are essential for event extraction. Our contributions in this paper are two-fold. First, we propose a novel model for biomedical event extraction based on MLNs that addresses the aforementioned limitations by leveraging the power of Support Vector Machines (SVMs) (Vapnik, 1995; Joachims, 1999) to handle high-dimensional features. Specifically, we (1) learn SVM models using rich linguistic features for trigger and argument detection and type labeling; (2) design an MLN composed of soft formulas (each of which encodes a soft constraint whose associated weight indicates how important it is to satisfy the constraint) and hard formulas (constraints that always need to be satisfied, thus having a weight of oc) to capture the relational dependencies between triggers and arguments; and (3) encode the SVM output as prior knowledge in the MLN in the form of soft formulas, wh</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer, New York, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
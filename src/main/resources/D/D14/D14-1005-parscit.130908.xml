<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014027">
<title confidence="0.9986735">
Learning Image Embeddings using Convolutional Neural Networks for
Improved Multi-Modal Semantics
</title>
<author confidence="0.971726">
Douwe Kiela∗
</author>
<affiliation confidence="0.980351">
University of Cambridge
Computer Laboratory
</affiliation>
<email confidence="0.992348">
douwe.kiela@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.997326" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999838866666667">
We construct multi-modal concept repre-
sentations by concatenating a skip-gram
linguistic representation vector with a vi-
sual concept representation vector com-
puted using the feature extraction layers
of a deep convolutional neural network
(CNN) trained on a large labeled object
recognition dataset. This transfer learn-
ing approach brings a clear performance
gain over features based on the traditional
bag-of-visual-word approach. Experimen-
tal results are reported on the WordSim353
and MEN semantic relatedness evaluation
tasks. We use visual features computed us-
ing either ImageNet or ESP Game images.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999199">
Recent works have shown that multi-modal se-
mantic representation models outperform uni-
modal linguistic models on a variety of tasks, in-
cluding modeling semantic relatedness and pre-
dicting compositionality (Feng and Lapata, 2010;
Leong and Mihalcea, 2011; Bruni et al., 2012;
Roller and Schulte im Walde, 2013; Kiela et al.,
2014). These results were obtained by combin-
ing linguistic feature representations with robust
visual features extracted from a set of images as-
sociated with the concept in question. This extrac-
tion of visual features usually follows the popular
computer vision approach consisting of comput-
ing local features, such as SIFT features (Lowe,
1999), and aggregating them as bags of visual
words (Sivic and Zisserman, 2003).
Meanwhile, deep transfer learning techniques
have gained considerable attention in the com-
puter vision community. First, a deep convolu-
tional neural network (CNN) is trained on a large
</bodyText>
<footnote confidence="0.4383925">
∗ This work was carried out while Douwe Kiela was an
intern at Microsoft Research, New York.
</footnote>
<author confidence="0.293109">
L´eon Bottou
</author>
<affiliation confidence="0.270377">
Microsoft Research
</affiliation>
<address confidence="0.527809">
New York
</address>
<email confidence="0.845378">
leon@bottou.org
</email>
<bodyText confidence="0.999877685714286">
labeled dataset (Krizhevsky et al., 2012). The
convolutional layers are then used as mid-level
feature extractors on a variety of computer vi-
sion tasks (Oquab et al., 2014; Girshick et al.,
2013; Zeiler and Fergus, 2013; Donahue et al.,
2014). Although transferring convolutional net-
work features is not a new idea (Driancourt and
Bottou, 1990), the simultaneous availability of
large datasets and cheap GPU co-processors has
contributed to the achievement of considerable
performance gains on a variety computer vision
benchmarks: “SIFT and HOG descriptors pro-
duced big performance gains a decade ago, and
now deep convolutional features are providing a
similar breakthrough” (Razavian et al., 2014).
This work reports on results obtained by using
CNN-extracted features in multi-modal semantic
representation models. These results are interest-
ing in several respects. First, these superior fea-
tures provide the opportunity to increase the per-
formance gap achieved by augmenting linguistic
features with multi-modal features. Second, this
increased performance confirms that the multi-
modal performance improvement results from the
information contained in the images and not the
information used to select which images to use
to represent a concept. Third, our evaluation re-
veals an intriguing property of the CNN-extracted
features. Finally, since we use the skip-gram ap-
proach of Mikolov et al. (2013) to generate our
linguistic features, we believe that this work rep-
resents the first approach to multimodal distribu-
tional semantics that exclusively relies on deep
learning for both its linguistic and visual compo-
nents.
</bodyText>
<sectionHeader confidence="0.99997" genericHeader="introduction">
2 Related work
</sectionHeader>
<subsectionHeader confidence="0.998184">
2.1 Multi-Modal Distributional Semantics
</subsectionHeader>
<bodyText confidence="0.999121">
Multi-modal models are motivated by parallels
with human concept acquisition. Standard se-
</bodyText>
<page confidence="0.982621">
36
</page>
<note confidence="0.9102815">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 36–45,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999734352941177">
mantic space models extract meanings solely from
linguistic data, even though we know that hu-
man semantic knowledge relies heavily on percep-
tual information (Louwerse, 2011). That is, there
exists substantial evidence that many concepts
are grounded in the perceptual system (Barsalou,
2008). One way to do this grounding in the context
of distributional semantics is to obtain represen-
tations that combine information from linguistic
corpora with information from another modality,
obtained from e.g. property norming experiments
(Silberer and Lapata, 2012; Roller and Schulte im
Walde, 2013) or from processing and extracting
features from images (Feng and Lapata, 2010;
Leong and Mihalcea, 2011; Bruni et al., 2012).
This approach has met with quite some success
(Bruni et al., 2014).
</bodyText>
<subsectionHeader confidence="0.998663">
2.2 Multi-modal Deep Learning
</subsectionHeader>
<bodyText confidence="0.999923222222222">
Other examples that apply multi-modal deep
learning use restricted Boltzmann machines (Sri-
vastava and Salakhutdinov, 2012; Feng et al.,
2013), auto-encoders (Wu et al., 2013) or recur-
sive neural networks (Socher et al., 2014). Multi-
modal models with deep learning components
have also successfully been employed in cross-
modal tasks (Lazaridou et al., 2014). Work that is
closely related in spirit to ours is by Silberer and
Lapata (2014). They use a stacked auto-encoder
to learn combined embeddings of textual and vi-
sual input. Their visual inputs consist of vectors
of visual attributes obtained from learning SVM
classifiers on attribute prediction tasks. In con-
trast, our work keeps the modalities separate and
follows the standard multi-modal approach of con-
catenating linguistic and visual representations in
a single semantic space model. This has the advan-
tage that it allows for separate data sources for the
individual modalities. We also learn visual repre-
sentations directly from the images (i.e., we apply
deep learning directly to the images), as opposed
to taking a higher-level representation as a start-
ing point. Frome et al. (2013) jointly learn multi-
modal representations as well, but apply them to
a visual object recognition task instead of concept
meaning.
</bodyText>
<subsectionHeader confidence="0.997038">
2.3 Deep Convolutional Neural Networks
</subsectionHeader>
<bodyText confidence="0.999875">
A flurry of recent results indicates that image de-
scriptors extracted from deep convolutional neu-
ral networks (CNNs) are very powerful and con-
sistently outperform highly tuned state-of-the-art
systems on a variety of visual recognition tasks
(Razavian et al., 2014). Embeddings from state-
of-the-art CNNs (such as Krizhevsky et al. (2012))
have been applied successfully to a number of
problems in computer vision (Girshick et al.,
2013; Zeiler and Fergus, 2013; Donahue et al.,
2014). This contribution follows the approach de-
scribed by Oquab et al. (2014): they train a CNN
on 1512 ImageNet synsets (Deng et al., 2009),
use the first seven layers of the trained network as
feature extractors on the Pascal VOC dataset, and
achieve state-of-the-art performance on the Pascal
VOC classification task.
</bodyText>
<sectionHeader confidence="0.9973985" genericHeader="method">
3 Improving Multi-Modal
Representations
</sectionHeader>
<figureCaption confidence="0.670813">
Figure 1 illustrates how our system computes
multi-modal semantic representations.
</figureCaption>
<subsectionHeader confidence="0.998343">
3.1 Perceptual Representations
</subsectionHeader>
<bodyText confidence="0.999571033333333">
The perceptual component of standard multi-
modal models that rely on visual data is often
an instance of the bag-of-visual-words (BOVW)
representation (Sivic and Zisserman, 2003). This
approach takes a collection of images associated
with words or tags representing the concept in
question. For each image, keypoints are laid out
as a dense grid. Each keypoint is represented by
a vector of robust local visual features such as
SIFT (Lowe, 1999), SURF (Bay et al., 2008) and
HOG (Dalal and Triggs, 2005), as well as pyra-
midal variants of these descriptors such as PHOW
(Bosch et al., 2007). These descriptors are sub-
sequently clustered into a discrete set of “visual
words” using a standard clustering algorithm like
k-means and quantized into vector representations
by comparing the local descriptors with the cluster
centroids. Visual representations are obtained by
taking the average of the BOVW vectors for the
images that correspond to a given word. We use
BOVW as a baseline.
Our approach similarly makes use of a collec-
tion of images associated with words or tags rep-
resenting a particular concept. Each image is pro-
cessed by the first seven layers of the convolu-
tional network defined by Krizhevsky et al. (2012)
and adapted by Oquab et al. (2014)1. This net-
work takes 224 × 224 pixel RGB images and ap-
plies five successive convolutional layers followed
by three fully connected layers. Its eighth and last
</bodyText>
<footnote confidence="0.992555">
1http://www.di.ens.fr/willow/research/cnn/
</footnote>
<page confidence="0.998945">
37
</page>
<figure confidence="0.99547332">
Multimodal word vector
Aggregate
6144-dim feature vectors
Training visual features (after Oquab et al., 2014)
FC6 FC7 FC8
6144-dim
feature
vector
C1-C2-C3-C4-C5
Convolutional layers Fully-connected layers
Imagenet labels
African elephant
Wall clock
...
Select images
from ImageNet or ESP
Word
100-dim word projections
w(t-2)
w(t-2)
100-dim word projections
w(t) w(t+1)
w(t+2)
C1-C2-C3-C4-C5 FC6 FC7
Training linguistic features (after Mikolov et al., 2013)
</figure>
<figureCaption confidence="0.999916">
Figure 1: Computing word feature vectors.
</figureCaption>
<bodyText confidence="0.999977833333333">
layer produces a vector of 1512 scores associated
with 1000 categories of the ILSVRC-2012 chal-
lenge and the 512 additional categories selected by
Oquab et al. (2014). This network was trained us-
ing about 1.6 million ImageNet images associated
with these 1512 categories. We then freeze the
trained parameters, chop the last network layer,
and use the remaining seventh layer as a filter to
compute a 6144-dimensional feature vector on ar-
bitrary 224 x 224 input images.
We consider two ways to aggregate the feature
vectors representing each image.
</bodyText>
<listItem confidence="0.932468">
1. The first method (CNN-Mean) simply com-
putes the average of all feature vectors.
2. The second method (CNN-Max) computes
the component-wise maximum of all feature
vectors. This approach makes sense because
the feature vectors extracted from this par-
ticular network are quite sparse (about 22%
non-zero coefficients) and can be interpreted
as bags of visual properties.
</listItem>
<subsectionHeader confidence="0.99978">
3.2 Linguistic representations
</subsectionHeader>
<bodyText confidence="0.999985166666667">
For our linguistic representations we extract 100-
dimensional continuous vector representations us-
ing the log-linear skip-gram model of Mikolov
et al. (2013) trained on a corpus consisting of
the 400M word Text8 corpus of Wikipedia text2
together with the 100M word British National
Corpus (Leech et al., 1994). We also experi-
mented with dependency-based skip-grams (Levy
and Goldberg, 2014) but this did not improve re-
sults. The skip-gram model learns high quality se-
mantic representations based on the distributional
properties of words in text, and outperforms stan-
dard distributional models on a variety of semantic
similarity and relatedness tasks. However we note
that Bruni et al. (2014) have recently reported an
even better performance for their linguistic com-
ponent using a standard distributional model, al-
though this may have been tuned to the task.
</bodyText>
<subsectionHeader confidence="0.998747">
3.3 Multi-modal Representations
</subsectionHeader>
<bodyText confidence="0.99996775">
Following Bruni et al. (2014), we construct multi-
modal semantic representations by concatenating
the centered and L2-normalized linguistic and per-
ceptual feature vectors vling and Vvis,
</bodyText>
<equation confidence="0.809863">
vconcept = α x vling  ||(1 − α) x vvis (1)
</equation>
<bodyText confidence="0.9996105">
where  ||denotes the concatenation operator and α
is an optional tuning parameter.
</bodyText>
<footnote confidence="0.936154">
2http://mattmahoney.net/dc/textdata.html
</footnote>
<page confidence="0.998126">
38
</page>
<figureCaption confidence="0.9998845">
Figure 2: Examples of dog in the ESP Game dataset.
Figure 3: Examples of golden retriever in ImageNet.
</figureCaption>
<sectionHeader confidence="0.996327" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9998778">
We carried out experiments using visual repre-
sentations computed using two canonical image
datasets. The resulting multi-modal concept rep-
resentations were evaluated using two well-known
semantic relatedness datasets.
</bodyText>
<subsectionHeader confidence="0.994797">
4.1 Visual Data
</subsectionHeader>
<bodyText confidence="0.999913377777778">
We carried out experiments using two distinct
sources of images to compute the visual represen-
tations.
The ImageNet dataset (Deng et al., 2009) is
a large-scale ontology of images organized ac-
cording to the hierarchy of WordNet (Fellbaum,
1999). The dataset was constructed by manually
re-labelling candidate images collected using web
searches for each WordNet synset. The images
tend to be of high quality with the designated ob-
ject roughly centered in the image. Our copy of
ImageNet contains about 12.5 million images or-
ganized in 22K synsets. This implies that Ima-
geNet covers only a small fraction of the existing
117K WordNet synsets.
The ESP Game dataset (Von Ahn and Dabbish,
2004) was famously collected as a “game with
a purpose”, in which two players must indepen-
dently and rapidly agree on a correct word label
for randomly selected images. Once a word label
has been used sufficiently frequently for a given
image, that word is added to the image’s tags. This
dataset contains 100K images, but with every im-
age having on average 14 tags, that amounts to a
coverage of 20,515 words. Since players are en-
couraged to produce as many terms per image, the
dataset’s increased coverage is at the expense of
accuracy in the word-to-image mapping: a dog in
a field with a house in the background might be a
golden retriever in ImageNet and could have tags
dog, golden retriever, grass, field, house, door in
the ESP Dataset. In other words, images in the
ESP dataset do not make a distinction between ob-
jects in the foreground and in the background, or
between the relative size of the objects (tags for
images are provided in a random order, so the top
tag is not necessarily the best one).
Figures 2 and 3 show typical examples of im-
ages belonging to these datasets. Both datasets
have attractive properties. On the one hand, Ima-
geNet has higher quality images with better labels.
On the other hand, the ESP dataset has an interest-
ing coverage because the MEN task (see section
4.4) was specifically designed to be covered by the
ESP dataset.
</bodyText>
<subsectionHeader confidence="0.991855">
4.2 Image Selection
</subsectionHeader>
<bodyText confidence="0.999968705882353">
Since ImageNet follows the WordNet hierarchy,
we would have to include almost all images in
the dataset to obtain representations for high-level
concepts such as entity, object and animal. Doing
so is both computationally expensive and unlikely
to improve the results. For this reason, we ran-
domly sample up to N distinct images from the
subtree associated with each concept. When this
returns less than N images, we attempt to increase
coverage by sampling images from the subtree of
the concept’s hypernym instead. In order to allow
for a fair comparison, we apply the same method
of sampling up to N on the ESP Game dataset. In
all following experiments, N = 1.000. We used
the WordNet lemmatizer from NLTK (Bird et al.,
2009) to lemmatize tags and concept words so as
to further improve the dataset’s coverage.
</bodyText>
<subsectionHeader confidence="0.99244">
4.3 Image Processing
</subsectionHeader>
<bodyText confidence="0.994128333333333">
The ImageNet images were preprocessed as de-
scribed by (Krizhevsky et al., 2012). The largest
centered square contained in each image is resam-
</bodyText>
<page confidence="0.99717">
39
</page>
<bodyText confidence="0.999985052631579">
pled to form a 256 × 256 image. The CNN input
is then formed by cropping 16 pixels off each bor-
der and subtracting 128 to the image components.
The ESP Game images were preprocessed slightly
differently because we do not expect the objects
to be centered. Each image was rescaled to fit in-
side a 224 × 224 rectangle. The CNN input is then
formed by centering this image into the 224 × 224
input field, subtracting 128 to the image compo-
nents, and zero padding.
The BOVW features were obtained by comput-
ing DSIFT descriptors using VLFeat (Vedaldi and
Fulkerson, 2008). These descriptors were subse-
quently clustered using mini-batch k-means (Scul-
ley, 2010) with 100 clusters. Each image is then
represented by a bag of clusters (visual words)
quantized as a 100-dimensional feature vector.
These vectors were then combined into visual con-
cept representations by taking their mean.
</bodyText>
<subsectionHeader confidence="0.934124">
4.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.9999213125">
We evaluate our multi-modal word representations
using two semantic relatedness datasets widely
used in distributional semantics (Agirre et al.,
2009; Feng and Lapata, 2010; Bruni et al., 2012;
Kiela and Clark, 2014; Bruni et al., 2014).
WordSim353 (Finkelstein et al., 2001) is a se-
lection of 353 concept pairs with a similarity rat-
ing provided by human annotators. Since this is
probably the most widely used evaluation dataset
for distributional semantics, we include it for com-
parison with other approaches. WordSim353 has
some known idiosyncracies: it includes named en-
tities, such as OPEC, Arafat, and Maradona, as
well as abstract words, such as antecedent and
credibility, for which it may be hard to find cor-
responding images. Multi-modal representations
are often evaluated on an unspecified subset of
WordSim353 (Feng and Lapata, 2010; Bruni et
al., 2012; Bruni et al., 2014), making it impossi-
ble to compare the reported scores. In this work,
we report scores on the full WordSim353 dataset
(W353) by setting the visual vector Vvis to zero for
concepts without images. We also report scores
on the subset (W353-Relevant) of pairs for which
both concepts have both ImageNet and ESP Game
images using the aforementioned selection proce-
dure.
MEN (Bruni et al., 2012) was in part designed
to alleviate the WordSim353 problems. It was con-
structed in such a way that only frequent words
with at least 50 images in the ESP Game dataset
were included in the evaluation pairs. The MEN
dataset has been found to mirror the aggregate
score over a variety of tasks and similarity datasets
(Kiela and Clark, 2014). It is also much larger,
with 3000 words pairs consisting of 751 individual
words. Although MEN was constructed so as to
have at least a minimum amount of images avail-
able in the ESP Game dataset for each concept,
this is not the case for ImageNet. Hence, simi-
larly to WordSim353, we also evaluate on a subset
(MEN-Relevant) for which images are available
in both datasets.
We evaluate the models in terms of their Spear-
man p correlation with the human relatedness rat-
ings. The similarity between the representations
associated with a pair of words is calculated using
the cosine similarity:
</bodyText>
<equation confidence="0.994802">
v1 · v2
cos(v1, v2) = (2)
kv1k kv2k
</equation>
<sectionHeader confidence="0.999837" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999985315789474">
We evaluate on the two semantic relatedness
datasets using solely linguistic, solely visual and
multi-modal representations. In the case of MEN-
Relevant and W353-Relevant, we report scores for
BOVW, CNN-Mean and CNN-Max visual repre-
sentations. For all datasets we report the scores
obtained by BOVW, CNN-Mean and CNN-Max
multi-modal representations. Since we have full
coverage with the ESP Game dataset on MEN, we
are able to report visual representation scores for
the entire dataset as well. The results can be seen
in Table 1.
There are a number of questions to ask. First
of all, do CNNs yield better visual representa-
tions? Second, do CNNs yield better multi-modal
representations? And third, is there a difference
between the high-quality low-coverage ImageNet
and the low-quality higher-coverage ESP Game
dataset representations?
</bodyText>
<subsectionHeader confidence="0.987106">
5.1 Visual Representations
</subsectionHeader>
<bodyText confidence="0.999956625">
In all cases, CNN-generated visual representations
perform better or as good as BOVW representa-
tions (we report results for BOVW-Mean, which
performs slightly better than taking the element-
wise maximum). This confirms the motivation
outlined in the introduction: by applying state-of-
the-art approaches from computer vision to multi-
modal semantics, we obtain a signficant perfor-
</bodyText>
<page confidence="0.99726">
40
</page>
<table confidence="0.999922833333333">
Dataset Linguistic Visual Multi-modal
BOVW CNN-Mean CNN-Max BOVW CNN-Mean CNN-Max
ImageNet visual features
MEN 0.64 - - - 0.64 0.70 0.67
MEN-Relevant 0.62 0.40 0.64 0.63 0.64 0.72 0.71
W353 0.57 - - - 0.58 0.59 0.60
W353-Relevant 0.51 0.30 0.32 0.30 0.55 0.56 0.57
ESP game visual features
MEN 0.64 0.17 0.51 0.20 0.64 0.71 0.65
MEN-Relevant 0.62 0.35 0.58 0.57 0.63 0.69 0.70
W353 0.57 - - - 0.58 0.59 0.60
W353-Relevant 0.51 0.38 0.44 0.56 0.52 0.55 0.61
</table>
<tableCaption confidence="0.999969">
Table 1: Results (see sections 4 and 5).
</tableCaption>
<bodyText confidence="0.9349605">
mance increase over standard multi-modal mod-
els.
</bodyText>
<subsectionHeader confidence="0.996509">
5.2 Multi-modal Representations
</subsectionHeader>
<bodyText confidence="0.999918888888889">
Higher-quality perceptual input leads to better-
performing multi-modal representations. In all
cases multi-modal models with CNNs outperform
multi-modal models with BOVW, occasionally by
quite a margin. In all cases, multi-modal rep-
resentations outperform purely linguistic vectors
that were obtained using a state-of-the-art system.
This re-affirms the importance of multi-modal rep-
resentations for distributional semantics.
</bodyText>
<subsectionHeader confidence="0.985956">
5.3 The Contribution of Images
</subsectionHeader>
<bodyText confidence="0.999962777777778">
Since the ESP Game images come with a multi-
tude of word labels, one could question whether
a performance increase of multi-modal models
based on that dataset comes from the images them-
selves, or from overlapping word labels. It might
also be possible that similar concepts are more
likely to occur in the same image, which encodes
relatedness information without necessarily tak-
ing the image data itself into account. In short,
it is a natural question to ask whether the perfor-
mance gain is due to image data or due to word
label associations? We conclusively show that the
image data matters in two ways: (a) using a dif-
ferent dataset (ImageNet) also results in a perfor-
mance boost, and (b) using higher-quality image
features on the ESP game images increases the
performance boost without changing the associa-
tion between word labels.
</bodyText>
<subsectionHeader confidence="0.988869">
5.4 Image Datasets
</subsectionHeader>
<bodyText confidence="0.999986928571429">
It is important to ask whether the source im-
age dataset has a large impact on performance.
Although the scores for the visual representa-
tion in some cases differ, performance of multi-
modal representations remains close for both im-
age datasets. This implies that our method is ro-
bust over different datasets. It also suggests that it
is beneficial to train on high-quality datasets like
ImageNet and to subsequently generate embed-
dings for other sets of images like the ESP Game
dataset that are more noisy but have better cover-
age. The results show the benefit of transfering
convolutional network features, corroborating re-
cent results in computer vision.
</bodyText>
<subsectionHeader confidence="0.9932">
5.5 Semantic Similarity/Relatedness Datasets
</subsectionHeader>
<bodyText confidence="0.999608555555555">
There is an interesting discrepancy between the
two types of network with respect to dataset per-
formance: CNN-Mean multi-modal models tend
to perform best on MEN and MEN-Relevant,
while CNN-Max multi-modal models perform
better on W353 and W353-Relevant. There also
appears to be some interplay between the source
corpus, the evaluation dataset and the best per-
forming CNN: the performance leap on W353-
</bodyText>
<page confidence="0.99951">
41
</page>
<figureCaption confidence="0.897983">
Figure 4: Varying the α parameter for MEN, MEN-Relevant, WordSim353 and WordSim353-Relevant,
respectively.
</figureCaption>
<bodyText confidence="0.999931473684211">
Relevant for CNN-Max is much larger using ESP
Game images than with ImageNet images.
We speculate that this is because CNN-Max per-
forms better than CNN-Mean on a somewhat dif-
ferent type of similarity. It has been noted (Agirre
et al., 2009) that WordSim353 captures both sim-
ilarity (as in tiger-cat, with a score of 7.35) as
well as relatedness (as in Maradona-football, with
a score of 8.62). MEN, however, is explicitly de-
signed to capture semantic relatedness only (Bruni
et al., 2012). CNN-Max using sparse feature vec-
tors means that we treat the dominant components
as definitive of the concept class, which is more
suited to similarity. CNN-Mean averages over
all the feature components, and as such might be
more suited to relatedness. We conjecture that the
performance increase on WordSim353 is due to
increased performance on the similarity subset of
that dataset.
</bodyText>
<subsectionHeader confidence="0.994354">
5.6 Tuning
</subsectionHeader>
<bodyText confidence="0.999978545454545">
The concatenation scheme in Equation 1 allows
for a tuning parameter α to weight the relative
contribution of the respective modalities. Previous
work on MEN has found that the optimal param-
eter for that dataset is close to 0.5 (Bruni et al.,
2014). We have found that this is indeed the case.
On WordSim353, however, we have found the pa-
rameter for optimal performance to be shifted to
the right, meaning that optimal performance is
achieved when we include less of the visual input
compared to the linguistic input. Figure 4 shows
what happens when we vary alpha over the four
datasets. There are a number of observations to be
made here.
First of all, we can see that the performance
peak for the MEN datastes is much higher than
for the WordSim353 ones, and that its peak is rel-
atively higher as well. This indicates that MEN is
in a sense a more balanced dataset. There are two
possible explanations: as indicated earlier, Word-
Sim353 contains slightly idiosyncratic word pairs
which may have a detrimental effect on perfor-
mance; or, WordSim353 was not constructed with
multi-modal semantics in mind, and contains a
substantial amount of abstract words that would
not benefit at all from including visual informa-
tion.
Due to the nature of the datasets and the tasks
at hand, it is arguably much more important that
CNNs beat standard bag-of-visual-words repre-
sentations on MEN than on W353, and indeed we
see that there exists no α for which BOVW would
beat any of the CNN networks.
</bodyText>
<sectionHeader confidence="0.998767" genericHeader="method">
6 Error Analysis
</sectionHeader>
<bodyText confidence="0.994763">
Table 2 shows the top 5 best and top 5 worst scor-
ing word pairs for the two datasets using CNN-
</bodyText>
<page confidence="0.997826">
42
</page>
<table confidence="0.99397752631579">
W353-Relevant
ImageNet ESP Game
word1 word2 system score gold standard word1 word2 system score gold standard
tiger tiger 1.00 1.00 tiger tiger 1.00 1.00
man governor 0.53 0.53 man governor 0.53 0.53
stock phone 0.15 0.16 stock phone 0.15 0.16
football tennis 0.68 0.66 football tennis 0.68 0.66
man woman 0.85 0.83 man woman 0.85 0.83
cell phone 0.27 0.78 law lawyer 0.33 0.84
discovery space 0.10 0.63 monk slave 0.58 0.09
closet clothes 0.22 0.80 gem jewel 0.41 0.90
king queen 0.26 0.86 stock market 0.33 0.81
wood forest 0.13 0.77 planet space 0.32 0.79
MEN-Relevant
ImageNet
word1 word2 system score gold standard
beef potatoes 0.35 0.35
art work 0.35 0.35
grass stop 0.06 0.06
shade tree 0.45 0.45
blonde rock 0.07 0.07
bread potatoes 0.88 0.34
fruit potatoes 0.80 0.26
dessert sandwich 0.76 0.23
pepper tomato 0.79 0.27
dessert tomato 0.66 0.14
ESP Game
word1 word2 system score gold standard
beef potatoes 0.35 0.35
art work 0.35 0.35
grass stop 0.06 0.06
shade tree 0.45 0.45
blonde rock 0.07 0.07
bread dessert 0.78 0.24
jacket shirt 0.89 0.34
fruit nuts 0.88 0.33
dinner lunch 0.93 0.37
dessert soup 0.81 0.23
</table>
<tableCaption confidence="0.999763">
Table 2: The top 5 best and top 5 worst scoring pairs with respect to the gold standard.
</tableCaption>
<bodyText confidence="0.999737222222222">
Mean multi-modal vectors. The most accurate
pairs are consistently the same across the two im-
age datasets. There are some clear differences
between the least accurate pairs, however. The
MEN words potatoes and tomato probably have
low quality ImageNet-derived representations, be-
cause they occur often in the bottom pairs for that
dataset. The MEN words dessert, bread and fruit
occur in the bottom 5 for both image datasets,
which implies that their linguistic representations
are probably not very good. For WordSim353, the
bottom pairs on ImageNet could be said to be sim-
ilarity mistakes; while the ESP Game dataset con-
tains more relatedness mistakes (king and queen
would evaluate similarity, while stock and market
would evaluate relatedness). It is difficult to say
anything conclusive about this discrepancy, but it
is clearly a direction for future research.
</bodyText>
<sectionHeader confidence="0.978649" genericHeader="method">
7 Image embeddings
</sectionHeader>
<bodyText confidence="0.998777">
To facilitate further research on image embed-
dings and multi-modal semantics, we publicly re-
lease embeddings for all the image labels occur-
ring in the ESP Game dataset. Please see the fol-
lowing web page: http://www.cl.cam.ac.
uk/˜dk427/imgembed.html
</bodyText>
<sectionHeader confidence="0.992842" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99999015">
We presented a novel approach to improving
multi-modal representations using deep convo-
lutional neural network-extracted features. We
reported high results on two well-known and
widely-used semantic relatedness benchmarks,
with increased performance both in the separate
visual representations and in the combined multi-
modal representations. Our results indicate that
such multi-modal representations outperform both
linguistic and standard bag-of-visual-words multi-
modal representations. We have shown that our
approach is robust and that CNN-extracted fea-
tures from separate image datasets can succesfully
be applied to semantic relatedness.
In addition to improving multi-modal represen-
tations, we have shown that the source of this im-
provement is due to image data and is not simply a
result of word label associations. We have shown
this by obtaining performance improvements on
two different image datasets, and by obtaining
</bodyText>
<page confidence="0.999249">
43
</page>
<bodyText confidence="0.9999528">
higher performance with higher-quality image fea-
tures on the ESP game images, without changing
the association between word labels.
In future work, we will investigate whether our
system can be further improved by including con-
creteness information or a substitute metric such
as image dispersion, as has been suggested by
other work on multi-modal semantics (Kiela et al.,
2014). Furthermore, a logical next step to increase
performance would be to jointly learn multi-modal
representations or to learn weighting parameters.
Another interesting possibility would be to exam-
ine multi-modal distributional compositional se-
mantics, where multi-modal representations are
composed to obtain phrasal representations.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998585">
We would like to thank Maxime Oquab for pro-
viding the feature extraction code.
</bodyText>
<sectionHeader confidence="0.996225" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988673963855421">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ’09, pages 19–27, Boulder, Colorado.
Lawrence W. Barsalou. 2008. Grounded cognition.
Annual Review of Psychology, 59:617—845.
Herbert Bay, Andreas Ess, Tinne Tuytelaars, and
Luc Van Gool. 2008. SURF: Speeded Up Robust
Features. In Computer Vision and Image Under-
standing (CVIU), volume 110, pages 346–359.
Steven Bird, Edward Loper, and Ewan Klein.
2009. Natural Language Processing with Python.
O’Reilly Media Inc.
Anna Bosch, Andrew Zisserman, and Xavier Munoz.
2007. Image classification using random forests and
ferns. In Proceedings of ICCV.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 136–145. Asso-
ciation for Computational Linguistics.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1–47.
Navneet Dalal and Bill Triggs. 2005. Histograms
of oriented gradients for human detection. In Pro-
ceedings of the 2005 IEEE Computer Society Con-
ference on Computer Vision and Pattern Recogni-
tion (CVPR’05) - Volume 1 - Volume 01, CVPR ’05,
pages 886–893.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hi-
erarchical image database. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Con-
ference on, pages 248–255. IEEE.
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoff-
man, Ning Zhang, Eric Tzeng, and Trevor Darrell.
2014. DeCAF: A Deep Convolutional Activation
Feature for Generic Visual Recognition. In Inter-
national Conference on Machine Learning (ICML
2014).
Xavier Driancourt and L´eon Bottou. 1990. TDNN-
extracted features. In Proceedings of Neuro Nimes
90, Nimes, France. EC2.
Christiane Fellbaum. 1999. WordNet. Wiley Online
Library.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 91–99. Asso-
ciation for Computational Linguistics.
Fangxiang Feng, Ruifan Li, and Xiaojie Wang. 2013.
Constructing hierarchical image-tags bimodal repre-
sentations for word tags alternative choice. CoRR.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web, pages 406–
414. ACM.
Andrea Frome, Greg Corrado, Jonathon Shlens, Samy
Bengio, Jeffrey Dean, Marc ´Aurelio Ranzato, and
Tomas Mikolov. 2013. DeViSE: A Deep Visual-
Semantic Embedding Model. In NIPS.
R. Girshick, J. Donahue, T. Darrell, and J. Malik.
2013. Rich feature hierarchies for accurate ob-
ject detection and semantic segmentation. arXiv
preprint:1311.2524, November.
Douwe Kiela and Stephen Clark. 2014. A Systematic
Study of Semantic Vector Space Model Parameters.
In Proceedings of EACL 2014, Workshop on Contin-
uous Vector Space Models and their Compositional-
ity (CVSC).
Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen
Clark. 2014. Improving Multi-Modal Representa-
tions Using Image Dispersion: Why Less is Some-
times More. In Proceedings of ACL 2014.
</reference>
<page confidence="0.979538">
44
</page>
<reference confidence="0.999374430107527">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-
ton. 2012. Imagenet classification with deep con-
volutional neural networks. In NIPS, pages 1106–
1114.
Angeliki Lazaridou, Elia Bruni, and Marco Baroni.
2014. Is this a wampimuk? cross-modal map-
ping between distributional semantics and the visual
world. In Proceedings of ACL 2014.
Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. Claws4: the tagging of the British National
Corpus. In Proceedings of the 15th conference
on Computational linguistics-Volume 1, pages 622–
628. Association for Computational Linguistics.
Ben Leong and Rada Mihalcea. 2011. Going Beyond
Text: A Hybrid Image-Text Approach for Measuring
Word Relatedness. In Proceedings of Joint Interna-
tional Conference on Natural Language Processing
(IJCNLP), Chiang Mai, Thailand.
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of ACL
2014.
M. M. Louwerse. 2011. Symbol interdependency in
symbolic and embodied cognition. TopiCS in Cog-
nitive Science, 3:273—302.
David G. Lowe. 1999. Object recognition from local
scale-invariant features. In Proceedings of the Inter-
national Conference on Computer Vision-Volume 2 -
Volume 2, ICCV ’99.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word repre-
sentations in vector space. In Proceedings of Inter-
national Conference of Learning Representations,
Scottsdale, Arizona, USA.
M. Oquab, L. Bottou, I. Laptev, and J. Sivic. 2014.
Learning and transferring mid-level image represen-
tations using convolutional neural networks. In Pro-
ceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition.
A.S. Razavian, H. Azizpour, J. Sullivan, and S. Carls-
son. 2014. CNN features off-the-shelf:
an astounding baseline for recognition. arXiv
preprint:1403.6382.
Stephen Roller and Sabine Schulte im Walde. 2013.
A multimodal LDA model integrating textual, cog-
nitive and visual modalities. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1146–1157, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
D Sculley. 2010. Web-scale k-means clustering. In
Proceedings of the 19th international conference on
World wide web, pages 1177–1178. ACM.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423–1433. As-
sociation for Computational Linguistics.
Carina Silberer and Mirella Lapata. 2014. Learning
Grounded Meaning Representations with Autoen-
coders. In Proceedings of ACL 2014, Baltimore,
MD.
J. Sivic and A. Zisserman. 2003. Video Google: a text
retrieval approach to object matching in videos. In
Proceedings of the Ninth IEEE International Con-
ference on Computer Vision, volume 2, pages 1470–
1477, Oct.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded Compositional Semantics for Finding and
Describing Images with Sentences. Transactions
of the Association for Computational Linguistics
(TACL 2014).
Nitish Srivastava and Ruslan Salakhutdinov. 2012.
Multimodal learning with deep boltzmann ma-
chines. In F. Pereira, C.J.C. Burges, L. Bottou, and
K.Q. Weinberger, editors, Advances in Neural Infor-
mation Processing Systems 25, pages 2222–2230.
A. Vedaldi and B. Fulkerson. 2008. VLFeat: An open
and portable library of computer vision algorithms.
http://www.vlfeat.org/.
Luis Von Ahn and Laura Dabbish. 2004. Labeling
images with a computer game. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 319–326. ACM.
Pengcheng Wu, Steven C.H. Hoi, Hao Xia, Peilin Zhao,
Dayong Wang, and Chunyan Miao. 2013. Online
multimodal deep similarity learning with application
to image retrieval. In Proceedings of the 21st ACM
International Conference on Multimedia, MM ’13,
pages 153–162.
Matthew D. Zeiler and Rob Fergus. 2013. Visualizing
and understanding convolutional networks. CoRR.
</reference>
<page confidence="0.999379">
45
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.563703">
<title confidence="0.999765">Learning Image Embeddings using Convolutional Neural Networks</title>
<author confidence="0.711437">Improved Multi-Modal Semantics</author>
<affiliation confidence="0.8962935">University of Computer</affiliation>
<email confidence="0.992624">douwe.kiela@cl.cam.ac.uk</email>
<abstract confidence="0.9980075625">We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network (CNN) trained on a large labeled object recognition dataset. This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach. Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks. We use visual features computed using either ImageNet or ESP Game images.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>pages</pages>
<location>Boulder, Colorado.</location>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 19–27, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence W Barsalou</author>
</authors>
<title>Grounded cognition.</title>
<date>2008</date>
<journal>Annual Review of Psychology,</journal>
<pages>59--617</pages>
<contexts>
<context position="4144" citStr="Barsalou, 2008" startWordPosition="606" endWordPosition="607">.1 Multi-Modal Distributional Semantics Multi-modal models are motivated by parallels with human concept acquisition. Standard se36 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 36–45, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics mantic space models extract meanings solely from linguistic data, even though we know that human semantic knowledge relies heavily on perceptual information (Louwerse, 2011). That is, there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to do this grounding in the context of distributional semantics is to obtain representations that combine information from linguistic corpora with information from another modality, obtained from e.g. property norming experiments (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013) or from processing and extracting features from images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012). This approach has met with quite some success (Bruni et al., 2014). 2.2 Multi-modal Deep Learning Other examples that apply multi-modal deep learning use restricted Boltzm</context>
</contexts>
<marker>Barsalou, 2008</marker>
<rawString>Lawrence W. Barsalou. 2008. Grounded cognition. Annual Review of Psychology, 59:617—845.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Bay</author>
<author>Andreas Ess</author>
<author>Tinne Tuytelaars</author>
<author>Luc Van Gool</author>
</authors>
<title>SURF: Speeded Up Robust Features.</title>
<date>2008</date>
<booktitle>In Computer Vision and Image Understanding (CVIU),</booktitle>
<volume>110</volume>
<pages>346--359</pages>
<marker>Bay, Ess, Tuytelaars, Van Gool, 2008</marker>
<rawString>Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc Van Gool. 2008. SURF: Speeded Up Robust Features. In Computer Vision and Image Understanding (CVIU), volume 110, pages 346–359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Edward Loper</author>
<author>Ewan Klein</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python. O’Reilly Media Inc.</booktitle>
<contexts>
<context position="14260" citStr="Bird et al., 2009" startWordPosition="2209" endWordPosition="2212">in representations for high-level concepts such as entity, object and animal. Doing so is both computationally expensive and unlikely to improve the results. For this reason, we randomly sample up to N distinct images from the subtree associated with each concept. When this returns less than N images, we attempt to increase coverage by sampling images from the subtree of the concept’s hypernym instead. In order to allow for a fair comparison, we apply the same method of sampling up to N on the ESP Game dataset. In all following experiments, N = 1.000. We used the WordNet lemmatizer from NLTK (Bird et al., 2009) to lemmatize tags and concept words so as to further improve the dataset’s coverage. 4.3 Image Processing The ImageNet images were preprocessed as described by (Krizhevsky et al., 2012). The largest centered square contained in each image is resam39 pled to form a 256 × 256 image. The CNN input is then formed by cropping 16 pixels off each border and subtracting 128 to the image components. The ESP Game images were preprocessed slightly differently because we do not expect the objects to be centered. Each image was rescaled to fit inside a 224 × 224 rectangle. The CNN input is then formed by </context>
</contexts>
<marker>Bird, Loper, Klein, 2009</marker>
<rawString>Steven Bird, Edward Loper, and Ewan Klein. 2009. Natural Language Processing with Python. O’Reilly Media Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Bosch</author>
<author>Andrew Zisserman</author>
<author>Xavier Munoz</author>
</authors>
<title>Image classification using random forests and ferns.</title>
<date>2007</date>
<booktitle>In Proceedings of ICCV.</booktitle>
<contexts>
<context position="7531" citStr="Bosch et al., 2007" startWordPosition="1131" endWordPosition="1134">3.1 Perceptual Representations The perceptual component of standard multimodal models that rely on visual data is often an instance of the bag-of-visual-words (BOVW) representation (Sivic and Zisserman, 2003). This approach takes a collection of images associated with words or tags representing the concept in question. For each image, keypoints are laid out as a dense grid. Each keypoint is represented by a vector of robust local visual features such as SIFT (Lowe, 1999), SURF (Bay et al., 2008) and HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). These descriptors are subsequently clustered into a discrete set of “visual words” using a standard clustering algorithm like k-means and quantized into vector representations by comparing the local descriptors with the cluster centroids. Visual representations are obtained by taking the average of the BOVW vectors for the images that correspond to a given word. We use BOVW as a baseline. Our approach similarly makes use of a collection of images associated with words or tags representing a particular concept. Each image is processed by the first seven layers of the convolutional network def</context>
</contexts>
<marker>Bosch, Zisserman, Munoz, 2007</marker>
<rawString>Anna Bosch, Andrew Zisserman, and Xavier Munoz. 2007. Image classification using random forests and ferns. In Proceedings of ICCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>NamKhanh Tran</author>
</authors>
<title>Distributional semantics in technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>136--145</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1081" citStr="Bruni et al., 2012" startWordPosition="146" endWordPosition="149">ect recognition dataset. This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach. Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks. We use visual features computed using either ImageNet or ESP Game images. 1 Introduction Recent works have shown that multi-modal semantic representation models outperform unimodal linguistic models on a variety of tasks, including modeling semantic relatedness and predicting compositionality (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012; Roller and Schulte im Walde, 2013; Kiela et al., 2014). These results were obtained by combining linguistic feature representations with robust visual features extracted from a set of images associated with the concept in question. This extraction of visual features usually follows the popular computer vision approach consisting of computing local features, such as SIFT features (Lowe, 1999), and aggregating them as bags of visual words (Sivic and Zisserman, 2003). Meanwhile, deep transfer learning techniques have gained considerable attention in the computer vision community. First, a deep </context>
<context position="4571" citStr="Bruni et al., 2012" startWordPosition="668" endWordPosition="671">mantic knowledge relies heavily on perceptual information (Louwerse, 2011). That is, there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to do this grounding in the context of distributional semantics is to obtain representations that combine information from linguistic corpora with information from another modality, obtained from e.g. property norming experiments (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013) or from processing and extracting features from images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012). This approach has met with quite some success (Bruni et al., 2014). 2.2 Multi-modal Deep Learning Other examples that apply multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), auto-encoders (Wu et al., 2013) or recursive neural networks (Socher et al., 2014). Multimodal models with deep learning components have also successfully been employed in crossmodal tasks (Lazaridou et al., 2014). Work that is closely related in spirit to ours is by Silberer and Lapata (2014). They use a stacked auto-encoder to learn combined embeddings </context>
<context position="15597" citStr="Bruni et al., 2012" startWordPosition="2430" endWordPosition="2433">W features were obtained by computing DSIFT descriptors using VLFeat (Vedaldi and Fulkerson, 2008). These descriptors were subsequently clustered using mini-batch k-means (Sculley, 2010) with 100 clusters. Each image is then represented by a bag of clusters (visual words) quantized as a 100-dimensional feature vector. These vectors were then combined into visual concept representations by taking their mean. 4.4 Evaluation We evaluate our multi-modal word representations using two semantic relatedness datasets widely used in distributional semantics (Agirre et al., 2009; Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Clark, 2014; Bruni et al., 2014). WordSim353 (Finkelstein et al., 2001) is a selection of 353 concept pairs with a similarity rating provided by human annotators. Since this is probably the most widely used evaluation dataset for distributional semantics, we include it for comparison with other approaches. WordSim353 has some known idiosyncracies: it includes named entities, such as OPEC, Arafat, and Maradona, as well as abstract words, such as antecedent and credibility, for which it may be hard to find corresponding images. Multi-modal representations are often evaluated on an un</context>
<context position="22496" citStr="Bruni et al., 2012" startWordPosition="3541" endWordPosition="3544">rmance leap on W353- 41 Figure 4: Varying the α parameter for MEN, MEN-Relevant, WordSim353 and WordSim353-Relevant, respectively. Relevant for CNN-Max is much larger using ESP Game images than with ImageNet images. We speculate that this is because CNN-Max performs better than CNN-Mean on a somewhat different type of similarity. It has been noted (Agirre et al., 2009) that WordSim353 captures both similarity (as in tiger-cat, with a score of 7.35) as well as relatedness (as in Maradona-football, with a score of 8.62). MEN, however, is explicitly designed to capture semantic relatedness only (Bruni et al., 2012). CNN-Max using sparse feature vectors means that we treat the dominant components as definitive of the concept class, which is more suited to similarity. CNN-Mean averages over all the feature components, and as such might be more suited to relatedness. We conjecture that the performance increase on WordSim353 is due to increased performance on the similarity subset of that dataset. 5.6 Tuning The concatenation scheme in Equation 1 allows for a tuning parameter α to weight the relative contribution of the respective modalities. Previous work on MEN has found that the optimal parameter for tha</context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Elia Bruni, Gemma Boleda, Marco Baroni, and NamKhanh Tran. 2012. Distributional semantics in technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 136–145. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>49--1</pages>
<contexts>
<context position="4639" citStr="Bruni et al., 2014" startWordPosition="680" endWordPosition="683"> 2011). That is, there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to do this grounding in the context of distributional semantics is to obtain representations that combine information from linguistic corpora with information from another modality, obtained from e.g. property norming experiments (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013) or from processing and extracting features from images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012). This approach has met with quite some success (Bruni et al., 2014). 2.2 Multi-modal Deep Learning Other examples that apply multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), auto-encoders (Wu et al., 2013) or recursive neural networks (Socher et al., 2014). Multimodal models with deep learning components have also successfully been employed in crossmodal tasks (Lazaridou et al., 2014). Work that is closely related in spirit to ours is by Silberer and Lapata (2014). They use a stacked auto-encoder to learn combined embeddings of textual and visual input. Their visual inputs consist of vectors </context>
<context position="10548" citStr="Bruni et al. (2014)" startWordPosition="1594" endWordPosition="1597">ctor representations using the log-linear skip-gram model of Mikolov et al. (2013) trained on a corpus consisting of the 400M word Text8 corpus of Wikipedia text2 together with the 100M word British National Corpus (Leech et al., 1994). We also experimented with dependency-based skip-grams (Levy and Goldberg, 2014) but this did not improve results. The skip-gram model learns high quality semantic representations based on the distributional properties of words in text, and outperforms standard distributional models on a variety of semantic similarity and relatedness tasks. However we note that Bruni et al. (2014) have recently reported an even better performance for their linguistic component using a standard distributional model, although this may have been tuned to the task. 3.3 Multi-modal Representations Following Bruni et al. (2014), we construct multimodal semantic representations by concatenating the centered and L2-normalized linguistic and perceptual feature vectors vling and Vvis, vconcept = α x vling ||(1 − α) x vvis (1) where ||denotes the concatenation operator and α is an optional tuning parameter. 2http://mattmahoney.net/dc/textdata.html 38 Figure 2: Examples of dog in the ESP Game data</context>
<context position="15641" citStr="Bruni et al., 2014" startWordPosition="2438" endWordPosition="2441"> descriptors using VLFeat (Vedaldi and Fulkerson, 2008). These descriptors were subsequently clustered using mini-batch k-means (Sculley, 2010) with 100 clusters. Each image is then represented by a bag of clusters (visual words) quantized as a 100-dimensional feature vector. These vectors were then combined into visual concept representations by taking their mean. 4.4 Evaluation We evaluate our multi-modal word representations using two semantic relatedness datasets widely used in distributional semantics (Agirre et al., 2009; Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Clark, 2014; Bruni et al., 2014). WordSim353 (Finkelstein et al., 2001) is a selection of 353 concept pairs with a similarity rating provided by human annotators. Since this is probably the most widely used evaluation dataset for distributional semantics, we include it for comparison with other approaches. WordSim353 has some known idiosyncracies: it includes named entities, such as OPEC, Arafat, and Maradona, as well as abstract words, such as antecedent and credibility, for which it may be hard to find corresponding images. Multi-modal representations are often evaluated on an unspecified subset of WordSim353 (Feng and Lap</context>
<context position="23142" citStr="Bruni et al., 2014" startWordPosition="3647" endWordPosition="3650">ure vectors means that we treat the dominant components as definitive of the concept class, which is more suited to similarity. CNN-Mean averages over all the feature components, and as such might be more suited to relatedness. We conjecture that the performance increase on WordSim353 is due to increased performance on the similarity subset of that dataset. 5.6 Tuning The concatenation scheme in Equation 1 allows for a tuning parameter α to weight the relative contribution of the respective modalities. Previous work on MEN has found that the optimal parameter for that dataset is close to 0.5 (Bruni et al., 2014). We have found that this is indeed the case. On WordSim353, however, we have found the parameter for optimal performance to be shifted to the right, meaning that optimal performance is achieved when we include less of the visual input compared to the linguistic input. Figure 4 shows what happens when we vary alpha over the four datasets. There are a number of observations to be made here. First of all, we can see that the performance peak for the MEN datastes is much higher than for the WordSim353 ones, and that its peak is relatively higher as well. This indicates that MEN is in a sense a mo</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. Journal of Artificial Intelligence Research, 49:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Navneet Dalal</author>
<author>Bill Triggs</author>
</authors>
<title>Histograms of oriented gradients for human detection.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05) - Volume 1 - Volume 01, CVPR ’05,</booktitle>
<pages>886--893</pages>
<contexts>
<context position="7445" citStr="Dalal and Triggs, 2005" startWordPosition="1115" endWordPosition="1118">ations Figure 1 illustrates how our system computes multi-modal semantic representations. 3.1 Perceptual Representations The perceptual component of standard multimodal models that rely on visual data is often an instance of the bag-of-visual-words (BOVW) representation (Sivic and Zisserman, 2003). This approach takes a collection of images associated with words or tags representing the concept in question. For each image, keypoints are laid out as a dense grid. Each keypoint is represented by a vector of robust local visual features such as SIFT (Lowe, 1999), SURF (Bay et al., 2008) and HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). These descriptors are subsequently clustered into a discrete set of “visual words” using a standard clustering algorithm like k-means and quantized into vector representations by comparing the local descriptors with the cluster centroids. Visual representations are obtained by taking the average of the BOVW vectors for the images that correspond to a given word. We use BOVW as a baseline. Our approach similarly makes use of a collection of images associated with words or tags representing a particular conce</context>
</contexts>
<marker>Dalal, Triggs, 2005</marker>
<rawString>Navneet Dalal and Bill Triggs. 2005. Histograms of oriented gradients for human detection. In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05) - Volume 1 - Volume 01, CVPR ’05, pages 886–893.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Wei Dong</author>
<author>Richard Socher</author>
<author>Li-Jia Li</author>
<author>Kai Li</author>
<author>Li Fei-Fei</author>
</authors>
<title>Imagenet: A large-scale hierarchical image database.</title>
<date>2009</date>
<booktitle>In Computer Vision and Pattern Recognition,</booktitle>
<pages>248--255</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6608" citStr="Deng et al., 2009" startWordPosition="989" endWordPosition="992"> flurry of recent results indicates that image descriptors extracted from deep convolutional neural networks (CNNs) are very powerful and consistently outperform highly tuned state-of-the-art systems on a variety of visual recognition tasks (Razavian et al., 2014). Embeddings from stateof-the-art CNNs (such as Krizhevsky et al. (2012)) have been applied successfully to a number of problems in computer vision (Girshick et al., 2013; Zeiler and Fergus, 2013; Donahue et al., 2014). This contribution follows the approach described by Oquab et al. (2014): they train a CNN on 1512 ImageNet synsets (Deng et al., 2009), use the first seven layers of the trained network as feature extractors on the Pascal VOC dataset, and achieve state-of-the-art performance on the Pascal VOC classification task. 3 Improving Multi-Modal Representations Figure 1 illustrates how our system computes multi-modal semantic representations. 3.1 Perceptual Representations The perceptual component of standard multimodal models that rely on visual data is often an instance of the bag-of-visual-words (BOVW) representation (Sivic and Zisserman, 2003). This approach takes a collection of images associated with words or tags representing </context>
<context position="11603" citStr="Deng et al., 2009" startWordPosition="1750" endWordPosition="1753">enotes the concatenation operator and α is an optional tuning parameter. 2http://mattmahoney.net/dc/textdata.html 38 Figure 2: Examples of dog in the ESP Game dataset. Figure 3: Examples of golden retriever in ImageNet. 4 Experimental Setup We carried out experiments using visual representations computed using two canonical image datasets. The resulting multi-modal concept representations were evaluated using two well-known semantic relatedness datasets. 4.1 Visual Data We carried out experiments using two distinct sources of images to compute the visual representations. The ImageNet dataset (Deng et al., 2009) is a large-scale ontology of images organized according to the hierarchy of WordNet (Fellbaum, 1999). The dataset was constructed by manually re-labelling candidate images collected using web searches for each WordNet synset. The images tend to be of high quality with the designated object roughly centered in the image. Our copy of ImageNet contains about 12.5 million images organized in 22K synsets. This implies that ImageNet covers only a small fraction of the existing 117K WordNet synsets. The ESP Game dataset (Von Ahn and Dabbish, 2004) was famously collected as a “game with a purpose”, i</context>
</contexts>
<marker>Deng, Dong, Socher, Li, Li, Fei-Fei, 2009</marker>
<rawString>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248–255. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Donahue</author>
<author>Yangqing Jia</author>
<author>Oriol Vinyals</author>
<author>Judy Hoffman</author>
<author>Ning Zhang</author>
<author>Eric Tzeng</author>
<author>Trevor Darrell</author>
</authors>
<title>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition.</title>
<date>2014</date>
<booktitle>In International Conference on Machine Learning (ICML</booktitle>
<contexts>
<context position="2130" citStr="Donahue et al., 2014" startWordPosition="312" endWordPosition="315">f visual words (Sivic and Zisserman, 2003). Meanwhile, deep transfer learning techniques have gained considerable attention in the computer vision community. First, a deep convolutional neural network (CNN) is trained on a large ∗ This work was carried out while Douwe Kiela was an intern at Microsoft Research, New York. L´eon Bottou Microsoft Research New York leon@bottou.org labeled dataset (Krizhevsky et al., 2012). The convolutional layers are then used as mid-level feature extractors on a variety of computer vision tasks (Oquab et al., 2014; Girshick et al., 2013; Zeiler and Fergus, 2013; Donahue et al., 2014). Although transferring convolutional network features is not a new idea (Driancourt and Bottou, 1990), the simultaneous availability of large datasets and cheap GPU co-processors has contributed to the achievement of considerable performance gains on a variety computer vision benchmarks: “SIFT and HOG descriptors produced big performance gains a decade ago, and now deep convolutional features are providing a similar breakthrough” (Razavian et al., 2014). This work reports on results obtained by using CNN-extracted features in multi-modal semantic representation models. These results are inter</context>
<context position="6472" citStr="Donahue et al., 2014" startWordPosition="965" endWordPosition="968">sentations as well, but apply them to a visual object recognition task instead of concept meaning. 2.3 Deep Convolutional Neural Networks A flurry of recent results indicates that image descriptors extracted from deep convolutional neural networks (CNNs) are very powerful and consistently outperform highly tuned state-of-the-art systems on a variety of visual recognition tasks (Razavian et al., 2014). Embeddings from stateof-the-art CNNs (such as Krizhevsky et al. (2012)) have been applied successfully to a number of problems in computer vision (Girshick et al., 2013; Zeiler and Fergus, 2013; Donahue et al., 2014). This contribution follows the approach described by Oquab et al. (2014): they train a CNN on 1512 ImageNet synsets (Deng et al., 2009), use the first seven layers of the trained network as feature extractors on the Pascal VOC dataset, and achieve state-of-the-art performance on the Pascal VOC classification task. 3 Improving Multi-Modal Representations Figure 1 illustrates how our system computes multi-modal semantic representations. 3.1 Perceptual Representations The perceptual component of standard multimodal models that rely on visual data is often an instance of the bag-of-visual-words (</context>
</contexts>
<marker>Donahue, Jia, Vinyals, Hoffman, Zhang, Tzeng, Darrell, 2014</marker>
<rawString>Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. 2014. DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition. In International Conference on Machine Learning (ICML 2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Driancourt</author>
<author>L´eon Bottou</author>
</authors>
<title>TDNNextracted features.</title>
<date>1990</date>
<booktitle>In Proceedings of Neuro Nimes 90,</booktitle>
<location>Nimes,</location>
<contexts>
<context position="2232" citStr="Driancourt and Bottou, 1990" startWordPosition="327" endWordPosition="330">ained considerable attention in the computer vision community. First, a deep convolutional neural network (CNN) is trained on a large ∗ This work was carried out while Douwe Kiela was an intern at Microsoft Research, New York. L´eon Bottou Microsoft Research New York leon@bottou.org labeled dataset (Krizhevsky et al., 2012). The convolutional layers are then used as mid-level feature extractors on a variety of computer vision tasks (Oquab et al., 2014; Girshick et al., 2013; Zeiler and Fergus, 2013; Donahue et al., 2014). Although transferring convolutional network features is not a new idea (Driancourt and Bottou, 1990), the simultaneous availability of large datasets and cheap GPU co-processors has contributed to the achievement of considerable performance gains on a variety computer vision benchmarks: “SIFT and HOG descriptors produced big performance gains a decade ago, and now deep convolutional features are providing a similar breakthrough” (Razavian et al., 2014). This work reports on results obtained by using CNN-extracted features in multi-modal semantic representation models. These results are interesting in several respects. First, these superior features provide the opportunity to increase the per</context>
</contexts>
<marker>Driancourt, Bottou, 1990</marker>
<rawString>Xavier Driancourt and L´eon Bottou. 1990. TDNNextracted features. In Proceedings of Neuro Nimes 90, Nimes, France. EC2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<date>1999</date>
<publisher>WordNet. Wiley Online Library.</publisher>
<contexts>
<context position="11704" citStr="Fellbaum, 1999" startWordPosition="1768" endWordPosition="1769">data.html 38 Figure 2: Examples of dog in the ESP Game dataset. Figure 3: Examples of golden retriever in ImageNet. 4 Experimental Setup We carried out experiments using visual representations computed using two canonical image datasets. The resulting multi-modal concept representations were evaluated using two well-known semantic relatedness datasets. 4.1 Visual Data We carried out experiments using two distinct sources of images to compute the visual representations. The ImageNet dataset (Deng et al., 2009) is a large-scale ontology of images organized according to the hierarchy of WordNet (Fellbaum, 1999). The dataset was constructed by manually re-labelling candidate images collected using web searches for each WordNet synset. The images tend to be of high quality with the designated object roughly centered in the image. Our copy of ImageNet contains about 12.5 million images organized in 22K synsets. This implies that ImageNet covers only a small fraction of the existing 117K WordNet synsets. The ESP Game dataset (Von Ahn and Dabbish, 2004) was famously collected as a “game with a purpose”, in which two players must independently and rapidly agree on a correct word label for randomly selecte</context>
</contexts>
<marker>Fellbaum, 1999</marker>
<rawString>Christiane Fellbaum. 1999. WordNet. Wiley Online Library.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Visual information in semantic representation.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>91--99</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1035" citStr="Feng and Lapata, 2010" startWordPosition="138" endWordPosition="141">ural network (CNN) trained on a large labeled object recognition dataset. This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach. Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks. We use visual features computed using either ImageNet or ESP Game images. 1 Introduction Recent works have shown that multi-modal semantic representation models outperform unimodal linguistic models on a variety of tasks, including modeling semantic relatedness and predicting compositionality (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012; Roller and Schulte im Walde, 2013; Kiela et al., 2014). These results were obtained by combining linguistic feature representations with robust visual features extracted from a set of images associated with the concept in question. This extraction of visual features usually follows the popular computer vision approach consisting of computing local features, such as SIFT features (Lowe, 1999), and aggregating them as bags of visual words (Sivic and Zisserman, 2003). Meanwhile, deep transfer learning techniques have gained considerable attention in</context>
<context position="4524" citStr="Feng and Lapata, 2010" startWordPosition="660" endWordPosition="663">inguistic data, even though we know that human semantic knowledge relies heavily on perceptual information (Louwerse, 2011). That is, there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to do this grounding in the context of distributional semantics is to obtain representations that combine information from linguistic corpora with information from another modality, obtained from e.g. property norming experiments (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013) or from processing and extracting features from images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012). This approach has met with quite some success (Bruni et al., 2014). 2.2 Multi-modal Deep Learning Other examples that apply multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), auto-encoders (Wu et al., 2013) or recursive neural networks (Socher et al., 2014). Multimodal models with deep learning components have also successfully been employed in crossmodal tasks (Lazaridou et al., 2014). Work that is closely related in spirit to ours is by Silberer and Lapata (2014). They use a sta</context>
<context position="15577" citStr="Feng and Lapata, 2010" startWordPosition="2426" endWordPosition="2429">d zero padding. The BOVW features were obtained by computing DSIFT descriptors using VLFeat (Vedaldi and Fulkerson, 2008). These descriptors were subsequently clustered using mini-batch k-means (Sculley, 2010) with 100 clusters. Each image is then represented by a bag of clusters (visual words) quantized as a 100-dimensional feature vector. These vectors were then combined into visual concept representations by taking their mean. 4.4 Evaluation We evaluate our multi-modal word representations using two semantic relatedness datasets widely used in distributional semantics (Agirre et al., 2009; Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Clark, 2014; Bruni et al., 2014). WordSim353 (Finkelstein et al., 2001) is a selection of 353 concept pairs with a similarity rating provided by human annotators. Since this is probably the most widely used evaluation dataset for distributional semantics, we include it for comparison with other approaches. WordSim353 has some known idiosyncracies: it includes named entities, such as OPEC, Arafat, and Maradona, as well as abstract words, such as antecedent and credibility, for which it may be hard to find corresponding images. Multi-modal representations are ofte</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010. Visual information in semantic representation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 91–99. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangxiang Feng</author>
<author>Ruifan Li</author>
<author>Xiaojie Wang</author>
</authors>
<title>Constructing hierarchical image-tags bimodal representations for word tags alternative choice.</title>
<date>2013</date>
<publisher>CoRR.</publisher>
<contexts>
<context position="4812" citStr="Feng et al., 2013" startWordPosition="704" endWordPosition="707">istributional semantics is to obtain representations that combine information from linguistic corpora with information from another modality, obtained from e.g. property norming experiments (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013) or from processing and extracting features from images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012). This approach has met with quite some success (Bruni et al., 2014). 2.2 Multi-modal Deep Learning Other examples that apply multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), auto-encoders (Wu et al., 2013) or recursive neural networks (Socher et al., 2014). Multimodal models with deep learning components have also successfully been employed in crossmodal tasks (Lazaridou et al., 2014). Work that is closely related in spirit to ours is by Silberer and Lapata (2014). They use a stacked auto-encoder to learn combined embeddings of textual and visual input. Their visual inputs consist of vectors of visual attributes obtained from learning SVM classifiers on attribute prediction tasks. In contrast, our work keeps the modalities separate and follows the standard multi</context>
</contexts>
<marker>Feng, Li, Wang, 2013</marker>
<rawString>Fangxiang Feng, Ruifan Li, and Xiaojie Wang. 2013. Constructing hierarchical image-tags bimodal representations for word tags alternative choice. CoRR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2001</date>
<booktitle>In Proceedings of the 10th international conference on World Wide Web,</booktitle>
<pages>406--414</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="15680" citStr="Finkelstein et al., 2001" startWordPosition="2443" endWordPosition="2446">i and Fulkerson, 2008). These descriptors were subsequently clustered using mini-batch k-means (Sculley, 2010) with 100 clusters. Each image is then represented by a bag of clusters (visual words) quantized as a 100-dimensional feature vector. These vectors were then combined into visual concept representations by taking their mean. 4.4 Evaluation We evaluate our multi-modal word representations using two semantic relatedness datasets widely used in distributional semantics (Agirre et al., 2009; Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Clark, 2014; Bruni et al., 2014). WordSim353 (Finkelstein et al., 2001) is a selection of 353 concept pairs with a similarity rating provided by human annotators. Since this is probably the most widely used evaluation dataset for distributional semantics, we include it for comparison with other approaches. WordSim353 has some known idiosyncracies: it includes named entities, such as OPEC, Arafat, and Maradona, as well as abstract words, such as antecedent and credibility, for which it may be hard to find corresponding images. Multi-modal representations are often evaluated on an unspecified subset of WordSim353 (Feng and Lapata, 2010; Bruni et al., 2012; Bruni et</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The concept revisited. In Proceedings of the 10th international conference on World Wide Web, pages 406– 414. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Frome</author>
<author>Greg Corrado</author>
<author>Jonathon Shlens</author>
<author>Samy Bengio</author>
<author>Jeffrey Dean</author>
<author>Marc ´Aurelio Ranzato</author>
<author>Tomas Mikolov</author>
</authors>
<title>DeViSE: A Deep VisualSemantic Embedding Model.</title>
<date>2013</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="5820" citStr="Frome et al. (2013)" startWordPosition="865" endWordPosition="868">heir visual inputs consist of vectors of visual attributes obtained from learning SVM classifiers on attribute prediction tasks. In contrast, our work keeps the modalities separate and follows the standard multi-modal approach of concatenating linguistic and visual representations in a single semantic space model. This has the advantage that it allows for separate data sources for the individual modalities. We also learn visual representations directly from the images (i.e., we apply deep learning directly to the images), as opposed to taking a higher-level representation as a starting point. Frome et al. (2013) jointly learn multimodal representations as well, but apply them to a visual object recognition task instead of concept meaning. 2.3 Deep Convolutional Neural Networks A flurry of recent results indicates that image descriptors extracted from deep convolutional neural networks (CNNs) are very powerful and consistently outperform highly tuned state-of-the-art systems on a variety of visual recognition tasks (Razavian et al., 2014). Embeddings from stateof-the-art CNNs (such as Krizhevsky et al. (2012)) have been applied successfully to a number of problems in computer vision (Girshick et al., </context>
</contexts>
<marker>Frome, Corrado, Shlens, Bengio, Dean, Ranzato, Mikolov, 2013</marker>
<rawString>Andrea Frome, Greg Corrado, Jonathon Shlens, Samy Bengio, Jeffrey Dean, Marc ´Aurelio Ranzato, and Tomas Mikolov. 2013. DeViSE: A Deep VisualSemantic Embedding Model. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girshick</author>
<author>J Donahue</author>
<author>T Darrell</author>
<author>J Malik</author>
</authors>
<title>Rich feature hierarchies for accurate object detection and semantic segmentation. arXiv preprint:1311.2524,</title>
<date>2013</date>
<contexts>
<context position="2082" citStr="Girshick et al., 2013" startWordPosition="304" endWordPosition="307">res (Lowe, 1999), and aggregating them as bags of visual words (Sivic and Zisserman, 2003). Meanwhile, deep transfer learning techniques have gained considerable attention in the computer vision community. First, a deep convolutional neural network (CNN) is trained on a large ∗ This work was carried out while Douwe Kiela was an intern at Microsoft Research, New York. L´eon Bottou Microsoft Research New York leon@bottou.org labeled dataset (Krizhevsky et al., 2012). The convolutional layers are then used as mid-level feature extractors on a variety of computer vision tasks (Oquab et al., 2014; Girshick et al., 2013; Zeiler and Fergus, 2013; Donahue et al., 2014). Although transferring convolutional network features is not a new idea (Driancourt and Bottou, 1990), the simultaneous availability of large datasets and cheap GPU co-processors has contributed to the achievement of considerable performance gains on a variety computer vision benchmarks: “SIFT and HOG descriptors produced big performance gains a decade ago, and now deep convolutional features are providing a similar breakthrough” (Razavian et al., 2014). This work reports on results obtained by using CNN-extracted features in multi-modal semanti</context>
<context position="6424" citStr="Girshick et al., 2013" startWordPosition="957" endWordPosition="960">ome et al. (2013) jointly learn multimodal representations as well, but apply them to a visual object recognition task instead of concept meaning. 2.3 Deep Convolutional Neural Networks A flurry of recent results indicates that image descriptors extracted from deep convolutional neural networks (CNNs) are very powerful and consistently outperform highly tuned state-of-the-art systems on a variety of visual recognition tasks (Razavian et al., 2014). Embeddings from stateof-the-art CNNs (such as Krizhevsky et al. (2012)) have been applied successfully to a number of problems in computer vision (Girshick et al., 2013; Zeiler and Fergus, 2013; Donahue et al., 2014). This contribution follows the approach described by Oquab et al. (2014): they train a CNN on 1512 ImageNet synsets (Deng et al., 2009), use the first seven layers of the trained network as feature extractors on the Pascal VOC dataset, and achieve state-of-the-art performance on the Pascal VOC classification task. 3 Improving Multi-Modal Representations Figure 1 illustrates how our system computes multi-modal semantic representations. 3.1 Perceptual Representations The perceptual component of standard multimodal models that rely on visual data i</context>
</contexts>
<marker>Girshick, Donahue, Darrell, Malik, 2013</marker>
<rawString>R. Girshick, J. Donahue, T. Darrell, and J. Malik. 2013. Rich feature hierarchies for accurate object detection and semantic segmentation. arXiv preprint:1311.2524, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>Stephen Clark</author>
</authors>
<title>A Systematic Study of Semantic Vector Space Model Parameters.</title>
<date>2014</date>
<booktitle>In Proceedings of EACL 2014, Workshop on Continuous Vector Space Models and their Compositionality (CVSC).</booktitle>
<contexts>
<context position="15620" citStr="Kiela and Clark, 2014" startWordPosition="2434" endWordPosition="2437">ined by computing DSIFT descriptors using VLFeat (Vedaldi and Fulkerson, 2008). These descriptors were subsequently clustered using mini-batch k-means (Sculley, 2010) with 100 clusters. Each image is then represented by a bag of clusters (visual words) quantized as a 100-dimensional feature vector. These vectors were then combined into visual concept representations by taking their mean. 4.4 Evaluation We evaluate our multi-modal word representations using two semantic relatedness datasets widely used in distributional semantics (Agirre et al., 2009; Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Clark, 2014; Bruni et al., 2014). WordSim353 (Finkelstein et al., 2001) is a selection of 353 concept pairs with a similarity rating provided by human annotators. Since this is probably the most widely used evaluation dataset for distributional semantics, we include it for comparison with other approaches. WordSim353 has some known idiosyncracies: it includes named entities, such as OPEC, Arafat, and Maradona, as well as abstract words, such as antecedent and credibility, for which it may be hard to find corresponding images. Multi-modal representations are often evaluated on an unspecified subset of Wor</context>
<context position="17017" citStr="Kiela and Clark, 2014" startWordPosition="2667" endWordPosition="2670">m353 dataset (W353) by setting the visual vector Vvis to zero for concepts without images. We also report scores on the subset (W353-Relevant) of pairs for which both concepts have both ImageNet and ESP Game images using the aforementioned selection procedure. MEN (Bruni et al., 2012) was in part designed to alleviate the WordSim353 problems. It was constructed in such a way that only frequent words with at least 50 images in the ESP Game dataset were included in the evaluation pairs. The MEN dataset has been found to mirror the aggregate score over a variety of tasks and similarity datasets (Kiela and Clark, 2014). It is also much larger, with 3000 words pairs consisting of 751 individual words. Although MEN was constructed so as to have at least a minimum amount of images available in the ESP Game dataset for each concept, this is not the case for ImageNet. Hence, similarly to WordSim353, we also evaluate on a subset (MEN-Relevant) for which images are available in both datasets. We evaluate the models in terms of their Spearman p correlation with the human relatedness ratings. The similarity between the representations associated with a pair of words is calculated using the cosine similarity: v1 · v2</context>
</contexts>
<marker>Kiela, Clark, 2014</marker>
<rawString>Douwe Kiela and Stephen Clark. 2014. A Systematic Study of Semantic Vector Space Model Parameters. In Proceedings of EACL 2014, Workshop on Continuous Vector Space Models and their Compositionality (CVSC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>Felix Hill</author>
<author>Anna Korhonen</author>
<author>Stephen Clark</author>
</authors>
<title>Improving Multi-Modal Representations Using Image Dispersion: Why Less is Sometimes More.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="1137" citStr="Kiela et al., 2014" startWordPosition="156" endWordPosition="159">h brings a clear performance gain over features based on the traditional bag-of-visual-word approach. Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks. We use visual features computed using either ImageNet or ESP Game images. 1 Introduction Recent works have shown that multi-modal semantic representation models outperform unimodal linguistic models on a variety of tasks, including modeling semantic relatedness and predicting compositionality (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012; Roller and Schulte im Walde, 2013; Kiela et al., 2014). These results were obtained by combining linguistic feature representations with robust visual features extracted from a set of images associated with the concept in question. This extraction of visual features usually follows the popular computer vision approach consisting of computing local features, such as SIFT features (Lowe, 1999), and aggregating them as bags of visual words (Sivic and Zisserman, 2003). Meanwhile, deep transfer learning techniques have gained considerable attention in the computer vision community. First, a deep convolutional neural network (CNN) is trained on a large</context>
</contexts>
<marker>Kiela, Hill, Korhonen, Clark, 2014</marker>
<rawString>Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen Clark. 2014. Improving Multi-Modal Representations Using Image Dispersion: Why Less is Sometimes More. In Proceedings of ACL 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Imagenet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>In NIPS,</booktitle>
<pages>1106--1114</pages>
<contexts>
<context position="1929" citStr="Krizhevsky et al., 2012" startWordPosition="278" endWordPosition="281">question. This extraction of visual features usually follows the popular computer vision approach consisting of computing local features, such as SIFT features (Lowe, 1999), and aggregating them as bags of visual words (Sivic and Zisserman, 2003). Meanwhile, deep transfer learning techniques have gained considerable attention in the computer vision community. First, a deep convolutional neural network (CNN) is trained on a large ∗ This work was carried out while Douwe Kiela was an intern at Microsoft Research, New York. L´eon Bottou Microsoft Research New York leon@bottou.org labeled dataset (Krizhevsky et al., 2012). The convolutional layers are then used as mid-level feature extractors on a variety of computer vision tasks (Oquab et al., 2014; Girshick et al., 2013; Zeiler and Fergus, 2013; Donahue et al., 2014). Although transferring convolutional network features is not a new idea (Driancourt and Bottou, 1990), the simultaneous availability of large datasets and cheap GPU co-processors has contributed to the achievement of considerable performance gains on a variety computer vision benchmarks: “SIFT and HOG descriptors produced big performance gains a decade ago, and now deep convolutional features ar</context>
<context position="6326" citStr="Krizhevsky et al. (2012)" startWordPosition="941" endWordPosition="944">g directly to the images), as opposed to taking a higher-level representation as a starting point. Frome et al. (2013) jointly learn multimodal representations as well, but apply them to a visual object recognition task instead of concept meaning. 2.3 Deep Convolutional Neural Networks A flurry of recent results indicates that image descriptors extracted from deep convolutional neural networks (CNNs) are very powerful and consistently outperform highly tuned state-of-the-art systems on a variety of visual recognition tasks (Razavian et al., 2014). Embeddings from stateof-the-art CNNs (such as Krizhevsky et al. (2012)) have been applied successfully to a number of problems in computer vision (Girshick et al., 2013; Zeiler and Fergus, 2013; Donahue et al., 2014). This contribution follows the approach described by Oquab et al. (2014): they train a CNN on 1512 ImageNet synsets (Deng et al., 2009), use the first seven layers of the trained network as feature extractors on the Pascal VOC dataset, and achieve state-of-the-art performance on the Pascal VOC classification task. 3 Improving Multi-Modal Representations Figure 1 illustrates how our system computes multi-modal semantic representations. 3.1 Perceptual</context>
<context position="8163" citStr="Krizhevsky et al. (2012)" startWordPosition="1234" endWordPosition="1237">descriptors are subsequently clustered into a discrete set of “visual words” using a standard clustering algorithm like k-means and quantized into vector representations by comparing the local descriptors with the cluster centroids. Visual representations are obtained by taking the average of the BOVW vectors for the images that correspond to a given word. We use BOVW as a baseline. Our approach similarly makes use of a collection of images associated with words or tags representing a particular concept. Each image is processed by the first seven layers of the convolutional network defined by Krizhevsky et al. (2012) and adapted by Oquab et al. (2014)1. This network takes 224 × 224 pixel RGB images and applies five successive convolutional layers followed by three fully connected layers. Its eighth and last 1http://www.di.ens.fr/willow/research/cnn/ 37 Multimodal word vector Aggregate 6144-dim feature vectors Training visual features (after Oquab et al., 2014) FC6 FC7 FC8 6144-dim feature vector C1-C2-C3-C4-C5 Convolutional layers Fully-connected layers Imagenet labels African elephant Wall clock ... Select images from ImageNet or ESP Word 100-dim word projections w(t-2) w(t-2) 100-dim word projections w(</context>
<context position="14446" citStr="Krizhevsky et al., 2012" startWordPosition="2239" endWordPosition="2242">ndomly sample up to N distinct images from the subtree associated with each concept. When this returns less than N images, we attempt to increase coverage by sampling images from the subtree of the concept’s hypernym instead. In order to allow for a fair comparison, we apply the same method of sampling up to N on the ESP Game dataset. In all following experiments, N = 1.000. We used the WordNet lemmatizer from NLTK (Bird et al., 2009) to lemmatize tags and concept words so as to further improve the dataset’s coverage. 4.3 Image Processing The ImageNet images were preprocessed as described by (Krizhevsky et al., 2012). The largest centered square contained in each image is resam39 pled to form a 256 × 256 image. The CNN input is then formed by cropping 16 pixels off each border and subtracting 128 to the image components. The ESP Game images were preprocessed slightly differently because we do not expect the objects to be centered. Each image was rescaled to fit inside a 224 × 224 rectangle. The CNN input is then formed by centering this image into the 224 × 224 input field, subtracting 128 to the image components, and zero padding. The BOVW features were obtained by computing DSIFT descriptors using VLFea</context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. Imagenet classification with deep convolutional neural networks. In NIPS, pages 1106– 1114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Elia Bruni</author>
<author>Marco Baroni</author>
</authors>
<title>Is this a wampimuk? cross-modal mapping between distributional semantics and the visual world.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="5027" citStr="Lazaridou et al., 2014" startWordPosition="738" endWordPosition="741">, 2012; Roller and Schulte im Walde, 2013) or from processing and extracting features from images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012). This approach has met with quite some success (Bruni et al., 2014). 2.2 Multi-modal Deep Learning Other examples that apply multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), auto-encoders (Wu et al., 2013) or recursive neural networks (Socher et al., 2014). Multimodal models with deep learning components have also successfully been employed in crossmodal tasks (Lazaridou et al., 2014). Work that is closely related in spirit to ours is by Silberer and Lapata (2014). They use a stacked auto-encoder to learn combined embeddings of textual and visual input. Their visual inputs consist of vectors of visual attributes obtained from learning SVM classifiers on attribute prediction tasks. In contrast, our work keeps the modalities separate and follows the standard multi-modal approach of concatenating linguistic and visual representations in a single semantic space model. This has the advantage that it allows for separate data sources for the individual modalities. We also learn v</context>
</contexts>
<marker>Lazaridou, Bruni, Baroni, 2014</marker>
<rawString>Angeliki Lazaridou, Elia Bruni, and Marco Baroni. 2014. Is this a wampimuk? cross-modal mapping between distributional semantics and the visual world. In Proceedings of ACL 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
<author>Roger Garside</author>
<author>Michael Bryant</author>
</authors>
<title>Claws4: the tagging of the British National Corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th conference on Computational linguistics-Volume 1,</booktitle>
<pages>622--628</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10164" citStr="Leech et al., 1994" startWordPosition="1535" endWordPosition="1538">nd method (CNN-Max) computes the component-wise maximum of all feature vectors. This approach makes sense because the feature vectors extracted from this particular network are quite sparse (about 22% non-zero coefficients) and can be interpreted as bags of visual properties. 3.2 Linguistic representations For our linguistic representations we extract 100- dimensional continuous vector representations using the log-linear skip-gram model of Mikolov et al. (2013) trained on a corpus consisting of the 400M word Text8 corpus of Wikipedia text2 together with the 100M word British National Corpus (Leech et al., 1994). We also experimented with dependency-based skip-grams (Levy and Goldberg, 2014) but this did not improve results. The skip-gram model learns high quality semantic representations based on the distributional properties of words in text, and outperforms standard distributional models on a variety of semantic similarity and relatedness tasks. However we note that Bruni et al. (2014) have recently reported an even better performance for their linguistic component using a standard distributional model, although this may have been tuned to the task. 3.3 Multi-modal Representations Following Bruni </context>
</contexts>
<marker>Leech, Garside, Bryant, 1994</marker>
<rawString>Geoffrey Leech, Roger Garside, and Michael Bryant. 1994. Claws4: the tagging of the British National Corpus. In Proceedings of the 15th conference on Computational linguistics-Volume 1, pages 622– 628. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Leong</author>
<author>Rada Mihalcea</author>
</authors>
<title>Going Beyond Text: A Hybrid Image-Text Approach for Measuring Word Relatedness.</title>
<date>2011</date>
<booktitle>In Proceedings of Joint International Conference on Natural Language Processing (IJCNLP),</booktitle>
<location>Chiang Mai, Thailand.</location>
<contexts>
<context position="1061" citStr="Leong and Mihalcea, 2011" startWordPosition="142" endWordPosition="145">ned on a large labeled object recognition dataset. This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach. Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks. We use visual features computed using either ImageNet or ESP Game images. 1 Introduction Recent works have shown that multi-modal semantic representation models outperform unimodal linguistic models on a variety of tasks, including modeling semantic relatedness and predicting compositionality (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012; Roller and Schulte im Walde, 2013; Kiela et al., 2014). These results were obtained by combining linguistic feature representations with robust visual features extracted from a set of images associated with the concept in question. This extraction of visual features usually follows the popular computer vision approach consisting of computing local features, such as SIFT features (Lowe, 1999), and aggregating them as bags of visual words (Sivic and Zisserman, 2003). Meanwhile, deep transfer learning techniques have gained considerable attention in the computer vision commu</context>
<context position="4550" citStr="Leong and Mihalcea, 2011" startWordPosition="664" endWordPosition="667">ough we know that human semantic knowledge relies heavily on perceptual information (Louwerse, 2011). That is, there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to do this grounding in the context of distributional semantics is to obtain representations that combine information from linguistic corpora with information from another modality, obtained from e.g. property norming experiments (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013) or from processing and extracting features from images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012). This approach has met with quite some success (Bruni et al., 2014). 2.2 Multi-modal Deep Learning Other examples that apply multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), auto-encoders (Wu et al., 2013) or recursive neural networks (Socher et al., 2014). Multimodal models with deep learning components have also successfully been employed in crossmodal tasks (Lazaridou et al., 2014). Work that is closely related in spirit to ours is by Silberer and Lapata (2014). They use a stacked auto-encoder to learn</context>
</contexts>
<marker>Leong, Mihalcea, 2011</marker>
<rawString>Ben Leong and Rada Mihalcea. 2011. Going Beyond Text: A Hybrid Image-Text Approach for Measuring Word Relatedness. In Proceedings of Joint International Conference on Natural Language Processing (IJCNLP), Chiang Mai, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="10245" citStr="Levy and Goldberg, 2014" startWordPosition="1546" endWordPosition="1549">rs. This approach makes sense because the feature vectors extracted from this particular network are quite sparse (about 22% non-zero coefficients) and can be interpreted as bags of visual properties. 3.2 Linguistic representations For our linguistic representations we extract 100- dimensional continuous vector representations using the log-linear skip-gram model of Mikolov et al. (2013) trained on a corpus consisting of the 400M word Text8 corpus of Wikipedia text2 together with the 100M word British National Corpus (Leech et al., 1994). We also experimented with dependency-based skip-grams (Levy and Goldberg, 2014) but this did not improve results. The skip-gram model learns high quality semantic representations based on the distributional properties of words in text, and outperforms standard distributional models on a variety of semantic similarity and relatedness tasks. However we note that Bruni et al. (2014) have recently reported an even better performance for their linguistic component using a standard distributional model, although this may have been tuned to the task. 3.3 Multi-modal Representations Following Bruni et al. (2014), we construct multimodal semantic representations by concatenating </context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Dependencybased word embeddings. In Proceedings of ACL 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M M Louwerse</author>
</authors>
<title>Symbol interdependency in symbolic and embodied cognition.</title>
<date>2011</date>
<journal>TopiCS in Cognitive Science,</journal>
<pages>3--273</pages>
<contexts>
<context position="4026" citStr="Louwerse, 2011" startWordPosition="589" endWordPosition="590">nal semantics that exclusively relies on deep learning for both its linguistic and visual components. 2 Related work 2.1 Multi-Modal Distributional Semantics Multi-modal models are motivated by parallels with human concept acquisition. Standard se36 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 36–45, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics mantic space models extract meanings solely from linguistic data, even though we know that human semantic knowledge relies heavily on perceptual information (Louwerse, 2011). That is, there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to do this grounding in the context of distributional semantics is to obtain representations that combine information from linguistic corpora with information from another modality, obtained from e.g. property norming experiments (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013) or from processing and extracting features from images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012). This approach has met with quite some success (Bruni </context>
</contexts>
<marker>Louwerse, 2011</marker>
<rawString>M. M. Louwerse. 2011. Symbol interdependency in symbolic and embodied cognition. TopiCS in Cognitive Science, 3:273—302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Lowe</author>
</authors>
<title>Object recognition from local scale-invariant features.</title>
<date>1999</date>
<booktitle>In Proceedings of the International Conference on Computer Vision-Volume</booktitle>
<volume>2</volume>
<contexts>
<context position="1477" citStr="Lowe, 1999" startWordPosition="210" endWordPosition="211">tation models outperform unimodal linguistic models on a variety of tasks, including modeling semantic relatedness and predicting compositionality (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012; Roller and Schulte im Walde, 2013; Kiela et al., 2014). These results were obtained by combining linguistic feature representations with robust visual features extracted from a set of images associated with the concept in question. This extraction of visual features usually follows the popular computer vision approach consisting of computing local features, such as SIFT features (Lowe, 1999), and aggregating them as bags of visual words (Sivic and Zisserman, 2003). Meanwhile, deep transfer learning techniques have gained considerable attention in the computer vision community. First, a deep convolutional neural network (CNN) is trained on a large ∗ This work was carried out while Douwe Kiela was an intern at Microsoft Research, New York. L´eon Bottou Microsoft Research New York leon@bottou.org labeled dataset (Krizhevsky et al., 2012). The convolutional layers are then used as mid-level feature extractors on a variety of computer vision tasks (Oquab et al., 2014; Girshick et al.,</context>
<context position="7387" citStr="Lowe, 1999" startWordPosition="1106" endWordPosition="1107">cation task. 3 Improving Multi-Modal Representations Figure 1 illustrates how our system computes multi-modal semantic representations. 3.1 Perceptual Representations The perceptual component of standard multimodal models that rely on visual data is often an instance of the bag-of-visual-words (BOVW) representation (Sivic and Zisserman, 2003). This approach takes a collection of images associated with words or tags representing the concept in question. For each image, keypoints are laid out as a dense grid. Each keypoint is represented by a vector of robust local visual features such as SIFT (Lowe, 1999), SURF (Bay et al., 2008) and HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). These descriptors are subsequently clustered into a discrete set of “visual words” using a standard clustering algorithm like k-means and quantized into vector representations by comparing the local descriptors with the cluster centroids. Visual representations are obtained by taking the average of the BOVW vectors for the images that correspond to a given word. We use BOVW as a baseline. Our approach similarly makes use of a collection of images ass</context>
</contexts>
<marker>Lowe, 1999</marker>
<rawString>David G. Lowe. 1999. Object recognition from local scale-invariant features. In Proceedings of the International Conference on Computer Vision-Volume 2 -Volume 2, ICCV ’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of International Conference of Learning Representations,</booktitle>
<location>Scottsdale, Arizona, USA.</location>
<contexts>
<context position="3292" citStr="Mikolov et al. (2013)" startWordPosition="482" endWordPosition="485">dal semantic representation models. These results are interesting in several respects. First, these superior features provide the opportunity to increase the performance gap achieved by augmenting linguistic features with multi-modal features. Second, this increased performance confirms that the multimodal performance improvement results from the information contained in the images and not the information used to select which images to use to represent a concept. Third, our evaluation reveals an intriguing property of the CNN-extracted features. Finally, since we use the skip-gram approach of Mikolov et al. (2013) to generate our linguistic features, we believe that this work represents the first approach to multimodal distributional semantics that exclusively relies on deep learning for both its linguistic and visual components. 2 Related work 2.1 Multi-Modal Distributional Semantics Multi-modal models are motivated by parallels with human concept acquisition. Standard se36 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 36–45, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics mantic space models extract meanings so</context>
<context position="8860" citStr="Mikolov et al., 2013" startWordPosition="1332" endWordPosition="1335"> images and applies five successive convolutional layers followed by three fully connected layers. Its eighth and last 1http://www.di.ens.fr/willow/research/cnn/ 37 Multimodal word vector Aggregate 6144-dim feature vectors Training visual features (after Oquab et al., 2014) FC6 FC7 FC8 6144-dim feature vector C1-C2-C3-C4-C5 Convolutional layers Fully-connected layers Imagenet labels African elephant Wall clock ... Select images from ImageNet or ESP Word 100-dim word projections w(t-2) w(t-2) 100-dim word projections w(t) w(t+1) w(t+2) C1-C2-C3-C4-C5 FC6 FC7 Training linguistic features (after Mikolov et al., 2013) Figure 1: Computing word feature vectors. layer produces a vector of 1512 scores associated with 1000 categories of the ILSVRC-2012 challenge and the 512 additional categories selected by Oquab et al. (2014). This network was trained using about 1.6 million ImageNet images associated with these 1512 categories. We then freeze the trained parameters, chop the last network layer, and use the remaining seventh layer as a filter to compute a 6144-dimensional feature vector on arbitrary 224 x 224 input images. We consider two ways to aggregate the feature vectors representing each image. 1. The fi</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In Proceedings of International Conference of Learning Representations, Scottsdale, Arizona, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Oquab</author>
<author>L Bottou</author>
<author>I Laptev</author>
<author>J Sivic</author>
</authors>
<title>Learning and transferring mid-level image representations using convolutional neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</booktitle>
<contexts>
<context position="2059" citStr="Oquab et al., 2014" startWordPosition="300" endWordPosition="303">, such as SIFT features (Lowe, 1999), and aggregating them as bags of visual words (Sivic and Zisserman, 2003). Meanwhile, deep transfer learning techniques have gained considerable attention in the computer vision community. First, a deep convolutional neural network (CNN) is trained on a large ∗ This work was carried out while Douwe Kiela was an intern at Microsoft Research, New York. L´eon Bottou Microsoft Research New York leon@bottou.org labeled dataset (Krizhevsky et al., 2012). The convolutional layers are then used as mid-level feature extractors on a variety of computer vision tasks (Oquab et al., 2014; Girshick et al., 2013; Zeiler and Fergus, 2013; Donahue et al., 2014). Although transferring convolutional network features is not a new idea (Driancourt and Bottou, 1990), the simultaneous availability of large datasets and cheap GPU co-processors has contributed to the achievement of considerable performance gains on a variety computer vision benchmarks: “SIFT and HOG descriptors produced big performance gains a decade ago, and now deep convolutional features are providing a similar breakthrough” (Razavian et al., 2014). This work reports on results obtained by using CNN-extracted features</context>
<context position="6545" citStr="Oquab et al. (2014)" startWordPosition="977" endWordPosition="980">ead of concept meaning. 2.3 Deep Convolutional Neural Networks A flurry of recent results indicates that image descriptors extracted from deep convolutional neural networks (CNNs) are very powerful and consistently outperform highly tuned state-of-the-art systems on a variety of visual recognition tasks (Razavian et al., 2014). Embeddings from stateof-the-art CNNs (such as Krizhevsky et al. (2012)) have been applied successfully to a number of problems in computer vision (Girshick et al., 2013; Zeiler and Fergus, 2013; Donahue et al., 2014). This contribution follows the approach described by Oquab et al. (2014): they train a CNN on 1512 ImageNet synsets (Deng et al., 2009), use the first seven layers of the trained network as feature extractors on the Pascal VOC dataset, and achieve state-of-the-art performance on the Pascal VOC classification task. 3 Improving Multi-Modal Representations Figure 1 illustrates how our system computes multi-modal semantic representations. 3.1 Perceptual Representations The perceptual component of standard multimodal models that rely on visual data is often an instance of the bag-of-visual-words (BOVW) representation (Sivic and Zisserman, 2003). This approach takes a c</context>
<context position="8198" citStr="Oquab et al. (2014)" startWordPosition="1241" endWordPosition="1244">nto a discrete set of “visual words” using a standard clustering algorithm like k-means and quantized into vector representations by comparing the local descriptors with the cluster centroids. Visual representations are obtained by taking the average of the BOVW vectors for the images that correspond to a given word. We use BOVW as a baseline. Our approach similarly makes use of a collection of images associated with words or tags representing a particular concept. Each image is processed by the first seven layers of the convolutional network defined by Krizhevsky et al. (2012) and adapted by Oquab et al. (2014)1. This network takes 224 × 224 pixel RGB images and applies five successive convolutional layers followed by three fully connected layers. Its eighth and last 1http://www.di.ens.fr/willow/research/cnn/ 37 Multimodal word vector Aggregate 6144-dim feature vectors Training visual features (after Oquab et al., 2014) FC6 FC7 FC8 6144-dim feature vector C1-C2-C3-C4-C5 Convolutional layers Fully-connected layers Imagenet labels African elephant Wall clock ... Select images from ImageNet or ESP Word 100-dim word projections w(t-2) w(t-2) 100-dim word projections w(t) w(t+1) w(t+2) C1-C2-C3-C4-C5 FC6</context>
</contexts>
<marker>Oquab, Bottou, Laptev, Sivic, 2014</marker>
<rawString>M. Oquab, L. Bottou, I. Laptev, and J. Sivic. 2014. Learning and transferring mid-level image representations using convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Razavian</author>
<author>H Azizpour</author>
<author>J Sullivan</author>
<author>S Carlsson</author>
</authors>
<title>CNN features off-the-shelf: an astounding baseline for recognition. arXiv preprint:1403.6382.</title>
<date>2014</date>
<contexts>
<context position="2588" citStr="Razavian et al., 2014" startWordPosition="378" endWordPosition="381">sed as mid-level feature extractors on a variety of computer vision tasks (Oquab et al., 2014; Girshick et al., 2013; Zeiler and Fergus, 2013; Donahue et al., 2014). Although transferring convolutional network features is not a new idea (Driancourt and Bottou, 1990), the simultaneous availability of large datasets and cheap GPU co-processors has contributed to the achievement of considerable performance gains on a variety computer vision benchmarks: “SIFT and HOG descriptors produced big performance gains a decade ago, and now deep convolutional features are providing a similar breakthrough” (Razavian et al., 2014). This work reports on results obtained by using CNN-extracted features in multi-modal semantic representation models. These results are interesting in several respects. First, these superior features provide the opportunity to increase the performance gap achieved by augmenting linguistic features with multi-modal features. Second, this increased performance confirms that the multimodal performance improvement results from the information contained in the images and not the information used to select which images to use to represent a concept. Third, our evaluation reveals an intriguing prope</context>
<context position="6254" citStr="Razavian et al., 2014" startWordPosition="930" endWordPosition="933"> representations directly from the images (i.e., we apply deep learning directly to the images), as opposed to taking a higher-level representation as a starting point. Frome et al. (2013) jointly learn multimodal representations as well, but apply them to a visual object recognition task instead of concept meaning. 2.3 Deep Convolutional Neural Networks A flurry of recent results indicates that image descriptors extracted from deep convolutional neural networks (CNNs) are very powerful and consistently outperform highly tuned state-of-the-art systems on a variety of visual recognition tasks (Razavian et al., 2014). Embeddings from stateof-the-art CNNs (such as Krizhevsky et al. (2012)) have been applied successfully to a number of problems in computer vision (Girshick et al., 2013; Zeiler and Fergus, 2013; Donahue et al., 2014). This contribution follows the approach described by Oquab et al. (2014): they train a CNN on 1512 ImageNet synsets (Deng et al., 2009), use the first seven layers of the trained network as feature extractors on the Pascal VOC dataset, and achieve state-of-the-art performance on the Pascal VOC classification task. 3 Improving Multi-Modal Representations Figure 1 illustrates how </context>
</contexts>
<marker>Razavian, Azizpour, Sullivan, Carlsson, 2014</marker>
<rawString>A.S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson. 2014. CNN features off-the-shelf: an astounding baseline for recognition. arXiv preprint:1403.6382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Roller</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>A multimodal LDA model integrating textual, cognitive and visual modalities.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1146--1157</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<marker>Roller, Walde, 2013</marker>
<rawString>Stephen Roller and Sabine Schulte im Walde. 2013. A multimodal LDA model integrating textual, cognitive and visual modalities. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1146–1157, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sculley</author>
</authors>
<title>Web-scale k-means clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on World wide web,</booktitle>
<pages>1177--1178</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="15165" citStr="Sculley, 2010" startWordPosition="2366" endWordPosition="2368">input is then formed by cropping 16 pixels off each border and subtracting 128 to the image components. The ESP Game images were preprocessed slightly differently because we do not expect the objects to be centered. Each image was rescaled to fit inside a 224 × 224 rectangle. The CNN input is then formed by centering this image into the 224 × 224 input field, subtracting 128 to the image components, and zero padding. The BOVW features were obtained by computing DSIFT descriptors using VLFeat (Vedaldi and Fulkerson, 2008). These descriptors were subsequently clustered using mini-batch k-means (Sculley, 2010) with 100 clusters. Each image is then represented by a bag of clusters (visual words) quantized as a 100-dimensional feature vector. These vectors were then combined into visual concept representations by taking their mean. 4.4 Evaluation We evaluate our multi-modal word representations using two semantic relatedness datasets widely used in distributional semantics (Agirre et al., 2009; Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Clark, 2014; Bruni et al., 2014). WordSim353 (Finkelstein et al., 2001) is a selection of 353 concept pairs with a similarity rating provided by human annot</context>
</contexts>
<marker>Sculley, 2010</marker>
<rawString>D Sculley. 2010. Web-scale k-means clustering. In Proceedings of the 19th international conference on World wide web, pages 1177–1178. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Grounded models of semantic representation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1423--1433</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4410" citStr="Silberer and Lapata, 2012" startWordPosition="642" endWordPosition="645">2014, Doha, Qatar. c�2014 Association for Computational Linguistics mantic space models extract meanings solely from linguistic data, even though we know that human semantic knowledge relies heavily on perceptual information (Louwerse, 2011). That is, there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to do this grounding in the context of distributional semantics is to obtain representations that combine information from linguistic corpora with information from another modality, obtained from e.g. property norming experiments (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013) or from processing and extracting features from images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012). This approach has met with quite some success (Bruni et al., 2014). 2.2 Multi-modal Deep Learning Other examples that apply multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), auto-encoders (Wu et al., 2013) or recursive neural networks (Socher et al., 2014). Multimodal models with deep learning components have also successfully been employed in crossmodal tasks (Lazari</context>
</contexts>
<marker>Silberer, Lapata, 2012</marker>
<rawString>Carina Silberer and Mirella Lapata. 2012. Grounded models of semantic representation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1423–1433. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Learning Grounded Meaning Representations with Autoencoders.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL 2014,</booktitle>
<location>Baltimore, MD.</location>
<contexts>
<context position="5108" citStr="Silberer and Lapata (2014)" startWordPosition="753" endWordPosition="756">eatures from images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012). This approach has met with quite some success (Bruni et al., 2014). 2.2 Multi-modal Deep Learning Other examples that apply multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), auto-encoders (Wu et al., 2013) or recursive neural networks (Socher et al., 2014). Multimodal models with deep learning components have also successfully been employed in crossmodal tasks (Lazaridou et al., 2014). Work that is closely related in spirit to ours is by Silberer and Lapata (2014). They use a stacked auto-encoder to learn combined embeddings of textual and visual input. Their visual inputs consist of vectors of visual attributes obtained from learning SVM classifiers on attribute prediction tasks. In contrast, our work keeps the modalities separate and follows the standard multi-modal approach of concatenating linguistic and visual representations in a single semantic space model. This has the advantage that it allows for separate data sources for the individual modalities. We also learn visual representations directly from the images (i.e., we apply deep learning dire</context>
</contexts>
<marker>Silberer, Lapata, 2014</marker>
<rawString>Carina Silberer and Mirella Lapata. 2014. Learning Grounded Meaning Representations with Autoencoders. In Proceedings of ACL 2014, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sivic</author>
<author>A Zisserman</author>
</authors>
<title>Video Google: a text retrieval approach to object matching in videos.</title>
<date>2003</date>
<booktitle>In Proceedings of the Ninth IEEE International Conference on Computer Vision,</booktitle>
<volume>2</volume>
<pages>1470--1477</pages>
<contexts>
<context position="1551" citStr="Sivic and Zisserman, 2003" startWordPosition="220" endWordPosition="223">riety of tasks, including modeling semantic relatedness and predicting compositionality (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012; Roller and Schulte im Walde, 2013; Kiela et al., 2014). These results were obtained by combining linguistic feature representations with robust visual features extracted from a set of images associated with the concept in question. This extraction of visual features usually follows the popular computer vision approach consisting of computing local features, such as SIFT features (Lowe, 1999), and aggregating them as bags of visual words (Sivic and Zisserman, 2003). Meanwhile, deep transfer learning techniques have gained considerable attention in the computer vision community. First, a deep convolutional neural network (CNN) is trained on a large ∗ This work was carried out while Douwe Kiela was an intern at Microsoft Research, New York. L´eon Bottou Microsoft Research New York leon@bottou.org labeled dataset (Krizhevsky et al., 2012). The convolutional layers are then used as mid-level feature extractors on a variety of computer vision tasks (Oquab et al., 2014; Girshick et al., 2013; Zeiler and Fergus, 2013; Donahue et al., 2014). Although transferri</context>
<context position="7120" citStr="Sivic and Zisserman, 2003" startWordPosition="1059" endWordPosition="1062">llows the approach described by Oquab et al. (2014): they train a CNN on 1512 ImageNet synsets (Deng et al., 2009), use the first seven layers of the trained network as feature extractors on the Pascal VOC dataset, and achieve state-of-the-art performance on the Pascal VOC classification task. 3 Improving Multi-Modal Representations Figure 1 illustrates how our system computes multi-modal semantic representations. 3.1 Perceptual Representations The perceptual component of standard multimodal models that rely on visual data is often an instance of the bag-of-visual-words (BOVW) representation (Sivic and Zisserman, 2003). This approach takes a collection of images associated with words or tags representing the concept in question. For each image, keypoints are laid out as a dense grid. Each keypoint is represented by a vector of robust local visual features such as SIFT (Lowe, 1999), SURF (Bay et al., 2008) and HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). These descriptors are subsequently clustered into a discrete set of “visual words” using a standard clustering algorithm like k-means and quantized into vector representations by comparin</context>
</contexts>
<marker>Sivic, Zisserman, 2003</marker>
<rawString>J. Sivic and A. Zisserman. 2003. Video Google: a text retrieval approach to object matching in videos. In Proceedings of the Ninth IEEE International Conference on Computer Vision, volume 2, pages 1470– 1477, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Andrej Karpathy</author>
<author>Quoc V Le</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Grounded Compositional Semantics for Finding and Describing Images with Sentences. Transactions of the Association for Computational Linguistics (TACL</title>
<date>2014</date>
<contexts>
<context position="4896" citStr="Socher et al., 2014" startWordPosition="718" endWordPosition="721"> linguistic corpora with information from another modality, obtained from e.g. property norming experiments (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013) or from processing and extracting features from images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012). This approach has met with quite some success (Bruni et al., 2014). 2.2 Multi-modal Deep Learning Other examples that apply multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), auto-encoders (Wu et al., 2013) or recursive neural networks (Socher et al., 2014). Multimodal models with deep learning components have also successfully been employed in crossmodal tasks (Lazaridou et al., 2014). Work that is closely related in spirit to ours is by Silberer and Lapata (2014). They use a stacked auto-encoder to learn combined embeddings of textual and visual input. Their visual inputs consist of vectors of visual attributes obtained from learning SVM classifiers on attribute prediction tasks. In contrast, our work keeps the modalities separate and follows the standard multi-modal approach of concatenating linguistic and visual representations in a single s</context>
</contexts>
<marker>Socher, Karpathy, Le, Manning, Ng, 2014</marker>
<rawString>Richard Socher, Andrej Karpathy, Quoc V. Le, Christopher D. Manning, and Andrew Y. Ng. 2014. Grounded Compositional Semantics for Finding and Describing Images with Sentences. Transactions of the Association for Computational Linguistics (TACL 2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Ruslan Salakhutdinov</author>
</authors>
<title>Multimodal learning with deep boltzmann machines.</title>
<date>2012</date>
<booktitle>Advances in Neural Information Processing Systems 25,</booktitle>
<pages>2222--2230</pages>
<editor>In F. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors,</editor>
<contexts>
<context position="4792" citStr="Srivastava and Salakhutdinov, 2012" startWordPosition="699" endWordPosition="703">o this grounding in the context of distributional semantics is to obtain representations that combine information from linguistic corpora with information from another modality, obtained from e.g. property norming experiments (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013) or from processing and extracting features from images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012). This approach has met with quite some success (Bruni et al., 2014). 2.2 Multi-modal Deep Learning Other examples that apply multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), auto-encoders (Wu et al., 2013) or recursive neural networks (Socher et al., 2014). Multimodal models with deep learning components have also successfully been employed in crossmodal tasks (Lazaridou et al., 2014). Work that is closely related in spirit to ours is by Silberer and Lapata (2014). They use a stacked auto-encoder to learn combined embeddings of textual and visual input. Their visual inputs consist of vectors of visual attributes obtained from learning SVM classifiers on attribute prediction tasks. In contrast, our work keeps the modalities separate and follow</context>
</contexts>
<marker>Srivastava, Salakhutdinov, 2012</marker>
<rawString>Nitish Srivastava and Ruslan Salakhutdinov. 2012. Multimodal learning with deep boltzmann machines. In F. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 2222–2230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vedaldi</author>
<author>B Fulkerson</author>
</authors>
<title>VLFeat: An open and portable library of computer vision algorithms.</title>
<date>2008</date>
<note>http://www.vlfeat.org/.</note>
<contexts>
<context position="15077" citStr="Vedaldi and Fulkerson, 2008" startWordPosition="2353" endWordPosition="2356">he largest centered square contained in each image is resam39 pled to form a 256 × 256 image. The CNN input is then formed by cropping 16 pixels off each border and subtracting 128 to the image components. The ESP Game images were preprocessed slightly differently because we do not expect the objects to be centered. Each image was rescaled to fit inside a 224 × 224 rectangle. The CNN input is then formed by centering this image into the 224 × 224 input field, subtracting 128 to the image components, and zero padding. The BOVW features were obtained by computing DSIFT descriptors using VLFeat (Vedaldi and Fulkerson, 2008). These descriptors were subsequently clustered using mini-batch k-means (Sculley, 2010) with 100 clusters. Each image is then represented by a bag of clusters (visual words) quantized as a 100-dimensional feature vector. These vectors were then combined into visual concept representations by taking their mean. 4.4 Evaluation We evaluate our multi-modal word representations using two semantic relatedness datasets widely used in distributional semantics (Agirre et al., 2009; Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Clark, 2014; Bruni et al., 2014). WordSim353 (Finkelstein et al., 20</context>
</contexts>
<marker>Vedaldi, Fulkerson, 2008</marker>
<rawString>A. Vedaldi and B. Fulkerson. 2008. VLFeat: An open and portable library of computer vision algorithms. http://www.vlfeat.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling images with a computer game.</title>
<date>2004</date>
<booktitle>In Proceedings of the SIGCHI conference on Human factors in computing systems,</booktitle>
<pages>319--326</pages>
<publisher>ACM.</publisher>
<marker>Von Ahn, Dabbish, 2004</marker>
<rawString>Luis Von Ahn and Laura Dabbish. 2004. Labeling images with a computer game. In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 319–326. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pengcheng Wu</author>
<author>Steven C H Hoi</author>
<author>Hao Xia</author>
<author>Peilin Zhao</author>
<author>Dayong Wang</author>
<author>Chunyan Miao</author>
</authors>
<title>Online multimodal deep similarity learning with application to image retrieval.</title>
<date>2013</date>
<booktitle>In Proceedings of the 21st ACM International Conference on Multimedia, MM ’13,</booktitle>
<pages>153--162</pages>
<contexts>
<context position="4845" citStr="Wu et al., 2013" startWordPosition="709" endWordPosition="712">n representations that combine information from linguistic corpora with information from another modality, obtained from e.g. property norming experiments (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013) or from processing and extracting features from images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012). This approach has met with quite some success (Bruni et al., 2014). 2.2 Multi-modal Deep Learning Other examples that apply multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), auto-encoders (Wu et al., 2013) or recursive neural networks (Socher et al., 2014). Multimodal models with deep learning components have also successfully been employed in crossmodal tasks (Lazaridou et al., 2014). Work that is closely related in spirit to ours is by Silberer and Lapata (2014). They use a stacked auto-encoder to learn combined embeddings of textual and visual input. Their visual inputs consist of vectors of visual attributes obtained from learning SVM classifiers on attribute prediction tasks. In contrast, our work keeps the modalities separate and follows the standard multi-modal approach of concatenating </context>
</contexts>
<marker>Wu, Hoi, Xia, Zhao, Wang, Miao, 2013</marker>
<rawString>Pengcheng Wu, Steven C.H. Hoi, Hao Xia, Peilin Zhao, Dayong Wang, and Chunyan Miao. 2013. Online multimodal deep similarity learning with application to image retrieval. In Proceedings of the 21st ACM International Conference on Multimedia, MM ’13, pages 153–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew D Zeiler</author>
<author>Rob Fergus</author>
</authors>
<title>Visualizing and understanding convolutional networks.</title>
<date>2013</date>
<publisher>CoRR.</publisher>
<contexts>
<context position="2107" citStr="Zeiler and Fergus, 2013" startWordPosition="308" endWordPosition="311">ggregating them as bags of visual words (Sivic and Zisserman, 2003). Meanwhile, deep transfer learning techniques have gained considerable attention in the computer vision community. First, a deep convolutional neural network (CNN) is trained on a large ∗ This work was carried out while Douwe Kiela was an intern at Microsoft Research, New York. L´eon Bottou Microsoft Research New York leon@bottou.org labeled dataset (Krizhevsky et al., 2012). The convolutional layers are then used as mid-level feature extractors on a variety of computer vision tasks (Oquab et al., 2014; Girshick et al., 2013; Zeiler and Fergus, 2013; Donahue et al., 2014). Although transferring convolutional network features is not a new idea (Driancourt and Bottou, 1990), the simultaneous availability of large datasets and cheap GPU co-processors has contributed to the achievement of considerable performance gains on a variety computer vision benchmarks: “SIFT and HOG descriptors produced big performance gains a decade ago, and now deep convolutional features are providing a similar breakthrough” (Razavian et al., 2014). This work reports on results obtained by using CNN-extracted features in multi-modal semantic representation models. </context>
<context position="6449" citStr="Zeiler and Fergus, 2013" startWordPosition="961" endWordPosition="964">ly learn multimodal representations as well, but apply them to a visual object recognition task instead of concept meaning. 2.3 Deep Convolutional Neural Networks A flurry of recent results indicates that image descriptors extracted from deep convolutional neural networks (CNNs) are very powerful and consistently outperform highly tuned state-of-the-art systems on a variety of visual recognition tasks (Razavian et al., 2014). Embeddings from stateof-the-art CNNs (such as Krizhevsky et al. (2012)) have been applied successfully to a number of problems in computer vision (Girshick et al., 2013; Zeiler and Fergus, 2013; Donahue et al., 2014). This contribution follows the approach described by Oquab et al. (2014): they train a CNN on 1512 ImageNet synsets (Deng et al., 2009), use the first seven layers of the trained network as feature extractors on the Pascal VOC dataset, and achieve state-of-the-art performance on the Pascal VOC classification task. 3 Improving Multi-Modal Representations Figure 1 illustrates how our system computes multi-modal semantic representations. 3.1 Perceptual Representations The perceptual component of standard multimodal models that rely on visual data is often an instance of th</context>
</contexts>
<marker>Zeiler, Fergus, 2013</marker>
<rawString>Matthew D. Zeiler and Rob Fergus. 2013. Visualizing and understanding convolutional networks. CoRR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
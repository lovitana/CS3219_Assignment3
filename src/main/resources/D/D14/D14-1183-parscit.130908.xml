<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000085">
<title confidence="0.998918">
Large-scale Reordering Model for Statistical Machine Translation
using Dual Multinomial Logistic Regression
</title>
<author confidence="0.977277">
Abdullah Alrajehab and Mahesan Niranjanb
</author>
<affiliation confidence="0.987875">
aComputer Research Institute, King Abdulaziz City for Science and Technology (KACST)
Riyadh, Saudi Arabia, asrajeh@kacst.edu.sa
bSchool of Electronics and Computer Science, University of Southampton
</affiliation>
<email confidence="0.504464">
Southampton, United Kingdom, {asar1a10, mn}@ecs.soton.ac.uk
</email>
<sectionHeader confidence="0.991053" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999959470588235">
Phrase reordering is a challenge for statis-
tical machine translation systems. Posing
phrase movements as a prediction prob-
lem using contextual features modeled by
maximum entropy-based classifier is su-
perior to the commonly used lexicalized
reordering model. However, Training
this discriminative model using large-scale
parallel corpus might be computationally
expensive. In this paper, we explore recent
advancements in solving large-scale clas-
sification problems. Using the dual prob-
lem to multinomial logistic regression, we
managed to shrink the training data while
iterating and produce significant saving in
computation and memory while preserv-
ing the accuracy.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955923076923">
Phrase reordering is a common problem when
translating between two grammatically different
languages. Analogous to speech recognition sys-
tems, statistical machine translation (SMT) sys-
tems relied on language models to produce more
fluent output. While early work penalized phrase
movements without considering reorderings aris-
ing from vastly differing grammatical structures
across language pairs like Arabic-English (Koehn,
2004a), many researchers considered lexicalized
reordering models that attempted to learn orienta-
tion based on the training corpus (Tillmann, 2004;
Kumar and Byrne, 2005; Koehn et al., 2005).
Building on this, some researchers have bor-
rowed powerful ideas from the machine learning
literature, to pose the phrase movement problem
as a prediction problem using contextual input fea-
tures whose importance is modeled as weights of
a linear classifier trained by entropic criteria. The
approach (so called maximum entropy classifier
or simply MaxEnt) is a popular choice (Zens and
Ney, 2006; Xiong et al., 2006; Nguyen et al.,
2009; Xiang et al., 2011). Max-margin structure
classifiers were also proposed (Ni et al., 2011).
Alternatively, Cherry (2013) proposed recently us-
ing sparse features optimize the translation quality
with the decoder instead of training a classifier in-
dependently.
While large-scale parallel corpus is advanta-
geous for improving such reordering model, this
improvement comes at a price of computational
complexity. This issue is particularly pronounced
when discriminative models are considered such
as maximum entropy-based model due to the re-
quired iterative learning.
Advancements in solving large-scale classifica-
tion problems have been shown to be effective
such as dual coordinate descent method for linear
support vector machines (Hsieh et al., 2008). Sim-
ilarly, Yu et al. (2011) proposed a two-level dual
coordinate descent method for maximum entropy
classifier.
In this work we explore the dual problem to
multinomial logistic regression for building large-
scale reordering model (section 3). One of the
main advantages of solving the dual problem is
providing a mechanism to shrink the training data
which is a serious issue in building such large-
scale system. We present empirical results com-
paring between the primal and the dual problems
(section 4). Our approach is shown to be fast and
memory-efficient.
</bodyText>
<sectionHeader confidence="0.97228" genericHeader="method">
2 Baseline System
</sectionHeader>
<bodyText confidence="0.999821">
In statistical machine translation, the most likely
translation ebest of an input sentence f can be
found by maximizing the probability p(e|f), as
follows:
</bodyText>
<equation confidence="0.920048666666667">
p(e|f). (1)
ebest = arg max
e
</equation>
<page confidence="0.730936">
1758
</page>
<bodyText confidence="0.955906166666667">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1758–1763,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
A log-linear combination of different models
(features) is used for direct modeling of the poste-
rior probability p(e|f) (Papineni et al., 1998; Och
and Ney, 2002):
</bodyText>
<equation confidence="0.981332">
λihi(f,e) (2)
</equation>
<bodyText confidence="0.999984545454546">
where the feature hi(f, e) is a score function
over sentence pairs. The translation model and the
language model are the main features in any sys-
tem although additional features h(.) can be inte-
grated easily (such as word penalty). State-of-the-
art systems usually have around ten features.
The language model, which ensures fluent
translation, plays an important role in reordering;
however, it has a bias towards short translations
(Koehn, 2010). Therefore, a need for developing a
specific model for the reordering problem.
</bodyText>
<subsectionHeader confidence="0.96514">
2.1 Lexicalized Reordering Model
</subsectionHeader>
<bodyText confidence="0.999967272727273">
Adding a lexicalized reordering model consis-
tently improved the translation quality for sev-
eral language pairs (Koehn et al., 2005). Re-
ordering modeling involves formulating phrase
movements as a classification problem where each
phrase position considered as a class (Tillmann,
2004). Some researchers classified phrase move-
ments into three categories (monotone, swap, and
discontinuous) but the classes can be extended to
any arbitrary number (Koehn and Monz, 2005). In
general, the distribution of phrase orientation is:
</bodyText>
<equation confidence="0.986205">
1
fi, ¯ei) = Z h(¯fi, ¯ei, ok) . (3)
</equation>
<bodyText confidence="0.9985626">
This lexicalized reordering model is estimated
by relative frequency where each phrase pair
(¯fi, ¯ei) with such an orientation (ok) is counted
and then normalized to yield the probability as fol-
lows:
</bodyText>
<equation confidence="0.862984">
count( fi, ¯ei, ok)
Eo count(¯fi, ¯ei, o).
</equation>
<bodyText confidence="0.999917333333333">
The orientation of a current phrase pair is de-
fined with respect to the previous target phrase.
Galley and Manning (2008) extended the model to
tackle long-distance reorderings. Their hierarchi-
cal model enables phrase movements that are more
complex than swaps between adjacent phrases.
</bodyText>
<sectionHeader confidence="0.984773" genericHeader="method">
3 Multinomial Logistic Regression
</sectionHeader>
<bodyText confidence="0.99971925">
Multinomial logistic regression (MLR), also
known as maximum entropy classifier (Zens and
Ney, 2006), is a probabilistic model for the multi-
class problem. The class probability is given by:
</bodyText>
<equation confidence="0.9398895">
p(ok |¯fi, ¯ei) = exp(w&gt; k φ( ¯fi, ¯ei))&gt;fi (5)
Ek, exp(wk,φ(, ei))
</equation>
<bodyText confidence="0.994978769230769">
where φ( fi, ¯ei) is the feature vector of the i-th
phrase pair. An equivalent notation to w&gt;k φ(¯fi, ¯ei)
is w&gt;f(φ( ¯fi, ¯ei), ok) where w is a long vector
composed of all classes parameters (i.e. w&gt; =
[w&gt;1 ... w&gt;K] ) and f(., .) is a joint feature vec-
tor decomposed via the orthogonal feature rep-
resentation (Rousu et al., 2006). This repre-
sentation simply means there is no crosstalk be-
tween two different feature vectors. For example,
f(φ( ¯fi, ¯ei), o1)&gt; = [φ(¯fi, ¯ei)&gt; 0 ... 0].
The model’s parameters can be estimated by
minimizing the following regularized negative
log-likelihood P(w) as follows (Bishop, 2006):
</bodyText>
<equation confidence="0.851614">
˜pik log p(ok |fi, ¯ei)
</equation>
<bodyText confidence="0.941885333333333">
(6)
Here σ is a penalty parameter and p˜ is the em-
pirical distribution where ˜pik equals zero for all
ok =6 oi.
Solving the primal optimization problem (6) us-
ing the gradient:
</bodyText>
<equation confidence="0.944609">
(˜pik − p(ok |¯fi, ¯ei)) φ(¯fi, ¯ei),
(7)
</equation>
<bodyText confidence="0.999558714285714">
do not constitute a closed-form solution. In our
experiments, we used stochastic gradient decent
method (i.e. online learning) to estimate w which
is shown to be fast and effictive for large-scale
problems (Bottou, 2010). The method approxi-
mates (7) by a gradient at a single randomly picked
phrase pair. The update rule is:
</bodyText>
<equation confidence="0.989207230769231">
w0k = wk − ηi ∇kPi(w), (8)
where ηi is a positive learning rate.
ebest = arg max
e
n
i=1
p(ok|
p(ok |fi, ¯ei) =
(4)
1
kwkk2−
min
w
N
i=1
2σ2
K
k=1
K
k=1
∂P(w)
∂wk
= wk
σ2
N
i=1
</equation>
<page confidence="0.938199">
1759
</page>
<bodyText confidence="0.9929225">
Clearly, the cost is affordable because wk(α) is
maintained throughout the algorithm as follows:
</bodyText>
<equation confidence="0.995074">
wk(α&apos;) = wk(α)−σ2(α&apos;ik −αik)φ(¯fi,¯ei) (14)
</equation>
<subsectionHeader confidence="0.970144">
3.1 The Dual Problem
</subsectionHeader>
<bodyText confidence="0.999691">
Lebanon and Lafferty (2002) derived an equiva-
lent dual problem to (6). Introducing Lagrange
multipliers α, the dual becomes
</bodyText>
<equation confidence="0.9599132">
min 1 K kwk(α)k2 + N K αik log αik,
W 2σ2 k=1 i=1 k=1
K
s.t. αik = 1 and αik ≥ 0, ∀i, k, (9)
k=1
</equation>
<bodyText confidence="0.756312">
where
</bodyText>
<equation confidence="0.992543333333333">
N
wk(α) = σ2 (˜pik − αik)φ( fi, ¯ei) (10)
i=1
</equation>
<bodyText confidence="0.9993535">
As mentioned in the introduction, Yu et al.
(2011) proposed a two-level dual coordinate de-
scent method to minimize D(α) in (9) but it has
some numerical difficulties. Collins et al. (2008)
proposed simple exponentiated gradient (EG) al-
gorithm for Conditional Random Feild (CRF). The
algorithm is applicable to our problem, a special
case of CRF. The rule update is:
</bodyText>
<equation confidence="0.766508">
a = αik exp(−ηi ∇ikD(α)) 11)
aik Ek, αik, exp(−ηi ∇ik,D(α))
</equation>
<bodyText confidence="0.825547">
where
</bodyText>
<equation confidence="0.978139714285714">
∂D(α)
∇ikD(α) ≡ ∂α
= 1 + log αik
ik
� �
+ wy(α)Tφ( ¯fi, ¯ei) − wk(α)Tφ(¯fi, ¯ei) .
(12)
</equation>
<bodyText confidence="0.990869">
Here y represents the true class (i.e. oy = oi).
To improve the convergence, ηi is adaptively ad-
justed for each example. If the objective function
(9) did not decrease, ηi is halved for number of tri-
als (Collins et al., 2008). Calculating the function
difference below is the main cost in EG algorithm,
</bodyText>
<equation confidence="0.998429857142857">
�α&apos; �
ik log α&apos; ik − αik log αik
K
− (α&apos;ik − αik)wk(α)Tφ(¯fi, ¯ei)
k=1
&apos; 2
(αik − αik).(13)
</equation>
<bodyText confidence="0.837612">
Following Yu et al. (2011), we initialize αik as
follows:
</bodyText>
<equation confidence="0.99695">
αik 15
(1 − E) if ok = oi; ( )
K−1 = Sl E else.
</equation>
<bodyText confidence="0.997819666666667">
where E is a small positive value. This is because
the objective function (9) is not well defined at
αik = 0 due to the logarithm appearance.
Finally, the optimal dual variables are achieved
when the following condition is satisfied for all ex-
amples (Yu et al., 2011):
</bodyText>
<equation confidence="0.960967">
∇ikD(α) (16)
</equation>
<bodyText confidence="0.947577166666667">
This condition is the key to accelerate EG al-
gorithm. Unlike the primal problem (6), the dual
variables αik are associated with each example
(i.e. phrase pair) therefore a training example can
be disregarded once its optimal dual variables ob-
tained. More data shrinking can be achieved by
tolerating a small difference between the two val-
ues in (16). Algorithm 1 presents the overall pro-
cedure (shrinking step is from line 6 to 9).
Algorithm 1 Shrinking stochastic exponentiated
gradient method for training the dual problem
Require: training set S = {φ(¯fi, ¯ei), oi}Ni=1
</bodyText>
<listItem confidence="0.992929368421053">
1: Given α and the corresponding w(α)
2: repeat
3: Randomly pick i from S
4: Claculate ∇ikD(α) ∀k by (12)
5: vi = maxk ∇ikD(α) − mink ∇ikD(α)
6: if vi ≤ E then
7: Remove i from S
8: Continue from line 3
9: end if
10: η = 0.5
11: for t = 1 to maxTrial do
12: Calculate α&apos;ik ∀k by (11)
13: if D(α&apos;) − D(α) ≤ 0 then
14: Update α and w(α) by (14)
15: Break
16: end if
17: η = 0.5η
18: end for
19: until vi ≤ E ∀i
</listItem>
<equation confidence="0.997305444444444">
K
D(α&apos;) − D(α) =
k=1
σ2 ¯fi, ¯ei)k2 K
+ 2 kφ( k=1
∇ikD(α) = min
k
max
k
</equation>
<page confidence="0.924963">
1760
</page>
<sectionHeader confidence="0.999521" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999090866666667">
We used MultiUN which is a large-scale parallel
corpus extracted from the United Nations website
(Eisele and Chen, 2010). We have used Arabic
and English portion of MultiUN where the English
side is about 300 million words.
We simplify the problem by classifying phrase
movements into three categories (monotone,
swap, discontinuous). To train the reordering
models, we used GIZA++ to produce word align-
ments (Och and Ney, 2000). Then, we used the
extract tool that comes with the Moses toolkit
(Koehn et al., 2007) in order to extract phrase pairs
along with their orientation classes.
As shown in Table 1, each extracted phrase pair
is represented by linguistic features as follows:
</bodyText>
<listItem confidence="0.966493666666667">
• Aligned source and target words in a phrase
pair. Each word alignment is a feature.
• Words within a window around the source
</listItem>
<bodyText confidence="0.8280388">
phrase to capture the context. We choose ad-
jacent words of the phrase boundary.
The extracted phrase pairs after filtering are
47,227,789. The features that occur more than 10
times are 670,154.
</bodyText>
<tableCaption confidence="0.876922">
Table 1: A generic example of the process of
phrase pair extraction and representation.
</tableCaption>
<subsectionHeader confidence="0.976079">
4.1 Classification
</subsectionHeader>
<bodyText confidence="0.999960181818182">
We trained our reordering models by both primal
and dual classifiers for 100 iterations. For the dual
MLR, different shrinking levels have been tried by
varying the parameter (E) in Algorithm 1. Table 2
reports the training time and classification error
rate of these models.
Training the dual MLR with moderate shrinking
level (i.e. c = 0.1) is almost four times faster than
training the primal one. Choosing larger value for
(E) leads to faster training but might harm the per-
formance as shown below.
</bodyText>
<table confidence="0.9504642">
Classifier
Primal MLR
Dual MLR e:0.1
Dual MLR e:1.0
Dual MLR e:0.01
</table>
<tableCaption confidence="0.9370885">
Table 2: Performance of the primal and dual MLR
based on held-out data.
</tableCaption>
<bodyText confidence="0.992068875">
Figure 1 shows the percentage of active set dur-
ing training dual MLR with various shrinking lev-
els. Interestingly, the dual MLR could disregard
more than 99% of the data after a couple of iter-
ations. For very large corpus, the data might not
fit in memory and training primal MLR will take
long time due to severe disk-swapping. In this sit-
uation, using dual MLR is very beneficial.
</bodyText>
<figure confidence="0.669418">
2 4 6 8 10 12 14 16 18 20
Training iteration
</figure>
<figureCaption confidence="0.699980333333333">
Figure 1: Percentage of active set in dual MLR.
As the data size decreases, each iteration takes far
less computation time (see Table 2 for total time).
</figureCaption>
<figure confidence="0.854260347826087">
Sentence pair:
f : f1 f2 1
f3 f4 f5
e2 e3 3
e4 e5
e :
e1 1
.
2
Extracted phrase pairs ( f, ¯e) :
All linguistic features:
1. f1&amp;e1 2. f2&amp;e1 3. f3 4. f3&amp;e5 5. f5&amp;e4
6. f2 7. f6 8. f6&amp;e2 9. f6&amp;e3 10. f5
Bag-of-words representation:
a phrase pair is represented as a vector where each feature
is a discrete number (0=not exist).
φ(¯fi, ¯ei) 1 2 3 4 5 6 7 8 9 10
φ(¯f1, ¯e1) = 1 1 1 0 0 0 0 0 0 0
φ(¯f2, ¯e2) = 0 0 0 1 1 1 1 0 0 0
φ(¯f3, ¯e3) = 0 0 0 0 0 0 1 1 1 1
2
f6
.
3
Percentage of active phrase pairs
100
90
80
50
40
30
20
70
60
10
0
6 = 0.1
6 = 1.0
6 = 0.01
Training Time Error Rate
1 hour 9 mins 17.81%
18 minutes 17.95%
13 minutes
22 minutes
21.13%
17.89%
</figure>
<page confidence="0.961519">
1761
</page>
<subsectionHeader confidence="0.980145">
4.2 Translation
</subsectionHeader>
<bodyText confidence="0.999287705882353">
We used the Moses toolkit (Koehn et al., 2007)
with its default settings to build three phrase-based
translation systems. They differ in how their re-
ordering models were estimated. The language
model is a 5-gram with interpolation and Kneser-
Ney smoothing (Kneser and Ney, 1995). We tuned
the system by using MERT technique (Och, 2003).
As commonly used in statistical machine trans-
lation, we evaluated the translation performance
by BLEU score (Papineni et al., 2002). The test
sets are NIST MT06 and MT08 where the En-
glish sides are 35,481 words (1056 sentences) and
116,840 words (3252 sentences), respectively. Ta-
ble 3 shows the BLEU scores for the translation
systems. We also computed statistical significance
for the models using the paired bootstrap resam-
pling method (Koehn, 2004b).
</bodyText>
<table confidence="0.99993125">
Translation System MT06 MT08
Baseline + Lexical. model 30.86 34.22
Baseline + Primal MLR 31.37* 34.85*
Baseline + Dual MLR c:0.1 31.36* 34.87*
</table>
<tableCaption confidence="0.995425">
Table 3: BLEU scores for Arabic-English transla-
</tableCaption>
<bodyText confidence="0.906565333333333">
tion systems with different reordering models (*:
better than the lexicalized model with at least 95%
statistical significance).
</bodyText>
<sectionHeader confidence="0.99622" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99992852631579">
In training such system with large data sizes and
big dimensionality, computational complexity be-
come a serious issue. In SMT, maximum entropy-
based reordering model is often introduced as a
better alternative to the commonly used lexical-
ized one. However, training this discriminative
model using large-scale corpus might be compu-
tationally expensive due to the iterative learning.
In this paper, we propose training the model
using the dual MLR with shrinking method. It
is almost four times faster than the primal MLR
(also know as MaxEnt) and much more memory-
efficient. For very large corpus, the data might not
fit in memory and training primal MLR will take
long time due to severe disk-swapping. In this sit-
uation, using dual MLR is very beneficial. The
proposed method is also useful for many classi-
fication problems in natural language processing
that require large-scale data.
</bodyText>
<sectionHeader confidence="0.977204" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999189181818182">
Christopher M. Bishop. 2006. Pattern Recognition
and Machine Learning (Information Science and
Statistics). Springer-Verlag New York, Inc., Secau-
cus, NJ, USA.
L´eon Bottou. 2010. Large-scale machine learning
with stochastic gradient descent. In Yves Lecheval-
lier and Gilbert Saporta, editors, Proceedings of
the 19th International Conference on Computa-
tional Statistics (COMPSTAT’2010), pages 177–
187, Paris, France, August. Springer.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 22–
31, Atlanta, Georgia, June. Association for Compu-
tational Linguistics.
Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras, and Peter L. Bartlett. 2008. Exponen-
tiated gradient algorithms for conditional random
fields and max-margin markov networks. Journal
of Machine Learning Research, 9:1775–1822, June.
Andreas Eisele and Yu Chen. 2010. Multiun:
A multilingual corpus from united nation docu-
ments. In Daniel Tapias, Mike Rosner, Ste-
lios Piperidis, Jan Odjik, Joseph Mariani, Bente
Maegaard, Khalid Choukri, and Nicoletta Calzo-
lari (Conference Chair), editors, Proceedings of the
Seventh conference on International Language Re-
sources and Evaluation, pages 2868–2872. Euro-
pean Language Resources Association (ELRA), 5.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848–856, Hawaii, October. Association
for Computational Linguistics.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin,
S. Sathiya Keerthi, and S. Sundararajan. 2008. A
dual coordinate descent method for large-scale lin-
ear svm. In Proceedings of the 25th International
Conference on Machine Learning, ICML ’08, pages
408–415.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. IEEE
International Conference on Acoustics, Speech and
Signal Processing, pages 181–184.
Philipp Koehn and Christof Monz. 2005. Shared
task: Statistical machine translation between euro-
pean languages. In Proceedings of ACL Workshop
on Building and Using Parallel Texts, pages 119–
124. Association for Computational Linguistics.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
</reference>
<page confidence="0.947106">
1762
</page>
<reference confidence="0.997757048076923">
for the 2005 IWSLT speech translation evaluation.
In Proceedings of International Workshop on Spo-
ken Language Translation, Pittsburgh, PA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Christopher J. Dyer, Ondˇrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the ACL 2007 Demo
and Poster Sessions, pages 177–180.
Philipp Koehn. 2004a. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of 6th Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA), pages 115—124, Washington DC.
Philipp Koehn. 2004b. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Shankar Kumar and William Byrne. 2005. Lo-
cal phrase reordering models for statistical machine
translation. In Proceedings of Human Language
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pages
161–168, Vancouver, British Columbia, Canada,
October. Association for Computational Linguistics.
Guy Lebanon and John D. Lafferty. 2002. Boosting
and maximum likelihood for exponential models. In
T.G. Dietterich, S. Becker, and Z. Ghahramani, ed-
itors, Advances in Neural Information Processing
Systems 14, pages 447–454. MIT Press.
Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen,
and Thai Phuong Nguyen. 2009. Improving a lex-
icalized hierarchical reordering model using maxi-
mum entropy. In Proceedings of the Twelfth Ma-
chine Translation Summit (MT Summit XII). Inter-
national Association for Machine Translation.
Yizhao Ni, Craig Saunders, Sandor Szedmak, and Ma-
hesan Niranjan. 2011. Exploitation of machine
learning techniques in modelling phrase movements
for machine translation. Journal of Machine Learn-
ing Research, 12:1–30, February.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association of Compu-
tational Linguistics (ACL).
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 160–
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kishore Papineni, Salim Roukos, and Todd Ward.
1998. Maximum likelihood and discriminative
training of direct translation models. In Proceedings
of ICASSP, pages 189–192.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311–318, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Juho Rousu, Craig Saunders, Sandor Szedmak, and
John Shawe-Taylor. 2006. Kernel-based learning of
hierarchical multilabel classification models. Jour-
nal of Machine Learning Research, pages 1601–
1626.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL: Short Papers, pages 101–
104.
Bing Xiang, Niyu Ge, and Abraham Ittycheriah. 2011.
Improving reordering for statistical machine transla-
tion with smoothed priors and syntactic features. In
Proceedings of SSST-5, Fifth Workshop on Syntax,
Semantics and Structure in Statistical Translation,
pages 61–69, Portland, Oregon, USA. Association
for Computational Linguistics.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for
statistical machine translation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the ACL,
pages 521–528, Sydney, July. Association for Com-
putational Linguistics.
Hsiang-Fu Yu, Fang-Lan Huang, and Chih-Jen Lin.
2011. Dual coordinate descent methods for logistic
regression and maximum entropy models. Machine
Learning, 85(1-2):41–75, October.
Richard Zens and Hermann Ney. 2006. Discrimina-
tive reordering models for statistical machine trans-
lation. In Proceedings on the Workshop on Statis-
tical Machine Translation, pages 55–63, New York
City, June. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.916573">
1763
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.468297">
<title confidence="0.992519">Large-scale Reordering Model for Statistical Machine using Dual Multinomial Logistic Regression</title>
<author confidence="0.959001">Mahesan</author>
<affiliation confidence="0.83679275">Research Institute, King Abdulaziz City for Science and Technology Riyadh, Saudi Arabia, of Electronics and Computer Science, University of United Kingdom,</affiliation>
<abstract confidence="0.999052166666667">Phrase reordering is a challenge for statistical machine translation systems. Posing phrase movements as a prediction problem using contextual features modeled by maximum entropy-based classifier is superior to the commonly used lexicalized reordering model. However, Training this discriminative model using large-scale parallel corpus might be computationally expensive. In this paper, we explore recent advancements in solving large-scale classification problems. Using the dual problem to multinomial logistic regression, we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>proposed method is also useful for many classification problems in natural language processing that require large-scale data.</title>
<marker></marker>
<rawString>proposed method is also useful for many classification problems in natural language processing that require large-scale data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Pattern Recognition and Machine Learning (Information Science and Statistics).</title>
<date>2006</date>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc., Secaucus, NJ, USA.</location>
<contexts>
<context position="6613" citStr="Bishop, 2006" startWordPosition="993" endWordPosition="994">( fi, ¯ei) is the feature vector of the i-th phrase pair. An equivalent notation to w&gt;k φ(¯fi, ¯ei) is w&gt;f(φ( ¯fi, ¯ei), ok) where w is a long vector composed of all classes parameters (i.e. w&gt; = [w&gt;1 ... w&gt;K] ) and f(., .) is a joint feature vector decomposed via the orthogonal feature representation (Rousu et al., 2006). This representation simply means there is no crosstalk between two different feature vectors. For example, f(φ( ¯fi, ¯ei), o1)&gt; = [φ(¯fi, ¯ei)&gt; 0 ... 0]. The model’s parameters can be estimated by minimizing the following regularized negative log-likelihood P(w) as follows (Bishop, 2006): ˜pik log p(ok |fi, ¯ei) (6) Here σ is a penalty parameter and p˜ is the empirical distribution where ˜pik equals zero for all ok =6 oi. Solving the primal optimization problem (6) using the gradient: (˜pik − p(ok |¯fi, ¯ei)) φ(¯fi, ¯ei), (7) do not constitute a closed-form solution. In our experiments, we used stochastic gradient decent method (i.e. online learning) to estimate w which is shown to be fast and effictive for large-scale problems (Bottou, 2010). The method approximates (7) by a gradient at a single randomly picked phrase pair. The update rule is: w0k = wk − ηi ∇kPi(w), (8) wher</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L´eon Bottou</author>
</authors>
<title>Large-scale machine learning with stochastic gradient descent.</title>
<date>2010</date>
<booktitle>In Yves Lechevallier and Gilbert Saporta, editors, Proceedings of the 19th International Conference on Computational Statistics (COMPSTAT’2010),</booktitle>
<pages>177--187</pages>
<publisher>August. Springer.</publisher>
<location>Paris, France,</location>
<contexts>
<context position="7077" citStr="Bottou, 2010" startWordPosition="1073" endWordPosition="1074">ei)&gt; 0 ... 0]. The model’s parameters can be estimated by minimizing the following regularized negative log-likelihood P(w) as follows (Bishop, 2006): ˜pik log p(ok |fi, ¯ei) (6) Here σ is a penalty parameter and p˜ is the empirical distribution where ˜pik equals zero for all ok =6 oi. Solving the primal optimization problem (6) using the gradient: (˜pik − p(ok |¯fi, ¯ei)) φ(¯fi, ¯ei), (7) do not constitute a closed-form solution. In our experiments, we used stochastic gradient decent method (i.e. online learning) to estimate w which is shown to be fast and effictive for large-scale problems (Bottou, 2010). The method approximates (7) by a gradient at a single randomly picked phrase pair. The update rule is: w0k = wk − ηi ∇kPi(w), (8) where ηi is a positive learning rate. ebest = arg max e n i=1 p(ok| p(ok |fi, ¯ei) = (4) 1 kwkk2− min w N i=1 2σ2 K k=1 K k=1 ∂P(w) ∂wk = wk σ2 N i=1 1759 Clearly, the cost is affordable because wk(α) is maintained throughout the algorithm as follows: wk(α&apos;) = wk(α)−σ2(α&apos;ik −αik)φ(¯fi,¯ei) (14) 3.1 The Dual Problem Lebanon and Lafferty (2002) derived an equivalent dual problem to (6). Introducing Lagrange multipliers α, the dual becomes min 1 K kwk(α)k2 + N K αik </context>
</contexts>
<marker>Bottou, 2010</marker>
<rawString>L´eon Bottou. 2010. Large-scale machine learning with stochastic gradient descent. In Yves Lechevallier and Gilbert Saporta, editors, Proceedings of the 19th International Conference on Computational Statistics (COMPSTAT’2010), pages 177– 187, Paris, France, August. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
</authors>
<title>Improved reordering for phrasebased translation using sparse features.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>22--31</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="2275" citStr="Cherry (2013)" startWordPosition="312" endWordPosition="313">mann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). Building on this, some researchers have borrowed powerful ideas from the machine learning literature, to pose the phrase movement problem as a prediction problem using contextual input features whose importance is modeled as weights of a linear classifier trained by entropic criteria. The approach (so called maximum entropy classifier or simply MaxEnt) is a popular choice (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011). Max-margin structure classifiers were also proposed (Ni et al., 2011). Alternatively, Cherry (2013) proposed recently using sparse features optimize the translation quality with the decoder instead of training a classifier independently. While large-scale parallel corpus is advantageous for improving such reordering model, this improvement comes at a price of computational complexity. This issue is particularly pronounced when discriminative models are considered such as maximum entropy-based model due to the required iterative learning. Advancements in solving large-scale classification problems have been shown to be effective such as dual coordinate descent method for linear support vecto</context>
</contexts>
<marker>Cherry, 2013</marker>
<rawString>Colin Cherry. 2013. Improved reordering for phrasebased translation using sparse features. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 22– 31, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Amir Globerson</author>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Peter L Bartlett</author>
</authors>
<title>Exponentiated gradient algorithms for conditional random fields and max-margin markov networks.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1775</pages>
<contexts>
<context position="7987" citStr="Collins et al. (2008)" startWordPosition="1250" endWordPosition="1253">early, the cost is affordable because wk(α) is maintained throughout the algorithm as follows: wk(α&apos;) = wk(α)−σ2(α&apos;ik −αik)φ(¯fi,¯ei) (14) 3.1 The Dual Problem Lebanon and Lafferty (2002) derived an equivalent dual problem to (6). Introducing Lagrange multipliers α, the dual becomes min 1 K kwk(α)k2 + N K αik log αik, W 2σ2 k=1 i=1 k=1 K s.t. αik = 1 and αik ≥ 0, ∀i, k, (9) k=1 where N wk(α) = σ2 (˜pik − αik)φ( fi, ¯ei) (10) i=1 As mentioned in the introduction, Yu et al. (2011) proposed a two-level dual coordinate descent method to minimize D(α) in (9) but it has some numerical difficulties. Collins et al. (2008) proposed simple exponentiated gradient (EG) algorithm for Conditional Random Feild (CRF). The algorithm is applicable to our problem, a special case of CRF. The rule update is: a = αik exp(−ηi ∇ikD(α)) 11) aik Ek, αik, exp(−ηi ∇ik,D(α)) where ∂D(α) ∇ikD(α) ≡ ∂α = 1 + log αik ik � � + wy(α)Tφ( ¯fi, ¯ei) − wk(α)Tφ(¯fi, ¯ei) . (12) Here y represents the true class (i.e. oy = oi). To improve the convergence, ηi is adaptively adjusted for each example. If the objective function (9) did not decrease, ηi is halved for number of trials (Collins et al., 2008). Calculating the function difference below</context>
</contexts>
<marker>Collins, Globerson, Koo, Carreras, Bartlett, 2008</marker>
<rawString>Michael Collins, Amir Globerson, Terry Koo, Xavier Carreras, and Peter L. Bartlett. 2008. Exponentiated gradient algorithms for conditional random fields and max-margin markov networks. Journal of Machine Learning Research, 9:1775–1822, June.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Andreas Eisele</author>
<author>Yu Chen</author>
</authors>
<title>Multiun: A multilingual corpus from united nation documents.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh conference on International Language Resources and Evaluation,</booktitle>
<pages>2868--2872</pages>
<editor>In Daniel Tapias, Mike Rosner, Stelios Piperidis, Jan Odjik, Joseph Mariani, Bente Maegaard, Khalid Choukri, and Nicoletta Calzolari (Conference Chair), editors,</editor>
<contexts>
<context position="10295" citStr="Eisele and Chen, 2010" startWordPosition="1698" endWordPosition="1701">1 1: Given α and the corresponding w(α) 2: repeat 3: Randomly pick i from S 4: Claculate ∇ikD(α) ∀k by (12) 5: vi = maxk ∇ikD(α) − mink ∇ikD(α) 6: if vi ≤ E then 7: Remove i from S 8: Continue from line 3 9: end if 10: η = 0.5 11: for t = 1 to maxTrial do 12: Calculate α&apos;ik ∀k by (11) 13: if D(α&apos;) − D(α) ≤ 0 then 14: Update α and w(α) by (14) 15: Break 16: end if 17: η = 0.5η 18: end for 19: until vi ≤ E ∀i K D(α&apos;) − D(α) = k=1 σ2 ¯fi, ¯ei)k2 K + 2 kφ( k=1 ∇ikD(α) = min k max k 1760 4 Experiments We used MultiUN which is a large-scale parallel corpus extracted from the United Nations website (Eisele and Chen, 2010). We have used Arabic and English portion of MultiUN where the English side is about 300 million words. We simplify the problem by classifying phrase movements into three categories (monotone, swap, discontinuous). To train the reordering models, we used GIZA++ to produce word alignments (Och and Ney, 2000). Then, we used the extract tool that comes with the Moses toolkit (Koehn et al., 2007) in order to extract phrase pairs along with their orientation classes. As shown in Table 1, each extracted phrase pair is represented by linguistic features as follows: • Aligned source and target words i</context>
</contexts>
<marker>Eisele, Chen, 2010</marker>
<rawString>Andreas Eisele and Yu Chen. 2010. Multiun: A multilingual corpus from united nation documents. In Daniel Tapias, Mike Rosner, Stelios Piperidis, Jan Odjik, Joseph Mariani, Bente Maegaard, Khalid Choukri, and Nicoletta Calzolari (Conference Chair), editors, Proceedings of the Seventh conference on International Language Resources and Evaluation, pages 2868–2872. European Language Resources Association (ELRA), 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>848--856</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Hawaii,</location>
<contexts>
<context position="5535" citStr="Galley and Manning (2008)" startWordPosition="814" endWordPosition="817">s classified phrase movements into three categories (monotone, swap, and discontinuous) but the classes can be extended to any arbitrary number (Koehn and Monz, 2005). In general, the distribution of phrase orientation is: 1 fi, ¯ei) = Z h(¯fi, ¯ei, ok) . (3) This lexicalized reordering model is estimated by relative frequency where each phrase pair (¯fi, ¯ei) with such an orientation (ok) is counted and then normalized to yield the probability as follows: count( fi, ¯ei, ok) Eo count(¯fi, ¯ei, o). The orientation of a current phrase pair is defined with respect to the previous target phrase. Galley and Manning (2008) extended the model to tackle long-distance reorderings. Their hierarchical model enables phrase movements that are more complex than swaps between adjacent phrases. 3 Multinomial Logistic Regression Multinomial logistic regression (MLR), also known as maximum entropy classifier (Zens and Ney, 2006), is a probabilistic model for the multiclass problem. The class probability is given by: p(ok |¯fi, ¯ei) = exp(w&gt; k φ( ¯fi, ¯ei))&gt;fi (5) Ek, exp(wk,φ(, ei)) where φ( fi, ¯ei) is the feature vector of the i-th phrase pair. An equivalent notation to w&gt;k φ(¯fi, ¯ei) is w&gt;f(φ( ¯fi, ¯ei), ok) where w is</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 848–856, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cho-Jui Hsieh</author>
<author>Kai-Wei Chang</author>
<author>Chih-Jen Lin</author>
<author>S Sathiya Keerthi</author>
<author>S Sundararajan</author>
</authors>
<title>A dual coordinate descent method for large-scale linear svm.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning, ICML ’08,</booktitle>
<pages>408--415</pages>
<contexts>
<context position="2906" citStr="Hsieh et al., 2008" startWordPosition="401" endWordPosition="404">cently using sparse features optimize the translation quality with the decoder instead of training a classifier independently. While large-scale parallel corpus is advantageous for improving such reordering model, this improvement comes at a price of computational complexity. This issue is particularly pronounced when discriminative models are considered such as maximum entropy-based model due to the required iterative learning. Advancements in solving large-scale classification problems have been shown to be effective such as dual coordinate descent method for linear support vector machines (Hsieh et al., 2008). Similarly, Yu et al. (2011) proposed a two-level dual coordinate descent method for maximum entropy classifier. In this work we explore the dual problem to multinomial logistic regression for building largescale reordering model (section 3). One of the main advantages of solving the dual problem is providing a mechanism to shrink the training data which is a serious issue in building such largescale system. We present empirical results comparing between the primal and the dual problems (section 4). Our approach is shown to be fast and memory-efficient. 2 Baseline System In statistical machin</context>
</contexts>
<marker>Hsieh, Chang, Lin, Keerthi, Sundararajan, 2008</marker>
<rawString>Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya Keerthi, and S. Sundararajan. 2008. A dual coordinate descent method for large-scale linear svm. In Proceedings of the 25th International Conference on Machine Learning, ICML ’08, pages 408–415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="13474" citStr="Kneser and Ney, 1995" startWordPosition="2296" endWordPosition="2299"> 2 3 4 5 6 7 8 9 10 φ(¯f1, ¯e1) = 1 1 1 0 0 0 0 0 0 0 φ(¯f2, ¯e2) = 0 0 0 1 1 1 1 0 0 0 φ(¯f3, ¯e3) = 0 0 0 0 0 0 1 1 1 1 2 f6 . 3 Percentage of active phrase pairs 100 90 80 50 40 30 20 70 60 10 0 6 = 0.1 6 = 1.0 6 = 0.01 Training Time Error Rate 1 hour 9 mins 17.81% 18 minutes 17.95% 13 minutes 22 minutes 21.13% 17.89% 1761 4.2 Translation We used the Moses toolkit (Koehn et al., 2007) with its default settings to build three phrase-based translation systems. They differ in how their reordering models were estimated. The language model is a 5-gram with interpolation and KneserNey smoothing (Kneser and Ney, 1995). We tuned the system by using MERT technique (Och, 2003). As commonly used in statistical machine translation, we evaluated the translation performance by BLEU score (Papineni et al., 2002). The test sets are NIST MT06 and MT08 where the English sides are 35,481 words (1056 sentences) and 116,840 words (3252 sentences), respectively. Table 3 shows the BLEU scores for the translation systems. We also computed statistical significance for the models using the paired bootstrap resampling method (Koehn, 2004b). Translation System MT06 MT08 Baseline + Lexical. model 30.86 34.22 Baseline + Primal M</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. IEEE International Conference on Acoustics, Speech and Signal Processing, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
</authors>
<title>Shared task: Statistical machine translation between european languages.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>119</pages>
<contexts>
<context position="5076" citStr="Koehn and Monz, 2005" startWordPosition="735" endWordPosition="738"> towards short translations (Koehn, 2010). Therefore, a need for developing a specific model for the reordering problem. 2.1 Lexicalized Reordering Model Adding a lexicalized reordering model consistently improved the translation quality for several language pairs (Koehn et al., 2005). Reordering modeling involves formulating phrase movements as a classification problem where each phrase position considered as a class (Tillmann, 2004). Some researchers classified phrase movements into three categories (monotone, swap, and discontinuous) but the classes can be extended to any arbitrary number (Koehn and Monz, 2005). In general, the distribution of phrase orientation is: 1 fi, ¯ei) = Z h(¯fi, ¯ei, ok) . (3) This lexicalized reordering model is estimated by relative frequency where each phrase pair (¯fi, ¯ei) with such an orientation (ok) is counted and then normalized to yield the probability as follows: count( fi, ¯ei, ok) Eo count(¯fi, ¯ei, o). The orientation of a current phrase pair is defined with respect to the previous target phrase. Galley and Manning (2008) extended the model to tackle long-distance reorderings. Their hierarchical model enables phrase movements that are more complex than swaps b</context>
</contexts>
<marker>Koehn, Monz, 2005</marker>
<rawString>Philipp Koehn and Christof Monz. 2005. Shared task: Statistical machine translation between european languages. In Proceedings of ACL Workshop on Building and Using Parallel Texts, pages 119–</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of International Workshop on Spoken Language Translation,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1716" citStr="Koehn et al., 2005" startWordPosition="223" endWordPosition="226">ion Phrase reordering is a common problem when translating between two grammatically different languages. Analogous to speech recognition systems, statistical machine translation (SMT) systems relied on language models to produce more fluent output. While early work penalized phrase movements without considering reorderings arising from vastly differing grammatical structures across language pairs like Arabic-English (Koehn, 2004a), many researchers considered lexicalized reordering models that attempted to learn orientation based on the training corpus (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). Building on this, some researchers have borrowed powerful ideas from the machine learning literature, to pose the phrase movement problem as a prediction problem using contextual input features whose importance is modeled as weights of a linear classifier trained by entropic criteria. The approach (so called maximum entropy classifier or simply MaxEnt) is a popular choice (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011). Max-margin structure classifiers were also proposed (Ni et al., 2011). Alternatively, Cherry (2013) proposed recently using sparse features </context>
<context position="4740" citStr="Koehn et al., 2005" startWordPosition="687" endWordPosition="690"> translation model and the language model are the main features in any system although additional features h(.) can be integrated easily (such as word penalty). State-of-theart systems usually have around ten features. The language model, which ensures fluent translation, plays an important role in reordering; however, it has a bias towards short translations (Koehn, 2010). Therefore, a need for developing a specific model for the reordering problem. 2.1 Lexicalized Reordering Model Adding a lexicalized reordering model consistently improved the translation quality for several language pairs (Koehn et al., 2005). Reordering modeling involves formulating phrase movements as a classification problem where each phrase position considered as a class (Tillmann, 2004). Some researchers classified phrase movements into three categories (monotone, swap, and discontinuous) but the classes can be extended to any arbitrary number (Koehn and Monz, 2005). In general, the distribution of phrase orientation is: 1 fi, ¯ei) = Z h(¯fi, ¯ei, ok) . (3) This lexicalized reordering model is estimated by relative frequency where each phrase pair (¯fi, ¯ei) with such an orientation (ok) is counted and then normalized to yie</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proceedings of International Workshop on Spoken Language Translation, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Christopher J Dyer</author>
</authors>
<title>Ondˇrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="10690" citStr="Koehn et al., 2007" startWordPosition="1763" endWordPosition="1766">il vi ≤ E ∀i K D(α&apos;) − D(α) = k=1 σ2 ¯fi, ¯ei)k2 K + 2 kφ( k=1 ∇ikD(α) = min k max k 1760 4 Experiments We used MultiUN which is a large-scale parallel corpus extracted from the United Nations website (Eisele and Chen, 2010). We have used Arabic and English portion of MultiUN where the English side is about 300 million words. We simplify the problem by classifying phrase movements into three categories (monotone, swap, discontinuous). To train the reordering models, we used GIZA++ to produce word alignments (Och and Ney, 2000). Then, we used the extract tool that comes with the Moses toolkit (Koehn et al., 2007) in order to extract phrase pairs along with their orientation classes. As shown in Table 1, each extracted phrase pair is represented by linguistic features as follows: • Aligned source and target words in a phrase pair. Each word alignment is a feature. • Words within a window around the source phrase to capture the context. We choose adjacent words of the phrase boundary. The extracted phrase pairs after filtering are 47,227,789. The features that occur more than 10 times are 670,154. Table 1: A generic example of the process of phrase pair extraction and representation. 4.1 Classification </context>
<context position="13243" citStr="Koehn et al., 2007" startWordPosition="2260" endWordPosition="2263">c features: 1. f1&amp;e1 2. f2&amp;e1 3. f3 4. f3&amp;e5 5. f5&amp;e4 6. f2 7. f6 8. f6&amp;e2 9. f6&amp;e3 10. f5 Bag-of-words representation: a phrase pair is represented as a vector where each feature is a discrete number (0=not exist). φ(¯fi, ¯ei) 1 2 3 4 5 6 7 8 9 10 φ(¯f1, ¯e1) = 1 1 1 0 0 0 0 0 0 0 φ(¯f2, ¯e2) = 0 0 0 1 1 1 1 0 0 0 φ(¯f3, ¯e3) = 0 0 0 0 0 0 1 1 1 1 2 f6 . 3 Percentage of active phrase pairs 100 90 80 50 40 30 20 70 60 10 0 6 = 0.1 6 = 1.0 6 = 0.01 Training Time Error Rate 1 hour 9 mins 17.81% 18 minutes 17.95% 13 minutes 22 minutes 21.13% 17.89% 1761 4.2 Translation We used the Moses toolkit (Koehn et al., 2007) with its default settings to build three phrase-based translation systems. They differ in how their reordering models were estimated. The language model is a 5-gram with interpolation and KneserNey smoothing (Kneser and Ney, 1995). We tuned the system by using MERT technique (Och, 2003). As commonly used in statistical machine translation, we evaluated the translation performance by BLEU score (Papineni et al., 2002). The test sets are NIST MT06 and MT08 where the English sides are 35,481 words (1056 sentences) and 116,840 words (3252 sentences), respectively. Table 3 shows the BLEU scores fo</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Christopher J. Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings of 6th Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<pages>115--124</pages>
<location>Washington DC.</location>
<contexts>
<context position="1530" citStr="Koehn, 2004" startWordPosition="198" endWordPosition="199">al logistic regression, we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy. 1 Introduction Phrase reordering is a common problem when translating between two grammatically different languages. Analogous to speech recognition systems, statistical machine translation (SMT) systems relied on language models to produce more fluent output. While early work penalized phrase movements without considering reorderings arising from vastly differing grammatical structures across language pairs like Arabic-English (Koehn, 2004a), many researchers considered lexicalized reordering models that attempted to learn orientation based on the training corpus (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). Building on this, some researchers have borrowed powerful ideas from the machine learning literature, to pose the phrase movement problem as a prediction problem using contextual input features whose importance is modeled as weights of a linear classifier trained by entropic criteria. The approach (so called maximum entropy classifier or simply MaxEnt) is a popular choice (Zens and Ney, 2006; Xiong et al., 20</context>
<context position="13984" citStr="Koehn, 2004" startWordPosition="2380" endWordPosition="2381">ted. The language model is a 5-gram with interpolation and KneserNey smoothing (Kneser and Ney, 1995). We tuned the system by using MERT technique (Och, 2003). As commonly used in statistical machine translation, we evaluated the translation performance by BLEU score (Papineni et al., 2002). The test sets are NIST MT06 and MT08 where the English sides are 35,481 words (1056 sentences) and 116,840 words (3252 sentences), respectively. Table 3 shows the BLEU scores for the translation systems. We also computed statistical significance for the models using the paired bootstrap resampling method (Koehn, 2004b). Translation System MT06 MT08 Baseline + Lexical. model 30.86 34.22 Baseline + Primal MLR 31.37* 34.85* Baseline + Dual MLR c:0.1 31.36* 34.87* Table 3: BLEU scores for Arabic-English translation systems with different reordering models (*: better than the lexicalized model with at least 95% statistical significance). 5 Conclusion In training such system with large data sizes and big dimensionality, computational complexity become a serious issue. In SMT, maximum entropybased reordering model is often introduced as a better alternative to the commonly used lexicalized one. However, training</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004a. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proceedings of 6th Conference of the Association for Machine Translation in the Americas (AMTA), pages 115—124, Washington DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1530" citStr="Koehn, 2004" startWordPosition="198" endWordPosition="199">al logistic regression, we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy. 1 Introduction Phrase reordering is a common problem when translating between two grammatically different languages. Analogous to speech recognition systems, statistical machine translation (SMT) systems relied on language models to produce more fluent output. While early work penalized phrase movements without considering reorderings arising from vastly differing grammatical structures across language pairs like Arabic-English (Koehn, 2004a), many researchers considered lexicalized reordering models that attempted to learn orientation based on the training corpus (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). Building on this, some researchers have borrowed powerful ideas from the machine learning literature, to pose the phrase movement problem as a prediction problem using contextual input features whose importance is modeled as weights of a linear classifier trained by entropic criteria. The approach (so called maximum entropy classifier or simply MaxEnt) is a popular choice (Zens and Ney, 2006; Xiong et al., 20</context>
<context position="13984" citStr="Koehn, 2004" startWordPosition="2380" endWordPosition="2381">ted. The language model is a 5-gram with interpolation and KneserNey smoothing (Kneser and Ney, 1995). We tuned the system by using MERT technique (Och, 2003). As commonly used in statistical machine translation, we evaluated the translation performance by BLEU score (Papineni et al., 2002). The test sets are NIST MT06 and MT08 where the English sides are 35,481 words (1056 sentences) and 116,840 words (3252 sentences), respectively. Table 3 shows the BLEU scores for the translation systems. We also computed statistical significance for the models using the paired bootstrap resampling method (Koehn, 2004b). Translation System MT06 MT08 Baseline + Lexical. model 30.86 34.22 Baseline + Primal MLR 31.37* 34.85* Baseline + Dual MLR c:0.1 31.36* 34.87* Table 3: BLEU scores for Arabic-English translation systems with different reordering models (*: better than the lexicalized model with at least 95% statistical significance). 5 Conclusion In training such system with large data sizes and big dimensionality, computational complexity become a serious issue. In SMT, maximum entropybased reordering model is often introduced as a better alternative to the commonly used lexicalized one. However, training</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004b. Statistical significance tests for machine translation evaluation. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="4496" citStr="Koehn, 2010" startWordPosition="653" endWordPosition="654"> combination of different models (features) is used for direct modeling of the posterior probability p(e|f) (Papineni et al., 1998; Och and Ney, 2002): λihi(f,e) (2) where the feature hi(f, e) is a score function over sentence pairs. The translation model and the language model are the main features in any system although additional features h(.) can be integrated easily (such as word penalty). State-of-theart systems usually have around ten features. The language model, which ensures fluent translation, plays an important role in reordering; however, it has a bias towards short translations (Koehn, 2010). Therefore, a need for developing a specific model for the reordering problem. 2.1 Lexicalized Reordering Model Adding a lexicalized reordering model consistently improved the translation quality for several language pairs (Koehn et al., 2005). Reordering modeling involves formulating phrase movements as a classification problem where each phrase position considered as a class (Tillmann, 2004). Some researchers classified phrase movements into three categories (monotone, swap, and discontinuous) but the classes can be extended to any arbitrary number (Koehn and Monz, 2005). In general, the di</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010. Statistical Machine Translation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Local phrase reordering models for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>161--168</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="1695" citStr="Kumar and Byrne, 2005" startWordPosition="219" endWordPosition="222">e accuracy. 1 Introduction Phrase reordering is a common problem when translating between two grammatically different languages. Analogous to speech recognition systems, statistical machine translation (SMT) systems relied on language models to produce more fluent output. While early work penalized phrase movements without considering reorderings arising from vastly differing grammatical structures across language pairs like Arabic-English (Koehn, 2004a), many researchers considered lexicalized reordering models that attempted to learn orientation based on the training corpus (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). Building on this, some researchers have borrowed powerful ideas from the machine learning literature, to pose the phrase movement problem as a prediction problem using contextual input features whose importance is modeled as weights of a linear classifier trained by entropic criteria. The approach (so called maximum entropy classifier or simply MaxEnt) is a popular choice (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011). Max-margin structure classifiers were also proposed (Ni et al., 2011). Alternatively, Cherry (2013) proposed recently u</context>
</contexts>
<marker>Kumar, Byrne, 2005</marker>
<rawString>Shankar Kumar and William Byrne. 2005. Local phrase reordering models for statistical machine translation. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 161–168, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guy Lebanon</author>
<author>John D Lafferty</author>
</authors>
<title>Boosting and maximum likelihood for exponential models.</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems 14,</booktitle>
<pages>447--454</pages>
<editor>In T.G. Dietterich, S. Becker, and Z. Ghahramani, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7553" citStr="Lebanon and Lafferty (2002)" startWordPosition="1163" endWordPosition="1166"> stochastic gradient decent method (i.e. online learning) to estimate w which is shown to be fast and effictive for large-scale problems (Bottou, 2010). The method approximates (7) by a gradient at a single randomly picked phrase pair. The update rule is: w0k = wk − ηi ∇kPi(w), (8) where ηi is a positive learning rate. ebest = arg max e n i=1 p(ok| p(ok |fi, ¯ei) = (4) 1 kwkk2− min w N i=1 2σ2 K k=1 K k=1 ∂P(w) ∂wk = wk σ2 N i=1 1759 Clearly, the cost is affordable because wk(α) is maintained throughout the algorithm as follows: wk(α&apos;) = wk(α)−σ2(α&apos;ik −αik)φ(¯fi,¯ei) (14) 3.1 The Dual Problem Lebanon and Lafferty (2002) derived an equivalent dual problem to (6). Introducing Lagrange multipliers α, the dual becomes min 1 K kwk(α)k2 + N K αik log αik, W 2σ2 k=1 i=1 k=1 K s.t. αik = 1 and αik ≥ 0, ∀i, k, (9) k=1 where N wk(α) = σ2 (˜pik − αik)φ( fi, ¯ei) (10) i=1 As mentioned in the introduction, Yu et al. (2011) proposed a two-level dual coordinate descent method to minimize D(α) in (9) but it has some numerical difficulties. Collins et al. (2008) proposed simple exponentiated gradient (EG) algorithm for Conditional Random Feild (CRF). The algorithm is applicable to our problem, a special case of CRF. The rule</context>
</contexts>
<marker>Lebanon, Lafferty, 2002</marker>
<rawString>Guy Lebanon and John D. Lafferty. 2002. Boosting and maximum likelihood for exponential models. In T.G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 447–454. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinh Van Nguyen</author>
<author>Akira Shimazu</author>
<author>Minh Le Nguyen</author>
<author>Thai Phuong Nguyen</author>
</authors>
<title>Improving a lexicalized hierarchical reordering model using maximum entropy.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twelfth Machine Translation Summit (MT Summit XII). International Association for Machine Translation.</booktitle>
<marker>Van Nguyen, Shimazu, Le Nguyen, Nguyen, 2009</marker>
<rawString>Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen, and Thai Phuong Nguyen. 2009. Improving a lexicalized hierarchical reordering model using maximum entropy. In Proceedings of the Twelfth Machine Translation Summit (MT Summit XII). International Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yizhao Ni</author>
<author>Craig Saunders</author>
<author>Sandor Szedmak</author>
<author>Mahesan Niranjan</author>
</authors>
<title>Exploitation of machine learning techniques in modelling phrase movements for machine translation.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--1</pages>
<contexts>
<context position="2245" citStr="Ni et al., 2011" startWordPosition="307" endWordPosition="310">ased on the training corpus (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). Building on this, some researchers have borrowed powerful ideas from the machine learning literature, to pose the phrase movement problem as a prediction problem using contextual input features whose importance is modeled as weights of a linear classifier trained by entropic criteria. The approach (so called maximum entropy classifier or simply MaxEnt) is a popular choice (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011). Max-margin structure classifiers were also proposed (Ni et al., 2011). Alternatively, Cherry (2013) proposed recently using sparse features optimize the translation quality with the decoder instead of training a classifier independently. While large-scale parallel corpus is advantageous for improving such reordering model, this improvement comes at a price of computational complexity. This issue is particularly pronounced when discriminative models are considered such as maximum entropy-based model due to the required iterative learning. Advancements in solving large-scale classification problems have been shown to be effective such as dual coordinate descent m</context>
</contexts>
<marker>Ni, Saunders, Szedmak, Niranjan, 2011</marker>
<rawString>Yizhao Ni, Craig Saunders, Sandor Szedmak, and Mahesan Niranjan. 2011. Exploitation of machine learning techniques in modelling phrase movements for machine translation. Journal of Machine Learning Research, 12:1–30, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="10603" citStr="Och and Ney, 2000" startWordPosition="1747" endWordPosition="1750">en 14: Update α and w(α) by (14) 15: Break 16: end if 17: η = 0.5η 18: end for 19: until vi ≤ E ∀i K D(α&apos;) − D(α) = k=1 σ2 ¯fi, ¯ei)k2 K + 2 kφ( k=1 ∇ikD(α) = min k max k 1760 4 Experiments We used MultiUN which is a large-scale parallel corpus extracted from the United Nations website (Eisele and Chen, 2010). We have used Arabic and English portion of MultiUN where the English side is about 300 million words. We simplify the problem by classifying phrase movements into three categories (monotone, swap, discontinuous). To train the reordering models, we used GIZA++ to produce word alignments (Och and Ney, 2000). Then, we used the extract tool that comes with the Moses toolkit (Koehn et al., 2007) in order to extract phrase pairs along with their orientation classes. As shown in Table 1, each extracted phrase pair is represented by linguistic features as follows: • Aligned source and target words in a phrase pair. Each word alignment is a feature. • Words within a window around the source phrase to capture the context. We choose adjacent words of the phrase boundary. The extracted phrase pairs after filtering are 47,227,789. The features that occur more than 10 times are 670,154. Table 1: A generic e</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4034" citStr="Och and Ney, 2002" startWordPosition="578" endWordPosition="581">approach is shown to be fast and memory-efficient. 2 Baseline System In statistical machine translation, the most likely translation ebest of an input sentence f can be found by maximizing the probability p(e|f), as follows: p(e|f). (1) ebest = arg max e 1758 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1758–1763, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics A log-linear combination of different models (features) is used for direct modeling of the posterior probability p(e|f) (Papineni et al., 1998; Och and Ney, 2002): λihi(f,e) (2) where the feature hi(f, e) is a score function over sentence pairs. The translation model and the language model are the main features in any system although additional features h(.) can be integrated easily (such as word penalty). State-of-theart systems usually have around ten features. The language model, which ensures fluent translation, plays an important role in reordering; however, it has a bias towards short translations (Koehn, 2010). Therefore, a need for developing a specific model for the reordering problem. 2.1 Lexicalized Reordering Model Adding a lexicalized reor</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13531" citStr="Och, 2003" startWordPosition="2308" endWordPosition="2309">0 0 0 1 1 1 1 0 0 0 φ(¯f3, ¯e3) = 0 0 0 0 0 0 1 1 1 1 2 f6 . 3 Percentage of active phrase pairs 100 90 80 50 40 30 20 70 60 10 0 6 = 0.1 6 = 1.0 6 = 0.01 Training Time Error Rate 1 hour 9 mins 17.81% 18 minutes 17.95% 13 minutes 22 minutes 21.13% 17.89% 1761 4.2 Translation We used the Moses toolkit (Koehn et al., 2007) with its default settings to build three phrase-based translation systems. They differ in how their reordering models were estimated. The language model is a 5-gram with interpolation and KneserNey smoothing (Kneser and Ney, 1995). We tuned the system by using MERT technique (Och, 2003). As commonly used in statistical machine translation, we evaluated the translation performance by BLEU score (Papineni et al., 2002). The test sets are NIST MT06 and MT08 where the English sides are 35,481 words (1056 sentences) and 116,840 words (3252 sentences), respectively. Table 3 shows the BLEU scores for the translation systems. We also computed statistical significance for the models using the paired bootstrap resampling method (Koehn, 2004b). Translation System MT06 MT08 Baseline + Lexical. model 30.86 34.22 Baseline + Primal MLR 31.37* 34.85* Baseline + Dual MLR c:0.1 31.36* 34.87* </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 160– 167, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
</authors>
<title>Maximum likelihood and discriminative training of direct translation models.</title>
<date>1998</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>189--192</pages>
<contexts>
<context position="4014" citStr="Papineni et al., 1998" startWordPosition="574" endWordPosition="577">blems (section 4). Our approach is shown to be fast and memory-efficient. 2 Baseline System In statistical machine translation, the most likely translation ebest of an input sentence f can be found by maximizing the probability p(e|f), as follows: p(e|f). (1) ebest = arg max e 1758 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1758–1763, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics A log-linear combination of different models (features) is used for direct modeling of the posterior probability p(e|f) (Papineni et al., 1998; Och and Ney, 2002): λihi(f,e) (2) where the feature hi(f, e) is a score function over sentence pairs. The translation model and the language model are the main features in any system although additional features h(.) can be integrated easily (such as word penalty). State-of-theart systems usually have around ten features. The language model, which ensures fluent translation, plays an important role in reordering; however, it has a bias towards short translations (Koehn, 2010). Therefore, a need for developing a specific model for the reordering problem. 2.1 Lexicalized Reordering Model Addin</context>
</contexts>
<marker>Papineni, Roukos, Ward, 1998</marker>
<rawString>Kishore Papineni, Salim Roukos, and Todd Ward. 1998. Maximum likelihood and discriminative training of direct translation models. In Proceedings of ICASSP, pages 189–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13664" citStr="Papineni et al., 2002" startWordPosition="2326" endWordPosition="2329"> 60 10 0 6 = 0.1 6 = 1.0 6 = 0.01 Training Time Error Rate 1 hour 9 mins 17.81% 18 minutes 17.95% 13 minutes 22 minutes 21.13% 17.89% 1761 4.2 Translation We used the Moses toolkit (Koehn et al., 2007) with its default settings to build three phrase-based translation systems. They differ in how their reordering models were estimated. The language model is a 5-gram with interpolation and KneserNey smoothing (Kneser and Ney, 1995). We tuned the system by using MERT technique (Och, 2003). As commonly used in statistical machine translation, we evaluated the translation performance by BLEU score (Papineni et al., 2002). The test sets are NIST MT06 and MT08 where the English sides are 35,481 words (1056 sentences) and 116,840 words (3252 sentences), respectively. Table 3 shows the BLEU scores for the translation systems. We also computed statistical significance for the models using the paired bootstrap resampling method (Koehn, 2004b). Translation System MT06 MT08 Baseline + Lexical. model 30.86 34.22 Baseline + Primal MLR 31.37* 34.85* Baseline + Dual MLR c:0.1 31.36* 34.87* Table 3: BLEU scores for Arabic-English translation systems with different reordering models (*: better than the lexicalized model wi</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juho Rousu</author>
<author>Craig Saunders</author>
<author>Sandor Szedmak</author>
<author>John Shawe-Taylor</author>
</authors>
<title>Kernel-based learning of hierarchical multilabel classification models.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>1601--1626</pages>
<contexts>
<context position="6323" citStr="Rousu et al., 2006" startWordPosition="946" endWordPosition="949">ultinomial Logistic Regression Multinomial logistic regression (MLR), also known as maximum entropy classifier (Zens and Ney, 2006), is a probabilistic model for the multiclass problem. The class probability is given by: p(ok |¯fi, ¯ei) = exp(w&gt; k φ( ¯fi, ¯ei))&gt;fi (5) Ek, exp(wk,φ(, ei)) where φ( fi, ¯ei) is the feature vector of the i-th phrase pair. An equivalent notation to w&gt;k φ(¯fi, ¯ei) is w&gt;f(φ( ¯fi, ¯ei), ok) where w is a long vector composed of all classes parameters (i.e. w&gt; = [w&gt;1 ... w&gt;K] ) and f(., .) is a joint feature vector decomposed via the orthogonal feature representation (Rousu et al., 2006). This representation simply means there is no crosstalk between two different feature vectors. For example, f(φ( ¯fi, ¯ei), o1)&gt; = [φ(¯fi, ¯ei)&gt; 0 ... 0]. The model’s parameters can be estimated by minimizing the following regularized negative log-likelihood P(w) as follows (Bishop, 2006): ˜pik log p(ok |fi, ¯ei) (6) Here σ is a penalty parameter and p˜ is the empirical distribution where ˜pik equals zero for all ok =6 oi. Solving the primal optimization problem (6) using the gradient: (˜pik − p(ok |¯fi, ¯ei)) φ(¯fi, ¯ei), (7) do not constitute a closed-form solution. In our experiments, we u</context>
</contexts>
<marker>Rousu, Saunders, Szedmak, Shawe-Taylor, 2006</marker>
<rawString>Juho Rousu, Craig Saunders, Sandor Szedmak, and John Shawe-Taylor. 2006. Kernel-based learning of hierarchical multilabel classification models. Journal of Machine Learning Research, pages 1601– 1626.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL: Short Papers,</booktitle>
<pages>101--104</pages>
<contexts>
<context position="1672" citStr="Tillmann, 2004" startWordPosition="217" endWordPosition="218">le preserving the accuracy. 1 Introduction Phrase reordering is a common problem when translating between two grammatically different languages. Analogous to speech recognition systems, statistical machine translation (SMT) systems relied on language models to produce more fluent output. While early work penalized phrase movements without considering reorderings arising from vastly differing grammatical structures across language pairs like Arabic-English (Koehn, 2004a), many researchers considered lexicalized reordering models that attempted to learn orientation based on the training corpus (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). Building on this, some researchers have borrowed powerful ideas from the machine learning literature, to pose the phrase movement problem as a prediction problem using contextual input features whose importance is modeled as weights of a linear classifier trained by entropic criteria. The approach (so called maximum entropy classifier or simply MaxEnt) is a popular choice (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011). Max-margin structure classifiers were also proposed (Ni et al., 2011). Alternatively, Cherry (20</context>
<context position="4893" citStr="Tillmann, 2004" startWordPosition="710" endWordPosition="711">y). State-of-theart systems usually have around ten features. The language model, which ensures fluent translation, plays an important role in reordering; however, it has a bias towards short translations (Koehn, 2010). Therefore, a need for developing a specific model for the reordering problem. 2.1 Lexicalized Reordering Model Adding a lexicalized reordering model consistently improved the translation quality for several language pairs (Koehn et al., 2005). Reordering modeling involves formulating phrase movements as a classification problem where each phrase position considered as a class (Tillmann, 2004). Some researchers classified phrase movements into three categories (monotone, swap, and discontinuous) but the classes can be extended to any arbitrary number (Koehn and Monz, 2005). In general, the distribution of phrase orientation is: 1 fi, ¯ei) = Z h(¯fi, ¯ei, ok) . (3) This lexicalized reordering model is estimated by relative frequency where each phrase pair (¯fi, ¯ei) with such an orientation (ok) is counted and then normalized to yield the probability as follows: count( fi, ¯ei, ok) Eo count(¯fi, ¯ei, o). The orientation of a current phrase pair is defined with respect to the previou</context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>Christoph Tillmann. 2004. A unigram orientation model for statistical machine translation. In Proceedings of HLT-NAACL: Short Papers, pages 101– 104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Xiang</author>
<author>Niyu Ge</author>
<author>Abraham Ittycheriah</author>
</authors>
<title>Improving reordering for statistical machine translation with smoothed priors and syntactic features.</title>
<date>2011</date>
<booktitle>In Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<pages>61--69</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="2174" citStr="Xiang et al., 2011" startWordPosition="297" endWordPosition="300">idered lexicalized reordering models that attempted to learn orientation based on the training corpus (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). Building on this, some researchers have borrowed powerful ideas from the machine learning literature, to pose the phrase movement problem as a prediction problem using contextual input features whose importance is modeled as weights of a linear classifier trained by entropic criteria. The approach (so called maximum entropy classifier or simply MaxEnt) is a popular choice (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011). Max-margin structure classifiers were also proposed (Ni et al., 2011). Alternatively, Cherry (2013) proposed recently using sparse features optimize the translation quality with the decoder instead of training a classifier independently. While large-scale parallel corpus is advantageous for improving such reordering model, this improvement comes at a price of computational complexity. This issue is particularly pronounced when discriminative models are considered such as maximum entropy-based model due to the required iterative learning. Advancements in solving large-scale classification pro</context>
</contexts>
<marker>Xiang, Ge, Ittycheriah, 2011</marker>
<rawString>Bing Xiang, Niyu Ge, and Abraham Ittycheriah. 2011. Improving reordering for statistical machine translation with smoothed priors and syntactic features. In Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 61–69, Portland, Oregon, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,</booktitle>
<pages>521--528</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney,</location>
<contexts>
<context position="2132" citStr="Xiong et al., 2006" startWordPosition="289" endWordPosition="292">ish (Koehn, 2004a), many researchers considered lexicalized reordering models that attempted to learn orientation based on the training corpus (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). Building on this, some researchers have borrowed powerful ideas from the machine learning literature, to pose the phrase movement problem as a prediction problem using contextual input features whose importance is modeled as weights of a linear classifier trained by entropic criteria. The approach (so called maximum entropy classifier or simply MaxEnt) is a popular choice (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011). Max-margin structure classifiers were also proposed (Ni et al., 2011). Alternatively, Cherry (2013) proposed recently using sparse features optimize the translation quality with the decoder instead of training a classifier independently. While large-scale parallel corpus is advantageous for improving such reordering model, this improvement comes at a price of computational complexity. This issue is particularly pronounced when discriminative models are considered such as maximum entropy-based model due to the required iterative learning. Advancements</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 521–528, Sydney, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsiang-Fu Yu</author>
<author>Fang-Lan Huang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Dual coordinate descent methods for logistic regression and maximum entropy models.</title>
<date>2011</date>
<booktitle>Machine Learning,</booktitle>
<pages>85--1</pages>
<contexts>
<context position="2935" citStr="Yu et al. (2011)" startWordPosition="407" endWordPosition="410">imize the translation quality with the decoder instead of training a classifier independently. While large-scale parallel corpus is advantageous for improving such reordering model, this improvement comes at a price of computational complexity. This issue is particularly pronounced when discriminative models are considered such as maximum entropy-based model due to the required iterative learning. Advancements in solving large-scale classification problems have been shown to be effective such as dual coordinate descent method for linear support vector machines (Hsieh et al., 2008). Similarly, Yu et al. (2011) proposed a two-level dual coordinate descent method for maximum entropy classifier. In this work we explore the dual problem to multinomial logistic regression for building largescale reordering model (section 3). One of the main advantages of solving the dual problem is providing a mechanism to shrink the training data which is a serious issue in building such largescale system. We present empirical results comparing between the primal and the dual problems (section 4). Our approach is shown to be fast and memory-efficient. 2 Baseline System In statistical machine translation, the most likel</context>
<context position="7849" citStr="Yu et al. (2011)" startWordPosition="1227" endWordPosition="1230">earning rate. ebest = arg max e n i=1 p(ok| p(ok |fi, ¯ei) = (4) 1 kwkk2− min w N i=1 2σ2 K k=1 K k=1 ∂P(w) ∂wk = wk σ2 N i=1 1759 Clearly, the cost is affordable because wk(α) is maintained throughout the algorithm as follows: wk(α&apos;) = wk(α)−σ2(α&apos;ik −αik)φ(¯fi,¯ei) (14) 3.1 The Dual Problem Lebanon and Lafferty (2002) derived an equivalent dual problem to (6). Introducing Lagrange multipliers α, the dual becomes min 1 K kwk(α)k2 + N K αik log αik, W 2σ2 k=1 i=1 k=1 K s.t. αik = 1 and αik ≥ 0, ∀i, k, (9) k=1 where N wk(α) = σ2 (˜pik − αik)φ( fi, ¯ei) (10) i=1 As mentioned in the introduction, Yu et al. (2011) proposed a two-level dual coordinate descent method to minimize D(α) in (9) but it has some numerical difficulties. Collins et al. (2008) proposed simple exponentiated gradient (EG) algorithm for Conditional Random Feild (CRF). The algorithm is applicable to our problem, a special case of CRF. The rule update is: a = αik exp(−ηi ∇ikD(α)) 11) aik Ek, αik, exp(−ηi ∇ik,D(α)) where ∂D(α) ∇ikD(α) ≡ ∂α = 1 + log αik ik � � + wy(α)Tφ( ¯fi, ¯ei) − wk(α)Tφ(¯fi, ¯ei) . (12) Here y represents the true class (i.e. oy = oi). To improve the convergence, ηi is adaptively adjusted for each example. If the ob</context>
<context position="9087" citStr="Yu et al., 2011" startWordPosition="1461" endWordPosition="1464">id not decrease, ηi is halved for number of trials (Collins et al., 2008). Calculating the function difference below is the main cost in EG algorithm, �α&apos; � ik log α&apos; ik − αik log αik K − (α&apos;ik − αik)wk(α)Tφ(¯fi, ¯ei) k=1 &apos; 2 (αik − αik).(13) Following Yu et al. (2011), we initialize αik as follows: αik 15 (1 − E) if ok = oi; ( ) K−1 = Sl E else. where E is a small positive value. This is because the objective function (9) is not well defined at αik = 0 due to the logarithm appearance. Finally, the optimal dual variables are achieved when the following condition is satisfied for all examples (Yu et al., 2011): ∇ikD(α) (16) This condition is the key to accelerate EG algorithm. Unlike the primal problem (6), the dual variables αik are associated with each example (i.e. phrase pair) therefore a training example can be disregarded once its optimal dual variables obtained. More data shrinking can be achieved by tolerating a small difference between the two values in (16). Algorithm 1 presents the overall procedure (shrinking step is from line 6 to 9). Algorithm 1 Shrinking stochastic exponentiated gradient method for training the dual problem Require: training set S = {φ(¯fi, ¯ei), oi}Ni=1 1: Given α a</context>
</contexts>
<marker>Yu, Huang, Lin, 2011</marker>
<rawString>Hsiang-Fu Yu, Fang-Lan Huang, and Chih-Jen Lin. 2011. Dual coordinate descent methods for logistic regression and maximum entropy models. Machine Learning, 85(1-2):41–75, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative reordering models for statistical machine trans-</title>
<date>2006</date>
<contexts>
<context position="2112" citStr="Zens and Ney, 2006" startWordPosition="285" endWordPosition="288">irs like Arabic-English (Koehn, 2004a), many researchers considered lexicalized reordering models that attempted to learn orientation based on the training corpus (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005). Building on this, some researchers have borrowed powerful ideas from the machine learning literature, to pose the phrase movement problem as a prediction problem using contextual input features whose importance is modeled as weights of a linear classifier trained by entropic criteria. The approach (so called maximum entropy classifier or simply MaxEnt) is a popular choice (Zens and Ney, 2006; Xiong et al., 2006; Nguyen et al., 2009; Xiang et al., 2011). Max-margin structure classifiers were also proposed (Ni et al., 2011). Alternatively, Cherry (2013) proposed recently using sparse features optimize the translation quality with the decoder instead of training a classifier independently. While large-scale parallel corpus is advantageous for improving such reordering model, this improvement comes at a price of computational complexity. This issue is particularly pronounced when discriminative models are considered such as maximum entropy-based model due to the required iterative le</context>
<context position="5835" citStr="Zens and Ney, 2006" startWordPosition="855" endWordPosition="858"> by relative frequency where each phrase pair (¯fi, ¯ei) with such an orientation (ok) is counted and then normalized to yield the probability as follows: count( fi, ¯ei, ok) Eo count(¯fi, ¯ei, o). The orientation of a current phrase pair is defined with respect to the previous target phrase. Galley and Manning (2008) extended the model to tackle long-distance reorderings. Their hierarchical model enables phrase movements that are more complex than swaps between adjacent phrases. 3 Multinomial Logistic Regression Multinomial logistic regression (MLR), also known as maximum entropy classifier (Zens and Ney, 2006), is a probabilistic model for the multiclass problem. The class probability is given by: p(ok |¯fi, ¯ei) = exp(w&gt; k φ( ¯fi, ¯ei))&gt;fi (5) Ek, exp(wk,φ(, ei)) where φ( fi, ¯ei) is the feature vector of the i-th phrase pair. An equivalent notation to w&gt;k φ(¯fi, ¯ei) is w&gt;f(φ( ¯fi, ¯ei), ok) where w is a long vector composed of all classes parameters (i.e. w&gt; = [w&gt;1 ... w&gt;K] ) and f(., .) is a joint feature vector decomposed via the orthogonal feature representation (Rousu et al., 2006). This representation simply means there is no crosstalk between two different feature vectors. For example, f(φ</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>Richard Zens and Hermann Ney. 2006. Discriminative reordering models for statistical machine trans-</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
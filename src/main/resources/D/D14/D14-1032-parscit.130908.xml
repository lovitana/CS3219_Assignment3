<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017780">
<title confidence="0.998961">
Learning Abstract Concept Embeddings from Multi-Modal Data:
Since You Probably Can’t See What I Mean
</title>
<author confidence="0.998547">
Felix Hill Anna Korhonen
</author>
<affiliation confidence="0.9994385">
Computer Laboratory Computer Laboratory
University of Cambridge University of Cambridge
</affiliation>
<email confidence="0.99753">
felix.hill@cl.cam.ac.uk anna.korhonen@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.993852" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991436">
Models that acquire semantic represen-
tations from both linguistic and percep-
tual input are of interest to researchers
in NLP because of the obvious parallels
with human language learning. Perfor-
mance advantages of the multi-modal ap-
proach over language-only models have
been clearly established when models are
required to learn concrete noun concepts.
However, such concepts are comparatively
rare in everyday language. In this work,
we present a new means of extending
the scope of multi-modal models to more
commonly-occurring abstract lexical con-
cepts via an approach that learns multi-
modal embeddings. Our architecture out-
performs previous approaches in combin-
ing input from distinct modalities, and
propagates perceptual information on con-
crete concepts to abstract concepts more
effectively than alternatives. We discuss
the implications of our results both for op-
timizing the performance of multi-modal
models and for theories of abstract con-
ceptual representation.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994225">
Multi-modal models that learn semantic represen-
tations from both language and information about
the perceptible properties of concepts were orig-
inally motivated by parallels with human word
learning (Andrews et al., 2009) and evidence that
many concepts are grounded in perception (Barsa-
lou and Wiemer-Hastings, 2005). The perceptual
information in such models is generally mined di-
rectly from images (Feng and Lapata, 2010; Bruni
et al., 2012) or from data collected in psychologi-
cal studies (Silberer and Lapata, 2012; Roller and
Schulte im Walde, 2013).
By exploiting the additional information en-
coded in perceptual input, multi-modal models
can outperform language-only models on a range
of semantic NLP tasks, including modelling sim-
ilarity (Bruni et al., 2014; Kiela et al., 2014) and
free association (Silberer and Lapata, 2012), pre-
dicting compositionality (Roller and Schulte im
Walde, 2013) and concept categorization (Silberer
and Lapata, 2014). However, to date, these pre-
vious approaches to multi-modal concept learning
focus on concrete words such as cat or dog, rather
than abstract concepts, such as curiosity or loyalty.
However, differences between abstract and con-
crete processing and representation (Paivio, 1991;
Hill et al., 2013; Kiela et al., 2014) suggest that
conclusions about concrete concept learning may
not necessarily hold in the general case. In this pa-
per, we therefore focus on multi-modal models for
learning both abstract and concrete concepts.
Although concrete concepts might seem more
basic or fundamental, the vast majority of open-
class, meaning-bearing words in everyday lan-
guage are in fact abstract. 72% of the noun or
verb tokens in the British National Corpus (Leech
et al., 1994) are rated by human judges1 as more
abstract than the noun war, for instance, a con-
cept many would already consider to be quite
abstract. Moreover, abstract concepts by defi-
nition encode higher-level (more general) princi-
ples than concrete concepts, which typically re-
side naturally in a single semantic category or do-
main (Crutch and Warrington, 2005). It is there-
fore likely that abstract representations may prove
highly applicable for multi-task, multi-domain or
transfer learning models, which aim to acquire
‘general-purpose’ conceptual knowledge without
reference to a specific objective or task (Collobert
and Weston, 2008; Mesnil et al., 2012).
In a recent paper, Hill et al. (2014) investigate
whether the multi-modal models cited above are
</bodyText>
<footnote confidence="0.518186">
1Contributors to the USF dataset (Nelson et al., 2004).
</footnote>
<page confidence="0.947592">
255
</page>
<note confidence="0.9113315">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 255–265,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999961616438356">
effective for learning concepts other than concrete
nouns. They observe that representations of cer-
tain abstract concepts can indeed be enhanced in
multi-modal models by combining perceptual and
linguistic input with an information propagation
step. Hill et al. (2014) propose ridge regression as
an alternative to the nearest-neighbour averaging
proposed by Johns and Jones (2012) for such prop-
agation, and show that it is more robust to changes
in the type of concept to be learned. However, both
methods are somewhat inelegant, in that they learn
separate linguistic and ‘pseudo-perceptual’ repre-
sentations, which must be combined via a separate
information combination step. Moreover, for the
majority of abstract concepts, the best performing
multi-modal model employing these techniques
remains less effective than conventional text-only
representation learning model.
Motivated by these observations, we introduce
an architecture for learning both abstract and con-
crete representations that generalizes the skipgram
model of Mikolov et al. (2013) from text-based to
multi-modal learning. Aspects of the model de-
sign are influenced by considering the process of
human language learning. The model moderates
the training input to include more perceptual infor-
mation about commonly-occurring concrete con-
cepts and less information about rarer concepts.
Moreover, it integrates the processes of combin-
ing perceptual and linguistic input and propagat-
ing information from concrete to abstract concepts
into a single representation update process based
on back-propagation.
We train our model on running-text language
and two sources of perceptual descriptors for con-
crete nouns: the ESPGame dataset of annotated
images (Von Ahn and Dabbish, 2004) and the
CSLB set of concept property norms (Devereux
et al., 2013). We find that our model combines in-
formation from the different modalities more ef-
fectively than previous methods, resulting in an
improved ability to model the USF free associa-
tion gold standard (Nelson et al., 2004) for con-
crete nouns. In addition, the architecture propa-
gates the extra-linguistic input for concrete nouns
to improve representations of abstract concepts
more effectively than alternative methods. While
this propagation can effectively extend the advan-
tage of the multi-modal approach to many more
concepts than simple concrete nouns, we observe
that the benefit of adding perceptual input appears
to decrease as target concepts become more ab-
stract. Indeed, for the most abstract concepts of
all, language-only models still provide the most
effective learning mechanism.
Finally, we investigate the optimum quantity
and type of perceptual input for such models. Be-
tween the most concrete concepts, which can be
effectively represented directly in the perceptual
modality, and the most abstract concepts, which
cannot, we identify a set of concepts that cannot
be represented effectively directly in the percep-
tual modality, but still benefit from perceptual in-
put propagated in the model via concrete concepts.
The motivation in designing our model and ex-
periments is both practical and theoretical. Taken
together, the empirical observations we present are
potentially important for optimizing the learning
of representations of concrete and abstract con-
cepts in multi-modal models. In addition, they of-
fer a degree of insight into the poorly understood
issue of how abstract concepts may be encoded in
human memory.
</bodyText>
<sectionHeader confidence="0.952044" genericHeader="method">
2 Model Design
</sectionHeader>
<bodyText confidence="0.999976304347826">
Before describing how our multi-modal architec-
ture encodes and integrates perceptual informa-
tion, we first describe the underlying corpus-based
representation learning model.
Language-only Model Our multi-modal archi-
tecture builds on the continuous log-linear skip-
gram language model proposed by Mikolov et
al. (2013). This model learns lexical representa-
tions in a similar way to neural-probabilistic lan-
guage models (NPLM) but without a non-linear
hidden layer, a simplification that facilitates the
efficient learning of large vocabularies of dense
representations, generally referred to as embed-
dings (Turian et al., 2010). Embeddings learned
by the model achieve state-of-the-art performance
on several evaluations including sentence comple-
tion and analogy modelling (Mikolov et al., 2013).
For each word type w in the vocabulary V , the
model learns both a ‘target-embedding’ r,,, ∈ Rd
and a ‘context-embedding’ ˆr,,, ∈ Rd such that,
given a target word, its ability to predict nearby
context words is maximized. The probability of
seeing context word c given target w is defined as:
</bodyText>
<equation confidence="0.979928">
p(c|w) = eˆrc rw
Ev∈V eˆrv rw
</equation>
<page confidence="0.993923">
256
</page>
<figure confidence="0.978746">
Target Representation Context Representations Information Source
w
n
Score: p(c|&apos;)
p&apos;
_
p&apos;
��,
p&apos;
m,
p&apos;
_
w
n+2
wn+1
w
n-1
w
n-2
Perceptual
Linguistic
Text8 Corpus
PESP
PCSBL
</figure>
<figureCaption confidence="0.93641575">
Figure 1: Our multi-modal model architecture. Light boxes are elements of the original Mikolov et
al. (2013) model. For target words wn in the domain of P (concrete concepts), the model updates its
representations based on corpus context words wnfi, then on words pwnfi in perceptual pseudo-sentences.
For wn not in the domain of P (abstract concepts), updates are based solely on the wnfi.
</figureCaption>
<bodyText confidence="0.999889210526316">
The model learns from a set of target-word,
context-word pairs, extracted from a corpus of
sentences as follows. In a given sentence S (of
length N), for each position n &lt; N, each word
wn is treated in turn as a target word. An inte-
ger t(n) is then sampled from a uniform distribu-
tion on {1,... k}, where k &gt; 0 is a predefined
maximum context-window parameter. The pair to-
kens {(wn, wn+j) : −t(n) &lt; j &lt; t(n), wi E S}
are then appended to the training data. Thus, tar-
get/context training pairs are such that (i) only
words within a k-window of the target are selected
as context words for that target, and (ii) words
closer to the target are more likely to be selected
than those further away.
The training objective is then to maximize the
sum of the log probabilities T across of all such
examples from S and across all sentences in the
corpus, where T is defined as follows:
</bodyText>
<equation confidence="0.997105">
log(p(wn+j|wn))
−t(n)&lt;j&lt;t(n),j7W
</equation>
<bodyText confidence="0.9999591">
The model free parameters (target-embeddings
and context-embeddings of dimension d for each
word in the corpus with frequency above a certain
threshold f) are updated according to stochastic
gradient descent and backpropation, with learning
rate controlled by Adagrad (Duchi et al., 2011).
For efficiency, the output layer is encoded as a
hierarchical softmax function based on a binary
Huffman tree (Morin and Bengio, 2005).
As with other distributional architectures, the
model captures conceptual semantics by exploit-
ing the fact that words appearing in similar lin-
guistic contexts are likely to have similar mean-
ings. Informally, the model adjusts its embeddings
to increase the ‘probability’ of seeing the language
in the training corpus. Since this probability in-
creases with the p(c|w), and the p(c|w) increase
with the dot product ˆrc · rw, the updates have the
effect of moving each target-embedding incremen-
tally ‘closer’ to the context-embeddings of its col-
locates. In the target-embedding space, this results
in embeddings of concept words that regularly oc-
cur in similar contexts moving closer together.
Multi-modal Extension We extend the Mikolov
et al. (2013) architecture via a simple means of in-
troducing perceptual information that aligns with
human language learning. Based on the assump-
tion that frequency in domain-general linguistic
corpora correlates with the likelihood of ‘experi-
encing’ a concept in the world (Bybee and Hop-
per, 2001; Chater and Manning, 2006), perceptual
information is introduced to the model whenever
designated concrete concepts are encountered in
the running-text linguistic input. This has the ef-
fect of introducing more perceptual input for com-
monly experienced concrete concepts and less in-
put for rarer concrete concepts.
To implement this process, perceptual informa-
tion is extracted from external sources and en-
coded in an associative array P, which maps (typ-
ically concrete) words w to bags of perceptual fea-
tures b(w). The construction of this array depends
on the perceptual information source; the process
for our chosen sources is detailed in Section 2.1.
Training our model begins as before on running-
text. When a sentence Sm containing a word w in
the domain of P is encountered, the model finishes
training on Sm and begins learning from a per-
ceptual pseudo-sentence ˆSm(w). ˆSm(w) is con-
structed by alternating the token w with a fea-
</bodyText>
<equation confidence="0.9659355">
1
T = N
N
n=1
</equation>
<page confidence="0.927052">
257
</page>
<figure confidence="0.77162675">
ˆS(crocodile) = Crocodile legs crocodile teeth crocodile
teeth crocodile scales crocodile green crocodile.
ˆS(screwdriver) = Screwdriver handle screwdriver flat
screwdriver long screwdriver handle screwdriver head.
</figure>
<figureCaption confidence="0.972107">
Figure 2: Example pseudo-sentences generated by
our model.
</figureCaption>
<bodyText confidence="0.999931243243243">
ture sampled at random from b(w) until ˆSm(w)
is the same length as Sm (see Figure 2). Because
we want the ensuing perceptual learning process
to focus on how w relates to its perceptual prop-
erties (rather than how those properties relate to
each other), we insert multiple instances of w into
ˆSm(w). This ensures that the majority of train-
ing cases derived from ˆSm(w) are instances of (w,
feature) rather than (feature, feature) pairs. Once
training on ˆSm(w) is complete, the model reverts
to the next ‘genuine’ (linguistic) sentence Sm+1,
and the process continues. Thus, when a concrete
concept is encountered in the corpus, its embed-
ding is first updated based on language (moved in-
crementally closer to concepts appearing in sim-
ilar linguistic contexts), and then on perception
(moved incrementally closer to concepts with the
same or similar perceptual features).
For greater flexibility, we introduce a parameter
α reflecting the raw quantity of perceptual infor-
mation relative to linguistic input. When α = 2,
two pseudo-sentences are generated and inserted
for every corpus occurrence of a token from the
domain of P. For non-integral α, the number of
sentences inserted is Lα], and a further sentence is
added with probability α − Lα].
In all experiments reported in the following sec-
tions we set the window size parameter k = 5 and
the minimum frequency parameter f = 3, which
guarantees that the model learns embeddings for
all concepts in our evaluation sets. While the
model learns both target and context-embeddings
for each word in the vocabulary, we conduct our
experiments with the target embeddings only. We
set the dimension parameter d = 300 as this pro-
duces high quality embeddings in the language-
only case (Mikolov et al., 2013).
</bodyText>
<subsectionHeader confidence="0.900356">
2.1 Information Sources
</subsectionHeader>
<bodyText confidence="0.997731">
We construct the associative array of perceptual
information P from two sources typical of those
used for multi-modal semantic models.
ESPGame Dataset The ESP-Game dataset
(ESP) (Von Ahn and Dabbish, 2004) consists of
100,000 images, each annotated with a list of lex-
ical concepts that appear in that image.
For any concept w identified in an ESP im-
age, we construct a corresponding bag of features
b(w). For each ESP image I that contains w, we
append the other concept tokens identified in I to
b(w). Thus, the more frequently a concept co-
occurs with w in images, the more its correspond-
ing lexical token occurs in b(w). The array PESP
in this case then consists of the (w, b(w)) pairs.
CSLB Property Norms The Centre for Speech,
Language and the Brain norms (CSLB) (Devereux
et al., 2013) is a recently-released dataset contain-
ing semantic properties for 638 concrete concepts
produced by human annotators. The CSLB dataset
was compiled in the same way as the McRae et
al. (2005) property norms used widely in multi-
modal models (Silberer and Lapata, 2012; Roller
and Schulte im Walde, 2013); we use CSLB be-
cause it contains more concepts. For each concept,
the proportion of the 30 annotators that produced
a given feature can also be employed as a measure
of the strength of that feature.
When encoding the CSLB data in P, we first
map properties to lexical forms (e.g. is green
becomes green). By directly identifying percep-
tual features and linguistic forms in this way,
we treat features observed in the perceptual data
as (sub)concepts to be acquired via the same
multi-modal input streams and stored in the same
domain-general memory as the evaluation con-
cepts. This design decision in fact corresponds
to a view of cognition that is sometimes disputed
(Fodor, 1983). In future studies we hope to com-
pare the present approach to architectures with
domain-specific conceptual memories.
For each concept w in CSLB, we then con-
struct a feature bag b(w) by appending lexical
forms to b(w) such that the count of each fea-
ture word is equal to the strength of that feature
for w. Thus, when features are sampled from
b(w) to create pseudo-sentences (as detailed pre-
viously) the probability of a feature word occur-
ring in a sentence reflects feature strength. The
array PCSLB then consists of all (w, b(w)) pairs.
Linguistic Input The linguistic input to all
models is the 400m word Text8 Corpus2 of
</bodyText>
<footnote confidence="0.997241">
2From http://mattmahoney.net/dc/textdata.html
</footnote>
<page confidence="0.985907">
258
</page>
<figure confidence="0.989168380952381">
ESPGame CSLB
Image 1
red
chihuaua
eyes
little
ear
Image 2 Crocodile
wreck has 4 legs (7)
cyan has tail (18)
man has jaw (7)
crash has scales (8)
accident has teeth (20)
Screwdriver
has handle (28)
has head (5)
is long (9)
is plastic (18)
is metal (28)
Concept Type
concrete nouns
abstract nouns
all nouns
concrete verbs
abstract verbs
all verbs
List Pairs
541 1418
100 295
666 1815
50 66
50 127
100 221
Examples
yacht, cup
fear, respect
fear, cup
kiss, launch
differ, obey
kiss, obey
street is green (10)
is large (10)
</figure>
<tableCaption confidence="0.7586726">
Table 1: Concepts identified in images in the ESP
Game (left) and features produced for concepts by
human annotators in the CSLB dataset (with fea-
ture strength, max=30).
Table 2: Example concept pairs (with mean con-
</tableCaption>
<bodyText confidence="0.972209">
creteness rating) and free-association scores from
the USF dataset.
Wikipedia text, split into sentences and with punc-
tuation removed.
</bodyText>
<subsectionHeader confidence="0.974585">
2.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.988953975">
We evaluate the quality of representations by how
well they reflect free association scores, an em-
pirical measure of cognitive conceptual proxim-
ity. The University of South Florida Norms
(USF) (Nelson et al., 2004) contain free associa-
tion scores for over 40,000 concept pairs, and have
been widely used in NLP to evaluate semantic rep-
resentations (Andrews et al., 2009; Feng and La-
pata, 2010; Silberer and Lapata, 2012; Roller and
Schulte im Walde, 2013). Each concept that we
extract from the USF database has also been rated
for conceptual concreteness on a Likert scale of
1-7 by at least 10 human annotators. Following
previous studies (Huang et al., 2012; Silberer and
Lapata, 2012), we measure the (Spearman p) cor-
relation between association scores and the cosine
similarity of vector representations.
We create separate abstract and concrete con-
cept lists by ranking the USF concepts accord-
ing to concreteness and sampling at random from
the first and fourth quartiles respectively. We also
introduce a complementary noun/verb dichotomy,
Table 3: Details the subsets of USF data used in
our evaluations, downloadable from our website.
on the intuition that information propagation may
occur differently from noun to noun or from noun
to verb (because of their distinct structural rela-
tionships in sentences). POS-tags are not assigned
as part of the USF data, so we draw the noun/verb
distinction based on the majority POS-tag of USF
concepts in the lemmatized British National Cor-
pus (Leech et al., 1994). The abstract/concrete
and noun/verb dichotomies yield four distinct con-
cept lists. For consistency, the concrete noun list
is filtered so that each concrete noun concept w
has a perceptual representation b(w) in both PESP
and PCSLB. For the four resulting concept lists
C (concrete/abstract, noun/verb), a correspond-
ing set of evaluation pairs {(w1, w2) E USF :
w1, w2 E C} is extracted (see Table 3 for details).
</bodyText>
<sectionHeader confidence="0.999727" genericHeader="evaluation">
3 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999853071428571">
Our experiments were designed to answer four
questions, outlined in the following subsec-
tions: (1) Which model architectures perform best
at combining information pertinent to multiple
modalities when such information exists explicitly
(as common for concrete concepts)? (2) Which
model architectures best propagate perceptual in-
formation to concepts for which it does not exist
explicitly (as is common for abstract concepts)?
(3) Is it preferable to include all of the perceptual
input that can be obtained from a given source, or
to filter this input stream in some way? (4) How
much perceptual vs. linguistic input is optimal for
learning various concept types?
</bodyText>
<subsectionHeader confidence="0.998054">
3.1 Combining information sources
</subsectionHeader>
<bodyText confidence="0.997286">
To evaluate our approach as a method of in-
formation combination we compared its perfor-
mance on the concrete noun evaluation set against
three alternative methods. The first alternative
is simple concatenation of these perceptual vec-
tors with linguistic vectors embeddings learned
</bodyText>
<figure confidence="0.987855176470588">
nose
small
Concept 1
Assoc.
Concept 2
abdomen (6.83)
stomach (6.04)
0.566
throw (4.05)
hope (1.18)
egg (5.79)
ball (6.08)
glory (3.53)
milk (6.66)
0.234
0.192
0.012
</figure>
<page confidence="0.991212">
259
</page>
<bodyText confidence="0.999905041666667">
by the Mikolov et al. (2013) model on the Text8
Corpus. In the second alternative (proposed
for multi-modal models by Silberer and Lapata
(2012)), canonical correlation analysis (CCA)
(Hardoon et al., 2004) was applied to the vec-
tors of both modalities. CCA yields reduced-
dimensionality representations that preserve un-
derlying inter-modal correlations, which are then
concatenated. The final alternative, proposed by
Bruni et al. (2014) involves applying Singular
Value Decomposition (SVD) to the matrix of con-
catenated multi-modal representations, yielding
smoothed representations.3
When implementing the concatenation, CCA
and SVD methods, we first encoded the percep-
tual input directly into sparse feature vectors, with
coordinates for each of the 2726 features in CSLB
and for each of the 100,000 images in ESP. This
sparse encoding matches the approach taken by
Silberer and Lapata (2012), for CCA and concate-
nation, and by Hill et al. (2014) for the ridge re-
gression method of propagation (see below).
We compare these alternatives to our proposed
model with α = 1. In The CSLB and ESP models,
all training pseudo-sentences are generated from
the arrays PCSLB and PESP respectively. In the
models classed as CSLB&amp;ESP, a random choice
between PCSLB and PESP is made every time
perceptual input is included (so that the overall
quantity of perceptual information is the same).
As shown in Figure 2 (left side), the embed-
dings learned by our model achieve a higher cor-
relation with the USF data than simple concatena-
tion, CCA and SVD regardless of perceptual input
source. With the optimal perceptual source (ESP
only), for instance, the correlation is 11% higher
that the next best alternative method, CCA.
One possible factor behind this improvement
is that, in our model, the learned representations
fully integrate the two modalities, whereas for
both CCA and the concatenation method each rep-
resentation feature (whether of reduced dimension
or not) corresponds to a particular modality. This
deeper integration may help our architecture to
overcome the challenges inherent in information
combination such as inter-modality differences in
information content and representation sparsity. It
is also important to note that Bruni et al. (2014) ap-
</bodyText>
<footnote confidence="0.94869">
3CCA was implemented using the CCA package in
R. SVD was implemented using SVDLIBC (http://
tedlab.mit.edu/˜dr/SVDLIBC/), with truncation
factor k = 1024 as per (Bruni et al., 2014).
</footnote>
<bodyText confidence="0.999176">
plied their SVD method with comparatively dense
perceptual representations extracted from images,
whereas our dataset-based perceptual vectors were
sparsely-encoded.
</bodyText>
<subsectionHeader confidence="0.996729">
3.2 Propagating input to abstract concepts
</subsectionHeader>
<bodyText confidence="0.999901736842105">
To test the process of information propagation in
our model, we evaluated the learned embeddings
of more abstract concepts. We compared our
approach with two recently-proposed alternative
methods for inferring perceptual features when ex-
plicit perceptual information is unavailable.
Johns and Jones In the method of Johns and
Jones (2012), pseudo-perceptual representations
for target concepts without a perceptual repre-
sentations (uni-modal concepts) are inferred as a
weighted average of the perceptual representations
of concepts that do have such a representation (bi-
modal concepts).
In the first step of their two-step method, for
each uni-modal concept k, a quasi-perceptual rep-
resentation is computed as an average of the
perceptual representations of bi-modal concepts,
weighted by the proximity between each of these
concepts and k
</bodyText>
<equation confidence="0.9840935">
�kp = S(kl, cl)λ · cp
c∈ C
</equation>
<bodyText confidence="0.999896380952381">
where C is the set of bi-modal concepts, cp and kp
are the perceptual representations for c and k re-
spectively, and cl and kl the linguistic representa-
tions. The exponent parameter A reflects the learn-
ing rate.
In step two, the initial quasi-perceptual repre-
sentations are inferred for a second time, but with
the weighted average calculated over the percep-
tual or initial quasi-perceptual representations of
all other words, not just those that were originally
bi-modal. As with Johns and Jones (2012), we set
the learning rate parameter A to be 3 in the first
step and 13 in the second.
Ridge Regression An alternative, proposed for
the present purpose by Hill et al. (2014), uses ridge
regression (Myers, 1990). Ridge regression is a
variant of least squares regression in which a reg-
ularization term is added to the training objective
to favor solutions with certain properties.
For bi-modal concepts of dimension np, we ap-
ply ridge regression to learn np linear functions
</bodyText>
<page confidence="0.985395">
260
</page>
<bodyText confidence="0.999260714285715">
fi : Rnl —* R that map the linguistic represen-
tations (of dimension nl) to a particular percep-
tual feature i. These functions are then applied
together to map the linguistic representations of
uni-modal concepts to full quasi-perceptual repre-
sentations.
Following Hill et al. (2014), we take the Euclid-
ian l2 norm of the inferred parameter vector as the
regularization term. This ensures that the regres-
sion favors lower coefficients and a smoother so-
lution function, which should provide better gen-
eralization performance than simple linear regres-
sion. The objective for learning the fi is then to
minimize
</bodyText>
<equation confidence="0.956523">
11aX − Yi1122 + 11a112 2
</equation>
<bodyText confidence="0.999964385714286">
where a is the vector of regression coefficients, X
is a matrix of linguistic representations and Yi a
vector of the perceptual feature i for the set of bi-
modal concepts.
Comparisons We applied the Johns and Jones
method and ridge regression starting from linguis-
tic embeddings acquired by the Mikolov et al.
(2013) model on the Text8 Corpus, and concate-
nated the resulting pseudo-perceptual and linguis-
tic representations. As with the implementation
of our model, the perceptual input for these alter-
native models was limited to concrete nouns (i.e.
concrete nouns were the only bi-modal concepts
in the models).
Figure 3 (right side) shows the propagation per-
formance of the three models. While the corre-
lations overall may seem somewhat low, this is
a consequence of the difficulty of modelling the
USF data. In fact, the performance of both the
language-only model and our multi-modal exten-
sion across the concept types (from 0.18 to 0.36) is
equal to or higher than previous models evaluated
on the same data (Feng and Lapata, 2010; Silberer
and Lapata, 2012; Silberer et al., 2013).
For learning representations of concrete verbs,
our approach achieves a 69% increase in perfor-
mance over the next best alternative. The perfor-
mance of the model on abstract verbs is marginally
inferior to Johns and Jones’ method. Neverthe-
less, the clear advantage for concrete verbs makes
our model the best choice for learning represen-
tations of verbs in general, as shown by perfor-
mance on the set all verbs, which also includes
mixed abstract-concrete pairs.
Our model is also marginally inferior to alterna-
tive approaches in learning representations of ab-
stract nouns. However, in this case, no method
improves on the linguistic-only baseline. It is
possible that perceptual information is simply so
removed from the core semantics of these con-
cepts that they are best acquired via the linguis-
tic medium alone, regardless of learning mecha-
nism. The moderately inferior performance of our
method in such cases is likely caused by its greater
inherent inter-modal dependence compared with
methods that simply concatenate uni-modal rep-
resentations. When the perceptual signal is of
low quality, this greater inter-modal dependence
allows the linguistic signal to be obscured.
The trade-off, however, is generally higher-
quality representations when the perceptual signal
is stronger, exemplified by the fact that our pro-
posed approach outperforms alternatives on pairs
generated from both abstract and concrete nouns
(all nouns). Indeed, the low performance of the
Johns and Jones method on all nouns is strik-
ing given that: (a) It performs best on abstract
nouns (p = .282), and (b) For concrete nouns it
reverts to simple concatenation, which also per-
forms comparatively well (p = .249). The poor
performance of the Jobns and Jones method on
all nouns must therefore derive its comparisons
of mixed abstract-concrete or concrete-abstract
pairs. This suggests that the pseudo-perceptual
representations inferred by this method for ab-
stract concepts method may not be compatible
with the directly-encoded perceptual representa-
tions of concrete concepts, rendering the compar-
ison computation between items of differing con-
creteness inaccurate.
</bodyText>
<subsectionHeader confidence="0.996117">
3.3 Direct representation vs. propagation
</subsectionHeader>
<bodyText confidence="0.999891">
Although property norm datasets such as the
CSLB data typically consist of perceptual fea-
ture information for concrete nouns only, image-
based datasets such as ESP do contain informa-
tion on more abstract concepts, which was omit-
ted from the previous experiments. Indeed, im-
age banks such as Google Images contain millions
of photographs portraying quite abstract concepts,
such as love or war. On the other hand, encod-
ings or descriptions of abstract concepts are gen-
erally more subjective and less reliable than those
of concrete concepts (Wiemer-Hastings and Xu,
2005). We therefore investigated whether or not
it is preferable to include this additional informa-
tion as model input or to restrict perceptual input
</bodyText>
<page confidence="0.989193">
261
</page>
<figure confidence="0.999549068181818">
Correlation
0.4 Combination Method
Vector Concatenation
CCA
SVD
Our Model (α=1)
0.3
0.2
0.1
0.0
0.203
0.22
0.15
0.239
0.271
0.259
0.256
0.301
0.249
0.24 0.231
0.296
Correlation
0.4
0.3
0.2
0.1
0.0
0.282
0.265
0.25
0.07
0.236
0.364
0.06
0.116
0.197
0.177 0.172 0.175 0.167 0.175
Propagation Method
Johns and Jones
Ridge Regre a i&apos;n)
Our Model
0.225
CSLB ESP CSLB &amp; ESP abstract nouns all nouns concrete verbs abstract verbs all verbs
Concrete nouns − information combination More abstract concepts − information propagation (CSLB &amp; ESP
</figure>
<figureCaption confidence="0.9908825">
Figure 3: The proposed approach compared with other methods of information combination (left) and
propagation. Dashed lines indicate language-only model baseline. For brevity we include both perceptual
input sources ESP and CSLB when comparing means of propagation; results with individual information
sources were similar.
</figureCaption>
<bodyText confidence="0.998228926829268">
to concrete nouns as previously.
Of our evaluation sets, it was possible to con-
struct from ESP (and add to PESP) representa-
tions for all of the concrete verbs, and for ap-
proximately half of the abstract verbs and abstract
nouns. Figure 4 (top), shows the performance of
a our model trained on all available perceptual in-
put versus the model in which the perceptual input
was restricted to concrete nouns.
The results reflect a clear manifestation of the
abstract/concrete distinction. Concrete verbs be-
have similarly to concrete nouns, in that they can
be effectively represented directly from perceptual
information sources. The information encoded in
these representations is beneficial to the model and
increases performance. In contrast, constructing
‘perceptual’ representations of abstract verbs and
abstract nouns directly from perceptual informa-
tion sources is clearly counter-productive (to the
extent that performance also degrades on the com-
bined sets all nouns and all verbs). It appears in
these cases that the perceptual input acts to ob-
scure or contradict the otherwise useful signal in-
ferred from the corpus.
As shown in the previous section, the inclusion
of any form of perceptual input inhibits the learn-
ing of abstract nouns. However, this is not the case
for abstract verbs. Our model learns higher qual-
ity representations of abstract verbs if perceptual
input is restricted to concrete nouns than if no per-
ceptual input is included at all and when percep-
tual input is included for both concrete nouns and
abstract verbs. This supports the idea of a grad-
ual scale of concreteness: The most concrete con-
cepts can be effectively represented directly in the
perceptual modality; somewhat more abstract con-
cepts cannot be represented directly in the percep-
tual modality, but have representations that are im-
proved by propagating perceptual input from con-
crete concepts via language; and the most abstract
concepts are best acquired via language alone.
</bodyText>
<subsectionHeader confidence="0.993159">
3.4 Source and quantity of perceptual input
</subsectionHeader>
<bodyText confidence="0.999994">
For different concept types, we tested the effect of
varying the proportion of perceptual to linguistic
input (the parameter α). Perceptual input was re-
stricted to concrete nouns as in Sections 3.1-3.2.
As shown in Figure 4, performance on concrete
nouns improves (albeit to a decreasing degree) as
α increases. When learning concrete noun rep-
resentations, linguistic input is apparently redun-
dant if perceptual input is of sufficient quality and
quantity. For the other concept types, in each case
there is an optimal value for α in the range .5–2,
above which perceptual input obscures the linguis-
tic signal and performance degrades. The prox-
imity of these optima to 1 suggests that for op-
timal learning, when a concrete concept is experi-
enced approximately equal weight should be given
to available perceptual and linguistic information.
</bodyText>
<sectionHeader confidence="0.998119" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999886125">
Motivated by the notable prevalence of abstract
concepts in everyday language, and their likely
importance to flexible, general-purpose represen-
tation learning, we have investigated how abstract
and concrete representations can be acquired by
multi-modal models. In doing so, we presented a
simple and easy-to-implement architecture for ac-
quiring semantic representations of both types of
</bodyText>
<page confidence="0.98994">
262
</page>
<figure confidence="0.998475333333333">
Concrete Nouns Abstract Nouns Concrete Verbs Abstract Verbs
Correlation
0.3
0.2
0.1
0.3
0.2
0.1
0.3
0.2
0.1
0.3
0.2
0.1
Perceptual Input
CSLB
ESP
CSLB &amp;
ESP
Text−only
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
α α α α
Our Model α = 1
Correlation
0.4
0.3
0.2
0.1
0.0
0.267
0.295
0.136
0.249
0.335
0.364
0.337
0.176
0.087
0.166
0.201
0.225
concrete abstract all nouns concrete abstract all verbs
nouns nouns verbs verbs
Concept Type
Perceptual Information Source Direct representation Propagation
</figure>
<figureCaption confidence="0.995886">
Figure 4: Top: Comparing the strategy of directly representing abstract concepts from perceptual in-
</figureCaption>
<bodyText confidence="0.979289576923077">
formation where available (yellow bars) vs. propagating via concrete concepts. Bottom: The effect of
increasing α on correlation with USF pairs (Spearman ρ) for each concept type. Horizontal dashed lines
indicate language-only model baseline.
concept from linguistic and perceptual input.
While neuro-probabilistic language models
have been applied to the problem of multi-modal
representation learning previously (Srivastava and
Salakhutdinov, 2012; Wu et al., 2013; Silberer and
Lapata, 2014) our model and experiments develop
this work in several important ways. First, we ad-
dress the problem of learning abstract concepts.
By isolating concepts of different concreteness
and part-of-speech in our evaluation sets, and sep-
arating the processes of information combination
and propagation, we demonstrate that the multi-
modal approach is indeed effective for some, but
perhaps not all, abstract concepts. In addition, our
model introduces a clear parallel with human lan-
guage learning. Perceptual input is introduced pre-
cisely when concrete concepts are ‘experienced’
by the model in the corpus text, much like a lan-
guage learner experiencing concrete entities via
sensory perception.
Taken together, our findings indicate the utility
of distinguishing three concept types when learn-
ing representations in the multi-modal setting.
Type I Concepts that can be effectively repre-
sented directly in the perceptual modality. For
such concepts, generally concrete nouns or con-
crete verbs, our proposed approach provides a sim-
ple means of combining perceptual and linguistic
input. The resulting multi-modal representations
are of higher quality than those learned via other
approaches, resulting in a performance improve-
ment of over 10% in modelling free association.
Type II Concepts, including abstract verbs, that
cannot be effectively represented directly in the
perceptual modality, but whose representations
can be improved by joint learning from linguis-
tic input and perceptual information about related
concepts. Our model can effectively propagate
perceptual input (exploiting the relations inferred
from the linguistic input) from Type I concepts to
enhance the representations of Type II concepts
above the language-only baseline. Because of the
frequency of abstract concepts, such propagation
extends the benefit of the multi-modal approach to
a far wider range of language than models based
solely in the concrete domain.
Type III Concepts that are more effectively
learned via language-only models than multi-
modal models, such as abstract nouns. Neither
</bodyText>
<page confidence="0.993994">
263
</page>
<bodyText confidence="0.999981666666667">
our proposed approach nor alternative propagation
methods achieve an improvement in representa-
tion quality for these concepts over the language-
only baseline. Of course, it is an empirical ques-
tion whether a multi-modal approach could ever
enhance the representation learning of these con-
cepts, one with potential implications for cognitive
theories of grounding (a topic of much debate in
psychology (Grafton, 2009; Barsalou, 2010)).
Additionally, we investigated the optimum type
and quantity of perceptual input for learning con-
cepts of different types. We showed that too much
perceptual input can result in degraded represen-
tations. For concepts of type I and II, the op-
timal quantity resulted from setting α = 1; i.e.
whenever a concrete concept was encountered, the
model learned from an equal number of language-
based and perception-based examples. While we
make no formal claims here, such observations
may ultimately provide insight into human lan-
guage learning and semantic memory.
In future we will address the question of
whether Type III concepts can ever be enhanced
via multi-modal learning, and investigate multi-
modal models that optimally learn concepts of
each type. This may involve filtering the percep-
tual input stream for concepts according to con-
creteness, and possibly more elaborate model ar-
chitectures that facilitate distinct representational
frameworks for abstract and concrete concepts.
</bodyText>
<sectionHeader confidence="0.994948" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999770666666667">
Thanks to the Royal Society and St John’s College
for supporting this research, and to Yoshua Bengio
and Diarmuid O´ S´eaghdha for helpful discussions.
</bodyText>
<sectionHeader confidence="0.998617" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999802123076923">
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological Review, 116(3):463.
Lawrence W Barsalou and Katja Wiemer-Hastings.
2005. Situating abstract concepts. Grounding Cog-
nition: The Role of Perception and Action in Mem-
ory, Language, and Thought, pages 129–163.
Lawrence W Barsalou. 2010. Grounded cognition:
past, present, and future. Topics in Cognitive Sci-
ence, 2(4):716–724.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 136–145. Asso-
ciation for Computational Linguistics.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1–47.
Joan L Bybee and Paul J Hopper. 2001. Frequency and
the Emergence of Linguistic Structure, volume 45.
John Benjamins Publishing.
Nick Chater and Christopher D Manning. 2006. Prob-
abilistic models of language processing and acquisi-
tion. Trends in Cognitive Sciences, 10(7):335–344.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, pages 160–167. ACM.
Sebastian J Crutch and Elizabeth K Warrington. 2005.
Abstract and concrete concepts have structurally
different representational frameworks. Brain,
128(3):615–627.
Barry J Devereux, Lorraine K Tyler, Jeroen Geertzen,
and Billi Randall. 2013. The centre for speech, lan-
guage and the brain (cslb) concept property norms.
Behavior Research Methods, pages 1–9.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 91–99. Asso-
ciation for Computational Linguistics.
Jerry A Fodor. 1983. The modularity of mind: An
essay on faculty psychology. MIT press.
Scott T Grafton. 2009. Embodied cognition and the
simulation of action to understand others. Annals of
the New York Academy of Sciences, 1156(1):97–117.
David R Hardoon, Sandor Szedmak, and John Shawe-
Taylor. 2004. Canonical correlation analysis:
An overview with application to learning methods.
Neural Computation, 16(12):2639–2664.
Felix Hill, Anna Korhonen, and Christian Bentz.
2013. A quantitative empirical analysis of the ab-
stract/concrete distinction. Cognitive Science.
Felix Hill, Roi Reichart, and Anna Korhonen. 2014.
Multi-modal models for abstract and concrete con-
cept semantics. Transactions of the Association for
Computational Linguistics.
</reference>
<page confidence="0.976544">
264
</page>
<reference confidence="0.999910896907217">
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873–882. Asso-
ciation for Computational Linguistics.
Brendan T Johns and Michael N Jones. 2012. Per-
ceptual inference through global lexical similarity.
Topics in Cognitive Science, 4(1):103–120.
Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen
Clark. 2014. Improving multi-modal representa-
tions using image dispersion: Why less is sometimes
more. In Proceedings of the annual meeting of the
Association for Computational Linguistics. ACL.
Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. Claws4: the tagging of the British National
Corpus. In Proceedings of the 15th conference
on Computational linguistics-Volume 1, pages 622–
628. Association for Computational Linguistics.
Ken McRae, George S Cree, Mark S Seidenberg, and
Chris McNorgan. 2005. Semantic feature pro-
duction norms for a large set of living and nonliv-
ing things. Behavior Research Methods, 37(4):547–
559.
Gr´egoire Mesnil, Yann Dauphin, Xavier Glorot, Salah
Rifai, Yoshua Bengio, Ian J Goodfellow, Erick
Lavoie, Xavier Muller, Guillaume Desjardins, David
Warde-Farley, et al. 2012. Unsupervised and trans-
fer learning challenge: a deep learning approach.
Journal of Machine Learning Research-Proceedings
Track, 27:97–110.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word repre-
sentations in vector space. In Proceedings of Inter-
national Conference of Learning Representations,
Scottsdale, Arizona, USA.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international Workshop on Arti-
ficial Intelligence and Statistics, pages 246–252.
Raymond H Myers. 1990. Classical and Modern
Regression with Applications, volume 2. Duxbury
Press Belmont, CA.
Douglas L Nelson, Cathy L McEvoy, and Thomas A
Schreiber. 2004. The University of South Florida
free association, rhyme, and word fragment norms.
Behavior Research Methods, Instruments, &amp; Com-
puters, 36(3):402–407.
Allan Paivio. 1991. Dual coding theory: Retrospect
and current status. Canadian Journal of Psychology,
45(3):255.
Stephen Roller and Sabine Schulte im Walde. 2013.
A multimodal LDA model integrating textual, cog-
nitive and visual modalities. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1146–1157, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423–1433. As-
sociation for Computational Linguistics.
Carina Silberer and Mirella Lapata. 2014. Learn-
ing grounded meaning representations with autoen-
coders. In Proceedings of the annual meeting of the
Association for Computational Linguistics. Associ-
ation for Computational Linguistics.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of semantic representation with vi-
sual attributes. In Proceedings of the 51th Annual
Meeting of the Association for Computational Lin-
guistics, Sofia, Bulgaria, August.
Nitish Srivastava and Ruslan Salakhutdinov. 2012.
Multimodal learning with deep boltzmann ma-
chines. In NIPS, pages 2231–2239.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394. Association for
Computational Linguistics.
Luis Von Ahn and Laura Dabbish. 2004. Labeling
images with a computer game. In Proceedings of the
SIGCHI conference on human factors in computing
systems, pages 319–326. ACM.
Katja Wiemer-Hastings and Xu Xu. 2005. Content
differences for abstract and concrete concepts. Cog-
nitive Science, 29(5):719–736.
Pengcheng Wu, Steven CH Hoi, Hao Xia, Peilin Zhao,
Dayong Wang, and Chunyan Miao. 2013. On-
line multimodal deep similarity learning with ap-
plication to image retrieval. In Proceedings of the
21st ACM International Conference on Multimedia,
pages 153–162. ACM.
</reference>
<page confidence="0.998453">
265
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.503261">
<title confidence="0.806879">Learning Abstract Concept Embeddings from Multi-Modal Since You Probably Can’t See What I Mean</title>
<author confidence="0.999753">Felix Hill Anna Korhonen</author>
<affiliation confidence="0.9999755">Computer Laboratory Computer Laboratory University of Cambridge University of Cambridge</affiliation>
<email confidence="0.994142">felix.hill@cl.cam.ac.ukanna.korhonen@cl.cam.ac.uk</email>
<abstract confidence="0.993223423076923">Models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in NLP because of the obvious parallels with human language learning. Performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts. However, such concepts are comparatively rare in everyday language. In this work, we present a new means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multimodal embeddings. Our architecture outperforms previous approaches in combining input from distinct modalities, and propagates perceptual information on concrete concepts to abstract concepts more effectively than alternatives. We discuss the implications of our results both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mark Andrews</author>
<author>Gabriella Vigliocco</author>
<author>David Vinson</author>
</authors>
<title>Integrating experiential and distributional data to learn semantic representations.</title>
<date>2009</date>
<journal>Psychological Review,</journal>
<volume>116</volume>
<issue>3</issue>
<contexts>
<context position="1484" citStr="Andrews et al., 2009" startWordPosition="206" endWordPosition="209">modal embeddings. Our architecture outperforms previous approaches in combining input from distinct modalities, and propagates perceptual information on concrete concepts to abstract concepts more effectively than alternatives. We discuss the implications of our results both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation. 1 Introduction Multi-modal models that learn semantic representations from both language and information about the perceptible properties of concepts were originally motivated by parallels with human word learning (Andrews et al., 2009) and evidence that many concepts are grounded in perception (Barsalou and Wiemer-Hastings, 2005). The perceptual information in such models is generally mined directly from images (Feng and Lapata, 2010; Bruni et al., 2012) or from data collected in psychological studies (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). By exploiting the additional information encoded in perceptual input, multi-modal models can outperform language-only models on a range of semantic NLP tasks, including modelling similarity (Bruni et al., 2014; Kiela et al., 2014) and free association (Silberer an</context>
<context position="18235" citStr="Andrews et al., 2009" startWordPosition="2883" endWordPosition="2886">rs in the CSLB dataset (with feature strength, max=30). Table 2: Example concept pairs (with mean concreteness rating) and free-association scores from the USF dataset. Wikipedia text, split into sentences and with punctuation removed. 2.2 Evaluation We evaluate the quality of representations by how well they reflect free association scores, an empirical measure of cognitive conceptual proximity. The University of South Florida Norms (USF) (Nelson et al., 2004) contain free association scores for over 40,000 concept pairs, and have been widely used in NLP to evaluate semantic representations (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Each concept that we extract from the USF database has also been rated for conceptual concreteness on a Likert scale of 1-7 by at least 10 human annotators. Following previous studies (Huang et al., 2012; Silberer and Lapata, 2012), we measure the (Spearman p) correlation between association scores and the cosine similarity of vector representations. We create separate abstract and concrete concept lists by ranking the USF concepts according to concreteness and sampling at random from the first and fourth q</context>
</contexts>
<marker>Andrews, Vigliocco, Vinson, 2009</marker>
<rawString>Mark Andrews, Gabriella Vigliocco, and David Vinson. 2009. Integrating experiential and distributional data to learn semantic representations. Psychological Review, 116(3):463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence W Barsalou</author>
<author>Katja Wiemer-Hastings</author>
</authors>
<title>Situating abstract concepts. Grounding Cognition: The Role of Perception and Action in Memory, Language, and Thought,</title>
<date>2005</date>
<pages>129--163</pages>
<contexts>
<context position="1580" citStr="Barsalou and Wiemer-Hastings, 2005" startWordPosition="219" endWordPosition="223">put from distinct modalities, and propagates perceptual information on concrete concepts to abstract concepts more effectively than alternatives. We discuss the implications of our results both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation. 1 Introduction Multi-modal models that learn semantic representations from both language and information about the perceptible properties of concepts were originally motivated by parallels with human word learning (Andrews et al., 2009) and evidence that many concepts are grounded in perception (Barsalou and Wiemer-Hastings, 2005). The perceptual information in such models is generally mined directly from images (Feng and Lapata, 2010; Bruni et al., 2012) or from data collected in psychological studies (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). By exploiting the additional information encoded in perceptual input, multi-modal models can outperform language-only models on a range of semantic NLP tasks, including modelling similarity (Bruni et al., 2014; Kiela et al., 2014) and free association (Silberer and Lapata, 2012), predicting compositionality (Roller and Schulte im Walde, 2013) and concept cat</context>
</contexts>
<marker>Barsalou, Wiemer-Hastings, 2005</marker>
<rawString>Lawrence W Barsalou and Katja Wiemer-Hastings. 2005. Situating abstract concepts. Grounding Cognition: The Role of Perception and Action in Memory, Language, and Thought, pages 129–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence W Barsalou</author>
</authors>
<title>Grounded cognition: past, present, and future.</title>
<date>2010</date>
<journal>Topics in Cognitive Science,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="37808" citStr="Barsalou, 2010" startWordPosition="5952" endWordPosition="5953">an models based solely in the concrete domain. Type III Concepts that are more effectively learned via language-only models than multimodal models, such as abstract nouns. Neither 263 our proposed approach nor alternative propagation methods achieve an improvement in representation quality for these concepts over the languageonly baseline. Of course, it is an empirical question whether a multi-modal approach could ever enhance the representation learning of these concepts, one with potential implications for cognitive theories of grounding (a topic of much debate in psychology (Grafton, 2009; Barsalou, 2010)). Additionally, we investigated the optimum type and quantity of perceptual input for learning concepts of different types. We showed that too much perceptual input can result in degraded representations. For concepts of type I and II, the optimal quantity resulted from setting α = 1; i.e. whenever a concrete concept was encountered, the model learned from an equal number of languagebased and perception-based examples. While we make no formal claims here, such observations may ultimately provide insight into human language learning and semantic memory. In future we will address the question o</context>
</contexts>
<marker>Barsalou, 2010</marker>
<rawString>Lawrence W Barsalou. 2010. Grounded cognition: past, present, and future. Topics in Cognitive Science, 2(4):716–724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>NamKhanh Tran</author>
</authors>
<title>Distributional semantics in technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>136--145</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1707" citStr="Bruni et al., 2012" startWordPosition="241" endWordPosition="244">s. We discuss the implications of our results both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation. 1 Introduction Multi-modal models that learn semantic representations from both language and information about the perceptible properties of concepts were originally motivated by parallels with human word learning (Andrews et al., 2009) and evidence that many concepts are grounded in perception (Barsalou and Wiemer-Hastings, 2005). The perceptual information in such models is generally mined directly from images (Feng and Lapata, 2010; Bruni et al., 2012) or from data collected in psychological studies (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). By exploiting the additional information encoded in perceptual input, multi-modal models can outperform language-only models on a range of semantic NLP tasks, including modelling similarity (Bruni et al., 2014; Kiela et al., 2014) and free association (Silberer and Lapata, 2012), predicting compositionality (Roller and Schulte im Walde, 2013) and concept categorization (Silberer and Lapata, 2014). However, to date, these previous approaches to multi-modal concept learning focus on c</context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Elia Bruni, Gemma Boleda, Marco Baroni, and NamKhanh Tran. 2012. Distributional semantics in technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 136–145. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>49--1</pages>
<contexts>
<context position="2029" citStr="Bruni et al., 2014" startWordPosition="290" endWordPosition="293">ly motivated by parallels with human word learning (Andrews et al., 2009) and evidence that many concepts are grounded in perception (Barsalou and Wiemer-Hastings, 2005). The perceptual information in such models is generally mined directly from images (Feng and Lapata, 2010; Bruni et al., 2012) or from data collected in psychological studies (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). By exploiting the additional information encoded in perceptual input, multi-modal models can outperform language-only models on a range of semantic NLP tasks, including modelling similarity (Bruni et al., 2014; Kiela et al., 2014) and free association (Silberer and Lapata, 2012), predicting compositionality (Roller and Schulte im Walde, 2013) and concept categorization (Silberer and Lapata, 2014). However, to date, these previous approaches to multi-modal concept learning focus on concrete words such as cat or dog, rather than abstract concepts, such as curiosity or loyalty. However, differences between abstract and concrete processing and representation (Paivio, 1991; Hill et al., 2013; Kiela et al., 2014) suggest that conclusions about concrete concept learning may not necessarily hold in the gen</context>
<context position="21407" citStr="Bruni et al. (2014)" startWordPosition="3382" endWordPosition="3385">ings learned nose small Concept 1 Assoc. Concept 2 abdomen (6.83) stomach (6.04) 0.566 throw (4.05) hope (1.18) egg (5.79) ball (6.08) glory (3.53) milk (6.66) 0.234 0.192 0.012 259 by the Mikolov et al. (2013) model on the Text8 Corpus. In the second alternative (proposed for multi-modal models by Silberer and Lapata (2012)), canonical correlation analysis (CCA) (Hardoon et al., 2004) was applied to the vectors of both modalities. CCA yields reduceddimensionality representations that preserve underlying inter-modal correlations, which are then concatenated. The final alternative, proposed by Bruni et al. (2014) involves applying Singular Value Decomposition (SVD) to the matrix of concatenated multi-modal representations, yielding smoothed representations.3 When implementing the concatenation, CCA and SVD methods, we first encoded the perceptual input directly into sparse feature vectors, with coordinates for each of the 2726 features in CSLB and for each of the 100,000 images in ESP. This sparse encoding matches the approach taken by Silberer and Lapata (2012), for CCA and concatenation, and by Hill et al. (2014) for the ridge regression method of propagation (see below). We compare these alternativ</context>
<context position="23221" citStr="Bruni et al. (2014)" startWordPosition="3667" endWordPosition="3670">tance, the correlation is 11% higher that the next best alternative method, CCA. One possible factor behind this improvement is that, in our model, the learned representations fully integrate the two modalities, whereas for both CCA and the concatenation method each representation feature (whether of reduced dimension or not) corresponds to a particular modality. This deeper integration may help our architecture to overcome the challenges inherent in information combination such as inter-modality differences in information content and representation sparsity. It is also important to note that Bruni et al. (2014) ap3CCA was implemented using the CCA package in R. SVD was implemented using SVDLIBC (http:// tedlab.mit.edu/˜dr/SVDLIBC/), with truncation factor k = 1024 as per (Bruni et al., 2014). plied their SVD method with comparatively dense perceptual representations extracted from images, whereas our dataset-based perceptual vectors were sparsely-encoded. 3.2 Propagating input to abstract concepts To test the process of information propagation in our model, we evaluated the learned embeddings of more abstract concepts. We compared our approach with two recently-proposed alternative methods for infer</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. Journal of Artificial Intelligence Research, 49:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan L Bybee</author>
<author>Paul J Hopper</author>
</authors>
<title>Frequency and the Emergence of Linguistic Structure,</title>
<date>2001</date>
<volume>45</volume>
<publisher>John Benjamins Publishing.</publisher>
<contexts>
<context position="11521" citStr="Bybee and Hopper, 2001" startWordPosition="1766" endWordPosition="1770">ct ˆrc · rw, the updates have the effect of moving each target-embedding incrementally ‘closer’ to the context-embeddings of its collocates. In the target-embedding space, this results in embeddings of concept words that regularly occur in similar contexts moving closer together. Multi-modal Extension We extend the Mikolov et al. (2013) architecture via a simple means of introducing perceptual information that aligns with human language learning. Based on the assumption that frequency in domain-general linguistic corpora correlates with the likelihood of ‘experiencing’ a concept in the world (Bybee and Hopper, 2001; Chater and Manning, 2006), perceptual information is introduced to the model whenever designated concrete concepts are encountered in the running-text linguistic input. This has the effect of introducing more perceptual input for commonly experienced concrete concepts and less input for rarer concrete concepts. To implement this process, perceptual information is extracted from external sources and encoded in an associative array P, which maps (typically concrete) words w to bags of perceptual features b(w). The construction of this array depends on the perceptual information source; the pro</context>
</contexts>
<marker>Bybee, Hopper, 2001</marker>
<rawString>Joan L Bybee and Paul J Hopper. 2001. Frequency and the Emergence of Linguistic Structure, volume 45. John Benjamins Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Chater</author>
<author>Christopher D Manning</author>
</authors>
<title>Probabilistic models of language processing and acquisition.</title>
<date>2006</date>
<booktitle>Trends in Cognitive Sciences,</booktitle>
<volume>10</volume>
<issue>7</issue>
<contexts>
<context position="11548" citStr="Chater and Manning, 2006" startWordPosition="1771" endWordPosition="1774"> have the effect of moving each target-embedding incrementally ‘closer’ to the context-embeddings of its collocates. In the target-embedding space, this results in embeddings of concept words that regularly occur in similar contexts moving closer together. Multi-modal Extension We extend the Mikolov et al. (2013) architecture via a simple means of introducing perceptual information that aligns with human language learning. Based on the assumption that frequency in domain-general linguistic corpora correlates with the likelihood of ‘experiencing’ a concept in the world (Bybee and Hopper, 2001; Chater and Manning, 2006), perceptual information is introduced to the model whenever designated concrete concepts are encountered in the running-text linguistic input. This has the effect of introducing more perceptual input for commonly experienced concrete concepts and less input for rarer concrete concepts. To implement this process, perceptual information is extracted from external sources and encoded in an associative array P, which maps (typically concrete) words w to bags of perceptual features b(w). The construction of this array depends on the perceptual information source; the process for our chosen sources</context>
</contexts>
<marker>Chater, Manning, 2006</marker>
<rawString>Nick Chater and Christopher D Manning. 2006. Probabilistic models of language processing and acquisition. Trends in Cognitive Sciences, 10(7):335–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3620" citStr="Collobert and Weston, 2008" startWordPosition="534" endWordPosition="537"> rated by human judges1 as more abstract than the noun war, for instance, a concept many would already consider to be quite abstract. Moreover, abstract concepts by definition encode higher-level (more general) principles than concrete concepts, which typically reside naturally in a single semantic category or domain (Crutch and Warrington, 2005). It is therefore likely that abstract representations may prove highly applicable for multi-task, multi-domain or transfer learning models, which aim to acquire ‘general-purpose’ conceptual knowledge without reference to a specific objective or task (Collobert and Weston, 2008; Mesnil et al., 2012). In a recent paper, Hill et al. (2014) investigate whether the multi-modal models cited above are 1Contributors to the USF dataset (Nelson et al., 2004). 255 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 255–265, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics effective for learning concepts other than concrete nouns. They observe that representations of certain abstract concepts can indeed be enhanced in multi-modal models by combining perceptual and linguistic input with an infor</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian J Crutch</author>
<author>Elizabeth K Warrington</author>
</authors>
<title>Abstract and concrete concepts have structurally different representational frameworks.</title>
<date>2005</date>
<journal>Brain,</journal>
<volume>128</volume>
<issue>3</issue>
<contexts>
<context position="3342" citStr="Crutch and Warrington, 2005" startWordPosition="496" endWordPosition="499">abstract and concrete concepts. Although concrete concepts might seem more basic or fundamental, the vast majority of openclass, meaning-bearing words in everyday language are in fact abstract. 72% of the noun or verb tokens in the British National Corpus (Leech et al., 1994) are rated by human judges1 as more abstract than the noun war, for instance, a concept many would already consider to be quite abstract. Moreover, abstract concepts by definition encode higher-level (more general) principles than concrete concepts, which typically reside naturally in a single semantic category or domain (Crutch and Warrington, 2005). It is therefore likely that abstract representations may prove highly applicable for multi-task, multi-domain or transfer learning models, which aim to acquire ‘general-purpose’ conceptual knowledge without reference to a specific objective or task (Collobert and Weston, 2008; Mesnil et al., 2012). In a recent paper, Hill et al. (2014) investigate whether the multi-modal models cited above are 1Contributors to the USF dataset (Nelson et al., 2004). 255 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 255–265, October 25-29, 2014, Doha, Qat</context>
</contexts>
<marker>Crutch, Warrington, 2005</marker>
<rawString>Sebastian J Crutch and Elizabeth K Warrington. 2005. Abstract and concrete concepts have structurally different representational frameworks. Brain, 128(3):615–627.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barry J Devereux</author>
<author>Lorraine K Tyler</author>
<author>Jeroen Geertzen</author>
<author>Billi Randall</author>
</authors>
<title>The centre for speech, language and the brain (cslb) concept property norms. Behavior Research Methods,</title>
<date>2013</date>
<pages>1--9</pages>
<contexts>
<context position="5809" citStr="Devereux et al., 2013" startWordPosition="855" endWordPosition="858"> The model moderates the training input to include more perceptual information about commonly-occurring concrete concepts and less information about rarer concepts. Moreover, it integrates the processes of combining perceptual and linguistic input and propagating information from concrete to abstract concepts into a single representation update process based on back-propagation. We train our model on running-text language and two sources of perceptual descriptors for concrete nouns: the ESPGame dataset of annotated images (Von Ahn and Dabbish, 2004) and the CSLB set of concept property norms (Devereux et al., 2013). We find that our model combines information from the different modalities more effectively than previous methods, resulting in an improved ability to model the USF free association gold standard (Nelson et al., 2004) for concrete nouns. In addition, the architecture propagates the extra-linguistic input for concrete nouns to improve representations of abstract concepts more effectively than alternative methods. While this propagation can effectively extend the advantage of the multi-modal approach to many more concepts than simple concrete nouns, we observe that the benefit of adding percept</context>
<context position="15321" citStr="Devereux et al., 2013" startWordPosition="2393" endWordPosition="2396">et (ESP) (Von Ahn and Dabbish, 2004) consists of 100,000 images, each annotated with a list of lexical concepts that appear in that image. For any concept w identified in an ESP image, we construct a corresponding bag of features b(w). For each ESP image I that contains w, we append the other concept tokens identified in I to b(w). Thus, the more frequently a concept cooccurs with w in images, the more its corresponding lexical token occurs in b(w). The array PESP in this case then consists of the (w, b(w)) pairs. CSLB Property Norms The Centre for Speech, Language and the Brain norms (CSLB) (Devereux et al., 2013) is a recently-released dataset containing semantic properties for 638 concrete concepts produced by human annotators. The CSLB dataset was compiled in the same way as the McRae et al. (2005) property norms used widely in multimodal models (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013); we use CSLB because it contains more concepts. For each concept, the proportion of the 30 annotators that produced a given feature can also be employed as a measure of the strength of that feature. When encoding the CSLB data in P, we first map properties to lexical forms (e.g. is green becomes </context>
</contexts>
<marker>Devereux, Tyler, Geertzen, Randall, 2013</marker>
<rawString>Barry J Devereux, Lorraine K Tyler, Jeroen Geertzen, and Billi Randall. 2013. The centre for speech, language and the brain (cslb) concept property norms. Behavior Research Methods, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="10351" citStr="Duchi et al., 2011" startWordPosition="1585" endWordPosition="1588">ords for that target, and (ii) words closer to the target are more likely to be selected than those further away. The training objective is then to maximize the sum of the log probabilities T across of all such examples from S and across all sentences in the corpus, where T is defined as follows: log(p(wn+j|wn)) −t(n)&lt;j&lt;t(n),j7W The model free parameters (target-embeddings and context-embeddings of dimension d for each word in the corpus with frequency above a certain threshold f) are updated according to stochastic gradient descent and backpropation, with learning rate controlled by Adagrad (Duchi et al., 2011). For efficiency, the output layer is encoded as a hierarchical softmax function based on a binary Huffman tree (Morin and Bengio, 2005). As with other distributional architectures, the model captures conceptual semantics by exploiting the fact that words appearing in similar linguistic contexts are likely to have similar meanings. Informally, the model adjusts its embeddings to increase the ‘probability’ of seeing the language in the training corpus. Since this probability increases with the p(c|w), and the p(c|w) increase with the dot product ˆrc · rw, the updates have the effect of moving e</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Visual information in semantic representation.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>91--99</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1686" citStr="Feng and Lapata, 2010" startWordPosition="237" endWordPosition="240">tively than alternatives. We discuss the implications of our results both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation. 1 Introduction Multi-modal models that learn semantic representations from both language and information about the perceptible properties of concepts were originally motivated by parallels with human word learning (Andrews et al., 2009) and evidence that many concepts are grounded in perception (Barsalou and Wiemer-Hastings, 2005). The perceptual information in such models is generally mined directly from images (Feng and Lapata, 2010; Bruni et al., 2012) or from data collected in psychological studies (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). By exploiting the additional information encoded in perceptual input, multi-modal models can outperform language-only models on a range of semantic NLP tasks, including modelling similarity (Bruni et al., 2014; Kiela et al., 2014) and free association (Silberer and Lapata, 2012), predicting compositionality (Roller and Schulte im Walde, 2013) and concept categorization (Silberer and Lapata, 2014). However, to date, these previous approaches to multi-modal concep</context>
<context position="18258" citStr="Feng and Lapata, 2010" startWordPosition="2887" endWordPosition="2891"> (with feature strength, max=30). Table 2: Example concept pairs (with mean concreteness rating) and free-association scores from the USF dataset. Wikipedia text, split into sentences and with punctuation removed. 2.2 Evaluation We evaluate the quality of representations by how well they reflect free association scores, an empirical measure of cognitive conceptual proximity. The University of South Florida Norms (USF) (Nelson et al., 2004) contain free association scores for over 40,000 concept pairs, and have been widely used in NLP to evaluate semantic representations (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Each concept that we extract from the USF database has also been rated for conceptual concreteness on a Likert scale of 1-7 by at least 10 human annotators. Following previous studies (Huang et al., 2012; Silberer and Lapata, 2012), we measure the (Spearman p) correlation between association scores and the cosine similarity of vector representations. We create separate abstract and concrete concept lists by ranking the USF concepts according to concreteness and sampling at random from the first and fourth quartiles respectively. </context>
<context position="27134" citStr="Feng and Lapata, 2010" startWordPosition="4294" endWordPosition="4297">tations. As with the implementation of our model, the perceptual input for these alternative models was limited to concrete nouns (i.e. concrete nouns were the only bi-modal concepts in the models). Figure 3 (right side) shows the propagation performance of the three models. While the correlations overall may seem somewhat low, this is a consequence of the difficulty of modelling the USF data. In fact, the performance of both the language-only model and our multi-modal extension across the concept types (from 0.18 to 0.36) is equal to or higher than previous models evaluated on the same data (Feng and Lapata, 2010; Silberer and Lapata, 2012; Silberer et al., 2013). For learning representations of concrete verbs, our approach achieves a 69% increase in performance over the next best alternative. The performance of the model on abstract verbs is marginally inferior to Johns and Jones’ method. Nevertheless, the clear advantage for concrete verbs makes our model the best choice for learning representations of verbs in general, as shown by performance on the set all verbs, which also includes mixed abstract-concrete pairs. Our model is also marginally inferior to alternative approaches in learning represent</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010. Visual information in semantic representation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 91–99. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry A Fodor</author>
</authors>
<title>The modularity of mind: An essay on faculty psychology.</title>
<date>1983</date>
<publisher>MIT press.</publisher>
<contexts>
<context position="16305" citStr="Fodor, 1983" startWordPosition="2559" endWordPosition="2560"> proportion of the 30 annotators that produced a given feature can also be employed as a measure of the strength of that feature. When encoding the CSLB data in P, we first map properties to lexical forms (e.g. is green becomes green). By directly identifying perceptual features and linguistic forms in this way, we treat features observed in the perceptual data as (sub)concepts to be acquired via the same multi-modal input streams and stored in the same domain-general memory as the evaluation concepts. This design decision in fact corresponds to a view of cognition that is sometimes disputed (Fodor, 1983). In future studies we hope to compare the present approach to architectures with domain-specific conceptual memories. For each concept w in CSLB, we then construct a feature bag b(w) by appending lexical forms to b(w) such that the count of each feature word is equal to the strength of that feature for w. Thus, when features are sampled from b(w) to create pseudo-sentences (as detailed previously) the probability of a feature word occurring in a sentence reflects feature strength. The array PCSLB then consists of all (w, b(w)) pairs. Linguistic Input The linguistic input to all models is the </context>
</contexts>
<marker>Fodor, 1983</marker>
<rawString>Jerry A Fodor. 1983. The modularity of mind: An essay on faculty psychology. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott T Grafton</author>
</authors>
<title>Embodied cognition and the simulation of action to understand others.</title>
<date>2009</date>
<journal>Annals of the New York Academy of Sciences,</journal>
<volume>1156</volume>
<issue>1</issue>
<contexts>
<context position="37791" citStr="Grafton, 2009" startWordPosition="5950" endWordPosition="5951"> of language than models based solely in the concrete domain. Type III Concepts that are more effectively learned via language-only models than multimodal models, such as abstract nouns. Neither 263 our proposed approach nor alternative propagation methods achieve an improvement in representation quality for these concepts over the languageonly baseline. Of course, it is an empirical question whether a multi-modal approach could ever enhance the representation learning of these concepts, one with potential implications for cognitive theories of grounding (a topic of much debate in psychology (Grafton, 2009; Barsalou, 2010)). Additionally, we investigated the optimum type and quantity of perceptual input for learning concepts of different types. We showed that too much perceptual input can result in degraded representations. For concepts of type I and II, the optimal quantity resulted from setting α = 1; i.e. whenever a concrete concept was encountered, the model learned from an equal number of languagebased and perception-based examples. While we make no formal claims here, such observations may ultimately provide insight into human language learning and semantic memory. In future we will addre</context>
</contexts>
<marker>Grafton, 2009</marker>
<rawString>Scott T Grafton. 2009. Embodied cognition and the simulation of action to understand others. Annals of the New York Academy of Sciences, 1156(1):97–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Hardoon</author>
<author>Sandor Szedmak</author>
<author>John ShaweTaylor</author>
</authors>
<title>Canonical correlation analysis: An overview with application to learning methods.</title>
<date>2004</date>
<journal>Neural Computation,</journal>
<volume>16</volume>
<issue>12</issue>
<contexts>
<context position="21176" citStr="Hardoon et al., 2004" startWordPosition="3349" endWordPosition="3352">od of information combination we compared its performance on the concrete noun evaluation set against three alternative methods. The first alternative is simple concatenation of these perceptual vectors with linguistic vectors embeddings learned nose small Concept 1 Assoc. Concept 2 abdomen (6.83) stomach (6.04) 0.566 throw (4.05) hope (1.18) egg (5.79) ball (6.08) glory (3.53) milk (6.66) 0.234 0.192 0.012 259 by the Mikolov et al. (2013) model on the Text8 Corpus. In the second alternative (proposed for multi-modal models by Silberer and Lapata (2012)), canonical correlation analysis (CCA) (Hardoon et al., 2004) was applied to the vectors of both modalities. CCA yields reduceddimensionality representations that preserve underlying inter-modal correlations, which are then concatenated. The final alternative, proposed by Bruni et al. (2014) involves applying Singular Value Decomposition (SVD) to the matrix of concatenated multi-modal representations, yielding smoothed representations.3 When implementing the concatenation, CCA and SVD methods, we first encoded the perceptual input directly into sparse feature vectors, with coordinates for each of the 2726 features in CSLB and for each of the 100,000 ima</context>
</contexts>
<marker>Hardoon, Szedmak, ShaweTaylor, 2004</marker>
<rawString>David R Hardoon, Sandor Szedmak, and John ShaweTaylor. 2004. Canonical correlation analysis: An overview with application to learning methods. Neural Computation, 16(12):2639–2664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Anna Korhonen</author>
<author>Christian Bentz</author>
</authors>
<title>A quantitative empirical analysis of the abstract/concrete distinction. Cognitive Science.</title>
<date>2013</date>
<contexts>
<context position="2515" citStr="Hill et al., 2013" startWordPosition="362" endWordPosition="365">dal models can outperform language-only models on a range of semantic NLP tasks, including modelling similarity (Bruni et al., 2014; Kiela et al., 2014) and free association (Silberer and Lapata, 2012), predicting compositionality (Roller and Schulte im Walde, 2013) and concept categorization (Silberer and Lapata, 2014). However, to date, these previous approaches to multi-modal concept learning focus on concrete words such as cat or dog, rather than abstract concepts, such as curiosity or loyalty. However, differences between abstract and concrete processing and representation (Paivio, 1991; Hill et al., 2013; Kiela et al., 2014) suggest that conclusions about concrete concept learning may not necessarily hold in the general case. In this paper, we therefore focus on multi-modal models for learning both abstract and concrete concepts. Although concrete concepts might seem more basic or fundamental, the vast majority of openclass, meaning-bearing words in everyday language are in fact abstract. 72% of the noun or verb tokens in the British National Corpus (Leech et al., 1994) are rated by human judges1 as more abstract than the noun war, for instance, a concept many would already consider to be qui</context>
</contexts>
<marker>Hill, Korhonen, Bentz, 2013</marker>
<rawString>Felix Hill, Anna Korhonen, and Christian Bentz. 2013. A quantitative empirical analysis of the abstract/concrete distinction. Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>Multi-modal models for abstract and concrete concept semantics. Transactions of the Association for Computational Linguistics.</title>
<date>2014</date>
<contexts>
<context position="3681" citStr="Hill et al. (2014)" startWordPosition="546" endWordPosition="549">nce, a concept many would already consider to be quite abstract. Moreover, abstract concepts by definition encode higher-level (more general) principles than concrete concepts, which typically reside naturally in a single semantic category or domain (Crutch and Warrington, 2005). It is therefore likely that abstract representations may prove highly applicable for multi-task, multi-domain or transfer learning models, which aim to acquire ‘general-purpose’ conceptual knowledge without reference to a specific objective or task (Collobert and Weston, 2008; Mesnil et al., 2012). In a recent paper, Hill et al. (2014) investigate whether the multi-modal models cited above are 1Contributors to the USF dataset (Nelson et al., 2004). 255 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 255–265, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics effective for learning concepts other than concrete nouns. They observe that representations of certain abstract concepts can indeed be enhanced in multi-modal models by combining perceptual and linguistic input with an information propagation step. Hill et al. (2014) propose ridge reg</context>
<context position="21919" citStr="Hill et al. (2014)" startWordPosition="3461" endWordPosition="3464">er-modal correlations, which are then concatenated. The final alternative, proposed by Bruni et al. (2014) involves applying Singular Value Decomposition (SVD) to the matrix of concatenated multi-modal representations, yielding smoothed representations.3 When implementing the concatenation, CCA and SVD methods, we first encoded the perceptual input directly into sparse feature vectors, with coordinates for each of the 2726 features in CSLB and for each of the 100,000 images in ESP. This sparse encoding matches the approach taken by Silberer and Lapata (2012), for CCA and concatenation, and by Hill et al. (2014) for the ridge regression method of propagation (see below). We compare these alternatives to our proposed model with α = 1. In The CSLB and ESP models, all training pseudo-sentences are generated from the arrays PCSLB and PESP respectively. In the models classed as CSLB&amp;ESP, a random choice between PCSLB and PESP is made every time perceptual input is included (so that the overall quantity of perceptual information is the same). As shown in Figure 2 (left side), the embeddings learned by our model achieve a higher correlation with the USF data than simple concatenation, CCA and SVD regardless</context>
<context position="25160" citStr="Hill et al. (2014)" startWordPosition="3968" endWordPosition="3971"> perceptual representations for c and k respectively, and cl and kl the linguistic representations. The exponent parameter A reflects the learning rate. In step two, the initial quasi-perceptual representations are inferred for a second time, but with the weighted average calculated over the perceptual or initial quasi-perceptual representations of all other words, not just those that were originally bi-modal. As with Johns and Jones (2012), we set the learning rate parameter A to be 3 in the first step and 13 in the second. Ridge Regression An alternative, proposed for the present purpose by Hill et al. (2014), uses ridge regression (Myers, 1990). Ridge regression is a variant of least squares regression in which a regularization term is added to the training objective to favor solutions with certain properties. For bi-modal concepts of dimension np, we apply ridge regression to learn np linear functions 260 fi : Rnl —* R that map the linguistic representations (of dimension nl) to a particular perceptual feature i. These functions are then applied together to map the linguistic representations of uni-modal concepts to full quasi-perceptual representations. Following Hill et al. (2014), we take the</context>
</contexts>
<marker>Hill, Reichart, Korhonen, 2014</marker>
<rawString>Felix Hill, Roi Reichart, and Anna Korhonen. 2014. Multi-modal models for abstract and concrete concept semantics. Transactions of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>873--882</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="18526" citStr="Huang et al., 2012" startWordPosition="2933" endWordPosition="2936"> by how well they reflect free association scores, an empirical measure of cognitive conceptual proximity. The University of South Florida Norms (USF) (Nelson et al., 2004) contain free association scores for over 40,000 concept pairs, and have been widely used in NLP to evaluate semantic representations (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Each concept that we extract from the USF database has also been rated for conceptual concreteness on a Likert scale of 1-7 by at least 10 human annotators. Following previous studies (Huang et al., 2012; Silberer and Lapata, 2012), we measure the (Spearman p) correlation between association scores and the cosine similarity of vector representations. We create separate abstract and concrete concept lists by ranking the USF concepts according to concreteness and sampling at random from the first and fourth quartiles respectively. We also introduce a complementary noun/verb dichotomy, Table 3: Details the subsets of USF data used in our evaluations, downloadable from our website. on the intuition that information propagation may occur differently from noun to noun or from noun to verb (because </context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 873–882. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan T Johns</author>
<author>Michael N Jones</author>
</authors>
<title>Perceptual inference through global lexical similarity.</title>
<date>2012</date>
<journal>Topics in Cognitive Science,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="4376" citStr="Johns and Jones (2012)" startWordPosition="644" endWordPosition="647"> to the USF dataset (Nelson et al., 2004). 255 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 255–265, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics effective for learning concepts other than concrete nouns. They observe that representations of certain abstract concepts can indeed be enhanced in multi-modal models by combining perceptual and linguistic input with an information propagation step. Hill et al. (2014) propose ridge regression as an alternative to the nearest-neighbour averaging proposed by Johns and Jones (2012) for such propagation, and show that it is more robust to changes in the type of concept to be learned. However, both methods are somewhat inelegant, in that they learn separate linguistic and ‘pseudo-perceptual’ representations, which must be combined via a separate information combination step. Moreover, for the majority of abstract concepts, the best performing multi-modal model employing these techniques remains less effective than conventional text-only representation learning model. Motivated by these observations, we introduce an architecture for learning both abstract and concrete repr</context>
<context position="23954" citStr="Johns and Jones (2012)" startWordPosition="3771" endWordPosition="3774">dr/SVDLIBC/), with truncation factor k = 1024 as per (Bruni et al., 2014). plied their SVD method with comparatively dense perceptual representations extracted from images, whereas our dataset-based perceptual vectors were sparsely-encoded. 3.2 Propagating input to abstract concepts To test the process of information propagation in our model, we evaluated the learned embeddings of more abstract concepts. We compared our approach with two recently-proposed alternative methods for inferring perceptual features when explicit perceptual information is unavailable. Johns and Jones In the method of Johns and Jones (2012), pseudo-perceptual representations for target concepts without a perceptual representations (uni-modal concepts) are inferred as a weighted average of the perceptual representations of concepts that do have such a representation (bimodal concepts). In the first step of their two-step method, for each uni-modal concept k, a quasi-perceptual representation is computed as an average of the perceptual representations of bi-modal concepts, weighted by the proximity between each of these concepts and k �kp = S(kl, cl)λ · cp c∈ C where C is the set of bi-modal concepts, cp and kp are the perceptual </context>
</contexts>
<marker>Johns, Jones, 2012</marker>
<rawString>Brendan T Johns and Michael N Jones. 2012. Perceptual inference through global lexical similarity. Topics in Cognitive Science, 4(1):103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>Felix Hill</author>
<author>Anna Korhonen</author>
<author>Stephen Clark</author>
</authors>
<title>Improving multi-modal representations using image dispersion: Why less is sometimes more.</title>
<date>2014</date>
<booktitle>In Proceedings of the annual meeting of the Association for Computational Linguistics. ACL.</booktitle>
<contexts>
<context position="2050" citStr="Kiela et al., 2014" startWordPosition="294" endWordPosition="297">llels with human word learning (Andrews et al., 2009) and evidence that many concepts are grounded in perception (Barsalou and Wiemer-Hastings, 2005). The perceptual information in such models is generally mined directly from images (Feng and Lapata, 2010; Bruni et al., 2012) or from data collected in psychological studies (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). By exploiting the additional information encoded in perceptual input, multi-modal models can outperform language-only models on a range of semantic NLP tasks, including modelling similarity (Bruni et al., 2014; Kiela et al., 2014) and free association (Silberer and Lapata, 2012), predicting compositionality (Roller and Schulte im Walde, 2013) and concept categorization (Silberer and Lapata, 2014). However, to date, these previous approaches to multi-modal concept learning focus on concrete words such as cat or dog, rather than abstract concepts, such as curiosity or loyalty. However, differences between abstract and concrete processing and representation (Paivio, 1991; Hill et al., 2013; Kiela et al., 2014) suggest that conclusions about concrete concept learning may not necessarily hold in the general case. In this pa</context>
</contexts>
<marker>Kiela, Hill, Korhonen, Clark, 2014</marker>
<rawString>Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen Clark. 2014. Improving multi-modal representations using image dispersion: Why less is sometimes more. In Proceedings of the annual meeting of the Association for Computational Linguistics. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
<author>Roger Garside</author>
<author>Michael Bryant</author>
</authors>
<title>Claws4: the tagging of the British National Corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th conference on Computational linguistics-Volume 1,</booktitle>
<pages>622--628</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2990" citStr="Leech et al., 1994" startWordPosition="439" endWordPosition="442"> such as curiosity or loyalty. However, differences between abstract and concrete processing and representation (Paivio, 1991; Hill et al., 2013; Kiela et al., 2014) suggest that conclusions about concrete concept learning may not necessarily hold in the general case. In this paper, we therefore focus on multi-modal models for learning both abstract and concrete concepts. Although concrete concepts might seem more basic or fundamental, the vast majority of openclass, meaning-bearing words in everyday language are in fact abstract. 72% of the noun or verb tokens in the British National Corpus (Leech et al., 1994) are rated by human judges1 as more abstract than the noun war, for instance, a concept many would already consider to be quite abstract. Moreover, abstract concepts by definition encode higher-level (more general) principles than concrete concepts, which typically reside naturally in a single semantic category or domain (Crutch and Warrington, 2005). It is therefore likely that abstract representations may prove highly applicable for multi-task, multi-domain or transfer learning models, which aim to acquire ‘general-purpose’ conceptual knowledge without reference to a specific objective or ta</context>
<context position="19380" citStr="Leech et al., 1994" startWordPosition="3068" endWordPosition="3071">cording to concreteness and sampling at random from the first and fourth quartiles respectively. We also introduce a complementary noun/verb dichotomy, Table 3: Details the subsets of USF data used in our evaluations, downloadable from our website. on the intuition that information propagation may occur differently from noun to noun or from noun to verb (because of their distinct structural relationships in sentences). POS-tags are not assigned as part of the USF data, so we draw the noun/verb distinction based on the majority POS-tag of USF concepts in the lemmatized British National Corpus (Leech et al., 1994). The abstract/concrete and noun/verb dichotomies yield four distinct concept lists. For consistency, the concrete noun list is filtered so that each concrete noun concept w has a perceptual representation b(w) in both PESP and PCSLB. For the four resulting concept lists C (concrete/abstract, noun/verb), a corresponding set of evaluation pairs {(w1, w2) E USF : w1, w2 E C} is extracted (see Table 3 for details). 3 Results and Discussion Our experiments were designed to answer four questions, outlined in the following subsections: (1) Which model architectures perform best at combining informat</context>
</contexts>
<marker>Leech, Garside, Bryant, 1994</marker>
<rawString>Geoffrey Leech, Roger Garside, and Michael Bryant. 1994. Claws4: the tagging of the British National Corpus. In Proceedings of the 15th conference on Computational linguistics-Volume 1, pages 622– 628. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken McRae</author>
<author>George S Cree</author>
<author>Mark S Seidenberg</author>
<author>Chris McNorgan</author>
</authors>
<title>Semantic feature production norms for a large set of living and nonliving things.</title>
<date>2005</date>
<journal>Behavior Research Methods,</journal>
<volume>37</volume>
<issue>4</issue>
<pages>559</pages>
<contexts>
<context position="15512" citStr="McRae et al. (2005)" startWordPosition="2424" endWordPosition="2427">uct a corresponding bag of features b(w). For each ESP image I that contains w, we append the other concept tokens identified in I to b(w). Thus, the more frequently a concept cooccurs with w in images, the more its corresponding lexical token occurs in b(w). The array PESP in this case then consists of the (w, b(w)) pairs. CSLB Property Norms The Centre for Speech, Language and the Brain norms (CSLB) (Devereux et al., 2013) is a recently-released dataset containing semantic properties for 638 concrete concepts produced by human annotators. The CSLB dataset was compiled in the same way as the McRae et al. (2005) property norms used widely in multimodal models (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013); we use CSLB because it contains more concepts. For each concept, the proportion of the 30 annotators that produced a given feature can also be employed as a measure of the strength of that feature. When encoding the CSLB data in P, we first map properties to lexical forms (e.g. is green becomes green). By directly identifying perceptual features and linguistic forms in this way, we treat features observed in the perceptual data as (sub)concepts to be acquired via the same multi-moda</context>
</contexts>
<marker>McRae, Cree, Seidenberg, McNorgan, 2005</marker>
<rawString>Ken McRae, George S Cree, Mark S Seidenberg, and Chris McNorgan. 2005. Semantic feature production norms for a large set of living and nonliving things. Behavior Research Methods, 37(4):547– 559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gr´egoire Mesnil</author>
<author>Yann Dauphin</author>
<author>Xavier Glorot</author>
<author>Salah Rifai</author>
<author>Yoshua Bengio</author>
<author>Ian J Goodfellow</author>
<author>Erick Lavoie</author>
<author>Xavier Muller</author>
<author>Guillaume Desjardins</author>
<author>David Warde-Farley</author>
</authors>
<title>Unsupervised and transfer learning challenge: a deep learning approach.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research-Proceedings Track,</journal>
<pages>27--97</pages>
<contexts>
<context position="3642" citStr="Mesnil et al., 2012" startWordPosition="538" endWordPosition="541">ore abstract than the noun war, for instance, a concept many would already consider to be quite abstract. Moreover, abstract concepts by definition encode higher-level (more general) principles than concrete concepts, which typically reside naturally in a single semantic category or domain (Crutch and Warrington, 2005). It is therefore likely that abstract representations may prove highly applicable for multi-task, multi-domain or transfer learning models, which aim to acquire ‘general-purpose’ conceptual knowledge without reference to a specific objective or task (Collobert and Weston, 2008; Mesnil et al., 2012). In a recent paper, Hill et al. (2014) investigate whether the multi-modal models cited above are 1Contributors to the USF dataset (Nelson et al., 2004). 255 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 255–265, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics effective for learning concepts other than concrete nouns. They observe that representations of certain abstract concepts can indeed be enhanced in multi-modal models by combining perceptual and linguistic input with an information propagation ste</context>
</contexts>
<marker>Mesnil, Dauphin, Glorot, Rifai, Bengio, Goodfellow, Lavoie, Muller, Desjardins, Warde-Farley, 2012</marker>
<rawString>Gr´egoire Mesnil, Yann Dauphin, Xavier Glorot, Salah Rifai, Yoshua Bengio, Ian J Goodfellow, Erick Lavoie, Xavier Muller, Guillaume Desjardins, David Warde-Farley, et al. 2012. Unsupervised and transfer learning challenge: a deep learning approach. Journal of Machine Learning Research-Proceedings Track, 27:97–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of International Conference of Learning Representations,</booktitle>
<location>Scottsdale, Arizona, USA.</location>
<contexts>
<context position="5048" citStr="Mikolov et al. (2013)" startWordPosition="741" endWordPosition="744">ust to changes in the type of concept to be learned. However, both methods are somewhat inelegant, in that they learn separate linguistic and ‘pseudo-perceptual’ representations, which must be combined via a separate information combination step. Moreover, for the majority of abstract concepts, the best performing multi-modal model employing these techniques remains less effective than conventional text-only representation learning model. Motivated by these observations, we introduce an architecture for learning both abstract and concrete representations that generalizes the skipgram model of Mikolov et al. (2013) from text-based to multi-modal learning. Aspects of the model design are influenced by considering the process of human language learning. The model moderates the training input to include more perceptual information about commonly-occurring concrete concepts and less information about rarer concepts. Moreover, it integrates the processes of combining perceptual and linguistic input and propagating information from concrete to abstract concepts into a single representation update process based on back-propagation. We train our model on running-text language and two sources of perceptual descr</context>
<context position="7780" citStr="Mikolov et al. (2013)" startWordPosition="1152" endWordPosition="1155"> observations we present are potentially important for optimizing the learning of representations of concrete and abstract concepts in multi-modal models. In addition, they offer a degree of insight into the poorly understood issue of how abstract concepts may be encoded in human memory. 2 Model Design Before describing how our multi-modal architecture encodes and integrates perceptual information, we first describe the underlying corpus-based representation learning model. Language-only Model Our multi-modal architecture builds on the continuous log-linear skipgram language model proposed by Mikolov et al. (2013). This model learns lexical representations in a similar way to neural-probabilistic language models (NPLM) but without a non-linear hidden layer, a simplification that facilitates the efficient learning of large vocabularies of dense representations, generally referred to as embeddings (Turian et al., 2010). Embeddings learned by the model achieve state-of-the-art performance on several evaluations including sentence completion and analogy modelling (Mikolov et al., 2013). For each word type w in the vocabulary V , the model learns both a ‘target-embedding’ r,,, ∈ Rd and a ‘context-embedding’</context>
<context position="11237" citStr="Mikolov et al. (2013)" startWordPosition="1723" endWordPosition="1726"> in similar linguistic contexts are likely to have similar meanings. Informally, the model adjusts its embeddings to increase the ‘probability’ of seeing the language in the training corpus. Since this probability increases with the p(c|w), and the p(c|w) increase with the dot product ˆrc · rw, the updates have the effect of moving each target-embedding incrementally ‘closer’ to the context-embeddings of its collocates. In the target-embedding space, this results in embeddings of concept words that regularly occur in similar contexts moving closer together. Multi-modal Extension We extend the Mikolov et al. (2013) architecture via a simple means of introducing perceptual information that aligns with human language learning. Based on the assumption that frequency in domain-general linguistic corpora correlates with the likelihood of ‘experiencing’ a concept in the world (Bybee and Hopper, 2001; Chater and Manning, 2006), perceptual information is introduced to the model whenever designated concrete concepts are encountered in the running-text linguistic input. This has the effect of introducing more perceptual input for commonly experienced concrete concepts and less input for rarer concrete concepts. T</context>
<context position="14504" citStr="Mikolov et al., 2013" startWordPosition="2252" endWordPosition="2255"> non-integral α, the number of sentences inserted is Lα], and a further sentence is added with probability α − Lα]. In all experiments reported in the following sections we set the window size parameter k = 5 and the minimum frequency parameter f = 3, which guarantees that the model learns embeddings for all concepts in our evaluation sets. While the model learns both target and context-embeddings for each word in the vocabulary, we conduct our experiments with the target embeddings only. We set the dimension parameter d = 300 as this produces high quality embeddings in the languageonly case (Mikolov et al., 2013). 2.1 Information Sources We construct the associative array of perceptual information P from two sources typical of those used for multi-modal semantic models. ESPGame Dataset The ESP-Game dataset (ESP) (Von Ahn and Dabbish, 2004) consists of 100,000 images, each annotated with a list of lexical concepts that appear in that image. For any concept w identified in an ESP image, we construct a corresponding bag of features b(w). For each ESP image I that contains w, we append the other concept tokens identified in I to b(w). Thus, the more frequently a concept cooccurs with w in images, the more</context>
<context position="20998" citStr="Mikolov et al. (2013)" startWordPosition="3323" endWordPosition="3326">ream in some way? (4) How much perceptual vs. linguistic input is optimal for learning various concept types? 3.1 Combining information sources To evaluate our approach as a method of information combination we compared its performance on the concrete noun evaluation set against three alternative methods. The first alternative is simple concatenation of these perceptual vectors with linguistic vectors embeddings learned nose small Concept 1 Assoc. Concept 2 abdomen (6.83) stomach (6.04) 0.566 throw (4.05) hope (1.18) egg (5.79) ball (6.08) glory (3.53) milk (6.66) 0.234 0.192 0.012 259 by the Mikolov et al. (2013) model on the Text8 Corpus. In the second alternative (proposed for multi-modal models by Silberer and Lapata (2012)), canonical correlation analysis (CCA) (Hardoon et al., 2004) was applied to the vectors of both modalities. CCA yields reduceddimensionality representations that preserve underlying inter-modal correlations, which are then concatenated. The final alternative, proposed by Bruni et al. (2014) involves applying Singular Value Decomposition (SVD) to the matrix of concatenated multi-modal representations, yielding smoothed representations.3 When implementing the concatenation, CCA a</context>
<context position="26413" citStr="Mikolov et al. (2013)" startWordPosition="4175" endWordPosition="4178">ed parameter vector as the regularization term. This ensures that the regression favors lower coefficients and a smoother solution function, which should provide better generalization performance than simple linear regression. The objective for learning the fi is then to minimize 11aX − Yi1122 + 11a112 2 where a is the vector of regression coefficients, X is a matrix of linguistic representations and Yi a vector of the perceptual feature i for the set of bimodal concepts. Comparisons We applied the Johns and Jones method and ridge regression starting from linguistic embeddings acquired by the Mikolov et al. (2013) model on the Text8 Corpus, and concatenated the resulting pseudo-perceptual and linguistic representations. As with the implementation of our model, the perceptual input for these alternative models was limited to concrete nouns (i.e. concrete nouns were the only bi-modal concepts in the models). Figure 3 (right side) shows the propagation performance of the three models. While the correlations overall may seem somewhat low, this is a consequence of the difficulty of modelling the USF data. In fact, the performance of both the language-only model and our multi-modal extension across the conce</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In Proceedings of International Conference of Learning Representations, Scottsdale, Arizona, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic Morin</author>
<author>Yoshua Bengio</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<booktitle>In Proceedings of the international Workshop on Artificial Intelligence and Statistics,</booktitle>
<pages>246--252</pages>
<contexts>
<context position="10487" citStr="Morin and Bengio, 2005" startWordPosition="1607" endWordPosition="1610">tive is then to maximize the sum of the log probabilities T across of all such examples from S and across all sentences in the corpus, where T is defined as follows: log(p(wn+j|wn)) −t(n)&lt;j&lt;t(n),j7W The model free parameters (target-embeddings and context-embeddings of dimension d for each word in the corpus with frequency above a certain threshold f) are updated according to stochastic gradient descent and backpropation, with learning rate controlled by Adagrad (Duchi et al., 2011). For efficiency, the output layer is encoded as a hierarchical softmax function based on a binary Huffman tree (Morin and Bengio, 2005). As with other distributional architectures, the model captures conceptual semantics by exploiting the fact that words appearing in similar linguistic contexts are likely to have similar meanings. Informally, the model adjusts its embeddings to increase the ‘probability’ of seeing the language in the training corpus. Since this probability increases with the p(c|w), and the p(c|w) increase with the dot product ˆrc · rw, the updates have the effect of moving each target-embedding incrementally ‘closer’ to the context-embeddings of its collocates. In the target-embedding space, this results in </context>
</contexts>
<marker>Morin, Bengio, 2005</marker>
<rawString>Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In Proceedings of the international Workshop on Artificial Intelligence and Statistics, pages 246–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond H Myers</author>
</authors>
<title>Classical and Modern Regression with Applications,</title>
<date>1990</date>
<volume>2</volume>
<publisher>Duxbury Press</publisher>
<location>Belmont, CA.</location>
<contexts>
<context position="25197" citStr="Myers, 1990" startWordPosition="3975" endWordPosition="3976">pectively, and cl and kl the linguistic representations. The exponent parameter A reflects the learning rate. In step two, the initial quasi-perceptual representations are inferred for a second time, but with the weighted average calculated over the perceptual or initial quasi-perceptual representations of all other words, not just those that were originally bi-modal. As with Johns and Jones (2012), we set the learning rate parameter A to be 3 in the first step and 13 in the second. Ridge Regression An alternative, proposed for the present purpose by Hill et al. (2014), uses ridge regression (Myers, 1990). Ridge regression is a variant of least squares regression in which a regularization term is added to the training objective to favor solutions with certain properties. For bi-modal concepts of dimension np, we apply ridge regression to learn np linear functions 260 fi : Rnl —* R that map the linguistic representations (of dimension nl) to a particular perceptual feature i. These functions are then applied together to map the linguistic representations of uni-modal concepts to full quasi-perceptual representations. Following Hill et al. (2014), we take the Euclidian l2 norm of the inferred pa</context>
</contexts>
<marker>Myers, 1990</marker>
<rawString>Raymond H Myers. 1990. Classical and Modern Regression with Applications, volume 2. Duxbury Press Belmont, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L Nelson</author>
<author>Cathy L McEvoy</author>
<author>Thomas A Schreiber</author>
</authors>
<title>The University of South Florida free association, rhyme, and word fragment norms.</title>
<date>2004</date>
<journal>Behavior Research Methods, Instruments, &amp; Computers,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="3795" citStr="Nelson et al., 2004" startWordPosition="563" endWordPosition="566">e higher-level (more general) principles than concrete concepts, which typically reside naturally in a single semantic category or domain (Crutch and Warrington, 2005). It is therefore likely that abstract representations may prove highly applicable for multi-task, multi-domain or transfer learning models, which aim to acquire ‘general-purpose’ conceptual knowledge without reference to a specific objective or task (Collobert and Weston, 2008; Mesnil et al., 2012). In a recent paper, Hill et al. (2014) investigate whether the multi-modal models cited above are 1Contributors to the USF dataset (Nelson et al., 2004). 255 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 255–265, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics effective for learning concepts other than concrete nouns. They observe that representations of certain abstract concepts can indeed be enhanced in multi-modal models by combining perceptual and linguistic input with an information propagation step. Hill et al. (2014) propose ridge regression as an alternative to the nearest-neighbour averaging proposed by Johns and Jones (2012) for such propagati</context>
<context position="6027" citStr="Nelson et al., 2004" startWordPosition="891" endWordPosition="894">eptual and linguistic input and propagating information from concrete to abstract concepts into a single representation update process based on back-propagation. We train our model on running-text language and two sources of perceptual descriptors for concrete nouns: the ESPGame dataset of annotated images (Von Ahn and Dabbish, 2004) and the CSLB set of concept property norms (Devereux et al., 2013). We find that our model combines information from the different modalities more effectively than previous methods, resulting in an improved ability to model the USF free association gold standard (Nelson et al., 2004) for concrete nouns. In addition, the architecture propagates the extra-linguistic input for concrete nouns to improve representations of abstract concepts more effectively than alternative methods. While this propagation can effectively extend the advantage of the multi-modal approach to many more concepts than simple concrete nouns, we observe that the benefit of adding perceptual input appears to decrease as target concepts become more abstract. Indeed, for the most abstract concepts of all, language-only models still provide the most effective learning mechanism. Finally, we investigate th</context>
<context position="18080" citStr="Nelson et al., 2004" startWordPosition="2857" endWordPosition="2860"> obey street is green (10) is large (10) Table 1: Concepts identified in images in the ESP Game (left) and features produced for concepts by human annotators in the CSLB dataset (with feature strength, max=30). Table 2: Example concept pairs (with mean concreteness rating) and free-association scores from the USF dataset. Wikipedia text, split into sentences and with punctuation removed. 2.2 Evaluation We evaluate the quality of representations by how well they reflect free association scores, an empirical measure of cognitive conceptual proximity. The University of South Florida Norms (USF) (Nelson et al., 2004) contain free association scores for over 40,000 concept pairs, and have been widely used in NLP to evaluate semantic representations (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Each concept that we extract from the USF database has also been rated for conceptual concreteness on a Likert scale of 1-7 by at least 10 human annotators. Following previous studies (Huang et al., 2012; Silberer and Lapata, 2012), we measure the (Spearman p) correlation between association scores and the cosine similarity of vector representations. We c</context>
</contexts>
<marker>Nelson, McEvoy, Schreiber, 2004</marker>
<rawString>Douglas L Nelson, Cathy L McEvoy, and Thomas A Schreiber. 2004. The University of South Florida free association, rhyme, and word fragment norms. Behavior Research Methods, Instruments, &amp; Computers, 36(3):402–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allan Paivio</author>
</authors>
<title>Dual coding theory: Retrospect and current status.</title>
<date>1991</date>
<journal>Canadian Journal of Psychology,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="2496" citStr="Paivio, 1991" startWordPosition="360" endWordPosition="361">nput, multi-modal models can outperform language-only models on a range of semantic NLP tasks, including modelling similarity (Bruni et al., 2014; Kiela et al., 2014) and free association (Silberer and Lapata, 2012), predicting compositionality (Roller and Schulte im Walde, 2013) and concept categorization (Silberer and Lapata, 2014). However, to date, these previous approaches to multi-modal concept learning focus on concrete words such as cat or dog, rather than abstract concepts, such as curiosity or loyalty. However, differences between abstract and concrete processing and representation (Paivio, 1991; Hill et al., 2013; Kiela et al., 2014) suggest that conclusions about concrete concept learning may not necessarily hold in the general case. In this paper, we therefore focus on multi-modal models for learning both abstract and concrete concepts. Although concrete concepts might seem more basic or fundamental, the vast majority of openclass, meaning-bearing words in everyday language are in fact abstract. 72% of the noun or verb tokens in the British National Corpus (Leech et al., 1994) are rated by human judges1 as more abstract than the noun war, for instance, a concept many would already</context>
</contexts>
<marker>Paivio, 1991</marker>
<rawString>Allan Paivio. 1991. Dual coding theory: Retrospect and current status. Canadian Journal of Psychology, 45(3):255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Roller</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>A multimodal LDA model integrating textual, cognitive and visual modalities.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1146--1157</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<marker>Roller, Walde, 2013</marker>
<rawString>Stephen Roller and Sabine Schulte im Walde. 2013. A multimodal LDA model integrating textual, cognitive and visual modalities. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1146–1157, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Grounded models of semantic representation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1423--1433</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1782" citStr="Silberer and Lapata, 2012" startWordPosition="253" endWordPosition="256"> performance of multi-modal models and for theories of abstract conceptual representation. 1 Introduction Multi-modal models that learn semantic representations from both language and information about the perceptible properties of concepts were originally motivated by parallels with human word learning (Andrews et al., 2009) and evidence that many concepts are grounded in perception (Barsalou and Wiemer-Hastings, 2005). The perceptual information in such models is generally mined directly from images (Feng and Lapata, 2010; Bruni et al., 2012) or from data collected in psychological studies (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). By exploiting the additional information encoded in perceptual input, multi-modal models can outperform language-only models on a range of semantic NLP tasks, including modelling similarity (Bruni et al., 2014; Kiela et al., 2014) and free association (Silberer and Lapata, 2012), predicting compositionality (Roller and Schulte im Walde, 2013) and concept categorization (Silberer and Lapata, 2014). However, to date, these previous approaches to multi-modal concept learning focus on concrete words such as cat or dog, rather than abstract concepts, such as cu</context>
<context position="15587" citStr="Silberer and Lapata, 2012" startWordPosition="2436" endWordPosition="2439">ontains w, we append the other concept tokens identified in I to b(w). Thus, the more frequently a concept cooccurs with w in images, the more its corresponding lexical token occurs in b(w). The array PESP in this case then consists of the (w, b(w)) pairs. CSLB Property Norms The Centre for Speech, Language and the Brain norms (CSLB) (Devereux et al., 2013) is a recently-released dataset containing semantic properties for 638 concrete concepts produced by human annotators. The CSLB dataset was compiled in the same way as the McRae et al. (2005) property norms used widely in multimodal models (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013); we use CSLB because it contains more concepts. For each concept, the proportion of the 30 annotators that produced a given feature can also be employed as a measure of the strength of that feature. When encoding the CSLB data in P, we first map properties to lexical forms (e.g. is green becomes green). By directly identifying perceptual features and linguistic forms in this way, we treat features observed in the perceptual data as (sub)concepts to be acquired via the same multi-modal input streams and stored in the same domain-general memory as the evaluat</context>
<context position="18285" citStr="Silberer and Lapata, 2012" startWordPosition="2892" endWordPosition="2895">, max=30). Table 2: Example concept pairs (with mean concreteness rating) and free-association scores from the USF dataset. Wikipedia text, split into sentences and with punctuation removed. 2.2 Evaluation We evaluate the quality of representations by how well they reflect free association scores, an empirical measure of cognitive conceptual proximity. The University of South Florida Norms (USF) (Nelson et al., 2004) contain free association scores for over 40,000 concept pairs, and have been widely used in NLP to evaluate semantic representations (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Each concept that we extract from the USF database has also been rated for conceptual concreteness on a Likert scale of 1-7 by at least 10 human annotators. Following previous studies (Huang et al., 2012; Silberer and Lapata, 2012), we measure the (Spearman p) correlation between association scores and the cosine similarity of vector representations. We create separate abstract and concrete concept lists by ranking the USF concepts according to concreteness and sampling at random from the first and fourth quartiles respectively. We also introduce a complem</context>
<context position="21114" citStr="Silberer and Lapata (2012)" startWordPosition="3341" endWordPosition="3344">.1 Combining information sources To evaluate our approach as a method of information combination we compared its performance on the concrete noun evaluation set against three alternative methods. The first alternative is simple concatenation of these perceptual vectors with linguistic vectors embeddings learned nose small Concept 1 Assoc. Concept 2 abdomen (6.83) stomach (6.04) 0.566 throw (4.05) hope (1.18) egg (5.79) ball (6.08) glory (3.53) milk (6.66) 0.234 0.192 0.012 259 by the Mikolov et al. (2013) model on the Text8 Corpus. In the second alternative (proposed for multi-modal models by Silberer and Lapata (2012)), canonical correlation analysis (CCA) (Hardoon et al., 2004) was applied to the vectors of both modalities. CCA yields reduceddimensionality representations that preserve underlying inter-modal correlations, which are then concatenated. The final alternative, proposed by Bruni et al. (2014) involves applying Singular Value Decomposition (SVD) to the matrix of concatenated multi-modal representations, yielding smoothed representations.3 When implementing the concatenation, CCA and SVD methods, we first encoded the perceptual input directly into sparse feature vectors, with coordinates for eac</context>
<context position="27161" citStr="Silberer and Lapata, 2012" startWordPosition="4298" endWordPosition="4301">plementation of our model, the perceptual input for these alternative models was limited to concrete nouns (i.e. concrete nouns were the only bi-modal concepts in the models). Figure 3 (right side) shows the propagation performance of the three models. While the correlations overall may seem somewhat low, this is a consequence of the difficulty of modelling the USF data. In fact, the performance of both the language-only model and our multi-modal extension across the concept types (from 0.18 to 0.36) is equal to or higher than previous models evaluated on the same data (Feng and Lapata, 2010; Silberer and Lapata, 2012; Silberer et al., 2013). For learning representations of concrete verbs, our approach achieves a 69% increase in performance over the next best alternative. The performance of the model on abstract verbs is marginally inferior to Johns and Jones’ method. Nevertheless, the clear advantage for concrete verbs makes our model the best choice for learning representations of verbs in general, as shown by performance on the set all verbs, which also includes mixed abstract-concrete pairs. Our model is also marginally inferior to alternative approaches in learning representations of abstract nouns. H</context>
</contexts>
<marker>Silberer, Lapata, 2012</marker>
<rawString>Carina Silberer and Mirella Lapata. 2012. Grounded models of semantic representation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1423–1433. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Learning grounded meaning representations with autoencoders.</title>
<date>2014</date>
<booktitle>In Proceedings of the annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2219" citStr="Silberer and Lapata, 2014" startWordPosition="317" endWordPosition="320">ual information in such models is generally mined directly from images (Feng and Lapata, 2010; Bruni et al., 2012) or from data collected in psychological studies (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). By exploiting the additional information encoded in perceptual input, multi-modal models can outperform language-only models on a range of semantic NLP tasks, including modelling similarity (Bruni et al., 2014; Kiela et al., 2014) and free association (Silberer and Lapata, 2012), predicting compositionality (Roller and Schulte im Walde, 2013) and concept categorization (Silberer and Lapata, 2014). However, to date, these previous approaches to multi-modal concept learning focus on concrete words such as cat or dog, rather than abstract concepts, such as curiosity or loyalty. However, differences between abstract and concrete processing and representation (Paivio, 1991; Hill et al., 2013; Kiela et al., 2014) suggest that conclusions about concrete concept learning may not necessarily hold in the general case. In this paper, we therefore focus on multi-modal models for learning both abstract and concrete concepts. Although concrete concepts might seem more basic or fundamental, the vast</context>
<context position="35304" citStr="Silberer and Lapata, 2014" startWordPosition="5579" endWordPosition="5582">irect representation Propagation Figure 4: Top: Comparing the strategy of directly representing abstract concepts from perceptual information where available (yellow bars) vs. propagating via concrete concepts. Bottom: The effect of increasing α on correlation with USF pairs (Spearman ρ) for each concept type. Horizontal dashed lines indicate language-only model baseline. concept from linguistic and perceptual input. While neuro-probabilistic language models have been applied to the problem of multi-modal representation learning previously (Srivastava and Salakhutdinov, 2012; Wu et al., 2013; Silberer and Lapata, 2014) our model and experiments develop this work in several important ways. First, we address the problem of learning abstract concepts. By isolating concepts of different concreteness and part-of-speech in our evaluation sets, and separating the processes of information combination and propagation, we demonstrate that the multimodal approach is indeed effective for some, but perhaps not all, abstract concepts. In addition, our model introduces a clear parallel with human language learning. Perceptual input is introduced precisely when concrete concepts are ‘experienced’ by the model in the corpus</context>
</contexts>
<marker>Silberer, Lapata, 2014</marker>
<rawString>Carina Silberer and Mirella Lapata. 2014. Learning grounded meaning representations with autoencoders. In Proceedings of the annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Vittorio Ferrari</author>
<author>Mirella Lapata</author>
</authors>
<title>Models of semantic representation with visual attributes.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="27185" citStr="Silberer et al., 2013" startWordPosition="4302" endWordPosition="4305">the perceptual input for these alternative models was limited to concrete nouns (i.e. concrete nouns were the only bi-modal concepts in the models). Figure 3 (right side) shows the propagation performance of the three models. While the correlations overall may seem somewhat low, this is a consequence of the difficulty of modelling the USF data. In fact, the performance of both the language-only model and our multi-modal extension across the concept types (from 0.18 to 0.36) is equal to or higher than previous models evaluated on the same data (Feng and Lapata, 2010; Silberer and Lapata, 2012; Silberer et al., 2013). For learning representations of concrete verbs, our approach achieves a 69% increase in performance over the next best alternative. The performance of the model on abstract verbs is marginally inferior to Johns and Jones’ method. Nevertheless, the clear advantage for concrete verbs makes our model the best choice for learning representations of verbs in general, as shown by performance on the set all verbs, which also includes mixed abstract-concrete pairs. Our model is also marginally inferior to alternative approaches in learning representations of abstract nouns. However, in this case, no</context>
</contexts>
<marker>Silberer, Ferrari, Lapata, 2013</marker>
<rawString>Carina Silberer, Vittorio Ferrari, and Mirella Lapata. 2013. Models of semantic representation with visual attributes. In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Ruslan Salakhutdinov</author>
</authors>
<title>Multimodal learning with deep boltzmann machines.</title>
<date>2012</date>
<booktitle>In NIPS,</booktitle>
<pages>2231--2239</pages>
<contexts>
<context position="35259" citStr="Srivastava and Salakhutdinov, 2012" startWordPosition="5571" endWordPosition="5574">bs verbs Concept Type Perceptual Information Source Direct representation Propagation Figure 4: Top: Comparing the strategy of directly representing abstract concepts from perceptual information where available (yellow bars) vs. propagating via concrete concepts. Bottom: The effect of increasing α on correlation with USF pairs (Spearman ρ) for each concept type. Horizontal dashed lines indicate language-only model baseline. concept from linguistic and perceptual input. While neuro-probabilistic language models have been applied to the problem of multi-modal representation learning previously (Srivastava and Salakhutdinov, 2012; Wu et al., 2013; Silberer and Lapata, 2014) our model and experiments develop this work in several important ways. First, we address the problem of learning abstract concepts. By isolating concepts of different concreteness and part-of-speech in our evaluation sets, and separating the processes of information combination and propagation, we demonstrate that the multimodal approach is indeed effective for some, but perhaps not all, abstract concepts. In addition, our model introduces a clear parallel with human language learning. Perceptual input is introduced precisely when concrete concepts</context>
</contexts>
<marker>Srivastava, Salakhutdinov, 2012</marker>
<rawString>Nitish Srivastava and Ruslan Salakhutdinov. 2012. Multimodal learning with deep boltzmann machines. In NIPS, pages 2231–2239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8089" citStr="Turian et al., 2010" startWordPosition="1197" endWordPosition="1200"> describing how our multi-modal architecture encodes and integrates perceptual information, we first describe the underlying corpus-based representation learning model. Language-only Model Our multi-modal architecture builds on the continuous log-linear skipgram language model proposed by Mikolov et al. (2013). This model learns lexical representations in a similar way to neural-probabilistic language models (NPLM) but without a non-linear hidden layer, a simplification that facilitates the efficient learning of large vocabularies of dense representations, generally referred to as embeddings (Turian et al., 2010). Embeddings learned by the model achieve state-of-the-art performance on several evaluations including sentence completion and analogy modelling (Mikolov et al., 2013). For each word type w in the vocabulary V , the model learns both a ‘target-embedding’ r,,, ∈ Rd and a ‘context-embedding’ ˆr,,, ∈ Rd such that, given a target word, its ability to predict nearby context words is maximized. The probability of seeing context word c given target w is defined as: p(c|w) = eˆrc rw Ev∈V eˆrv rw 256 Target Representation Context Representations Information Source w n Score: p(c|&apos;) p&apos; _ p&apos; ��, p&apos; m, p</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling images with a computer game.</title>
<date>2004</date>
<booktitle>In Proceedings of the SIGCHI conference on human factors in computing systems,</booktitle>
<pages>319--326</pages>
<publisher>ACM.</publisher>
<marker>Von Ahn, Dabbish, 2004</marker>
<rawString>Luis Von Ahn and Laura Dabbish. 2004. Labeling images with a computer game. In Proceedings of the SIGCHI conference on human factors in computing systems, pages 319–326. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Wiemer-Hastings</author>
<author>Xu Xu</author>
</authors>
<title>Content differences for abstract and concrete concepts.</title>
<date>2005</date>
<journal>Cognitive Science,</journal>
<volume>29</volume>
<issue>5</issue>
<contexts>
<context position="29941" citStr="Wiemer-Hastings and Xu, 2005" startWordPosition="4728" endWordPosition="4731">eness inaccurate. 3.3 Direct representation vs. propagation Although property norm datasets such as the CSLB data typically consist of perceptual feature information for concrete nouns only, imagebased datasets such as ESP do contain information on more abstract concepts, which was omitted from the previous experiments. Indeed, image banks such as Google Images contain millions of photographs portraying quite abstract concepts, such as love or war. On the other hand, encodings or descriptions of abstract concepts are generally more subjective and less reliable than those of concrete concepts (Wiemer-Hastings and Xu, 2005). We therefore investigated whether or not it is preferable to include this additional information as model input or to restrict perceptual input 261 Correlation 0.4 Combination Method Vector Concatenation CCA SVD Our Model (α=1) 0.3 0.2 0.1 0.0 0.203 0.22 0.15 0.239 0.271 0.259 0.256 0.301 0.249 0.24 0.231 0.296 Correlation 0.4 0.3 0.2 0.1 0.0 0.282 0.265 0.25 0.07 0.236 0.364 0.06 0.116 0.197 0.177 0.172 0.175 0.167 0.175 Propagation Method Johns and Jones Ridge Regre a i&apos;n) Our Model 0.225 CSLB ESP CSLB &amp; ESP abstract nouns all nouns concrete verbs abstract verbs all verbs Concrete nouns − </context>
</contexts>
<marker>Wiemer-Hastings, Xu, 2005</marker>
<rawString>Katja Wiemer-Hastings and Xu Xu. 2005. Content differences for abstract and concrete concepts. Cognitive Science, 29(5):719–736.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pengcheng Wu</author>
<author>Steven CH Hoi</author>
<author>Hao Xia</author>
<author>Peilin Zhao</author>
<author>Dayong Wang</author>
<author>Chunyan Miao</author>
</authors>
<title>Online multimodal deep similarity learning with application to image retrieval.</title>
<date>2013</date>
<booktitle>In Proceedings of the 21st ACM International Conference on Multimedia,</booktitle>
<pages>153--162</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="35276" citStr="Wu et al., 2013" startWordPosition="5575" endWordPosition="5578">ormation Source Direct representation Propagation Figure 4: Top: Comparing the strategy of directly representing abstract concepts from perceptual information where available (yellow bars) vs. propagating via concrete concepts. Bottom: The effect of increasing α on correlation with USF pairs (Spearman ρ) for each concept type. Horizontal dashed lines indicate language-only model baseline. concept from linguistic and perceptual input. While neuro-probabilistic language models have been applied to the problem of multi-modal representation learning previously (Srivastava and Salakhutdinov, 2012; Wu et al., 2013; Silberer and Lapata, 2014) our model and experiments develop this work in several important ways. First, we address the problem of learning abstract concepts. By isolating concepts of different concreteness and part-of-speech in our evaluation sets, and separating the processes of information combination and propagation, we demonstrate that the multimodal approach is indeed effective for some, but perhaps not all, abstract concepts. In addition, our model introduces a clear parallel with human language learning. Perceptual input is introduced precisely when concrete concepts are ‘experienced</context>
</contexts>
<marker>Wu, Hoi, Xia, Zhao, Wang, Miao, 2013</marker>
<rawString>Pengcheng Wu, Steven CH Hoi, Hao Xia, Peilin Zhao, Dayong Wang, and Chunyan Miao. 2013. Online multimodal deep similarity learning with application to image retrieval. In Proceedings of the 21st ACM International Conference on Multimedia, pages 153–162. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
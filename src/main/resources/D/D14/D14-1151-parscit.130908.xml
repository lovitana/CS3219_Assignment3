<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.124522">
<title confidence="0.983108">
Using Mined Coreference Chains as a Resource for a Semantic Task
</title>
<author confidence="0.968569">
Heike Adel and Hinrich Sch¨utze
</author>
<affiliation confidence="0.911427">
Center for Information and Language Processing
University of Munich
Germany
</affiliation>
<email confidence="0.988994">
heike.adel@cis.lmu.de
</email>
<sectionHeader confidence="0.993609" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998031">
We propose to use coreference chains ex-
tracted from a large corpus as a resource
for semantic tasks. We extract three mil-
lion coreference chains and train word
embeddings on them. Then, we com-
pare these embeddings to word vectors de-
rived from raw text data and show that
coreference-based word embeddings im-
prove F1 on the task of antonym classifi-
cation by up to .09.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983523809524">
After more than a decade of work on coreference
resolution, coreference resolution systems have
reached a certain level of maturity (e.g., Recasens
et al. (2010)). While accuracy is far from perfect
and many phenomena such as bridging still pose
difficult research problems, the quality of the out-
put of these systems is high enough to be useful
for many applications.
In this paper, we propose to run coreference res-
olution systems on large corpora, to collect the
coreference chains found and to use them as a re-
source for solving semantic tasks. This amounts
to using mined coreference chains as an automat-
ically compiled resource similar to the way cooc-
currence statistics, dependency pairs and aligned
parallel corpora are used in many applications in
NLP. Coreference chains have interesting comple-
mentary properties compared to these other re-
sources. For example, it is difficult to distinguish
true semantic similarity (e.g., “cows” – “cattle”)
from mere associational relatedness (e.g., “cows”
– “milk”) based on cooccurrence statistics. In con-
trast, coreference chains should be able to make
that distinction since only “cows” and “cattle” can
occur in the same coreference chain, not “cows”
and “milk”.
As a proof of concept we compile a resource
of mined coreference chains from the Gigaword
corpus and apply it to the task of identifying
antonyms. We induce distributed representations
for words based on (i) cooccurrence statistics and
(ii) mined coreference chains and show that a com-
bination of both outperforms cooccurrence statis-
tics on antonym identification.
In summary, we make two contributions. First,
we propose to use coreference chains mined from
large corpora as a resource in NLP and publish the
first such resource. Second, in a proof of concept
study, we show that they can be used to solve a se-
mantic task – antonym identification – better than
is possible with existing resources.
We focus on the task of finding antonyms in this
paper since antonyms usually are distributionally
similar but semantically dissimilar words. Hence,
it is often not possible to distinguish them from
synonyms with distributional models only. In con-
trast, we expect that the coreference-based repre-
sentations can provide useful complementary in-
formation to this task. In general, coreference-
based similarity can however be used as an addi-
tional feature for any task that distributional simi-
larity is useful for. Thus, our coreference resource
can be applied to a variety of NLP tasks, e.g. find-
ing alternative names for entities (in a way similar
to Wikipedia anchors) for tasks in the context of
knowledge base population.
The remainder of the paper is organized as fol-
lows. In Section 2, we describe how we create
word embeddings and how our antonym classi-
fier works. The word embeddings are then eval-
uated qualitatively, quantitatively and for the task
of antonym detection (Section 3). Section 4 dis-
cusses related work and Section 5 concludes.
</bodyText>
<sectionHeader confidence="0.987301" genericHeader="method">
2 System description
</sectionHeader>
<subsectionHeader confidence="0.99291">
2.1 Coreference-based embeddings
</subsectionHeader>
<bodyText confidence="0.9949225">
Standard word embeddings derived from text data
may not be able to distinguish between semantic
</bodyText>
<page confidence="0.913398">
1447
</page>
<bodyText confidence="0.738275428571429">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1447–1452,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
text-based coref.-based
my, their, her, your, our he, him, himself, zechariah, ancestor
man, girl, believer, pharisee, guy girl, prostitute, lupita, betsy, lehia
his
woman
</bodyText>
<tableCaption confidence="0.990226">
Table 1: Nearest neighbors of “his” / “woman” for text-based &amp; coreference-based embeddings
</tableCaption>
<bodyText confidence="0.99960947368421">
association and true synonymy. As a result, syn-
onyms and antonyms may be mapped to similar
word vectors (Yih et al., 2012). For many NLP
tasks, however, information about true synonymy
or antonymy may be important.
In this paper, we develop two different word
embeddings: embeddings calculated on raw text
data and embeddings derived from automatically
extracted coreference chains. For the calcula-
tion of the vector representations, the word2vec
toolkit1 by Mikolov et al. (2013) is applied. We
use the skip-gram model for our experiments be-
cause its results for semantic similarity are better
according to Mikolov et al. (2013). We train a
first model on a subset of English Gigaword data.2
In the following sections, we call the resulting
embeddings text-based. To improve the seman-
tic similarities of the vectors, we prepare another
training text consisting of coreference chains. We
use CoreNLP (Lee et al., 2011) to extract coref-
erence chains from the Gigaword corpus. Then
we build a skip-gram model on these coreference
chains. The extracted coreference chains are pro-
vided as an additional resource to this paper3. Al-
though they have been developed using only a
publicly available toolkit, we expect this resource
to be helpful for other researchers since the pro-
cess to extract the coreference chains of such a
large text corpus takes several weeks on multi-core
machines. In total, we extracted 3.1M coreference
chains. 2.7M of them consist of at least two differ-
ent markables. The median (mean) length of the
chains is 3 (4.0) and the median (mean) length of
a markable is 1 (2.7). To train word embeddings,
the markables of each coreference chain are con-
catenated to one text line. These lines are used as
input sentences for word2vec. We refer to the re-
sulting embeddings as coreference-based.
</bodyText>
<subsectionHeader confidence="0.999106">
2.2 Antonym detection
</subsectionHeader>
<bodyText confidence="0.999932666666667">
In the following experiments, we use word em-
beddings to discriminate antonyms from non-
antonyms. We formalize this as a supervised clas-
</bodyText>
<footnote confidence="0.999756666666667">
1https://code.google.com/p/word2vec
2LDC2012T21, Agence France-Presse 2010
3https://code.google.com/p/cistern
</footnote>
<bodyText confidence="0.9972365">
sification task and apply SVMs (Chang and Lin,
2011).
The following features are used to represent a
pair of two words w and v:
</bodyText>
<listItem confidence="0.9993815">
1. cosine similarity of the text-based embed-
dings of w and v;
2. inverse rank of v in the nearest text-based
neighbors of w;
3. cosine similarity of the coreference-based
embeddings of w and v;
4. inverse rank of v in the nearest coreference-
based neighbors of w;
5. difference of (1) and (3);
6. difference of (2) and (4).
</listItem>
<bodyText confidence="0.993674666666667">
We experiment with three different subsets of
these features: text-based (1 and 2), coreference-
based (3 and 4) and all features.
</bodyText>
<sectionHeader confidence="0.997288" genericHeader="method">
3 Experiments and results
</sectionHeader>
<subsectionHeader confidence="0.999987">
3.1 Qualitative analysis of word vectors
</subsectionHeader>
<bodyText confidence="0.999895789473684">
Table 1 lists the five nearest neighbors based on
cosine similarity of text-based and coreference-
based word vectors for “his” and “woman”.
We see that the two types of embeddings cap-
ture different notions of similarity. Unlike the text-
based neighbors, the coreference-based neighbors
have the same gender. The text-based neighbors
are mutually substitutable words, but substitution
seems to change the meaning more than for the
coreference-based neighbors.
In Figure 1, we illustrate the vectors for some
antonyms (connected by lines).
For reducing the dimensionality of the vector
space to 2D, we applied the t-SNE toolkit4. It uses
stochastic neighbor embedding with a Student’s
t-distribution to map high dimensional vectors
into a lower dimensional space (Van der Maaten
and Hinton, 2008). The Figure shows that the
coreference-based word embeddings are able to
</bodyText>
<footnote confidence="0.945164">
4http://homepage.tudelft.nl/19j49/t-SNE.html
</footnote>
<page confidence="0.97042">
1448
</page>
<figure confidence="0.999792944444445">
0.2
0.0
0.2
1.2
0.4
0.6
0.8
1.0
0.2
0.0
1.2
1.0
0.8
0.6
0.2
0.4
toughness
frailty
literacy
innocence
guilt
illiteracy
unwillingness
willingness
literacy
frailty
toughness
innocence
guilt
illiteracy
unwillingness
willingness
1.4
1.5 1.0 0.5 0.0 0.5 1.0
1.4
1.5 1.0 0.5 0.0 0.5 1.0
</figure>
<figureCaption confidence="0.999998">
Figure 1: 2D-positions of words in the text-based (top) and coreference-based embeddings (bottom)
</figureCaption>
<bodyText confidence="0.996827333333333">
enlarge the distance between antonyms (especially
for guilt vs. innocence and toughness vs. frailty)
compared to text-based word vectors.
</bodyText>
<subsectionHeader confidence="0.999867">
3.2 Quantitative analysis of word vectors
</subsectionHeader>
<bodyText confidence="0.999886454545455">
To verify that coreference-based embeddings bet-
ter represent semantic components relevant to
coreference, we split our coreference resource into
two parts (about 85% and 15% of the data), trained
embeddings on the first part and computed the co-
sine similarity – both text-based and coreference-
based – for each pair of words occurring in the
same coreference chain in the second part. The
statistics in Table 2 confirm that coreference-based
vectors have higher similarity within chains than
text-based vectors.
</bodyText>
<subsectionHeader confidence="0.990154">
3.3 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999384133333333">
We formalize antonym detection as a binary classi-
fication task. Given a target word w and one of its
nearest neighbors v, the classifier decides whether
v is an antonym of w. Our data set is a set of pairs,
each consisting of a target word w and a candi-
date v. For all word types of our vocabulary, we
search for antonyms using the online dictionary
Merriam Webster.5 The resulting list is provided
as an additional resource6. It contains 6225 words
with antonyms. Positive training examples are col-
lected by checking if the 500 nearest text-based
neighbors of w contain one of the antonyms listed
by Webster. Negative training examples are cre-
ated by replacing the antonym with a random word
from the 500 nearest neighbors that is not listed as
</bodyText>
<footnote confidence="0.999865">
5http://www.merriam-webster.com
6https://code.google.com/p/cistern
</footnote>
<bodyText confidence="0.999962333333333">
an antonym. By selecting both the positive and
the negative examples from the nearest neighbors
of the word vectors, we intend to develop a task
which is hard to solve: The classifier has to find
the small portion of semantically dissimilar words
(i.e., antonyms) among distributionally very simi-
lar words. The total number of positive and nega-
tive examples is 2337 each. The data are split into
training (80%), development (10%) and test (10%)
sets.
In initial experiments, we found only a small
difference in antonym classification performance
between text-based and coreference-based fea-
tures. When analyzing the errors, we realized that
our rationale for using coreference-based embed-
dings only applies to nouns, not to other parts of
speech. This will be discussed in detail below. We
therefore run our experiments in two modes: all
word classification (all pairs are considered) and
noun classification (only pairs are considered for
which the target word is a noun). We use the Stan-
ford part-of-speech tagger (Toutanova et al., 2003)
to determine whether a word is a noun or not.
Our classifier is a radial basis function (rbf) sup-
port vector machine (SVM). The rbf kernel per-
formed better than a linear kernel in initial exper-
iments. The SVM parameters C and γ are opti-
mized on the development set. The representation
of target-candidate pairs consists of the features
described in Section 2.
</bodyText>
<subsectionHeader confidence="0.994514">
3.4 Experimental results and discussion
</subsectionHeader>
<bodyText confidence="0.99962">
We perform the experiments with the three differ-
ent feature sets described in Section 2: text-based,
coreference-based and all features. Table 3 shows
</bodyText>
<page confidence="0.987014">
1449
</page>
<table confidence="0.9588487">
P development set classification test F1 P development set classification est se F1
all word F1 P se noun F1 P R
R R R
.83 .66 .74 .74 .55 .63 .91 .61 .73 .74 .51 .60
.67 .42 .51 .65 .43 .52 .86 .47 .61 .77 .45 .57
.79 .65 .72 .75 .58 .66 .88 .70 .78 .79 .61 .69
feature set
text-based
coreference-based
text+coref
</table>
<tableCaption confidence="0.926793">
Table 3: Results for different feature sets. Best result in each column in bold.
</tableCaption>
<table confidence="0.978037666666667">
minimum maximum median
text-based vectors -0.350 0.998 0.156
coref.-based vectors -0.318 0.999 0.161
</table>
<tableCaption confidence="0.995905">
Table 2: Cosine similarity of words in the same
coreference chain
</tableCaption>
<bodyText confidence="0.9984484">
results for development and test sets.
For all word classification, coreference-based
features do not improve performance on the de-
velopment set (e.g., F1 is .74 for text-based vs .72
for text+coref). On the test set, however, the com-
bination of all features (text+coref) has better per-
formance than text-based alone: .66 vs .63.
For noun classification, using coreference-
based features in addition to text-based features
improves results on development set (F1 is .78 vs
.73) and test set (.69 vs .60).
These results show that mined coreference
chains are a useful resource and provide infor-
mation that is complementary to other methods.
Even though adding coreference-based embed-
dings improves performance on antonym classi-
fication, the experiments also show that using
only coreference-based embeddings is almost al-
ways worse than using only text-based embed-
dings. This is not surprising given that the amount
of training data for the word embeddings is differ-
ent in the two cases. Coreference chains provide
only a small subset of the word-word relations that
are given to the word2vec skip-gram model when
applied to raw text. If the sizes of the training data
sets were similar in the two cases, we would ex-
pect performance to be comparable.
In the beginning, our hypothesis was that coref-
erence information should be helpful for antonym
classification in general. When we performed an
error analysis for our initial results, we realized
that this hypothesis only holds for nouns. Other
types of words cooccurring in coreference chains
are not more likely to be synonyms than words
cooccurring in text windows. Two contexts that
illustrate this point are “bright sides, but also dif-
ficult and dark ones” and “a series of black and
white shots” (elements of coreference chains in
italics). Thus, adjectives with opposite meanings
can cooccur in coreference chains just as they can
cooccur in window-based contexts. For nouns, it
is much less likely that the same coreference chain
will contain both a noun and its antonym since –
by definition – markables in a coreference chain
refer to the same identical entity.
</bodyText>
<sectionHeader confidence="0.999957" genericHeader="method">
4 Related work
</sectionHeader>
<bodyText confidence="0.999985193548387">
Traditionally, words have been represented by
vectors of the size of the vocabulary with a one at
the word index and zeros otherwise (one-hot vec-
tors). However, this approach cannot handle un-
known words (Turian et al., 2010) and similari-
ties among words cannot be represented (Mikolov
et al., 2013). Therefore, distributed word repre-
sentations (embeddings) become more and more
popular. They are low-dimensional, real-valued
vectors. Mikolov et al. (2013) have published
word2vec, a toolkit that provides different possi-
bilities to estimate word embeddings (cbow model
and skip-gram model). They show that the re-
sulting word vectors capture semantic and syntac-
tic relationships of words. Baroni et al. (2014)
show that word embeddings are able to outper-
form count based word vectors on a variety of
NLP tasks. Recently, Levy and Goldberg (2014)
have generalized the skip-gram model to include
not only linear but arbitrary contexts like contexts
derived from dependency parse trees. Andreas and
Klein (2014) investigate the amount of additional
information continuous word embeddings could
add to a constituency parser and find that most
of their information is redundant to what can be
learned from labeled parse trees. In (Yih et al.,
2012), the vector space representation of words is
modified so that high positive similarities are as-
signed to synonyms and high negative similarities
to antonyms. For this, latent semantic analysis is
applied to a matrix of thesaurus entries. The val-
</bodyText>
<page confidence="0.976769">
1450
</page>
<bodyText confidence="0.997807826923077">
ues representing antonyms are negated.
There has been a great deal of work on apply-
ing the vector space model and cosine similarity
to find synonyms or antonyms. Hagiwara et al.
(2006) represent each word as a vector with cooc-
currence frequencies of words and contexts as el-
ements, normalized by the inverse document fre-
quency. The authors investigate three types of con-
textual information (dependency, sentence cooc-
currence and proximity) and find that a combi-
nation of them leads to the most stable results.
Schulte im Walde and K¨oper (2013) build a vector
space model on lexico-syntactic patterns and ap-
ply a Rocchio classifier to distinguish synonyms
from antonyms, among other tasks. Van der Plas
and Tiedemann (2006) use automatically aligned
translations of the same text in different languages
to build context vectors. Based on these vectors,
they detect synonyms.
In contrast, there are also studies using linguis-
tic knowledge from external resources: Senellart
and Blondel (2008) propose a method for syn-
onym detection based on graph similarity in a
graph generated using the definitions of a mono-
lingual dictionary. Harabagiu et al. (2006) rec-
ognize antonymy by generating antonymy chains
based on WordNet relations. Mohammad et al.
(2008) look for the word with the highest degree of
antonymy to a given target word among five candi-
dates. For this task, they use thesaurus information
and the similarity of the contexts of two contrast-
ing words. Lin et al. (2003) use Hearst patterns
to distiguish synonyms from antonyms. Work by
Turney (2008) is similar except that the patterns
are learned.
Except for the publicly available coreference
resolution system, our approach does not need ex-
ternal resources such as dictionaries or bilingual
corpora and no human labor is required. Thus,
it can be easily applied to any corpus in any lan-
guage as long as there exists a coreference resolu-
tion system in this language. The pattern-based
approach (Lin et al., 2003; Turney, 2008) dis-
cussed above also needs few resources. In contrast
to our work, it relies on patterns and might there-
fore restrict the number of recognizable synonyms
and antonyms to those appearing in the context of
the pre-defined patterns. On the other hand, pat-
terns could explicitely distinguish contexts typical
for synonyms from contexts for antonyms. Hence,
we plan to combine our coreference-based method
with pattern-based methods in the future.
</bodyText>
<sectionHeader confidence="0.988967" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999609">
In this paper, we showed that mined corefer-
ence chains can be used for creating word em-
beddings that capture a type of semantic sim-
ilarity that is different from the one captured
by standard text-based embeddings. We showed
that coreference-based embeddings improve per-
formance of antonym classification by .09 F1
compared to using only text-based embeddings.
We achieved precision values of up to .79, recall
values of up to .61 and F1 scores of up to .69.
</bodyText>
<sectionHeader confidence="0.998562" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9973325">
This work was supported by DFG (grant SCHU
2246/4-2).
</bodyText>
<sectionHeader confidence="0.998873" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999471125">
Jacob Andreas and Dan Klein. 2014. How much do
word embeddings encode about syntax? In ACL,
pages 822–827.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In ACL, pages
238–247.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology
(TIST), 2(3):27.
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko
Toyama. 2006. Selection of effective contextual
information for automatic synonym acquisition. In
COLING/ACL, pages 353–360.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text
processing. In AAAI, volume 6, pages 755–762.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford’s multi-pass sieve corefer-
ence resolution system at the CoNLL-2011 shared
task. In CoNLL: Shared Task, pages 28–34.
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In ACL, pages 302–308.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among distribu-
tionally similar words. In IJCAI, pages 1492–1493.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. Workshop at ICLR.
</reference>
<page confidence="0.825509">
1451
</page>
<reference confidence="0.999751763157895">
Saif Mohammad, Bonnie Dorr, and Graeme Hirst.
2008. Computing word-pair antonymy. In EMNLP,
pages 982–991.
Marta Recasens, Llu´ıs M`arquez, Emili Sapena,
M Ant`onia Mart´ı, Mariona Taul´e, V´eronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In 5th International Workshop
on Semantic Evaluation, pages 1–8.
Sabine Schulte im Walde and Maximilian K¨oper. 2013.
Pattern-based distinction of paradigmatic relations
for German nouns, verbs, adjectives. In Language
Processing and Knowledge in the Web, pages 184–
198. Springer.
Pierre Senellart and Vincent D Blondel. 2008. Auto-
matic discovery of similar words. In Survey of Text
Mining II, pages 25–44. Springer.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In NAACL-HLT, pages 252–259.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In ACL, pages 384–
394.
Peter D. Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In
COLING, pages 905–912.
Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-SNE. Journal of Machine
Learning Research, 9(11):2579–2605.
Lonneke Van der Plas and J¨org Tiedemann. 2006.
Finding synonyms using automatic word alignment
and measures of distributional similarity. In COL-
ING/ACL, pages 866–873.
Wen-tau Yih, Geoffrey Zweig, and John C Platt.
2012. Polarity inducing latent semantic analysis. In
EMNLP/CoNLL, pages 1212–1222.
</reference>
<page confidence="0.994428">
1452
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.259503">
<title confidence="0.999809">Using Mined Coreference Chains as a Resource for a Semantic Task</title>
<author confidence="0.618241">Heike Adel</author>
<author confidence="0.618241">Hinrich</author>
<affiliation confidence="0.987739">Center for Information and Language University of</affiliation>
<email confidence="0.946819">heike.adel@cis.lmu.de</email>
<abstract confidence="0.945003363636364">We propose to use coreference chains extracted from a large corpus as a resource for semantic tasks. We extract three million coreference chains and train word embeddings on them. Then, we compare these embeddings to word vectors derived from raw text data and show that coreference-based word embeddings imthe task of antonym classification by up to .09.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jacob Andreas</author>
<author>Dan Klein</author>
</authors>
<title>How much do word embeddings encode about syntax?</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>822--827</pages>
<contexts>
<context position="15005" citStr="Andreas and Klein (2014)" startWordPosition="2397" endWordPosition="2400">ar. They are low-dimensional, real-valued vectors. Mikolov et al. (2013) have published word2vec, a toolkit that provides different possibilities to estimate word embeddings (cbow model and skip-gram model). They show that the resulting word vectors capture semantic and syntactic relationships of words. Baroni et al. (2014) show that word embeddings are able to outperform count based word vectors on a variety of NLP tasks. Recently, Levy and Goldberg (2014) have generalized the skip-gram model to include not only linear but arbitrary contexts like contexts derived from dependency parse trees. Andreas and Klein (2014) investigate the amount of additional information continuous word embeddings could add to a constituency parser and find that most of their information is redundant to what can be learned from labeled parse trees. In (Yih et al., 2012), the vector space representation of words is modified so that high positive similarities are assigned to synonyms and high negative similarities to antonyms. For this, latent semantic analysis is applied to a matrix of thesaurus entries. The val1450 ues representing antonyms are negated. There has been a great deal of work on applying the vector space model and </context>
</contexts>
<marker>Andreas, Klein, 2014</marker>
<rawString>Jacob Andreas and Dan Klein. 2014. How much do word embeddings encode about syntax? In ACL, pages 822–827.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="14706" citStr="Baroni et al. (2014)" startWordPosition="2349" endWordPosition="2352"> a one at the word index and zeros otherwise (one-hot vectors). However, this approach cannot handle unknown words (Turian et al., 2010) and similarities among words cannot be represented (Mikolov et al., 2013). Therefore, distributed word representations (embeddings) become more and more popular. They are low-dimensional, real-valued vectors. Mikolov et al. (2013) have published word2vec, a toolkit that provides different possibilities to estimate word embeddings (cbow model and skip-gram model). They show that the resulting word vectors capture semantic and syntactic relationships of words. Baroni et al. (2014) show that word embeddings are able to outperform count based word vectors on a variety of NLP tasks. Recently, Levy and Goldberg (2014) have generalized the skip-gram model to include not only linear but arbitrary contexts like contexts derived from dependency parse trees. Andreas and Klein (2014) investigate the amount of additional information continuous word embeddings could add to a constituency parser and find that most of their information is redundant to what can be learned from labeled parse trees. In (Yih et al., 2012), the vector space representation of words is modified so that hig</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In ACL, pages 238–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology (TIST),</booktitle>
<pages>2--3</pages>
<contexts>
<context position="6287" citStr="Chang and Lin, 2011" startWordPosition="991" endWordPosition="994"> length of the chains is 3 (4.0) and the median (mean) length of a markable is 1 (2.7). To train word embeddings, the markables of each coreference chain are concatenated to one text line. These lines are used as input sentences for word2vec. We refer to the resulting embeddings as coreference-based. 2.2 Antonym detection In the following experiments, we use word embeddings to discriminate antonyms from nonantonyms. We formalize this as a supervised clas1https://code.google.com/p/word2vec 2LDC2012T21, Agence France-Presse 2010 3https://code.google.com/p/cistern sification task and apply SVMs (Chang and Lin, 2011). The following features are used to represent a pair of two words w and v: 1. cosine similarity of the text-based embeddings of w and v; 2. inverse rank of v in the nearest text-based neighbors of w; 3. cosine similarity of the coreference-based embeddings of w and v; 4. inverse rank of v in the nearest coreferencebased neighbors of w; 5. difference of (1) and (3); 6. difference of (2) and (4). We experiment with three different subsets of these features: text-based (1 and 2), coreferencebased (3 and 4) and all features. 3 Experiments and results 3.1 Qualitative analysis of word vectors Table</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masato Hagiwara</author>
<author>Yasuhiro Ogawa</author>
<author>Katsuhiko Toyama</author>
</authors>
<title>Selection of effective contextual information for automatic synonym acquisition.</title>
<date>2006</date>
<booktitle>In COLING/ACL,</booktitle>
<pages>353--360</pages>
<contexts>
<context position="15675" citStr="Hagiwara et al. (2006)" startWordPosition="2507" endWordPosition="2510">n continuous word embeddings could add to a constituency parser and find that most of their information is redundant to what can be learned from labeled parse trees. In (Yih et al., 2012), the vector space representation of words is modified so that high positive similarities are assigned to synonyms and high negative similarities to antonyms. For this, latent semantic analysis is applied to a matrix of thesaurus entries. The val1450 ues representing antonyms are negated. There has been a great deal of work on applying the vector space model and cosine similarity to find synonyms or antonyms. Hagiwara et al. (2006) represent each word as a vector with cooccurrence frequencies of words and contexts as elements, normalized by the inverse document frequency. The authors investigate three types of contextual information (dependency, sentence cooccurrence and proximity) and find that a combination of them leads to the most stable results. Schulte im Walde and K¨oper (2013) build a vector space model on lexico-syntactic patterns and apply a Rocchio classifier to distinguish synonyms from antonyms, among other tasks. Van der Plas and Tiedemann (2006) use automatically aligned translations of the same text in d</context>
</contexts>
<marker>Hagiwara, Ogawa, Toyama, 2006</marker>
<rawString>Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko Toyama. 2006. Selection of effective contextual information for automatic synonym acquisition. In COLING/ACL, pages 353–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Andrew Hickl</author>
<author>Finley Lacatusu</author>
</authors>
<title>Negation, contrast and contradiction in text processing.</title>
<date>2006</date>
<booktitle>In AAAI,</booktitle>
<volume>6</volume>
<pages>755--762</pages>
<contexts>
<context position="16643" citStr="Harabagiu et al. (2006)" startWordPosition="2660" endWordPosition="2663">d K¨oper (2013) build a vector space model on lexico-syntactic patterns and apply a Rocchio classifier to distinguish synonyms from antonyms, among other tasks. Van der Plas and Tiedemann (2006) use automatically aligned translations of the same text in different languages to build context vectors. Based on these vectors, they detect synonyms. In contrast, there are also studies using linguistic knowledge from external resources: Senellart and Blondel (2008) propose a method for synonym detection based on graph similarity in a graph generated using the definitions of a monolingual dictionary. Harabagiu et al. (2006) recognize antonymy by generating antonymy chains based on WordNet relations. Mohammad et al. (2008) look for the word with the highest degree of antonymy to a given target word among five candidates. For this task, they use thesaurus information and the similarity of the contexts of two contrasting words. Lin et al. (2003) use Hearst patterns to distiguish synonyms from antonyms. Work by Turney (2008) is similar except that the patterns are learned. Except for the publicly available coreference resolution system, our approach does not need external resources such as dictionaries or bilingual </context>
</contexts>
<marker>Harabagiu, Hickl, Lacatusu, 2006</marker>
<rawString>Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu. 2006. Negation, contrast and contradiction in text processing. In AAAI, volume 6, pages 755–762.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task. In CoNLL: Shared Task,</title>
<date>2011</date>
<pages>28--34</pages>
<contexts>
<context position="5079" citStr="Lee et al., 2011" startWordPosition="801" endWordPosition="804">text data and embeddings derived from automatically extracted coreference chains. For the calculation of the vector representations, the word2vec toolkit1 by Mikolov et al. (2013) is applied. We use the skip-gram model for our experiments because its results for semantic similarity are better according to Mikolov et al. (2013). We train a first model on a subset of English Gigaword data.2 In the following sections, we call the resulting embeddings text-based. To improve the semantic similarities of the vectors, we prepare another training text consisting of coreference chains. We use CoreNLP (Lee et al., 2011) to extract coreference chains from the Gigaword corpus. Then we build a skip-gram model on these coreference chains. The extracted coreference chains are provided as an additional resource to this paper3. Although they have been developed using only a publicly available toolkit, we expect this resource to be helpful for other researchers since the process to extract the coreference chains of such a large text corpus takes several weeks on multi-core machines. In total, we extracted 3.1M coreference chains. 2.7M of them consist of at least two different markables. The median (mean) length of t</context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task. In CoNLL: Shared Task, pages 28–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>302--308</pages>
<contexts>
<context position="14842" citStr="Levy and Goldberg (2014)" startWordPosition="2373" endWordPosition="2376">010) and similarities among words cannot be represented (Mikolov et al., 2013). Therefore, distributed word representations (embeddings) become more and more popular. They are low-dimensional, real-valued vectors. Mikolov et al. (2013) have published word2vec, a toolkit that provides different possibilities to estimate word embeddings (cbow model and skip-gram model). They show that the resulting word vectors capture semantic and syntactic relationships of words. Baroni et al. (2014) show that word embeddings are able to outperform count based word vectors on a variety of NLP tasks. Recently, Levy and Goldberg (2014) have generalized the skip-gram model to include not only linear but arbitrary contexts like contexts derived from dependency parse trees. Andreas and Klein (2014) investigate the amount of additional information continuous word embeddings could add to a constituency parser and find that most of their information is redundant to what can be learned from labeled parse trees. In (Yih et al., 2012), the vector space representation of words is modified so that high positive similarities are assigned to synonyms and high negative similarities to antonyms. For this, latent semantic analysis is appli</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Dependencybased word embeddings. In ACL, pages 302–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Shaojun Zhao</author>
<author>Lijuan Qin</author>
<author>Ming Zhou</author>
</authors>
<title>Identifying synonyms among distributionally similar words.</title>
<date>2003</date>
<booktitle>In IJCAI,</booktitle>
<pages>1492--1493</pages>
<contexts>
<context position="16968" citStr="Lin et al. (2003)" startWordPosition="2716" endWordPosition="2719">tect synonyms. In contrast, there are also studies using linguistic knowledge from external resources: Senellart and Blondel (2008) propose a method for synonym detection based on graph similarity in a graph generated using the definitions of a monolingual dictionary. Harabagiu et al. (2006) recognize antonymy by generating antonymy chains based on WordNet relations. Mohammad et al. (2008) look for the word with the highest degree of antonymy to a given target word among five candidates. For this task, they use thesaurus information and the similarity of the contexts of two contrasting words. Lin et al. (2003) use Hearst patterns to distiguish synonyms from antonyms. Work by Turney (2008) is similar except that the patterns are learned. Except for the publicly available coreference resolution system, our approach does not need external resources such as dictionaries or bilingual corpora and no human labor is required. Thus, it can be easily applied to any corpus in any language as long as there exists a coreference resolution system in this language. The pattern-based approach (Lin et al., 2003; Turney, 2008) discussed above also needs few resources. In contrast to our work, it relies on patterns a</context>
</contexts>
<marker>Lin, Zhao, Qin, Zhou, 2003</marker>
<rawString>Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou. 2003. Identifying synonyms among distributionally similar words. In IJCAI, pages 1492–1493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. Workshop at ICLR.</title>
<date>2013</date>
<contexts>
<context position="4641" citStr="Mikolov et al. (2013)" startWordPosition="729" endWordPosition="732">girl, prostitute, lupita, betsy, lehia his woman Table 1: Nearest neighbors of “his” / “woman” for text-based &amp; coreference-based embeddings association and true synonymy. As a result, synonyms and antonyms may be mapped to similar word vectors (Yih et al., 2012). For many NLP tasks, however, information about true synonymy or antonymy may be important. In this paper, we develop two different word embeddings: embeddings calculated on raw text data and embeddings derived from automatically extracted coreference chains. For the calculation of the vector representations, the word2vec toolkit1 by Mikolov et al. (2013) is applied. We use the skip-gram model for our experiments because its results for semantic similarity are better according to Mikolov et al. (2013). We train a first model on a subset of English Gigaword data.2 In the following sections, we call the resulting embeddings text-based. To improve the semantic similarities of the vectors, we prepare another training text consisting of coreference chains. We use CoreNLP (Lee et al., 2011) to extract coreference chains from the Gigaword corpus. Then we build a skip-gram model on these coreference chains. The extracted coreference chains are provide</context>
<context position="14296" citStr="Mikolov et al., 2013" startWordPosition="2290" endWordPosition="2293">with opposite meanings can cooccur in coreference chains just as they can cooccur in window-based contexts. For nouns, it is much less likely that the same coreference chain will contain both a noun and its antonym since – by definition – markables in a coreference chain refer to the same identical entity. 4 Related work Traditionally, words have been represented by vectors of the size of the vocabulary with a one at the word index and zeros otherwise (one-hot vectors). However, this approach cannot handle unknown words (Turian et al., 2010) and similarities among words cannot be represented (Mikolov et al., 2013). Therefore, distributed word representations (embeddings) become more and more popular. They are low-dimensional, real-valued vectors. Mikolov et al. (2013) have published word2vec, a toolkit that provides different possibilities to estimate word embeddings (cbow model and skip-gram model). They show that the resulting word vectors capture semantic and syntactic relationships of words. Baroni et al. (2014) show that word embeddings are able to outperform count based word vectors on a variety of NLP tasks. Recently, Levy and Goldberg (2014) have generalized the skip-gram model to include not o</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. Workshop at ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Bonnie Dorr</author>
<author>Graeme Hirst</author>
</authors>
<title>Computing word-pair antonymy.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>982--991</pages>
<contexts>
<context position="16743" citStr="Mohammad et al. (2008)" startWordPosition="2675" endWordPosition="2678">r to distinguish synonyms from antonyms, among other tasks. Van der Plas and Tiedemann (2006) use automatically aligned translations of the same text in different languages to build context vectors. Based on these vectors, they detect synonyms. In contrast, there are also studies using linguistic knowledge from external resources: Senellart and Blondel (2008) propose a method for synonym detection based on graph similarity in a graph generated using the definitions of a monolingual dictionary. Harabagiu et al. (2006) recognize antonymy by generating antonymy chains based on WordNet relations. Mohammad et al. (2008) look for the word with the highest degree of antonymy to a given target word among five candidates. For this task, they use thesaurus information and the similarity of the contexts of two contrasting words. Lin et al. (2003) use Hearst patterns to distiguish synonyms from antonyms. Work by Turney (2008) is similar except that the patterns are learned. Except for the publicly available coreference resolution system, our approach does not need external resources such as dictionaries or bilingual corpora and no human labor is required. Thus, it can be easily applied to any corpus in any language</context>
</contexts>
<marker>Mohammad, Dorr, Hirst, 2008</marker>
<rawString>Saif Mohammad, Bonnie Dorr, and Graeme Hirst. 2008. Computing word-pair antonymy. In EMNLP, pages 982–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Llu´ıs M`arquez</author>
<author>Emili Sapena</author>
<author>M Ant`onia Mart´ı</author>
<author>Mariona Taul´e</author>
<author>V´eronique Hoste</author>
<author>Massimo Poesio</author>
<author>Yannick Versley</author>
</authors>
<date>2010</date>
<booktitle>Semeval-2010 task 1: Coreference resolution in multiple languages. In 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>1--8</pages>
<marker>Recasens, M`arquez, Sapena, Mart´ı, Taul´e, Hoste, Poesio, Versley, 2010</marker>
<rawString>Marta Recasens, Llu´ıs M`arquez, Emili Sapena, M Ant`onia Mart´ı, Mariona Taul´e, V´eronique Hoste, Massimo Poesio, and Yannick Versley. 2010. Semeval-2010 task 1: Coreference resolution in multiple languages. In 5th International Workshop on Semantic Evaluation, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Maximilian K¨oper</author>
</authors>
<title>Pattern-based distinction of paradigmatic relations for German nouns, verbs, adjectives.</title>
<date>2013</date>
<booktitle>In Language Processing and Knowledge in the Web,</booktitle>
<pages>184--198</pages>
<publisher>Springer.</publisher>
<marker>Walde, K¨oper, 2013</marker>
<rawString>Sabine Schulte im Walde and Maximilian K¨oper. 2013. Pattern-based distinction of paradigmatic relations for German nouns, verbs, adjectives. In Language Processing and Knowledge in the Web, pages 184– 198. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Senellart</author>
<author>Vincent D Blondel</author>
</authors>
<title>Automatic discovery of similar words.</title>
<date>2008</date>
<booktitle>In Survey of Text Mining II,</booktitle>
<pages>25--44</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="16482" citStr="Senellart and Blondel (2008)" startWordPosition="2633" endWordPosition="2636"> of contextual information (dependency, sentence cooccurrence and proximity) and find that a combination of them leads to the most stable results. Schulte im Walde and K¨oper (2013) build a vector space model on lexico-syntactic patterns and apply a Rocchio classifier to distinguish synonyms from antonyms, among other tasks. Van der Plas and Tiedemann (2006) use automatically aligned translations of the same text in different languages to build context vectors. Based on these vectors, they detect synonyms. In contrast, there are also studies using linguistic knowledge from external resources: Senellart and Blondel (2008) propose a method for synonym detection based on graph similarity in a graph generated using the definitions of a monolingual dictionary. Harabagiu et al. (2006) recognize antonymy by generating antonymy chains based on WordNet relations. Mohammad et al. (2008) look for the word with the highest degree of antonymy to a given target word among five candidates. For this task, they use thesaurus information and the similarity of the contexts of two contrasting words. Lin et al. (2003) use Hearst patterns to distiguish synonyms from antonyms. Work by Turney (2008) is similar except that the patter</context>
</contexts>
<marker>Senellart, Blondel, 2008</marker>
<rawString>Pierre Senellart and Vincent D Blondel. 2008. Automatic discovery of similar words. In Survey of Text Mining II, pages 25–44. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>NAACL-HLT,</booktitle>
<pages>252--259</pages>
<contexts>
<context position="10743" citStr="Toutanova et al., 2003" startWordPosition="1697" endWordPosition="1700">opment (10%) and test (10%) sets. In initial experiments, we found only a small difference in antonym classification performance between text-based and coreference-based features. When analyzing the errors, we realized that our rationale for using coreference-based embeddings only applies to nouns, not to other parts of speech. This will be discussed in detail below. We therefore run our experiments in two modes: all word classification (all pairs are considered) and noun classification (only pairs are considered for which the target word is a noun). We use the Stanford part-of-speech tagger (Toutanova et al., 2003) to determine whether a word is a noun or not. Our classifier is a radial basis function (rbf) support vector machine (SVM). The rbf kernel performed better than a linear kernel in initial experiments. The SVM parameters C and γ are optimized on the development set. The representation of target-candidate pairs consists of the features described in Section 2. 3.4 Experimental results and discussion We perform the experiments with the three different feature sets described in Section 2: text-based, coreference-based and all features. Table 3 shows 1449 P development set classification test F1 P </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In NAACL-HLT, pages 252–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="14222" citStr="Turian et al., 2010" startWordPosition="2278" endWordPosition="2281">ite shots” (elements of coreference chains in italics). Thus, adjectives with opposite meanings can cooccur in coreference chains just as they can cooccur in window-based contexts. For nouns, it is much less likely that the same coreference chain will contain both a noun and its antonym since – by definition – markables in a coreference chain refer to the same identical entity. 4 Related work Traditionally, words have been represented by vectors of the size of the vocabulary with a one at the word index and zeros otherwise (one-hot vectors). However, this approach cannot handle unknown words (Turian et al., 2010) and similarities among words cannot be represented (Mikolov et al., 2013). Therefore, distributed word representations (embeddings) become more and more popular. They are low-dimensional, real-valued vectors. Mikolov et al. (2013) have published word2vec, a toolkit that provides different possibilities to estimate word embeddings (cbow model and skip-gram model). They show that the resulting word vectors capture semantic and syntactic relationships of words. Baroni et al. (2014) show that word embeddings are able to outperform count based word vectors on a variety of NLP tasks. Recently, Levy</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In ACL, pages 384– 394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>A uniform approach to analogies, synonyms, antonyms, and associations.</title>
<date>2008</date>
<booktitle>In COLING,</booktitle>
<pages>905--912</pages>
<contexts>
<context position="17048" citStr="Turney (2008)" startWordPosition="2730" endWordPosition="2731">xternal resources: Senellart and Blondel (2008) propose a method for synonym detection based on graph similarity in a graph generated using the definitions of a monolingual dictionary. Harabagiu et al. (2006) recognize antonymy by generating antonymy chains based on WordNet relations. Mohammad et al. (2008) look for the word with the highest degree of antonymy to a given target word among five candidates. For this task, they use thesaurus information and the similarity of the contexts of two contrasting words. Lin et al. (2003) use Hearst patterns to distiguish synonyms from antonyms. Work by Turney (2008) is similar except that the patterns are learned. Except for the publicly available coreference resolution system, our approach does not need external resources such as dictionaries or bilingual corpora and no human labor is required. Thus, it can be easily applied to any corpus in any language as long as there exists a coreference resolution system in this language. The pattern-based approach (Lin et al., 2003; Turney, 2008) discussed above also needs few resources. In contrast to our work, it relies on patterns and might therefore restrict the number of recognizable synonyms and antonyms to </context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter D. Turney. 2008. A uniform approach to analogies, synonyms, antonyms, and associations. In COLING, pages 905–912.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurens Van der Maaten</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Visualizing data using t-SNE.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>9</volume>
<issue>11</issue>
<marker>Van der Maaten, Hinton, 2008</marker>
<rawString>Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of Machine Learning Research, 9(11):2579–2605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke Van der Plas</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Finding synonyms using automatic word alignment and measures of distributional similarity.</title>
<date>2006</date>
<booktitle>In COLING/ACL,</booktitle>
<pages>866--873</pages>
<marker>Van der Plas, Tiedemann, 2006</marker>
<rawString>Lonneke Van der Plas and J¨org Tiedemann. 2006. Finding synonyms using automatic word alignment and measures of distributional similarity. In COLING/ACL, pages 866–873.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
<author>John C Platt</author>
</authors>
<title>Polarity inducing latent semantic analysis.</title>
<date>2012</date>
<booktitle>In EMNLP/CoNLL,</booktitle>
<pages>1212--1222</pages>
<contexts>
<context position="4283" citStr="Yih et al., 2012" startWordPosition="676" endWordPosition="679"> distinguish between semantic 1447 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1447–1452, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics text-based coref.-based my, their, her, your, our he, him, himself, zechariah, ancestor man, girl, believer, pharisee, guy girl, prostitute, lupita, betsy, lehia his woman Table 1: Nearest neighbors of “his” / “woman” for text-based &amp; coreference-based embeddings association and true synonymy. As a result, synonyms and antonyms may be mapped to similar word vectors (Yih et al., 2012). For many NLP tasks, however, information about true synonymy or antonymy may be important. In this paper, we develop two different word embeddings: embeddings calculated on raw text data and embeddings derived from automatically extracted coreference chains. For the calculation of the vector representations, the word2vec toolkit1 by Mikolov et al. (2013) is applied. We use the skip-gram model for our experiments because its results for semantic similarity are better according to Mikolov et al. (2013). We train a first model on a subset of English Gigaword data.2 In the following sections, we</context>
<context position="15240" citStr="Yih et al., 2012" startWordPosition="2435" endWordPosition="2438">vectors capture semantic and syntactic relationships of words. Baroni et al. (2014) show that word embeddings are able to outperform count based word vectors on a variety of NLP tasks. Recently, Levy and Goldberg (2014) have generalized the skip-gram model to include not only linear but arbitrary contexts like contexts derived from dependency parse trees. Andreas and Klein (2014) investigate the amount of additional information continuous word embeddings could add to a constituency parser and find that most of their information is redundant to what can be learned from labeled parse trees. In (Yih et al., 2012), the vector space representation of words is modified so that high positive similarities are assigned to synonyms and high negative similarities to antonyms. For this, latent semantic analysis is applied to a matrix of thesaurus entries. The val1450 ues representing antonyms are negated. There has been a great deal of work on applying the vector space model and cosine similarity to find synonyms or antonyms. Hagiwara et al. (2006) represent each word as a vector with cooccurrence frequencies of words and contexts as elements, normalized by the inverse document frequency. The authors investiga</context>
</contexts>
<marker>Yih, Zweig, Platt, 2012</marker>
<rawString>Wen-tau Yih, Geoffrey Zweig, and John C Platt. 2012. Polarity inducing latent semantic analysis. In EMNLP/CoNLL, pages 1212–1222.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
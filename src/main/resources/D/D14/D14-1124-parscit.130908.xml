<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000625">
<title confidence="0.998658">
Detecting Disagreement in Conversations using Pseudo-Monologic
Rhetorical Structure
</title>
<author confidence="0.996483">
Kelsey Allen Giuseppe Carenini Raymond T. Ng
</author>
<affiliation confidence="0.999217">
Department of Computer Science, University of British Columbia
</affiliation>
<address confidence="0.964358">
Vancouver, BC, V6T 1Z4, Canada
</address>
<email confidence="0.993334">
{kelseyra, carenini, rng}@cs.ubc.ca
</email>
<sectionHeader confidence="0.993875" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999903318181818">
Casual online forums such as Reddit,
Slashdot and Digg, are continuing to in-
crease in popularity as a means of com-
munication. Detecting disagreement in
this domain is a considerable challenge.
Many topics are unique to the conversa-
tion on the forum, and the appearance
of disagreement may be much more sub-
tle than on political blogs or social me-
dia sites such as twitter. In this analy-
sis we present a crowd-sourced annotated
corpus for topic level disagreement detec-
tion in Slashdot, showing that disagree-
ment detection in this domain is difficult
even for humans. We then proceed to
show that a new set of features determined
from the rhetorical structure of the con-
versation significantly improves the per-
formance on disagreement detection over
a baseline consisting of unigram/bigram
features, discourse markers, structural fea-
tures and meta-post features.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999935392857143">
How does disagreement arise in conversation? Be-
ing able to detect agreement and disagreement has
a range of applications. For an online educator,
dissent over a newly introduced topic may alert
the teacher to fundamental misconceptions about
the material. For a business, understanding dis-
putes over features of a product may be helpful in
future design iterations. By better understanding
how debate arises and propagates in a conversa-
tion, we may also gain insight into how authors’
opinions on a topic can be influenced over time.
The long term goal of our research is to lay
the foundations for understanding argumentative
structure in conversations, which could be applied
to NLP tasks such as summarization, information
retrieval, and text visualization. Argumentative
structure theory has been thoroughly studied in
both psychology and rhetoric, with negation and
discourse markers, as well as hedging and dis-
preferred responses, being known to be indicative
of argument (Horn, 1989; Brown and Levinson,
1987). As a starting point, in this paper we focus
on the detection of disagreement in casual con-
versations between users. This requires a gener-
alized approach that can accurately identify dis-
agreement in topics ranging from something as
mundane as whether GPS stands for galactic po-
sitioning system or global positioning system, to
more ideological debates about distrust in science.
Motivated by the widespread consensus in both
computational and theoretical linguistics on the
utility of discourse markers for signalling prag-
matic functions such as disagreement and personal
opinions (Webber and Prasad, 2008; Abbott et
al., 2011; J. E. Fox-Tree, 2010), we introduce a
new set of features based on the Discourse Tree
(DT) of a conversational text. Discourse Trees
were formalized by Mann and Thompson (1988)
as part of their Rhetorical Structure Theory (RST)
to represent the structure of discourse. Although
this theory is for monologic discourse, we propose
to treat conversational dialogue as a collection of
linked monologues, and subsequently build a rela-
tion graph describing both rhetorical connections
within user posts, as well as between different
users. Features obtained from this graph offer sig-
nificant improvements on disagreement detection
over a baseline consisting of meta-post features,
lexical features, discourse markers and conversa-
tional features. Not only do these features improve
disagreement detection, but the discovered impor-
tance of relations known to be theoretically rele-
vant to disagreement detection, such as COMPAR-
ISON (Horn, 1989), suggest that this approach may
be capturing the essential aspects of the conversa-
tional argumentative structure.
</bodyText>
<page confidence="0.96749">
1169
</page>
<note confidence="0.8995945">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1169–1180,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999703705882353">
As a second contribution of this work, we pro-
vide a new dataset consisting of 95 topics anno-
tated for disagreement. Unlike the publicly avail-
able ARGUE corpus based on the online debate
forum 4forums.com (Abbott et al., 2011), our cor-
pus is based on Slashdot, which is a general pur-
pose forum not targeted to debates. Therefore, we
expect that detecting disagreement may be a more
difficult task in our new corpus, as certain topics
(like discussing GPS systems) may be targeted to-
wards objective information sharing without any
participants expressing opinions or stances. Be-
cause of this, our corpus represents an excellent
testbed to examine methods for more subtle dis-
agreement detection, as well as the major differ-
ences between news-style and argument-style dia-
logue.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999975060240964">
In the past decade, there have been a number of
computational approaches developed for the task
of disagreement and controversy detection, partic-
ularly in synchronous conversations such as meet-
ings (Somasundaran et al., 2007; Raaijmakers et
al., 2008) and in monologic corpora such as news
collections (Awadallah et al., 2012) and reviews
(Popescu et al., 2005; Mukherjee and Liu, 2012).
In the domain of synchronous conversations,
prosodic features such as duration, speech rate and
pause have been used for spoken dialogue (Wang
et al., 2011; Galley et al., 2004). Galley et al.
(2004) found that local features, such as lexical
and structural features, as well as global contex-
tual features, were particularly useful for identify-
ing agreement/disagreement in the ICSI meeting
corpus. Germesin and Wilson (2009) also showed
accuracies of 98% in detecting agreement in the
AMI corpus using lexical, subjectivity and dia-
logue act features. However, they note that their
system could not classify disagreement accurately
due to the small number of training examples in
this category. Somasundaran et al. additionally
show that dialogue act features complement lexi-
cal features in the AMI meeting corpus (Somasun-
daran et al., 2007). These observations are taken
into account with our feature choices, and we use
contextual, discourse and lexical features in our
analysis.
In the monologic domain, Conrad et al. (2012)
recently found rhetorical relations to be useful for
argument labelling and detection in articles on the
topic of healthcare. Additionally, discourse mark-
ers and sentiment features have been found to as-
sist with disagreement detection in collections of
news documents on a particular topic, as well as
reviews (Choi et al., 2010; Awadallah et al., 2012;
Popescu et al., 2005).
In the asynchronous domain, there has been re-
cent work in disagreement detection, especially as
it pertains to stance identification. Content based
features, including sentiment, duration, and dis-
course markers have been used for this task (Yin
et al., 2012; Somasundaran and Wiebe, 2009;
Somasundaran and Wiebe, 2010). The structure
of a conversation has also been used, although
these approaches have focused on simple rules for
disagreement identification (Murakami and Ray-
mond, 2010), or have assumed that adjacent posts
always disagree (Agrawal et al., 2003). More re-
cent work has focused on identifying users’ atti-
tudes towards each other (Hassan et al., 2010), in-
fluential users and posts (Nguyen et al., 2014), as
well as identifying subgroups of users who share
viewpoints (Abu-Jbara et al., 2010). In Slashdot,
the h-index of a discussion has been used to rank
articles according to controversiality, although no
quantitative evaluation of this approach has been
given, and, unlike in our analysis, they did not
consider any other features (Gomez et al., 2008).
Content based features such as polarity and co-
sine similarity have also been used to study influ-
ence, controversy and opinion changes on micro-
blogging sites such as Twitter (Lin et al., 2013;
Popescu and Pennacchiotti, 2010).
The simplified task of detecting disagreement
between just two users (either a question/response
pair (Abbott et al., 2011) or two adjacent para-
graphs (Misra and Walker, 2013)) has also been re-
cently approached on the ARGUE corpus. Abbott
et al. (2011) use discourse markers, generalized
dependency features, punctuation and structural
features, while Misra and Walker (2013) focus on
n-grams indicative of denial, hedging and agree-
ment, as well as cue words and punctuation. Most
similar to our work is that by Mishne and Glance
(2006). They performed a general analysis of we-
blog comments, using punctuation, quotes, lexi-
con counts, subjectivity, polarity and referrals to
detect disputative and non-disputative comments.
Referrals and questions, as well as polarity mea-
sures in the first section of the post, were found to
be most useful. However, their analysis did not
</bodyText>
<page confidence="0.984591">
1170
</page>
<table confidence="0.9995656">
Type Num P/A S/P W/P Num Authors W/S TBP TT TP Length
Disagreement - C 19.00 1.02 3.21 65.86 15.84 20.47 4.60 50.90 16.21 49.11
Disagreement - NC 27.00 1.00 3.08 59.80 14.07 19.33 3.89 42.29 14.11 42.26
No disagreement - NC 28.00 1.03 2.85 57.25 10.29 19.94 6.83 50.12 10.50 28.00
No disagreement - C 21.00 1.00 3.69 69.66 6.29 20.22 6.14 18.22 6.29 19.81
</table>
<tableCaption confidence="0.998873">
Table 1: Characteristics of the four categories determined from the crowd-sourced annotation. All values except for the number
</tableCaption>
<bodyText confidence="0.972301791666667">
of topics in the category are given as the average score per topic across all topics in that category. Key: C and NC: Confident
(score ≥ 0.75) and Not confident (score &lt; 0.75), Num: Number of topics in category, P/A: Posts per author, S/P: Sentences
per post, W/P: Words per post, Num Authors: Number of authors, W/S: Words per sentence, TBP: Time between posts
(minutes), TT: Total time in minutes, TP: Total posts, and Length: Length of topic in sentences
take into account many features that have been
subsequently shown to be relevant, such as dis-
course markers and conversational structure, and
was hampered by a severe imbalance in the test
set (with very few disputative comments).
Our method takes advantage of insights from
many of these previous studies, focusing on dis-
cussion thread structure as well as text based fea-
tures to form our basic feature set. It is unlike
Mishne and Glance’s work in that we incorporate
several new features, have a balanced testing and
training set, and only use comments from one type
of online blog. Furthermore, it is a very different
task from those so far performed on the ARGUE
corpus, as we consider topics discussed by more
than two users. We aim to compare our features to
those found to be previously useful in these related
tasks, and expect similar feature sets to be useful
for the task of disagreement detection in this new
corpus.
</bodyText>
<sectionHeader confidence="0.995378" genericHeader="method">
3 Corpus
</sectionHeader>
<bodyText confidence="0.999662388888889">
The corpus stems from the online forum Slash-
dot.1 Slashdot is a casual internet forum, includ-
ing sections for users to ask questions, post arti-
cles, and review books and games. For the task of
disagreement detection, we focus our analysis on
the section of the site where users can post arti-
cles, and then comment either on the article or re-
spond to other users’ posts. This results in a tree-
like dialogue structure for which the posted arti-
cle is the root, and branches correspond to threads
of comments. Each comment has a timestamp at
the minute resolution as well as author information
(although it is possible to post on the forum anony-
mously). Additionally, other users can give differ-
ent posts scores (in the range -1 to 5) as well as cat-
egorizing posts under “funny”, “interesting”, “in-
formative”, “insightful”, “flamebait”, “off topic”,
or “troll”. This user moderation, as well as the
</bodyText>
<footnote confidence="0.963367">
1www.slashdot.org
</footnote>
<bodyText confidence="0.984609925">
formalized reply-to structure between comments,
makes Slashdot attractive over other internet fo-
rums as it allows for high-quality and structured
conversations.
In a previous study, Joty et al. (2013) selected
20 articles and their associated comments to be an-
notated for topic segmentation boundaries and la-
bels by an expert Slashdot contributor. They de-
fine a topic as a subset of the utterances in a con-
versation, while a topic label describes what the
given topic is about (e.g., Physics in videogames).
Of the 98 annotated topics from their dataset, we
filtered out those with only one contributing user,
for a total of 95 topics. Next, we developed a
Human Intelligence Task (HIT) using the crowd-
sourcing platform Crowdflower.2 The objective of
this task was to both develop a corpus for testing
our disagreement detection system, as well as to
investigate how easily human annotators can de-
tect disagreement in casual online forums. For
training, users were shown 3 sample topics, la-
belling them as containing disagreement or not. In
each round, annotators were shown 5 topics, with
a set of radio buttons for participants to choose
“Yes”, “No”, or “Not sure” in response to asking
whether or not the users in the conversation dis-
agree on the topic. In order to limit the number of
spam responses, users were shown test questions,
which consisted of topics where there was obvi-
ous disagreement, as well as topics where there
was obviously no disagreement (either agreement,
or more news-style information sharing). We re-
quired that users correctly identify 4 of these test
topics before they were allowed to continue with
the task. Users were also shown test questions
throughout the task, which, if answered incor-
rectly, would reduce the amount of money they re-
ceived for the task, and ultimately disqualify them.
For each topic, five different judgements were
obtained. We consider the trust of each partici-
</bodyText>
<footnote confidence="0.987211">
2www.Crowdflower.com
</footnote>
<page confidence="0.98968">
1171
</page>
<figure confidence="0.988897">
(a) Discourse tree (b) Relation graph
</figure>
<figureCaption confidence="0.9937155">
Figure 1: Discourse tree (left) with extracted relation graph (right) for a sample conversation involving three users with three
different posts P1, P2 and P3. N1, N2 and N3 are the corresponding nodes in the relation graph.
</figureCaption>
<bodyText confidence="0.9990292">
pant as the fraction of test questions which they
answered correctly. Then, each topic is assigned
a score according to a weighted average of the re-
sponses, with the weight being the trust of each
participant:
</bodyText>
<equation confidence="0.979322">
(testcorrect) × (0, 0.5, 1)
test useri
total
(1)
</equation>
<bodyText confidence="0.999995285714286">
where 0, 0.5 and 1 represent the answers “No”,
“Not sure” and “Yes” to the question of disagree-
ment existence, and A is a normalization factor.
If the score is less than 0.5, its confidence would
be 1−score towards “No disagreement”, whereas
greater than 0.5 would be a confidence of score
towards “Disagreement”. The average confidence
score across all topics was 0.73. Our corpus con-
sists of 49 topics without disagreement and 46 top-
ics with disagreement. Interestingly, 22 topics had
confidence scores below 55%, which suggests that
subtle disagreement detection is a subjective and
difficult task. Further statistics for the developed
corpus are given in Table 1.
</bodyText>
<sectionHeader confidence="0.999911" genericHeader="method">
4 Features for Disagreement Detection
</sectionHeader>
<bodyText confidence="0.999923">
The features we use in our experiments combine
information from conversational structure, rhetor-
ical relations, sentiment features, n-gram models,
Slashdot meta-features, structural features, and
lexicon features.
</bodyText>
<subsectionHeader confidence="0.994532">
4.1 Rhetorical Relation Graphs
</subsectionHeader>
<bodyText confidence="0.999965046511628">
Discourse markers have been found to aid in
argument and disagreement detection, and for
tasks such as stance identification (Abbott et al.,
2011; Misra and Walker, 2013; Somasundaran
and Wiebe, 2009). We aim to improve over dis-
course markers by capturing the underlying dis-
course structure of the conversation in terms of
discourse relations.
In Rhetorical Structure Theory, Discourse trees
are a hierarchical representation of document
structure for monologues (Mann and Thompson,
1988). At the lowest level, Elementary Discourse
Units (EDUs) are connected by discourse rela-
tions (such as ELABORATION and COMPARISON),
which in turn form larger discourse units that are
also linked by these relations. Computational sys-
tems (discourse parsers) have been recently devel-
oped to automatically generate a discourse tree for
a given monologue (Joty et al., 2013). Although
theoretically the rhetorical relations expected in
dialogues are different from those in monologues
(Stent and Allen, 2000), no sophisticated compu-
tational tools exist yet for detecting these relations
reliably. The core idea of this work is that some
useful (although noisy) information about the dis-
course structure of a conversation can be obtained
by applying state-of-the-art document level dis-
course parsing to parts of the conversation.
More specifically, posts on a particular topic
are concatenated according to their temporal order.
This pseudo-monologic document is then fed to a
publicly available document level discourse parser
(Joty et al., 2013). A discourse tree such as that
seen in Figure 1a is output by the parser. Then,
we extract the novel relation graph (Figure 1b)
from the discourse tree. In this graph, each node
(N1, N2, N3) corresponds to a post (P1, P2, P3)
and links aim to capture the argumentative struc-
ture. There are three cases when a link is added
between two nodes in the relation graph. Firstly,
links existing between two posts directly, such as
the COMPARISON relation between P2 and P3, are
added between the corresponding nodes in the re-
</bodyText>
<equation confidence="0.534296">
�score = A
users
</equation>
<page confidence="0.924621">
1172
</page>
<bodyText confidence="0.99951602">
lation graph (N2 and N3). Secondly, links existing
between fractions of posts in the discourse tree are
added to the relation graph (e.g. if (S2,P2) was
connected to (S1,P3) directly, N2 and N3 would
have an additional link with that label). Finally,
when posts are connected through internal nodes
(such as P1 and I1 in Figure 1a), a labelled link is
added for each post in the internal node to the re-
lation graph (N1-&gt;N2 and N1-&gt;N3 in Figure 1b).
This relation graph allows for the extraction
of many features that may reflect argumenta-
tive structure, such as the number of connec-
tions, the frequency of each rhetorical relation
in the graph per post (diff per post), and the
frequency as a percentage of all rhetorical rela-
tions (diff percentage). For example, COMPAR-
ISON relations are known to indicate disagree-
ment (Horn, 1989), so we expect higher fre-
quencies of this relation if the conversation con-
tains argument. Features from the discourse tree
such as the average depth of each rhetorical re-
lation are also added to reflect the cohesiveness
of conversation. Moreover, features combining
the graph and tree representations, such as the ra-
tio of the frequency of a rhetorical relation occur-
ring between different posts to the average depth
(CONTRAST between different posts) called avg ratio
Average depth of CONTRAST
are implemented. These reflect the hypothesis
that relations connecting larger chunks of text (or
whole posts) may be more important than those
connecting sentences or only partial posts.
Finally, the sub-trees corresponding to individ-
ual posts are used to extract the average frequency
of rhetorical relations within a post (same per
post) and the average frequency of a rhetorical re-
lation with respect to other rhetorical relations in
the post (same percentage). A measure of how
often a rhetorical relation connects different users
compared to how often it connects discourse units
in the same post (same to diff), is also added.
These capture the intuition that certain rhetorical
relations such as CAUSE, EVIDENCE and EXPLA-
NATION are expected to appear more within a post
if users are trying to support their perspective in
an argument. In total, there are 18 (rhetorical
relations)x7 (avg ratio, avg depth, same per post,
same percentage, diff percentage, diff per post,
same to diff) + 1 (number of connections) = 127
features.
</bodyText>
<subsectionHeader confidence="0.977047">
4.2 Discourse Markers
</subsectionHeader>
<bodyText confidence="0.999992166666667">
Motivated by previous work, we include a fre-
quency count of 17 discourse markers which were
found to be the most common across the ARGUE
corpus (Abbott et al., 2011). Furthermore, we hy-
pothesize that individual discourse markers might
have low frequency counts in the text. Therefore,
we also include an aggregated count of all 17 dis-
course markers in each fifth of the posts in a topic
(e.g. the count of all 17 discourse markers in the
first fifth of every post in the topic). Altogether,
there are 5 aggregated discourse marker features
in addition to the 17 frequency count features.
</bodyText>
<subsectionHeader confidence="0.998901">
4.3 Sentiment Features
</subsectionHeader>
<bodyText confidence="0.999923114285714">
Sentiment polarity features have been shown to be
useful in argument detection (Mishne and Glance,
2006). For this work, we use four sentiment
scoring categories: the variance, average score,
number of negative sentences, and controversiality
score (Carenini and Cheung, 2008) of sentences
in a post. These are determined using SoCAL
(Taboada et al., 2011), which gives each sentence
a polarity score and has been shown to work well
on user-generated content.
Overall, we have two main classes of sentiment
features. The first type splits all the posts in a topic
into 4 sections corresponding to the sentences in
each quarter of the post. The sentiment scores de-
scribed above are then applied to each section of
the posts (e.g. one feature is the number of neg-
ative sentences in the first quarter of each post).
As a separate feature, we also include the scores
on just the first sentence, as Mishne and Glance
(2006) previously found this to be beneficial. This
gives a total of 4x5 = 20 features. We refer to this
set as “sentiment”.
Motivated by the rhetorical features, our sec-
ond main class of sentiment features aims to iden-
tify “more important” posts for argument detec-
tion by applying the four categories of sentiment
scores to only those posts connected by each of
our 18 rhetorical relations. This is done for both
posts with an inner rhetorical connection (iden-
tified by the sub-tree for that post), as well as
for posts connected by a rhetorical relation in
the relation graph. This results in a total of (4
sentiment categories)x(2 (different + same post
connections))x(18 rhetorical relations) = 144 fea-
tures. This set is referred to as “RhetSent”.
</bodyText>
<page confidence="0.951825">
1173
</page>
<subsectionHeader confidence="0.993952">
4.4 Fragment Quotation Graphs
</subsectionHeader>
<bodyText confidence="0.999965838709677">
As previously noted in (Murakami and Raymond,
2010; Gomez et al., 2008), the structure of dis-
cussion threads can aid in disagreement detec-
tion. In online, threaded conversations, the stan-
dard approach to extracting conversational struc-
ture is through reply-to relations usually present
in online forums. However, if users strongly dis-
agree on a topic (or sub-topic), they may choose
to quote a specific paragraph (defined as a frag-
ment) of a previous post in their reply. Being able
to determine which specific fragments are linked
by relations may then be useful for more targeted
content-based features, helping to reduce noise. In
order to address this, we use the Fragment Quo-
tation Graph (FQG), an approach previously de-
veloped by Carenini et al. (2007) for dialogue
act modelling and topic segmentation (Joty et al.,
2011; Joty et al., 2013).
For our analysis, the FQG is found over the
entire Slashdot article. We then select the sub-
graph corresponding to those fragments in the tar-
get topic. From the fragment quotation sub-graph,
we are then able to extract features for disagree-
ment detection such as the number of connections,
total number of fragments, and the average path
length between nodes which we hypothesize to
be useful. We additionally extract the h-index
(Gomez et al., 2008) and average branching ratio
per fragment of the topic from the simpler reply-to
conversational structure. In total, there are 8 FQG
features.
</bodyText>
<subsectionHeader confidence="0.992408">
4.6 Structural Features
</subsectionHeader>
<bodyText confidence="0.96885775">
Length features have been well documented in
the literature to provide useful information about
whether or not arguments exist, especially in on-
line conversations that may be more informative
than subjective (Biyani et al., 2014; Yin et al.,
2012). In this work, length features include the
length of the post in sentences, the average num-
ber of words per sentence, the average number of
sentences per post, the number of contributing au-
thors, the rate of posting, and the total amount of
time of the conversation. This results in a total of
9 features.
</bodyText>
<subsectionHeader confidence="0.960599">
4.7 Punctuation
</subsectionHeader>
<bodyText confidence="0.9999894">
Like many other features already described, fre-
quency counts of ‘?’,‘!’,‘”’,‘”, and ‘.’ are found for
each fifth of the post (the first fifth, second fifth,
etc.). These counts are then aggregated across all
posts for a total of 5×5 = 25 features.
</bodyText>
<subsectionHeader confidence="0.919885">
4.8 Referrals
</subsectionHeader>
<bodyText confidence="0.99994425">
Referrals have been found to help with the detec-
tion of disagreement (Mishne and Glance, 2006),
especially with respect to other authors. Since
there are no direct referrals to previous authors
in this corpus, references to variations of “you”,
“they”, “us”, “I”, and “he/she” in each fifth of the
post are included instead, for a total of 5×5 = 25
features.
</bodyText>
<subsectionHeader confidence="0.9079655">
4.9 Meta-Post Features
4.5 N-gram models
</subsectionHeader>
<bodyText confidence="0.9860635625">
Slashdot allows users to rank others’ posts with the
As noted previously (Somasundaran and Wiebe, equivalent of a “like” button, changing the “score”
2010; Thomas et al., 2006), it is often difficult to of the post (to a maximum of 5). They are also
outperform a unigram/bigram model in the task of encouraged to tag posts as either “Interesting”,
disagreement and argument detection. In this anal- “Informative”, “Insightful”, “Flamebait”, “Troll”,
ysis, because of the very small number of sam- “Off-topic” or “Funny”. Frequency counts of
ples, we do not consider dependency or part-of- these tags as a percentage of the total number of
speech features, but do make a comparison with comments are included as features, as well as the
a filtered unigram/bigram model. In the filtering, overall fraction of posts which were tagged with
we remove stop words and any words that occur in any category. The average score across the topic,
fewer than three topics. This helps to prevent topic as well as the number of comments with a score
specific words from being selected, and limits the of 4 or S, are also added. These are expected to
number of possible matches slightly. Additionally, be informative features, since controversial topics
we use a lexicon of bias-words (Recasens et al., may encourage more up and down-voting on spe-
2013) to extract a bias-word frequency score over cific posts, and generally more user involvement.
all posts in the topic as a separate feature. This results in 9 meta-post features.
</bodyText>
<page confidence="0.982145">
1174
</page>
<table confidence="0.999914615384615">
Feature Set P R Random Forest ROC-AUC P R SVM ROC-AUC
F1 A F1 A
N-grams 0.71 0.57 0.63 0.67 0.69 0.52 0.60 0.56 0.53 0.53
Basic 0.69 0.67 0.68 0.69 0.73 0.62 0.62 0.62 0.63 0.67
Basic+N-grams 0.73 0.66 0.69 0.70 0.73 0.57 0.65 0.60 0.59 0.61
Basic+FQG 0.69 0.66 0.67 0.69 0.71 0.64 0.63 0.63 0.65 0.70
Basic+Sentiment 0.68 0.65 0.66 0.68 0.73 0.61 0.59 0.60 0.62 0.67
Basic+RhetStruct 0.71 0.70 0.70 0.71 0.73 0.73 0.70 0.71 0.73 0.78
Basic+RhetStruct+FQG 0.69 0.69 0.69 0.70 0.73 0.74 0.74 0.74 0.75 0.80
Basic+RhetAll 0.72 0.73 0.73 0.73 0.75 0.76 0.76 0.76 0.77 0.79
RhetStructOnly 0.69 0.72 0.71 0.71 0.72 0.76 0.72 0.74 0.75 0.79
RhetAllOnly 0.69 0.74 0.71 0.71 0.73 0.75 0.72 0.73 0.75 0.78
All 0.71 0.72 0.71 0.72 0.74 0.74 0.77 0.75 0.76 0.77
</table>
<tableCaption confidence="0.777384">
Table 2: Basic: Meta-post, all structural, bias words, discourse markers, referrals, punctuation RhetAll: Structural and sen-
timent based rhetorical features All: Basic, all rhetorical, sentiment and FQG features. The N-gram models include unigrams
and bi-grams. All feature sets in the bottom part of the table include rhetorical reatures.
</tableCaption>
<sectionHeader confidence="0.998633" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999978466666667">
Experiments were all performed using the Weka
machine learning toolkit. Two different types of
experiments were conducted - one using all an-
notated topics in a binary classification of con-
taining disagreement or not, and one using only
those topics with confidence scores greater than
0.75 (corresponding to the more certain cases). All
results were obtained by performing 10 fold cross-
validation on a balanced test set. Additionally, in-
fold cross-validation was performed to determine
the optimal number of features to use for each fea-
ture set. Since this is done in-fold, a paired t-test
is still a valid comparison of different feature sets
to determine significant differences in F-score and
accuracy.
</bodyText>
<subsectionHeader confidence="0.961134">
5.1 Classifiers
</subsectionHeader>
<bodyText confidence="0.99999152631579">
Two classifiers were used for this task: Random
Forest and SVM. Random Forest was selected be-
cause of its ability to avoid over-fitting data despite
large numbers of features for relatively few sam-
ples. For all runs, 100 trees were generated in the
Random Forest, with the number of features to use
being determined by in-fold optimization on the F-
score. For the SVM classifier, we use the normal-
ized poly-vector kernel with a maximum exponent
of 2 (the lowest possible), and a C parameter of
1.0 (Weka’s default value). This was chosen to
avoid over-fitting our data. We additionally use
a supervised in-fold feature selection algorithm,
Chi-Squared, to limit over-fitting in the SVM. The
number of features to be used is also optimized us-
ing in-fold cross-validation on the F-score. Both
the SVM classifier and the Random Forest classi-
fier were tested on the same training/testing fold
pairs, with a total of 10 iterations.
</bodyText>
<sectionHeader confidence="0.99995" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.9999253">
The results of the experiments are shown in Ta-
bles 2 and 3. In order to compare to previous anal-
yses, unigram and bigram features are shown, as
well as a combination of the basic features with
the n-grams. When performing the experiments,
we noticed that the n-gram features were hurting
the performance of the classifiers when included
with most of our other feature sets (or not chang-
ing results significantly), and therefore those re-
sults are not shown here. As seen in the table,
</bodyText>
<table confidence="0.999125333333333">
Feature Set P R Random Forest ROC-AUC P R SVM ROC-AUC
F1 A F1 A
N-grams 0.70 0.70 0.70 0.71 0.77 0.63 0.70 0.66 0.63 0.66
Basic 0.74 0.69 0.72 0.72 0.77 0.73 0.70 0.72 0.71 0.78
Basic+FQG 0.72 0.67 0.69 0.69 0.76 0.73 0.63 0.68 0.69 0.76
Basic+Sentiment 0.71 0.65 0.68 0.68 0.76 0.73 0.67 0.70 0.70 0.76
Basic+RhetStruct 0.79 0.75 0.77 0.77 0.78 0.79 0.67 0.72 0.74 0.79
Basic+RhetStruct+FQG 0.76 0.71 0.73 0.73 0.77 0.74 0.64 0.69 0.70 0.78
Basic+RhetAll 0.77 0.75 0.76 0.76 0.78 0.72 0.69 0.71 0.70 0.76
RhetStructOnly 0.75 0.71 0.73 0.73 0.75 0.76 0.63 0.69 0.70 0.76
RhetAllOnly 0.73 0.76 0.74 0.73 0.76 0.67 0.62 0.65 0.65 0.67
All 0.73 0.69 0.71 0.70 0.76 0.71 0.70 0.70 0.69 0.74
</table>
<tableCaption confidence="0.9926295">
Table 3: Precision, recall, F1, accuracy and ROC-AUC scores for the simpler task of identifying the cases deemed “high
confidence” in the crowd-sourcing task.
</tableCaption>
<page confidence="0.993464">
1175
</page>
<bodyText confidence="0.999964689655172">
the best performing feature sets are those that in-
clude rhetorical features under the SVM+x2 clas-
sifier. In fact, these feature sets perform signifi-
cantly better than a unigram/bigram baseline ac-
cording to a paired t-test between the best clas-
sifiers for each set (p &lt; 0.0001). The inclusion
of rhetorical structure also significantly outper-
forms the “basic” and “basic+N-grams” feature
baselines (which includes discourse markers, re-
ferrals, punctuation, bias word counts and struc-
tural features) with respect to both the F-score
and accuracy (p &lt; 0.02 for all feature sets with
rhetorical features). Overall, the feature sets “Ba-
sic+RhetAll” and “All” under the SVM+x2 clas-
sifier perform best. This performance is also bet-
ter than previously reported results for the ARGUE
Corpus (Abbott et al., 2011), even though the ba-
sic and unigram/bigram features perform similarly
to that reported in previous analyses.
As an additional check, we also conduct exper-
iments on the “high confidence” data (those topics
with a confidence score greater than 0.75). These
results are shown in Table 3. Clearly the basic
features perform better on this subset of the sam-
ples, although the addition of rhetorical structure
still provides significant improvement (p &lt; 0.001).
Overall, this suggests that the rhetorical, sentiment
and FQG features help more when the disagree-
ment is more subtle.
</bodyText>
<sectionHeader confidence="0.927467" genericHeader="method">
7 Analysis and Discussion
</sectionHeader>
<bodyText confidence="0.99813005">
In order to examine the quality of our features,
we report on the rhetorical features selected, and
show that these are reasonable and in many cases,
theoretically motivated. Furthermore, we check
whether the commonly selected features in each
of our feature categories are similar to those found
to be useful over the ARGUE corpus, as well as
within other argument detection tasks in online fo-
rums.
The rhetorical features that are consistently se-
lected are very well motivated in the context of ar-
gument detection. From the rhetorical structural
features, we find COMPARISON relation features
to be most commonly selected across all rhetorical
feature sets. Other highly ranked features include
the proportion of JOINT relations linking different
authors, EXPLANATION relations between differ-
ent authors, and the average depth of ELABORA-
TION relations.
The COMPARISON relations are expected to in-
</bodyText>
<table confidence="0.852568638888889">
Structural Length of topic in sentences, to-
tal number of authors, quotes in
first sentence, quotes in second
sentence, questions in first sen-
tence, questions in second sen-
tence, referrals to you and they
in first half of post
Meta Number of comments with la-
bels, number of comments la-
belled ’Flamebait’, number of
comments with scores of 4 or 5
FQG Number of connections, Num-
ber of fragments, Maximum
number of links from one node
RhetStruct COMPARISON (same to diff, diff
per post, diff percentage, avg
ratio, same per post, same per-
centage), EXPLANATION (avg
ratio, diff per post), JOINT
(diff percentage), ELABORA-
TION (average depth)
Discourse Markers Aggregated first sentence, Ag-
gregated middle, ’and’, ’oh’,
’but’ frequency counts
N-grams ‘don’t ?’, ‘plus’, ‘private’,
‘anti’, ‘hey’, ‘present’, ‘mak-
ing’, ‘developers’
RhetSent ELABORATION (variance in
same post), ATTRIBUTION
(variance in same post),
CONTRAST (range in same
post)
Sentiment Range of sentiment scores in
first sentence of all posts, range
of sentiment scores over all
posts
</table>
<tableCaption confidence="0.987289">
Table 4: Features found to be commonly selected over dif-
ferent iterations of the classifiers
</tableCaption>
<bodyText confidence="0.999886285714286">
dicate disagreement as motivated by theoretical
considerations (Horn, 1989). The importance of
other rhetorical relation features can also be ex-
plained by examining conversations in which they
appear. In particular, EXPLANATION relations of-
ten link authors who share viewpoints in a debate,
especially when one author is trying to support the
claim of another. The JOINT relations are also very
well motivated. In the extreme case, conversations
with a very high number of JOINT relations be-
tween different users are usually news based. The
high proportion of these relations indicates that
many users have added information to the conver-
sation about a specific item, such as adding new
suggested videogame features to an ongoing list.
Fewer JOINT relations seem to indicate disagree-
ment, especially when found in conjunction with
COMPARISON relations between different users.
This appears to generally indicate that users are
taking sides in a debate, and commenting specifi-
cally on evidence which supports their viewpoint.
</bodyText>
<page confidence="0.984897">
1176
</page>
<bodyText confidence="0.999972214285714">
The average depth of ELABORATION relations
reveals how deep the perceived connections are
between users in a conversation over time. Deeper
ELABORATION connections seem to indicate that
the conversation is more cohesive. Alone, this
does not signify disagreement/agreement but does
seem to signify argument-style over news-style di-
alogues. This is particularly helpful for differ-
entiating between articles with many COMPARI-
SON relations, as COMPARISON may be present
in both news-style dialogues (e.g. comparing ci-
tation styles) as well as argument style dialogues
(e.g. arguing over which of two operating systems
is superior).
For the combined sentiment and rhetorical rela-
tions, range and variance in ELABORATION, CON-
TRAST and ATTRIBUTION within the same post
are found to be the most informative features. Ad-
ditionally, neither ATTRIBUTION nor CONTRAST
are useful features when only their structural in-
formation is considered. In the case of ATTRI-
BUTION, we hypothesize that the added sentiment
score within the post differentiates between a neu-
tral attribution (which would not signify disagree-
ment) and a negative or positive attribution (which
may signify disagreement). For CONTRAST, the
added sentiment helps to distinguish between re-
sponses such as “We will be trying the Microsoft
software. We won’t, however, be able to test the
Apple equivalent.” and “We will be trying the Mi-
crosoft software. We won’t, however, be trying the
inferior Apple equivalent.” where the second ex-
ample more likely signals, or even provokes, dis-
agreement.
Outside of the rhetorical features, the discourse
markers which are found to be the most useful in
our experiments agree with those found in the AR-
GUE corpus (Abbott et al., 2011). Namely, ‘oh’,
‘but’, ‘because’ and ‘and’ are discovered to be the
most informative features. We also find the aggre-
gated discourse marker frequency count in the first
part of each post to be useful.
Previous analysis on Slashdot as a social net-
work (Gomez et al., 2008) suggests that the h-
index of the conversation is relevant for detecting
controversy in a posted article. We include the
h-index as part of the Fragment Quotation Graph
feature set, but surprisingly do not find this to be
a useful feature. This may be due to our corpus
involving relatively shallow conversational trees -
the maximum h-index across all topics is three.
Comparing to Mishne and Glance’s work, we
also find quotations, questions and sentiment
range near the beginning of a post to be very in-
formative features. These are often selected across
all feature sets which include the “basic” set.
The topics most often misclassified across all
feature sets are those with relatively few sen-
tences. In these cases, the rhetorical structure
is not very well defined, and there is much less
content available for detecting quotes, punctuation
and referrals. Additionally, the feature sets which
only use rhetorical and sentiment features consis-
tently misclassify the same set of conversations
(those that have lower quality discourse trees with
few connections). When combined with the “ba-
sic” feature set, these errors are mitigated, and the
topics which the “basic” features miss are picked
up by the rhetorical features. This leads to the best
overall accuracy and F-score.
</bodyText>
<subsectionHeader confidence="0.982842">
7.1 Discourse Parser
</subsectionHeader>
<bodyText confidence="0.999969">
A major source of error in detecting disagreement
arises because of inaccuracies in our discourse
parser. In particular, document-level discourse
parsing is a challenging task, with relatively few
parsers available at the time of this analysis (Joty
et al., 2013; Hernault et al., 2010). We chose to
use the discourse parser developed by Joty et al.
which both identifies elementary discourse units in
a text, and then builds a document-level discourse
tree using Conditional Random Fields. Because
their approach uses an optimal parsing algorithm
as opposed to a greedy parsing algorithm, they are
able to achieve much higher accuracies in rela-
tion and structure identification than other avail-
able parsers. Here, results from their parser on the
standard RST-DT dataset are presented since there
is no currently available dialogic corpora to com-
pare to.
</bodyText>
<sectionHeader confidence="0.621596" genericHeader="method">
RST-DT Instructional
</sectionHeader>
<table confidence="0.93823775">
Metrics Joty HILDA Human Joty
Span 83.84 74.68 88.70 81.88
Nuclearity 68.90 58.99 77.72 63.13
Relation 55.87 44.32 65.75 43.60
</table>
<tableCaption confidence="0.786502">
Table 5: Joty et al. document-level parser accuracy of the
parser used in this paper. The parser was originally tested
on two corpora: RST-DT and Instructional. HILDA was the
state-of-the-art parser at that time. Span and Nuclearity met-
rics assess the quality of the structure of the resulting tree,
while the Relation metric assesses the quality of the relation
labels.
</tableCaption>
<bodyText confidence="0.535821">
Examining the relation labels confusion matrix
</bodyText>
<page confidence="0.993352">
1177
</page>
<figureCaption confidence="0.931370222222222">
Figure 2: Confusion matrix for relation labels on RST-DT.
The X-axis represents predicted relations, while the Y-axis
corresponds to true values. The relations are Topic-Change
(T-C), Topic-Comment (T-CM), Textual Organization (T-
O), Manner-Means (M-M), Comparison (CMP), Evaluation
(EV), Summary (SU), Condition (CND), Enablement (EN),
Cause (CA), Temporal (TE), Explanation (EX), Background
(BA), Contrast (CO), Joint (JO), Same-Unit (S-U), Attribu-
tion (AT) and Elaboration (EL).
</figureCaption>
<bodyText confidence="0.999992">
for the discourse parser in Figure 2, some of the
chosen rhetorical features make even more sense.
In particular, the confusion of ELABORATION and
EXPLANATION may account for the perceived im-
portance of ELABORATION relations in the analy-
sis. Likewise, CAUSE (which maybe present when
users attribute positive or negative qualities to an
entity, signalling disagreement) is often confused
with JOINT and ELABORATION which were often
picked as important features by our classifiers.
</bodyText>
<sectionHeader confidence="0.995823" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999975976744186">
In this paper, we have described a new set of fea-
tures for detecting disagreement in online blog fo-
rums. By treating a written conversation as a series
of linked monologues, we can apply a document
level discourse parser to extract a discourse tree
for the conversation. We then aggregate this infor-
mation in a relation graph, which allows us to cap-
ture post-level rhetorical relations between users.
Combining this approach with sentiment features
shows significantly improved performance in both
accuracy and F-score over a baseline consisting
of structural and lexical features as well as re-
ferral counts, punctuation, and discourse markers.
In building our new crowd-sourced corpus from
Slashdot, we have also shown the challenges of
detecting subtle disagreement in a dataset that con-
tains a significant number of news-style discus-
sions.
In future work, we will improve sentiment fea-
tures by considering methods to detect opinion-
topic pairs in conversation, similar to Somasun-
daran and Wiebe (2009). Additionally, we will
incorporate generalized dependency and POS fea-
tures (Abbott et al., 2011), which were not used
in this analysis due to the very small number of
training samples in our dataset. The fragment quo-
tation graph features did not perform as well as we
expected, and in future work we would like to in-
vestigate this further. Furthermore, we will also
explore how to create a discourse tree from the
thread structure of a conversation (instead of from
its temporal structure), and verify whether this im-
proves the accuracy of the relation graphs, espe-
cially when the temporal structure is not represen-
tative of the reply-to relationships.
Finally, we plan to apply our novel feature set to
other corpora (e.g., ARGUE) in order to study the
utility of these features across genres and with re-
spect to the accuracy of the discourse parser. This
may provide insights into where discourse parsers
can be most effectively used, as well as how to
modify parsers to better capture rhetorical rela-
tions between participants in conversation.
</bodyText>
<sectionHeader confidence="0.998576" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.981471416666667">
Rob Abbott, Marilyn Walker, Pranav Anand, Jean
E. Fox Tree, Robeson Bowmani and Joseph King.
2011. How can you say such things?!?: Recogniz-
ing Disagreement in Informal Political Argument. In
Proceedings of LSM, pages 2-11.
Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and
Dragomir Radev. 2012. Subgroup detection in ide-
ological discussions. In Proceedings of ACL, pages
399-409.
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In Pro-
ceedings of WWW, pages 529-535.
Rawia Awadallah, Maya Ramanath, Gerhard Weikum.
2012. Harmony and Dissonance: Organizing the
People’s Voices on Political Controversies. In Pro-
ceedings of WSDM, pages 523-532.
Prakhar Biyani, Sumit Bhatia, Cornelia Caragea,
Prasenjit Mitra. 2014. Using non-lexical features for
identifying factual and opinionative threads in online
forums. In Knowledge-Based Systems, in press.
Penelope Brown and Stephen Levinson. 1987. Polite-
ness: Some universals in language usage. Cam-
bridge University Press.
</reference>
<page confidence="0.953575">
1178
</page>
<reference confidence="0.999412813084112">
Giuseppe Carenini and Jackie Cheung. 2008. Extrac-
tive vs. NLG-based Abstractive Summarization of
Evaluative Text: The Effect of Corpus Controver-
siality. In Proceedings of INLG, pages 33-41.
Giuseppe Carenini, Raymond Ng, Xiaodong Zhou.
2007. Summarizing Email Conversations with Clue
Words. In Proceedings of WWW, pages 91-100.
Yoonjung Choi, Yuchul Jung, and Sung-Hyon Myaeng.
2010. Identifying controversial issues and their sub-
topics in news articles. In Proceedings of PAISI,
pages 140-153.
Alexander Conrad, Janyce Wiebe and Rebecca
Hwa. 2012. Recognizing Arguing Subjectivity and
Argument Tags. In ACL Workshop on Extra-
Propositional Aspects of Meaning, pages 80-88.
Jean E. Fox Tree. 2010. Discourse markers across
speakers and settings. Language and Linguistics
Compass, 3(1):1-113.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agreement
and disagreement in conversational speech: Use of
bayesian networks to model pragmatic dependen-
cies. In Proceedings of ACL, pages 669-es.
Sebastian Germesin and Theresa Wilson. 2009. Agree-
ment Detection in Multiparty Conversation. In Pro-
ceedings of International Conference on Multimodal
Interfaces pages 7-14.
Vicenc Gomez, Andreas Kaltenbrunner and Vicente
Lopez. 2008. Statistical Analysis of the Social Net-
work and Discussion Threads in Slashdot. In Pro-
ceedings of WWW, pages 645-654.
Ahmed Hassan, Vahed Qazvinian, and Dragomir
Radev. 2010. What’s with the attitude?: identifying
sentences with attitude in online discussions. In Pro-
ceedings of EMNLP, pages 1245-1255.
Hugo Hernault, Helmut Prendinger, David A. duVerle
and Mitsuru Ishizuka. 2010. HILDA: A Discourse
Parser Using Support Vector Machine Classification.
Dialogue and Discourse, 1(3):1-33.
Laurence R. Horn. 1989. A natural history of negation.
Chicago University Press.
Shafiq Joty, Giuseppe Carenini, and Chin-Yew Lin.
2011. Unsupervised modeling of dialog acts in asyn-
chronous conversations. In Proceedings of IJCAI,
pages 1807-1813.
Shafiq Joty, Giuseppe Carenini, Raymond Ng and
Yashar Mehdad. 2013. Combining Intra- and Multi-
sentential Rhetorical Parsing for Document-level
Discourse Analysis. In Proceedings of ACL.
Shafiq Joty, Giuseppe Carenini and Raymond Ng.
2013. Topic Segmentation and Labeling in Asyn-
chronous Conversations. Journal of AI Research,
47:521-573.
Ching-Sheng Lin, Samira Shaikh, Jennifer Stromer-
Galley, Jennifer Crowley, Tomek Strzalkowski,
Veena Ravishankar. 2013. Topical Positioning: A
New Method for Predicting Opinion Changes in
Conversation. In Proceedings of LASM, pages 41-
48.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243-281.
Gilad Mishne and Natalie Glance. 2006. Leave a reply:
An analysis of weblog comments. In Proceedings of
WWW.
Amita Misra and Marilyn Walker. 2013. Topic Inde-
pendent Identification of Agreement and Disagree-
ment in Social Media Dialogue. In Proceedings of
SIGDIAL, pages 41-50.
Arjun Mukherjee and Bing Liu. 2012. Modeling review
comments. In Proceedings of ACL, pages 320-329.
Akiko Murakami and Rudy Raymond. 2010. Support
or Oppose? Classifying Positions in Online De-
bates from Reply Activities and Opinion Expres-
sions. In Proceedings of the International Confer-
ence on Computational Linguistics, pages 869-875.
Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,
Deborah Cai, Jennifer Midberry, Yuanxin Wang.
2014. Modeling topic control to detect influence
in conversations using nonparametric topic models.
Machine Learning 95:381-421.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing Product Features and Opinions from Reviews.
In Proceedings of HLT/EMNLP, pages 339-346.
Ana-Maria Popescu and Marco Pennacchiotti. 2010.
Detecting controversial events from twitter. In Pro-
ceedings of CIKM, pages 1873-1876.
Stephan Raaijmakers, Khiet Truong, Theresa Wilson.
2008. Multimodal subjectivity analysis of multiparty
conversation. In Proceedings of EMNLP, pages 466-
474.
Marta Recasens, Cristian Danescu-Niculescu-Mizil,
and Dan Jurafsky. 2013. Linguistic Models for Ana-
lyzing and Detecting Biased Language. In Proceed-
ings of ACL, pages 16501659.
Swapna Somasundaran, Josef Ruppenhofer, Janyce
Wiebe. 2007. Detecting Arguing and Sentiment in
Meetings. In Proceedings of SIGDIAL Workshop on
Discourse and Dialogue.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings
ofACL, pages 226-234.
Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of NAACL, Workshop on Computa-
tional Approaches to Analysis and Generation of
Emotion in Text, pages 116124.
</reference>
<page confidence="0.915438">
1179
</page>
<reference confidence="0.9992346">
Amanda Stent and James Allen. 2000. Annotating Ar-
gumentation Acts in Spoken Dialog. Technical Re-
port.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, Manfred Stede. 2011. Lexicon-based
methods for sentiment analysis. Journal of Compu-
tational Linguistics, 37(2):267-307.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of EMNLP, pages 327-335.
Wen Wang, Sibel Yaman, Kristen Precoda, Colleen
Richey, and Geoffrey Raymond. 2011. Detection of
agreement and disagreement in broadcast conversa-
tions. In Proceedings of ACL, pages 374-378.
Bonnie Webber and Rashmi Prasad. 2008. Sentence-
initial discourse connectives, discourse structure and
semantics. In Proceedings of the Workshop on For-
mal and Experimental Approaches to Discourse Par-
ticles and Modal Adverbs.
Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris.
2012. Unifying local and global agreement and dis-
agreement classification in online debates. In Pro-
ceedings of Computational Approaches to Subjectiv-
ity and Sentiment Analysis, pages 61-69.
</reference>
<page confidence="0.992528">
1180
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.877528">
<title confidence="0.990286">Detecting Disagreement in Conversations using Rhetorical Structure</title>
<author confidence="0.999991">Kelsey Allen Giuseppe Carenini Raymond T Ng</author>
<affiliation confidence="0.999877">Department of Computer Science, University of British</affiliation>
<address confidence="0.961725">Vancouver, BC, V6T 1Z4,</address>
<email confidence="0.983489">carenini,</email>
<abstract confidence="0.997529130434783">Casual online forums such as Reddit, Slashdot and Digg, are continuing to increase in popularity as a means of communication. Detecting disagreement in this domain is a considerable challenge. Many topics are unique to the conversation on the forum, and the appearance of disagreement may be much more subtle than on political blogs or social media sites such as twitter. In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot, showing that disagreement detection in this domain is difficult even for humans. We then proceed to show that a new set of features determined from the rhetorical structure of the conversation significantly improves the performance on disagreement detection over a baseline consisting of unigram/bigram features, discourse markers, structural features and meta-post features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rob Abbott</author>
<author>Marilyn Walker</author>
<author>Pranav Anand</author>
<author>Jean E Fox Tree</author>
<author>Robeson Bowmani</author>
<author>Joseph King</author>
</authors>
<title>How can you say such things?!?: Recognizing Disagreement in Informal Political Argument.</title>
<date>2011</date>
<booktitle>In Proceedings of LSM,</booktitle>
<pages>2--11</pages>
<contexts>
<context position="2787" citStr="Abbott et al., 2011" startWordPosition="422" endWordPosition="425">tarting point, in this paper we focus on the detection of disagreement in casual conversations between users. This requires a generalized approach that can accurately identify disagreement in topics ranging from something as mundane as whether GPS stands for galactic positioning system or global positioning system, to more ideological debates about distrust in science. Motivated by the widespread consensus in both computational and theoretical linguistics on the utility of discourse markers for signalling pragmatic functions such as disagreement and personal opinions (Webber and Prasad, 2008; Abbott et al., 2011; J. E. Fox-Tree, 2010), we introduce a new set of features based on the Discourse Tree (DT) of a conversational text. Discourse Trees were formalized by Mann and Thompson (1988) as part of their Rhetorical Structure Theory (RST) to represent the structure of discourse. Although this theory is for monologic discourse, we propose to treat conversational dialogue as a collection of linked monologues, and subsequently build a relation graph describing both rhetorical connections within user posts, as well as between different users. Features obtained from this graph offer significant improvements</context>
<context position="4265" citStr="Abbott et al., 2011" startWordPosition="643" endWordPosition="646">eoretically relevant to disagreement detection, such as COMPARISON (Horn, 1989), suggest that this approach may be capturing the essential aspects of the conversational argumentative structure. 1169 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1169–1180, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics As a second contribution of this work, we provide a new dataset consisting of 95 topics annotated for disagreement. Unlike the publicly available ARGUE corpus based on the online debate forum 4forums.com (Abbott et al., 2011), our corpus is based on Slashdot, which is a general purpose forum not targeted to debates. Therefore, we expect that detecting disagreement may be a more difficult task in our new corpus, as certain topics (like discussing GPS systems) may be targeted towards objective information sharing without any participants expressing opinions or stances. Because of this, our corpus represents an excellent testbed to examine methods for more subtle disagreement detection, as well as the major differences between news-style and argument-style dialogue. 2 Related Work In the past decade, there have been </context>
<context position="8043" citStr="Abbott et al., 2011" startWordPosition="1237" endWordPosition="1240">2010). In Slashdot, the h-index of a discussion has been used to rank articles according to controversiality, although no quantitative evaluation of this approach has been given, and, unlike in our analysis, they did not consider any other features (Gomez et al., 2008). Content based features such as polarity and cosine similarity have also been used to study influence, controversy and opinion changes on microblogging sites such as Twitter (Lin et al., 2013; Popescu and Pennacchiotti, 2010). The simplified task of detecting disagreement between just two users (either a question/response pair (Abbott et al., 2011) or two adjacent paragraphs (Misra and Walker, 2013)) has also been recently approached on the ARGUE corpus. Abbott et al. (2011) use discourse markers, generalized dependency features, punctuation and structural features, while Misra and Walker (2013) focus on n-grams indicative of denial, hedging and agreement, as well as cue words and punctuation. Most similar to our work is that by Mishne and Glance (2006). They performed a general analysis of weblog comments, using punctuation, quotes, lexicon counts, subjectivity, polarity and referrals to detect disputative and non-disputative comments.</context>
<context position="15152" citStr="Abbott et al., 2011" startWordPosition="2411" endWordPosition="2414">opics had confidence scores below 55%, which suggests that subtle disagreement detection is a subjective and difficult task. Further statistics for the developed corpus are given in Table 1. 4 Features for Disagreement Detection The features we use in our experiments combine information from conversational structure, rhetorical relations, sentiment features, n-gram models, Slashdot meta-features, structural features, and lexicon features. 4.1 Rhetorical Relation Graphs Discourse markers have been found to aid in argument and disagreement detection, and for tasks such as stance identification (Abbott et al., 2011; Misra and Walker, 2013; Somasundaran and Wiebe, 2009). We aim to improve over discourse markers by capturing the underlying discourse structure of the conversation in terms of discourse relations. In Rhetorical Structure Theory, Discourse trees are a hierarchical representation of document structure for monologues (Mann and Thompson, 1988). At the lowest level, Elementary Discourse Units (EDUs) are connected by discourse relations (such as ELABORATION and COMPARISON), which in turn form larger discourse units that are also linked by these relations. Computational systems (discourse parsers) </context>
<context position="19628" citStr="Abbott et al., 2011" startWordPosition="3137" endWordPosition="3140">(same to diff), is also added. These capture the intuition that certain rhetorical relations such as CAUSE, EVIDENCE and EXPLANATION are expected to appear more within a post if users are trying to support their perspective in an argument. In total, there are 18 (rhetorical relations)x7 (avg ratio, avg depth, same per post, same percentage, diff percentage, diff per post, same to diff) + 1 (number of connections) = 127 features. 4.2 Discourse Markers Motivated by previous work, we include a frequency count of 17 discourse markers which were found to be the most common across the ARGUE corpus (Abbott et al., 2011). Furthermore, we hypothesize that individual discourse markers might have low frequency counts in the text. Therefore, we also include an aggregated count of all 17 discourse markers in each fifth of the posts in a topic (e.g. the count of all 17 discourse markers in the first fifth of every post in the topic). Altogether, there are 5 aggregated discourse marker features in addition to the 17 frequency count features. 4.3 Sentiment Features Sentiment polarity features have been shown to be useful in argument detection (Mishne and Glance, 2006). For this work, we use four sentiment scoring cat</context>
<context position="30891" citStr="Abbott et al., 2011" startWordPosition="5014" endWordPosition="5017"> baseline according to a paired t-test between the best classifiers for each set (p &lt; 0.0001). The inclusion of rhetorical structure also significantly outperforms the “basic” and “basic+N-grams” feature baselines (which includes discourse markers, referrals, punctuation, bias word counts and structural features) with respect to both the F-score and accuracy (p &lt; 0.02 for all feature sets with rhetorical features). Overall, the feature sets “Basic+RhetAll” and “All” under the SVM+x2 classifier perform best. This performance is also better than previously reported results for the ARGUE Corpus (Abbott et al., 2011), even though the basic and unigram/bigram features perform similarly to that reported in previous analyses. As an additional check, we also conduct experiments on the “high confidence” data (those topics with a confidence score greater than 0.75). These results are shown in Table 3. Clearly the basic features perform better on this subset of the samples, although the addition of rhetorical structure still provides significant improvement (p &lt; 0.001). Overall, this suggests that the rhetorical, sentiment and FQG features help more when the disagreement is more subtle. 7 Analysis and Discussion</context>
<context position="36284" citStr="Abbott et al., 2011" startWordPosition="5855" endWordPosition="5858">and a negative or positive attribution (which may signify disagreement). For CONTRAST, the added sentiment helps to distinguish between responses such as “We will be trying the Microsoft software. We won’t, however, be able to test the Apple equivalent.” and “We will be trying the Microsoft software. We won’t, however, be trying the inferior Apple equivalent.” where the second example more likely signals, or even provokes, disagreement. Outside of the rhetorical features, the discourse markers which are found to be the most useful in our experiments agree with those found in the ARGUE corpus (Abbott et al., 2011). Namely, ‘oh’, ‘but’, ‘because’ and ‘and’ are discovered to be the most informative features. We also find the aggregated discourse marker frequency count in the first part of each post to be useful. Previous analysis on Slashdot as a social network (Gomez et al., 2008) suggests that the hindex of the conversation is relevant for detecting controversy in a posted article. We include the h-index as part of the Fragment Quotation Graph feature set, but surprisingly do not find this to be a useful feature. This may be due to our corpus involving relatively shallow conversational trees - the maxi</context>
<context position="41377" citStr="Abbott et al., 2011" startWordPosition="6649" endWordPosition="6652">rformance in both accuracy and F-score over a baseline consisting of structural and lexical features as well as referral counts, punctuation, and discourse markers. In building our new crowd-sourced corpus from Slashdot, we have also shown the challenges of detecting subtle disagreement in a dataset that contains a significant number of news-style discussions. In future work, we will improve sentiment features by considering methods to detect opiniontopic pairs in conversation, similar to Somasundaran and Wiebe (2009). Additionally, we will incorporate generalized dependency and POS features (Abbott et al., 2011), which were not used in this analysis due to the very small number of training samples in our dataset. The fragment quotation graph features did not perform as well as we expected, and in future work we would like to investigate this further. Furthermore, we will also explore how to create a discourse tree from the thread structure of a conversation (instead of from its temporal structure), and verify whether this improves the accuracy of the relation graphs, especially when the temporal structure is not representative of the reply-to relationships. Finally, we plan to apply our novel feature</context>
</contexts>
<marker>Abbott, Walker, Anand, Tree, Bowmani, King, 2011</marker>
<rawString>Rob Abbott, Marilyn Walker, Pranav Anand, Jean E. Fox Tree, Robeson Bowmani and Joseph King. 2011. How can you say such things?!?: Recognizing Disagreement in Informal Political Argument. In Proceedings of LSM, pages 2-11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amjad Abu-Jbara</author>
<author>Mona Diab</author>
<author>Pradeep Dasigi</author>
<author>Dragomir Radev</author>
</authors>
<title>Subgroup detection in ideological discussions.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>399--409</pages>
<marker>Abu-Jbara, Diab, Dasigi, Radev, 2012</marker>
<rawString>Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and Dragomir Radev. 2012. Subgroup detection in ideological discussions. In Proceedings of ACL, pages 399-409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rakesh Agrawal</author>
<author>Sridhar Rajagopalan</author>
<author>Ramakrishnan Srikant</author>
<author>Yirong Xu</author>
</authors>
<title>Mining newsgroups using networks arising from social behavior.</title>
<date>2003</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>529--535</pages>
<contexts>
<context position="7185" citStr="Agrawal et al., 2003" startWordPosition="1099" endWordPosition="1102">al., 2010; Awadallah et al., 2012; Popescu et al., 2005). In the asynchronous domain, there has been recent work in disagreement detection, especially as it pertains to stance identification. Content based features, including sentiment, duration, and discourse markers have been used for this task (Yin et al., 2012; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). The structure of a conversation has also been used, although these approaches have focused on simple rules for disagreement identification (Murakami and Raymond, 2010), or have assumed that adjacent posts always disagree (Agrawal et al., 2003). More recent work has focused on identifying users’ attitudes towards each other (Hassan et al., 2010), influential users and posts (Nguyen et al., 2014), as well as identifying subgroups of users who share viewpoints (Abu-Jbara et al., 2010). In Slashdot, the h-index of a discussion has been used to rank articles according to controversiality, although no quantitative evaluation of this approach has been given, and, unlike in our analysis, they did not consider any other features (Gomez et al., 2008). Content based features such as polarity and cosine similarity have also been used to study </context>
</contexts>
<marker>Agrawal, Rajagopalan, Srikant, Xu, 2003</marker>
<rawString>Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan Srikant, and Yirong Xu. 2003. Mining newsgroups using networks arising from social behavior. In Proceedings of WWW, pages 529-535.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rawia Awadallah</author>
<author>Maya Ramanath</author>
<author>Gerhard Weikum</author>
</authors>
<title>Harmony and Dissonance: Organizing the People’s Voices on Political Controversies.</title>
<date>2012</date>
<booktitle>In Proceedings of WSDM,</booktitle>
<pages>523--532</pages>
<contexts>
<context position="5155" citStr="Awadallah et al., 2012" startWordPosition="784" endWordPosition="787">tive information sharing without any participants expressing opinions or stances. Because of this, our corpus represents an excellent testbed to examine methods for more subtle disagreement detection, as well as the major differences between news-style and argument-style dialogue. 2 Related Work In the past decade, there have been a number of computational approaches developed for the task of disagreement and controversy detection, particularly in synchronous conversations such as meetings (Somasundaran et al., 2007; Raaijmakers et al., 2008) and in monologic corpora such as news collections (Awadallah et al., 2012) and reviews (Popescu et al., 2005; Mukherjee and Liu, 2012). In the domain of synchronous conversations, prosodic features such as duration, speech rate and pause have been used for spoken dialogue (Wang et al., 2011; Galley et al., 2004). Galley et al. (2004) found that local features, such as lexical and structural features, as well as global contextual features, were particularly useful for identifying agreement/disagreement in the ICSI meeting corpus. Germesin and Wilson (2009) also showed accuracies of 98% in detecting agreement in the AMI corpus using lexical, subjectivity and dialogue </context>
<context position="6597" citStr="Awadallah et al., 2012" startWordPosition="1010" endWordPosition="1013">s complement lexical features in the AMI meeting corpus (Somasundaran et al., 2007). These observations are taken into account with our feature choices, and we use contextual, discourse and lexical features in our analysis. In the monologic domain, Conrad et al. (2012) recently found rhetorical relations to be useful for argument labelling and detection in articles on the topic of healthcare. Additionally, discourse markers and sentiment features have been found to assist with disagreement detection in collections of news documents on a particular topic, as well as reviews (Choi et al., 2010; Awadallah et al., 2012; Popescu et al., 2005). In the asynchronous domain, there has been recent work in disagreement detection, especially as it pertains to stance identification. Content based features, including sentiment, duration, and discourse markers have been used for this task (Yin et al., 2012; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). The structure of a conversation has also been used, although these approaches have focused on simple rules for disagreement identification (Murakami and Raymond, 2010), or have assumed that adjacent posts always disagree (Agrawal et al., 2003). More recen</context>
</contexts>
<marker>Awadallah, Ramanath, Weikum, 2012</marker>
<rawString>Rawia Awadallah, Maya Ramanath, Gerhard Weikum. 2012. Harmony and Dissonance: Organizing the People’s Voices on Political Controversies. In Proceedings of WSDM, pages 523-532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prakhar Biyani</author>
<author>Sumit Bhatia</author>
<author>Cornelia Caragea</author>
<author>Prasenjit Mitra</author>
</authors>
<title>Using non-lexical features for identifying factual and opinionative threads in online forums. In Knowledge-Based Systems,</title>
<date>2014</date>
<note>in press.</note>
<contexts>
<context position="23466" citStr="Biyani et al., 2014" startWordPosition="3771" endWordPosition="3774">s for disagreement detection such as the number of connections, total number of fragments, and the average path length between nodes which we hypothesize to be useful. We additionally extract the h-index (Gomez et al., 2008) and average branching ratio per fragment of the topic from the simpler reply-to conversational structure. In total, there are 8 FQG features. 4.6 Structural Features Length features have been well documented in the literature to provide useful information about whether or not arguments exist, especially in online conversations that may be more informative than subjective (Biyani et al., 2014; Yin et al., 2012). In this work, length features include the length of the post in sentences, the average number of words per sentence, the average number of sentences per post, the number of contributing authors, the rate of posting, and the total amount of time of the conversation. This results in a total of 9 features. 4.7 Punctuation Like many other features already described, frequency counts of ‘?’,‘!’,‘”’,‘”, and ‘.’ are found for each fifth of the post (the first fifth, second fifth, etc.). These counts are then aggregated across all posts for a total of 5×5 = 25 features. 4.8 Referr</context>
</contexts>
<marker>Biyani, Bhatia, Caragea, Mitra, 2014</marker>
<rawString>Prakhar Biyani, Sumit Bhatia, Cornelia Caragea, Prasenjit Mitra. 2014. Using non-lexical features for identifying factual and opinionative threads in online forums. In Knowledge-Based Systems, in press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penelope Brown</author>
<author>Stephen Levinson</author>
</authors>
<title>Politeness: Some universals in language usage.</title>
<date>1987</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2160" citStr="Brown and Levinson, 1987" startWordPosition="326" endWordPosition="329">ow debate arises and propagates in a conversation, we may also gain insight into how authors’ opinions on a topic can be influenced over time. The long term goal of our research is to lay the foundations for understanding argumentative structure in conversations, which could be applied to NLP tasks such as summarization, information retrieval, and text visualization. Argumentative structure theory has been thoroughly studied in both psychology and rhetoric, with negation and discourse markers, as well as hedging and dispreferred responses, being known to be indicative of argument (Horn, 1989; Brown and Levinson, 1987). As a starting point, in this paper we focus on the detection of disagreement in casual conversations between users. This requires a generalized approach that can accurately identify disagreement in topics ranging from something as mundane as whether GPS stands for galactic positioning system or global positioning system, to more ideological debates about distrust in science. Motivated by the widespread consensus in both computational and theoretical linguistics on the utility of discourse markers for signalling pragmatic functions such as disagreement and personal opinions (Webber and Prasad</context>
</contexts>
<marker>Brown, Levinson, 1987</marker>
<rawString>Penelope Brown and Stephen Levinson. 1987. Politeness: Some universals in language usage. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Jackie Cheung</author>
</authors>
<title>Extractive vs. NLG-based Abstractive Summarization of Evaluative Text: The Effect of Corpus Controversiality.</title>
<date>2008</date>
<booktitle>In Proceedings of INLG,</booktitle>
<pages>33--41</pages>
<contexts>
<context position="20350" citStr="Carenini and Cheung, 2008" startWordPosition="3252" endWordPosition="3255">in the text. Therefore, we also include an aggregated count of all 17 discourse markers in each fifth of the posts in a topic (e.g. the count of all 17 discourse markers in the first fifth of every post in the topic). Altogether, there are 5 aggregated discourse marker features in addition to the 17 frequency count features. 4.3 Sentiment Features Sentiment polarity features have been shown to be useful in argument detection (Mishne and Glance, 2006). For this work, we use four sentiment scoring categories: the variance, average score, number of negative sentences, and controversiality score (Carenini and Cheung, 2008) of sentences in a post. These are determined using SoCAL (Taboada et al., 2011), which gives each sentence a polarity score and has been shown to work well on user-generated content. Overall, we have two main classes of sentiment features. The first type splits all the posts in a topic into 4 sections corresponding to the sentences in each quarter of the post. The sentiment scores described above are then applied to each section of the posts (e.g. one feature is the number of negative sentences in the first quarter of each post). As a separate feature, we also include the scores on just the f</context>
</contexts>
<marker>Carenini, Cheung, 2008</marker>
<rawString>Giuseppe Carenini and Jackie Cheung. 2008. Extractive vs. NLG-based Abstractive Summarization of Evaluative Text: The Effect of Corpus Controversiality. In Proceedings of INLG, pages 33-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
<author>Xiaodong Zhou</author>
</authors>
<title>Summarizing Email Conversations with Clue Words.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>91--100</pages>
<contexts>
<context position="22531" citStr="Carenini et al. (2007)" startWordPosition="3621" endWordPosition="3624">ection. In online, threaded conversations, the standard approach to extracting conversational structure is through reply-to relations usually present in online forums. However, if users strongly disagree on a topic (or sub-topic), they may choose to quote a specific paragraph (defined as a fragment) of a previous post in their reply. Being able to determine which specific fragments are linked by relations may then be useful for more targeted content-based features, helping to reduce noise. In order to address this, we use the Fragment Quotation Graph (FQG), an approach previously developed by Carenini et al. (2007) for dialogue act modelling and topic segmentation (Joty et al., 2011; Joty et al., 2013). For our analysis, the FQG is found over the entire Slashdot article. We then select the subgraph corresponding to those fragments in the target topic. From the fragment quotation sub-graph, we are then able to extract features for disagreement detection such as the number of connections, total number of fragments, and the average path length between nodes which we hypothesize to be useful. We additionally extract the h-index (Gomez et al., 2008) and average branching ratio per fragment of the topic from </context>
</contexts>
<marker>Carenini, Ng, Zhou, 2007</marker>
<rawString>Giuseppe Carenini, Raymond Ng, Xiaodong Zhou. 2007. Summarizing Email Conversations with Clue Words. In Proceedings of WWW, pages 91-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoonjung Choi</author>
<author>Yuchul Jung</author>
<author>Sung-Hyon Myaeng</author>
</authors>
<title>Identifying controversial issues and their subtopics in news articles.</title>
<date>2010</date>
<booktitle>In Proceedings of PAISI,</booktitle>
<pages>140--153</pages>
<contexts>
<context position="6573" citStr="Choi et al., 2010" startWordPosition="1006" endWordPosition="1009">ialogue act features complement lexical features in the AMI meeting corpus (Somasundaran et al., 2007). These observations are taken into account with our feature choices, and we use contextual, discourse and lexical features in our analysis. In the monologic domain, Conrad et al. (2012) recently found rhetorical relations to be useful for argument labelling and detection in articles on the topic of healthcare. Additionally, discourse markers and sentiment features have been found to assist with disagreement detection in collections of news documents on a particular topic, as well as reviews (Choi et al., 2010; Awadallah et al., 2012; Popescu et al., 2005). In the asynchronous domain, there has been recent work in disagreement detection, especially as it pertains to stance identification. Content based features, including sentiment, duration, and discourse markers have been used for this task (Yin et al., 2012; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). The structure of a conversation has also been used, although these approaches have focused on simple rules for disagreement identification (Murakami and Raymond, 2010), or have assumed that adjacent posts always disagree (Agrawal e</context>
</contexts>
<marker>Choi, Jung, Myaeng, 2010</marker>
<rawString>Yoonjung Choi, Yuchul Jung, and Sung-Hyon Myaeng. 2010. Identifying controversial issues and their subtopics in news articles. In Proceedings of PAISI, pages 140-153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Conrad</author>
<author>Janyce Wiebe</author>
<author>Rebecca Hwa</author>
</authors>
<title>Recognizing Arguing Subjectivity and Argument Tags.</title>
<date>2012</date>
<booktitle>In ACL Workshop on ExtraPropositional Aspects of Meaning,</booktitle>
<pages>80--88</pages>
<contexts>
<context position="6244" citStr="Conrad et al. (2012)" startWordPosition="954" endWordPosition="957"> and Wilson (2009) also showed accuracies of 98% in detecting agreement in the AMI corpus using lexical, subjectivity and dialogue act features. However, they note that their system could not classify disagreement accurately due to the small number of training examples in this category. Somasundaran et al. additionally show that dialogue act features complement lexical features in the AMI meeting corpus (Somasundaran et al., 2007). These observations are taken into account with our feature choices, and we use contextual, discourse and lexical features in our analysis. In the monologic domain, Conrad et al. (2012) recently found rhetorical relations to be useful for argument labelling and detection in articles on the topic of healthcare. Additionally, discourse markers and sentiment features have been found to assist with disagreement detection in collections of news documents on a particular topic, as well as reviews (Choi et al., 2010; Awadallah et al., 2012; Popescu et al., 2005). In the asynchronous domain, there has been recent work in disagreement detection, especially as it pertains to stance identification. Content based features, including sentiment, duration, and discourse markers have been u</context>
</contexts>
<marker>Conrad, Wiebe, Hwa, 2012</marker>
<rawString>Alexander Conrad, Janyce Wiebe and Rebecca Hwa. 2012. Recognizing Arguing Subjectivity and Argument Tags. In ACL Workshop on ExtraPropositional Aspects of Meaning, pages 80-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Jean</author>
</authors>
<title>Fox Tree.</title>
<date>2010</date>
<journal>Language and Linguistics Compass,</journal>
<pages>3--1</pages>
<marker>Jean, 2010</marker>
<rawString>Jean E. Fox Tree. 2010. Discourse markers across speakers and settings. Language and Linguistics Compass, 3(1):1-113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
<author>Julia Hirschberg</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Identifying agreement and disagreement in conversational speech: Use of bayesian networks to model pragmatic dependencies.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>669</pages>
<contexts>
<context position="5394" citStr="Galley et al., 2004" startWordPosition="823" endWordPosition="826">news-style and argument-style dialogue. 2 Related Work In the past decade, there have been a number of computational approaches developed for the task of disagreement and controversy detection, particularly in synchronous conversations such as meetings (Somasundaran et al., 2007; Raaijmakers et al., 2008) and in monologic corpora such as news collections (Awadallah et al., 2012) and reviews (Popescu et al., 2005; Mukherjee and Liu, 2012). In the domain of synchronous conversations, prosodic features such as duration, speech rate and pause have been used for spoken dialogue (Wang et al., 2011; Galley et al., 2004). Galley et al. (2004) found that local features, such as lexical and structural features, as well as global contextual features, were particularly useful for identifying agreement/disagreement in the ICSI meeting corpus. Germesin and Wilson (2009) also showed accuracies of 98% in detecting agreement in the AMI corpus using lexical, subjectivity and dialogue act features. However, they note that their system could not classify disagreement accurately due to the small number of training examples in this category. Somasundaran et al. additionally show that dialogue act features complement lexica</context>
</contexts>
<marker>Galley, McKeown, Hirschberg, Shriberg, 2004</marker>
<rawString>Michel Galley, Kathleen McKeown, Julia Hirschberg, and Elizabeth Shriberg. 2004. Identifying agreement and disagreement in conversational speech: Use of bayesian networks to model pragmatic dependencies. In Proceedings of ACL, pages 669-es.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Germesin</author>
<author>Theresa Wilson</author>
</authors>
<title>Agreement Detection in Multiparty Conversation.</title>
<date>2009</date>
<booktitle>In Proceedings of International Conference on Multimodal Interfaces</booktitle>
<pages>7--14</pages>
<contexts>
<context position="5642" citStr="Germesin and Wilson (2009)" startWordPosition="860" endWordPosition="863">s meetings (Somasundaran et al., 2007; Raaijmakers et al., 2008) and in monologic corpora such as news collections (Awadallah et al., 2012) and reviews (Popescu et al., 2005; Mukherjee and Liu, 2012). In the domain of synchronous conversations, prosodic features such as duration, speech rate and pause have been used for spoken dialogue (Wang et al., 2011; Galley et al., 2004). Galley et al. (2004) found that local features, such as lexical and structural features, as well as global contextual features, were particularly useful for identifying agreement/disagreement in the ICSI meeting corpus. Germesin and Wilson (2009) also showed accuracies of 98% in detecting agreement in the AMI corpus using lexical, subjectivity and dialogue act features. However, they note that their system could not classify disagreement accurately due to the small number of training examples in this category. Somasundaran et al. additionally show that dialogue act features complement lexical features in the AMI meeting corpus (Somasundaran et al., 2007). These observations are taken into account with our feature choices, and we use contextual, discourse and lexical features in our analysis. In the monologic domain, Conrad et al. (201</context>
</contexts>
<marker>Germesin, Wilson, 2009</marker>
<rawString>Sebastian Germesin and Theresa Wilson. 2009. Agreement Detection in Multiparty Conversation. In Proceedings of International Conference on Multimodal Interfaces pages 7-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vicenc Gomez</author>
<author>Andreas Kaltenbrunner</author>
<author>Vicente Lopez</author>
</authors>
<title>Statistical Analysis of the Social Network and Discussion Threads in Slashdot.</title>
<date>2008</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>645--654</pages>
<contexts>
<context position="7692" citStr="Gomez et al., 2008" startWordPosition="1182" endWordPosition="1185">ication (Murakami and Raymond, 2010), or have assumed that adjacent posts always disagree (Agrawal et al., 2003). More recent work has focused on identifying users’ attitudes towards each other (Hassan et al., 2010), influential users and posts (Nguyen et al., 2014), as well as identifying subgroups of users who share viewpoints (Abu-Jbara et al., 2010). In Slashdot, the h-index of a discussion has been used to rank articles according to controversiality, although no quantitative evaluation of this approach has been given, and, unlike in our analysis, they did not consider any other features (Gomez et al., 2008). Content based features such as polarity and cosine similarity have also been used to study influence, controversy and opinion changes on microblogging sites such as Twitter (Lin et al., 2013; Popescu and Pennacchiotti, 2010). The simplified task of detecting disagreement between just two users (either a question/response pair (Abbott et al., 2011) or two adjacent paragraphs (Misra and Walker, 2013)) has also been recently approached on the ARGUE corpus. Abbott et al. (2011) use discourse markers, generalized dependency features, punctuation and structural features, while Misra and Walker (20</context>
<context position="21844" citStr="Gomez et al., 2008" startWordPosition="3509" endWordPosition="3512">” posts for argument detection by applying the four categories of sentiment scores to only those posts connected by each of our 18 rhetorical relations. This is done for both posts with an inner rhetorical connection (identified by the sub-tree for that post), as well as for posts connected by a rhetorical relation in the relation graph. This results in a total of (4 sentiment categories)x(2 (different + same post connections))x(18 rhetorical relations) = 144 features. This set is referred to as “RhetSent”. 1173 4.4 Fragment Quotation Graphs As previously noted in (Murakami and Raymond, 2010; Gomez et al., 2008), the structure of discussion threads can aid in disagreement detection. In online, threaded conversations, the standard approach to extracting conversational structure is through reply-to relations usually present in online forums. However, if users strongly disagree on a topic (or sub-topic), they may choose to quote a specific paragraph (defined as a fragment) of a previous post in their reply. Being able to determine which specific fragments are linked by relations may then be useful for more targeted content-based features, helping to reduce noise. In order to address this, we use the Fra</context>
<context position="23071" citStr="Gomez et al., 2008" startWordPosition="3711" endWordPosition="3714">otation Graph (FQG), an approach previously developed by Carenini et al. (2007) for dialogue act modelling and topic segmentation (Joty et al., 2011; Joty et al., 2013). For our analysis, the FQG is found over the entire Slashdot article. We then select the subgraph corresponding to those fragments in the target topic. From the fragment quotation sub-graph, we are then able to extract features for disagreement detection such as the number of connections, total number of fragments, and the average path length between nodes which we hypothesize to be useful. We additionally extract the h-index (Gomez et al., 2008) and average branching ratio per fragment of the topic from the simpler reply-to conversational structure. In total, there are 8 FQG features. 4.6 Structural Features Length features have been well documented in the literature to provide useful information about whether or not arguments exist, especially in online conversations that may be more informative than subjective (Biyani et al., 2014; Yin et al., 2012). In this work, length features include the length of the post in sentences, the average number of words per sentence, the average number of sentences per post, the number of contributin</context>
<context position="36555" citStr="Gomez et al., 2008" startWordPosition="5902" endWordPosition="5905">rying the Microsoft software. We won’t, however, be trying the inferior Apple equivalent.” where the second example more likely signals, or even provokes, disagreement. Outside of the rhetorical features, the discourse markers which are found to be the most useful in our experiments agree with those found in the ARGUE corpus (Abbott et al., 2011). Namely, ‘oh’, ‘but’, ‘because’ and ‘and’ are discovered to be the most informative features. We also find the aggregated discourse marker frequency count in the first part of each post to be useful. Previous analysis on Slashdot as a social network (Gomez et al., 2008) suggests that the hindex of the conversation is relevant for detecting controversy in a posted article. We include the h-index as part of the Fragment Quotation Graph feature set, but surprisingly do not find this to be a useful feature. This may be due to our corpus involving relatively shallow conversational trees - the maximum h-index across all topics is three. Comparing to Mishne and Glance’s work, we also find quotations, questions and sentiment range near the beginning of a post to be very informative features. These are often selected across all feature sets which include the “basic” </context>
</contexts>
<marker>Gomez, Kaltenbrunner, Lopez, 2008</marker>
<rawString>Vicenc Gomez, Andreas Kaltenbrunner and Vicente Lopez. 2008. Statistical Analysis of the Social Network and Discussion Threads in Slashdot. In Proceedings of WWW, pages 645-654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Hassan</author>
<author>Vahed Qazvinian</author>
<author>Dragomir Radev</author>
</authors>
<title>What’s with the attitude?: identifying sentences with attitude in online discussions.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1245--1255</pages>
<contexts>
<context position="7288" citStr="Hassan et al., 2010" startWordPosition="1117" endWordPosition="1120">ent work in disagreement detection, especially as it pertains to stance identification. Content based features, including sentiment, duration, and discourse markers have been used for this task (Yin et al., 2012; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). The structure of a conversation has also been used, although these approaches have focused on simple rules for disagreement identification (Murakami and Raymond, 2010), or have assumed that adjacent posts always disagree (Agrawal et al., 2003). More recent work has focused on identifying users’ attitudes towards each other (Hassan et al., 2010), influential users and posts (Nguyen et al., 2014), as well as identifying subgroups of users who share viewpoints (Abu-Jbara et al., 2010). In Slashdot, the h-index of a discussion has been used to rank articles according to controversiality, although no quantitative evaluation of this approach has been given, and, unlike in our analysis, they did not consider any other features (Gomez et al., 2008). Content based features such as polarity and cosine similarity have also been used to study influence, controversy and opinion changes on microblogging sites such as Twitter (Lin et al., 2013; Po</context>
</contexts>
<marker>Hassan, Qazvinian, Radev, 2010</marker>
<rawString>Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev. 2010. What’s with the attitude?: identifying sentences with attitude in online discussions. In Proceedings of EMNLP, pages 1245-1255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Hernault</author>
<author>Helmut Prendinger</author>
<author>David A duVerle</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>HILDA: A Discourse Parser Using Support Vector Machine Classification. Dialogue and Discourse,</title>
<date>2010</date>
<pages>1--3</pages>
<contexts>
<context position="38144" citStr="Hernault et al., 2010" startWordPosition="6155" endWordPosition="6158"> the same set of conversations (those that have lower quality discourse trees with few connections). When combined with the “basic” feature set, these errors are mitigated, and the topics which the “basic” features miss are picked up by the rhetorical features. This leads to the best overall accuracy and F-score. 7.1 Discourse Parser A major source of error in detecting disagreement arises because of inaccuracies in our discourse parser. In particular, document-level discourse parsing is a challenging task, with relatively few parsers available at the time of this analysis (Joty et al., 2013; Hernault et al., 2010). We chose to use the discourse parser developed by Joty et al. which both identifies elementary discourse units in a text, and then builds a document-level discourse tree using Conditional Random Fields. Because their approach uses an optimal parsing algorithm as opposed to a greedy parsing algorithm, they are able to achieve much higher accuracies in relation and structure identification than other available parsers. Here, results from their parser on the standard RST-DT dataset are presented since there is no currently available dialogic corpora to compare to. RST-DT Instructional Metrics J</context>
</contexts>
<marker>Hernault, Prendinger, duVerle, Ishizuka, 2010</marker>
<rawString>Hugo Hernault, Helmut Prendinger, David A. duVerle and Mitsuru Ishizuka. 2010. HILDA: A Discourse Parser Using Support Vector Machine Classification. Dialogue and Discourse, 1(3):1-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurence R Horn</author>
</authors>
<title>A natural history of negation.</title>
<date>1989</date>
<publisher>Chicago University Press.</publisher>
<contexts>
<context position="2133" citStr="Horn, 1989" startWordPosition="324" endWordPosition="325">erstanding how debate arises and propagates in a conversation, we may also gain insight into how authors’ opinions on a topic can be influenced over time. The long term goal of our research is to lay the foundations for understanding argumentative structure in conversations, which could be applied to NLP tasks such as summarization, information retrieval, and text visualization. Argumentative structure theory has been thoroughly studied in both psychology and rhetoric, with negation and discourse markers, as well as hedging and dispreferred responses, being known to be indicative of argument (Horn, 1989; Brown and Levinson, 1987). As a starting point, in this paper we focus on the detection of disagreement in casual conversations between users. This requires a generalized approach that can accurately identify disagreement in topics ranging from something as mundane as whether GPS stands for galactic positioning system or global positioning system, to more ideological debates about distrust in science. Motivated by the widespread consensus in both computational and theoretical linguistics on the utility of discourse markers for signalling pragmatic functions such as disagreement and personal </context>
<context position="3724" citStr="Horn, 1989" startWordPosition="564" endWordPosition="565">pose to treat conversational dialogue as a collection of linked monologues, and subsequently build a relation graph describing both rhetorical connections within user posts, as well as between different users. Features obtained from this graph offer significant improvements on disagreement detection over a baseline consisting of meta-post features, lexical features, discourse markers and conversational features. Not only do these features improve disagreement detection, but the discovered importance of relations known to be theoretically relevant to disagreement detection, such as COMPARISON (Horn, 1989), suggest that this approach may be capturing the essential aspects of the conversational argumentative structure. 1169 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1169–1180, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics As a second contribution of this work, we provide a new dataset consisting of 95 topics annotated for disagreement. Unlike the publicly available ARGUE corpus based on the online debate forum 4forums.com (Abbott et al., 2011), our corpus is based on Slashdot, which is a general purpo</context>
<context position="17910" citStr="Horn, 1989" startWordPosition="2860" endWordPosition="2861">nk with that label). Finally, when posts are connected through internal nodes (such as P1 and I1 in Figure 1a), a labelled link is added for each post in the internal node to the relation graph (N1-&gt;N2 and N1-&gt;N3 in Figure 1b). This relation graph allows for the extraction of many features that may reflect argumentative structure, such as the number of connections, the frequency of each rhetorical relation in the graph per post (diff per post), and the frequency as a percentage of all rhetorical relations (diff percentage). For example, COMPARISON relations are known to indicate disagreement (Horn, 1989), so we expect higher frequencies of this relation if the conversation contains argument. Features from the discourse tree such as the average depth of each rhetorical relation are also added to reflect the cohesiveness of conversation. Moreover, features combining the graph and tree representations, such as the ratio of the frequency of a rhetorical relation occurring between different posts to the average depth (CONTRAST between different posts) called avg ratio Average depth of CONTRAST are implemented. These reflect the hypothesis that relations connecting larger chunks of text (or whole p</context>
<context position="33604" citStr="Horn, 1989" startWordPosition="5439" endWordPosition="5440">age), ELABORATION (average depth) Discourse Markers Aggregated first sentence, Aggregated middle, ’and’, ’oh’, ’but’ frequency counts N-grams ‘don’t ?’, ‘plus’, ‘private’, ‘anti’, ‘hey’, ‘present’, ‘making’, ‘developers’ RhetSent ELABORATION (variance in same post), ATTRIBUTION (variance in same post), CONTRAST (range in same post) Sentiment Range of sentiment scores in first sentence of all posts, range of sentiment scores over all posts Table 4: Features found to be commonly selected over different iterations of the classifiers dicate disagreement as motivated by theoretical considerations (Horn, 1989). The importance of other rhetorical relation features can also be explained by examining conversations in which they appear. In particular, EXPLANATION relations often link authors who share viewpoints in a debate, especially when one author is trying to support the claim of another. The JOINT relations are also very well motivated. In the extreme case, conversations with a very high number of JOINT relations between different users are usually news based. The high proportion of these relations indicates that many users have added information to the conversation about a specific item, such as</context>
</contexts>
<marker>Horn, 1989</marker>
<rawString>Laurence R. Horn. 1989. A natural history of negation. Chicago University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Unsupervised modeling of dialog acts in asynchronous conversations.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>1807--1813</pages>
<contexts>
<context position="22600" citStr="Joty et al., 2011" startWordPosition="3632" endWordPosition="3635">ting conversational structure is through reply-to relations usually present in online forums. However, if users strongly disagree on a topic (or sub-topic), they may choose to quote a specific paragraph (defined as a fragment) of a previous post in their reply. Being able to determine which specific fragments are linked by relations may then be useful for more targeted content-based features, helping to reduce noise. In order to address this, we use the Fragment Quotation Graph (FQG), an approach previously developed by Carenini et al. (2007) for dialogue act modelling and topic segmentation (Joty et al., 2011; Joty et al., 2013). For our analysis, the FQG is found over the entire Slashdot article. We then select the subgraph corresponding to those fragments in the target topic. From the fragment quotation sub-graph, we are then able to extract features for disagreement detection such as the number of connections, total number of fragments, and the average path length between nodes which we hypothesize to be useful. We additionally extract the h-index (Gomez et al., 2008) and average branching ratio per fragment of the topic from the simpler reply-to conversational structure. In total, there are 8 </context>
</contexts>
<marker>Joty, Carenini, Lin, 2011</marker>
<rawString>Shafiq Joty, Giuseppe Carenini, and Chin-Yew Lin. 2011. Unsupervised modeling of dialog acts in asynchronous conversations. In Proceedings of IJCAI, pages 1807-1813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
<author>Yashar Mehdad</author>
</authors>
<title>Combining Intra- and Multisentential Rhetorical Parsing for Document-level Discourse Analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="11792" citStr="Joty et al. (2013)" startWordPosition="1868" endWordPosition="1871"> comments. Each comment has a timestamp at the minute resolution as well as author information (although it is possible to post on the forum anonymously). Additionally, other users can give different posts scores (in the range -1 to 5) as well as categorizing posts under “funny”, “interesting”, “informative”, “insightful”, “flamebait”, “off topic”, or “troll”. This user moderation, as well as the 1www.slashdot.org formalized reply-to structure between comments, makes Slashdot attractive over other internet forums as it allows for high-quality and structured conversations. In a previous study, Joty et al. (2013) selected 20 articles and their associated comments to be annotated for topic segmentation boundaries and labels by an expert Slashdot contributor. They define a topic as a subset of the utterances in a conversation, while a topic label describes what the given topic is about (e.g., Physics in videogames). Of the 98 annotated topics from their dataset, we filtered out those with only one contributing user, for a total of 95 topics. Next, we developed a Human Intelligence Task (HIT) using the crowdsourcing platform Crowdflower.2 The objective of this task was to both develop a corpus for testin</context>
<context position="15865" citStr="Joty et al., 2013" startWordPosition="2518" endWordPosition="2521">rs by capturing the underlying discourse structure of the conversation in terms of discourse relations. In Rhetorical Structure Theory, Discourse trees are a hierarchical representation of document structure for monologues (Mann and Thompson, 1988). At the lowest level, Elementary Discourse Units (EDUs) are connected by discourse relations (such as ELABORATION and COMPARISON), which in turn form larger discourse units that are also linked by these relations. Computational systems (discourse parsers) have been recently developed to automatically generate a discourse tree for a given monologue (Joty et al., 2013). Although theoretically the rhetorical relations expected in dialogues are different from those in monologues (Stent and Allen, 2000), no sophisticated computational tools exist yet for detecting these relations reliably. The core idea of this work is that some useful (although noisy) information about the discourse structure of a conversation can be obtained by applying state-of-the-art document level discourse parsing to parts of the conversation. More specifically, posts on a particular topic are concatenated according to their temporal order. This pseudo-monologic document is then fed to </context>
<context position="22620" citStr="Joty et al., 2013" startWordPosition="3636" endWordPosition="3639"> structure is through reply-to relations usually present in online forums. However, if users strongly disagree on a topic (or sub-topic), they may choose to quote a specific paragraph (defined as a fragment) of a previous post in their reply. Being able to determine which specific fragments are linked by relations may then be useful for more targeted content-based features, helping to reduce noise. In order to address this, we use the Fragment Quotation Graph (FQG), an approach previously developed by Carenini et al. (2007) for dialogue act modelling and topic segmentation (Joty et al., 2011; Joty et al., 2013). For our analysis, the FQG is found over the entire Slashdot article. We then select the subgraph corresponding to those fragments in the target topic. From the fragment quotation sub-graph, we are then able to extract features for disagreement detection such as the number of connections, total number of fragments, and the average path length between nodes which we hypothesize to be useful. We additionally extract the h-index (Gomez et al., 2008) and average branching ratio per fragment of the topic from the simpler reply-to conversational structure. In total, there are 8 FQG features. 4.6 St</context>
<context position="38120" citStr="Joty et al., 2013" startWordPosition="6151" endWordPosition="6154">stently misclassify the same set of conversations (those that have lower quality discourse trees with few connections). When combined with the “basic” feature set, these errors are mitigated, and the topics which the “basic” features miss are picked up by the rhetorical features. This leads to the best overall accuracy and F-score. 7.1 Discourse Parser A major source of error in detecting disagreement arises because of inaccuracies in our discourse parser. In particular, document-level discourse parsing is a challenging task, with relatively few parsers available at the time of this analysis (Joty et al., 2013; Hernault et al., 2010). We chose to use the discourse parser developed by Joty et al. which both identifies elementary discourse units in a text, and then builds a document-level discourse tree using Conditional Random Fields. Because their approach uses an optimal parsing algorithm as opposed to a greedy parsing algorithm, they are able to achieve much higher accuracies in relation and structure identification than other available parsers. Here, results from their parser on the standard RST-DT dataset are presented since there is no currently available dialogic corpora to compare to. RST-DT</context>
</contexts>
<marker>Joty, Carenini, Ng, Mehdad, 2013</marker>
<rawString>Shafiq Joty, Giuseppe Carenini, Raymond Ng and Yashar Mehdad. 2013. Combining Intra- and Multisentential Rhetorical Parsing for Document-level Discourse Analysis. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
</authors>
<title>Topic Segmentation and Labeling in Asynchronous Conversations.</title>
<date>2013</date>
<journal>Journal of AI Research,</journal>
<pages>47--521</pages>
<contexts>
<context position="11792" citStr="Joty et al. (2013)" startWordPosition="1868" endWordPosition="1871"> comments. Each comment has a timestamp at the minute resolution as well as author information (although it is possible to post on the forum anonymously). Additionally, other users can give different posts scores (in the range -1 to 5) as well as categorizing posts under “funny”, “interesting”, “informative”, “insightful”, “flamebait”, “off topic”, or “troll”. This user moderation, as well as the 1www.slashdot.org formalized reply-to structure between comments, makes Slashdot attractive over other internet forums as it allows for high-quality and structured conversations. In a previous study, Joty et al. (2013) selected 20 articles and their associated comments to be annotated for topic segmentation boundaries and labels by an expert Slashdot contributor. They define a topic as a subset of the utterances in a conversation, while a topic label describes what the given topic is about (e.g., Physics in videogames). Of the 98 annotated topics from their dataset, we filtered out those with only one contributing user, for a total of 95 topics. Next, we developed a Human Intelligence Task (HIT) using the crowdsourcing platform Crowdflower.2 The objective of this task was to both develop a corpus for testin</context>
<context position="15865" citStr="Joty et al., 2013" startWordPosition="2518" endWordPosition="2521">rs by capturing the underlying discourse structure of the conversation in terms of discourse relations. In Rhetorical Structure Theory, Discourse trees are a hierarchical representation of document structure for monologues (Mann and Thompson, 1988). At the lowest level, Elementary Discourse Units (EDUs) are connected by discourse relations (such as ELABORATION and COMPARISON), which in turn form larger discourse units that are also linked by these relations. Computational systems (discourse parsers) have been recently developed to automatically generate a discourse tree for a given monologue (Joty et al., 2013). Although theoretically the rhetorical relations expected in dialogues are different from those in monologues (Stent and Allen, 2000), no sophisticated computational tools exist yet for detecting these relations reliably. The core idea of this work is that some useful (although noisy) information about the discourse structure of a conversation can be obtained by applying state-of-the-art document level discourse parsing to parts of the conversation. More specifically, posts on a particular topic are concatenated according to their temporal order. This pseudo-monologic document is then fed to </context>
<context position="22620" citStr="Joty et al., 2013" startWordPosition="3636" endWordPosition="3639"> structure is through reply-to relations usually present in online forums. However, if users strongly disagree on a topic (or sub-topic), they may choose to quote a specific paragraph (defined as a fragment) of a previous post in their reply. Being able to determine which specific fragments are linked by relations may then be useful for more targeted content-based features, helping to reduce noise. In order to address this, we use the Fragment Quotation Graph (FQG), an approach previously developed by Carenini et al. (2007) for dialogue act modelling and topic segmentation (Joty et al., 2011; Joty et al., 2013). For our analysis, the FQG is found over the entire Slashdot article. We then select the subgraph corresponding to those fragments in the target topic. From the fragment quotation sub-graph, we are then able to extract features for disagreement detection such as the number of connections, total number of fragments, and the average path length between nodes which we hypothesize to be useful. We additionally extract the h-index (Gomez et al., 2008) and average branching ratio per fragment of the topic from the simpler reply-to conversational structure. In total, there are 8 FQG features. 4.6 St</context>
<context position="38120" citStr="Joty et al., 2013" startWordPosition="6151" endWordPosition="6154">stently misclassify the same set of conversations (those that have lower quality discourse trees with few connections). When combined with the “basic” feature set, these errors are mitigated, and the topics which the “basic” features miss are picked up by the rhetorical features. This leads to the best overall accuracy and F-score. 7.1 Discourse Parser A major source of error in detecting disagreement arises because of inaccuracies in our discourse parser. In particular, document-level discourse parsing is a challenging task, with relatively few parsers available at the time of this analysis (Joty et al., 2013; Hernault et al., 2010). We chose to use the discourse parser developed by Joty et al. which both identifies elementary discourse units in a text, and then builds a document-level discourse tree using Conditional Random Fields. Because their approach uses an optimal parsing algorithm as opposed to a greedy parsing algorithm, they are able to achieve much higher accuracies in relation and structure identification than other available parsers. Here, results from their parser on the standard RST-DT dataset are presented since there is no currently available dialogic corpora to compare to. RST-DT</context>
</contexts>
<marker>Joty, Carenini, Ng, 2013</marker>
<rawString>Shafiq Joty, Giuseppe Carenini and Raymond Ng. 2013. Topic Segmentation and Labeling in Asynchronous Conversations. Journal of AI Research, 47:521-573.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ching-Sheng Lin</author>
<author>Samira Shaikh</author>
<author>Jennifer StromerGalley</author>
<author>Jennifer Crowley</author>
<author>Tomek Strzalkowski</author>
<author>Veena Ravishankar</author>
</authors>
<title>Topical Positioning: A New Method for Predicting Opinion Changes in Conversation.</title>
<date>2013</date>
<booktitle>In Proceedings of LASM,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="7884" citStr="Lin et al., 2013" startWordPosition="1215" endWordPosition="1218">Hassan et al., 2010), influential users and posts (Nguyen et al., 2014), as well as identifying subgroups of users who share viewpoints (Abu-Jbara et al., 2010). In Slashdot, the h-index of a discussion has been used to rank articles according to controversiality, although no quantitative evaluation of this approach has been given, and, unlike in our analysis, they did not consider any other features (Gomez et al., 2008). Content based features such as polarity and cosine similarity have also been used to study influence, controversy and opinion changes on microblogging sites such as Twitter (Lin et al., 2013; Popescu and Pennacchiotti, 2010). The simplified task of detecting disagreement between just two users (either a question/response pair (Abbott et al., 2011) or two adjacent paragraphs (Misra and Walker, 2013)) has also been recently approached on the ARGUE corpus. Abbott et al. (2011) use discourse markers, generalized dependency features, punctuation and structural features, while Misra and Walker (2013) focus on n-grams indicative of denial, hedging and agreement, as well as cue words and punctuation. Most similar to our work is that by Mishne and Glance (2006). They performed a general a</context>
</contexts>
<marker>Lin, Shaikh, StromerGalley, Crowley, Strzalkowski, Ravishankar, 2013</marker>
<rawString>Ching-Sheng Lin, Samira Shaikh, Jennifer StromerGalley, Jennifer Crowley, Tomek Strzalkowski, Veena Ravishankar. 2013. Topical Positioning: A New Method for Predicting Opinion Changes in Conversation. In Proceedings of LASM, pages 41-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text,</tech>
<pages>8--3</pages>
<contexts>
<context position="2965" citStr="Mann and Thompson (1988)" startWordPosition="452" endWordPosition="455">fy disagreement in topics ranging from something as mundane as whether GPS stands for galactic positioning system or global positioning system, to more ideological debates about distrust in science. Motivated by the widespread consensus in both computational and theoretical linguistics on the utility of discourse markers for signalling pragmatic functions such as disagreement and personal opinions (Webber and Prasad, 2008; Abbott et al., 2011; J. E. Fox-Tree, 2010), we introduce a new set of features based on the Discourse Tree (DT) of a conversational text. Discourse Trees were formalized by Mann and Thompson (1988) as part of their Rhetorical Structure Theory (RST) to represent the structure of discourse. Although this theory is for monologic discourse, we propose to treat conversational dialogue as a collection of linked monologues, and subsequently build a relation graph describing both rhetorical connections within user posts, as well as between different users. Features obtained from this graph offer significant improvements on disagreement detection over a baseline consisting of meta-post features, lexical features, discourse markers and conversational features. Not only do these features improve d</context>
<context position="15495" citStr="Mann and Thompson, 1988" startWordPosition="2461" endWordPosition="2464">s, sentiment features, n-gram models, Slashdot meta-features, structural features, and lexicon features. 4.1 Rhetorical Relation Graphs Discourse markers have been found to aid in argument and disagreement detection, and for tasks such as stance identification (Abbott et al., 2011; Misra and Walker, 2013; Somasundaran and Wiebe, 2009). We aim to improve over discourse markers by capturing the underlying discourse structure of the conversation in terms of discourse relations. In Rhetorical Structure Theory, Discourse trees are a hierarchical representation of document structure for monologues (Mann and Thompson, 1988). At the lowest level, Elementary Discourse Units (EDUs) are connected by discourse relations (such as ELABORATION and COMPARISON), which in turn form larger discourse units that are also linked by these relations. Computational systems (discourse parsers) have been recently developed to automatically generate a discourse tree for a given monologue (Joty et al., 2013). Although theoretically the rhetorical relations expected in dialogues are different from those in monologues (Stent and Allen, 2000), no sophisticated computational tools exist yet for detecting these relations reliably. The cor</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gilad Mishne</author>
<author>Natalie Glance</author>
</authors>
<title>Leave a reply: An analysis of weblog comments.</title>
<date>2006</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="8456" citStr="Mishne and Glance (2006)" startWordPosition="1304" endWordPosition="1307">icroblogging sites such as Twitter (Lin et al., 2013; Popescu and Pennacchiotti, 2010). The simplified task of detecting disagreement between just two users (either a question/response pair (Abbott et al., 2011) or two adjacent paragraphs (Misra and Walker, 2013)) has also been recently approached on the ARGUE corpus. Abbott et al. (2011) use discourse markers, generalized dependency features, punctuation and structural features, while Misra and Walker (2013) focus on n-grams indicative of denial, hedging and agreement, as well as cue words and punctuation. Most similar to our work is that by Mishne and Glance (2006). They performed a general analysis of weblog comments, using punctuation, quotes, lexicon counts, subjectivity, polarity and referrals to detect disputative and non-disputative comments. Referrals and questions, as well as polarity measures in the first section of the post, were found to be most useful. However, their analysis did not 1170 Type Num P/A S/P W/P Num Authors W/S TBP TT TP Length Disagreement - C 19.00 1.02 3.21 65.86 15.84 20.47 4.60 50.90 16.21 49.11 Disagreement - NC 27.00 1.00 3.08 59.80 14.07 19.33 3.89 42.29 14.11 42.26 No disagreement - NC 28.00 1.03 2.85 57.25 10.29 19.94</context>
<context position="20178" citStr="Mishne and Glance, 2006" startWordPosition="3228" endWordPosition="3231"> found to be the most common across the ARGUE corpus (Abbott et al., 2011). Furthermore, we hypothesize that individual discourse markers might have low frequency counts in the text. Therefore, we also include an aggregated count of all 17 discourse markers in each fifth of the posts in a topic (e.g. the count of all 17 discourse markers in the first fifth of every post in the topic). Altogether, there are 5 aggregated discourse marker features in addition to the 17 frequency count features. 4.3 Sentiment Features Sentiment polarity features have been shown to be useful in argument detection (Mishne and Glance, 2006). For this work, we use four sentiment scoring categories: the variance, average score, number of negative sentences, and controversiality score (Carenini and Cheung, 2008) of sentences in a post. These are determined using SoCAL (Taboada et al., 2011), which gives each sentence a polarity score and has been shown to work well on user-generated content. Overall, we have two main classes of sentiment features. The first type splits all the posts in a topic into 4 sections corresponding to the sentences in each quarter of the post. The sentiment scores described above are then applied to each se</context>
<context position="24164" citStr="Mishne and Glance, 2006" startWordPosition="3893" endWordPosition="3896">he post in sentences, the average number of words per sentence, the average number of sentences per post, the number of contributing authors, the rate of posting, and the total amount of time of the conversation. This results in a total of 9 features. 4.7 Punctuation Like many other features already described, frequency counts of ‘?’,‘!’,‘”’,‘”, and ‘.’ are found for each fifth of the post (the first fifth, second fifth, etc.). These counts are then aggregated across all posts for a total of 5×5 = 25 features. 4.8 Referrals Referrals have been found to help with the detection of disagreement (Mishne and Glance, 2006), especially with respect to other authors. Since there are no direct referrals to previous authors in this corpus, references to variations of “you”, “they”, “us”, “I”, and “he/she” in each fifth of the post are included instead, for a total of 5×5 = 25 features. 4.9 Meta-Post Features 4.5 N-gram models Slashdot allows users to rank others’ posts with the As noted previously (Somasundaran and Wiebe, equivalent of a “like” button, changing the “score” 2010; Thomas et al., 2006), it is often difficult to of the post (to a maximum of 5). They are also outperform a unigram/bigram model in the tas</context>
</contexts>
<marker>Mishne, Glance, 2006</marker>
<rawString>Gilad Mishne and Natalie Glance. 2006. Leave a reply: An analysis of weblog comments. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amita Misra</author>
<author>Marilyn Walker</author>
</authors>
<title>Topic Independent Identification of Agreement and Disagreement in Social Media Dialogue.</title>
<date>2013</date>
<booktitle>In Proceedings of SIGDIAL,</booktitle>
<pages>41--50</pages>
<contexts>
<context position="8095" citStr="Misra and Walker, 2013" startWordPosition="1246" endWordPosition="1249">as been used to rank articles according to controversiality, although no quantitative evaluation of this approach has been given, and, unlike in our analysis, they did not consider any other features (Gomez et al., 2008). Content based features such as polarity and cosine similarity have also been used to study influence, controversy and opinion changes on microblogging sites such as Twitter (Lin et al., 2013; Popescu and Pennacchiotti, 2010). The simplified task of detecting disagreement between just two users (either a question/response pair (Abbott et al., 2011) or two adjacent paragraphs (Misra and Walker, 2013)) has also been recently approached on the ARGUE corpus. Abbott et al. (2011) use discourse markers, generalized dependency features, punctuation and structural features, while Misra and Walker (2013) focus on n-grams indicative of denial, hedging and agreement, as well as cue words and punctuation. Most similar to our work is that by Mishne and Glance (2006). They performed a general analysis of weblog comments, using punctuation, quotes, lexicon counts, subjectivity, polarity and referrals to detect disputative and non-disputative comments. Referrals and questions, as well as polarity measur</context>
<context position="15176" citStr="Misra and Walker, 2013" startWordPosition="2415" endWordPosition="2418">scores below 55%, which suggests that subtle disagreement detection is a subjective and difficult task. Further statistics for the developed corpus are given in Table 1. 4 Features for Disagreement Detection The features we use in our experiments combine information from conversational structure, rhetorical relations, sentiment features, n-gram models, Slashdot meta-features, structural features, and lexicon features. 4.1 Rhetorical Relation Graphs Discourse markers have been found to aid in argument and disagreement detection, and for tasks such as stance identification (Abbott et al., 2011; Misra and Walker, 2013; Somasundaran and Wiebe, 2009). We aim to improve over discourse markers by capturing the underlying discourse structure of the conversation in terms of discourse relations. In Rhetorical Structure Theory, Discourse trees are a hierarchical representation of document structure for monologues (Mann and Thompson, 1988). At the lowest level, Elementary Discourse Units (EDUs) are connected by discourse relations (such as ELABORATION and COMPARISON), which in turn form larger discourse units that are also linked by these relations. Computational systems (discourse parsers) have been recently devel</context>
</contexts>
<marker>Misra, Walker, 2013</marker>
<rawString>Amita Misra and Marilyn Walker. 2013. Topic Independent Identification of Agreement and Disagreement in Social Media Dialogue. In Proceedings of SIGDIAL, pages 41-50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjun Mukherjee</author>
<author>Bing Liu</author>
</authors>
<title>Modeling review comments.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>320--329</pages>
<contexts>
<context position="5215" citStr="Mukherjee and Liu, 2012" startWordPosition="794" endWordPosition="797">g opinions or stances. Because of this, our corpus represents an excellent testbed to examine methods for more subtle disagreement detection, as well as the major differences between news-style and argument-style dialogue. 2 Related Work In the past decade, there have been a number of computational approaches developed for the task of disagreement and controversy detection, particularly in synchronous conversations such as meetings (Somasundaran et al., 2007; Raaijmakers et al., 2008) and in monologic corpora such as news collections (Awadallah et al., 2012) and reviews (Popescu et al., 2005; Mukherjee and Liu, 2012). In the domain of synchronous conversations, prosodic features such as duration, speech rate and pause have been used for spoken dialogue (Wang et al., 2011; Galley et al., 2004). Galley et al. (2004) found that local features, such as lexical and structural features, as well as global contextual features, were particularly useful for identifying agreement/disagreement in the ICSI meeting corpus. Germesin and Wilson (2009) also showed accuracies of 98% in detecting agreement in the AMI corpus using lexical, subjectivity and dialogue act features. However, they note that their system could not</context>
</contexts>
<marker>Mukherjee, Liu, 2012</marker>
<rawString>Arjun Mukherjee and Bing Liu. 2012. Modeling review comments. In Proceedings of ACL, pages 320-329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akiko Murakami</author>
<author>Rudy Raymond</author>
</authors>
<title>Support or Oppose? Classifying Positions in Online Debates from Reply Activities and Opinion Expressions.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics,</booktitle>
<pages>869--875</pages>
<contexts>
<context position="7109" citStr="Murakami and Raymond, 2010" startWordPosition="1086" endWordPosition="1090"> collections of news documents on a particular topic, as well as reviews (Choi et al., 2010; Awadallah et al., 2012; Popescu et al., 2005). In the asynchronous domain, there has been recent work in disagreement detection, especially as it pertains to stance identification. Content based features, including sentiment, duration, and discourse markers have been used for this task (Yin et al., 2012; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). The structure of a conversation has also been used, although these approaches have focused on simple rules for disagreement identification (Murakami and Raymond, 2010), or have assumed that adjacent posts always disagree (Agrawal et al., 2003). More recent work has focused on identifying users’ attitudes towards each other (Hassan et al., 2010), influential users and posts (Nguyen et al., 2014), as well as identifying subgroups of users who share viewpoints (Abu-Jbara et al., 2010). In Slashdot, the h-index of a discussion has been used to rank articles according to controversiality, although no quantitative evaluation of this approach has been given, and, unlike in our analysis, they did not consider any other features (Gomez et al., 2008). Content based f</context>
<context position="21823" citStr="Murakami and Raymond, 2010" startWordPosition="3505" endWordPosition="3508"> to identify “more important” posts for argument detection by applying the four categories of sentiment scores to only those posts connected by each of our 18 rhetorical relations. This is done for both posts with an inner rhetorical connection (identified by the sub-tree for that post), as well as for posts connected by a rhetorical relation in the relation graph. This results in a total of (4 sentiment categories)x(2 (different + same post connections))x(18 rhetorical relations) = 144 features. This set is referred to as “RhetSent”. 1173 4.4 Fragment Quotation Graphs As previously noted in (Murakami and Raymond, 2010; Gomez et al., 2008), the structure of discussion threads can aid in disagreement detection. In online, threaded conversations, the standard approach to extracting conversational structure is through reply-to relations usually present in online forums. However, if users strongly disagree on a topic (or sub-topic), they may choose to quote a specific paragraph (defined as a fragment) of a previous post in their reply. Being able to determine which specific fragments are linked by relations may then be useful for more targeted content-based features, helping to reduce noise. In order to address</context>
</contexts>
<marker>Murakami, Raymond, 2010</marker>
<rawString>Akiko Murakami and Rudy Raymond. 2010. Support or Oppose? Classifying Positions in Online Debates from Reply Activities and Opinion Expressions. In Proceedings of the International Conference on Computational Linguistics, pages 869-875.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viet-An Nguyen</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
<author>Deborah Cai</author>
<author>Jennifer Midberry</author>
<author>Yuanxin Wang</author>
</authors>
<title>Modeling topic control to detect influence in conversations using nonparametric topic models.</title>
<date>2014</date>
<journal>Machine Learning</journal>
<pages>95--381</pages>
<contexts>
<context position="7339" citStr="Nguyen et al., 2014" startWordPosition="1126" endWordPosition="1129">t pertains to stance identification. Content based features, including sentiment, duration, and discourse markers have been used for this task (Yin et al., 2012; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). The structure of a conversation has also been used, although these approaches have focused on simple rules for disagreement identification (Murakami and Raymond, 2010), or have assumed that adjacent posts always disagree (Agrawal et al., 2003). More recent work has focused on identifying users’ attitudes towards each other (Hassan et al., 2010), influential users and posts (Nguyen et al., 2014), as well as identifying subgroups of users who share viewpoints (Abu-Jbara et al., 2010). In Slashdot, the h-index of a discussion has been used to rank articles according to controversiality, although no quantitative evaluation of this approach has been given, and, unlike in our analysis, they did not consider any other features (Gomez et al., 2008). Content based features such as polarity and cosine similarity have also been used to study influence, controversy and opinion changes on microblogging sites such as Twitter (Lin et al., 2013; Popescu and Pennacchiotti, 2010). The simplified task</context>
</contexts>
<marker>Nguyen, Boyd-Graber, Resnik, Cai, Midberry, Wang, 2014</marker>
<rawString>Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik, Deborah Cai, Jennifer Midberry, Yuanxin Wang. 2014. Modeling topic control to detect influence in conversations using nonparametric topic models. Machine Learning 95:381-421.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting Product Features and Opinions from Reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<pages>339--346</pages>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting Product Features and Opinions from Reviews. In Proceedings of HLT/EMNLP, pages 339-346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Detecting controversial events from twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>1873--1876</pages>
<contexts>
<context position="7918" citStr="Popescu and Pennacchiotti, 2010" startWordPosition="1219" endWordPosition="1222">0), influential users and posts (Nguyen et al., 2014), as well as identifying subgroups of users who share viewpoints (Abu-Jbara et al., 2010). In Slashdot, the h-index of a discussion has been used to rank articles according to controversiality, although no quantitative evaluation of this approach has been given, and, unlike in our analysis, they did not consider any other features (Gomez et al., 2008). Content based features such as polarity and cosine similarity have also been used to study influence, controversy and opinion changes on microblogging sites such as Twitter (Lin et al., 2013; Popescu and Pennacchiotti, 2010). The simplified task of detecting disagreement between just two users (either a question/response pair (Abbott et al., 2011) or two adjacent paragraphs (Misra and Walker, 2013)) has also been recently approached on the ARGUE corpus. Abbott et al. (2011) use discourse markers, generalized dependency features, punctuation and structural features, while Misra and Walker (2013) focus on n-grams indicative of denial, hedging and agreement, as well as cue words and punctuation. Most similar to our work is that by Mishne and Glance (2006). They performed a general analysis of weblog comments, using </context>
</contexts>
<marker>Popescu, Pennacchiotti, 2010</marker>
<rawString>Ana-Maria Popescu and Marco Pennacchiotti. 2010. Detecting controversial events from twitter. In Proceedings of CIKM, pages 1873-1876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Raaijmakers</author>
<author>Khiet Truong</author>
<author>Theresa Wilson</author>
</authors>
<title>Multimodal subjectivity analysis of multiparty conversation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>466--474</pages>
<contexts>
<context position="5080" citStr="Raaijmakers et al., 2008" startWordPosition="772" endWordPosition="775">as certain topics (like discussing GPS systems) may be targeted towards objective information sharing without any participants expressing opinions or stances. Because of this, our corpus represents an excellent testbed to examine methods for more subtle disagreement detection, as well as the major differences between news-style and argument-style dialogue. 2 Related Work In the past decade, there have been a number of computational approaches developed for the task of disagreement and controversy detection, particularly in synchronous conversations such as meetings (Somasundaran et al., 2007; Raaijmakers et al., 2008) and in monologic corpora such as news collections (Awadallah et al., 2012) and reviews (Popescu et al., 2005; Mukherjee and Liu, 2012). In the domain of synchronous conversations, prosodic features such as duration, speech rate and pause have been used for spoken dialogue (Wang et al., 2011; Galley et al., 2004). Galley et al. (2004) found that local features, such as lexical and structural features, as well as global contextual features, were particularly useful for identifying agreement/disagreement in the ICSI meeting corpus. Germesin and Wilson (2009) also showed accuracies of 98% in dete</context>
</contexts>
<marker>Raaijmakers, Truong, Wilson, 2008</marker>
<rawString>Stephan Raaijmakers, Khiet Truong, Theresa Wilson. 2008. Multimodal subjectivity analysis of multiparty conversation. In Proceedings of EMNLP, pages 466-474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Dan Jurafsky</author>
</authors>
<title>Linguistic Models for Analyzing and Detecting Biased Language.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>16501659</pages>
<marker>Recasens, Danescu-Niculescu-Mizil, Jurafsky, 2013</marker>
<rawString>Marta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky. 2013. Linguistic Models for Analyzing and Detecting Biased Language. In Proceedings of ACL, pages 16501659.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Josef Ruppenhofer</author>
<author>Janyce Wiebe</author>
</authors>
<title>Detecting Arguing and Sentiment in Meetings.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGDIAL Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="5053" citStr="Somasundaran et al., 2007" startWordPosition="768" endWordPosition="771">lt task in our new corpus, as certain topics (like discussing GPS systems) may be targeted towards objective information sharing without any participants expressing opinions or stances. Because of this, our corpus represents an excellent testbed to examine methods for more subtle disagreement detection, as well as the major differences between news-style and argument-style dialogue. 2 Related Work In the past decade, there have been a number of computational approaches developed for the task of disagreement and controversy detection, particularly in synchronous conversations such as meetings (Somasundaran et al., 2007; Raaijmakers et al., 2008) and in monologic corpora such as news collections (Awadallah et al., 2012) and reviews (Popescu et al., 2005; Mukherjee and Liu, 2012). In the domain of synchronous conversations, prosodic features such as duration, speech rate and pause have been used for spoken dialogue (Wang et al., 2011; Galley et al., 2004). Galley et al. (2004) found that local features, such as lexical and structural features, as well as global contextual features, were particularly useful for identifying agreement/disagreement in the ICSI meeting corpus. Germesin and Wilson (2009) also showe</context>
</contexts>
<marker>Somasundaran, Ruppenhofer, Wiebe, 2007</marker>
<rawString>Swapna Somasundaran, Josef Ruppenhofer, Janyce Wiebe. 2007. Detecting Arguing and Sentiment in Meetings. In Proceedings of SIGDIAL Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in online debates.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>226--234</pages>
<contexts>
<context position="6909" citStr="Somasundaran and Wiebe, 2009" startWordPosition="1058" endWordPosition="1061">to be useful for argument labelling and detection in articles on the topic of healthcare. Additionally, discourse markers and sentiment features have been found to assist with disagreement detection in collections of news documents on a particular topic, as well as reviews (Choi et al., 2010; Awadallah et al., 2012; Popescu et al., 2005). In the asynchronous domain, there has been recent work in disagreement detection, especially as it pertains to stance identification. Content based features, including sentiment, duration, and discourse markers have been used for this task (Yin et al., 2012; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). The structure of a conversation has also been used, although these approaches have focused on simple rules for disagreement identification (Murakami and Raymond, 2010), or have assumed that adjacent posts always disagree (Agrawal et al., 2003). More recent work has focused on identifying users’ attitudes towards each other (Hassan et al., 2010), influential users and posts (Nguyen et al., 2014), as well as identifying subgroups of users who share viewpoints (Abu-Jbara et al., 2010). In Slashdot, the h-index of a discussion has been used to rank articles accordi</context>
<context position="15207" citStr="Somasundaran and Wiebe, 2009" startWordPosition="2419" endWordPosition="2422">suggests that subtle disagreement detection is a subjective and difficult task. Further statistics for the developed corpus are given in Table 1. 4 Features for Disagreement Detection The features we use in our experiments combine information from conversational structure, rhetorical relations, sentiment features, n-gram models, Slashdot meta-features, structural features, and lexicon features. 4.1 Rhetorical Relation Graphs Discourse markers have been found to aid in argument and disagreement detection, and for tasks such as stance identification (Abbott et al., 2011; Misra and Walker, 2013; Somasundaran and Wiebe, 2009). We aim to improve over discourse markers by capturing the underlying discourse structure of the conversation in terms of discourse relations. In Rhetorical Structure Theory, Discourse trees are a hierarchical representation of document structure for monologues (Mann and Thompson, 1988). At the lowest level, Elementary Discourse Units (EDUs) are connected by discourse relations (such as ELABORATION and COMPARISON), which in turn form larger discourse units that are also linked by these relations. Computational systems (discourse parsers) have been recently developed to automatically generate </context>
<context position="41280" citStr="Somasundaran and Wiebe (2009)" startWordPosition="6634" endWordPosition="6638">l relations between users. Combining this approach with sentiment features shows significantly improved performance in both accuracy and F-score over a baseline consisting of structural and lexical features as well as referral counts, punctuation, and discourse markers. In building our new crowd-sourced corpus from Slashdot, we have also shown the challenges of detecting subtle disagreement in a dataset that contains a significant number of news-style discussions. In future work, we will improve sentiment features by considering methods to detect opiniontopic pairs in conversation, similar to Somasundaran and Wiebe (2009). Additionally, we will incorporate generalized dependency and POS features (Abbott et al., 2011), which were not used in this analysis due to the very small number of training samples in our dataset. The fragment quotation graph features did not perform as well as we expected, and in future work we would like to investigate this further. Furthermore, we will also explore how to create a discourse tree from the thread structure of a conversation (instead of from its temporal structure), and verify whether this improves the accuracy of the relation graphs, especially when the temporal structure</context>
</contexts>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>Swapna Somasundaran and Janyce Wiebe. 2009. Recognizing stances in online debates. In Proceedings ofACL, pages 226-234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in ideological on-line debates.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL, Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<pages>116124</pages>
<contexts>
<context position="6940" citStr="Somasundaran and Wiebe, 2010" startWordPosition="1062" endWordPosition="1065">lling and detection in articles on the topic of healthcare. Additionally, discourse markers and sentiment features have been found to assist with disagreement detection in collections of news documents on a particular topic, as well as reviews (Choi et al., 2010; Awadallah et al., 2012; Popescu et al., 2005). In the asynchronous domain, there has been recent work in disagreement detection, especially as it pertains to stance identification. Content based features, including sentiment, duration, and discourse markers have been used for this task (Yin et al., 2012; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). The structure of a conversation has also been used, although these approaches have focused on simple rules for disagreement identification (Murakami and Raymond, 2010), or have assumed that adjacent posts always disagree (Agrawal et al., 2003). More recent work has focused on identifying users’ attitudes towards each other (Hassan et al., 2010), influential users and posts (Nguyen et al., 2014), as well as identifying subgroups of users who share viewpoints (Abu-Jbara et al., 2010). In Slashdot, the h-index of a discussion has been used to rank articles according to controversiality, althoug</context>
</contexts>
<marker>Somasundaran, Wiebe, 2010</marker>
<rawString>Swapna Somasundaran and Janyce Wiebe. 2010. Recognizing stances in ideological on-line debates. In Proceedings of NAACL, Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 116124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amanda Stent</author>
<author>James Allen</author>
</authors>
<title>Annotating Argumentation Acts in Spoken Dialog.</title>
<date>2000</date>
<tech>Technical Report.</tech>
<contexts>
<context position="15999" citStr="Stent and Allen, 2000" startWordPosition="2536" endWordPosition="2539">eory, Discourse trees are a hierarchical representation of document structure for monologues (Mann and Thompson, 1988). At the lowest level, Elementary Discourse Units (EDUs) are connected by discourse relations (such as ELABORATION and COMPARISON), which in turn form larger discourse units that are also linked by these relations. Computational systems (discourse parsers) have been recently developed to automatically generate a discourse tree for a given monologue (Joty et al., 2013). Although theoretically the rhetorical relations expected in dialogues are different from those in monologues (Stent and Allen, 2000), no sophisticated computational tools exist yet for detecting these relations reliably. The core idea of this work is that some useful (although noisy) information about the discourse structure of a conversation can be obtained by applying state-of-the-art document level discourse parsing to parts of the conversation. More specifically, posts on a particular topic are concatenated according to their temporal order. This pseudo-monologic document is then fed to a publicly available document level discourse parser (Joty et al., 2013). A discourse tree such as that seen in Figure 1a is output by</context>
</contexts>
<marker>Stent, Allen, 2000</marker>
<rawString>Amanda Stent and James Allen. 2000. Annotating Argumentation Acts in Spoken Dialog. Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Kimberly Voll</author>
<author>Manfred Stede</author>
</authors>
<title>Lexicon-based methods for sentiment analysis.</title>
<date>2011</date>
<journal>Journal of Computational Linguistics,</journal>
<pages>37--2</pages>
<contexts>
<context position="20430" citStr="Taboada et al., 2011" startWordPosition="3266" endWordPosition="3269">rs in each fifth of the posts in a topic (e.g. the count of all 17 discourse markers in the first fifth of every post in the topic). Altogether, there are 5 aggregated discourse marker features in addition to the 17 frequency count features. 4.3 Sentiment Features Sentiment polarity features have been shown to be useful in argument detection (Mishne and Glance, 2006). For this work, we use four sentiment scoring categories: the variance, average score, number of negative sentences, and controversiality score (Carenini and Cheung, 2008) of sentences in a post. These are determined using SoCAL (Taboada et al., 2011), which gives each sentence a polarity score and has been shown to work well on user-generated content. Overall, we have two main classes of sentiment features. The first type splits all the posts in a topic into 4 sections corresponding to the sentences in each quarter of the post. The sentiment scores described above are then applied to each section of the posts (e.g. one feature is the number of negative sentences in the first quarter of each post). As a separate feature, we also include the scores on just the first sentence, as Mishne and Glance (2006) previously found this to be beneficia</context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, Manfred Stede. 2011. Lexicon-based methods for sentiment analysis. Journal of Computational Linguistics, 37(2):267-307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from Congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>327--335</pages>
<contexts>
<context position="24646" citStr="Thomas et al., 2006" startWordPosition="3972" endWordPosition="3975">a total of 5×5 = 25 features. 4.8 Referrals Referrals have been found to help with the detection of disagreement (Mishne and Glance, 2006), especially with respect to other authors. Since there are no direct referrals to previous authors in this corpus, references to variations of “you”, “they”, “us”, “I”, and “he/she” in each fifth of the post are included instead, for a total of 5×5 = 25 features. 4.9 Meta-Post Features 4.5 N-gram models Slashdot allows users to rank others’ posts with the As noted previously (Somasundaran and Wiebe, equivalent of a “like” button, changing the “score” 2010; Thomas et al., 2006), it is often difficult to of the post (to a maximum of 5). They are also outperform a unigram/bigram model in the task of encouraged to tag posts as either “Interesting”, disagreement and argument detection. In this anal- “Informative”, “Insightful”, “Flamebait”, “Troll”, ysis, because of the very small number of sam- “Off-topic” or “Funny”. Frequency counts of ples, we do not consider dependency or part-of- these tags as a percentage of the total number of speech features, but do make a comparison with comments are included as features, as well as the a filtered unigram/bigram model. In the </context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the vote: Determining support or opposition from Congressional floor-debate transcripts. In Proceedings of EMNLP, pages 327-335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Sibel Yaman</author>
<author>Kristen Precoda</author>
<author>Colleen Richey</author>
<author>Geoffrey Raymond</author>
</authors>
<title>Detection of agreement and disagreement in broadcast conversations.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>374--378</pages>
<contexts>
<context position="5372" citStr="Wang et al., 2011" startWordPosition="819" endWordPosition="822">ifferences between news-style and argument-style dialogue. 2 Related Work In the past decade, there have been a number of computational approaches developed for the task of disagreement and controversy detection, particularly in synchronous conversations such as meetings (Somasundaran et al., 2007; Raaijmakers et al., 2008) and in monologic corpora such as news collections (Awadallah et al., 2012) and reviews (Popescu et al., 2005; Mukherjee and Liu, 2012). In the domain of synchronous conversations, prosodic features such as duration, speech rate and pause have been used for spoken dialogue (Wang et al., 2011; Galley et al., 2004). Galley et al. (2004) found that local features, such as lexical and structural features, as well as global contextual features, were particularly useful for identifying agreement/disagreement in the ICSI meeting corpus. Germesin and Wilson (2009) also showed accuracies of 98% in detecting agreement in the AMI corpus using lexical, subjectivity and dialogue act features. However, they note that their system could not classify disagreement accurately due to the small number of training examples in this category. Somasundaran et al. additionally show that dialogue act feat</context>
</contexts>
<marker>Wang, Yaman, Precoda, Richey, Raymond, 2011</marker>
<rawString>Wen Wang, Sibel Yaman, Kristen Precoda, Colleen Richey, and Geoffrey Raymond. 2011. Detection of agreement and disagreement in broadcast conversations. In Proceedings of ACL, pages 374-378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
<author>Rashmi Prasad</author>
</authors>
<title>Sentenceinitial discourse connectives, discourse structure and semantics.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Formal and Experimental Approaches to Discourse Particles and Modal Adverbs.</booktitle>
<contexts>
<context position="2766" citStr="Webber and Prasad, 2008" startWordPosition="418" endWordPosition="421">d Levinson, 1987). As a starting point, in this paper we focus on the detection of disagreement in casual conversations between users. This requires a generalized approach that can accurately identify disagreement in topics ranging from something as mundane as whether GPS stands for galactic positioning system or global positioning system, to more ideological debates about distrust in science. Motivated by the widespread consensus in both computational and theoretical linguistics on the utility of discourse markers for signalling pragmatic functions such as disagreement and personal opinions (Webber and Prasad, 2008; Abbott et al., 2011; J. E. Fox-Tree, 2010), we introduce a new set of features based on the Discourse Tree (DT) of a conversational text. Discourse Trees were formalized by Mann and Thompson (1988) as part of their Rhetorical Structure Theory (RST) to represent the structure of discourse. Although this theory is for monologic discourse, we propose to treat conversational dialogue as a collection of linked monologues, and subsequently build a relation graph describing both rhetorical connections within user posts, as well as between different users. Features obtained from this graph offer sig</context>
</contexts>
<marker>Webber, Prasad, 2008</marker>
<rawString>Bonnie Webber and Rashmi Prasad. 2008. Sentenceinitial discourse connectives, discourse structure and semantics. In Proceedings of the Workshop on Formal and Experimental Approaches to Discourse Particles and Modal Adverbs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Yin</author>
<author>Paul Thomas</author>
<author>Nalin Narang</author>
<author>Cecile Paris</author>
</authors>
<title>Unifying local and global agreement and disagreement classification in online debates.</title>
<date>2012</date>
<booktitle>In Proceedings of Computational Approaches to Subjectivity and Sentiment Analysis,</booktitle>
<pages>61--69</pages>
<contexts>
<context position="6879" citStr="Yin et al., 2012" startWordPosition="1054" endWordPosition="1057">torical relations to be useful for argument labelling and detection in articles on the topic of healthcare. Additionally, discourse markers and sentiment features have been found to assist with disagreement detection in collections of news documents on a particular topic, as well as reviews (Choi et al., 2010; Awadallah et al., 2012; Popescu et al., 2005). In the asynchronous domain, there has been recent work in disagreement detection, especially as it pertains to stance identification. Content based features, including sentiment, duration, and discourse markers have been used for this task (Yin et al., 2012; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). The structure of a conversation has also been used, although these approaches have focused on simple rules for disagreement identification (Murakami and Raymond, 2010), or have assumed that adjacent posts always disagree (Agrawal et al., 2003). More recent work has focused on identifying users’ attitudes towards each other (Hassan et al., 2010), influential users and posts (Nguyen et al., 2014), as well as identifying subgroups of users who share viewpoints (Abu-Jbara et al., 2010). In Slashdot, the h-index of a discussion has been</context>
<context position="23485" citStr="Yin et al., 2012" startWordPosition="3775" endWordPosition="3778">tection such as the number of connections, total number of fragments, and the average path length between nodes which we hypothesize to be useful. We additionally extract the h-index (Gomez et al., 2008) and average branching ratio per fragment of the topic from the simpler reply-to conversational structure. In total, there are 8 FQG features. 4.6 Structural Features Length features have been well documented in the literature to provide useful information about whether or not arguments exist, especially in online conversations that may be more informative than subjective (Biyani et al., 2014; Yin et al., 2012). In this work, length features include the length of the post in sentences, the average number of words per sentence, the average number of sentences per post, the number of contributing authors, the rate of posting, and the total amount of time of the conversation. This results in a total of 9 features. 4.7 Punctuation Like many other features already described, frequency counts of ‘?’,‘!’,‘”’,‘”, and ‘.’ are found for each fifth of the post (the first fifth, second fifth, etc.). These counts are then aggregated across all posts for a total of 5×5 = 25 features. 4.8 Referrals Referrals have </context>
</contexts>
<marker>Yin, Thomas, Narang, Paris, 2012</marker>
<rawString>Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris. 2012. Unifying local and global agreement and disagreement classification in online debates. In Proceedings of Computational Approaches to Subjectivity and Sentiment Analysis, pages 61-69.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001069">
<title confidence="0.994001">
Explaining the Stars: Weighted Multiple-Instance Learning for
Aspect-Based Sentiment Analysis
</title>
<author confidence="0.536932">
Nikolaos Pappas
</author>
<note confidence="0.933833333333333">
EPFL and Idiap Research Institute
Rue Marconi 19
CH-1920 Martigny, Switzerland
</note>
<email confidence="0.982898">
nikolaos.pappas@idiap.ch
</email>
<author confidence="0.825215">
Andrei Popescu-Belis
</author>
<affiliation confidence="0.823566">
Idiap Research Institute
</affiliation>
<note confidence="0.614161">
Rue Marconi 19
CH-1920 Martigny, Switzerland
</note>
<email confidence="0.992291">
andrei.popescu-belis@idiap.ch
</email>
<sectionHeader confidence="0.997325" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999776739130435">
This paper introduces a model of multiple-
instance learning applied to the predic-
tion of aspect ratings or judgments of
specific properties of an item from user-
contributed texts such as product reviews.
Each variable-length text is represented by
several independent feature vectors; one
word vector per sentence or paragraph.
For learning from texts with known as-
pect ratings, the model performs multiple-
instance regression (MIR) and assigns im-
portance weights to each of the sentences
or paragraphs of a text, uncovering their
contribution to the aspect ratings. Next,
the model is used to predict aspect ratings
in previously unseen texts, demonstrating
interpretability and explanatory power for
its predictions. We evaluate the model on
seven multi-aspect sentiment analysis data
sets, improving over four MIR baselines
and two strong bag-of-words linear mod-
els, namely SVR and Lasso, by more than
10% relative in terms of MSE.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999609454545455">
Sentiment analysis of texts provides a coarse-
grained view of their overall attitude towards an
item, either positive or negative. The recent abun-
dance of user texts accompanied by real-valued la-
bels e.g. on a 5-star scale has contributed to the de-
velopment of automatic sentiment analysis of re-
views of items such as movies, books, music or
other products, with applications in social com-
puting, user modeling, and recommender systems.
The overall sentiment of a text towards an item
often results from the ratings of several specific
aspects of the item. For instance, the author of
a review might have a rather positive sentiment
about a movie, having particularly liked the plot
and the music, but not too much the actors. De-
termining the ratings of each aspect automatically
is a challenging task, which may seem to require
the engineering of a large number of features de-
signed to capture each aspect. Our goal is to put
forward a new feature-agnostic solution for ana-
lyzing aspect-related ratings expressed in a text,
thus aiming for a finer-grained, deeper analysis of
text meaning than overall sentiment analysis.
Current state-of-the-art approaches to sentiment
analysis and aspect-based sentiment analysis, at-
tempt to go beyond word-level features either by
using higher-level linguistic features such as POS
tagging, parsing, and knowledge infusion, or by
learning features that capture syntactic and seman-
tic dependencies between words. Once an appro-
priate feature space is found, the ratings are typi-
cally modeled using a linear model, such as Sup-
port Vector Regression (SVR) with E2 norm for
regularization or Lasso Regression with E1 norm.
By treating a text globally, these models ignore the
fact that the sentences of a text have diverse con-
tributions to the overall sentiment or to the attitude
towards a specific aspect of an item.
In this paper, we propose a new learning model
which answers the following question: “To what
extent does each part of a text contribute to the
prediction of its overall sentiment or the rating of
a particular aspect?” The model uses multiple-
instance regression (MIR), based on the assump-
tion that not all the parts of a text have the same
contribution to the prediction of the rating. Specif-
ically, a text is seen as a bag of sentences (in-
stances), each of them modeled as a word vector.
The overall challenge is to learn which sentences
refer to a given aspect, and how they contribute to
the text’s attitude towards it, but the model applies
to overall sentiment analysis as well. For instance,
Figure 1 displays a positive global comment on a
TED talk and the weights assigned to two of its
sentences by MIR.
</bodyText>
<page confidence="0.986813">
455
</page>
<note confidence="0.9741295">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 455–466,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.7779024">
Figure 1: Analysis of a comment (bag of sentences
{si, ..., sj}) annotated by humans with the maxi-
mal positive sentiment score (5 stars). The weights
assigned by MIR reveal that sl has the greatest rel-
evance to the overall sentiment.
</figureCaption>
<bodyText confidence="0.999982485714286">
Using regularized least squares, we formulate
an optimization objective to jointly assign instance
weights and regression hyperplane weights. Then,
an instance relevance estimation method is used
to predict aspect ratings, or global ones, in previ-
ously unseen texts. The parameters of the model
are learned using an alternating optimization pro-
cedure inspired by Wagstaff and Lane (2007). Our
model requires only text with ratings for training,
with no particular assumption on the word fea-
tures to be extracted, and provides interpretable
explanations of the predicted ratings through the
relevance weights assigned to sentences. We also
show that the model has reasonable computational
demands. The model is evaluated on aspect and
sentiment rating prediction over seven datasets:
five of them contain reviews with aspect labels
about beers, audiobooks and toys (McAuley et al.,
2012), and two contain TED talks with emotion la-
bels, and comments on them with sentiment labels
(Pappas and Popescu-Belis, 2013). Our model
outperforms previous MIR models and two strong
linear models for rating prediction, namely SVR
and Lasso by more than 10% relative in terms of
MSE. The improvement is observed even when the
sophistication of the feature space increases.
The paper is organized as follows. Section 2
shows how our model innovates with respect to
previous work on MIR and rating prediction. Sec-
tion 3 formulates the problem while Section 4 de-
scribes previous MIR models. Section 5 presents
our MIR model and learning procedure. Section 6
presents the datasets and evaluation methods. Sec-
tion 7 reports our results on rating prediction tasks,
and provides examples of rating explanation.
</bodyText>
<sectionHeader confidence="0.999891" genericHeader="introduction">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.991633">
2.1 Multiple-Instance Regression
</subsectionHeader>
<bodyText confidence="0.999828473684211">
Multiple-instance regression (MIR) belongs to the
class of multiple-instance learning (MIL) prob-
lems for real-valued output, and it is a variant
of multiple regression where each data point may
be described by more than one vectors of values.
Many MIL studies focused on classification (An-
drews et al., 2003; Bunescu and Mooney, 2007;
Settles et al., 2008; Foulds and Frank, 2010; Wang
et al., 2011) while fewer focused on regression
(Ray and Page, 2001; Davis and others, 2007;
Wagstaff et al., 2008; Wagstaff and Lane, 2007).
Related to document analysis, several MIR stud-
ies have focused on news categorization (Zhang
and Zhou, 2008; Zhou et al., 2009) or web-index
recommendation (Zhou et al., 2005) but, to our
knowledge, no study has attempted to use MIR for
aspect rating prediction or sentiment analysis with
real-valued labels.
MIR was firstly introduced by Ray et al. (2001),
proposing an EM algorithm which assumes that
one primary instance per bag is responsible for
its label. Wagstaff and Lane (2007) proposed to
simultaneously learn a regression model and es-
timate instance weights per bag for crop yield
modeling (not applicable to prediction). A simi-
lar method which learns the internal structure of
bags using clustering was proposed by Wagstaff et
al. (2008) for crop yield prediction, and we will
use it for comparison in the present study. Later,
the method was adapted to map bags into a single-
instance feature space by Zhang et al. (2009).
Wang et al. (2008) assumed that each bag is gener-
ated by random noise around a primary instance,
while Wang et al. (2012) represented bag labels
with a probabilistic mixture model. Foulds et
al. (2010) concluded that various assumptions are
differently suited to different tasks, and should be
stated clearly when describing an MIR model.
</bodyText>
<subsectionHeader confidence="0.999782">
2.2 Rating Prediction from Text
</subsectionHeader>
<bodyText confidence="0.999900666666667">
Sentiment analysis aims at analyzing the polar-
ity of a given text, either with classification (for
discrete labels) or regression (for real-valued la-
bels). Early studies introduced machine learning
techniques for sentiment classification, e.g. Pang
et al. (2002), including unsupervised techniques
based on the notion of semantic orientation of
phrases, e.g. Turney et al. (2002). Other studies
focused on subjectivity detection, i.e. whether a
</bodyText>
<page confidence="0.998569">
456
</page>
<bodyText confidence="0.999830228571429">
text span expresses opinions or not (Wiebe et al.,
2004). Rating inference was defined by Pang et
al. (2005) as multi-class classification or regres-
sion with respect to rating scales. Pang and Lee
(2008) discusses the large range of features engi-
neered for this task, though several recent stud-
ies focus on feature learning (Maas et al., 2011;
Socher et al., 2011), including the use of a deep
neural network (Socher et al., 2013). In contrast,
we do not make any assumption about the nature
or dimensionality of the feature space.
The fine-grained analysis of opinions regarding
specific aspects or features of items is known as
multi-aspect sentiment analysis. This task usu-
ally requires aspect-related text segmentation, fol-
lowed by prediction or summarization (Hu and
Liu, 2004; Zhuang et al., 2006). Most attempts to
perform this task have engineered various feature
sets, augmenting words with topic or content mod-
els (Mei et al., 2007; Titov and McDonald, 2008;
Sauper et al., 2010; Lu et al., 2011), or with lin-
guistic features (Pang and Lee, 2005; Baccianella
et al., 2009; Qu et al., 2010; Zhu et al., 2012).
Other studies have advocated joint modeling of
multiple aspects (Snyder and Barzilay, 2007) or
multiple reviews for the same product (Li et al.,
2011). McAuley et al. (2012) introduced new cor-
pora of multi-aspect reviews, which we also partly
use here, and proposed models for aspect detec-
tion, sentiment summarization and rating predic-
tion. Lastly, joint aspect identification and senti-
ment classification have been used for aggregating
product review snippets by Sauper at al. (2013).
None of the above studies considers the multiple-
instance property of text in their modeling.
</bodyText>
<sectionHeader confidence="0.99531" genericHeader="method">
3 MIR Definition
</sectionHeader>
<bodyText confidence="0.995961722222222">
Let us consider a set B of m bags with
numerical labels Y as input data D =
{({b1j}dn1, y1), ..., ({bmj}dn,,, ym)}, where bij E
Rd (for 1 G j G ni) and yi E R. Each bag
Bi consists of ni data points (called ‘instances’),
hence it is a matrix of ni d-dimensional vectors,
e.g. word vectors. The challenge is to infer the
label of the bag given a variable number of in-
stances ni. This requires finding a set of bag rep-
resentations X = {x1, ... , xm} of size m where
xi E Rd, from which the class labels can be com-
puted. The goal is then to find a mapping from
this representation, noted Φ : Rd → R, which is
able to predict the label of a given bag. Ideally,
assuming that X is the best bag representation for
our task, we look for the optimal regression hyper-
plane Φ which minimizes a loss function L plus a
regularization term Ω as follows:
</bodyText>
<equation confidence="0.9890562">
�
Φ = arg min L(Y, X, Φ)
� �� �
Φ
loss
</equation>
<bodyText confidence="0.999781333333333">
Since the best set of representations X for a task is
generally unknown, one has to make assumptions
to define it or compute it jointly with the regres-
sion hyperplane Φ. Thus, the main difficulty lies
in finding a good assumption for X, as we will
now discuss.
</bodyText>
<sectionHeader confidence="0.998761" genericHeader="method">
4 Previous MIR Assumptions
</sectionHeader>
<bodyText confidence="0.9997585">
We describe here three assumptions frequently
made in past MIR studies, to which we will later
compare our model: aggregating all instances,
keeping them as separate examples, or choosing
the most representative one (Wang et al., 2012).
For each assumption, we will experiment with
two state-of-the-art regression models (noted ab-
stractly as f), namely SVR (Drucker et al., 1996)
and Lasso (Tibshirani, 1996) with respectively the
E2 and E1 norms for regularization.
The Aggregated algorithm assumes that each
bag is represented as a single d-dimensional vec-
tor, which is the average of its instances (hence
xi E Rd). Then, a regression model f is trained
on pairs of vectors and class labels, Dagg =
{(xi, yi)  |i = 1, ... , m}, and the predicted class
of an unlabeled bag Bi = {bij  |j = 1, ... , ni} is
computed as follows:
</bodyText>
<equation confidence="0.997492">
ˆy(Bi) = f(mean({bij  |j = 1, ... , ni})) (2)
</equation>
<bodyText confidence="0.9991236">
In fact, a simple sum can also be used instead of
the mean, and we observed in practice that with an
appropriate regularization there is no difference on
the prediction performance between these options.
This baseline corresponds to the typical approach
for text regression tasks, where each text sample is
represented by a single vector in the feature space
(e.g. BOW with counts or TF-IDF weights).
The Instance algorithm considers each of the in-
stances in a bag as separate examples, by labeling
each of them with the bag’s label. A regression
model f is learned over the training set made of
all vectors of all bags, Dins = {(bij,yi)  |j =
1, . . . ,ni; i = 1, ... , m}, assuming that there are
m labeled bags. To label a new bag Bi, given that
</bodyText>
<equation confidence="0.998593">
+ Ω(Φ)
� Y �
reg.
) (1)
</equation>
<page confidence="0.97898">
457
</page>
<bodyText confidence="0.9924865">
there is no representation xi, the method simply
averages the predicted labels of its instances:
</bodyText>
<equation confidence="0.998806">
ˆy(Bi) = mean({f(bij)  |j = 1, ... , ni}) (3)
</equation>
<bodyText confidence="0.999984736842105">
Instead of the average, the median value can also
be used, which is more appropriate when the bags
contain outlying instances.
The Prime algorithm assumes that a single in-
stance in each bag is responsible for its label (Ray
and Page, 2001). This instance is called the pri-
mary or prime one. The method is similar to the
previous one, except that only one instance per bag
is used as training data: Dpri = {(bpi , yi)  |i =
1, ... , m}, where bpi is the prime instance of the
ith bag Bi and m is the number of bags. The
prime instances are discovered through an itera-
tive algorithm which refines the regression model
f. Given an initial model f, in each iteration the
algorithm selects from each bag a prime candidate
which is the instance with the lowest prediction er-
ror. Then, a new model is trained over the selected
prime candidates, until convergence. For a new
bag, the target class is computed as in Eq. 3.
</bodyText>
<sectionHeader confidence="0.999156" genericHeader="method">
5 Proposed MIR Model
</sectionHeader>
<bodyText confidence="0.9999974">
We propose a new MIR model which assigns in-
dividual relevance values (weights) to each in-
stance of a bag, thus making fewer simplifying
assumptions than previous models. We extend
instance-relevance algorithms such as (Wagstaff
and Lane, 2007) by supporting high-dimensional
feature spaces, as required for text regression, and
by predicting both the class label and the con-
tent structure of previously unseen (hence unla-
beled) bags. The former is achieved by minimiz-
ing a regularized least squares loss (RLS) instead
of solving normal equations, which is prohibitive
in large spaces. The latter represents a significant
improvement over Aggregated and Instance algo-
rithms, which are unable to pinpoint the most rel-
evant instances with respect to the label of each
bag, being thus applicable only to bag label pre-
diction. Similarly, Prime only identifies the prime
instance when the bag is already labeled. Instead,
our model learns an optimal method to aggregate
instances, rather than a pre-defined one, and al-
lows more degrees of freedom in the regression
model than previous ones. Moreover, the weight
of an instance is interpreted as its relevance both
in training and prediction.
</bodyText>
<subsectionHeader confidence="0.991374">
5.1 Instance Relevance Assumption
</subsectionHeader>
<bodyText confidence="0.999971888888889">
Each bag defines a bounded region of a hyper-
plane orthogonal to the y-axis (the envelope of all
its points). The goal is to find a regression hy-
perplane that passes through each bag Bi and to
predict its label by using at least one data point
xi within that bounded region. Thus, the point xi
is a convex combination of the points in the bag,
in other words Bi is represented by the weighted
average of its instances bij:
</bodyText>
<equation confidence="0.898698666666667">
ni ni
xi = Oijbij, Oij ≥ 0 and Oij = 1 (4)
j=1 j=1
</equation>
<bodyText confidence="0.999978">
where Oij is the weight of the jth instance of the
ith bag. Each weight Oij indicates the relevance
of an instance j to the prediction of the class yi of
the ith bag. The constraint forces xi to fall within
the bounded region of the points in bag i and guar-
antees that the ith bag will influence the regressor.
</bodyText>
<subsectionHeader confidence="0.99982">
5.2 Modeling Bag Structure and Labels
</subsectionHeader>
<bodyText confidence="0.98914825">
Let us consider a set of m bags, where each bag Bi
is represented by its ni d-dimensional instances,
i.e. Bi = {bij}dni along with the set of target class
labels for each bag, Y = {yi}N, yi ∈ R. The
representation set of all Bi in the feature space,
X = {x1, ... , xm}, xi ∈ Rd, is obtained using
the ni instance weights associated to each bag Bi,
Oi = {Oij}ni, Oij ∈ [0, 1] which are initially
unknown. Thus, we look for a linear regression
model f that is able to model the target values us-
ing the regression coefficients b ∈ Rd, where X
and Y are respectively the sets of training bags and
their labels: Y = f(X) = bTX. We define a loss
function according to the least squares objective
dependent on X, Y , b and the set of weight vec-
tors Ψ = {O1, ... , Om} using Eq. 4 as follows:
</bodyText>
<equation confidence="0.9698045">
/, 2
4&apos;ijbij~~
N ~ ~2
i=1 yi − bT (BiOi) (5)
</equation>
<bodyText confidence="0.990454">
Using the above loss function, accounting for the
constraints of our assumption in Eq. 4 and assum-
ing `2-norm for regularization with E1 and E2 terms
for each Oi ∈ Ψ and b respectively, we obtain the
</bodyText>
<equation confidence="0.999771333333333">
L(Y,X,Ψ,b) = ||Y − bTX||22
(4) = N ~yi − bT ( ni
i=1 j=1
</equation>
<page confidence="0.952379">
458
</page>
<figure confidence="0.946662846153846">
(7)
XN
i=1
(ψij − OT bij)2
Xni
j=1
 |{z }
f3 loss function
+~3||O||2
 |{z }
f3 reg.
arg min
O
</figure>
<bodyText confidence="0.651512">
following least squares objective from Eq. 1:
</bodyText>
<equation confidence="0.7451085">
� ~2
where Δ2 i = yi − ΦT(Biψi) , (6)
subject to ψij &gt;_ 0 Vi, j and Pni
j=1 ψij = 1 Vi.
</equation>
<bodyText confidence="0.999886045454545">
The selection of the `2-norm was based on prelim-
inary results showing that it outperforms `1-norm.
Other combinations of p-norm regularization can
be explored for f1 and f2, e.g. to learn sparser in-
stance weights and denser regression coefficients
or vice versa.
The above objective is non-convex and difficult
to optimize because the minimization is with re-
spect to all ψ1, ... , ψm and Φ at the same time. As
indicated in Eq. 6 above, we will note f1 a model
that is learned from the minimization only with re-
spect to ψ1, ... , ψm and f2 a model obtained from
the minimization with respect to Φ only. In Eq. 6,
we can observe that if one of the two is known or
held fixed, then the other one is convex and can be
learned with the well-known least squares solving
techniques. In Section 5.3, we will describe an al-
gorithm that is able to exploit this observation.
Having computed ψ1, ... , ψm and Φ, we could
predict a label for an unlabeled bag using Eq. 3,
but would not be able to compute the weights
of the instances. Moreover, information that has
been learned about the instances during the train-
ing phase would not be used during prediction.
For these reasons, we introduce a third regression
model f3 with regression coefficients O E Rd as-
suming a `2-norm for the regularization with E3
term, which is trained on the relevance weights
obtained from the Eq. 6, Dw = {(bij, ψij)  |i =
1, ..., m; j = 1, ..., ni1. The optimization objec-
tive for the f3 model is the following:
This minimization can be easily performed with
the well-known least squares solving techniques.
The learned model is able to estimate the weights
of the instances of an unlabeled bag during pre-
diction time as: ˆψi = f3(Bi) = ΩTBi. The ˆψi
weights are estimations which are influenced by
the relevance weights learned in our minimization
objective of Eq. 6 but they are not constrained at
prediction time. To obtain interpretable weights,
we can convert the estimated scores to the [0, 1]
interval as follows: ˆψi = ˆψi/sum(ˆψi). Finally,
the prediction of the label for the ith bag using the
estimated instance weights ˆψi is done as follows:
</bodyText>
<equation confidence="0.995486">
yˆ = f2(Bi) = ΦT Biˆψi (8)
</equation>
<subsectionHeader confidence="0.999383">
5.3 Learning with Alternating Projections
</subsectionHeader>
<bodyText confidence="0.999531454545455">
Algorithm 1 solves the non-convex optimization
problem of Eq. 6 by using a powerful class of
methods for finding the intersection of convex sets,
namely alternating projections (AP). The prob-
lem is firstly divided into two convex problems,
namely f1 loss function and f2 loss function,
which are then solved in an alternating fashion.
Like EM algorithms, AP algorithms do not have
general guarantees on their convergence rate, al-
though, in practice, we found it acceptable at gen-
erally fewer than 20 iterations.
</bodyText>
<construct confidence="0.731626">
Algorithm 1 APWeights(B, Y , E1, e2, E3)
</construct>
<listItem confidence="0.903448">
1: Initialize(ψ1,..., ψN, Φ, X)
2: while not converged do
3: for Bi in B do
4: ψi = cRLS(ΦT Bi, Yi, e1) # f1 model
5: xi = BiψT i
6: end for
7: Φ = RLS(X, Y, E2) # f2 model
8: end while
9: Ω = RLS({bij∀i, j}, {ψij∀i, j}, E3) # f3 model
</listItem>
<figureCaption confidence="0.994119">
Figure 2: Visual representation for the training and
testing procedure of Algorithm 1.
</figureCaption>
<bodyText confidence="0.9999543">
The algorithm takes as input the bags Bi, their
target class labels Y and the regularization terms
E1, E2, E3 and proceeds as follows. First, under a
fixed regression model (f2), it proceeds with f1
to the optimal assignment of weights to the in-
stances of each bag (projection of Φ vectors on
the ψi space which is a ni-simplex) and com-
putes its new representation set X. Second, given
the fixed instance weights, it trains a new regres-
sion model (f2) using X (projection back to the Φ
</bodyText>
<figure confidence="0.977549875">
arg min
ψ1,...,ψm,Φ
Xm�
Δ2
|{z} i
i=1
f1 loss
+�1||ψi||
 |{z }
f1 reg.
�
+�2||Φ||2
 |{z }
f2 reg.
 |{z }
f2 loss
</figure>
<page confidence="0.984354">
459
</page>
<table confidence="0.999633416666667">
Bags Instances Dimension Aspect ratings
Dataset Type Count Type Count Count Classes
BeerAdvocate review 1,200 sentence 12,189 19,418 feel, look, smell, taste, overall
RateBeer (ES) 1,200 3,269 2,120 appearance, aroma, overall, palate, taste
RateBeer (FR) 1,200 4,472 903 appearance, aroma, overall, palate, taste
Audiobooks 1,200 4,886 3,971 performance, story, overall
Toys &amp; Games 1,200 6,463 31,984 educational, durability, fun, overall
TED comments comment 1,200 sentence 3,814 957 sentiment (polarity)
TED talks comments 1,200 comment 11,993 5,000 unconvincing, fascinating, persuasive,
per talk ingenious, longwinded, funny, inspir-
ing, jaw-dropping, courageous, beauti-
ful, confusing, obnoxious
</table>
<tableCaption confidence="0.999851">
Table 1: Description of the seven datasets used for aspect, sentiment and emotion rating prediction.
</tableCaption>
<bodyText confidence="0.999887">
space). This procedure repeats until convergence,
i.e. when there is no more decrease on the training
error, or until a maximum number of iterations has
been reached. The regression model f3 is trained
on the weights learned from the previous steps.
</bodyText>
<subsectionHeader confidence="0.987466">
5.4 Complexity Analysis
</subsectionHeader>
<bodyText confidence="0.999907333333333">
The overall time complexity T of Algorithm 1 in
terms of the input variables, noted h = {m, ˆn, d},
with m being the number of bags, nˆ the average
size of the bags, and d the dimensionality of the
feature space (here, the size of word vectors), is
derived as follows:
</bodyText>
<equation confidence="0.999937666666667">
T (h) = Tap(h) + Tf3(h)
= O(m(ˆn2 + d2)) + O(mˆnd2)
= O(m(ˆn2 + d2 + ˆnd2)), (9)
</equation>
<bodyText confidence="0.99997725">
where Tap and Tf3 are respectively the time com-
plexity of the AP procedure and of training the f3
model. Eq. 9 shows that when nˆ « m, the model
complexity is linear with the input bags m and al-
ways quadratic with the number of features d.
Previous works on relevance assignment for
MIR have prohibitive complexity for high-
dimensional feature spaces or numerous bags and
hence they are not most appropriate for text regres-
sion tasks. Wagstaff and Lane (2007) have cubic
time complexity with the average bag size nˆ and
number of features d; Zhou et al. (2009) use ker-
nels, thus their complexity is quadratic with the
number of bags m; and Wang et al. (2011) have
cubic time wrt. d. Our formulation is thus com-
petitive in terms of complexity.
</bodyText>
<sectionHeader confidence="0.974376" genericHeader="method">
6 Data, Protocol and Metrics
</sectionHeader>
<subsectionHeader confidence="0.992303">
6.1 Aspect Rating Datasets
</subsectionHeader>
<bodyText confidence="0.999979888888889">
We use seven datasets summarized in Table 1.
Five publicly available datasets were built for as-
pect prediction by McAuley et al. (2012) – Beer-
Advocate, Ratebeer (ES), RateBeer (FR), Audio-
books and Toys &amp; Games – and have aspect rat-
ings assigned by their creators on the respective
websites. On the set of comments on TED talks
from Pappas and Popescu-Belis (2013), we aim
to predict two things: talk-level emotion dimen-
sions assigned by viewers through voting, and
comment polarity scores assigned by crowdsourc-
ing. The distributions of aspect ratings per dataset
are shown in Figure 3. Five datasets are in En-
glish, one in Spanish (Ratebeer) and one in French
(RateBeer), so our results will also demonstrate
the language-independence of our method.
From every dataset we kept 1,200 texts as bags
of sentences, but we also used three full-size
datasets, namely Ratebeer ES (1,259 labeled re-
views), Ratebeer FR (17,998) and Audiobooks
(10,989). The features for each of them are word
vectors with binary attributes signaling word pres-
ence or absence, in a traditional bag-of-words
model (BOW). The word vectors are provided
with the first five datasets and we generated them
for the latter two, after lowercasing and stopword
removal. Moreover, for TED comments, we com-
puted TF-IDF scores using the same dimension-
ality as with BOW to experiment with a different
feature space. The target class labels were nor-
malized by the maximum rating in their scale, ex-
cept for TED talks where the votes were normal-
ized by the maximum number of votes over all the
emotion classes for each talk, and two emotions,
‘informative’ and ‘ok’, were excluded as they are
neutral ones.
</bodyText>
<subsectionHeader confidence="0.995212">
6.2 Evaluation Protocol
</subsectionHeader>
<bodyText confidence="0.998817">
We compare the proposed model, noted AP-
Weights, with four baseline ones – Aggre-
gated, Instance, Prime (Section 4) and Clus-
</bodyText>
<page confidence="0.99934">
460
</page>
<figureCaption confidence="0.999954">
Figure 3: Distributions of rating values per aspect rating class for the seven datasets.
</figureCaption>
<bodyText confidence="0.999970521739131">
tering (from github.com/garydoranjr/
mcr), which is an instance relevance method pro-
posed by Wagstaff et al. (2008) for aspect rating
prediction. First, for each aspect class, we opti-
mize all methods on a development set of 25%
of the data (300 randomly selected bags). Then,
we perform 5-fold cross-validation for every as-
pect on each entire data set and report the average
error scores using the optimal hyper-parameters
per method. In addition, we report for compar-
ison the scores of AverageRating, which always
predicts the average rating over the training set.
We report standard error metrics for regression,
namely the Mean Absolute Error (MAE) and the
Mean Squared Error (MSE). The former measures
the average magnitude of errors in a set of predic-
tions while the latter measures the average of their
squares, which are defined over the test set of bags
Bi respectively as MAE = (I:ki=1 |f(Bi)−yi|)/k
and MSE = (Ek i=1(f(Bi) − yi)2)/k. The cross-
validation scores are obtained by averaging the
MAE and MSE scores on each fold.
To find the optimal hyper-parameters for each
model, we perform 3-fold cross-validation on the
development set using exhaustive grid-search over
a fine-grained range of possible values and se-
lect the ones that perform best in terms of MAE.
The hyper-parameters to be optimized for the
baselines (except AverageRating) are the regular-
ization terms λ2, λ1 of their possible regression
model f, namely SVR which uses the `2 norm
and Lasso which uses the `1 norm. As for AP-
Weights, it relies on three regularization terms,
namely E1, E2, E3 of the `2-norm for f1, f2 and
f3 regression models. Lastly, for the Clustering
baseline, we use the f2 regression model, which
relies on E2 and the number of clusters k, opti-
mized over {5,..., 50} with step 5, for its cluster-
ing algorithm, here k-Means. All the regulariza-
tion terms are optimized over the same range of
possible values, noted a · 10b with a ∈ {1, ... , 9}
and b ∈ {−4,..., +4}, hence 81 values per term.
For the regression models and evaluation proto-
col, we use the scikit-learn machine learning li-
brary (Pedregosa et al., 2012). Our code and data
are available in the first author’s website.
</bodyText>
<sectionHeader confidence="0.99219" genericHeader="evaluation">
7 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.997736">
7.1 Aspect Rating Prediction
</subsectionHeader>
<bodyText confidence="0.995951958333333">
The results for aspect rating prediction are given
in Table 2. The proposed APWeights method
outperforms Aggregated (`2) and Aggregated (`1)
i.e. SVR and Lasso along with all other baselines
on each case. The SVR baseline has on average
11% lower performance than APWeights in terms
of MSE and about 6% in terms of MAE. Simi-
larly, the Lasso baseline has on average 13% lower
MSE and 8% MAE than APWeights. As shown
in Figure 4, APWeights also outperforms them for
each aspect in the five review datasets. The In-
stance method with `1 performed well on BeerAd-
vocate and Toys &amp; Games (for MSE), and with `2
performed well on Ratebeer (ES), RateBeer (FR)
and Toys &amp; Games (for MAE). Therefore, the
instance-as-example assumption is quite appropri-
ate for this task, however both options score be-
low APWeights – by about 5% MAE, and 8%/9%
MSE, respectively. The Prime method with `1 per-
formed well only on the BeerAdvocate dataset and
Prime with `2 only on the Toys &amp; Games dataset,
always with lower scores than APWeights, namely
about 9% MAE for both and 15%/18% MSE re-
spectively. This suggests that the primary-instance
</bodyText>
<page confidence="0.998606">
461
</page>
<table confidence="0.999222066666667">
REVIEW LABELS
BeerAdvocate RateBeer (ES) RateBeer (FR) Audiobooks Toys &amp; Games
Model \ Error MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE
AverageRating 14.20 3.32 16.59 4.31 12.67 2.69 21.07 6.75 20.96 6.75
Aggregated (`1) 13.62 3.13 15.94 4.02 12.21 2.58 20.10 6.14 20.15 6.33
Aggregated (`2) 14.58 3.68 14.47 3.41 12.32 2.70 19.08 5.99 18.99 5.93
Instance (`1) 12.67 2.89 14.91 3.54 11.89 2.48 20.13 6.17 20.33 6.34
Instance (`2) 13.74 3.28 14.40 3.39 11.82 2.40 19.26 6.04 19.70 6.59
Prime (`1) 12.90 2.97 15.78 3.97 12.70 2.76 20.65 6.46 21.09 6.79
Prime (`2) 14.60 3.64 15.05 3.68 12.92 2.98 20.12 6.59 20.11 6.92
Clustering (`2) 13.95 3.26 15.06 3.64 12.23 2.60 20.50 6.48 20.59 6.52
APWeights (`2) 12.24 2.66 14.18 3.28 11.37 2.27 18.89 5.71 18.50 5.57
APW vs. SVR (%) +16.0 +27.7 +2.0 +3.8 +7.6 +15.6 +1.0 +4.5 +2.6 +6.0
APW vs. Lasso (%) +10.1 +15.1 +11.0 +18.4 +6.8 +11.8 +6.0 +6.9 +8.1 +11.9
APW vs. 2nd best (%) +3.3 +7.8 +1.5 +3.3 +3.7 +4.9 +1.0 +4.5 +2.6 +6.0
</table>
<tableCaption confidence="0.994576">
Table 2: Performance of aspect rating prediction (the lower the better) in terms of MAE and MSE (× 100)
</tableCaption>
<bodyText confidence="0.994585621621622">
with 5-fold cross-validation. All scores are averaged over all aspects in each dataset. The scores of the
best method are in bold and the second best ones are underlined. Significant improvements (paired t-test,
p &lt; 0.05) are in italics. Fig. 4 shows MSE scores per aspect for three methods on five datasets.
assumption is not the most appropriate for this
task. Lastly, even though Clustering is an instance
relevance method, it has similar scores to Prime,
presumably because the relevances are assigned
according to the computed clusters and they are
not directly influenced by the task’s objective.
To compare with the state-of-the-art results ob-
tained by McAuley et al. (2012), we experimented
with three of their full-size datasets. Splitting each
dataset in half for training vs. testing, and using
the optimal settings from our experiments above,
we measured the average MSE over all aspects.
APWeights improved over Lasso by 10%, 26%
and 17% MSE respectively on each dataset – the
absolute MSE scores are .038 for Lasso vs. .034
for APWeights on Ratebeer SP; .023 vs. .017 on
Ratebeer FR; .063 vs. .052 on Audiobooks. Sim-
ilarly, when compared to the best SVM baseline
provided by the McAuley et al., our method im-
proved by 32%, 43% and 35% respectively on
each dataset, though it did not use their rating
model. Moreover, the best model proposed by
McAuley et al., which uses a joint rating model
and an aspect-specific text segmenter trained on
hand-labeled data, reaches MSE scores of .03,
.02 and .03, which is comparable to our model
that does not use these features (.034, .017, .052),
though it could benefit from them in the future.
Lastly, as mentioned by the same authors, predic-
tors which use segmented text, for example with
topic models as in (Lu et al., 2011), do not neces-
sarly outperform SVR baselines; instead they have
marginal or even no improvements, therefore, we
did not further experiment with them. Interes-
</bodyText>
<table confidence="0.999532066666667">
SENT. LABELS EMO. LABELS
TED comm. TED talks
Model \ Error MAE MSE MAE MSE
AverageRating 19.47 5.05 17.86 6.06
Aggregated (`1) 17.08 4.17 15.98 5.03
Aggregated (`2) 16.88 4.47 15.24 4.97
Instance (`1) 17.69 4.37 16.48 5.30
Instance (`2) 16.93 4.24 16.10 5.57
Prime (`1) 17.39 4.37 15.98 5.78
Prime (`2) 18.03 4.91 16.74 5.94
Clustering (`2) 17.64 4.34 17.71 6.02
APWeights (`2) 15.91 3.95 15.02 4.89
APW vs SVR (%) +5.7 +11.5 +1.5 +1.6
APW vs Lasso (%) +6.8 +5.3 +6.0 +2.9
APW vs 2nd (%) +5.7 +5.3 +1.5 +1.6
</table>
<tableCaption confidence="0.990648">
Table 3: MAE and MSE (× 100) on sentiment
</tableCaption>
<bodyText confidence="0.988699333333333">
and emotion prediction with 5-fold c.-v. Scores
on TED talks are averaged over the 12 emotions.
The scores of the best method are in bold and the
second best ones are underlined. Significant im-
provements (paired t-test, p &lt; 0.05) are in italics.
tignly, multiple-instance learning algorithms un-
der several assumptions go beyond SVR baselines
with BOW and even more sophisticated features
such as TF-IDF (see below).
</bodyText>
<subsectionHeader confidence="0.999255">
7.2 Sentiment and Emotion Prediction
</subsectionHeader>
<bodyText confidence="0.99971975">
Our method is also competitive for sentiment pre-
diction over comments on TED talks, as well as
for talk-level emotion prediction with 12 dimen-
sions from subsets of 10 comments on each talk
(see Table 3). APWeights outperforms SVR and
Lasso, as well as all other methods for each task.
For sentiment prediction, SVR is outperformed by
11% MSE and Lasso by 5%. For emotion pre-
</bodyText>
<page confidence="0.998751">
462
</page>
<figureCaption confidence="0.999397">
Figure 4: MSE scores of SVR, Lasso and APWeights for each aspect over the five review datasets.
</figureCaption>
<bodyText confidence="0.9999745">
diction (averaged over all 12 aspects), differences
are smaller, at 1.6% and 2.9% respectively. These
smaller differences could be explained by the fact
that among the 10 most recent comments for each
talk, many are not related to the emotion that the
system tries to predict.
As mentioned earlier, the proposed model does
not make any assumption about the feature space.
Thus, we examined whether the improvements it
brings remain present even with a different fea-
ture space, for instance based on TF-IDF instead
of BOW with counts. For sentiment prediction on
TED comments, we found that by changing the
feature space to TF-IDF, strong baselines such as
Aggregated (`1) and (`2), i.e. SVR and Lasso, im-
prove their performance (16.25 and 16.59 MAE;
4.16 and 3.97 MSE respectively). However, AP-
Weights still outperforms them on both MAE and
MSE scores (15.35 and 3.63), improving over
SVR by 5.5% on MAE and 12.5% on MSE, and
over Lasso by 7.4% on MAE and 8.5% on MSE.
These promising results suggest that improve-
ments with APWeights could be observed also on
more sophisticated feature spaces.
</bodyText>
<subsectionHeader confidence="0.997874">
7.3 Interpreting the Relevance Weights
</subsectionHeader>
<bodyText confidence="0.86489753125">
Apart from predicting ratings, the MIR scores as-
signed by our model reflect the contribution of
each sentence to these predictions.
To illustrate the explanatory power of our model
(until a dataset for quantitative analysis becomes
available), we provide examples of predictions
on test data taken from the cross-validation folds
above. Table 5 displays the most relevant com-
Sentences per comment ˆψi ˆyi yi
“Very brilliant and witty, as well as 0.64
great improvisation.” 5.0 5.0
“I enjoyed this one a lot.” 0.36
“That’s great idea, I really like it!” 0.56
“I can’t wait to try it, but first thing, 0.44 4.2 4.0
I need a house with big windows,
next year, maybe I can do that.”
“Unfortunately countries are not led 0.48
by gifted children.”
“They are either dictated by the 0.52
most extreme personalities who 2.4 2.0
crave nothing but power or man-
aged by politicians who are voted in
by a far from gifted population.”
“I am very disappointed by this, 0.43
smug, cliched and missing so much
information as to be almost (...)”’
“No mention of ship transport lets 0.29
say 50% of all material transport, 1.8 1.0
no mention of rail transport, (...)”
“I am sorry to be so negative, this 0.28
just sounds like a sales pitch that he
has given too many times (...).”
</bodyText>
<tableCaption confidence="0.709336">
Table 4: Predicted sentiment for TED comments:
</tableCaption>
<bodyText confidence="0.983601090909091">
yi is the actual sentiment, ˆyi the predicted one, and
ˆψi the estimated relevance of each sentence.
ment for two correctly predicted emotions on two
TED talks, based on the ˆψi relevance scores, along
with the ˆψi scores of the other comments, for
two emotion classes: ‘beautiful’ and ‘courageous’.
These comments appear to reflect correctly the
fact that the respective emotion is the majority one
in each of the comments. As noted earlier, this
task is quite challenging since we use only the ten
most recent comments for each talk.
</bodyText>
<tableCaption confidence="0.692378">
Table 4 displays four TED comments selected
</tableCaption>
<page confidence="0.997082">
463
</page>
<bodyText confidence="0.990100529411765">
Class Top comment per talk (according to weights ψi) ˆψi distribution
inspiring “It seems to me that the idea worth spreading of this TED Talk is inspiring and key for
a full life. ‘No-one else is the authority on your potential. You’re the only person that
decides how far you go and what you’re capable of.’ It seems to me that teens actually
think that. As a child one is all knowing and all capable. How did we get to the (...)”
beautiful “The beauty of the nature. It would be more interesting just integrates his thought and
idea into a mobile device, like a mobile, so we can just turn on the nature gallery in any
time. The paintings don’t look incidental but genuinely thought out, random perhaps, but
with a clear grand design behind the randomness. Drawing is an art where it doesn’t (...)”
funny “Funny story, but not as funny as a good ’knock, knock’ joke. My favorite knock-knock
joke of all time is Cheech &amp; Chong’s ‘Dave’s Not Here’ gag from the early 1970s. I’m
still waiting for someone to top it after all these years. [Knock, knock] ‘Who is it?’ the
voice of an obviously stoned male answers from the other side of a door, (...)”
courageous “I was a soldier in Iraq and part of the unit represented in this documentary. I would ques-
tion anyone that told you we went over there to kill Iraqi people. I spent the better part
of my time in Iraq protecting the Iraqi people from insurgents who came from countries
outside of Iraq to kill Iraqi people. We protected families men, women, and (...)”
</bodyText>
<tableCaption confidence="0.764423">
Table 5: Two examples of top comments (according to weights ψi) for correctly predicted emotions in
four TED talks (score 1.0) and the distribution of weights over the 10 most recent comments in each talk.
</tableCaption>
<figureCaption confidence="0.994004">
Figure 5: Top words based on Φ for predicting four emotions from comments on TED talks.
</figureCaption>
<bodyText confidence="0.9999436">
from the test set of a given fold, for the comment-
level sentiment prediction task. The table also
shows the ˆψi relevance scores assigned to each
of the composing sentences, the predicted polar-
ity scores ˆyi and the actual ones yi. We observe
that the sentences that convey the most sentiment
are assigned higher scores than sentences with less
sentiment, always with respect to the global polar-
ity level. These examples suggest that, given that
APWeights has more degrees of freedom for inter-
pretation, it is able to assign relevance to parts of
a text (here, sentences) and even to words, while
other models can only consider words. Hence, the
assigned weights might be useful for other NLP
tasks mentioned below.
</bodyText>
<sectionHeader confidence="0.99484" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999966967741935">
This paper introduced a novel MIR model for as-
pect rating prediction from text, which learns in-
stance relevance together with target labels. To the
best of our knowledge, this has not been consid-
ered before. Compared to previous work on MIR,
the proposed model is competitive and more effi-
cient in terms of complexity. Moreover, it is not
only able to assign instance relevances on labeled
bags, but also to predict them on unseen bags.
Compared to previous work on aspect rating
prediction, our model performs significantly bet-
ter than BOW regression baselines (SVR, Lasso)
without using additional knowledge or features.
The improvements persist even when the sophis-
tication of the features increases, suggesting that
our contribution may be orthogonal to feature en-
gineering or learning. Lastly, the qualitative eval-
uation on test examples demonstrates that the pa-
rameters learned by the model are not only useful
for prediction, but they are also interpretable.
In the future, we intend to test our model on sen-
timent classification at the sentence-level, based
only on document-level supervision (T¨ackstr¨om
and McDonald, 2011). Moreover, we will experi-
ment with other model settings, such as regulariza-
tion norms other than E2 and feature spaces other
than BOW or TF-IDF. In the longer term, we plan
to investigate new methods to estimate instance
weights at prediction time, and to evaluate the im-
pact of assigned weights on sentence ranking, seg-
mentation or summarization.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9990165">
The work described in this article was sup-
ported by the European Union through the inEvent
project FP7-ICT n. 287872 (see http://www.
inevent-project.eu).
</bodyText>
<page confidence="0.999348">
464
</page>
<sectionHeader confidence="0.996396" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999572801801802">
Stuart Andrews, Ioannis Tsochantaridis, and Thomas
Hofmann. 2003. Support vector machines for
multiple-instance learning. In Advances in Neu-
ral Information Processing Systems, pages 561–568,
Vancouver, British Columbia, Canada.
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2009. Multi-facet rating of product re-
views. In Mohand Boughanem, Catherine Berrut,
Josiane Mothe, and Chantal Soule-Dupuy, editors,
Advances in Information Retrieval, volume 5478 of
Lecture Notes in Computer Science, pages 461–472.
Springer Berlin Heidelberg.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Multiple instance learning for sparse positive bags.
In Proceedings of the 24th Annual International
Conference on Machine Learning, ICML ’07, Cor-
vallis, OR, USA.
Jesse Davis et al. 2007. Tightly integrating rela-
tional learning and multiple-instance regression for
real-valued drug activity prediction. In Proceedings
of the 24th International Conference on Machine
Learning, ICML ’07, pages 425–432, Corvallis, OR,
USA.
Harris Drucker, Chris J.C. Burges, Linda Kaufman,
Alex Smola, and Vladimir Vapnik. 1996. Support
vector regression machines. In Advances in Neu-
ral Information Processing systems, pages 155–161,
Denver, CO, USA.
James Foulds and Eibe Frank. 2010. A review of
multi-instance learning assumptions. The Knowl-
edge Engineering Review, 25:1:1–25.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD Int. Conf. on Knowledge discovery
and data mining, KDD ’04, pages 168–177, Seattle,
WA.
Fangtao Li, Nathan Liu, Hongwei Jin, Kai Zhao, Qiang
Yang, and Xiaoyan Zhu. 2011. Incorporating re-
viewer and product information for review rating
prediction. In Proceedings of the 22nd International
Joint Conference on Artificial Intelligence - Volume
3, IJCAI ’11, pages 1820–1825, Barcelona, Catalo-
nia, Spain.
Bin Lu, Myle Ott, Claire Cardie, and Benjamin K.
Tsou. 2011. Multi-aspect sentiment analysis with
topic models. In Proceedings of the 11th IEEE In-
ternational Conference on Data Mining Workshops,
ICDMW ’11, pages 81–88, Washington, DC.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, HLT ’11, pages
142–150, Portland, OR.
J. McAuley, J. Leskovec, and D. Jurafsky. 2012.
Learning attitudes and attributes from multi-aspect
reviews. In Proceedings of the 12th IEEE Inter-
national Conference on Data Mining, ICDM ’12,
pages 1020–1025, Brussels, Belgium.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of the 16th Int. Conf. on the World Wide
Web, WWW ’07, pages 171–180, Banff, AB.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, ACL ’05, pages 115–124, Ann
Arbor, MI.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1–135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification us-
ing machine learning techniques. In Proceedings
of the ACL Conf. on Empirical Methods in Natu-
ral Language Processing, EMNLP ’02, pages 79–
86, Philadelphia, PA.
Nikolaos Pappas and Andrei Popescu-Belis. 2013.
Sentiment analysis of user comments for one-class
collaborative filtering over TED talks. In 36th ACM
SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ’13, Dublin, Ireland.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake VanderPlas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2012.
Scikit-learn: Machine learning in python. CoRR,
abs/1201.0490.
Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum.
2010. The bag-of-opinions method for review rating
prediction from sparse text patterns. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, COLING ’10, pages 913–921,
Beijing, China.
Soumya Ray and David Page. 2001. Multiple instance
regression. In Proceedings of the 18th International
Conference on Machine Learning, ICML ’01, pages
425–432.
Christina Sauper and Regina Barzilay. 2013. Auto-
matic aggregation by joint modeling of aspects and
values. Journal of Artificial Intelligence Research,
46(1):89–127.
Christina Sauper, Aria Haghighi, and Regina Barzi-
lay. 2010. Incorporating content structure into
text analysis applications. In Proceedings of the
2010 Conference on Empirical Methods in Natural
</reference>
<page confidence="0.994488">
465
</page>
<reference confidence="0.999548555555555">
Language Processing, EMNLP ’10, pages 377–387,
Cambridge, MA.
Burr Settles, Mark Craven, and Soumya Ray. 2008.
Multiple-instance active learning. In Advances in
Neural Information Processing Systems, NIPS ’08,
pages 1289–1296, Vancouver, BC.
Benjamin Snyder and Regina Barzilay. 2007. Multi-
ple aspect ranking using the good grief algorithm.
In In Proceedings of the Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, HLT-
NAACL ’07, pages 300–307, Rochester, NY, USA.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’11, pages 151–161, Ed-
inburgh, UK.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’13, pages 1631–1642, Portland, OR.
Oscar T¨ackstr¨om and Ryan McDonald. 2011. Dis-
covering fine-grained sentiment with latent variable
structured prediction models. In Proceedings of the
33rd European Conference on Advances in Infor-
mation Retrieval, ECIR’11, pages 368–374, Berlin,
Heidelberg. Springer-Verlag.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society (Series B), 58:267–288.
Ivan Titov and Ryan McDonald. 2008. Modeling
online reviews with multi-grain topic models. In
Proceedings of the 17th international conference on
World Wide Web, WWW ’08, pages 111–120, Bei-
jing, China.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, ACL ’02, pages 417–424, Philadelphia, PA.
Kiri L. Wagstaff and Terran Lane. 2007. Salience as-
signment for multiple-instance regression. In ICML
2007 Workshop on Constrained Optimization and
Structured Output Spaces, Corvallis, Oregon, USA.
Kiri L. Wagstaff, Terran Lane, and Alex Roper. 2008.
Multiple-instance regression with structured data. In
Proceedings of the IEEE International Conference
on Data Mining Workshops, ICDMW ’08, pages
291–300.
Zhuang Wang, Vladan Radosavljevic, Bo Han, Zoran
Obradovic, and Slobodan Vucetic. 2008. Aerosol
optical depth prediction from satellite observations
by multiple instance regression. In Proceedings of
the SIAM Int. Conf. on Data Mining, SDM ’08,
pages 165–176, Atlanta, GA.
Hua Wang, Feiping Nie, and Heng Huang. 2011.
Learning instance specific distance for multi-
instance classification. In AAAI Conference on Arti-
ficial Intelligence.
Zhuang Wang, Liang Lan, and S. Vucetic. 2012. Mix-
ture model for multiple instance regression and ap-
plications in remote sensing. IEEE Transactions on
Geoscience and Remote Sensing, 50(6):2226–2237.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing subjective language. Computational Linguistics,
30(3):277–308, September.
Min-Ling Zhang and Zhi-Hua Zhou. 2008. M3MIML:
A maximum margin method for multi-instance
multi-label learning. In Data Mining, 2008. ICDM
’08. Eighth IEEE International Conference on,
pages 688–697, Dec.
Min-Ling Zhang and Zhi-Hua Zhou. 2009. Multi-
instance clustering with applications to multi-
instance prediction. Applied Intelligence, 31(1):47–
68.
Zhi-Hua Zhou, Kai Jiang, and Ming Li. 2005. Multi-
instance learning based web mining. Applied Intel-
ligence, 22(2):135–147.
Zhi-Hua Zhou, Yu-Yin Sun, and Yu-Feng Li. 2009.
Multi-instance learning by treating instances as non-
i.i.d. samples. In Proceedings of the 26th An-
nual International Conference on Machine Learn-
ing, ICML ’09, pages 1249–1256, Montreal, Que-
bec, Canada.
Jingbo Zhu, Chunliang Zhang, and Matthew Y. Ma.
2012. Multi-aspect rating inference with aspect-
based segmentation. IEEE Trans. on Affective Com-
puting, 3(4):469–481.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In Pro-
ceedings of the 15th ACM International Conference
on Information and Knowledge Management, CIKM
’06, pages 43–50, Arlington, VA.
</reference>
<page confidence="0.999595">
466
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.224079">
<title confidence="0.95798575">Explaining the Stars: Weighted Multiple-Instance Learning Aspect-Based Sentiment Analysis Nikolaos EPFL and Idiap Research</title>
<author confidence="0.993241">Rue Marconi</author>
<affiliation confidence="0.731429">CH-1920 Martigny,</affiliation>
<email confidence="0.881713">nikolaos.pappas@idiap.ch</email>
<author confidence="0.765468">Andrei</author>
<affiliation confidence="0.793486">Idiap Research</affiliation>
<author confidence="0.896719">Rue Marconi</author>
<affiliation confidence="0.685845">CH-1920 Martigny,</affiliation>
<email confidence="0.972524">andrei.popescu-belis@idiap.ch</email>
<abstract confidence="0.996738875">This paper introduces a model of multipleinstance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from usercontributed texts such as product reviews. Each variable-length text is represented by several independent feature vectors; one word vector per sentence or paragraph. For learning from texts with known aspect ratings, the model performs multipleinstance regression (MIR) and assigns importance weights to each of the sentences or paragraphs of a text, uncovering their contribution to the aspect ratings. Next, the model is used to predict aspect ratings in previously unseen texts, demonstrating interpretability and explanatory power for its predictions. We evaluate the model on seven multi-aspect sentiment analysis data sets, improving over four MIR baselines and two strong bag-of-words linear models, namely SVR and Lasso, by more than 10% relative in terms of MSE.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stuart Andrews</author>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
</authors>
<title>Support vector machines for multiple-instance learning.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>561--568</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="6432" citStr="Andrews et al., 2003" startWordPosition="1011" endWordPosition="1015">m while Section 4 describes previous MIR models. Section 5 presents our MIR model and learning procedure. Section 6 presents the datasets and evaluation methods. Section 7 reports our results on rating prediction tasks, and provides examples of rating explanation. 2 Related Work 2.1 Multiple-Instance Regression Multiple-instance regression (MIR) belongs to the class of multiple-instance learning (MIL) problems for real-valued output, and it is a variant of multiple regression where each data point may be described by more than one vectors of values. Many MIL studies focused on classification (Andrews et al., 2003; Bunescu and Mooney, 2007; Settles et al., 2008; Foulds and Frank, 2010; Wang et al., 2011) while fewer focused on regression (Ray and Page, 2001; Davis and others, 2007; Wagstaff et al., 2008; Wagstaff and Lane, 2007). Related to document analysis, several MIR studies have focused on news categorization (Zhang and Zhou, 2008; Zhou et al., 2009) or web-index recommendation (Zhou et al., 2005) but, to our knowledge, no study has attempted to use MIR for aspect rating prediction or sentiment analysis with real-valued labels. MIR was firstly introduced by Ray et al. (2001), proposing an EM algor</context>
</contexts>
<marker>Andrews, Tsochantaridis, Hofmann, 2003</marker>
<rawString>Stuart Andrews, Ioannis Tsochantaridis, and Thomas Hofmann. 2003. Support vector machines for multiple-instance learning. In Advances in Neural Information Processing Systems, pages 561–568, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Multi-facet rating of product reviews.</title>
<date>2009</date>
<booktitle>In Mohand Boughanem, Catherine Berrut, Josiane Mothe, and Chantal Soule-Dupuy, editors, Advances in Information Retrieval,</booktitle>
<volume>5478</volume>
<pages>461--472</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="9489" citStr="Baccianella et al., 2009" startWordPosition="1508" endWordPosition="1511">ke any assumption about the nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Baccianella et al., 2009; Qu et al., 2010; Zhu et al., 2012). Other studies have advocated joint modeling of multiple aspects (Snyder and Barzilay, 2007) or multiple reviews for the same product (Li et al., 2011). McAuley et al. (2012) introduced new corpora of multi-aspect reviews, which we also partly use here, and proposed models for aspect detection, sentiment summarization and rating prediction. Lastly, joint aspect identification and sentiment classification have been used for aggregating product review snippets by Sauper at al. (2013). None of the above studies considers the multipleinstance property of text i</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2009</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2009. Multi-facet rating of product reviews. In Mohand Boughanem, Catherine Berrut, Josiane Mothe, and Chantal Soule-Dupuy, editors, Advances in Information Retrieval, volume 5478 of Lecture Notes in Computer Science, pages 461–472. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Multiple instance learning for sparse positive bags.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th Annual International Conference on Machine Learning, ICML ’07,</booktitle>
<location>Corvallis, OR, USA.</location>
<contexts>
<context position="6458" citStr="Bunescu and Mooney, 2007" startWordPosition="1016" endWordPosition="1019">ribes previous MIR models. Section 5 presents our MIR model and learning procedure. Section 6 presents the datasets and evaluation methods. Section 7 reports our results on rating prediction tasks, and provides examples of rating explanation. 2 Related Work 2.1 Multiple-Instance Regression Multiple-instance regression (MIR) belongs to the class of multiple-instance learning (MIL) problems for real-valued output, and it is a variant of multiple regression where each data point may be described by more than one vectors of values. Many MIL studies focused on classification (Andrews et al., 2003; Bunescu and Mooney, 2007; Settles et al., 2008; Foulds and Frank, 2010; Wang et al., 2011) while fewer focused on regression (Ray and Page, 2001; Davis and others, 2007; Wagstaff et al., 2008; Wagstaff and Lane, 2007). Related to document analysis, several MIR studies have focused on news categorization (Zhang and Zhou, 2008; Zhou et al., 2009) or web-index recommendation (Zhou et al., 2005) but, to our knowledge, no study has attempted to use MIR for aspect rating prediction or sentiment analysis with real-valued labels. MIR was firstly introduced by Ray et al. (2001), proposing an EM algorithm which assumes that on</context>
</contexts>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2007. Multiple instance learning for sparse positive bags. In Proceedings of the 24th Annual International Conference on Machine Learning, ICML ’07, Corvallis, OR, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesse Davis</author>
</authors>
<title>Tightly integrating relational learning and multiple-instance regression for real-valued drug activity prediction.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine Learning, ICML ’07,</booktitle>
<pages>425--432</pages>
<location>Corvallis, OR, USA.</location>
<marker>Davis, 2007</marker>
<rawString>Jesse Davis et al. 2007. Tightly integrating relational learning and multiple-instance regression for real-valued drug activity prediction. In Proceedings of the 24th International Conference on Machine Learning, ICML ’07, pages 425–432, Corvallis, OR, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harris Drucker</author>
<author>Chris J C Burges</author>
<author>Linda Kaufman</author>
<author>Alex Smola</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Support vector regression machines.</title>
<date>1996</date>
<booktitle>In Advances in Neural Information Processing systems,</booktitle>
<pages>155--161</pages>
<location>Denver, CO, USA.</location>
<contexts>
<context position="11671" citStr="Drucker et al., 1996" startWordPosition="1903" endWordPosition="1906">task is generally unknown, one has to make assumptions to define it or compute it jointly with the regression hyperplane Φ. Thus, the main difficulty lies in finding a good assumption for X, as we will now discuss. 4 Previous MIR Assumptions We describe here three assumptions frequently made in past MIR studies, to which we will later compare our model: aggregating all instances, keeping them as separate examples, or choosing the most representative one (Wang et al., 2012). For each assumption, we will experiment with two state-of-the-art regression models (noted abstractly as f), namely SVR (Drucker et al., 1996) and Lasso (Tibshirani, 1996) with respectively the E2 and E1 norms for regularization. The Aggregated algorithm assumes that each bag is represented as a single d-dimensional vector, which is the average of its instances (hence xi E Rd). Then, a regression model f is trained on pairs of vectors and class labels, Dagg = {(xi, yi) |i = 1, ... , m}, and the predicted class of an unlabeled bag Bi = {bij |j = 1, ... , ni} is computed as follows: ˆy(Bi) = f(mean({bij |j = 1, ... , ni})) (2) In fact, a simple sum can also be used instead of the mean, and we observed in practice that with an appropri</context>
</contexts>
<marker>Drucker, Burges, Kaufman, Smola, Vapnik, 1996</marker>
<rawString>Harris Drucker, Chris J.C. Burges, Linda Kaufman, Alex Smola, and Vladimir Vapnik. 1996. Support vector regression machines. In Advances in Neural Information Processing systems, pages 155–161, Denver, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Foulds</author>
<author>Eibe Frank</author>
</authors>
<title>A review of multi-instance learning assumptions. The Knowledge Engineering Review,</title>
<date>2010</date>
<pages>25--1</pages>
<contexts>
<context position="6504" citStr="Foulds and Frank, 2010" startWordPosition="1024" endWordPosition="1027">r MIR model and learning procedure. Section 6 presents the datasets and evaluation methods. Section 7 reports our results on rating prediction tasks, and provides examples of rating explanation. 2 Related Work 2.1 Multiple-Instance Regression Multiple-instance regression (MIR) belongs to the class of multiple-instance learning (MIL) problems for real-valued output, and it is a variant of multiple regression where each data point may be described by more than one vectors of values. Many MIL studies focused on classification (Andrews et al., 2003; Bunescu and Mooney, 2007; Settles et al., 2008; Foulds and Frank, 2010; Wang et al., 2011) while fewer focused on regression (Ray and Page, 2001; Davis and others, 2007; Wagstaff et al., 2008; Wagstaff and Lane, 2007). Related to document analysis, several MIR studies have focused on news categorization (Zhang and Zhou, 2008; Zhou et al., 2009) or web-index recommendation (Zhou et al., 2005) but, to our knowledge, no study has attempted to use MIR for aspect rating prediction or sentiment analysis with real-valued labels. MIR was firstly introduced by Ray et al. (2001), proposing an EM algorithm which assumes that one primary instance per bag is responsible for </context>
</contexts>
<marker>Foulds, Frank, 2010</marker>
<rawString>James Foulds and Eibe Frank. 2010. A review of multi-instance learning assumptions. The Knowledge Engineering Review, 25:1:1–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th ACM SIGKDD Int. Conf. on Knowledge discovery and data mining, KDD ’04,</booktitle>
<pages>168--177</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="9189" citStr="Hu and Liu, 2004" startWordPosition="1456" endWordPosition="1459"> to rating scales. Pang and Lee (2008) discusses the large range of features engineered for this task, though several recent studies focus on feature learning (Maas et al., 2011; Socher et al., 2011), including the use of a deep neural network (Socher et al., 2013). In contrast, we do not make any assumption about the nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Baccianella et al., 2009; Qu et al., 2010; Zhu et al., 2012). Other studies have advocated joint modeling of multiple aspects (Snyder and Barzilay, 2007) or multiple reviews for the same product (Li et al., 2011). McAuley et al. (2012) introduced new corpora of multi-aspect reviews, which we also partly use here, and propo</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the 10th ACM SIGKDD Int. Conf. on Knowledge discovery and data mining, KDD ’04, pages 168–177, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangtao Li</author>
<author>Nathan Liu</author>
<author>Hongwei Jin</author>
<author>Kai Zhao</author>
<author>Qiang Yang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Incorporating reviewer and product information for review rating prediction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 22nd International Joint Conference on Artificial Intelligence - Volume 3, IJCAI ’11,</booktitle>
<pages>1820--1825</pages>
<location>Barcelona, Catalonia,</location>
<contexts>
<context position="9677" citStr="Li et al., 2011" startWordPosition="1540" endWordPosition="1543">analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Baccianella et al., 2009; Qu et al., 2010; Zhu et al., 2012). Other studies have advocated joint modeling of multiple aspects (Snyder and Barzilay, 2007) or multiple reviews for the same product (Li et al., 2011). McAuley et al. (2012) introduced new corpora of multi-aspect reviews, which we also partly use here, and proposed models for aspect detection, sentiment summarization and rating prediction. Lastly, joint aspect identification and sentiment classification have been used for aggregating product review snippets by Sauper at al. (2013). None of the above studies considers the multipleinstance property of text in their modeling. 3 MIR Definition Let us consider a set B of m bags with numerical labels Y as input data D = {({b1j}dn1, y1), ..., ({bmj}dn,,, ym)}, where bij E Rd (for 1 G j G ni) and y</context>
</contexts>
<marker>Li, Liu, Jin, Zhao, Yang, Zhu, 2011</marker>
<rawString>Fangtao Li, Nathan Liu, Hongwei Jin, Kai Zhao, Qiang Yang, and Xiaoyan Zhu. 2011. Incorporating reviewer and product information for review rating prediction. In Proceedings of the 22nd International Joint Conference on Artificial Intelligence - Volume 3, IJCAI ’11, pages 1820–1825, Barcelona, Catalonia, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Lu</author>
<author>Myle Ott</author>
<author>Claire Cardie</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Multi-aspect sentiment analysis with topic models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 11th IEEE International Conference on Data Mining Workshops, ICDMW ’11,</booktitle>
<pages>81--88</pages>
<location>Washington, DC.</location>
<contexts>
<context position="9414" citStr="Lu et al., 2011" startWordPosition="1495" endWordPosition="1498">eep neural network (Socher et al., 2013). In contrast, we do not make any assumption about the nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Baccianella et al., 2009; Qu et al., 2010; Zhu et al., 2012). Other studies have advocated joint modeling of multiple aspects (Snyder and Barzilay, 2007) or multiple reviews for the same product (Li et al., 2011). McAuley et al. (2012) introduced new corpora of multi-aspect reviews, which we also partly use here, and proposed models for aspect detection, sentiment summarization and rating prediction. Lastly, joint aspect identification and sentiment classification have been used for aggregating product review snippets by Sauper at al. (2013). </context>
<context position="31377" citStr="Lu et al., 2011" startWordPosition="5399" endWordPosition="5402">VM baseline provided by the McAuley et al., our method improved by 32%, 43% and 35% respectively on each dataset, though it did not use their rating model. Moreover, the best model proposed by McAuley et al., which uses a joint rating model and an aspect-specific text segmenter trained on hand-labeled data, reaches MSE scores of .03, .02 and .03, which is comparable to our model that does not use these features (.034, .017, .052), though it could benefit from them in the future. Lastly, as mentioned by the same authors, predictors which use segmented text, for example with topic models as in (Lu et al., 2011), do not necessarly outperform SVR baselines; instead they have marginal or even no improvements, therefore, we did not further experiment with them. InteresSENT. LABELS EMO. LABELS TED comm. TED talks Model \ Error MAE MSE MAE MSE AverageRating 19.47 5.05 17.86 6.06 Aggregated (`1) 17.08 4.17 15.98 5.03 Aggregated (`2) 16.88 4.47 15.24 4.97 Instance (`1) 17.69 4.37 16.48 5.30 Instance (`2) 16.93 4.24 16.10 5.57 Prime (`1) 17.39 4.37 15.98 5.78 Prime (`2) 18.03 4.91 16.74 5.94 Clustering (`2) 17.64 4.34 17.71 6.02 APWeights (`2) 15.91 3.95 15.02 4.89 APW vs SVR (%) +5.7 +11.5 +1.5 +1.6 APW vs </context>
</contexts>
<marker>Lu, Ott, Cardie, Tsou, 2011</marker>
<rawString>Bin Lu, Myle Ott, Claire Cardie, and Benjamin K. Tsou. 2011. Multi-aspect sentiment analysis with topic models. In Proceedings of the 11th IEEE International Conference on Data Mining Workshops, ICDMW ’11, pages 81–88, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>142--150</pages>
<location>Portland, OR.</location>
<contexts>
<context position="8750" citStr="Maas et al., 2011" startWordPosition="1387" endWordPosition="1390">oduced machine learning techniques for sentiment classification, e.g. Pang et al. (2002), including unsupervised techniques based on the notion of semantic orientation of phrases, e.g. Turney et al. (2002). Other studies focused on subjectivity detection, i.e. whether a 456 text span expresses opinions or not (Wiebe et al., 2004). Rating inference was defined by Pang et al. (2005) as multi-class classification or regression with respect to rating scales. Pang and Lee (2008) discusses the large range of features engineered for this task, though several recent studies focus on feature learning (Maas et al., 2011; Socher et al., 2011), including the use of a deep neural network (Socher et al., 2013). In contrast, we do not make any assumption about the nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007;</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 142–150, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J McAuley</author>
<author>J Leskovec</author>
<author>D Jurafsky</author>
</authors>
<title>Learning attitudes and attributes from multi-aspect reviews.</title>
<date>2012</date>
<booktitle>In Proceedings of the 12th IEEE International Conference on Data Mining, ICDM ’12,</booktitle>
<pages>1020--1025</pages>
<location>Brussels, Belgium.</location>
<contexts>
<context position="5273" citStr="McAuley et al., 2012" startWordPosition="827" endWordPosition="830">. The parameters of the model are learned using an alternating optimization procedure inspired by Wagstaff and Lane (2007). Our model requires only text with ratings for training, with no particular assumption on the word features to be extracted, and provides interpretable explanations of the predicted ratings through the relevance weights assigned to sentences. We also show that the model has reasonable computational demands. The model is evaluated on aspect and sentiment rating prediction over seven datasets: five of them contain reviews with aspect labels about beers, audiobooks and toys (McAuley et al., 2012), and two contain TED talks with emotion labels, and comments on them with sentiment labels (Pappas and Popescu-Belis, 2013). Our model outperforms previous MIR models and two strong linear models for rating prediction, namely SVR and Lasso by more than 10% relative in terms of MSE. The improvement is observed even when the sophistication of the feature space increases. The paper is organized as follows. Section 2 shows how our model innovates with respect to previous work on MIR and rating prediction. Section 3 formulates the problem while Section 4 describes previous MIR models. Section 5 pr</context>
<context position="9700" citStr="McAuley et al. (2012)" startWordPosition="1544" endWordPosition="1547">k usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Baccianella et al., 2009; Qu et al., 2010; Zhu et al., 2012). Other studies have advocated joint modeling of multiple aspects (Snyder and Barzilay, 2007) or multiple reviews for the same product (Li et al., 2011). McAuley et al. (2012) introduced new corpora of multi-aspect reviews, which we also partly use here, and proposed models for aspect detection, sentiment summarization and rating prediction. Lastly, joint aspect identification and sentiment classification have been used for aggregating product review snippets by Sauper at al. (2013). None of the above studies considers the multipleinstance property of text in their modeling. 3 MIR Definition Let us consider a set B of m bags with numerical labels Y as input data D = {({b1j}dn1, y1), ..., ({bmj}dn,,, ym)}, where bij E Rd (for 1 G j G ni) and yi E R. Each bag Bi cons</context>
<context position="23400" citStr="McAuley et al. (2012)" startWordPosition="4036" endWordPosition="4039">ty for highdimensional feature spaces or numerous bags and hence they are not most appropriate for text regression tasks. Wagstaff and Lane (2007) have cubic time complexity with the average bag size nˆ and number of features d; Zhou et al. (2009) use kernels, thus their complexity is quadratic with the number of bags m; and Wang et al. (2011) have cubic time wrt. d. Our formulation is thus competitive in terms of complexity. 6 Data, Protocol and Metrics 6.1 Aspect Rating Datasets We use seven datasets summarized in Table 1. Five publicly available datasets were built for aspect prediction by McAuley et al. (2012) – BeerAdvocate, Ratebeer (ES), RateBeer (FR), Audiobooks and Toys &amp; Games – and have aspect ratings assigned by their creators on the respective websites. On the set of comments on TED talks from Pappas and Popescu-Belis (2013), we aim to predict two things: talk-level emotion dimensions assigned by viewers through voting, and comment polarity scores assigned by crowdsourcing. The distributions of aspect ratings per dataset are shown in Figure 3. Five datasets are in English, one in Spanish (Ratebeer) and one in French (RateBeer), so our results will also demonstrate the language-independence</context>
<context position="30277" citStr="McAuley et al. (2012)" startWordPosition="5208" endWordPosition="5211">ll aspects in each dataset. The scores of the best method are in bold and the second best ones are underlined. Significant improvements (paired t-test, p &lt; 0.05) are in italics. Fig. 4 shows MSE scores per aspect for three methods on five datasets. assumption is not the most appropriate for this task. Lastly, even though Clustering is an instance relevance method, it has similar scores to Prime, presumably because the relevances are assigned according to the computed clusters and they are not directly influenced by the task’s objective. To compare with the state-of-the-art results obtained by McAuley et al. (2012), we experimented with three of their full-size datasets. Splitting each dataset in half for training vs. testing, and using the optimal settings from our experiments above, we measured the average MSE over all aspects. APWeights improved over Lasso by 10%, 26% and 17% MSE respectively on each dataset – the absolute MSE scores are .038 for Lasso vs. .034 for APWeights on Ratebeer SP; .023 vs. .017 on Ratebeer FR; .063 vs. .052 on Audiobooks. Similarly, when compared to the best SVM baseline provided by the McAuley et al., our method improved by 32%, 43% and 35% respectively on each dataset, th</context>
</contexts>
<marker>McAuley, Leskovec, Jurafsky, 2012</marker>
<rawString>J. McAuley, J. Leskovec, and D. Jurafsky. 2012. Learning attitudes and attributes from multi-aspect reviews. In Proceedings of the 12th IEEE International Conference on Data Mining, ICDM ’12, pages 1020–1025, Brussels, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xu Ling</author>
<author>Matthew Wondra</author>
<author>Hang Su</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Topic sentiment mixture: modeling facets and opinions in weblogs.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th Int. Conf. on the World Wide Web, WWW ’07,</booktitle>
<pages>171--180</pages>
<location>Banff, AB.</location>
<contexts>
<context position="9349" citStr="Mei et al., 2007" startWordPosition="1483" endWordPosition="1486">Maas et al., 2011; Socher et al., 2011), including the use of a deep neural network (Socher et al., 2013). In contrast, we do not make any assumption about the nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Baccianella et al., 2009; Qu et al., 2010; Zhu et al., 2012). Other studies have advocated joint modeling of multiple aspects (Snyder and Barzilay, 2007) or multiple reviews for the same product (Li et al., 2011). McAuley et al. (2012) introduced new corpora of multi-aspect reviews, which we also partly use here, and proposed models for aspect detection, sentiment summarization and rating prediction. Lastly, joint aspect identification and sentiment classification have been used </context>
</contexts>
<marker>Mei, Ling, Wondra, Su, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007. Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings of the 16th Int. Conf. on the World Wide Web, WWW ’07, pages 171–180, Banff, AB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>115--124</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="9463" citStr="Pang and Lee, 2005" startWordPosition="1504" endWordPosition="1507">ntrast, we do not make any assumption about the nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Baccianella et al., 2009; Qu et al., 2010; Zhu et al., 2012). Other studies have advocated joint modeling of multiple aspects (Snyder and Barzilay, 2007) or multiple reviews for the same product (Li et al., 2011). McAuley et al. (2012) introduced new corpora of multi-aspect reviews, which we also partly use here, and proposed models for aspect detection, sentiment summarization and rating prediction. Lastly, joint aspect identification and sentiment classification have been used for aggregating product review snippets by Sauper at al. (2013). None of the above studies considers the multiplei</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 115–124, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="8611" citStr="Pang and Lee (2008)" startWordPosition="1363" endWordPosition="1366">ing the polarity of a given text, either with classification (for discrete labels) or regression (for real-valued labels). Early studies introduced machine learning techniques for sentiment classification, e.g. Pang et al. (2002), including unsupervised techniques based on the notion of semantic orientation of phrases, e.g. Turney et al. (2002). Other studies focused on subjectivity detection, i.e. whether a 456 text span expresses opinions or not (Wiebe et al., 2004). Rating inference was defined by Pang et al. (2005) as multi-class classification or regression with respect to rating scales. Pang and Lee (2008) discusses the large range of features engineered for this task, though several recent studies focus on feature learning (Maas et al., 2011; Socher et al., 2011), including the use of a deep neural network (Socher et al., 2013). In contrast, we do not make any assumption about the nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006)</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Conf. on Empirical Methods in Natural Language Processing, EMNLP ’02,</booktitle>
<pages>79--86</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="8221" citStr="Pang et al. (2002)" startWordPosition="1302" endWordPosition="1305">08) assumed that each bag is generated by random noise around a primary instance, while Wang et al. (2012) represented bag labels with a probabilistic mixture model. Foulds et al. (2010) concluded that various assumptions are differently suited to different tasks, and should be stated clearly when describing an MIR model. 2.2 Rating Prediction from Text Sentiment analysis aims at analyzing the polarity of a given text, either with classification (for discrete labels) or regression (for real-valued labels). Early studies introduced machine learning techniques for sentiment classification, e.g. Pang et al. (2002), including unsupervised techniques based on the notion of semantic orientation of phrases, e.g. Turney et al. (2002). Other studies focused on subjectivity detection, i.e. whether a 456 text span expresses opinions or not (Wiebe et al., 2004). Rating inference was defined by Pang et al. (2005) as multi-class classification or regression with respect to rating scales. Pang and Lee (2008) discusses the large range of features engineered for this task, though several recent studies focus on feature learning (Maas et al., 2011; Socher et al., 2011), including the use of a deep neural network (Soc</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: Sentiment classification using machine learning techniques. In Proceedings of the ACL Conf. on Empirical Methods in Natural Language Processing, EMNLP ’02, pages 79– 86, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikolaos Pappas</author>
<author>Andrei Popescu-Belis</author>
</authors>
<title>Sentiment analysis of user comments for one-class collaborative filtering over TED talks.</title>
<date>2013</date>
<booktitle>In 36th ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’13,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="5397" citStr="Pappas and Popescu-Belis, 2013" startWordPosition="847" endWordPosition="850">e (2007). Our model requires only text with ratings for training, with no particular assumption on the word features to be extracted, and provides interpretable explanations of the predicted ratings through the relevance weights assigned to sentences. We also show that the model has reasonable computational demands. The model is evaluated on aspect and sentiment rating prediction over seven datasets: five of them contain reviews with aspect labels about beers, audiobooks and toys (McAuley et al., 2012), and two contain TED talks with emotion labels, and comments on them with sentiment labels (Pappas and Popescu-Belis, 2013). Our model outperforms previous MIR models and two strong linear models for rating prediction, namely SVR and Lasso by more than 10% relative in terms of MSE. The improvement is observed even when the sophistication of the feature space increases. The paper is organized as follows. Section 2 shows how our model innovates with respect to previous work on MIR and rating prediction. Section 3 formulates the problem while Section 4 describes previous MIR models. Section 5 presents our MIR model and learning procedure. Section 6 presents the datasets and evaluation methods. Section 7 reports our r</context>
<context position="23628" citStr="Pappas and Popescu-Belis (2013)" startWordPosition="4076" endWordPosition="4079">eatures d; Zhou et al. (2009) use kernels, thus their complexity is quadratic with the number of bags m; and Wang et al. (2011) have cubic time wrt. d. Our formulation is thus competitive in terms of complexity. 6 Data, Protocol and Metrics 6.1 Aspect Rating Datasets We use seven datasets summarized in Table 1. Five publicly available datasets were built for aspect prediction by McAuley et al. (2012) – BeerAdvocate, Ratebeer (ES), RateBeer (FR), Audiobooks and Toys &amp; Games – and have aspect ratings assigned by their creators on the respective websites. On the set of comments on TED talks from Pappas and Popescu-Belis (2013), we aim to predict two things: talk-level emotion dimensions assigned by viewers through voting, and comment polarity scores assigned by crowdsourcing. The distributions of aspect ratings per dataset are shown in Figure 3. Five datasets are in English, one in Spanish (Ratebeer) and one in French (RateBeer), so our results will also demonstrate the language-independence of our method. From every dataset we kept 1,200 texts as bags of sentences, but we also used three full-size datasets, namely Ratebeer ES (1,259 labeled reviews), Ratebeer FR (17,998) and Audiobooks (10,989). The features for e</context>
</contexts>
<marker>Pappas, Popescu-Belis, 2013</marker>
<rawString>Nikolaos Pappas and Andrei Popescu-Belis. 2013. Sentiment analysis of user comments for one-class collaborative filtering over TED talks. In 36th ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’13, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine learning in python.</title>
<date>2012</date>
<journal>CoRR,</journal>
<pages>1201--0490</pages>
<location>Ron Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2012</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. 2012. Scikit-learn: Machine learning in python. CoRR, abs/1201.0490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lizhen Qu</author>
<author>Georgiana Ifrim</author>
<author>Gerhard Weikum</author>
</authors>
<title>The bag-of-opinions method for review rating prediction from sparse text patterns.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>913--921</pages>
<location>Beijing, China.</location>
<contexts>
<context position="9506" citStr="Qu et al., 2010" startWordPosition="1512" endWordPosition="1515">e nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Baccianella et al., 2009; Qu et al., 2010; Zhu et al., 2012). Other studies have advocated joint modeling of multiple aspects (Snyder and Barzilay, 2007) or multiple reviews for the same product (Li et al., 2011). McAuley et al. (2012) introduced new corpora of multi-aspect reviews, which we also partly use here, and proposed models for aspect detection, sentiment summarization and rating prediction. Lastly, joint aspect identification and sentiment classification have been used for aggregating product review snippets by Sauper at al. (2013). None of the above studies considers the multipleinstance property of text in their modeling.</context>
</contexts>
<marker>Qu, Ifrim, Weikum, 2010</marker>
<rawString>Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum. 2010. The bag-of-opinions method for review rating prediction from sparse text patterns. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 913–921, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soumya Ray</author>
<author>David Page</author>
</authors>
<title>Multiple instance regression.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>425--432</pages>
<contexts>
<context position="6578" citStr="Ray and Page, 2001" startWordPosition="1037" endWordPosition="1040">tion methods. Section 7 reports our results on rating prediction tasks, and provides examples of rating explanation. 2 Related Work 2.1 Multiple-Instance Regression Multiple-instance regression (MIR) belongs to the class of multiple-instance learning (MIL) problems for real-valued output, and it is a variant of multiple regression where each data point may be described by more than one vectors of values. Many MIL studies focused on classification (Andrews et al., 2003; Bunescu and Mooney, 2007; Settles et al., 2008; Foulds and Frank, 2010; Wang et al., 2011) while fewer focused on regression (Ray and Page, 2001; Davis and others, 2007; Wagstaff et al., 2008; Wagstaff and Lane, 2007). Related to document analysis, several MIR studies have focused on news categorization (Zhang and Zhou, 2008; Zhou et al., 2009) or web-index recommendation (Zhou et al., 2005) but, to our knowledge, no study has attempted to use MIR for aspect rating prediction or sentiment analysis with real-valued labels. MIR was firstly introduced by Ray et al. (2001), proposing an EM algorithm which assumes that one primary instance per bag is responsible for its label. Wagstaff and Lane (2007) proposed to simultaneously learn a reg</context>
<context position="13319" citStr="Ray and Page, 2001" startWordPosition="2211" endWordPosition="2214">gression model f is learned over the training set made of all vectors of all bags, Dins = {(bij,yi) |j = 1, . . . ,ni; i = 1, ... , m}, assuming that there are m labeled bags. To label a new bag Bi, given that + Ω(Φ) � Y � reg. ) (1) 457 there is no representation xi, the method simply averages the predicted labels of its instances: ˆy(Bi) = mean({f(bij) |j = 1, ... , ni}) (3) Instead of the average, the median value can also be used, which is more appropriate when the bags contain outlying instances. The Prime algorithm assumes that a single instance in each bag is responsible for its label (Ray and Page, 2001). This instance is called the primary or prime one. The method is similar to the previous one, except that only one instance per bag is used as training data: Dpri = {(bpi , yi) |i = 1, ... , m}, where bpi is the prime instance of the ith bag Bi and m is the number of bags. The prime instances are discovered through an iterative algorithm which refines the regression model f. Given an initial model f, in each iteration the algorithm selects from each bag a prime candidate which is the instance with the lowest prediction error. Then, a new model is trained over the selected prime candidates, un</context>
</contexts>
<marker>Ray, Page, 2001</marker>
<rawString>Soumya Ray and David Page. 2001. Multiple instance regression. In Proceedings of the 18th International Conference on Machine Learning, ICML ’01, pages 425–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina Sauper</author>
<author>Regina Barzilay</author>
</authors>
<title>Automatic aggregation by joint modeling of aspects and values.</title>
<date>2013</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>46</volume>
<issue>1</issue>
<marker>Sauper, Barzilay, 2013</marker>
<rawString>Christina Sauper and Regina Barzilay. 2013. Automatic aggregation by joint modeling of aspects and values. Journal of Artificial Intelligence Research, 46(1):89–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina Sauper</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Incorporating content structure into text analysis applications.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>377--387</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="9396" citStr="Sauper et al., 2010" startWordPosition="1491" endWordPosition="1494">luding the use of a deep neural network (Socher et al., 2013). In contrast, we do not make any assumption about the nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Baccianella et al., 2009; Qu et al., 2010; Zhu et al., 2012). Other studies have advocated joint modeling of multiple aspects (Snyder and Barzilay, 2007) or multiple reviews for the same product (Li et al., 2011). McAuley et al. (2012) introduced new corpora of multi-aspect reviews, which we also partly use here, and proposed models for aspect detection, sentiment summarization and rating prediction. Lastly, joint aspect identification and sentiment classification have been used for aggregating product review snippets by Saup</context>
</contexts>
<marker>Sauper, Haghighi, Barzilay, 2010</marker>
<rawString>Christina Sauper, Aria Haghighi, and Regina Barzilay. 2010. Incorporating content structure into text analysis applications. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 377–387, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
<author>Mark Craven</author>
<author>Soumya Ray</author>
</authors>
<title>Multiple-instance active learning.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems, NIPS ’08,</booktitle>
<pages>1289--1296</pages>
<location>Vancouver, BC.</location>
<contexts>
<context position="6480" citStr="Settles et al., 2008" startWordPosition="1020" endWordPosition="1023"> Section 5 presents our MIR model and learning procedure. Section 6 presents the datasets and evaluation methods. Section 7 reports our results on rating prediction tasks, and provides examples of rating explanation. 2 Related Work 2.1 Multiple-Instance Regression Multiple-instance regression (MIR) belongs to the class of multiple-instance learning (MIL) problems for real-valued output, and it is a variant of multiple regression where each data point may be described by more than one vectors of values. Many MIL studies focused on classification (Andrews et al., 2003; Bunescu and Mooney, 2007; Settles et al., 2008; Foulds and Frank, 2010; Wang et al., 2011) while fewer focused on regression (Ray and Page, 2001; Davis and others, 2007; Wagstaff et al., 2008; Wagstaff and Lane, 2007). Related to document analysis, several MIR studies have focused on news categorization (Zhang and Zhou, 2008; Zhou et al., 2009) or web-index recommendation (Zhou et al., 2005) but, to our knowledge, no study has attempted to use MIR for aspect rating prediction or sentiment analysis with real-valued labels. MIR was firstly introduced by Ray et al. (2001), proposing an EM algorithm which assumes that one primary instance per</context>
</contexts>
<marker>Settles, Craven, Ray, 2008</marker>
<rawString>Burr Settles, Mark Craven, and Soumya Ray. 2008. Multiple-instance active learning. In Advances in Neural Information Processing Systems, NIPS ’08, pages 1289–1296, Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Multiple aspect ranking using the good grief algorithm. In</title>
<date>2007</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLTNAACL ’07,</booktitle>
<pages>300--307</pages>
<location>Rochester, NY, USA.</location>
<contexts>
<context position="9618" citStr="Snyder and Barzilay, 2007" startWordPosition="1529" endWordPosition="1532">ific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Baccianella et al., 2009; Qu et al., 2010; Zhu et al., 2012). Other studies have advocated joint modeling of multiple aspects (Snyder and Barzilay, 2007) or multiple reviews for the same product (Li et al., 2011). McAuley et al. (2012) introduced new corpora of multi-aspect reviews, which we also partly use here, and proposed models for aspect detection, sentiment summarization and rating prediction. Lastly, joint aspect identification and sentiment classification have been used for aggregating product review snippets by Sauper at al. (2013). None of the above studies considers the multipleinstance property of text in their modeling. 3 MIR Definition Let us consider a set B of m bags with numerical labels Y as input data D = {({b1j}dn1, y1), .</context>
</contexts>
<marker>Snyder, Barzilay, 2007</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2007. Multiple aspect ranking using the good grief algorithm. In In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLTNAACL ’07, pages 300–307, Rochester, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>151--161</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="8772" citStr="Socher et al., 2011" startWordPosition="1391" endWordPosition="1394">ning techniques for sentiment classification, e.g. Pang et al. (2002), including unsupervised techniques based on the notion of semantic orientation of phrases, e.g. Turney et al. (2002). Other studies focused on subjectivity detection, i.e. whether a 456 text span expresses opinions or not (Wiebe et al., 2004). Rating inference was defined by Pang et al. (2005) as multi-class classification or regression with respect to rating scales. Pang and Lee (2008) discusses the large range of features engineered for this task, though several recent studies focus on feature learning (Maas et al., 2011; Socher et al., 2011), including the use of a deep neural network (Socher et al., 2013). In contrast, we do not make any assumption about the nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 151–161, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’13,</booktitle>
<pages>1631--1642</pages>
<location>Portland, OR.</location>
<contexts>
<context position="8838" citStr="Socher et al., 2013" startWordPosition="1403" endWordPosition="1406">02), including unsupervised techniques based on the notion of semantic orientation of phrases, e.g. Turney et al. (2002). Other studies focused on subjectivity detection, i.e. whether a 456 text span expresses opinions or not (Wiebe et al., 2004). Rating inference was defined by Pang et al. (2005) as multi-class classification or regression with respect to rating scales. Pang and Lee (2008) discusses the large range of features engineered for this task, though several recent studies focus on feature learning (Maas et al., 2011; Socher et al., 2011), including the use of a deep neural network (Socher et al., 2013). In contrast, we do not make any assumption about the nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic fea</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’13, pages 1631–1642, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
</authors>
<title>Discovering fine-grained sentiment with latent variable structured prediction models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 33rd European Conference on Advances in Information Retrieval, ECIR’11,</booktitle>
<pages>368--374</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<marker>T¨ackstr¨om, McDonald, 2011</marker>
<rawString>Oscar T¨ackstr¨om and Ryan McDonald. 2011. Discovering fine-grained sentiment with latent variable structured prediction models. In Proceedings of the 33rd European Conference on Advances in Information Retrieval, ECIR’11, pages 368–374, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Tibshirani</author>
</authors>
<title>Regression shrinkage and selection via the lasso.</title>
<date>1996</date>
<journal>Journal of the Royal Statistical Society (Series B),</journal>
<pages>58--267</pages>
<contexts>
<context position="11700" citStr="Tibshirani, 1996" startWordPosition="1909" endWordPosition="1910">s to make assumptions to define it or compute it jointly with the regression hyperplane Φ. Thus, the main difficulty lies in finding a good assumption for X, as we will now discuss. 4 Previous MIR Assumptions We describe here three assumptions frequently made in past MIR studies, to which we will later compare our model: aggregating all instances, keeping them as separate examples, or choosing the most representative one (Wang et al., 2012). For each assumption, we will experiment with two state-of-the-art regression models (noted abstractly as f), namely SVR (Drucker et al., 1996) and Lasso (Tibshirani, 1996) with respectively the E2 and E1 norms for regularization. The Aggregated algorithm assumes that each bag is represented as a single d-dimensional vector, which is the average of its instances (hence xi E Rd). Then, a regression model f is trained on pairs of vectors and class labels, Dagg = {(xi, yi) |i = 1, ... , m}, and the predicted class of an unlabeled bag Bi = {bij |j = 1, ... , ni} is computed as follows: ˆy(Bi) = f(mean({bij |j = 1, ... , ni})) (2) In fact, a simple sum can also be used instead of the mean, and we observed in practice that with an appropriate regularization there is n</context>
</contexts>
<marker>Tibshirani, 1996</marker>
<rawString>Robert Tibshirani. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society (Series B), 58:267–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>Modeling online reviews with multi-grain topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th international conference on World Wide Web, WWW ’08,</booktitle>
<pages>111--120</pages>
<location>Beijing, China.</location>
<contexts>
<context position="9375" citStr="Titov and McDonald, 2008" startWordPosition="1487" endWordPosition="1490"> Socher et al., 2011), including the use of a deep neural network (Socher et al., 2013). In contrast, we do not make any assumption about the nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Baccianella et al., 2009; Qu et al., 2010; Zhu et al., 2012). Other studies have advocated joint modeling of multiple aspects (Snyder and Barzilay, 2007) or multiple reviews for the same product (Li et al., 2011). McAuley et al. (2012) introduced new corpora of multi-aspect reviews, which we also partly use here, and proposed models for aspect detection, sentiment summarization and rating prediction. Lastly, joint aspect identification and sentiment classification have been used for aggregating product re</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008. Modeling online reviews with multi-grain topic models. In Proceedings of the 17th international conference on World Wide Web, WWW ’08, pages 111–120, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>417--424</pages>
<location>Philadelphia, PA.</location>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 417–424, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiri L Wagstaff</author>
<author>Terran Lane</author>
</authors>
<title>Salience assignment for multiple-instance regression.</title>
<date>2007</date>
<booktitle>In ICML 2007 Workshop on Constrained Optimization and Structured Output Spaces,</booktitle>
<location>Corvallis, Oregon, USA.</location>
<contexts>
<context position="4774" citStr="Wagstaff and Lane (2007)" startWordPosition="751" endWordPosition="754">cs Figure 1: Analysis of a comment (bag of sentences {si, ..., sj}) annotated by humans with the maximal positive sentiment score (5 stars). The weights assigned by MIR reveal that sl has the greatest relevance to the overall sentiment. Using regularized least squares, we formulate an optimization objective to jointly assign instance weights and regression hyperplane weights. Then, an instance relevance estimation method is used to predict aspect ratings, or global ones, in previously unseen texts. The parameters of the model are learned using an alternating optimization procedure inspired by Wagstaff and Lane (2007). Our model requires only text with ratings for training, with no particular assumption on the word features to be extracted, and provides interpretable explanations of the predicted ratings through the relevance weights assigned to sentences. We also show that the model has reasonable computational demands. The model is evaluated on aspect and sentiment rating prediction over seven datasets: five of them contain reviews with aspect labels about beers, audiobooks and toys (McAuley et al., 2012), and two contain TED talks with emotion labels, and comments on them with sentiment labels (Pappas a</context>
<context position="6651" citStr="Wagstaff and Lane, 2007" startWordPosition="1049" endWordPosition="1052">sks, and provides examples of rating explanation. 2 Related Work 2.1 Multiple-Instance Regression Multiple-instance regression (MIR) belongs to the class of multiple-instance learning (MIL) problems for real-valued output, and it is a variant of multiple regression where each data point may be described by more than one vectors of values. Many MIL studies focused on classification (Andrews et al., 2003; Bunescu and Mooney, 2007; Settles et al., 2008; Foulds and Frank, 2010; Wang et al., 2011) while fewer focused on regression (Ray and Page, 2001; Davis and others, 2007; Wagstaff et al., 2008; Wagstaff and Lane, 2007). Related to document analysis, several MIR studies have focused on news categorization (Zhang and Zhou, 2008; Zhou et al., 2009) or web-index recommendation (Zhou et al., 2005) but, to our knowledge, no study has attempted to use MIR for aspect rating prediction or sentiment analysis with real-valued labels. MIR was firstly introduced by Ray et al. (2001), proposing an EM algorithm which assumes that one primary instance per bag is responsible for its label. Wagstaff and Lane (2007) proposed to simultaneously learn a regression model and estimate instance weights per bag for crop yield modeli</context>
<context position="14257" citStr="Wagstaff and Lane, 2007" startWordPosition="2381" endWordPosition="2384">ugh an iterative algorithm which refines the regression model f. Given an initial model f, in each iteration the algorithm selects from each bag a prime candidate which is the instance with the lowest prediction error. Then, a new model is trained over the selected prime candidates, until convergence. For a new bag, the target class is computed as in Eq. 3. 5 Proposed MIR Model We propose a new MIR model which assigns individual relevance values (weights) to each instance of a bag, thus making fewer simplifying assumptions than previous models. We extend instance-relevance algorithms such as (Wagstaff and Lane, 2007) by supporting high-dimensional feature spaces, as required for text regression, and by predicting both the class label and the content structure of previously unseen (hence unlabeled) bags. The former is achieved by minimizing a regularized least squares loss (RLS) instead of solving normal equations, which is prohibitive in large spaces. The latter represents a significant improvement over Aggregated and Instance algorithms, which are unable to pinpoint the most relevant instances with respect to the label of each bag, being thus applicable only to bag label prediction. Similarly, Prime only</context>
<context position="22925" citStr="Wagstaff and Lane (2007)" startWordPosition="3951" endWordPosition="3954">nality of the feature space (here, the size of word vectors), is derived as follows: T (h) = Tap(h) + Tf3(h) = O(m(ˆn2 + d2)) + O(mˆnd2) = O(m(ˆn2 + d2 + ˆnd2)), (9) where Tap and Tf3 are respectively the time complexity of the AP procedure and of training the f3 model. Eq. 9 shows that when nˆ « m, the model complexity is linear with the input bags m and always quadratic with the number of features d. Previous works on relevance assignment for MIR have prohibitive complexity for highdimensional feature spaces or numerous bags and hence they are not most appropriate for text regression tasks. Wagstaff and Lane (2007) have cubic time complexity with the average bag size nˆ and number of features d; Zhou et al. (2009) use kernels, thus their complexity is quadratic with the number of bags m; and Wang et al. (2011) have cubic time wrt. d. Our formulation is thus competitive in terms of complexity. 6 Data, Protocol and Metrics 6.1 Aspect Rating Datasets We use seven datasets summarized in Table 1. Five publicly available datasets were built for aspect prediction by McAuley et al. (2012) – BeerAdvocate, Ratebeer (ES), RateBeer (FR), Audiobooks and Toys &amp; Games – and have aspect ratings assigned by their creato</context>
</contexts>
<marker>Wagstaff, Lane, 2007</marker>
<rawString>Kiri L. Wagstaff and Terran Lane. 2007. Salience assignment for multiple-instance regression. In ICML 2007 Workshop on Constrained Optimization and Structured Output Spaces, Corvallis, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiri L Wagstaff</author>
<author>Terran Lane</author>
<author>Alex Roper</author>
</authors>
<title>Multiple-instance regression with structured data.</title>
<date>2008</date>
<booktitle>In Proceedings of the IEEE International Conference on Data Mining Workshops, ICDMW ’08,</booktitle>
<pages>291--300</pages>
<contexts>
<context position="6625" citStr="Wagstaff et al., 2008" startWordPosition="1045" endWordPosition="1048">on rating prediction tasks, and provides examples of rating explanation. 2 Related Work 2.1 Multiple-Instance Regression Multiple-instance regression (MIR) belongs to the class of multiple-instance learning (MIL) problems for real-valued output, and it is a variant of multiple regression where each data point may be described by more than one vectors of values. Many MIL studies focused on classification (Andrews et al., 2003; Bunescu and Mooney, 2007; Settles et al., 2008; Foulds and Frank, 2010; Wang et al., 2011) while fewer focused on regression (Ray and Page, 2001; Davis and others, 2007; Wagstaff et al., 2008; Wagstaff and Lane, 2007). Related to document analysis, several MIR studies have focused on news categorization (Zhang and Zhou, 2008; Zhou et al., 2009) or web-index recommendation (Zhou et al., 2005) but, to our knowledge, no study has attempted to use MIR for aspect rating prediction or sentiment analysis with real-valued labels. MIR was firstly introduced by Ray et al. (2001), proposing an EM algorithm which assumes that one primary instance per bag is responsible for its label. Wagstaff and Lane (2007) proposed to simultaneously learn a regression model and estimate instance weights per</context>
<context position="25284" citStr="Wagstaff et al. (2008)" startWordPosition="4347" endWordPosition="4350">els were normalized by the maximum rating in their scale, except for TED talks where the votes were normalized by the maximum number of votes over all the emotion classes for each talk, and two emotions, ‘informative’ and ‘ok’, were excluded as they are neutral ones. 6.2 Evaluation Protocol We compare the proposed model, noted APWeights, with four baseline ones – Aggregated, Instance, Prime (Section 4) and Clus460 Figure 3: Distributions of rating values per aspect rating class for the seven datasets. tering (from github.com/garydoranjr/ mcr), which is an instance relevance method proposed by Wagstaff et al. (2008) for aspect rating prediction. First, for each aspect class, we optimize all methods on a development set of 25% of the data (300 randomly selected bags). Then, we perform 5-fold cross-validation for every aspect on each entire data set and report the average error scores using the optimal hyper-parameters per method. In addition, we report for comparison the scores of AverageRating, which always predicts the average rating over the training set. We report standard error metrics for regression, namely the Mean Absolute Error (MAE) and the Mean Squared Error (MSE). The former measures the avera</context>
</contexts>
<marker>Wagstaff, Lane, Roper, 2008</marker>
<rawString>Kiri L. Wagstaff, Terran Lane, and Alex Roper. 2008. Multiple-instance regression with structured data. In Proceedings of the IEEE International Conference on Data Mining Workshops, ICDMW ’08, pages 291–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuang Wang</author>
<author>Vladan Radosavljevic</author>
<author>Bo Han</author>
<author>Zoran Obradovic</author>
<author>Slobodan Vucetic</author>
</authors>
<title>Aerosol optical depth prediction from satellite observations by multiple instance regression.</title>
<date>2008</date>
<booktitle>In Proceedings of the SIAM Int. Conf. on Data Mining, SDM ’08,</booktitle>
<pages>165--176</pages>
<location>Atlanta, GA.</location>
<contexts>
<context position="7606" citStr="Wang et al. (2008)" startWordPosition="1208" endWordPosition="1211"> by Ray et al. (2001), proposing an EM algorithm which assumes that one primary instance per bag is responsible for its label. Wagstaff and Lane (2007) proposed to simultaneously learn a regression model and estimate instance weights per bag for crop yield modeling (not applicable to prediction). A similar method which learns the internal structure of bags using clustering was proposed by Wagstaff et al. (2008) for crop yield prediction, and we will use it for comparison in the present study. Later, the method was adapted to map bags into a singleinstance feature space by Zhang et al. (2009). Wang et al. (2008) assumed that each bag is generated by random noise around a primary instance, while Wang et al. (2012) represented bag labels with a probabilistic mixture model. Foulds et al. (2010) concluded that various assumptions are differently suited to different tasks, and should be stated clearly when describing an MIR model. 2.2 Rating Prediction from Text Sentiment analysis aims at analyzing the polarity of a given text, either with classification (for discrete labels) or regression (for real-valued labels). Early studies introduced machine learning techniques for sentiment classification, e.g. Pan</context>
</contexts>
<marker>Wang, Radosavljevic, Han, Obradovic, Vucetic, 2008</marker>
<rawString>Zhuang Wang, Vladan Radosavljevic, Bo Han, Zoran Obradovic, and Slobodan Vucetic. 2008. Aerosol optical depth prediction from satellite observations by multiple instance regression. In Proceedings of the SIAM Int. Conf. on Data Mining, SDM ’08, pages 165–176, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wang</author>
<author>Feiping Nie</author>
<author>Heng Huang</author>
</authors>
<title>Learning instance specific distance for multiinstance classification.</title>
<date>2011</date>
<booktitle>In AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="6524" citStr="Wang et al., 2011" startWordPosition="1028" endWordPosition="1031"> procedure. Section 6 presents the datasets and evaluation methods. Section 7 reports our results on rating prediction tasks, and provides examples of rating explanation. 2 Related Work 2.1 Multiple-Instance Regression Multiple-instance regression (MIR) belongs to the class of multiple-instance learning (MIL) problems for real-valued output, and it is a variant of multiple regression where each data point may be described by more than one vectors of values. Many MIL studies focused on classification (Andrews et al., 2003; Bunescu and Mooney, 2007; Settles et al., 2008; Foulds and Frank, 2010; Wang et al., 2011) while fewer focused on regression (Ray and Page, 2001; Davis and others, 2007; Wagstaff et al., 2008; Wagstaff and Lane, 2007). Related to document analysis, several MIR studies have focused on news categorization (Zhang and Zhou, 2008; Zhou et al., 2009) or web-index recommendation (Zhou et al., 2005) but, to our knowledge, no study has attempted to use MIR for aspect rating prediction or sentiment analysis with real-valued labels. MIR was firstly introduced by Ray et al. (2001), proposing an EM algorithm which assumes that one primary instance per bag is responsible for its label. Wagstaff </context>
<context position="23124" citStr="Wang et al. (2011)" startWordPosition="3989" endWordPosition="3992">time complexity of the AP procedure and of training the f3 model. Eq. 9 shows that when nˆ « m, the model complexity is linear with the input bags m and always quadratic with the number of features d. Previous works on relevance assignment for MIR have prohibitive complexity for highdimensional feature spaces or numerous bags and hence they are not most appropriate for text regression tasks. Wagstaff and Lane (2007) have cubic time complexity with the average bag size nˆ and number of features d; Zhou et al. (2009) use kernels, thus their complexity is quadratic with the number of bags m; and Wang et al. (2011) have cubic time wrt. d. Our formulation is thus competitive in terms of complexity. 6 Data, Protocol and Metrics 6.1 Aspect Rating Datasets We use seven datasets summarized in Table 1. Five publicly available datasets were built for aspect prediction by McAuley et al. (2012) – BeerAdvocate, Ratebeer (ES), RateBeer (FR), Audiobooks and Toys &amp; Games – and have aspect ratings assigned by their creators on the respective websites. On the set of comments on TED talks from Pappas and Popescu-Belis (2013), we aim to predict two things: talk-level emotion dimensions assigned by viewers through voting</context>
</contexts>
<marker>Wang, Nie, Huang, 2011</marker>
<rawString>Hua Wang, Feiping Nie, and Heng Huang. 2011. Learning instance specific distance for multiinstance classification. In AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuang Wang</author>
<author>Liang Lan</author>
<author>S Vucetic</author>
</authors>
<title>Mixture model for multiple instance regression and applications in remote sensing.</title>
<date>2012</date>
<journal>IEEE Transactions on Geoscience and Remote Sensing,</journal>
<volume>50</volume>
<issue>6</issue>
<contexts>
<context position="7709" citStr="Wang et al. (2012)" startWordPosition="1227" endWordPosition="1230">ponsible for its label. Wagstaff and Lane (2007) proposed to simultaneously learn a regression model and estimate instance weights per bag for crop yield modeling (not applicable to prediction). A similar method which learns the internal structure of bags using clustering was proposed by Wagstaff et al. (2008) for crop yield prediction, and we will use it for comparison in the present study. Later, the method was adapted to map bags into a singleinstance feature space by Zhang et al. (2009). Wang et al. (2008) assumed that each bag is generated by random noise around a primary instance, while Wang et al. (2012) represented bag labels with a probabilistic mixture model. Foulds et al. (2010) concluded that various assumptions are differently suited to different tasks, and should be stated clearly when describing an MIR model. 2.2 Rating Prediction from Text Sentiment analysis aims at analyzing the polarity of a given text, either with classification (for discrete labels) or regression (for real-valued labels). Early studies introduced machine learning techniques for sentiment classification, e.g. Pang et al. (2002), including unsupervised techniques based on the notion of semantic orientation of phras</context>
<context position="11527" citStr="Wang et al., 2012" startWordPosition="1881" endWordPosition="1884">oss function L plus a regularization term Ω as follows: � Φ = arg min L(Y, X, Φ) � �� � Φ loss Since the best set of representations X for a task is generally unknown, one has to make assumptions to define it or compute it jointly with the regression hyperplane Φ. Thus, the main difficulty lies in finding a good assumption for X, as we will now discuss. 4 Previous MIR Assumptions We describe here three assumptions frequently made in past MIR studies, to which we will later compare our model: aggregating all instances, keeping them as separate examples, or choosing the most representative one (Wang et al., 2012). For each assumption, we will experiment with two state-of-the-art regression models (noted abstractly as f), namely SVR (Drucker et al., 1996) and Lasso (Tibshirani, 1996) with respectively the E2 and E1 norms for regularization. The Aggregated algorithm assumes that each bag is represented as a single d-dimensional vector, which is the average of its instances (hence xi E Rd). Then, a regression model f is trained on pairs of vectors and class labels, Dagg = {(xi, yi) |i = 1, ... , m}, and the predicted class of an unlabeled bag Bi = {bij |j = 1, ... , ni} is computed as follows: ˆy(Bi) = f</context>
</contexts>
<marker>Wang, Lan, Vucetic, 2012</marker>
<rawString>Zhuang Wang, Liang Lan, and S. Vucetic. 2012. Mixture model for multiple instance regression and applications in remote sensing. IEEE Transactions on Geoscience and Remote Sensing, 50(6):2226–2237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Rebecca Bruce</author>
<author>Matthew Bell</author>
<author>Melanie Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="8464" citStr="Wiebe et al., 2004" startWordPosition="1339" endWordPosition="1342">ed to different tasks, and should be stated clearly when describing an MIR model. 2.2 Rating Prediction from Text Sentiment analysis aims at analyzing the polarity of a given text, either with classification (for discrete labels) or regression (for real-valued labels). Early studies introduced machine learning techniques for sentiment classification, e.g. Pang et al. (2002), including unsupervised techniques based on the notion of semantic orientation of phrases, e.g. Turney et al. (2002). Other studies focused on subjectivity detection, i.e. whether a 456 text span expresses opinions or not (Wiebe et al., 2004). Rating inference was defined by Pang et al. (2005) as multi-class classification or regression with respect to rating scales. Pang and Lee (2008) discusses the large range of features engineered for this task, though several recent studies focus on feature learning (Maas et al., 2011; Socher et al., 2011), including the use of a deep neural network (Socher et al., 2013). In contrast, we do not make any assumption about the nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment ana</context>
</contexts>
<marker>Wiebe, Wilson, Bruce, Bell, Martin, 2004</marker>
<rawString>Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and Melanie Martin. 2004. Learning subjective language. Computational Linguistics, 30(3):277–308, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min-Ling Zhang</author>
<author>Zhi-Hua Zhou</author>
</authors>
<title>M3MIML: A maximum margin method for multi-instance multi-label learning.</title>
<date>2008</date>
<booktitle>In Data Mining,</booktitle>
<pages>688--697</pages>
<contexts>
<context position="6760" citStr="Zhang and Zhou, 2008" startWordPosition="1066" endWordPosition="1069">ce regression (MIR) belongs to the class of multiple-instance learning (MIL) problems for real-valued output, and it is a variant of multiple regression where each data point may be described by more than one vectors of values. Many MIL studies focused on classification (Andrews et al., 2003; Bunescu and Mooney, 2007; Settles et al., 2008; Foulds and Frank, 2010; Wang et al., 2011) while fewer focused on regression (Ray and Page, 2001; Davis and others, 2007; Wagstaff et al., 2008; Wagstaff and Lane, 2007). Related to document analysis, several MIR studies have focused on news categorization (Zhang and Zhou, 2008; Zhou et al., 2009) or web-index recommendation (Zhou et al., 2005) but, to our knowledge, no study has attempted to use MIR for aspect rating prediction or sentiment analysis with real-valued labels. MIR was firstly introduced by Ray et al. (2001), proposing an EM algorithm which assumes that one primary instance per bag is responsible for its label. Wagstaff and Lane (2007) proposed to simultaneously learn a regression model and estimate instance weights per bag for crop yield modeling (not applicable to prediction). A similar method which learns the internal structure of bags using cluster</context>
</contexts>
<marker>Zhang, Zhou, 2008</marker>
<rawString>Min-Ling Zhang and Zhi-Hua Zhou. 2008. M3MIML: A maximum margin method for multi-instance multi-label learning. In Data Mining, 2008. ICDM ’08. Eighth IEEE International Conference on, pages 688–697, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min-Ling Zhang</author>
<author>Zhi-Hua Zhou</author>
</authors>
<title>Multiinstance clustering with applications to multiinstance prediction.</title>
<date>2009</date>
<journal>Applied Intelligence,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>68</pages>
<marker>Zhang, Zhou, 2009</marker>
<rawString>Min-Ling Zhang and Zhi-Hua Zhou. 2009. Multiinstance clustering with applications to multiinstance prediction. Applied Intelligence, 31(1):47– 68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi-Hua Zhou</author>
<author>Kai Jiang</author>
<author>Ming Li</author>
</authors>
<title>Multiinstance learning based web mining.</title>
<date>2005</date>
<journal>Applied Intelligence,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="6828" citStr="Zhou et al., 2005" startWordPosition="1077" endWordPosition="1080"> (MIL) problems for real-valued output, and it is a variant of multiple regression where each data point may be described by more than one vectors of values. Many MIL studies focused on classification (Andrews et al., 2003; Bunescu and Mooney, 2007; Settles et al., 2008; Foulds and Frank, 2010; Wang et al., 2011) while fewer focused on regression (Ray and Page, 2001; Davis and others, 2007; Wagstaff et al., 2008; Wagstaff and Lane, 2007). Related to document analysis, several MIR studies have focused on news categorization (Zhang and Zhou, 2008; Zhou et al., 2009) or web-index recommendation (Zhou et al., 2005) but, to our knowledge, no study has attempted to use MIR for aspect rating prediction or sentiment analysis with real-valued labels. MIR was firstly introduced by Ray et al. (2001), proposing an EM algorithm which assumes that one primary instance per bag is responsible for its label. Wagstaff and Lane (2007) proposed to simultaneously learn a regression model and estimate instance weights per bag for crop yield modeling (not applicable to prediction). A similar method which learns the internal structure of bags using clustering was proposed by Wagstaff et al. (2008) for crop yield prediction</context>
</contexts>
<marker>Zhou, Jiang, Li, 2005</marker>
<rawString>Zhi-Hua Zhou, Kai Jiang, and Ming Li. 2005. Multiinstance learning based web mining. Applied Intelligence, 22(2):135–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi-Hua Zhou</author>
<author>Yu-Yin Sun</author>
<author>Yu-Feng Li</author>
</authors>
<title>Multi-instance learning by treating instances as noni.i.d. samples.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09,</booktitle>
<pages>1249--1256</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="6780" citStr="Zhou et al., 2009" startWordPosition="1070" endWordPosition="1073">longs to the class of multiple-instance learning (MIL) problems for real-valued output, and it is a variant of multiple regression where each data point may be described by more than one vectors of values. Many MIL studies focused on classification (Andrews et al., 2003; Bunescu and Mooney, 2007; Settles et al., 2008; Foulds and Frank, 2010; Wang et al., 2011) while fewer focused on regression (Ray and Page, 2001; Davis and others, 2007; Wagstaff et al., 2008; Wagstaff and Lane, 2007). Related to document analysis, several MIR studies have focused on news categorization (Zhang and Zhou, 2008; Zhou et al., 2009) or web-index recommendation (Zhou et al., 2005) but, to our knowledge, no study has attempted to use MIR for aspect rating prediction or sentiment analysis with real-valued labels. MIR was firstly introduced by Ray et al. (2001), proposing an EM algorithm which assumes that one primary instance per bag is responsible for its label. Wagstaff and Lane (2007) proposed to simultaneously learn a regression model and estimate instance weights per bag for crop yield modeling (not applicable to prediction). A similar method which learns the internal structure of bags using clustering was proposed by </context>
<context position="23026" citStr="Zhou et al. (2009)" startWordPosition="3970" endWordPosition="3973">) = O(m(ˆn2 + d2)) + O(mˆnd2) = O(m(ˆn2 + d2 + ˆnd2)), (9) where Tap and Tf3 are respectively the time complexity of the AP procedure and of training the f3 model. Eq. 9 shows that when nˆ « m, the model complexity is linear with the input bags m and always quadratic with the number of features d. Previous works on relevance assignment for MIR have prohibitive complexity for highdimensional feature spaces or numerous bags and hence they are not most appropriate for text regression tasks. Wagstaff and Lane (2007) have cubic time complexity with the average bag size nˆ and number of features d; Zhou et al. (2009) use kernels, thus their complexity is quadratic with the number of bags m; and Wang et al. (2011) have cubic time wrt. d. Our formulation is thus competitive in terms of complexity. 6 Data, Protocol and Metrics 6.1 Aspect Rating Datasets We use seven datasets summarized in Table 1. Five publicly available datasets were built for aspect prediction by McAuley et al. (2012) – BeerAdvocate, Ratebeer (ES), RateBeer (FR), Audiobooks and Toys &amp; Games – and have aspect ratings assigned by their creators on the respective websites. On the set of comments on TED talks from Pappas and Popescu-Belis (201</context>
</contexts>
<marker>Zhou, Sun, Li, 2009</marker>
<rawString>Zhi-Hua Zhou, Yu-Yin Sun, and Yu-Feng Li. 2009. Multi-instance learning by treating instances as noni.i.d. samples. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pages 1249–1256, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingbo Zhu</author>
<author>Chunliang Zhang</author>
<author>Matthew Y Ma</author>
</authors>
<title>Multi-aspect rating inference with aspectbased segmentation.</title>
<date>2012</date>
<journal>IEEE Trans. on Affective Computing,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="9525" citStr="Zhu et al., 2012" startWordPosition="1516" endWordPosition="1519">sionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Baccianella et al., 2009; Qu et al., 2010; Zhu et al., 2012). Other studies have advocated joint modeling of multiple aspects (Snyder and Barzilay, 2007) or multiple reviews for the same product (Li et al., 2011). McAuley et al. (2012) introduced new corpora of multi-aspect reviews, which we also partly use here, and proposed models for aspect detection, sentiment summarization and rating prediction. Lastly, joint aspect identification and sentiment classification have been used for aggregating product review snippets by Sauper at al. (2013). None of the above studies considers the multipleinstance property of text in their modeling. 3 MIR Definition L</context>
</contexts>
<marker>Zhu, Zhang, Ma, 2012</marker>
<rawString>Jingbo Zhu, Chunliang Zhang, and Matthew Y. Ma. 2012. Multi-aspect rating inference with aspectbased segmentation. IEEE Trans. on Affective Computing, 3(4):469–481.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Zhuang</author>
<author>Feng Jing</author>
<author>Xiao-Yan Zhu</author>
</authors>
<title>Movie review mining and summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th ACM International Conference on Information and Knowledge Management, CIKM ’06,</booktitle>
<pages>43--50</pages>
<location>Arlington, VA.</location>
<contexts>
<context position="9211" citStr="Zhuang et al., 2006" startWordPosition="1460" endWordPosition="1463"> Pang and Lee (2008) discusses the large range of features engineered for this task, though several recent studies focus on feature learning (Maas et al., 2011; Socher et al., 2011), including the use of a deep neural network (Socher et al., 2013). In contrast, we do not make any assumption about the nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Baccianella et al., 2009; Qu et al., 2010; Zhu et al., 2012). Other studies have advocated joint modeling of multiple aspects (Snyder and Barzilay, 2007) or multiple reviews for the same product (Li et al., 2011). McAuley et al. (2012) introduced new corpora of multi-aspect reviews, which we also partly use here, and proposed models for aspect </context>
</contexts>
<marker>Zhuang, Jing, Zhu, 2006</marker>
<rawString>Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie review mining and summarization. In Proceedings of the 15th ACM International Conference on Information and Knowledge Management, CIKM ’06, pages 43–50, Arlington, VA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
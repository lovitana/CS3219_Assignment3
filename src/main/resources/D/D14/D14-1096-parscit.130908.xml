<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9783415">
What Can We Get From 1000 Tokens?
A Case Study of Multilingual POS Tagging For Resource-Poor Languages
</title>
<author confidence="0.997483">
Long Duong,12 Trevor Cohn,1 Karin Verspoor,1 Steven Bird,1 and Paul Cook1
</author>
<affiliation confidence="0.965107666666667">
1Department of Computing and Information Systems,
The University of Melbourne
2National ICT Australia, Victoria Research Laboratory
</affiliation>
<email confidence="0.945695">
lduong@student.unimelb.edu.au
{t.cohn, karin.verspoor, sbird, paulcook}@unimelb.edu.au
</email>
<sectionHeader confidence="0.994627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999196055555556">
In this paper we address the problem
of multilingual part-of-speech tagging for
resource-poor languages. We use par-
allel data to transfer part-of-speech in-
formation from resource-rich to resource-
poor languages. Additionally, we use a
small amount of annotated data to learn to
“correct” errors from projected approach
such as tagset mismatch between lan-
guages, achieving state-of-the-art perfor-
mance (91.3%) across 8 languages. Our
approach is based on modest data require-
ments, and uses minimum divergence clas-
sification. For situations where no uni-
versal tagset mapping is available, we
propose an alternate method, resulting
in state-of-the-art 85.6% accuracy on the
resource-poor language Malagasy.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999943416666667">
Part-of-speech (POS) tagging is a crucial task for
natural language processing (NLP) tasks, provid-
ing basic information about syntax. Supervised
POS tagging has achieved great success, reach-
ing as high as 95% accuracy for many languages
(Petrov et al., 2012). However, supervised tech-
niques need manually annotated data, and this
is either lacking or limited in most resource-
poor languages. Fully unsupervised POS tagging
is not yet useful in practice due to low accu-
racy (Christodoulopoulos et al., 2010). In this pa-
per, we propose a semi-supervised method to nar-
row the gap between supervised and unsupervised
approaches. We demonstrate that even a small
amount of supervised data leads to substantial im-
provement.
Our method is motivated by the availability of
parallel data. Thanks to the development of mul-
tilingual documents from government projects,
book translations, multilingual websites, and so
forth, parallel data between resource-rich and
resource-poor languages is relatively easy to ac-
quire. This parallel data provides the bridge that
permits us to transfer POS information from a
resource-rich to a resource-poor language.
Systems that make use of cross-lingual tag
projection typically face several issues, includ-
ing mismatches between the tagsets used for the
languages, artifacts from noisy alignments and
cross-lingual syntactic divergence. Our approach
compensates for these issues by training on a
small amount of annotated data on the target side,
demonstrating that only 1k tokens of annotated
data is sufficient to improve performance.
We first tag the resource-rich language using a
supervised POS tagger. We then project POS tags
from the resource-rich language to the resource-
poor language using parallel word alignments.
The projected labels are noisy, and so we use
various heuristics to select only “good” training
examples. We train the model in two stages.
First, we build a maximum entropy classifier T
on the (noisy) projected data. Next, we train
a supervised classifier P on a small amount of
annotated data (1,000 tokens) in the target lan-
guage, using a minimum divergence technique
to incorporate the first model, T. Compared
with the state of the art (T¨ackstr¨om et al., 2013),
we make more-realistic assumptions (e.g. relying
on a tiny amount of annotated data rather than
a huge crowd-sourced dictionary) and use less
parallel data, yet achieve a better overall result.
We achieved 91.3% average accuracy over 8 lan-
guages, exceeding T¨ackstr¨om et al. (2013)’s result
of 88.8%.
The test data we employ makes use of map-
pings from language-specific POS tag inventories
to a universal tagset (Petrov et al., 2012). How-
ever, such a mapping might not be available for
resource-poor languages. Therefore, we also pro-
</bodyText>
<page confidence="0.977538">
886
</page>
<note confidence="0.910507">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 886–897,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999741375">
pose a variant of our method which removes the
need for identical tagsets between the projection
model T and the correction model P, based on
a two-output maximum entropy model over tag
pairs. Evaluating on the resource-poor language
Malagasy, we achieved 85.6% accuracy, exceed-
ing the state-of-the-art of 81.2% (Garrette et al.,
2013).
</bodyText>
<sectionHeader confidence="0.919176" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<bodyText confidence="0.999977164835165">
There is a wealth of prior work on multilingual
POS tagging. The simplest approach takes advan-
tage of the typological similarities that exist be-
tween languages pairs such as Czech and Russian,
or Serbian and Croatian. They build the tagger
— or estimate part of the tagger — on one lan-
guage and apply it to the other language (Reddy
and Sharoff, 2011, Hana et al., 2004).
Yarowsky and Ngai (2001) pioneered the use of
parallel data for projecting tag information from
a resource-rich language to a resource-poor lan-
guage. Duong et al. (2013b) used a similar method
on using sentence alignment scores to rank the
goodness of sentences. They trained a seed model
from a small part of the data, then applied this
model to the rest of the data using self-training
with revision.
Das and Petrov (2011) also used parallel data
but additionally exploited graph-based label prop-
agation to expand the coverage of labelled tokens.
Each node in the graph represents a trigram in the
target language. Each edge connects two nodes
which have similar context. Originally, only some
nodes received a label from direct label projection,
and then labels were propagated to the rest of the
graph. They only extracted the dictionary from
the graph because the labels of nodes are noisy.
They used the dictionary as the constraints for a
feature-based HMM tagger (Berg-Kirkpatrick et
al., 2010). Both Duong et al. (2013b) and Das and
Petrov (2011) achieved 83.4% accuracy on the test
set of 8 European languages.
Goldberg et al. (2008) pointed out that, with the
presence of a dictionary, even an incomplete one,
a modest POS tagger can be built using simple
methods such as expectation maximization. This
is because most of the time, words have a very
limited number of possible tags, thus a dictionary
that specifies the allowable tags for a word helps
to restrict the search space. With a gold-standard
dictionary, Das and Petrov (2011) achieved an ac-
curacy of approximately 94% on the same 8 lan-
guages. The effectiveness of a gold-standard dic-
tionary is undeniable, however it is costly to build
one, especially for resource-poor languages. Li et
al. (2012) used the dictionary from Wiktionary,1 a
crowd-sourced dictionary. They scored 84.8% ac-
curacy on the same 8 languages. Currently, Wik-
tionary covers over 170 languages, but the cov-
erage varies substantially between languages and,
unsurprisingly, it is poor for resource-poor lan-
guages. Therefore, relying on Wiktionary is not
effective for building POS taggers for resource-
poor languages.
T¨ackstr¨om et al. (2013) combined both token
information (from direct projected data) and type
constraints (from Wiktionary’s dictionary) to form
the state-of-the-art multilingual tagger. They built
a tag lattice and used these token and type con-
straints to prune it. The remaining paths are the
training data for a CRF tagger. They achieved
88.8% accuracy on the same 8 languages.
Table 1 summarises the performance of the
above models across all 8 languages. Note that
these methods vary in their reliance on external
resources. Duong et al. (2013b) use the least, i.e.
only the Europarl Corpus (Koehn, 2005). Das and
Petrov (2011) additionally use the United Nation
Parallel Corpus. Li et al. (2012) didn’t use any par-
allel text but used Wiktionary instead. T¨ackstr¨om
et al. (2013) exploited more parallel data than Das
and Petrov (2011) and also used a dictionary
from Li et al. (2012).
Another approach for resource-poor languages
is based on the availability of a small amount
of annotated data. Garrette et al. (2013) built a
POS tagger for Kinyarwanda and Malagasy. They
didn’t use parallel data but instead exploited four
hours of manual annotation to build ∼4,000 tokens
or ∼3,000 word-types of annotated data. These
tokens or word-types were used to build a tag dic-
tionary. They employed label propagation for ex-
panding the coverage of this dictionary in a sim-
ilar vein to Das and Petrov (2011), but they also
used an external dictionary. They built training
examples using the combined dictionary and then
trained the tagger on this data. They achieved
81.9% and 81.2% accuracy for Kinyarwanda and
Malagasy respectively. Note that their usage of an
external dictionary compromises their claim of us-
ing only 4 hours of annotation.
</bodyText>
<footnote confidence="0.993083">
1http://www.wiktionary.org/
</footnote>
<page confidence="0.994931">
887
</page>
<note confidence="0.7351676">
da nl de el it pt es sv Average
Das and Petrov (2011) 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4
Duong et al. (2013b) 85.6 84.0 85.4 80.4 81.4 86.3 83.3 81.0 83.4
Li et al. (2012) 83.3 86.3 85.4 79.2 86.5 84.5 86.4 86.1 84.8
T¨ackstr¨om et al. (2013) 88.2 85.9 90.5 89.5 89.3 91.0 87.1 88.9 88.8
</note>
<tableCaption confidence="0.728277666666667">
Table 1: Previously published token-level POS tagging accuracy for various models across 8 languages
— Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es), Swedish
(sv) — evaluated on CoNLL data (Buchholz and Marsi, 2006).
</tableCaption>
<bodyText confidence="0.999910333333333">
The method we propose in this paper is similar
in only using a small amount of annotation. How-
ever, we directly use the annotated data to train
the model rather than using a dictionary. We argue
that with a proper “guide”, we can take advantage
of very limited annotated data.
</bodyText>
<subsectionHeader confidence="0.994059">
2.1 Annotated data
</subsectionHeader>
<bodyText confidence="0.9999882">
Our annotated data mainly comes from CoNLL
shared tasks on dependency parsing (Buchholz
and Marsi, 2006). The language specific tagsets
are mapped into the universal tagset. We will
use this annotated data mainly for evaluation. Ta-
ble 2 shows the size of annotated data for each
language. The 8 languages we are considering
in this experiment are not actually resource-poor
languages. However, running on these 8 lan-
guages makes our system comparable with pre-
viously proposed methods. Nevertheless, we try
to use as few resources as possible, in order to
simulate the situation for resource-poor languages.
Later in Section 6 we adapt the approach for Mala-
gasy, a truly resource-poor language.
</bodyText>
<subsectionHeader confidence="0.999572">
2.2 Universal tagset
</subsectionHeader>
<bodyText confidence="0.999982394736842">
We employ the universal tagset from (Petrov et
al., 2012) for our experiment. It consists of 12
common tags: NOUN, VERB, ADJ (adjective),
ADV (adverb), PRON (pronoun), DET (deter-
miner and article), ADP (preposition and post-
position), CONJ (conjunctions), NUM (numeri-
cal), PRT (particle), PUNC (punctuation) and X
(all other categories including foreign words and
abbreviations). Petrov et al. (2012) provide the
mapping from each language-specific tagset to the
universal tagset.
The idea of using the universal tagset is of great
use in multilingual applications, enabling compar-
ison across languages. However, the mapping is
not always straightforward. Table 2 shows the size
of the annotated data for each language, the num-
ber of tags presented in the data, and the list of
tags that are not matched. We can see that only 8
tags are presented in the annotated data for Dan-
ish, i.e, 4 tags (DET, PRT, PUNC, and NUM) are
missing.2 Thus, a classifier using all 12 tags will
be heavily penalized in the evaluation.
Li et al. (2012) considered this problem and
tried to manually modify the Danish mappings.
Moreover, PRT is not really a universal tag since
it only appears in 3 out of the 8 languages. Plank
et al. (2014) pointed out that PRT often gets con-
fused with ADP even in English. We will later
show that the mapping problem causes substantial
degradation in the performance of a POS tagger
exploiting parallel data. The method we present
here is more target-language oriented: our model
is trained on the target language, in this way, only
relevant information from the source language is
retained. Thus, we automatically correct the map-
ping, and other incompatibilities arising from in-
correct alignments and syntactic divergence be-
tween the source and target languages.
</bodyText>
<equation confidence="0.936286">
Lang Size(k) # Tags Not Matched
da 94 8 DET, PRT, PUNC, NUM
nl 203 11 PRT
de 712 12
el 70 12
it 76 11 PRT
pt 207 11 PRT
es 89 11 PRT
sv 191 11 DET
AVG 205
</equation>
<tableCaption confidence="0.687138">
Table 2: The size of annotated data from
</tableCaption>
<bodyText confidence="0.730146">
CoNLL (Buchholz and Marsi, 2006), and the
number of tags included and missing for 8 lan-
guages.
</bodyText>
<footnote confidence="0.993598333333333">
2Many of these are mistakes in the mapping, however,
they are indicative of the kinds of issues expected in low-
resource languages.
</footnote>
<page confidence="0.996466">
888
</page>
<sectionHeader confidence="0.997906" genericHeader="method">
3 Directly Projected Model (DPM)
</sectionHeader>
<bodyText confidence="0.999927333333333">
In this section we describe a maximum entropy
tagger that only uses information from directly
projected data.
</bodyText>
<subsectionHeader confidence="0.999798">
3.1 Parallel data
</subsectionHeader>
<bodyText confidence="0.999992578947369">
We first collect Europarl data having English as
the source language, an average of 1.85 million
parallel sentences for each of the 8 language pairs.
In terms of parallel data, we use far less data com-
pared with other recent work. Das and Petrov
(2011) used Europarl and the ODS United Na-
tion dataset, while T¨ackstr¨om et al. (2013) addi-
tionally used parallel data crawled from the web.
The amount of parallel data is crucial for align-
ment quality. Since DPM uses alignments to trans-
fer tags from source to target language, the per-
formance of DPM (and other models that exploit
projection) largely depends on the quantity of par-
allel data. The “No LP” model of Das and Petrov
(2011), which only uses directly projected labels
(without label propagation), scored 81.3% for 8
languages. However, using the same model but
with more parallel data, T¨ackstr¨om et al. (2013)
scored 84.9% on the same test set.
</bodyText>
<subsectionHeader confidence="0.999902">
3.2 Label projection
</subsectionHeader>
<bodyText confidence="0.999979547619048">
We use the standard alignment tool Giza++ (Och
and Ney, 2003) to word align the parallel data. We
employ the Stanford POS tagger (Toutanova et al.,
2003) to tag the English side of the parallel data
and then project the label to the target side. It has
been confirmed in many studies (T¨ackstr¨om et al.,
2013, Das and Petrov, 2011, Toutanova and John-
son, 2008) that directly projected labels are noisy.
Thus we need a method to reduce the noise. We
employ the strategy of Yarowsky and Ngai (2001)
of ranking sentences using a their alignment scores
from IBM model 3.
Firstly, we want to know how noisy the pro-
jected data is. Thus, we use the test data to build
a simple supervised POS tagger using the TnT
tagger (Brants, 2000) which employs a second-
order Hidden Markov Model (HMM). We tag the
projected data and compare the label from direct
projection and from the TnT tagger. The labels
from the TnT Tagger are considered as pseudo-
gold labels. Column “Without Mapping” from Ta-
ble 3 shows the average accuracy for the first n-
sentences (n = 60k, 100k, 200k, 500k) for 8 lan-
guages according to the ranking. Column “Cov-
erage” shows the percentages of projected label
(the other tokens are Null aligned). We can see
that when we select more data, both coverage and
accuracy fall. In other words, using the sentence
alignment score, we can rank sentences with high
coverage and accuracy first. However, even after
ranking, the accuracy of projected labels is less
than 80% demonstrating how noisy the projected
labels are.
Table 3 (column “With Mapping”) additionally
shows the accuracy using simple tagset mapping,
i.e. mapping each tag to the tag it is assigned most
frequently in the test data. For example DET, PRT,
PUNC, NUM, missing from Danish gold data, will
be matched to PRON, X, X, ADJ respectively. This
simple matching yields a ∼ 4% (absolute) im-
provement in average accuracy. This illustrates the
importance of handling tagset mapping carefully.
</bodyText>
<subsectionHeader confidence="0.995943">
3.3 The model
</subsectionHeader>
<bodyText confidence="0.999783">
In this section, we introduce a maximum entropy
tagger exploiting the projected data. We select the
first 200k sentences from Table 3 for this experi-
ment. This number represents a trade-off between
size and accuracy. More sentences provide more
information but at the cost of noisier data. Duong
et al. (2013b) also used sentence alignment scores
to rank sentences. Their model stabilizes after us-
ing 200k sentences. We conclude that 200k sen-
tences is enough and capture most information
from the parallel data.
</bodyText>
<table confidence="0.748584777777778">
Features Descriptions
W@-1 Previous word
W@+1 Next word
W@0 Current word
CAP First character is capitalized
NUMBER Is number
PUNCT Is punctuation
SUFFIX@k Suffix up to length 3 (k &lt;= 3)
WC Word class
</table>
<tableCaption confidence="0.686924">
Table 4: Feature template for a maximum entropy
tagger
</tableCaption>
<bodyText confidence="0.999791571428571">
We ignore tokens that don’t have labels, which
arise from null alignments and constitute approxi-
mately 14% of the data. The remaining data (∼1.4
million tokens) are used to train a maximum en-
tropy (MaxEnt) model. MaxEnt is one of the
simplest forms of probabilistic classifier, and is
appropriate in this setting due to the incomplete
</bodyText>
<page confidence="0.996194">
889
</page>
<table confidence="0.9996442">
Data Size (k) Coverage (%) Without Mapping With Mapping
60 91.5 79.9 84.2
100 89.1 79.4 83.6
200 86.1 79.1 82.9
500 82.4 78.0 81.5
</table>
<tableCaption confidence="0.992803">
Table 3: The coverage, and POS tagging accuracy with and without tagset mapping of directly projected
labels, averaged over 8 languages for different data sizes
</tableCaption>
<table confidence="0.999527833333334">
Model da nl de el it pt es sv Avg
All features 64.4 83.3 86.3 79.7 82.0 86.5 82.5 76.5 80.2
- Word Class 64.7 82.6 86.6 79.0 82.8 84.6 82.2 76.9 79.9
- Suffix 64.0 82.8 86.3 78.1 81.0 85.9 82.3 76.2 79.6
- Prev, Next Word 62.6 82.5 87.4 79.0 81.9 86.5 82.2 74.8 79.6
- Cap, Num, Punct 64.0 81.9 84.0 78.0 79.1 86.3 81.8 75.6 78.8
</table>
<tableCaption confidence="0.8455195">
Table 5: The accuracy of Directed Project Model (DPM) with different feature sets, removing one feature
set at a time
</tableCaption>
<bodyText confidence="0.998386444444444">
sequence data. While sequence models such as
HMMs or CRFs can provide more accurate mod-
els of label sequences, they impose a more strin-
gent training requirement.3 We also experimented
with a first-order linear chain CRF trained on con-
tiguous sub-sequences but observed ∼ 4% (abso-
lute) drop in performance.
The maximum entropy classifier estimates the
probability of tag t given a word w as
</bodyText>
<equation confidence="0.98855925">
D
1
P(t|w) = Z(w) exp X Ajfj(w, t) ,
j=1
</equation>
<bodyText confidence="0.984856108695652">
where Z(w) = Pt exp PDj=1 Ajfj(w, t) is the
normalization factor to ensure the probabilities
P(t|w) sum to one. Here fj is a feature function
and Aj is the weight for this feature, learned as
part of training. We use Maximum A Posteriori
(MAP) estimation to maximize the log likelihood
of the training data, D = {wi, ti}Ni=1, subject to a
zero-mean Gaussian regularisation term,
where the regularisation term limits over-fitting,
an important concern when using large feature
3T¨ackstr¨om et al. (2013) train a CRF on incomplete data,
using a tag dictionary heuristic to define a ‘gold standard’
lattice over label sequences.
sets. For our experiments we set S2 = 1. We use
L-BFGS which performs gradient ascent to maxi-
mize L. Table 4 shows the features we considered
for building the DPM. We use mkcls, an unsu-
pervised method for word class induction which is
widely used in machine translation (Och, 1999).
We run mkcls to obtain 100 word classes, using
only the target language side of the parallel data.
Table 5 shows the accuracy of the DPM evalu-
ated on 8 languages (“All features model”). DPM
performs poorly on Danish, probably because of
the tagset mapping issue discussed above. The
DPM result of 80.2% accuracy is encouraging,
particularly because the model had no explicit su-
pervision.
To see what features are meaningful for our
model, we remove features in turn and report
the result. The result in Table 5 disagrees with
T¨ackstr¨om et al. (2013) on the word class features.
They reported a gain of approximately 3% (ab-
solute) using the word class. However, it seems
to us that these features are not especially mean-
ingful (at least in the present setting). Possible
reasons for the discrepancy are that they train the
word class model on a massive quantity of exter-
nal monolingual data, or their algorithms for word
clustering are better (Uszkoreit and Brants, 2008).
We can see that the most informative features are
Capitalization, Number and Punctuation. This
makes sense because in languages such as Ger-
man, capitalization is a strong indicator of NOUN.
Number and punctuation features ensure that we
classify NUM and PUNCT tags correctly.
</bodyText>
<equation confidence="0.993393">
L = log P(A) YN P(t(i)|w(i))
i=1
= XD A2j + XN XD Ajfj(wi, ti) − log Z(wi)
j=1 2S2 i=1 j=1
</equation>
<page confidence="0.991478">
890
</page>
<sectionHeader confidence="0.99614" genericHeader="method">
4 Correction Model
</sectionHeader>
<bodyText confidence="0.999994021739131">
In this section we incorporate the directly pro-
jected model into a second correction model
trained on a small supervised sample of 1,000 an-
notated tokens. Our DPM model is not very accu-
rate; as we have discussed it makes many errors,
due to invalid or inconsistent tag mappings, noisy
alignments, and cross-linguistic syntactic diver-
gence. However, our aim is to see how effectively
we can exploit the strengths of the DPM model
while correcting for its inadequacies using direct
supervision. We select only 1,000 annotated to-
kens to reflect a low resource scenario. A small
supervised training sample is a more realistic form
of supervision than a tag dictionary (noisy or oth-
erwise). Although used in most prior work, a tag
dictionary for a new language requires significant
manual effort to construct. Garrette and Baldridge
(2013) showed that a 1,000 token dataset could be
collected very cheaply, requiring less than 2 hours
of non-expert time.
Our correction model makes use of a mini-
mum divergence (MD) model (Berger et al., 1996),
a variant of the maximum entropy model which
biases the target distribution to be similar to a
static reference distribution. The method has been
used in several language applications including
machine translation (Foster, 2000) and parsing
(Plank and van Noord, 2008, Johnson and Riezler,
2000). These previous approaches have used var-
ious sources of reference distribution, e.g., incor-
porating information from a simpler model (John-
son and Riezler, 2000) or combining in- and out-
of-domain models (Plank and van Noord, 2008).
Plank and van Noord (2008) concluded that this
method for adding prior knowledge only works
with high quality reference distributions, other-
wise performance suffers.
In contrast to these previous approaches, we
consider the specific setting where both the
learned model and the reference model so =
P(t|w) are both maximum entropy models. In this
case we show that the MD setup can be simplified
to a regularization term, namely a Gaussian prior
with a non-zero mean. We model the classification
probability, P&apos;(t|w) as the product between a base
model and a maximum entropy classifier,
</bodyText>
<equation confidence="0.4609335">
P&apos;(t|w) a P(t|w) exp XD -yjfj(w, t)
j=1
</equation>
<bodyText confidence="0.99951575">
where here we use the DPM model as base model
P(t|w). Under this setup, where P&apos; uses the same
features as P, and both are log-linear models, this
simplifies to
</bodyText>
<equation confidence="0.96168475">
⎛D D
P&apos;(t|w) a expXAj fj (w, t) + X -yjfj(w, t)
j=1 j=1
(Aj + -yj) fj(w, t) (1)
</equation>
<bodyText confidence="0.999513615384615">
where the constant of proportionality is Z&apos;(w) =
Pt exp PDj=1 (Aj + -yj) fj(w, t). It is clear that
Equation (1) also defines a maximum entropy clas-
sifier, with parameters αj = Aj + -yj, and conse-
quently this might seem to be a pointless exercise.
The utility of this approach arises from the prior:
MAP training with a zero mean Gaussian prior
over -y is equivalent to a Gaussian prior over the
aggregate weights, αj — N(Aj, u2). This prior
enforces parameter sharing between the two mod-
els by penalising parameter divergence from the
underlying DPM model A. The resulting training
objective is
</bodyText>
<equation confidence="0.976926">
XD
Lcorr = log P(t|w, α) —
1
2u2
j=1
</equation>
<bodyText confidence="0.7727834">
which can be easily optimised using standard
gradient-based methods, e.g., L-BFGS. The con-
tribution of the regulariser is scaled by the constant
1
2σ2.
</bodyText>
<subsectionHeader confidence="0.971333">
4.1 Regulariser sensitivity
</subsectionHeader>
<bodyText confidence="0.999866611111111">
Careful tuning of the regularisation term u2 is crit-
ical for the correction model, both to limit over-
fitting on the very small training sample of 1,000
tokens, and to control the extent of the influence
of the DPM model over the correction model.
A larger value of u2 lessens the reliance on the
DPM and allows for more flexible modelling of
the training set, while a small value of u2 forces
the parameters to be close to the DPM estimates at
the expense of data fit. We expect the best value
to be somewhere between these extremes, and use
line-search to find the optimal value for u2. For
this purpose, we hold out 100 tokens from the
1,000 instance training set, for use as our devel-
opment set for hyper-parameter selection.
From Figure 1, we can see that the model per-
forms poorly on small values of u2. This is under-
standable because the small u2 makes the model
</bodyText>
<figure confidence="0.971723098039216">
XD
j=1
a exp
(αj — Aj)2
891
80 84 88
Accuracy (% )
65 75 85 95
Accuracy (%)
Data Size
● ● ● ●
● ●
● ● ●
● Average Acc
●
●
●
●
●
●
●
●
●
●
●
●
●Correction Model
Supervised Model
Variance
0.01
0.1
1
10
70
100
1000
10000
1e+05
1e+06
1e+07
1 00
300
500
700
i 1000
1500
2000
5000
10000
15000
50000
</figure>
<figureCaption confidence="0.777491666666667">
Figure 1: Sensitivity of regularisation parameter
Q2 against the average accuracy measured on 8
languages on the development set
</figureCaption>
<bodyText confidence="0.929498333333333">
too similar to DPM, which is not very accurate
(80.2%). At the other extreme, if Q2 is large, the
DPM model is ignored, and the correction model
is equivalent with the supervised model (∼ 88%
accuracy). We select the value of Q2 = 70, which
maximizes the accuracy on the development set.
</bodyText>
<subsectionHeader confidence="0.954054">
4.2 The model
</subsectionHeader>
<bodyText confidence="0.975954472222222">
Using the value of Q2 = 70, we retrain the model
on the whole 1,000-token training set and evalu-
ate the model on the rest of the annotated data.
Table 6 shows the performance of DPM, Super-
vised model, Correction model and the state-of-
the-art model (T¨ackstr¨om et al., 2013). The super-
vised model trains a maximum entropy tagger us-
ing the same features as in Table 4 on this 1000 to-
kens. The only difference between the supervised
model and the correction model is that in the cor-
rection model we additionally incorporate DPM as
the prior.
The supervised model performs surprisingly
well confirming that our features are meaning-
ful in distinguishing between tags. This model
achieves high accuracy on Danish compared with
other languages probably because Danish is eas-
ier to learn since it contains only 8 tags. Despite
the fact that the DPM is not very accurate, the cor-
rection model consistently outperforms the super-
vised model on all considered languages, approx-
imately 4.3% (absolute) better on average. This
shows that our method of incorporating DPM to
the model is efficient and robust.
The correction model performs much bet-
ter than the state-of-the-art for 7 languages but
Figure 2: Learning curve for correction model and
supervised model: the x-axis is the size of data
(number of tokens); the y-axis is the average ac-
curacy measured on 8 languages; the dashed line
shows the data condition reported in Table 6
slightly worse for 1 language. On average we
achieve 91.3% accuracy compared with 88.8%
for the state-of-the-art, an error rate reduction of
22.3%. This is despite using fewer resources and
only modest supervision.
</bodyText>
<sectionHeader confidence="0.988832" genericHeader="method">
5 Analysis
</sectionHeader>
<bodyText confidence="0.999894076923077">
Tagset mismatch In the correction model, we
implicitly resolve the mismatched tagset issue.
DPM might contain tags that don’t appear in the
target language or generally are errors in the map-
ping. However, when incorporating DPM into the
correction model, only the feature weight of tags
that appear in the target language are retained. In
general, because we don’t explicitly do any map-
ping between languages, we might have trouble if
the tagset size of the target language is bigger than
the source language tagset. However, this is not
the case for our experiment because we choose En-
glish as the source-side and English has the full 12
tags.
Learning curve We investigate the impact of
the number of available annotated tokens on the
correction model. Figure 2 shows the learning
curve of the correction model and the supervised
model. We can clearly see the differences be-
tween 2 models when the size of training data is
small. For example, at 100 tokens, the difference
is very large, approximately 18% (absolute), it is
also 6% (absolute) better than DPM. This differ-
ence diminishes as we add more data. This make
sense because when we add more data, the super-
vised model become stronger, while the effective-
</bodyText>
<page confidence="0.996713">
892
</page>
<table confidence="0.999447142857143">
Model da nl de el it pt es sv Avg
DPM 64.4 83.3 86.3 79.7 82.0 86.5 82.5 76.5 80.2
T¨ackstr¨om et al. (2013) 88.2 85.9 90.5 89.5 89.3 91.0 87.1 88.9 88.8
Supervised model 90.1 84.6 89.6 88.2 81.4 87.6 88.9 85.4 87.0
Correction Model 92.1 91.1 92.5 92.1 89.9 92.5 91.6 88.7 91.3
DPM (with dict) 65.2 83.9 87.0 79.1 83.5 87.1 83.0 77.5 80.8
Correction Model (with dict) 93.3 92.2 93.7 93.2 92.2 93.1 92.8 90.0 92.6
</table>
<tableCaption confidence="0.679825333333333">
Table 6: The comparison of our Directly Projected Model, Supervised Model, Correction Model and the
state-of-the-art system (T¨ackstr¨om et al., 2013). The best performance for each language is shown in
bold. The models that are built with a dictionary are provided for reference.
</tableCaption>
<bodyText confidence="0.999885161764706">
ness of the DPM prior on the correction model is
wearing off. An interesting observation is that the
correction model is always better, even when we
add massive amounts of annotated data. At 50,000
tokens, when the supervised model reaches 96%
accuracy, the correction model is still 0.3% (abso-
lute) better, reaching 96.3%. It means that even
at that high level of confidence, some informa-
tion can still be added from DPM to the correc-
tion model. This improvement probably comes
from the observation that the ambiguity in one
language is explained through the alignment. It
also suggests that this method could improve the
performance of a supervised POS tagger even for
resource-rich languages.
Our methods are also relevant for annotation
projects for resource-poor languages. Assuming
that it is very costly to annotate even 100 tokens,
applying our methods can save annotation effort
but maintain high performance. For example, we
just need 100 tokens to match the accuracy of a su-
pervised method trained on 700 tokens, or we just
need 500 tokens to match the performance with
nearly 2,000 tokens of supervised learning.
Our method is simple, but particularly suitable
for resource-poor languages. We need a small
amount of annotated data for a high performance
POS tagger. For example, we need only around
300 annotated tokens to reach the same accuracy
as the state-of-the-art unsupervised POS tagger
(88.8%).
Tag dictionary Although, it is not our objec-
tive to rely on the dictionary, we are interested
in whether the gains from the correction model
still persist when the DPM performance is im-
proved. We attempt to improve DPM, following
the method of Li et al. (2012) by building a tag dic-
tionary using Wiktionary. This dictionary is then
used as a feature which fires for word-tag pairings
present in the dictionary. We expect that when we
add this additional supervision, the DPM model
should perform better. Table 6 shows the perfor-
mance of DPM and the correction model when in-
corporating the dictionary. The DPM model only
increases 0.6% absolute but the correction model
increases 1.3%. Additionally, it shows that our
model can improve further by incorporating exter-
nal information where available.
CRF Our approach of using simple classifiers
begs the question of whether better results could
be obtained using sequence models, such as con-
ditional random fields (CRFs). As mentioned pre-
viously, a CRF is not well suited for incomplete
data. However, as our second ‘correction’ model
is trained on complete sequences, we now con-
sider using a CRF in this stage. The training al-
gorithm is as follows: first we estimate the DPM
feature weights on the incomplete data as before,
and next we incorporate the feature weights into a
CRF trained on the 1,000 annotated tokens. This is
complicated by the different feature sets between
the MaxEnt classifier and the CRF, however the
classifier uses a strict subset of the CRF features.
Thus, we use the minimum divergence prior for
the token level features, and a standard zero-mean
prior for the sequence features. That is, the ob-
jective function of the CRF correction model be-
comes:
</bodyText>
<equation confidence="0.991869">
Gcorr = log P(t|w α)
crf
12δ2 1:(αj − λj)2 − 2δ2 1:α2j (2)
1 2
j∈F1 j∈F2
</equation>
<bodyText confidence="0.9997476">
where F1 is the set of features referring to only
one label as in the DPM maxent model and F2
is the set of features over label pairs. The union
of F = F1 U F2 is the set of all features for
the CRF. We perform grid search using held out
</bodyText>
<page confidence="0.997794">
893
</page>
<bodyText confidence="0.999959142857143">
data as before for δi and δ22. The CRF correc-
tion model scores 88.1% compared with 86.5% of
the supervised CRF model trained on the 1,000
tokens. Clearly, this is beneficial, however, the
CRF correction model still performs worse than
the MaxEnt correction model (91.3%). We are not
sure why but one reason might be overfitting of
the CRF, due to its large feature set and tiny train-
ing sample. Moreover, this CRF approach is or-
thogonal to T¨ackstr¨om et al. (2013): we could use
their CRF model as the DPM model and train the
CRF correction model using the same minimum
divergence method, presumably resulting in even
higher performance.
</bodyText>
<sectionHeader confidence="0.998064" genericHeader="method">
6 Two-output model
</sectionHeader>
<bodyText confidence="0.999992583333334">
Garrette and Baldridge (2013) also use only a
small amount of annotated data, evaluating on
two resource-poor languages Kinyarwanda (KIN)
and Malagasy (MLG). As a simple baseline, we
trained a maxent supervised classifier on this data,
achieving competitive results of 76.4% and 80.0%
accuracy compared with their published results
of 81.9% and 81.2% for KIN and MLG, respec-
tively. Note that the Garrette and Baldridge (2013)
method is much more complicated than this base-
line, and additionally uses an external dictionary.
We want to further improve the accuracy of
MLG using parallel data. Applying the technique
from Section 4 will not work directly, due to the
tagset mismatch (the Malagasy tagset contains 24
tags) which results in highly different feature sets.
Moreover, we don’t have the language expertise
to manually map the tagset. Thus, in this section,
we propose a method capable of handling tagset
mismatch. For data, we use a parallel English-
Malagasy corpus of —100k sentences,4 and the
POS annotated dataset developed by Garrette and
Baldridge (2013), which comprises 4230 tokens
for training and 5300 tokens for testing.
</bodyText>
<subsectionHeader confidence="0.996387">
6.1 The model
</subsectionHeader>
<bodyText confidence="0.998442666666667">
Traditionally, MaxEnt classifiers are trained us-
ing a single label.5 The method we propose is
trained with pairs of output labels: one for the
</bodyText>
<footnote confidence="0.766159125">
4http://www.ark.cs.cmu.edu/global-voices/
5Or else a sequence of labels, in the case of a conditional
random field (Lafferty et al., 2001). However, even in this
case, each token is usually assigned a single label. An excep-
tion is the factorial CRF (Sutton et al., 2007), which models
several co-dependent sequences. Our approach is equivalent
to a factorial CRF without edges between tags for adjacent
tokens in the input.
</footnote>
<bodyText confidence="0.99858875">
Malagasy tag (tM) and one for the universal tag
(tU), which are both predicted conditioned on a
Malagasy word (wM) in context. Our two-output
model is defined as
</bodyText>
<equation confidence="0.995319428571429">
� D
1
P(tM, tU|wM) = Z(wM) exp λjfMj (w, tM)
1
j=
)αjfB j (w, tM, tU)
(3)
</equation>
<bodyText confidence="0.9999668">
where fM, fU, fB are the feature functions con-
sidering tM only, tU only, and over both outputs
tM and tU respectively, and Z(wM) is the parti-
tion function. We can think of Eq. (3) as the com-
bination of 3 models: the Malagasy maxent super-
vised model, the DPM model, and the tagset map-
ping model. The central idea behind this model is
to learn to predict not just the MLG tags, as in a
standard supervised model, but also to learn the
mapping between MLG and the noisy projected
universal tags. Framing this as a two output model
allows for information to flow both ways, such that
confident taggings in either space can inform the
other, and accordingly the mapping weights α are
optimised to maximally exploit this effect.
One important question is how to obtain la-
belled data for training the two-output model, as
our small supervised sample of MLG text is only
annotated for MLG labels tM. We resolve this
by first learning the DPM model on the projected
labels, after which we automatically label our
correction training set with predicted tags from
the DPM model. That is, we augment the an-
notated training data from (tM, wM) to become
(tM, tU, wM). This is then used to train the two-
output maxent classifier, optimising a MAP ob-
jective using standard gradient descent. Note that
it would be possible to apply the same minimum
divergence technique for the two-output maxent
model. In this case the correction model would
include a regularization term over the λ to bias to-
wards the DPM parameters, while γ and α would
use a zero-mean regularizer. However, we leave
this for future work.
Table 7 summarises the performance of the
state-of-the-art (Garrette et al., 2013), the super-
vised model and the two-output maxent model
evaluated on the Malagasy test set. The two-output
maxent model performs much better than the su-
pervised model, achieving —5.3% (absolute) im-
</bodyText>
<figure confidence="0.545004333333333">
E F
+1: γjfUj (w, tU) +
j=1 j=1
</figure>
<page confidence="0.886425">
894
</page>
<tableCaption confidence="0.676110333333333">
Model Accuracy (%)
Table 7: The performance of different models for
Malagasy.
</tableCaption>
<bodyText confidence="0.9874026">
provement. An interesting property of this ap-
proach is that we can use different tagsets for the
DPM. We also tried the original Penn treebank
tagset which is much larger than the universal
tagset (48 vs. 12 tags). We observed a small im-
provement reaching 85.6%, suggesting that some
pertinent information is lost in the universal tagset.
All in all, this is a substantial improvement over
the state-of-the-art result of 81.2% (Garrette et al.,
2013) and an error reduction of 23.4%.
</bodyText>
<sectionHeader confidence="0.99897" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999993169811321">
In this paper, we thoroughly review the work on
multilingual POS tagging of the past decade. We
propose a simple method for building a POS tag-
ger for resource-poor languages by taking advan-
tage of parallel data and a small amount of anno-
tated data. Our method also efficiently resolves
the tagset mismatch issue identified for some lan-
guage pairs. We carefully choose and tune the
model. Comparing with the state-of-the-art, we
are using the more realistic assumption that a
small amount of labelled data can be made avail-
able rather than requiring a crowd-sourced dic-
tionary. We use less parallel data which as we
pointed out in section 3.1, could have been a huge
disadvantage for us. Moreover, we did not exploit
any external monolingual data. Importantly, our
method is simpler but performs better than previ-
ously proposed methods. With only 1,000 anno-
tated tokens, less than 1% of the test data, we can
achieve an average accuracy of 91.3% compared
with 88.8% of the state-of-the-art (error reduction
rate ∼22%). Across the 8 languages we are sub-
stantially better at 7 and slightly worse at one. Our
method is reliable and could even be used to im-
prove the performance of a supervised POS tagger.
Currently, we are building the tagger and eval-
uating through several layers of mapping. Each
layer might introduce some noise which accumu-
lates and leads to a biased model. Moreover,
the tagset mappings are not available for many
resource-poor languages. We therefore also pro-
posed a method to automatically match between
tagsets based on a two-output maximum entropy
model. On the resource-poor language Mala-
gasy, we achieved the accuracy of 85.6% com-
pared with the state-of-the-art of 81.2% (Garrette
et al., 2013). Unlike their method, we didn’t use an
external dictionary but instead use a small amount
of parallel data.
In future work, we would like to improve the
performance of DPM by collecting more parallel
data. Duong et al. (2013a) pointed out that using
a different source language can greatly alter the
performance of the target language POS tagger.
We would like to experiment with different source
languages other than English. We assume that we
have 1,000 tokens for each language. Thus, for the
8 languages we considered we will have 8,000 an-
notated tokens. Currently, we treat each language
independently, however, it might also be interest-
ing to find some way to incorporate information
from multiple languages simultaneously to build
the tagger for a single target language.
</bodyText>
<sectionHeader confidence="0.997237" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9991972">
We would like to thank Dan Garreette, Jason
Baldridge and Noah Smith for Malagasy and Kin-
yarwanda datasets. This work was supported by
the University of Melbourne and National ICT
Australia (NICTA). NICTA is funded by the Aus-
tralian Federal and Victoria State Governments,
and the Australian Research Council through the
ICT Centre of Excellence program. Dr Cohn is the
recipient of an Australian Research Council Fu-
ture Fellowship (project number FT130101105).
</bodyText>
<table confidence="0.8623715">
Garrette et al. (2013) 81.2
MaxEnt Supervised 80.0
2-output MaxEnt (Universal tagset) 85.3
2-output MaxEnt (Penn tagset) 85.6
</table>
<page confidence="0.996672">
895
</page>
<sectionHeader confidence="0.983217" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999764945454546">
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceeding of
HLT-NAACL, pages 582–590.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. COMPU-
TATIONAL LINGUISTICS, 22:39–71.
Thorsten Brants. 2000. TnT: A statistical part-of-
speech tagger. In Proceedings of the Sixth Con-
ference on Applied Natural Language Processing
(ANLP ’00), pages 224–231, Seattle, Washington,
USA.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsuper-
vised pos induction: How far have we come? In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
’10, pages 575–584.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume
1, HLT ’11, pages 600–609.
Long Duong, Paul Cook, Steven Bird, and Pavel
Pecina. 2013a. Increasing the quality and quan-
tity of source language data for Unsupervised Cross-
Lingual POS tagging. Proceedings of the Sixth In-
ternational Joint Conference on Natural Language
Processing, pages 1243–1249. Asian Federation of
Natural Language Processing.
Long Duong, Paul Cook, Steven Bird, and Pavel
Pecina. 2013b. Simpler unsupervised POS tagging
with bilingual projections. Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
634–639. Association for Computational Linguis-
tics.
George Foster. 2000. A maximum entropy/minimum
divergence translation model. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 45–52.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
pages 138–147, June.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-world semi-supervised learning of pos-
taggers for low-resource languages. pages 583–592,
August.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. Em can find pretty good hmm pos-taggers
(when given a good start. In In Proc. ACL, pages
746–754.
Jiri Hana, Anna Feldman, and Chris Brew. 2004.
A resource-light approach to Russian morphology:
Tagging Russian using Czech resources. In Pro-
ceedings of the 2004 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP ’04),
pages 222–229, Barcelona, Spain, July.
Mark Johnson and Stefan Riezler. 2000. Exploit-
ing auxiliary distributions in stochastic unification-
based grammars. In Proceedings of the 1st North
American Chapter of the Association for Computa-
tional Linguistics Conference, NAACL 2000, pages
154–161.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the Tenth Machine Translation Summit (MT Summit
X), pages 79–86, Phuket, Thailand. AAMT.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
’01, pages 282–289.
Shen Li, Jo˜ao V. Grac¸a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ’12,
pages 1389–1398.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19–51, March.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
Ninth Conference on European Chapter of the As-
sociation for Computational Linguistics, EACL ’99,
pages 71–76.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC’12), Istan-
bul, Turkey, may. European Language Resources
Association (ELRA).
Barbara Plank and Gertjan van Noord. 2008. Ex-
ploring an auxiliary distribution based approach
to domain adaptation of a syntactic disambigua-
tion model. In Coling 2008: Proceedings of the
Workshop on Cross-Framework and Cross-Domain
Parser Evaluation, CrossParser ’08, pages 9–16.
Barbara Plank, Dirk Hovy, and Anders Søgaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
</reference>
<page confidence="0.988682">
896
</page>
<reference confidence="0.999221954545455">
Computational Linguistics, pages 742–751, Gothen-
burg, Sweden, April.
Siva Reddy and Serge Sharoff. 2011. Cross lan-
guage POS taggers (and other tools) for Indian lan-
guages: An experiment with Kannada using Telugu
resources. In Proceedings of IJCNLP workshop on
Cross Lingual Information Access: Computational
Linguistics and the Information Need of Multilin-
gual Societies. (CLIA 2011 at IJNCLP 2011), Chi-
ang Mai, Thailand, November.
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. J. Mach. Learn.
Res., 8:693–723, May.
Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics, 1:1–12.
Kristina Toutanova and Mark Johnson. 2008. A
bayesian lda-based model for semi-supervised part-
of-speech tagging. In J.C. Platt, D. Koller, and
Y. Singer a nd S.T. Roweis, editors, Advances in
Neural Information Processing Systems 20, pages
1521–1528. Curran Associates, Inc.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology -
Volume 1 (NAACL ’03), pages 173–180, Edmonton,
Canada.
Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In In
ACL International Conference Proceedings.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
the Second Meeting of the North American Chapter
of the Association for Computational Linguistics on
Language technologies, NAACL ’01, pages 1–8.
</reference>
<page confidence="0.998136">
897
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.801518">
<title confidence="0.99869">What Can We Get From 1000 Tokens? A Case Study of Multilingual POS Tagging For Resource-Poor Languages</title>
<author confidence="0.999551">Trevor Karin Steven</author>
<affiliation confidence="0.972900666666667">of Computing and Information The University of ICT Australia, Victoria Research</affiliation>
<email confidence="0.869269">karin.verspoor,sbird,</email>
<abstract confidence="0.999029947368421">In this paper we address the problem of multilingual part-of-speech tagging for resource-poor languages. We use parallel data to transfer part-of-speech information from resource-rich to resourcepoor languages. Additionally, we use a small amount of annotated data to learn to “correct” errors from projected approach such as tagset mismatch between languages, achieving state-of-the-art performance (91.3%) across 8 languages. Our approach is based on modest data requirements, and uses minimum divergence classification. For situations where no universal tagset mapping is available, we propose an alternate method, resulting in state-of-the-art 85.6% accuracy on the resource-poor language Malagasy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Proceeding of HLT-NAACL,</booktitle>
<pages>582--590</pages>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e, John DeNero, and Dan Klein. 2010. Painless unsupervised learning with features. In Proceeding of HLT-NAACL, pages 582–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<booktitle>COMPUTATIONAL LINGUISTICS,</booktitle>
<pages>22--39</pages>
<contexts>
<context position="21174" citStr="Berger et al., 1996" startWordPosition="3504" endWordPosition="3507">e DPM model while correcting for its inadequacies using direct supervision. We select only 1,000 annotated tokens to reflect a low resource scenario. A small supervised training sample is a more realistic form of supervision than a tag dictionary (noisy or otherwise). Although used in most prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating information from a simpler model (Johnson and Riezler, 2000) or combining in- and outof-domain models (Plank and van Noord, 2008). Plank and van Noord (2008) concluded that this method for adding prior </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. COMPUTATIONAL LINGUISTICS, 22:39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT: A statistical part-ofspeech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth Conference on Applied Natural Language Processing (ANLP ’00),</booktitle>
<pages>224--231</pages>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="14343" citStr="Brants, 2000" startWordPosition="2338" endWordPosition="2339">rd POS tagger (Toutanova et al., 2003) to tag the English side of the parallel data and then project the label to the target side. It has been confirmed in many studies (T¨ackstr¨om et al., 2013, Das and Petrov, 2011, Toutanova and Johnson, 2008) that directly projected labels are noisy. Thus we need a method to reduce the noise. We employ the strategy of Yarowsky and Ngai (2001) of ranking sentences using a their alignment scores from IBM model 3. Firstly, we want to know how noisy the projected data is. Thus, we use the test data to build a simple supervised POS tagger using the TnT tagger (Brants, 2000) which employs a secondorder Hidden Markov Model (HMM). We tag the projected data and compare the label from direct projection and from the TnT tagger. The labels from the TnT Tagger are considered as pseudogold labels. Column “Without Mapping” from Table 3 shows the average accuracy for the first nsentences (n = 60k, 100k, 200k, 500k) for 8 languages according to the ranking. Column “Coverage” shows the percentages of projected label (the other tokens are Null aligned). We can see that when we select more data, both coverage and accuracy fall. In other words, using the sentence alignment scor</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT: A statistical part-ofspeech tagger. In Proceedings of the Sixth Conference on Applied Natural Language Processing (ANLP ’00), pages 224–231, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="9310" citStr="Buchholz and Marsi, 2006" startWordPosition="1471" endWordPosition="1474">ing only 4 hours of annotation. 1http://www.wiktionary.org/ 887 da nl de el it pt es sv Average Das and Petrov (2011) 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4 Duong et al. (2013b) 85.6 84.0 85.4 80.4 81.4 86.3 83.3 81.0 83.4 Li et al. (2012) 83.3 86.3 85.4 79.2 86.5 84.5 86.4 86.1 84.8 T¨ackstr¨om et al. (2013) 88.2 85.9 90.5 89.5 89.3 91.0 87.1 88.9 88.8 Table 1: Previously published token-level POS tagging accuracy for various models across 8 languages — Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es), Swedish (sv) — evaluated on CoNLL data (Buchholz and Marsi, 2006). The method we propose in this paper is similar in only using a small amount of annotation. However, we directly use the annotated data to train the model rather than using a dictionary. We argue that with a proper “guide”, we can take advantage of very limited annotated data. 2.1 Annotated data Our annotated data mainly comes from CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006). The language specific tagsets are mapped into the universal tagset. We will use this annotated data mainly for evaluation. Table 2 shows the size of annotated data for each language. The 8 languag</context>
<context position="12329" citStr="Buchholz and Marsi, 2006" startWordPosition="1985" endWordPosition="1988">of a POS tagger exploiting parallel data. The method we present here is more target-language oriented: our model is trained on the target language, in this way, only relevant information from the source language is retained. Thus, we automatically correct the mapping, and other incompatibilities arising from incorrect alignments and syntactic divergence between the source and target languages. Lang Size(k) # Tags Not Matched da 94 8 DET, PRT, PUNC, NUM nl 203 11 PRT de 712 12 el 70 12 it 76 11 PRT pt 207 11 PRT es 89 11 PRT sv 191 11 DET AVG 205 Table 2: The size of annotated data from CoNLL (Buchholz and Marsi, 2006), and the number of tags included and missing for 8 languages. 2Many of these are mistakes in the mapping, however, they are indicative of the kinds of issues expected in lowresource languages. 888 3 Directly Projected Model (DPM) In this section we describe a maximum entropy tagger that only uses information from directly projected data. 3.1 Parallel data We first collect Europarl data having English as the source language, an average of 1.85 million parallel sentences for each of the 8 language pairs. In terms of parallel data, we use far less data compared with other recent work. Das and Pe</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Two decades of unsupervised pos induction: How far have we come?</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>575--584</pages>
<contexts>
<context position="1628" citStr="Christodoulopoulos et al., 2010" startWordPosition="229" endWordPosition="232">opose an alternate method, resulting in state-of-the-art 85.6% accuracy on the resource-poor language Malagasy. 1 Introduction Part-of-speech (POS) tagging is a crucial task for natural language processing (NLP) tasks, providing basic information about syntax. Supervised POS tagging has achieved great success, reaching as high as 95% accuracy for many languages (Petrov et al., 2012). However, supervised techniques need manually annotated data, and this is either lacking or limited in most resourcepoor languages. Fully unsupervised POS tagging is not yet useful in practice due to low accuracy (Christodoulopoulos et al., 2010). In this paper, we propose a semi-supervised method to narrow the gap between supervised and unsupervised approaches. We demonstrate that even a small amount of supervised data leads to substantial improvement. Our method is motivated by the availability of parallel data. Thanks to the development of multilingual documents from government projects, book translations, multilingual websites, and so forth, parallel data between resource-rich and resource-poor languages is relatively easy to acquire. This parallel data provides the bridge that permits us to transfer POS information from a resourc</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2010</marker>
<rawString>Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. 2010. Two decades of unsupervised pos induction: How far have we come? In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 575–584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised part-of-speech tagging with bilingual graph-based projections.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>600--609</pages>
<contexts>
<context position="5239" citStr="Das and Petrov (2011)" startWordPosition="804" endWordPosition="807">zech and Russian, or Serbian and Croatian. They build the tagger — or estimate part of the tagger — on one language and apply it to the other language (Reddy and Sharoff, 2011, Hana et al., 2004). Yarowsky and Ngai (2001) pioneered the use of parallel data for projecting tag information from a resource-rich language to a resource-poor language. Duong et al. (2013b) used a similar method on using sentence alignment scores to rank the goodness of sentences. They trained a seed model from a small part of the data, then applied this model to the rest of the data using self-training with revision. Das and Petrov (2011) also used parallel data but additionally exploited graph-based label propagation to expand the coverage of labelled tokens. Each node in the graph represents a trigram in the target language. Each edge connects two nodes which have similar context. Originally, only some nodes received a label from direct label projection, and then labels were propagated to the rest of the graph. They only extracted the dictionary from the graph because the labels of nodes are noisy. They used the dictionary as the constraints for a feature-based HMM tagger (Berg-Kirkpatrick et al., 2010). Both Duong et al. (2</context>
<context position="7606" citStr="Das and Petrov (2011)" startWordPosition="1185" endWordPosition="1188">t al. (2013) combined both token information (from direct projected data) and type constraints (from Wiktionary’s dictionary) to form the state-of-the-art multilingual tagger. They built a tag lattice and used these token and type constraints to prune it. The remaining paths are the training data for a CRF tagger. They achieved 88.8% accuracy on the same 8 languages. Table 1 summarises the performance of the above models across all 8 languages. Note that these methods vary in their reliance on external resources. Duong et al. (2013b) use the least, i.e. only the Europarl Corpus (Koehn, 2005). Das and Petrov (2011) additionally use the United Nation Parallel Corpus. Li et al. (2012) didn’t use any parallel text but used Wiktionary instead. T¨ackstr¨om et al. (2013) exploited more parallel data than Das and Petrov (2011) and also used a dictionary from Li et al. (2012). Another approach for resource-poor languages is based on the availability of a small amount of annotated data. Garrette et al. (2013) built a POS tagger for Kinyarwanda and Malagasy. They didn’t use parallel data but instead exploited four hours of manual annotation to build ∼4,000 tokens or ∼3,000 word-types of annotated data. These toke</context>
<context position="12940" citStr="Das and Petrov (2011)" startWordPosition="2090" endWordPosition="2093">rsi, 2006), and the number of tags included and missing for 8 languages. 2Many of these are mistakes in the mapping, however, they are indicative of the kinds of issues expected in lowresource languages. 888 3 Directly Projected Model (DPM) In this section we describe a maximum entropy tagger that only uses information from directly projected data. 3.1 Parallel data We first collect Europarl data having English as the source language, an average of 1.85 million parallel sentences for each of the 8 language pairs. In terms of parallel data, we use far less data compared with other recent work. Das and Petrov (2011) used Europarl and the ODS United Nation dataset, while T¨ackstr¨om et al. (2013) additionally used parallel data crawled from the web. The amount of parallel data is crucial for alignment quality. Since DPM uses alignments to transfer tags from source to target language, the performance of DPM (and other models that exploit projection) largely depends on the quantity of parallel data. The “No LP” model of Das and Petrov (2011), which only uses directly projected labels (without label propagation), scored 81.3% for 8 languages. However, using the same model but with more parallel data, T¨ackst</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 600–609.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Duong</author>
<author>Paul Cook</author>
<author>Steven Bird</author>
<author>Pavel Pecina</author>
</authors>
<title>Increasing the quality and quantity of source language data for Unsupervised CrossLingual POS tagging.</title>
<date>2013</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>Proceedings of the Sixth International Joint Conference on Natural Language Processing,</booktitle>
<pages>1243--1249</pages>
<contexts>
<context position="4983" citStr="Duong et al. (2013" startWordPosition="759" endWordPosition="762">te-of-the-art of 81.2% (Garrette et al., 2013). 2 Background and Related Work There is a wealth of prior work on multilingual POS tagging. The simplest approach takes advantage of the typological similarities that exist between languages pairs such as Czech and Russian, or Serbian and Croatian. They build the tagger — or estimate part of the tagger — on one language and apply it to the other language (Reddy and Sharoff, 2011, Hana et al., 2004). Yarowsky and Ngai (2001) pioneered the use of parallel data for projecting tag information from a resource-rich language to a resource-poor language. Duong et al. (2013b) used a similar method on using sentence alignment scores to rank the goodness of sentences. They trained a seed model from a small part of the data, then applied this model to the rest of the data using self-training with revision. Das and Petrov (2011) also used parallel data but additionally exploited graph-based label propagation to expand the coverage of labelled tokens. Each node in the graph represents a trigram in the target language. Each edge connects two nodes which have similar context. Originally, only some nodes received a label from direct label projection, and then labels wer</context>
<context position="7522" citStr="Duong et al. (2013" startWordPosition="1171" endWordPosition="1174"> not effective for building POS taggers for resourcepoor languages. T¨ackstr¨om et al. (2013) combined both token information (from direct projected data) and type constraints (from Wiktionary’s dictionary) to form the state-of-the-art multilingual tagger. They built a tag lattice and used these token and type constraints to prune it. The remaining paths are the training data for a CRF tagger. They achieved 88.8% accuracy on the same 8 languages. Table 1 summarises the performance of the above models across all 8 languages. Note that these methods vary in their reliance on external resources. Duong et al. (2013b) use the least, i.e. only the Europarl Corpus (Koehn, 2005). Das and Petrov (2011) additionally use the United Nation Parallel Corpus. Li et al. (2012) didn’t use any parallel text but used Wiktionary instead. T¨ackstr¨om et al. (2013) exploited more parallel data than Das and Petrov (2011) and also used a dictionary from Li et al. (2012). Another approach for resource-poor languages is based on the availability of a small amount of annotated data. Garrette et al. (2013) built a POS tagger for Kinyarwanda and Malagasy. They didn’t use parallel data but instead exploited four hours of manual </context>
<context position="8866" citStr="Duong et al. (2013" startWordPosition="1396" endWordPosition="1399">dictionary. They employed label propagation for expanding the coverage of this dictionary in a similar vein to Das and Petrov (2011), but they also used an external dictionary. They built training examples using the combined dictionary and then trained the tagger on this data. They achieved 81.9% and 81.2% accuracy for Kinyarwanda and Malagasy respectively. Note that their usage of an external dictionary compromises their claim of using only 4 hours of annotation. 1http://www.wiktionary.org/ 887 da nl de el it pt es sv Average Das and Petrov (2011) 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4 Duong et al. (2013b) 85.6 84.0 85.4 80.4 81.4 86.3 83.3 81.0 83.4 Li et al. (2012) 83.3 86.3 85.4 79.2 86.5 84.5 86.4 86.1 84.8 T¨ackstr¨om et al. (2013) 88.2 85.9 90.5 89.5 89.3 91.0 87.1 88.9 88.8 Table 1: Previously published token-level POS tagging accuracy for various models across 8 languages — Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es), Swedish (sv) — evaluated on CoNLL data (Buchholz and Marsi, 2006). The method we propose in this paper is similar in only using a small amount of annotation. However, we directly use the annotated data to train the model </context>
<context position="15895" citStr="Duong et al. (2013" startWordPosition="2596" endWordPosition="2599">most frequently in the test data. For example DET, PRT, PUNC, NUM, missing from Danish gold data, will be matched to PRON, X, X, ADJ respectively. This simple matching yields a ∼ 4% (absolute) improvement in average accuracy. This illustrates the importance of handling tagset mapping carefully. 3.3 The model In this section, we introduce a maximum entropy tagger exploiting the projected data. We select the first 200k sentences from Table 3 for this experiment. This number represents a trade-off between size and accuracy. More sentences provide more information but at the cost of noisier data. Duong et al. (2013b) also used sentence alignment scores to rank sentences. Their model stabilizes after using 200k sentences. We conclude that 200k sentences is enough and capture most information from the parallel data. Features Descriptions W@-1 Previous word W@+1 Next word W@0 Current word CAP First character is capitalized NUMBER Is number PUNCT Is punctuation SUFFIX@k Suffix up to length 3 (k &lt;= 3) WC Word class Table 4: Feature template for a maximum entropy tagger We ignore tokens that don’t have labels, which arise from null alignments and constitute approximately 14% of the data. The remaining data (∼</context>
<context position="39031" citStr="Duong et al. (2013" startWordPosition="6575" endWordPosition="6578">se which accumulates and leads to a biased model. Moreover, the tagset mappings are not available for many resource-poor languages. We therefore also proposed a method to automatically match between tagsets based on a two-output maximum entropy model. On the resource-poor language Malagasy, we achieved the accuracy of 85.6% compared with the state-of-the-art of 81.2% (Garrette et al., 2013). Unlike their method, we didn’t use an external dictionary but instead use a small amount of parallel data. In future work, we would like to improve the performance of DPM by collecting more parallel data. Duong et al. (2013a) pointed out that using a different source language can greatly alter the performance of the target language POS tagger. We would like to experiment with different source languages other than English. We assume that we have 1,000 tokens for each language. Thus, for the 8 languages we considered we will have 8,000 annotated tokens. Currently, we treat each language independently, however, it might also be interesting to find some way to incorporate information from multiple languages simultaneously to build the tagger for a single target language. Acknowledgments We would like to thank Dan Ga</context>
</contexts>
<marker>Duong, Cook, Bird, Pecina, 2013</marker>
<rawString>Long Duong, Paul Cook, Steven Bird, and Pavel Pecina. 2013a. Increasing the quality and quantity of source language data for Unsupervised CrossLingual POS tagging. Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 1243–1249. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Duong</author>
<author>Paul Cook</author>
<author>Steven Bird</author>
<author>Pavel Pecina</author>
</authors>
<title>Simpler unsupervised POS tagging with bilingual projections.</title>
<date>2013</date>
<booktitle>Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>634--639</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4983" citStr="Duong et al. (2013" startWordPosition="759" endWordPosition="762">te-of-the-art of 81.2% (Garrette et al., 2013). 2 Background and Related Work There is a wealth of prior work on multilingual POS tagging. The simplest approach takes advantage of the typological similarities that exist between languages pairs such as Czech and Russian, or Serbian and Croatian. They build the tagger — or estimate part of the tagger — on one language and apply it to the other language (Reddy and Sharoff, 2011, Hana et al., 2004). Yarowsky and Ngai (2001) pioneered the use of parallel data for projecting tag information from a resource-rich language to a resource-poor language. Duong et al. (2013b) used a similar method on using sentence alignment scores to rank the goodness of sentences. They trained a seed model from a small part of the data, then applied this model to the rest of the data using self-training with revision. Das and Petrov (2011) also used parallel data but additionally exploited graph-based label propagation to expand the coverage of labelled tokens. Each node in the graph represents a trigram in the target language. Each edge connects two nodes which have similar context. Originally, only some nodes received a label from direct label projection, and then labels wer</context>
<context position="7522" citStr="Duong et al. (2013" startWordPosition="1171" endWordPosition="1174"> not effective for building POS taggers for resourcepoor languages. T¨ackstr¨om et al. (2013) combined both token information (from direct projected data) and type constraints (from Wiktionary’s dictionary) to form the state-of-the-art multilingual tagger. They built a tag lattice and used these token and type constraints to prune it. The remaining paths are the training data for a CRF tagger. They achieved 88.8% accuracy on the same 8 languages. Table 1 summarises the performance of the above models across all 8 languages. Note that these methods vary in their reliance on external resources. Duong et al. (2013b) use the least, i.e. only the Europarl Corpus (Koehn, 2005). Das and Petrov (2011) additionally use the United Nation Parallel Corpus. Li et al. (2012) didn’t use any parallel text but used Wiktionary instead. T¨ackstr¨om et al. (2013) exploited more parallel data than Das and Petrov (2011) and also used a dictionary from Li et al. (2012). Another approach for resource-poor languages is based on the availability of a small amount of annotated data. Garrette et al. (2013) built a POS tagger for Kinyarwanda and Malagasy. They didn’t use parallel data but instead exploited four hours of manual </context>
<context position="8866" citStr="Duong et al. (2013" startWordPosition="1396" endWordPosition="1399">dictionary. They employed label propagation for expanding the coverage of this dictionary in a similar vein to Das and Petrov (2011), but they also used an external dictionary. They built training examples using the combined dictionary and then trained the tagger on this data. They achieved 81.9% and 81.2% accuracy for Kinyarwanda and Malagasy respectively. Note that their usage of an external dictionary compromises their claim of using only 4 hours of annotation. 1http://www.wiktionary.org/ 887 da nl de el it pt es sv Average Das and Petrov (2011) 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4 Duong et al. (2013b) 85.6 84.0 85.4 80.4 81.4 86.3 83.3 81.0 83.4 Li et al. (2012) 83.3 86.3 85.4 79.2 86.5 84.5 86.4 86.1 84.8 T¨ackstr¨om et al. (2013) 88.2 85.9 90.5 89.5 89.3 91.0 87.1 88.9 88.8 Table 1: Previously published token-level POS tagging accuracy for various models across 8 languages — Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es), Swedish (sv) — evaluated on CoNLL data (Buchholz and Marsi, 2006). The method we propose in this paper is similar in only using a small amount of annotation. However, we directly use the annotated data to train the model </context>
<context position="15895" citStr="Duong et al. (2013" startWordPosition="2596" endWordPosition="2599">most frequently in the test data. For example DET, PRT, PUNC, NUM, missing from Danish gold data, will be matched to PRON, X, X, ADJ respectively. This simple matching yields a ∼ 4% (absolute) improvement in average accuracy. This illustrates the importance of handling tagset mapping carefully. 3.3 The model In this section, we introduce a maximum entropy tagger exploiting the projected data. We select the first 200k sentences from Table 3 for this experiment. This number represents a trade-off between size and accuracy. More sentences provide more information but at the cost of noisier data. Duong et al. (2013b) also used sentence alignment scores to rank sentences. Their model stabilizes after using 200k sentences. We conclude that 200k sentences is enough and capture most information from the parallel data. Features Descriptions W@-1 Previous word W@+1 Next word W@0 Current word CAP First character is capitalized NUMBER Is number PUNCT Is punctuation SUFFIX@k Suffix up to length 3 (k &lt;= 3) WC Word class Table 4: Feature template for a maximum entropy tagger We ignore tokens that don’t have labels, which arise from null alignments and constitute approximately 14% of the data. The remaining data (∼</context>
<context position="39031" citStr="Duong et al. (2013" startWordPosition="6575" endWordPosition="6578">se which accumulates and leads to a biased model. Moreover, the tagset mappings are not available for many resource-poor languages. We therefore also proposed a method to automatically match between tagsets based on a two-output maximum entropy model. On the resource-poor language Malagasy, we achieved the accuracy of 85.6% compared with the state-of-the-art of 81.2% (Garrette et al., 2013). Unlike their method, we didn’t use an external dictionary but instead use a small amount of parallel data. In future work, we would like to improve the performance of DPM by collecting more parallel data. Duong et al. (2013a) pointed out that using a different source language can greatly alter the performance of the target language POS tagger. We would like to experiment with different source languages other than English. We assume that we have 1,000 tokens for each language. Thus, for the 8 languages we considered we will have 8,000 annotated tokens. Currently, we treat each language independently, however, it might also be interesting to find some way to incorporate information from multiple languages simultaneously to build the tagger for a single target language. Acknowledgments We would like to thank Dan Ga</context>
</contexts>
<marker>Duong, Cook, Bird, Pecina, 2013</marker>
<rawString>Long Duong, Paul Cook, Steven Bird, and Pavel Pecina. 2013b. Simpler unsupervised POS tagging with bilingual projections. Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 634–639. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
</authors>
<title>A maximum entropy/minimum divergence translation model.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="21404" citStr="Foster, 2000" startWordPosition="3540" endWordPosition="3541">ictionary (noisy or otherwise). Although used in most prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating information from a simpler model (Johnson and Riezler, 2000) or combining in- and outof-domain models (Plank and van Noord, 2008). Plank and van Noord (2008) concluded that this method for adding prior knowledge only works with high quality reference distributions, otherwise performance suffers. In contrast to these previous approaches, we consider the specific setting where both the learned model and the reference model so = P(</context>
</contexts>
<marker>Foster, 2000</marker>
<rawString>George Foster. 2000. A maximum entropy/minimum divergence translation model. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Jason Baldridge</author>
</authors>
<title>Learning a part-of-speech tagger from two hours of annotation.</title>
<date>2013</date>
<pages>138--147</pages>
<contexts>
<context position="20971" citStr="Garrette and Baldridge (2013)" startWordPosition="3469" endWordPosition="3472">cussed it makes many errors, due to invalid or inconsistent tag mappings, noisy alignments, and cross-linguistic syntactic divergence. However, our aim is to see how effectively we can exploit the strengths of the DPM model while correcting for its inadequacies using direct supervision. We select only 1,000 annotated tokens to reflect a low resource scenario. A small supervised training sample is a more realistic form of supervision than a tag dictionary (noisy or otherwise). Although used in most prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating</context>
<context position="32681" citStr="Garrette and Baldridge (2013)" startWordPosition="5502" endWordPosition="5505">ores 88.1% compared with 86.5% of the supervised CRF model trained on the 1,000 tokens. Clearly, this is beneficial, however, the CRF correction model still performs worse than the MaxEnt correction model (91.3%). We are not sure why but one reason might be overfitting of the CRF, due to its large feature set and tiny training sample. Moreover, this CRF approach is orthogonal to T¨ackstr¨om et al. (2013): we could use their CRF model as the DPM model and train the CRF correction model using the same minimum divergence method, presumably resulting in even higher performance. 6 Two-output model Garrette and Baldridge (2013) also use only a small amount of annotated data, evaluating on two resource-poor languages Kinyarwanda (KIN) and Malagasy (MLG). As a simple baseline, we trained a maxent supervised classifier on this data, achieving competitive results of 76.4% and 80.0% accuracy compared with their published results of 81.9% and 81.2% for KIN and MLG, respectively. Note that the Garrette and Baldridge (2013) method is much more complicated than this baseline, and additionally uses an external dictionary. We want to further improve the accuracy of MLG using parallel data. Applying the technique from Section 4</context>
</contexts>
<marker>Garrette, Baldridge, 2013</marker>
<rawString>Dan Garrette and Jason Baldridge. 2013. Learning a part-of-speech tagger from two hours of annotation. pages 138–147, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Jason Mielens</author>
<author>Jason Baldridge</author>
</authors>
<title>Real-world semi-supervised learning of postaggers for low-resource languages.</title>
<date>2013</date>
<pages>583--592</pages>
<contexts>
<context position="4411" citStr="Garrette et al., 2013" startWordPosition="660" endWordPosition="663">such a mapping might not be available for resource-poor languages. Therefore, we also pro886 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 886–897, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics pose a variant of our method which removes the need for identical tagsets between the projection model T and the correction model P, based on a two-output maximum entropy model over tag pairs. Evaluating on the resource-poor language Malagasy, we achieved 85.6% accuracy, exceeding the state-of-the-art of 81.2% (Garrette et al., 2013). 2 Background and Related Work There is a wealth of prior work on multilingual POS tagging. The simplest approach takes advantage of the typological similarities that exist between languages pairs such as Czech and Russian, or Serbian and Croatian. They build the tagger — or estimate part of the tagger — on one language and apply it to the other language (Reddy and Sharoff, 2011, Hana et al., 2004). Yarowsky and Ngai (2001) pioneered the use of parallel data for projecting tag information from a resource-rich language to a resource-poor language. Duong et al. (2013b) used a similar method on </context>
<context position="7999" citStr="Garrette et al. (2013)" startWordPosition="1250" endWordPosition="1253"> performance of the above models across all 8 languages. Note that these methods vary in their reliance on external resources. Duong et al. (2013b) use the least, i.e. only the Europarl Corpus (Koehn, 2005). Das and Petrov (2011) additionally use the United Nation Parallel Corpus. Li et al. (2012) didn’t use any parallel text but used Wiktionary instead. T¨ackstr¨om et al. (2013) exploited more parallel data than Das and Petrov (2011) and also used a dictionary from Li et al. (2012). Another approach for resource-poor languages is based on the availability of a small amount of annotated data. Garrette et al. (2013) built a POS tagger for Kinyarwanda and Malagasy. They didn’t use parallel data but instead exploited four hours of manual annotation to build ∼4,000 tokens or ∼3,000 word-types of annotated data. These tokens or word-types were used to build a tag dictionary. They employed label propagation for expanding the coverage of this dictionary in a similar vein to Das and Petrov (2011), but they also used an external dictionary. They built training examples using the combined dictionary and then trained the tagger on this data. They achieved 81.9% and 81.2% accuracy for Kinyarwanda and Malagasy respe</context>
<context position="36281" citStr="Garrette et al., 2013" startWordPosition="6112" endWordPosition="6115">m the DPM model. That is, we augment the annotated training data from (tM, wM) to become (tM, tU, wM). This is then used to train the twooutput maxent classifier, optimising a MAP objective using standard gradient descent. Note that it would be possible to apply the same minimum divergence technique for the two-output maxent model. In this case the correction model would include a regularization term over the λ to bias towards the DPM parameters, while γ and α would use a zero-mean regularizer. However, we leave this for future work. Table 7 summarises the performance of the state-of-the-art (Garrette et al., 2013), the supervised model and the two-output maxent model evaluated on the Malagasy test set. The two-output maxent model performs much better than the supervised model, achieving —5.3% (absolute) imE F +1: γjfUj (w, tU) + j=1 j=1 894 Model Accuracy (%) Table 7: The performance of different models for Malagasy. provement. An interesting property of this approach is that we can use different tagsets for the DPM. We also tried the original Penn treebank tagset which is much larger than the universal tagset (48 vs. 12 tags). We observed a small improvement reaching 85.6%, suggesting that some pertin</context>
<context position="38806" citStr="Garrette et al., 2013" startWordPosition="6536" endWordPosition="6539">one. Our method is reliable and could even be used to improve the performance of a supervised POS tagger. Currently, we are building the tagger and evaluating through several layers of mapping. Each layer might introduce some noise which accumulates and leads to a biased model. Moreover, the tagset mappings are not available for many resource-poor languages. We therefore also proposed a method to automatically match between tagsets based on a two-output maximum entropy model. On the resource-poor language Malagasy, we achieved the accuracy of 85.6% compared with the state-of-the-art of 81.2% (Garrette et al., 2013). Unlike their method, we didn’t use an external dictionary but instead use a small amount of parallel data. In future work, we would like to improve the performance of DPM by collecting more parallel data. Duong et al. (2013a) pointed out that using a different source language can greatly alter the performance of the target language POS tagger. We would like to experiment with different source languages other than English. We assume that we have 1,000 tokens for each language. Thus, for the 8 languages we considered we will have 8,000 annotated tokens. Currently, we treat each language indepe</context>
</contexts>
<marker>Garrette, Mielens, Baldridge, 2013</marker>
<rawString>Dan Garrette, Jason Mielens, and Jason Baldridge. 2013. Real-world semi-supervised learning of postaggers for low-resource languages. pages 583–592, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>Em can find pretty good hmm pos-taggers (when given a good start. In</title>
<date>2008</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>746--754</pages>
<contexts>
<context position="5958" citStr="Goldberg et al. (2008)" startWordPosition="921" endWordPosition="924">overage of labelled tokens. Each node in the graph represents a trigram in the target language. Each edge connects two nodes which have similar context. Originally, only some nodes received a label from direct label projection, and then labels were propagated to the rest of the graph. They only extracted the dictionary from the graph because the labels of nodes are noisy. They used the dictionary as the constraints for a feature-based HMM tagger (Berg-Kirkpatrick et al., 2010). Both Duong et al. (2013b) and Das and Petrov (2011) achieved 83.4% accuracy on the test set of 8 European languages. Goldberg et al. (2008) pointed out that, with the presence of a dictionary, even an incomplete one, a modest POS tagger can be built using simple methods such as expectation maximization. This is because most of the time, words have a very limited number of possible tags, thus a dictionary that specifies the allowable tags for a word helps to restrict the search space. With a gold-standard dictionary, Das and Petrov (2011) achieved an accuracy of approximately 94% on the same 8 languages. The effectiveness of a gold-standard dictionary is undeniable, however it is costly to build one, especially for resource-poor l</context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2008</marker>
<rawString>Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008. Em can find pretty good hmm pos-taggers (when given a good start. In In Proc. ACL, pages 746–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiri Hana</author>
<author>Anna Feldman</author>
<author>Chris Brew</author>
</authors>
<title>A resource-light approach to Russian morphology: Tagging Russian using Czech resources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP ’04),</booktitle>
<pages>222--229</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="4813" citStr="Hana et al., 2004" startWordPosition="732" endWordPosition="735">tion model P, based on a two-output maximum entropy model over tag pairs. Evaluating on the resource-poor language Malagasy, we achieved 85.6% accuracy, exceeding the state-of-the-art of 81.2% (Garrette et al., 2013). 2 Background and Related Work There is a wealth of prior work on multilingual POS tagging. The simplest approach takes advantage of the typological similarities that exist between languages pairs such as Czech and Russian, or Serbian and Croatian. They build the tagger — or estimate part of the tagger — on one language and apply it to the other language (Reddy and Sharoff, 2011, Hana et al., 2004). Yarowsky and Ngai (2001) pioneered the use of parallel data for projecting tag information from a resource-rich language to a resource-poor language. Duong et al. (2013b) used a similar method on using sentence alignment scores to rank the goodness of sentences. They trained a seed model from a small part of the data, then applied this model to the rest of the data using self-training with revision. Das and Petrov (2011) also used parallel data but additionally exploited graph-based label propagation to expand the coverage of labelled tokens. Each node in the graph represents a trigram in th</context>
</contexts>
<marker>Hana, Feldman, Brew, 2004</marker>
<rawString>Jiri Hana, Anna Feldman, and Chris Brew. 2004. A resource-light approach to Russian morphology: Tagging Russian using Czech resources. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP ’04), pages 222–229, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stefan Riezler</author>
</authors>
<title>Exploiting auxiliary distributions in stochastic unificationbased grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference, NAACL</booktitle>
<pages>154--161</pages>
<contexts>
<context position="21471" citStr="Johnson and Riezler, 2000" startWordPosition="3549" endWordPosition="3552">prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating information from a simpler model (Johnson and Riezler, 2000) or combining in- and outof-domain models (Plank and van Noord, 2008). Plank and van Noord (2008) concluded that this method for adding prior knowledge only works with high quality reference distributions, otherwise performance suffers. In contrast to these previous approaches, we consider the specific setting where both the learned model and the reference model so = P(t|w) are both maximum entropy models. In this case we show that the</context>
</contexts>
<marker>Johnson, Riezler, 2000</marker>
<rawString>Mark Johnson and Stefan Riezler. 2000. Exploiting auxiliary distributions in stochastic unificationbased grammars. In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference, NAACL 2000, pages 154–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth Machine Translation Summit (MT Summit X),</booktitle>
<pages>79--86</pages>
<publisher>AAMT.</publisher>
<location>Phuket, Thailand.</location>
<contexts>
<context position="7583" citStr="Koehn, 2005" startWordPosition="1183" endWordPosition="1184"> T¨ackstr¨om et al. (2013) combined both token information (from direct projected data) and type constraints (from Wiktionary’s dictionary) to form the state-of-the-art multilingual tagger. They built a tag lattice and used these token and type constraints to prune it. The remaining paths are the training data for a CRF tagger. They achieved 88.8% accuracy on the same 8 languages. Table 1 summarises the performance of the above models across all 8 languages. Note that these methods vary in their reliance on external resources. Duong et al. (2013b) use the least, i.e. only the Europarl Corpus (Koehn, 2005). Das and Petrov (2011) additionally use the United Nation Parallel Corpus. Li et al. (2012) didn’t use any parallel text but used Wiktionary instead. T¨ackstr¨om et al. (2013) exploited more parallel data than Das and Petrov (2011) and also used a dictionary from Li et al. (2012). Another approach for resource-poor languages is based on the availability of a small amount of annotated data. Garrette et al. (2013) built a POS tagger for Kinyarwanda and Malagasy. They didn’t use parallel data but instead exploited four hours of manual annotation to build ∼4,000 tokens or ∼3,000 word-types of ann</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Proceedings of the Tenth Machine Translation Summit (MT Summit X), pages 79–86, Phuket, Thailand. AAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="34086" citStr="Lafferty et al., 2001" startWordPosition="5724" endWordPosition="5727">se to manually map the tagset. Thus, in this section, we propose a method capable of handling tagset mismatch. For data, we use a parallel EnglishMalagasy corpus of —100k sentences,4 and the POS annotated dataset developed by Garrette and Baldridge (2013), which comprises 4230 tokens for training and 5300 tokens for testing. 6.1 The model Traditionally, MaxEnt classifiers are trained using a single label.5 The method we propose is trained with pairs of output labels: one for the 4http://www.ark.cs.cmu.edu/global-voices/ 5Or else a sequence of labels, in the case of a conditional random field (Lafferty et al., 2001). However, even in this case, each token is usually assigned a single label. An exception is the factorial CRF (Sutton et al., 2007), which models several co-dependent sequences. Our approach is equivalent to a factorial CRF without edges between tags for adjacent tokens in the input. Malagasy tag (tM) and one for the universal tag (tU), which are both predicted conditioned on a Malagasy word (wM) in context. Our two-output model is defined as � D 1 P(tM, tU|wM) = Z(wM) exp λjfMj (w, tM) 1 j= )αjfB j (w, tM, tU) (3) where fM, fU, fB are the feature functions considering tM only, tU only, and o</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shen Li</author>
<author>Jo˜ao V Grac¸a</author>
<author>Ben Taskar</author>
</authors>
<title>Wiki-ly supervised part-of-speech tagging.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>1389--1398</pages>
<marker>Li, Grac¸a, Taskar, 2012</marker>
<rawString>Shen Li, Jo˜ao V. Grac¸a, and Ben Taskar. 2012. Wiki-ly supervised part-of-speech tagging. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 1389–1398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="13676" citStr="Och and Ney, 2003" startWordPosition="2215" endWordPosition="2218">led from the web. The amount of parallel data is crucial for alignment quality. Since DPM uses alignments to transfer tags from source to target language, the performance of DPM (and other models that exploit projection) largely depends on the quantity of parallel data. The “No LP” model of Das and Petrov (2011), which only uses directly projected labels (without label propagation), scored 81.3% for 8 languages. However, using the same model but with more parallel data, T¨ackstr¨om et al. (2013) scored 84.9% on the same test set. 3.2 Label projection We use the standard alignment tool Giza++ (Och and Ney, 2003) to word align the parallel data. We employ the Stanford POS tagger (Toutanova et al., 2003) to tag the English side of the parallel data and then project the label to the target side. It has been confirmed in many studies (T¨ackstr¨om et al., 2013, Das and Petrov, 2011, Toutanova and Johnson, 2008) that directly projected labels are noisy. Thus we need a method to reduce the noise. We employ the strategy of Yarowsky and Ngai (2001) of ranking sentences using a their alignment scores from IBM model 3. Firstly, we want to know how noisy the projected data is. Thus, we use the test data to build</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Comput. Linguist., 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>An efficient method for determining bilingual word classes.</title>
<date>1999</date>
<booktitle>In Proceedings of the Ninth Conference on European Chapter of the Association for Computational Linguistics, EACL ’99,</booktitle>
<pages>71--76</pages>
<contexts>
<context position="18771" citStr="Och, 1999" startWordPosition="3100" endWordPosition="3101">ning data, D = {wi, ti}Ni=1, subject to a zero-mean Gaussian regularisation term, where the regularisation term limits over-fitting, an important concern when using large feature 3T¨ackstr¨om et al. (2013) train a CRF on incomplete data, using a tag dictionary heuristic to define a ‘gold standard’ lattice over label sequences. sets. For our experiments we set S2 = 1. We use L-BFGS which performs gradient ascent to maximize L. Table 4 shows the features we considered for building the DPM. We use mkcls, an unsupervised method for word class induction which is widely used in machine translation (Och, 1999). We run mkcls to obtain 100 word classes, using only the target language side of the parallel data. Table 5 shows the accuracy of the DPM evaluated on 8 languages (“All features model”). DPM performs poorly on Danish, probably because of the tagset mapping issue discussed above. The DPM result of 80.2% accuracy is encouraging, particularly because the model had no explicit supervision. To see what features are meaningful for our model, we remove features in turn and report the result. The result in Table 5 disagrees with T¨ackstr¨om et al. (2013) on the word class features. They reported a ga</context>
</contexts>
<marker>Och, 1999</marker>
<rawString>Franz Josef Och. 1999. An efficient method for determining bilingual word classes. In Proceedings of the Ninth Conference on European Chapter of the Association for Computational Linguistics, EACL ’99, pages 71–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="1381" citStr="Petrov et al., 2012" startWordPosition="190" endWordPosition="193">ges, achieving state-of-the-art performance (91.3%) across 8 languages. Our approach is based on modest data requirements, and uses minimum divergence classification. For situations where no universal tagset mapping is available, we propose an alternate method, resulting in state-of-the-art 85.6% accuracy on the resource-poor language Malagasy. 1 Introduction Part-of-speech (POS) tagging is a crucial task for natural language processing (NLP) tasks, providing basic information about syntax. Supervised POS tagging has achieved great success, reaching as high as 95% accuracy for many languages (Petrov et al., 2012). However, supervised techniques need manually annotated data, and this is either lacking or limited in most resourcepoor languages. Fully unsupervised POS tagging is not yet useful in practice due to low accuracy (Christodoulopoulos et al., 2010). In this paper, we propose a semi-supervised method to narrow the gap between supervised and unsupervised approaches. We demonstrate that even a small amount of supervised data leads to substantial improvement. Our method is motivated by the availability of parallel data. Thanks to the development of multilingual documents from government projects, b</context>
<context position="3778" citStr="Petrov et al., 2012" startWordPosition="565" endWordPosition="568">otated data (1,000 tokens) in the target language, using a minimum divergence technique to incorporate the first model, T. Compared with the state of the art (T¨ackstr¨om et al., 2013), we make more-realistic assumptions (e.g. relying on a tiny amount of annotated data rather than a huge crowd-sourced dictionary) and use less parallel data, yet achieve a better overall result. We achieved 91.3% average accuracy over 8 languages, exceeding T¨ackstr¨om et al. (2013)’s result of 88.8%. The test data we employ makes use of mappings from language-specific POS tag inventories to a universal tagset (Petrov et al., 2012). However, such a mapping might not be available for resource-poor languages. Therefore, we also pro886 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 886–897, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics pose a variant of our method which removes the need for identical tagsets between the projection model T and the correction model P, based on a two-output maximum entropy model over tag pairs. Evaluating on the resource-poor language Malagasy, we achieved 85.6% accuracy, exceeding the state-of-the-art</context>
<context position="10380" citStr="Petrov et al., 2012" startWordPosition="1648" endWordPosition="1651">the universal tagset. We will use this annotated data mainly for evaluation. Table 2 shows the size of annotated data for each language. The 8 languages we are considering in this experiment are not actually resource-poor languages. However, running on these 8 languages makes our system comparable with previously proposed methods. Nevertheless, we try to use as few resources as possible, in order to simulate the situation for resource-poor languages. Later in Section 6 we adapt the approach for Malagasy, a truly resource-poor language. 2.2 Universal tagset We employ the universal tagset from (Petrov et al., 2012) for our experiment. It consists of 12 common tags: NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner and article), ADP (preposition and postposition), CONJ (conjunctions), NUM (numerical), PRT (particle), PUNC (punctuation) and X (all other categories including foreign words and abbreviations). Petrov et al. (2012) provide the mapping from each language-specific tagset to the universal tagset. The idea of using the universal tagset is of great use in multilingual applications, enabling comparison across languages. However, the mapping is not always straightforward. Ta</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Gertjan van Noord</author>
</authors>
<title>Exploring an auxiliary distribution based approach to domain adaptation of a syntactic disambiguation model.</title>
<date>2008</date>
<booktitle>In Coling 2008: Proceedings of the Workshop on Cross-Framework and Cross-Domain Parser Evaluation, CrossParser ’08,</booktitle>
<pages>9--16</pages>
<marker>Plank, van Noord, 2008</marker>
<rawString>Barbara Plank and Gertjan van Noord. 2008. Exploring an auxiliary distribution based approach to domain adaptation of a syntactic disambiguation model. In Coling 2008: Proceedings of the Workshop on Cross-Framework and Cross-Domain Parser Evaluation, CrossParser ’08, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Dirk Hovy</author>
<author>Anders Søgaard</author>
</authors>
<title>Learning part-of-speech taggers with inter-annotator agreement loss.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>742--751</pages>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="11542" citStr="Plank et al. (2014)" startWordPosition="1843" endWordPosition="1846">s. However, the mapping is not always straightforward. Table 2 shows the size of the annotated data for each language, the number of tags presented in the data, and the list of tags that are not matched. We can see that only 8 tags are presented in the annotated data for Danish, i.e, 4 tags (DET, PRT, PUNC, and NUM) are missing.2 Thus, a classifier using all 12 tags will be heavily penalized in the evaluation. Li et al. (2012) considered this problem and tried to manually modify the Danish mappings. Moreover, PRT is not really a universal tag since it only appears in 3 out of the 8 languages. Plank et al. (2014) pointed out that PRT often gets confused with ADP even in English. We will later show that the mapping problem causes substantial degradation in the performance of a POS tagger exploiting parallel data. The method we present here is more target-language oriented: our model is trained on the target language, in this way, only relevant information from the source language is retained. Thus, we automatically correct the mapping, and other incompatibilities arising from incorrect alignments and syntactic divergence between the source and target languages. Lang Size(k) # Tags Not Matched da 94 8 D</context>
</contexts>
<marker>Plank, Hovy, Søgaard, 2014</marker>
<rawString>Barbara Plank, Dirk Hovy, and Anders Søgaard. 2014. Learning part-of-speech taggers with inter-annotator agreement loss. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742–751, Gothenburg, Sweden, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siva Reddy</author>
<author>Serge Sharoff</author>
</authors>
<title>Cross language POS taggers (and other tools) for Indian languages: An experiment with Kannada using Telugu resources.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP workshop on Cross Lingual Information Access: Computational Linguistics and the Information Need of Multilingual Societies. (CLIA 2011 at IJNCLP 2011),</booktitle>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="4793" citStr="Reddy and Sharoff, 2011" startWordPosition="728" endWordPosition="731">on model T and the correction model P, based on a two-output maximum entropy model over tag pairs. Evaluating on the resource-poor language Malagasy, we achieved 85.6% accuracy, exceeding the state-of-the-art of 81.2% (Garrette et al., 2013). 2 Background and Related Work There is a wealth of prior work on multilingual POS tagging. The simplest approach takes advantage of the typological similarities that exist between languages pairs such as Czech and Russian, or Serbian and Croatian. They build the tagger — or estimate part of the tagger — on one language and apply it to the other language (Reddy and Sharoff, 2011, Hana et al., 2004). Yarowsky and Ngai (2001) pioneered the use of parallel data for projecting tag information from a resource-rich language to a resource-poor language. Duong et al. (2013b) used a similar method on using sentence alignment scores to rank the goodness of sentences. They trained a seed model from a small part of the data, then applied this model to the rest of the data using self-training with revision. Das and Petrov (2011) also used parallel data but additionally exploited graph-based label propagation to expand the coverage of labelled tokens. Each node in the graph repres</context>
</contexts>
<marker>Reddy, Sharoff, 2011</marker>
<rawString>Siva Reddy and Serge Sharoff. 2011. Cross language POS taggers (and other tools) for Indian languages: An experiment with Kannada using Telugu resources. In Proceedings of IJCNLP workshop on Cross Lingual Information Access: Computational Linguistics and the Information Need of Multilingual Societies. (CLIA 2011 at IJNCLP 2011), Chiang Mai, Thailand, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
<author>Khashayar Rohanimanesh</author>
</authors>
<title>Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data.</title>
<date>2007</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>8</volume>
<contexts>
<context position="34218" citStr="Sutton et al., 2007" startWordPosition="5748" endWordPosition="5751">llel EnglishMalagasy corpus of —100k sentences,4 and the POS annotated dataset developed by Garrette and Baldridge (2013), which comprises 4230 tokens for training and 5300 tokens for testing. 6.1 The model Traditionally, MaxEnt classifiers are trained using a single label.5 The method we propose is trained with pairs of output labels: one for the 4http://www.ark.cs.cmu.edu/global-voices/ 5Or else a sequence of labels, in the case of a conditional random field (Lafferty et al., 2001). However, even in this case, each token is usually assigned a single label. An exception is the factorial CRF (Sutton et al., 2007), which models several co-dependent sequences. Our approach is equivalent to a factorial CRF without edges between tags for adjacent tokens in the input. Malagasy tag (tM) and one for the universal tag (tU), which are both predicted conditioned on a Malagasy word (wM) in context. Our two-output model is defined as � D 1 P(tM, tU|wM) = Z(wM) exp λjfMj (w, tM) 1 j= )αjfB j (w, tM, tU) (3) where fM, fU, fB are the feature functions considering tM only, tU only, and over both outputs tM and tU respectively, and Z(wM) is the partition function. We can think of Eq. (3) as the combination of 3 models</context>
</contexts>
<marker>Sutton, McCallum, Rohanimanesh, 2007</marker>
<rawString>Charles Sutton, Andrew McCallum, and Khashayar Rohanimanesh. 2007. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. J. Mach. Learn. Res., 8:693–723, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Token and type constraints for cross-lingual part-of-speech tagging.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--1</pages>
<marker>T¨ackstr¨om, Das, Petrov, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and type constraints for cross-lingual part-of-speech tagging. Transactions of the Association for Computational Linguistics, 1:1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Mark Johnson</author>
</authors>
<title>A bayesian lda-based model for semi-supervised partof-speech tagging.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems 20,</booktitle>
<pages>1521--1528</pages>
<editor>In J.C. Platt, D. Koller, and Y. Singer a nd S.T. Roweis, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="13976" citStr="Toutanova and Johnson, 2008" startWordPosition="2268" endWordPosition="2272">f Das and Petrov (2011), which only uses directly projected labels (without label propagation), scored 81.3% for 8 languages. However, using the same model but with more parallel data, T¨ackstr¨om et al. (2013) scored 84.9% on the same test set. 3.2 Label projection We use the standard alignment tool Giza++ (Och and Ney, 2003) to word align the parallel data. We employ the Stanford POS tagger (Toutanova et al., 2003) to tag the English side of the parallel data and then project the label to the target side. It has been confirmed in many studies (T¨ackstr¨om et al., 2013, Das and Petrov, 2011, Toutanova and Johnson, 2008) that directly projected labels are noisy. Thus we need a method to reduce the noise. We employ the strategy of Yarowsky and Ngai (2001) of ranking sentences using a their alignment scores from IBM model 3. Firstly, we want to know how noisy the projected data is. Thus, we use the test data to build a simple supervised POS tagger using the TnT tagger (Brants, 2000) which employs a secondorder Hidden Markov Model (HMM). We tag the projected data and compare the label from direct projection and from the TnT tagger. The labels from the TnT Tagger are considered as pseudogold labels. Column “Witho</context>
</contexts>
<marker>Toutanova, Johnson, 2008</marker>
<rawString>Kristina Toutanova and Mark Johnson. 2008. A bayesian lda-based model for semi-supervised partof-speech tagging. In J.C. Platt, D. Koller, and Y. Singer a nd S.T. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1521–1528. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -Volume</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="13768" citStr="Toutanova et al., 2003" startWordPosition="2231" endWordPosition="2234">PM uses alignments to transfer tags from source to target language, the performance of DPM (and other models that exploit projection) largely depends on the quantity of parallel data. The “No LP” model of Das and Petrov (2011), which only uses directly projected labels (without label propagation), scored 81.3% for 8 languages. However, using the same model but with more parallel data, T¨ackstr¨om et al. (2013) scored 84.9% on the same test set. 3.2 Label projection We use the standard alignment tool Giza++ (Och and Ney, 2003) to word align the parallel data. We employ the Stanford POS tagger (Toutanova et al., 2003) to tag the English side of the parallel data and then project the label to the target side. It has been confirmed in many studies (T¨ackstr¨om et al., 2013, Das and Petrov, 2011, Toutanova and Johnson, 2008) that directly projected labels are noisy. Thus we need a method to reduce the noise. We employ the strategy of Yarowsky and Ngai (2001) of ranking sentences using a their alignment scores from IBM model 3. Firstly, we want to know how noisy the projected data is. Thus, we use the test data to build a simple supervised POS tagger using the TnT tagger (Brants, 2000) which employs a secondor</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -Volume 1 (NAACL ’03), pages 173–180, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Thorsten Brants</author>
</authors>
<title>Distributed word clustering for large scale class-based language modeling in machine translation. In</title>
<date>2008</date>
<booktitle>In ACL International Conference Proceedings.</booktitle>
<contexts>
<context position="19745" citStr="Uszkoreit and Brants, 2008" startWordPosition="3263" endWordPosition="3266">use the model had no explicit supervision. To see what features are meaningful for our model, we remove features in turn and report the result. The result in Table 5 disagrees with T¨ackstr¨om et al. (2013) on the word class features. They reported a gain of approximately 3% (absolute) using the word class. However, it seems to us that these features are not especially meaningful (at least in the present setting). Possible reasons for the discrepancy are that they train the word class model on a massive quantity of external monolingual data, or their algorithms for word clustering are better (Uszkoreit and Brants, 2008). We can see that the most informative features are Capitalization, Number and Punctuation. This makes sense because in languages such as German, capitalization is a strong indicator of NOUN. Number and punctuation features ensure that we classify NUM and PUNCT tags correctly. L = log P(A) YN P(t(i)|w(i)) i=1 = XD A2j + XN XD Ajfj(wi, ti) − log Z(wi) j=1 2S2 i=1 j=1 890 4 Correction Model In this section we incorporate the directly projected model into a second correction model trained on a small supervised sample of 1,000 annotated tokens. Our DPM model is not very accurate; as we have discus</context>
</contexts>
<marker>Uszkoreit, Brants, 2008</marker>
<rawString>Jakob Uszkoreit and Thorsten Brants. 2008. Distributed word clustering for large scale class-based language modeling in machine translation. In In ACL International Conference Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, NAACL ’01,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="4839" citStr="Yarowsky and Ngai (2001)" startWordPosition="736" endWordPosition="739">on a two-output maximum entropy model over tag pairs. Evaluating on the resource-poor language Malagasy, we achieved 85.6% accuracy, exceeding the state-of-the-art of 81.2% (Garrette et al., 2013). 2 Background and Related Work There is a wealth of prior work on multilingual POS tagging. The simplest approach takes advantage of the typological similarities that exist between languages pairs such as Czech and Russian, or Serbian and Croatian. They build the tagger — or estimate part of the tagger — on one language and apply it to the other language (Reddy and Sharoff, 2011, Hana et al., 2004). Yarowsky and Ngai (2001) pioneered the use of parallel data for projecting tag information from a resource-rich language to a resource-poor language. Duong et al. (2013b) used a similar method on using sentence alignment scores to rank the goodness of sentences. They trained a seed model from a small part of the data, then applied this model to the rest of the data using self-training with revision. Das and Petrov (2011) also used parallel data but additionally exploited graph-based label propagation to expand the coverage of labelled tokens. Each node in the graph represents a trigram in the target language. Each ed</context>
<context position="14112" citStr="Yarowsky and Ngai (2001)" startWordPosition="2293" endWordPosition="2296"> the same model but with more parallel data, T¨ackstr¨om et al. (2013) scored 84.9% on the same test set. 3.2 Label projection We use the standard alignment tool Giza++ (Och and Ney, 2003) to word align the parallel data. We employ the Stanford POS tagger (Toutanova et al., 2003) to tag the English side of the parallel data and then project the label to the target side. It has been confirmed in many studies (T¨ackstr¨om et al., 2013, Das and Petrov, 2011, Toutanova and Johnson, 2008) that directly projected labels are noisy. Thus we need a method to reduce the noise. We employ the strategy of Yarowsky and Ngai (2001) of ranking sentences using a their alignment scores from IBM model 3. Firstly, we want to know how noisy the projected data is. Thus, we use the test data to build a simple supervised POS tagger using the TnT tagger (Brants, 2000) which employs a secondorder Hidden Markov Model (HMM). We tag the projected data and compare the label from direct projection and from the TnT tagger. The labels from the TnT Tagger are considered as pseudogold labels. Column “Without Mapping” from Table 3 shows the average accuracy for the first nsentences (n = 60k, 100k, 200k, 500k) for 8 languages according to th</context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>David Yarowsky and Grace Ngai. 2001. Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, NAACL ’01, pages 1–8.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
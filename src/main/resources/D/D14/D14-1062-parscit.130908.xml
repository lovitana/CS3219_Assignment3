<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.988213">
Latent Domain Phrase-based Models for Adaptation
</title>
<author confidence="0.998211">
Hoang Cuong and Khalil Sima’an
</author>
<affiliation confidence="0.997028">
Institute for Logic, Language and Computation
University of Amsterdam
</affiliation>
<address confidence="0.933049">
Science Park 107, 1098 XG Amsterdam, The Netherlands
</address>
<email confidence="0.998139">
{c.hoang,k.simaan}@uva.nl
</email>
<sectionHeader confidence="0.994774" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999840714285714">
Phrase-based models directly trained
on mix-of-domain corpora can be
sub-optimal. In this paper we equip
phrase-based models with a latent domain
variable and present a novel method for
adapting them to an in-domain task rep-
resented by a seed corpus. We derive an
EM algorithm which alternates between
inducing domain-focused phrase pair
estimates, and weights for mix-domain
sentence pairs reflecting their relevance
for the in-domain task. By embedding
our latent domain phrase model in a
sentence-level model and training the
two in tandem, we are able to adapt all
core translation components together
– phrase, lexical and reordering. We
show experiments on weighing sentence
pairs for relevance as well as adapting
phrase-based models, showing significant
performance improvement in both tasks.
</bodyText>
<sectionHeader confidence="0.684655" genericHeader="keywords">
1 Mix vs. Latent Domain Models
</sectionHeader>
<bodyText confidence="0.999594307692308">
Domain adaptation is usually perceived as utiliz-
ing a small seed in-domain corpus to adapt an ex-
isting system trained on an out-of-domain corpus.
Here we are interested in adapting an SMT sys-
tem trained on a large mix-domain corpus Cmix
to an in-domain task represented by a seed paral-
lel corpus Cin. The mix-domain scenario is in-
teresting because often a large corpus consists of
sentence pairs representing diverse domains, e.g.,
news, politics, finance, sports, etc.
At the core of a standard state-of-the-art phrase-
based system (Och and Ney, 2004) is a phrase
table {(˜e, ˜f)} extracted from the word-aligned
</bodyText>
<figure confidence="0.5295584">
data together with estimates for Pt
f )
˜e|
training
and Pt(
</figure>
<bodyText confidence="0.999603933333334">
words often vary across domains, it is likely
that in a mix-domain corpus Cmix the translation
ambiguity will increase with the domain diver-
sity. Furthermore, the statistics in Cmix will re-
flect translation preferences averaged over the di-
verse domains. In this sense, phrase-based mod-
els trained on Cmix can be considered domain-
confused. This often leads to suboptimal perfor-
mance (Gasc´o et al., 2012; Irvine et al., 2013).
Recent adaptation techniques can be seen as
mixture models, where two or more phrase ta-
bles, estimated from in- and mix-domain corpora,
are combined together by interpolation, fill-up, or
multiple-decoding paths (Koehn and Schroeder,
2007; Bisazza et al., 2011; Sennrich, 2012; Raz-
mara et al., 2012; Sennrich et al., 2013). Here
we are interested in the specific question how to
induce a phrase-based model from Cmix for in-
domain translation? We view this as in-domain
focused training on Cmix, a complementary adap-
tation step which might precede any further com-
bination with other models, e.g., in-, mix- or
general-domain.
The main challenge is how to induce from Cmix
a phrase-based model for the in-domain task,
given only Cin as evidence? We present an ap-
proach whereby the contrast between in-domain
prior distributions and “out-domain” distributions
is exploited for softly inviting (or recruiting) Cmix
phrase pairs to either camp. To this end we in-
</bodyText>
<page confidence="0.975686">
566
</page>
<bodyText confidence="0.560602">
f  |˜e). Because the translations of
</bodyText>
<note confidence="0.8795775">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 566–576,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.979937833333333">
troduce a latent domain variable D to signify in-
(D1) and out-domain (D0) respectively.1
With the introduction of the latent variables, we
extend the translation tables in phrase-based mod-
els from generic Pt(˜e  |˜f) to domain-focused by
conditioning them on D, i.e., Pt(˜e  |˜f, D) and de-
composing them as follows:
Where P(D  |˜e, ˜f) is viewed as the latent phrase-
relevance models, i.e., the probability that a
phrase pair is in- (D1) or out-domain (D0). In the
end, our goal is to replace the domain-confused
tables, Pt(˜e  |˜f) and Pt( f˜  |˜e), with the in-domain
</bodyText>
<figure confidence="0.791753714285714">
d Pt( f  |˜e, D1 ).2
f
focused ones, Pt(˜e  |˜f, D1)
Note how Pt (˜e  |f , D1) and Pt
an
|˜e
˜f) and Pt( f˜  |˜e) as special case.
</figure>
<bodyText confidence="0.998069114285714">
Eq. 1 shows that the key to training the latent
phrase-based translation models is to train the la-
tent phrase-relevance models, P(D  |˜e, ˜f). Our
approach is to embed P(D  |˜e, ˜f) in asymmetric
sentence-level models P(D  |e, f) and train them
on Cmzx. We devise an EM algorithm where at
every iteration, in- or out-domain estimates pro-
vide full sentence pairs (e, f) with expectations
{P(D  |e, f)  |D E {0,1}}. Once these ex-
pectation are in Cmzx, we induce re-estimates for
the latent phrase-relevance models, P(D  |˜e, ˜f).
Metaphorically, during each EM iteration the cur-
rent in- or out-domain phrase pairs compete on
inviting Cmzx sentence pairs to be in- or out-
domain, which bring in new (weights for) in- and
out-domain phrases. Using the same algorithm we
also show how to adapt all core translation com-
ponents in tandem, including also lexical weights
and lexicalized reordering models.
Next we detail our model, the EM-based invita-
tion training algorithm and provide technical so-
lutions to a range of difficulties. We report exper-
1Crucially, the lack of explicit out-domain data in Cmix is
a major technical difficulty. We follow (Cuong and Sima’an,
2014) and in the sequel present a relatively efficient solution
based on a kind of “burn-in” procedure.
2It is common to use these domain-focused models as
additional features besides the domain-confused features.
However, here we are more interested in replacing the
domain-confused features rather than complementing them.
This distinguishes this work from other domain adaptation
literature for MT.
iments showing good instance weighting perfor-
mance as well as significantly improved phrase-
based translation performance.
</bodyText>
<sectionHeader confidence="0.385959" genericHeader="introduction">
2 Model and training by invitation
</sectionHeader>
<bodyText confidence="0.998990571428571">
Eq. 1 shows that the key to training the latent
phrase-based translation models is to train the la-
tent phrase-relevance models, P(D  |˜e, ˜f). As
mentioned, for training P(D  |˜e, ˜f) on parallel
sentences in Cmzx we embed them in two asym-
metric sentence-level models {P(D  |e, f)  |D E
{0,1}}.
</bodyText>
<subsectionHeader confidence="0.997077">
2.1 Domain relevance sentence models
</subsectionHeader>
<bodyText confidence="0.9995434">
Intuitively, sentence models for domain relevance
P(D  |e, f) are somewhat related to data selec-
tion approaches (Moore and Lewis, 2010; Axel-
rod et al., 2011). The dominant approach to data
selection uses the contrast between perplexities
of in- and mix-domain language models.3 In the
translation context, however, often a source phrase
has different senses/translations in different do-
mains, which cannot be distinguished with mono-
lingual language models (Cuong and Sima’an,
2014). Therefore, our proposed latent sentence-
relevance model includes two major latent com-
ponents - monolingual domain-focused relevance
models and domain-focused translation models
derives as follows:
</bodyText>
<equation confidence="0.995566666666667">
P(e, f, D)
P(D  |e, f) = (2) f,D), (2)
ED∈{D1,D0}
</equation>
<bodyText confidence="0.828621">
where P(e, f, D) can be decomposed as:
</bodyText>
<equation confidence="0.996764">
P(f, e, D) = 1 (P(D)Plm(e  |D)Pt(f  |e, D)
2
)+ P(D)Plm(f  |D)Pt(e  |f, D) .
(3)
</equation>
<bodyText confidence="0.651321">
Here
</bodyText>
<listItem confidence="0.96854375">
• Pt(e|f, D) and similarly Pt(f|e, D): the latent
domain-focused translation models aim at cap-
turing the faithfulness of translation with re-
spect to different domains. We simplify this as
</listItem>
<footnote confidence="0.8126038">
3Note that earlier work on data selection exploits the con-
trast between in- and mix-domain. In (Cuong and Sima’an,
2014), we present the idea of using the language and transla-
tion models derived separately from in- and out-domain data,
and show how it helps for data selection.
</footnote>
<equation confidence="0.966420222222222">
Pt(˜e
f, D) = |
Ee˜ Pt(˜e |
Pt(˜e |
˜f)P(D  |˜e,
˜f)
, f)
˜e
f)P(D
e
˜
|
, D1) contains
Pt (
567
“bag-of-possible-phrases” translation models:4
YPt(e|f, D) := (˜e, ˜f)EA(e,f) Pt(˜e |˜f, D)c(˜e,
(4)
</equation>
<bodyText confidence="0.914716666666667">
where A(e, f) is the multiset of phrases in
he, fi and c(·) denotes their count. Sub-model
Pt(˜e |˜f, D) is given by Eq. 1.
</bodyText>
<listItem confidence="0.9839191875">
• Plm(e|D), Plm(f|D): the latent monolingual
domain-focused relevance models aim at cap-
turing the relevance of e and f for identifying
domain D but here we consider them language
models (LMs).5 As mentioned, the out-domain
LMs differ from previous works, e.g., (Axel-
rod et al., 2011), which employ mix-domain
LMs. Here, we stress the difficulty in finding
data to train out-domain LMs and present a so-
lution based on identifying pseudo out-domain
data.
• P(D): the domain priors aim at modeling
the percentage of relevant data that the learn-
ing framework induces. It can be estimated
via phrase-level parameters but here we prefer
sentence-level parameters:6
</listItem>
<equation confidence="0.975119666666667">
= P(e,f)ECmix P(D  |e, f)
P(D) : P P(e,f)ECmix P(D  |e, f) (5)
D
</equation>
<subsectionHeader confidence="0.992495">
2.2 Training by invitation
</subsectionHeader>
<bodyText confidence="0.99972425">
Generally, our model can be viewed to have latent
parameters O = {OD0, OD1}. The training pro-
cedure seeks O that maximize the log-likelihood
of the observed sentence pairs he, fi ∈ Cmix:
</bodyText>
<equation confidence="0.999043">
L = X (e,f)ECmix log XD PΘD (D, e, f). (6)
</equation>
<bodyText confidence="0.7085194">
It is obvious that there does not exist a closed-form
solution for Equation 6 because of the existence of
4We design our latent domain translation models with ef-
ficiency as our main concern. Future extensions could in-
clude the lexical and reordering sub-models (as suggested by
an anonymous reviewer.)
5Relevance for identification or retrieval could be differ-
ent from frequency or fluency. We leave this extension for
future work.
6It should be noted that in most phrase-based SMT sys-
tems bilingual phrase probabilities are estimated heuristically
from word alignmened data which often leads to overfitting.
Estimating P(D) from sentence-level parameters rather than
from phrase-level parameters helps us avoid the overfitting
which often accompanies phrase extraction.
the log-term log P. The EM algorithm (Dempster
et al., 1977) comes as an alternative solution to fit
the model. It can be seen to maximize L via block-
coordinate ascent on a lower bound F(q, O) using
an auxiliary distribution q(D  |e, f)
</bodyText>
<equation confidence="0.95385775">
f) e,
F(q, O) = P(e,f) PD q(D  |e, f) log q(D PΘ (D,
� e, f)
(7)
</equation>
<bodyText confidence="0.9995915">
where the inequality results, i.e., L ≥ F(q, O),
derived from log being concave and Jensen’s in-
equality. We rewrite the Free Energy F(q, O)
(Neal and Hinton, 1999) as follows:
</bodyText>
<equation confidence="0.9984775">
D e, f
= X(e,f) XD q(D  |e, f) log Pq(D  |e, f)
+ X(e,f) XD q(D  |e, f) log PΘ (e, f)
X
=(e,f) log PΘ(e, f) (8)
− KL[q(D  |e, f)  ||PΘD(D  |e, f)],
</equation>
<bodyText confidence="0.95348225">
where KL[·  ||·] is the KL-divergence.
With the introduction of the KL-divergence, the
alternating E and M steps for our EM algorithm
are easily derived as
</bodyText>
<equation confidence="0.999847125">
E-step : qt+1 (9)
argmaxq(D �e,f) F(q, Ot) =
argminq(D �e,f) KL[q(D|e,f)  ||PΘtD(D|e,f)]
= PΘtD(D  |e, f)
M-step : Ot+1 (10)
argmaxΘ F(qt+1, O) =
XargmaxΘ X q(D  |e, f) log PΘD(D, e, f)
(e,f) D
</equation>
<bodyText confidence="0.8695935">
The iterative procedure is illustrated in Fig-
ure 1.7 At the E-step, a guess for P(D  |˜e, ˜f) can
be used to update Pt( f˜  |˜e, D) and Pt(˜e  |˜f, D)
(i.e., using Eq. 1) and consequently Pt(f  |e, D)
and Pt(e  |f, D) (i.e., using Eq. 4). These resulting
table estimates, together with the domain-focused
LMs and the domain priors are served as expected
counts to update P(D  |e, f).8 At the M-step,
7For simplicity, we ignore the LMs and prior models in
the illustration in Fig. 1.
8Since we only use the in-domain corpus as priors to ini-
tilize the EM parameters, in technical perspective we do not
want P(D I e, f) parameters to go too far off from the initial-
ization. We therefore prefer the averaged style in practice,
i.e., at the iteration n we update the P(D Ie, f) parameters,
P(n)(D|e, f) as 1n(P(n)(D I e, f) + �n−1
</bodyText>
<equation confidence="0.961828666666667">
i=1 P(i)(D I e, f)).
˜f)
,
</equation>
<page confidence="0.9817">
568
</page>
<figureCaption confidence="0.993519">
Figure 1: Our probabilistic invitation framework.
</figureCaption>
<bodyText confidence="0.997968611111111">
the new estimates for P(D  |e, f) can be used to
(softly) fill in the values of hidden variable D and
estimate parameters P(D  |˜e, ˜f) and P(D). The
EM is guaranteed to converge to a local maximum
of the likelihood under mild conditions (Neal and
Hinton, 1999).
Before EM training starts we must provide a
“reasonable” initial guess for P(D  |˜e, ˜f). We
must also train the out-domain LMs, which needs
the construction of pseudo out-domain data.9
One simple way to do that is inspired by burn-
in in sampling, under the guidance of an in-
domain data set, Cin as prior. At the begin-
ning, we train Pt(˜e  |˜f, D1) and Pt( f˜  |˜e, D1)
for all phrases learned from Cin. We also train
Pt(˜e  |˜f) and Pt( f˜  |˜e) for all phrases learned
from Cmix. During burn-in we assume that the
out-domain phrase-based models are the domain-
</bodyText>
<equation confidence="0.4527555">
confused phrase-bas |
ed models, i.e., Pt ( ˜e f , D0)
˜f) and Pt( f  |e, D0) ≈ Pt ( f  |˜e). We
≈ Pt(˜e |
</equation>
<bodyText confidence="0.905375619047619">
isolate all the LMs and the prior models from our
model, and apply a single EM iteration to update
P(D  |e, f) based on those domain-focused mod-
els Pt(˜e  |˜f,D) and Pt( f˜  |˜e, D).
In the end, we use P(D  |e, f) to fill in the val-
ues of hidden variable D in Cmix, so it provides
us with an initialization for P(D  |˜e, ˜f). Subse-
quently, we also rank sentence pairs in Cmix with
P(D1  |e, f) and select a subset of smallest scor-
ing pairs as a pseudo out-domain subset to train
Plm(e  |D0) and Plm(f  |D0). Once the latent
domain-focused LMs have been trained, the LM
probabilities stay fixed during EM. Crucially, it
9The in-domain LMs Pl.(e  |Dl) and Pl.(f  |Dl) can
be simply trained on the source and target sides of Cj� re-
spectively.
is important to scale the probabilities of the four
LMs to make them comparable: we normalize the
probability that a LM assigns to a sentence by the
total probability this LM assigns to all sentences
in Cmix.
</bodyText>
<sectionHeader confidence="0.993042" genericHeader="method">
3 Intrinsic evaluation
</sectionHeader>
<bodyText confidence="0.99997496875">
We evaluate the ability of our model to retrieve
“hidden” in-domain data in a large mix-domain
corpus, i.e., we hide some in-domain data in a
large mix-domain corpus. We weigh sentence
pairs under our model with P(D1  |˜e, ˜f) and
P(D1  |e, f) respectively. We report pseudo-
precision/recall at the sentence-level using a
range of cut-off criteria for selecting the top
scoring instances in the mix-domain corpus. A
good relevance model expects to score higher for
the hidden in-domain data.
Baselines Two standard perplexity-based se-
lection models in the literature have been
implemented as the baselines: cross-entropy
difference (Moore and Lewis, 2010) and bilingual
cross-entropy difference (Axelrod et al., 2011),
investigating their ability to retrieve the hiding
data as well. Training them over the data to learn
the sentences with their relevance, we then rank
the sentences to select top of pairs to evaluate the
pseudo-precision/recall at the sentence-level.
Results We use a mix-domain corpus Cg of 770K
sentence pairs of different genres.10 There is also
a Legal corpus of 183K pairs that serves as the
in-domain data. We create Cmix by selecting an
arbitrary 83K pairs of in-domain pairs and adding
them to Cg (the hidden in-domain data); we use
the remaining 100k in-domain pairs as Cin.
To train the baselines, we construct interpo-
lated 4-gram Kneser-Ney LMs using BerkeleyLM
(Pauls and Klein, 2011). Training our model on
the data takes six EM-iterations to converge.11
</bodyText>
<footnote confidence="0.783526333333333">
10Count of sentence pairs: European Parliament (Koehn,
2005): 183, 793; Pharmaceuticals: 190, 443, Software:
196, 168, Hardware: 196, 501.
11After the fifth EM iteration we do not observe any sig-
nificant increase in the likelihood of the data. Note that we
use the same setting as for the baselines to train the latent
domain-focused LMs for use in our model – interpolated 4-
gram Kneser-Ney LMs using BerkeleyLM. This training set-
ting is used for all experiments in this work.
</footnote>
<equation confidence="0.882952333333333">
Update sentence-level parameters
P(D|˜e, ˜f) P(D|e, f)
P(f, e, D)
P(˜e |˜f, D)
P(˜f|˜e, D)
P(e|f, D)
P(f|e, D)
Phrase-level Sentence-level
Re-update phrase-level parameters
</equation>
<page confidence="0.727555">
569
</page>
<figure confidence="0.999435170212766">
Pseudo-Precision (Sentence-Level)
100
95
90
85
80
45
40
75
70
65
60
55
50
35
30
25
20
15
10
0
5
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Top Percentage
Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5
(a): Pseudo-Precision (Sentence-Level)
Pseudo-Recall (Sentence-Level)
45
40
70
65
60
55
50
35
30
25
20
15
10
0
5
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Top Percentage
Iter. 1 Iter. 2
(b): Pseudo-Recall (Sentence-Level)
Iter. 3 Iter. 4 Iter. 5
</figure>
<figureCaption confidence="0.710663">
Fig. 2 helps us examine how the pseudo sen-
</figureCaption>
<bodyText confidence="0.991981681818182">
tence invitation are done during each EM iter-
ation. For later iterations we observe a better
pseudo-precision and pseudo-recall at sentence-
level (Fig. 2(a), Fig. 2(b)). Fig. 2 also reveals
a good learning capacity of our learning frame-
work. Nevertheless, we observe that the baselines
do not work well for this task. This is not new,
as pointed out in our previous work (Cuong and
Sima’an, 2014).
Which component type contributes more to the
performance, the latent domain language models
or the latent domain translation models? Further
experiments have been carried on to neutralize
each component type in turn and build a selection
system with the rest of our model parameters. It
turns out that the latent domain translation mod-
els are crucial for performance for the learning
framework, while the latent domain LMs make a
far smaller yet substantial contribution. We refer
readers to our previous work (Cuong and Sima’an,
2014), which provides detail analysis of the data
selection problem.
</bodyText>
<sectionHeader confidence="0.983997" genericHeader="method">
4 Translation experiments: Setting
</sectionHeader>
<bodyText confidence="0.998960333333333">
Data We use a mix-domain corpus consisting of
4M sentence pairs, collected from multiple re-
sources including EuroParl (Koehn, 2005), Com-
mon Crawl Corpus, UN Corpus, News Commen-
tary. As in-domain corpus we use “Consumer
and Industrial Electronics” manually collected
by Translation Automation Society (TAUS.com).
The corpus statistics are summarized in Table 1.
System We train a standard state-of-the-art
</bodyText>
<table confidence="0.999513666666667">
English Spanish
Sents 113.7M 4M
Words 127.1M
Sents 109K
Words 1, 485, 558 1, 685, 716
Sents 984
Words 13130 14, 955
Sents 982
Words 13, 493 15, 392
</table>
<tableCaption confidence="0.999827">
Table 1: The data preparation.
</tableCaption>
<bodyText confidence="0.999787166666667">
phrase-based system, using it as the baseline.12
There are three main kinds of features for the
translation model in the baseline - phrase-based
translation features, lexical weights (Koehn et al.,
2003) and lexicalized reordering features (Koehn
et al., 2005).13 Other features include the penal-
ties for word, phrase and distance-based reorder-
ing.
The mix-domain corpus is word-aligned using
GIZA++ (Och and Ney, 2003) and symmetrized
with grow(-diag)-final-and (Koehn et al., 2003).
We limit phrase length to a maximum of seven
words. The LMs are interpolated 4-grams with
Kneser-Ney, trained on 2.2M English sentences
from Europarl augmented with 248.8K sentences
from News Commentary Corpus (WMT 2013).
We tune the system using k-best batch MIRA
(Cherry and Foster, 2012). Finally, we use Moses
</bodyText>
<footnote confidence="0.8641494">
12We use Stanford Phrasal - a standard state-of-the-art
phrase-based translation system developed by Cer et al.
(2010).
13The lexical weights and the lexical reordering features
will be described in more detail in Section 6.
</footnote>
<figureCaption confidence="0.997148">
Figure 2: Intrinsic evaluation.
</figureCaption>
<figure confidence="0.928013714285714">
Cmix
Cin
Domain:
Electronics
Dev
Test
570
</figure>
<figureCaption confidence="0.991493">
Figure 3: BLEU averaged over multiple runs.
</figureCaption>
<figure confidence="0.967308933333333">
Electrics (Training Data: 1 Million)
21.2
21
20.8
20
19.91
20.64
20.51 20.52
20.6
20.4
20.2
20.5
20.48
19.8
Baseline Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5
</figure>
<figureCaption confidence="0.661904">
(Koehn et al., 2007) as decoder.14
</figureCaption>
<bodyText confidence="0.997641733333333">
We report BLEU (Papineni et al., 2002), ME-
TEOR 1.4 (Denkowski and Lavie, 2011) and TER
(Snover et al., 2006), with statistical significance
at 95% confidence interval under paired bootstrap
re-sampling (Press et al., 1992). For every system
reported, we run the optimizer at least three times,
before running MultEval (Clark et al., 2011) for
resampling and significance testing.
Outlook In Section 5 we examine the effect of
training only the latent domain-focused phrase ta-
ble using our model. In Section 6 we proceed fur-
ther to estimate also latent domain-focused lexical
weights and lexicalized reordering models, exam-
ining how they incrementally improve the transla-
tion as well.
</bodyText>
<sectionHeader confidence="0.945562" genericHeader="method">
5 Adapting phrase table only
</sectionHeader>
<bodyText confidence="0.998498545454546">
Here we investigate the effect of adapting the
phrase table only; we will delay adapting the
lexical weights and lexicalized reordering fea-
tures to Section 6. We build a phrase-based sys-
tem with the usual features as the baseline, in-
cluding two bi-directional phrase-based models,
plus the penalties for word, phrase and distortion.
We also build a latent domain-focused phrase-
based system with the two bi-directional latent
phrase-based models, and the standard penalties
described above.
We explore training data sizes 1M, 2M
and 4M sentence pairs. Three baselines are
trained yielding 95.77M, 176.29M and 323.88M
phrases respectively. We run 5 EM iterations to
14While we implement the latent domain phrase-based
models using Phrasal for some advantages, we prefer to use
Moses for decoding.
train our learning framework. We use the pa-
rameter estimates for P(D  |˜e, ˜f) derived at each
EM iteration to train our latent domain-focused
phrase-based systems. Fig. 3 presents the results
(in BLEU) at each iteration in detail for the case of
1M sentence pairs. Similar improvements are ob-
served for METEOR and TER. Here, we consis-
tently observe improvements at p-value = 0.0001
for all cases.
It should be noted that when doubling the train-
ing data to 2M and 4M, we observe the similar
results.
Finally, for all cases we report their best result
in Table 2. Here, note how the improvement could
be gained when doubling the training data.
</bodyText>
<table confidence="0.927303285714286">
Data System Avg A P-value
1M Baseline 19.91 − −
Our System 20.64 +0.73 0.0001
2M Baseline 20.54 − −
Our System 21.41 +0.87 0.0001
4M Baseline 21.44 − −
Our System 22.62 +1.18 0.0001
</table>
<tableCaption confidence="0.999457">
Table 2: BLEU averaged over multiple runs.
</tableCaption>
<bodyText confidence="0.992849666666667">
It is also interesting to consider the average
entropy of phrase table entries in the domain-
confused systems, i.e.,
</bodyText>
<equation confidence="0.9201518">
− E(˜e,˜f) pt(˜e |˜f) log pt(˜e |˜f)
number of phrases(˜e, ˜f)
against that in the domain-focused systems
E(˜e,˜f) pt(˜e |˜f, D1) log pt(˜e |˜f, D1)
number of phrases(˜e, ˜f) .
</equation>
<bodyText confidence="0.9996555">
Following (Hasler et al., 2014) in Table 3 we also
show that the entropy decreases significantly in
</bodyText>
<page confidence="0.99496">
571
</page>
<bodyText confidence="0.948053666666667">
the adapted tables in all cases, which indicates that
the distributions over translations of phrases have
become sharper.
</bodyText>
<table confidence="0.937306">
Baseline Iter. 1 Iter.2 Iter.3 Iter.4 Iter.5
0.210 0.187 0.186 0.185 0.185 0.184
</table>
<tableCaption confidence="0.999685">
Table 3: Average entropy of distributions.
</tableCaption>
<bodyText confidence="0.999752461538462">
In practice, the third iteration systems usually
produce best translations. This is somewhat ex-
pected because as EM invites more pseudo in-
domain pairs in later iterations, it sharpens the
estimates of P(D1  |˜e, ˜f), making pseudo out-
domain pairs tend to 0.0. Table 4 shows the per-
centage of entries with P(D1  |˜e, ˜f) &lt; 0.01 at
every iteration, e.g., 34.52% at the fifth iteration.
This induced schism in Cmix diminishes the dif-
ference between the relevance scores for certain
sentence pairs, limiting the ability of the latent
phrase-based models to further discriminate in the
gray zone.
</bodyText>
<table confidence="0.926023">
Entries P(D1 |˜f, ˜e) &lt; 0.01
Iter. 1 22.82%
Iter.2 27.06%
Iter.3 30.07%
Iter.4 32.47%
Iter.5 34.52%
</table>
<tableCaption confidence="0.998129">
Table 4: Phrase analyses.
</tableCaption>
<bodyText confidence="0.999735615384616">
Finally, to give a sense of the improvement
in translation, we (randomly) select cases where
the systems produce different translations and
present some of them in Table 5. These ex-
amples are indeed illuminating, e.g., “can repro-
duce signs of audio”/“can play signals audio”,
“password teacher”/“password master”, reveal-
ing thoroughly the benefit derived from adapting
the phrase models from being domain-confused to
being domain-focused. Table 6 presents phrase ta-
ble entries, i.e., pt(e  |f) and pt(e  |f, D1), for the
“can reproduce signs of audio”/“can play signals
audio” example.
</bodyText>
<sectionHeader confidence="0.974943" genericHeader="method">
6 Fully adapted translation model
</sectionHeader>
<bodyText confidence="0.997791666666667">
The preceding experiments reveal that adapting
the phrase tables significantly improves transla-
tion performance. Now we also adapt the lexical
</bodyText>
<table confidence="0.9985925">
Entries se˜nales reproducir
signals signs play reproduce
Baseline 0.29 0.36 0.15 0.20
Iter.1 0.36 0.23 0.29 0.16
Iter.2 0.37 0.19 0.32 0.17
Iter.3 0.37 0.17 0.34 0.16
Iter.4 0.37 0.16 0.36 0.16
Iter.5 0.37 0.15 0.37 0.16
</table>
<tableCaption confidence="0.998427">
Table 6: Phrase entry examples.
</tableCaption>
<figureCaption confidence="0.4839075">
and reordering components. The result is a fully
adapted, domain-focused, phrase-based system.
</figureCaption>
<bodyText confidence="0.998548333333333">
Briefly, the lexical weights provide smooth es-
timates for the phrase pair based on word trans-
lation scores P(e  |f) between pairs of words
</bodyText>
<equation confidence="0.946972">
(e, f), i.e., P(e  |f) = c(e,f)
� e c(e,f) (Koehn et
</equation>
<bodyText confidence="0.741579">
al., 2003). Our latent domain-focused lexical
weights, on the other hand, are estimated ac-
</bodyText>
<equation confidence="0.948729333333333">
cording to P(e  |f, D1), i.e., P(e  |f, D1) =
P(e  |f)P(D1  |e, f)
Ef P(e  |f)P(D1  |e, f).
</equation>
<bodyText confidence="0.999755272727273">
The lexicalized reordering models with orien-
tation variable O, P(O  |˜e, ˜f), model how likely
a phrase (˜e, ˜f) directly follows a previous phrase
(monotone), swaps positions with it (swap), or
is not adjacent to it (discontinous) (Koehn et al.,
2005). We make these domain-focused:
the estimates of P(D1  |e, f) during EM.
The baseline for the following experiments is a
standard state-of-the-art phrase-based system, in-
cluding two bi-directional phrase-based transla-
tion features, two bi-directional lexical weights,
six lexicalized reordering features, as well as the
penalties for word, phrase and distortion. We de-
velop three kinds of domain-adapted systems that
are different at their adaptation level to fit the task.
The first (Sys. 1) adapts only the phrase-based
models, using the same lexical weights, lexical-
ized reordering models and other penalties as the
baseline. The second (Sys. 2) adapts also the lex-
ical weights, fixing all other features as the base-
line. The third (Sys. 3) adapts both the phrase-
based models, lexical weights and lexicalized re-
</bodyText>
<figure confidence="0.9876003">
Estimating P(D1  |O, ˜e,
˜f) and P(D1  |e, f) is
˜f) and hinges on
similar to estimating P(D1  |˜e,
P(O  |˜e, ˜f, D1) = P(O  |˜e,
EO P(O  |˜e, ˜f)P(D1  |O, ˜e,
˜f)P(D1  |O, ˜e,
˜f)
˜f)
(11)
</figure>
<page confidence="0.629871">
572
</page>
<table confidence="0.580629846153846">
Translation Examples
Input El reproductor puede reproducir se˜nales de audio grabadas en mix-mode cd, cd-g, cd-extra y cd text.
Reference The player can play back audio signals recorded in mix-mode cd, cd-g, cd-extra and cd text.
Baseline The player can reproduce signs of audio recorded in mix-mode cd, cd-g, cd-extra and cd text.
Our System The player can play signals audio recorded in mix-mode cd, cd-g, cd-extra and cd text.
Input Se puede crear un archivo autodescodificable cuando el archivo codificado se abre con la contrase˜na maestra.
Reference A self-decrypting file can be created when the encrypted file is opened with the master password.
Baseline To create an file autodescodificable when the file codified commenced with the password teacher.
Our System You can create an archive autodescodificable when the file codified opens with the password master.
Input Repite todas las pistas (´unicamente cds de v´ıdeo sin pbc)
Reference Repeat all tracks (non-pbc video cds only)
Baseline Repeated all avenues (only cds video without pbc)
Our System Repeated all the tracks (only cds video without pbc)
</table>
<tableCaption confidence="0.7177825">
Table 5: Translation examples yielded by a domain-confused phrase-based system (the baseline) and a
domain-focused phrase-based system (our system).
ordering models15, fixing other penalties as the
baseline.
</tableCaption>
<table confidence="0.930981086956522">
Metric System Avg A p-value
Consumer and Industrial Electronics
(In-domain: 109K pairs; Dev: 982 pairs; Test: 984 pairs)
BLEU Baseline 22.9 − −
METEOR Sys. 1 23.4 +0.5 0.008
TER Sys. 2 23.9 +1.0 0.0001
Sys. 3 24.0 +1.1 0.0001
Baseline − −
Sys. 1 +0.4 0.0001
Sys. 2 +0.8 0.0001
Sys. 3 +0.9 0.0001
Baseline − −
Sys. 1 -0.7 0.0001
Sys. 2 -1.5 0.0001
Sys. 3 -1.6 0.0001
30.0
30.4
30.8
30.9
59.5
58.8
58.0
57.9
</table>
<tableCaption confidence="0.980401666666667">
Table 7: Metric scores for the systems, which are
averages over multiple runs.
Table 7 presents results for training data size
</tableCaption>
<bodyText confidence="0.920506875">
of 4M parallel sentences. It shows that the fully
domain-focused system (Sys. 3) significantly im-
proves over the baseline. The table also shows
that the latent domain-focused phrase-based mod-
els and lexical weights are crucial for the im-
proved performance, whereas adapting the re-
ordering models makes a far smaller contribution.
Finally we also apply our approach to other
</bodyText>
<footnote confidence="0.5534434">
15We run three EM iterations to train our invitation frame-
work, and then use the parameter estimates for P(Dl  |˜e, ˜f),
P(Dl  |e, f) and P(D,  |O, ˜e, ˜f) to train these domain-
focused features. We adopt this training setting for all other
different tasks in the sequel.
</footnote>
<bodyText confidence="0.999925428571429">
tasks where the relation between their in-domain
data and the mix-domain data varies substantially.
Table 8 presents their in-domain, tuning and test
data in detail, as well as the translation results
over them. It shows that the fully domain-focused
systems consistently and significantly improve the
translation accuracy for all the tasks.
</bodyText>
<sectionHeader confidence="0.81795" genericHeader="method">
7 Combining multiple models
</sectionHeader>
<bodyText confidence="0.99956025">
Finally, we proceed further to test our latent
domain-focused phrase-based translation model
on standard domain adaptation. We conduct ex-
periments on the task “Professional &amp; Business
Services” as an example.16 For standard adap-
tation we follow (Koehn and Schroeder, 2007)
where we pass multiple phrase tables directly to
the Moses decoder and tune them together. For
baseline we combine the standard phrase-based
system trained on Cmix with the one trained on
the in-domain data Cin. We also combine our la-
tent domain-focused phrase-based system with the
one trained on Cin. Table 9 presents the results
showing that combining our domain-focused sys-
tem adapted from Cmix with the in-domain model
outperforms the baseline.
</bodyText>
<footnote confidence="0.92673875">
16We choose this task for additional experiments because
it has very small in-domain data (23K). This is supposed
to make adaptation difficult because of the robust large-scale
systems trained on C.j..
</footnote>
<page confidence="0.994897">
573
</page>
<note confidence="0.769123">
Metric System Avg A P-value Metric System Avg A P-value
</note>
<table confidence="0.965778882352941">
Professional &amp; Business Services
(In-domain: 23K pairs; Dev: 1, 000 pairs; Test: 998 pairs)
Baseline 22.0 − −
BLEU Our System 23.1 +1.1 0.0001
METEOR Baseline 30.8 − −
Our System 31.4 +0.6 0.0001
Baseline 58.0 − −
TER Our System 56.6 -1.4 0.0001
Financials
(In-domain: 31K pairs; Dev: 1, 000 pairs; Test: 1, 000 pairs)
Baseline 31.1 − −
BLEU Our System 31.8 +0.7 0.0001
METEOR Baseline 36.3 − −
Our System 36.6 +0.3 0.0001
Baseline 48.8 − −
TER Our System 48.3 -0.5 0.0001
Computer Hardware
(In-domain: 52K pairs; Dev: 1, 021 pairs; Test: 1, 054 pairs)
Baseline 24.6 − −
BLEU Our System 25.3 +0.7 0.0001
METEOR Baseline 32.4 − −
Our System 33.1 +0.7 0.0001
Baseline 56.4 − −
TER Our System 55.0 -1.4 0.0001
Computer Software
(In-domain: 65K pairs; Dev: 1, 100 pairs; Test: 1, 000 pairs)
Baseline 27.4 − −
BLEU Our System 28.3 +0.9 0.0001
METEOR Baseline 34.0 − −
Our System 34.7 +0.7 0.0001
Baseline 51.7 − −
TER Our System 50.6 -1.1 0.0001
Pharmaceuticals &amp; Biotechnology
(In-domain: 85K pairs; Dev: 920 pairs; Test: 1, 000 pairs)
</table>
<tableCaption confidence="0.994364">
Table 8: Metric scores for the systems, which are
averages over multiple runs.
</tableCaption>
<sectionHeader confidence="0.998913" genericHeader="method">
8 Related work
</sectionHeader>
<bodyText confidence="0.999931538461538">
A distantly related, but clearly complementary,
line of research focuses on the role of docu-
ment topics (Eidelman et al., 2012; Zhang et al.,
2014; Hasler et al., 2014). An off-the-shelf Latent
Dirichlet Allocation tool is usually used to infer
document-topic distributions. On one hand, this
setting may not require in-domain data as prior.
On the other hand, it requires meta-information
(e.g., document information).
Part of this work (the latent sentence-relevance
models) relates to data selection (Moore and
Lewis, 2010; Axelrod et al., 2011), where
sentence-relevance weights are used for hard-
</bodyText>
<subsectionHeader confidence="0.630604">
Professional &amp; Business Services
</subsectionHeader>
<note confidence="0.474652">
(In-domain: 23K pairs; Dev: 1, 000 pairs; Test: 998 pairs)
</note>
<table confidence="0.999915888888889">
BLEU In-domain 46.5 − −
+ Mix-domain 46.6 − −
+ Our system 47.9 +1.3 0.0001
METEOR In-domain 39.8 − −
+ Mix-domain 40.1 − −
+ Our System 41.1 +1.0 0.0001
In-domain 38.2 − −
TER + Mix-domain 38.0 − −
+ Our System 36.9 -1.1 0.0001
</table>
<tableCaption confidence="0.963135">
Table 9: Domain adaptation experiments. Metric
scores for the systems, which are averages over
multiple runs.
</tableCaption>
<bodyText confidence="0.983460238095238">
filtering rather than weighting. The idea of using
sentence-relevance estimates for phrase-relevance
estimates relates to Matsoukas et al. (2009) who
estimate the former using meta-information over
documents as main features. In contrast, our work
overcomes the mutual dependence of sentence and
phrase estimates on one another by training both
models in tandem.
Adaptation using small in-domain data has
a different but complementary goal to another
line of research aiming at combining a domain-
adapted system with the another trained on the in-
domain data (Koehn and Schroeder, 2007; Bisazza
et al., 2011; Sennrich, 2012; Razmara et al., 2012;
Sennrich et al., 2013). Our work is somewhat re-
lated to, but markedly different from, phrase pair
weighting (Foster et al., 2010). Finally, our latent
domain-focused phrase-based models and invita-
tion training paradigm can be seen to shift atten-
tion from adaptation to making explicit the role of
domain-focused models in SMT.
</bodyText>
<sectionHeader confidence="0.994744" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999801">
We present a novel approach for in-domain fo-
cused training of a phrase-based system on a
mix-of-domain corpus by using prior distributions
from a small in-domain corpus. We derive an EM
training algorithm for learning latent domain rel-
evance models for the phrase- and sentence-levels
in tandem. We also show how to overcome the
difficulty of lack of explicit out-domain data by
bootstrapping pseudo out-domain data.
In future work, we plan to explore generative
Bayesian models as well as discriminative learn-
ing approaches with different ways for estimat-
</bodyText>
<table confidence="0.999619666666667">
Baseline 31.6 − −
BLEU Our System 32.4 +0.8 0.0001
METEOR Baseline 34.0 − −
Our System 34.4 +0.4 0.0001
Baseline 51.4 − −
TER Our System 50.6 -0.8 0.0001
</table>
<page confidence="0.997248">
574
</page>
<bodyText confidence="0.99981475">
ing the latent domain relevance models. We hy-
pothesize that bilingual, but also monolingual, rel-
evance models can be key to improved perfor-
mance.
</bodyText>
<sectionHeader confidence="0.990991" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999892727272727">
We thank Ivan Titov for stimulating discussions,
and three anonymous reviewers for their com-
ments on earlier versions. The first author is sup-
ported by the EXPERT (EXPloiting Empirical ap-
pRoaches to Translation) Initial Training Network
(ITN) of the European Union’s Seventh Frame-
work Programme. The second author is sup-
ported by VICI grant nr. 277-89-002 from the
Netherlands Organization for Scientific Research
(NWO). We thank TAUS for providing us with
suitable data.
</bodyText>
<sectionHeader confidence="0.998344" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999765952941177">
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’11, pages 355–362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus interpolation methods for
phrase-based smt adaptation. In IWSLT, pages 136–
143.
Daniel Cer, Michel Galley, Daniel Jurafsky, and
Christopher D. Manning. 2010. Phrasal: A toolkit
for statistical machine translation with facilities for
extraction and incorporation of arbitrary model fea-
tures. In Proceedings of the NAACL HLT 2010
Demonstration Session, HLT-DEMO ’10, pages 9–
12, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ’12, pages 427–436, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for opti-
mizer instability. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: Short Pa-
pers - Volume 2, HLT ’11, pages 176–181, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Hoang Cuong and Khalil Sima’an. 2014. La-
tent domain translation models in mix-of-domains
haystack. In Proceedings of COLING 2014, the
25th International Conference on Computational
Linguistics: Technical Papers, pages 1928–1939,
Dublin, Ireland, August. Dublin City University and
Association for Computational Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. JOURNAL OF THE ROYAL STATIS-
TICAL SOCIETY, SERIES B, 39(1):1–38.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ’11, pages 85–91, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic transla-
tion model adaptation. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics: Short Papers - Volume 2, ACL
’12, pages 115–119, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’10,
pages 451–459, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Guillem Gasc´o, Martha-Alicia Rocha, Germ´an
Sanchis-Trilles, Jes´us Andr´es-Ferrer, and Francisco
Casacuberta. 2012. Does more data always yield
better translations? In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL ’12, pages
152–161, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Eva Hasler, Phil Blunsom, Philipp Koehn, and Barry
Haddow. 2014. Dynamic topic adaptation for
phrase-based mt. In Proceedings of the 14th Con-
ference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 328–337,
Gothenburg, Sweden, April. Association for Com-
putational Linguistics.
Ann Irvine, John Morgan, Marine Carpuat, Daume Hal
III, and Dragos Munteanu. 2013. Measuring ma-
chine translation errors in new domains. pages 429–
440.
</reference>
<page confidence="0.981563">
575
</page>
<reference confidence="0.999734083333333">
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT
’07, pages 224–227, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ’03, pages 48–54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Philipp Koehn, Amittai Axelrod, Alexandra Birch,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
International Workshop on Spoken Language Trans-
lation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ’07, pages 177–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Conference Pro-
ceedings: the tenth Machine Translation Summit,
pages 79–86, Phuket, Thailand. AAMT, AAMT.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight es-
timation for machine translation. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2 - Volume
2, EMNLP ’09, pages 708–717, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ’10, pages 220–224, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Radford M. Neal and Geoffrey E. Hinton. 1999.
Learning in graphical models. chapter A View
of the EM Algorithm That Justifies Incremental,
Sparse, and Other Variants, pages 355–368. MIT
Press, Cambridge, MA, USA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19–51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Comput. Linguist., 30(4):417–449, Decem-
ber.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ’11, pages 258–267, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 1992. Numerical
Recipes in C (2Nd Ed.): The Art of Scientific Com-
puting. Cambridge University Press, New York,
NY, USA.
Majid Razmara, George Foster, Baskaran Sankaran,
and Anoop Sarkar. 2012. Mixing multiple trans-
lation models in statistical machine translation. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers
- Volume 1, ACL ’12, pages 940–949, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Rico Sennrich, Holger Schwenk, and Walid Aransa.
2013. A multi-domain translation model frame-
work for statistical machine translation. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 832–840, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the 13th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, EACL ’12,
pages 539–549, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Matthew Snover, Bonnie Dorr, R. Schwartz, L. Micci-
ulla, and J. Makhoul. 2006. A study of translation
edit rate with targeted human annotation. In Pro-
ceedings of Association for Machine Translation in
the Americas, pages 223–231.
Min Zhang, Xinyan Xiao, Deyi Xiong, and Qun Liu.
2014. Topic-based dissimilarity and sensitivity
models for translation rule selection. Journal of Ar-
tificial Intelligence Research, 50(1):1–30.
</reference>
<page confidence="0.998485">
576
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.273795">
<title confidence="0.997673">Latent Domain Phrase-based Models for Adaptation</title>
<author confidence="0.534866">Cuong</author>
<affiliation confidence="0.975269">Institute for Logic, Language and University of</affiliation>
<address confidence="0.810335">Science Park 107, 1098 XG Amsterdam, The</address>
<email confidence="0.977756">c.hoang@uva.nl</email>
<email confidence="0.977756">k.simaan@uva.nl</email>
<abstract confidence="0.998539786666667">Phrase-based models directly trained corpora be sub-optimal. In this paper we equip models with a domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus. We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates, and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task. By embedding our latent domain phrase model in a sentence-level model and training the in tandem, we are able to adapt components together – phrase, lexical and reordering. We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models, showing significant performance improvement in both tasks. 1 Mix vs. Latent Domain Models Domain adaptation is usually perceived as utilizing a small seed in-domain corpus to adapt an exsystem trained on an Here we are interested in adapting an SMT systrained on a large an task by a seed paralcorpus The mix-domain scenario is interesting because often a large corpus consists of sentence pairs representing diverse domains, e.g., news, politics, finance, sports, etc. At the core of a standard state-of-the-art phrasebased system (Och and Ney, 2004) is a phrase from the word-aligned data together with estimates for Pt f ) ˜e| training words often vary across domains, it is likely in a mix-domain corpus translation will increase with the diver- Furthermore, the statistics in retranslation preferences the diverse domains. In this sense, phrase-based modtrained on be considered domain- This often leads to suboptimal performance (Gasc´o et al., 2012; Irvine et al., 2013). Recent adaptation techniques can be seen as mixture models, where two or more phrase tables, estimated from inand mix-domain corpora, are combined together by interpolation, fill-up, or multiple-decoding paths (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Here are interested in the specific question to phrase-based model in- We view this as training a complementary adaptation step which might precede any further combination with other models, e.g., in-, mixor general-domain. main challenge is how to induce from a phrase-based model for the in-domain task, only evidence? We present an approach whereby the contrast between in-domain prior distributions and “out-domain” distributions exploited for softly inviting (or recruiting) pairs to either camp. To this end we in- 566 f  |˜e). Because the translations of of the 2014 Conference on Empirical Methods in Natural Language Processing pages 566–576, 25-29, 2014, Doha, Qatar. Association for Computational Linguistics a domain variable signify in-</abstract>
<intro confidence="0.675307">and out-domain</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>355--362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6249" citStr="Axelrod et al., 2011" startWordPosition="1000" endWordPosition="1004">instance weighting performance as well as significantly improved phrasebased translation performance. 2 Model and training by invitation Eq. 1 shows that the key to training the latent phrase-based translation models is to train the latent phrase-relevance models, P(D |˜e, ˜f). As mentioned, for training P(D |˜e, ˜f) on parallel sentences in Cmzx we embed them in two asymmetric sentence-level models {P(D |e, f) |D E {0,1}}. 2.1 Domain relevance sentence models Intuitively, sentence models for domain relevance P(D |e, f) are somewhat related to data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011). The dominant approach to data selection uses the contrast between perplexities of in- and mix-domain language models.3 In the translation context, however, often a source phrase has different senses/translations in different domains, which cannot be distinguished with monolingual language models (Cuong and Sima’an, 2014). Therefore, our proposed latent sentencerelevance model includes two major latent components - monolingual domain-focused relevance models and domain-focused translation models derives as follows: P(e, f, D) P(D |e, f) = (2) f,D), (2) ED∈{D1,D0} where P(e, f, D) can be decom</context>
<context position="8011" citStr="Axelrod et al., 2011" startWordPosition="1294" endWordPosition="1298"> data selection. Pt(˜e f, D) = | Ee˜ Pt(˜e | Pt(˜e | ˜f)P(D |˜e, ˜f) , f) ˜e f)P(D e ˜ | , D1) contains Pt ( 567 “bag-of-possible-phrases” translation models:4 YPt(e|f, D) := (˜e, ˜f)EA(e,f) Pt(˜e |˜f, D)c(˜e, (4) where A(e, f) is the multiset of phrases in he, fi and c(·) denotes their count. Sub-model Pt(˜e |˜f, D) is given by Eq. 1. • Plm(e|D), Plm(f|D): the latent monolingual domain-focused relevance models aim at capturing the relevance of e and f for identifying domain D but here we consider them language models (LMs).5 As mentioned, the out-domain LMs differ from previous works, e.g., (Axelrod et al., 2011), which employ mix-domain LMs. Here, we stress the difficulty in finding data to train out-domain LMs and present a solution based on identifying pseudo out-domain data. • P(D): the domain priors aim at modeling the percentage of relevant data that the learning framework induces. It can be estimated via phrase-level parameters but here we prefer sentence-level parameters:6 = P(e,f)ECmix P(D |e, f) P(D) : P P(e,f)ECmix P(D |e, f) (5) D 2.2 Training by invitation Generally, our model can be viewed to have latent parameters O = {OD0, OD1}. The training procedure seeks O that maximize the log-like</context>
<context position="13940" citStr="Axelrod et al., 2011" startWordPosition="2356" endWordPosition="2359">n corpus, i.e., we hide some in-domain data in a large mix-domain corpus. We weigh sentence pairs under our model with P(D1 |˜e, ˜f) and P(D1 |e, f) respectively. We report pseudoprecision/recall at the sentence-level using a range of cut-off criteria for selecting the top scoring instances in the mix-domain corpus. A good relevance model expects to score higher for the hidden in-domain data. Baselines Two standard perplexity-based selection models in the literature have been implemented as the baselines: cross-entropy difference (Moore and Lewis, 2010) and bilingual cross-entropy difference (Axelrod et al., 2011), investigating their ability to retrieve the hiding data as well. Training them over the data to learn the sentences with their relevance, we then rank the sentences to select top of pairs to evaluate the pseudo-precision/recall at the sentence-level. Results We use a mix-domain corpus Cg of 770K sentence pairs of different genres.10 There is also a Legal corpus of 183K pairs that serves as the in-domain data. We create Cmix by selecting an arbitrary 83K pairs of in-domain pairs and adding them to Cg (the hidden in-domain data); we use the remaining 100k in-domain pairs as Cin. To train the b</context>
<context position="30768" citStr="Axelrod et al., 2011" startWordPosition="5121" endWordPosition="5124">s for the systems, which are averages over multiple runs. 8 Related work A distantly related, but clearly complementary, line of research focuses on the role of document topics (Eidelman et al., 2012; Zhang et al., 2014; Hasler et al., 2014). An off-the-shelf Latent Dirichlet Allocation tool is usually used to infer document-topic distributions. On one hand, this setting may not require in-domain data as prior. On the other hand, it requires meta-information (e.g., document information). Part of this work (the latent sentence-relevance models) relates to data selection (Moore and Lewis, 2010; Axelrod et al., 2011), where sentence-relevance weights are used for hardProfessional &amp; Business Services (In-domain: 23K pairs; Dev: 1, 000 pairs; Test: 998 pairs) BLEU In-domain 46.5 − − + Mix-domain 46.6 − − + Our system 47.9 +1.3 0.0001 METEOR In-domain 39.8 − − + Mix-domain 40.1 − − + Our System 41.1 +1.0 0.0001 In-domain 38.2 − − TER + Mix-domain 38.0 − − + Our System 36.9 -1.1 0.0001 Table 9: Domain adaptation experiments. Metric scores for the systems, which are averages over multiple runs. filtering rather than weighting. The idea of using sentence-relevance estimates for phrase-relevance estimates relate</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 355–362, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arianna Bisazza</author>
<author>Nick Ruiz</author>
<author>Marcello Federico</author>
</authors>
<title>Fill-up versus interpolation methods for phrase-based smt adaptation.</title>
<date>2011</date>
<booktitle>In IWSLT,</booktitle>
<pages>136--143</pages>
<contexts>
<context position="2430" citStr="Bisazza et al., 2011" startWordPosition="373" endWordPosition="376"> corpus Cmix the translation ambiguity will increase with the domain diversity. Furthermore, the statistics in Cmix will reflect translation preferences averaged over the diverse domains. In this sense, phrase-based models trained on Cmix can be considered domainconfused. This often leads to suboptimal performance (Gasc´o et al., 2012; Irvine et al., 2013). Recent adaptation techniques can be seen as mixture models, where two or more phrase tables, estimated from in- and mix-domain corpora, are combined together by interpolation, fill-up, or multiple-decoding paths (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Here we are interested in the specific question how to induce a phrase-based model from Cmix for indomain translation? We view this as in-domain focused training on Cmix, a complementary adaptation step which might precede any further combination with other models, e.g., in-, mix- or general-domain. The main challenge is how to induce from Cmix a phrase-based model for the in-domain task, given only Cin as evidence? We present an approach whereby the contrast between in-domain prior distributions and “out-domain” distributions is </context>
<context position="31856" citStr="Bisazza et al., 2011" startWordPosition="5298" endWordPosition="5301">multiple runs. filtering rather than weighting. The idea of using sentence-relevance estimates for phrase-relevance estimates relates to Matsoukas et al. (2009) who estimate the former using meta-information over documents as main features. In contrast, our work overcomes the mutual dependence of sentence and phrase estimates on one another by training both models in tandem. Adaptation using small in-domain data has a different but complementary goal to another line of research aiming at combining a domainadapted system with the another trained on the indomain data (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Our work is somewhat related to, but markedly different from, phrase pair weighting (Foster et al., 2010). Finally, our latent domain-focused phrase-based models and invitation training paradigm can be seen to shift attention from adaptation to making explicit the role of domain-focused models in SMT. 9 Conclusion We present a novel approach for in-domain focused training of a phrase-based system on a mix-of-domain corpus by using prior distributions from a small in-domain corpus. We derive an EM training algorithm for learning la</context>
</contexts>
<marker>Bisazza, Ruiz, Federico, 2011</marker>
<rawString>Arianna Bisazza, Nick Ruiz, and Marcello Federico. 2011. Fill-up versus interpolation methods for phrase-based smt adaptation. In IWSLT, pages 136– 143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Cer</author>
<author>Michel Galley</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Phrasal: A toolkit for statistical machine translation with facilities for extraction and incorporation of arbitrary model features.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Demonstration Session, HLT-DEMO ’10,</booktitle>
<pages>9--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18367" citStr="Cer et al. (2010)" startWordPosition="3092" endWordPosition="3095"> for word, phrase and distance-based reordering. The mix-domain corpus is word-aligned using GIZA++ (Och and Ney, 2003) and symmetrized with grow(-diag)-final-and (Koehn et al., 2003). We limit phrase length to a maximum of seven words. The LMs are interpolated 4-grams with Kneser-Ney, trained on 2.2M English sentences from Europarl augmented with 248.8K sentences from News Commentary Corpus (WMT 2013). We tune the system using k-best batch MIRA (Cherry and Foster, 2012). Finally, we use Moses 12We use Stanford Phrasal - a standard state-of-the-art phrase-based translation system developed by Cer et al. (2010). 13The lexical weights and the lexical reordering features will be described in more detail in Section 6. Figure 2: Intrinsic evaluation. Cmix Cin Domain: Electronics Dev Test 570 Figure 3: BLEU averaged over multiple runs. Electrics (Training Data: 1 Million) 21.2 21 20.8 20 19.91 20.64 20.51 20.52 20.6 20.4 20.2 20.5 20.48 19.8 Baseline Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5 (Koehn et al., 2007) as decoder.14 We report BLEU (Papineni et al., 2002), METEOR 1.4 (Denkowski and Lavie, 2011) and TER (Snover et al., 2006), with statistical significance at 95% confidence interval under paired boo</context>
</contexts>
<marker>Cer, Galley, Jurafsky, Manning, 2010</marker>
<rawString>Daniel Cer, Michel Galley, Daniel Jurafsky, and Christopher D. Manning. 2010. Phrasal: A toolkit for statistical machine translation with facilities for extraction and incorporation of arbitrary model features. In Proceedings of the NAACL HLT 2010 Demonstration Session, HLT-DEMO ’10, pages 9– 12, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12,</booktitle>
<pages>427--436</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18225" citStr="Cherry and Foster, 2012" startWordPosition="3071" endWordPosition="3074">ation features, lexical weights (Koehn et al., 2003) and lexicalized reordering features (Koehn et al., 2005).13 Other features include the penalties for word, phrase and distance-based reordering. The mix-domain corpus is word-aligned using GIZA++ (Och and Ney, 2003) and symmetrized with grow(-diag)-final-and (Koehn et al., 2003). We limit phrase length to a maximum of seven words. The LMs are interpolated 4-grams with Kneser-Ney, trained on 2.2M English sentences from Europarl augmented with 248.8K sentences from News Commentary Corpus (WMT 2013). We tune the system using k-best batch MIRA (Cherry and Foster, 2012). Finally, we use Moses 12We use Stanford Phrasal - a standard state-of-the-art phrase-based translation system developed by Cer et al. (2010). 13The lexical weights and the lexical reordering features will be described in more detail in Section 6. Figure 2: Intrinsic evaluation. Cmix Cin Domain: Electronics Dev Test 570 Figure 3: BLEU averaged over multiple runs. Electrics (Training Data: 1 Million) 21.2 21 20.8 20 19.91 20.64 20.51 20.52 20.6 20.4 20.2 20.5 20.48 19.8 Baseline Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5 (Koehn et al., 2007) as decoder.14 We report BLEU (Papineni et al., 2002), M</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12, pages 427–436, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11,</booktitle>
<pages>176--181</pages>
<location>Strouds-</location>
<contexts>
<context position="19122" citStr="Clark et al., 2011" startWordPosition="3217" endWordPosition="3220">n. Cmix Cin Domain: Electronics Dev Test 570 Figure 3: BLEU averaged over multiple runs. Electrics (Training Data: 1 Million) 21.2 21 20.8 20 19.91 20.64 20.51 20.52 20.6 20.4 20.2 20.5 20.48 19.8 Baseline Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5 (Koehn et al., 2007) as decoder.14 We report BLEU (Papineni et al., 2002), METEOR 1.4 (Denkowski and Lavie, 2011) and TER (Snover et al., 2006), with statistical significance at 95% confidence interval under paired bootstrap re-sampling (Press et al., 1992). For every system reported, we run the optimizer at least three times, before running MultEval (Clark et al., 2011) for resampling and significance testing. Outlook In Section 5 we examine the effect of training only the latent domain-focused phrase table using our model. In Section 6 we proceed further to estimate also latent domain-focused lexical weights and lexicalized reordering models, examining how they incrementally improve the translation as well. 5 Adapting phrase table only Here we investigate the effect of adapting the phrase table only; we will delay adapting the lexical weights and lexicalized reordering features to Section 6. We build a phrase-based system with the usual features as the base</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11, pages 176–181, Strouds-</rawString>
</citation>
<citation valid="false">
<authors>
<author>PA burg</author>
</authors>
<institution>USA. Association for Computational Linguistics.</institution>
<marker>burg, </marker>
<rawString>burg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoang Cuong</author>
<author>Khalil Sima’an</author>
</authors>
<title>Latent domain translation models in mix-of-domains haystack.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>1928--1939</pages>
<institution>Dublin City University and Association for Computational Linguistics.</institution>
<location>Dublin, Ireland,</location>
<marker>Cuong, Sima’an, 2014</marker>
<rawString>Hoang Cuong and Khalil Sima’an. 2014. Latent domain translation models in mix-of-domains haystack. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1928–1939, Dublin, Ireland, August. Dublin City University and Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="9538" citStr="Dempster et al., 1977" startWordPosition="1542" endWordPosition="1545">could include the lexical and reordering sub-models (as suggested by an anonymous reviewer.) 5Relevance for identification or retrieval could be different from frequency or fluency. We leave this extension for future work. 6It should be noted that in most phrase-based SMT systems bilingual phrase probabilities are estimated heuristically from word alignmened data which often leads to overfitting. Estimating P(D) from sentence-level parameters rather than from phrase-level parameters helps us avoid the overfitting which often accompanies phrase extraction. the log-term log P. The EM algorithm (Dempster et al., 1977) comes as an alternative solution to fit the model. It can be seen to maximize L via blockcoordinate ascent on a lower bound F(q, O) using an auxiliary distribution q(D |e, f) f) e, F(q, O) = P(e,f) PD q(D |e, f) log q(D PΘ (D, � e, f) (7) where the inequality results, i.e., L ≥ F(q, O), derived from log being concave and Jensen’s inequality. We rewrite the Free Energy F(q, O) (Neal and Hinton, 1999) as follows: D e, f = X(e,f) XD q(D |e, f) log Pq(D |e, f) + X(e,f) XD q(D |e, f) log PΘ (e, f) X =(e,f) log PΘ(e, f) (8) − KL[q(D |e, f) ||PΘD(D |e, f)], where KL[· ||·] is the KL-divergence. With</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11,</booktitle>
<pages>85--91</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18862" citStr="Denkowski and Lavie, 2011" startWordPosition="3177" endWordPosition="3180">we use Moses 12We use Stanford Phrasal - a standard state-of-the-art phrase-based translation system developed by Cer et al. (2010). 13The lexical weights and the lexical reordering features will be described in more detail in Section 6. Figure 2: Intrinsic evaluation. Cmix Cin Domain: Electronics Dev Test 570 Figure 3: BLEU averaged over multiple runs. Electrics (Training Data: 1 Million) 21.2 21 20.8 20 19.91 20.64 20.51 20.52 20.6 20.4 20.2 20.5 20.48 19.8 Baseline Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5 (Koehn et al., 2007) as decoder.14 We report BLEU (Papineni et al., 2002), METEOR 1.4 (Denkowski and Lavie, 2011) and TER (Snover et al., 2006), with statistical significance at 95% confidence interval under paired bootstrap re-sampling (Press et al., 1992). For every system reported, we run the optimizer at least three times, before running MultEval (Clark et al., 2011) for resampling and significance testing. Outlook In Section 5 we examine the effect of training only the latent domain-focused phrase table using our model. In Section 6 we proceed further to estimate also latent domain-focused lexical weights and lexicalized reordering models, examining how they incrementally improve the translation as </context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11, pages 85–91, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Eidelman</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Topic models for dynamic translation model adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12,</booktitle>
<pages>115--119</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="30346" citStr="Eidelman et al., 2012" startWordPosition="5058" endWordPosition="5061">r System 33.1 +0.7 0.0001 Baseline 56.4 − − TER Our System 55.0 -1.4 0.0001 Computer Software (In-domain: 65K pairs; Dev: 1, 100 pairs; Test: 1, 000 pairs) Baseline 27.4 − − BLEU Our System 28.3 +0.9 0.0001 METEOR Baseline 34.0 − − Our System 34.7 +0.7 0.0001 Baseline 51.7 − − TER Our System 50.6 -1.1 0.0001 Pharmaceuticals &amp; Biotechnology (In-domain: 85K pairs; Dev: 920 pairs; Test: 1, 000 pairs) Table 8: Metric scores for the systems, which are averages over multiple runs. 8 Related work A distantly related, but clearly complementary, line of research focuses on the role of document topics (Eidelman et al., 2012; Zhang et al., 2014; Hasler et al., 2014). An off-the-shelf Latent Dirichlet Allocation tool is usually used to infer document-topic distributions. On one hand, this setting may not require in-domain data as prior. On the other hand, it requires meta-information (e.g., document information). Part of this work (the latent sentence-relevance models) relates to data selection (Moore and Lewis, 2010; Axelrod et al., 2011), where sentence-relevance weights are used for hardProfessional &amp; Business Services (In-domain: 23K pairs; Dev: 1, 000 pairs; Test: 998 pairs) BLEU In-domain 46.5 − − + Mix-doma</context>
</contexts>
<marker>Eidelman, Boyd-Graber, Resnik, 2012</marker>
<rawString>Vladimir Eidelman, Jordan Boyd-Graber, and Philip Resnik. 2012. Topic models for dynamic translation model adaptation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12, pages 115–119, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>451--459</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="32025" citStr="Foster et al., 2010" startWordPosition="5326" endWordPosition="5329">mate the former using meta-information over documents as main features. In contrast, our work overcomes the mutual dependence of sentence and phrase estimates on one another by training both models in tandem. Adaptation using small in-domain data has a different but complementary goal to another line of research aiming at combining a domainadapted system with the another trained on the indomain data (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Our work is somewhat related to, but markedly different from, phrase pair weighting (Foster et al., 2010). Finally, our latent domain-focused phrase-based models and invitation training paradigm can be seen to shift attention from adaptation to making explicit the role of domain-focused models in SMT. 9 Conclusion We present a novel approach for in-domain focused training of a phrase-based system on a mix-of-domain corpus by using prior distributions from a small in-domain corpus. We derive an EM training algorithm for learning latent domain relevance models for the phrase- and sentence-levels in tandem. We also show how to overcome the difficulty of lack of explicit out-domain data by bootstrapp</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 451–459, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guillem Gasc´o</author>
<author>Martha-Alicia Rocha</author>
<author>Germ´an Sanchis-Trilles</author>
<author>Jes´us Andr´es-Ferrer</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Does more data always yield better translations?</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12,</booktitle>
<pages>152--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Gasc´o, Rocha, Sanchis-Trilles, Andr´es-Ferrer, Casacuberta, 2012</marker>
<rawString>Guillem Gasc´o, Martha-Alicia Rocha, Germ´an Sanchis-Trilles, Jes´us Andr´es-Ferrer, and Francisco Casacuberta. 2012. Does more data always yield better translations? In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12, pages 152–161, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hasler</author>
<author>Phil Blunsom</author>
<author>Philipp Koehn</author>
<author>Barry Haddow</author>
</authors>
<title>Dynamic topic adaptation for phrase-based mt.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>328--337</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="21483" citStr="Hasler et al., 2014" startWordPosition="3607" endWordPosition="3610">e how the improvement could be gained when doubling the training data. Data System Avg A P-value 1M Baseline 19.91 − − Our System 20.64 +0.73 0.0001 2M Baseline 20.54 − − Our System 21.41 +0.87 0.0001 4M Baseline 21.44 − − Our System 22.62 +1.18 0.0001 Table 2: BLEU averaged over multiple runs. It is also interesting to consider the average entropy of phrase table entries in the domainconfused systems, i.e., − E(˜e,˜f) pt(˜e |˜f) log pt(˜e |˜f) number of phrases(˜e, ˜f) against that in the domain-focused systems E(˜e,˜f) pt(˜e |˜f, D1) log pt(˜e |˜f, D1) number of phrases(˜e, ˜f) . Following (Hasler et al., 2014) in Table 3 we also show that the entropy decreases significantly in 571 the adapted tables in all cases, which indicates that the distributions over translations of phrases have become sharper. Baseline Iter. 1 Iter.2 Iter.3 Iter.4 Iter.5 0.210 0.187 0.186 0.185 0.185 0.184 Table 3: Average entropy of distributions. In practice, the third iteration systems usually produce best translations. This is somewhat expected because as EM invites more pseudo indomain pairs in later iterations, it sharpens the estimates of P(D1 |˜e, ˜f), making pseudo outdomain pairs tend to 0.0. Table 4 shows the perc</context>
<context position="30388" citStr="Hasler et al., 2014" startWordPosition="5066" endWordPosition="5069"> TER Our System 55.0 -1.4 0.0001 Computer Software (In-domain: 65K pairs; Dev: 1, 100 pairs; Test: 1, 000 pairs) Baseline 27.4 − − BLEU Our System 28.3 +0.9 0.0001 METEOR Baseline 34.0 − − Our System 34.7 +0.7 0.0001 Baseline 51.7 − − TER Our System 50.6 -1.1 0.0001 Pharmaceuticals &amp; Biotechnology (In-domain: 85K pairs; Dev: 920 pairs; Test: 1, 000 pairs) Table 8: Metric scores for the systems, which are averages over multiple runs. 8 Related work A distantly related, but clearly complementary, line of research focuses on the role of document topics (Eidelman et al., 2012; Zhang et al., 2014; Hasler et al., 2014). An off-the-shelf Latent Dirichlet Allocation tool is usually used to infer document-topic distributions. On one hand, this setting may not require in-domain data as prior. On the other hand, it requires meta-information (e.g., document information). Part of this work (the latent sentence-relevance models) relates to data selection (Moore and Lewis, 2010; Axelrod et al., 2011), where sentence-relevance weights are used for hardProfessional &amp; Business Services (In-domain: 23K pairs; Dev: 1, 000 pairs; Test: 998 pairs) BLEU In-domain 46.5 − − + Mix-domain 46.6 − − + Our system 47.9 +1.3 0.0001 </context>
</contexts>
<marker>Hasler, Blunsom, Koehn, Haddow, 2014</marker>
<rawString>Eva Hasler, Phil Blunsom, Philipp Koehn, and Barry Haddow. 2014. Dynamic topic adaptation for phrase-based mt. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 328–337, Gothenburg, Sweden, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>John Morgan</author>
<author>Marine Carpuat</author>
<author>Daume Hal</author>
<author>Dragos Munteanu</author>
</authors>
<title>Measuring machine translation errors in new domains.</title>
<date>2013</date>
<pages>429--440</pages>
<contexts>
<context position="2168" citStr="Irvine et al., 2013" startWordPosition="334" endWordPosition="337">ore of a standard state-of-the-art phrasebased system (Och and Ney, 2004) is a phrase table {(˜e, ˜f)} extracted from the word-aligned data together with estimates for Pt f ) ˜e| training and Pt( words often vary across domains, it is likely that in a mix-domain corpus Cmix the translation ambiguity will increase with the domain diversity. Furthermore, the statistics in Cmix will reflect translation preferences averaged over the diverse domains. In this sense, phrase-based models trained on Cmix can be considered domainconfused. This often leads to suboptimal performance (Gasc´o et al., 2012; Irvine et al., 2013). Recent adaptation techniques can be seen as mixture models, where two or more phrase tables, estimated from in- and mix-domain corpora, are combined together by interpolation, fill-up, or multiple-decoding paths (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Here we are interested in the specific question how to induce a phrase-based model from Cmix for indomain translation? We view this as in-domain focused training on Cmix, a complementary adaptation step which might precede any further combination with other models, e.g., in</context>
</contexts>
<marker>Irvine, Morgan, Carpuat, Hal, Munteanu, 2013</marker>
<rawString>Ann Irvine, John Morgan, Marine Carpuat, Daume Hal III, and Dragos Munteanu. 2013. Measuring machine translation errors in new domains. pages 429– 440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07,</booktitle>
<pages>224--227</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2408" citStr="Koehn and Schroeder, 2007" startWordPosition="369" endWordPosition="372">likely that in a mix-domain corpus Cmix the translation ambiguity will increase with the domain diversity. Furthermore, the statistics in Cmix will reflect translation preferences averaged over the diverse domains. In this sense, phrase-based models trained on Cmix can be considered domainconfused. This often leads to suboptimal performance (Gasc´o et al., 2012; Irvine et al., 2013). Recent adaptation techniques can be seen as mixture models, where two or more phrase tables, estimated from in- and mix-domain corpora, are combined together by interpolation, fill-up, or multiple-decoding paths (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Here we are interested in the specific question how to induce a phrase-based model from Cmix for indomain translation? We view this as in-domain focused training on Cmix, a complementary adaptation step which might precede any further combination with other models, e.g., in-, mix- or general-domain. The main challenge is how to induce from Cmix a phrase-based model for the in-domain task, given only Cin as evidence? We present an approach whereby the contrast between in-domain prior distributions and “out-dom</context>
<context position="28381" citStr="Koehn and Schroeder, 2007" startWordPosition="4722" endWordPosition="4725">relation between their in-domain data and the mix-domain data varies substantially. Table 8 presents their in-domain, tuning and test data in detail, as well as the translation results over them. It shows that the fully domain-focused systems consistently and significantly improve the translation accuracy for all the tasks. 7 Combining multiple models Finally, we proceed further to test our latent domain-focused phrase-based translation model on standard domain adaptation. We conduct experiments on the task “Professional &amp; Business Services” as an example.16 For standard adaptation we follow (Koehn and Schroeder, 2007) where we pass multiple phrase tables directly to the Moses decoder and tune them together. For baseline we combine the standard phrase-based system trained on Cmix with the one trained on the in-domain data Cin. We also combine our latent domain-focused phrase-based system with the one trained on Cin. Table 9 presents the results showing that combining our domain-focused system adapted from Cmix with the in-domain model outperforms the baseline. 16We choose this task for additional experiments because it has very small in-domain data (23K). This is supposed to make adaptation difficult becaus</context>
<context position="31834" citStr="Koehn and Schroeder, 2007" startWordPosition="5294" endWordPosition="5297">s, which are averages over multiple runs. filtering rather than weighting. The idea of using sentence-relevance estimates for phrase-relevance estimates relates to Matsoukas et al. (2009) who estimate the former using meta-information over documents as main features. In contrast, our work overcomes the mutual dependence of sentence and phrase estimates on one another by training both models in tandem. Adaptation using small in-domain data has a different but complementary goal to another line of research aiming at combining a domainadapted system with the another trained on the indomain data (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Our work is somewhat related to, but markedly different from, phrase pair weighting (Foster et al., 2010). Finally, our latent domain-focused phrase-based models and invitation training paradigm can be seen to shift attention from adaptation to making explicit the role of domain-focused models in SMT. 9 Conclusion We present a novel approach for in-domain focused training of a phrase-based system on a mix-of-domain corpus by using prior distributions from a small in-domain corpus. We derive an EM training alg</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07, pages 224–227, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17653" citStr="Koehn et al., 2003" startWordPosition="2985" endWordPosition="2988">s Commentary. As in-domain corpus we use “Consumer and Industrial Electronics” manually collected by Translation Automation Society (TAUS.com). The corpus statistics are summarized in Table 1. System We train a standard state-of-the-art English Spanish Sents 113.7M 4M Words 127.1M Sents 109K Words 1, 485, 558 1, 685, 716 Sents 984 Words 13130 14, 955 Sents 982 Words 13, 493 15, 392 Table 1: The data preparation. phrase-based system, using it as the baseline.12 There are three main kinds of features for the translation model in the baseline - phrase-based translation features, lexical weights (Koehn et al., 2003) and lexicalized reordering features (Koehn et al., 2005).13 Other features include the penalties for word, phrase and distance-based reordering. The mix-domain corpus is word-aligned using GIZA++ (Och and Ney, 2003) and symmetrized with grow(-diag)-final-and (Koehn et al., 2003). We limit phrase length to a maximum of seven words. The LMs are interpolated 4-grams with Kneser-Ney, trained on 2.2M English sentences from Europarl augmented with 248.8K sentences from News Commentary Corpus (WMT 2013). We tune the system using k-best batch MIRA (Cherry and Foster, 2012). Finally, we use Moses 12We</context>
<context position="23827" citStr="Koehn et al., 2003" startWordPosition="3982" endWordPosition="3985">cantly improves translation performance. Now we also adapt the lexical Entries se˜nales reproducir signals signs play reproduce Baseline 0.29 0.36 0.15 0.20 Iter.1 0.36 0.23 0.29 0.16 Iter.2 0.37 0.19 0.32 0.17 Iter.3 0.37 0.17 0.34 0.16 Iter.4 0.37 0.16 0.36 0.16 Iter.5 0.37 0.15 0.37 0.16 Table 6: Phrase entry examples. and reordering components. The result is a fully adapted, domain-focused, phrase-based system. Briefly, the lexical weights provide smooth estimates for the phrase pair based on word translation scores P(e |f) between pairs of words (e, f), i.e., P(e |f) = c(e,f) � e c(e,f) (Koehn et al., 2003). Our latent domain-focused lexical weights, on the other hand, are estimated according to P(e |f, D1), i.e., P(e |f, D1) = P(e |f)P(D1 |e, f) Ef P(e |f)P(D1 |e, f). The lexicalized reordering models with orientation variable O, P(O |˜e, ˜f), model how likely a phrase (˜e, ˜f) directly follows a previous phrase (monotone), swaps positions with it (swap), or is not adjacent to it (discontinous) (Koehn et al., 2005). We make these domain-focused: the estimates of P(D1 |e, f) during EM. The baseline for the following experiments is a standard state-of-the-art phrase-based system, including two bi</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 48–54, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>IWSLT Speech Translation Evaluation.</title>
<date>2005</date>
<booktitle>Edinburgh System Description for the</booktitle>
<contexts>
<context position="17710" citStr="Koehn et al., 2005" startWordPosition="2993" endWordPosition="2996">dustrial Electronics” manually collected by Translation Automation Society (TAUS.com). The corpus statistics are summarized in Table 1. System We train a standard state-of-the-art English Spanish Sents 113.7M 4M Words 127.1M Sents 109K Words 1, 485, 558 1, 685, 716 Sents 984 Words 13130 14, 955 Sents 982 Words 13, 493 15, 392 Table 1: The data preparation. phrase-based system, using it as the baseline.12 There are three main kinds of features for the translation model in the baseline - phrase-based translation features, lexical weights (Koehn et al., 2003) and lexicalized reordering features (Koehn et al., 2005).13 Other features include the penalties for word, phrase and distance-based reordering. The mix-domain corpus is word-aligned using GIZA++ (Och and Ney, 2003) and symmetrized with grow(-diag)-final-and (Koehn et al., 2003). We limit phrase length to a maximum of seven words. The LMs are interpolated 4-grams with Kneser-Ney, trained on 2.2M English sentences from Europarl augmented with 248.8K sentences from News Commentary Corpus (WMT 2013). We tune the system using k-best batch MIRA (Cherry and Foster, 2012). Finally, we use Moses 12We use Stanford Phrasal - a standard state-of-the-art phras</context>
<context position="24244" citStr="Koehn et al., 2005" startWordPosition="4054" endWordPosition="4057">. Briefly, the lexical weights provide smooth estimates for the phrase pair based on word translation scores P(e |f) between pairs of words (e, f), i.e., P(e |f) = c(e,f) � e c(e,f) (Koehn et al., 2003). Our latent domain-focused lexical weights, on the other hand, are estimated according to P(e |f, D1), i.e., P(e |f, D1) = P(e |f)P(D1 |e, f) Ef P(e |f)P(D1 |e, f). The lexicalized reordering models with orientation variable O, P(O |˜e, ˜f), model how likely a phrase (˜e, ˜f) directly follows a previous phrase (monotone), swaps positions with it (swap), or is not adjacent to it (discontinous) (Koehn et al., 2005). We make these domain-focused: the estimates of P(D1 |e, f) during EM. The baseline for the following experiments is a standard state-of-the-art phrase-based system, including two bi-directional phrase-based translation features, two bi-directional lexical weights, six lexicalized reordering features, as well as the penalties for word, phrase and distortion. We develop three kinds of domain-adapted systems that are different at their adaptation level to fit the task. The first (Sys. 1) adapts only the phrase-based models, using the same lexical weights, lexicalized reordering models and other</context>
</contexts>
<marker>Koehn, Axelrod, Birch, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18769" citStr="Koehn et al., 2007" startWordPosition="3161" endWordPosition="3164">2013). We tune the system using k-best batch MIRA (Cherry and Foster, 2012). Finally, we use Moses 12We use Stanford Phrasal - a standard state-of-the-art phrase-based translation system developed by Cer et al. (2010). 13The lexical weights and the lexical reordering features will be described in more detail in Section 6. Figure 2: Intrinsic evaluation. Cmix Cin Domain: Electronics Dev Test 570 Figure 3: BLEU averaged over multiple runs. Electrics (Training Data: 1 Million) 21.2 21 20.8 20 19.91 20.64 20.51 20.52 20.6 20.4 20.2 20.5 20.48 19.8 Baseline Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5 (Koehn et al., 2007) as decoder.14 We report BLEU (Papineni et al., 2002), METEOR 1.4 (Denkowski and Lavie, 2011) and TER (Snover et al., 2006), with statistical significance at 95% confidence interval under paired bootstrap re-sampling (Press et al., 1992). For every system reported, we run the optimizer at least three times, before running MultEval (Clark et al., 2011) for resampling and significance testing. Outlook In Section 5 we examine the effect of training only the latent domain-focused phrase table using our model. In Section 6 we proceed further to estimate also latent domain-focused lexical weights an</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 177–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Conference Proceedings: the tenth Machine Translation Summit,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand. AAMT, AAMT.</location>
<contexts>
<context position="14770" citStr="Koehn, 2005" startWordPosition="2492" endWordPosition="2493">sion/recall at the sentence-level. Results We use a mix-domain corpus Cg of 770K sentence pairs of different genres.10 There is also a Legal corpus of 183K pairs that serves as the in-domain data. We create Cmix by selecting an arbitrary 83K pairs of in-domain pairs and adding them to Cg (the hidden in-domain data); we use the remaining 100k in-domain pairs as Cin. To train the baselines, we construct interpolated 4-gram Kneser-Ney LMs using BerkeleyLM (Pauls and Klein, 2011). Training our model on the data takes six EM-iterations to converge.11 10Count of sentence pairs: European Parliament (Koehn, 2005): 183, 793; Pharmaceuticals: 190, 443, Software: 196, 168, Hardware: 196, 501. 11After the fifth EM iteration we do not observe any significant increase in the likelihood of the data. Note that we use the same setting as for the baselines to train the latent domain-focused LMs for use in our model – interpolated 4- gram Kneser-Ney LMs using BerkeleyLM. This training setting is used for all experiments in this work. Update sentence-level parameters P(D|˜e, ˜f) P(D|e, f) P(f, e, D) P(˜e |˜f, D) P(˜f|˜e, D) P(e|f, D) P(f|e, D) Phrase-level Sentence-level Re-update phrase-level parameters 569 Pseu</context>
<context position="16997" citStr="Koehn, 2005" startWordPosition="2883" endWordPosition="2884">have been carried on to neutralize each component type in turn and build a selection system with the rest of our model parameters. It turns out that the latent domain translation models are crucial for performance for the learning framework, while the latent domain LMs make a far smaller yet substantial contribution. We refer readers to our previous work (Cuong and Sima’an, 2014), which provides detail analysis of the data selection problem. 4 Translation experiments: Setting Data We use a mix-domain corpus consisting of 4M sentence pairs, collected from multiple resources including EuroParl (Koehn, 2005), Common Crawl Corpus, UN Corpus, News Commentary. As in-domain corpus we use “Consumer and Industrial Electronics” manually collected by Translation Automation Society (TAUS.com). The corpus statistics are summarized in Table 1. System We train a standard state-of-the-art English Spanish Sents 113.7M 4M Words 127.1M Sents 109K Words 1, 485, 558 1, 685, 716 Sents 984 Words 13130 14, 955 Sents 982 Words 13, 493 15, 392 Table 1: The data preparation. phrase-based system, using it as the baseline.12 There are three main kinds of features for the translation model in the baseline - phrase-based tr</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceedings: the tenth Machine Translation Summit, pages 79–86, Phuket, Thailand. AAMT, AAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyros Matsoukas</author>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
</authors>
<title>Discriminative corpus weight estimation for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09,</booktitle>
<pages>708--717</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="31396" citStr="Matsoukas et al. (2009)" startWordPosition="5225" endWordPosition="5228">re sentence-relevance weights are used for hardProfessional &amp; Business Services (In-domain: 23K pairs; Dev: 1, 000 pairs; Test: 998 pairs) BLEU In-domain 46.5 − − + Mix-domain 46.6 − − + Our system 47.9 +1.3 0.0001 METEOR In-domain 39.8 − − + Mix-domain 40.1 − − + Our System 41.1 +1.0 0.0001 In-domain 38.2 − − TER + Mix-domain 38.0 − − + Our System 36.9 -1.1 0.0001 Table 9: Domain adaptation experiments. Metric scores for the systems, which are averages over multiple runs. filtering rather than weighting. The idea of using sentence-relevance estimates for phrase-relevance estimates relates to Matsoukas et al. (2009) who estimate the former using meta-information over documents as main features. In contrast, our work overcomes the mutual dependence of sentence and phrase estimates on one another by training both models in tandem. Adaptation using small in-domain data has a different but complementary goal to another line of research aiming at combining a domainadapted system with the another trained on the indomain data (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Our work is somewhat related to, but markedly different from, phrase pair we</context>
</contexts>
<marker>Matsoukas, Rosti, Zhang, 2009</marker>
<rawString>Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing Zhang. 2009. Discriminative corpus weight estimation for machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09, pages 708–717, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10,</booktitle>
<pages>220--224</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6226" citStr="Moore and Lewis, 2010" startWordPosition="996" endWordPosition="999">T. iments showing good instance weighting performance as well as significantly improved phrasebased translation performance. 2 Model and training by invitation Eq. 1 shows that the key to training the latent phrase-based translation models is to train the latent phrase-relevance models, P(D |˜e, ˜f). As mentioned, for training P(D |˜e, ˜f) on parallel sentences in Cmzx we embed them in two asymmetric sentence-level models {P(D |e, f) |D E {0,1}}. 2.1 Domain relevance sentence models Intuitively, sentence models for domain relevance P(D |e, f) are somewhat related to data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011). The dominant approach to data selection uses the contrast between perplexities of in- and mix-domain language models.3 In the translation context, however, often a source phrase has different senses/translations in different domains, which cannot be distinguished with monolingual language models (Cuong and Sima’an, 2014). Therefore, our proposed latent sentencerelevance model includes two major latent components - monolingual domain-focused relevance models and domain-focused translation models derives as follows: P(e, f, D) P(D |e, f) = (2) f,D), (2) ED∈{D1,D0} where </context>
<context position="13878" citStr="Moore and Lewis, 2010" startWordPosition="2348" endWordPosition="2351"> model to retrieve “hidden” in-domain data in a large mix-domain corpus, i.e., we hide some in-domain data in a large mix-domain corpus. We weigh sentence pairs under our model with P(D1 |˜e, ˜f) and P(D1 |e, f) respectively. We report pseudoprecision/recall at the sentence-level using a range of cut-off criteria for selecting the top scoring instances in the mix-domain corpus. A good relevance model expects to score higher for the hidden in-domain data. Baselines Two standard perplexity-based selection models in the literature have been implemented as the baselines: cross-entropy difference (Moore and Lewis, 2010) and bilingual cross-entropy difference (Axelrod et al., 2011), investigating their ability to retrieve the hiding data as well. Training them over the data to learn the sentences with their relevance, we then rank the sentences to select top of pairs to evaluate the pseudo-precision/recall at the sentence-level. Results We use a mix-domain corpus Cg of 770K sentence pairs of different genres.10 There is also a Legal corpus of 183K pairs that serves as the in-domain data. We create Cmix by selecting an arbitrary 83K pairs of in-domain pairs and adding them to Cg (the hidden in-domain data); we</context>
<context position="30745" citStr="Moore and Lewis, 2010" startWordPosition="5117" endWordPosition="5120">) Table 8: Metric scores for the systems, which are averages over multiple runs. 8 Related work A distantly related, but clearly complementary, line of research focuses on the role of document topics (Eidelman et al., 2012; Zhang et al., 2014; Hasler et al., 2014). An off-the-shelf Latent Dirichlet Allocation tool is usually used to infer document-topic distributions. On one hand, this setting may not require in-domain data as prior. On the other hand, it requires meta-information (e.g., document information). Part of this work (the latent sentence-relevance models) relates to data selection (Moore and Lewis, 2010; Axelrod et al., 2011), where sentence-relevance weights are used for hardProfessional &amp; Business Services (In-domain: 23K pairs; Dev: 1, 000 pairs; Test: 998 pairs) BLEU In-domain 46.5 − − + Mix-domain 46.6 − − + Our system 47.9 +1.3 0.0001 METEOR In-domain 39.8 − − + Mix-domain 40.1 − − + Our System 41.1 +1.0 0.0001 In-domain 38.2 − − TER + Mix-domain 38.0 − − + Our System 36.9 -1.1 0.0001 Table 9: Domain adaptation experiments. Metric scores for the systems, which are averages over multiple runs. filtering rather than weighting. The idea of using sentence-relevance estimates for phrase-rel</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10, pages 220–224, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Learning in graphical models. chapter A View of the EM Algorithm That Justifies Incremental, Sparse, and Other Variants,</title>
<date>1999</date>
<pages>355--368</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="9941" citStr="Neal and Hinton, 1999" startWordPosition="1622" endWordPosition="1625">imating P(D) from sentence-level parameters rather than from phrase-level parameters helps us avoid the overfitting which often accompanies phrase extraction. the log-term log P. The EM algorithm (Dempster et al., 1977) comes as an alternative solution to fit the model. It can be seen to maximize L via blockcoordinate ascent on a lower bound F(q, O) using an auxiliary distribution q(D |e, f) f) e, F(q, O) = P(e,f) PD q(D |e, f) log q(D PΘ (D, � e, f) (7) where the inequality results, i.e., L ≥ F(q, O), derived from log being concave and Jensen’s inequality. We rewrite the Free Energy F(q, O) (Neal and Hinton, 1999) as follows: D e, f = X(e,f) XD q(D |e, f) log Pq(D |e, f) + X(e,f) XD q(D |e, f) log PΘ (e, f) X =(e,f) log PΘ(e, f) (8) − KL[q(D |e, f) ||PΘD(D |e, f)], where KL[· ||·] is the KL-divergence. With the introduction of the KL-divergence, the alternating E and M steps for our EM algorithm are easily derived as E-step : qt+1 (9) argmaxq(D �e,f) F(q, Ot) = argminq(D �e,f) KL[q(D|e,f) ||PΘtD(D|e,f)] = PΘtD(D |e, f) M-step : Ot+1 (10) argmaxΘ F(qt+1, O) = XargmaxΘ X q(D |e, f) log PΘD(D, e, f) (e,f) D The iterative procedure is illustrated in Figure 1.7 At the E-step, a guess for P(D |˜e, ˜f) can be</context>
<context position="11602" citStr="Neal and Hinton, 1999" startWordPosition="1938" endWordPosition="1941">e the EM parameters, in technical perspective we do not want P(D I e, f) parameters to go too far off from the initialization. We therefore prefer the averaged style in practice, i.e., at the iteration n we update the P(D Ie, f) parameters, P(n)(D|e, f) as 1n(P(n)(D I e, f) + �n−1 i=1 P(i)(D I e, f)). ˜f) , 568 Figure 1: Our probabilistic invitation framework. the new estimates for P(D |e, f) can be used to (softly) fill in the values of hidden variable D and estimate parameters P(D |˜e, ˜f) and P(D). The EM is guaranteed to converge to a local maximum of the likelihood under mild conditions (Neal and Hinton, 1999). Before EM training starts we must provide a “reasonable” initial guess for P(D |˜e, ˜f). We must also train the out-domain LMs, which needs the construction of pseudo out-domain data.9 One simple way to do that is inspired by burnin in sampling, under the guidance of an indomain data set, Cin as prior. At the beginning, we train Pt(˜e |˜f, D1) and Pt( f˜ |˜e, D1) for all phrases learned from Cin. We also train Pt(˜e |˜f) and Pt( f˜ |˜e) for all phrases learned from Cmix. During burn-in we assume that the out-domain phrase-based models are the domainconfused phrase-bas | ed models, i.e., Pt (</context>
</contexts>
<marker>Neal, Hinton, 1999</marker>
<rawString>Radford M. Neal and Geoffrey E. Hinton. 1999. Learning in graphical models. chapter A View of the EM Algorithm That Justifies Incremental, Sparse, and Other Variants, pages 355–368. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="17869" citStr="Och and Ney, 2003" startWordPosition="3017" endWordPosition="3020">rd state-of-the-art English Spanish Sents 113.7M 4M Words 127.1M Sents 109K Words 1, 485, 558 1, 685, 716 Sents 984 Words 13130 14, 955 Sents 982 Words 13, 493 15, 392 Table 1: The data preparation. phrase-based system, using it as the baseline.12 There are three main kinds of features for the translation model in the baseline - phrase-based translation features, lexical weights (Koehn et al., 2003) and lexicalized reordering features (Koehn et al., 2005).13 Other features include the penalties for word, phrase and distance-based reordering. The mix-domain corpus is word-aligned using GIZA++ (Och and Ney, 2003) and symmetrized with grow(-diag)-final-and (Koehn et al., 2003). We limit phrase length to a maximum of seven words. The LMs are interpolated 4-grams with Kneser-Ney, trained on 2.2M English sentences from Europarl augmented with 248.8K sentences from News Commentary Corpus (WMT 2013). We tune the system using k-best batch MIRA (Cherry and Foster, 2012). Finally, we use Moses 12We use Stanford Phrasal - a standard state-of-the-art phrase-based translation system developed by Cer et al. (2010). 13The lexical weights and the lexical reordering features will be described in more detail in Sectio</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Comput. Linguist., 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Comput. Linguist.,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1621" citStr="Och and Ney, 2004" startWordPosition="243" endWordPosition="246">formance improvement in both tasks. 1 Mix vs. Latent Domain Models Domain adaptation is usually perceived as utilizing a small seed in-domain corpus to adapt an existing system trained on an out-of-domain corpus. Here we are interested in adapting an SMT system trained on a large mix-domain corpus Cmix to an in-domain task represented by a seed parallel corpus Cin. The mix-domain scenario is interesting because often a large corpus consists of sentence pairs representing diverse domains, e.g., news, politics, finance, sports, etc. At the core of a standard state-of-the-art phrasebased system (Och and Ney, 2004) is a phrase table {(˜e, ˜f)} extracted from the word-aligned data together with estimates for Pt f ) ˜e| training and Pt( words often vary across domains, it is likely that in a mix-domain corpus Cmix the translation ambiguity will increase with the domain diversity. Furthermore, the statistics in Cmix will reflect translation preferences averaged over the diverse domains. In this sense, phrase-based models trained on Cmix can be considered domainconfused. This often leads to suboptimal performance (Gasc´o et al., 2012; Irvine et al., 2013). Recent adaptation techniques can be seen as mixture</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Comput. Linguist., 30(4):417–449, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18822" citStr="Papineni et al., 2002" startWordPosition="3170" endWordPosition="3173">(Cherry and Foster, 2012). Finally, we use Moses 12We use Stanford Phrasal - a standard state-of-the-art phrase-based translation system developed by Cer et al. (2010). 13The lexical weights and the lexical reordering features will be described in more detail in Section 6. Figure 2: Intrinsic evaluation. Cmix Cin Domain: Electronics Dev Test 570 Figure 3: BLEU averaged over multiple runs. Electrics (Training Data: 1 Million) 21.2 21 20.8 20 19.91 20.64 20.51 20.52 20.6 20.4 20.2 20.5 20.48 19.8 Baseline Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5 (Koehn et al., 2007) as decoder.14 We report BLEU (Papineni et al., 2002), METEOR 1.4 (Denkowski and Lavie, 2011) and TER (Snover et al., 2006), with statistical significance at 95% confidence interval under paired bootstrap re-sampling (Press et al., 1992). For every system reported, we run the optimizer at least three times, before running MultEval (Clark et al., 2011) for resampling and significance testing. Outlook In Section 5 we examine the effect of training only the latent domain-focused phrase table using our model. In Section 6 we proceed further to estimate also latent domain-focused lexical weights and lexicalized reordering models, examining how they i</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Faster and smaller n-gram language models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>258--267</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14638" citStr="Pauls and Klein, 2011" startWordPosition="2471" endWordPosition="2474">them over the data to learn the sentences with their relevance, we then rank the sentences to select top of pairs to evaluate the pseudo-precision/recall at the sentence-level. Results We use a mix-domain corpus Cg of 770K sentence pairs of different genres.10 There is also a Legal corpus of 183K pairs that serves as the in-domain data. We create Cmix by selecting an arbitrary 83K pairs of in-domain pairs and adding them to Cg (the hidden in-domain data); we use the remaining 100k in-domain pairs as Cin. To train the baselines, we construct interpolated 4-gram Kneser-Ney LMs using BerkeleyLM (Pauls and Klein, 2011). Training our model on the data takes six EM-iterations to converge.11 10Count of sentence pairs: European Parliament (Koehn, 2005): 183, 793; Pharmaceuticals: 190, 443, Software: 196, 168, Hardware: 196, 501. 11After the fifth EM iteration we do not observe any significant increase in the likelihood of the data. Note that we use the same setting as for the baselines to train the latent domain-focused LMs for use in our model – interpolated 4- gram Kneser-Ney LMs using BerkeleyLM. This training setting is used for all experiments in this work. Update sentence-level parameters P(D|˜e, ˜f) P(D|</context>
</contexts>
<marker>Pauls, Klein, 2011</marker>
<rawString>Adam Pauls and Dan Klein. 2011. Faster and smaller n-gram language models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 258–267, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Saul A Teukolsky</author>
<author>William T Vetterling</author>
<author>Brian P Flannery</author>
</authors>
<title>Numerical Recipes in C (2Nd Ed.): The Art of Scientific Computing.</title>
<date>1992</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="19006" citStr="Press et al., 1992" startWordPosition="3198" endWordPosition="3201">hts and the lexical reordering features will be described in more detail in Section 6. Figure 2: Intrinsic evaluation. Cmix Cin Domain: Electronics Dev Test 570 Figure 3: BLEU averaged over multiple runs. Electrics (Training Data: 1 Million) 21.2 21 20.8 20 19.91 20.64 20.51 20.52 20.6 20.4 20.2 20.5 20.48 19.8 Baseline Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5 (Koehn et al., 2007) as decoder.14 We report BLEU (Papineni et al., 2002), METEOR 1.4 (Denkowski and Lavie, 2011) and TER (Snover et al., 2006), with statistical significance at 95% confidence interval under paired bootstrap re-sampling (Press et al., 1992). For every system reported, we run the optimizer at least three times, before running MultEval (Clark et al., 2011) for resampling and significance testing. Outlook In Section 5 we examine the effect of training only the latent domain-focused phrase table using our model. In Section 6 we proceed further to estimate also latent domain-focused lexical weights and lexicalized reordering models, examining how they incrementally improve the translation as well. 5 Adapting phrase table only Here we investigate the effect of adapting the phrase table only; we will delay adapting the lexical weights </context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 1992</marker>
<rawString>William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 1992. Numerical Recipes in C (2Nd Ed.): The Art of Scientific Computing. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Majid Razmara</author>
<author>George Foster</author>
<author>Baskaran Sankaran</author>
<author>Anoop Sarkar</author>
</authors>
<title>Mixing multiple translation models in statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12,</booktitle>
<pages>940--949</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2468" citStr="Razmara et al., 2012" startWordPosition="379" endWordPosition="383"> will increase with the domain diversity. Furthermore, the statistics in Cmix will reflect translation preferences averaged over the diverse domains. In this sense, phrase-based models trained on Cmix can be considered domainconfused. This often leads to suboptimal performance (Gasc´o et al., 2012; Irvine et al., 2013). Recent adaptation techniques can be seen as mixture models, where two or more phrase tables, estimated from in- and mix-domain corpora, are combined together by interpolation, fill-up, or multiple-decoding paths (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Here we are interested in the specific question how to induce a phrase-based model from Cmix for indomain translation? We view this as in-domain focused training on Cmix, a complementary adaptation step which might precede any further combination with other models, e.g., in-, mix- or general-domain. The main challenge is how to induce from Cmix a phrase-based model for the in-domain task, given only Cin as evidence? We present an approach whereby the contrast between in-domain prior distributions and “out-domain” distributions is exploited for softly inviting (or recr</context>
<context position="31894" citStr="Razmara et al., 2012" startWordPosition="5304" endWordPosition="5307">eighting. The idea of using sentence-relevance estimates for phrase-relevance estimates relates to Matsoukas et al. (2009) who estimate the former using meta-information over documents as main features. In contrast, our work overcomes the mutual dependence of sentence and phrase estimates on one another by training both models in tandem. Adaptation using small in-domain data has a different but complementary goal to another line of research aiming at combining a domainadapted system with the another trained on the indomain data (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Our work is somewhat related to, but markedly different from, phrase pair weighting (Foster et al., 2010). Finally, our latent domain-focused phrase-based models and invitation training paradigm can be seen to shift attention from adaptation to making explicit the role of domain-focused models in SMT. 9 Conclusion We present a novel approach for in-domain focused training of a phrase-based system on a mix-of-domain corpus by using prior distributions from a small in-domain corpus. We derive an EM training algorithm for learning latent domain relevance models for the p</context>
</contexts>
<marker>Razmara, Foster, Sankaran, Sarkar, 2012</marker>
<rawString>Majid Razmara, George Foster, Baskaran Sankaran, and Anoop Sarkar. 2012. Mixing multiple translation models in statistical machine translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 940–949, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
<author>Holger Schwenk</author>
<author>Walid Aransa</author>
</authors>
<title>A multi-domain translation model framework for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>832--840</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2492" citStr="Sennrich et al., 2013" startWordPosition="384" endWordPosition="387">e domain diversity. Furthermore, the statistics in Cmix will reflect translation preferences averaged over the diverse domains. In this sense, phrase-based models trained on Cmix can be considered domainconfused. This often leads to suboptimal performance (Gasc´o et al., 2012; Irvine et al., 2013). Recent adaptation techniques can be seen as mixture models, where two or more phrase tables, estimated from in- and mix-domain corpora, are combined together by interpolation, fill-up, or multiple-decoding paths (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Here we are interested in the specific question how to induce a phrase-based model from Cmix for indomain translation? We view this as in-domain focused training on Cmix, a complementary adaptation step which might precede any further combination with other models, e.g., in-, mix- or general-domain. The main challenge is how to induce from Cmix a phrase-based model for the in-domain task, given only Cin as evidence? We present an approach whereby the contrast between in-domain prior distributions and “out-domain” distributions is exploited for softly inviting (or recruiting) Cmix phrase pair</context>
<context position="31918" citStr="Sennrich et al., 2013" startWordPosition="5308" endWordPosition="5311">using sentence-relevance estimates for phrase-relevance estimates relates to Matsoukas et al. (2009) who estimate the former using meta-information over documents as main features. In contrast, our work overcomes the mutual dependence of sentence and phrase estimates on one another by training both models in tandem. Adaptation using small in-domain data has a different but complementary goal to another line of research aiming at combining a domainadapted system with the another trained on the indomain data (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Our work is somewhat related to, but markedly different from, phrase pair weighting (Foster et al., 2010). Finally, our latent domain-focused phrase-based models and invitation training paradigm can be seen to shift attention from adaptation to making explicit the role of domain-focused models in SMT. 9 Conclusion We present a novel approach for in-domain focused training of a phrase-based system on a mix-of-domain corpus by using prior distributions from a small in-domain corpus. We derive an EM training algorithm for learning latent domain relevance models for the phrase- and sentence-leve</context>
</contexts>
<marker>Sennrich, Schwenk, Aransa, 2013</marker>
<rawString>Rico Sennrich, Holger Schwenk, and Walid Aransa. 2013. A multi-domain translation model framework for statistical machine translation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 832–840, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
</authors>
<title>Perplexity minimization for translation model domain adaptation in statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12,</booktitle>
<pages>539--549</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2446" citStr="Sennrich, 2012" startWordPosition="377" endWordPosition="378">lation ambiguity will increase with the domain diversity. Furthermore, the statistics in Cmix will reflect translation preferences averaged over the diverse domains. In this sense, phrase-based models trained on Cmix can be considered domainconfused. This often leads to suboptimal performance (Gasc´o et al., 2012; Irvine et al., 2013). Recent adaptation techniques can be seen as mixture models, where two or more phrase tables, estimated from in- and mix-domain corpora, are combined together by interpolation, fill-up, or multiple-decoding paths (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Here we are interested in the specific question how to induce a phrase-based model from Cmix for indomain translation? We view this as in-domain focused training on Cmix, a complementary adaptation step which might precede any further combination with other models, e.g., in-, mix- or general-domain. The main challenge is how to induce from Cmix a phrase-based model for the in-domain task, given only Cin as evidence? We present an approach whereby the contrast between in-domain prior distributions and “out-domain” distributions is exploited for so</context>
<context position="31872" citStr="Sennrich, 2012" startWordPosition="5302" endWordPosition="5303">ng rather than weighting. The idea of using sentence-relevance estimates for phrase-relevance estimates relates to Matsoukas et al. (2009) who estimate the former using meta-information over documents as main features. In contrast, our work overcomes the mutual dependence of sentence and phrase estimates on one another by training both models in tandem. Adaptation using small in-domain data has a different but complementary goal to another line of research aiming at combining a domainadapted system with the another trained on the indomain data (Koehn and Schroeder, 2007; Bisazza et al., 2011; Sennrich, 2012; Razmara et al., 2012; Sennrich et al., 2013). Our work is somewhat related to, but markedly different from, phrase pair weighting (Foster et al., 2010). Finally, our latent domain-focused phrase-based models and invitation training paradigm can be seen to shift attention from adaptation to making explicit the role of domain-focused models in SMT. 9 Conclusion We present a novel approach for in-domain focused training of a phrase-based system on a mix-of-domain corpus by using prior distributions from a small in-domain corpus. We derive an EM training algorithm for learning latent domain rele</context>
</contexts>
<marker>Sennrich, 2012</marker>
<rawString>Rico Sennrich. 2012. Perplexity minimization for translation model domain adaptation in statistical machine translation. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12, pages 539–549, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="18892" citStr="Snover et al., 2006" startWordPosition="3183" endWordPosition="3186">al - a standard state-of-the-art phrase-based translation system developed by Cer et al. (2010). 13The lexical weights and the lexical reordering features will be described in more detail in Section 6. Figure 2: Intrinsic evaluation. Cmix Cin Domain: Electronics Dev Test 570 Figure 3: BLEU averaged over multiple runs. Electrics (Training Data: 1 Million) 21.2 21 20.8 20 19.91 20.64 20.51 20.52 20.6 20.4 20.2 20.5 20.48 19.8 Baseline Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5 (Koehn et al., 2007) as decoder.14 We report BLEU (Papineni et al., 2002), METEOR 1.4 (Denkowski and Lavie, 2011) and TER (Snover et al., 2006), with statistical significance at 95% confidence interval under paired bootstrap re-sampling (Press et al., 1992). For every system reported, we run the optimizer at least three times, before running MultEval (Clark et al., 2011) for resampling and significance testing. Outlook In Section 5 we examine the effect of training only the latent domain-focused phrase table using our model. In Section 6 we proceed further to estimate also latent domain-focused lexical weights and lexicalized reordering models, examining how they incrementally improve the translation as well. 5 Adapting phrase table </context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Xinyan Xiao</author>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
</authors>
<title>Topic-based dissimilarity and sensitivity models for translation rule selection.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>50</volume>
<issue>1</issue>
<contexts>
<context position="30366" citStr="Zhang et al., 2014" startWordPosition="5062" endWordPosition="5065">01 Baseline 56.4 − − TER Our System 55.0 -1.4 0.0001 Computer Software (In-domain: 65K pairs; Dev: 1, 100 pairs; Test: 1, 000 pairs) Baseline 27.4 − − BLEU Our System 28.3 +0.9 0.0001 METEOR Baseline 34.0 − − Our System 34.7 +0.7 0.0001 Baseline 51.7 − − TER Our System 50.6 -1.1 0.0001 Pharmaceuticals &amp; Biotechnology (In-domain: 85K pairs; Dev: 920 pairs; Test: 1, 000 pairs) Table 8: Metric scores for the systems, which are averages over multiple runs. 8 Related work A distantly related, but clearly complementary, line of research focuses on the role of document topics (Eidelman et al., 2012; Zhang et al., 2014; Hasler et al., 2014). An off-the-shelf Latent Dirichlet Allocation tool is usually used to infer document-topic distributions. On one hand, this setting may not require in-domain data as prior. On the other hand, it requires meta-information (e.g., document information). Part of this work (the latent sentence-relevance models) relates to data selection (Moore and Lewis, 2010; Axelrod et al., 2011), where sentence-relevance weights are used for hardProfessional &amp; Business Services (In-domain: 23K pairs; Dev: 1, 000 pairs; Test: 998 pairs) BLEU In-domain 46.5 − − + Mix-domain 46.6 − − + Our sy</context>
</contexts>
<marker>Zhang, Xiao, Xiong, Liu, 2014</marker>
<rawString>Min Zhang, Xinyan Xiao, Deyi Xiong, and Qun Liu. 2014. Topic-based dissimilarity and sensitivity models for translation rule selection. Journal of Artificial Intelligence Research, 50(1):1–30.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
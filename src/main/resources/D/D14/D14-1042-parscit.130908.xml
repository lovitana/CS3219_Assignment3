<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016280">
<title confidence="0.466133">
Werdy: Recognition and Disambiguation of Verbs and Verb Phrases
with Syntactic and Semantic Pruning
</title>
<author confidence="0.693902">
Luciano Del Corro Rainer Gemulla Gerhard Weikum
</author>
<affiliation confidence="0.530997">
Max-Planck-Institut f¨ur Informatik
</affiliation>
<address confidence="0.3129605">
Saarbr¨ucken, Germany
{ delcorro, rgemulla, weikum }@mpi-inf.mpg.de
</address>
<sectionHeader confidence="0.981833" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99995456">
Word-sense recognition and disambigua-
tion (WERD) is the task of identifying
word phrases and their senses in natural
language text. Though it is well under-
stood how to disambiguate noun phrases,
this task is much less studied for verbs
and verbal phrases. We present Werdy,
a framework for WERD with particular
focus on verbs and verbal phrases. Our
framework first identifies multi-word ex-
pressions based on the syntactic structure
of the sentence; this allows us to recog-
nize both contiguous and non-contiguous
phrases. We then generate a list of can-
didate senses for each word or phrase, us-
ing novel syntactic and semantic pruning
techniques. We also construct and lever-
age a new resource of pairs of senses for
verbs and their object arguments. Finally,
we feed the so-obtained candidate senses
into standard word-sense disambiguation
(WSD) methods, and boost their precision
and recall. Our experiments indicate that
Werdy significantly increases the perfor-
mance of existing WSD methods.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999247127659575">
Understanding the semantics of words and multi-
word expressions in natural language text is an
important task for automatic knowledge acquisi-
tion. It serves as a fundamental building block
in a wide area of applications, including semantic
parsing, question answering, paraphrasing, knowl-
edge base construction, etc. In this paper, we
study the task of word-sense recognition and dis-
ambiguation (WERD) with a focus on verbs and
verbal phrases. Verbs are the central element in a
sentence, and the key to understand the relations
between sets of entities expressed in a sentence.
We propose Werdy, a method to (i) automati-
cally recognize in natural language text both sin-
gle words and multi-word phrases that match en-
tries in a lexical knowledge base (KB) like Word-
Net (Fellbaum, 1998), and (ii) disambiguate these
words or phrases by identifying their senses in the
KB. WordNet is a comprehensive lexical resource
for word-sense disambiguation (WSD), covering
nouns, verbs, adjectives, adverbs, and many multi-
word expressions. In the following, the notion of
an entry refers to a word or phrase in the KB,
whereas a sense denotes the lexical synset of the
entry’s meaning in the given sentence.
A key challenge for recognizing KB entries in
natural language text is that entries often consist of
multiple words. In WordNet-3.0 more than 40%
of the entries are multi-word. Such entries are
challenging to recognize accurately for two main
reasons: First, the components of multi-word en-
tries in the KB (such as fiscal year) often consist
of components that are themselves KB entries (fis-
cal and year). Second, multi-word entries (such
as take a breath) may not appear consecutively in
a sentence (“He takes a deep breath.”). Werdy
addresses the latter problem by (conceptually)
matching the syntactic structure of the KB entries
to the syntactic structure of the input sentence.
To address the former problem, Werdy identifies
all possible entries in a sentence and passes them
to the disambiguation phase (take, breath, take a
breath, ... ); the disambiguation phase provides
more information about which multi-word entries
to keep. Thus, our method solves the recognition
and the disambiguation tasks jointly.
Once KB entries have been identified, Werdy
</bodyText>
<page confidence="0.981675">
374
</page>
<note confidence="0.912208">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 374–385,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999404083333333">
disambiguates each entry against its possible
senses. State-of-the-art methods for WSD (Nav-
igli, 2009) work fairly well for nouns and noun
phrases. However, the disambiguation of verbs
and verbal phrases has received much less atten-
tion in the literature.
WSD methods can be roughly categorized into
(i) methods that are based on supervised training
over sense-annotated corpora (e.g., Zhong and Ng
(2010)), and (ii) methods that harness KB’s to as-
sess the semantic relatedness among word senses
for mapping entries to senses (e.g., Ponzetto and
Navigli (2010)). For these methods, mapping
verbs to senses is a difficult task since verbs tend
to have more senses than nouns. In WordNet, in-
cluding monosemous words, there are on average
1.24 senses per noun and 2.17 per verb.
To disambiguate verbs and verbal phrases,
Werdy proceeds in multiple steps. First, Werdy
obtains the set of candidate senses for each recog-
nized entry from the KB. Second, it reduces the
set of candidate entries using novel syntactic and
semantic pruning techniques. The key insight be-
hind our syntactic pruning is that each verb sense
tends to occur in a only limited number of syn-
tactic patterns. For example, the sentence “Al-
bert Einstein remained in Princeton” has a sub-
ject (“Albert Einstein”), a verb (“remained”) and
an adverbial (“in Princeton”), it follows an SVA
clause pattern. We can thus safely prune verb
senses that do not match the syntactic structure of
the sentence. Moreover, each verb sense is com-
patible with only a limited number of semantic
argument types (such as location, river, person,
musician, etc); this phenomena is called selec-
tional preference or selectional restriction. Senses
that are compatible only with argument types not
present in the sentence can be pruned. Our prun-
ing steps are based on the idea that a verb selects
the categories of its arguments both syntactically
(c-selection) and semantically (s-selection). In the
final step, Werdy employs a state-of-the-art gen-
eral WSD method to select the most suitable sense
from the remaining candidates. Since incorrect
senses have already been greatly pruned, this step
significantly gains accuracy and efficiency over
standard WSD.
Our semantic pruning technique builds on a
newly created resource of pairs of senses for verbs
and their object arguments. For example, the
WordNet verb sense (play-1) (i.e., the 1st sense of
the verb entry “play”) selects as direct object the
noun sense (sport-1). We refer to this novel re-
source as the VO Sense Repository, or VOS repos-
itory for short.1 It is constructed from the Word-
Net gloss-tags corpus, the SemCor dataset, and a
small set of manually created VO sense pairs.
We evaluated Werdy on the SemEval-2007
coarse-grained WSD task (Navigli et al., 2007),
both with and without automatic recognition of en-
tries. We found that our techniques boost state-of-
the-art WSD methods and obtain high-quality re-
sults. Werdy significantly increases the precision
and recall of the best performing baselines.
The rest of the paper is organized as follows.
Section 2 gives an overview of Werdy compo-
nents. Section 3 presents the entry recognition,
and Sections 4 and 5 discuss our novel syntac-
tic and semantic pruning techniques. Section 6
presents the Semantic VO Repository and how we
constructed it. Section 7 gives the results of our
evaluation. Section 8 discusses related work.
</bodyText>
<sectionHeader confidence="0.879087" genericHeader="introduction">
2 Overview of Werdy
</sectionHeader>
<bodyText confidence="0.999735">
Werdy consists of four steps: (i) entry recognition,
(ii) syntactic pruning, (iii) semantic pruning, and
(iv) word-sense disambiguation. The novel con-
tribution of this paper is in the first three steps,
and in the construction of the VO sense repository.
Each of these steps operates on the clause level,
i.e., we first determine the set of clauses present
in the input sentence and then process clauses sep-
arately. A clause is a part of a sentence that ex-
presses some statement or coherent piece of infor-
mation. Clauses are thus suitable minimal units
for automatic text understanding tasks (Del Corro
and Gemulla, 2013); see Sec.3 for details.
In the entry-recognition step (Sec. 3), Werdy
obtains for the input sentence a set of potential
KB entries along with their part-of-speech tags.
The candidate senses of each entry are obtained
from WordNet. For instance, in the sentence “He
takes a deep and long breath”, the set of potential
entries includes take (verb, 44 candidate senses),
take a breath (verb, 1 candidate sense), and breath
(noun, 5 candidate senses). Note that in contrast to
Werdy, most existing word-sense disambiguation
methods assume that entries have already been
(correctly) identified.
</bodyText>
<footnote confidence="0.997029666666667">
1The VOS repository, Werdy’s source code, and results of
our experimental study are available at http://people.
mpi-inf.mpg.de/˜corrogg/.
</footnote>
<page confidence="0.999044">
375
</page>
<bodyText confidence="0.999921580645161">
In the syntactic-pruning step (Sec. 4), we elim-
inate candidate senses that do not agree with
the syntactic structure of the clause. It is well-
established that the syntactic realization of a
clause is intrinsically related with the sense of
its verb (Quirk et al., 1985; Levin, 1993; Hanks,
1996; Baker et al., 1998; Palmer et al., 2005).
Quirk et al. (1985) identified seven possible clause
types in the English language (such as “subject
verb adverbial”, SVA). We make use of techniques
inspired by Del Corro and Gemulla (2013) to iden-
tify the clause type of each clause in the sen-
tence. We then match the clause type with the set
of WordNet frames (e.g., “somebody verb some-
thing”) that WordNet provides for each verb sense,
and prune verb senses for which there is no match.
In the semantic-pruning step (Sec. 5), we fur-
ther prune the set of candidate senses by taking the
semantic types of direct objects into account. Sim-
ilarly to the syntactic relation mentioned above,
a verb sense also imposes a (selectional) restric-
tion on the semantic type of its arguments (Quirk
et al., 1985; Levin, 1993; Hanks, 1996; Baker et
al., 1998; Palmer et al., 2005). For instance, the
verb play with sense participate in games or sports
requires an object argument of type (game-1)2,
(game-3), or (sport-1). Senses that do not match
the arguments found in the clause are pruned.
This step is based on the newly constructed VOS
Repository (Sec. 6). Note that when there is no di-
rect object, only the syntactic pruning step applies.
</bodyText>
<sectionHeader confidence="0.997426" genericHeader="method">
3 Entry Recognition
</sectionHeader>
<bodyText confidence="0.993830294117647">
The key challenge in recognizing lexical KB en-
tries in text is that entries are not restricted to sin-
gle words. In addition to named entities (such as
people, places, etc.), KB’s contain multi-word ex-
pressions. For example, WordNet-3.0 contains en-
tries such as take place (verb), let down (verb),
take into account (verb), be born (verb), high
school (noun), fiscal year (noun), and Prime Min-
ister (noun). Note that each individual word in a
multi-word entry is usually also an entry by itself,
and can even be part of several multi-word en-
tries. To ensure correct disambiguation, all poten-
tial multi-word entries need to be recognized (Fin-
layson and Kulkarni, 2011), even when they do not
appear as consecutive words in a sentence.
Werdy addresses these challenges by explor-
ing the syntactic structure of both the input sen-
</bodyText>
<footnote confidence="0.950223">
2We use the notation (WordNet entry-sense number).
</footnote>
<figureCaption confidence="0.8901945">
He takes my hand and a deep breath .
Figure 1: An example dependency parse
</figureCaption>
<bodyText confidence="0.999830785714286">
tence and the lexical KB entries. The structure
of the sentence is captured in a dependency parse
(DP). Given a word in a sentence, Werdy con-
ceptually generates all subtrees of the DP starting
at that word, and matches them against the KB.
This process can be performed efficiently as Word-
Net entries are short and can be indexed appro-
priately. To match the individual words of a sen-
tence against the words of a KB entry, we follow
the standard approach and perform lemmatization
and stemming (Finlayson, 2014). To further han-
dle personal pronouns and possessives, we follow
Arranz et al. (2005) and normalize personal pro-
nouns (I, you, my, your, ...) to one’s, and reflex-
ive pronouns (myself, yourself,...) to oneself.
Consider the example sentence “He takes my
hand and a deep breath”. We first identify the
clauses and their DP’s (Fig. 1) using the method
of Del Corro and Gemulla (2013), which also
processes coordinating conjunctions. We obtain
clauses “He takes my hand” and “He takes a deep
breath”, which we process separately. To obtain
possible entries for the first clause, we start with
its head word (take) and incrementally consider
its descendants (take hand, take one’s hand, ... ).
The exploration is terminated as early as possible;
for example, we do not consider take one’s hand
because there is no WordNet entry that contains
both take and hand. For the second clause, we
start with take (found in WordNet), then expand
to take breath (not found but can occur together),
then take a breath (found), then take a deep breath
(not found, cannot occur together) and so on.
Note that the word “take” in the sentence re-
fer to two different entries and senses: “take” for
the first clause and “take a breath” for the sec-
ond clause. In this stage no decisions are made
about selecting entries and disambiguating them;
these decisions are made in the final WSD stage
of Werdy.
We tested Werdy’s entry-recognizer on the
SemEval-2007 corpus. We detected the correct en-
</bodyText>
<figure confidence="0.994229571428571">
nsubj poss
root
dobj
cc
conj
det
amod
</figure>
<page confidence="0.99282">
376
</page>
<table confidence="0.9947233">
Pattern Clause type Example WN frame example [frame number]
SVi SV AE died. Somebody verb [2]
SVeA SVA AE remained in Princeton. Somebody verb PP [22]
SVcC SVC AE is smart. Somebody verb adjective [6]
SVmtO SVO AE has won the Nobel Prize. Somebody verb something [8]
SVdtOiO SVOO RSAS gave AE the Nobel Prize. Somebody verb somebody something [14]
SVctOA SVOA The doorman showed AE to his office. Somebody verb somebody PP [20]
SVctOC SVOC AE declared the meeting open. Something verb something adjective/noun [5]
S: Subject, V: Verb, C: Complement, O: Direct object, Oi: Indirect object, A: Adverbial, Vi: Intransitive verb, Vc: Copular verb,
Vc: Extended-copular verb, Vmt: Monotransitive verb, Vdt: Ditransitive verb, Vct: Complex-transitive verb
</table>
<tableCaption confidence="0.999863">
Table 1: Clause types and examples of matching WordNet frames
</tableCaption>
<bodyText confidence="0.948496">
tries for all but two verbs (out of more than 400).
The two missed entries (take up and get rolling)
resulted from incorrect dependency parses.
</bodyText>
<sectionHeader confidence="0.990956" genericHeader="method">
4 Syntactic Pruning
</sectionHeader>
<bodyText confidence="0.999969115942029">
Once the KB entries have been recognized, Werdy
prunes the set of possible senses of each verb entry
by considering the syntactic structure of the clause
in which the entry occurs. This pruning is based
on the observation that each verb sense may occur
only in a limited number of “clause types”, each
having specific semantic functions (Quirk et al.,
1985). When the clause type of the sentence is
incompatible with a candidate sense of an entry,
this sense is eliminated.
Werdy first detects in the input sentence the set
of clauses and their constituents. A clause con-
sists of one subject (S), one verb (V), and option-
ally an indirect object (O), a direct object (O), a
complement (C) and one or more adverbials (A).
Not all combinations of clause constituents ap-
pear in the English language. When we classify
clauses according to the grammatical function of
their constituents, we obtain only seven different
clause types (Quirk et al., 1985); see Table 1. For
example, the sentence “He takes my hand” is of
type SVO; here “He” is the subject, “takes” the
verb, and “my hand” the object. The clause type
can (in principle) be determined by observing the
verb type and its complementation (Del Corro and
Gemulla, 2013).
For instance, consider the SVA clause “The stu-
dent remained in Princeton”. The verb remain has
four senses in WN: (1) stay the same; remain in
a certain state (e.g., “The dress remained wet”),
(2) continue in a place, position, or situation (“He
remained dean for another year”), (3) be left; of
persons, questions, problems (“There remains the
question of who pulled the trigger”) or (4) stay be-
hind (“The hostility remained long after they made
up”). The first sense of remain requires an SVC
pattern; the other cases require either SV or SVA.
Our example clause is of type SVA so that we can
safely prune the first sense.
WordNet provides an important resource for ob-
taining the set of clause types that are compatible
with each sense of a verb. In particular, each verb
sense in WordNet is annotated with a set offrames
(e.g., “somebody verb something”) in which they
may occur, capturing both syntactic and semantic
constraints. There are 35 different frames in to-
tal.3 We manually assigned a set of clause types to
each frame (e.g., SVO to frame “somebody verb
something”). Table 1 shows an example frame for
each of the seven clause types. On average, each
WordNet-3.0 verb sense is associated with 1.57
frames; the maximum number of frames per sense
is 9. The distribution of frames is highly skewed:
More than 61% of the 21,649 frame annotations
belong to one of four simple SVO frames (num-
bers 8, 9, 10 and 11), and 22 out of the 35 frames
have less than 100 instances. This skew makes
the syntactic pruning step effective for non-SVO
clauses, but less effective for SVO clauses.
Werdy directly determines a set of possible
frame types for each clause of the input sentence.
Our approach is based on the clause-type detection
method of Del Corro and Gemulla (2013), but we
also consider additional information that is cap-
tured in frames but not in clause types. For ex-
ample, we distinguish different realizations of ob-
jects (such as clausal objects from non-clausal ob-
jects), which are not captured in the clause type.
Given the DP of a clause, Werdy identifies the
</bodyText>
<footnote confidence="0.9170315">
3See http://wordnet.princeton.edu/
wordnet/man/wninput.5WN.html.
</footnote>
<page confidence="0.983127">
377
</page>
<figure confidence="0.998810578947369">
Frames
4,6,7
Frames
1-3
Frames
1,2,12,13,22,27
Frames
1,2,8-11,33
Frames
1,2,8-11,15-21, 30, 31,33
Yes
Yes
No
Yes
No
Q11
Adverbial?
Q2 Q3
No
Complement? Adverbial?
Frames
24,28,29,32,35
No
Yes No
Clause Object?
Q1
Yes
QZ Qs Q9 Q10
Dir. and in- No No No infinitive/
Complement? that-clause?
direct object? to-infinitive?
Yes Yes Yes
Frames
14,15
Frame
5
Frames
26,34
</figure>
<figureCaption confidence="0.99955">
Figure 2: Flow chart for frame detection
</figureCaption>
<bodyText confidence="0.999762">
set of WN frames that can potentially match the
clause as outlined in the flowchart of Fig. 2. Werdy
walks through the flowchart; for each question,
we check for the presence or absence of a specific
constituent of a clause (e.g., a direct object for Q1)
and proceed appropriately until we obtain a set of
possible frames. This set is further reduced by
considering additional information in the frames
(not shown; e.g., that the verb must end on “-ing”).
For our example clause “The student remained
in Princeton”, we first identify possible frames
{1, 2,12,13, 22, 27 } using the flowchart (Q1 no,
Q2 no, Q3 yes); using the additional information
in the frames, Werdy then further prunes this set
to {1, 2, 22 }. The corresponding set of remaining
candidate sense for remain is as given above, i.e.,
{ hremain-2i, hremain-3i, hremain-4i }.
Our mapping of clause types to WordNet frames
is judiciously designed for the way WordNet is or-
ganized. For instance, frames containing adver-
bials generally do not specify whether or not the
adverbial is obligatory; here we are conservative
in that we do not prune such frames if the input
clause does not contain an adverbial. As another
example, some frames overlap or subsume each
other; e.g, frame “somebody verb something” (8)
subsumes “somebody verb that clause” (26). In
some word senses annotated with the more general
frame, the more specific one can also apply (e.g.,
hpoint out-1i is annotated with 8 but not 26; 26
can apply), in others it does not (e.g., hplay-1i is
also annotated with 8 but not 26; but here 26 can-
not apply). To ensure the effectiveness of syntactic
pruning, we only consider the frames that are di-
rectly specified in WordNet. This procedure often
produces the desired results; in a few cases, how-
ever, we do prune the correct sense (e.g., frame 26
for clause “He points out that... ”).
</bodyText>
<sectionHeader confidence="0.997316" genericHeader="method">
5 Semantic Pruning
</sectionHeader>
<bodyText confidence="0.999941764705882">
A verb sense imposes a restriction on the semantic
type of the arguments it may take and vice versa
(Quirk et al., 1985; Levin, 1993; Hanks, 1996;
Baker et al., 1998; Palmer et al., 2005; Kipper et
al., 2008). This allows us to further prune the verb
candidate set by discarding verb senses whose se-
mantic argument is not present in the clause.
WordNet frames potentially allow a shallow
type pruning based on the semantics provided for
the clause constituents. However we could solely
distinguish people (“somebody”) from things
(“something”), which is too crude to obtain sub-
stantial pruning effects. Moreover, this distinction
is sometimes ambiguous.
Instead, we have developed a more powerful
approach to semantic pruning based on our VOS
repository. We remove from the verb candidate set
those senses whose semantic argument cannot be
present in the sentence. For instance, consider the
clause “The man plays football.” Suppose that we
know that the verb entry play with sense hplay-
1i (“participate in sports”) takes an object of type
hsport-1i; i.e., we have a tuple hplay-1, sport-1i
in our repository. Then, we check whether any
of the possible senses of football—(i) sport or (ii)
ball—is of type hsport-1i. Here the first sense has
the correct type (the second sense does not); thus
we retain hplay-1i as a possible sense for the verb
entry play. Next, suppose that we consider sense
hplay-3i (“play on an instrument”), which accord-
ing to our corpus takes hinstrument-6i as argument
(i.e., there is a tuple hplay-3, instrument-6i in our
VOS repository). Since none of the senses of foot-
ball is of type hinstrument-6i, we can safely drop
</bodyText>
<page confidence="0.994615">
378
</page>
<bodyText confidence="0.999874521739131">
(play-3) from our candidate set. We perform this
procedure for every verb sense in the candidate set.
Semantic pruning makes use of both VOS
repository and the hypernym structure of the noun
senses in WordNet. For each sentence, we obtain
the possible senses of the direct-object argument
of the verb. We then consider each candidate sense
of the verb (e.g., (play-1)), and check whether any
of its compatible object-argument senses (from
our repository) is a hypernym of any of the possi-
ble senses of its actual object argument (in the sen-
tence); e.g., (sport-1) is a hypernym of (football-
1). If so, we retain the verb’s candidate sense. If
not, either the candidate sense of the verb is in-
deed incompatible with the object argument in the
sentence, or our repository is incomplete. To han-
dle incompleteness to some extent, we also con-
sider hyponyms of the object-argument senses in
our repository; e.g., if we observe object sport in a
sentence and have verb-sense argument (football-
1) in our corpus, we consider this a match. If the
hyponyms lead to a match, we retain the verb’s
candidate sense; otherwise, we discard it.
</bodyText>
<sectionHeader confidence="0.989952" genericHeader="method">
6 Verb-Object Sense Repository
</sectionHeader>
<bodyText confidence="0.999931130434783">
We use three different methods to construct the
repository. In particular, we harness the sense-
annotated WordNet glosses4 as well as the sense-
annotated SemCor corpus (Landes et al., 1998).5
The major part of the VOS repository was ac-
quired from WordNet’s gloss tags. According
to Atkins and Rundell (2008), noun definitions
should be expressed in terms of the class to which
they belong, and verb definitions should refer to
the types of the subjects or objects related to the
action. Based on this rationale, we extracted all
noun senses that appear in the gloss of each verb
sense; each of these noun senses is treated as a
possible sense of the object argument of the cor-
responding verb sense. For example, the gloss of
(play-1) is “participate in games or sports;” each
noun is annotated with its senses (2 and 3 for
“games”, 1 for “sports”). We extract tuples (play-
1, game-2), (play-1, game-3), and (play-1, sport-
1) from this gloss. Note that we only extract
direct-object arguments, i.e., we do not consider
the type of the subject argument of a verb sense.
Since the constituents of the predicate are much
</bodyText>
<footnote confidence="0.98717675">
4http://wordnet.princeton.edu/
glosstag.shtml
5http://web.eecs.umich.edu/˜mihalcea/
downloads.html
</footnote>
<bodyText confidence="0.999853352941176">
more important than the subject to determine or
describe a verb sense, lexical resources rarely con-
tain information on the subject (Atkins and Run-
dell, 2008). Similarly, WordNet glosses typically
do not provide any information about adverbials.
Overall, we collected arguments for 8,657 verb
senses (out of WordNet’s 13,767 verb senses) and
a total of 13,050 (verb-#, object-#)-pairs.
We leveraged the sense-annotated SemCor cor-
pus to further extend our VOS repository. We
parsed each sentence in the corpus to obtain
the respective pairs of verb sense and object
sense. Since sentences are often more specific
than glosses, and thus less helpful for construct-
ing our repository, we generalized the so-found
object senses using a heuristic method. In particu-
lar, we first obtained all the object senses of each
verb sense, and then repeatedly generalized sets of
at least two senses that share a direct hypernym
to this hypernym. The rationale is that we only
want to generalize if we have some evidence that
a more general sense may apply; we thus require
at least two hyponyms before we generalize. Us-
ing this method, we collected arguments for 1,516
verb senses and a total of 4,131 sense pairs.
Finally, we noticed that the most frequent
senses used in the English language are usually
so general that their glosses do not contain any
relevant semantic argument. For instance, one of
the most frequent verbs is (see-1), which has gloss
“perceive by (sight-3)”. The correct semantic ar-
gument (entity-1) is so general that it is omitted
from the gloss. In fact, our gloss-tag extractor
generates tuple (see-1, sight-3), which is incorrect.
We thus manually annotated the 30 most frequent
verb senses with their object argument types.
Our final repository contains arguments for
9,335 verb senses and a total of 17,181 pairs. Pairs
from SemCor tend to be more specific because
they refer to text occurrences. The assumption of
taking the nouns of the glosses as arguments seems
to be mostly correct, although some errors may
be introduced. Consider the pair (play-28, stream-
2) extracted from the gloss “discharge or direct
or be discharged or directed as if in a continu-
ous (stream-2)”. Also, in some cases, the glosses
may refer to adverbials as in (play-14, location-1),
taken from gloss “perform on a certain (location-
1)”. Note that if an argument is missing from our
repository, we may prune the correct sense of the
verb. If, however, there is an additional, incorrect
</bodyText>
<page confidence="0.998166">
379
</page>
<bodyText confidence="0.9996845">
argument in the repository, the correct verb sense
is retained but pruning may be less effective.
</bodyText>
<sectionHeader confidence="0.997285" genericHeader="evaluation">
7 Evaluation
</sectionHeader>
<bodyText confidence="0.999149523809524">
Dataset. We tested Werdy on the SemEval-2007
coarse-grained dataset.6 It consists of five sense-
annotated documents; the sense annotations refer
to a coarse-grained version of WordNet. In addi-
tion to sense annotations, the corpus also provides
the corresponding KB entries (henceforth termed
“gold entries”) as well as a POS tag. We restrict
our evaluation to verbs that act as clause heads. In
total, 461 such verbs were recognized by ClausIE
(Del Corro and Gemulla, 2013) and the Stanford
Parser (Klein and Manning, 2003).7
WSD Algorithms. For the final step of Werdy,
we used the KB-based WSD algorithms of
Ponzetto and Navigli (2010) and It-Makes-
Sense (Zhong and Ng, 2010), a state-of-the-art
supervised system that was the best performer in
SemEval-2007. Each method only labels entries
for which it is sufficiently confident.
Simplified Extended Lesk (SimpleExtLesk). A
version of Lesk (1986). Each entry is assigned the
sense with highest term overlap between the en-
try’s context (words in the sentence) and both the
sense’s gloss (Kilgarriff and Rosenzweig, 2000)
as well as the glosses of its neighbors (Baner-
jee and Pedersen, 2003). A sense is output only
if the overlap exceeds some threshold; we used
thresholds in the range of 1–20 in our experi-
ments. There are many subtleties and details
in the implementation of SimpleExtLesk so we
used two different libraries: a Java implementation
of WordNet::Similarity (Pedersen et al., 2004),8
which we modified to accept a context string, and
DKPro-WSD (Miller et al., 2013) version 1.1.0,
with lemmatization, removal of stop words, paired
overlap enabled and normalization disabled.
Degree Centrality. Proposed by Navigli and La-
pata (2010). The method collects all paths con-
necting each candidate sense of an entry to the set
of candidate senses of the words the entry’s con-
text. The candidate sense with the highest degree
in the resulting subgraph is selected. We imple-
mented this algorithm using the Neo4j library.9
</bodyText>
<footnote confidence="0.9966935">
6The data is annotated with WordNet 2.1 senses; we
converted the annotations to WordNet-3.0 using DKPro-
WSD (Miller et al., 2013).
7Version 3.3.1, model englishRNN.ser.gz
8http://www.sussex.ac.uk/Users/drh21/
9http://www.neo4j.org/
</footnote>
<bodyText confidence="0.999329265306122">
We used a fixed threshold of 1 and vary the search
depth in range 1–20. We used the candidate senses
of all nouns and verbs in a sentence as context.
It-Makes-Sense (IMS). A state-of-the-art, pub-
licly available supervised system (Zhong and Ng,
2010) and a refined version of Chan et al. (2007),
which ranked first in the SemEval-2007 coarse
grained task. We modified the code to accept KB
entries and their candidate senses. We tested both
in WordNet-2.1 and 3.0; for the later we mapped
Werdy’s set of candidates to WordNet-2.1.
Most Frequent Sense (MFS). Selects the most
frequent sense (according to WordNet frequen-
cies) among the set of candidate senses of an en-
try. If there is a tie, we do not label. Note that
this procedure differs slightly from the standard of
picking the entry with the smallest sense id. We
do not follow this approach since it cannot handle
well overlapping entries.
MFS back-off. When one of the above meth-
ods fails to provide a sense label (or provides more
than one), we used the MFS method above with a
threshold of 1. This procedure increased the per-
formance in all cases.
Methodology. The disambiguation was per-
formed with respect to coarse-grained sense clus-
ters. The score of a cluster is the sum of the indi-
vidual scores of its senses (except for IMS which
provides only one answer per word); the cluster
with the highest score was selected. Our source
code and the results of our evaluation are publicly
available10.
The SemEval-2007 task was not designed for
automatic entry recognition, for each word or
multi-word expression it provides the WordNet
entry and the POS tag. We proceeded as follows
to handle multi-word entries. In the WSD step, we
considered the candidate senses of all recognized
entries that overlap with the gold entry. For exam-
ple, we considered the candidate senses of entries
take, breath, and take a breath for gold entry take
a breath.
The SemEval-2007 task uses WordNet-2.1 but
Werdy uses WordNet-3.0. We mapped both the
sense keys and clusters from WordNet-2.1 to
WordNet-3.0. All senses in WordNet-3.0 that
could not be mapped to any cluster were consider
to belong each of them to a single sense cluster.
Note that this procedure is fair: for such senses
</bodyText>
<footnote confidence="0.967864">
10http://people.mpi-inf.mpg.de/
˜corrogg/
</footnote>
<page confidence="0.983958">
380
</page>
<table confidence="0.960140052631579">
Algorithm Gold Pruning MFS threshold Verbs (clause heads) F1
Entry back-off /depth P R F1 points
Degree + - + 5 73.54 73.54 73.54
Centrality + + + 11 79.61 79.61 79.61 + 6.07
+ - - 5 73.99 71.58 72.77
+ + - 8 79.91 78.52 79.21 + 6.44
- - + 5 70.41 70.41 70.41
- + + 10 76.46 76.46 76.46 + 6.05
- - - 4 71.05 68.90 69.96
- + - 10 76.81 75.81 76.30 + 6.34
SimpleExtLesk + - + 6 77.28 75.27 76.26
(DKPro) + + + 5 81.90 80.48 81.18 + 4.92
+ - - 1 73.70 52.28 61.17
+ + - 1 81.99 64.21 72.02 + 10.85
- - + 5 74.33 72.57 73.44
- + + 5 79.30 77.75 78.52 + 5.08
- - - 1 69.85 50.54 58.65
- + - 1 78.69 62.20 69.48 + 10.83
SimpleExtLesk + - + 5 77.11 75.27 76.18
(WordNet::Sim) + + + 5 80.57 79.18 79.87 + 3.69
+ - - 1 74.82 68.98 71.78
+ + - 1 79.04 75.27 77.11 + 5.33
- - + 6 74.12 72.35 73.22
- + + 7 77.97 76.46 77.21 + 3.99
- - - 1 71.36 65.66 68.39
- + - 1 76.20 71.92 74.00 + 5.61
MFS + - - 1 76.61 74.62 75.60
+ + - 1 80.35 78.96 79.65 + 4.05
- - - 1 73.67 71.92 72.79
- + - 1 77.75 76.24 76.99 + 4.20
IMS + - + n.a. 79.60 79.60 79.60
(WordNet-2.1) + + + n.a. 80.04 80.04 80.04 + 0.44
- - + n.a. 76.21 75.05 75.63
- + + n.a. 77.53 76.36 76.94 + 1.31
IMS + - + n.a. 78.96 78.96 78.96
(WordNet-3.0) + + + n.a. 79.83 79.83 79.83 + 0.87
- - + n.a. 75.77 74.62 75.19
- + + n.a. 77.53 76.36 76.94 + 1.75
</table>
<tableCaption confidence="0.995785">
Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)
</tableCaption>
<bodyText confidence="0.997283833333333">
the disambiguation is equivalent to a fine-grained
disambiguation, which is harder.
Results. Our results are displayed in Table 2.
We ran each algorithm with the gold KB entries
provided by in the dataset (+ in column “gold en-
try) as well as the entries obtained by our method
of Sec. 3 (-). We also enabled (+) and disabled
(-) the pruning steps as well as the MFS back-off
strategy. The highest F1 score was achieved by
SimpleExtLesk (DKPro) with pruning and MFS
back-off: 81.18 with gold entries and 78.52 with
automatic entry recognition. In all cases, our syn-
tactic and semantic pruning strategy increased per-
formance (up to +10.85 F1 points). We next dis-
cuss the impact of the various steps of Werdy in
detail.
Detailed Analysis. Table 3 displays step-by-
step results for DKPro’s SimpleExtLesk, for MFS,
as well as SimpleExtLesk with MFS back-off, the
best performing strategy. The table shows results
when only some Werdy’s steps are used. We start
from a direct use of the respective algorithm with
the gold entries of SemEval-2007 after each hor-
izontal line, and then successively add the Werdy
steps indicated in the table.
When no gold entries were provided, perfor-
mance dropped due to the increase of sense can-
didates for multi-word expressions, which include
the possible senses of the expression itself as well
as the senses of the entry’s parts that are them-
</bodyText>
<page confidence="0.99489">
381
</page>
<table confidence="0.999774258064516">
Steps Performed threshold P R F1 F1 points
SimpleExtLesk (DKPro)
Plain with gold entries 1 73.70 52.28 61.17
+ Entry Recognition 1 69.85 50.54 58.65 - 2.52
+ Syntactic Pruning 1 76.47 58.84 66.50 + 7.85
+ Semantic Pruning 1 78.69 62.20 69.48 + 2.98
+ Entry Recognition 1 69.85 50.54 58.65 - 2.52
+ Semantic Pruning 1 73.85 55.39 63.30 + 4.65
+ Syntactic Pruning 1 79.33 61.21 69.10 + 7.93
+ Semantic Pruning 1 81.99 64.21 72.02 + 2.92
+ Semantic Pruning 1 78.11 56.90 65.84 + 4.67
MFS
Plain with gold entries 1 76.61 74.62 75.60
+ Entry Recognition 1 73.67 71.92 72.79 - 2.81
+ Syntactic Pruning 1 75.77 74.14 74.95 + 2.16
+ Semantic Pruning 1 77.75 76.24 76.99 + 2.04
+ Entry Recognition 1 73.67 71.92 72.79 - 2.81
+ Semantic Pruning 1 77.09 75.43 76.25 + 3.46
+ Syntactic Pruning 1 78.46 76.94 77.69 + 2.09
+ Semantic Pruning 1 80.35 78.96 79.65 + 1.96
+ Semantic Pruning 1 79.91 78.02 78.95 + 3.35
SimpleExtLesk (DKPro) with MFS back-off
Plain with gold entries 6 77.28 75.27 76.26
+ Entry Recognition 6 74.33 72.57 73.44 - 2.82
+ Syntactic Pruning 5 76.65 75.00 75.82 + 2.38
+ Semantic Pruning 5 79.30 77.75 78.52 + 2.70
+ Entry Recognition 5 74.33 72.57 73.44 - 2.82
+ Semantic Pruning 5 78.19 76.51 77.34 +3.90
+ Syntactic Pruning 5 79.34 77.80 78.56 + 2.30
+ Semantic Pruning 5 81.90 80.48 81.18 + 2.62
+ Semantic Pruning 5 81.02 79.09 80.04 + 3.78
</table>
<tableCaption confidence="0.99946">
Table 3: Step-by-step results
</tableCaption>
<bodyText confidence="0.999852526315789">
selves WordNet entries. Our entry recognizer
tends to do a good job since it managed to cor-
rectly identify all the relevant entries except in two
cases (i.e. “take up” and “get rolling”), in which
the dependency parse was incorrect. The drop in
F1 for our automatic entry recognition was mainly
due to incorrect selection of the correct entry of a
set of alternative, overlapping entries.
Syntactic pruning did not prune the correct
sense in most cases. In 16 cases (with gold en-
tries), however, the correct sense was pruned. Five
of these senses were pruned due to incorrect de-
pendency parses, which led to incorrect frame
identification. In two cases, the sense was not
annotated with the recognized frame in WordNet,
although it seemed adequate. In the remaining
cases, a general frame from WordNet was incor-
rectly omitted. Improvements to WordNet’s frame
annotations may thus make syntactic pruning even
more effective.
Semantic pruning also improves performance.
Here the correct sense was pruned for 11 verbs,
mainly due to the noisiness and incompleteness
of our VOS repository. Without using gold en-
tries, we found in total 237 semantic matches be-
tween possible verbs senses and possible object
senses (200 with gold entries). We also found that
our manual annotations in the VOS repository (see
Sec. 6) did not affect our experiments.
The results show that syntactic and semantic
pruning are beneficial for verb sense disambigua-
tion, but also stress the necessity to improve ex-
isting resources. Ideally, each verb sense would
be annotated with both the possible clause types
or syntactic patterns in which it can occur as well
as the possible senses of its objects. Annotations
for subjects and adverbial arguments may also be
beneficial.
</bodyText>
<page confidence="0.996342">
382
</page>
<sectionHeader confidence="0.999918" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999851355555556">
WSD is a classification task where for every word
there is a set of possible senses given by some ex-
ternal resource (as a KB). Two types of methods
can be distinguished in WSD. Supervised systems
(Dang and Palmer, 2005; Dligach and Palmer,
2008; Chen and Palmer, 2009; Zhong and Ng,
2010) use a classifier to assign senses to words,
mostly relying on manually annotated data for
training. In principle, these systems suffer from
low coverage since the training data is usually
sparse. Some authors have tried to overcome this
limitation by exploiting linked resources as train-
ing data (Shen et al., 2013; Cholakov et al., 2014).
The second WSD approach corresponds to the
so-called KB methods (Agirre and Soroa, 2009;
Ponzetto and Navigli, 2010; Miller et al., 2012;
Agirre et al., 2014). They rely on a back-
ground KB (typically WordNet or extended ver-
sions (Navigli and Ponzetto, 2012)), where related
senses appear close to each other. KB-based al-
gorithms often differ in the way the KB is ex-
plored. It has been shown that a key point to en-
hance performance is the amount of semantic in-
formation in the KB (Ponzetto and Navigli, 2010;
Miller et al., 2012). Our framework fits this line of
work since it is also unsupervised and enriches the
background knowledge in order to enhance perfor-
mance of standard WSD algorithms. A compre-
hensive overview of WSD systems can be found
in Navigli (2009) and Navigli (2012).
To bring WSD to real-world applications, the
mapping between text and KB entries is a funda-
mental first step. It has been pointed that the ex-
istence of multi-word expressions imposes multi-
ple challenges to text understanding tasks (Sag et
al., 2002). The problem has been addressed by
Arranz et al. (2005) and Finlayson and Kulkarni
(2011). They find multi-word entries by match-
ing word sequences allowing some morphological
and POS variations according to predefined pat-
terns. Our method differs in that we can recognize
KB entries that appear discontinuously and in that
we do not select the correct entry but generate a
set of potential entries.
Linguists have noted the link between verb
senses and the syntactic structure and argument
types (Quirk et al., 1985; Levin, 1993; Hanks,
1996), and supervised WSD systems were devel-
oped to capture this relation (Dang and Palmer,
2005; Chen and Palmer, 2009; Dligach and
Palmer, 2008; Cholakov et al., 2014). In Dang
and Palmer (2005) and Chen and Palmer (2009),
it is shown that WSD tasks can be improved with
features that capture the syntactic structure and in-
formation about verb arguments and their types.
They use features as shallow named entity recog-
nition and the hypernyms of the possible senses
of the noun arguments. Dang and Palmer (2005)
also included features extracted from PropBank
(Palmer et al., 2005) from role labels and frames.
Dligach and Palmer (2008) generated a corpus of
verb and their arguments (both surface forms),
which was used to incorporate a semantic feature
to the supervised system.
In our work, we also incorporate syntactic and
semantic information. Instead of learning the re-
lation between the verb senses and the syntactic
structure, however we incorporate it explicitly us-
ing the WordNet frames, which provide informa-
tion about which verb sense should be consider
for a given syntactic pattern. We also incorporate
explicitly the semantic relation between each verb
sense and its arguments using our VOS repository.
Different resources of semantic arguments for
automatic text understanding tasks have been con-
structed (Baker et al., 1998; Palmer et al., 2005;
Kipper et al., 2008; Gurevych et al., 2012; Nakas-
hole et al., 2012; Flati and Navigli, 2013). In
(Baker et al., 1998; Palmer et al., 2005; Kipper
et al., 2008; Gurevych et al., 2012), the classifica-
tion of verbs and arguments is focused toward se-
mantic or thematic roles. Nakashole et al. (2012)
uses semantic types to construct a taxonomy of bi-
nary relations and Flati and Navigli (2013) col-
lected semantic arguments for given textual ex-
pressions. For instance, given the verb “break”,
they extract a pattern “break (body part-1)”. In
contrast to existing resources, our VOS repository
disambiguates both the verb sense and the senses
of its arguments.
</bodyText>
<sectionHeader confidence="0.996805" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999993444444444">
We presented Werdy, a framework for word-sense
recognition and disambiguation with a particular
focus on verbs and verbal phrases. Our results
indicate that incorporating syntactic and seman-
tic constraints improves the performance of verb
sense disambiguation methods. This stresses the
necessity of extending and improving the available
syntactic and semantic resources, such as Word-
Net or our VOS repository.
</bodyText>
<page confidence="0.998838">
383
</page>
<sectionHeader confidence="0.99612" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999334233009708">
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Pro-
ceedings of EACL, pages 33–41.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2014. Random walks for knowledge-based word
sense disambiguation. Computational Linguistics,
40(1):57–84.
Victoria Arranz, Jordi Atserias, and Mauro Castillo.
2005. Multiwords and word sense disambiguation.
In Computational Linguistics and Intelligent Text
Processing, volume 3406 of Lecture Notes in Com-
puter Science, pages 250–262.
B. T. Sue Atkins and Michael Rundell. 2008. The Ox-
ford Guide to Practical Lexicography. Oxford Uni-
versity Press.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of ACL, pages 86–90.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of IJCAI, pages 805–810.
Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007.
Nus-pt: Exploiting parallel texts for word sense dis-
ambiguation in the english all-words tasks. In Pro-
ceedings of SemEval, pages 253–256.
Jinying Chen and Martha Palmer. 2009. Improving
english verb sense disambiguation performance with
linguistically motivated features and clear sense dis-
tinction boundaries. Language Resources and Eval-
uation, 43(2):181–208.
Kostadin Cholakov, Judith Eckle-Kohler, and Iryna
Gurevych. 2014. Automated verb sense labelling
based on linked lexical resources. In Proceedings of
EACL, pages 68–77.
Hoa Trang Dang and Martha Palmer. 2005. The role
of semantic roles in disambiguating verb senses. In
Proceedings of ACL, pages 42–49.
Luciano Del Corro and Rainer Gemulla. 2013.
Clausie: clause-based open information extraction.
In Proceedings of WWW, pages 355–366.
Dmitriy Dligach and Martha Palmer. 2008. Improv-
ing verb sense disambiguation with automatically
retrieved semantic knowledge. In Proceedings of
ICSC, pages 182–189.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Mark Alan Finlayson and Nidhi Kulkarni. 2011. De-
tecting multi-word expressions improves word sense
disambiguation. In Proceedings of MWE, pages 20–
24.
Mark Alan Finlayson. 2014. Java libraries for access-
ing the princeton wordnet: Comparison and evalua-
tion. In Proceedings of GWC.
Tiziano Flati and Roberto Navigli. 2013. Spred:
Large-scale harvesting of semantic predicates. In
Proceedings ofACL, pages 1222–1232.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-
mann, Michael Matuschek, Christian M. Meyer, and
Christian Wirth. 2012. Uby - a large-scale unified
lexical-semantic resource based on lmf. In Proceed-
ings of EACL, pages 580–590.
Patrick Hanks. 1996. Contextual dependency and lex-
ical sets. International Journal of Corpus Linguis-
tics, 1(1):75–98.
Adam Kilgarriff and Joseph Rosenzweig. 2000.
Framework and results for english senseval. Com-
puters and the Humanities, 34(1-2):15–48.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification
of English verbs. Language Resources and Evalua-
tion, 42(1):21–40.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL,
pages 423–430.
Shari Landes, Claudia Leacock, and Randee I. Tengi,
1998. Building Semantic Concordances. MIT
Press.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In Proceedings
of SIGDOC, pages 24–26.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. In Proceedings of COLING,
pages 1781–1796.
Tristan Miller, Nicolai Erbs, Hans-Peter Zorn, Torsten
Zesch, and Iryna Gurevych. 2013. Dkpro wsd: A
generalized uima-based framework for word sense
disambiguation. In Proceedings of ACL: System
Demonstrations, pages 37–42.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. In Proceedings of
EMNLP, pages 1135–1145.
Roberto Navigli and Mirella Lapata. 2010. An ex-
perimental study of graph connectivity for unsuper-
vised word sense disambiguation. EEE Transac-
tions on Pattern Analysis and Machine Intelligence,
32(4):678–692.
</reference>
<page confidence="0.987876">
384
</page>
<reference confidence="0.998685536585366">
Roberto Navigli and Simone Paolo Ponzetto. 2012.
Babelnet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193(0):217–
250.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained english all-words task. In Proceedings of
SemEval, pages 30–35.
Roberto Navigli. 2009. Word sense disambiguation:
A survey. ACM Computing Surveys, 41(2):10:1–
10:69.
Roberto Navigli. 2012. A quick tour of word sense dis-
ambiguation, induction and related approaches. In
Proceedings of SOFSEM, pages 115–129.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: Measuring the
relatedness of concepts. In Proceedings of HLT-
NAACL: Demonstration Papers, pages 38–41.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of ACL, pages
1522–1531.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for nlp. In Proceed-
ings of CICLing, pages 1–15.
Hui Shen, Razvan Bunescu, and Rada Mihalcea. 2013.
Coarse to fine grained sense disambiguation in
wikipedia. In Proceedings of *SEM, pages 22–31.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage word sense disambiguation sys-
tem for free text. In Proceedings of ACL: System
Demonstrations, pages 78–83.
</reference>
<page confidence="0.999084">
385
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.418020">
<title confidence="0.999595">Werdy: Recognition and Disambiguation of Verbs and Verb with Syntactic and Semantic Pruning</title>
<author confidence="0.719922">Luciano Del Corro Rainer Gemulla Gerhard Weikum Max-Planck-Institut f¨ur Saarbr¨ucken</author>
<author confidence="0.719922">Germany</author>
<email confidence="0.998544">rgemulla,weikum</email>
<abstract confidence="0.999218192307692">Word-sense recognition and disambiguation (WERD) is the task of identifying word phrases and their senses in natural language text. Though it is well understood how to disambiguate noun phrases, this task is much less studied for verbs and verbal phrases. We present Werdy, a framework for WERD with particular focus on verbs and verbal phrases. Our framework first identifies multi-word expressions based on the syntactic structure of the sentence; this allows us to recognize both contiguous and non-contiguous phrases. We then generate a list of candidate senses for each word or phrase, using novel syntactic and semantic pruning techniques. We also construct and leverage a new resource of pairs of senses for verbs and their object arguments. Finally, we feed the so-obtained candidate senses into standard word-sense disambiguation (WSD) methods, and boost their precision and recall. Our experiments indicate that Werdy significantly increases the performance of existing WSD methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing pagerank for word sense disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>33--41</pages>
<contexts>
<context position="37320" citStr="Agirre and Soroa, 2009" startWordPosition="6282" endWordPosition="6285">ome external resource (as a KB). Two types of methods can be distinguished in WSD. Supervised systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign senses to words, mostly relying on manually annotated data for training. In principle, these systems suffer from low coverage since the training data is usually sparse. Some authors have tried to overcome this limitation by exploiting linked resources as training data (Shen et al., 2013; Cholakov et al., 2014). The second WSD approach corresponds to the so-called KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). They rely on a background KB (typically WordNet or extended versions (Navigli and Ponzetto, 2012)), where related senses appear close to each other. KB-based algorithms often differ in the way the KB is explored. It has been shown that a key point to enhance performance is the amount of semantic information in the KB (Ponzetto and Navigli, 2010; Miller et al., 2012). Our framework fits this line of work since it is also unsupervised and enriches the background knowledge in order to enhance performance of standard WSD algo</context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing pagerank for word sense disambiguation. In Proceedings of EACL, pages 33–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
</authors>
<title>Oier Lopez de Lacalle, and Aitor Soroa.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>1</issue>
<marker>Agirre, 2014</marker>
<rawString>Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa. 2014. Random walks for knowledge-based word sense disambiguation. Computational Linguistics, 40(1):57–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Arranz</author>
<author>Jordi Atserias</author>
<author>Mauro Castillo</author>
</authors>
<title>Multiwords and word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>3406</volume>
<pages>250--262</pages>
<contexts>
<context position="11564" citStr="Arranz et al. (2005)" startWordPosition="1868" endWordPosition="1871">example dependency parse tence and the lexical KB entries. The structure of the sentence is captured in a dependency parse (DP). Given a word in a sentence, Werdy conceptually generates all subtrees of the DP starting at that word, and matches them against the KB. This process can be performed efficiently as WordNet entries are short and can be indexed appropriately. To match the individual words of a sentence against the words of a KB entry, we follow the standard approach and perform lemmatization and stemming (Finlayson, 2014). To further handle personal pronouns and possessives, we follow Arranz et al. (2005) and normalize personal pronouns (I, you, my, your, ...) to one’s, and reflexive pronouns (myself, yourself,...) to oneself. Consider the example sentence “He takes my hand and a deep breath”. We first identify the clauses and their DP’s (Fig. 1) using the method of Del Corro and Gemulla (2013), which also processes coordinating conjunctions. We obtain clauses “He takes my hand” and “He takes a deep breath”, which we process separately. To obtain possible entries for the first clause, we start with its head word (take) and incrementally consider its descendants (take hand, take one’s hand, ...</context>
<context position="38324" citStr="Arranz et al. (2005)" startWordPosition="6457" endWordPosition="6460"> the KB (Ponzetto and Navigli, 2010; Miller et al., 2012). Our framework fits this line of work since it is also unsupervised and enriches the background knowledge in order to enhance performance of standard WSD algorithms. A comprehensive overview of WSD systems can be found in Navigli (2009) and Navigli (2012). To bring WSD to real-world applications, the mapping between text and KB entries is a fundamental first step. It has been pointed that the existence of multi-word expressions imposes multiple challenges to text understanding tasks (Sag et al., 2002). The problem has been addressed by Arranz et al. (2005) and Finlayson and Kulkarni (2011). They find multi-word entries by matching word sequences allowing some morphological and POS variations according to predefined patterns. Our method differs in that we can recognize KB entries that appear discontinuously and in that we do not select the correct entry but generate a set of potential entries. Linguists have noted the link between verb senses and the syntactic structure and argument types (Quirk et al., 1985; Levin, 1993; Hanks, 1996), and supervised WSD systems were developed to capture this relation (Dang and Palmer, 2005; Chen and Palmer, 200</context>
</contexts>
<marker>Arranz, Atserias, Castillo, 2005</marker>
<rawString>Victoria Arranz, Jordi Atserias, and Mauro Castillo. 2005. Multiwords and word sense disambiguation. In Computational Linguistics and Intelligent Text Processing, volume 3406 of Lecture Notes in Computer Science, pages 250–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B T Sue Atkins</author>
<author>Michael Rundell</author>
</authors>
<title>The Oxford Guide to Practical Lexicography.</title>
<date>2008</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="22733" citStr="Atkins and Rundell (2008)" startWordPosition="3753" endWordPosition="3756">ider hyponyms of the object-argument senses in our repository; e.g., if we observe object sport in a sentence and have verb-sense argument (football1) in our corpus, we consider this a match. If the hyponyms lead to a match, we retain the verb’s candidate sense; otherwise, we discard it. 6 Verb-Object Sense Repository We use three different methods to construct the repository. In particular, we harness the senseannotated WordNet glosses4 as well as the senseannotated SemCor corpus (Landes et al., 1998).5 The major part of the VOS repository was acquired from WordNet’s gloss tags. According to Atkins and Rundell (2008), noun definitions should be expressed in terms of the class to which they belong, and verb definitions should refer to the types of the subjects or objects related to the action. Based on this rationale, we extracted all noun senses that appear in the gloss of each verb sense; each of these noun senses is treated as a possible sense of the object argument of the corresponding verb sense. For example, the gloss of (play-1) is “participate in games or sports;” each noun is annotated with its senses (2 and 3 for “games”, 1 for “sports”). We extract tuples (play1, game-2), (play-1, game-3), and (</context>
</contexts>
<marker>Atkins, Rundell, 2008</marker>
<rawString>B. T. Sue Atkins and Michael Rundell. 2008. The Oxford Guide to Practical Lexicography. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>86--90</pages>
<contexts>
<context position="8792" citStr="Baker et al., 1998" startWordPosition="1392" endWordPosition="1395"> candidate senses). Note that in contrast to Werdy, most existing word-sense disambiguation methods assume that entries have already been (correctly) identified. 1The VOS repository, Werdy’s source code, and results of our experimental study are available at http://people. mpi-inf.mpg.de/˜corrogg/. 375 In the syntactic-pruning step (Sec. 4), we eliminate candidate senses that do not agree with the syntactic structure of the clause. It is wellestablished that the syntactic realization of a clause is intrinsically related with the sense of its verb (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005). Quirk et al. (1985) identified seven possible clause types in the English language (such as “subject verb adverbial”, SVA). We make use of techniques inspired by Del Corro and Gemulla (2013) to identify the clause type of each clause in the sentence. We then match the clause type with the set of WordNet frames (e.g., “somebody verb something”) that WordNet provides for each verb sense, and prune verb senses for which there is no match. In the semantic-pruning step (Sec. 5), we further prune the set of candidate senses by taking the semantic types of direct objects into </context>
<context position="19785" citStr="Baker et al., 1998" startWordPosition="3264" endWordPosition="3267">out-1i is annotated with 8 but not 26; 26 can apply), in others it does not (e.g., hplay-1i is also annotated with 8 but not 26; but here 26 cannot apply). To ensure the effectiveness of syntactic pruning, we only consider the frames that are directly specified in WordNet. This procedure often produces the desired results; in a few cases, however, we do prune the correct sense (e.g., frame 26 for clause “He points out that... ”). 5 Semantic Pruning A verb sense imposes a restriction on the semantic type of the arguments it may take and vice versa (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008). This allows us to further prune the verb candidate set by discarding verb senses whose semantic argument is not present in the clause. WordNet frames potentially allow a shallow type pruning based on the semantics provided for the clause constituents. However we could solely distinguish people (“somebody”) from things (“something”), which is too crude to obtain substantial pruning effects. Moreover, this distinction is sometimes ambiguous. Instead, we have developed a more powerful approach to semantic pruning based on our VOS repository. We remove </context>
<context position="40148" citStr="Baker et al., 1998" startWordPosition="6749" endWordPosition="6752"> incorporate a semantic feature to the supervised system. In our work, we also incorporate syntactic and semantic information. Instead of learning the relation between the verb senses and the syntactic structure, however we incorporate it explicitly using the WordNet frames, which provide information about which verb sense should be consider for a given syntactic pattern. We also incorporate explicitly the semantic relation between each verb sense and its arguments using our VOS repository. Different resources of semantic arguments for automatic text understanding tasks have been constructed (Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008; Gurevych et al., 2012; Nakashole et al., 2012; Flati and Navigli, 2013). In (Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008; Gurevych et al., 2012), the classification of verbs and arguments is focused toward semantic or thematic roles. Nakashole et al. (2012) uses semantic types to construct a taxonomy of binary relations and Flati and Navigli (2013) collected semantic arguments for given textual expressions. For instance, given the verb “break”, they extract a pattern “break (body part-1)”. In contrast to existing resources, our VOS r</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceedings of ACL, pages 86–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>805--810</pages>
<contexts>
<context position="27370" citStr="Banerjee and Pedersen, 2003" startWordPosition="4506" endWordPosition="4510">ing, 2003).7 WSD Algorithms. For the final step of Werdy, we used the KB-based WSD algorithms of Ponzetto and Navigli (2010) and It-MakesSense (Zhong and Ng, 2010), a state-of-the-art supervised system that was the best performer in SemEval-2007. Each method only labels entries for which it is sufficiently confident. Simplified Extended Lesk (SimpleExtLesk). A version of Lesk (1986). Each entry is assigned the sense with highest term overlap between the entry’s context (words in the sentence) and both the sense’s gloss (Kilgarriff and Rosenzweig, 2000) as well as the glosses of its neighbors (Banerjee and Pedersen, 2003). A sense is output only if the overlap exceeds some threshold; we used thresholds in the range of 1–20 in our experiments. There are many subtleties and details in the implementation of SimpleExtLesk so we used two different libraries: a Java implementation of WordNet::Similarity (Pedersen et al., 2004),8 which we modified to accept a context string, and DKPro-WSD (Miller et al., 2013) version 1.1.0, with lemmatization, removal of stop words, paired overlap enabled and normalization disabled. Degree Centrality. Proposed by Navigli and Lapata (2010). The method collects all paths connecting ea</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In Proceedings of IJCAI, pages 805–810.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>Zhi Zhong</author>
</authors>
<title>Nus-pt: Exploiting parallel texts for word sense disambiguation in the english all-words tasks.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval,</booktitle>
<pages>253--256</pages>
<contexts>
<context position="28729" citStr="Chan et al. (2007)" startWordPosition="4720" endWordPosition="4723">n the resulting subgraph is selected. We implemented this algorithm using the Neo4j library.9 6The data is annotated with WordNet 2.1 senses; we converted the annotations to WordNet-3.0 using DKProWSD (Miller et al., 2013). 7Version 3.3.1, model englishRNN.ser.gz 8http://www.sussex.ac.uk/Users/drh21/ 9http://www.neo4j.org/ We used a fixed threshold of 1 and vary the search depth in range 1–20. We used the candidate senses of all nouns and verbs in a sentence as context. It-Makes-Sense (IMS). A state-of-the-art, publicly available supervised system (Zhong and Ng, 2010) and a refined version of Chan et al. (2007), which ranked first in the SemEval-2007 coarse grained task. We modified the code to accept KB entries and their candidate senses. We tested both in WordNet-2.1 and 3.0; for the later we mapped Werdy’s set of candidates to WordNet-2.1. Most Frequent Sense (MFS). Selects the most frequent sense (according to WordNet frequencies) among the set of candidate senses of an entry. If there is a tie, we do not label. Note that this procedure differs slightly from the standard of picking the entry with the smallest sense id. We do not follow this approach since it cannot handle well overlapping entrie</context>
</contexts>
<marker>Chan, Ng, Zhong, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007. Nus-pt: Exploiting parallel texts for word sense disambiguation in the english all-words tasks. In Proceedings of SemEval, pages 253–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinying Chen</author>
<author>Martha Palmer</author>
</authors>
<title>Improving english verb sense disambiguation performance with linguistically motivated features and clear sense distinction boundaries.</title>
<date>2009</date>
<journal>Language Resources and Evaluation,</journal>
<volume>43</volume>
<issue>2</issue>
<contexts>
<context position="36871" citStr="Chen and Palmer, 2009" startWordPosition="6209" endWordPosition="6212">or verb sense disambiguation, but also stress the necessity to improve existing resources. Ideally, each verb sense would be annotated with both the possible clause types or syntactic patterns in which it can occur as well as the possible senses of its objects. Annotations for subjects and adverbial arguments may also be beneficial. 382 8 Related Work WSD is a classification task where for every word there is a set of possible senses given by some external resource (as a KB). Two types of methods can be distinguished in WSD. Supervised systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign senses to words, mostly relying on manually annotated data for training. In principle, these systems suffer from low coverage since the training data is usually sparse. Some authors have tried to overcome this limitation by exploiting linked resources as training data (Shen et al., 2013; Cholakov et al., 2014). The second WSD approach corresponds to the so-called KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). They rely on a background KB (typically WordNet or extended versions (Navigli </context>
<context position="38925" citStr="Chen and Palmer, 2009" startWordPosition="6554" endWordPosition="6557">Arranz et al. (2005) and Finlayson and Kulkarni (2011). They find multi-word entries by matching word sequences allowing some morphological and POS variations according to predefined patterns. Our method differs in that we can recognize KB entries that appear discontinuously and in that we do not select the correct entry but generate a set of potential entries. Linguists have noted the link between verb senses and the syntactic structure and argument types (Quirk et al., 1985; Levin, 1993; Hanks, 1996), and supervised WSD systems were developed to capture this relation (Dang and Palmer, 2005; Chen and Palmer, 2009; Dligach and Palmer, 2008; Cholakov et al., 2014). In Dang and Palmer (2005) and Chen and Palmer (2009), it is shown that WSD tasks can be improved with features that capture the syntactic structure and information about verb arguments and their types. They use features as shallow named entity recognition and the hypernyms of the possible senses of the noun arguments. Dang and Palmer (2005) also included features extracted from PropBank (Palmer et al., 2005) from role labels and frames. Dligach and Palmer (2008) generated a corpus of verb and their arguments (both surface forms), which was us</context>
</contexts>
<marker>Chen, Palmer, 2009</marker>
<rawString>Jinying Chen and Martha Palmer. 2009. Improving english verb sense disambiguation performance with linguistically motivated features and clear sense distinction boundaries. Language Resources and Evaluation, 43(2):181–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kostadin Cholakov</author>
<author>Judith Eckle-Kohler</author>
<author>Iryna Gurevych</author>
</authors>
<title>Automated verb sense labelling based on linked lexical resources.</title>
<date>2014</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>68--77</pages>
<contexts>
<context position="37231" citStr="Cholakov et al., 2014" startWordPosition="6268" endWordPosition="6271">s a classification task where for every word there is a set of possible senses given by some external resource (as a KB). Two types of methods can be distinguished in WSD. Supervised systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign senses to words, mostly relying on manually annotated data for training. In principle, these systems suffer from low coverage since the training data is usually sparse. Some authors have tried to overcome this limitation by exploiting linked resources as training data (Shen et al., 2013; Cholakov et al., 2014). The second WSD approach corresponds to the so-called KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). They rely on a background KB (typically WordNet or extended versions (Navigli and Ponzetto, 2012)), where related senses appear close to each other. KB-based algorithms often differ in the way the KB is explored. It has been shown that a key point to enhance performance is the amount of semantic information in the KB (Ponzetto and Navigli, 2010; Miller et al., 2012). Our framework fits this line of work since it is also unsupervised a</context>
<context position="38975" citStr="Cholakov et al., 2014" startWordPosition="6562" endWordPosition="6565">2011). They find multi-word entries by matching word sequences allowing some morphological and POS variations according to predefined patterns. Our method differs in that we can recognize KB entries that appear discontinuously and in that we do not select the correct entry but generate a set of potential entries. Linguists have noted the link between verb senses and the syntactic structure and argument types (Quirk et al., 1985; Levin, 1993; Hanks, 1996), and supervised WSD systems were developed to capture this relation (Dang and Palmer, 2005; Chen and Palmer, 2009; Dligach and Palmer, 2008; Cholakov et al., 2014). In Dang and Palmer (2005) and Chen and Palmer (2009), it is shown that WSD tasks can be improved with features that capture the syntactic structure and information about verb arguments and their types. They use features as shallow named entity recognition and the hypernyms of the possible senses of the noun arguments. Dang and Palmer (2005) also included features extracted from PropBank (Palmer et al., 2005) from role labels and frames. Dligach and Palmer (2008) generated a corpus of verb and their arguments (both surface forms), which was used to incorporate a semantic feature to the superv</context>
</contexts>
<marker>Cholakov, Eckle-Kohler, Gurevych, 2014</marker>
<rawString>Kostadin Cholakov, Judith Eckle-Kohler, and Iryna Gurevych. 2014. Automated verb sense labelling based on linked lexical resources. In Proceedings of EACL, pages 68–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
<author>Martha Palmer</author>
</authors>
<title>The role of semantic roles in disambiguating verb senses.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>42--49</pages>
<contexts>
<context position="36822" citStr="Dang and Palmer, 2005" startWordPosition="6201" endWordPosition="6204">t syntactic and semantic pruning are beneficial for verb sense disambiguation, but also stress the necessity to improve existing resources. Ideally, each verb sense would be annotated with both the possible clause types or syntactic patterns in which it can occur as well as the possible senses of its objects. Annotations for subjects and adverbial arguments may also be beneficial. 382 8 Related Work WSD is a classification task where for every word there is a set of possible senses given by some external resource (as a KB). Two types of methods can be distinguished in WSD. Supervised systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign senses to words, mostly relying on manually annotated data for training. In principle, these systems suffer from low coverage since the training data is usually sparse. Some authors have tried to overcome this limitation by exploiting linked resources as training data (Shen et al., 2013; Cholakov et al., 2014). The second WSD approach corresponds to the so-called KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). They rely on a background KB </context>
<context position="38902" citStr="Dang and Palmer, 2005" startWordPosition="6550" endWordPosition="6553"> has been addressed by Arranz et al. (2005) and Finlayson and Kulkarni (2011). They find multi-word entries by matching word sequences allowing some morphological and POS variations according to predefined patterns. Our method differs in that we can recognize KB entries that appear discontinuously and in that we do not select the correct entry but generate a set of potential entries. Linguists have noted the link between verb senses and the syntactic structure and argument types (Quirk et al., 1985; Levin, 1993; Hanks, 1996), and supervised WSD systems were developed to capture this relation (Dang and Palmer, 2005; Chen and Palmer, 2009; Dligach and Palmer, 2008; Cholakov et al., 2014). In Dang and Palmer (2005) and Chen and Palmer (2009), it is shown that WSD tasks can be improved with features that capture the syntactic structure and information about verb arguments and their types. They use features as shallow named entity recognition and the hypernyms of the possible senses of the noun arguments. Dang and Palmer (2005) also included features extracted from PropBank (Palmer et al., 2005) from role labels and frames. Dligach and Palmer (2008) generated a corpus of verb and their arguments (both surfa</context>
</contexts>
<marker>Dang, Palmer, 2005</marker>
<rawString>Hoa Trang Dang and Martha Palmer. 2005. The role of semantic roles in disambiguating verb senses. In Proceedings of ACL, pages 42–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luciano Del Corro</author>
<author>Rainer Gemulla</author>
</authors>
<title>Clausie: clause-based open information extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>355--366</pages>
<contexts>
<context position="7745" citStr="Corro and Gemulla, 2013" startWordPosition="1228" endWordPosition="1231">dy Werdy consists of four steps: (i) entry recognition, (ii) syntactic pruning, (iii) semantic pruning, and (iv) word-sense disambiguation. The novel contribution of this paper is in the first three steps, and in the construction of the VO sense repository. Each of these steps operates on the clause level, i.e., we first determine the set of clauses present in the input sentence and then process clauses separately. A clause is a part of a sentence that expresses some statement or coherent piece of information. Clauses are thus suitable minimal units for automatic text understanding tasks (Del Corro and Gemulla, 2013); see Sec.3 for details. In the entry-recognition step (Sec. 3), Werdy obtains for the input sentence a set of potential KB entries along with their part-of-speech tags. The candidate senses of each entry are obtained from WordNet. For instance, in the sentence “He takes a deep and long breath”, the set of potential entries includes take (verb, 44 candidate senses), take a breath (verb, 1 candidate sense), and breath (noun, 5 candidate senses). Note that in contrast to Werdy, most existing word-sense disambiguation methods assume that entries have already been (correctly) identified. 1The VOS </context>
<context position="9006" citStr="Corro and Gemulla (2013)" startWordPosition="1427" endWordPosition="1430">results of our experimental study are available at http://people. mpi-inf.mpg.de/˜corrogg/. 375 In the syntactic-pruning step (Sec. 4), we eliminate candidate senses that do not agree with the syntactic structure of the clause. It is wellestablished that the syntactic realization of a clause is intrinsically related with the sense of its verb (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005). Quirk et al. (1985) identified seven possible clause types in the English language (such as “subject verb adverbial”, SVA). We make use of techniques inspired by Del Corro and Gemulla (2013) to identify the clause type of each clause in the sentence. We then match the clause type with the set of WordNet frames (e.g., “somebody verb something”) that WordNet provides for each verb sense, and prune verb senses for which there is no match. In the semantic-pruning step (Sec. 5), we further prune the set of candidate senses by taking the semantic types of direct objects into account. Similarly to the syntactic relation mentioned above, a verb sense also imposes a (selectional) restriction on the semantic type of its arguments (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al.,</context>
<context position="11859" citStr="Corro and Gemulla (2013)" startWordPosition="1919" endWordPosition="1922">med efficiently as WordNet entries are short and can be indexed appropriately. To match the individual words of a sentence against the words of a KB entry, we follow the standard approach and perform lemmatization and stemming (Finlayson, 2014). To further handle personal pronouns and possessives, we follow Arranz et al. (2005) and normalize personal pronouns (I, you, my, your, ...) to one’s, and reflexive pronouns (myself, yourself,...) to oneself. Consider the example sentence “He takes my hand and a deep breath”. We first identify the clauses and their DP’s (Fig. 1) using the method of Del Corro and Gemulla (2013), which also processes coordinating conjunctions. We obtain clauses “He takes my hand” and “He takes a deep breath”, which we process separately. To obtain possible entries for the first clause, we start with its head word (take) and incrementally consider its descendants (take hand, take one’s hand, ... ). The exploration is terminated as early as possible; for example, we do not consider take one’s hand because there is no WordNet entry that contains both take and hand. For the second clause, we start with take (found in WordNet), then expand to take breath (not found but can occur together)</context>
<context position="15186" citStr="Corro and Gemulla, 2013" startWordPosition="2479" endWordPosition="2482">one verb (V), and optionally an indirect object (O), a direct object (O), a complement (C) and one or more adverbials (A). Not all combinations of clause constituents appear in the English language. When we classify clauses according to the grammatical function of their constituents, we obtain only seven different clause types (Quirk et al., 1985); see Table 1. For example, the sentence “He takes my hand” is of type SVO; here “He” is the subject, “takes” the verb, and “my hand” the object. The clause type can (in principle) be determined by observing the verb type and its complementation (Del Corro and Gemulla, 2013). For instance, consider the SVA clause “The student remained in Princeton”. The verb remain has four senses in WN: (1) stay the same; remain in a certain state (e.g., “The dress remained wet”), (2) continue in a place, position, or situation (“He remained dean for another year”), (3) be left; of persons, questions, problems (“There remains the question of who pulled the trigger”) or (4) stay behind (“The hostility remained long after they made up”). The first sense of remain requires an SVC pattern; the other cases require either SV or SVA. Our example clause is of type SVA so that we can saf</context>
<context position="16962" citStr="Corro and Gemulla (2013)" startWordPosition="2788" endWordPosition="2791">average, each WordNet-3.0 verb sense is associated with 1.57 frames; the maximum number of frames per sense is 9. The distribution of frames is highly skewed: More than 61% of the 21,649 frame annotations belong to one of four simple SVO frames (numbers 8, 9, 10 and 11), and 22 out of the 35 frames have less than 100 instances. This skew makes the syntactic pruning step effective for non-SVO clauses, but less effective for SVO clauses. Werdy directly determines a set of possible frame types for each clause of the input sentence. Our approach is based on the clause-type detection method of Del Corro and Gemulla (2013), but we also consider additional information that is captured in frames but not in clause types. For example, we distinguish different realizations of objects (such as clausal objects from non-clausal objects), which are not captured in the clause type. Given the DP of a clause, Werdy identifies the 3See http://wordnet.princeton.edu/ wordnet/man/wninput.5WN.html. 377 Frames 4,6,7 Frames 1-3 Frames 1,2,12,13,22,27 Frames 1,2,8-11,33 Frames 1,2,8-11,15-21, 30, 31,33 Yes Yes No Yes No Q11 Adverbial? Q2 Q3 No Complement? Adverbial? Frames 24,28,29,32,35 No Yes No Clause Object? Q1 Yes QZ Qs Q9 Q1</context>
<context position="26702" citStr="Corro and Gemulla, 2013" startWordPosition="4401" endWordPosition="4404">r, there is an additional, incorrect 379 argument in the repository, the correct verb sense is retained but pruning may be less effective. 7 Evaluation Dataset. We tested Werdy on the SemEval-2007 coarse-grained dataset.6 It consists of five senseannotated documents; the sense annotations refer to a coarse-grained version of WordNet. In addition to sense annotations, the corpus also provides the corresponding KB entries (henceforth termed “gold entries”) as well as a POS tag. We restrict our evaluation to verbs that act as clause heads. In total, 461 such verbs were recognized by ClausIE (Del Corro and Gemulla, 2013) and the Stanford Parser (Klein and Manning, 2003).7 WSD Algorithms. For the final step of Werdy, we used the KB-based WSD algorithms of Ponzetto and Navigli (2010) and It-MakesSense (Zhong and Ng, 2010), a state-of-the-art supervised system that was the best performer in SemEval-2007. Each method only labels entries for which it is sufficiently confident. Simplified Extended Lesk (SimpleExtLesk). A version of Lesk (1986). Each entry is assigned the sense with highest term overlap between the entry’s context (words in the sentence) and both the sense’s gloss (Kilgarriff and Rosenzweig, 2000) a</context>
</contexts>
<marker>Corro, Gemulla, 2013</marker>
<rawString>Luciano Del Corro and Rainer Gemulla. 2013. Clausie: clause-based open information extraction. In Proceedings of WWW, pages 355–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Improving verb sense disambiguation with automatically retrieved semantic knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of ICSC,</booktitle>
<pages>182--189</pages>
<contexts>
<context position="36848" citStr="Dligach and Palmer, 2008" startWordPosition="6205" endWordPosition="6208">c pruning are beneficial for verb sense disambiguation, but also stress the necessity to improve existing resources. Ideally, each verb sense would be annotated with both the possible clause types or syntactic patterns in which it can occur as well as the possible senses of its objects. Annotations for subjects and adverbial arguments may also be beneficial. 382 8 Related Work WSD is a classification task where for every word there is a set of possible senses given by some external resource (as a KB). Two types of methods can be distinguished in WSD. Supervised systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign senses to words, mostly relying on manually annotated data for training. In principle, these systems suffer from low coverage since the training data is usually sparse. Some authors have tried to overcome this limitation by exploiting linked resources as training data (Shen et al., 2013; Cholakov et al., 2014). The second WSD approach corresponds to the so-called KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). They rely on a background KB (typically WordNet or exte</context>
<context position="38951" citStr="Dligach and Palmer, 2008" startWordPosition="6558" endWordPosition="6561">d Finlayson and Kulkarni (2011). They find multi-word entries by matching word sequences allowing some morphological and POS variations according to predefined patterns. Our method differs in that we can recognize KB entries that appear discontinuously and in that we do not select the correct entry but generate a set of potential entries. Linguists have noted the link between verb senses and the syntactic structure and argument types (Quirk et al., 1985; Levin, 1993; Hanks, 1996), and supervised WSD systems were developed to capture this relation (Dang and Palmer, 2005; Chen and Palmer, 2009; Dligach and Palmer, 2008; Cholakov et al., 2014). In Dang and Palmer (2005) and Chen and Palmer (2009), it is shown that WSD tasks can be improved with features that capture the syntactic structure and information about verb arguments and their types. They use features as shallow named entity recognition and the hypernyms of the possible senses of the noun arguments. Dang and Palmer (2005) also included features extracted from PropBank (Palmer et al., 2005) from role labels and frames. Dligach and Palmer (2008) generated a corpus of verb and their arguments (both surface forms), which was used to incorporate a semant</context>
</contexts>
<marker>Dligach, Palmer, 2008</marker>
<rawString>Dmitriy Dligach and Martha Palmer. 2008. Improving verb sense disambiguation with automatically retrieved semantic knowledge. In Proceedings of ICSC, pages 182–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>Bradford Books.</publisher>
<contexts>
<context position="2052" citStr="Fellbaum, 1998" startWordPosition="316" endWordPosition="317">ntal building block in a wide area of applications, including semantic parsing, question answering, paraphrasing, knowledge base construction, etc. In this paper, we study the task of word-sense recognition and disambiguation (WERD) with a focus on verbs and verbal phrases. Verbs are the central element in a sentence, and the key to understand the relations between sets of entities expressed in a sentence. We propose Werdy, a method to (i) automatically recognize in natural language text both single words and multi-word phrases that match entries in a lexical knowledge base (KB) like WordNet (Fellbaum, 1998), and (ii) disambiguate these words or phrases by identifying their senses in the KB. WordNet is a comprehensive lexical resource for word-sense disambiguation (WSD), covering nouns, verbs, adjectives, adverbs, and many multiword expressions. In the following, the notion of an entry refers to a word or phrase in the KB, whereas a sense denotes the lexical synset of the entry’s meaning in the given sentence. A key challenge for recognizing KB entries in natural language text is that entries often consist of multiple words. In WordNet-3.0 more than 40% of the entries are multi-word. Such entries</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Alan Finlayson</author>
<author>Nidhi Kulkarni</author>
</authors>
<title>Detecting multi-word expressions improves word sense disambiguation.</title>
<date>2011</date>
<booktitle>In Proceedings of MWE,</booktitle>
<pages>20--24</pages>
<contexts>
<context position="10685" citStr="Finlayson and Kulkarni, 2011" startWordPosition="1715" endWordPosition="1719">cal KB entries in text is that entries are not restricted to single words. In addition to named entities (such as people, places, etc.), KB’s contain multi-word expressions. For example, WordNet-3.0 contains entries such as take place (verb), let down (verb), take into account (verb), be born (verb), high school (noun), fiscal year (noun), and Prime Minister (noun). Note that each individual word in a multi-word entry is usually also an entry by itself, and can even be part of several multi-word entries. To ensure correct disambiguation, all potential multi-word entries need to be recognized (Finlayson and Kulkarni, 2011), even when they do not appear as consecutive words in a sentence. Werdy addresses these challenges by exploring the syntactic structure of both the input sen2We use the notation (WordNet entry-sense number). He takes my hand and a deep breath . Figure 1: An example dependency parse tence and the lexical KB entries. The structure of the sentence is captured in a dependency parse (DP). Given a word in a sentence, Werdy conceptually generates all subtrees of the DP starting at that word, and matches them against the KB. This process can be performed efficiently as WordNet entries are short and c</context>
<context position="38358" citStr="Finlayson and Kulkarni (2011)" startWordPosition="6462" endWordPosition="6465">igli, 2010; Miller et al., 2012). Our framework fits this line of work since it is also unsupervised and enriches the background knowledge in order to enhance performance of standard WSD algorithms. A comprehensive overview of WSD systems can be found in Navigli (2009) and Navigli (2012). To bring WSD to real-world applications, the mapping between text and KB entries is a fundamental first step. It has been pointed that the existence of multi-word expressions imposes multiple challenges to text understanding tasks (Sag et al., 2002). The problem has been addressed by Arranz et al. (2005) and Finlayson and Kulkarni (2011). They find multi-word entries by matching word sequences allowing some morphological and POS variations according to predefined patterns. Our method differs in that we can recognize KB entries that appear discontinuously and in that we do not select the correct entry but generate a set of potential entries. Linguists have noted the link between verb senses and the syntactic structure and argument types (Quirk et al., 1985; Levin, 1993; Hanks, 1996), and supervised WSD systems were developed to capture this relation (Dang and Palmer, 2005; Chen and Palmer, 2009; Dligach and Palmer, 2008; Chola</context>
</contexts>
<marker>Finlayson, Kulkarni, 2011</marker>
<rawString>Mark Alan Finlayson and Nidhi Kulkarni. 2011. Detecting multi-word expressions improves word sense disambiguation. In Proceedings of MWE, pages 20– 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Alan Finlayson</author>
</authors>
<title>Java libraries for accessing the princeton wordnet: Comparison and evaluation.</title>
<date>2014</date>
<booktitle>In Proceedings of GWC.</booktitle>
<contexts>
<context position="11479" citStr="Finlayson, 2014" startWordPosition="1856" endWordPosition="1857"> (WordNet entry-sense number). He takes my hand and a deep breath . Figure 1: An example dependency parse tence and the lexical KB entries. The structure of the sentence is captured in a dependency parse (DP). Given a word in a sentence, Werdy conceptually generates all subtrees of the DP starting at that word, and matches them against the KB. This process can be performed efficiently as WordNet entries are short and can be indexed appropriately. To match the individual words of a sentence against the words of a KB entry, we follow the standard approach and perform lemmatization and stemming (Finlayson, 2014). To further handle personal pronouns and possessives, we follow Arranz et al. (2005) and normalize personal pronouns (I, you, my, your, ...) to one’s, and reflexive pronouns (myself, yourself,...) to oneself. Consider the example sentence “He takes my hand and a deep breath”. We first identify the clauses and their DP’s (Fig. 1) using the method of Del Corro and Gemulla (2013), which also processes coordinating conjunctions. We obtain clauses “He takes my hand” and “He takes a deep breath”, which we process separately. To obtain possible entries for the first clause, we start with its head wo</context>
</contexts>
<marker>Finlayson, 2014</marker>
<rawString>Mark Alan Finlayson. 2014. Java libraries for accessing the princeton wordnet: Comparison and evaluation. In Proceedings of GWC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tiziano Flati</author>
<author>Roberto Navigli</author>
</authors>
<title>Spred: Large-scale harvesting of semantic predicates.</title>
<date>2013</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>1222--1232</pages>
<contexts>
<context position="40263" citStr="Flati and Navigli, 2013" startWordPosition="6770" endWordPosition="6773">ntic information. Instead of learning the relation between the verb senses and the syntactic structure, however we incorporate it explicitly using the WordNet frames, which provide information about which verb sense should be consider for a given syntactic pattern. We also incorporate explicitly the semantic relation between each verb sense and its arguments using our VOS repository. Different resources of semantic arguments for automatic text understanding tasks have been constructed (Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008; Gurevych et al., 2012; Nakashole et al., 2012; Flati and Navigli, 2013). In (Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008; Gurevych et al., 2012), the classification of verbs and arguments is focused toward semantic or thematic roles. Nakashole et al. (2012) uses semantic types to construct a taxonomy of binary relations and Flati and Navigli (2013) collected semantic arguments for given textual expressions. For instance, given the verb “break”, they extract a pattern “break (body part-1)”. In contrast to existing resources, our VOS repository disambiguates both the verb sense and the senses of its arguments. 9 Conclusion We presented Werdy, a fra</context>
</contexts>
<marker>Flati, Navigli, 2013</marker>
<rawString>Tiziano Flati and Roberto Navigli. 2013. Spred: Large-scale harvesting of semantic predicates. In Proceedings ofACL, pages 1222–1232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Judith Eckle-Kohler</author>
<author>Silvana Hartmann</author>
<author>Michael Matuschek</author>
<author>Christian M Meyer</author>
<author>Christian Wirth</author>
</authors>
<title>Uby - a large-scale unified lexical-semantic resource based on lmf.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>580--590</pages>
<contexts>
<context position="40213" citStr="Gurevych et al., 2012" startWordPosition="6761" endWordPosition="6764">ur work, we also incorporate syntactic and semantic information. Instead of learning the relation between the verb senses and the syntactic structure, however we incorporate it explicitly using the WordNet frames, which provide information about which verb sense should be consider for a given syntactic pattern. We also incorporate explicitly the semantic relation between each verb sense and its arguments using our VOS repository. Different resources of semantic arguments for automatic text understanding tasks have been constructed (Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008; Gurevych et al., 2012; Nakashole et al., 2012; Flati and Navigli, 2013). In (Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008; Gurevych et al., 2012), the classification of verbs and arguments is focused toward semantic or thematic roles. Nakashole et al. (2012) uses semantic types to construct a taxonomy of binary relations and Flati and Navigli (2013) collected semantic arguments for given textual expressions. For instance, given the verb “break”, they extract a pattern “break (body part-1)”. In contrast to existing resources, our VOS repository disambiguates both the verb sense and the senses of its</context>
</contexts>
<marker>Gurevych, Eckle-Kohler, Hartmann, Matuschek, Meyer, Wirth, 2012</marker>
<rawString>Iryna Gurevych, Judith Eckle-Kohler, Silvana Hartmann, Michael Matuschek, Christian M. Meyer, and Christian Wirth. 2012. Uby - a large-scale unified lexical-semantic resource based on lmf. In Proceedings of EACL, pages 580–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Hanks</author>
</authors>
<title>Contextual dependency and lexical sets.</title>
<date>1996</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="8772" citStr="Hanks, 1996" startWordPosition="1390" endWordPosition="1391">eath (noun, 5 candidate senses). Note that in contrast to Werdy, most existing word-sense disambiguation methods assume that entries have already been (correctly) identified. 1The VOS repository, Werdy’s source code, and results of our experimental study are available at http://people. mpi-inf.mpg.de/˜corrogg/. 375 In the syntactic-pruning step (Sec. 4), we eliminate candidate senses that do not agree with the syntactic structure of the clause. It is wellestablished that the syntactic realization of a clause is intrinsically related with the sense of its verb (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005). Quirk et al. (1985) identified seven possible clause types in the English language (such as “subject verb adverbial”, SVA). We make use of techniques inspired by Del Corro and Gemulla (2013) to identify the clause type of each clause in the sentence. We then match the clause type with the set of WordNet frames (e.g., “somebody verb something”) that WordNet provides for each verb sense, and prune verb senses for which there is no match. In the semantic-pruning step (Sec. 5), we further prune the set of candidate senses by taking the semantic types of </context>
<context position="19765" citStr="Hanks, 1996" startWordPosition="3262" endWordPosition="3263">e.g., hpoint out-1i is annotated with 8 but not 26; 26 can apply), in others it does not (e.g., hplay-1i is also annotated with 8 but not 26; but here 26 cannot apply). To ensure the effectiveness of syntactic pruning, we only consider the frames that are directly specified in WordNet. This procedure often produces the desired results; in a few cases, however, we do prune the correct sense (e.g., frame 26 for clause “He points out that... ”). 5 Semantic Pruning A verb sense imposes a restriction on the semantic type of the arguments it may take and vice versa (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008). This allows us to further prune the verb candidate set by discarding verb senses whose semantic argument is not present in the clause. WordNet frames potentially allow a shallow type pruning based on the semantics provided for the clause constituents. However we could solely distinguish people (“somebody”) from things (“something”), which is too crude to obtain substantial pruning effects. Moreover, this distinction is sometimes ambiguous. Instead, we have developed a more powerful approach to semantic pruning based on our VOS re</context>
<context position="38811" citStr="Hanks, 1996" startWordPosition="6537" endWordPosition="6538">es multiple challenges to text understanding tasks (Sag et al., 2002). The problem has been addressed by Arranz et al. (2005) and Finlayson and Kulkarni (2011). They find multi-word entries by matching word sequences allowing some morphological and POS variations according to predefined patterns. Our method differs in that we can recognize KB entries that appear discontinuously and in that we do not select the correct entry but generate a set of potential entries. Linguists have noted the link between verb senses and the syntactic structure and argument types (Quirk et al., 1985; Levin, 1993; Hanks, 1996), and supervised WSD systems were developed to capture this relation (Dang and Palmer, 2005; Chen and Palmer, 2009; Dligach and Palmer, 2008; Cholakov et al., 2014). In Dang and Palmer (2005) and Chen and Palmer (2009), it is shown that WSD tasks can be improved with features that capture the syntactic structure and information about verb arguments and their types. They use features as shallow named entity recognition and the hypernyms of the possible senses of the noun arguments. Dang and Palmer (2005) also included features extracted from PropBank (Palmer et al., 2005) from role labels and f</context>
</contexts>
<marker>Hanks, 1996</marker>
<rawString>Patrick Hanks. 1996. Contextual dependency and lexical sets. International Journal of Corpus Linguistics, 1(1):75–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Joseph Rosenzweig</author>
</authors>
<title>Framework and results for english senseval. Computers and the Humanities,</title>
<date>2000</date>
<pages>34--1</pages>
<contexts>
<context position="27300" citStr="Kilgarriff and Rosenzweig, 2000" startWordPosition="4494" endWordPosition="4497">usIE (Del Corro and Gemulla, 2013) and the Stanford Parser (Klein and Manning, 2003).7 WSD Algorithms. For the final step of Werdy, we used the KB-based WSD algorithms of Ponzetto and Navigli (2010) and It-MakesSense (Zhong and Ng, 2010), a state-of-the-art supervised system that was the best performer in SemEval-2007. Each method only labels entries for which it is sufficiently confident. Simplified Extended Lesk (SimpleExtLesk). A version of Lesk (1986). Each entry is assigned the sense with highest term overlap between the entry’s context (words in the sentence) and both the sense’s gloss (Kilgarriff and Rosenzweig, 2000) as well as the glosses of its neighbors (Banerjee and Pedersen, 2003). A sense is output only if the overlap exceeds some threshold; we used thresholds in the range of 1–20 in our experiments. There are many subtleties and details in the implementation of SimpleExtLesk so we used two different libraries: a Java implementation of WordNet::Similarity (Pedersen et al., 2004),8 which we modified to accept a context string, and DKPro-WSD (Miller et al., 2013) version 1.1.0, with lemmatization, removal of stop words, paired overlap enabled and normalization disabled. Degree Centrality. Proposed by </context>
</contexts>
<marker>Kilgarriff, Rosenzweig, 2000</marker>
<rawString>Adam Kilgarriff and Joseph Rosenzweig. 2000. Framework and results for english senseval. Computers and the Humanities, 34(1-2):15–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
<author>Anna Korhonen</author>
<author>Neville Ryant</author>
<author>Martha Palmer</author>
</authors>
<title>A large-scale classification of English verbs.</title>
<date>2008</date>
<journal>Language Resources and Evaluation,</journal>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="19828" citStr="Kipper et al., 2008" startWordPosition="3272" endWordPosition="3275"> can apply), in others it does not (e.g., hplay-1i is also annotated with 8 but not 26; but here 26 cannot apply). To ensure the effectiveness of syntactic pruning, we only consider the frames that are directly specified in WordNet. This procedure often produces the desired results; in a few cases, however, we do prune the correct sense (e.g., frame 26 for clause “He points out that... ”). 5 Semantic Pruning A verb sense imposes a restriction on the semantic type of the arguments it may take and vice versa (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008). This allows us to further prune the verb candidate set by discarding verb senses whose semantic argument is not present in the clause. WordNet frames potentially allow a shallow type pruning based on the semantics provided for the clause constituents. However we could solely distinguish people (“somebody”) from things (“something”), which is too crude to obtain substantial pruning effects. Moreover, this distinction is sometimes ambiguous. Instead, we have developed a more powerful approach to semantic pruning based on our VOS repository. We remove from the verb candidate set those senses wh</context>
<context position="40190" citStr="Kipper et al., 2008" startWordPosition="6757" endWordPosition="6760">pervised system. In our work, we also incorporate syntactic and semantic information. Instead of learning the relation between the verb senses and the syntactic structure, however we incorporate it explicitly using the WordNet frames, which provide information about which verb sense should be consider for a given syntactic pattern. We also incorporate explicitly the semantic relation between each verb sense and its arguments using our VOS repository. Different resources of semantic arguments for automatic text understanding tasks have been constructed (Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008; Gurevych et al., 2012; Nakashole et al., 2012; Flati and Navigli, 2013). In (Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008; Gurevych et al., 2012), the classification of verbs and arguments is focused toward semantic or thematic roles. Nakashole et al. (2012) uses semantic types to construct a taxonomy of binary relations and Flati and Navigli (2013) collected semantic arguments for given textual expressions. For instance, given the verb “break”, they extract a pattern “break (body part-1)”. In contrast to existing resources, our VOS repository disambiguates both the verb sens</context>
</contexts>
<marker>Kipper, Korhonen, Ryant, Palmer, 2008</marker>
<rawString>Karin Kipper, Anna Korhonen, Neville Ryant, and Martha Palmer. 2008. A large-scale classification of English verbs. Language Resources and Evaluation, 42(1):21–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="26752" citStr="Klein and Manning, 2003" startWordPosition="4409" endWordPosition="4412">in the repository, the correct verb sense is retained but pruning may be less effective. 7 Evaluation Dataset. We tested Werdy on the SemEval-2007 coarse-grained dataset.6 It consists of five senseannotated documents; the sense annotations refer to a coarse-grained version of WordNet. In addition to sense annotations, the corpus also provides the corresponding KB entries (henceforth termed “gold entries”) as well as a POS tag. We restrict our evaluation to verbs that act as clause heads. In total, 461 such verbs were recognized by ClausIE (Del Corro and Gemulla, 2013) and the Stanford Parser (Klein and Manning, 2003).7 WSD Algorithms. For the final step of Werdy, we used the KB-based WSD algorithms of Ponzetto and Navigli (2010) and It-MakesSense (Zhong and Ng, 2010), a state-of-the-art supervised system that was the best performer in SemEval-2007. Each method only labels entries for which it is sufficiently confident. Simplified Extended Lesk (SimpleExtLesk). A version of Lesk (1986). Each entry is assigned the sense with highest term overlap between the entry’s context (words in the sentence) and both the sense’s gloss (Kilgarriff and Rosenzweig, 2000) as well as the glosses of its neighbors (Banerjee a</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shari Landes</author>
<author>Claudia Leacock</author>
<author>Randee I Tengi</author>
</authors>
<title>Building Semantic Concordances.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="22615" citStr="Landes et al., 1998" startWordPosition="3733" endWordPosition="3736"> argument in the sentence, or our repository is incomplete. To handle incompleteness to some extent, we also consider hyponyms of the object-argument senses in our repository; e.g., if we observe object sport in a sentence and have verb-sense argument (football1) in our corpus, we consider this a match. If the hyponyms lead to a match, we retain the verb’s candidate sense; otherwise, we discard it. 6 Verb-Object Sense Repository We use three different methods to construct the repository. In particular, we harness the senseannotated WordNet glosses4 as well as the senseannotated SemCor corpus (Landes et al., 1998).5 The major part of the VOS repository was acquired from WordNet’s gloss tags. According to Atkins and Rundell (2008), noun definitions should be expressed in terms of the class to which they belong, and verb definitions should refer to the types of the subjects or objects related to the action. Based on this rationale, we extracted all noun senses that appear in the gloss of each verb sense; each of these noun senses is treated as a possible sense of the object argument of the corresponding verb sense. For example, the gloss of (play-1) is “participate in games or sports;” each noun is annot</context>
</contexts>
<marker>Landes, Leacock, Tengi, 1998</marker>
<rawString>Shari Landes, Claudia Leacock, and Randee I. Tengi, 1998. Building Semantic Concordances. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of SIGDOC,</booktitle>
<pages>24--26</pages>
<contexts>
<context position="27127" citStr="Lesk (1986)" startWordPosition="4468" endWordPosition="4469">rmed “gold entries”) as well as a POS tag. We restrict our evaluation to verbs that act as clause heads. In total, 461 such verbs were recognized by ClausIE (Del Corro and Gemulla, 2013) and the Stanford Parser (Klein and Manning, 2003).7 WSD Algorithms. For the final step of Werdy, we used the KB-based WSD algorithms of Ponzetto and Navigli (2010) and It-MakesSense (Zhong and Ng, 2010), a state-of-the-art supervised system that was the best performer in SemEval-2007. Each method only labels entries for which it is sufficiently confident. Simplified Extended Lesk (SimpleExtLesk). A version of Lesk (1986). Each entry is assigned the sense with highest term overlap between the entry’s context (words in the sentence) and both the sense’s gloss (Kilgarriff and Rosenzweig, 2000) as well as the glosses of its neighbors (Banerjee and Pedersen, 2003). A sense is output only if the overlap exceeds some threshold; we used thresholds in the range of 1–20 in our experiments. There are many subtleties and details in the implementation of SimpleExtLesk so we used two different libraries: a Java implementation of WordNet::Similarity (Pedersen et al., 2004),8 which we modified to accept a context string, and</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of SIGDOC, pages 24–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="8759" citStr="Levin, 1993" startWordPosition="1388" endWordPosition="1389">ense), and breath (noun, 5 candidate senses). Note that in contrast to Werdy, most existing word-sense disambiguation methods assume that entries have already been (correctly) identified. 1The VOS repository, Werdy’s source code, and results of our experimental study are available at http://people. mpi-inf.mpg.de/˜corrogg/. 375 In the syntactic-pruning step (Sec. 4), we eliminate candidate senses that do not agree with the syntactic structure of the clause. It is wellestablished that the syntactic realization of a clause is intrinsically related with the sense of its verb (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005). Quirk et al. (1985) identified seven possible clause types in the English language (such as “subject verb adverbial”, SVA). We make use of techniques inspired by Del Corro and Gemulla (2013) to identify the clause type of each clause in the sentence. We then match the clause type with the set of WordNet frames (e.g., “somebody verb something”) that WordNet provides for each verb sense, and prune verb senses for which there is no match. In the semantic-pruning step (Sec. 5), we further prune the set of candidate senses by taking the seman</context>
<context position="19752" citStr="Levin, 1993" startWordPosition="3260" endWordPosition="3261"> also apply (e.g., hpoint out-1i is annotated with 8 but not 26; 26 can apply), in others it does not (e.g., hplay-1i is also annotated with 8 but not 26; but here 26 cannot apply). To ensure the effectiveness of syntactic pruning, we only consider the frames that are directly specified in WordNet. This procedure often produces the desired results; in a few cases, however, we do prune the correct sense (e.g., frame 26 for clause “He points out that... ”). 5 Semantic Pruning A verb sense imposes a restriction on the semantic type of the arguments it may take and vice versa (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008). This allows us to further prune the verb candidate set by discarding verb senses whose semantic argument is not present in the clause. WordNet frames potentially allow a shallow type pruning based on the semantics provided for the clause constituents. However we could solely distinguish people (“somebody”) from things (“something”), which is too crude to obtain substantial pruning effects. Moreover, this distinction is sometimes ambiguous. Instead, we have developed a more powerful approach to semantic pruning based </context>
<context position="38797" citStr="Levin, 1993" startWordPosition="6535" endWordPosition="6536">essions imposes multiple challenges to text understanding tasks (Sag et al., 2002). The problem has been addressed by Arranz et al. (2005) and Finlayson and Kulkarni (2011). They find multi-word entries by matching word sequences allowing some morphological and POS variations according to predefined patterns. Our method differs in that we can recognize KB entries that appear discontinuously and in that we do not select the correct entry but generate a set of potential entries. Linguists have noted the link between verb senses and the syntactic structure and argument types (Quirk et al., 1985; Levin, 1993; Hanks, 1996), and supervised WSD systems were developed to capture this relation (Dang and Palmer, 2005; Chen and Palmer, 2009; Dligach and Palmer, 2008; Cholakov et al., 2014). In Dang and Palmer (2005) and Chen and Palmer (2009), it is shown that WSD tasks can be improved with features that capture the syntactic structure and information about verb arguments and their types. They use features as shallow named entity recognition and the hypernyms of the possible senses of the noun arguments. Dang and Palmer (2005) also included features extracted from PropBank (Palmer et al., 2005) from rol</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tristan Miller</author>
<author>Chris Biemann</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Using distributional similarity for lexical expansion in knowledge-based word sense disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1781--1796</pages>
<contexts>
<context position="37369" citStr="Miller et al., 2012" startWordPosition="6290" endWordPosition="6293">s can be distinguished in WSD. Supervised systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign senses to words, mostly relying on manually annotated data for training. In principle, these systems suffer from low coverage since the training data is usually sparse. Some authors have tried to overcome this limitation by exploiting linked resources as training data (Shen et al., 2013; Cholakov et al., 2014). The second WSD approach corresponds to the so-called KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). They rely on a background KB (typically WordNet or extended versions (Navigli and Ponzetto, 2012)), where related senses appear close to each other. KB-based algorithms often differ in the way the KB is explored. It has been shown that a key point to enhance performance is the amount of semantic information in the KB (Ponzetto and Navigli, 2010; Miller et al., 2012). Our framework fits this line of work since it is also unsupervised and enriches the background knowledge in order to enhance performance of standard WSD algorithms. A comprehensive overview of WSD systems c</context>
</contexts>
<marker>Miller, Biemann, Zesch, Gurevych, 2012</marker>
<rawString>Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna Gurevych. 2012. Using distributional similarity for lexical expansion in knowledge-based word sense disambiguation. In Proceedings of COLING, pages 1781–1796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tristan Miller</author>
<author>Nicolai Erbs</author>
<author>Hans-Peter Zorn</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Dkpro wsd: A generalized uima-based framework for word sense disambiguation.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL: System Demonstrations,</booktitle>
<pages>37--42</pages>
<contexts>
<context position="27759" citStr="Miller et al., 2013" startWordPosition="4570" endWordPosition="4573"> is assigned the sense with highest term overlap between the entry’s context (words in the sentence) and both the sense’s gloss (Kilgarriff and Rosenzweig, 2000) as well as the glosses of its neighbors (Banerjee and Pedersen, 2003). A sense is output only if the overlap exceeds some threshold; we used thresholds in the range of 1–20 in our experiments. There are many subtleties and details in the implementation of SimpleExtLesk so we used two different libraries: a Java implementation of WordNet::Similarity (Pedersen et al., 2004),8 which we modified to accept a context string, and DKPro-WSD (Miller et al., 2013) version 1.1.0, with lemmatization, removal of stop words, paired overlap enabled and normalization disabled. Degree Centrality. Proposed by Navigli and Lapata (2010). The method collects all paths connecting each candidate sense of an entry to the set of candidate senses of the words the entry’s context. The candidate sense with the highest degree in the resulting subgraph is selected. We implemented this algorithm using the Neo4j library.9 6The data is annotated with WordNet 2.1 senses; we converted the annotations to WordNet-3.0 using DKProWSD (Miller et al., 2013). 7Version 3.3.1, model en</context>
</contexts>
<marker>Miller, Erbs, Zorn, Zesch, Gurevych, 2013</marker>
<rawString>Tristan Miller, Nicolai Erbs, Hans-Peter Zorn, Torsten Zesch, and Iryna Gurevych. 2013. Dkpro wsd: A generalized uima-based framework for word sense disambiguation. In Proceedings of ACL: System Demonstrations, pages 37–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ndapandula Nakashole</author>
<author>Gerhard Weikum</author>
<author>Fabian Suchanek</author>
</authors>
<title>Patty: A taxonomy of relational patterns with semantic types.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1135--1145</pages>
<contexts>
<context position="40237" citStr="Nakashole et al., 2012" startWordPosition="6765" endWordPosition="6769">orate syntactic and semantic information. Instead of learning the relation between the verb senses and the syntactic structure, however we incorporate it explicitly using the WordNet frames, which provide information about which verb sense should be consider for a given syntactic pattern. We also incorporate explicitly the semantic relation between each verb sense and its arguments using our VOS repository. Different resources of semantic arguments for automatic text understanding tasks have been constructed (Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008; Gurevych et al., 2012; Nakashole et al., 2012; Flati and Navigli, 2013). In (Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008; Gurevych et al., 2012), the classification of verbs and arguments is focused toward semantic or thematic roles. Nakashole et al. (2012) uses semantic types to construct a taxonomy of binary relations and Flati and Navigli (2013) collected semantic arguments for given textual expressions. For instance, given the verb “break”, they extract a pattern “break (body part-1)”. In contrast to existing resources, our VOS repository disambiguates both the verb sense and the senses of its arguments. 9 Conclusion</context>
</contexts>
<marker>Nakashole, Weikum, Suchanek, 2012</marker>
<rawString>Ndapandula Nakashole, Gerhard Weikum, and Fabian Suchanek. 2012. Patty: A taxonomy of relational patterns with semantic types. In Proceedings of EMNLP, pages 1135–1145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Mirella Lapata</author>
</authors>
<title>An experimental study of graph connectivity for unsupervised word sense disambiguation.</title>
<date>2010</date>
<journal>EEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="27925" citStr="Navigli and Lapata (2010)" startWordPosition="4592" endWordPosition="4596"> as well as the glosses of its neighbors (Banerjee and Pedersen, 2003). A sense is output only if the overlap exceeds some threshold; we used thresholds in the range of 1–20 in our experiments. There are many subtleties and details in the implementation of SimpleExtLesk so we used two different libraries: a Java implementation of WordNet::Similarity (Pedersen et al., 2004),8 which we modified to accept a context string, and DKPro-WSD (Miller et al., 2013) version 1.1.0, with lemmatization, removal of stop words, paired overlap enabled and normalization disabled. Degree Centrality. Proposed by Navigli and Lapata (2010). The method collects all paths connecting each candidate sense of an entry to the set of candidate senses of the words the entry’s context. The candidate sense with the highest degree in the resulting subgraph is selected. We implemented this algorithm using the Neo4j library.9 6The data is annotated with WordNet 2.1 senses; we converted the annotations to WordNet-3.0 using DKProWSD (Miller et al., 2013). 7Version 3.3.1, model englishRNN.ser.gz 8http://www.sussex.ac.uk/Users/drh21/ 9http://www.neo4j.org/ We used a fixed threshold of 1 and vary the search depth in range 1–20. We used the candi</context>
</contexts>
<marker>Navigli, Lapata, 2010</marker>
<rawString>Roberto Navigli and Mirella Lapata. 2010. An experimental study of graph connectivity for unsupervised word sense disambiguation. EEE Transactions on Pattern Analysis and Machine Intelligence, 32(4):678–692.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Babelnet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<volume>193</volume>
<issue>0</issue>
<pages>250</pages>
<contexts>
<context position="37490" citStr="Navigli and Ponzetto, 2012" startWordPosition="6311" endWordPosition="6314">er, 2009; Zhong and Ng, 2010) use a classifier to assign senses to words, mostly relying on manually annotated data for training. In principle, these systems suffer from low coverage since the training data is usually sparse. Some authors have tried to overcome this limitation by exploiting linked resources as training data (Shen et al., 2013; Cholakov et al., 2014). The second WSD approach corresponds to the so-called KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). They rely on a background KB (typically WordNet or extended versions (Navigli and Ponzetto, 2012)), where related senses appear close to each other. KB-based algorithms often differ in the way the KB is explored. It has been shown that a key point to enhance performance is the amount of semantic information in the KB (Ponzetto and Navigli, 2010; Miller et al., 2012). Our framework fits this line of work since it is also unsupervised and enriches the background knowledge in order to enhance performance of standard WSD algorithms. A comprehensive overview of WSD systems can be found in Navigli (2009) and Navigli (2012). To bring WSD to real-world applications, the mapping between text and K</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. Babelnet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193(0):217– 250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Kenneth C Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>Semeval-2007 task 07: Coarsegrained english all-words task.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval,</booktitle>
<pages>30--35</pages>
<contexts>
<context position="6487" citStr="Navigli et al., 2007" startWordPosition="1022" endWordPosition="1025">ntly gains accuracy and efficiency over standard WSD. Our semantic pruning technique builds on a newly created resource of pairs of senses for verbs and their object arguments. For example, the WordNet verb sense (play-1) (i.e., the 1st sense of the verb entry “play”) selects as direct object the noun sense (sport-1). We refer to this novel resource as the VO Sense Repository, or VOS repository for short.1 It is constructed from the WordNet gloss-tags corpus, the SemCor dataset, and a small set of manually created VO sense pairs. We evaluated Werdy on the SemEval-2007 coarse-grained WSD task (Navigli et al., 2007), both with and without automatic recognition of entries. We found that our techniques boost state-ofthe-art WSD methods and obtain high-quality results. Werdy significantly increases the precision and recall of the best performing baselines. The rest of the paper is organized as follows. Section 2 gives an overview of Werdy components. Section 3 presents the entry recognition, and Sections 4 and 5 discuss our novel syntactic and semantic pruning techniques. Section 6 presents the Semantic VO Repository and how we constructed it. Section 7 gives the results of our evaluation. Section 8 discuss</context>
</contexts>
<marker>Navigli, Litkowski, Hargraves, 2007</marker>
<rawString>Roberto Navigli, Kenneth C. Litkowski, and Orin Hargraves. 2007. Semeval-2007 task 07: Coarsegrained english all-words task. In Proceedings of SemEval, pages 30–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<volume>41</volume>
<issue>2</issue>
<pages>10--69</pages>
<contexts>
<context position="3823" citStr="Navigli, 2009" startWordPosition="588" endWordPosition="590">tries in a sentence and passes them to the disambiguation phase (take, breath, take a breath, ... ); the disambiguation phase provides more information about which multi-word entries to keep. Thus, our method solves the recognition and the disambiguation tasks jointly. Once KB entries have been identified, Werdy 374 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 374–385, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics disambiguates each entry against its possible senses. State-of-the-art methods for WSD (Navigli, 2009) work fairly well for nouns and noun phrases. However, the disambiguation of verbs and verbal phrases has received much less attention in the literature. WSD methods can be roughly categorized into (i) methods that are based on supervised training over sense-annotated corpora (e.g., Zhong and Ng (2010)), and (ii) methods that harness KB’s to assess the semantic relatedness among word senses for mapping entries to senses (e.g., Ponzetto and Navigli (2010)). For these methods, mapping verbs to senses is a difficult task since verbs tend to have more senses than nouns. In WordNet, including monos</context>
<context position="37998" citStr="Navigli (2009)" startWordPosition="6404" endWordPosition="6405">, 2014). They rely on a background KB (typically WordNet or extended versions (Navigli and Ponzetto, 2012)), where related senses appear close to each other. KB-based algorithms often differ in the way the KB is explored. It has been shown that a key point to enhance performance is the amount of semantic information in the KB (Ponzetto and Navigli, 2010; Miller et al., 2012). Our framework fits this line of work since it is also unsupervised and enriches the background knowledge in order to enhance performance of standard WSD algorithms. A comprehensive overview of WSD systems can be found in Navigli (2009) and Navigli (2012). To bring WSD to real-world applications, the mapping between text and KB entries is a fundamental first step. It has been pointed that the existence of multi-word expressions imposes multiple challenges to text understanding tasks (Sag et al., 2002). The problem has been addressed by Arranz et al. (2005) and Finlayson and Kulkarni (2011). They find multi-word entries by matching word sequences allowing some morphological and POS variations according to predefined patterns. Our method differs in that we can recognize KB entries that appear discontinuously and in that we do </context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Computing Surveys, 41(2):10:1– 10:69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>A quick tour of word sense disambiguation, induction and related approaches.</title>
<date>2012</date>
<booktitle>In Proceedings of SOFSEM,</booktitle>
<pages>115--129</pages>
<contexts>
<context position="38017" citStr="Navigli (2012)" startWordPosition="6407" endWordPosition="6408">on a background KB (typically WordNet or extended versions (Navigli and Ponzetto, 2012)), where related senses appear close to each other. KB-based algorithms often differ in the way the KB is explored. It has been shown that a key point to enhance performance is the amount of semantic information in the KB (Ponzetto and Navigli, 2010; Miller et al., 2012). Our framework fits this line of work since it is also unsupervised and enriches the background knowledge in order to enhance performance of standard WSD algorithms. A comprehensive overview of WSD systems can be found in Navigli (2009) and Navigli (2012). To bring WSD to real-world applications, the mapping between text and KB entries is a fundamental first step. It has been pointed that the existence of multi-word expressions imposes multiple challenges to text understanding tasks (Sag et al., 2002). The problem has been addressed by Arranz et al. (2005) and Finlayson and Kulkarni (2011). They find multi-word entries by matching word sequences allowing some morphological and POS variations according to predefined patterns. Our method differs in that we can recognize KB entries that appear discontinuously and in that we do not select the corr</context>
</contexts>
<marker>Navigli, 2012</marker>
<rawString>Roberto Navigli. 2012. A quick tour of word sense disambiguation, induction and related approaches. In Proceedings of SOFSEM, pages 115–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="8814" citStr="Palmer et al., 2005" startWordPosition="1396" endWordPosition="1399">Note that in contrast to Werdy, most existing word-sense disambiguation methods assume that entries have already been (correctly) identified. 1The VOS repository, Werdy’s source code, and results of our experimental study are available at http://people. mpi-inf.mpg.de/˜corrogg/. 375 In the syntactic-pruning step (Sec. 4), we eliminate candidate senses that do not agree with the syntactic structure of the clause. It is wellestablished that the syntactic realization of a clause is intrinsically related with the sense of its verb (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005). Quirk et al. (1985) identified seven possible clause types in the English language (such as “subject verb adverbial”, SVA). We make use of techniques inspired by Del Corro and Gemulla (2013) to identify the clause type of each clause in the sentence. We then match the clause type with the set of WordNet frames (e.g., “somebody verb something”) that WordNet provides for each verb sense, and prune verb senses for which there is no match. In the semantic-pruning step (Sec. 5), we further prune the set of candidate senses by taking the semantic types of direct objects into account. Similarly to </context>
<context position="19806" citStr="Palmer et al., 2005" startWordPosition="3268" endWordPosition="3271">with 8 but not 26; 26 can apply), in others it does not (e.g., hplay-1i is also annotated with 8 but not 26; but here 26 cannot apply). To ensure the effectiveness of syntactic pruning, we only consider the frames that are directly specified in WordNet. This procedure often produces the desired results; in a few cases, however, we do prune the correct sense (e.g., frame 26 for clause “He points out that... ”). 5 Semantic Pruning A verb sense imposes a restriction on the semantic type of the arguments it may take and vice versa (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008). This allows us to further prune the verb candidate set by discarding verb senses whose semantic argument is not present in the clause. WordNet frames potentially allow a shallow type pruning based on the semantics provided for the clause constituents. However we could solely distinguish people (“somebody”) from things (“something”), which is too crude to obtain substantial pruning effects. Moreover, this distinction is sometimes ambiguous. Instead, we have developed a more powerful approach to semantic pruning based on our VOS repository. We remove from the verb candida</context>
<context position="39388" citStr="Palmer et al., 2005" startWordPosition="6631" endWordPosition="6634">irk et al., 1985; Levin, 1993; Hanks, 1996), and supervised WSD systems were developed to capture this relation (Dang and Palmer, 2005; Chen and Palmer, 2009; Dligach and Palmer, 2008; Cholakov et al., 2014). In Dang and Palmer (2005) and Chen and Palmer (2009), it is shown that WSD tasks can be improved with features that capture the syntactic structure and information about verb arguments and their types. They use features as shallow named entity recognition and the hypernyms of the possible senses of the noun arguments. Dang and Palmer (2005) also included features extracted from PropBank (Palmer et al., 2005) from role labels and frames. Dligach and Palmer (2008) generated a corpus of verb and their arguments (both surface forms), which was used to incorporate a semantic feature to the supervised system. In our work, we also incorporate syntactic and semantic information. Instead of learning the relation between the verb senses and the syntactic structure, however we incorporate it explicitly using the WordNet frames, which provide information about which verb sense should be consider for a given syntactic pattern. We also incorporate explicitly the semantic relation between each verb sense and it</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::similarity: Measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of HLTNAACL: Demonstration Papers,</booktitle>
<pages>38--41</pages>
<contexts>
<context position="27675" citStr="Pedersen et al., 2004" startWordPosition="4556" endWordPosition="4559">fident. Simplified Extended Lesk (SimpleExtLesk). A version of Lesk (1986). Each entry is assigned the sense with highest term overlap between the entry’s context (words in the sentence) and both the sense’s gloss (Kilgarriff and Rosenzweig, 2000) as well as the glosses of its neighbors (Banerjee and Pedersen, 2003). A sense is output only if the overlap exceeds some threshold; we used thresholds in the range of 1–20 in our experiments. There are many subtleties and details in the implementation of SimpleExtLesk so we used two different libraries: a Java implementation of WordNet::Similarity (Pedersen et al., 2004),8 which we modified to accept a context string, and DKPro-WSD (Miller et al., 2013) version 1.1.0, with lemmatization, removal of stop words, paired overlap enabled and normalization disabled. Degree Centrality. Proposed by Navigli and Lapata (2010). The method collects all paths connecting each candidate sense of an entry to the set of candidate senses of the words the entry’s context. The candidate sense with the highest degree in the resulting subgraph is selected. We implemented this algorithm using the Neo4j library.9 6The data is annotated with WordNet 2.1 senses; we converted the annot</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet::similarity: Measuring the relatedness of concepts. In Proceedings of HLTNAACL: Demonstration Papers, pages 38–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Roberto Navigli</author>
</authors>
<title>Knowledge-rich word sense disambiguation rivaling supervised systems.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1522--1531</pages>
<contexts>
<context position="4281" citStr="Ponzetto and Navigli (2010)" startWordPosition="660" endWordPosition="663">014, Doha, Qatar. c�2014 Association for Computational Linguistics disambiguates each entry against its possible senses. State-of-the-art methods for WSD (Navigli, 2009) work fairly well for nouns and noun phrases. However, the disambiguation of verbs and verbal phrases has received much less attention in the literature. WSD methods can be roughly categorized into (i) methods that are based on supervised training over sense-annotated corpora (e.g., Zhong and Ng (2010)), and (ii) methods that harness KB’s to assess the semantic relatedness among word senses for mapping entries to senses (e.g., Ponzetto and Navigli (2010)). For these methods, mapping verbs to senses is a difficult task since verbs tend to have more senses than nouns. In WordNet, including monosemous words, there are on average 1.24 senses per noun and 2.17 per verb. To disambiguate verbs and verbal phrases, Werdy proceeds in multiple steps. First, Werdy obtains the set of candidate senses for each recognized entry from the KB. Second, it reduces the set of candidate entries using novel syntactic and semantic pruning techniques. The key insight behind our syntactic pruning is that each verb sense tends to occur in a only limited number of synta</context>
<context position="26866" citStr="Ponzetto and Navigli (2010)" startWordPosition="4428" endWordPosition="4431">We tested Werdy on the SemEval-2007 coarse-grained dataset.6 It consists of five senseannotated documents; the sense annotations refer to a coarse-grained version of WordNet. In addition to sense annotations, the corpus also provides the corresponding KB entries (henceforth termed “gold entries”) as well as a POS tag. We restrict our evaluation to verbs that act as clause heads. In total, 461 such verbs were recognized by ClausIE (Del Corro and Gemulla, 2013) and the Stanford Parser (Klein and Manning, 2003).7 WSD Algorithms. For the final step of Werdy, we used the KB-based WSD algorithms of Ponzetto and Navigli (2010) and It-MakesSense (Zhong and Ng, 2010), a state-of-the-art supervised system that was the best performer in SemEval-2007. Each method only labels entries for which it is sufficiently confident. Simplified Extended Lesk (SimpleExtLesk). A version of Lesk (1986). Each entry is assigned the sense with highest term overlap between the entry’s context (words in the sentence) and both the sense’s gloss (Kilgarriff and Rosenzweig, 2000) as well as the glosses of its neighbors (Banerjee and Pedersen, 2003). A sense is output only if the overlap exceeds some threshold; we used thresholds in the range </context>
<context position="37348" citStr="Ponzetto and Navigli, 2010" startWordPosition="6286" endWordPosition="6289">s a KB). Two types of methods can be distinguished in WSD. Supervised systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign senses to words, mostly relying on manually annotated data for training. In principle, these systems suffer from low coverage since the training data is usually sparse. Some authors have tried to overcome this limitation by exploiting linked resources as training data (Shen et al., 2013; Cholakov et al., 2014). The second WSD approach corresponds to the so-called KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). They rely on a background KB (typically WordNet or extended versions (Navigli and Ponzetto, 2012)), where related senses appear close to each other. KB-based algorithms often differ in the way the KB is explored. It has been shown that a key point to enhance performance is the amount of semantic information in the KB (Ponzetto and Navigli, 2010; Miller et al., 2012). Our framework fits this line of work since it is also unsupervised and enriches the background knowledge in order to enhance performance of standard WSD algorithms. A comprehensive over</context>
</contexts>
<marker>Ponzetto, Navigli, 2010</marker>
<rawString>Simone Paolo Ponzetto and Roberto Navigli. 2010. Knowledge-rich word sense disambiguation rivaling supervised systems. In Proceedings of ACL, pages 1522–1531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Geoffrey Leech</author>
<author>Jan Svartvik</author>
</authors>
<date>1985</date>
<journal>A Comprehensive Grammar of the English Language. Longman.</journal>
<contexts>
<context position="8746" citStr="Quirk et al., 1985" startWordPosition="1384" endWordPosition="1387">(verb, 1 candidate sense), and breath (noun, 5 candidate senses). Note that in contrast to Werdy, most existing word-sense disambiguation methods assume that entries have already been (correctly) identified. 1The VOS repository, Werdy’s source code, and results of our experimental study are available at http://people. mpi-inf.mpg.de/˜corrogg/. 375 In the syntactic-pruning step (Sec. 4), we eliminate candidate senses that do not agree with the syntactic structure of the clause. It is wellestablished that the syntactic realization of a clause is intrinsically related with the sense of its verb (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005). Quirk et al. (1985) identified seven possible clause types in the English language (such as “subject verb adverbial”, SVA). We make use of techniques inspired by Del Corro and Gemulla (2013) to identify the clause type of each clause in the sentence. We then match the clause type with the set of WordNet frames (e.g., “somebody verb something”) that WordNet provides for each verb sense, and prune verb senses for which there is no match. In the semantic-pruning step (Sec. 5), we further prune the set of candidate senses by tak</context>
<context position="14322" citStr="Quirk et al., 1985" startWordPosition="2330" endWordPosition="2333"> verb, Vct: Complex-transitive verb Table 1: Clause types and examples of matching WordNet frames tries for all but two verbs (out of more than 400). The two missed entries (take up and get rolling) resulted from incorrect dependency parses. 4 Syntactic Pruning Once the KB entries have been recognized, Werdy prunes the set of possible senses of each verb entry by considering the syntactic structure of the clause in which the entry occurs. This pruning is based on the observation that each verb sense may occur only in a limited number of “clause types”, each having specific semantic functions (Quirk et al., 1985). When the clause type of the sentence is incompatible with a candidate sense of an entry, this sense is eliminated. Werdy first detects in the input sentence the set of clauses and their constituents. A clause consists of one subject (S), one verb (V), and optionally an indirect object (O), a direct object (O), a complement (C) and one or more adverbials (A). Not all combinations of clause constituents appear in the English language. When we classify clauses according to the grammatical function of their constituents, we obtain only seven different clause types (Quirk et al., 1985); see Table</context>
<context position="19739" citStr="Quirk et al., 1985" startWordPosition="3256" endWordPosition="3259">ore specific one can also apply (e.g., hpoint out-1i is annotated with 8 but not 26; 26 can apply), in others it does not (e.g., hplay-1i is also annotated with 8 but not 26; but here 26 cannot apply). To ensure the effectiveness of syntactic pruning, we only consider the frames that are directly specified in WordNet. This procedure often produces the desired results; in a few cases, however, we do prune the correct sense (e.g., frame 26 for clause “He points out that... ”). 5 Semantic Pruning A verb sense imposes a restriction on the semantic type of the arguments it may take and vice versa (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008). This allows us to further prune the verb candidate set by discarding verb senses whose semantic argument is not present in the clause. WordNet frames potentially allow a shallow type pruning based on the semantics provided for the clause constituents. However we could solely distinguish people (“somebody”) from things (“something”), which is too crude to obtain substantial pruning effects. Moreover, this distinction is sometimes ambiguous. Instead, we have developed a more powerful approach to semantic p</context>
<context position="38784" citStr="Quirk et al., 1985" startWordPosition="6531" endWordPosition="6534">e of multi-word expressions imposes multiple challenges to text understanding tasks (Sag et al., 2002). The problem has been addressed by Arranz et al. (2005) and Finlayson and Kulkarni (2011). They find multi-word entries by matching word sequences allowing some morphological and POS variations according to predefined patterns. Our method differs in that we can recognize KB entries that appear discontinuously and in that we do not select the correct entry but generate a set of potential entries. Linguists have noted the link between verb senses and the syntactic structure and argument types (Quirk et al., 1985; Levin, 1993; Hanks, 1996), and supervised WSD systems were developed to capture this relation (Dang and Palmer, 2005; Chen and Palmer, 2009; Dligach and Palmer, 2008; Cholakov et al., 2014). In Dang and Palmer (2005) and Chen and Palmer (2009), it is shown that WSD tasks can be improved with features that capture the syntactic structure and information about verb arguments and their types. They use features as shallow named entity recognition and the hypernyms of the possible senses of the noun arguments. Dang and Palmer (2005) also included features extracted from PropBank (Palmer et al., 2</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. 1985. A Comprehensive Grammar of the English Language. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann A Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multiword expressions: A pain in the neck for nlp.</title>
<date>2002</date>
<booktitle>In Proceedings of CICLing,</booktitle>
<pages>1--15</pages>
<contexts>
<context position="38268" citStr="Sag et al., 2002" startWordPosition="6447" endWordPosition="6450"> performance is the amount of semantic information in the KB (Ponzetto and Navigli, 2010; Miller et al., 2012). Our framework fits this line of work since it is also unsupervised and enriches the background knowledge in order to enhance performance of standard WSD algorithms. A comprehensive overview of WSD systems can be found in Navigli (2009) and Navigli (2012). To bring WSD to real-world applications, the mapping between text and KB entries is a fundamental first step. It has been pointed that the existence of multi-word expressions imposes multiple challenges to text understanding tasks (Sag et al., 2002). The problem has been addressed by Arranz et al. (2005) and Finlayson and Kulkarni (2011). They find multi-word entries by matching word sequences allowing some morphological and POS variations according to predefined patterns. Our method differs in that we can recognize KB entries that appear discontinuously and in that we do not select the correct entry but generate a set of potential entries. Linguists have noted the link between verb senses and the syntactic structure and argument types (Quirk et al., 1985; Levin, 1993; Hanks, 1996), and supervised WSD systems were developed to capture th</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A. Copestake, and Dan Flickinger. 2002. Multiword expressions: A pain in the neck for nlp. In Proceedings of CICLing, pages 1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Shen</author>
<author>Razvan Bunescu</author>
<author>Rada Mihalcea</author>
</authors>
<title>Coarse to fine grained sense disambiguation in wikipedia.</title>
<date>2013</date>
<booktitle>In Proceedings of *SEM,</booktitle>
<pages>22--31</pages>
<contexts>
<context position="37207" citStr="Shen et al., 2013" startWordPosition="6264" endWordPosition="6267"> Related Work WSD is a classification task where for every word there is a set of possible senses given by some external resource (as a KB). Two types of methods can be distinguished in WSD. Supervised systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign senses to words, mostly relying on manually annotated data for training. In principle, these systems suffer from low coverage since the training data is usually sparse. Some authors have tried to overcome this limitation by exploiting linked resources as training data (Shen et al., 2013; Cholakov et al., 2014). The second WSD approach corresponds to the so-called KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). They rely on a background KB (typically WordNet or extended versions (Navigli and Ponzetto, 2012)), where related senses appear close to each other. KB-based algorithms often differ in the way the KB is explored. It has been shown that a key point to enhance performance is the amount of semantic information in the KB (Ponzetto and Navigli, 2010; Miller et al., 2012). Our framework fits this line of work since i</context>
</contexts>
<marker>Shen, Bunescu, Mihalcea, 2013</marker>
<rawString>Hui Shen, Razvan Bunescu, and Rada Mihalcea. 2013. Coarse to fine grained sense disambiguation in wikipedia. In Proceedings of *SEM, pages 22–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi Zhong</author>
<author>Hwee Tou Ng</author>
</authors>
<title>It makes sense: A wide-coverage word sense disambiguation system for free text.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL: System Demonstrations,</booktitle>
<pages>78--83</pages>
<contexts>
<context position="4126" citStr="Zhong and Ng (2010)" startWordPosition="635" endWordPosition="638">entified, Werdy 374 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 374–385, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics disambiguates each entry against its possible senses. State-of-the-art methods for WSD (Navigli, 2009) work fairly well for nouns and noun phrases. However, the disambiguation of verbs and verbal phrases has received much less attention in the literature. WSD methods can be roughly categorized into (i) methods that are based on supervised training over sense-annotated corpora (e.g., Zhong and Ng (2010)), and (ii) methods that harness KB’s to assess the semantic relatedness among word senses for mapping entries to senses (e.g., Ponzetto and Navigli (2010)). For these methods, mapping verbs to senses is a difficult task since verbs tend to have more senses than nouns. In WordNet, including monosemous words, there are on average 1.24 senses per noun and 2.17 per verb. To disambiguate verbs and verbal phrases, Werdy proceeds in multiple steps. First, Werdy obtains the set of candidate senses for each recognized entry from the KB. Second, it reduces the set of candidate entries using novel synta</context>
<context position="26905" citStr="Zhong and Ng, 2010" startWordPosition="4435" endWordPosition="4438">ned dataset.6 It consists of five senseannotated documents; the sense annotations refer to a coarse-grained version of WordNet. In addition to sense annotations, the corpus also provides the corresponding KB entries (henceforth termed “gold entries”) as well as a POS tag. We restrict our evaluation to verbs that act as clause heads. In total, 461 such verbs were recognized by ClausIE (Del Corro and Gemulla, 2013) and the Stanford Parser (Klein and Manning, 2003).7 WSD Algorithms. For the final step of Werdy, we used the KB-based WSD algorithms of Ponzetto and Navigli (2010) and It-MakesSense (Zhong and Ng, 2010), a state-of-the-art supervised system that was the best performer in SemEval-2007. Each method only labels entries for which it is sufficiently confident. Simplified Extended Lesk (SimpleExtLesk). A version of Lesk (1986). Each entry is assigned the sense with highest term overlap between the entry’s context (words in the sentence) and both the sense’s gloss (Kilgarriff and Rosenzweig, 2000) as well as the glosses of its neighbors (Banerjee and Pedersen, 2003). A sense is output only if the overlap exceeds some threshold; we used thresholds in the range of 1–20 in our experiments. There are m</context>
<context position="28685" citStr="Zhong and Ng, 2010" startWordPosition="4711" endWordPosition="4714">The candidate sense with the highest degree in the resulting subgraph is selected. We implemented this algorithm using the Neo4j library.9 6The data is annotated with WordNet 2.1 senses; we converted the annotations to WordNet-3.0 using DKProWSD (Miller et al., 2013). 7Version 3.3.1, model englishRNN.ser.gz 8http://www.sussex.ac.uk/Users/drh21/ 9http://www.neo4j.org/ We used a fixed threshold of 1 and vary the search depth in range 1–20. We used the candidate senses of all nouns and verbs in a sentence as context. It-Makes-Sense (IMS). A state-of-the-art, publicly available supervised system (Zhong and Ng, 2010) and a refined version of Chan et al. (2007), which ranked first in the SemEval-2007 coarse grained task. We modified the code to accept KB entries and their candidate senses. We tested both in WordNet-2.1 and 3.0; for the later we mapped Werdy’s set of candidates to WordNet-2.1. Most Frequent Sense (MFS). Selects the most frequent sense (according to WordNet frequencies) among the set of candidate senses of an entry. If there is a tie, we do not label. Note that this procedure differs slightly from the standard of picking the entry with the smallest sense id. We do not follow this approach si</context>
<context position="36892" citStr="Zhong and Ng, 2010" startWordPosition="6213" endWordPosition="6216">ation, but also stress the necessity to improve existing resources. Ideally, each verb sense would be annotated with both the possible clause types or syntactic patterns in which it can occur as well as the possible senses of its objects. Annotations for subjects and adverbial arguments may also be beneficial. 382 8 Related Work WSD is a classification task where for every word there is a set of possible senses given by some external resource (as a KB). Two types of methods can be distinguished in WSD. Supervised systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign senses to words, mostly relying on manually annotated data for training. In principle, these systems suffer from low coverage since the training data is usually sparse. Some authors have tried to overcome this limitation by exploiting linked resources as training data (Shen et al., 2013; Cholakov et al., 2014). The second WSD approach corresponds to the so-called KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014). They rely on a background KB (typically WordNet or extended versions (Navigli and Ponzetto, 2012)),</context>
</contexts>
<marker>Zhong, Ng, 2010</marker>
<rawString>Zhi Zhong and Hwee Tou Ng. 2010. It makes sense: A wide-coverage word sense disambiguation system for free text. In Proceedings of ACL: System Demonstrations, pages 78–83.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.926692">
Question Answering over Linked Data Using First-order Logic∗
</title>
<author confidence="0.998212">
Shizhu He, Kang Liu, Yuanzhe Zhang, Liheng Xu and Jun Zhao
</author>
<affiliation confidence="0.99277">
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
</affiliation>
<email confidence="0.979339">
{shizhu.he, kliu, yzzhang, lhxu, jzhao}@nlpr.ia.ac.cn
</email>
<sectionHeader confidence="0.994799" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.990180357142857">
Question Answering over Linked Data
(QALD) aims to evaluate a question an-
swering system over structured data, the
key objective of which is to translate
questions posed using natural language
into structured queries. This technique
can help common users to directly ac-
cess open-structured knowledge on the
Web and, accordingly, has attracted much
attention. To this end, we propose a
novel method using first-order logic. We
formulate the knowledge for resolving
the ambiguities in the main three steps
of QALD (phrase detection, phrase-to-
semantic-item mapping and semantic item
grouping) as first-order logic clauses in a
Markov Logic Network. All clauses can
then produce interacted effects in a unified
framework and can jointly resolve all am-
biguities. Moreover, our method adopts a
pattern-learning strategy for semantic item
grouping. In this way, our method can
cover more text expressions and answer
more questions than previous methods us-
ing manually designed patterns. The ex-
perimental results using open benchmarks
demonstrate the effectiveness of the pro-
posed method.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989008895833333">
With the rapid development of the Web of Data,
many RDF datasets have been published as Linked
Data (Bizer et al., 2009), such as DBpedia (Auer
et al., 2007), Freebase (Bollacker et al., 2008)
and YAGO (Suchanek et al., 2007). The grow-
ing amount of Linked Data contains a wealth of
knowledge, including entities, classes and rela-
tions. Moreover, these linked data usually have
∗Shizhu He and Kang Liu have equal contribution to this
work.
complex structures and are highly heterogeneous.
As a result, there are gaps for users regarding ac-
cess. Although a few experts can write queries us-
ing structured languages (such as SPARQL) based
on their needs, this skill cannot be easily utilized
by common users (Christina and Freitas, 2014).
Thus, providing user-friendly, simple interfaces
to access these linked data becomes increasingly
more urgent.
Because of this, question answering over linked
data (QALD) (Walter et al., 2012) has recently
received much interest, and most studies on this
topic have focused on translating natural lan-
guage questions into structured queries (Freitas
and Curry, 2014; Yahya et al., 2012; Unger et al.,
2012; Shekarpour et al., 2013; Yahya et al., 2013;
Bao et al., 2014; Zou et al., 2014). For example,
with respect to the question
“Which software has been developed by organi-
zations founded in California, USA?”,
the aim is to automatically convert this utterance
into an SPARQL query that contains the follow-
ing subject-property-object (SPO) triple format:
?url rdf:type dbo:Software, ?url dbo:developer ?x1,
?x1 rdf:type dbo:Company, ?x1 dbo:foundationPlace
dbr:California)1.
To fulfill this objective, existing systems (Lopez
et al., 2006; Unger et al., 2012; Yahya et al., 2012;
Zou et al., 2014) usually adopt a pipeline frame-
work that contains four major steps: 1) decompos-
ing the question and detecting phrases, 2) map-
ping the detected phrases into semantic items of
Linked Data, 3) grouping the mapped semantic
items into semantic triples, and 4) generating the
correct SPARQL query.
However, completing these four steps and con-
structing such a structured query is not easy. The
first three steps mentioned above are subject to the
</bodyText>
<footnote confidence="0.999072">
1The prefixes in semantic items indicate the source of
their vocabularies.
</footnote>
<page confidence="0.893276">
1092
</page>
<note confidence="0.9091355">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1092–1103,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999872591836735">
problem of ambiguity, which is the major chal-
lenge in QALD. Using the question mentioned
above as an example, we can choose Califor-
nia or California, USA when detecting phrases,
the phrase California can be mapped to the en-
tity California State or California Film, and the class
Software (mapped from the phrase software) can
be matched with the first argument of the rela-
tion producer or developer (these two relations can
be mapped from the phrase developed). Previ-
ous methods (Lopez et al., 2006; Lehmann et
al., 2012; Freitas and Curry, 2014) have usu-
ally performed disambiguation at each step only,
and the subsequent step was performed based on
the disambiguation results in the previous step(s).
However, we argue that the three steps men-
tioned above have mutual effects. In the previ-
ous example, the phrase founded in (verb) can
be mapped to the entities (Founding of Rome and
Founder (company)), classes (Company and Depart-
ment) or relations (foundedBy and foundationPlace).
If we know that the phrase California can refer
to the entity California State, and which can be the
second argument of the relation foundationPlace,
together with a verb phrase being more likely
to be mapped to Relation, we should map the
phrase founded in to foundationPlace in this ques-
tion. Thus, we aim to determine if joint disam-
biguation is better than individual disambigua-
tion. (Question One)
In addition, previous systems usually employed
manually designed patterns to extract predicate-
argument structures that are used to guide the dis-
ambiguation process in the three steps mentioned
above (Yahya et al., 2012; Unger et al., 2012; Zou
et al., 2014). For example, (Yahya et al., 2012)
used only three dependency patterns to group the
mapped semantic items into semantic triples. Nev-
ertheless, these three manually designed patterns
miss many cases because of the diversity of the
question expressions. We gathered statistics on
144 questions and found that the macro-average
F1 and micro-average F1 of the three patterns2
used in (Yahya et al., 2012) are only 62.8 and
66.2%, respectively. Furthermore, these specially
designed patterns may not be valid with variations
in domains or languages. Therefore, another im-
portant question arises: can we automatically
learn rules or patterns to achieve the same ob-
</bodyText>
<footnote confidence="0.862047666666667">
2They are 1) verbs and their arguments, 2) adjectives and
their arguments and 3) propositionally modified tokens and
objects of prepositions.
</footnote>
<bodyText confidence="0.994972545454545">
jective? (Question Two)
Focusing on the two problems mentioned
above, this paper proposes a novel algorithm based
on a learning framework, Markov Logic Networks
(MLNs) (Richardson and Domingos, 2006), to
learn a joint model for constructing structured
queries from natural language utterances. MLN
is a statistical relational learning framework that
combines first-order logic and Markov networks.
The appealing property of MLN is that it is read-
ily interpretable by humans and that it is a natural
framework for performing joint learning. We for-
mulate the knowledge for resolving the ambigui-
ties in the main three steps of QALD (phrase de-
tection, phrase-to-semantic-item mapping and se-
mantic item grouping) as first-order logic clauses
in an MLN. In the framework of MLN, all clauses
will produce interacted effects that jointly resolve
all problems into a unified process. In this way,
the result in each step can be globally optimized.
Moreover, in contrast to previous methods, we
adopt a learning strategy to automatically learn
the patterns for semantic item grouping. We de-
sign several meta patterns as opposed to the spe-
cific patterns. In addition, these meta patterns are
formulated as the first-order logic formulas in the
MLN. The specific patterns can be generated by
these meta patterns based on the training data. The
model will learn the weights of each clause to de-
termine the most effective patterns for semantic
triple construction. In this way, with little effort,
our approach can cover more semantic expressions
and answer more questions than previous meth-
ods, which depend on manually designed patterns.
We evaluate the proposed method using several
benchmarks (QALD-1, QALD-3, QALD-4). The
experimental results demonstrate the advantage of
the joint disambiguation process mentioned above.
They also prove that our approach, employing
MLN to automatically learn the patterns of seman-
tic triple grouping, is effective. Our system can
answer more questions and obtain better perfor-
mance than the traditional methods based on man-
ually designed heuristic rules.
</bodyText>
<sectionHeader confidence="0.99614" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.998136">
2.1 Linked Data Sources
</subsectionHeader>
<footnote confidence="0.6171465">
Linked Data consist of many relational data,
which are usually inter-linked as subject-property-
object (SPO) triple statements (such as using the
owl:sameAs relation). In this paper, we mainly use
</footnote>
<page confidence="0.923388">
1093
</page>
<table confidence="0.995776666666667">
Subject(Arg1) Relation(Property) Object(Arg2)
ProgrammingLanguage subClassOf Software
Java_(programming_language) type Software
Java_(programming_language) developer Oracle_Corporation
Oracle_Corporation foundationPlace California_(State)
foundationPlace domain Organisation
California_(State) label “California”
California_(1977_film) label “California”
Oracle_Corporation numEmployees 118119(xsd:integer)
</table>
<figureCaption confidence="0.99634">
Figure 1: Sample knowledge base facts.
</figureCaption>
<bodyText confidence="0.862368533333333">
DBpedia3 and some classes from Yago4. These
knowledge bases (KBs) are composed of many on-
tological and instance statements, and all state-
ments are expressed by SPO triple facts. Figure
1 shows some triple fact samples from DBpedia.
Each fact is composed of three semantic items. A
semantic item can be an entity (California (State),
Oracle Corporation, etc.), a class (Software, Organ-
isation, etc.) or a relation (called a property
or predicate in some occasions). Some entities
are literals including strings, numbers and dates
(118119(xsd:integer), etc.). Relations contain stan-
dard Semantic Web relations (subClassOf, type, do-
main and label) and ontological relations (developer,
foundationPlace and numEmployees).
</bodyText>
<subsectionHeader confidence="0.999508">
2.2 Task Statement
</subsectionHeader>
<bodyText confidence="0.999942214285714">
Given a knowledge base (KB), our objective is to
translate a natural language question qNL into a
formal language query qFL that targets the seman-
tic vocabularies given by the KB, and the query
qFL should capture the user information needs ex-
pressed by qNL.
Following (Yahya et al., 2012), we focus on the
factoid questions, and the answers to such ques-
tions are an entity or a set of entities. We ignore
the questions that need the aggregation5 (max/min,
etc.) and negation operations. That is, we generate
queries that consist of a plentiful number of triple
patterns, which are multiple conjunctions of SPO
search conditions.
</bodyText>
<sectionHeader confidence="0.997398" genericHeader="method">
3 Framework
</sectionHeader>
<bodyText confidence="0.9995692">
Figure 2 shows the entire framework of our system
for translating a question into a formal SPARQL
query. The first three steps address the input ques-
tion through 1) Phrase Detection (detecting pos-
sible phrases), 2) Phrase Mapping (mapping all
</bodyText>
<footnote confidence="0.9980455">
3http://dbpedia.org/
4http://www.mpi-inf.mpg.de/yago-naga/yago/
5We can address the count query questions, which will
be explained in Section 3.
</footnote>
<bodyText confidence="0.999885">
phrase candidates to the corresponding seman-
tic items), and 3) Feature Extraction (extracting
the linguistic features and semantic item features
from the question and the Linked Data, respec-
tively). As a result, a space of candidates is con-
structed, including possible phrases, mapped se-
mantic items and the possible argument match re-
lations among them. Next, the fourth step (In-
ference) formulates the joint disambiguation as a
generalized inference task. We employ rich fea-
tures and constraints (including hard and soft con-
straints) to infer a joint decision through an MLN.
Finally, with the inference results, we can con-
struct a semantic item query graph and generate
an executable SPARQL query. In the following
subsection, we demonstrate each step in detail.
</bodyText>
<listItem confidence="0.68783355">
1) Phrase detection. In this step, we detect
phrases (sequences of tokens) that probably indi-
cate semantic items in the KB. We do not use a
named entity recognizer (NER) because of its low
coverage. We perform testing on two commonly
used question corpora, QALD-3 and free9176, us-
ing the Stanford NER tool7. The results demon-
strate that only 51.5 and 23.8% of the NEs are
correctly recognized, respectively. To avoid miss-
ing useful phrases, we retain all n-grams as phrase
candidates, and then use some rules to filter them.
The rules include the following: the span length
must be less than 4 (accepting that all contiguous
tokens are capitalizations), the POS tag of the start
token must be jj, nn, rb and vb, all contiguous
capitalization tokens must not be split, etc. For
instance, software, developed by, organizations,
founded in and California are detected in the ex-
ample of the first section.
2) Phrase mapping. After the phrases are de-
</listItem>
<bodyText confidence="0.969283923076923">
tected, each phrase can be mapped to the corre-
sponding semantic item in KB (entity, class and
relation). For example, software is mapped to
dbo:Software, dbo:developer, etc., and California is
mapped to dbr:California, dbr:California (wine), etc.
For different types of semantic items, we use dif-
ferent techniques. For mapping phrases to en-
tities, considering that the entities in DBpedia
and Wikipedia are consistent, we employ anchor,
redirection and disambiguation information from
Wikipedia. For mapping phrases to classes, con-
sidering that classes have lexical variation, espe-
cially synonyms, e.g., dbo:Film can be mapped
</bodyText>
<footnote confidence="0.997048333333333">
6http://www.cis.temple.edu/∼yates/open-sem-
parsing/index.html
7http://nlp.stanford.edu/software/CRF-NER.shtml
</footnote>
<page confidence="0.993173">
1094
</page>
<figureCaption confidence="0.999681">
Figure 2: Framework of our system.
</figureCaption>
<bodyText confidence="0.99762788">
from film, movie and show, we compute the simi-
larity between the phrase and the class in the KB
with the word2vec tool8. The word2vec tool com-
putes fixed-length vector representations of words
with a recurrent-neural-network based language
model (Mikolov et al., 2010). The similarity scor-
ing methods are introduced in Section 4.2. Then,
the top-N most similar classes for each phrase are
returned. For mapping phrases to relations, we
employ the resources from PATTY (Nakashole et
al., 2012) and ReVerb (Fader et al., 2011). Specif-
ically, we first compute the associations between
the ontological relations in DBpedia and the re-
lation patterns in PATTY and ReVerb through in-
stance alignments as in (Berant et al., 2013). Next,
if a detected phrase is matched to some relation
pattern, the corresponding ontological relations in
DBpedia will be returned as a candidate. This step
only generates candidates for every possible map-
ping, and the decision of the best selection will be
performed in the next step.
3) Feature extraction and joint inference.
There exist ambiguities in phrase detection and in
mapping phrases to semantic items. This step fo-
cuses on addressing these ambiguities and deter-
</bodyText>
<footnote confidence="0.739078">
8https://code.google.com/p/word2vec/
</footnote>
<bodyText confidence="0.91857364">
mining the argument match relations among the
mapped semantic items. This is the core compo-
nent of our system, and it performs disambigua-
tion in a unified manner. First, feature extraction
is performed to prepare a rich number of features
from the input question and from the KB. Next,
the disambiguation is performed in a joint fashion
with a Markov Logic Network. Detailed informa-
tion will be presented in Section 4.
4) Semantic item query graph construction.
Based on the inference results, we construct a
query graph. The vertices contain the following:
the detected phrase, the token span indexes of
the phrases, the mapped semantic items and their
types. The edge indicates the argument match re-
lation between two semantic items. For example,
we use 1 2 to indicate that the first argument of
an item matches the second argument of another
item9. The right bottom in Figure 2 shows an ex-
ample of this.
5) Query generation. The SPARQL queries
require the grouped triples of semantic items.
Thus, in this step, we convert a query graph
into multiple joined semantic triples. Three in-
terconnected semantic items, whereby it must
</bodyText>
<footnote confidence="0.949918">
9The other marks will be introduced in Section 4.2.
</footnote>
<page confidence="0.992929">
1095
</page>
<bodyText confidence="0.94766775">
be ensured that the middle item is a rela-
tion, are converted into a semantic triple (mul-
tiple joined facts containing variables). For
example, the query graph Tdbo:Book[Class] 1←→2
dbo:author[Relation] 1 1 dbr:Danielle Steel[Entity]T is
←→
converted into (?x rdf:type dbo:Book, dbr:Danielle
dbo:author ?x), and Tdbo:populationTotal[Relation]
</bodyText>
<equation confidence="0.7043105">
1 2 ←→dbr:Australia[Entity]�10 is
←→ dbo:capital[Relation] 1 1
</equation>
<bodyText confidence="0.986713318181818">
converted into (?x1 dbo:populationTotal ?answer, ?x1
dbo:capital dbr:Australia). If the query graph only
contains one vertex that indicates a class ClassURI,
we generate (?x rdf:type ClassURI). If the query
graph only contains two connected vertexes, we
append a variable to bind the missing match argu-
ment of the semantic item.
The final SPARQL query is constructed by join-
ing the semantic item triples based on the cor-
responding SPARQL template. We divide the
questions into three types: Yes/No, Normal and
Number. Yes/No questions use the ASK WHERE
template. Normal questions use the SELECT ?url
WHERE template. Number questions first use the
normal question template, and if they cannot ob-
tain a correct answer (a valid numeric value), we
use the SELECT COUNT(?url) WHERE template to
generate a query again. For instance, we construct
the SPARQL query SELECT(?url) WHERE{ ?url
rdf:type dbo:Software. ?url dbo:developer ?x1. ?x1 rdf:type
dbo:Company. ?x1 dbo:foundationPlace dbr:California.}
for this example.
</bodyText>
<sectionHeader confidence="0.975609" genericHeader="method">
4 Joint Disambiguation with MLN
</sectionHeader>
<bodyText confidence="0.999914166666667">
In this section, we present our method for ques-
tion answering over linked data using a Markov
Logic Network (MLN). In the following subsec-
tions, we first briefly describe the MLN. Then, we
present the predicates and the first-order logic for-
mulas used in the model.
</bodyText>
<subsectionHeader confidence="0.998375">
4.1 Markov Logic Networks
</subsectionHeader>
<bodyText confidence="0.9914763125">
Markov logic networks combine Markov networks
with first-order logic in a probabilistic framework
(Richardson and Domingos, 2006). An MLN M
consists of several weighted formulas {(Oi , wi)}i,
where Oi is a first order formula and wi is the
penalty (the formula’s weight). In contrast to
the first-order logic, whereby a formula repre-
sents a hard constraint, these logic formulas are
relaxed and can be violated with penalties in the
10This corresponds to the question “How many people live
in the capital of Australia?”
MLN. Each formula Oi consists of a set of first-
order predicates, logical connectors and variables.
These weighted formulas define a probability dis-
tribution over a possible world. Let y denote a pos-
sible world. Then p(y) is defined as follows:
</bodyText>
<equation confidence="0.9952585">
p(y) = Z exp wi Xfc i (y)
((φi,wi)∈M c∈Cnφi
</equation>
<bodyText confidence="0.973702">
where each c is a binding of the free variables in
Oi to constants; fφi
c is a binary feature function
that returns 1 if the ground formula that we ob-
tain through replacing the free variables in Oi with
the constants in c under the given possible world
y is true and is 0 otherwise; and Cnφi is the set of
all possible bindings for the free variables in Oi.
Z is a normalized constant. The Markov network
corresponds to this distribution, where nodes rep-
resent ground atoms and factors represent ground
formulas.
</bodyText>
<subsectionHeader confidence="0.936956">
4.2 Predicates
</subsectionHeader>
<bodyText confidence="0.999915294117647">
In the MLN, we design several predicates to re-
solve the ambiguities in phrase detection, map-
ping phrases to semantic items and semantic item
grouping. Specifically, we design a hidden pred-
icate hasPhrase(i) to indicate that the i-th candi-
date phrase has been chosen. The predicate hasRe-
source(i,j) indicates that the i-th phrase is mapped
to the j-th semantic item. The predicate hasRe-
lation(j,k,rr) indicates that the j-th semantic item
and the k-th semantic item should be grouped to-
gether with the argument-match-type rr. Note that
we define four argument match types between two
semantic items: 1 1, 1 2, 2 1 and 2 2. Here, the
argument match type t s denotes that the t-th argu-
ment of the first semantic item corresponds to the
s-th argument of the second semantic item11. The
detailed illustration is shown in Table 1.
</bodyText>
<table confidence="0.9988904">
Type Example Question
1 1 dbo:height 1 1 dbr:Michael Jordan How tall is Michael Jor-
dan?
1 2 dbo:River 1 2 dbo:crosses Which river does the
Brooklyn Bridge cross?
2 1 dbo:creator 2 1 dbr:Walt Disney Which television shows
were created by Walt
Disney?
2 2 dbo:birthPlace 2 2 dbo:capital Which actors were born in
the capital ofAmerican?
</table>
<tableCaption confidence="0.999888">
Table 1: Examples of the argument match types.
</tableCaption>
<bodyText confidence="0.62805625">
11The 2-nd argument is corresponding to the object argu-
ment of the relation, and the 1-st argument is corresponding
with the subject argument of the relation and the entity (in-
cluding the class) itself.
</bodyText>
<page confidence="0.880204">
1096
</page>
<table confidence="0.9725556">
Describing the attributes of phrases and relation between two phrases
phraseIndex(p, i, j) The start and end position of phrase p in question.
phrasePosTag(p, pt) The POS tag of the head word in phrase p.
phraseDepTag(p, q, dt) The dependency path tags between phrase p and q.
phraseDepOne (p, q) If there is only one tag in the dependency path, the predicate is true.
hasMeanWord (p, q) If there is any one meaning word in the dependency path of two phrases, the predicate is true.
Describing the attributes of semantic item and the mappings between phrases and semantic items
resourceType(r, rt) The type of semantic item r. Types of semantic items include Entity, Class and Relation
priorMatchScore(p, r, s) The prior score of phrase p mapping to semantic item r.
Describing the attributes of relation between two semantic items in a knowledge base
</table>
<tableCaption confidence="0.981449">
Table 2: Descriptions of observed predicates.
</tableCaption>
<table confidence="0.3709674">
The semantic coherence of semantic items.
hasRelatedness(p, q, s)
isTypeCompatible(p, q, rr)
If the semantic items p are type-compatible with the semantic items q, the predicate is true.
hasQueryResult(s, p, o, rr1, rr2)
</table>
<bodyText confidence="0.998484137254902">
If the triple pattern consisting of semantic items s, p, o and argument-match-types rr1 and rr2 have query
results, the predicate is true.
Moreover, we define a set of observed predi-
cates to describe the properties of phrases, seman-
tic items, relations between phrases and relations
between semantic items. The observed predicates
and descriptions are shown in Table 2.
Previous methods usually designed some
heuristic patterns to group semantic items, which
usually employed a human-designed syntactic
path between two phrases to determine their re-
lations. In contrast, we collect all the tokens in
the dependency path between two phrases as pos-
sible patterns. The predicates phraseDepTag and
hasMeanWord are designed to indicate the possi-
ble patterns. Note that if these tokens only contain
POS tags dt|in|wdt|to|cc|ex|pos|wp or stop words,
the value of the predicate hasMeanWord is false;
otherwise, it is true. In this way, our system is ex-
pected to cover more question expressions. More-
over, the SPARQL endpoint is used to verify the
type compatibility of two semantic items and if
one triple pattern can obtain query results.
The predicate hasRelatedness needs to compute
the coherence score between two semantic items.
Following (Yahya et al., 2012), we use the Jaccard
coefficient (Jaccard, 1908) based on the inlinks be-
tween two semantic items.
The predicate priorMatchScore assigns a prior
score when mapping a phrase to a semantic item.
We use different methods to compute this score
according to different semantic item types. For
entities, we use a normalized score based on the
frequencies of a phrase referring to an entity.
For classes and relations, we use different meth-
ods. We first define the following three similar-
ity metrics: a) s1: The Levenshtein distance score
(Navarro, 2001) between the labels of the seman-
tic item and the phrase; b) s2: The word embed-
ding (Mikolov et al., 2010) score, which measures
the similarity between two phrases and is the max-
imum cosine value of the words’ word embed-
dings between two phrases; and c) s3: the instance
overlap score, which is computed using the Jac-
card coefficient of the instance overlap. All scores
are normalized to produce a comparable scores
in the interval of (0, 1). The final prior scores
for mapping phrases to classes and relations are
rys1 + (1 − ry)s2 and αs1 + Qs2 + (1 − α − Q)s3,
respectively. The parameters are set to empirical
values12.
</bodyText>
<subsectionHeader confidence="0.977609">
4.3 Formulas
</subsectionHeader>
<bodyText confidence="0.99987325">
According to these predicates, we design several
first-order logic formulas for joint disambiguation.
As mentioned in the first section, these formulas
represent the meta patterns. The concrete pat-
terns can be generated through these meta pat-
terns with training data. Specifically, we use two
types of formulas for the joint decisions: Boolean
and Weighted formulas. Boolean formulas are
hard constraints, which must be satisfied by all
of the ground atoms in the final inference results.
Weighted formulas are soft constraints, which can
be violated with some penalties.
</bodyText>
<subsectionHeader confidence="0.820159">
4.3.1 Boolean Formulas (Hard Constraints)
</subsectionHeader>
<bodyText confidence="0.998939666666667">
Table 3 lists the Boolean formulas used in this
work. The “ ” notation in the formulas indicates
an arbitrary constant. The “|f|” notation expresses
the number of true grounded atoms in the formula
f. These formulas express the following con-
straints:
</bodyText>
<listItem confidence="0.846291857142857">
hf1: If a phrase is chosen, then it must have a
mapped semantic item;
hf2: If a semantic item is chosen, then its mapped
phrase must be chosen;
hf3: A phrase can be mapped to at most one se-
mantic item;
hf4: If the phrase is not chosen, then its mapped
</listItem>
<footnote confidence="0.6104065">
12Set γ to 0.6 for Class and set α and β to 0.3 and 0.3 for
Relation, respectively.
</footnote>
<page confidence="0.923314">
1097
</page>
<table confidence="0.996974769230769">
hf1 hasPhrase(p) ⇒ hasResource(p, )
hf2 hasResource(p, ) ⇒ hasPhrase(p)
hf3 |hasResource(p, ) |≤ 1
hf4 !hasPhrase(p) ⇒!hasResource(p, r)
hf5 hasResource( , r) ⇒ hasRelation(r, , ) ∨ hasRelation( , r, )
hf6 |hasRelation(r1, r2, ) |≤ 1
hf7 hasRelation(r1, r2, ) ⇒ hasResource(, r1) ∧ hasResource( , r2)
hf8 phraseIndex(p1, s1, e1) ∧ phraseIndex(p2, s2, e2) ∧ overlap(s1, e1, s2, e2) ∧ hasPhrase(p1) ⇒!hasPhrase(p2)
hf9 resourceType(r, “Entity”) ⇒!hasRelation(r, , “2 1”) ∧ !hasRelation(r, , “2 2”)
hf10 resourceType(r, “Entity”) ⇒!hasRelation( , r, “2 1”) ∧ !hasRelation(r, , “2 2”)
hf11 resourceType(r, “Class”) ⇒!hasRelation(r, , “2 1”) ∧ !hasRelation(r, , “2 2”)
hf12 resourceType(r, “Class”) ⇒!hasRelation( , r, “2 1”) ∧ !hasRelation(r, , “2 2”)
hf13 !isTypeCompatible(r1, r2, rr) ⇒!hasRelation(r1, r2, rr)
</table>
<tableCaption confidence="0.993816">
Table 3: Descriptions of Boolean formulas.
</tableCaption>
<table confidence="0.9987928">
sf1 priorMatchScore(p, r, s) ⇒ hasPhrase(p)
sf2 priorMatchScore(p, r, s) ⇒ hasResource(p)
sf3 phrasePosTag(p, pt+) ∧ resourceType(r, rt+) ⇒ hasResource(p, r)
sf4 phraseDepTag(p1, p2, dp+) ∧ hasResource(p1, r1) ∧ hasResource(p2, r2) ⇒ hasRelation(r1, r2, rr+)
sf5 phraseDepTag(p1, p2, dp+) ∧ hasResource(p1, r1) ∧ hasResource(p2, r2)∧!hasMeanWord(p1, p2) ⇒
hasRelation(r1, r2, rr+)
sf6 phraseDepTag(p1, p2, dp+) ∧ hasResource(p1, r1) ∧ hasResource(p2, r2) ∧ phraseDepOne(p1, p2) ⇒
hasRelation(r1, r2, rr+)
sf7 hasRelatedness(r1, r2, s) ∧ hasResource( , r1) ∧ hasResource( , r2) ⇒ hasRelation(r1, r2, )
sf8 hasQueryResult(r1, r2, r3, rr1, rr2) ⇒ hasRelation(r1, r2, rr1) ∧ hasRelation(r2, r3, rr2)
</table>
<tableCaption confidence="0.999418">
Table 4: Descriptions of weighted formulas.
</tableCaption>
<bodyText confidence="0.8272555">
semantic item should not be chosen;
hf5: If a semantic item is chosen, then it should
have at least one argument match relation with
other semantic items;
hf6: Two semantic items have at most one argu-
ment match relation;
hf7: If an argument match relation for two seman-
tic items is chosen, then they must be chosen;
hf8: Each of two chosen phrases must not overlap;
hf9, hf10, hf11, hf12: The semantic item with
type Entity and Class should not have a second ar-
gument that matches with others;
hf13: The chosen argument match relation for two
sematic items must be type compatible.
</bodyText>
<subsectionHeader confidence="0.81475">
4.3.2 Weighted Formulas (Soft Constraints)
</subsectionHeader>
<bodyText confidence="0.996634909090909">
Table 4 lists the weighted formulas used in this
work. The “+” notation in the formulas indicates
that each constant of the logic variable should be
weighted separately. Those formulas express the
following properties in joint decisions:
sf1, sf2: The larger the score of the phrase map-
ping to a semantic item, the more likely the cor-
responding phrase and semantic item should been
chosen;
sf3: There are some associations between the POS
tags of phase and the types of mapped semantic
items;
sf4, sf5, sf6: There are some associations be-
tween the dependency tags in the dependency pat-
tern path of two phases and the types of argument
match relations of two mapped semantic items;
sh7: The larger the relatedness of two seman-
tic items, the more likely they have an argument
match relation;
sf8: If the triple pattern has query results, these se-
mantic items should have corresponding argument
match relations.
</bodyText>
<sectionHeader confidence="0.999795" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996181">
5.1 Dataset &amp; Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999988315789474">
We use the following three collections of questions
from the QALD13 task for question answering
over linked data: QALD-1, QALD-3 and QALD-
4. The generated SPARQL queries are evaluated
on Linked Data from DBpedia and YAGO using
a Virtuoso engine14. A typical example question
from the QALD benchmark is “Which books writ-
ten by Kerouac were published by Viking Press?”.
As mentioned in Section 2.2, our system is not de-
signed to answer questions that contain numbers,
date comparisons and aggregation operations such
as group by or order by. Therefore, we remove
these types of questions and retain 110 questions
from the QALD-4 training set for generating the
specific formulas and for training their weights in
MLN. We test our system using 37, 75 and 26
questions from the training set of QALD-115, and
the testing set of QALD-3 and QALD-4 respec-
tively. We use #T, #Q and #A to indicate the total
</bodyText>
<footnote confidence="0.9953755">
13www.sc.cit-ec.uni-bielefeld.de/qald/
14https://github.com/openlink/virtuoso-opensource
15We use the training set because we try to make a fair
comparison with (Yahya et al., 2012).
</footnote>
<page confidence="0.997334">
1098
</page>
<bodyText confidence="0.972554">
number of questions in the testing set, the num-
ber of questions we could address and the number
of questions answered correct, respectively. We
select Precision (P = #A
</bodyText>
<equation confidence="0.597819">
#Q), Recall (R = #A
#T ),
</equation>
<bodyText confidence="0.99974">
and F1-score (F1 = 2·P·R P +R) as the evaluation met-
rics. To assess the effectiveness of the disambigua-
tion process in the MLN, we computed the overall
quality measures by precision and recall with the
manually obtained results.
</bodyText>
<subsectionHeader confidence="0.989091">
5.2 Experimental Configurations
</subsectionHeader>
<bodyText confidence="0.999977083333333">
The Stanford dependency parser (De Marneffe et
al., 2006) is used for extracting features from the
dependency parse trees. We use the toolkit the-
beast16 to learn the weights of the formulas and
to perform the MAP inference. The inference al-
gorithm uses a cutting plane approach. In addi-
tion, for the parameter learning, we set all ini-
tial weights to zero and use an online learning
algorithm with MIRA update rules to update the
weights of the formulas. The number of iterations
for the training and testing are set to 10 and 200,
respectively.
</bodyText>
<subsectionHeader confidence="0.643135">
5.3 Results and Discussion
5.3.1 The Effect of Joint Learning
</subsectionHeader>
<bodyText confidence="0.999980086956522">
To demonstrate the advantages of our joint learn-
ing, we design a pipeline system for compari-
son, which independently performs phrase detec-
tion, phrase mapping, and semantic item grouping
by removing the unrelated formulas in MLN. For
example, the formulas17 related to the predicates
hasResource and hasRelation are removed when
detecting phrases in questions.
Table 5 shows the results, where Joint de-
notes the proposed method with joint inference
and Pipeline denotes the compared method per-
forming each step independently. We perform a
comparison with the question answering results of
QALD (QA), and comparisons at each of the fol-
lowing steps: PD (phrase detection), PM (phrase
mapping) and MG (mapped semantic items group-
ing). From the results, we observe that our method
answers over half of the questions. Moreover, our
joint model based on MLN can obtain better per-
formance in question answering compared to the
pipeline system. We also observe that Joint ex-
hibits better performance than Pipeline in most
steps, except for MG in QALD-3. We believe this
</bodyText>
<footnote confidence="0.9442425">
16http://code.google.com/p/thebeast
17including entire formulas, excluding hf8 and sf1
</footnote>
<bodyText confidence="0.999900666666667">
is because the three tasks (phrase detection, phrase
mapping, and semantic item grouping) are con-
nected with each other. Each step can provide use-
ful information for the other two tasks. Therefore,
performing joint inference can effectively improve
the performance. Finally, we observe that the for-
mer task usually produces better results than the
subsequent tasks (phrase detection exhibits a bet-
ter performance than phrase mapping, and phrase
mapping exhibits a better performance than se-
mantic item grouping). The main reason is that
the latter subtask is more complex than the former
task. The decisions of the latter subtask strongly
rely on the former results even though they have
interacted effects.
</bodyText>
<subsubsectionHeader confidence="0.577388">
5.3.2 The Effect of Pattern Learning
</subsubsectionHeader>
<bodyText confidence="0.999692">
Table 6 shows a comparison of our system with
DEANNA (Yahya et al., 2012), which is based
on a joint disambiguation model but which em-
ploys hand-written patterns in its system. Because
DEANNA only reports its results of the QALD-1
dataset, we do not show the results for QALD-3
and QALD-4 for equity. From the results, we can
see that our system solved more questions and ex-
hibited a better performance than did DEANNA.
One of the greatest strengths of our system is that
the learning system can address more questions
than hand-written pattern rules.
</bodyText>
<table confidence="0.968385">
System #T #Q #A P R F1
DEANNA (Yahya et al., 2012) 50 27 13 0.48 0.26 0.33
Ours 50 37 20 0.54 0.4 0.46
</table>
<tableCaption confidence="0.9552895">
Table 6: Comparisons with DEANNA using the
QALD-1 test questions.
</tableCaption>
<bodyText confidence="0.999066352941177">
Compared to the ILP (Integer Linear Program-
ming) used in (Yahya et al., 2012) for joint disam-
biguation, we argue that there are two major dif-
ferences to our method. 1) Our method is a data-
driven approach that can learn effective patterns
or rules for the task. Therefore, it exhibits more
robustness and adaptability for various KBs. 2)
We design several meta rules in MLN as opposed
to specific ones. The specific rules can be gen-
erated by these meta rules based on the training
data. By contrast, the traditional approach using
ILP needs to set specific rules in advance, which
requires more intensive labor than our approach.
To further illustrate the effectiveness of our
pattern-learning strategy, we show the weights of
the learned patterns corresponding to formula sf3
in the MLN, as shown in Table 7. From the table,
</bodyText>
<page confidence="0.985151">
1099
</page>
<table confidence="0.9997395">
Benchmark PD PM MG QA
P R F1 P R F1 P R F1 #T #Q #A P R F1
QALD-1(Joint) 0.93 0.981 0.955 0.895 0.944 0.919 0.703 0.813 0.754 50 37 20 0.54 0.4 0.46
QALD-1(Pipeine) 0.921 0.972 0.946 0.868 0.917 0.892 0.585 0.859 0.696 50 34 17 0.5 0.34 0.41
QALD-3(Joint) 0.941 0.941 0.941 0.878 0.918 0.898 0.636 0.798 0.708 99 75 45 0.6 0.46 0.52
QALD-3(Pipeline) 0.912 0.912 0.912 0.829 0.867 0.848 0.677 0.789 0.729 99 75 42 0.56 0.42 0.48
QALD-4(Joint) 0.947 0.978 0.963 0.937 0.967 0.952 0.776 0.865 0.817 50 26 15 0.58 0.3 0.4
QALD-4(Pipeline) 0.937 0.967 0.952 0.905 0.935 0.920 0.683 0.827 0.748 50 24 13 0.54 0.26 0.35
</table>
<tableCaption confidence="0.927019166666667">
Table 5: The performance of joint learning on three benchmark datasets.
we can see that nn18 is more likely mapped to En-
tity19 than to Class and Relation, and vb is most
likely mapped to Relation. This proves that our
model can learn effective and reasonable patterns
for QALD.
</tableCaption>
<table confidence="0.997701142857143">
POS tag of Phrase type of mapped Item Weight
nn Entity 2.11
nn Class 0.243
nn Relation 0.335
vb Relation 0.517
wp Class 0.143
wr Class 0.025
</table>
<tableCaption confidence="0.991777">
Table 7: Sample weights of formulas, correspond-
ing with formula sf3.
</tableCaption>
<subsubsectionHeader confidence="0.748104">
5.3.3 Comparison to the state of the art
</subsubsectionHeader>
<bodyText confidence="0.999757777777778">
To illustrate the effectiveness of the proposed
method, we perform comparisons to the state-of-
the-art methods. Table 8 shows the results using
QALD-3 and QALD-4. These systems are the
participants in the QALD evaluation campaigns.
From the results, we can see that our system out-
performs most systems at a competitive perfor-
mance. They further prove the effectiveness of the
proposed method.
</bodyText>
<table confidence="0.998445588235294">
Test set System #T #Q #A P R F1
QALD-3 CASIA (He et al., 99 52 29 0.56 0.3 0.38
2013)
Scalewelis (Joris 99 70 32 0.46 0.32 0.38
and Ferr´e, 2013)
RTV (Cristina et 99 55 30 0.55 0.3 0.39
al., 2013)
Intui2 (Corina, 99 99 28 0.28 28 0.28
2013)
SWIP (Pradel et al., 99 21 15 0.71 0.15 0.25
2013)
Ours 99 75 45 0.6 0.46 0.52
QALD-420 gAnswer 50 25 16 0.64 0.32 0.43
Intui3 50 33 10 0.30 0.2 0.24
ISOFT 50 50 10 0.2 0.2 0.2
RO FII 50 50 6 0.12 0.12 0.12
Ours 50 26 15 0.58 0.3 0.4
</table>
<tableCaption confidence="0.958139">
Table 8: Comparisons with state-of-the-art sys-
tems using the QALD benchmark.
</tableCaption>
<footnote confidence="0.9979455">
18The POS tag of the head word in the phrase
19The type of semantic item
20Because the QALD-4 conference does not start un-
til after submission, we have no citation for the state-of-
</footnote>
<subsubsectionHeader confidence="0.447983">
5.3.4 The Effect of Different Formulas
</subsubsectionHeader>
<bodyText confidence="0.99998575">
To determine which formulas are more useful for
QALD, we evaluate the performance of the pro-
posed method with different predicate sets. We
subtract one weighted formula from the original
sets at a time, except retaining the first two for-
mulas sf1 and sf2 for basic inference. Because of
space limitations, only the results using QALD-3
testing set are shown in Table 9.
From the results, we can observe that remov-
ing some formulas can boost the performance on
some single tasks, but employing all formulas can
produce the best performance. This illustrates that
solely resolving the steps in QALD (phrase detec-
tion, phrase mapping, semantic items grouping)
can obtain local results, and that making joint in-
ference is necessary and useful.
</bodyText>
<sectionHeader confidence="0.999848" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9882475">
Our proposed method is related to two lines of
work: Question Answering over Knowledge bases
and Markov Logic Networks.
Question answering over knowledge bases
has attracted a substantial amount of interest over
a long period of time. The initial attempts in-
cluded BaseBall (Green Jr et al., 1961) and Lu-
nar (Woods, 1977). However, these systems were
mostly limited to closed domains due to a lack of
knowledge resources. With the rapid development
of structured data, such as DBpedia, Freebase and
Yago, the need for providing user-friendly inter-
face to these data has become increasingly urgent.
Keyword (Elbassuoni and Blanco, 2011) and se-
mantic (Pound et al., 2010) searches are limited
to their ability to specify the relations among the
different keywords.
The open topic progress has also been pushed
by the QALD evaluation campaigns (Walter et al.,
2012). Lopez et al. (2011) gave a comprehensive
survey in this research area. The authors devel-
oped the PowerAqua system (Lopez et al., 2006) to
</bodyText>
<footnote confidence="0.6917705">
the-art systems in QALD-4. The results can be found at
http://greententacle.techfak.uni-bielefeld.de/ cunger/qald.
</footnote>
<page confidence="0.934203">
1100
</page>
<table confidence="0.999576888888889">
Formulas PD PM MG Avg
P R F1 P R F1 P R F1 P R F1
All Formulas 0.941 0.941 0.941 0.878 0.918 0.898 0.636 0.798 0.708 0.839 0.901 0.869
-sf3 0.931 0.927 0.929 0.877 0.913 0.895 0.637 0.816 0.715 0.834 0.897 0.864
-sf4 0.926 0.917 0.922 0.852 0.883 0.867 0.63 0.763 0.69 0.824 0.87 0.846
-sf5 0.931 0.927 0.929 0.873 0.908 0.89 0.633 0.816 0.713 0.831 0.895 0.862
-sf6 0.922 0.922 0.922 0.844 0.883 0.863 0.702 0.746 0.723 0.842 0.868 0.855
-sf7 0.931 0.917 0.924 0.881 0.908 0.894 0.621 0.763 0.685 0.833 0.88 0.856
-sf8 0.927 0.927 0.927 0.868 0.908 0.888 0.639 0.807 0.713 0.83 0.893 0.861
</table>
<tableCaption confidence="0.9129875">
Table 9: Performance comparisons of different weighted formulas evaluated using the QALD-3 question
set.
</tableCaption>
<bodyText confidence="0.999532041666667">
answer questions on large, heterogeneous datasets.
For questions containing quantifiers, comparatives
or superlatives, Unger et al. (2012) translated
NL to FL using several SPARQL templates and
using a set of heuristic rules mapping phrases
to semantic items. The system most similar to
ours is DEANNA (Yahya et al., 2012). However,
DEANNA extracts predicate-argument structures
from the questions using three hand-written pat-
terns. Our system jointly learns these mappings
and extractions completely from scratch.
Recently, the Semantic Parsing (SP) community
targeted this problem from limited domains (Tang
and Mooney, 2001; Liang et al., 2013) to open do-
mains (Cai and Yates, 2013; Berant et al., 2013).
The methods in semantic parsing answer questions
by first converting natural language utterances into
meaningful representations (e.g., the lambda cal-
culus) and subsequently executing the formal log-
ical forms over KBs. Compared to deriving the
complete logical representation, our method aims
to parse a question into a limited logic form with
the semantic item query, which we believe is more
appropriate for answering factoid questions.
Markov Logic Networks have been widely
used in NLP tasks. Huang (2012) applied MLN
to compress sentences by formulating the task as a
word/phrase deletion problem. Fahrni and Strube
(2012) jointly disambiguated and clustered con-
cepts using MLN. MLN has also been used in
coreference resolution (Song et al., 2012). For
the task of identifying subjective text segments
and of extracting their corresponding explanations
from product reviews, Zhang et al. (2013) mod-
eled these segments with MLN. To discover log-
ical knowledge for deep question answering, Liu
(2012) used MLN to resolve the inconsistencies
of multiple knowledge bases.
Meza-Ruiz and Riedel (2009) employed MLN
for Semantic Role Labeling (SRL). They jointly
performed the following tasks for a sentence:
predicate identification, frame disambiguation, ar-
gument identification and argument classification.
The semantic analysis of SRL solely rested on
the lexical level, but our analysis focuses on the
knowledge-base level and aims to obtain an exe-
cutable query and to support natural language in-
ference.
</bodyText>
<sectionHeader confidence="0.998051" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999985714285714">
For the task of QALD, we present a joint learn-
ing framework for phrase detection, phrase map-
ping and semantic item grouping. The novelty of
our method lies in the fact that we perform joint
inference and pattern learning for all subtasks in
QALD using first-order logic. Our experimental
results demonstrate the effectiveness of the pro-
posed method.
In the future, we plan to address the follow-
ing limitations that still exist in the current sys-
tem: a) numerous hand-labeled data are required
for training the MLN, and we could use a la-
tent form of semantic item query graphs (Liang et
al., 2013); b) more robust solutions can be devel-
oped to find the implicit relations in questions; c)
our system can be scaled up to large-scale open-
domain knowledge bases (Fader et al., 2013; Yao
and Van Durme, 2014); and d) the learning system
has the advantage of being easily adapted to new
settings, and we plan to extend it to other domains
and languages (Liang and Potts, 2014).
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999677375">
The authors are grateful to the anonymous re-
viewers for their constructive comments. This
work was sponsored by the National Basic Re-
search Program of China (No. 2014CB340503)
and the National Natural Science Foundation of
China (No. 61202329, 61272332), CCF-Tencent
Open Fund. This work was also supported in part
by Noahs Ark Lab of Huawei Tech. Ltm.
</bodyText>
<page confidence="0.992314">
1101
</page>
<sectionHeader confidence="0.989773" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999868037735849">
S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open data.
In The semantic web, pages 722–735. Springer.
Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.
2014. Knowledge-based question answering as ma-
chine translation. In ACL.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In EMNLP.
Christian Bizer, Tom Heath, and Tim Berners-Lee.
2009. Linked data-the story so far. International
journal on semantic web and information systems,
5(3):1–22.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In SIGMOD.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In ACL.
Unger Christina and Andr Freitas. 2014. Question an-
swering over linked data: Challenges, approaches,
trends. In ESWC.
Dima Corina. 2013. Intui2: A prototype system
for question answering over linked data. In Work.
Multilingual Question Answering over Linked Data
(QALD-3).
Giannone Cristina, Bellomaria Valentina, and Basili
Roberto. 2013. A hmm-based approach to question
answering against linked data. In Work. Multilin-
gual Question Answering over Linked Data (QALD-
3).
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In LREC.
Shady Elbassuoni and Roi Blanco. 2011. Keyword
search over rdf graphs. In CIKM.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In EMNLP.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In ACL.
Angela Fahrni and Michael Strube. 2012. Jointly
disambiguating and clustering concepts and entities
with markov logic. In COLING.
Andre Freitas and Edward Curry. 2014. Natural
language queries over heterogeneous linked data
graphs: A distributional-compositional semantics
approach. In IUI.
Bert F Green Jr, Alice K Wolf, Carol Chomsky, and
Kenneth Laughery. 1961. Baseball: an automatic
question-answerer. In Papers presented at the May
9-11, 1961, western joint IRE-AIEE-ACM computer
conference, pages 219–224. ACM.
Shizhu He, Shulin Liu, Yubo Chen, Guangyou Zhou,
Kang Liu, and Jun Zhao. 2013. Casia@qald-3:
A question answering system over linked data. In
Work. Multilingual Question Answering over Linked
Data (QALD-3).
Minlie Huang, Xing Shi, Feng Jin, and Xiaoyan Zhu.
2012. Using first-order logic to compress sentences.
In AAAI.
Paul. Jaccard. 1908. Nouvelles recherches sur la dis-
tribution florale. Bulletin de la Soci`ete Vaudense des
Sciences Naturelles, 44:223–270.
Guyonvarc’H Joris and S´ebastien Ferr´e. 2013.
Scalewelis: a scalable query-based faceted search
system on top of sparql endpoints. In Work.
Multilingual Question Answering over Linked Data
(QALD-3).
Jens Lehmann, Tim Furche, Giovanni Grasso, Axel-
Cyrille Ngonga Ngomo, Christian Schallhart, An-
drew Sellers, Christina Unger, Lorenz B¨uhmann,
Daniel Gerber, Konrad H¨offner, et al. 2012. Deqa:
deep web extraction for question answering. In
ISWC.
Percy Liang and Christopher Potts. 2014. Bringing
machine learning and compositional semantics to-
gether. Annual Reviews of Linguistics (to appear).
Percy Liang, Michael I Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2):389–446.
Zhao Liu, Xipeng Qiu, Ling Cao, and Xuanjing Huang.
2012. Discovering logical knowledge for deep ques-
tion answering. In CIKM.
Vanessa Lopez, Enrico Motta, and Victoria Uren.
2006. Poweraqua: Fishing the semantic web. In
The Semantic Web: research and applications, pages
393–410. Springer.
Vanessa Lopez, Victoria Uren, Marta Sabou, and En-
rico Motta. 2011. Is question answering fit for the
semantic web?: a survey. Semantic Web, 2(2):125–
155.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
markov logic. In NAACL.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045–1048.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: a taxonomy of relational
patterns with semantic types. In EMNLP.
</reference>
<page confidence="0.943023">
1102
</page>
<reference confidence="0.99958174137931">
Gonzalo Navarro. 2001. A guided tour to approximate
string matching. ACM Comput. Surv., 33(1):31–88.
Jeffrey Pound, Ihab F Ilyas, and Grant Weddell. 2010.
Expressive and flexible access to web-extracted
data: a keyword-based structured query language.
In SIGMOD.
C Pradel, G Peyet, O Haemmerl´e, and N Hernandez.
2013. Swip at qald-3: results, criticisms and les-
son learned (working notes). In Work. Multilingual
Question Answering over Linked Data (QALD-3).
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine learning, 62(1-
2):107–136.
Saeedeh Shekarpour, Axel-Cyrille Ngonga Ngomo,
and S¨oren Auer. 2013. Question answering on in-
terlinked data. In WWW.
Yang Song, Jing Jiang, Wayne Xin Zhao, Sujian Li,
and Houfeng Wang. 2012. Joint learning for coref-
erence resolution with markov logic. In EMNLP.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In WWW.
Lappoon R. Tang and Raymond J. Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Proceedings
of the 12th European Conference on Machine Learn-
ing, pages 466–477.
Christina Unger, Lorenz B¨uhmann, Jens Lehmann,
Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and
Philipp Cimiano. 2012. Template-based question
answering over rdf data. In WWW.
Sebastian Walter, Christina Unger, Philipp Cimiano,
and Daniel B¨ar. 2012. Evaluation of a layered
approach to question answering over linked data.
In The Semantic Web–ISWC 2012, pages 362–374.
Springer.
William A Woods. 1977. Lunar rocks in natural en-
glish: Explorations in natural language question an-
swering. In Linguistic structures processing, pages
521–569.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural language questions for the
web of data. In EMNLP.
Mohamed Yahya, Klaus Berberich, Shady Elbassuoni,
and Gerhard Weikum. 2013. Robust question an-
swering over the web of linked data. In CIKM.
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In ACL.
Qi Zhang, Jin Qian, Huan Chen, Jihua Kang, and Xu-
anjing Huang. 2013. Discourse level explanatory
relation extraction from product reviews using first-
order logic. In ACL.
Lei Zou, Ruizhe Huang, Haixun WangZou, Jeffrey Xu
Yu, Wenqiang He, and Dongyan Zhao. 2014. Natu-
ral language question answering over rdf — a graph
data driven approach. In SIGMOD.
</reference>
<page confidence="0.985242">
1103
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.502677">
<title confidence="0.99916">Answering over Linked Data Using First-order</title>
<author confidence="0.992698">Shizhu He</author>
<author confidence="0.992698">Kang Liu</author>
<author confidence="0.992698">Yuanzhe Zhang</author>
<author confidence="0.992698">Liheng Xu</author>
<author confidence="0.992698">Jun</author>
<affiliation confidence="0.998177">National Laboratory of Pattern</affiliation>
<address confidence="0.573814">Institute of Automation, Chinese Academy of Sciences, Beijing, 100190,</address>
<email confidence="0.95735">kliu,yzzhang,lhxu,</email>
<abstract confidence="0.996680689655172">Question Answering over Linked Data (QALD) aims to evaluate a question answering system over structured data, the key objective of which is to translate questions posed using natural language into structured queries. This technique can help common users to directly access open-structured knowledge on the Web and, accordingly, has attracted much attention. To this end, we propose a novel method using first-order logic. We formulate the knowledge for resolving the ambiguities in the main three steps of QALD (phrase detection, phrase-tosemantic-item mapping and semantic item grouping) as first-order logic clauses in a Markov Logic Network. All clauses can then produce interacted effects in a unified framework and can jointly resolve all ambiguities. Moreover, our method adopts a pattern-learning strategy for semantic item grouping. In this way, our method can cover more text expressions and answer more questions than previous methods using manually designed patterns. The experimental results using open benchmarks demonstrate the effectiveness of the proposed method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S¨oren Auer</author>
<author>Christian Bizer</author>
<author>Georgi Kobilarov</author>
<author>Jens Lehmann</author>
<author>Richard Cyganiak</author>
<author>Zachary Ives</author>
</authors>
<title>Dbpedia: A nucleus for a web of open data.</title>
<date>2007</date>
<booktitle>In The semantic web,</booktitle>
<pages>722--735</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1555" citStr="Auer et al., 2007" startWordPosition="231" endWordPosition="234">c Network. All clauses can then produce interacted effects in a unified framework and can jointly resolve all ambiguities. Moreover, our method adopts a pattern-learning strategy for semantic item grouping. In this way, our method can cover more text expressions and answer more questions than previous methods using manually designed patterns. The experimental results using open benchmarks demonstrate the effectiveness of the proposed method. 1 Introduction With the rapid development of the Web of Data, many RDF datasets have been published as Linked Data (Bizer et al., 2009), such as DBpedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and YAGO (Suchanek et al., 2007). The growing amount of Linked Data contains a wealth of knowledge, including entities, classes and relations. Moreover, these linked data usually have ∗Shizhu He and Kang Liu have equal contribution to this work. complex structures and are highly heterogeneous. As a result, there are gaps for users regarding access. Although a few experts can write queries using structured languages (such as SPARQL) based on their needs, this skill cannot be easily utilized by common users (Christina and Freitas, 2014). Thus, providing user-f</context>
</contexts>
<marker>Auer, Bizer, Kobilarov, Lehmann, Cyganiak, Ives, 2007</marker>
<rawString>S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. In The semantic web, pages 722–735. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junwei Bao</author>
<author>Nan Duan</author>
<author>Ming Zhou</author>
<author>Tiejun Zhao</author>
</authors>
<title>Knowledge-based question answering as machine translation.</title>
<date>2014</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2600" citStr="Bao et al., 2014" startWordPosition="398" endWordPosition="401">sing structured languages (such as SPARQL) based on their needs, this skill cannot be easily utilized by common users (Christina and Freitas, 2014). Thus, providing user-friendly, simple interfaces to access these linked data becomes increasingly more urgent. Because of this, question answering over linked data (QALD) (Walter et al., 2012) has recently received much interest, and most studies on this topic have focused on translating natural language questions into structured queries (Freitas and Curry, 2014; Yahya et al., 2012; Unger et al., 2012; Shekarpour et al., 2013; Yahya et al., 2013; Bao et al., 2014; Zou et al., 2014). For example, with respect to the question “Which software has been developed by organizations founded in California, USA?”, the aim is to automatically convert this utterance into an SPARQL query that contains the following subject-property-object (SPO) triple format: ?url rdf:type dbo:Software, ?url dbo:developer ?x1, ?x1 rdf:type dbo:Company, ?x1 dbo:foundationPlace dbr:California)1. To fulfill this objective, existing systems (Lopez et al., 2006; Unger et al., 2012; Yahya et al., 2012; Zou et al., 2014) usually adopt a pipeline framework that contains four major steps: </context>
</contexts>
<marker>Bao, Duan, Zhou, Zhao, 2014</marker>
<rawString>Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao. 2014. Knowledge-based question answering as machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Andrew Chou</author>
<author>Roy Frostig</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing on freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="14029" citStr="Berant et al., 2013" startWordPosition="2145" endWordPosition="2148"> the word2vec tool8. The word2vec tool computes fixed-length vector representations of words with a recurrent-neural-network based language model (Mikolov et al., 2010). The similarity scoring methods are introduced in Section 4.2. Then, the top-N most similar classes for each phrase are returned. For mapping phrases to relations, we employ the resources from PATTY (Nakashole et al., 2012) and ReVerb (Fader et al., 2011). Specifically, we first compute the associations between the ontological relations in DBpedia and the relation patterns in PATTY and ReVerb through instance alignments as in (Berant et al., 2013). Next, if a detected phrase is matched to some relation pattern, the corresponding ontological relations in DBpedia will be returned as a candidate. This step only generates candidates for every possible mapping, and the decision of the best selection will be performed in the next step. 3) Feature extraction and joint inference. There exist ambiguities in phrase detection and in mapping phrases to semantic items. This step focuses on addressing these ambiguities and deter8https://code.google.com/p/word2vec/ mining the argument match relations among the mapped semantic items. This is the core </context>
<context position="39338" citStr="Berant et al., 2013" startWordPosition="6327" endWordPosition="6330">s, comparatives or superlatives, Unger et al. (2012) translated NL to FL using several SPARQL templates and using a set of heuristic rules mapping phrases to semantic items. The system most similar to ours is DEANNA (Yahya et al., 2012). However, DEANNA extracts predicate-argument structures from the questions using three hand-written patterns. Our system jointly learns these mappings and extractions completely from scratch. Recently, the Semantic Parsing (SP) community targeted this problem from limited domains (Tang and Mooney, 2001; Liang et al., 2013) to open domains (Cai and Yates, 2013; Berant et al., 2013). The methods in semantic parsing answer questions by first converting natural language utterances into meaningful representations (e.g., the lambda calculus) and subsequently executing the formal logical forms over KBs. Compared to deriving the complete logical representation, our method aims to parse a question into a limited logic form with the semantic item query, which we believe is more appropriate for answering factoid questions. Markov Logic Networks have been widely used in NLP tasks. Huang (2012) applied MLN to compress sentences by formulating the task as a word/phrase deletion prob</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Bizer</author>
<author>Tom Heath</author>
<author>Tim Berners-Lee</author>
</authors>
<title>Linked data-the story so far. International journal on semantic web and information systems,</title>
<date>2009</date>
<pages>5--3</pages>
<contexts>
<context position="1518" citStr="Bizer et al., 2009" startWordPosition="224" endWordPosition="227">t-order logic clauses in a Markov Logic Network. All clauses can then produce interacted effects in a unified framework and can jointly resolve all ambiguities. Moreover, our method adopts a pattern-learning strategy for semantic item grouping. In this way, our method can cover more text expressions and answer more questions than previous methods using manually designed patterns. The experimental results using open benchmarks demonstrate the effectiveness of the proposed method. 1 Introduction With the rapid development of the Web of Data, many RDF datasets have been published as Linked Data (Bizer et al., 2009), such as DBpedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and YAGO (Suchanek et al., 2007). The growing amount of Linked Data contains a wealth of knowledge, including entities, classes and relations. Moreover, these linked data usually have ∗Shizhu He and Kang Liu have equal contribution to this work. complex structures and are highly heterogeneous. As a result, there are gaps for users regarding access. Although a few experts can write queries using structured languages (such as SPARQL) based on their needs, this skill cannot be easily utilized by common users (Christina and F</context>
</contexts>
<marker>Bizer, Heath, Berners-Lee, 2009</marker>
<rawString>Christian Bizer, Tom Heath, and Tim Berners-Lee. 2009. Linked data-the story so far. International journal on semantic web and information systems, 5(3):1–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In SIGMOD.</booktitle>
<contexts>
<context position="1590" citStr="Bollacker et al., 2008" startWordPosition="236" endWordPosition="239">n produce interacted effects in a unified framework and can jointly resolve all ambiguities. Moreover, our method adopts a pattern-learning strategy for semantic item grouping. In this way, our method can cover more text expressions and answer more questions than previous methods using manually designed patterns. The experimental results using open benchmarks demonstrate the effectiveness of the proposed method. 1 Introduction With the rapid development of the Web of Data, many RDF datasets have been published as Linked Data (Bizer et al., 2009), such as DBpedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and YAGO (Suchanek et al., 2007). The growing amount of Linked Data contains a wealth of knowledge, including entities, classes and relations. Moreover, these linked data usually have ∗Shizhu He and Kang Liu have equal contribution to this work. complex structures and are highly heterogeneous. As a result, there are gaps for users regarding access. Although a few experts can write queries using structured languages (such as SPARQL) based on their needs, this skill cannot be easily utilized by common users (Christina and Freitas, 2014). Thus, providing user-friendly, simple interfaces to acces</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In SIGMOD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingqing Cai</author>
<author>Alexander Yates</author>
</authors>
<title>Large-scale semantic parsing via schema matching and lexicon extension.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="39316" citStr="Cai and Yates, 2013" startWordPosition="6323" endWordPosition="6326">containing quantifiers, comparatives or superlatives, Unger et al. (2012) translated NL to FL using several SPARQL templates and using a set of heuristic rules mapping phrases to semantic items. The system most similar to ours is DEANNA (Yahya et al., 2012). However, DEANNA extracts predicate-argument structures from the questions using three hand-written patterns. Our system jointly learns these mappings and extractions completely from scratch. Recently, the Semantic Parsing (SP) community targeted this problem from limited domains (Tang and Mooney, 2001; Liang et al., 2013) to open domains (Cai and Yates, 2013; Berant et al., 2013). The methods in semantic parsing answer questions by first converting natural language utterances into meaningful representations (e.g., the lambda calculus) and subsequently executing the formal logical forms over KBs. Compared to deriving the complete logical representation, our method aims to parse a question into a limited logic form with the semantic item query, which we believe is more appropriate for answering factoid questions. Markov Logic Networks have been widely used in NLP tasks. Huang (2012) applied MLN to compress sentences by formulating the task as a wor</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Qingqing Cai and Alexander Yates. 2013. Large-scale semantic parsing via schema matching and lexicon extension. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Unger Christina</author>
<author>Andr Freitas</author>
</authors>
<title>Question answering over linked data: Challenges, approaches, trends.</title>
<date>2014</date>
<booktitle>In ESWC.</booktitle>
<contexts>
<context position="2131" citStr="Christina and Freitas, 2014" startWordPosition="325" endWordPosition="328">r et al., 2009), such as DBpedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and YAGO (Suchanek et al., 2007). The growing amount of Linked Data contains a wealth of knowledge, including entities, classes and relations. Moreover, these linked data usually have ∗Shizhu He and Kang Liu have equal contribution to this work. complex structures and are highly heterogeneous. As a result, there are gaps for users regarding access. Although a few experts can write queries using structured languages (such as SPARQL) based on their needs, this skill cannot be easily utilized by common users (Christina and Freitas, 2014). Thus, providing user-friendly, simple interfaces to access these linked data becomes increasingly more urgent. Because of this, question answering over linked data (QALD) (Walter et al., 2012) has recently received much interest, and most studies on this topic have focused on translating natural language questions into structured queries (Freitas and Curry, 2014; Yahya et al., 2012; Unger et al., 2012; Shekarpour et al., 2013; Yahya et al., 2013; Bao et al., 2014; Zou et al., 2014). For example, with respect to the question “Which software has been developed by organizations founded in Calif</context>
</contexts>
<marker>Christina, Freitas, 2014</marker>
<rawString>Unger Christina and Andr Freitas. 2014. Question answering over linked data: Challenges, approaches, trends. In ESWC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dima Corina</author>
</authors>
<title>Intui2: A prototype system for question answering over linked data. In Work. Multilingual Question Answering over Linked Data (QALD-3).</title>
<date>2013</date>
<marker>Corina, 2013</marker>
<rawString>Dima Corina. 2013. Intui2: A prototype system for question answering over linked data. In Work. Multilingual Question Answering over Linked Data (QALD-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giannone Cristina</author>
<author>Bellomaria Valentina</author>
<author>Basili Roberto</author>
</authors>
<title>A hmm-based approach to question answering against linked data. In Work. Multilingual Question Answering over Linked Data (QALD3).</title>
<date>2013</date>
<marker>Cristina, Valentina, Roberto, 2013</marker>
<rawString>Giannone Cristina, Bellomaria Valentina, and Basili Roberto. 2013. A hmm-based approach to question answering against linked data. In Work. Multilingual Question Answering over Linked Data (QALD3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, Christopher D Manning, et al. 2006. Generating typed dependency parses from phrase structure parses. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shady Elbassuoni</author>
<author>Roi Blanco</author>
</authors>
<title>Keyword search over rdf graphs.</title>
<date>2011</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="37449" citStr="Elbassuoni and Blanco, 2011" startWordPosition="6024" endWordPosition="6027">posed method is related to two lines of work: Question Answering over Knowledge bases and Markov Logic Networks. Question answering over knowledge bases has attracted a substantial amount of interest over a long period of time. The initial attempts included BaseBall (Green Jr et al., 1961) and Lunar (Woods, 1977). However, these systems were mostly limited to closed domains due to a lack of knowledge resources. With the rapid development of structured data, such as DBpedia, Freebase and Yago, the need for providing user-friendly interface to these data has become increasingly urgent. Keyword (Elbassuoni and Blanco, 2011) and semantic (Pound et al., 2010) searches are limited to their ability to specify the relations among the different keywords. The open topic progress has also been pushed by the QALD evaluation campaigns (Walter et al., 2012). Lopez et al. (2011) gave a comprehensive survey in this research area. The authors developed the PowerAqua system (Lopez et al., 2006) to the-art systems in QALD-4. The results can be found at http://greententacle.techfak.uni-bielefeld.de/ cunger/qald. 1100 Formulas PD PM MG Avg P R F1 P R F1 P R F1 P R F1 All Formulas 0.941 0.941 0.941 0.878 0.918 0.898 0.636 0.798 0.</context>
</contexts>
<marker>Elbassuoni, Blanco, 2011</marker>
<rawString>Shady Elbassuoni and Roi Blanco. 2011. Keyword search over rdf graphs. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="13833" citStr="Fader et al., 2011" startWordPosition="2113" endWordPosition="2116">x.html 7http://nlp.stanford.edu/software/CRF-NER.shtml 1094 Figure 2: Framework of our system. from film, movie and show, we compute the similarity between the phrase and the class in the KB with the word2vec tool8. The word2vec tool computes fixed-length vector representations of words with a recurrent-neural-network based language model (Mikolov et al., 2010). The similarity scoring methods are introduced in Section 4.2. Then, the top-N most similar classes for each phrase are returned. For mapping phrases to relations, we employ the resources from PATTY (Nakashole et al., 2012) and ReVerb (Fader et al., 2011). Specifically, we first compute the associations between the ontological relations in DBpedia and the relation patterns in PATTY and ReVerb through instance alignments as in (Berant et al., 2013). Next, if a detected phrase is matched to some relation pattern, the corresponding ontological relations in DBpedia will be returned as a candidate. This step only generates candidates for every possible mapping, and the decision of the best selection will be performed in the next step. 3) Feature extraction and joint inference. There exist ambiguities in phrase detection and in mapping phrases to se</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Luke Zettlemoyer</author>
<author>Oren Etzioni</author>
</authors>
<title>Paraphrase-driven learning for open question answering.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<marker>Fader, Zettlemoyer, Etzioni, 2013</marker>
<rawString>Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2013. Paraphrase-driven learning for open question answering. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angela Fahrni</author>
<author>Michael Strube</author>
</authors>
<title>Jointly disambiguating and clustering concepts and entities with markov logic.</title>
<date>2012</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="39967" citStr="Fahrni and Strube (2012)" startWordPosition="6422" endWordPosition="6425"> methods in semantic parsing answer questions by first converting natural language utterances into meaningful representations (e.g., the lambda calculus) and subsequently executing the formal logical forms over KBs. Compared to deriving the complete logical representation, our method aims to parse a question into a limited logic form with the semantic item query, which we believe is more appropriate for answering factoid questions. Markov Logic Networks have been widely used in NLP tasks. Huang (2012) applied MLN to compress sentences by formulating the task as a word/phrase deletion problem. Fahrni and Strube (2012) jointly disambiguated and clustered concepts using MLN. MLN has also been used in coreference resolution (Song et al., 2012). For the task of identifying subjective text segments and of extracting their corresponding explanations from product reviews, Zhang et al. (2013) modeled these segments with MLN. To discover logical knowledge for deep question answering, Liu (2012) used MLN to resolve the inconsistencies of multiple knowledge bases. Meza-Ruiz and Riedel (2009) employed MLN for Semantic Role Labeling (SRL). They jointly performed the following tasks for a sentence: predicate identificat</context>
</contexts>
<marker>Fahrni, Strube, 2012</marker>
<rawString>Angela Fahrni and Michael Strube. 2012. Jointly disambiguating and clustering concepts and entities with markov logic. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Freitas</author>
<author>Edward Curry</author>
</authors>
<title>Natural language queries over heterogeneous linked data graphs: A distributional-compositional semantics approach.</title>
<date>2014</date>
<booktitle>In IUI.</booktitle>
<contexts>
<context position="2497" citStr="Freitas and Curry, 2014" startWordPosition="378" endWordPosition="381">erogeneous. As a result, there are gaps for users regarding access. Although a few experts can write queries using structured languages (such as SPARQL) based on their needs, this skill cannot be easily utilized by common users (Christina and Freitas, 2014). Thus, providing user-friendly, simple interfaces to access these linked data becomes increasingly more urgent. Because of this, question answering over linked data (QALD) (Walter et al., 2012) has recently received much interest, and most studies on this topic have focused on translating natural language questions into structured queries (Freitas and Curry, 2014; Yahya et al., 2012; Unger et al., 2012; Shekarpour et al., 2013; Yahya et al., 2013; Bao et al., 2014; Zou et al., 2014). For example, with respect to the question “Which software has been developed by organizations founded in California, USA?”, the aim is to automatically convert this utterance into an SPARQL query that contains the following subject-property-object (SPO) triple format: ?url rdf:type dbo:Software, ?url dbo:developer ?x1, ?x1 rdf:type dbo:Company, ?x1 dbo:foundationPlace dbr:California)1. To fulfill this objective, existing systems (Lopez et al., 2006; Unger et al., 2012; Ya</context>
<context position="4394" citStr="Freitas and Curry, 2014" startWordPosition="678" endWordPosition="681">er 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics problem of ambiguity, which is the major challenge in QALD. Using the question mentioned above as an example, we can choose California or California, USA when detecting phrases, the phrase California can be mapped to the entity California State or California Film, and the class Software (mapped from the phrase software) can be matched with the first argument of the relation producer or developer (these two relations can be mapped from the phrase developed). Previous methods (Lopez et al., 2006; Lehmann et al., 2012; Freitas and Curry, 2014) have usually performed disambiguation at each step only, and the subsequent step was performed based on the disambiguation results in the previous step(s). However, we argue that the three steps mentioned above have mutual effects. In the previous example, the phrase founded in (verb) can be mapped to the entities (Founding of Rome and Founder (company)), classes (Company and Department) or relations (foundedBy and foundationPlace). If we know that the phrase California can refer to the entity California State, and which can be the second argument of the relation foundationPlace, together wit</context>
</contexts>
<marker>Freitas, Curry, 2014</marker>
<rawString>Andre Freitas and Edward Curry. 2014. Natural language queries over heterogeneous linked data graphs: A distributional-compositional semantics approach. In IUI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bert F Green Jr</author>
<author>Alice K Wolf</author>
<author>Carol Chomsky</author>
<author>Kenneth Laughery</author>
</authors>
<title>Baseball: an automatic question-answerer.</title>
<date>1961</date>
<booktitle>In Papers presented at the May 9-11, 1961, western joint IRE-AIEE-ACM computer conference,</booktitle>
<pages>219--224</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="37111" citStr="Jr et al., 1961" startWordPosition="5971" endWordPosition="5974">oost the performance on some single tasks, but employing all formulas can produce the best performance. This illustrates that solely resolving the steps in QALD (phrase detection, phrase mapping, semantic items grouping) can obtain local results, and that making joint inference is necessary and useful. 6 Related Work Our proposed method is related to two lines of work: Question Answering over Knowledge bases and Markov Logic Networks. Question answering over knowledge bases has attracted a substantial amount of interest over a long period of time. The initial attempts included BaseBall (Green Jr et al., 1961) and Lunar (Woods, 1977). However, these systems were mostly limited to closed domains due to a lack of knowledge resources. With the rapid development of structured data, such as DBpedia, Freebase and Yago, the need for providing user-friendly interface to these data has become increasingly urgent. Keyword (Elbassuoni and Blanco, 2011) and semantic (Pound et al., 2010) searches are limited to their ability to specify the relations among the different keywords. The open topic progress has also been pushed by the QALD evaluation campaigns (Walter et al., 2012). Lopez et al. (2011) gave a compre</context>
</contexts>
<marker>Jr, Wolf, Chomsky, Laughery, 1961</marker>
<rawString>Bert F Green Jr, Alice K Wolf, Carol Chomsky, and Kenneth Laughery. 1961. Baseball: an automatic question-answerer. In Papers presented at the May 9-11, 1961, western joint IRE-AIEE-ACM computer conference, pages 219–224. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shizhu He</author>
<author>Shulin Liu</author>
<author>Yubo Chen</author>
<author>Guangyou Zhou</author>
<author>Kang Liu</author>
<author>Jun Zhao</author>
</authors>
<title>Casia@qald-3: A question answering system over linked data. In Work. Multilingual Question Answering over Linked Data (QALD-3).</title>
<date>2013</date>
<marker>He, Liu, Chen, Zhou, Liu, Zhao, 2013</marker>
<rawString>Shizhu He, Shulin Liu, Yubo Chen, Guangyou Zhou, Kang Liu, and Jun Zhao. 2013. Casia@qald-3: A question answering system over linked data. In Work. Multilingual Question Answering over Linked Data (QALD-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minlie Huang</author>
<author>Xing Shi</author>
<author>Feng Jin</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Using first-order logic to compress sentences.</title>
<date>2012</date>
<booktitle>In AAAI.</booktitle>
<marker>Huang, Shi, Jin, Zhu, 2012</marker>
<rawString>Minlie Huang, Xing Shi, Feng Jin, and Xiaoyan Zhu. 2012. Using first-order logic to compress sentences. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaccard</author>
</authors>
<title>Nouvelles recherches sur la distribution florale.</title>
<date>1908</date>
<booktitle>Bulletin de la Soci`ete Vaudense des Sciences Naturelles,</booktitle>
<pages>44--223</pages>
<contexts>
<context position="22648" citStr="Jaccard, 1908" startWordPosition="3545" endWordPosition="3546">and hasMeanWord are designed to indicate the possible patterns. Note that if these tokens only contain POS tags dt|in|wdt|to|cc|ex|pos|wp or stop words, the value of the predicate hasMeanWord is false; otherwise, it is true. In this way, our system is expected to cover more question expressions. Moreover, the SPARQL endpoint is used to verify the type compatibility of two semantic items and if one triple pattern can obtain query results. The predicate hasRelatedness needs to compute the coherence score between two semantic items. Following (Yahya et al., 2012), we use the Jaccard coefficient (Jaccard, 1908) based on the inlinks between two semantic items. The predicate priorMatchScore assigns a prior score when mapping a phrase to a semantic item. We use different methods to compute this score according to different semantic item types. For entities, we use a normalized score based on the frequencies of a phrase referring to an entity. For classes and relations, we use different methods. We first define the following three similarity metrics: a) s1: The Levenshtein distance score (Navarro, 2001) between the labels of the semantic item and the phrase; b) s2: The word embedding (Mikolov et al., 20</context>
</contexts>
<marker>Jaccard, 1908</marker>
<rawString>Paul. Jaccard. 1908. Nouvelles recherches sur la distribution florale. Bulletin de la Soci`ete Vaudense des Sciences Naturelles, 44:223–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guyonvarc’H Joris</author>
<author>S´ebastien Ferr´e</author>
</authors>
<title>Scalewelis: a scalable query-based faceted search system on top of sparql endpoints.</title>
<date>2013</date>
<booktitle>In Work. Multilingual Question Answering over Linked Data (QALD-3).</booktitle>
<marker>Joris, Ferr´e, 2013</marker>
<rawString>Guyonvarc’H Joris and S´ebastien Ferr´e. 2013. Scalewelis: a scalable query-based faceted search system on top of sparql endpoints. In Work. Multilingual Question Answering over Linked Data (QALD-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Lehmann</author>
<author>Tim Furche</author>
<author>Giovanni Grasso</author>
<author>AxelCyrille Ngonga Ngomo</author>
<author>Christian Schallhart</author>
<author>Andrew Sellers</author>
<author>Christina Unger</author>
<author>Lorenz B¨uhmann</author>
<author>Daniel Gerber</author>
<author>Konrad H¨offner</author>
</authors>
<title>Deqa: deep web extraction for question answering.</title>
<date>2012</date>
<booktitle>In ISWC.</booktitle>
<marker>Lehmann, Furche, Grasso, Ngomo, Schallhart, Sellers, Unger, B¨uhmann, Gerber, H¨offner, 2012</marker>
<rawString>Jens Lehmann, Tim Furche, Giovanni Grasso, AxelCyrille Ngonga Ngomo, Christian Schallhart, Andrew Sellers, Christina Unger, Lorenz B¨uhmann, Daniel Gerber, Konrad H¨offner, et al. 2012. Deqa: deep web extraction for question answering. In ISWC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Christopher Potts</author>
</authors>
<title>Bringing machine learning and compositional semantics together. Annual Reviews of Linguistics</title>
<date>2014</date>
<note>(to appear).</note>
<marker>Liang, Potts, 2014</marker>
<rawString>Percy Liang and Christopher Potts. 2014. Bringing machine learning and compositional semantics together. Annual Reviews of Linguistics (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="39279" citStr="Liang et al., 2013" startWordPosition="6315" endWordPosition="6318">eterogeneous datasets. For questions containing quantifiers, comparatives or superlatives, Unger et al. (2012) translated NL to FL using several SPARQL templates and using a set of heuristic rules mapping phrases to semantic items. The system most similar to ours is DEANNA (Yahya et al., 2012). However, DEANNA extracts predicate-argument structures from the questions using three hand-written patterns. Our system jointly learns these mappings and extractions completely from scratch. Recently, the Semantic Parsing (SP) community targeted this problem from limited domains (Tang and Mooney, 2001; Liang et al., 2013) to open domains (Cai and Yates, 2013; Berant et al., 2013). The methods in semantic parsing answer questions by first converting natural language utterances into meaningful representations (e.g., the lambda calculus) and subsequently executing the formal logical forms over KBs. Compared to deriving the complete logical representation, our method aims to parse a question into a limited logic form with the semantic item query, which we believe is more appropriate for answering factoid questions. Markov Logic Networks have been widely used in NLP tasks. Huang (2012) applied MLN to compress sente</context>
<context position="41473" citStr="Liang et al., 2013" startWordPosition="6663" endWordPosition="6666">usions and Future Work For the task of QALD, we present a joint learning framework for phrase detection, phrase mapping and semantic item grouping. The novelty of our method lies in the fact that we perform joint inference and pattern learning for all subtasks in QALD using first-order logic. Our experimental results demonstrate the effectiveness of the proposed method. In the future, we plan to address the following limitations that still exist in the current system: a) numerous hand-labeled data are required for training the MLN, and we could use a latent form of semantic item query graphs (Liang et al., 2013); b) more robust solutions can be developed to find the implicit relations in questions; c) our system can be scaled up to large-scale opendomain knowledge bases (Fader et al., 2013; Yao and Van Durme, 2014); and d) the learning system has the advantage of being easily adapted to new settings, and we plan to extend it to other domains and languages (Liang and Potts, 2014). Acknowledgments The authors are grateful to the anonymous reviewers for their constructive comments. This work was sponsored by the National Basic Research Program of China (No. 2014CB340503) and the National Natural Science</context>
</contexts>
<marker>Liang, Jordan, Klein, 2013</marker>
<rawString>Percy Liang, Michael I Jordan, and Dan Klein. 2013. Learning dependency-based compositional semantics. Computational Linguistics, 39(2):389–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhao Liu</author>
<author>Xipeng Qiu</author>
<author>Ling Cao</author>
<author>Xuanjing Huang</author>
</authors>
<title>Discovering logical knowledge for deep question answering.</title>
<date>2012</date>
<booktitle>In CIKM.</booktitle>
<marker>Liu, Qiu, Cao, Huang, 2012</marker>
<rawString>Zhao Liu, Xipeng Qiu, Ling Cao, and Xuanjing Huang. 2012. Discovering logical knowledge for deep question answering. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Lopez</author>
<author>Enrico Motta</author>
<author>Victoria Uren</author>
</authors>
<title>Poweraqua: Fishing the semantic web. In The Semantic Web: research and applications,</title>
<date>2006</date>
<pages>393--410</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3073" citStr="Lopez et al., 2006" startWordPosition="464" endWordPosition="467">tructured queries (Freitas and Curry, 2014; Yahya et al., 2012; Unger et al., 2012; Shekarpour et al., 2013; Yahya et al., 2013; Bao et al., 2014; Zou et al., 2014). For example, with respect to the question “Which software has been developed by organizations founded in California, USA?”, the aim is to automatically convert this utterance into an SPARQL query that contains the following subject-property-object (SPO) triple format: ?url rdf:type dbo:Software, ?url dbo:developer ?x1, ?x1 rdf:type dbo:Company, ?x1 dbo:foundationPlace dbr:California)1. To fulfill this objective, existing systems (Lopez et al., 2006; Unger et al., 2012; Yahya et al., 2012; Zou et al., 2014) usually adopt a pipeline framework that contains four major steps: 1) decomposing the question and detecting phrases, 2) mapping the detected phrases into semantic items of Linked Data, 3) grouping the mapped semantic items into semantic triples, and 4) generating the correct SPARQL query. However, completing these four steps and constructing such a structured query is not easy. The first three steps mentioned above are subject to the 1The prefixes in semantic items indicate the source of their vocabularies. 1092 Proceedings of the 20</context>
<context position="4346" citStr="Lopez et al., 2006" startWordPosition="670" endWordPosition="673">Processing (EMNLP), pages 1092–1103, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics problem of ambiguity, which is the major challenge in QALD. Using the question mentioned above as an example, we can choose California or California, USA when detecting phrases, the phrase California can be mapped to the entity California State or California Film, and the class Software (mapped from the phrase software) can be matched with the first argument of the relation producer or developer (these two relations can be mapped from the phrase developed). Previous methods (Lopez et al., 2006; Lehmann et al., 2012; Freitas and Curry, 2014) have usually performed disambiguation at each step only, and the subsequent step was performed based on the disambiguation results in the previous step(s). However, we argue that the three steps mentioned above have mutual effects. In the previous example, the phrase founded in (verb) can be mapped to the entities (Founding of Rome and Founder (company)), classes (Company and Department) or relations (foundedBy and foundationPlace). If we know that the phrase California can refer to the entity California State, and which can be the second argume</context>
<context position="37812" citStr="Lopez et al., 2006" startWordPosition="6085" endWordPosition="6088">ains due to a lack of knowledge resources. With the rapid development of structured data, such as DBpedia, Freebase and Yago, the need for providing user-friendly interface to these data has become increasingly urgent. Keyword (Elbassuoni and Blanco, 2011) and semantic (Pound et al., 2010) searches are limited to their ability to specify the relations among the different keywords. The open topic progress has also been pushed by the QALD evaluation campaigns (Walter et al., 2012). Lopez et al. (2011) gave a comprehensive survey in this research area. The authors developed the PowerAqua system (Lopez et al., 2006) to the-art systems in QALD-4. The results can be found at http://greententacle.techfak.uni-bielefeld.de/ cunger/qald. 1100 Formulas PD PM MG Avg P R F1 P R F1 P R F1 P R F1 All Formulas 0.941 0.941 0.941 0.878 0.918 0.898 0.636 0.798 0.708 0.839 0.901 0.869 -sf3 0.931 0.927 0.929 0.877 0.913 0.895 0.637 0.816 0.715 0.834 0.897 0.864 -sf4 0.926 0.917 0.922 0.852 0.883 0.867 0.63 0.763 0.69 0.824 0.87 0.846 -sf5 0.931 0.927 0.929 0.873 0.908 0.89 0.633 0.816 0.713 0.831 0.895 0.862 -sf6 0.922 0.922 0.922 0.844 0.883 0.863 0.702 0.746 0.723 0.842 0.868 0.855 -sf7 0.931 0.917 0.924 0.881 0.908 0.</context>
</contexts>
<marker>Lopez, Motta, Uren, 2006</marker>
<rawString>Vanessa Lopez, Enrico Motta, and Victoria Uren. 2006. Poweraqua: Fishing the semantic web. In The Semantic Web: research and applications, pages 393–410. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Lopez</author>
<author>Victoria Uren</author>
<author>Marta Sabou</author>
<author>Enrico Motta</author>
</authors>
<title>Is question answering fit for the semantic web?: a survey. Semantic Web,</title>
<date>2011</date>
<volume>2</volume>
<issue>2</issue>
<pages>155</pages>
<contexts>
<context position="37697" citStr="Lopez et al. (2011)" startWordPosition="6066" endWordPosition="6069"> BaseBall (Green Jr et al., 1961) and Lunar (Woods, 1977). However, these systems were mostly limited to closed domains due to a lack of knowledge resources. With the rapid development of structured data, such as DBpedia, Freebase and Yago, the need for providing user-friendly interface to these data has become increasingly urgent. Keyword (Elbassuoni and Blanco, 2011) and semantic (Pound et al., 2010) searches are limited to their ability to specify the relations among the different keywords. The open topic progress has also been pushed by the QALD evaluation campaigns (Walter et al., 2012). Lopez et al. (2011) gave a comprehensive survey in this research area. The authors developed the PowerAqua system (Lopez et al., 2006) to the-art systems in QALD-4. The results can be found at http://greententacle.techfak.uni-bielefeld.de/ cunger/qald. 1100 Formulas PD PM MG Avg P R F1 P R F1 P R F1 P R F1 All Formulas 0.941 0.941 0.941 0.878 0.918 0.898 0.636 0.798 0.708 0.839 0.901 0.869 -sf3 0.931 0.927 0.929 0.877 0.913 0.895 0.637 0.816 0.715 0.834 0.897 0.864 -sf4 0.926 0.917 0.922 0.852 0.883 0.867 0.63 0.763 0.69 0.824 0.87 0.846 -sf5 0.931 0.927 0.929 0.873 0.908 0.89 0.633 0.816 0.713 0.831 0.895 0.862</context>
</contexts>
<marker>Lopez, Uren, Sabou, Motta, 2011</marker>
<rawString>Vanessa Lopez, Victoria Uren, Marta Sabou, and Enrico Motta. 2011. Is question answering fit for the semantic web?: a survey. Semantic Web, 2(2):125– 155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Meza-Ruiz</author>
<author>Sebastian Riedel</author>
</authors>
<title>Jointly identifying predicates, arguments and senses using markov logic.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="40439" citStr="Meza-Ruiz and Riedel (2009)" startWordPosition="6494" endWordPosition="6497">idely used in NLP tasks. Huang (2012) applied MLN to compress sentences by formulating the task as a word/phrase deletion problem. Fahrni and Strube (2012) jointly disambiguated and clustered concepts using MLN. MLN has also been used in coreference resolution (Song et al., 2012). For the task of identifying subjective text segments and of extracting their corresponding explanations from product reviews, Zhang et al. (2013) modeled these segments with MLN. To discover logical knowledge for deep question answering, Liu (2012) used MLN to resolve the inconsistencies of multiple knowledge bases. Meza-Ruiz and Riedel (2009) employed MLN for Semantic Role Labeling (SRL). They jointly performed the following tasks for a sentence: predicate identification, frame disambiguation, argument identification and argument classification. The semantic analysis of SRL solely rested on the lexical level, but our analysis focuses on the knowledge-base level and aims to obtain an executable query and to support natural language inference. 7 Conclusions and Future Work For the task of QALD, we present a joint learning framework for phrase detection, phrase mapping and semantic item grouping. The novelty of our method lies in the</context>
</contexts>
<marker>Meza-Ruiz, Riedel, 2009</marker>
<rawString>Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly identifying predicates, arguments and senses using markov logic. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ndapandula Nakashole</author>
<author>Gerhard Weikum</author>
<author>Fabian Suchanek</author>
</authors>
<title>Patty: a taxonomy of relational patterns with semantic types.</title>
<date>2012</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="13801" citStr="Nakashole et al., 2012" startWordPosition="2107" endWordPosition="2110">mple.edu/∼yates/open-semparsing/index.html 7http://nlp.stanford.edu/software/CRF-NER.shtml 1094 Figure 2: Framework of our system. from film, movie and show, we compute the similarity between the phrase and the class in the KB with the word2vec tool8. The word2vec tool computes fixed-length vector representations of words with a recurrent-neural-network based language model (Mikolov et al., 2010). The similarity scoring methods are introduced in Section 4.2. Then, the top-N most similar classes for each phrase are returned. For mapping phrases to relations, we employ the resources from PATTY (Nakashole et al., 2012) and ReVerb (Fader et al., 2011). Specifically, we first compute the associations between the ontological relations in DBpedia and the relation patterns in PATTY and ReVerb through instance alignments as in (Berant et al., 2013). Next, if a detected phrase is matched to some relation pattern, the corresponding ontological relations in DBpedia will be returned as a candidate. This step only generates candidates for every possible mapping, and the decision of the best selection will be performed in the next step. 3) Feature extraction and joint inference. There exist ambiguities in phrase detect</context>
</contexts>
<marker>Nakashole, Weikum, Suchanek, 2012</marker>
<rawString>Ndapandula Nakashole, Gerhard Weikum, and Fabian Suchanek. 2012. Patty: a taxonomy of relational patterns with semantic types. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Navarro</author>
</authors>
<title>A guided tour to approximate string matching.</title>
<date>2001</date>
<journal>ACM Comput. Surv.,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="23146" citStr="Navarro, 2001" startWordPosition="3627" endWordPosition="3628">nce score between two semantic items. Following (Yahya et al., 2012), we use the Jaccard coefficient (Jaccard, 1908) based on the inlinks between two semantic items. The predicate priorMatchScore assigns a prior score when mapping a phrase to a semantic item. We use different methods to compute this score according to different semantic item types. For entities, we use a normalized score based on the frequencies of a phrase referring to an entity. For classes and relations, we use different methods. We first define the following three similarity metrics: a) s1: The Levenshtein distance score (Navarro, 2001) between the labels of the semantic item and the phrase; b) s2: The word embedding (Mikolov et al., 2010) score, which measures the similarity between two phrases and is the maximum cosine value of the words’ word embeddings between two phrases; and c) s3: the instance overlap score, which is computed using the Jaccard coefficient of the instance overlap. All scores are normalized to produce a comparable scores in the interval of (0, 1). The final prior scores for mapping phrases to classes and relations are rys1 + (1 − ry)s2 and αs1 + Qs2 + (1 − α − Q)s3, respectively. The parameters are set </context>
</contexts>
<marker>Navarro, 2001</marker>
<rawString>Gonzalo Navarro. 2001. A guided tour to approximate string matching. ACM Comput. Surv., 33(1):31–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pound</author>
<author>Ihab F Ilyas</author>
<author>Grant Weddell</author>
</authors>
<title>Expressive and flexible access to web-extracted data: a keyword-based structured query language.</title>
<date>2010</date>
<booktitle>In SIGMOD.</booktitle>
<contexts>
<context position="37483" citStr="Pound et al., 2010" startWordPosition="6031" endWordPosition="6034">k: Question Answering over Knowledge bases and Markov Logic Networks. Question answering over knowledge bases has attracted a substantial amount of interest over a long period of time. The initial attempts included BaseBall (Green Jr et al., 1961) and Lunar (Woods, 1977). However, these systems were mostly limited to closed domains due to a lack of knowledge resources. With the rapid development of structured data, such as DBpedia, Freebase and Yago, the need for providing user-friendly interface to these data has become increasingly urgent. Keyword (Elbassuoni and Blanco, 2011) and semantic (Pound et al., 2010) searches are limited to their ability to specify the relations among the different keywords. The open topic progress has also been pushed by the QALD evaluation campaigns (Walter et al., 2012). Lopez et al. (2011) gave a comprehensive survey in this research area. The authors developed the PowerAqua system (Lopez et al., 2006) to the-art systems in QALD-4. The results can be found at http://greententacle.techfak.uni-bielefeld.de/ cunger/qald. 1100 Formulas PD PM MG Avg P R F1 P R F1 P R F1 P R F1 All Formulas 0.941 0.941 0.941 0.878 0.918 0.898 0.636 0.798 0.708 0.839 0.901 0.869 -sf3 0.931 0</context>
</contexts>
<marker>Pound, Ilyas, Weddell, 2010</marker>
<rawString>Jeffrey Pound, Ihab F Ilyas, and Grant Weddell. 2010. Expressive and flexible access to web-extracted data: a keyword-based structured query language. In SIGMOD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pradel</author>
<author>G Peyet</author>
<author>O Haemmerl´e</author>
<author>N Hernandez</author>
</authors>
<title>Swip at qald-3: results, criticisms and lesson learned (working notes). In Work. Multilingual Question Answering over Linked Data (QALD-3).</title>
<date>2013</date>
<marker>Pradel, Peyet, Haemmerl´e, Hernandez, 2013</marker>
<rawString>C Pradel, G Peyet, O Haemmerl´e, and N Hernandez. 2013. Swip at qald-3: results, criticisms and lesson learned (working notes). In Work. Multilingual Question Answering over Linked Data (QALD-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine learning,</booktitle>
<pages>62--1</pages>
<contexts>
<context position="6491" citStr="Richardson and Domingos, 2006" startWordPosition="1008" endWordPosition="1011">s2 used in (Yahya et al., 2012) are only 62.8 and 66.2%, respectively. Furthermore, these specially designed patterns may not be valid with variations in domains or languages. Therefore, another important question arises: can we automatically learn rules or patterns to achieve the same ob2They are 1) verbs and their arguments, 2) adjectives and their arguments and 3) propositionally modified tokens and objects of prepositions. jective? (Question Two) Focusing on the two problems mentioned above, this paper proposes a novel algorithm based on a learning framework, Markov Logic Networks (MLNs) (Richardson and Domingos, 2006), to learn a joint model for constructing structured queries from natural language utterances. MLN is a statistical relational learning framework that combines first-order logic and Markov networks. The appealing property of MLN is that it is readily interpretable by humans and that it is a natural framework for performing joint learning. We formulate the knowledge for resolving the ambiguities in the main three steps of QALD (phrase detection, phrase-to-semantic-item mapping and semantic item grouping) as first-order logic clauses in an MLN. In the framework of MLN, all clauses will produce i</context>
<context position="17603" citStr="Richardson and Domingos, 2006" startWordPosition="2701" endWordPosition="2704">ct the SPARQL query SELECT(?url) WHERE{ ?url rdf:type dbo:Software. ?url dbo:developer ?x1. ?x1 rdf:type dbo:Company. ?x1 dbo:foundationPlace dbr:California.} for this example. 4 Joint Disambiguation with MLN In this section, we present our method for question answering over linked data using a Markov Logic Network (MLN). In the following subsections, we first briefly describe the MLN. Then, we present the predicates and the first-order logic formulas used in the model. 4.1 Markov Logic Networks Markov logic networks combine Markov networks with first-order logic in a probabilistic framework (Richardson and Domingos, 2006). An MLN M consists of several weighted formulas {(Oi , wi)}i, where Oi is a first order formula and wi is the penalty (the formula’s weight). In contrast to the first-order logic, whereby a formula represents a hard constraint, these logic formulas are relaxed and can be violated with penalties in the 10This corresponds to the question “How many people live in the capital of Australia?” MLN. Each formula Oi consists of a set of firstorder predicates, logical connectors and variables. These weighted formulas define a probability distribution over a possible world. Let y denote a possible world</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Matthew Richardson and Pedro Domingos. 2006. Markov logic networks. Machine learning, 62(1-2):107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saeedeh Shekarpour</author>
<author>Axel-Cyrille Ngonga Ngomo</author>
<author>S¨oren Auer</author>
</authors>
<title>Question answering on interlinked data.</title>
<date>2013</date>
<booktitle>In WWW.</booktitle>
<contexts>
<context position="2562" citStr="Shekarpour et al., 2013" startWordPosition="390" endWordPosition="393">s. Although a few experts can write queries using structured languages (such as SPARQL) based on their needs, this skill cannot be easily utilized by common users (Christina and Freitas, 2014). Thus, providing user-friendly, simple interfaces to access these linked data becomes increasingly more urgent. Because of this, question answering over linked data (QALD) (Walter et al., 2012) has recently received much interest, and most studies on this topic have focused on translating natural language questions into structured queries (Freitas and Curry, 2014; Yahya et al., 2012; Unger et al., 2012; Shekarpour et al., 2013; Yahya et al., 2013; Bao et al., 2014; Zou et al., 2014). For example, with respect to the question “Which software has been developed by organizations founded in California, USA?”, the aim is to automatically convert this utterance into an SPARQL query that contains the following subject-property-object (SPO) triple format: ?url rdf:type dbo:Software, ?url dbo:developer ?x1, ?x1 rdf:type dbo:Company, ?x1 dbo:foundationPlace dbr:California)1. To fulfill this objective, existing systems (Lopez et al., 2006; Unger et al., 2012; Yahya et al., 2012; Zou et al., 2014) usually adopt a pipeline fram</context>
</contexts>
<marker>Shekarpour, Ngomo, Auer, 2013</marker>
<rawString>Saeedeh Shekarpour, Axel-Cyrille Ngonga Ngomo, and S¨oren Auer. 2013. Question answering on interlinked data. In WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Song</author>
<author>Jing Jiang</author>
<author>Wayne Xin Zhao</author>
<author>Sujian Li</author>
<author>Houfeng Wang</author>
</authors>
<title>Joint learning for coreference resolution with markov logic.</title>
<date>2012</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="40092" citStr="Song et al., 2012" startWordPosition="6442" endWordPosition="6445">, the lambda calculus) and subsequently executing the formal logical forms over KBs. Compared to deriving the complete logical representation, our method aims to parse a question into a limited logic form with the semantic item query, which we believe is more appropriate for answering factoid questions. Markov Logic Networks have been widely used in NLP tasks. Huang (2012) applied MLN to compress sentences by formulating the task as a word/phrase deletion problem. Fahrni and Strube (2012) jointly disambiguated and clustered concepts using MLN. MLN has also been used in coreference resolution (Song et al., 2012). For the task of identifying subjective text segments and of extracting their corresponding explanations from product reviews, Zhang et al. (2013) modeled these segments with MLN. To discover logical knowledge for deep question answering, Liu (2012) used MLN to resolve the inconsistencies of multiple knowledge bases. Meza-Ruiz and Riedel (2009) employed MLN for Semantic Role Labeling (SRL). They jointly performed the following tasks for a sentence: predicate identification, frame disambiguation, argument identification and argument classification. The semantic analysis of SRL solely rested on</context>
</contexts>
<marker>Song, Jiang, Zhao, Li, Wang, 2012</marker>
<rawString>Yang Song, Jing Jiang, Wayne Xin Zhao, Sujian Li, and Houfeng Wang. 2012. Joint learning for coreference resolution with markov logic. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: a core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In WWW.</booktitle>
<contexts>
<context position="1623" citStr="Suchanek et al., 2007" startWordPosition="242" endWordPosition="245">unified framework and can jointly resolve all ambiguities. Moreover, our method adopts a pattern-learning strategy for semantic item grouping. In this way, our method can cover more text expressions and answer more questions than previous methods using manually designed patterns. The experimental results using open benchmarks demonstrate the effectiveness of the proposed method. 1 Introduction With the rapid development of the Web of Data, many RDF datasets have been published as Linked Data (Bizer et al., 2009), such as DBpedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and YAGO (Suchanek et al., 2007). The growing amount of Linked Data contains a wealth of knowledge, including entities, classes and relations. Moreover, these linked data usually have ∗Shizhu He and Kang Liu have equal contribution to this work. complex structures and are highly heterogeneous. As a result, there are gaps for users regarding access. Although a few experts can write queries using structured languages (such as SPARQL) based on their needs, this skill cannot be easily utilized by common users (Christina and Freitas, 2014). Thus, providing user-friendly, simple interfaces to access these linked data becomes incre</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lappoon R Tang</author>
<author>Raymond J Mooney</author>
</authors>
<title>Using multiple clause constructors in inductive logic programming for semantic parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of the 12th European Conference on Machine Learning,</booktitle>
<pages>466--477</pages>
<contexts>
<context position="39258" citStr="Tang and Mooney, 2001" startWordPosition="6311" endWordPosition="6314">r questions on large, heterogeneous datasets. For questions containing quantifiers, comparatives or superlatives, Unger et al. (2012) translated NL to FL using several SPARQL templates and using a set of heuristic rules mapping phrases to semantic items. The system most similar to ours is DEANNA (Yahya et al., 2012). However, DEANNA extracts predicate-argument structures from the questions using three hand-written patterns. Our system jointly learns these mappings and extractions completely from scratch. Recently, the Semantic Parsing (SP) community targeted this problem from limited domains (Tang and Mooney, 2001; Liang et al., 2013) to open domains (Cai and Yates, 2013; Berant et al., 2013). The methods in semantic parsing answer questions by first converting natural language utterances into meaningful representations (e.g., the lambda calculus) and subsequently executing the formal logical forms over KBs. Compared to deriving the complete logical representation, our method aims to parse a question into a limited logic form with the semantic item query, which we believe is more appropriate for answering factoid questions. Markov Logic Networks have been widely used in NLP tasks. Huang (2012) applied </context>
</contexts>
<marker>Tang, Mooney, 2001</marker>
<rawString>Lappoon R. Tang and Raymond J. Mooney. 2001. Using multiple clause constructors in inductive logic programming for semantic parsing. In Proceedings of the 12th European Conference on Machine Learning, pages 466–477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina Unger</author>
<author>Lorenz B¨uhmann</author>
<author>Jens Lehmann</author>
<author>Axel-Cyrille Ngonga Ngomo</author>
<author>Daniel Gerber</author>
<author>Philipp Cimiano</author>
</authors>
<title>Template-based question answering over rdf data. In WWW.</title>
<date>2012</date>
<marker>Unger, B¨uhmann, Lehmann, Ngomo, Gerber, Cimiano, 2012</marker>
<rawString>Christina Unger, Lorenz B¨uhmann, Jens Lehmann, Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and Philipp Cimiano. 2012. Template-based question answering over rdf data. In WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Walter</author>
<author>Christina Unger</author>
<author>Philipp Cimiano</author>
<author>Daniel B¨ar</author>
</authors>
<title>Evaluation of a layered approach to question answering over linked data. In The Semantic Web–ISWC</title>
<date>2012</date>
<pages>362--374</pages>
<publisher>Springer.</publisher>
<marker>Walter, Unger, Cimiano, B¨ar, 2012</marker>
<rawString>Sebastian Walter, Christina Unger, Philipp Cimiano, and Daniel B¨ar. 2012. Evaluation of a layered approach to question answering over linked data. In The Semantic Web–ISWC 2012, pages 362–374. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Woods</author>
</authors>
<title>Lunar rocks in natural english: Explorations in natural language question answering. In Linguistic structures processing,</title>
<date>1977</date>
<pages>521--569</pages>
<contexts>
<context position="37135" citStr="Woods, 1977" startWordPosition="5978" endWordPosition="5979"> single tasks, but employing all formulas can produce the best performance. This illustrates that solely resolving the steps in QALD (phrase detection, phrase mapping, semantic items grouping) can obtain local results, and that making joint inference is necessary and useful. 6 Related Work Our proposed method is related to two lines of work: Question Answering over Knowledge bases and Markov Logic Networks. Question answering over knowledge bases has attracted a substantial amount of interest over a long period of time. The initial attempts included BaseBall (Green Jr et al., 1961) and Lunar (Woods, 1977). However, these systems were mostly limited to closed domains due to a lack of knowledge resources. With the rapid development of structured data, such as DBpedia, Freebase and Yago, the need for providing user-friendly interface to these data has become increasingly urgent. Keyword (Elbassuoni and Blanco, 2011) and semantic (Pound et al., 2010) searches are limited to their ability to specify the relations among the different keywords. The open topic progress has also been pushed by the QALD evaluation campaigns (Walter et al., 2012). Lopez et al. (2011) gave a comprehensive survey in this r</context>
</contexts>
<marker>Woods, 1977</marker>
<rawString>William A Woods. 1977. Lunar rocks in natural english: Explorations in natural language question answering. In Linguistic structures processing, pages 521–569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Yahya</author>
<author>Klaus Berberich</author>
<author>Shady Elbassuoni</author>
<author>Maya Ramanath</author>
<author>Volker Tresp</author>
<author>Gerhard Weikum</author>
</authors>
<title>Natural language questions for the web of data.</title>
<date>2012</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2517" citStr="Yahya et al., 2012" startWordPosition="382" endWordPosition="385">there are gaps for users regarding access. Although a few experts can write queries using structured languages (such as SPARQL) based on their needs, this skill cannot be easily utilized by common users (Christina and Freitas, 2014). Thus, providing user-friendly, simple interfaces to access these linked data becomes increasingly more urgent. Because of this, question answering over linked data (QALD) (Walter et al., 2012) has recently received much interest, and most studies on this topic have focused on translating natural language questions into structured queries (Freitas and Curry, 2014; Yahya et al., 2012; Unger et al., 2012; Shekarpour et al., 2013; Yahya et al., 2013; Bao et al., 2014; Zou et al., 2014). For example, with respect to the question “Which software has been developed by organizations founded in California, USA?”, the aim is to automatically convert this utterance into an SPARQL query that contains the following subject-property-object (SPO) triple format: ?url rdf:type dbo:Software, ?url dbo:developer ?x1, ?x1 rdf:type dbo:Company, ?x1 dbo:foundationPlace dbr:California)1. To fulfill this objective, existing systems (Lopez et al., 2006; Unger et al., 2012; Yahya et al., 2012; Zo</context>
<context position="5452" citStr="Yahya et al., 2012" startWordPosition="848" endWordPosition="851"> we know that the phrase California can refer to the entity California State, and which can be the second argument of the relation foundationPlace, together with a verb phrase being more likely to be mapped to Relation, we should map the phrase founded in to foundationPlace in this question. Thus, we aim to determine if joint disambiguation is better than individual disambiguation. (Question One) In addition, previous systems usually employed manually designed patterns to extract predicateargument structures that are used to guide the disambiguation process in the three steps mentioned above (Yahya et al., 2012; Unger et al., 2012; Zou et al., 2014). For example, (Yahya et al., 2012) used only three dependency patterns to group the mapped semantic items into semantic triples. Nevertheless, these three manually designed patterns miss many cases because of the diversity of the question expressions. We gathered statistics on 144 questions and found that the macro-average F1 and micro-average F1 of the three patterns2 used in (Yahya et al., 2012) are only 62.8 and 66.2%, respectively. Furthermore, these specially designed patterns may not be valid with variations in domains or languages. Therefore, anot</context>
<context position="10082" citStr="Yahya et al., 2012" startWordPosition="1530" endWordPosition="1533">lation (called a property or predicate in some occasions). Some entities are literals including strings, numbers and dates (118119(xsd:integer), etc.). Relations contain standard Semantic Web relations (subClassOf, type, domain and label) and ontological relations (developer, foundationPlace and numEmployees). 2.2 Task Statement Given a knowledge base (KB), our objective is to translate a natural language question qNL into a formal language query qFL that targets the semantic vocabularies given by the KB, and the query qFL should capture the user information needs expressed by qNL. Following (Yahya et al., 2012), we focus on the factoid questions, and the answers to such questions are an entity or a set of entities. We ignore the questions that need the aggregation5 (max/min, etc.) and negation operations. That is, we generate queries that consist of a plentiful number of triple patterns, which are multiple conjunctions of SPO search conditions. 3 Framework Figure 2 shows the entire framework of our system for translating a question into a formal SPARQL query. The first three steps address the input question through 1) Phrase Detection (detecting possible phrases), 2) Phrase Mapping (mapping all 3htt</context>
<context position="22600" citStr="Yahya et al., 2012" startWordPosition="3536" endWordPosition="3539">es as possible patterns. The predicates phraseDepTag and hasMeanWord are designed to indicate the possible patterns. Note that if these tokens only contain POS tags dt|in|wdt|to|cc|ex|pos|wp or stop words, the value of the predicate hasMeanWord is false; otherwise, it is true. In this way, our system is expected to cover more question expressions. Moreover, the SPARQL endpoint is used to verify the type compatibility of two semantic items and if one triple pattern can obtain query results. The predicate hasRelatedness needs to compute the coherence score between two semantic items. Following (Yahya et al., 2012), we use the Jaccard coefficient (Jaccard, 1908) based on the inlinks between two semantic items. The predicate priorMatchScore assigns a prior score when mapping a phrase to a semantic item. We use different methods to compute this score according to different semantic item types. For entities, we use a normalized score based on the frequencies of a phrase referring to an entity. For classes and relations, we use different methods. We first define the following three similarity metrics: a) s1: The Levenshtein distance score (Navarro, 2001) between the labels of the semantic item and the phras</context>
<context position="29239" citStr="Yahya et al., 2012" startWordPosition="4617" endWordPosition="4620">bers, date comparisons and aggregation operations such as group by or order by. Therefore, we remove these types of questions and retain 110 questions from the QALD-4 training set for generating the specific formulas and for training their weights in MLN. We test our system using 37, 75 and 26 questions from the training set of QALD-115, and the testing set of QALD-3 and QALD-4 respectively. We use #T, #Q and #A to indicate the total 13www.sc.cit-ec.uni-bielefeld.de/qald/ 14https://github.com/openlink/virtuoso-opensource 15We use the training set because we try to make a fair comparison with (Yahya et al., 2012). 1098 number of questions in the testing set, the number of questions we could address and the number of questions answered correct, respectively. We select Precision (P = #A #Q), Recall (R = #A #T ), and F1-score (F1 = 2·P·R P +R) as the evaluation metrics. To assess the effectiveness of the disambiguation process in the MLN, we computed the overall quality measures by precision and recall with the manually obtained results. 5.2 Experimental Configurations The Stanford dependency parser (De Marneffe et al., 2006) is used for extracting features from the dependency parse trees. We use the too</context>
<context position="32276" citStr="Yahya et al., 2012" startWordPosition="5110" endWordPosition="5113">erforming joint inference can effectively improve the performance. Finally, we observe that the former task usually produces better results than the subsequent tasks (phrase detection exhibits a better performance than phrase mapping, and phrase mapping exhibits a better performance than semantic item grouping). The main reason is that the latter subtask is more complex than the former task. The decisions of the latter subtask strongly rely on the former results even though they have interacted effects. 5.3.2 The Effect of Pattern Learning Table 6 shows a comparison of our system with DEANNA (Yahya et al., 2012), which is based on a joint disambiguation model but which employs hand-written patterns in its system. Because DEANNA only reports its results of the QALD-1 dataset, we do not show the results for QALD-3 and QALD-4 for equity. From the results, we can see that our system solved more questions and exhibited a better performance than did DEANNA. One of the greatest strengths of our system is that the learning system can address more questions than hand-written pattern rules. System #T #Q #A P R F1 DEANNA (Yahya et al., 2012) 50 27 13 0.48 0.26 0.33 Ours 50 37 20 0.54 0.4 0.46 Table 6: Compariso</context>
<context position="38954" citStr="Yahya et al., 2012" startWordPosition="6271" endWordPosition="6274">863 0.702 0.746 0.723 0.842 0.868 0.855 -sf7 0.931 0.917 0.924 0.881 0.908 0.894 0.621 0.763 0.685 0.833 0.88 0.856 -sf8 0.927 0.927 0.927 0.868 0.908 0.888 0.639 0.807 0.713 0.83 0.893 0.861 Table 9: Performance comparisons of different weighted formulas evaluated using the QALD-3 question set. answer questions on large, heterogeneous datasets. For questions containing quantifiers, comparatives or superlatives, Unger et al. (2012) translated NL to FL using several SPARQL templates and using a set of heuristic rules mapping phrases to semantic items. The system most similar to ours is DEANNA (Yahya et al., 2012). However, DEANNA extracts predicate-argument structures from the questions using three hand-written patterns. Our system jointly learns these mappings and extractions completely from scratch. Recently, the Semantic Parsing (SP) community targeted this problem from limited domains (Tang and Mooney, 2001; Liang et al., 2013) to open domains (Cai and Yates, 2013; Berant et al., 2013). The methods in semantic parsing answer questions by first converting natural language utterances into meaningful representations (e.g., the lambda calculus) and subsequently executing the formal logical forms over </context>
</contexts>
<marker>Yahya, Berberich, Elbassuoni, Ramanath, Tresp, Weikum, 2012</marker>
<rawString>Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, Maya Ramanath, Volker Tresp, and Gerhard Weikum. 2012. Natural language questions for the web of data. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Yahya</author>
<author>Klaus Berberich</author>
<author>Shady Elbassuoni</author>
<author>Gerhard Weikum</author>
</authors>
<title>Robust question answering over the web of linked data.</title>
<date>2013</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="2582" citStr="Yahya et al., 2013" startWordPosition="394" endWordPosition="397"> can write queries using structured languages (such as SPARQL) based on their needs, this skill cannot be easily utilized by common users (Christina and Freitas, 2014). Thus, providing user-friendly, simple interfaces to access these linked data becomes increasingly more urgent. Because of this, question answering over linked data (QALD) (Walter et al., 2012) has recently received much interest, and most studies on this topic have focused on translating natural language questions into structured queries (Freitas and Curry, 2014; Yahya et al., 2012; Unger et al., 2012; Shekarpour et al., 2013; Yahya et al., 2013; Bao et al., 2014; Zou et al., 2014). For example, with respect to the question “Which software has been developed by organizations founded in California, USA?”, the aim is to automatically convert this utterance into an SPARQL query that contains the following subject-property-object (SPO) triple format: ?url rdf:type dbo:Software, ?url dbo:developer ?x1, ?x1 rdf:type dbo:Company, ?x1 dbo:foundationPlace dbr:California)1. To fulfill this objective, existing systems (Lopez et al., 2006; Unger et al., 2012; Yahya et al., 2012; Zou et al., 2014) usually adopt a pipeline framework that contains </context>
</contexts>
<marker>Yahya, Berberich, Elbassuoni, Weikum, 2013</marker>
<rawString>Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, and Gerhard Weikum. 2013. Robust question answering over the web of linked data. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Information extraction over structured data: Question answering with freebase.</title>
<date>2014</date>
<booktitle>In ACL.</booktitle>
<marker>Yao, Van Durme, 2014</marker>
<rawString>Xuchen Yao and Benjamin Van Durme. 2014. Information extraction over structured data: Question answering with freebase. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Zhang</author>
<author>Jin Qian</author>
<author>Huan Chen</author>
<author>Jihua Kang</author>
<author>Xuanjing Huang</author>
</authors>
<title>Discourse level explanatory relation extraction from product reviews using firstorder logic.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="40239" citStr="Zhang et al. (2013)" startWordPosition="6463" endWordPosition="6466"> method aims to parse a question into a limited logic form with the semantic item query, which we believe is more appropriate for answering factoid questions. Markov Logic Networks have been widely used in NLP tasks. Huang (2012) applied MLN to compress sentences by formulating the task as a word/phrase deletion problem. Fahrni and Strube (2012) jointly disambiguated and clustered concepts using MLN. MLN has also been used in coreference resolution (Song et al., 2012). For the task of identifying subjective text segments and of extracting their corresponding explanations from product reviews, Zhang et al. (2013) modeled these segments with MLN. To discover logical knowledge for deep question answering, Liu (2012) used MLN to resolve the inconsistencies of multiple knowledge bases. Meza-Ruiz and Riedel (2009) employed MLN for Semantic Role Labeling (SRL). They jointly performed the following tasks for a sentence: predicate identification, frame disambiguation, argument identification and argument classification. The semantic analysis of SRL solely rested on the lexical level, but our analysis focuses on the knowledge-base level and aims to obtain an executable query and to support natural language inf</context>
</contexts>
<marker>Zhang, Qian, Chen, Kang, Huang, 2013</marker>
<rawString>Qi Zhang, Jin Qian, Huan Chen, Jihua Kang, and Xuanjing Huang. 2013. Discourse level explanatory relation extraction from product reviews using firstorder logic. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Zou</author>
<author>Ruizhe Huang</author>
<author>Haixun WangZou</author>
<author>Jeffrey Xu Yu</author>
<author>Wenqiang He</author>
<author>Dongyan Zhao</author>
</authors>
<title>Natural language question answering over rdf — a graph data driven approach.</title>
<date>2014</date>
<booktitle>In SIGMOD.</booktitle>
<contexts>
<context position="2619" citStr="Zou et al., 2014" startWordPosition="402" endWordPosition="405">nguages (such as SPARQL) based on their needs, this skill cannot be easily utilized by common users (Christina and Freitas, 2014). Thus, providing user-friendly, simple interfaces to access these linked data becomes increasingly more urgent. Because of this, question answering over linked data (QALD) (Walter et al., 2012) has recently received much interest, and most studies on this topic have focused on translating natural language questions into structured queries (Freitas and Curry, 2014; Yahya et al., 2012; Unger et al., 2012; Shekarpour et al., 2013; Yahya et al., 2013; Bao et al., 2014; Zou et al., 2014). For example, with respect to the question “Which software has been developed by organizations founded in California, USA?”, the aim is to automatically convert this utterance into an SPARQL query that contains the following subject-property-object (SPO) triple format: ?url rdf:type dbo:Software, ?url dbo:developer ?x1, ?x1 rdf:type dbo:Company, ?x1 dbo:foundationPlace dbr:California)1. To fulfill this objective, existing systems (Lopez et al., 2006; Unger et al., 2012; Yahya et al., 2012; Zou et al., 2014) usually adopt a pipeline framework that contains four major steps: 1) decomposing the </context>
<context position="5491" citStr="Zou et al., 2014" startWordPosition="856" endWordPosition="859">refer to the entity California State, and which can be the second argument of the relation foundationPlace, together with a verb phrase being more likely to be mapped to Relation, we should map the phrase founded in to foundationPlace in this question. Thus, we aim to determine if joint disambiguation is better than individual disambiguation. (Question One) In addition, previous systems usually employed manually designed patterns to extract predicateargument structures that are used to guide the disambiguation process in the three steps mentioned above (Yahya et al., 2012; Unger et al., 2012; Zou et al., 2014). For example, (Yahya et al., 2012) used only three dependency patterns to group the mapped semantic items into semantic triples. Nevertheless, these three manually designed patterns miss many cases because of the diversity of the question expressions. We gathered statistics on 144 questions and found that the macro-average F1 and micro-average F1 of the three patterns2 used in (Yahya et al., 2012) are only 62.8 and 66.2%, respectively. Furthermore, these specially designed patterns may not be valid with variations in domains or languages. Therefore, another important question arises: can we a</context>
</contexts>
<marker>Zou, Huang, WangZou, Yu, He, Zhao, 2014</marker>
<rawString>Lei Zou, Ruizhe Huang, Haixun WangZou, Jeffrey Xu Yu, Wenqiang He, and Dongyan Zhao. 2014. Natural language question answering over rdf — a graph data driven approach. In SIGMOD.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
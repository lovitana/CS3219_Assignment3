<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.956622">
Semantic Parsing Using Content and Context:
A Case Study from Requirements Elicitation
</title>
<author confidence="0.74197">
Reut Tsarfaty
</author>
<affiliation confidence="0.469105">
Weizmann Institute
Rehovot, Israel
</affiliation>
<author confidence="0.761093">
Yaarit Natan
</author>
<affiliation confidence="0.524812">
Weizmann Institute
Rehovot, Israel
</affiliation>
<note confidence="0.408592666666667">
Ilia Pogrebezky
Interdisciplinary Center
Herzliya, Israel
</note>
<author confidence="0.668817">
Smadar Szekely
</author>
<affiliation confidence="0.501903">
Weizmann Institute
Rehovot, Israel
</affiliation>
<author confidence="0.663967">
Guy Weiss
</author>
<affiliation confidence="0.425886">
Weizmann Institute
Rehovot, Israel
</affiliation>
<author confidence="0.858582">
David Harel
</author>
<affiliation confidence="0.5583675">
Weizmann Institute
Rehovot, Israel
</affiliation>
<sectionHeader confidence="0.945777" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999992166666667">
We present a model for the automatic se-
mantic analysis of requirements elicitation
documents. Our target semantic repre-
sentation employs live sequence charts, a
multi-modal visual language for scenario-
based programming, which can be directly
translated into executable code. The ar-
chitecture we propose integrates sentence-
level and discourse-level processing in a
generative probabilistic framework for the
analysis and disambiguation of individual
sentences in context. We show empiri-
cally that the discourse-based model con-
sistently outperforms the sentence-based
model when constructing a system that re-
flects all the static (entities, properties) and
dynamic (behavioral scenarios) require-
ments in the document.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997275">
Requirements elicitation is a process whereby a
system analyst gathers information from a stake-
holder about a desired system (software or hard-
ware) to be implemented. The knowledge col-
lected by the analyst may be static, referring to
the conceptual model (the entities, their properties,
the possible values) or dynamic, referring to the
behavior that the system should follow (who does
what to whom, when, how, etc). A stakeholder in-
terested in the system typically has a specific static
and dynamic domain in mind, but he or she cannot
necessarily prescribe any formal models or code
artifacts. The term requirements elicitation we use
here refers to a piece of discourse in natural lan-
guage, by means of which a stakeholder commu-
nicates their desiderata to the system analyst.
The role of a system analyst is to understand
the different requirements and transform them into
formal constructs, formal diagrams or executable
code. Moreover, the analyst needs to consolidate
the different pieces of information to uncover a
single shared domain. Studies in software engi-
neering aim to develop intuitive symbolic systems
with which human agents can encode require-
ments that would then be unambiguously trans-
lated into a formal model (Fuchs and Schwitter,
1995; Bryant and Lee, 2002).
More recently, Gordon and Harel (2009) de-
fined a natural fragment of English that can be
used for specifying requirements which can be
effectively translated into live sequence charts
(LSC) (Damm and Harel, 2001; Harel and
Marelly, 2003), a formal language for specifying
the dynamic behavior of reactive systems. How-
ever, the grammar that underlies this language
fragment is highly ambiguous, and all disam-
biguation has to be conducted manually by a hu-
man agent. Indeed, it is commonly accepted that
the more natural a controlled language fragment
is, the harder it is to develop an unambiguous
translation mechanism (Kuhn, 2014).
In this paper we accept the ambiguity of re-
quirements descriptions as a premise, and aim to
answer the following question: can we automati-
cally recover a formal representation of the com-
plete system — one that best reflects the human-
perceived interpretation of the entire document?
Recent advances in natural language processing,
with an eye to semantic parsing (Zettlemoyer and
Collins, 2005; Liang et al., 2011; Artzi and Zettle-
moyer, 2013; Liang and Potts, 2014), use differ-
ent formalisms and various kinds of learning sig-
nals for statistical semantic parsing. In particu-
lar, the model of Lei et al. (2013) induces input
parsers from format descriptions. However, rarely
do these models take into account the entire docu-
ment’s context.
The key idea we promote here is that discourse
context provides substantial disambiguating infor-
mation for sentence analysis. We suggest a novel
</bodyText>
<page confidence="0.919002">
1296
</page>
<note confidence="0.99085">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1296–1307,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.993704">
Figure 1: An LSC scenario: ”When the user clicks
the button, the display color must change to red.”
</figureCaption>
<bodyText confidence="0.999882">
model for integrated sentence-level and discourse-
level processing, in a joint generative probabilistic
framework. The input for the requirements elici-
tation task is given in a simplified, yet highly am-
biguous, fragment of English, as specified in Gor-
don and Harel (2009). The output, in contrast, is
a sequence of unambiguous and well-formed live
sequence charts (LSC) (Damm and Harel, 2001;
Harel and Marelly, 2003) describing the dynamic
behavior of the system, tied to a single shared
code-base called a system model (SM).
Our solution takes the form of a hidden markov
model (HMM) where emission probabilities re-
flect the grammaticality and interpretability of tex-
tual requirements via a probabilistic grammar and
transition probabilities model the overlap between
SM snapshots of a single, shared, domain. Using
efficient viterbi decoding, we search for the best
sequence of domain snapshots that has most likely
generated the entire requirements document. We
empirically show that such an integrated model
consistently outperforms a sentence-based model
learned from the same set of data.
The remainder of this document is organized as
follows. In Section 2 we describe the task, and
spell out our formal assumptions concerning the
input and the output. In Section 3 we present
our target semantic representation and a specially
tailored notion of grounding for anchoring the
requirements in a code-base. In Section 4 we
develop our sentence-based and discourse-based
models, and in Section 5 we evaluate the models
on various case studies. In Section 6 we discuss
applications and future extensions, and in Sec-
tion 7 we summarize and conclude.
</bodyText>
<sectionHeader confidence="0.971478" genericHeader="introduction">
2 Parsing Requirements Elicitation
Documents: Task Description
</sectionHeader>
<bodyText confidence="0.999980295454546">
There is an inherent discrepancy between the in-
put and the output of the software engineering pro-
cess. The input, system requirements, is specified
in a natural, informal, language. The output, the
system, is ultimately implemented in a formal un-
ambiguous programming language. Can we auto-
matically recover such a formal representation of
a complete system from a set of requirements? In
this work we explore this challenge empirically.
The Input. We assume a scenario-based pro-
gramming paradigm (a.k.a behavioral program-
ming (BP) (Harel et al., 2012)) in which system
development is seen as a process whereby humans
describe the expected behavior of the system by
means of “short-stories”, formally called scenar-
ios (Harel, 2001). We further assume that a given
requirements document describes exactly one sys-
tem, and that each sentence describes a single,
possibly complex, scenario. The requirements we
aim to parse are given in a simplified form of En-
glish (specifically, the English fragment described
in Gordon and Harel (2009)). Contrary to strictly
formal specification languages, which are closed
and unambiguous, this fragment of English em-
ploys an open-ended lexicon and exhibits exten-
sive syntactic and semantic ambiguity.1
The Output. Our target semantic representation
employs live sequence charts (LSC), a diagram-
matic formal language for scenario-based pro-
gramming (Damm and Harel, 2001). Formally,
LSCs are an extension of the well-known UML
message sequence diagrams (Harel and Maoz,
2006), and they have a direct translation into ex-
ecutable code (Harel and Marelly, 2003).2 Using
LSC diagrams for software modelling enjoys the
advantages of being easily learnable (Harel and
Gordon, 2009), intuitively interpretable (Eitan et
al., 2011) and straightforwardly amenable to exe-
cution (Harel et al., 2002) and verification (Harel
et al., 2013). The LSC language is particularly
suited for representing natural language require-
ments, since its basic formal constructs, scenar-
ios, nicely align with events, the primitive objects
of Neo-Davidsonian Semantics (Parsons, 1990).
</bodyText>
<footnote confidence="0.9910142">
1Formally, this variant may be viewed as a CNL of degree
P2 E3 N4 S4 with properties C,F,W,A (Kuhn, 2014, pp 6-12).
2It can be shown that the execution semantics of the LSC
language is embedded in a fragment of a branching temporal
logic called CTL* (Kugler et al., 2005).
</footnote>
<page confidence="0.992127">
1297
</page>
<bodyText confidence="0.98542188">
Live Sequence Charts and Code Artifacts. A
live sequence chart (LSC) is a diagram that de-
scribes a possible or necessary run of a specified
system. In a single LSC diagram, entities are rep-
resented as vertical lines called lifelines, and inter-
actions between entities are represented using hor-
izontal arrows between lifelines called messages,
connecting a sender to a receiver. Messages may
refer to other entities (or properties of entities) as
arguments. Time in LSCs proceeds from top to
bottom, imposing a partial order on the execution
of messages. LSC messages can be hot (red, “must
happen”) or cold (blue, “may happen”). A mes-
sage may have an execution status, which desig-
nates it as monitored (dashed arrow, “wait for”)
or executed (full arrow, “execute”). The LSC lan-
guage also encompasses conditions and control
structures, and it allows defining requirements in
terms of the negation of charts. Figure 1 illustrates
the LSC for the scenario “When the user clicks
the button, the display color must change to red.”.
The respective system model (SM) is a code-base
hierarchy containing the classes USER, BUTTON,
DISPLAY, the method BUTTON.CLICK() and the
property DISPLAY.COLOR.
</bodyText>
<sectionHeader confidence="0.996042" genericHeader="method">
3 Formal Settings
</sectionHeader>
<bodyText confidence="0.990780816326531">
In the text-to-code generation task, we aim to im-
plement a prediction function f : D → M, such
that D ∈ D is a piece of discourse consisting of an
ordered set of requirements D = d1, d2...dn, and
f(D) = M ∈ M is a code-base hierarchy that
grounds the semantic interpretation of D; we de-
note this by M &gt; sem(d1, ..., dn). We now define
the objects D, M, and describe how to construct
the semantic interpretation function (sem(.)). We
then spell out the notion of grounding (�).
Surface Structures: Let Σ be a finite lexicon
and let Lreq ⊆ Σ* be a language for specifying
requirements. We assume the sentences in Lreq
have been generated by a context-free grammar
G = hN, Σ, 5 ∈ N, Ri, where N is a set of non-
terminals, Σ is the aforementioned lexicon, 5 ∈
N is the start symbol and R is a set of context-free
rules {A → α|A ∈ N, α ∈ (N ∪ Σ)*}. For each
utterance u ∈ Lreq, we can find a sequential appli-
cation of rules that generates it: u = r1 ◦ ... ◦ rk;
∀i : ri ∈ R. We call such a sequence a deriva-
tion of u. These derivations may be graphically
depicted as parse trees, where the utterance u de-
fines the sequence of tree terminals in the leaves.
We define Treq to be the set of trees strongly
generated by G, and utilize an auxiliary yield
function yield : Treq → Lreq returning the leaves
of the given tree t ∈ .Lreq. Different parse-trees
can generate the same utterance, so the task of an-
alyzing the structure of an utterance u ∈ Lreq is
modeled via a function syn : Lreq → Treq that
returns the correct, human-perceived, parse of u.
Semantic Structures: Our target semantic rep-
resentation of a requirement d ∈ Lreq is a dia-
grammatic structure called a live sequence chart
(LSC). The LSC formal definition we provide here
is based on the appendix of Harel and Marelly
(2003), but rephrased in set-theoretic, event-based,
terms. We defined this alternative formalization
in order to make LSCs compatible with Neo-
Davidsonian, event-based, semantic theories. As
a result, this form of LSC formalization is well-
suited for representing the semantics of natural
language utterances.
Let us assume that L is a dictionary of entities
(lifelines), A is a dictionary of actions, P is a dic-
tionary of attribute names and V a dictionary of
attribute values. The set of simple events in the
LSC formal system is defined as follows:
</bodyText>
<equation confidence="0.701315">
Eactive ⊂ L × A × L × (L × P × V )*
×{hot, cold} × {executed, monitored}
</equation>
<bodyText confidence="0.999455133333333">
where e = hl1, a, l2, {li : pi : vi}ki=3, temp, exei
and li ∈ L, a ∈ A, pi ∈ P, temp ∈ {hot, cold},
exe ∈ {executed, monitored}. The event e is
called a message in which an action a is carried
over from a sender l1 to a receiver l2.3 The set
{li : pi : vi}ki=3 depicts a set of attribute:value
pairs provided as arguments to action a. The tem-
perature temp marks the modality of the action
(may, must), and the status exe distinguishes ac-
tions to be taken from actions to be waited for.
An event e can also refer to a state, where a
logical expression is being evaluated over a set of
property:value pairs. We call such an event a con-
dition, and specify the set of possible conditions
as follows:
</bodyText>
<equation confidence="0.5120275">
Econd ⊂ Exp × (L × P × V )*
×{hot, cold} × {executed, monitored}
</equation>
<footnote confidence="0.9604915">
3The LSC language also distinguishes static lifelines from
dynamically-bound lifelines. For brevity, we omit this from
the formal description of events, and simply assert that it may
be listed as one of the properties of the relevant lifeline.
</footnote>
<page confidence="0.991273">
1298
</page>
<bodyText confidence="0.999069533333333">
Specifically, e = (exp, {l : p : v}ki=0, temp, exe)
is a condition to be evaluated, where li E L, pi E
P, vi E V, temp E {hot, cold} and exe E
{executed, monitored} are as specified above.
The condition exp E Exp is a first-order logic for-
mula using the usual operators (V, n, —*, , 1, V).
The set {l : p : v}ki=0 depicts a (possibly empty)
set of attribute:value pairs that participates as pred-
icates in exp. Executing a condition, that is, evalu-
ating the logical expression specified by exp, also
has a modality (may/must) and an execution status
(performed/waited for).
The LSC language further allows us to define
more complex events by combining partially or-
dered sets of events with control structures.
</bodyText>
<equation confidence="0.8664375">
Ecomplex C N x Econdx
{(Ec, &lt;)|(Ec, &lt;) is a poset }
</equation>
<bodyText confidence="0.996077">
N is a set of non-negative integers, Econd is a set
of conditions as described above, and each ele-
ment (Ec, &lt;) is a partially ordered set of events.
This structure allows us to derive three kinds of
control structures:
</bodyText>
<listItem confidence="0.875959857142857">
• e = (#, 0, (E, &lt;)) is a loop in which (E, &lt;)
is executed # times.
• e = (0, cond, (E, &lt;)) is a conditioned exe-
cution. If cond holds, (E, &lt;) is executed.
• e = (#, {cond}#i=1, {(Ec,&lt;)}#i=1) is a
switch: in case i, if the condition i holds,
(Ec, &lt;)i is executed.
</listItem>
<bodyText confidence="0.978654">
Definition 1 (LSC) An LSC c = (E, &lt;) is a
partially ordered set of events such that
</bodyText>
<equation confidence="0.67961">
Ve E E : e E Eactive V e E Econd V e E Ecomplex
</equation>
<bodyText confidence="0.9951454">
Grounded Semantics: The information repre-
sented in the LSC provides the recipe for a rig-
orous construction of the code-base that will im-
plement the program. This code-base is said to
ground the semantic representation. For exam-
ple, if our target programming language is an
Object-Oriented programming language such as
Java, then the code-base will include the objects,
the methods and the properties that are minimally
required for executing the scenario that is repre-
sented by the LSC. We refer to this code-base as
a system model (henceforth, SM), and define it as
follows.
Definition 2: (SM) Let Lm be a set of imple-
mented objects, Am a set of implemented meth-
ods, Pm a set of arguments and Vm argument
values. We further define the auxiliary functions
methods : Am —* Lm, props : Pm —* Lm and
values : Vm —* Lm x Pm, for identifying the
entity l E Lm that implements the method a E
Am, the entity l E Lm that contains the property
p E Pm, and the entity property (l, p) E Lm x Pm
that assumes that value v E Vm, respectively. A
system model (SM) is a tuple m representing the
implemented architecture.
</bodyText>
<equation confidence="0.763772">
m = (Lm, Am, Pm, Vm, methods, props, values)
</equation>
<bodyText confidence="0.953318166666667">
Analogously to interpretation functions in logic
and natural language semantics, we assume here
an implementation function, denoted [[.]], which
maps each formal entity in the LSC semantic rep-
resentation to its instantiation in the code-base.
Using this function we define a notion of ground-
ing that captures the fact that a certain code-base
permits the execution of a given LSC c.
Definition 3(a): (Grounding) Let M be the set
of system models and let C be the set of LSC
charts. We say that m grounds c = (E, &lt;), and
write m &gt; c, if Ve E E : m &gt; e, where:
</bodyText>
<listItem confidence="0.947094">
• if e E Eactive then
m &gt; e #&gt;
</listItem>
<equation confidence="0.904586875">
[[l1]],[[l2]] E L &amp;
[[a]] E methods([[l2]]) &amp;
Vi : (l : p : v)i =&gt; [[l]] E Lm&amp;[[p]] E
props[[l]]&amp;v E values([[l]], [[p]])
• if e E Econd then
m &gt; e #&gt;
Vi : (l : p : v)i =&gt; [[l]] E Lm&amp;[[p]] E
props[[l]]&amp;v E values([[l]], [[p]])
</equation>
<listItem confidence="0.894817">
• if e = (#, es, (Ec, &lt;) E Ecomplex then
m &gt; e #&gt; m &gt; es &amp; Ve&apos; E Ec : m &gt; e&apos;
</listItem>
<bodyText confidence="0.999973555555555">
We have thus far defined how the semantics of
a single LSC can be grounded in a single SM. In
the real world, however, a requirements document
typically contains multiple different requirements,
but it is interpreted as a complete whole. The de-
sired SM is then one that represents a single do-
main shared by all the specified requirements. Let
us then assume a document d = d1, ..., dn con-
taining n requirements, where Vi : di E Lreq, and
</bodyText>
<page confidence="0.985172">
1299
</page>
<bodyText confidence="0.981884631578947">
let t be a unification operation that returns the for-
mal unification of two SMs if such exists, and an
empty SM otherwise. We define a discourse in-
terpretation function sem(d) that returns a single
SM for the entire document, where different men-
tions across sentences may share the same refer-
ence. The discourse interpretation function sem
can be as simple as unifying all individual SMs
for di, and asserting that all elements that have the
same name in different SMs refer to a single ele-
ment in the overall SM. Or, it can be as complex as
taking into account synonyms (“clicks the button”
and “presses the button”), anaphora (“when the
user clicks the button, it changes colour”), bind-
ing (“when the user clicks any button, this button
is highlighted”), and so on. We can now define the
grounding of an entire requirements document.
Definition 3(b): (Grounding) Let d = d1...dn
be a requirements document and let m = m1...mn
be a sequence of system models. M = hm, ti is
a sequence of models and a unification operation,
and M &gt; sem(d) if and only if ∀i : mi &gt; sem(di)
and ((m1 t m2).... t mn) &gt; sem(d1,...., dn).
In this work we assume that sem(d) is a simple
discourse interpretation function, where entities,
methods, properties, etc. that are referred to using
the same name in different local SMs refer to a sin-
gle element in the overall code-base. This simple
assumption already carries a substantial amount of
disambiguating information concerning individual
requirements. For example, assume that we have
seen a “click” method over a “button” object in
sentence i. This may help us disambiguate future
attachment ambiguity, favoring structures where
a “button” is attached to “click” over other at-
tachment alternatives. Our goal is then to model
discourse-level context for supporting the accurate
semantic analysis of individual requirements.
</bodyText>
<sectionHeader confidence="0.97654" genericHeader="method">
4 Probabilistic Modeling
</sectionHeader>
<bodyText confidence="0.999982">
In this section we set out to explicitly model
the requirement’s context, formally captured as a
document-level SM, in order to support the accu-
rate disambiguation of the requirements’ content.
We first specify our probabilistic content model,
a sentence-level model which is based on a prob-
abilistic grammar augmented with compositional
semantic rules. We then specify our probabilistic
context model, a document-level sequence model
that takes into account the content as well as the
relation between SMs at different time points.
</bodyText>
<subsectionHeader confidence="0.960602">
4.1 Sentence-Based Modeling
</subsectionHeader>
<bodyText confidence="0.999978533333333">
The task of our sentence-based model is to learn
a function that maps each requirement sentence
to its correct LSC diagram and SM snapshot.
In a nutshell, we do this via a (partially lexi-
calized) probabilistic context-free grammar aug-
mented with a semantic interpretation function.
More formally, given a discourse D = d1...dn
we think of each di as having been generated by
a probabilistic context-free grammar (PCFG) G.
The syntactic analysis of di may be ambiguous,
so we first implement a syntactic analysis function
syn : Lreq → Treq using a probabilistic model
that selects the most likely syntax tree t of each
d individually. We can simplify syn(d), with d
constant with respect to the maximization:
</bodyText>
<equation confidence="0.9930664">
syn(d) = argmaxtETreqP(t|d)
P(t,d)
p(d)
= argmaxtETreqP(t, d)
= argmaxtE{t|tETreq,yield(t)=d}P(t)
</equation>
<bodyText confidence="0.998262857142857">
Because of the context-freeness assumption, it
holds that P(t) = HrEder(t) P(r), where der(t)
returns the rules that derive t. The resulting proba-
bility distribution P : Treq → [0, 1] defines a prob-
abilistic language model over all requirements d ∈
Lreq, i.e., EdELreq &amp;ETreq,yield(t)=d P(t) = 1.
We assume a function sem : T → C mapping
syntactic parse trees to semantic constructs in the
LSC language. Syntactic parse trees are complex
entities, assigning structures to the flat sequences
of words. The principle of compositionality as-
serts that the meaning of a complex syntactic en-
tity is a function of the meaning of its parts and
their mode of combination. Here, the semantics of
a tree t ∈ Treq is derived compositionally from the
interpretation of the rules in the grammar G. We
overload the sem notation to define sem : R → C
as a function assigning rules to LSC constructs
(events or parts of events),4 with ◦ˆ merging the
resulting sets of events. Our sentence-based com-
positional semantics is summarized as:
</bodyText>
<equation confidence="0.999993">
sem(u) = sem(syn(u)) = sem(r1 ◦ ... ◦ rn) =
sem(r1)ˆ◦...ˆ◦sem(rn) = c1ˆ◦...ˆ◦cn = c
</equation>
<bodyText confidence="0.521060142857143">
4Here, it suffices to say that sem maps edges in the
syntax tree to functions in the API of an existing LSC
editor. For example: sem(NP → DET NN) =
fCreateObject(DET.sem,NN.sem). We specify the
function sem in the supplementary materials. The code of
sem is available as part of PlayGo (www.playgo.co).
= argmaxtETreq
</bodyText>
<page confidence="0.91059">
1300
</page>
<bodyText confidence="0.998078">
For a single chart c, one can easily construct an
implementation for every entity, action and prop-
erty in the chart. Then, by design, we get an
SM m such that m &gt; c. To construct the SM of
the entire discourse in the sentence-based model
we simply return f(d1, ..., dn) = Un i=1mi where
∀i : mi &gt; sem(syn(di)) and U unifies different
mentions of the same string to a single element.
</bodyText>
<subsectionHeader confidence="0.815014">
4.2 Discourse-Based Modeling
</subsectionHeader>
<bodyText confidence="0.999963">
We assume a given document D E D and aim to
find the most probable system model M E M that
satisfies the requirements. We assume that M re-
flects a single domain that the stakeholders have in
mind, and we are provided with an ambiguous nat-
ural language evidence, an elicited discourse D, in
which they convey it. We instantiate this view as a
noisy channel model (Shannon, 1948), which pro-
vides the foundation for many NLP applications,
such as speech recognition (Bahl et al., 1983) and
machine translation (Brown et al., 1993).
According to the noisy channel model, when a
signal is received it does not uniquely identify the
message being sent. A probabilistic model is then
used to decode the original message. In our case,
the signal is the discourse and the message is the
overall system model. In formal terms, we want to
find a model M that maximises the following:
</bodyText>
<equation confidence="0.987173">
f(D) = argmaxMEMP(M|D)
</equation>
<bodyText confidence="0.997377611111111">
We can simplify further, using Bayes law, where
D is constant with respect to the maximisation.
We would thus like to estimate two types of prob-
ability distributions, P(M) over the source and
P(D|M) over the channel.
Both M and D are structured objects with com-
plex internal structure. In order to assign prob-
abilities to objects involving such complex struc-
tures it is customary to break them down into sim-
pler, more basic, events. We know that D =
d1, d2, ..., dn is composed of n individual sen-
tences, each representing a certain aspect of the
model M. We assume a sequence of snapshots of
M that correspond to the timestamps 1...n, that is:
m1, m2, ..., mn E M where ∀i : mi &gt; sem(di).
The complete SM is given by the union of the
different snapshots reflected in different require-
ments, i.e., M = LJi mi. We then rephrase:
</bodyText>
<equation confidence="0.99997">
P(M) = P(m1, ..., mn)
P(D|M) = P(d1, ...., dn|m1, ..., mn)
</equation>
<bodyText confidence="0.999819">
These events may be seen as points in a high di-
mensional space. In actuality, they are too com-
plex and would be too hard to estimate directly.
We then define two independence assumptions.
First, we assume that a system model snapshot at
time i depends only on k previous snapshots (a
stationary distribution). Secondly, we assume that
each sentence i depends only on the SM snapshot
at time i. We now get:
</bodyText>
<equation confidence="0.9998995">
P(m1...mn) ≈ fli P(mi|mi−1...mi−k)
P(d1...dn|m1...mn) ≈ fli P(di|mi)
</equation>
<bodyText confidence="0.635852">
Furthermore, assuming bi-gram transitions, our
objective function is now represented as follows:
</bodyText>
<equation confidence="0.996993333333333">
n
f(D) = argmaxMEM P(mi|mi−1)P(di|mi)
i=1
</equation>
<bodyText confidence="0.999825">
Note that m0 may be empty if the system is im-
plemented from scratch, and non-empty if the re-
quirements assume an existing code-base, which
makes p(m1|m0) a non-trivial transition.
</bodyText>
<subsectionHeader confidence="0.993504">
4.3 Training and Decoding
</subsectionHeader>
<bodyText confidence="0.999834555555555">
Our model is in essence a Hidden Markov Model
in which states capture SM snapshots, state-
transition probabilities model transitions between
SM snapshots, and emission probabilities model
the verbal description of each state. To implement
this, we need to implement a decoding algorithm
that searches through all possible state sequences,
and a training algorithm that can automatically
learn the values of the still rather complex param-
</bodyText>
<equation confidence="0.77694125">
eters P(mi|mi−1), P(di|mi) from data.
f(D) = argmaxMEM
� Y J
decoding
</equation>
<bodyText confidence="0.999962">
Training: We assume a supervised training set-
ting in which we are given a set of examples anno-
tated by a human expert. For instance, these can
be requirements an analyst has formulated and en-
coded using an LSC editor, while manually pro-
viding disambiguating information. We are pro-
vided with a set of pairs {Di, MiJni=1 containing n
documents, where each of the pairs in i = 1..n is a
</bodyText>
<equation confidence="0.988124818181818">
f(D) = argmaxMEMP(M|D)
P(D|M)xP(M)
P(D)
= argmaxMEMP(D|M) x P(M)
= argmaxMEM
P(mi|mi−1)P(di|mi)
�
n
i=1
Y
training
</equation>
<page confidence="0.666992">
1301
</page>
<bodyText confidence="0.953799785714286">
tuple set {dij, tij, cij, mij}ni
j=1. For all i, j, it holds
that tij = syn(dij), cij = sem(tij), and mij .
sem(syn(dij)). The union of the ni SM snapshots
yields the entire model LUjmij = Mi, that satisfies
the set of requirements Mi . sem(di1, ..., dini).
(i) Emission Parameters Our emission parame-
ters P(di|mi) represent the probability of a verbal
description of a requirement given an SM snap-
shot which grounds the semantics of the descrip-
tion. A single SM may result from different syn-
tactic derivations. We calculate this probability
by marginalizing over the syntactic trees that are
grounded in the same SM snapshot.
</bodyText>
<equation confidence="0.999645666666667">
P(d, m) EtE{t|yield(t)=d,m.sem(t)} P(t)
=
P(m) EtE{t|tETreq,m.sem(t)} P(t)
</equation>
<bodyText confidence="0.996926733333333">
The probability of P(t) is estimated using a tree-
bank PCFG (Charniak, 1996), based on all pairs
(dij, tij) in the annotated corpus. We estimate
rule probabilities using maximum-likelihood es-
timates, and use simple smoothing for unknown
lexical items, using rare-words distributions.
(ii) Transition Parameters Our transition pa-
rameters P(mi|mi_1) represent the amount of
overlap between the SM snapshots. We look at the
current and the previous system model, and aim
to estimate how likely the current SM is given the
previous one. There are different assumptions that
may underly this probability distribution, reflect-
ing different principles of human communication.
We first define a generic estimator as follows:
</bodyText>
<equation confidence="0.9865015">
Pˆ(mi|mj) = gap(mi, mj)
Emj gap(mi, mj)
</equation>
<bodyText confidence="0.99912725">
where gap(.) quantifies the information sharing
between SM snapshots. Regardless of the im-
plementation of gap, it can be easily shown that
Pˆ is a conditional probability distribution where
</bodyText>
<equation confidence="0.9458385">
E
mj
</equation>
<bodyText confidence="0.9938719">
consider M to be a restricted universe that is con-
sidered be the decoder, as specified shortly.)
We define different gap implementations, re-
flecting different assumptions about the discourse.
Our first assumption here is that different SM
snapshots refer to the same conceptual world, so
there should be a large overlap between them. We
call this the max-overlap assumption. A second
assumption is that, in collaborative communica-
tion, a new requirement will only be stated if it
</bodyText>
<equation confidence="0.986151125">
Transition: gap(mcurr, mprev)
max-overlap|set(mcurr)nse ( prev)|
)
anon —
max-expansion 1 set(mcurr)nset(mprev)
p |set(mprev)Uset(mcurr)|
min-distance 1 − ted(mprev,mcurr)
|set(mprev)|+|set(mcurr)|
</equation>
<tableCaption confidence="0.665684">
Table 1: Quantifying the gap between snapshots.
set(mi) is a set of nodes marked by path to root.
</tableCaption>
<bodyText confidence="0.991831477272727">
provides new information, akin to Grice (1975).
This is the max-expansion assumption. An addi-
tional assumption prefers “easy” transitions over
“hard” ones, this is the min-distance assumption.
The different gap calculations are listed in Table 1.
Decoding An input document contains n re-
quirements. Our decoding algorithm considers the
N-best syntactic analyses for each requirement. At
each time step i = 1...n we assume N, states rep-
resenting the semantics of the N best syntax trees,
retrieved via a CKY chart parser. Thus, setting
N = 1 is equal to a sentence-based model, in
which for each sentence we simply select the most
likely tree according to a probabilistic grammar,
and construct a semantic representation for it.
For each document of length n, we assume that
our entire universe of system models M is com-
posed of N x n SM snapshots, reflecting the N
most-likely analyses of n sentences, as provided
by the probabilistic syntactic model. (As shall be
seen shortly, even with this restricted5 universe ap-
proximating M, our discourse-based model pro-
vides substantial improvements over a sentence-
based model).
Our discourse-based model is an HMM where
each requirement is an observed signal, and each
i = 1..N is a state representing the SM that
grounds the i th best tree. Because of the
Markov independence assumption our setup satis-
fies the optimal subproblem and overlapping prob-
lem properties, and we can use efficient viterbi de-
coding to exhaustively search through the differ-
ent state sequences, and find the most probable
sequence that has generated the sequence of re-
quirements according to our discourse-based prob-
abilistic model.
5This restriction is akin to pseudo-likelihood estimation,
as in Arnold and Strauss (1991). In pseudo-likelihood estima-
tion, instead of normalizing over the entire set of elements,
one uses a subset that reflects only the possible outcomes.
Here, instead of summing SM probabilities over all possible
sentences in the language, we sum up the SM analyses of the
sentences observed in the document only. This estimation
could also be addressed via, e.g., sampling methods.
</bodyText>
<equation confidence="0.9998475">
Pˆ : M x M → [0, 1] and, for all mi, mj, :
Pˆ(mi|mj) = 1. (For efficiency reasons, we
</equation>
<page confidence="0.940904">
1302
</page>
<bodyText confidence="0.999786333333333">
The overall complexity decoding a document
with n sentences of which max length is l, using a
grammar G of size |G |and a fixed N, is given by:
</bodyText>
<equation confidence="0.851616">
O(n x l3 x |G|3 + l2 x N2 x n + n3 x N2)
</equation>
<bodyText confidence="0.994166375">
We can break this expression down as follows: (i)
In O(n x l3 x |G|3) we generate N best trees for
each one of the n requirements, using a CKY chart
(Younger, 1967). (ii) In O(l2 x N2 x n) we create
the universe M based on the N best trees of the
n requirements, and calculate N x N transitions.
(iii) In O((N xn)2xn) = O(N2xn3) we decode
the n x N grid using Viterbi (1967) decoding.
</bodyText>
<sectionHeader confidence="0.998866" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999956090909091">
Goal. We set out to evaluate the accuracy of a se-
mantic parser for requirements documents, in the
two modes of analysis presented above. Our eval-
uation methodology is as standardly assumed in
machine learning and NLP: given a set of anno-
tated examples — that is, given a set of require-
ments documents, where each requirement is an-
notated with its correct LSC representation and
each document is associated with a complete SM
— we partition this set into a training set and a test
set that are disjoint. We train our statistical model
on the examples in the training set and automati-
cally analyze the requirements in the test set. We
then compare the predicted semantic analyses of
the test set with the human-annotated (henceforth,
gold) semantic analyses of this test set, and empir-
ically quantify our prediction accuracy.
Metrics. Our semantic LSC objects have the
form of a tree (reflecting the sequence of nested
events in our scenarios). Therefore, we can use
standard tree evaluation metrics, such as ParseE-
val (Black et al., 1992), to evaluate the accuracy
of the prediction. Overall, we define three metrics
to evaluate the accuracy of the LSC trees:
POS: the POS metric is the percentage of
part-of-speech tags predicted correctly.
LSC-F1: F1 is the harmonic means of the
precision and recall of the predicted tree.
LSC-EM: EM is 1 if the predicted tree is an
exact match to the gold tree, and 0 otherwise.
In the case of SM trees, as opposed to the LSC
trees, we cannot assume identity of the yield be-
tween the gold and parse trees for the same sen-
</bodyText>
<table confidence="0.998428166666667">
System #Scenarios avg sentence length
Phone 21 24.33
WristWatch 15 29.8
Chess 18 15.83
Baby Monitor 14 20
Total 68 22.395
</table>
<tableCaption confidence="0.907338">
Table 2: Seed Gold-Annotated Requirements
</tableCaption>
<table confidence="0.999554">
N=1 POS LSC-F1 LSC-EM SM-TED SM-EM
Gen-Only 85.52 84.40 9.52 84.25 9.52
Gen+Seed 91.59 88.05 14.29 85.17 14.29
</table>
<tableCaption confidence="0.8239745">
Table 3: Sentence-Based modeling: Accuracy re-
sults on the Phone development set.
</tableCaption>
<bodyText confidence="0.997611166666667">
tence,6 so we cannot use ParseEval. Therefore, we
implement a distance-based metrics in the spirit of
Tsarfaty et al. (2012). Then, to evaluate the accu-
racy of the SM, we use two kinds of scores:
SM-TED: TED is the normalized edit dis-
tance between the predicted and gold SM
trees, subtracted from a unity.
SM-EM: EM is 1 if the predicted SM is an
exact match with the gold SM, 0 otherwise.
Data. We have a small seed of correctly anno-
tated requirements-specification case studies that
describe simple reactive systems in the LSC lan-
guage. Each document contains a sequence of
requirements, each of which is annotated with
the correct LSC diagram. The entire program is
grounded in a java implementation. As training
data, we use the case studies provided by Gordon
and Harel (2009). Table 2 lists the case studies and
basic statistics concerning these data.
As our annotated seed is quite small, it is hard to
generalize from it to unseen examples. In particu-
lar, we are not guaranteed to have observed all pos-
sible structures that are theoretically permitted by
the assumed grammar. To cope with this, we cre-
ate a synthetic set of examples using the grammar
of Gordon and Harel (2009) in generation mode,
and randomly generate trees t E Treq.
The grammar we use to generate the synthetic
examples clearly over-generates. That is to say,
it creates many trees that do not have a sound in-
terpretation. In fact, only 3000 our of 10000 gen-
erated examples turn out to have a sound seman-
tic interpretation grounded in an SM. Nonetheless,
these data allow us to smooth the syntactic distri-
butions that are observed in the seed, and increase
the coverage of the grammar learned from it.
</bodyText>
<footnote confidence="0.9998355">
6This is because the LSC trees are predicted bottom up
and the SM trees are predicted top-down.
</footnote>
<page confidence="0.981602">
1303
</page>
<bodyText confidence="0.999951450980392">
Results. Table 3 presents the results for pars-
ing the Phone document, our development set,
with the sentence-based model, varying the train-
ing data. We see that despite the small size of the
seed, adding it to our set if synthetics examples
substantially improves results over a model trained
on synthetic examples only.
In our next experiment, we provide empirical
upper-bounds and lower-bounds for the discourse-
based model. Table 4 presents the results of
the discourse-based model for N &gt; 1 on the
Phone example. Gen-Only presents the results of
the discourse-based model with a PCFG learned
from synthetic trees only, incorporating transitions
obeying the max-overlap assumption. Already
here, we see a mild improvement for N &gt; 1 rel-
ative to the N = 1 results, indicating that even a
weak signal such as the overlap between neighbor-
ing sentences already improves sentence disam-
biguation in context. We next present the results of
an Oracle experiment, where every requirement is
assigned the highest scoring tree in terms of LSC-
F1 with respect to the gold tree, keeping the same
transitions. Again we see that results improve with
N, indicating that the syntactic model alone does
not provide optimal disambiguation. These re-
sults provides an upper bound on the parser perfor-
mance for each N. Gen+Seed presents results of
the discourse-based model where the PCFG inter-
polates the seed set and the synthetic train set, with
max-overlap transitions. Here, we see larger im-
provements over the synthetic-only PCFG. That is,
modeling grammaticality of individual sentences
improves the interpretation of the document.
Next we compare the performance for differ-
ent implementations of the gap(mi, mj) function.
We estimate probability distributions that reflect
each of the assumptions we discussed, and add
an additional method called hybrid, in which we
interpolate the max-expansion and max-overlap
estimates (equal weights). In Table 5, the trend
from the previous experiment persists. Notably,
the hybrid model provides a larger error reduc-
tion than its components used separately, indicat-
ing that in order to capture discourse context we
may need to balance possibly conflicting factors.
In no emissions we rely solely on the probability
of state transitions, and again increasing N leads
to improvement. This result confirms that con-
text is indispensable for sentence interpretation —
even when probabilities for the sentence’s seman-
</bodyText>
<table confidence="0.999839315789474">
System N=2 4 8 16 32 64 128
Gen-Only
POS 85.52 86.30 87.67 88.45 88.85 88.85 88.85
LSC-F1 84.40 85.35 86.31 87.51 88.81 89.30 89.51
LSC-EM 9.52 9.52 14.29 14.29 14.29 14.29 14.29
SM-TED 84.25 85.94 89.14 91.90 92.81 93.31 92.70
SM-EM 9.52 19.05 33.33 33.33 33.33 38.10 33.33
Gen+Seed
POS 91.78 92.95 93.54 93.35 94.32 94.52 93.93
LSC-F1 88.11 90.18 91.00 90.99 91.81 92.09 91.73
LSC-EM 19.05 38.10 42.86 42.86 42.86 42.86 42.86
SM-TED 85.49 90.78 93.59 93.02 94.81 95.69 93.76
SM-EM 19.05 38.10 52.38 52.38 52.38 52.38 52.38
Oracle
POS 91.98 93.54 94.91 95.30 96.09 96.67 96.87
LSC-F1 88.73 91.33 93.19 94.39 95.11 95.91 96.70
LSC-EM 23.81 42.86 61.90 61.90 66.67 76.19 76.19
SM-TED 86.54 91.28 94.28 94.88 96.24 97.51 98.80
SM-EM 23.81 42.86 66.67 71.43 76.19 76.19 76.19
</table>
<tableCaption confidence="0.989874">
Table 4: Discourse-Based Modeling: Accuracy re-
</tableCaption>
<bodyText confidence="0.99595">
sults on the Phone dev set. The Oracle selects the
highest scoring LSC tree among the N-candidates,
providing an upper bound on accuracy. Gen-Only
selects the most probable tree, relying on synthetic
examples only, providing a lower bound.
tics (content) are entirely absent.
We finally perform a cross-fold experiment in
which we leave one document out as a test set
and take the rest as our seed. The results are pro-
vided in Table 6. The discourse-based model out-
performs the sentence-based model N = 1 in all
cases. Moreover, the drop in N = 128 for Phone
seems incidental to this set, and the other cases
level off beforehand. Despite our small seed, the
persistent improvement on all metrics is consistent
with our hypothesis that modeling the interpreta-
tion process within the discourse has substantial
benefits for automatic understanding of the text.
</bodyText>
<sectionHeader confidence="0.990324" genericHeader="evaluation">
6 Applications and Discussion
</sectionHeader>
<bodyText confidence="0.999834357142857">
The statistical models we present here are ap-
plied in the context of PlayGo,7 a comprehensive
tool for behavioral, scenario-based, programming.
PlayGo now provides two modes of playing-in
natural language requirements: interactive play-in,
where a user manually disambiguates the analyses
of the requirements (Gordon and Harel, 2009), and
statistical play-in, where disambiguation decisions
are taken using our discourse-based model.
The fragment of English we use is very ex-
pressive. It covers not only entities and predi-
cates, but also temporal and aspectual information,
modalities, and program flow. Beyond that, we as-
sume an open-ended lexicon. Overall, we are not
</bodyText>
<footnote confidence="0.951791">
7www.playgo.co.
</footnote>
<page confidence="0.95919">
1304
</page>
<table confidence="0.999836303571428">
Transitions N=2 4 8 16 32 64 128
Min Dist
POS 91.98 92.76 93.54 93.35 94.32 94.52 93.93
LSC-F1 88.39 89.77 91.00 90.99 91.81 92.09 91.73
LSC-EM 23.81 42.86 47.62 47.62 47.62 47.62 47.62
SM-TED 86.54 91.71 94.38 93.81 95.57 96.43 94.53
SM-EM 23.81 42.86 57.14 57.14 57.14 57.14 57.14
Max Overlap
POS 91.78 92.95 93.54 93.35 94.32 94.52 93.93
LSC-F1 88.11 90.18 91.00 90.99 91.81 92.09 91.73
LSC-EM 19.05 38.10 42.86 42.86 42.86 42.86 42.86
SM-TED 85.49 90.78 93.59 93.02 94.81 95.69 93.76
SM-EM 19.05 38.10 52.38 52.38 52.38 52.38 52.38
Max Expand
POS 91.98 92.76 93.74 93.54 94.32 94.52 93.93
LSC-F1 88.39 89.71 91.00 90.99 91.68 91.96 91.60
LSC-EM 23.81 42.86 47.62 47.62 47.62 47.62 47.62
SM-TED 86.54 91.93 93.75 93.18 94.79 95.66 93.75
SM-EM 23.81 42.86 57.14 57.14 57.14 57.14 57.14
Hybrid
POS 91.78 92.95 93.93 93.74 94.72 94.91 94.32
LSC-F1 88.11 90.18 91.34 91.33 92.15 92.42 92.07
LSC-EM 19.05 38.10 47.62 47.62 47.62 47.62 47.62
SM-TED 85.49 90.78 93.66 93.09 94.87 95.75 93.83
SM-EM 19.05 38.10 57.14 57.14 57.14 57.14 57.14
No Emissions
POS 91.78 91.98 92.37 92.37 92.17 92.76 93.15
LSC-F1 88.11 88.79 89.12 89.12 89.39 89.67 89.89
LSC-EM 19.05 19.05 23.81 23.81 23.81 23.81 23.81
SM-TED 85.49 85.74 85.82 85.82 85.87 86.85 86.92
SM-EM 19.05 19.05 23.81 23.81 23.81 23.81 23.81
Data Set N=1 32 64 128
Baby Monitor
POS 94.29 96.07 96.07 96.07
LSC-F1 91.50 94.96 94.96 94.96
LSC-EM 14.29 21.43 21.43 21.43
SM-TED 88.63 91.11 91.11 91.11
SM-EM 28.57 50.00 50.00 50.00
Chess
POS 92.63 93.68 93.68 93.68
LSC-F1 95.79 96.16 96.16 96.16
LSC-EM 5.56 11.11 11.11 11.11
SM-TED 94.90 97.10 97.10 97.10
SM-EM 61.11 66.67 66.67 66.67
Phone
POS 91.59 94.72 94.91 94.32
LSC-F1 88.05 92.15 92.42 92.07
LSC-EM 14.29 47.62 47.62 47.62
SM-TED 85.17 94.87 95.75 93.83
SM-EM 14.29 57.14 57.14 57.14
WristWatch
POS 34.23 34.45 34.45 34.45
LSC-F1 50.06 51.05 51.05 51.05
LSC-EM 26.67 26.67 26.67 26.67
SM-TED 71.15 72.73 72.73 72.73
SM-EM 26.67 33.33 33.33 33.33
</table>
<tableCaption confidence="0.8327205">
Table 6: Cross-Fold Validation for N=1..128.
Seed+Generated emissions, Hybrid transitions.
</tableCaption>
<sectionHeader confidence="0.776904" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.986345620689655">
Table 5: Discourse-Based modeling: Experiments
on the Phone development set. Estimation proce-
dure for transition probabilities. All experiments
use the Gen+Seed emission probablities.
only translating English sentences into executable
LSCs — we provide a fully generative model for
translating a complete document (text) into a com-
plete system model (code).
This text-to-code problem may be thought of as
a machine translation (MT) problem, where one
aims to translate sentences in English to the formal
language of LSCs. However, standard statistical
MT techniques rely on the assumption that textual
requirements and code are aligned at a sentence
level. Creating a formal model that aligns text and
code on a sentence-by-sentence basis is precisely
our technical contribution in Section 3.
To our knowledge, modeling syntax and dis-
course processing via a fully joint generative
model, where a discourse level HMM is in-
terleaved with PCFG sentence-based emissions,
is novel. By plugging in different models for
p(dlm), different languages may be parsed. This
method may further be utilized for relating content
and context in other tasks: parsing and document-
level NER, parsing and document-level IE, etc. To
do so, one only needs to redefine the PCFG (emis-
sions) and state-overlap (transition) parameters, as
appropriate for their data.8
</bodyText>
<footnote confidence="0.711655">
8Our code, annotated data, four case studies, and the LSC
</footnote>
<bodyText confidence="0.999932647058824">
The requirements understanding task presents an
exciting challenge for CL/NLP. We ought to au-
tomatically discover the entities in the discourse,
the actions they take, conditions, temporal con-
straints, and execution modalities. Furthermore, it
requires us to extract a single ontology that satis-
fies all individual requirements. The contributions
of this paper are three-fold: we formalize the text-
to-code prediction task, propose a semantic rep-
resentation with well-defined grounding, and em-
pirically evaluate models for this prediction. We
show consistent improvement of discourse-based
over sentence-based models, in all case studies.
In the future, we intend to extend this model for
interpreting requirements in un-restricted, or less-
restricted, English, endowed with a more sophisti-
cated discourse interpretation function.
</bodyText>
<sectionHeader confidence="0.996522" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.993088909090909">
We thank Shahar Maoz, Rami Marelly, Yoav
Goldberg and three anonymous reviewers for
their insightful comments on an earlier draft.
This research was supported by an Advanced
Research Grant to D. Harel from the Euro-
pean Research Council (ERC) under the Eu-
ropean Community‘s Seventh Framework Pro-
gramme (FP7/2007-2013), and by a grant to D.
Harel from the Israel Science Foundation (ISF).
visual editor are available via http://wiki.weizmann.
ac.il/playgo/index.php/Download_PlayGo.
</bodyText>
<page confidence="0.98816">
1305
</page>
<sectionHeader confidence="0.993629" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999859355769231">
B. C. Arnold and D. Strauss. 1991. Pseudolikelihood
Estimation: Some Examples. Sankhy¯a: The Indian
Journal of Statistics, Series B (1960-2002), 53(2).
Y. Artzi and L. Zettlemoyer. 2013. Weakly super-
vised learning of semantic parsers for mapping in-
structions to actions. TACL, 1:49–62.
L. R. Bahl, F. Jelinek, and R. L. Mercer. 1983. A
maximum likelihood approach to continuous speech
recognition. IEEE Trans. Pattern Anal. Mach. In-
tell., 5(2):179–190.
E. Black, J. D. Lafferty, and S. Roukos. 1992. De-
velopment and evaluation of a broad-coverage prob-
abilistic grammar of English-language computer
manuals. In Proceedings of ACL, pages 185–192.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Comput.
Linguist., 19(2):263–311, June.
B. Bryant and B.-S. Lee. 2002. Two-level gram-
mar as an object-oriented requirements specifica-
tion language. In Proceedings of the 35th Annual
Hawaii International Conference on System Sci-
ences (HICSS’02)-Volume 9 - Volume 9, HICSS ’02,
pages 280–, Washington, DC, USA. IEEE Computer
Society.
E. Charniak. 1996. Tree-bank grammars. In Proceed-
ings of the Thirteenth National Conference on Arti-
ficial Intelligence, pages 1031–1036.
W. Damm and D. Harel. 2001. LSCs: Breathing life
into message sequence charts. Form. Methods Syst.
Des., 19(1):45–80, July.
N. Eitan, M. Gordon, D. Harel, A. Marron, and
G. Weiss. 2011. On visualization and compre-
hension of scenario-based programs. In Proceed-
ings of the 2011 IEEE 19th International Conference
on Program Comprehension, ICPC ’11, pages 189–
192, Washington, DC, USA. IEEE Computer Soci-
ety.
N. E. Fuchs and R. Schwitter. 1995. Attempto: Con-
trolled natural language for requirements specifica-
tions. In Markus P. J. Fromherz, Marc Kirschen-
baum, and Anthony J. Kusalik, editors, LPE.
M. Gordon and D. Harel. 2009. Generating executable
scenarios from natural language. In Proceedings of
the 10th International Conference on Computational
Linguistics and Intelligent Text Processing, CICLing
’09, pages 456–467, Berlin, Heidelberg. Springer-
Verlag.
H. P. Grice. 1975. Logic and conversation. In P. Cole
and J. L. Morgan, editors, Syntax and Semantics:
Vol. 3: Speech Acts, pages 41–58. Academic Press,
San Diego, CA.
D. Harel and M. Gordon. 2009. On teaching visual
formalisms. IEEE Softw., 26(3):87–95, May.
D. Harel and S. Maoz. 2006. Assert and negate revis-
ited: Modal semantics for UML sequence diagrams.
In Proceedings of the 2006 International Workshop
on Scenarios and State Machines: Models, Algo-
rithms, and Tools, SCESM ’06, pages 13–20, New
York, NY, USA. ACM.
D. Harel and R. Marelly. 2003. Come, Let’s Play:
Scenario-Based Programming Using LSCs and the
Play-Engine. Springer-Verlag New York, Inc., Se-
caucus, NJ, USA.
D. Harel, H. Kugler, R. Marelly, and A. Pnueli. 2002.
Smart play-out of behavioral requirements. In Pro-
ceedings of the 4th International Conference on For-
mal Methods in Computer-Aided Design, FMCAD
’02, pages 378–398, London, UK. Springer-Verlag.
D. Harel, A. Marron, and G. Weiss. 2012. Behavioral
programming. Commun. ACM, 55(7):90–100, July.
D. Harel, A. Kantor, G. Katz, A. Marron, L. Mizrahi,
and G. Weiss. 2013. On composing and proving
the correctness of reactive behavior. In Embedded
Software (EMSOFT), 2013 Proceedings of the Inter-
national Conference on, pages 1–10, Sept.
D. Harel. 2001. From play-in scenarios to code: An
achievable dream. Computer, 34(1):53–60, January.
H. Kugler, D. Harel, A. Pnueli, Y. Lu, and Y. Bon-
temps. 2005. Temporal logic for scenario-based
specifications. In Proceedings of the 11th In-
ternational Conference on Tools and Algorithms
for the Construction and Analysis of Systems,
TACAS’05, pages 445–460, Berlin, Heidelberg.
Springer-Verlag.
T. Kuhn. 2014. A survey and classification of con-
trolled natural languages. Computational Linguis-
tics, 40(1):121–170.
T. Lei, F. Long, R. Barzilay, and M. C. Rinard. 2013.
From natural language specifications to program in-
put parsers. In ACL (1), pages 1294–1303.
P. Liang and C. Potts. 2014. Bringing machine learn-
ing and compositional semantics together. Annual
Reviews of Linguistics (submitted), 0.
P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Association for Computational Linguistics (ACL),
pages 590–599.
T. Parsons. 1990. Events in the Semantics of English:
A study in subatomic semantics. MIT Press, Cam-
bridge, MA.
C. Shannon. 1948. A mathematical theory of com-
munication. Bell System Technical Journal, 27:379–
423, 623–656, July, October.
</reference>
<page confidence="0.819722">
1306
</page>
<reference confidence="0.998762466666667">
R. Tsarfaty, J. Nivre, and E. Andersson. 2012. Cross-
framework evaluation for statistical parsing. In
W. Daelemans, M. Lapata, and L. M`arquez, editors,
Proceedings of EACL, pages 44–54. The Associa-
tion for Computer Linguistics.
A. Viterbi. 1967. Error bounds for convolutional codes
and an asymptotically optimum decoding algorithm.
IEEE Trans. Inf. Theor.
D. H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189–208.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In UAI,
pages 658–666. AUAI Press.
</reference>
<page confidence="0.993787">
1307
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.010549">
<title confidence="0.934767">Semantic Parsing Using Content and A Case Study from Requirements Elicitation Reut</title>
<author confidence="0.365064">Weizmann</author>
<address confidence="0.963983">Rehovot, Israel</address>
<email confidence="0.915143">Yaarit</email>
<affiliation confidence="0.618938">Weizmann</affiliation>
<address confidence="0.983422">Rehovot, Israel</address>
<email confidence="0.590227">Ilia</email>
<affiliation confidence="0.884103">Interdisciplinary</affiliation>
<address confidence="0.995256">Herzliya, Israel</address>
<email confidence="0.819282">Smadar</email>
<affiliation confidence="0.525186">Weizmann</affiliation>
<address confidence="0.932845">Rehovot, Israel</address>
<author confidence="0.8943">Guy</author>
<affiliation confidence="0.81943">Weizmann</affiliation>
<address confidence="0.984759">Rehovot, Israel</address>
<author confidence="0.434017">David</author>
<affiliation confidence="0.671718">Weizmann</affiliation>
<address confidence="0.982534">Rehovot, Israel</address>
<abstract confidence="0.99460347368421">We present a model for the automatic semantic analysis of requirements elicitation documents. Our target semantic repreemploys sequence a multi-modal visual language for scenariobased programming, which can be directly translated into executable code. The architecture we propose integrates sentencelevel and discourse-level processing in a generative probabilistic framework for the analysis and disambiguation of individual sentences in context. We show empirically that the discourse-based model consistently outperforms the sentence-based model when constructing a system that reflects all the static (entities, properties) and dynamic (behavioral scenarios) requirements in the document.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B C Arnold</author>
<author>D Strauss</author>
</authors>
<title>Pseudolikelihood Estimation: Some Examples.</title>
<date>1991</date>
<journal>Sankhy¯a: The Indian Journal of Statistics, Series B</journal>
<contexts>
<context position="29654" citStr="Arnold and Strauss (1991)" startWordPosition="4997" endWordPosition="5000">l). Our discourse-based model is an HMM where each requirement is an observed signal, and each i = 1..N is a state representing the SM that grounds the i th best tree. Because of the Markov independence assumption our setup satisfies the optimal subproblem and overlapping problem properties, and we can use efficient viterbi decoding to exhaustively search through the different state sequences, and find the most probable sequence that has generated the sequence of requirements according to our discourse-based probabilistic model. 5This restriction is akin to pseudo-likelihood estimation, as in Arnold and Strauss (1991). In pseudo-likelihood estimation, instead of normalizing over the entire set of elements, one uses a subset that reflects only the possible outcomes. Here, instead of summing SM probabilities over all possible sentences in the language, we sum up the SM analyses of the sentences observed in the document only. This estimation could also be addressed via, e.g., sampling methods. Pˆ : M x M → [0, 1] and, for all mi, mj, : Pˆ(mi|mj) = 1. (For efficiency reasons, we 1302 The overall complexity decoding a document with n sentences of which max length is l, using a grammar G of size |G |and a fixed </context>
</contexts>
<marker>Arnold, Strauss, 1991</marker>
<rawString>B. C. Arnold and D. Strauss. 1991. Pseudolikelihood Estimation: Some Examples. Sankhy¯a: The Indian Journal of Statistics, Series B (1960-2002), 53(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions.</title>
<date>2013</date>
<tech>TACL,</tech>
<pages>1--49</pages>
<contexts>
<context position="3480" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="525" endWordPosition="529">by a human agent. Indeed, it is commonly accepted that the more natural a controlled language fragment is, the harder it is to develop an unambiguous translation mechanism (Kuhn, 2014). In this paper we accept the ambiguity of requirements descriptions as a premise, and aim to answer the following question: can we automatically recover a formal representation of the complete system — one that best reflects the humanperceived interpretation of the entire document? Recent advances in natural language processing, with an eye to semantic parsing (Zettlemoyer and Collins, 2005; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Liang and Potts, 2014), use different formalisms and various kinds of learning signals for statistical semantic parsing. In particular, the model of Lei et al. (2013) induces input parsers from format descriptions. However, rarely do these models take into account the entire document’s context. The key idea we promote here is that discourse context provides substantial disambiguating information for sentence analysis. We suggest a novel 1296 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1296–1307, October 25-29, 2014, Doha, Qatar. c�201</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Y. Artzi and L. Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. TACL, 1:49–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Bahl</author>
<author>F Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>A maximum likelihood approach to continuous speech recognition.</title>
<date>1983</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="22351" citStr="Bahl et al., 1983" startWordPosition="3803" endWordPosition="3806">=1mi where ∀i : mi &gt; sem(syn(di)) and U unifies different mentions of the same string to a single element. 4.2 Discourse-Based Modeling We assume a given document D E D and aim to find the most probable system model M E M that satisfies the requirements. We assume that M reflects a single domain that the stakeholders have in mind, and we are provided with an ambiguous natural language evidence, an elicited discourse D, in which they convey it. We instantiate this view as a noisy channel model (Shannon, 1948), which provides the foundation for many NLP applications, such as speech recognition (Bahl et al., 1983) and machine translation (Brown et al., 1993). According to the noisy channel model, when a signal is received it does not uniquely identify the message being sent. A probabilistic model is then used to decode the original message. In our case, the signal is the discourse and the message is the overall system model. In formal terms, we want to find a model M that maximises the following: f(D) = argmaxMEMP(M|D) We can simplify further, using Bayes law, where D is constant with respect to the maximisation. We would thus like to estimate two types of probability distributions, P(M) over the sourc</context>
</contexts>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>L. R. Bahl, F. Jelinek, and R. L. Mercer. 1983. A maximum likelihood approach to continuous speech recognition. IEEE Trans. Pattern Anal. Mach. Intell., 5(2):179–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>J D Lafferty</author>
<author>S Roukos</author>
</authors>
<title>Development and evaluation of a broad-coverage probabilistic grammar of English-language computer manuals.</title>
<date>1992</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>185--192</pages>
<contexts>
<context position="31747" citStr="Black et al., 1992" startWordPosition="5380" endWordPosition="5383">ete SM — we partition this set into a training set and a test set that are disjoint. We train our statistical model on the examples in the training set and automatically analyze the requirements in the test set. We then compare the predicted semantic analyses of the test set with the human-annotated (henceforth, gold) semantic analyses of this test set, and empirically quantify our prediction accuracy. Metrics. Our semantic LSC objects have the form of a tree (reflecting the sequence of nested events in our scenarios). Therefore, we can use standard tree evaluation metrics, such as ParseEval (Black et al., 1992), to evaluate the accuracy of the prediction. Overall, we define three metrics to evaluate the accuracy of the LSC trees: POS: the POS metric is the percentage of part-of-speech tags predicted correctly. LSC-F1: F1 is the harmonic means of the precision and recall of the predicted tree. LSC-EM: EM is 1 if the predicted tree is an exact match to the gold tree, and 0 otherwise. In the case of SM trees, as opposed to the LSC trees, we cannot assume identity of the yield between the gold and parse trees for the same senSystem #Scenarios avg sentence length Phone 21 24.33 WristWatch 15 29.8 Chess 1</context>
</contexts>
<marker>Black, Lafferty, Roukos, 1992</marker>
<rawString>E. Black, J. D. Lafferty, and S. Roukos. 1992. Development and evaluation of a broad-coverage probabilistic grammar of English-language computer manuals. In Proceedings of ACL, pages 185–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>S A Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="22396" citStr="Brown et al., 1993" startWordPosition="3810" endWordPosition="3813">ies different mentions of the same string to a single element. 4.2 Discourse-Based Modeling We assume a given document D E D and aim to find the most probable system model M E M that satisfies the requirements. We assume that M reflects a single domain that the stakeholders have in mind, and we are provided with an ambiguous natural language evidence, an elicited discourse D, in which they convey it. We instantiate this view as a noisy channel model (Shannon, 1948), which provides the foundation for many NLP applications, such as speech recognition (Bahl et al., 1983) and machine translation (Brown et al., 1993). According to the noisy channel model, when a signal is received it does not uniquely identify the message being sent. A probabilistic model is then used to decode the original message. In our case, the signal is the discourse and the message is the overall system model. In formal terms, we want to find a model M that maximises the following: f(D) = argmaxMEMP(M|D) We can simplify further, using Bayes law, where D is constant with respect to the maximisation. We would thus like to estimate two types of probability distributions, P(M) over the source and P(D|M) over the channel. Both M and D a</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Comput. Linguist., 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bryant</author>
<author>B-S Lee</author>
</authors>
<title>Two-level grammar as an object-oriented requirements specification language.</title>
<date>2002</date>
<booktitle>In Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS’02)-Volume 9 - Volume 9, HICSS ’02,</booktitle>
<pages>280</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="2405" citStr="Bryant and Lee, 2002" startWordPosition="353" endWordPosition="356"> of discourse in natural language, by means of which a stakeholder communicates their desiderata to the system analyst. The role of a system analyst is to understand the different requirements and transform them into formal constructs, formal diagrams or executable code. Moreover, the analyst needs to consolidate the different pieces of information to uncover a single shared domain. Studies in software engineering aim to develop intuitive symbolic systems with which human agents can encode requirements that would then be unambiguously translated into a formal model (Fuchs and Schwitter, 1995; Bryant and Lee, 2002). More recently, Gordon and Harel (2009) defined a natural fragment of English that can be used for specifying requirements which can be effectively translated into live sequence charts (LSC) (Damm and Harel, 2001; Harel and Marelly, 2003), a formal language for specifying the dynamic behavior of reactive systems. However, the grammar that underlies this language fragment is highly ambiguous, and all disambiguation has to be conducted manually by a human agent. Indeed, it is commonly accepted that the more natural a controlled language fragment is, the harder it is to develop an unambiguous tr</context>
</contexts>
<marker>Bryant, Lee, 2002</marker>
<rawString>B. Bryant and B.-S. Lee. 2002. Two-level grammar as an object-oriented requirements specification language. In Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS’02)-Volume 9 - Volume 9, HICSS ’02, pages 280–, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Tree-bank grammars.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence,</booktitle>
<pages>1031--1036</pages>
<contexts>
<context position="26260" citStr="Charniak, 1996" startWordPosition="4470" endWordPosition="4471"> the entire model LUjmij = Mi, that satisfies the set of requirements Mi . sem(di1, ..., dini). (i) Emission Parameters Our emission parameters P(di|mi) represent the probability of a verbal description of a requirement given an SM snapshot which grounds the semantics of the description. A single SM may result from different syntactic derivations. We calculate this probability by marginalizing over the syntactic trees that are grounded in the same SM snapshot. P(d, m) EtE{t|yield(t)=d,m.sem(t)} P(t) = P(m) EtE{t|tETreq,m.sem(t)} P(t) The probability of P(t) is estimated using a treebank PCFG (Charniak, 1996), based on all pairs (dij, tij) in the annotated corpus. We estimate rule probabilities using maximum-likelihood estimates, and use simple smoothing for unknown lexical items, using rare-words distributions. (ii) Transition Parameters Our transition parameters P(mi|mi_1) represent the amount of overlap between the SM snapshots. We look at the current and the previous system model, and aim to estimate how likely the current SM is given the previous one. There are different assumptions that may underly this probability distribution, reflecting different principles of human communication. We firs</context>
</contexts>
<marker>Charniak, 1996</marker>
<rawString>E. Charniak. 1996. Tree-bank grammars. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 1031–1036.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Damm</author>
<author>D Harel</author>
</authors>
<title>LSCs: Breathing life into message sequence charts.</title>
<date>2001</date>
<journal>Form. Methods Syst. Des.,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="2618" citStr="Damm and Harel, 2001" startWordPosition="387" endWordPosition="390">to formal constructs, formal diagrams or executable code. Moreover, the analyst needs to consolidate the different pieces of information to uncover a single shared domain. Studies in software engineering aim to develop intuitive symbolic systems with which human agents can encode requirements that would then be unambiguously translated into a formal model (Fuchs and Schwitter, 1995; Bryant and Lee, 2002). More recently, Gordon and Harel (2009) defined a natural fragment of English that can be used for specifying requirements which can be effectively translated into live sequence charts (LSC) (Damm and Harel, 2001; Harel and Marelly, 2003), a formal language for specifying the dynamic behavior of reactive systems. However, the grammar that underlies this language fragment is highly ambiguous, and all disambiguation has to be conducted manually by a human agent. Indeed, it is commonly accepted that the more natural a controlled language fragment is, the harder it is to develop an unambiguous translation mechanism (Kuhn, 2014). In this paper we accept the ambiguity of requirements descriptions as a premise, and aim to answer the following question: can we automatically recover a formal representation of </context>
<context position="4614" citStr="Damm and Harel, 2001" startWordPosition="701" endWordPosition="704"> Language Processing (EMNLP), pages 1296–1307, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: An LSC scenario: ”When the user clicks the button, the display color must change to red.” model for integrated sentence-level and discourselevel processing, in a joint generative probabilistic framework. The input for the requirements elicitation task is given in a simplified, yet highly ambiguous, fragment of English, as specified in Gordon and Harel (2009). The output, in contrast, is a sequence of unambiguous and well-formed live sequence charts (LSC) (Damm and Harel, 2001; Harel and Marelly, 2003) describing the dynamic behavior of the system, tied to a single shared code-base called a system model (SM). Our solution takes the form of a hidden markov model (HMM) where emission probabilities reflect the grammaticality and interpretability of textual requirements via a probabilistic grammar and transition probabilities model the overlap between SM snapshots of a single, shared, domain. Using efficient viterbi decoding, we search for the best sequence of domain snapshots that has most likely generated the entire requirements document. We empirically show that suc</context>
<context position="7341" citStr="Damm and Harel, 2001" startWordPosition="1120" endWordPosition="1123">nt describes exactly one system, and that each sentence describes a single, possibly complex, scenario. The requirements we aim to parse are given in a simplified form of English (specifically, the English fragment described in Gordon and Harel (2009)). Contrary to strictly formal specification languages, which are closed and unambiguous, this fragment of English employs an open-ended lexicon and exhibits extensive syntactic and semantic ambiguity.1 The Output. Our target semantic representation employs live sequence charts (LSC), a diagrammatic formal language for scenario-based programming (Damm and Harel, 2001). Formally, LSCs are an extension of the well-known UML message sequence diagrams (Harel and Maoz, 2006), and they have a direct translation into executable code (Harel and Marelly, 2003).2 Using LSC diagrams for software modelling enjoys the advantages of being easily learnable (Harel and Gordon, 2009), intuitively interpretable (Eitan et al., 2011) and straightforwardly amenable to execution (Harel et al., 2002) and verification (Harel et al., 2013). The LSC language is particularly suited for representing natural language requirements, since its basic formal constructs, scenarios, nicely al</context>
</contexts>
<marker>Damm, Harel, 2001</marker>
<rawString>W. Damm and D. Harel. 2001. LSCs: Breathing life into message sequence charts. Form. Methods Syst. Des., 19(1):45–80, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Eitan</author>
<author>M Gordon</author>
<author>D Harel</author>
<author>A Marron</author>
<author>G Weiss</author>
</authors>
<title>On visualization and comprehension of scenario-based programs.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 IEEE 19th International Conference on Program Comprehension, ICPC ’11,</booktitle>
<pages>189--192</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="7693" citStr="Eitan et al., 2011" startWordPosition="1173" endWordPosition="1176"> of English employs an open-ended lexicon and exhibits extensive syntactic and semantic ambiguity.1 The Output. Our target semantic representation employs live sequence charts (LSC), a diagrammatic formal language for scenario-based programming (Damm and Harel, 2001). Formally, LSCs are an extension of the well-known UML message sequence diagrams (Harel and Maoz, 2006), and they have a direct translation into executable code (Harel and Marelly, 2003).2 Using LSC diagrams for software modelling enjoys the advantages of being easily learnable (Harel and Gordon, 2009), intuitively interpretable (Eitan et al., 2011) and straightforwardly amenable to execution (Harel et al., 2002) and verification (Harel et al., 2013). The LSC language is particularly suited for representing natural language requirements, since its basic formal constructs, scenarios, nicely align with events, the primitive objects of Neo-Davidsonian Semantics (Parsons, 1990). 1Formally, this variant may be viewed as a CNL of degree P2 E3 N4 S4 with properties C,F,W,A (Kuhn, 2014, pp 6-12). 2It can be shown that the execution semantics of the LSC language is embedded in a fragment of a branching temporal logic called CTL* (Kugler et al., 2</context>
</contexts>
<marker>Eitan, Gordon, Harel, Marron, Weiss, 2011</marker>
<rawString>N. Eitan, M. Gordon, D. Harel, A. Marron, and G. Weiss. 2011. On visualization and comprehension of scenario-based programs. In Proceedings of the 2011 IEEE 19th International Conference on Program Comprehension, ICPC ’11, pages 189– 192, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N E Fuchs</author>
<author>R Schwitter</author>
</authors>
<title>Attempto: Controlled natural language for requirements specifications.</title>
<date>1995</date>
<editor>In Markus P. J. Fromherz, Marc Kirschenbaum, and Anthony J. Kusalik, editors, LPE.</editor>
<contexts>
<context position="2382" citStr="Fuchs and Schwitter, 1995" startWordPosition="349" endWordPosition="352"> use here refers to a piece of discourse in natural language, by means of which a stakeholder communicates their desiderata to the system analyst. The role of a system analyst is to understand the different requirements and transform them into formal constructs, formal diagrams or executable code. Moreover, the analyst needs to consolidate the different pieces of information to uncover a single shared domain. Studies in software engineering aim to develop intuitive symbolic systems with which human agents can encode requirements that would then be unambiguously translated into a formal model (Fuchs and Schwitter, 1995; Bryant and Lee, 2002). More recently, Gordon and Harel (2009) defined a natural fragment of English that can be used for specifying requirements which can be effectively translated into live sequence charts (LSC) (Damm and Harel, 2001; Harel and Marelly, 2003), a formal language for specifying the dynamic behavior of reactive systems. However, the grammar that underlies this language fragment is highly ambiguous, and all disambiguation has to be conducted manually by a human agent. Indeed, it is commonly accepted that the more natural a controlled language fragment is, the harder it is to de</context>
</contexts>
<marker>Fuchs, Schwitter, 1995</marker>
<rawString>N. E. Fuchs and R. Schwitter. 1995. Attempto: Controlled natural language for requirements specifications. In Markus P. J. Fromherz, Marc Kirschenbaum, and Anthony J. Kusalik, editors, LPE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gordon</author>
<author>D Harel</author>
</authors>
<title>Generating executable scenarios from natural language.</title>
<date>2009</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Linguistics and Intelligent Text Processing, CICLing ’09,</booktitle>
<pages>456--467</pages>
<publisher>SpringerVerlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="2445" citStr="Gordon and Harel (2009)" startWordPosition="359" endWordPosition="362">means of which a stakeholder communicates their desiderata to the system analyst. The role of a system analyst is to understand the different requirements and transform them into formal constructs, formal diagrams or executable code. Moreover, the analyst needs to consolidate the different pieces of information to uncover a single shared domain. Studies in software engineering aim to develop intuitive symbolic systems with which human agents can encode requirements that would then be unambiguously translated into a formal model (Fuchs and Schwitter, 1995; Bryant and Lee, 2002). More recently, Gordon and Harel (2009) defined a natural fragment of English that can be used for specifying requirements which can be effectively translated into live sequence charts (LSC) (Damm and Harel, 2001; Harel and Marelly, 2003), a formal language for specifying the dynamic behavior of reactive systems. However, the grammar that underlies this language fragment is highly ambiguous, and all disambiguation has to be conducted manually by a human agent. Indeed, it is commonly accepted that the more natural a controlled language fragment is, the harder it is to develop an unambiguous translation mechanism (Kuhn, 2014). In thi</context>
<context position="4494" citStr="Gordon and Harel (2009)" startWordPosition="681" endWordPosition="685">formation for sentence analysis. We suggest a novel 1296 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1296–1307, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: An LSC scenario: ”When the user clicks the button, the display color must change to red.” model for integrated sentence-level and discourselevel processing, in a joint generative probabilistic framework. The input for the requirements elicitation task is given in a simplified, yet highly ambiguous, fragment of English, as specified in Gordon and Harel (2009). The output, in contrast, is a sequence of unambiguous and well-formed live sequence charts (LSC) (Damm and Harel, 2001; Harel and Marelly, 2003) describing the dynamic behavior of the system, tied to a single shared code-base called a system model (SM). Our solution takes the form of a hidden markov model (HMM) where emission probabilities reflect the grammaticality and interpretability of textual requirements via a probabilistic grammar and transition probabilities model the overlap between SM snapshots of a single, shared, domain. Using efficient viterbi decoding, we search for the best se</context>
<context position="6971" citStr="Gordon and Harel (2009)" startWordPosition="1068" endWordPosition="1071">his challenge empirically. The Input. We assume a scenario-based programming paradigm (a.k.a behavioral programming (BP) (Harel et al., 2012)) in which system development is seen as a process whereby humans describe the expected behavior of the system by means of “short-stories”, formally called scenarios (Harel, 2001). We further assume that a given requirements document describes exactly one system, and that each sentence describes a single, possibly complex, scenario. The requirements we aim to parse are given in a simplified form of English (specifically, the English fragment described in Gordon and Harel (2009)). Contrary to strictly formal specification languages, which are closed and unambiguous, this fragment of English employs an open-ended lexicon and exhibits extensive syntactic and semantic ambiguity.1 The Output. Our target semantic representation employs live sequence charts (LSC), a diagrammatic formal language for scenario-based programming (Damm and Harel, 2001). Formally, LSCs are an extension of the well-known UML message sequence diagrams (Harel and Maoz, 2006), and they have a direct translation into executable code (Harel and Marelly, 2003).2 Using LSC diagrams for software modellin</context>
<context position="33405" citStr="Gordon and Harel (2009)" startWordPosition="5667" endWordPosition="5670">f the SM, we use two kinds of scores: SM-TED: TED is the normalized edit distance between the predicted and gold SM trees, subtracted from a unity. SM-EM: EM is 1 if the predicted SM is an exact match with the gold SM, 0 otherwise. Data. We have a small seed of correctly annotated requirements-specification case studies that describe simple reactive systems in the LSC language. Each document contains a sequence of requirements, each of which is annotated with the correct LSC diagram. The entire program is grounded in a java implementation. As training data, we use the case studies provided by Gordon and Harel (2009). Table 2 lists the case studies and basic statistics concerning these data. As our annotated seed is quite small, it is hard to generalize from it to unseen examples. In particular, we are not guaranteed to have observed all possible structures that are theoretically permitted by the assumed grammar. To cope with this, we create a synthetic set of examples using the grammar of Gordon and Harel (2009) in generation mode, and randomly generate trees t E Treq. The grammar we use to generate the synthetic examples clearly over-generates. That is to say, it creates many trees that do not have a so</context>
<context position="38883" citStr="Gordon and Harel, 2009" startWordPosition="6558" endWordPosition="6561">her cases level off beforehand. Despite our small seed, the persistent improvement on all metrics is consistent with our hypothesis that modeling the interpretation process within the discourse has substantial benefits for automatic understanding of the text. 6 Applications and Discussion The statistical models we present here are applied in the context of PlayGo,7 a comprehensive tool for behavioral, scenario-based, programming. PlayGo now provides two modes of playing-in natural language requirements: interactive play-in, where a user manually disambiguates the analyses of the requirements (Gordon and Harel, 2009), and statistical play-in, where disambiguation decisions are taken using our discourse-based model. The fragment of English we use is very expressive. It covers not only entities and predicates, but also temporal and aspectual information, modalities, and program flow. Beyond that, we assume an open-ended lexicon. Overall, we are not 7www.playgo.co. 1304 Transitions N=2 4 8 16 32 64 128 Min Dist POS 91.98 92.76 93.54 93.35 94.32 94.52 93.93 LSC-F1 88.39 89.77 91.00 90.99 91.81 92.09 91.73 LSC-EM 23.81 42.86 47.62 47.62 47.62 47.62 47.62 SM-TED 86.54 91.71 94.38 93.81 95.57 96.43 94.53 SM-EM 2</context>
</contexts>
<marker>Gordon, Harel, 2009</marker>
<rawString>M. Gordon and D. Harel. 2009. Generating executable scenarios from natural language. In Proceedings of the 10th International Conference on Computational Linguistics and Intelligent Text Processing, CICLing ’09, pages 456–467, Berlin, Heidelberg. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Grice</author>
</authors>
<title>Logic and conversation.</title>
<date>1975</date>
<booktitle>Syntax and Semantics: Vol. 3: Speech Acts,</booktitle>
<pages>41--58</pages>
<editor>In P. Cole and J. L. Morgan, editors,</editor>
<publisher>Academic Press,</publisher>
<location>San Diego, CA.</location>
<contexts>
<context position="27958" citStr="Grice (1975)" startWordPosition="4724" endWordPosition="4725">on here is that different SM snapshots refer to the same conceptual world, so there should be a large overlap between them. We call this the max-overlap assumption. A second assumption is that, in collaborative communication, a new requirement will only be stated if it Transition: gap(mcurr, mprev) max-overlap|set(mcurr)nse ( prev)| ) anon — max-expansion 1 set(mcurr)nset(mprev) p |set(mprev)Uset(mcurr)| min-distance 1 − ted(mprev,mcurr) |set(mprev)|+|set(mcurr)| Table 1: Quantifying the gap between snapshots. set(mi) is a set of nodes marked by path to root. provides new information, akin to Grice (1975). This is the max-expansion assumption. An additional assumption prefers “easy” transitions over “hard” ones, this is the min-distance assumption. The different gap calculations are listed in Table 1. Decoding An input document contains n requirements. Our decoding algorithm considers the N-best syntactic analyses for each requirement. At each time step i = 1...n we assume N, states representing the semantics of the N best syntax trees, retrieved via a CKY chart parser. Thus, setting N = 1 is equal to a sentence-based model, in which for each sentence we simply select the most likely tree acco</context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>H. P. Grice. 1975. Logic and conversation. In P. Cole and J. L. Morgan, editors, Syntax and Semantics: Vol. 3: Speech Acts, pages 41–58. Academic Press, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Harel</author>
<author>M Gordon</author>
</authors>
<title>On teaching visual formalisms.</title>
<date>2009</date>
<journal>IEEE Softw.,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="7645" citStr="Harel and Gordon, 2009" startWordPosition="1167" endWordPosition="1170">ges, which are closed and unambiguous, this fragment of English employs an open-ended lexicon and exhibits extensive syntactic and semantic ambiguity.1 The Output. Our target semantic representation employs live sequence charts (LSC), a diagrammatic formal language for scenario-based programming (Damm and Harel, 2001). Formally, LSCs are an extension of the well-known UML message sequence diagrams (Harel and Maoz, 2006), and they have a direct translation into executable code (Harel and Marelly, 2003).2 Using LSC diagrams for software modelling enjoys the advantages of being easily learnable (Harel and Gordon, 2009), intuitively interpretable (Eitan et al., 2011) and straightforwardly amenable to execution (Harel et al., 2002) and verification (Harel et al., 2013). The LSC language is particularly suited for representing natural language requirements, since its basic formal constructs, scenarios, nicely align with events, the primitive objects of Neo-Davidsonian Semantics (Parsons, 1990). 1Formally, this variant may be viewed as a CNL of degree P2 E3 N4 S4 with properties C,F,W,A (Kuhn, 2014, pp 6-12). 2It can be shown that the execution semantics of the LSC language is embedded in a fragment of a branch</context>
</contexts>
<marker>Harel, Gordon, 2009</marker>
<rawString>D. Harel and M. Gordon. 2009. On teaching visual formalisms. IEEE Softw., 26(3):87–95, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Harel</author>
<author>S Maoz</author>
</authors>
<title>Assert and negate revisited: Modal semantics for UML sequence diagrams.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 International Workshop on Scenarios and State Machines: Models, Algorithms, and Tools, SCESM ’06,</booktitle>
<pages>13--20</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7445" citStr="Harel and Maoz, 2006" startWordPosition="1136" endWordPosition="1139">The requirements we aim to parse are given in a simplified form of English (specifically, the English fragment described in Gordon and Harel (2009)). Contrary to strictly formal specification languages, which are closed and unambiguous, this fragment of English employs an open-ended lexicon and exhibits extensive syntactic and semantic ambiguity.1 The Output. Our target semantic representation employs live sequence charts (LSC), a diagrammatic formal language for scenario-based programming (Damm and Harel, 2001). Formally, LSCs are an extension of the well-known UML message sequence diagrams (Harel and Maoz, 2006), and they have a direct translation into executable code (Harel and Marelly, 2003).2 Using LSC diagrams for software modelling enjoys the advantages of being easily learnable (Harel and Gordon, 2009), intuitively interpretable (Eitan et al., 2011) and straightforwardly amenable to execution (Harel et al., 2002) and verification (Harel et al., 2013). The LSC language is particularly suited for representing natural language requirements, since its basic formal constructs, scenarios, nicely align with events, the primitive objects of Neo-Davidsonian Semantics (Parsons, 1990). 1Formally, this var</context>
</contexts>
<marker>Harel, Maoz, 2006</marker>
<rawString>D. Harel and S. Maoz. 2006. Assert and negate revisited: Modal semantics for UML sequence diagrams. In Proceedings of the 2006 International Workshop on Scenarios and State Machines: Models, Algorithms, and Tools, SCESM ’06, pages 13–20, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Harel</author>
<author>R Marelly</author>
</authors>
<title>Come, Let’s Play: Scenario-Based Programming Using LSCs and the Play-Engine.</title>
<date>2003</date>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc., Secaucus, NJ, USA.</location>
<contexts>
<context position="2644" citStr="Harel and Marelly, 2003" startWordPosition="391" endWordPosition="394">formal diagrams or executable code. Moreover, the analyst needs to consolidate the different pieces of information to uncover a single shared domain. Studies in software engineering aim to develop intuitive symbolic systems with which human agents can encode requirements that would then be unambiguously translated into a formal model (Fuchs and Schwitter, 1995; Bryant and Lee, 2002). More recently, Gordon and Harel (2009) defined a natural fragment of English that can be used for specifying requirements which can be effectively translated into live sequence charts (LSC) (Damm and Harel, 2001; Harel and Marelly, 2003), a formal language for specifying the dynamic behavior of reactive systems. However, the grammar that underlies this language fragment is highly ambiguous, and all disambiguation has to be conducted manually by a human agent. Indeed, it is commonly accepted that the more natural a controlled language fragment is, the harder it is to develop an unambiguous translation mechanism (Kuhn, 2014). In this paper we accept the ambiguity of requirements descriptions as a premise, and aim to answer the following question: can we automatically recover a formal representation of the complete system — one </context>
<context position="4640" citStr="Harel and Marelly, 2003" startWordPosition="705" endWordPosition="708">EMNLP), pages 1296–1307, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: An LSC scenario: ”When the user clicks the button, the display color must change to red.” model for integrated sentence-level and discourselevel processing, in a joint generative probabilistic framework. The input for the requirements elicitation task is given in a simplified, yet highly ambiguous, fragment of English, as specified in Gordon and Harel (2009). The output, in contrast, is a sequence of unambiguous and well-formed live sequence charts (LSC) (Damm and Harel, 2001; Harel and Marelly, 2003) describing the dynamic behavior of the system, tied to a single shared code-base called a system model (SM). Our solution takes the form of a hidden markov model (HMM) where emission probabilities reflect the grammaticality and interpretability of textual requirements via a probabilistic grammar and transition probabilities model the overlap between SM snapshots of a single, shared, domain. Using efficient viterbi decoding, we search for the best sequence of domain snapshots that has most likely generated the entire requirements document. We empirically show that such an integrated model cons</context>
<context position="7528" citStr="Harel and Marelly, 2003" startWordPosition="1150" endWordPosition="1153">ifically, the English fragment described in Gordon and Harel (2009)). Contrary to strictly formal specification languages, which are closed and unambiguous, this fragment of English employs an open-ended lexicon and exhibits extensive syntactic and semantic ambiguity.1 The Output. Our target semantic representation employs live sequence charts (LSC), a diagrammatic formal language for scenario-based programming (Damm and Harel, 2001). Formally, LSCs are an extension of the well-known UML message sequence diagrams (Harel and Maoz, 2006), and they have a direct translation into executable code (Harel and Marelly, 2003).2 Using LSC diagrams for software modelling enjoys the advantages of being easily learnable (Harel and Gordon, 2009), intuitively interpretable (Eitan et al., 2011) and straightforwardly amenable to execution (Harel et al., 2002) and verification (Harel et al., 2013). The LSC language is particularly suited for representing natural language requirements, since its basic formal constructs, scenarios, nicely align with events, the primitive objects of Neo-Davidsonian Semantics (Parsons, 1990). 1Formally, this variant may be viewed as a CNL of degree P2 E3 N4 S4 with properties C,F,W,A (Kuhn, 20</context>
<context position="11292" citStr="Harel and Marelly (2003)" startWordPosition="1812" endWordPosition="1815">ne Treq to be the set of trees strongly generated by G, and utilize an auxiliary yield function yield : Treq → Lreq returning the leaves of the given tree t ∈ .Lreq. Different parse-trees can generate the same utterance, so the task of analyzing the structure of an utterance u ∈ Lreq is modeled via a function syn : Lreq → Treq that returns the correct, human-perceived, parse of u. Semantic Structures: Our target semantic representation of a requirement d ∈ Lreq is a diagrammatic structure called a live sequence chart (LSC). The LSC formal definition we provide here is based on the appendix of Harel and Marelly (2003), but rephrased in set-theoretic, event-based, terms. We defined this alternative formalization in order to make LSCs compatible with NeoDavidsonian, event-based, semantic theories. As a result, this form of LSC formalization is wellsuited for representing the semantics of natural language utterances. Let us assume that L is a dictionary of entities (lifelines), A is a dictionary of actions, P is a dictionary of attribute names and V a dictionary of attribute values. The set of simple events in the LSC formal system is defined as follows: Eactive ⊂ L × A × L × (L × P × V )* ×{hot, cold} × {exe</context>
</contexts>
<marker>Harel, Marelly, 2003</marker>
<rawString>D. Harel and R. Marelly. 2003. Come, Let’s Play: Scenario-Based Programming Using LSCs and the Play-Engine. Springer-Verlag New York, Inc., Secaucus, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Harel</author>
<author>H Kugler</author>
<author>R Marelly</author>
<author>A Pnueli</author>
</authors>
<title>Smart play-out of behavioral requirements.</title>
<date>2002</date>
<booktitle>In Proceedings of the 4th International Conference on Formal Methods in Computer-Aided Design, FMCAD ’02,</booktitle>
<pages>378--398</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="7758" citStr="Harel et al., 2002" startWordPosition="1183" endWordPosition="1186">syntactic and semantic ambiguity.1 The Output. Our target semantic representation employs live sequence charts (LSC), a diagrammatic formal language for scenario-based programming (Damm and Harel, 2001). Formally, LSCs are an extension of the well-known UML message sequence diagrams (Harel and Maoz, 2006), and they have a direct translation into executable code (Harel and Marelly, 2003).2 Using LSC diagrams for software modelling enjoys the advantages of being easily learnable (Harel and Gordon, 2009), intuitively interpretable (Eitan et al., 2011) and straightforwardly amenable to execution (Harel et al., 2002) and verification (Harel et al., 2013). The LSC language is particularly suited for representing natural language requirements, since its basic formal constructs, scenarios, nicely align with events, the primitive objects of Neo-Davidsonian Semantics (Parsons, 1990). 1Formally, this variant may be viewed as a CNL of degree P2 E3 N4 S4 with properties C,F,W,A (Kuhn, 2014, pp 6-12). 2It can be shown that the execution semantics of the LSC language is embedded in a fragment of a branching temporal logic called CTL* (Kugler et al., 2005). 1297 Live Sequence Charts and Code Artifacts. A live sequen</context>
</contexts>
<marker>Harel, Kugler, Marelly, Pnueli, 2002</marker>
<rawString>D. Harel, H. Kugler, R. Marelly, and A. Pnueli. 2002. Smart play-out of behavioral requirements. In Proceedings of the 4th International Conference on Formal Methods in Computer-Aided Design, FMCAD ’02, pages 378–398, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Harel</author>
<author>A Marron</author>
<author>G Weiss</author>
</authors>
<title>Behavioral programming.</title>
<date>2012</date>
<journal>Commun. ACM,</journal>
<volume>55</volume>
<issue>7</issue>
<contexts>
<context position="6489" citStr="Harel et al., 2012" startWordPosition="992" endWordPosition="995">ude. 2 Parsing Requirements Elicitation Documents: Task Description There is an inherent discrepancy between the input and the output of the software engineering process. The input, system requirements, is specified in a natural, informal, language. The output, the system, is ultimately implemented in a formal unambiguous programming language. Can we automatically recover such a formal representation of a complete system from a set of requirements? In this work we explore this challenge empirically. The Input. We assume a scenario-based programming paradigm (a.k.a behavioral programming (BP) (Harel et al., 2012)) in which system development is seen as a process whereby humans describe the expected behavior of the system by means of “short-stories”, formally called scenarios (Harel, 2001). We further assume that a given requirements document describes exactly one system, and that each sentence describes a single, possibly complex, scenario. The requirements we aim to parse are given in a simplified form of English (specifically, the English fragment described in Gordon and Harel (2009)). Contrary to strictly formal specification languages, which are closed and unambiguous, this fragment of English emp</context>
</contexts>
<marker>Harel, Marron, Weiss, 2012</marker>
<rawString>D. Harel, A. Marron, and G. Weiss. 2012. Behavioral programming. Commun. ACM, 55(7):90–100, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Harel</author>
<author>A Kantor</author>
<author>G Katz</author>
<author>A Marron</author>
<author>L Mizrahi</author>
<author>G Weiss</author>
</authors>
<title>On composing and proving the correctness of reactive behavior.</title>
<date>2013</date>
<booktitle>In Embedded Software (EMSOFT), 2013 Proceedings of the International Conference on,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="7796" citStr="Harel et al., 2013" startWordPosition="1189" endWordPosition="1192"> Output. Our target semantic representation employs live sequence charts (LSC), a diagrammatic formal language for scenario-based programming (Damm and Harel, 2001). Formally, LSCs are an extension of the well-known UML message sequence diagrams (Harel and Maoz, 2006), and they have a direct translation into executable code (Harel and Marelly, 2003).2 Using LSC diagrams for software modelling enjoys the advantages of being easily learnable (Harel and Gordon, 2009), intuitively interpretable (Eitan et al., 2011) and straightforwardly amenable to execution (Harel et al., 2002) and verification (Harel et al., 2013). The LSC language is particularly suited for representing natural language requirements, since its basic formal constructs, scenarios, nicely align with events, the primitive objects of Neo-Davidsonian Semantics (Parsons, 1990). 1Formally, this variant may be viewed as a CNL of degree P2 E3 N4 S4 with properties C,F,W,A (Kuhn, 2014, pp 6-12). 2It can be shown that the execution semantics of the LSC language is embedded in a fragment of a branching temporal logic called CTL* (Kugler et al., 2005). 1297 Live Sequence Charts and Code Artifacts. A live sequence chart (LSC) is a diagram that descr</context>
</contexts>
<marker>Harel, Kantor, Katz, Marron, Mizrahi, Weiss, 2013</marker>
<rawString>D. Harel, A. Kantor, G. Katz, A. Marron, L. Mizrahi, and G. Weiss. 2013. On composing and proving the correctness of reactive behavior. In Embedded Software (EMSOFT), 2013 Proceedings of the International Conference on, pages 1–10, Sept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Harel</author>
</authors>
<title>From play-in scenarios to code: An achievable dream.</title>
<date>2001</date>
<journal>Computer,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="2618" citStr="Harel, 2001" startWordPosition="389" endWordPosition="390"> constructs, formal diagrams or executable code. Moreover, the analyst needs to consolidate the different pieces of information to uncover a single shared domain. Studies in software engineering aim to develop intuitive symbolic systems with which human agents can encode requirements that would then be unambiguously translated into a formal model (Fuchs and Schwitter, 1995; Bryant and Lee, 2002). More recently, Gordon and Harel (2009) defined a natural fragment of English that can be used for specifying requirements which can be effectively translated into live sequence charts (LSC) (Damm and Harel, 2001; Harel and Marelly, 2003), a formal language for specifying the dynamic behavior of reactive systems. However, the grammar that underlies this language fragment is highly ambiguous, and all disambiguation has to be conducted manually by a human agent. Indeed, it is commonly accepted that the more natural a controlled language fragment is, the harder it is to develop an unambiguous translation mechanism (Kuhn, 2014). In this paper we accept the ambiguity of requirements descriptions as a premise, and aim to answer the following question: can we automatically recover a formal representation of </context>
<context position="4614" citStr="Harel, 2001" startWordPosition="703" endWordPosition="704"> Processing (EMNLP), pages 1296–1307, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: An LSC scenario: ”When the user clicks the button, the display color must change to red.” model for integrated sentence-level and discourselevel processing, in a joint generative probabilistic framework. The input for the requirements elicitation task is given in a simplified, yet highly ambiguous, fragment of English, as specified in Gordon and Harel (2009). The output, in contrast, is a sequence of unambiguous and well-formed live sequence charts (LSC) (Damm and Harel, 2001; Harel and Marelly, 2003) describing the dynamic behavior of the system, tied to a single shared code-base called a system model (SM). Our solution takes the form of a hidden markov model (HMM) where emission probabilities reflect the grammaticality and interpretability of textual requirements via a probabilistic grammar and transition probabilities model the overlap between SM snapshots of a single, shared, domain. Using efficient viterbi decoding, we search for the best sequence of domain snapshots that has most likely generated the entire requirements document. We empirically show that suc</context>
<context position="6668" citStr="Harel, 2001" startWordPosition="1022" endWordPosition="1023">em requirements, is specified in a natural, informal, language. The output, the system, is ultimately implemented in a formal unambiguous programming language. Can we automatically recover such a formal representation of a complete system from a set of requirements? In this work we explore this challenge empirically. The Input. We assume a scenario-based programming paradigm (a.k.a behavioral programming (BP) (Harel et al., 2012)) in which system development is seen as a process whereby humans describe the expected behavior of the system by means of “short-stories”, formally called scenarios (Harel, 2001). We further assume that a given requirements document describes exactly one system, and that each sentence describes a single, possibly complex, scenario. The requirements we aim to parse are given in a simplified form of English (specifically, the English fragment described in Gordon and Harel (2009)). Contrary to strictly formal specification languages, which are closed and unambiguous, this fragment of English employs an open-ended lexicon and exhibits extensive syntactic and semantic ambiguity.1 The Output. Our target semantic representation employs live sequence charts (LSC), a diagramma</context>
</contexts>
<marker>Harel, 2001</marker>
<rawString>D. Harel. 2001. From play-in scenarios to code: An achievable dream. Computer, 34(1):53–60, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kugler</author>
<author>D Harel</author>
<author>A Pnueli</author>
<author>Y Lu</author>
<author>Y Bontemps</author>
</authors>
<title>Temporal logic for scenario-based specifications.</title>
<date>2005</date>
<booktitle>In Proceedings of the 11th International Conference on Tools and Algorithms for the Construction and Analysis of Systems, TACAS’05,</booktitle>
<pages>445--460</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="8297" citStr="Kugler et al., 2005" startWordPosition="1270" endWordPosition="1273">an et al., 2011) and straightforwardly amenable to execution (Harel et al., 2002) and verification (Harel et al., 2013). The LSC language is particularly suited for representing natural language requirements, since its basic formal constructs, scenarios, nicely align with events, the primitive objects of Neo-Davidsonian Semantics (Parsons, 1990). 1Formally, this variant may be viewed as a CNL of degree P2 E3 N4 S4 with properties C,F,W,A (Kuhn, 2014, pp 6-12). 2It can be shown that the execution semantics of the LSC language is embedded in a fragment of a branching temporal logic called CTL* (Kugler et al., 2005). 1297 Live Sequence Charts and Code Artifacts. A live sequence chart (LSC) is a diagram that describes a possible or necessary run of a specified system. In a single LSC diagram, entities are represented as vertical lines called lifelines, and interactions between entities are represented using horizontal arrows between lifelines called messages, connecting a sender to a receiver. Messages may refer to other entities (or properties of entities) as arguments. Time in LSCs proceeds from top to bottom, imposing a partial order on the execution of messages. LSC messages can be hot (red, “must hap</context>
</contexts>
<marker>Kugler, Harel, Pnueli, Lu, Bontemps, 2005</marker>
<rawString>H. Kugler, D. Harel, A. Pnueli, Y. Lu, and Y. Bontemps. 2005. Temporal logic for scenario-based specifications. In Proceedings of the 11th International Conference on Tools and Algorithms for the Construction and Analysis of Systems, TACAS’05, pages 445–460, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kuhn</author>
</authors>
<title>A survey and classification of controlled natural languages.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="3037" citStr="Kuhn, 2014" startWordPosition="456" endWordPosition="457">on and Harel (2009) defined a natural fragment of English that can be used for specifying requirements which can be effectively translated into live sequence charts (LSC) (Damm and Harel, 2001; Harel and Marelly, 2003), a formal language for specifying the dynamic behavior of reactive systems. However, the grammar that underlies this language fragment is highly ambiguous, and all disambiguation has to be conducted manually by a human agent. Indeed, it is commonly accepted that the more natural a controlled language fragment is, the harder it is to develop an unambiguous translation mechanism (Kuhn, 2014). In this paper we accept the ambiguity of requirements descriptions as a premise, and aim to answer the following question: can we automatically recover a formal representation of the complete system — one that best reflects the humanperceived interpretation of the entire document? Recent advances in natural language processing, with an eye to semantic parsing (Zettlemoyer and Collins, 2005; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Liang and Potts, 2014), use different formalisms and various kinds of learning signals for statistical semantic parsing. In particular, the model of Lei et</context>
<context position="8130" citStr="Kuhn, 2014" startWordPosition="1242" endWordPosition="1243">y, 2003).2 Using LSC diagrams for software modelling enjoys the advantages of being easily learnable (Harel and Gordon, 2009), intuitively interpretable (Eitan et al., 2011) and straightforwardly amenable to execution (Harel et al., 2002) and verification (Harel et al., 2013). The LSC language is particularly suited for representing natural language requirements, since its basic formal constructs, scenarios, nicely align with events, the primitive objects of Neo-Davidsonian Semantics (Parsons, 1990). 1Formally, this variant may be viewed as a CNL of degree P2 E3 N4 S4 with properties C,F,W,A (Kuhn, 2014, pp 6-12). 2It can be shown that the execution semantics of the LSC language is embedded in a fragment of a branching temporal logic called CTL* (Kugler et al., 2005). 1297 Live Sequence Charts and Code Artifacts. A live sequence chart (LSC) is a diagram that describes a possible or necessary run of a specified system. In a single LSC diagram, entities are represented as vertical lines called lifelines, and interactions between entities are represented using horizontal arrows between lifelines called messages, connecting a sender to a receiver. Messages may refer to other entities (or propert</context>
</contexts>
<marker>Kuhn, 2014</marker>
<rawString>T. Kuhn. 2014. A survey and classification of controlled natural languages. Computational Linguistics, 40(1):121–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Lei</author>
<author>F Long</author>
<author>R Barzilay</author>
<author>M C Rinard</author>
</authors>
<title>From natural language specifications to program input parsers.</title>
<date>2013</date>
<journal>In ACL</journal>
<volume>1</volume>
<pages>1294--1303</pages>
<contexts>
<context position="3648" citStr="Lei et al. (2013)" startWordPosition="555" endWordPosition="558"> 2014). In this paper we accept the ambiguity of requirements descriptions as a premise, and aim to answer the following question: can we automatically recover a formal representation of the complete system — one that best reflects the humanperceived interpretation of the entire document? Recent advances in natural language processing, with an eye to semantic parsing (Zettlemoyer and Collins, 2005; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Liang and Potts, 2014), use different formalisms and various kinds of learning signals for statistical semantic parsing. In particular, the model of Lei et al. (2013) induces input parsers from format descriptions. However, rarely do these models take into account the entire document’s context. The key idea we promote here is that discourse context provides substantial disambiguating information for sentence analysis. We suggest a novel 1296 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1296–1307, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: An LSC scenario: ”When the user clicks the button, the display color must change to red.” model for integrated sen</context>
</contexts>
<marker>Lei, Long, Barzilay, Rinard, 2013</marker>
<rawString>T. Lei, F. Long, R. Barzilay, and M. C. Rinard. 2013. From natural language specifications to program input parsers. In ACL (1), pages 1294–1303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>C Potts</author>
</authors>
<title>Bringing machine learning and compositional semantics together. Annual Reviews of Linguistics (submitted),</title>
<date>2014</date>
<pages>0</pages>
<contexts>
<context position="3504" citStr="Liang and Potts, 2014" startWordPosition="530" endWordPosition="533">is commonly accepted that the more natural a controlled language fragment is, the harder it is to develop an unambiguous translation mechanism (Kuhn, 2014). In this paper we accept the ambiguity of requirements descriptions as a premise, and aim to answer the following question: can we automatically recover a formal representation of the complete system — one that best reflects the humanperceived interpretation of the entire document? Recent advances in natural language processing, with an eye to semantic parsing (Zettlemoyer and Collins, 2005; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Liang and Potts, 2014), use different formalisms and various kinds of learning signals for statistical semantic parsing. In particular, the model of Lei et al. (2013) induces input parsers from format descriptions. However, rarely do these models take into account the entire document’s context. The key idea we promote here is that discourse context provides substantial disambiguating information for sentence analysis. We suggest a novel 1296 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1296–1307, October 25-29, 2014, Doha, Qatar. c�2014 Association for Comput</context>
</contexts>
<marker>Liang, Potts, 2014</marker>
<rawString>P. Liang and C. Potts. 2014. Bringing machine learning and compositional semantics together. Annual Reviews of Linguistics (submitted), 0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>590--599</pages>
<contexts>
<context position="3451" citStr="Liang et al., 2011" startWordPosition="521" endWordPosition="524"> conducted manually by a human agent. Indeed, it is commonly accepted that the more natural a controlled language fragment is, the harder it is to develop an unambiguous translation mechanism (Kuhn, 2014). In this paper we accept the ambiguity of requirements descriptions as a premise, and aim to answer the following question: can we automatically recover a formal representation of the complete system — one that best reflects the humanperceived interpretation of the entire document? Recent advances in natural language processing, with an eye to semantic parsing (Zettlemoyer and Collins, 2005; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Liang and Potts, 2014), use different formalisms and various kinds of learning signals for statistical semantic parsing. In particular, the model of Lei et al. (2013) induces input parsers from format descriptions. However, rarely do these models take into account the entire document’s context. The key idea we promote here is that discourse context provides substantial disambiguating information for sentence analysis. We suggest a novel 1296 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1296–1307, October 25</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2011. Learning dependency-based compositional semantics. In Association for Computational Linguistics (ACL), pages 590–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Parsons</author>
</authors>
<title>Events in the Semantics of English: A study in subatomic semantics.</title>
<date>1990</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="8024" citStr="Parsons, 1990" startWordPosition="1222" endWordPosition="1223">nce diagrams (Harel and Maoz, 2006), and they have a direct translation into executable code (Harel and Marelly, 2003).2 Using LSC diagrams for software modelling enjoys the advantages of being easily learnable (Harel and Gordon, 2009), intuitively interpretable (Eitan et al., 2011) and straightforwardly amenable to execution (Harel et al., 2002) and verification (Harel et al., 2013). The LSC language is particularly suited for representing natural language requirements, since its basic formal constructs, scenarios, nicely align with events, the primitive objects of Neo-Davidsonian Semantics (Parsons, 1990). 1Formally, this variant may be viewed as a CNL of degree P2 E3 N4 S4 with properties C,F,W,A (Kuhn, 2014, pp 6-12). 2It can be shown that the execution semantics of the LSC language is embedded in a fragment of a branching temporal logic called CTL* (Kugler et al., 2005). 1297 Live Sequence Charts and Code Artifacts. A live sequence chart (LSC) is a diagram that describes a possible or necessary run of a specified system. In a single LSC diagram, entities are represented as vertical lines called lifelines, and interactions between entities are represented using horizontal arrows between life</context>
</contexts>
<marker>Parsons, 1990</marker>
<rawString>T. Parsons. 1990. Events in the Semantics of English: A study in subatomic semantics. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>1948</date>
<journal>Bell System Technical Journal,</journal>
<volume>27</volume>
<pages>623--656</pages>
<contexts>
<context position="22246" citStr="Shannon, 1948" startWordPosition="3788" endWordPosition="3789">uct the SM of the entire discourse in the sentence-based model we simply return f(d1, ..., dn) = Un i=1mi where ∀i : mi &gt; sem(syn(di)) and U unifies different mentions of the same string to a single element. 4.2 Discourse-Based Modeling We assume a given document D E D and aim to find the most probable system model M E M that satisfies the requirements. We assume that M reflects a single domain that the stakeholders have in mind, and we are provided with an ambiguous natural language evidence, an elicited discourse D, in which they convey it. We instantiate this view as a noisy channel model (Shannon, 1948), which provides the foundation for many NLP applications, such as speech recognition (Bahl et al., 1983) and machine translation (Brown et al., 1993). According to the noisy channel model, when a signal is received it does not uniquely identify the message being sent. A probabilistic model is then used to decode the original message. In our case, the signal is the discourse and the message is the overall system model. In formal terms, we want to find a model M that maximises the following: f(D) = argmaxMEMP(M|D) We can simplify further, using Bayes law, where D is constant with respect to the</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>C. Shannon. 1948. A mathematical theory of communication. Bell System Technical Journal, 27:379– 423, 623–656, July, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tsarfaty</author>
<author>J Nivre</author>
<author>E Andersson</author>
</authors>
<title>Crossframework evaluation for statistical parsing.</title>
<date>2012</date>
<booktitle>Proceedings of EACL,</booktitle>
<pages>44--54</pages>
<editor>In W. Daelemans, M. Lapata, and L. M`arquez, editors,</editor>
<publisher>The Association for Computer Linguistics.</publisher>
<contexts>
<context position="32748" citStr="Tsarfaty et al. (2012)" startWordPosition="5552" endWordPosition="5555">In the case of SM trees, as opposed to the LSC trees, we cannot assume identity of the yield between the gold and parse trees for the same senSystem #Scenarios avg sentence length Phone 21 24.33 WristWatch 15 29.8 Chess 18 15.83 Baby Monitor 14 20 Total 68 22.395 Table 2: Seed Gold-Annotated Requirements N=1 POS LSC-F1 LSC-EM SM-TED SM-EM Gen-Only 85.52 84.40 9.52 84.25 9.52 Gen+Seed 91.59 88.05 14.29 85.17 14.29 Table 3: Sentence-Based modeling: Accuracy results on the Phone development set. tence,6 so we cannot use ParseEval. Therefore, we implement a distance-based metrics in the spirit of Tsarfaty et al. (2012). Then, to evaluate the accuracy of the SM, we use two kinds of scores: SM-TED: TED is the normalized edit distance between the predicted and gold SM trees, subtracted from a unity. SM-EM: EM is 1 if the predicted SM is an exact match with the gold SM, 0 otherwise. Data. We have a small seed of correctly annotated requirements-specification case studies that describe simple reactive systems in the LSC language. Each document contains a sequence of requirements, each of which is annotated with the correct LSC diagram. The entire program is grounded in a java implementation. As training data, we</context>
</contexts>
<marker>Tsarfaty, Nivre, Andersson, 2012</marker>
<rawString>R. Tsarfaty, J. Nivre, and E. Andersson. 2012. Crossframework evaluation for statistical parsing. In W. Daelemans, M. Lapata, and L. M`arquez, editors, Proceedings of EACL, pages 44–54. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimum decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Trans. Inf. Theor.</journal>
<contexts>
<context position="30685" citStr="Viterbi (1967)" startWordPosition="5202" endWordPosition="5203"> mj, : Pˆ(mi|mj) = 1. (For efficiency reasons, we 1302 The overall complexity decoding a document with n sentences of which max length is l, using a grammar G of size |G |and a fixed N, is given by: O(n x l3 x |G|3 + l2 x N2 x n + n3 x N2) We can break this expression down as follows: (i) In O(n x l3 x |G|3) we generate N best trees for each one of the n requirements, using a CKY chart (Younger, 1967). (ii) In O(l2 x N2 x n) we create the universe M based on the N best trees of the n requirements, and calculate N x N transitions. (iii) In O((N xn)2xn) = O(N2xn3) we decode the n x N grid using Viterbi (1967) decoding. 5 Experiments Goal. We set out to evaluate the accuracy of a semantic parser for requirements documents, in the two modes of analysis presented above. Our evaluation methodology is as standardly assumed in machine learning and NLP: given a set of annotated examples — that is, given a set of requirements documents, where each requirement is annotated with its correct LSC representation and each document is associated with a complete SM — we partition this set into a training set and a test set that are disjoint. We train our statistical model on the examples in the training set and a</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>A. Viterbi. 1967. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Trans. Inf. Theor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages in time n3.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="30475" citStr="Younger, 1967" startWordPosition="5158" endWordPosition="5159">ble sentences in the language, we sum up the SM analyses of the sentences observed in the document only. This estimation could also be addressed via, e.g., sampling methods. Pˆ : M x M → [0, 1] and, for all mi, mj, : Pˆ(mi|mj) = 1. (For efficiency reasons, we 1302 The overall complexity decoding a document with n sentences of which max length is l, using a grammar G of size |G |and a fixed N, is given by: O(n x l3 x |G|3 + l2 x N2 x n + n3 x N2) We can break this expression down as follows: (i) In O(n x l3 x |G|3) we generate N best trees for each one of the n requirements, using a CKY chart (Younger, 1967). (ii) In O(l2 x N2 x n) we create the universe M based on the N best trees of the n requirements, and calculate N x N transitions. (iii) In O((N xn)2xn) = O(N2xn3) we decode the n x N grid using Viterbi (1967) decoding. 5 Experiments Goal. We set out to evaluate the accuracy of a semantic parser for requirements documents, in the two modes of analysis presented above. Our evaluation methodology is as standardly assumed in machine learning and NLP: given a set of annotated examples — that is, given a set of requirements documents, where each requirement is annotated with its correct LSC repres</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>D. H. Younger. 1967. Recognition and parsing of context-free languages in time n3. Information and Control, 10(2):189–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In UAI,</booktitle>
<pages>658--666</pages>
<publisher>AUAI Press.</publisher>
<contexts>
<context position="3431" citStr="Zettlemoyer and Collins, 2005" startWordPosition="517" endWordPosition="520">nd all disambiguation has to be conducted manually by a human agent. Indeed, it is commonly accepted that the more natural a controlled language fragment is, the harder it is to develop an unambiguous translation mechanism (Kuhn, 2014). In this paper we accept the ambiguity of requirements descriptions as a premise, and aim to answer the following question: can we automatically recover a formal representation of the complete system — one that best reflects the humanperceived interpretation of the entire document? Recent advances in natural language processing, with an eye to semantic parsing (Zettlemoyer and Collins, 2005; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Liang and Potts, 2014), use different formalisms and various kinds of learning signals for statistical semantic parsing. In particular, the model of Lei et al. (2013) induces input parsers from format descriptions. However, rarely do these models take into account the entire document’s context. The key idea we promote here is that discourse context provides substantial disambiguating information for sentence analysis. We suggest a novel 1296 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>L. S. Zettlemoyer and M. Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In UAI, pages 658–666. AUAI Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
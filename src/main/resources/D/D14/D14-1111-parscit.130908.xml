<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000306">
<title confidence="0.983294">
Reducing Dimensions of Tensors in Type-Driven Distributional Semantics
</title>
<author confidence="0.949157">
Tamara Polajnar Luana Fˇagˇarˇas¸an Stephen Clark
</author>
<affiliation confidence="0.9797505">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.754019">
Cambridge, UK
</address>
<email confidence="0.998402">
first.last@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.997369" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99984485">
Compositional distributional semantics is
a subfield of Computational Linguistics
which investigates methods for represent-
ing the meanings of phrases and sen-
tences. In this paper, we explore im-
plementations of a framework based on
Combinatory Categorial Grammar (CCG),
in which words with certain grammatical
types have meanings represented by multi-
linear maps (i.e. multi-dimensional arrays,
or tensors). An obstacle to full implemen-
tation of the framework is the size of these
tensors. We examine the performance of
lower dimensional approximations of tran-
sitive verb tensors on a sentence plausi-
bility/selectional preference task. We find
that the matrices perform as well as, and
sometimes even better than, full tensors,
allowing a reduction in the number of pa-
rameters needed to model the framework.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937607142858">
An emerging subfield of computational linguis-
tics is concerned with learning compositional dis-
tributional representations of meaning (Mitchell
and Lapata, 2008; Baroni and Zamparelli, 2010;
Coecke et al., 2010; Grefenstette and Sadrzadeh,
2011; Clarke, 2012; Socher et al., 2012; Clark,
2013). The advantage of such representations lies
in their potential to combine the benefits of dis-
tributional approachs to word meaning (Sch¨utze,
1998; Turney and Pantel, 2010) with the more tra-
ditional compositional methods from formal se-
mantics (Dowty et al., 1981). Distributional repre-
sentations have the properties of robustness, learn-
ability from data, ease of handling ambiguity,
and the ability to represent gradations of mean-
ing; whereas compositional models handle the un-
bounded nature of natural language, as well as
providing established accounts of logical words,
quantification, and inference.
One promising approach which attempts to
combine elements of compositional and distribu-
tional semantics is by Coecke et al. (2010). The
underlying idea is to take the type-driven approach
from formal semantics — in particular the idea
that the meanings of complex grammatical types
should be represented as functions — and ap-
ply it to distributional representations. Since the
mathematics of distributional semantics is pro-
vided by linear algebra, a natural set of functions
to consider is the set of linear maps. Coecke et
al. recognize that there is a natural correspon-
dence from complex grammatical types to tensors
(multi-linear maps), so that the meaning of an ad-
jective, for example, is represented by a matrix (a
2nd-order tensor)1 and the meaning of a transitive
verb is represented by a 3rd-order tensor.
Coecke et al. use the grammar of pregroups
as the syntactic machinery to construct distribu-
tional meaning representations, since both pre-
groups and vector spaces can be seen as exam-
ples of the same abstract structure, which leads
to a particularly clean mathematical description of
the compositional process. However, the approach
applies more generally, for example to other forms
of categorial grammar, such as Combinatory Cate-
gorial Grammar (Steedman, 2000; Maillard et al.,
2014), and also to phrase-structure grammars in a
way that a formal linguist would recognize (Ba-
roni et al., 2014). Clark (2013) provides a descrip-
tion of the tensor-based framework aimed more at
computational linguists, relying only on the math-
ematics of multi-linear algebra rather than the cat-
egory theory used in Coecke et al. (2010). Sec-
tion 2 repeats some of this description.
A major open question associated with the
tensor-based semantic framework is how to learn
</bodyText>
<footnote confidence="0.99624">
1This same insight lies behind the work of Baroni and
Zamparelli (2010).
</footnote>
<page confidence="0.880239">
1036
</page>
<note confidence="0.909752">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1036–1046,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999979160714285">
the tensors representing the meanings of words
with complex types, such as verbs and adjec-
tives. The framework is essentially a composi-
tional framework, providing a recipe for how to
combine distributional representations, but leav-
ing open what the underlying vector spaces are and
how they can be acquired. One significant chal-
lenge is an engineering one: in a wide-coverage
grammar, which is able to handle naturally occur-
ring text, there will be a) a large lexicon with many
word-category pairs requiring tensor representa-
tions; and b) many higher-order tensors with large
numbers of parameters which need to be learned.
In this paper we take a first step towards learning
such representations, by learning tensors for tran-
sitive verbs.
One feature of the tensor-based framework is
that it allows the meanings of words and phrases
with different basic types, for example nouns and
sentences, to live in different vector spaces. This
means that the sentence space is task specific, and
must be defined in advance. For example, to calcu-
late sentence similarity, we would have to learn a
vector space where distances between vectors rep-
resenting the meanings of sentences reflect simi-
larity scores assigned by human annotators.
In this paper we describe an initial investi-
gation into the learning of word meanings with
complex syntactic types, together with a simple
sentence space. The space we consider is the
“plausibility space” described by Clark (2013), to-
gether with sentences of the form subject-verb-
object. This space is defined to distinguish se-
mantically plausible sentences (e.g. Animals eat
plants) from implausible ones (e.g. Animals eat
planets). Plausibility can be either represented
as a single continuous variable between 0 and 1,
or as a two-dimensional probability distribution
over the classes plausible (T) and implausible (1).
Whether we consider a one- or two-dimensional
sentence space depends on the architecture of the
logistic regression classifier that is used to learn
the verb (Section 3).
We begin with this simple plausibility sentence
space to determine if, in fact, the tensor-based rep-
resentation can be learned to a sufficiently useful
degree. Other simple sentence spaces which can
perhaps be represented using one or two variables
include a “sentence space” for the sentiment anal-
ysis task (Socher et al., 2013), where one variable
represents positive sentiment and the other nega-
tive. We also expect that the insights gained from
research on this task can be applied to more com-
plex sentence spaces, for example a semantic simi-
larity space which will require more than two vari-
ables.
</bodyText>
<sectionHeader confidence="0.931824" genericHeader="method">
2 Syntactic Types to Tensors
</sectionHeader>
<bodyText confidence="0.994310166666667">
The syntactic type of a transitive verb in English
is (SNP)/NP (using notation from Steedman
(2000)), meaning that a transitive verb is a func-
tion which takes an NP argument to the right, an
NP argument to the left, and results in a sentence
S. Such categories with slashes are complex cate-
gories; S and NP are basic or atomic categories.
Interpreting such categories under the Coecke et
al. framework is straightforward. First, for each
atomic category there is a corresponding vector
space; in this case the sentence space S and the
noun space N.2 Hence the meaning of a noun or
noun phrase, for example people, will be a vector
����
in the noun space: people E N. In order to obtain
the meaning of a transitive verb, each slash is re-
placed with a tensor product operator, so that the
meaning of eat, for example, is a 3rd-order tensor:
eat E S ® N ® N. Just as in the syntactic case,
the meaning of a transitive verb is a function (a
multi-linear map) which takes two noun vectors as
arguments and returns a sentence vector.
Meanings combine using tensor contraction,
which can be thought of as a multi-linear gen-
eralisation of matrix multiplication (Grefenstette,
2013). Consider first the adjective-noun case, for
example black cat. The syntactic type of black
is N/N; hence its meaning is a 2nd-order tensor
(matrix): black E N ® N. In the syntax, N/N
combines with N using the rule of forward appli-
cation (N/N N ==&gt;. N), which is an instance of
function application. Function application is also
used in the tensor-based semantics, which, for a
matrix and vector argument, corresponds to ma-
trix multiplication.
Figure 1 shows how the syntactic types com-
bine with a transitive verb, and the corresponding
tensor-based semantic types. Note that, after the
verb has combined with its object NP, the type
of the verb phrase is SNP, with a correspond-
ing meaning tensor (matrix) in S ® N. This ma-
trix then combines with the subject vector, through
</bodyText>
<footnote confidence="0.997938666666667">
2In practice, for example using the CCG parser of Clark
and Curran (2007), there will be additional atomic categories,
such as PP, but not many more.
</footnote>
<page confidence="0.994512">
1037
</page>
<figure confidence="0.999487416666667">
DMat
2Mat
Tensor
SKMat
KKMat
people eat fish
K2
V
Θ
0
4K
8
2K2
4
2K
4
K2
0
NP (S\NP)/NP NP
N S ® N ® N N
S\NP
S ® N
S
S
</figure>
<figureCaption confidence="0.999514">
Figure 1: Syntactic reduction and tensor-based se-
</figureCaption>
<bodyText confidence="0.987455409090909">
mantic types for a transitive verb sentence
matrix multiplication, to give a sentence vector.
In practice, using for example the wide-
coverage grammar from CCGbank (Hockenmaier
and Steedman, 2007), there will be many types
with more than 3 slashes, with corresponding
higher-order tensors. For example, a com-
mon category for a preposition is the follow-
ing: ((S\NP)\(S\NP))/NP, which would be
assigned to WITH in eat WITH a fork. (The way
to read the syntactic type is as follows: with re-
quires an NP argument to the right – a fork in
this example – and then a verb phrase to the
left – eat with type S\NP – resulting in a verb
phrase S\NP.) The corresponding meaning ten-
sor lives in the tensor space S ® N ® S ® N ® N,
i.e. a 5th-order tensor. Categories with even
more slashes are not uncommon, for example
((N/N)/(N/N))/((N/N)/(N/N)). Clearly
learning parameters for such tensors is highly
challenging, and it is likely that lower dimensional
approximations will be required.
</bodyText>
<sectionHeader confidence="0.998594" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.997049882352941">
In this paper we compare five different methods
for modelling the type-driven semantic represen-
tation of subject-verb-object sentences. The ten-
sor is a function that encodes the meaning of a
verb. It takes two vectors from the K-dimensional
noun space as input, and produces a representa-
tion of the sentence in the S-dimensional sentence
space. In this paper, we consider a plausibility
space where S is either a single variable or a two-
dimensional space over two classes: plausible (T)
and implausible (⊥).
The first method (Tensor) follows Krishna-
murthy and Mitchell (2013) by learning a tensor as
parameters in a softmax classifier. We introduce
three related methods (2Mat, SKMat, KKMat),
all of which model the verb as a matrix or a pair of
matrices (Figure 2). Table 1 gives the number of
</bodyText>
<tableCaption confidence="0.985235">
Table 1: Number of parameters per method.
</tableCaption>
<bodyText confidence="0.998959709677419">
parameters for each method. Tensor, 2Mat, and
SKMat all have a two-dimensional S space, while
KKMat produces a scalar value. In all of these
learning-based methods the derivatives were ob-
tained via the chain rule with respect to each set
of parameters and gradient descent was performed
using the Adagrad algorithm (Duchi et al., 2011).
We also reimplement a distributional method
(DMat), which was previously used in SVO
experiments with the type-driven framework
(Grefenstette and Sadrzadeh, 2011). While the
other methods are trained as plausibility classi-
fiers, in DMat we estimate the class boundary
from cosine similarity via training data (see expla-
nation below).
Tensor If subject (ns) and object (no) nouns are
K-dimensional vectors and the plausibility vec-
tor is S-dimensional with S = 2, we can learn
the values of the K x K x S tensor represent-
ing the verb as parameters (V) of a regression al-
gorithm. To represent this space as a distribution
over two classes (T,⊥) we apply a sigmoid func-
tion (Q) to restrict the output to the [0,1] range and
the softmax activation function (g) to balance the
class probabilities. The full parameter set which
needs to be optimised for is B = {V, O}, where
O = {BT, B⊥} are the softmax parameters for
the two classes. For each verb we optimise the
KL-divergence L between the training labels tz
and classifier predictions using the following reg-
ularised objective:
</bodyText>
<equation confidence="0.999252333333333">
O(B) = N G (ti, g (σ (hV (nis, nio)) , Θ)) + 2 ||B||2
i=1
(1)
</equation>
<bodyText confidence="0.998419">
where nzs and nzo are the subject and object of
</bodyText>
<equation confidence="0.681613">
) =
</equation>
<bodyText confidence="0.997355125">
the training instance i ∈ N, and hV (nz s, nz o
(nzs)V(nzo)T describes tensor contraction. The
function hV is described diagrammatically in Fig-
ure 2-(a), where the verb tensor parameters are
drawn as a cube with the subject and object noun
vectors as operands on either side. The output
is a two-dimensional vector which is then trans-
formed using the sigmoid and softmax functions.
</bodyText>
<figure confidence="0.976851214285714">
&gt;
&lt;
1038
(d)
S
K
x
K
S
K
S
K
x
S
</figure>
<figureCaption confidence="0.995947">
Figure 2: Illustrations of the hV function for the regression-based methods (a)-Tensor, (b)-2Mat, (c)-
SKMat, (d)-KKMat. The operation in (a) is tensor contraction, T denotes transpose, and x denotes
matrix multiplication.
</figureCaption>
<bodyText confidence="0.999846117647059">
The gold-standard distribution over training labels
is defined as (1, 0) or (0, 1), depending on whether
the training instance is a positive (plausible) or
negative (implausible) example. Tensor contrac-
tion is implemented using the Matlab Tensor Tool-
box (Bader et al., 2012).
2Mat An alternative approach is to decouple
the interaction between the object and subject by
learning a pair of 5 x K (5 = 2) matrices (Vs,
Vo) for each of the input noun vectors (one ma-
trix for the subject slot of the verb and one for the
object slot). The resulting 5-vectors are concate-
nated, after the subject and object nouns have been
combined with their matrices, and combined with
the softmax component to produce the output dis-
tribution. Therefore the objective function is the
same as in Equation 1, but hV is defined as:
</bodyText>
<equation confidence="0.994514">
( i i) = ( i i T)T
hV ns, no (ns)VsT) (   ||Vo(no
</equation>
<bodyText confidence="0.999354434782609">
where  ||represents vector concatenation. The in-
tention is to test whether we can learn the verb
without directly multiplying subject and object
features, nis and njo. The function hV is shown in
Figure 2-(b), where the verb tensor parameters are
drawn as two 2 x K matrices, one of which inter-
acts with the subject and the other with the object
noun vector. The output is a four-dimensional vec-
tor whose values are then restricted to [0,1] using
the sigmoid function and then transformed into a
two-dimensional distribution over the classes us-
ing the softmax function.
SKMat A third option for generating a sentence
vector with 5 = 2 dimensions is to consider the
verb as an 5 x K matrix. If we transform the ob-
ject vector into a K x K matrix with the noun on
the diagonal and zeroes elsewhere, we can com-
bine the verb and object to produce a new 5 x K
matrix, which is encoding the meaning of the verb
phrase. We can then complete the sentence re-
duction by multiplying the subject vector with this
verb phrase vector to produce an 5-dimensional
sentence vector. Formally, we define SKMat as:
</bodyText>
<equation confidence="0.62213">
hV (nis, nio) = nis (Vdiag(nio))T
</equation>
<bodyText confidence="0.999875">
and use it in Equation 1. The function hV is
described in Figure 2-(c), where the verb ten-
sor parameters are drawn as a matrix, the sub-
ject as a vector, and the object as a diagonal ma-
</bodyText>
<page confidence="0.982213">
1039
</page>
<bodyText confidence="0.999965176470588">
trix. The graphic demonstrates the two-step com-
bination and the intermediate S x K verb phrase
matrix, as well as the the noun vector product
that results in a two-dimensional vector which is
then transformed using the sigmoid and softmax
functions. Whilst the tensor method captures the
interactions between all pairs of context features
(nsi - noj), SKMat only captures the interactions
between matching features (nsi - noi).
KKMat Given a two-class problem, such as
plausibility classification, the softmax implemen-
tation is overparameterised because the class
membership can be estimated with a single vari-
able. To produce a scalar output, we can learn the
parameters for a single K x K matrix (V) using
standard logistic regression with the mean squared
error cost function:
</bodyText>
<equation confidence="0.983324">
N r l r �
�t&amp;quot;log hV (�s, �o) + (1—t&amp;quot;)loghv (�s, �o)
</equation>
<bodyText confidence="0.999979285714286">
where hV (nis, nio) = (ns)V(nio)T and the objec-
tive is regularised: O(V) + λ2 V 2. This function
is shown in Figure 2-(d), where the verb parame-
ters are shown as a matrix, while the subject and
object are vectors. The output is a single scalar,
which is then transformed with the sigmoid func-
tion. Values over 0.5 are considered plausible.
DMat The final method produces a scalar as in
KKMat, but is distributional and based on corpus
counts rather than regression-based. Grefenstette
and Sadrzadeh (2011) introduced a corpus-based
approach for generating a K x K matrix for each
verb from an average of Kronecker products of the
subject and object vectors from the positively la-
belled subset of the training data. The intuition is
that, for example, the matrix for eat may have a
high value for the contextual topic pair describing
animate subjects and edible objects. To determine
the plausibility of a new subject-object pair for a
particular verb, we calculate the Kronecker prod-
uct of the subject and object noun vectors for this
pair, and compare the resulting matrix with the av-
erage verb matrix using cosine similarity.
For label prediction, we calculate the similar-
ity between each of the training data pairs and the
learned average matrix. Unlike for KKmat, the
class cutoff is estimated at the break-even point
of the receiver operator characteristic (ROC) gen-
erated by comparing the training labels with this
cosine similarity value. The break-even point is
when the true positive rate is equal to the false pos-
itive rate. In practice it would be more accurate
to estimate the cutoff on a validation dataset, but
some of the verbs have so few training instances
that this was not possible.
</bodyText>
<sectionHeader confidence="0.999794" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.98261175">
In order to examine the quality of learning we run
several experiments where we compare the differ-
ent methods. In these experiments we consider
the DMat method as the baseline. Some of the
experiments employ cross-validation, in particular
five repetitions of 2-fold cross validation (5x2cv),
which has been shown to be statistically more ro-
bust than the traditional 10-fold cross validation
(Alpaydin, 1999; Ulas¸ et al., 2012). The results of
5x2cv experiments can be compared using the reg-
ular paired t-test, but the specially designed 5x2cv
F-test has been proven to produce fewer statistical
errors (Ulas¸ et al., 2012).
The performance was evaluated using the area
under the ROC (AUC) and the F1 measure (based
on precision and recall over the plausible class).
The AUC evaluates whether a method is ranking
positive examples above negative ones, regardless
of the class cutoff value. F1 shows how accurately
a method assigns the correct class label. Another
way to interpret the results is to consider the AUC
as the measure of the quality of the parameters in
the verb matrix or tensor, while the F-score indi-
cates how well the softmax, the sigmoid, and the
DMat cutoff algorithm are estimating class partic-
ipation.
Ex-1. In the first experiment, we compare the
different transitive verb representations by running
5x2cv experiments on ten verbs chosen to cover a
range of concreteness and frequency values (Sec-
tion 4.2).
Ex-2. In the initial experiments we found that
some models had low performance, so we applied
the column normalisation technique, which is of-
ten used with regression learning to standardise
the numerical range of features:
</bodyText>
<equation confidence="0.995956666666667">
x� − min(x)
x := (2)
max(�x) − min(�x)
</equation>
<bodyText confidence="0.999929">
This preserves the relative values of features be-
tween training samples, while moving the values
to the [0,1] range.
</bodyText>
<equation confidence="0.98658075">
t
ML
1
O(V) = −
</equation>
<page confidence="0.824393">
1040
</page>
<bodyText confidence="0.999924538461539">
Ex-3. There are varying numbers of training ex-
amples for each of the verbs, so we repeated the
5x2cv with datasets of 52 training points for each
verb, since this is the size of the smallest dataset of
the verb CENSOR. The points were randomly sam-
pled from the datasets used in the first experiment.
Finally, the four verbs with the largest datasets
were used to examine how the performance of the
methods changes as the amount of training data
increases. The 4,000 training samples were ran-
domised and half were used for testing. We sam-
pled between 10 and 1000 training triples from the
other half (Figure 4).
</bodyText>
<subsectionHeader confidence="0.988097">
4.1 Noun vectors
</subsectionHeader>
<bodyText confidence="0.999952352941177">
Distributional semantic models (Turney and Pan-
tel, 2010) encode word meaning in a vector for-
mat by counting co-occurrences with other words
within a specified context window. We con-
structed the vectors from the October 2013 dump
of Wikipedia articles, which was tokenised us-
ing the Stanford NLP tools3, lemmatised with the
Morpha lemmatiser (Minnen et al., 2001), and
parsed with the C&amp;C parser (Clark and Curran,
2007). In this paper we use sentence boundaries to
define context windows and the top 10,000 most
frequent lemmatised words in the whole corpus
(excluding stopwords) as context words. The raw
co-occurrence counts are re-weighted using the
standard tTest weighting scheme (Curran, 2004),
where fwicj is the number of times target noun wi
occurs with context word cj:
</bodyText>
<equation confidence="0.766009666666667">
tTest( ~wi,cj) = p(wi,cj) − p(wi)p(cj) (3)
V p(wi)p(cj)
where p(wi) = Ej fwicj p(c71 = Ei fwicj
Ek El fwkcl Ek El fwkcl
_ fwicj
wi
</equation>
<bodyText confidence="0.986036363636364">
and p(, cj) Ek El fwkcl .
Using all 10,000 context words would result in
a large number of parameters for each verb ten-
sor, and so we apply singular value decomposition
(SVD) (Turney and Pantel, 2010) with 40 latent
dimensions to the target-context word matrix. We
use context selection (with N = 140) and row
normalisation as described in Polajnar and Clark
(2014) to markedly improve the performance of
SVD on smaller dimensions (K) and enable us to
train the verb tensors using very low-dimensional
</bodyText>
<footnote confidence="0.893918">
3http://nlp.stanford.edu/software/index.shtml
</footnote>
<table confidence="0.999813181818182">
Verb Concreteness # of Positive Frequency
APPLY 2.5 5618 47361762
CENSOR 3 26 278525
COMB 5 164 644447
DEPOSE 2.5 118 874463
EAT 4.44 5067 26396728
IDEALIZE 1.17 99 485580
INCUBATE 3.5 82 833621
JUSTIFY 1.45 5636 10517616
REDUCE 2 26917 40336784
WIPE 4 1090 6348595
</table>
<tableCaption confidence="0.862827">
Table 2: The 10 chosen verbs together with their
concreteness scores. The number of positive SVO
</tableCaption>
<bodyText confidence="0.932429333333333">
examples was capped at 2000. Frequency is the
frequency of the verb in the GSN corpus.
noun vectors. Performance of the noun vectors
was measured on standard word similarity datasets
and the results were comparable to those reported
by Polajnar and Clark (2014).
</bodyText>
<subsectionHeader confidence="0.997184">
4.2 Training data
</subsectionHeader>
<bodyText confidence="0.9999555">
In order to generate training data we made use
of two large corpora: the Google Syntactic N-
grams (GSN) (Goldberg and Orwant, 2013) and
the Wikipedia October 2013 dump. We first chose
ten transitive verbs with different concreteness
scores (Brysbaert et al., 2013) and frequencies, in
order to obtain a variety of verb types. Then the
positive (plausible) SVO examples were extracted
from the GSN corpus. More precisely, we col-
lected all distinct syntactic trigrams of the form
nsubj ROOT dobj, where the root of the phrase was
one of our target verbs. We lemmatised the words
using the NLTK4 lemmatiser and filtered these ex-
amples to retain only the ones that contain nouns
that also occur in Wikipedia, obtaining the counts
reported in Table 2.
For every positive training example, we con-
structed a negative (implausible) one by replac-
ing both the subject and the object with a con-
founder, using a standard technique from the se-
lectional preference literature (Chambers and Ju-
rafsky, 2010). A confounder was generated by
choosing a random noun from the same frequency
bucket as the original noun.5 Frequency buckets
of size 10 were constructed by collecting noun fre-
quency counts from the Wikipedia corpus. For ex-
</bodyText>
<footnote confidence="0.934198333333333">
4http://nltk.org/
5Note that the random selection of the confounder could
result in a plausible negative example by chance, but man-
ual inspection of a subset of the data suggests this happens
infrequently for those verbs which select strongly for their
arguments, but more often for those verbs that don’t.
</footnote>
<page confidence="0.931411">
1041
</page>
<table confidence="0.993248066666667">
Verb
Tensor DMat KKMat SKMat 2Mat
Tensor DMat KKMat SKMat 2Mat
AUC F1
APPLY 85.68† 81.46‡ 88.88†‡ 68.02 88.92†‡ 79.27 64.00 81.24‡ 54.06 80.80‡
CENSOR 79.40 85.54 80.55 78.52 83.19 70.66 47.93 73.52 37.86 71.07
COMB 89.41 85.65 88.38 69.20†‡ 89.56 81.15 45.02 81.38 39.67 82.36
DEPOSE 92.70 94.44 93.12 84.47† 93.20 84.60 54.77 84.79 43.79 86.15
EAT 94.62 93.81 95.17 67.92 95.88‡ 88.91 52.45 88.83 56.22 89.95
IDEALIZE 69.56 75.84 72.46 61.19 70.23 66.53 48.28 68.39 31.03 67.43
INCUBATE 89.33 85.53 88.61 70.59 91.40 80.30 50.84 80.90 31.99 84.55
JUSTIFY 85.27† 88.70‡ 89.97‡ 73.56 90.10‡ 79.73 73.71 81.10 54.09 82.52
REDUCE 96.13 95.48 96.69† 79.32 97.21 91.24 71.24‡ 87.46 76.67‡ 92.22
WIPE 85.19 84.47 87.84† 64.93†‡ 81.29 78.57 47.62 80.65 39.50 78.90
MEAN 86.93 87.29 88.37 71.96 88.30 80.30 55.79 81.03 46.69 81.79
</table>
<tableCaption confidence="0.886297">
Table 3: The best AUC and F1 results for all the verbs, where † denotes statistical significance compared
to DMat and ‡ denotes significance compared to Tensor according to the 5x2cv F-test with p &lt; 0.05.
</tableCaption>
<bodyText confidence="0.6101896">
ample, for the plausible triple animal EAT plant,
we generate the implausible triple mountain EAT
product. Some verbs were well represented in the
corpus, so we used up to the top 2,000 most fre-
quent triples for training.
</bodyText>
<figureCaption confidence="0.927093">
Figure 3: The effect of column normalisation (*)
on Tensor and SKMat. Top table shows AUC and
the bottom F1-score, while the error bars indicate
standard deviation.
</figureCaption>
<sectionHeader confidence="0.999932" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999885641025641">
The results from Ex-1 are summarised in Ta-
ble 3. We can see that linear regression can lead
to models that are able to distinguish between
plausible and implausible SVO triples. The Ten-
sor method outperforms DMat, which was pre-
viously shown to produce reasonable verb repre-
sentations in related experiments (Grefenstette and
Sadrzadeh, 2011). 2Mat and KKMat, in turn,
outperform Tensor demonstrating that it is pos-
sible to learn lower dimensional approximations
of the tensor-based framework. 2Mat is an appro-
priate approximation for functions with two inputs
and a sentence space of any dimensionality, while
KKMat is only appropriate for a single valued
sentence space, such as the plausibility or senti-
ment space. Due to method variance and dataset
size there are very few AUC results that are sig-
nificantly better than DMat and even fewer that
outperform Tensor. All methods perform poorly
on the verb IDEALIZE, probably because it has
the lowest concreteness value and is in one of the
smallest datasets. This verb is also particularly dif-
ficult because it does not select strongly for either
its subject or object, and so some of the pseudo-
negative examples are in fact somewhat plausible
(e.g. town IDEALIZE authority or child IDEALIZE
racehorse). In general, this would indicate that
more concrete verbs are easier to learn, as they
have a clearer pattern of preferred property types,
but there is no distinct correlation.
The results of the normalisation experiments
(Ex-2) are shown in Table 4. We can see that the
SKMat method, which performed poorly in Ex-
1 notably improves with normalisation. Tensor
AUC scores also improve through normalisation,
but the F-scores decrease. The rest of the methods,
and in particular DMat are negatively affected by
column normalisation. The results from Ex-1 and
Ex-2 for SKMat and Tensor are summarised in
</bodyText>
<figure confidence="0.995431857142857">
F−Score
AUC
−0.2
0.5
0.8
0.6
0.4
0.2
1.2
0
0
1
1
Tensor
Tensor*
SKMat
SKMat*
Tensor
Tensor*
SKMat
SKMat*
</figure>
<page confidence="0.966231">
1042
</page>
<table confidence="0.998679307692308">
Verb Tensor DMat KKMat SKMat 2Mat Tensor DMat KKMat SKMat 2Mat
AUC F1
APPLY 86.16† 48.63‡ 82.63†‡ 85.73† 85.65† 45.57 46.99 46.17 60.86 76.60†
CENSOR 75.74 71.20 78.00 82.77 78.64 30.43 55.16 65.19 49.59 44.22
COMB 91.67† 62.42‡ 90.85† 89.79† 91.42† 33.37 61.05 71.20 64.56 75.96
DEPOSE 93.96† 54.93‡ 93.56† 93.87† 93.81† 42.73 39.71 73.07 54.51 56.54
EAT 95.64† 47.68‡ 92.92† 94.99†‡ 94.76† 60.42 47.42 58.80 69.05 87.44†
IDEALIZE 69.64 55.98 72.20†‡ 76.71†‡ 71.85† 39.14 49.16 41.75 31.57 50.59
INCUBATE 90.97† 61.31‡ 89.69† 90.19† 90.05† 46.35 53.33 70.45 41.57 63.61
JUSTIFY 89.76† 54.87‡ 87.26†‡ 89.64† 89.05† 47.38 51.40 41.91 63.96 80.55†
REDUCE 96.63† 59.58‡ 94.99†‡ 96.14† 96.53† 51.63 54.27 69.18 69.76 90.77†
WIPE 86.82† 58.02‡ 84.18† 83.65† 86.02† 44.04 55.19 47.84 49.89 75.80
MEAN 87.90 57.66 86.83 88.55 87.98 44.31 51.57 58.76 55.73 70.41
</table>
<tableCaption confidence="0.918979">
Table 4: The best AUC and F1 results for all the verbs with normalised vectors, where † denotes statistical
significance compared to DMat and ‡ denotes significance compared to Tensor according to the 5x2cv
F-test with p &lt; 0.05.
</tableCaption>
<bodyText confidence="0.929699269230769">
Figure 3. This figure also shows that AUC values
have much lower variance, but that high variance
in F-score leads to results that are not statistically
significant.
When considering the size of the datasets (Ex-
3), it would seem from Table 5 that 2Mat is able to
learn from less data than DMat or Tensor. While
this may be true over a 5x2cv experiment on small
data, Figure 4 shows that this view may be overly
simplistic and that different training examples can
influence learning. Analysis of errors shows that
the baseline method mostly generates false nega-
tive errors (i.e. predicting implausible when the
gold standard label is plausible). In contrast, Ten-
sor produces almost equal numbers of false posi-
tives and false negatives, but sometimes produces
false negatives with low frequency nouns (e.g.
bourgeoisie IDEALIZE work), presumably because
there is not enough information in the noun vec-
tor to decide on the correct class. It also produces
some false positive errors when either of the nouns
is plausible (but the triple is implausible), which
would suggest results may be improved by train-
ing with data where only one noun is confounded
or by treating negative data as possibly positive
(Lee and Liu, 2003).
</bodyText>
<sectionHeader confidence="0.999151" genericHeader="evaluation">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999121">
Current methods which derive distributed repre-
sentations for phrases, for example the work of
Socher et al. (2012), typically use only matrix rep-
resentations, and also assume that words, phrases
and sentences all live in the same vector space.
The tensor-based semantic framework is more
flexible, in that it allows different spaces for dif-
ferent grammatical types, which results from it be-
</bodyText>
<table confidence="0.99993925">
Verb Tensor DMat 2Mat
APPLY 95.76 86.50 86.31
CENSOR 82.97 84.09 77.79
COMB 90.13 92.93 95.18
DEPOSE 92.41 91.27 95.61
EAT 99.64 98.25 99.58
IDEALIZE 75.03 76.68 88.98
INCUBATE 91.10 87.20 96.42
JUSTIFY 88.96 88.99 87.31
REDUCE 100.0 99.87 99.46
WIPE 97.20 91.63 96.36
MEAN 91.52 89.94 92.50
</table>
<tableCaption confidence="0.97333">
Table 5: Results show average of 5x2cv AUC on
</tableCaption>
<bodyText confidence="0.996294541666667">
small data (26 positive + 26 negative per verb).
None of the results are significant.
ing tied more closely to a type-driven syntactic de-
scription; however, this flexibility comes at a cost,
since there are many more paramaters to learn.
Various communities are beginning to recog-
nize the additional power that tensor representa-
tions can provide, through the capturing of interac-
tions that are difficult to represent with vectors and
matrices (see e.g. (Ranzato et al., 2010; Sutskever
et al., 2009; Van de Cruys et al., 2012)). Hierar-
chical recursive structures in language potentially
represent a large number of such interactions – the
obvious example for this paper being the interac-
tion between a transitive verb’s subject and object
– and present a significant challenge for machine
learning.
This paper is a practical extension of the work
in Krishnamurthy and Mitchell (2013), which in-
troduced learning of CCG-based function tensors
with logistic regression on a compositional se-
mantics task, but was implemented as a proof-of-
concept with vectors of length 2 and on small,
manually created datasets based on propositional
</bodyText>
<page confidence="0.971197">
1043
</page>
<figure confidence="0.9990675">
apply eat
10 20 40 80 150 300 600 800 1000 2000
1
0.95
0.9
AUC
0.85
DMat
Tensor
2Mat
10 20 40 80 150 300 600 800 1000 2000
0.8
0.75
0.7
AUC
0.95
0.85
0.75
0.65
0.9
0.8
0.7
1
DMat
Tensor
2Mat
10 20 40 80 150 300 600 800 1000 2000
# Training Examples
# Training Examples
reduce
DMat
Tensor
2Mat
10 20 40 80 150 300 600 800 1000 2000
# Training Examples
# Training Examples
justify
AUC
0.95
0.85
0.75
0.65
0.9
0.8
0.7
1
DMat
Tensor
2Mat
AUC 1
0.95
0.9
0.85
0.8
</figure>
<figureCaption confidence="0.970148">
Figure 4: Comparison of DMat, Tensor, and 2Mat methods as the number of training instances in-
creases.
</figureCaption>
<bodyText confidence="0.993967222222222">
logic examples. Here, we go beyond this by learn-
ing tensors using corpus data and by deriving sev-
eral different matrix representations for the verb in
the subject-verb-object (SVO) sentence.
This work can also be thought of as applying
neural network learning techniques to the clas-
sic problem of selectional preference acquisition,
since the design of the pseudo-disambiguation ex-
periments is taken from the literature on selec-
tional preferences (Clark and Weir, 2002; Cham-
bers and Jurafsky, 2010). We do not compare di-
rectly with methods from this literature, e.g. those
based on WordNet (Resnik, 1996; Clark and Weir,
2002) or topic modelling techniques (Seaghdha,
2010), since our goal in this paper is not to ex-
tend the state-of-the-art in that area, but rather to
use selectional preference acquisition as a test bed
for the tensor-based semantic framework.
</bodyText>
<sectionHeader confidence="0.999292" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999693138888889">
In this paper we introduced three dimensionally
reduced representations of the transitive verb ten-
sor defined in the type-driven framework for com-
positional distributional semantics (Coecke et al.,
2010). In a comprehensive experiment on ten dif-
ferent verbs we find no significant difference be-
tween the full tensor representation and the re-
duced representations. The SKMat and 2Mat rep-
resentations have the lowest number of parame-
ters and offer a promising avenue of research for
more complex sentence structures and sentence
spaces. KKMat and DMat also had high scores
on some verbs, but these representations are appli-
cable only in spaces where a single-value output is
appropriate.
In experiments where we varied the amount of
training data, we found that in general more con-
crete verbs can learn from less data. Low con-
creteness verbs require particular care with dataset
design, since some of the seemingly random ex-
amples can be plausible. This problem may be
circumvented by using semi-supervised learning
techniques.
We also found that simple numerical tech-
niques, such as column normalisation, can
markedly alter the values and quality of learning.
On our data, column normalisation has a side-
effect of removing the negative values that were
introduced by the use of tTest weighting measure.
The use of the PPMI weighting scheme and non-
negative matrix factorisation (NMF) (Grefenstette
et al., 2013; Van de Cruys, 2010) could lead to a
similar effect, and should be investigated. Further
numerical techniques for improving the estimation
of the class decision boundary, and consequently
the F-score, will also constitute future work.
</bodyText>
<page confidence="0.994691">
1044
</page>
<sectionHeader confidence="0.995767" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998765509433962">
Ethem Alpaydin. 1999. Combined 5x2 CV F-test
for comparing supervised classification learning al-
gorithms. Neural Computation, 11(8):1885–1892,
November.
Brett W. Bader, Tamara G. Kolda, et al. 2012. Matlab
tensor toolbox version 2.5. Available online, Jan.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-10), pages 1183–1193,
Cambridge, MA.
Marco Baroni, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. Frege in space: A program for com-
positional distributional semantics. Linguistic Is-
sues in Language Technology, 9:5–110.
Marc Brysbaert, Amy Beth Warriner, and Victor Ku-
perman. 2013. Concreteness ratings for 40 thou-
sand generally known English word lemmas. Be-
havior research methods, pages 1–8.
Nathanael Chambers and Dan Jurafsky. 2010. Im-
proving the use of pseudo-words for evaluating se-
lectional preferences. In Proceedings of ACL 2010,
Uppsala, Sweden.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493–552.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187–206.
Stephen Clark. 2013. Type-driven syntax and seman-
tics for composing meaning vectors. In Chris He-
unen, Mehrnoosh Sadrzadeh, and Edward Grefen-
stette, editors, Quantum Physics and Linguistics:
A Compositional, Diagrammatic Discourse, pages
359–377. Oxford University Press.
Daoud Clarke. 2012. A context-theoretic frame-
work for compositionality in distributional seman-
tics. Computational Linguistics, 38(1):41–71.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. In J. van
Bentham, M. Moortgat, and W. Buszkowski, edi-
tors, Linguistic Analysis (Lambek Festschrift), vol-
ume 36, pages 345–384.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh.
David R. Dowty, Robert E. Wall, and Stanley Peters.
1981. Introduction to Montague Semantics. Dor-
drecht.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121–2159, July.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large corpus
of English books. In Second Joint Conference on
Lexical and Computational Semantics, pages 241–
247, Atlanta,Georgia.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 1394–1404,
Edinburgh, Scotland, UK, July.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for compo-
sitional distributional semantics. Proceedings of
the 10th International Conference on Computational
Semantics (IWCS 2013).
Edward Grefenstette. 2013. Category-Theoretic
Quantitative Compositional Distributional Models
of Natural Language Semantics. Ph.D. thesis, Uni-
versity of Oxford.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355–396.
Jayant Krishnamurthy and Tom M Mitchell. 2013.
Vector space semantic parsing: A framework for
compositional vector space models. In Proceed-
ings of the 2013 ACL Workshop on Continuous Vec-
tor Space Models and their Compositionality, Sofia,
Bulgaria.
Wee Sun Lee and Bing Liu. 2003. Learning with posi-
tive and unlabeled examples using weighted logistic
regression. In Proceedings of the Twentieth Interna-
tional Conference on Machine Learning (ICML).
Jean Maillard, Stephen Clark, and Edward Grefen-
stette. 2014. A type-driven tensor-based semantics
for CCG. In Proceedings of the EACL 2014 Type
Theory and Natural Language Semantics Workshop
(TTNLS), Gothenburg, Sweden.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207–223.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08, pages 236–244, Columbus, OH.
Tamara Polajnar and Stephen Clark. 2014. Improving
distributional semantic vectors through context se-
lection and normalisation. In 14th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, EACL’14, Gothenburg, Sweden.
</reference>
<page confidence="0.743618">
1045
</page>
<reference confidence="0.99986124">
M. Ranzato, A. Krizhevsky, and G. E. Hinton. 2010.
Factored 3-way restricted boltzmann machines for
modeling natural images. In Proceedings of the
Thirteenth International Conference on Artificial In-
telligence and Statistics (AISTATS), Sardinia, Italy.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, 61:127–159.
Hinrich Sch¨utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
124.
Diarmuid O Seaghdha. 2010. Latent variable mod-
els of selectional preference. In Proceedings of ACL
2010, Uppsala, Sweden.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1201–
1211, Jeju, Korea.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Chris Manning, Andrew Ng, and Chris
Potts. 2013. Recursive deep models for semantic
compositionality over a sentiment treebank. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP 2013), Seat-
tle, USA.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, MA.
I. Sutskever, R. Salakhutdinov, and J. B. Tenenbaum.
2009. Modelling relational data using bayesian clus-
tered tensor factorization. In Proceedings of Ad-
vances in Neural Information Processing Systems
(NIPS 2009), Vancouver, Canada.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.
Aydın Ulas¸, Olcay Taner Yıldız, and Ethem Alpaydın.
2012. Cost-conscious comparison of supervised
learning algorithms over multiple data sets. Pattern
Recognition, 45(4):1772–1781, April.
Tim Van de Cruys, Laura Rimell, Thierry Poibeau, and
Anna Korhonen. 2012. Multi-way tensor factor-
ization for unsupervised lexical acquisition. In Pro-
ceedings of COLING 2012, Mumbai, India.
Tim Van de Cruys. 2010. A non-negative tensor fac-
torization model for selectional preference induc-
tion. Journal of Natural Language Engineering,
16(4):417–437.
</reference>
<page confidence="0.990891">
1046
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.774482">
<title confidence="0.998737">Reducing Dimensions of Tensors in Type-Driven Distributional Semantics</title>
<author confidence="0.998198">Polajnar Luana Stephen</author>
<affiliation confidence="0.9965615">Computer University of</affiliation>
<address confidence="0.811276">Cambridge,</address>
<email confidence="0.999853">first.last@cl.cam.ac.uk</email>
<abstract confidence="0.998235">Compositional distributional semantics is a subfield of Computational Linguistics which investigates methods for representing the meanings of phrases and sentences. In this paper, we explore implementations of a framework based on Combinatory Categorial Grammar (CCG), in which words with certain grammatical types have meanings represented by multilinear maps (i.e. multi-dimensional arrays, or tensors). An obstacle to full implementation of the framework is the size of these tensors. We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task. We find that the matrices perform as well as, and sometimes even better than, full tensors, allowing a reduction in the number of parameters needed to model the framework.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ethem Alpaydin</author>
</authors>
<title>Combined 5x2 CV F-test for comparing supervised classification learning algorithms.</title>
<date>1999</date>
<journal>Neural Computation,</journal>
<volume>11</volume>
<issue>8</issue>
<contexts>
<context position="18013" citStr="Alpaydin, 1999" startWordPosition="3007" endWordPosition="3008"> the false positive rate. In practice it would be more accurate to estimate the cutoff on a validation dataset, but some of the verbs have so few training instances that this was not possible. 4 Experiments In order to examine the quality of learning we run several experiments where we compare the different methods. In these experiments we consider the DMat method as the baseline. Some of the experiments employ cross-validation, in particular five repetitions of 2-fold cross validation (5x2cv), which has been shown to be statistically more robust than the traditional 10-fold cross validation (Alpaydin, 1999; Ulas¸ et al., 2012). The results of 5x2cv experiments can be compared using the regular paired t-test, but the specially designed 5x2cv F-test has been proven to produce fewer statistical errors (Ulas¸ et al., 2012). The performance was evaluated using the area under the ROC (AUC) and the F1 measure (based on precision and recall over the plausible class). The AUC evaluates whether a method is ranking positive examples above negative ones, regardless of the class cutoff value. F1 shows how accurately a method assigns the correct class label. Another way to interpret the results is to conside</context>
</contexts>
<marker>Alpaydin, 1999</marker>
<rawString>Ethem Alpaydin. 1999. Combined 5x2 CV F-test for comparing supervised classification learning algorithms. Neural Computation, 11(8):1885–1892, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brett W Bader</author>
<author>Tamara G Kolda</author>
</authors>
<title>Matlab tensor toolbox version 2.5. Available online,</title>
<date>2012</date>
<marker>Bader, Kolda, 2012</marker>
<rawString>Brett W. Bader, Tamara G. Kolda, et al. 2012. Matlab tensor toolbox version 2.5. Available online, Jan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP-10),</booktitle>
<pages>1183--1193</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="1221" citStr="Baroni and Zamparelli, 2010" startWordPosition="168" endWordPosition="171">al arrays, or tensors). An obstacle to full implementation of the framework is the size of these tensors. We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task. We find that the matrices perform as well as, and sometimes even better than, full tensors, allowing a reduction in the number of parameters needed to model the framework. 1 Introduction An emerging subfield of computational linguistics is concerned with learning compositional distributional representations of meaning (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Clarke, 2012; Socher et al., 2012; Clark, 2013). The advantage of such representations lies in their potential to combine the benefits of distributional approachs to word meaning (Sch¨utze, 1998; Turney and Pantel, 2010) with the more traditional compositional methods from formal semantics (Dowty et al., 1981). Distributional representations have the properties of robustness, learnability from data, ease of handling ambiguity, and the ability to represent gradations of meaning; whereas compositional models handle the unbounded nature of </context>
<context position="3766" citStr="Baroni and Zamparelli (2010)" startWordPosition="568" endWordPosition="571">l grammar, such as Combinatory Categorial Grammar (Steedman, 2000; Maillard et al., 2014), and also to phrase-structure grammars in a way that a formal linguist would recognize (Baroni et al., 2014). Clark (2013) provides a description of the tensor-based framework aimed more at computational linguists, relying only on the mathematics of multi-linear algebra rather than the category theory used in Coecke et al. (2010). Section 2 repeats some of this description. A major open question associated with the tensor-based semantic framework is how to learn 1This same insight lies behind the work of Baroni and Zamparelli (2010). 1036 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1036–1046, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics the tensors representing the meanings of words with complex types, such as verbs and adjectives. The framework is essentially a compositional framework, providing a recipe for how to combine distributional representations, but leaving open what the underlying vector spaces are and how they can be acquired. One significant challenge is an engineering one: in a wide-coverage grammar, which is abl</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Conference on Empirical Methods in Natural Language Processing (EMNLP-10), pages 1183–1193, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Frege in space: A program for compositional distributional semantics. Linguistic Issues in Language Technology,</title>
<date>2014</date>
<pages>9--5</pages>
<contexts>
<context position="3336" citStr="Baroni et al., 2014" startWordPosition="497" endWordPosition="501">nted by a 3rd-order tensor. Coecke et al. use the grammar of pregroups as the syntactic machinery to construct distributional meaning representations, since both pregroups and vector spaces can be seen as examples of the same abstract structure, which leads to a particularly clean mathematical description of the compositional process. However, the approach applies more generally, for example to other forms of categorial grammar, such as Combinatory Categorial Grammar (Steedman, 2000; Maillard et al., 2014), and also to phrase-structure grammars in a way that a formal linguist would recognize (Baroni et al., 2014). Clark (2013) provides a description of the tensor-based framework aimed more at computational linguists, relying only on the mathematics of multi-linear algebra rather than the category theory used in Coecke et al. (2010). Section 2 repeats some of this description. A major open question associated with the tensor-based semantic framework is how to learn 1This same insight lies behind the work of Baroni and Zamparelli (2010). 1036 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1036–1046, October 25-29, 2014, Doha, Qatar. c�2014 Associati</context>
</contexts>
<marker>Baroni, Bernardi, Zamparelli, 2014</marker>
<rawString>Marco Baroni, Raffaella Bernardi, and Roberto Zamparelli. 2014. Frege in space: A program for compositional distributional semantics. Linguistic Issues in Language Technology, 9:5–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Brysbaert</author>
<author>Amy Beth Warriner</author>
<author>Victor Kuperman</author>
</authors>
<title>Concreteness ratings for 40 thousand generally known English word lemmas. Behavior research methods,</title>
<date>2013</date>
<pages>1--8</pages>
<contexts>
<context position="22430" citStr="Brysbaert et al., 2013" startWordPosition="3746" endWordPosition="3749">osen verbs together with their concreteness scores. The number of positive SVO examples was capped at 2000. Frequency is the frequency of the verb in the GSN corpus. noun vectors. Performance of the noun vectors was measured on standard word similarity datasets and the results were comparable to those reported by Polajnar and Clark (2014). 4.2 Training data In order to generate training data we made use of two large corpora: the Google Syntactic Ngrams (GSN) (Goldberg and Orwant, 2013) and the Wikipedia October 2013 dump. We first chose ten transitive verbs with different concreteness scores (Brysbaert et al., 2013) and frequencies, in order to obtain a variety of verb types. Then the positive (plausible) SVO examples were extracted from the GSN corpus. More precisely, we collected all distinct syntactic trigrams of the form nsubj ROOT dobj, where the root of the phrase was one of our target verbs. We lemmatised the words using the NLTK4 lemmatiser and filtered these examples to retain only the ones that contain nouns that also occur in Wikipedia, obtaining the counts reported in Table 2. For every positive training example, we constructed a negative (implausible) one by replacing both the subject and th</context>
</contexts>
<marker>Brysbaert, Warriner, Kuperman, 2013</marker>
<rawString>Marc Brysbaert, Amy Beth Warriner, and Victor Kuperman. 2013. Concreteness ratings for 40 thousand generally known English word lemmas. Behavior research methods, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Improving the use of pseudo-words for evaluating selectional preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL 2010,</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="23157" citStr="Chambers and Jurafsky, 2010" startWordPosition="3868" endWordPosition="3872">mples were extracted from the GSN corpus. More precisely, we collected all distinct syntactic trigrams of the form nsubj ROOT dobj, where the root of the phrase was one of our target verbs. We lemmatised the words using the NLTK4 lemmatiser and filtered these examples to retain only the ones that contain nouns that also occur in Wikipedia, obtaining the counts reported in Table 2. For every positive training example, we constructed a negative (implausible) one by replacing both the subject and the object with a confounder, using a standard technique from the selectional preference literature (Chambers and Jurafsky, 2010). A confounder was generated by choosing a random noun from the same frequency bucket as the original noun.5 Frequency buckets of size 10 were constructed by collecting noun frequency counts from the Wikipedia corpus. For ex4http://nltk.org/ 5Note that the random selection of the confounder could result in a plausible negative example by chance, but manual inspection of a subset of the data suggests this happens infrequently for those verbs which select strongly for their arguments, but more often for those verbs that don’t. 1041 Verb Tensor DMat KKMat SKMat 2Mat Tensor DMat KKMat SKMat 2Mat A</context>
<context position="32322" citStr="Chambers and Jurafsky, 2010" startWordPosition="5389" endWordPosition="5393">Mat AUC 1 0.95 0.9 0.85 0.8 Figure 4: Comparison of DMat, Tensor, and 2Mat methods as the number of training instances increases. logic examples. Here, we go beyond this by learning tensors using corpus data and by deriving several different matrix representations for the verb in the subject-verb-object (SVO) sentence. This work can also be thought of as applying neural network learning techniques to the classic problem of selectional preference acquisition, since the design of the pseudo-disambiguation experiments is taken from the literature on selectional preferences (Clark and Weir, 2002; Chambers and Jurafsky, 2010). We do not compare directly with methods from this literature, e.g. those based on WordNet (Resnik, 1996; Clark and Weir, 2002) or topic modelling techniques (Seaghdha, 2010), since our goal in this paper is not to extend the state-of-the-art in that area, but rather to use selectional preference acquisition as a test bed for the tensor-based semantic framework. 7 Conclusion In this paper we introduced three dimensionally reduced representations of the transitive verb tensor defined in the type-driven framework for compositional distributional semantics (Coecke et al., 2010). In a comprehensi</context>
</contexts>
<marker>Chambers, Jurafsky, 2010</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2010. Improving the use of pseudo-words for evaluating selectional preferences. In Proceedings of ACL 2010, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="8645" citStr="Clark and Curran (2007)" startWordPosition="1374" endWordPosition="1377"> application (N/N N ==&gt;. N), which is an instance of function application. Function application is also used in the tensor-based semantics, which, for a matrix and vector argument, corresponds to matrix multiplication. Figure 1 shows how the syntactic types combine with a transitive verb, and the corresponding tensor-based semantic types. Note that, after the verb has combined with its object NP, the type of the verb phrase is SNP, with a corresponding meaning tensor (matrix) in S ® N. This matrix then combines with the subject vector, through 2In practice, for example using the CCG parser of Clark and Curran (2007), there will be additional atomic categories, such as PP, but not many more. 1037 DMat 2Mat Tensor SKMat KKMat people eat fish K2 V Θ 0 4K 8 2K2 4 2K 4 K2 0 NP (S\NP)/NP NP N S ® N ® N N S\NP S ® N S S Figure 1: Syntactic reduction and tensor-based semantic types for a transitive verb sentence matrix multiplication, to give a sentence vector. In practice, using for example the widecoverage grammar from CCGbank (Hockenmaier and Steedman, 2007), there will be many types with more than 3 slashes, with corresponding higher-order tensors. For example, a common category for a preposition is the foll</context>
<context position="20483" citStr="Clark and Curran, 2007" startWordPosition="3424" endWordPosition="3427">unt of training data increases. The 4,000 training samples were randomised and half were used for testing. We sampled between 10 and 1000 training triples from the other half (Figure 4). 4.1 Noun vectors Distributional semantic models (Turney and Pantel, 2010) encode word meaning in a vector format by counting co-occurrences with other words within a specified context window. We constructed the vectors from the October 2013 dump of Wikipedia articles, which was tokenised using the Stanford NLP tools3, lemmatised with the Morpha lemmatiser (Minnen et al., 2001), and parsed with the C&amp;C parser (Clark and Curran, 2007). In this paper we use sentence boundaries to define context windows and the top 10,000 most frequent lemmatised words in the whole corpus (excluding stopwords) as context words. The raw co-occurrence counts are re-weighted using the standard tTest weighting scheme (Curran, 2004), where fwicj is the number of times target noun wi occurs with context word cj: tTest( ~wi,cj) = p(wi,cj) − p(wi)p(cj) (3) V p(wi)p(cj) where p(wi) = Ej fwicj p(c71 = Ei fwicj Ek El fwkcl Ek El fwkcl _ fwicj wi and p(, cj) Ek El fwkcl . Using all 10,000 context words would result in a large number of parameters for ea</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>David Weir</author>
</authors>
<title>Class-based probability estimation using a semantic hierarchy.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="32292" citStr="Clark and Weir, 2002" startWordPosition="5385" endWordPosition="5388">.8 0.7 1 DMat Tensor 2Mat AUC 1 0.95 0.9 0.85 0.8 Figure 4: Comparison of DMat, Tensor, and 2Mat methods as the number of training instances increases. logic examples. Here, we go beyond this by learning tensors using corpus data and by deriving several different matrix representations for the verb in the subject-verb-object (SVO) sentence. This work can also be thought of as applying neural network learning techniques to the classic problem of selectional preference acquisition, since the design of the pseudo-disambiguation experiments is taken from the literature on selectional preferences (Clark and Weir, 2002; Chambers and Jurafsky, 2010). We do not compare directly with methods from this literature, e.g. those based on WordNet (Resnik, 1996; Clark and Weir, 2002) or topic modelling techniques (Seaghdha, 2010), since our goal in this paper is not to extend the state-of-the-art in that area, but rather to use selectional preference acquisition as a test bed for the tensor-based semantic framework. 7 Conclusion In this paper we introduced three dimensionally reduced representations of the transitive verb tensor defined in the type-driven framework for compositional distributional semantics (Coecke e</context>
</contexts>
<marker>Clark, Weir, 2002</marker>
<rawString>Stephen Clark and David Weir. 2002. Class-based probability estimation using a semantic hierarchy. Computational Linguistics, 28(2):187–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>Type-driven syntax and semantics for composing meaning vectors.</title>
<date>2013</date>
<booktitle>Quantum Physics and Linguistics: A Compositional, Diagrammatic Discourse,</booktitle>
<pages>359--377</pages>
<editor>In Chris Heunen, Mehrnoosh Sadrzadeh, and Edward Grefenstette, editors,</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="1325" citStr="Clark, 2013" startWordPosition="186" endWordPosition="187">e performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task. We find that the matrices perform as well as, and sometimes even better than, full tensors, allowing a reduction in the number of parameters needed to model the framework. 1 Introduction An emerging subfield of computational linguistics is concerned with learning compositional distributional representations of meaning (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Clarke, 2012; Socher et al., 2012; Clark, 2013). The advantage of such representations lies in their potential to combine the benefits of distributional approachs to word meaning (Sch¨utze, 1998; Turney and Pantel, 2010) with the more traditional compositional methods from formal semantics (Dowty et al., 1981). Distributional representations have the properties of robustness, learnability from data, ease of handling ambiguity, and the ability to represent gradations of meaning; whereas compositional models handle the unbounded nature of natural language, as well as providing established accounts of logical words, quantification, and infere</context>
<context position="3350" citStr="Clark (2013)" startWordPosition="502" endWordPosition="503">nsor. Coecke et al. use the grammar of pregroups as the syntactic machinery to construct distributional meaning representations, since both pregroups and vector spaces can be seen as examples of the same abstract structure, which leads to a particularly clean mathematical description of the compositional process. However, the approach applies more generally, for example to other forms of categorial grammar, such as Combinatory Categorial Grammar (Steedman, 2000; Maillard et al., 2014), and also to phrase-structure grammars in a way that a formal linguist would recognize (Baroni et al., 2014). Clark (2013) provides a description of the tensor-based framework aimed more at computational linguists, relying only on the mathematics of multi-linear algebra rather than the category theory used in Coecke et al. (2010). Section 2 repeats some of this description. A major open question associated with the tensor-based semantic framework is how to learn 1This same insight lies behind the work of Baroni and Zamparelli (2010). 1036 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1036–1046, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computa</context>
<context position="5427" citStr="Clark (2013)" startWordPosition="831" endWordPosition="832">nt basic types, for example nouns and sentences, to live in different vector spaces. This means that the sentence space is task specific, and must be defined in advance. For example, to calculate sentence similarity, we would have to learn a vector space where distances between vectors representing the meanings of sentences reflect similarity scores assigned by human annotators. In this paper we describe an initial investigation into the learning of word meanings with complex syntactic types, together with a simple sentence space. The space we consider is the “plausibility space” described by Clark (2013), together with sentences of the form subject-verbobject. This space is defined to distinguish semantically plausible sentences (e.g. Animals eat plants) from implausible ones (e.g. Animals eat planets). Plausibility can be either represented as a single continuous variable between 0 and 1, or as a two-dimensional probability distribution over the classes plausible (T) and implausible (1). Whether we consider a one- or two-dimensional sentence space depends on the architecture of the logistic regression classifier that is used to learn the verb (Section 3). We begin with this simple plausibili</context>
</contexts>
<marker>Clark, 2013</marker>
<rawString>Stephen Clark. 2013. Type-driven syntax and semantics for composing meaning vectors. In Chris Heunen, Mehrnoosh Sadrzadeh, and Edward Grefenstette, editors, Quantum Physics and Linguistics: A Compositional, Diagrammatic Discourse, pages 359–377. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>A context-theoretic framework for compositionality in distributional semantics.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="1290" citStr="Clarke, 2012" startWordPosition="180" endWordPosition="181">ize of these tensors. We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task. We find that the matrices perform as well as, and sometimes even better than, full tensors, allowing a reduction in the number of parameters needed to model the framework. 1 Introduction An emerging subfield of computational linguistics is concerned with learning compositional distributional representations of meaning (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Clarke, 2012; Socher et al., 2012; Clark, 2013). The advantage of such representations lies in their potential to combine the benefits of distributional approachs to word meaning (Sch¨utze, 1998; Turney and Pantel, 2010) with the more traditional compositional methods from formal semantics (Dowty et al., 1981). Distributional representations have the properties of robustness, learnability from data, ease of handling ambiguity, and the ability to represent gradations of meaning; whereas compositional models handle the unbounded nature of natural language, as well as providing established accounts of logica</context>
</contexts>
<marker>Clarke, 2012</marker>
<rawString>Daoud Clarke. 2012. A context-theoretic framework for compositionality in distributional semantics. Computational Linguistics, 38(1):41–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning.</title>
<date>2010</date>
<booktitle>Linguistic Analysis (Lambek Festschrift),</booktitle>
<volume>36</volume>
<pages>345--384</pages>
<editor>In J. van Bentham, M. Moortgat, and W. Buszkowski, editors,</editor>
<contexts>
<context position="1242" citStr="Coecke et al., 2010" startWordPosition="172" endWordPosition="175">stacle to full implementation of the framework is the size of these tensors. We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task. We find that the matrices perform as well as, and sometimes even better than, full tensors, allowing a reduction in the number of parameters needed to model the framework. 1 Introduction An emerging subfield of computational linguistics is concerned with learning compositional distributional representations of meaning (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Clarke, 2012; Socher et al., 2012; Clark, 2013). The advantage of such representations lies in their potential to combine the benefits of distributional approachs to word meaning (Sch¨utze, 1998; Turney and Pantel, 2010) with the more traditional compositional methods from formal semantics (Dowty et al., 1981). Distributional representations have the properties of robustness, learnability from data, ease of handling ambiguity, and the ability to represent gradations of meaning; whereas compositional models handle the unbounded nature of natural language, as </context>
<context position="3559" citStr="Coecke et al. (2010)" startWordPosition="534" endWordPosition="537">me abstract structure, which leads to a particularly clean mathematical description of the compositional process. However, the approach applies more generally, for example to other forms of categorial grammar, such as Combinatory Categorial Grammar (Steedman, 2000; Maillard et al., 2014), and also to phrase-structure grammars in a way that a formal linguist would recognize (Baroni et al., 2014). Clark (2013) provides a description of the tensor-based framework aimed more at computational linguists, relying only on the mathematics of multi-linear algebra rather than the category theory used in Coecke et al. (2010). Section 2 repeats some of this description. A major open question associated with the tensor-based semantic framework is how to learn 1This same insight lies behind the work of Baroni and Zamparelli (2010). 1036 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1036–1046, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics the tensors representing the meanings of words with complex types, such as verbs and adjectives. The framework is essentially a compositional framework, providing a recipe for how to combine</context>
<context position="32904" citStr="Coecke et al., 2010" startWordPosition="5481" endWordPosition="5484">ir, 2002; Chambers and Jurafsky, 2010). We do not compare directly with methods from this literature, e.g. those based on WordNet (Resnik, 1996; Clark and Weir, 2002) or topic modelling techniques (Seaghdha, 2010), since our goal in this paper is not to extend the state-of-the-art in that area, but rather to use selectional preference acquisition as a test bed for the tensor-based semantic framework. 7 Conclusion In this paper we introduced three dimensionally reduced representations of the transitive verb tensor defined in the type-driven framework for compositional distributional semantics (Coecke et al., 2010). In a comprehensive experiment on ten different verbs we find no significant difference between the full tensor representation and the reduced representations. The SKMat and 2Mat representations have the lowest number of parameters and offer a promising avenue of research for more complex sentence structures and sentence spaces. KKMat and DMat also had high scores on some verbs, but these representations are applicable only in spaces where a single-value output is appropriate. In experiments where we varied the amount of training data, we found that in general more concrete verbs can learn fr</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010. Mathematical foundations for a compositional distributional model of meaning. In J. van Bentham, M. Moortgat, and W. Buszkowski, editors, Linguistic Analysis (Lambek Festschrift), volume 36, pages 345–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="20763" citStr="Curran, 2004" startWordPosition="3468" endWordPosition="3469">or format by counting co-occurrences with other words within a specified context window. We constructed the vectors from the October 2013 dump of Wikipedia articles, which was tokenised using the Stanford NLP tools3, lemmatised with the Morpha lemmatiser (Minnen et al., 2001), and parsed with the C&amp;C parser (Clark and Curran, 2007). In this paper we use sentence boundaries to define context windows and the top 10,000 most frequent lemmatised words in the whole corpus (excluding stopwords) as context words. The raw co-occurrence counts are re-weighted using the standard tTest weighting scheme (Curran, 2004), where fwicj is the number of times target noun wi occurs with context word cj: tTest( ~wi,cj) = p(wi,cj) − p(wi)p(cj) (3) V p(wi)p(cj) where p(wi) = Ej fwicj p(c71 = Ei fwicj Ek El fwkcl Ek El fwkcl _ fwicj wi and p(, cj) Ek El fwkcl . Using all 10,000 context words would result in a large number of parameters for each verb tensor, and so we apply singular value decomposition (SVD) (Turney and Pantel, 2010) with 40 latent dimensions to the target-context word matrix. We use context selection (with N = 140) and row normalisation as described in Polajnar and Clark (2014) to markedly improve th</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>James R. Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Dowty</author>
<author>Robert E Wall</author>
<author>Stanley Peters</author>
</authors>
<title>Introduction to Montague Semantics.</title>
<date>1981</date>
<location>Dordrecht.</location>
<contexts>
<context position="1589" citStr="Dowty et al., 1981" startWordPosition="225" endWordPosition="228">er of parameters needed to model the framework. 1 Introduction An emerging subfield of computational linguistics is concerned with learning compositional distributional representations of meaning (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Clarke, 2012; Socher et al., 2012; Clark, 2013). The advantage of such representations lies in their potential to combine the benefits of distributional approachs to word meaning (Sch¨utze, 1998; Turney and Pantel, 2010) with the more traditional compositional methods from formal semantics (Dowty et al., 1981). Distributional representations have the properties of robustness, learnability from data, ease of handling ambiguity, and the ability to represent gradations of meaning; whereas compositional models handle the unbounded nature of natural language, as well as providing established accounts of logical words, quantification, and inference. One promising approach which attempts to combine elements of compositional and distributional semantics is by Coecke et al. (2010). The underlying idea is to take the type-driven approach from formal semantics — in particular the idea that the meanings of com</context>
</contexts>
<marker>Dowty, Wall, Peters, 1981</marker>
<rawString>David R. Dowty, Robert E. Wall, and Stanley Peters. 1981. Introduction to Montague Semantics. Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>12--2121</pages>
<contexts>
<context position="11055" citStr="Duchi et al., 2011" startWordPosition="1796" endWordPosition="1799">hy and Mitchell (2013) by learning a tensor as parameters in a softmax classifier. We introduce three related methods (2Mat, SKMat, KKMat), all of which model the verb as a matrix or a pair of matrices (Figure 2). Table 1 gives the number of Table 1: Number of parameters per method. parameters for each method. Tensor, 2Mat, and SKMat all have a two-dimensional S space, while KKMat produces a scalar value. In all of these learning-based methods the derivatives were obtained via the chain rule with respect to each set of parameters and gradient descent was performed using the Adagrad algorithm (Duchi et al., 2011). We also reimplement a distributional method (DMat), which was previously used in SVO experiments with the type-driven framework (Grefenstette and Sadrzadeh, 2011). While the other methods are trained as plausibility classifiers, in DMat we estimate the class boundary from cosine similarity via training data (see explanation below). Tensor If subject (ns) and object (no) nouns are K-dimensional vectors and the plausibility vector is S-dimensional with S = 2, we can learn the values of the K x K x S tensor representing the verb as parameters (V) of a regression algorithm. To represent this spa</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Res., 12:2121–2159, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Jon Orwant</author>
</authors>
<title>A dataset of syntactic-ngrams over time from a very large corpus of English books.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>241--247</pages>
<contexts>
<context position="22297" citStr="Goldberg and Orwant, 2013" startWordPosition="3726" endWordPosition="3729">IDEALIZE 1.17 99 485580 INCUBATE 3.5 82 833621 JUSTIFY 1.45 5636 10517616 REDUCE 2 26917 40336784 WIPE 4 1090 6348595 Table 2: The 10 chosen verbs together with their concreteness scores. The number of positive SVO examples was capped at 2000. Frequency is the frequency of the verb in the GSN corpus. noun vectors. Performance of the noun vectors was measured on standard word similarity datasets and the results were comparable to those reported by Polajnar and Clark (2014). 4.2 Training data In order to generate training data we made use of two large corpora: the Google Syntactic Ngrams (GSN) (Goldberg and Orwant, 2013) and the Wikipedia October 2013 dump. We first chose ten transitive verbs with different concreteness scores (Brysbaert et al., 2013) and frequencies, in order to obtain a variety of verb types. Then the positive (plausible) SVO examples were extracted from the GSN corpus. More precisely, we collected all distinct syntactic trigrams of the form nsubj ROOT dobj, where the root of the phrase was one of our target verbs. We lemmatised the words using the NLTK4 lemmatiser and filtered these examples to retain only the ones that contain nouns that also occur in Wikipedia, obtaining the counts repor</context>
</contexts>
<marker>Goldberg, Orwant, 2013</marker>
<rawString>Yoav Goldberg and Jon Orwant. 2013. A dataset of syntactic-ngrams over time from a very large corpus of English books. In Second Joint Conference on Lexical and Computational Semantics, pages 241– 247, Atlanta,Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1394--1404</pages>
<location>Edinburgh, Scotland, UK,</location>
<contexts>
<context position="1276" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="176" endWordPosition="179">entation of the framework is the size of these tensors. We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task. We find that the matrices perform as well as, and sometimes even better than, full tensors, allowing a reduction in the number of parameters needed to model the framework. 1 Introduction An emerging subfield of computational linguistics is concerned with learning compositional distributional representations of meaning (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Clarke, 2012; Socher et al., 2012; Clark, 2013). The advantage of such representations lies in their potential to combine the benefits of distributional approachs to word meaning (Sch¨utze, 1998; Turney and Pantel, 2010) with the more traditional compositional methods from formal semantics (Dowty et al., 1981). Distributional representations have the properties of robustness, learnability from data, ease of handling ambiguity, and the ability to represent gradations of meaning; whereas compositional models handle the unbounded nature of natural language, as well as providing established acco</context>
<context position="11219" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="1818" endWordPosition="1821">ch model the verb as a matrix or a pair of matrices (Figure 2). Table 1 gives the number of Table 1: Number of parameters per method. parameters for each method. Tensor, 2Mat, and SKMat all have a two-dimensional S space, while KKMat produces a scalar value. In all of these learning-based methods the derivatives were obtained via the chain rule with respect to each set of parameters and gradient descent was performed using the Adagrad algorithm (Duchi et al., 2011). We also reimplement a distributional method (DMat), which was previously used in SVO experiments with the type-driven framework (Grefenstette and Sadrzadeh, 2011). While the other methods are trained as plausibility classifiers, in DMat we estimate the class boundary from cosine similarity via training data (see explanation below). Tensor If subject (ns) and object (no) nouns are K-dimensional vectors and the plausibility vector is S-dimensional with S = 2, we can learn the values of the K x K x S tensor representing the verb as parameters (V) of a regression algorithm. To represent this space as a distribution over two classes (T,⊥) we apply a sigmoid function (Q) to restrict the output to the [0,1] range and the softmax activation function (g) to bal</context>
<context position="16395" citStr="Grefenstette and Sadrzadeh (2011)" startWordPosition="2738" endWordPosition="2741">ing standard logistic regression with the mean squared error cost function: N r l r � �t&amp;quot;log hV (�s, �o) + (1—t&amp;quot;)loghv (�s, �o) where hV (nis, nio) = (ns)V(nio)T and the objective is regularised: O(V) + λ2 V 2. This function is shown in Figure 2-(d), where the verb parameters are shown as a matrix, while the subject and object are vectors. The output is a single scalar, which is then transformed with the sigmoid function. Values over 0.5 are considered plausible. DMat The final method produces a scalar as in KKMat, but is distributional and based on corpus counts rather than regression-based. Grefenstette and Sadrzadeh (2011) introduced a corpus-based approach for generating a K x K matrix for each verb from an average of Kronecker products of the subject and object vectors from the positively labelled subset of the training data. The intuition is that, for example, the matrix for eat may have a high value for the contextual topic pair describing animate subjects and edible objects. To determine the plausibility of a new subject-object pair for a particular verb, we calculate the Kronecker product of the subject and object noun vectors for this pair, and compare the resulting matrix with the average verb matrix us</context>
<context position="25460" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="4248" endWordPosition="4251">. Some verbs were well represented in the corpus, so we used up to the top 2,000 most frequent triples for training. Figure 3: The effect of column normalisation (*) on Tensor and SKMat. Top table shows AUC and the bottom F1-score, while the error bars indicate standard deviation. 5 Results The results from Ex-1 are summarised in Table 3. We can see that linear regression can lead to models that are able to distinguish between plausible and implausible SVO triples. The Tensor method outperforms DMat, which was previously shown to produce reasonable verb representations in related experiments (Grefenstette and Sadrzadeh, 2011). 2Mat and KKMat, in turn, outperform Tensor demonstrating that it is possible to learn lower dimensional approximations of the tensor-based framework. 2Mat is an appropriate approximation for functions with two inputs and a sentence space of any dimensionality, while KKMat is only appropriate for a single valued sentence space, such as the plausibility or sentiment space. Due to method variance and dataset size there are very few AUC results that are significantly better than DMat and even fewer that outperform Tensor. All methods perform poorly on the verb IDEALIZE, probably because it has t</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394–1404, Edinburgh, Scotland, UK, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Georgiana Dinu</author>
<author>Yao-Zhong Zhang</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Marco Baroni</author>
</authors>
<title>Multi-step regression learning for compositional distributional semantics.</title>
<date>2013</date>
<booktitle>Proceedings of the 10th International Conference on Computational Semantics (IWCS</booktitle>
<marker>Grefenstette, Dinu, Zhang, Sadrzadeh, Baroni, 2013</marker>
<rawString>Edward Grefenstette, Georgiana Dinu, Yao-Zhong Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni. 2013. Multi-step regression learning for compositional distributional semantics. Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
</authors>
<title>Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics.</title>
<date>2013</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Oxford.</institution>
<contexts>
<context position="7796" citStr="Grefenstette, 2013" startWordPosition="1229" endWordPosition="1230">e the meaning of a noun or noun phrase, for example people, will be a vector ���� in the noun space: people E N. In order to obtain the meaning of a transitive verb, each slash is replaced with a tensor product operator, so that the meaning of eat, for example, is a 3rd-order tensor: eat E S ® N ® N. Just as in the syntactic case, the meaning of a transitive verb is a function (a multi-linear map) which takes two noun vectors as arguments and returns a sentence vector. Meanings combine using tensor contraction, which can be thought of as a multi-linear generalisation of matrix multiplication (Grefenstette, 2013). Consider first the adjective-noun case, for example black cat. The syntactic type of black is N/N; hence its meaning is a 2nd-order tensor (matrix): black E N ® N. In the syntax, N/N combines with N using the rule of forward application (N/N N ==&gt;. N), which is an instance of function application. Function application is also used in the tensor-based semantics, which, for a matrix and vector argument, corresponds to matrix multiplication. Figure 1 shows how the syntactic types combine with a transitive verb, and the corresponding tensor-based semantic types. Note that, after the verb has com</context>
</contexts>
<marker>Grefenstette, 2013</marker>
<rawString>Edward Grefenstette. 2013. Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics. Ph.D. thesis, University of Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="9091" citStr="Hockenmaier and Steedman, 2007" startWordPosition="1460" endWordPosition="1463">th a corresponding meaning tensor (matrix) in S ® N. This matrix then combines with the subject vector, through 2In practice, for example using the CCG parser of Clark and Curran (2007), there will be additional atomic categories, such as PP, but not many more. 1037 DMat 2Mat Tensor SKMat KKMat people eat fish K2 V Θ 0 4K 8 2K2 4 2K 4 K2 0 NP (S\NP)/NP NP N S ® N ® N N S\NP S ® N S S Figure 1: Syntactic reduction and tensor-based semantic types for a transitive verb sentence matrix multiplication, to give a sentence vector. In practice, using for example the widecoverage grammar from CCGbank (Hockenmaier and Steedman, 2007), there will be many types with more than 3 slashes, with corresponding higher-order tensors. For example, a common category for a preposition is the following: ((S\NP)\(S\NP))/NP, which would be assigned to WITH in eat WITH a fork. (The way to read the syntactic type is as follows: with requires an NP argument to the right – a fork in this example – and then a verb phrase to the left – eat with type S\NP – resulting in a verb phrase S\NP.) The corresponding meaning tensor lives in the tensor space S ® N ® S ® N ® N, i.e. a 5th-order tensor. Categories with even more slashes are not uncommon, </context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Tom M Mitchell</author>
</authors>
<title>Vector space semantic parsing: A framework for compositional vector space models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 ACL Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="10458" citStr="Krishnamurthy and Mitchell (2013)" startWordPosition="1693" endWordPosition="1697">er dimensional approximations will be required. 3 Methods In this paper we compare five different methods for modelling the type-driven semantic representation of subject-verb-object sentences. The tensor is a function that encodes the meaning of a verb. It takes two vectors from the K-dimensional noun space as input, and produces a representation of the sentence in the S-dimensional sentence space. In this paper, we consider a plausibility space where S is either a single variable or a twodimensional space over two classes: plausible (T) and implausible (⊥). The first method (Tensor) follows Krishnamurthy and Mitchell (2013) by learning a tensor as parameters in a softmax classifier. We introduce three related methods (2Mat, SKMat, KKMat), all of which model the verb as a matrix or a pair of matrices (Figure 2). Table 1 gives the number of Table 1: Number of parameters per method. parameters for each method. Tensor, 2Mat, and SKMat all have a two-dimensional S space, while KKMat produces a scalar value. In all of these learning-based methods the derivatives were obtained via the chain rule with respect to each set of parameters and gradient descent was performed using the Adagrad algorithm (Duchi et al., 2011). W</context>
<context position="31012" citStr="Krishnamurthy and Mitchell (2013)" startWordPosition="5156" endWordPosition="5159">unities are beginning to recognize the additional power that tensor representations can provide, through the capturing of interactions that are difficult to represent with vectors and matrices (see e.g. (Ranzato et al., 2010; Sutskever et al., 2009; Van de Cruys et al., 2012)). Hierarchical recursive structures in language potentially represent a large number of such interactions – the obvious example for this paper being the interaction between a transitive verb’s subject and object – and present a significant challenge for machine learning. This paper is a practical extension of the work in Krishnamurthy and Mitchell (2013), which introduced learning of CCG-based function tensors with logistic regression on a compositional semantics task, but was implemented as a proof-ofconcept with vectors of length 2 and on small, manually created datasets based on propositional 1043 apply eat 10 20 40 80 150 300 600 800 1000 2000 1 0.95 0.9 AUC 0.85 DMat Tensor 2Mat 10 20 40 80 150 300 600 800 1000 2000 0.8 0.75 0.7 AUC 0.95 0.85 0.75 0.65 0.9 0.8 0.7 1 DMat Tensor 2Mat 10 20 40 80 150 300 600 800 1000 2000 # Training Examples # Training Examples reduce DMat Tensor 2Mat 10 20 40 80 150 300 600 800 1000 2000 # Training Exampl</context>
</contexts>
<marker>Krishnamurthy, Mitchell, 2013</marker>
<rawString>Jayant Krishnamurthy and Tom M Mitchell. 2013. Vector space semantic parsing: A framework for compositional vector space models. In Proceedings of the 2013 ACL Workshop on Continuous Vector Space Models and their Compositionality, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Sun Lee</author>
<author>Bing Liu</author>
</authors>
<title>Learning with positive and unlabeled examples using weighted logistic regression.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twentieth International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="29386" citStr="Lee and Liu, 2003" startWordPosition="4893" endWordPosition="4896">when the gold standard label is plausible). In contrast, Tensor produces almost equal numbers of false positives and false negatives, but sometimes produces false negatives with low frequency nouns (e.g. bourgeoisie IDEALIZE work), presumably because there is not enough information in the noun vector to decide on the correct class. It also produces some false positive errors when either of the nouns is plausible (but the triple is implausible), which would suggest results may be improved by training with data where only one noun is confounded or by treating negative data as possibly positive (Lee and Liu, 2003). 6 Discussion Current methods which derive distributed representations for phrases, for example the work of Socher et al. (2012), typically use only matrix representations, and also assume that words, phrases and sentences all live in the same vector space. The tensor-based semantic framework is more flexible, in that it allows different spaces for different grammatical types, which results from it beVerb Tensor DMat 2Mat APPLY 95.76 86.50 86.31 CENSOR 82.97 84.09 77.79 COMB 90.13 92.93 95.18 DEPOSE 92.41 91.27 95.61 EAT 99.64 98.25 99.58 IDEALIZE 75.03 76.68 88.98 INCUBATE 91.10 87.20 96.42 </context>
</contexts>
<marker>Lee, Liu, 2003</marker>
<rawString>Wee Sun Lee and Bing Liu. 2003. Learning with positive and unlabeled examples using weighted logistic regression. In Proceedings of the Twentieth International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Maillard</author>
<author>Stephen Clark</author>
<author>Edward Grefenstette</author>
</authors>
<title>A type-driven tensor-based semantics for CCG.</title>
<date>2014</date>
<booktitle>In Proceedings of the EACL 2014 Type Theory and Natural Language Semantics Workshop (TTNLS), Gothenburg,</booktitle>
<contexts>
<context position="3227" citStr="Maillard et al., 2014" startWordPosition="479" endWordPosition="482">, for example, is represented by a matrix (a 2nd-order tensor)1 and the meaning of a transitive verb is represented by a 3rd-order tensor. Coecke et al. use the grammar of pregroups as the syntactic machinery to construct distributional meaning representations, since both pregroups and vector spaces can be seen as examples of the same abstract structure, which leads to a particularly clean mathematical description of the compositional process. However, the approach applies more generally, for example to other forms of categorial grammar, such as Combinatory Categorial Grammar (Steedman, 2000; Maillard et al., 2014), and also to phrase-structure grammars in a way that a formal linguist would recognize (Baroni et al., 2014). Clark (2013) provides a description of the tensor-based framework aimed more at computational linguists, relying only on the mathematics of multi-linear algebra rather than the category theory used in Coecke et al. (2010). Section 2 repeats some of this description. A major open question associated with the tensor-based semantic framework is how to learn 1This same insight lies behind the work of Baroni and Zamparelli (2010). 1036 Proceedings of the 2014 Conference on Empirical Method</context>
</contexts>
<marker>Maillard, Clark, Grefenstette, 2014</marker>
<rawString>Jean Maillard, Stephen Clark, and Edward Grefenstette. 2014. A type-driven tensor-based semantics for CCG. In Proceedings of the EACL 2014 Type Theory and Natural Language Semantics Workshop (TTNLS), Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="20426" citStr="Minnen et al., 2001" startWordPosition="3414" endWordPosition="3417"> how the performance of the methods changes as the amount of training data increases. The 4,000 training samples were randomised and half were used for testing. We sampled between 10 and 1000 training triples from the other half (Figure 4). 4.1 Noun vectors Distributional semantic models (Turney and Pantel, 2010) encode word meaning in a vector format by counting co-occurrences with other words within a specified context window. We constructed the vectors from the October 2013 dump of Wikipedia articles, which was tokenised using the Stanford NLP tools3, lemmatised with the Morpha lemmatiser (Minnen et al., 2001), and parsed with the C&amp;C parser (Clark and Curran, 2007). In this paper we use sentence boundaries to define context windows and the top 10,000 most frequent lemmatised words in the whole corpus (excluding stopwords) as context words. The raw co-occurrence counts are re-weighted using the standard tTest weighting scheme (Curran, 2004), where fwicj is the number of times target noun wi occurs with context word cj: tTest( ~wi,cj) = p(wi,cj) − p(wi)p(cj) (3) V p(wi)p(cj) where p(wi) = Ej fwicj p(c71 = Ei fwicj Ek El fwkcl Ek El fwkcl _ fwicj wi and p(, cj) Ek El fwkcl . Using all 10,000 context </context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(3):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08,</booktitle>
<pages>236--244</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="1192" citStr="Mitchell and Lapata, 2008" startWordPosition="164" endWordPosition="167"> maps (i.e. multi-dimensional arrays, or tensors). An obstacle to full implementation of the framework is the size of these tensors. We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task. We find that the matrices perform as well as, and sometimes even better than, full tensors, allowing a reduction in the number of parameters needed to model the framework. 1 Introduction An emerging subfield of computational linguistics is concerned with learning compositional distributional representations of meaning (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Clarke, 2012; Socher et al., 2012; Clark, 2013). The advantage of such representations lies in their potential to combine the benefits of distributional approachs to word meaning (Sch¨utze, 1998; Turney and Pantel, 2010) with the more traditional compositional methods from formal semantics (Dowty et al., 1981). Distributional representations have the properties of robustness, learnability from data, ease of handling ambiguity, and the ability to represent gradations of meaning; whereas compositional models ha</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08, pages 236–244, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara Polajnar</author>
<author>Stephen Clark</author>
</authors>
<title>Improving distributional semantic vectors through context selection and normalisation.</title>
<date>2014</date>
<booktitle>In 14th Conference of the European Chapter of the Association for Computational Linguistics, EACL’14,</booktitle>
<location>Gothenburg,</location>
<contexts>
<context position="21340" citStr="Polajnar and Clark (2014)" startWordPosition="3572" endWordPosition="3575">e standard tTest weighting scheme (Curran, 2004), where fwicj is the number of times target noun wi occurs with context word cj: tTest( ~wi,cj) = p(wi,cj) − p(wi)p(cj) (3) V p(wi)p(cj) where p(wi) = Ej fwicj p(c71 = Ei fwicj Ek El fwkcl Ek El fwkcl _ fwicj wi and p(, cj) Ek El fwkcl . Using all 10,000 context words would result in a large number of parameters for each verb tensor, and so we apply singular value decomposition (SVD) (Turney and Pantel, 2010) with 40 latent dimensions to the target-context word matrix. We use context selection (with N = 140) and row normalisation as described in Polajnar and Clark (2014) to markedly improve the performance of SVD on smaller dimensions (K) and enable us to train the verb tensors using very low-dimensional 3http://nlp.stanford.edu/software/index.shtml Verb Concreteness # of Positive Frequency APPLY 2.5 5618 47361762 CENSOR 3 26 278525 COMB 5 164 644447 DEPOSE 2.5 118 874463 EAT 4.44 5067 26396728 IDEALIZE 1.17 99 485580 INCUBATE 3.5 82 833621 JUSTIFY 1.45 5636 10517616 REDUCE 2 26917 40336784 WIPE 4 1090 6348595 Table 2: The 10 chosen verbs together with their concreteness scores. The number of positive SVO examples was capped at 2000. Frequency is the frequenc</context>
</contexts>
<marker>Polajnar, Clark, 2014</marker>
<rawString>Tamara Polajnar and Stephen Clark. 2014. Improving distributional semantic vectors through context selection and normalisation. In 14th Conference of the European Chapter of the Association for Computational Linguistics, EACL’14, Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ranzato</author>
<author>A Krizhevsky</author>
<author>G E Hinton</author>
</authors>
<title>Factored 3-way restricted boltzmann machines for modeling natural images.</title>
<date>2010</date>
<booktitle>In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS),</booktitle>
<location>Sardinia, Italy.</location>
<contexts>
<context position="30603" citStr="Ranzato et al., 2010" startWordPosition="5091" endWordPosition="5094">42 JUSTIFY 88.96 88.99 87.31 REDUCE 100.0 99.87 99.46 WIPE 97.20 91.63 96.36 MEAN 91.52 89.94 92.50 Table 5: Results show average of 5x2cv AUC on small data (26 positive + 26 negative per verb). None of the results are significant. ing tied more closely to a type-driven syntactic description; however, this flexibility comes at a cost, since there are many more paramaters to learn. Various communities are beginning to recognize the additional power that tensor representations can provide, through the capturing of interactions that are difficult to represent with vectors and matrices (see e.g. (Ranzato et al., 2010; Sutskever et al., 2009; Van de Cruys et al., 2012)). Hierarchical recursive structures in language potentially represent a large number of such interactions – the obvious example for this paper being the interaction between a transitive verb’s subject and object – and present a significant challenge for machine learning. This paper is a practical extension of the work in Krishnamurthy and Mitchell (2013), which introduced learning of CCG-based function tensors with logistic regression on a compositional semantics task, but was implemented as a proof-ofconcept with vectors of length 2 and on </context>
</contexts>
<marker>Ranzato, Krizhevsky, Hinton, 2010</marker>
<rawString>M. Ranzato, A. Krizhevsky, and G. E. Hinton. 2010. Factored 3-way restricted boltzmann machines for modeling natural images. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS), Sardinia, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional constraints: An information-theoretic model and its computational realization.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--127</pages>
<contexts>
<context position="32427" citStr="Resnik, 1996" startWordPosition="5410" endWordPosition="5411">creases. logic examples. Here, we go beyond this by learning tensors using corpus data and by deriving several different matrix representations for the verb in the subject-verb-object (SVO) sentence. This work can also be thought of as applying neural network learning techniques to the classic problem of selectional preference acquisition, since the design of the pseudo-disambiguation experiments is taken from the literature on selectional preferences (Clark and Weir, 2002; Chambers and Jurafsky, 2010). We do not compare directly with methods from this literature, e.g. those based on WordNet (Resnik, 1996; Clark and Weir, 2002) or topic modelling techniques (Seaghdha, 2010), since our goal in this paper is not to extend the state-of-the-art in that area, but rather to use selectional preference acquisition as a test bed for the tensor-based semantic framework. 7 Conclusion In this paper we introduced three dimensionally reduced representations of the transitive verb tensor defined in the type-driven framework for compositional distributional semantics (Coecke et al., 2010). In a comprehensive experiment on ten different verbs we find no significant difference between the full tensor representa</context>
</contexts>
<marker>Resnik, 1996</marker>
<rawString>Philip Resnik. 1996. Selectional constraints: An information-theoretic model and its computational realization. Cognition, 61:127–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>124</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97– 124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O Seaghdha</author>
</authors>
<title>Latent variable models of selectional preference.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL 2010,</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="32497" citStr="Seaghdha, 2010" startWordPosition="5420" endWordPosition="5421">s using corpus data and by deriving several different matrix representations for the verb in the subject-verb-object (SVO) sentence. This work can also be thought of as applying neural network learning techniques to the classic problem of selectional preference acquisition, since the design of the pseudo-disambiguation experiments is taken from the literature on selectional preferences (Clark and Weir, 2002; Chambers and Jurafsky, 2010). We do not compare directly with methods from this literature, e.g. those based on WordNet (Resnik, 1996; Clark and Weir, 2002) or topic modelling techniques (Seaghdha, 2010), since our goal in this paper is not to extend the state-of-the-art in that area, but rather to use selectional preference acquisition as a test bed for the tensor-based semantic framework. 7 Conclusion In this paper we introduced three dimensionally reduced representations of the transitive verb tensor defined in the type-driven framework for compositional distributional semantics (Coecke et al., 2010). In a comprehensive experiment on ten different verbs we find no significant difference between the full tensor representation and the reduced representations. The SKMat and 2Mat representatio</context>
</contexts>
<marker>Seaghdha, 2010</marker>
<rawString>Diarmuid O Seaghdha. 2010. Latent variable models of selectional preference. In Proceedings of ACL 2010, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1201--1211</pages>
<location>Jeju,</location>
<contexts>
<context position="1311" citStr="Socher et al., 2012" startWordPosition="182" endWordPosition="185">ensors. We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task. We find that the matrices perform as well as, and sometimes even better than, full tensors, allowing a reduction in the number of parameters needed to model the framework. 1 Introduction An emerging subfield of computational linguistics is concerned with learning compositional distributional representations of meaning (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Clarke, 2012; Socher et al., 2012; Clark, 2013). The advantage of such representations lies in their potential to combine the benefits of distributional approachs to word meaning (Sch¨utze, 1998; Turney and Pantel, 2010) with the more traditional compositional methods from formal semantics (Dowty et al., 1981). Distributional representations have the properties of robustness, learnability from data, ease of handling ambiguity, and the ability to represent gradations of meaning; whereas compositional models handle the unbounded nature of natural language, as well as providing established accounts of logical words, quantificati</context>
<context position="29515" citStr="Socher et al. (2012)" startWordPosition="4913" endWordPosition="4916">tives, but sometimes produces false negatives with low frequency nouns (e.g. bourgeoisie IDEALIZE work), presumably because there is not enough information in the noun vector to decide on the correct class. It also produces some false positive errors when either of the nouns is plausible (but the triple is implausible), which would suggest results may be improved by training with data where only one noun is confounded or by treating negative data as possibly positive (Lee and Liu, 2003). 6 Discussion Current methods which derive distributed representations for phrases, for example the work of Socher et al. (2012), typically use only matrix representations, and also assume that words, phrases and sentences all live in the same vector space. The tensor-based semantic framework is more flexible, in that it allows different spaces for different grammatical types, which results from it beVerb Tensor DMat 2Mat APPLY 95.76 86.50 86.31 CENSOR 82.97 84.09 77.79 COMB 90.13 92.93 95.18 DEPOSE 92.41 91.27 95.61 EAT 99.64 98.25 99.58 IDEALIZE 75.03 76.68 88.98 INCUBATE 91.10 87.20 96.42 JUSTIFY 88.96 88.99 87.31 REDUCE 100.0 99.87 99.46 WIPE 97.20 91.63 96.36 MEAN 91.52 89.94 92.50 Table 5: Results show average of</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1201– 1211, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Chris Manning</author>
<author>Andrew Ng</author>
<author>Chris Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2013),</booktitle>
<location>Seattle, USA.</location>
<contexts>
<context position="6320" citStr="Socher et al., 2013" startWordPosition="967" endWordPosition="970">e between 0 and 1, or as a two-dimensional probability distribution over the classes plausible (T) and implausible (1). Whether we consider a one- or two-dimensional sentence space depends on the architecture of the logistic regression classifier that is used to learn the verb (Section 3). We begin with this simple plausibility sentence space to determine if, in fact, the tensor-based representation can be learned to a sufficiently useful degree. Other simple sentence spaces which can perhaps be represented using one or two variables include a “sentence space” for the sentiment analysis task (Socher et al., 2013), where one variable represents positive sentiment and the other negative. We also expect that the insights gained from research on this task can be applied to more complex sentence spaces, for example a semantic similarity space which will require more than two variables. 2 Syntactic Types to Tensors The syntactic type of a transitive verb in English is (SNP)/NP (using notation from Steedman (2000)), meaning that a transitive verb is a function which takes an NP argument to the right, an NP argument to the left, and results in a sentence S. Such categories with slashes are complex categories;</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Chris Manning, Andrew Ng, and Chris Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2013), Seattle, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3203" citStr="Steedman, 2000" startWordPosition="477" endWordPosition="478"> of an adjective, for example, is represented by a matrix (a 2nd-order tensor)1 and the meaning of a transitive verb is represented by a 3rd-order tensor. Coecke et al. use the grammar of pregroups as the syntactic machinery to construct distributional meaning representations, since both pregroups and vector spaces can be seen as examples of the same abstract structure, which leads to a particularly clean mathematical description of the compositional process. However, the approach applies more generally, for example to other forms of categorial grammar, such as Combinatory Categorial Grammar (Steedman, 2000; Maillard et al., 2014), and also to phrase-structure grammars in a way that a formal linguist would recognize (Baroni et al., 2014). Clark (2013) provides a description of the tensor-based framework aimed more at computational linguists, relying only on the mathematics of multi-linear algebra rather than the category theory used in Coecke et al. (2010). Section 2 repeats some of this description. A major open question associated with the tensor-based semantic framework is how to learn 1This same insight lies behind the work of Baroni and Zamparelli (2010). 1036 Proceedings of the 2014 Confer</context>
<context position="6722" citStr="Steedman (2000)" startWordPosition="1037" endWordPosition="1038">be learned to a sufficiently useful degree. Other simple sentence spaces which can perhaps be represented using one or two variables include a “sentence space” for the sentiment analysis task (Socher et al., 2013), where one variable represents positive sentiment and the other negative. We also expect that the insights gained from research on this task can be applied to more complex sentence spaces, for example a semantic similarity space which will require more than two variables. 2 Syntactic Types to Tensors The syntactic type of a transitive verb in English is (SNP)/NP (using notation from Steedman (2000)), meaning that a transitive verb is a function which takes an NP argument to the right, an NP argument to the left, and results in a sentence S. Such categories with slashes are complex categories; S and NP are basic or atomic categories. Interpreting such categories under the Coecke et al. framework is straightforward. First, for each atomic category there is a corresponding vector space; in this case the sentence space S and the noun space N.2 Hence the meaning of a noun or noun phrase, for example people, will be a vector ���� in the noun space: people E N. In order to obtain the meaning o</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Sutskever</author>
<author>R Salakhutdinov</author>
<author>J B Tenenbaum</author>
</authors>
<title>Modelling relational data using bayesian clustered tensor factorization.</title>
<date>2009</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems (NIPS</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="30627" citStr="Sutskever et al., 2009" startWordPosition="5095" endWordPosition="5098"> 87.31 REDUCE 100.0 99.87 99.46 WIPE 97.20 91.63 96.36 MEAN 91.52 89.94 92.50 Table 5: Results show average of 5x2cv AUC on small data (26 positive + 26 negative per verb). None of the results are significant. ing tied more closely to a type-driven syntactic description; however, this flexibility comes at a cost, since there are many more paramaters to learn. Various communities are beginning to recognize the additional power that tensor representations can provide, through the capturing of interactions that are difficult to represent with vectors and matrices (see e.g. (Ranzato et al., 2010; Sutskever et al., 2009; Van de Cruys et al., 2012)). Hierarchical recursive structures in language potentially represent a large number of such interactions – the obvious example for this paper being the interaction between a transitive verb’s subject and object – and present a significant challenge for machine learning. This paper is a practical extension of the work in Krishnamurthy and Mitchell (2013), which introduced learning of CCG-based function tensors with logistic regression on a compositional semantics task, but was implemented as a proof-ofconcept with vectors of length 2 and on small, manually created </context>
</contexts>
<marker>Sutskever, Salakhutdinov, Tenenbaum, 2009</marker>
<rawString>I. Sutskever, R. Salakhutdinov, and J. B. Tenenbaum. 2009. Modelling relational data using bayesian clustered tensor factorization. In Proceedings of Advances in Neural Information Processing Systems (NIPS 2009), Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1498" citStr="Turney and Pantel, 2010" startWordPosition="210" endWordPosition="213">rform as well as, and sometimes even better than, full tensors, allowing a reduction in the number of parameters needed to model the framework. 1 Introduction An emerging subfield of computational linguistics is concerned with learning compositional distributional representations of meaning (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Clarke, 2012; Socher et al., 2012; Clark, 2013). The advantage of such representations lies in their potential to combine the benefits of distributional approachs to word meaning (Sch¨utze, 1998; Turney and Pantel, 2010) with the more traditional compositional methods from formal semantics (Dowty et al., 1981). Distributional representations have the properties of robustness, learnability from data, ease of handling ambiguity, and the ability to represent gradations of meaning; whereas compositional models handle the unbounded nature of natural language, as well as providing established accounts of logical words, quantification, and inference. One promising approach which attempts to combine elements of compositional and distributional semantics is by Coecke et al. (2010). The underlying idea is to take the t</context>
<context position="20120" citStr="Turney and Pantel, 2010" startWordPosition="3363" endWordPosition="3367">h of the verbs, so we repeated the 5x2cv with datasets of 52 training points for each verb, since this is the size of the smallest dataset of the verb CENSOR. The points were randomly sampled from the datasets used in the first experiment. Finally, the four verbs with the largest datasets were used to examine how the performance of the methods changes as the amount of training data increases. The 4,000 training samples were randomised and half were used for testing. We sampled between 10 and 1000 training triples from the other half (Figure 4). 4.1 Noun vectors Distributional semantic models (Turney and Pantel, 2010) encode word meaning in a vector format by counting co-occurrences with other words within a specified context window. We constructed the vectors from the October 2013 dump of Wikipedia articles, which was tokenised using the Stanford NLP tools3, lemmatised with the Morpha lemmatiser (Minnen et al., 2001), and parsed with the C&amp;C parser (Clark and Curran, 2007). In this paper we use sentence boundaries to define context windows and the top 10,000 most frequent lemmatised words in the whole corpus (excluding stopwords) as context words. The raw co-occurrence counts are re-weighted using the sta</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aydın Ulas¸</author>
</authors>
<title>Olcay Taner Yıldız, and Ethem Alpaydın.</title>
<date>2012</date>
<journal>Pattern Recognition,</journal>
<volume>45</volume>
<issue>4</issue>
<marker>Ulas¸, 2012</marker>
<rawString>Aydın Ulas¸, Olcay Taner Yıldız, and Ethem Alpaydın. 2012. Cost-conscious comparison of supervised learning algorithms over multiple data sets. Pattern Recognition, 45(4):1772–1781, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
<author>Laura Rimell</author>
<author>Thierry Poibeau</author>
<author>Anna Korhonen</author>
</authors>
<title>Multi-way tensor factorization for unsupervised lexical acquisition.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<location>Mumbai, India.</location>
<marker>Van de Cruys, Rimell, Poibeau, Korhonen, 2012</marker>
<rawString>Tim Van de Cruys, Laura Rimell, Thierry Poibeau, and Anna Korhonen. 2012. Multi-way tensor factorization for unsupervised lexical acquisition. In Proceedings of COLING 2012, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
</authors>
<title>A non-negative tensor factorization model for selectional preference induction.</title>
<date>2010</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<marker>Van de Cruys, 2010</marker>
<rawString>Tim Van de Cruys. 2010. A non-negative tensor factorization model for selectional preference induction. Journal of Natural Language Engineering, 16(4):417–437.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9949755">
Self-disclosure topic model for classifying and analyzing Twitter
conversations
</title>
<author confidence="0.998222">
JinYeong Bak∗ Chin-Yew Lin Alice Oh
</author>
<affiliation confidence="0.9221815">
Department of Computer Science Microsoft Research Department of Computer Science
KAIST Beijing 100080, P.R. China KAIST
</affiliation>
<email confidence="0.7087485">
Daejeon, South Korea cyl@microsoft.com Daejeon, South Korea
jy.bak@kaist.ac.kr alice.oh@kaist.edu
</email>
<sectionHeader confidence="0.993855" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999948242424243">
Self-disclosure, the act of revealing one-
self to others, is an important social be-
havior that strengthens interpersonal rela-
tionships and increases social support. Al-
though there are many social science stud-
ies of self-disclosure, they are based on
manual coding of small datasets and ques-
tionnaires. We conduct a computational
analysis of self-disclosure with a large
dataset of naturally-occurring conversa-
tions, a semi-supervised machine learning
algorithm, and a computational analysis
of the effects of self-disclosure on subse-
quent conversations. We use a longitu-
dinal dataset of 17 million tweets, all of
which occurred in conversations that con-
sist of five or more tweets directly reply-
ing to the previous tweet, and from dyads
with twenty of more conversations each.
We develop self-disclosure topic model
(SDTM), a variant of latent Dirichlet al-
location (LDA) for automatically classi-
fying the level of self-disclosure for each
tweet. We take the results of SDTM and
analyze the effects of self-disclosure on
subsequent conversations. Our model sig-
nificantly outperforms several comparable
methods on classifying the level of self-
disclosure, and the analysis of the longitu-
dinal data using SDTM uncovers signifi-
cant and positive correlation between self-
disclosure and conversation frequency and
length.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998821586956522">
Self-disclosure is an important and pervasive so-
cial behavior. People disclose personal informa-
tion about themselves to improve and maintain
∗This work was done when JinYeong Bak was a visiting
student at Microsoft Research, Beijing, China.
relationships (Jourard, 1971; Joinson and Paine,
2007). A common instance of self-disclosure is
the start of a conversation with an exchange of
names and additional self-introductions. Another
example of self-disclosure, shown in Figure 1c,
where the information disclosed about a family
member’s serious illness, is much more personal
than the exchange of names. In this paper, we seek
to understand this important social behavior using
a large-scale Twitter conversation data, automati-
cally classifying the level of self-disclosure using
machine learning and correlating the patterns with
conversational behaviors which can serve as prox-
ies for measuring intimacy between two conversa-
tional partners.
Twitter conversation data, explained in more
detail in section 4.1, enable an extremely large
scale study of naturally-occurring self-disclosure
behavior, compared to traditional social science
studies. One challenge of such large scale study,
though, remains in the lack of labeled ground-
truth data of self-disclosure level. That is,
naturally-occurring Twitter conversations do not
come tagged with the level of self-disclosure in
each conversation. To overcome that challenge,
we propose a semi-supervised machine learning
approach using probabilistic topic modeling. Our
self-disclosure topic model (SDTM) assumes that
self-disclosure behavior can be modeled using a
combination of simple linguistic features (e.g.,
pronouns) with automatically discovered seman-
tic themes (i.e., topics). For instance, an utterance
“I am finally through with this disastrous relation-
ship” uses a first-person pronoun and contains a
topic about personal relationships.
In comparison with various other models,
SDTM shows the highest accuracy, and the result-
ing conversation frequency and length patterns on
self-disclosure are shown different over time. Our
contributions to the research community include
the following:
</bodyText>
<page confidence="0.947143">
1986
</page>
<note confidence="0.8950555">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1986–1996,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<listItem confidence="0.828662176470588">
• We present key features and prior knowl-
edge for identifying self-disclosure level, and
show relevance of it with experiment results
(Sec. 2).
• We present a topic model that explicitly in-
cludes the level of self-disclosure in a conver-
sation using linguistic features and the latent
semantic topics (Sec. 3).
• We collect a large dataset of Twitter conver-
sations over three years and annotate a small
subset with self-disclosure level (Sec. 4).
• We compare the classification accuracy of
SDTM with other models and show that it
performs the best (Sec. 5).
• We correlate the self-disclosure patterns and
conversation behaviors to show that there is
significant relationship over time (Sec. 6).
</listItem>
<sectionHeader confidence="0.860435" genericHeader="introduction">
2 Self-Disclosure
</sectionHeader>
<bodyText confidence="0.99991625">
In this section, we look at social science literature
for definition of the levels of self-disclosure. Us-
ing that definition, we devise an approach to au-
tomatically identify the levels of self-disclosure
in a large corpus of OSN conversations. We dis-
cuss three approaches, first, using first-person pro-
noun features, and second, extracting seed words
and phrases from the Twitter conversation cor-
pus, and third, extracting seed words and phrases
from an external corpus of anonymously posted
secrets, and we demonstrate the efficacy of those
approaches with an annotated corpus.
</bodyText>
<subsectionHeader confidence="0.985549">
2.1 Self-disclosure (SD) level
</subsectionHeader>
<bodyText confidence="0.9999775">
To analyze self-disclosure, researchers categorize
self-disclosure language into three levels: G (gen-
eral) for no disclosure, M for medium disclosure,
and H for high disclosure (Vondracek and Von-
dracek, 1971; Barak and Gluck-Ofri, 2007). Ut-
terances that contain general (non-sensitive) in-
formation about the self or someone close (e.g.,
a family member) are categorized as M. Exam-
ples are personal events, past history, or future
plans. Utterances about age, occupation and hob-
bies are also included. Utterances that contain
sensitive information about the self or someone
close are categorized as H. Sensitive information
includes personal characteristics, problematic be-
haviors, physical appearance and wishful ideas.
Generally, these are thoughts and information that
</bodyText>
<figure confidence="0.6600385">
(f) A M level Twitter conversation
(g) A H level Twitter conversation
</figure>
<figureCaption confidence="0.984964">
Figure 1: An example of a Twitter conversation
</figureCaption>
<bodyText confidence="0.950380545454545">
(from annotated dataset) with G, M and H level of
self-disclosure.
one would keep as secrets to himself. All other
utterances, those that do not contain information
about the self or someone close are categorized
as G. Examples include gossip about celebrities
or factual discourse about current events. Figure
1 shows Twitter conversation examples with G,
M and H levels from annotated dataset (see Sec-
tion 4.2 for a detailed description of the annotated
dataset).
</bodyText>
<subsectionHeader confidence="0.996864">
2.2 G Level of Self-Disclosure
</subsectionHeader>
<bodyText confidence="0.999763625">
An obvious clue of self-disclosure is the use of
first-person pronouns. For example, phrases such
as ‘I live’ or ‘My name is’ indicate that the ut-
terance contains personal information. In pre-
vious research, the simple method of counting
first-person pronouns was used to measure the de-
gree of self-disclosure (Joinson, 2001; Barak and
Gluck-Ofri, 2007). Consequently, the absence of a
first-person pronoun signals that the utterance be-
longs in the G level of self-disclosure. We ver-
ify this pattern with a dataset of Tweets annotated
with G, M, and H levels. We divide the annotated
Tweets into two classes, G and M/H. Then we com-
pute mutual information of each unigram, bigram,
or trigram feature to see which features are most
discriminative. As Table 1 shows, 18 out of 30
</bodyText>
<figure confidence="0.950547">
(a) A G level Twitter conversation
</figure>
<page confidence="0.688887">
1987
</page>
<construct confidence="0.62795">
Category Words/Expressions
Unigram my, I, I’m, I’ll, but, was, I’ve, love, dad, have
Bigram I love, I was, I have, my dad, go to, my mom,
with my, have to, to go, my mum
Trigram I have a, is going to, to go to, want to go, and I
was, going to miss, I love him, I think I, I was
like, I wish I
Category Words - SECRET Words - Annotated
</construct>
<tableCaption confidence="0.882315666666667">
Table 3: Example words for identifying H level of
SD from secret posts (2nd column) and annotated
data (3rd column). Categories are hand-labeled.
</tableCaption>
<figure confidence="0.805855166666667">
physical
appear-
ance
mental/
physical
condition
</figure>
<bodyText confidence="0.749458909090909">
acne, hair, overweight,
stomach, chest, hand,
scar, thighs, chubby
addicted, bulimia, doc-
tor, illness, alcoholic,
disease, drugs, pills
ankle, face, toe,
skin
ache, epilepsy,
pain, chiropractor,
codeine
</bodyText>
<tableCaption confidence="0.6198936">
Table 1: High ranked words and expressions by
mutual information between G and M/H level in
annotated conversations.
most highly ranked discriminative features contain
a first-person pronoun.
</tableCaption>
<subsectionHeader confidence="0.998369">
2.3 M Level of Self-Disclosure
</subsectionHeader>
<bodyText confidence="0.989085611111111">
Utterances with M level include two types: 1)
information related with past events and future
plans, and 2) general information about self
(Barak and Gluck-Ofri, 2007). For the former, we
add as seed trigrams ‘I have been’ and ‘I will’.
For the latter, we use seven types of information
generally accepted to be personally identifiable in-
formation (McCallister, 2010), as listed in the left
column of Table 2. To find the appropriate tri-
grams for those, we take Twitter conversation data
(described in Section 4.1) and look for trigrams
that begin with ‘I’ and ‘my’ and occur more than
200 times. We then check each one to see whether
it is related with any of the seven types listed in
the table. As a result, we find 57 seed trigrams for
M level. Table 2 shows several examples.
Table 2: Example seed trigrams for identifying M
level of SD. There are 51 of these used in SDTM.
</bodyText>
<subsectionHeader confidence="0.992467">
2.4 H Level of Self-Disclosure
</subsectionHeader>
<bodyText confidence="0.999483333333333">
Utterances with H level express secretive wishes
or sensitive information that exposes self or some-
one close (Barak and Gluck-Ofri, 2007). These
are generally kept as secrets. With this intuition,
we crawled 26,523 posts from Six Billion Secrets1
site where users post secrets anonymously2. We
</bodyText>
<footnote confidence="0.999947">
1http://www.sixbillionsecrets.com
2This site is regularly monitored for spam.
</footnote>
<bodyText confidence="0.99975525">
call this external dataset SECRET. Unlike G and M
levels, evidence of H level of self-disclosure tends
to be topical, such as physical appearance, mental
and physical illnesses, and family problems, so we
take an approach of fitting a topic model driven by
seed words. A similar approach has been success-
ful in sentiment classification (Jo and Oh, 2011;
Kim et al., 2013).
A critical component of this approach is the set
of seed words with which to drive the discovery
of topics that are most indicative of H level self-
disclosure. To extract the seed words that express
secretive personal information, we compute mu-
tual information (Manning et al., 2008) with SE-
CRET and 24,610 randomly selected tweets. We
select 1,000 words with high mutual information
and filter out stop words. Table 3 shows some of
these words. To extract seed trigrams of secretive
wishes, we again look for trigrams that start with
‘I’ or ‘my’, occur more than 200 times, and select
trigrams of wishful thinking, such as ‘I want to’,
and ‘I wish I’. In total, there are 88 seed words
and 8 seed trigrams for H.
Since SECRET is quite different from Twitter,
we must show that posts in SECRET are seman-
tically similar to the H level Tweets. Rather than
directly comparing SECRET posts and Tweets, we
use the same method of extracting discriminative
word features from the annotated H level Tweets
(see Section 4.2). Table 3 shows the seed words
extracted from SECRET as well as the annotated
Tweets. Because the annotated dataset consists of
only 200 conversations, the coverage of the topics
seems narrower than the much larger SECRETS,
but both datasets show similarities in the topics.
This, combined with the results of the model with
the two sets of seed words (see Section 5 for the
results), shows that SECRETS is an effective and
simple-to-obtain substitute for an annotated cor-
pus of H level of self-disclosure.
</bodyText>
<figure confidence="0.97740375">
Type Trigram
My name is, My last name
My birthday is, My birthday party
I live in, I lived in, I live on
My email address, My phone number
My job is, My new job
My high school, My college is
My dad is, My mom is, My family is
Name
Birthday
Location
Contact
Occupation
Education
Family
1988
</figure>
<figureCaption confidence="0.982152">
Figure 2: Graphical model of SDTM
</figureCaption>
<table confidence="0.987399518518519">
Notation Description
G; M; H {general; medium; high} SD level
C; T; N Number of conversations; tweets;
KG; KM; KH words
c; ct Number of topics for {G; M; H}
yct Conversation; tweet in conversation c
rct SD level of tweet ct, G or M/H
zct SD level of tweet ct, M or H
wctn Topic of tweet ct
nth word in tweet ct
A Learned Maximum entropy parameters
xct First-person pronouns features
Wct Distribution over SD level of tweet ct
ac SD level proportion of conversation c
6Gc ; 6Mc ; 6H Topic proportion of {G; M; H} in con-
OG; OM; OH versation c
a; ^y Word distribution of {G; M; H}
)3G, )3M; )3H Dirichlet prior for θ; it
Dirichlet prior for OG; OM; OH
ncl Number of tweets assigned SD level l
nl in conversation c
ck Number of tweets assigned SD level l
nl and topic k in conversation c
kv Number of instances of word v assigned
mctkv SD level l and topic k
Number of instances of word v assigned
topic k in tweet ct
</table>
<tableCaption confidence="0.998447">
Table 4: Summary of notations used in SDTM
</tableCaption>
<sectionHeader confidence="0.962877" genericHeader="method">
3 Self-Disclosure Topic Model
</sectionHeader>
<bodyText confidence="0.99995">
This section describes our model, the self-
disclosure topic model (SDTM), for classifying
self-disclosure level and discovering topics for
each self-disclosure level.
</bodyText>
<subsectionHeader confidence="0.997924">
3.1 Model
</subsectionHeader>
<bodyText confidence="0.998823375">
In section 2, we discussed different approaches
to identifying each level of self-disclosure, based
on social science literature, annotated and unan-
notated Tweets, and an external corpus of se-
cret posts. In this section, we describe our
self-disclosure topic model, based on the widely
used latent Dirichlet allocation (Blei et al., 2003),
which incorporates those approaches.
</bodyText>
<figureCaption confidence="0.533828">
Figure 2 illustrates the graphical model of
</figureCaption>
<figure confidence="0.99233152173913">
1. For each level l ∈ {G, M, H}:
For each topic k ∈ {1, ... ,Kl}:
Draw olk ∼ Dir(/.l)
2. For each conversation c ∈ {1,... ,C}:
(a) Draw 9G c ∼ Dir(ct)
(b) Draw 9M c ∼ Dir(ct)
(c) Draw 9Hc ∼ Dir(ct)
(d) Draw Trc ∼ Dir(y)
(e) For each message t ∈ {1, ... ,T}:
i. Observe first-person pronouns features xct
ii. Draw wct ∼ MaxEnt(xct, A)
iii. Draw yct ∼ Bernoulli(wct)
iv. If yct = 0 which is G level:
A. Draw zct ∼ Mult(9Gc )
B. For each word n ∈ {1, ... , N}:
Draw word wctn ∼ Mult(oGzct)
Else which can be M or H level:
A. Draw rct ∼ Mult(Trc)
B. Draw zct ∼ Mult(9rct
c )
C. For each word n ∈ {1, ... , N}:
Draw word wctn∼ Mult(orct
zct)
</figure>
<figureCaption confidence="0.999966">
Figure 3: Generative process of SDTM.
</figureCaption>
<bodyText confidence="0.999988384615385">
SDTM and how those approaches are embodied
in it. The first approach based on the first-person
pronouns is implemented by the observed vari-
able xct and the parameters A from a maximum
entropy classifier for G vs. M/H level. The ap-
proach of seed words and phrases for levels M and
H is implemented by the three separate word-topic
probability vectors for the three levels of SD: 0l
which has a Bayesian informative prior )3l where
l ∈ {G, M, H}, the three levels of self-disclosure.
Table 4 lists the notations used in the model and
the generative process, and Figure 3 describes the
generative process.
</bodyText>
<subsectionHeader confidence="0.999307">
3.2 Classifying G vs M/H levels
</subsectionHeader>
<bodyText confidence="0.999898714285714">
Classifying the SD level for each tweet is done in
two parts, and the first part classifies G vs. M/H
levels with first-person pronouns (I, my, me). In
the graphical model, y is the latent variable that
represents this classification, and w is the distri-
bution over y. x is the observation of the first-
person pronoun in the tweets, and A are the param-
eters learned from the maximum entropy classifier.
With the annotated Twitter conversation dataset
(described in Section 4.2), we experimented with
several classifiers (Decision tree, Naive Bayes)
and chose the maximum entropy classifier because
it performed the best, similar to other joint topic
models (Zhao et al., 2010; Mukherjee et al., 2013).
</bodyText>
<figure confidence="0.977285238095238">
Rl
IPl
Kl
3
Y
Ir
r
0l
3
W
a
z
N
W
Y
A
X
T
C
1989
Dyads
</figure>
<subsectionHeader confidence="0.659637">
3.3 Classifying M vs H levels
</subsectionHeader>
<bodyText confidence="0.999833272727273">
The second part of the classification, the M and the
H level, is driven by informative priors with seed
words and seed trigrams. In the graphical model,
r is the latent variable that represents this classi-
fication, and π is the distribution over r. γ is a
non-informative prior for π, and βl is an informa-
tive prior for each SD level by seed words. For
example, we assign a high value for the seed word
‘acne’ for βH, and a low value for ‘My name is’.
This approach is the same as joint models of topic
and sentiment (Jo and Oh, 2011; Kim et al., 2013).
</bodyText>
<sectionHeader confidence="0.812633" genericHeader="method">
3.4 Inference
</sectionHeader>
<bodyText confidence="0.9999406">
For posterior inference of SDTM, we use col-
lapsed Gibbs sampling which integrates out la-
tent random variables ω, π, θ, and φ. Then we
only need to compute y, r and z for each tweet.
We compute full conditional distribution p(yct =
</bodyText>
<equation confidence="0.888921444444444">
j0,rct = l0, zct = k0|y−ct, r− ct, z−ct, w, x) for
tweet ct as follows:
p(yct = 0, zct = k0|y−ct, r−ct, z−ct, w, x)
exp(λ0 · xct)
∝ P1 j=0 exp(λj · xct) g(c, t, l0, k0),
p(yct = 1, rct = l0, zct = k0|y−ct, r−ct, z−ct, w, x)
∝ j=0 exp(λj · xct)(γl0 + n(−ct)
exp(λ1 · xct)
P1 cl0 ) g(c,t, l0,k0),
</equation>
<bodyText confidence="0.841526">
where z−ct, r−ct, y−ct are z, r, y without tweet
ct, mctk0(·) is the marginalized sum over word v of
mctk0v and the function g(c, t, l0, k0) as follows:
</bodyText>
<equation confidence="0.999515">
g(c, t, l0, k0) = Γ(V l0l0−(ct
Pv=1 βv + nk0v + mctk0(·))
Γ(PV v=1 βl0 v + nl0−(ct)
k0v )
! YV v + nl0−(ct)
αk0 + nl0(−ct) Γ(βl0 k0v + mctk0v)
ck0
PK l0 l0 l0−(ct)
k=1 αk + nck v=1 Γ(βv + nk0v )
</equation>
<sectionHeader confidence="0.974899" genericHeader="method">
4 Data Collection and Annotation
</sectionHeader>
<bodyText confidence="0.999896222222222">
To test our self-disclosure topic model, we use a
large dataset of conversations consisting of Tweets
over three years such that we can analyze the re-
lationship between self-disclosure behavior and
conversation frequency and length over time. We
chose to crawl Twitter because it offers a prac-
tical and large source of conversations (Ritter et
al., 2010). Others have also analyzed Twitter con-
versations for natural language and social media
</bodyText>
<table confidence="0.3715125">
Conv’s Tweets
1,956,993 17,178,638
</table>
<tableCaption confidence="0.558999">
Table 5: Dataset of Twitter conversations. We
</tableCaption>
<bodyText confidence="0.955433888888889">
chose conversations consisting of five or more
tweets each. We chose dyads with twenty or more
conversations.
research (boyd et al., 2010; Danescu-Niculescu-
Mizil et al., 2011), but we collect conversations
from the same set of dyads over several months for
a unique longitudinal dataset. We also make sure
that each conversation is at least five tweets, and
that each dyad has at least twenty conversations.
</bodyText>
<subsectionHeader confidence="0.999354">
4.1 Collecting Twitter conversations
</subsectionHeader>
<bodyText confidence="0.99996715">
We define a Twitter conversation as a chain of
tweets where two users are consecutively reply-
ing to each other’s tweets using the Twitter reply
button. We initialize the set of users by randomly
sampling thirteen users who reply to other users
in English from the Twitter public streams3. Then
we crawl each user’s public tweets, and look at
users who are mentioned in those tweets. It is
a breadth-first search in the network defined by
users as nodes and edges as conversations. We
run this search for dyads until the depth of four,
and filter out users who tweet in a non-English
language. We use an open source tool for de-
tecting English tweets4. To protect users’ privacy,
we replace Twitter userid, usernames and url in
tweets with random strings. This dataset consists
of 101,686 users, 61,451 dyads, 1,956,993 conver-
sations and 17,178,638 tweets which were posted
between August 2007 to July 2013. Table 5 sum-
marizes the dataset.
</bodyText>
<subsectionHeader confidence="0.999517">
4.2 Annotating self-disclosure level
</subsectionHeader>
<bodyText confidence="0.999722545454545">
To measure the accuracy of our model, we ran-
domly sample 301 conversations, each with ten or
fewer tweets, and ask three judges, fluent in En-
glish and graduate students/researchers, to anno-
tate each tweet with the level of self-disclosure.
Judges first read and discussed the definitions and
examples of self-disclosure level shown in (Barak
and Gluck-Ofri, 2007), then they worked sepa-
rately on a Web-based platform.
As a result of annotation, there are 122 G level
converstaions, 147 M level and 32 H level con-
</bodyText>
<footnote confidence="0.998000666666667">
3https://dev.twitter.com/docs/api/
streaming
4https://github.com/shuyo/ldig
</footnote>
<figure confidence="0.7676855">
Users
101,686
61,451
1990
</figure>
<figureCaption confidence="0.700502">
Figure 4: Screenshot of annotation web-based
platform. Annotators read a Twitter conversation
and annotate self-disclosure level to each tweet.
</figureCaption>
<bodyText confidence="0.972716333333333">
versations, and inter-rater agreement using Fleiss
kappa (Fleiss, 1971) is 0.68, which is substantial
agreement result (Landis and Koch, 1977).
</bodyText>
<sectionHeader confidence="0.948001" genericHeader="method">
5 Classification of Self-Disclosure Level
</sectionHeader>
<bodyText confidence="0.999982857142858">
This section describes experiments and results of
SDTM as well as several other methods for classi-
fication of self-disclosure level.
We first start with the annotated dataset in sec-
tion 4.2 in which each tweet is annotated with 5D
level. We then aggregate all of the tweets of a
conversation, and we compute the proportions of
tweets in each 5D level. When the proportion of
tweets at M or H level is equal to or greater than
0.2, we take the level of the larger proportion and
assign that level to the conversation. When the
proportions of tweets at M or H level are both less
than 0.2, we assign G to the 5D level. The reason
for setting 0.2 as the threshold is that a conversa-
tion containing tweets with H or M level of self-
disclosure usually starts with a greeting or a gen-
eral comment, and contains one or more questions
or comments before or after the self-disclosure
tweet.
We compare SDTM with the following methods
for classifying conversations for 5D level:
</bodyText>
<listItem confidence="0.961008285714286">
• LDA (Blei et al., 2003): A Bayesian topic
model. Each conversation is treated as a doc-
ument. Used in previous work (Bak et al.,
2012).
• MedLDA (Zhu et al., 2012): A super-
vised topic model for document classifica-
tion. Each conversation is treated as a doc-
ument and response variable can be mapped
to a 5D level.
• LIWC (Tausczik and Pennebaker, 2010):
Word counts of particular categories5. Used
in previous work (Houghton and Joinson,
2012).
• Bag of Words + Bigrams + Trigrams
(BOW+): A bag of words, bigram and tri-
gram features. We exclude features that ap-
pear only once or twice.
• Seed words and trigrams (SEED): Occur-
rences of seed words/trigrams from SECRET
which are described in section 3.3.
• SDTM with seed words from annotated
Tweets (SDTM−): To compare with SDTM
below using seed words from SECRET, this
uses seed words from the annotated data de-
scribed in section 2.4.
• ASUM (Jo and Oh, 2011): A joint model
of sentiments and topics. We map each 5D
level to one sentiment and use the same seed
words/trigrams from SECRET as in SDTM
below. Used in previous work (Bak et al.,
2012).
• First-person pronouns (FirstP): Occurrence
of first-person pronouns which are described
in section 3.2. To identify first-person pro-
nouns, we tagged parts of speech in each
tweet with the Twitter POS tagger (Owoputi
et al., 2013).
• First-person pronouns + Seed words/trigrams
(FP+SE1): First-person pronouns and seed
words/trigrams from SECRET.
• Two stage classifier with First-person pro-
nouns + Seed words/trigrams (FP+SE2): A
</listItem>
<footnote confidence="0.9064035">
5personal pronouns, 3rd person singular words, family
words, human words, sexual words, etc
</footnote>
<page confidence="0.957206">
1991
</page>
<table confidence="0.999817583333333">
Method Acc G F1 M F1 H F1 Avg F1
LDA 49.2 0.00 0.65 0.05 0.23
MedLDA 43.3 0.41 0.52 0.09 0.34
LIWC 49.2 0.34 0.61 0.18 0.38
BOW+ 54.1 0.50 0.59 0.15 0.41
SEED 54.4 0.52 0.60 0.14 0.42
ASUM 56.6 0.32 0.70 0.38 0.47
SDTM− 60.4 0.57 0.70 0.14 0.47
FirstP 63.2 0.63 0.69 0.10 0.47
FP+SE1 61.0 0.61 0.67 0.16 0.48
FP+SE2 60.4 0.64 0.69 0.17 0.50
SDTM 64.5 0.61 0.71 0.43 0.58
</table>
<tableCaption confidence="0.774821">
Table 6: SD level classification accuracies and F-
measures using annotated data. Acc is accuracy,
</tableCaption>
<bodyText confidence="0.998354083333333">
and G F1 is F-measure for classifying the G level.
Avg F1 is the macroaveraged value of G F1, M F1
and H F1. SDTM outperforms all other methods
compared. The difference between SDTM and
FirstP is statistically significant (p-value &lt; 0.05
for accuracy, &lt; 0.0001 for Avg F1).
two stage classifier with first-person pro-
nouns and seed words/trigrams from SE-
CRET. In the first stage, the classifier identi-
fies G with first-person pronouns. Then in the
second stage, the classifier uses seed words
and trigrams to identify M and H levels.
</bodyText>
<listItem confidence="0.984306666666667">
• SDTM: Our model with first-person pro-
nouns and seed words/trigrams from SE-
CRET.
</listItem>
<bodyText confidence="0.996731625">
SEED, LIWC, LDA and FirstP cannot be used
directly for classification, so we use Maximum en-
tropy model with outputs of each of those models
as features6. BOW+ uses SVM with a radial ba-
sis kernel which performs better than all other set-
tings tried including maximum entropy. We split
the data randomly into 80/20 for train/test. We run
MedLDA, ASUM and SDTM 20 times each and
compute the average accuracies and F-measure for
each level. We run LDA and MedLDA with var-
ious number of topics from 80 to 140, and 120
topics shows best outputs. So we set 120 topics
for LDA, MedLDA and ASUM, 60; 40; 40 topics
for SDTM KG, KM and KH respectively which
is best perform from 40; 40; 40 to 60; 60; 60 top-
ics. We assume that a conversation has few topics
</bodyText>
<footnote confidence="0.907376333333333">
6It performs better than other classifiers (C4.5, Naive-
Bayes, SVM with linear kernel, polynomial kernel and radial
basis)
</footnote>
<bodyText confidence="0.999870257142857">
and self-disclosure levels, so we set α = γ = 0.1
(Tang et al., 2014). To incorporate the seed words
and trigrams into ASUM and SDTM, we initial-
ize βG, βM and βH differently. We assign a high
value of 2.0 for each seed word and trigram for
that level, and a low value of 10−6 for each word
that is a seed word for another level, and a default
value of 0.01 for all other words. This approach
is the same as previous papers (Jo and Oh, 2011;
Kim et al., 2013).
As Table 6 shows, SDTM performs better than
the other methods for accuracy as well as F-
measure. LDA and MedLDA generally show
the lowest performance, which is not surprising
given these models are quite general and not tuned
specifically for this type of semi-supervised clas-
sification task. BOW which is simple word fea-
tures also does not perform well, showing espe-
cially low F-measure for the H level. LIWC and
SEED perform better than LDA, but these have
quite low F-measure for G and H levels. ASUM
shows better performance for classifying H level
than others, confirming the effectiveness of a topic
modeling approach to this difficult task, but not as
well as SDTM. FirstP shows good F-measure for
the G level, but the H level F-measure is quite low,
even lower than SEED. Combining first-person
pronouns and seed words and trigrams (FP+SE1)
shows better than each feature alone, and the two
stage classifier (FP+SE2) which is a similar ap-
proach taken in SDTM shows better results. Fi-
nally, SDTM classifies G and M level at a similar
accuracy with FirstP, FP+SE1 and FP+SE2, but
it significantly improves accuracy for the H level
compared to all other methods.
</bodyText>
<sectionHeader confidence="0.606202" genericHeader="method">
6 Relations of Self-Disclosure and
Conversation Behaviors
</sectionHeader>
<bodyText confidence="0.999817">
In this section, we investigate whether there is
a relationship between self-disclosure and con-
versation behaviors over time. Self-disclosure is
one way to maintain and improve relationships
(Jourard, 1971; Joinson and Paine, 2007). So
two people’s intimacy changes over time has rela-
tionship with self-disclosure in their conversation.
However, it is hard to identify intimacy between
users in large scale online social network. So we
choose conversation behaviors such as conversa-
tion frequency and length which can be treated as
proxies for measuring intimacy between two peo-
ple (Emmers-Sommer, 2004; Bak et al., 2012).
</bodyText>
<page confidence="0.983238">
1992
</page>
<note confidence="0.532767">
Subsequent SD level
</note>
<bodyText confidence="0.992225235294118">
With SDTM, we can automatically classify the
5D level of a large number of conversations, so
we investigate whether there is a similar relation-
ship between self-disclosure in conversations and
subsequent conversation behaviors with the same
partner on Twitter.
For comparing conversation behaviors over
time, we divided the conversations into two sets
for each dyad. For the initial period, we include
conversations from the dyad’s first conversation to
20 days later. And for the subsequent period,
we include conversations during the subsequent 10
days. We compute proportions of conversation for
each 5D level for each dyad in the initial and
subsequent periods.
More specifically, we ask the following three
questions:
</bodyText>
<listItem confidence="0.996638909090909">
1. If a dyad shows high conversation frequency
at a particular time period, would they dis-
play higher 5D in their subsequent conver-
sations?
2. If a dyad displays high 5D level in their con-
versations at a particular time period, would
their subsequent conversations be longer?
3. If a dyad displays high overall 5D level,
would their conversations increase in length
over time more than dyads with lower overall
5D level?
</listItem>
<subsectionHeader confidence="0.998924">
6.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.999265411764706">
We first run SDTM with all of our Twitter con-
versation data with 150; 120; 120 topics for
SDTM KG, KM and KH respectively. The
hyper-parameters are the same as in section 5. To
handle a large dataset, we employ a distributed al-
gorithm (Newman et al., 2009), and run with 28
threads.
Table 7 shows some of the topics that were
prominent in each 5D level by KL-divergence. As
expected, G level includes general topics such as
food, celebrity, soccer and IT devices, M level in-
cludes personal communication and birthday, and
finally, H level includes sickness and profanity.
We define a new measurement, 5D level score
for a dyad in the period, which is a weighted sum
of each conversation with 5D levels mapped to 1,
2, and 3, for the levels G, M, and H, respectively.
</bodyText>
<figure confidence="0.9738189">
2.14
2.12
2.10
2.08
2.06
2.04
2.02
2.00
0 5 10 15 20 25 30 35
Initial conversation frequency
</figure>
<figureCaption confidence="0.986667">
Figure 5: Relationship between initial conversa-
</figureCaption>
<bodyText confidence="0.6473205">
tion frequency and subsequent 5D level. The
solid line is the linear regression line, and the co-
efficient is 0.0020 with p &lt; 0.0001, which shows
a significant positive relationship.
</bodyText>
<subsectionHeader confidence="0.9964375">
6.2 Does high frequency of conversation lead
to more self-disclosure?
</subsectionHeader>
<bodyText confidence="0.999881">
We investigate whether the initial conversation
frequency is correlated with the 5D level in the
subsequent period. We run linear regression with
the initial conversation frequency as the indepen-
dent variable, and 5D level in the subsequent pe-
riod as the dependent variable.
The regression coefficient is 0.0020 with low p-
value (p &lt; 0.0001). Figure 5 shows the scatter
plot. We can see that the slope of the regression
line is positive.
</bodyText>
<subsectionHeader confidence="0.9805455">
6.3 Does high self-disclosure lead to longer
conversations?
</subsectionHeader>
<bodyText confidence="0.999992230769231">
Now we investigate the effect of the self-
disclosure level to conversation length. We run
linear regression with the intial 5D level score as
the independent variable, and the rate of change
in conversation length between initial period
and subsequent period as the dependent variable.
Conversation length is measured by the number of
tweets in a conversation.
The result of regression is that the independent
variable’s coefficient is 0.048 with a low p-value
(p &lt; 0.0001). Figure 6 shows the scatter plot with
the regression line, and we can see that the slope
of regression line is positive.
</bodyText>
<page confidence="0.937761">
1993
</page>
<table confidence="0.998683823529412">
G level M level H level
101 184 176 36 104 82 113 33 19
chocolate obama league send twitter going ass better lips
butter he’s win email follow party bitch sick kisses
good romney game i’ll tumblr weekend fuck feel love
cake vote season sent tweet day yo throat smiles
peanut right team dm following night shit cold softly
milk president cup address account dinner fucking hope hand
sugar people city know fb birthday lmao pain eyes
cream good arsenal check followers tomorrow shut good neck
make going chelsea link facebook come dick cough arms
love time liverpool need followed i’ll kick bad head
yum party won message omg family face i’ve smirks
hot election football let right fun hoe need slowly
cookies gop united sure saw friends lmfao sore hair
banana paul final thanks page tonight nigga flu face
bread way away my email timeline plans bi today chest
</table>
<tableCaption confidence="0.999572">
Table 7: High ranked topics in each level by comparing KL-divergence with other level’s topics
</tableCaption>
<figure confidence="0.99927155">
.0 1.5 2.0 2.5 3.0
Initial SD level
10.5
10.0
9.5
9.0
8.5
8.0
0 5 10 15 20 25 30 35 40
Conversation order
high mid low
# Tweets in conversation
0.15
0.10
0.05
0.00
0.05
# Tweets in conversation changes proportion over time
0.10
1
</figure>
<figureCaption confidence="0.991472">
Figure 6: Relationship between initial 5D level
</figureCaption>
<bodyText confidence="0.94618625">
and conversation length changes over time. The
solid line is the linear regression line, and the co-
efficient is 0.048 with p &lt; 0.0001, which shows a
significant positive relationship.
6.4 Is there a difference in conversation
length patterns over time depending on
overall SD level?
Now we investigate the conversation length
changes over time with three groups, low,
medium, and high, by overall 5D level. Then
we investigate changes in conversation length over
time.
</bodyText>
<figureCaption confidence="0.989222">
Figure 7 shows the results of this investigation.
First, conversations are generally lengthier when
5D level is high. This phenomenon is also ob-
Figure 7: Changes in conversation length over
</figureCaption>
<bodyText confidence="0.990865363636364">
time. We divide dyads into three groups by SD
level score as low, medium, and high. Conversa-
tion length noticeably increases over time in the
medium and high groups, but only slight in the low
group.
served in figure 6, but here we can see it as a
long-term persistent pattern. Second, conversation
length increases consistently and significantly for
the high and medium groups, but for the low 5D
group, there is not a significant increase of conver-
sation length over time.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.956215">
Prior work on quantitatively analyzing self-
disclosure has relied on user surveys (Ledbetter et
</bodyText>
<page confidence="0.987319">
1994
</page>
<bodyText confidence="0.999098384615385">
al., 2011; Trepte and Reinecke, 2013) or human
annotation (Barak and Gluck-Ofri, 2007; Court-
ney Walton and Rice, 2013). These methods con-
sume much time and effort, so they are not suit-
able for large-scale studies. In prior work clos-
est to ours, Bak et al. (2012) showed that a topic
model can be used to identify self-disclosure, but
that work applies a two-step process in which a
basic topic model is first applied to find the top-
ics, and then the topics are post-processed for bi-
nary classification of self-disclosure. We improve
upon this work by applying a single unified model
of topics and self-disclosure for high accuracy in
classifying the three levels of self-disclosure.
Subjectivity which is aspect of expressing opin-
ions (Pang and Lee, 2008; Wiebe et al., 2004) is
related with self-disclosure, but they are different
dimensions of linguistic behavior. Because there
indeed are many high self-disclosure tweets that
are subjective, but there are also counter examples
in annotated dataset. The tweet “England manager
is Roy Hodgson.” is low self-disclosure and low
subjectivity, “I have barely any hair left.” is high
self-disclosure but low subjectivity, and “Senator
stop lying!” is low self-disclosure but high subjec-
tivity.
</bodyText>
<sectionHeader confidence="0.976556" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999929333333333">
In this paper, we have presented the self-disclosure
topic model (SDTM) for discovering topics and
classifying SD levels from Twitter conversation
data. We devised a set of effective seed words
and trigrams, mined from a dataset of secrets. We
also annotated Twitter conversations to make a
ground-truth dataset for SD level. With anno-
tated data, we showed that SDTM outperforms
previous methods in classification accuracy and F-
measure. We publish the source code of SDTM
and the dataset include annotated Twitter conver-
sations and SECRET publicly7.
We also analyzed the relationship between SD
level and conversation behaviors over time. We
found that there is a positive correlation be-
tween initial SD level and subsequent conversa-
tion length. Also, dyads show higher level of
SD if they initially display high conversation fre-
quency. Finally, dyads with overall medium and
high SD level will have longer conversations over
time. These results support previous results in so-
</bodyText>
<footnote confidence="0.960119">
7http://uilab.kaist.ac.kr/research/
EMNLP2014
</footnote>
<bodyText confidence="0.999955555555556">
cial psychology research with more robust results
from a large-scale dataset, and show the effective-
ness of computationally analyzing at SD behavior.
There are several future directions for this re-
search. First, we can improve our modeling for
higher accuracy and better interpretability. For
instance, SDTM only considers first-person pro-
nouns and topics. Naturally, there are other lin-
guistic patterns that can be identified by humans
but not captured by pronouns and topics. Sec-
ond, the number of topics for each level is varied,
and so we can explore nonparametric topic mod-
els (Teh et al., 2006) which infer the number of
topics from the data. Third, we can look at the
relationship between self-disclosure behavior and
general online social network usage beyond con-
versations. We will explore these directions in our
future work.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999186">
We would like to thank Jing Liu and Wayne Xin
Zhao for inspiring discussions, and the anony-
mous reviewers for helpful comments. Alice Oh
is supported by ICT R&amp;D program of MSIP/IITP
[10041313, UX-oriented Mobile SW Platform].
</bodyText>
<sectionHeader confidence="0.999103" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99924356">
JinYeong Bak, Suin Kim, and Alice Oh. 2012. Self-
disclosure and relationship strength in twitter con-
versations. In Proceedings ofACL.
Azy Barak and Orit Gluck-Ofri. 2007. Degree and
reciprocity of self-disclosure in online forums. Cy-
berPsychology &amp; Behavior, 10(3):407–417.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
danah boyd, Scott Golder, and Gilad Lotan. 2010.
Tweet, tweet, retweet: Conversational aspects of
retweeting on twitter. In Proceedings of HICSS.
S Courtney Walton and Ronald E Rice. 2013. Medi-
ated disclosure on twitter: The roles of gender and
identity in boundary impermeability, valence, dis-
closure, and stage. Computers in Human Behavior,
29(4):1465–1474.
Cristian Danescu-Niculescu-Mizil, Michael Gamon,
and Susan Dumais. 2011. Mark my words!: Lin-
guistic style accommodation in social media. In
Proceedings of WWW.
Tara M Emmers-Sommer. 2004. The effect of com-
munication quality and quantity indicators on inti-
macy and relational satisfaction. Journal of Social
and Personal Relationships, 21(3):399–411.
</reference>
<page confidence="0.824202">
1995
</page>
<reference confidence="0.99987895505618">
Joseph L Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological bul-
letin, 76(5):378.
David J Houghton and Adam N Joinson. 2012.
Linguistic markers of secrets and sensitive self-
disclosure in twitter. In Proceedings of HICSS.
Yohan Jo and Alice H Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In Proceedings of WSDM.
Adam N Joinson and Carina B Paine. 2007. Self-
disclosure, privacy and the internet. The Oxford
handbook of Internet psychology, pages 237–252.
Adam N Joinson. 2001. Self-disclosure in
computer-mediated communication: The role of
self-awareness and visual anonymity. European
Journal of Social Psychology, 31(2):177–192.
Sidney M Jourard. 1971. Self-disclosure: An experi-
mental analysis of the transparent self.
Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, and
Shixia Liu. 2013. A hierarchical aspect-sentiment
model for online reviews. In Proceedings of AAAI.
J Richard Landis and Gary G Koch. 1977. The mea-
surement of observer agreement for categorical data.
biometrics, pages 159–174.
Andrew M Ledbetter, Joseph P Mazer, Jocelyn M DeG-
root, Kevin R Meyer, Yuping Mao, and Brian Swaf-
ford. 2011. Attitudes toward online social con-
nection and self-disclosure as predictors of facebook
communication and relational closeness. Communi-
cation Research, 38(1):27–53.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Erika McCallister. 2010. Guide to protecting the confi-
dentiality ofpersonally identifiable information. DI-
ANE Publishing.
Arjun Mukherjee, Vivek Venkataraman, Bing Liu, and
Sharon Meraz. 2013. Public dialogue: Analysis of
tolerance in online discussions. In Proceedings of
ACL.
David Newman, Arthur Asuncion, Padhraic Smyth,
and Max Welling. 2009. Distributed algorithms
for topic models. Journal of Machine Learning Re-
search, 10:1801–1828.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of HLT-NAACL.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1–135.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Pro-
ceedings of HLT-NAACL.
Jian Tang, Zhaoshi Meng, Xuanlong Nguyen, Qiaozhu
Mei, and Ming Zhang. 2014. Understanding the
limiting factors of topic modeling via posterior con-
traction analysis. In Proceedings of The 31st In-
ternational Conference on Machine Learning, pages
190–198.
Yla R Tausczik and James W Pennebaker. 2010. The
psychological meaning of words: Liwc and comput-
erized text analysis methods. Journal of Language
and Social Psychology.
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the american statistical associ-
ation, 101(476).
Sabine Trepte and Leonard Reinecke. 2013. The re-
ciprocal effects of social network site use and the
disposition for self-disclosure: A longitudinal study.
Computers in Human Behavior, 29(3):1102 – 1112.
Sarah I Vondracek and Fred W Vondracek. 1971. The
manipulation and measurement of self-disclosure in
preadolescents. Merrill-Palmer Quarterly of Behav-
ior and Development, 17(1):51–58.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing subjective language. Computational linguistics,
30(3):277–308.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a maxent-lda hybrid. In Proceedings of
EMNLP.
Jun Zhu, Amr Ahmed, and Eric P Xing. 2012. Medlda:
maximum margin supervised topic models. Journal
of Machine Learning Research, 13:2237–2278.
</reference>
<page confidence="0.993659">
1996
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.315647">
<title confidence="0.991792">Self-disclosure topic model for classifying and analyzing conversations</title>
<author confidence="0.999848">Lin Alice Oh</author>
<affiliation confidence="0.999624">Department of Computer Science Microsoft Research Department of Computer Science</affiliation>
<address confidence="0.415285">KAIST Beijing 100080, P.R. China KAIST</address>
<author confidence="0.778919">South Korea South Korea</author>
<email confidence="0.996707">jy.bak@kaist.ac.kralice.oh@kaist.edu</email>
<abstract confidence="0.996165470588235">Self-disclosure, the act of revealing oneself to others, is an important social behavior that strengthens interpersonal relationships and increases social support. Although there are many social science studies of self-disclosure, they are based on manual coding of small datasets and questionnaires. We conduct a computational analysis of self-disclosure with a large dataset of naturally-occurring conversations, a semi-supervised machine learning algorithm, and a computational analysis of the effects of self-disclosure on subsequent conversations. We use a longitudinal dataset of 17 million tweets, all of which occurred in conversations that consist of five or more tweets directly replying to the previous tweet, and from dyads with twenty of more conversations each. We develop self-disclosure topic model (SDTM), a variant of latent Dirichlet allocation (LDA) for automatically classifying the level of self-disclosure for each tweet. We take the results of SDTM and analyze the effects of self-disclosure on subsequent conversations. Our model significantly outperforms several comparable methods on classifying the level of selfdisclosure, and the analysis of the longitudinal data using SDTM uncovers significant and positive correlation between selfdisclosure and conversation frequency and length.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>JinYeong Bak</author>
<author>Suin Kim</author>
<author>Alice Oh</author>
</authors>
<title>Selfdisclosure and relationship strength in twitter conversations.</title>
<date>2012</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="21147" citStr="Bak et al., 2012" startWordPosition="3528" endWordPosition="3531">hat level to the conversation. When the proportions of tweets at M or H level are both less than 0.2, we assign G to the 5D level. The reason for setting 0.2 as the threshold is that a conversation containing tweets with H or M level of selfdisclosure usually starts with a greeting or a general comment, and contains one or more questions or comments before or after the self-disclosure tweet. We compare SDTM with the following methods for classifying conversations for 5D level: • LDA (Blei et al., 2003): A Bayesian topic model. Each conversation is treated as a document. Used in previous work (Bak et al., 2012). • MedLDA (Zhu et al., 2012): A supervised topic model for document classification. Each conversation is treated as a document and response variable can be mapped to a 5D level. • LIWC (Tausczik and Pennebaker, 2010): Word counts of particular categories5. Used in previous work (Houghton and Joinson, 2012). • Bag of Words + Bigrams + Trigrams (BOW+): A bag of words, bigram and trigram features. We exclude features that appear only once or twice. • Seed words and trigrams (SEED): Occurrences of seed words/trigrams from SECRET which are described in section 3.3. • SDTM with seed words from anno</context>
<context position="26895" citStr="Bak et al., 2012" startWordPosition="4525" endWordPosition="4528">his section, we investigate whether there is a relationship between self-disclosure and conversation behaviors over time. Self-disclosure is one way to maintain and improve relationships (Jourard, 1971; Joinson and Paine, 2007). So two people’s intimacy changes over time has relationship with self-disclosure in their conversation. However, it is hard to identify intimacy between users in large scale online social network. So we choose conversation behaviors such as conversation frequency and length which can be treated as proxies for measuring intimacy between two people (Emmers-Sommer, 2004; Bak et al., 2012). 1992 Subsequent SD level With SDTM, we can automatically classify the 5D level of a large number of conversations, so we investigate whether there is a similar relationship between self-disclosure in conversations and subsequent conversation behaviors with the same partner on Twitter. For comparing conversation behaviors over time, we divided the conversations into two sets for each dyad. For the initial period, we include conversations from the dyad’s first conversation to 20 days later. And for the subsequent period, we include conversations during the subsequent 10 days. We compute propor</context>
<context position="33085" citStr="Bak et al. (2012)" startWordPosition="5570" endWordPosition="5573"> see it as a long-term persistent pattern. Second, conversation length increases consistently and significantly for the high and medium groups, but for the low 5D group, there is not a significant increase of conversation length over time. 7 Related Work Prior work on quantitatively analyzing selfdisclosure has relied on user surveys (Ledbetter et 1994 al., 2011; Trepte and Reinecke, 2013) or human annotation (Barak and Gluck-Ofri, 2007; Courtney Walton and Rice, 2013). These methods consume much time and effort, so they are not suitable for large-scale studies. In prior work closest to ours, Bak et al. (2012) showed that a topic model can be used to identify self-disclosure, but that work applies a two-step process in which a basic topic model is first applied to find the topics, and then the topics are post-processed for binary classification of self-disclosure. We improve upon this work by applying a single unified model of topics and self-disclosure for high accuracy in classifying the three levels of self-disclosure. Subjectivity which is aspect of expressing opinions (Pang and Lee, 2008; Wiebe et al., 2004) is related with self-disclosure, but they are different dimensions of linguistic behav</context>
</contexts>
<marker>Bak, Kim, Oh, 2012</marker>
<rawString>JinYeong Bak, Suin Kim, and Alice Oh. 2012. Selfdisclosure and relationship strength in twitter conversations. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Azy Barak</author>
<author>Orit Gluck-Ofri</author>
</authors>
<title>Degree and reciprocity of self-disclosure in online forums.</title>
<date>2007</date>
<journal>CyberPsychology &amp; Behavior,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="5583" citStr="Barak and Gluck-Ofri, 2007" startWordPosition="812" endWordPosition="815">us of OSN conversations. We discuss three approaches, first, using first-person pronoun features, and second, extracting seed words and phrases from the Twitter conversation corpus, and third, extracting seed words and phrases from an external corpus of anonymously posted secrets, and we demonstrate the efficacy of those approaches with an annotated corpus. 2.1 Self-disclosure (SD) level To analyze self-disclosure, researchers categorize self-disclosure language into three levels: G (general) for no disclosure, M for medium disclosure, and H for high disclosure (Vondracek and Vondracek, 1971; Barak and Gluck-Ofri, 2007). Utterances that contain general (non-sensitive) information about the self or someone close (e.g., a family member) are categorized as M. Examples are personal events, past history, or future plans. Utterances about age, occupation and hobbies are also included. Utterances that contain sensitive information about the self or someone close are categorized as H. Sensitive information includes personal characteristics, problematic behaviors, physical appearance and wishful ideas. Generally, these are thoughts and information that (f) A M level Twitter conversation (g) A H level Twitter conversa</context>
<context position="7084" citStr="Barak and Gluck-Ofri, 2007" startWordPosition="1045" endWordPosition="1048">mples include gossip about celebrities or factual discourse about current events. Figure 1 shows Twitter conversation examples with G, M and H levels from annotated dataset (see Section 4.2 for a detailed description of the annotated dataset). 2.2 G Level of Self-Disclosure An obvious clue of self-disclosure is the use of first-person pronouns. For example, phrases such as ‘I live’ or ‘My name is’ indicate that the utterance contains personal information. In previous research, the simple method of counting first-person pronouns was used to measure the degree of self-disclosure (Joinson, 2001; Barak and Gluck-Ofri, 2007). Consequently, the absence of a first-person pronoun signals that the utterance belongs in the G level of self-disclosure. We verify this pattern with a dataset of Tweets annotated with G, M, and H levels. We divide the annotated Tweets into two classes, G and M/H. Then we compute mutual information of each unigram, bigram, or trigram feature to see which features are most discriminative. As Table 1 shows, 18 out of 30 (a) A G level Twitter conversation 1987 Category Words/Expressions Unigram my, I, I’m, I’ll, but, was, I’ve, love, dad, have Bigram I love, I was, I have, my dad, go to, my mom</context>
<context position="8669" citStr="Barak and Gluck-Ofri, 2007" startWordPosition="1313" endWordPosition="1316">arance mental/ physical condition acne, hair, overweight, stomach, chest, hand, scar, thighs, chubby addicted, bulimia, doctor, illness, alcoholic, disease, drugs, pills ankle, face, toe, skin ache, epilepsy, pain, chiropractor, codeine Table 1: High ranked words and expressions by mutual information between G and M/H level in annotated conversations. most highly ranked discriminative features contain a first-person pronoun. 2.3 M Level of Self-Disclosure Utterances with M level include two types: 1) information related with past events and future plans, and 2) general information about self (Barak and Gluck-Ofri, 2007). For the former, we add as seed trigrams ‘I have been’ and ‘I will’. For the latter, we use seven types of information generally accepted to be personally identifiable information (McCallister, 2010), as listed in the left column of Table 2. To find the appropriate trigrams for those, we take Twitter conversation data (described in Section 4.1) and look for trigrams that begin with ‘I’ and ‘my’ and occur more than 200 times. We then check each one to see whether it is related with any of the seven types listed in the table. As a result, we find 57 seed trigrams for M level. Table 2 shows seve</context>
<context position="19463" citStr="Barak and Gluck-Ofri, 2007" startWordPosition="3248" endWordPosition="3251">mes and url in tweets with random strings. This dataset consists of 101,686 users, 61,451 dyads, 1,956,993 conversations and 17,178,638 tweets which were posted between August 2007 to July 2013. Table 5 summarizes the dataset. 4.2 Annotating self-disclosure level To measure the accuracy of our model, we randomly sample 301 conversations, each with ten or fewer tweets, and ask three judges, fluent in English and graduate students/researchers, to annotate each tweet with the level of self-disclosure. Judges first read and discussed the definitions and examples of self-disclosure level shown in (Barak and Gluck-Ofri, 2007), then they worked separately on a Web-based platform. As a result of annotation, there are 122 G level converstaions, 147 M level and 32 H level con3https://dev.twitter.com/docs/api/ streaming 4https://github.com/shuyo/ldig Users 101,686 61,451 1990 Figure 4: Screenshot of annotation web-based platform. Annotators read a Twitter conversation and annotate self-disclosure level to each tweet. versations, and inter-rater agreement using Fleiss kappa (Fleiss, 1971) is 0.68, which is substantial agreement result (Landis and Koch, 1977). 5 Classification of Self-Disclosure Level This section descri</context>
<context position="32908" citStr="Barak and Gluck-Ofri, 2007" startWordPosition="5536" endWordPosition="5539">el score as low, medium, and high. Conversation length noticeably increases over time in the medium and high groups, but only slight in the low group. served in figure 6, but here we can see it as a long-term persistent pattern. Second, conversation length increases consistently and significantly for the high and medium groups, but for the low 5D group, there is not a significant increase of conversation length over time. 7 Related Work Prior work on quantitatively analyzing selfdisclosure has relied on user surveys (Ledbetter et 1994 al., 2011; Trepte and Reinecke, 2013) or human annotation (Barak and Gluck-Ofri, 2007; Courtney Walton and Rice, 2013). These methods consume much time and effort, so they are not suitable for large-scale studies. In prior work closest to ours, Bak et al. (2012) showed that a topic model can be used to identify self-disclosure, but that work applies a two-step process in which a basic topic model is first applied to find the topics, and then the topics are post-processed for binary classification of self-disclosure. We improve upon this work by applying a single unified model of topics and self-disclosure for high accuracy in classifying the three levels of self-disclosure. Su</context>
</contexts>
<marker>Barak, Gluck-Ofri, 2007</marker>
<rawString>Azy Barak and Orit Gluck-Ofri. 2007. Degree and reciprocity of self-disclosure in online forums. CyberPsychology &amp; Behavior, 10(3):407–417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="13503" citStr="Blei et al., 2003" startWordPosition="2160" endWordPosition="2163">v assigned topic k in tweet ct Table 4: Summary of notations used in SDTM 3 Self-Disclosure Topic Model This section describes our model, the selfdisclosure topic model (SDTM), for classifying self-disclosure level and discovering topics for each self-disclosure level. 3.1 Model In section 2, we discussed different approaches to identifying each level of self-disclosure, based on social science literature, annotated and unannotated Tweets, and an external corpus of secret posts. In this section, we describe our self-disclosure topic model, based on the widely used latent Dirichlet allocation (Blei et al., 2003), which incorporates those approaches. Figure 2 illustrates the graphical model of 1. For each level l ∈ {G, M, H}: For each topic k ∈ {1, ... ,Kl}: Draw olk ∼ Dir(/.l) 2. For each conversation c ∈ {1,... ,C}: (a) Draw 9G c ∼ Dir(ct) (b) Draw 9M c ∼ Dir(ct) (c) Draw 9Hc ∼ Dir(ct) (d) Draw Trc ∼ Dir(y) (e) For each message t ∈ {1, ... ,T}: i. Observe first-person pronouns features xct ii. Draw wct ∼ MaxEnt(xct, A) iii. Draw yct ∼ Bernoulli(wct) iv. If yct = 0 which is G level: A. Draw zct ∼ Mult(9Gc ) B. For each word n ∈ {1, ... , N}: Draw word wctn ∼ Mult(oGzct) Else which can be M or H level</context>
<context position="21037" citStr="Blei et al., 2003" startWordPosition="3508" endWordPosition="3511">tweets at M or H level is equal to or greater than 0.2, we take the level of the larger proportion and assign that level to the conversation. When the proportions of tweets at M or H level are both less than 0.2, we assign G to the 5D level. The reason for setting 0.2 as the threshold is that a conversation containing tweets with H or M level of selfdisclosure usually starts with a greeting or a general comment, and contains one or more questions or comments before or after the self-disclosure tweet. We compare SDTM with the following methods for classifying conversations for 5D level: • LDA (Blei et al., 2003): A Bayesian topic model. Each conversation is treated as a document. Used in previous work (Bak et al., 2012). • MedLDA (Zhu et al., 2012): A supervised topic model for document classification. Each conversation is treated as a document and response variable can be mapped to a 5D level. • LIWC (Tausczik and Pennebaker, 2010): Word counts of particular categories5. Used in previous work (Houghton and Joinson, 2012). • Bag of Words + Bigrams + Trigrams (BOW+): A bag of words, bigram and trigram features. We exclude features that appear only once or twice. • Seed words and trigrams (SEED): Occur</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>danah boyd</author>
<author>Scott Golder</author>
<author>Gilad Lotan</author>
</authors>
<title>Tweet, tweet, retweet: Conversational aspects of retweeting on twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of HICSS.</booktitle>
<contexts>
<context position="17817" citStr="boyd et al., 2010" startWordPosition="2978" endWordPosition="2981"> use a large dataset of conversations consisting of Tweets over three years such that we can analyze the relationship between self-disclosure behavior and conversation frequency and length over time. We chose to crawl Twitter because it offers a practical and large source of conversations (Ritter et al., 2010). Others have also analyzed Twitter conversations for natural language and social media Conv’s Tweets 1,956,993 17,178,638 Table 5: Dataset of Twitter conversations. We chose conversations consisting of five or more tweets each. We chose dyads with twenty or more conversations. research (boyd et al., 2010; Danescu-NiculescuMizil et al., 2011), but we collect conversations from the same set of dyads over several months for a unique longitudinal dataset. We also make sure that each conversation is at least five tweets, and that each dyad has at least twenty conversations. 4.1 Collecting Twitter conversations We define a Twitter conversation as a chain of tweets where two users are consecutively replying to each other’s tweets using the Twitter reply button. We initialize the set of users by randomly sampling thirteen users who reply to other users in English from the Twitter public streams3. The</context>
</contexts>
<marker>boyd, Golder, Lotan, 2010</marker>
<rawString>danah boyd, Scott Golder, and Gilad Lotan. 2010. Tweet, tweet, retweet: Conversational aspects of retweeting on twitter. In Proceedings of HICSS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Courtney Walton</author>
<author>Ronald E Rice</author>
</authors>
<title>Mediated disclosure on twitter: The roles of gender and identity in boundary impermeability, valence, disclosure, and stage.</title>
<date>2013</date>
<journal>Computers in Human Behavior,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="32941" citStr="Walton and Rice, 2013" startWordPosition="5542" endWordPosition="5545">nversation length noticeably increases over time in the medium and high groups, but only slight in the low group. served in figure 6, but here we can see it as a long-term persistent pattern. Second, conversation length increases consistently and significantly for the high and medium groups, but for the low 5D group, there is not a significant increase of conversation length over time. 7 Related Work Prior work on quantitatively analyzing selfdisclosure has relied on user surveys (Ledbetter et 1994 al., 2011; Trepte and Reinecke, 2013) or human annotation (Barak and Gluck-Ofri, 2007; Courtney Walton and Rice, 2013). These methods consume much time and effort, so they are not suitable for large-scale studies. In prior work closest to ours, Bak et al. (2012) showed that a topic model can be used to identify self-disclosure, but that work applies a two-step process in which a basic topic model is first applied to find the topics, and then the topics are post-processed for binary classification of self-disclosure. We improve upon this work by applying a single unified model of topics and self-disclosure for high accuracy in classifying the three levels of self-disclosure. Subjectivity which is aspect of exp</context>
</contexts>
<marker>Walton, Rice, 2013</marker>
<rawString>S Courtney Walton and Ronald E Rice. 2013. Mediated disclosure on twitter: The roles of gender and identity in boundary impermeability, valence, disclosure, and stage. Computers in Human Behavior, 29(4):1465–1474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Michael Gamon</author>
<author>Susan Dumais</author>
</authors>
<title>Mark my words!: Linguistic style accommodation in social media.</title>
<date>2011</date>
<booktitle>In Proceedings of WWW.</booktitle>
<marker>Danescu-Niculescu-Mizil, Gamon, Dumais, 2011</marker>
<rawString>Cristian Danescu-Niculescu-Mizil, Michael Gamon, and Susan Dumais. 2011. Mark my words!: Linguistic style accommodation in social media. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tara M Emmers-Sommer</author>
</authors>
<title>The effect of communication quality and quantity indicators on intimacy and relational satisfaction.</title>
<date>2004</date>
<journal>Journal of Social and Personal Relationships,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="26876" citStr="Emmers-Sommer, 2004" startWordPosition="4523" endWordPosition="4524">sation Behaviors In this section, we investigate whether there is a relationship between self-disclosure and conversation behaviors over time. Self-disclosure is one way to maintain and improve relationships (Jourard, 1971; Joinson and Paine, 2007). So two people’s intimacy changes over time has relationship with self-disclosure in their conversation. However, it is hard to identify intimacy between users in large scale online social network. So we choose conversation behaviors such as conversation frequency and length which can be treated as proxies for measuring intimacy between two people (Emmers-Sommer, 2004; Bak et al., 2012). 1992 Subsequent SD level With SDTM, we can automatically classify the 5D level of a large number of conversations, so we investigate whether there is a similar relationship between self-disclosure in conversations and subsequent conversation behaviors with the same partner on Twitter. For comparing conversation behaviors over time, we divided the conversations into two sets for each dyad. For the initial period, we include conversations from the dyad’s first conversation to 20 days later. And for the subsequent period, we include conversations during the subsequent 10 days</context>
</contexts>
<marker>Emmers-Sommer, 2004</marker>
<rawString>Tara M Emmers-Sommer. 2004. The effect of communication quality and quantity indicators on intimacy and relational satisfaction. Journal of Social and Personal Relationships, 21(3):399–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<contexts>
<context position="19929" citStr="Fleiss, 1971" startWordPosition="3313" endWordPosition="3314">vel of self-disclosure. Judges first read and discussed the definitions and examples of self-disclosure level shown in (Barak and Gluck-Ofri, 2007), then they worked separately on a Web-based platform. As a result of annotation, there are 122 G level converstaions, 147 M level and 32 H level con3https://dev.twitter.com/docs/api/ streaming 4https://github.com/shuyo/ldig Users 101,686 61,451 1990 Figure 4: Screenshot of annotation web-based platform. Annotators read a Twitter conversation and annotate self-disclosure level to each tweet. versations, and inter-rater agreement using Fleiss kappa (Fleiss, 1971) is 0.68, which is substantial agreement result (Landis and Koch, 1977). 5 Classification of Self-Disclosure Level This section describes experiments and results of SDTM as well as several other methods for classification of self-disclosure level. We first start with the annotated dataset in section 4.2 in which each tweet is annotated with 5D level. We then aggregate all of the tweets of a conversation, and we compute the proportions of tweets in each 5D level. When the proportion of tweets at M or H level is equal to or greater than 0.2, we take the level of the larger proportion and assign </context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J Houghton</author>
<author>Adam N Joinson</author>
</authors>
<title>Linguistic markers of secrets and sensitive selfdisclosure in twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of HICSS.</booktitle>
<contexts>
<context position="21455" citStr="Houghton and Joinson, 2012" startWordPosition="3580" endWordPosition="3583">mment, and contains one or more questions or comments before or after the self-disclosure tweet. We compare SDTM with the following methods for classifying conversations for 5D level: • LDA (Blei et al., 2003): A Bayesian topic model. Each conversation is treated as a document. Used in previous work (Bak et al., 2012). • MedLDA (Zhu et al., 2012): A supervised topic model for document classification. Each conversation is treated as a document and response variable can be mapped to a 5D level. • LIWC (Tausczik and Pennebaker, 2010): Word counts of particular categories5. Used in previous work (Houghton and Joinson, 2012). • Bag of Words + Bigrams + Trigrams (BOW+): A bag of words, bigram and trigram features. We exclude features that appear only once or twice. • Seed words and trigrams (SEED): Occurrences of seed words/trigrams from SECRET which are described in section 3.3. • SDTM with seed words from annotated Tweets (SDTM−): To compare with SDTM below using seed words from SECRET, this uses seed words from the annotated data described in section 2.4. • ASUM (Jo and Oh, 2011): A joint model of sentiments and topics. We map each 5D level to one sentiment and use the same seed words/trigrams from SECRET as in</context>
</contexts>
<marker>Houghton, Joinson, 2012</marker>
<rawString>David J Houghton and Adam N Joinson. 2012. Linguistic markers of secrets and sensitive selfdisclosure in twitter. In Proceedings of HICSS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohan Jo</author>
<author>Alice H Oh</author>
</authors>
<title>Aspect and sentiment unification model for online review analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of WSDM.</booktitle>
<contexts>
<context position="10136" citStr="Jo and Oh, 2011" startWordPosition="1561" endWordPosition="1564">(Barak and Gluck-Ofri, 2007). These are generally kept as secrets. With this intuition, we crawled 26,523 posts from Six Billion Secrets1 site where users post secrets anonymously2. We 1http://www.sixbillionsecrets.com 2This site is regularly monitored for spam. call this external dataset SECRET. Unlike G and M levels, evidence of H level of self-disclosure tends to be topical, such as physical appearance, mental and physical illnesses, and family problems, so we take an approach of fitting a topic model driven by seed words. A similar approach has been successful in sentiment classification (Jo and Oh, 2011; Kim et al., 2013). A critical component of this approach is the set of seed words with which to drive the discovery of topics that are most indicative of H level selfdisclosure. To extract the seed words that express secretive personal information, we compute mutual information (Manning et al., 2008) with SECRET and 24,610 randomly selected tweets. We select 1,000 words with high mutual information and filter out stop words. Table 3 shows some of these words. To extract seed trigrams of secretive wishes, we again look for trigrams that start with ‘I’ or ‘my’, occur more than 200 times, and s</context>
<context position="16215" citStr="Jo and Oh, 2011" startWordPosition="2677" endWordPosition="2680">IPl Kl 3 Y Ir r 0l 3 W a z N W Y A X T C 1989 Dyads 3.3 Classifying M vs H levels The second part of the classification, the M and the H level, is driven by informative priors with seed words and seed trigrams. In the graphical model, r is the latent variable that represents this classification, and π is the distribution over r. γ is a non-informative prior for π, and βl is an informative prior for each SD level by seed words. For example, we assign a high value for the seed word ‘acne’ for βH, and a low value for ‘My name is’. This approach is the same as joint models of topic and sentiment (Jo and Oh, 2011; Kim et al., 2013). 3.4 Inference For posterior inference of SDTM, we use collapsed Gibbs sampling which integrates out latent random variables ω, π, θ, and φ. Then we only need to compute y, r and z for each tweet. We compute full conditional distribution p(yct = j0,rct = l0, zct = k0|y−ct, r− ct, z−ct, w, x) for tweet ct as follows: p(yct = 0, zct = k0|y−ct, r−ct, z−ct, w, x) exp(λ0 · xct) ∝ P1 j=0 exp(λj · xct) g(c, t, l0, k0), p(yct = 1, rct = l0, zct = k0|y−ct, r−ct, z−ct, w, x) ∝ j=0 exp(λj · xct)(γl0 + n(−ct) exp(λ1 · xct) P1 cl0 ) g(c,t, l0,k0), where z−ct, r−ct, y−ct are z, r, y with</context>
<context position="21921" citStr="Jo and Oh, 2011" startWordPosition="3665" endWordPosition="3668">apped to a 5D level. • LIWC (Tausczik and Pennebaker, 2010): Word counts of particular categories5. Used in previous work (Houghton and Joinson, 2012). • Bag of Words + Bigrams + Trigrams (BOW+): A bag of words, bigram and trigram features. We exclude features that appear only once or twice. • Seed words and trigrams (SEED): Occurrences of seed words/trigrams from SECRET which are described in section 3.3. • SDTM with seed words from annotated Tweets (SDTM−): To compare with SDTM below using seed words from SECRET, this uses seed words from the annotated data described in section 2.4. • ASUM (Jo and Oh, 2011): A joint model of sentiments and topics. We map each 5D level to one sentiment and use the same seed words/trigrams from SECRET as in SDTM below. Used in previous work (Bak et al., 2012). • First-person pronouns (FirstP): Occurrence of first-person pronouns which are described in section 3.2. To identify first-person pronouns, we tagged parts of speech in each tweet with the Twitter POS tagger (Owoputi et al., 2013). • First-person pronouns + Seed words/trigrams (FP+SE1): First-person pronouns and seed words/trigrams from SECRET. • Two stage classifier with First-person pronouns + Seed words/</context>
<context position="25028" citStr="Jo and Oh, 2011" startWordPosition="4221" endWordPosition="4224"> topics. We assume that a conversation has few topics 6It performs better than other classifiers (C4.5, NaiveBayes, SVM with linear kernel, polynomial kernel and radial basis) and self-disclosure levels, so we set α = γ = 0.1 (Tang et al., 2014). To incorporate the seed words and trigrams into ASUM and SDTM, we initialize βG, βM and βH differently. We assign a high value of 2.0 for each seed word and trigram for that level, and a low value of 10−6 for each word that is a seed word for another level, and a default value of 0.01 for all other words. This approach is the same as previous papers (Jo and Oh, 2011; Kim et al., 2013). As Table 6 shows, SDTM performs better than the other methods for accuracy as well as Fmeasure. LDA and MedLDA generally show the lowest performance, which is not surprising given these models are quite general and not tuned specifically for this type of semi-supervised classification task. BOW which is simple word features also does not perform well, showing especially low F-measure for the H level. LIWC and SEED perform better than LDA, but these have quite low F-measure for G and H levels. ASUM shows better performance for classifying H level than others, confirming the</context>
</contexts>
<marker>Jo, Oh, 2011</marker>
<rawString>Yohan Jo and Alice H Oh. 2011. Aspect and sentiment unification model for online review analysis. In Proceedings of WSDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam N Joinson</author>
<author>Carina B Paine</author>
</authors>
<title>Selfdisclosure, privacy and the internet. The Oxford handbook of Internet psychology,</title>
<date>2007</date>
<pages>237--252</pages>
<contexts>
<context position="1966" citStr="Joinson and Paine, 2007" startWordPosition="281" endWordPosition="284"> self-disclosure on subsequent conversations. Our model significantly outperforms several comparable methods on classifying the level of selfdisclosure, and the analysis of the longitudinal data using SDTM uncovers significant and positive correlation between selfdisclosure and conversation frequency and length. 1 Introduction Self-disclosure is an important and pervasive social behavior. People disclose personal information about themselves to improve and maintain ∗This work was done when JinYeong Bak was a visiting student at Microsoft Research, Beijing, China. relationships (Jourard, 1971; Joinson and Paine, 2007). A common instance of self-disclosure is the start of a conversation with an exchange of names and additional self-introductions. Another example of self-disclosure, shown in Figure 1c, where the information disclosed about a family member’s serious illness, is much more personal than the exchange of names. In this paper, we seek to understand this important social behavior using a large-scale Twitter conversation data, automatically classifying the level of self-disclosure using machine learning and correlating the patterns with conversational behaviors which can serve as proxies for measuri</context>
<context position="26505" citStr="Joinson and Paine, 2007" startWordPosition="4464" endWordPosition="4467">igrams (FP+SE1) shows better than each feature alone, and the two stage classifier (FP+SE2) which is a similar approach taken in SDTM shows better results. Finally, SDTM classifies G and M level at a similar accuracy with FirstP, FP+SE1 and FP+SE2, but it significantly improves accuracy for the H level compared to all other methods. 6 Relations of Self-Disclosure and Conversation Behaviors In this section, we investigate whether there is a relationship between self-disclosure and conversation behaviors over time. Self-disclosure is one way to maintain and improve relationships (Jourard, 1971; Joinson and Paine, 2007). So two people’s intimacy changes over time has relationship with self-disclosure in their conversation. However, it is hard to identify intimacy between users in large scale online social network. So we choose conversation behaviors such as conversation frequency and length which can be treated as proxies for measuring intimacy between two people (Emmers-Sommer, 2004; Bak et al., 2012). 1992 Subsequent SD level With SDTM, we can automatically classify the 5D level of a large number of conversations, so we investigate whether there is a similar relationship between self-disclosure in conversa</context>
</contexts>
<marker>Joinson, Paine, 2007</marker>
<rawString>Adam N Joinson and Carina B Paine. 2007. Selfdisclosure, privacy and the internet. The Oxford handbook of Internet psychology, pages 237–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam N Joinson</author>
</authors>
<title>Self-disclosure in computer-mediated communication: The role of self-awareness and visual anonymity.</title>
<date>2001</date>
<journal>European Journal of Social Psychology,</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="7055" citStr="Joinson, 2001" startWordPosition="1043" endWordPosition="1044">rized as G. Examples include gossip about celebrities or factual discourse about current events. Figure 1 shows Twitter conversation examples with G, M and H levels from annotated dataset (see Section 4.2 for a detailed description of the annotated dataset). 2.2 G Level of Self-Disclosure An obvious clue of self-disclosure is the use of first-person pronouns. For example, phrases such as ‘I live’ or ‘My name is’ indicate that the utterance contains personal information. In previous research, the simple method of counting first-person pronouns was used to measure the degree of self-disclosure (Joinson, 2001; Barak and Gluck-Ofri, 2007). Consequently, the absence of a first-person pronoun signals that the utterance belongs in the G level of self-disclosure. We verify this pattern with a dataset of Tweets annotated with G, M, and H levels. We divide the annotated Tweets into two classes, G and M/H. Then we compute mutual information of each unigram, bigram, or trigram feature to see which features are most discriminative. As Table 1 shows, 18 out of 30 (a) A G level Twitter conversation 1987 Category Words/Expressions Unigram my, I, I’m, I’ll, but, was, I’ve, love, dad, have Bigram I love, I was, </context>
</contexts>
<marker>Joinson, 2001</marker>
<rawString>Adam N Joinson. 2001. Self-disclosure in computer-mediated communication: The role of self-awareness and visual anonymity. European Journal of Social Psychology, 31(2):177–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney M Jourard</author>
</authors>
<title>Self-disclosure: An experimental analysis of the transparent self.</title>
<date>1971</date>
<contexts>
<context position="1940" citStr="Jourard, 1971" startWordPosition="279" endWordPosition="280"> the effects of self-disclosure on subsequent conversations. Our model significantly outperforms several comparable methods on classifying the level of selfdisclosure, and the analysis of the longitudinal data using SDTM uncovers significant and positive correlation between selfdisclosure and conversation frequency and length. 1 Introduction Self-disclosure is an important and pervasive social behavior. People disclose personal information about themselves to improve and maintain ∗This work was done when JinYeong Bak was a visiting student at Microsoft Research, Beijing, China. relationships (Jourard, 1971; Joinson and Paine, 2007). A common instance of self-disclosure is the start of a conversation with an exchange of names and additional self-introductions. Another example of self-disclosure, shown in Figure 1c, where the information disclosed about a family member’s serious illness, is much more personal than the exchange of names. In this paper, we seek to understand this important social behavior using a large-scale Twitter conversation data, automatically classifying the level of self-disclosure using machine learning and correlating the patterns with conversational behaviors which can se</context>
<context position="26479" citStr="Jourard, 1971" startWordPosition="4462" endWordPosition="4463">ed words and trigrams (FP+SE1) shows better than each feature alone, and the two stage classifier (FP+SE2) which is a similar approach taken in SDTM shows better results. Finally, SDTM classifies G and M level at a similar accuracy with FirstP, FP+SE1 and FP+SE2, but it significantly improves accuracy for the H level compared to all other methods. 6 Relations of Self-Disclosure and Conversation Behaviors In this section, we investigate whether there is a relationship between self-disclosure and conversation behaviors over time. Self-disclosure is one way to maintain and improve relationships (Jourard, 1971; Joinson and Paine, 2007). So two people’s intimacy changes over time has relationship with self-disclosure in their conversation. However, it is hard to identify intimacy between users in large scale online social network. So we choose conversation behaviors such as conversation frequency and length which can be treated as proxies for measuring intimacy between two people (Emmers-Sommer, 2004; Bak et al., 2012). 1992 Subsequent SD level With SDTM, we can automatically classify the 5D level of a large number of conversations, so we investigate whether there is a similar relationship between s</context>
</contexts>
<marker>Jourard, 1971</marker>
<rawString>Sidney M Jourard. 1971. Self-disclosure: An experimental analysis of the transparent self.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suin Kim</author>
<author>Jianwen Zhang</author>
<author>Zheng Chen</author>
<author>Alice Oh</author>
<author>Shixia Liu</author>
</authors>
<title>A hierarchical aspect-sentiment model for online reviews.</title>
<date>2013</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="10155" citStr="Kim et al., 2013" startWordPosition="1565" endWordPosition="1568">Ofri, 2007). These are generally kept as secrets. With this intuition, we crawled 26,523 posts from Six Billion Secrets1 site where users post secrets anonymously2. We 1http://www.sixbillionsecrets.com 2This site is regularly monitored for spam. call this external dataset SECRET. Unlike G and M levels, evidence of H level of self-disclosure tends to be topical, such as physical appearance, mental and physical illnesses, and family problems, so we take an approach of fitting a topic model driven by seed words. A similar approach has been successful in sentiment classification (Jo and Oh, 2011; Kim et al., 2013). A critical component of this approach is the set of seed words with which to drive the discovery of topics that are most indicative of H level selfdisclosure. To extract the seed words that express secretive personal information, we compute mutual information (Manning et al., 2008) with SECRET and 24,610 randomly selected tweets. We select 1,000 words with high mutual information and filter out stop words. Table 3 shows some of these words. To extract seed trigrams of secretive wishes, we again look for trigrams that start with ‘I’ or ‘my’, occur more than 200 times, and select trigrams of w</context>
<context position="16234" citStr="Kim et al., 2013" startWordPosition="2681" endWordPosition="2684">l 3 W a z N W Y A X T C 1989 Dyads 3.3 Classifying M vs H levels The second part of the classification, the M and the H level, is driven by informative priors with seed words and seed trigrams. In the graphical model, r is the latent variable that represents this classification, and π is the distribution over r. γ is a non-informative prior for π, and βl is an informative prior for each SD level by seed words. For example, we assign a high value for the seed word ‘acne’ for βH, and a low value for ‘My name is’. This approach is the same as joint models of topic and sentiment (Jo and Oh, 2011; Kim et al., 2013). 3.4 Inference For posterior inference of SDTM, we use collapsed Gibbs sampling which integrates out latent random variables ω, π, θ, and φ. Then we only need to compute y, r and z for each tweet. We compute full conditional distribution p(yct = j0,rct = l0, zct = k0|y−ct, r− ct, z−ct, w, x) for tweet ct as follows: p(yct = 0, zct = k0|y−ct, r−ct, z−ct, w, x) exp(λ0 · xct) ∝ P1 j=0 exp(λj · xct) g(c, t, l0, k0), p(yct = 1, rct = l0, zct = k0|y−ct, r−ct, z−ct, w, x) ∝ j=0 exp(λj · xct)(γl0 + n(−ct) exp(λ1 · xct) P1 cl0 ) g(c,t, l0,k0), where z−ct, r−ct, y−ct are z, r, y without tweet ct, mctk0</context>
<context position="25047" citStr="Kim et al., 2013" startWordPosition="4225" endWordPosition="4228">e that a conversation has few topics 6It performs better than other classifiers (C4.5, NaiveBayes, SVM with linear kernel, polynomial kernel and radial basis) and self-disclosure levels, so we set α = γ = 0.1 (Tang et al., 2014). To incorporate the seed words and trigrams into ASUM and SDTM, we initialize βG, βM and βH differently. We assign a high value of 2.0 for each seed word and trigram for that level, and a low value of 10−6 for each word that is a seed word for another level, and a default value of 0.01 for all other words. This approach is the same as previous papers (Jo and Oh, 2011; Kim et al., 2013). As Table 6 shows, SDTM performs better than the other methods for accuracy as well as Fmeasure. LDA and MedLDA generally show the lowest performance, which is not surprising given these models are quite general and not tuned specifically for this type of semi-supervised classification task. BOW which is simple word features also does not perform well, showing especially low F-measure for the H level. LIWC and SEED perform better than LDA, but these have quite low F-measure for G and H levels. ASUM shows better performance for classifying H level than others, confirming the effectiveness of a</context>
</contexts>
<marker>Kim, Zhang, Chen, Oh, Liu, 2013</marker>
<rawString>Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, and Shixia Liu. 2013. A hierarchical aspect-sentiment model for online reviews. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data. biometrics,</title>
<date>1977</date>
<pages>159--174</pages>
<contexts>
<context position="20000" citStr="Landis and Koch, 1977" startWordPosition="3322" endWordPosition="3325">finitions and examples of self-disclosure level shown in (Barak and Gluck-Ofri, 2007), then they worked separately on a Web-based platform. As a result of annotation, there are 122 G level converstaions, 147 M level and 32 H level con3https://dev.twitter.com/docs/api/ streaming 4https://github.com/shuyo/ldig Users 101,686 61,451 1990 Figure 4: Screenshot of annotation web-based platform. Annotators read a Twitter conversation and annotate self-disclosure level to each tweet. versations, and inter-rater agreement using Fleiss kappa (Fleiss, 1971) is 0.68, which is substantial agreement result (Landis and Koch, 1977). 5 Classification of Self-Disclosure Level This section describes experiments and results of SDTM as well as several other methods for classification of self-disclosure level. We first start with the annotated dataset in section 4.2 in which each tweet is annotated with 5D level. We then aggregate all of the tweets of a conversation, and we compute the proportions of tweets in each 5D level. When the proportion of tweets at M or H level is equal to or greater than 0.2, we take the level of the larger proportion and assign that level to the conversation. When the proportions of tweets at M or </context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J Richard Landis and Gary G Koch. 1977. The measurement of observer agreement for categorical data. biometrics, pages 159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew M Ledbetter</author>
<author>Joseph P Mazer</author>
<author>Jocelyn M DeGroot</author>
<author>Kevin R Meyer</author>
<author>Yuping Mao</author>
<author>Brian Swafford</author>
</authors>
<title>Attitudes toward online social connection and self-disclosure as predictors of facebook communication and relational closeness.</title>
<date>2011</date>
<journal>Communication Research,</journal>
<volume>38</volume>
<issue>1</issue>
<marker>Ledbetter, Mazer, DeGroot, Meyer, Mao, Swafford, 2011</marker>
<rawString>Andrew M Ledbetter, Joseph P Mazer, Jocelyn M DeGroot, Kevin R Meyer, Yuping Mao, and Brian Swafford. 2011. Attitudes toward online social connection and self-disclosure as predictors of facebook communication and relational closeness. Communication Research, 38(1):27–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to information retrieval, volume 1.</title>
<date>2008</date>
<publisher>Cambridge University Press Cambridge.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to information retrieval, volume 1. Cambridge University Press Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erika McCallister</author>
</authors>
<title>Guide to protecting the confidentiality ofpersonally identifiable information.</title>
<date>2010</date>
<publisher>DIANE Publishing.</publisher>
<contexts>
<context position="8869" citStr="McCallister, 2010" startWordPosition="1348" endWordPosition="1349">pain, chiropractor, codeine Table 1: High ranked words and expressions by mutual information between G and M/H level in annotated conversations. most highly ranked discriminative features contain a first-person pronoun. 2.3 M Level of Self-Disclosure Utterances with M level include two types: 1) information related with past events and future plans, and 2) general information about self (Barak and Gluck-Ofri, 2007). For the former, we add as seed trigrams ‘I have been’ and ‘I will’. For the latter, we use seven types of information generally accepted to be personally identifiable information (McCallister, 2010), as listed in the left column of Table 2. To find the appropriate trigrams for those, we take Twitter conversation data (described in Section 4.1) and look for trigrams that begin with ‘I’ and ‘my’ and occur more than 200 times. We then check each one to see whether it is related with any of the seven types listed in the table. As a result, we find 57 seed trigrams for M level. Table 2 shows several examples. Table 2: Example seed trigrams for identifying M level of SD. There are 51 of these used in SDTM. 2.4 H Level of Self-Disclosure Utterances with H level express secretive wishes or sensi</context>
</contexts>
<marker>McCallister, 2010</marker>
<rawString>Erika McCallister. 2010. Guide to protecting the confidentiality ofpersonally identifiable information. DIANE Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjun Mukherjee</author>
<author>Vivek Venkataraman</author>
<author>Bing Liu</author>
<author>Sharon Meraz</author>
</authors>
<title>Public dialogue: Analysis of tolerance in online discussions.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="15595" citStr="Mukherjee et al., 2013" startWordPosition="2547" endWordPosition="2550">part classifies G vs. M/H levels with first-person pronouns (I, my, me). In the graphical model, y is the latent variable that represents this classification, and w is the distribution over y. x is the observation of the firstperson pronoun in the tweets, and A are the parameters learned from the maximum entropy classifier. With the annotated Twitter conversation dataset (described in Section 4.2), we experimented with several classifiers (Decision tree, Naive Bayes) and chose the maximum entropy classifier because it performed the best, similar to other joint topic models (Zhao et al., 2010; Mukherjee et al., 2013). Rl IPl Kl 3 Y Ir r 0l 3 W a z N W Y A X T C 1989 Dyads 3.3 Classifying M vs H levels The second part of the classification, the M and the H level, is driven by informative priors with seed words and seed trigrams. In the graphical model, r is the latent variable that represents this classification, and π is the distribution over r. γ is a non-informative prior for π, and βl is an informative prior for each SD level by seed words. For example, we assign a high value for the seed word ‘acne’ for βH, and a low value for ‘My name is’. This approach is the same as joint models of topic and sentim</context>
</contexts>
<marker>Mukherjee, Venkataraman, Liu, Meraz, 2013</marker>
<rawString>Arjun Mukherjee, Vivek Venkataraman, Bing Liu, and Sharon Meraz. 2013. Public dialogue: Analysis of tolerance in online discussions. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Arthur Asuncion</author>
<author>Padhraic Smyth</author>
<author>Max Welling</author>
</authors>
<title>Distributed algorithms for topic models.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>10--1801</pages>
<contexts>
<context position="28343" citStr="Newman et al., 2009" startWordPosition="4762" endWordPosition="4765">uld they display higher 5D in their subsequent conversations? 2. If a dyad displays high 5D level in their conversations at a particular time period, would their subsequent conversations be longer? 3. If a dyad displays high overall 5D level, would their conversations increase in length over time more than dyads with lower overall 5D level? 6.1 Experiment Setup We first run SDTM with all of our Twitter conversation data with 150; 120; 120 topics for SDTM KG, KM and KH respectively. The hyper-parameters are the same as in section 5. To handle a large dataset, we employ a distributed algorithm (Newman et al., 2009), and run with 28 threads. Table 7 shows some of the topics that were prominent in each 5D level by KL-divergence. As expected, G level includes general topics such as food, celebrity, soccer and IT devices, M level includes personal communication and birthday, and finally, H level includes sickness and profanity. We define a new measurement, 5D level score for a dyad in the period, which is a weighted sum of each conversation with 5D levels mapped to 1, 2, and 3, for the levels G, M, and H, respectively. 2.14 2.12 2.10 2.08 2.06 2.04 2.02 2.00 0 5 10 15 20 25 30 35 Initial conversation freque</context>
</contexts>
<marker>Newman, Asuncion, Smyth, Welling, 2009</marker>
<rawString>David Newman, Arthur Asuncion, Padhraic Smyth, and Max Welling. 2009. Distributed algorithms for topic models. Journal of Machine Learning Research, 10:1801–1828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan OConnor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="22341" citStr="Owoputi et al., 2013" startWordPosition="3736" endWordPosition="3739">M with seed words from annotated Tweets (SDTM−): To compare with SDTM below using seed words from SECRET, this uses seed words from the annotated data described in section 2.4. • ASUM (Jo and Oh, 2011): A joint model of sentiments and topics. We map each 5D level to one sentiment and use the same seed words/trigrams from SECRET as in SDTM below. Used in previous work (Bak et al., 2012). • First-person pronouns (FirstP): Occurrence of first-person pronouns which are described in section 3.2. To identify first-person pronouns, we tagged parts of speech in each tweet with the Twitter POS tagger (Owoputi et al., 2013). • First-person pronouns + Seed words/trigrams (FP+SE1): First-person pronouns and seed words/trigrams from SECRET. • Two stage classifier with First-person pronouns + Seed words/trigrams (FP+SE2): A 5personal pronouns, 3rd person singular words, family words, human words, sexual words, etc 1991 Method Acc G F1 M F1 H F1 Avg F1 LDA 49.2 0.00 0.65 0.05 0.23 MedLDA 43.3 0.41 0.52 0.09 0.34 LIWC 49.2 0.34 0.61 0.18 0.38 BOW+ 54.1 0.50 0.59 0.15 0.41 SEED 54.4 0.52 0.60 0.14 0.42 ASUM 56.6 0.32 0.70 0.38 0.47 SDTM− 60.4 0.57 0.70 0.14 0.47 FirstP 63.2 0.63 0.69 0.10 0.47 FP+SE1 61.0 0.61 0.67 0.1</context>
</contexts>
<marker>Owoputi, OConnor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan OConnor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<journal>Found. Trends Inf. Retr.,</journal>
<pages>2--1</pages>
<contexts>
<context position="33577" citStr="Pang and Lee, 2008" startWordPosition="5651" endWordPosition="5654">nsume much time and effort, so they are not suitable for large-scale studies. In prior work closest to ours, Bak et al. (2012) showed that a topic model can be used to identify self-disclosure, but that work applies a two-step process in which a basic topic model is first applied to find the topics, and then the topics are post-processed for binary classification of self-disclosure. We improve upon this work by applying a single unified model of topics and self-disclosure for high accuracy in classifying the three levels of self-disclosure. Subjectivity which is aspect of expressing opinions (Pang and Lee, 2008; Wiebe et al., 2004) is related with self-disclosure, but they are different dimensions of linguistic behavior. Because there indeed are many high self-disclosure tweets that are subjective, but there are also counter examples in annotated dataset. The tweet “England manager is Roy Hodgson.” is low self-disclosure and low subjectivity, “I have barely any hair left.” is high self-disclosure but low subjectivity, and “Senator stop lying!” is low self-disclosure but high subjectivity. 8 Conclusion and Future Work In this paper, we have presented the self-disclosure topic model (SDTM) for discove</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>Bill Dolan</author>
</authors>
<title>Unsupervised modeling of twitter conversations.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="17511" citStr="Ritter et al., 2010" startWordPosition="2932" endWordPosition="2935"> function g(c, t, l0, k0) as follows: g(c, t, l0, k0) = Γ(V l0l0−(ct Pv=1 βv + nk0v + mctk0(·)) Γ(PV v=1 βl0 v + nl0−(ct) k0v ) ! YV v + nl0−(ct) αk0 + nl0(−ct) Γ(βl0 k0v + mctk0v) ck0 PK l0 l0 l0−(ct) k=1 αk + nck v=1 Γ(βv + nk0v ) 4 Data Collection and Annotation To test our self-disclosure topic model, we use a large dataset of conversations consisting of Tweets over three years such that we can analyze the relationship between self-disclosure behavior and conversation frequency and length over time. We chose to crawl Twitter because it offers a practical and large source of conversations (Ritter et al., 2010). Others have also analyzed Twitter conversations for natural language and social media Conv’s Tweets 1,956,993 17,178,638 Table 5: Dataset of Twitter conversations. We chose conversations consisting of five or more tweets each. We chose dyads with twenty or more conversations. research (boyd et al., 2010; Danescu-NiculescuMizil et al., 2011), but we collect conversations from the same set of dyads over several months for a unique longitudinal dataset. We also make sure that each conversation is at least five tweets, and that each dyad has at least twenty conversations. 4.1 Collecting Twitter </context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2010</marker>
<rawString>Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of twitter conversations. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Tang</author>
<author>Zhaoshi Meng</author>
<author>Xuanlong Nguyen</author>
<author>Qiaozhu Mei</author>
<author>Ming Zhang</author>
</authors>
<title>Understanding the limiting factors of topic modeling via posterior contraction analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of The 31st International Conference on Machine Learning,</booktitle>
<pages>190--198</pages>
<contexts>
<context position="24658" citStr="Tang et al., 2014" startWordPosition="4147" endWordPosition="4150">ain/test. We run MedLDA, ASUM and SDTM 20 times each and compute the average accuracies and F-measure for each level. We run LDA and MedLDA with various number of topics from 80 to 140, and 120 topics shows best outputs. So we set 120 topics for LDA, MedLDA and ASUM, 60; 40; 40 topics for SDTM KG, KM and KH respectively which is best perform from 40; 40; 40 to 60; 60; 60 topics. We assume that a conversation has few topics 6It performs better than other classifiers (C4.5, NaiveBayes, SVM with linear kernel, polynomial kernel and radial basis) and self-disclosure levels, so we set α = γ = 0.1 (Tang et al., 2014). To incorporate the seed words and trigrams into ASUM and SDTM, we initialize βG, βM and βH differently. We assign a high value of 2.0 for each seed word and trigram for that level, and a low value of 10−6 for each word that is a seed word for another level, and a default value of 0.01 for all other words. This approach is the same as previous papers (Jo and Oh, 2011; Kim et al., 2013). As Table 6 shows, SDTM performs better than the other methods for accuracy as well as Fmeasure. LDA and MedLDA generally show the lowest performance, which is not surprising given these models are quite genera</context>
</contexts>
<marker>Tang, Meng, Nguyen, Mei, Zhang, 2014</marker>
<rawString>Jian Tang, Zhaoshi Meng, Xuanlong Nguyen, Qiaozhu Mei, and Ming Zhang. 2014. Understanding the limiting factors of topic modeling via posterior contraction analysis. In Proceedings of The 31st International Conference on Machine Learning, pages 190–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yla R Tausczik</author>
<author>James W Pennebaker</author>
</authors>
<title>The psychological meaning of words: Liwc and computerized text analysis methods.</title>
<date>2010</date>
<journal>Journal of Language and Social Psychology.</journal>
<contexts>
<context position="21364" citStr="Tausczik and Pennebaker, 2010" startWordPosition="3567" endWordPosition="3570">ning tweets with H or M level of selfdisclosure usually starts with a greeting or a general comment, and contains one or more questions or comments before or after the self-disclosure tweet. We compare SDTM with the following methods for classifying conversations for 5D level: • LDA (Blei et al., 2003): A Bayesian topic model. Each conversation is treated as a document. Used in previous work (Bak et al., 2012). • MedLDA (Zhu et al., 2012): A supervised topic model for document classification. Each conversation is treated as a document and response variable can be mapped to a 5D level. • LIWC (Tausczik and Pennebaker, 2010): Word counts of particular categories5. Used in previous work (Houghton and Joinson, 2012). • Bag of Words + Bigrams + Trigrams (BOW+): A bag of words, bigram and trigram features. We exclude features that appear only once or twice. • Seed words and trigrams (SEED): Occurrences of seed words/trigrams from SECRET which are described in section 3.3. • SDTM with seed words from annotated Tweets (SDTM−): To compare with SDTM below using seed words from SECRET, this uses seed words from the annotated data described in section 2.4. • ASUM (Jo and Oh, 2011): A joint model of sentiments and topics. W</context>
</contexts>
<marker>Tausczik, Pennebaker, 2010</marker>
<rawString>Yla R Tausczik and James W Pennebaker. 2010. The psychological meaning of words: Liwc and computerized text analysis methods. Journal of Language and Social Psychology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the american statistical association,</journal>
<volume>101</volume>
<issue>476</issue>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. 2006. Hierarchical dirichlet processes. Journal of the american statistical association, 101(476).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Trepte</author>
<author>Leonard Reinecke</author>
</authors>
<title>The reciprocal effects of social network site use and the disposition for self-disclosure: A longitudinal study.</title>
<date>2013</date>
<journal>Computers in Human Behavior,</journal>
<volume>29</volume>
<issue>3</issue>
<pages>1112</pages>
<contexts>
<context position="32860" citStr="Trepte and Reinecke, 2013" startWordPosition="5529" endWordPosition="5532">ime. We divide dyads into three groups by SD level score as low, medium, and high. Conversation length noticeably increases over time in the medium and high groups, but only slight in the low group. served in figure 6, but here we can see it as a long-term persistent pattern. Second, conversation length increases consistently and significantly for the high and medium groups, but for the low 5D group, there is not a significant increase of conversation length over time. 7 Related Work Prior work on quantitatively analyzing selfdisclosure has relied on user surveys (Ledbetter et 1994 al., 2011; Trepte and Reinecke, 2013) or human annotation (Barak and Gluck-Ofri, 2007; Courtney Walton and Rice, 2013). These methods consume much time and effort, so they are not suitable for large-scale studies. In prior work closest to ours, Bak et al. (2012) showed that a topic model can be used to identify self-disclosure, but that work applies a two-step process in which a basic topic model is first applied to find the topics, and then the topics are post-processed for binary classification of self-disclosure. We improve upon this work by applying a single unified model of topics and self-disclosure for high accuracy in cla</context>
</contexts>
<marker>Trepte, Reinecke, 2013</marker>
<rawString>Sabine Trepte and Leonard Reinecke. 2013. The reciprocal effects of social network site use and the disposition for self-disclosure: A longitudinal study. Computers in Human Behavior, 29(3):1102 – 1112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah I Vondracek</author>
<author>Fred W Vondracek</author>
</authors>
<title>The manipulation and measurement of self-disclosure in preadolescents.</title>
<date>1971</date>
<booktitle>Merrill-Palmer Quarterly of Behavior and Development,</booktitle>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="5554" citStr="Vondracek and Vondracek, 1971" startWordPosition="807" endWordPosition="811">self-disclosure in a large corpus of OSN conversations. We discuss three approaches, first, using first-person pronoun features, and second, extracting seed words and phrases from the Twitter conversation corpus, and third, extracting seed words and phrases from an external corpus of anonymously posted secrets, and we demonstrate the efficacy of those approaches with an annotated corpus. 2.1 Self-disclosure (SD) level To analyze self-disclosure, researchers categorize self-disclosure language into three levels: G (general) for no disclosure, M for medium disclosure, and H for high disclosure (Vondracek and Vondracek, 1971; Barak and Gluck-Ofri, 2007). Utterances that contain general (non-sensitive) information about the self or someone close (e.g., a family member) are categorized as M. Examples are personal events, past history, or future plans. Utterances about age, occupation and hobbies are also included. Utterances that contain sensitive information about the self or someone close are categorized as H. Sensitive information includes personal characteristics, problematic behaviors, physical appearance and wishful ideas. Generally, these are thoughts and information that (f) A M level Twitter conversation (</context>
</contexts>
<marker>Vondracek, Vondracek, 1971</marker>
<rawString>Sarah I Vondracek and Fred W Vondracek. 1971. The manipulation and measurement of self-disclosure in preadolescents. Merrill-Palmer Quarterly of Behavior and Development, 17(1):51–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Rebecca Bruce</author>
<author>Matthew Bell</author>
<author>Melanie Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>Computational linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="33598" citStr="Wiebe et al., 2004" startWordPosition="5655" endWordPosition="5658">effort, so they are not suitable for large-scale studies. In prior work closest to ours, Bak et al. (2012) showed that a topic model can be used to identify self-disclosure, but that work applies a two-step process in which a basic topic model is first applied to find the topics, and then the topics are post-processed for binary classification of self-disclosure. We improve upon this work by applying a single unified model of topics and self-disclosure for high accuracy in classifying the three levels of self-disclosure. Subjectivity which is aspect of expressing opinions (Pang and Lee, 2008; Wiebe et al., 2004) is related with self-disclosure, but they are different dimensions of linguistic behavior. Because there indeed are many high self-disclosure tweets that are subjective, but there are also counter examples in annotated dataset. The tweet “England manager is Roy Hodgson.” is low self-disclosure and low subjectivity, “I have barely any hair left.” is high self-disclosure but low subjectivity, and “Senator stop lying!” is low self-disclosure but high subjectivity. 8 Conclusion and Future Work In this paper, we have presented the self-disclosure topic model (SDTM) for discovering topics and class</context>
</contexts>
<marker>Wiebe, Wilson, Bruce, Bell, Martin, 2004</marker>
<rawString>Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and Melanie Martin. 2004. Learning subjective language. Computational linguistics, 30(3):277–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne Xin Zhao</author>
<author>Jing Jiang</author>
<author>Hongfei Yan</author>
<author>Xiaoming Li</author>
</authors>
<title>Jointly modeling aspects and opinions with a maxent-lda hybrid.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="15570" citStr="Zhao et al., 2010" startWordPosition="2543" endWordPosition="2546">rts, and the first part classifies G vs. M/H levels with first-person pronouns (I, my, me). In the graphical model, y is the latent variable that represents this classification, and w is the distribution over y. x is the observation of the firstperson pronoun in the tweets, and A are the parameters learned from the maximum entropy classifier. With the annotated Twitter conversation dataset (described in Section 4.2), we experimented with several classifiers (Decision tree, Naive Bayes) and chose the maximum entropy classifier because it performed the best, similar to other joint topic models (Zhao et al., 2010; Mukherjee et al., 2013). Rl IPl Kl 3 Y Ir r 0l 3 W a z N W Y A X T C 1989 Dyads 3.3 Classifying M vs H levels The second part of the classification, the M and the H level, is driven by informative priors with seed words and seed trigrams. In the graphical model, r is the latent variable that represents this classification, and π is the distribution over r. γ is a non-informative prior for π, and βl is an informative prior for each SD level by seed words. For example, we assign a high value for the seed word ‘acne’ for βH, and a low value for ‘My name is’. This approach is the same as joint m</context>
</contexts>
<marker>Zhao, Jiang, Yan, Li, 2010</marker>
<rawString>Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming Li. 2010. Jointly modeling aspects and opinions with a maxent-lda hybrid. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhu</author>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>Medlda: maximum margin supervised topic models.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>13--2237</pages>
<contexts>
<context position="21176" citStr="Zhu et al., 2012" startWordPosition="3534" endWordPosition="3537">. When the proportions of tweets at M or H level are both less than 0.2, we assign G to the 5D level. The reason for setting 0.2 as the threshold is that a conversation containing tweets with H or M level of selfdisclosure usually starts with a greeting or a general comment, and contains one or more questions or comments before or after the self-disclosure tweet. We compare SDTM with the following methods for classifying conversations for 5D level: • LDA (Blei et al., 2003): A Bayesian topic model. Each conversation is treated as a document. Used in previous work (Bak et al., 2012). • MedLDA (Zhu et al., 2012): A supervised topic model for document classification. Each conversation is treated as a document and response variable can be mapped to a 5D level. • LIWC (Tausczik and Pennebaker, 2010): Word counts of particular categories5. Used in previous work (Houghton and Joinson, 2012). • Bag of Words + Bigrams + Trigrams (BOW+): A bag of words, bigram and trigram features. We exclude features that appear only once or twice. • Seed words and trigrams (SEED): Occurrences of seed words/trigrams from SECRET which are described in section 3.3. • SDTM with seed words from annotated Tweets (SDTM−): To comp</context>
</contexts>
<marker>Zhu, Ahmed, Xing, 2012</marker>
<rawString>Jun Zhu, Amr Ahmed, and Eric P Xing. 2012. Medlda: maximum margin supervised topic models. Journal of Machine Learning Research, 13:2237–2278.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014629">
<title confidence="0.878259">
Verifiably Effective Arabic Dialect Identification
</title>
<author confidence="0.991118">
Kareem Darwish, Hassan Sajjad, Hamdy Mubarak
</author>
<affiliation confidence="0.7806945">
Qatar Computing Research Institute
Qatar Foundation
</affiliation>
<email confidence="0.980544">
fkdarwish,hsajjad,hmubarakl@qf.org.qa
</email>
<sectionHeader confidence="0.994277" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999055">
Several recent papers on Arabic dialect identi-
fication have hinted that using a word unigram
model is sufficient and effective for the task.
However, most previous work was done on a
standard fairly homogeneous dataset of dialec-
tal user comments. In this paper, we show
that training on the standard dataset does not
generalize, because a unigram model may be
tuned to topics in the comments and does not
capture the distinguishing features of dialects.
We show that effective dialect identification
requires that we account for the distinguishing
lexical, morphological, and phonological phe-
nomena of dialects. We show that accounting
for such can improve dialect detection accu-
racy by nearly 10% absolute.
</bodyText>
<sectionHeader confidence="0.998372" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999705113207547">
Modern Standard Arabic (MSA) is the lingua franca
of the so-called Arab world, which includes north-
ern Africa, the Arabian Peninsula, and Mesopotamia.
However, Arabic speakers generally use dramatically
different languages (or dialects) in daily interactions
and in social media. These dialects may differ in vocab-
ulary, morphology, and spelling from MSA and most
do not have standard spellings. There is often large
lexical overlap between dialects and MSA. Performing
proper Arabic dialect identification may positively im-
pact many Natural Language Processing (NLP) appli-
cation. For example, transcribing dialectal speech or
automatically translating into a particular dialect would
be aided by the use of targeted language models that are
trained on texts in that dialect.
This has led to recent interest in automatic identifi-
cation of different Arabic dialects (Elfardy et al., 2013;
Cotterell et al., 2014; Zaidan et al., 2014). Though pre-
vious work (Cotterell et al., 2014) have reported high
accuracies for dialect identification using word uni-
gram model, which implies that this is a solved prob-
lem, we argue that the problem is far from being solved.
The reason for this assertion stems from the fact that the
available dialectal data is drawn from singular sources,
namely online news sites, for each dialect. This is prob-
lematic because comments on singular news site are
likely to have some homogeneity in topics and jargon.
Such homogeneity has caused fairly simple classifica-
tion techniques that use word unigrams and character n-
grams to yield very high identification accuracies. Per-
haps, this can be attributed to topical similarity and not
just differences between dialects. To showcase this, we
trained a classifier using the best reported methods, and
we tested the classifier on a new test set of 700 tweets,
with dialectal Egyptian (ARZ) and MSA tweets, which
led to a low accuracy of 83.3%. We also sorted words
in the ARZ part from our training dataset by how much
they discriminate between ARZ and MSA (using mu-
tual information) and indeed many of the top words
were in fact MSA words.
There seems to be a necessity to identify lexical and
linguistic features that discriminate between MSA and
different dialects. In this paper, we highlight some
such features that help in separating between MSA
and ARZ. We identify common ARZ words that do
not overlap with MSA and identify specific linguistic
phenomena that exist in ARZ, and not MSA, such as
morphological patterns, word concatenations, and verb
negation constructs (Section 3). We also devise meth-
ods for capturing the linguistic phenomena, and we use
the appearance of such phenomena as features (Sec-
tion 4). Further, we show the positive impact of using
the new features in identifying ARZ (Section 5).
</bodyText>
<sectionHeader confidence="0.993131" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999789052631579">
Previous work on Arabic dialect identification uses n-
gram based features at both word-level and character-
level to identify dialectal sentences (Elfardy et al.,
2013; Cotterell et al., 2014; Zaidan et al., 2011; Zaidan
et al., 2014). Zaidan et al. (2011) created a dataset of
dialectal Arabic. They performed cross-validation ex-
periments for dialect identification using word n-gram
based features. Elfardy et al. (2013) built a system to
distinguish between ARZ and MSA. They used word
n-gram features combined with core (token-based and
perplexity-based features) and meta features for train-
ing. Their system showed a 5% improvement over
the system of Zaidan et al. (2011). Later, Zaidan et
al. (2014) used several word n-gram based and char-
acter n-gram based features for dialect identification.
The system trained on word unigram-based feature per-
formed the best with character five-gram-based feature
being second best. A similar result is shown by Cot-
terell et al. (2014) where word unigram model performs
</bodyText>
<page confidence="0.97327">
1465
</page>
<bodyText confidence="0.98680996">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1465–1468,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
the best.
All of the previous work except Cotterell et al.
(2014)1 evaluate their systems using cross-validation.
These models heavily rely on the coverage of training
data to achieve better identification. This limits the ro-
bustness of identification to genres inline with the train-
ing data.
Language identification is a related area to dialect
identification. It has raised some of the issues which we
discussed in this paper in the context of dialect identi-
fication. Lui et al. (2011) showed that in-domain lan-
guage identification performs better than cross domain
language identification. Tiedemann et al. (2012) argued
that the linguistic understanding of the differences be-
tween languages can lead to a better language identi-
fication system. kilgarriff (2001) discussed the differ-
ences between datasets as a poor representation of dif-
ferences between dialects of English.
In this paper, we exploit the linguistic phenomena
that are specific to Arabic dialects to show that they
produce significant improvements in accuracy. We
show that this also helps in achieving high quality
cross-domain dialect identification system.
</bodyText>
<sectionHeader confidence="0.987228" genericHeader="method">
3 Dialectal Egyptian Phenomena
</sectionHeader>
<bodyText confidence="0.995981857142857">
There are several phenomena in ARZ that set it apart
from MSA. Some of them are as follows:
Dialectal words: ARZ uses unique words that do
not overlap with MSA and may not overlap with other
dialects. Some of the common ARZ words are: “zy”
(like), “kdh” (like this), and “Azyk” (how are you) 2.
These dialectal terms stem from the following:
</bodyText>
<listItem confidence="0.998953956521739">
• Using proper Arabic words that are rarely used in
MSA such as “$nTp” (bag) and “n$wf” (we see).
• Fusing multiple words together by concatenating and
dropping letters such as the word “mEl$” (no worry),
which is a fusion of “mA Elyh $y’ ”.
• Using non-standard spelling of words such as
“SAbE” (finger) instead of “&lt;sbE” in MSA. Conse-
quently, broken plurals may also be non-standard.
• using non-Arabic words such as “&lt;y$Arb” (scarf),
which is transliterated from the French ´echarpe.
• altering the forms of some pronouns such as the fem-
inine second person pronoun from “k” to “ky”, the sec-
ond person plural pronoun “tm” to “tw”, and the object
pronoun “km” to “kw”.
Morphological differences: ARZ makes use of par-
ticular morphological patterns that do not exist in MSA
and often alters some morphological constructs. Some
examples include:
• Adding the letter “b” in front of verb in present tense.
Ex. MSA: “ylEb” (he plays) → EG: “bylEb”.
• Using the letters “H” or “h”, instead of “s”, to indi-
cate future tense. Ex. MSA: “sylEb” (he will play) →
EG: “hylEb” or “HylEb”.
</listItem>
<footnote confidence="0.998488">
1Zaidan et al. (2014) applied their classifier to a different
genre but did not evaluate it’s performance.
2Buckwalter encoding is used throughout the paper.
</footnote>
<listItem confidence="0.987686476190476">
• Adding the letters “At” to passive past tense verbs.
Ex. MSA: “luEiba” (was played) → “AtlaEab”.
• Adding the letters “m” or “mA” before the verb and
“$” or “$y” after the verb to express negation. Ex.
MSA: “lm ylEb” (he did not play) → “mlEb$”.
• the merging of verbs and prepositional phrases of the
form (to-pronoun) that follow it. Ex. MSA: “ylEb lh”
(he plays for/to him) → “bylEblh”.
• Replacing a short vowel with a long vowel in im-
perative verbs that are derived from hollow roots. Ex.
MSA: “qul” (say) → “qwl”.
Letter substitution: in ARZ the following letter
substitutions are common:
• “v” → “t”. Ex. MSA: “kvyr” (a lot) → EG: “ktyr”.
• “}” → “y”. Ex. MSA: “b}r” (well) → “byr”.
• Trailing “y” → “Y”. Ex. MSA: “Hqy” (my right) →
“HqY”.
• “*” → “d”. Ex. MSA: “xu*” (take) → “xud”.
• middle or trailing “&gt;” → “A”. Ex. MSA: “f&gt;r”
(mouse) → “fAr”.
• “D” → “Z”. Ex. MSA: “DAbT” (officer) → “ZAbT”.
• “Z” → “D”. Ex. MSA: “Zhr” (back) → “Dhr”.
• Middle “|” → “yA”. Ex. MSA: “ml|n” (full) →
“mlyAn”.
• Removal of trailing “ ’ ”. Ex. MSA: “AlsmA’ ” (the
sky) → “AlsmA”.
Syntactic differences: some of the following phe-
nomena are generally observed:
• Common use of masculine plural or singular noun
forms instead dual and feminine plural. Ex. MSA “jny-
hyn” (two pounds) → EG: “Atnyn jnyh”.
• Dropping some articles and preposition in some syn-
tactic constructs. For example, the preposition “&lt;lY”
(to) in “&gt;nA rAyH &lt;lY Al$gl” (I am going to work)
is typically dropped. Also, the particle “&gt;n” (to) is
dropped in the sentence “&gt;nA mHtAj &gt;n &gt;nAm” (I
need to sleep).
• Using only one form of noun and verb suffixes such
as “yn” instead of “wn” and “wA” instead of “wn” re-
spectively. Also, so-called “five nouns”, are used in
only one form (ex. “&gt;bw” (father of) instead of “&gt;bA”
or “&gt;by”).
</listItem>
<sectionHeader confidence="0.949476" genericHeader="method">
4 Detecting Dialectal Peculiarities
</sectionHeader>
<bodyText confidence="0.999959142857143">
ARZ is different from MSA lexically, morphologically,
phonetically, and syntactically. Here, we present meth-
ods to handle such peculiarities. We chose not to han-
dle syntactic differences, because they may be captured
using word n-gram models.
To capture lexical variations, we extracted and sorted
by frequency all the unigrams from the Egyptian side of
the LDC2012T09 corpus (Zbib et al., 2012), which has
≈ 38k Egyptian-English parallel sentences. A linguist
was tasked with manually reviewing the words from the
top until 1,300 dialectal words were found. Some of the
words on the list included dialectal words, commonly
used foreign words, words that exhibit morphological
variations, and others with letter substitution.
</bodyText>
<page confidence="0.983312">
1466
</page>
<bodyText confidence="0.8925275">
For morphological phenomenon, we employed three
methods, namely:
</bodyText>
<listItem confidence="0.997891642857143">
• Unsupervised Morphology Induction: We em-
ployed the unsupervised morpheme segmentation tool,
Morfessor (Virpioja et al., 2013). It is a data driven
tool that automatically learns morphemes from data in
an unsupervised fashion. We used the trained model to
segment the training and test sets.
• Morphological Rules: In contrast to Morfessor, we
developed only 15 morphological rules (based on the
analysis proposed in Section 3) to segment ARZ text.
These rules would separate prefixes and suffixes like a
light stemmer. Example rules would segment a leading
“b” and segment a combination of a leading “m” and
trailing “$”.
• Morphological Generator: For morphological gen-
eration, we enumerated a list of ,: 200 morphological
patterns that derive dialectal verbs from Arabic roots.
One such pattern is ytCCC that would generate the di-
alectal verb-form ytktb (to be written) from the root
“ktb”. We used the root list that is distributed with Se-
bawai (Darwish, 2002). We also expanded the list by
attaching negation affixes and pronouns. We retained
generated word forms that: a) exist in a large corpus of
63 million Arabic tweets from 2012 with more than 1
billion tokens; and b) don’t appear in a large MSA cor-
pus of 10 years worth of Aljazeera articles containing
114 million tokens 3. The resulting list included 94k
verb surface forms such as “mbyEmlhA$” (he does not
do it).
</listItem>
<bodyText confidence="0.999693416666667">
For phonological differences, we used a morpholog-
ical generator that makes use of the aforementioned
root list and an inventory of ,: 605 morphological pat-
terns (with diacritization) to generate possible Arabic
stems. The generated stems with their diacritics were
checked against a large diacritized Arabic corpus con-
taining more than 200 million diacritized words 4. If
generated words contained the letters “v”, “}”, “*”, and
“D”, we used the aforementioned letter substitutions.
We retained words that exist in the large tweet corpus
but not in the Aljazeera corpus. The list contained 8k
surface forms.
</bodyText>
<sectionHeader confidence="0.993447" genericHeader="evaluation">
5 Evaluation Setup
</sectionHeader>
<bodyText confidence="0.99813925">
Dataset: We performed dialect identification exper-
iment for ARZ and MSA. For ARZ, we used the
Egyptian side of the LDC2012T09 corpus (Zbib et
al., 2012) 5. For MSA, we used the Arabic side
of the English/Arabic parallel corpus from the Inter-
national Workshop on Arabic Language Translation6
which consists of ,: 150k sentences. For testing, we
constructed an evaluation set that is markedly different
</bodyText>
<footnote confidence="0.913969666666667">
3http://aljazeera.net
4http://www.sh.rewayat2.com
5We did not use the Arabic Online Commentary data
(Zaidan et al., 2011) as annotations were often not reliable.
6https://wit3.fbk.eu/mt.php?release=
2013-01
</footnote>
<bodyText confidence="0.999824055555556">
from the training set. We crawled Arabic tweets from
Twitter during March 2014 and selected those where
user location was set to Egypt or a geographic location
within Egypt, leading to 880k tweets. We randomly
selected 2k tweets, and we manually annotated them
as ARZ, MSA, or neither until we obtained 350 ARZ
and 350 MSA tweets. We used these tweets for testing.
We plan to release the tweet ID’s and our annotations.
We preprocessed the training and test sets using the
method described by Darwish et al. (2012), which in-
cludes performing letter and word normalizations, and
segmented all data using an open-source MSA word
segmentor (Darwish et al., 2012). We also removed
punctuations, hashtags, and name mentions from the
test set. We used a Random Forest (RF) ensemble clas-
sifier that generates many decision trees, each of which
is trained on a subset of the features.7 We used the RF
implementation in Weka (Breiman, 2001).
</bodyText>
<subsectionHeader confidence="0.994913">
5.1 Classification Runs
</subsectionHeader>
<bodyText confidence="0.999959121212121">
Baseline BL: In our baseline experiments, we used
word unigram, bigram, and trigram models and charac-
ter unigram to 5-gram models as features. We first per-
formed a cross-validation experiment using ARZ and
MSA training sets. The classifier achieved fairly high
results (+95%) which are much higher than the results
mentioned in the literature. This could be due in part
to the fact that we were doing ARZ-MSA classification
instead of multi-dialect classification and MSA data is
fairly different in genre from ARZ data. We did not fur-
ther discuss these results. This experiment was a sanity
check to see how does in-domain dialect identification
perform.
Later, we trained the RF classifier on the complete
training set using word n-gram features (WRD), char-
acter n-gram features (CHAR), or both (BOTH) and
tested it on the tweets test set. We referred to this sys-
tem as BL later on.
Dialectal Egyptian Lexicon Slex: As mentioned ear-
lier, we constructed three word lists containing 1,300
manually reviewed ARZ words (MAN), 94k dialectal
verbs (VERB), and 8k words with letter substitutions
(SUBT). Using the lists, we counted the number of
words in a tweet that exist in the word lists and used it
as a standalone feature for classifications. LEX refers
to concatenation of all three lists.
Morphological Features: For Smrph, we trained Mor-
fessor separately on the MSA and Egyptian training
data and applied to the same training data for segmen-
tation. For Srule, we segmented Egyptian part of the
training data using the morphological rules mentioned
in Section 4. For both, word and character n-gram fea-
tures were calculated from the segmented data and the
</bodyText>
<footnote confidence="0.6365452">
7We tried also the multi-class Bayesian classifier and
SVM classifier. SVM classifier had comparable results with
Random Forest classifier. However, it was very slow. So, we
decided to go with Random Forest classifier for the rest of the
experiments.
</footnote>
<page confidence="0.959558">
1467
</page>
<table confidence="0.99865425">
SYS WRD CHR BOTH BEST+LEX
BL 53.0 74.0 83.3 84.7
Smrph 72.0 88.0 62.1 89.3
Srule 53.9 85.9 85.9 90.1
</table>
<tableCaption confidence="0.8392922">
Table 1: Dialect identification accuracy using
various classification settings: only word-based
(WRD), character-based (CHAR), and both features.
BEST+LEX is built on the best feature of that system
plus a feature built on the concatenation of all lists
</tableCaption>
<table confidence="0.817309">
SYS MAN +VERB +SUBT
Slex 93.6 94.6 94.4
</table>
<tableCaption confidence="0.9238415">
Table 2: Accuracy of the dialect identification system
with the addition of various types of lexicon
</tableCaption>
<bodyText confidence="0.6620925">
classifier was trained on them and tested on the tweet
test set.
</bodyText>
<subsectionHeader confidence="0.59524">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999979565217391">
Table 1 summarizes the results. Unlike results in the lit-
erature, character-based n-gram features outperformed
word-based n-gram features, as they seemed to better
generalize to the new test set, where lexical overlap be-
tween the training and test sets was low. Except for
Smrph, adding both character and word n-gram fea-
tures led to improved results. We observed that Mor-
fessor over-segmented the text, which in turns created
small character segments and enabled the character-
based language model to learn the phenomenon inherit
in a word. The baseline system achieved an accuracy
of 84.7% when combined with the Slex feature. Com-
bining Smrph and Srule features with the Slex feature
led to further improvement. However, as shown in Ta-
ble 2, using the Slex feature alone with the MAN and
VERB lists led to the best results (94.6%), outperform-
ing using all other features either alone or in combina-
tion. This suggests that having a clean list of dialectal
words that cover common dialectal phenomena is more
effective than using word and character n-grams. It also
highlights the shortcomings of using a homogeneous
training set where word unigrams could be capturing
topical cues along with dialectal ones.
</bodyText>
<sectionHeader confidence="0.999004" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999388">
In this paper, we identified lexical, morphological,
phonological, and syntactic features that help distin-
guish between dialectal Egyptian and MSA. Given the
substantial lexical overlap between dialectal Egyptian
and MSA, targeting words that exhibit distinguishing
traits is essential to proper dialect identification. We
used some of these features for dialect detection lead-
ing to nearly 10% (absolute) improvement in classifi-
cation accuracy. We plan to extend our work to other
dialects.
</bodyText>
<sectionHeader confidence="0.994267" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999618139534884">
Leo Breiman. 2001. Random Forests. Machine Learn-
ing. 45(1):5-32.
Ryan Cotterell, Chris Callison-Burch. 2014. A Multi-
Dialect, Multi-Genre Corpus of Informal Written
Arabic. LREC-2014, pages 241–245.
Kareem Darwish. 2002. Building a shallow morpho-
logical analyzer in one day. In Proceedings of the
ACL-2002 Workshop on Computational Approaches
to Semitic Languages.
Kareem Darwish, Walid Magdy, Ahmed Mourad.
2012. Language Processing for Arabic Microblog
Retrieval. CIKM-2012, pages 2427–2430.
Kareem Darwish, Ahmed Abdelali, Hamdy Mubarak.
2014. Using Stem-Templates to improve Arabic POS
and Gender/Number Tagging. LREC-2014.
Heba Elfardy, Mona Diab. 2013. Sentence Level Di-
alect Identification in Arabic. ACL-2013, pages
456–461.
Sami Virpioja, Peter Smit, Stig-Arne Grnroos, and
Mikko Kurimo. 2013. Morfessor 2.0: Python Im-
plementation and Extensions for Morfessor Base-
line. Aalto University publication series SCI-
ENCE + TECHNOLOGY, 25/2013. Aalto Univer-
sity, Helsinki, 2013.
Omar F. Zaidan, Chris Callison-Burch. 2011. The Ara-
bic Online Commentary Dataset: An Annotated
Dataset of Informal Arabic with High Dialectal Con-
tent. ACL-11, pages 37–41.
Omar F. Zaidan, Chris Callison-Burch. 2014. Arabic
Dialect Identification. CL-11, 52(1).
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz,
John Makhoul, Omar F. Zaidan, Chris Callison-
Burch. 2012. Machine translation of Arabic dialects.
NAACL-2012, pages 49–59.
Marco Lui and Timothy Baldwin. 2011. Cross-
domain feature selection for language identification.
IJCNLP-2011, page 553–561.
J¨org Tiedemann and Nikola Ljubesic. 2012. Efficient
discrimination between closely related languages.
COLING-2012, 2619–2634.
Adam Kilgarriff. 2001. Comparing corpora. CL-01,
6(1).
</reference>
<page confidence="0.994046">
1468
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.500739">
<title confidence="0.999846">Verifiably Effective Arabic Dialect Identification</title>
<author confidence="0.998085">Kareem Darwish</author>
<author confidence="0.998085">Hassan Sajjad</author>
<author confidence="0.998085">Hamdy</author>
<affiliation confidence="0.991686">Qatar Computing Research</affiliation>
<address confidence="0.583259">Qatar</address>
<abstract confidence="0.991545176470588">Several recent papers on Arabic dialect identification have hinted that using a word unigram model is sufficient and effective for the task. However, most previous work was done on a standard fairly homogeneous dataset of dialectal user comments. In this paper, we show that training on the standard dataset does not generalize, because a unigram model may be tuned to topics in the comments and does not capture the distinguishing features of dialects. We show that effective dialect identification requires that we account for the distinguishing lexical, morphological, and phonological phenomena of dialects. We show that accounting for such can improve dialect detection accuracy by nearly 10% absolute.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random Forests.</title>
<date>2001</date>
<journal>Machine Learning.</journal>
<pages>45--1</pages>
<contexts>
<context position="13836" citStr="Breiman, 2001" startWordPosition="2250" endWordPosition="2251">0 MSA tweets. We used these tweets for testing. We plan to release the tweet ID’s and our annotations. We preprocessed the training and test sets using the method described by Darwish et al. (2012), which includes performing letter and word normalizations, and segmented all data using an open-source MSA word segmentor (Darwish et al., 2012). We also removed punctuations, hashtags, and name mentions from the test set. We used a Random Forest (RF) ensemble classifier that generates many decision trees, each of which is trained on a subset of the features.7 We used the RF implementation in Weka (Breiman, 2001). 5.1 Classification Runs Baseline BL: In our baseline experiments, we used word unigram, bigram, and trigram models and character unigram to 5-gram models as features. We first performed a cross-validation experiment using ARZ and MSA training sets. The classifier achieved fairly high results (+95%) which are much higher than the results mentioned in the literature. This could be due in part to the fact that we were doing ARZ-MSA classification instead of multi-dialect classification and MSA data is fairly different in genre from ARZ data. We did not further discuss these results. This experi</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. 2001. Random Forests. Machine Learning. 45(1):5-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Cotterell</author>
<author>Chris Callison-Burch</author>
</authors>
<title>A MultiDialect, Multi-Genre Corpus of Informal Written Arabic.</title>
<date>2014</date>
<booktitle>LREC-2014,</booktitle>
<pages>241--245</pages>
<marker>Cotterell, Callison-Burch, 2014</marker>
<rawString>Ryan Cotterell, Chris Callison-Burch. 2014. A MultiDialect, Multi-Genre Corpus of Informal Written Arabic. LREC-2014, pages 241–245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kareem Darwish</author>
</authors>
<title>Building a shallow morphological analyzer in one day.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-2002 Workshop on Computational Approaches to Semitic Languages.</booktitle>
<contexts>
<context position="11254" citStr="Darwish, 2002" startWordPosition="1833" endWordPosition="1834">ped only 15 morphological rules (based on the analysis proposed in Section 3) to segment ARZ text. These rules would separate prefixes and suffixes like a light stemmer. Example rules would segment a leading “b” and segment a combination of a leading “m” and trailing “$”. • Morphological Generator: For morphological generation, we enumerated a list of ,: 200 morphological patterns that derive dialectal verbs from Arabic roots. One such pattern is ytCCC that would generate the dialectal verb-form ytktb (to be written) from the root “ktb”. We used the root list that is distributed with Sebawai (Darwish, 2002). We also expanded the list by attaching negation affixes and pronouns. We retained generated word forms that: a) exist in a large corpus of 63 million Arabic tweets from 2012 with more than 1 billion tokens; and b) don’t appear in a large MSA corpus of 10 years worth of Aljazeera articles containing 114 million tokens 3. The resulting list included 94k verb surface forms such as “mbyEmlhA$” (he does not do it). For phonological differences, we used a morphological generator that makes use of the aforementioned root list and an inventory of ,: 605 morphological patterns (with diacritization) t</context>
</contexts>
<marker>Darwish, 2002</marker>
<rawString>Kareem Darwish. 2002. Building a shallow morphological analyzer in one day. In Proceedings of the ACL-2002 Workshop on Computational Approaches to Semitic Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kareem Darwish</author>
<author>Walid Magdy</author>
<author>Ahmed Mourad</author>
</authors>
<title>Language Processing for Arabic Microblog Retrieval.</title>
<date>2012</date>
<booktitle>CIKM-2012,</booktitle>
<pages>2427--2430</pages>
<contexts>
<context position="13419" citStr="Darwish et al. (2012)" startWordPosition="2180" endWordPosition="2183">(Zaidan et al., 2011) as annotations were often not reliable. 6https://wit3.fbk.eu/mt.php?release= 2013-01 from the training set. We crawled Arabic tweets from Twitter during March 2014 and selected those where user location was set to Egypt or a geographic location within Egypt, leading to 880k tweets. We randomly selected 2k tweets, and we manually annotated them as ARZ, MSA, or neither until we obtained 350 ARZ and 350 MSA tweets. We used these tweets for testing. We plan to release the tweet ID’s and our annotations. We preprocessed the training and test sets using the method described by Darwish et al. (2012), which includes performing letter and word normalizations, and segmented all data using an open-source MSA word segmentor (Darwish et al., 2012). We also removed punctuations, hashtags, and name mentions from the test set. We used a Random Forest (RF) ensemble classifier that generates many decision trees, each of which is trained on a subset of the features.7 We used the RF implementation in Weka (Breiman, 2001). 5.1 Classification Runs Baseline BL: In our baseline experiments, we used word unigram, bigram, and trigram models and character unigram to 5-gram models as features. We first perfo</context>
</contexts>
<marker>Darwish, Magdy, Mourad, 2012</marker>
<rawString>Kareem Darwish, Walid Magdy, Ahmed Mourad. 2012. Language Processing for Arabic Microblog Retrieval. CIKM-2012, pages 2427–2430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kareem Darwish</author>
</authors>
<title>Ahmed Abdelali, Hamdy Mubarak.</title>
<date>2014</date>
<booktitle>Using Stem-Templates to improve Arabic POS and Gender/Number Tagging. LREC-2014.</booktitle>
<marker>Darwish, 2014</marker>
<rawString>Kareem Darwish, Ahmed Abdelali, Hamdy Mubarak. 2014. Using Stem-Templates to improve Arabic POS and Gender/Number Tagging. LREC-2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heba Elfardy</author>
<author>Mona Diab</author>
</authors>
<date>2013</date>
<booktitle>Sentence Level Dialect Identification in Arabic. ACL-2013,</booktitle>
<pages>456--461</pages>
<marker>Elfardy, Diab, 2013</marker>
<rawString>Heba Elfardy, Mona Diab. 2013. Sentence Level Dialect Identification in Arabic. ACL-2013, pages 456–461.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sami Virpioja</author>
<author>Peter Smit</author>
<author>Stig-Arne Grnroos</author>
<author>Mikko Kurimo</author>
</authors>
<title>Morfessor 2.0: Python Implementation and Extensions for Morfessor Baseline. Aalto</title>
<date>2013</date>
<booktitle>University publication series SCIENCE + TECHNOLOGY,</booktitle>
<pages>25--2013</pages>
<institution>Aalto University,</institution>
<location>Helsinki,</location>
<contexts>
<context position="10416" citStr="Virpioja et al., 2013" startWordPosition="1693" endWordPosition="1696">frequency all the unigrams from the Egyptian side of the LDC2012T09 corpus (Zbib et al., 2012), which has ≈ 38k Egyptian-English parallel sentences. A linguist was tasked with manually reviewing the words from the top until 1,300 dialectal words were found. Some of the words on the list included dialectal words, commonly used foreign words, words that exhibit morphological variations, and others with letter substitution. 1466 For morphological phenomenon, we employed three methods, namely: • Unsupervised Morphology Induction: We employed the unsupervised morpheme segmentation tool, Morfessor (Virpioja et al., 2013). It is a data driven tool that automatically learns morphemes from data in an unsupervised fashion. We used the trained model to segment the training and test sets. • Morphological Rules: In contrast to Morfessor, we developed only 15 morphological rules (based on the analysis proposed in Section 3) to segment ARZ text. These rules would separate prefixes and suffixes like a light stemmer. Example rules would segment a leading “b” and segment a combination of a leading “m” and trailing “$”. • Morphological Generator: For morphological generation, we enumerated a list of ,: 200 morphological p</context>
</contexts>
<marker>Virpioja, Smit, Grnroos, Kurimo, 2013</marker>
<rawString>Sami Virpioja, Peter Smit, Stig-Arne Grnroos, and Mikko Kurimo. 2013. Morfessor 2.0: Python Implementation and Extensions for Morfessor Baseline. Aalto University publication series SCIENCE + TECHNOLOGY, 25/2013. Aalto University, Helsinki, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>The Arabic Online Commentary Dataset: An Annotated Dataset</title>
<date>2011</date>
<booktitle>of Informal Arabic with High Dialectal Content. ACL-11,</booktitle>
<pages>37--41</pages>
<marker>Zaidan, Callison-Burch, 2011</marker>
<rawString>Omar F. Zaidan, Chris Callison-Burch. 2011. The Arabic Online Commentary Dataset: An Annotated Dataset of Informal Arabic with High Dialectal Content. ACL-11, pages 37–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<date>2014</date>
<booktitle>Arabic Dialect Identification. CL-11,</booktitle>
<pages>52--1</pages>
<marker>Zaidan, Callison-Burch, 2014</marker>
<rawString>Omar F. Zaidan, Chris Callison-Burch. 2014. Arabic Dialect Identification. CL-11, 52(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rabih Zbib</author>
<author>Erika Malchiodi</author>
<author>Jacob Devlin</author>
<author>David Stallard</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
<author>Omar F Zaidan</author>
<author>Chris CallisonBurch</author>
</authors>
<date>2012</date>
<booktitle>Machine translation of Arabic dialects. NAACL-2012,</booktitle>
<pages>49--59</pages>
<contexts>
<context position="9888" citStr="Zbib et al., 2012" startWordPosition="1618" endWordPosition="1621">rb suffixes such as “yn” instead of “wn” and “wA” instead of “wn” respectively. Also, so-called “five nouns”, are used in only one form (ex. “&gt;bw” (father of) instead of “&gt;bA” or “&gt;by”). 4 Detecting Dialectal Peculiarities ARZ is different from MSA lexically, morphologically, phonetically, and syntactically. Here, we present methods to handle such peculiarities. We chose not to handle syntactic differences, because they may be captured using word n-gram models. To capture lexical variations, we extracted and sorted by frequency all the unigrams from the Egyptian side of the LDC2012T09 corpus (Zbib et al., 2012), which has ≈ 38k Egyptian-English parallel sentences. A linguist was tasked with manually reviewing the words from the top until 1,300 dialectal words were found. Some of the words on the list included dialectal words, commonly used foreign words, words that exhibit morphological variations, and others with letter substitution. 1466 For morphological phenomenon, we employed three methods, namely: • Unsupervised Morphology Induction: We employed the unsupervised morpheme segmentation tool, Morfessor (Virpioja et al., 2013). It is a data driven tool that automatically learns morphemes from data</context>
<context position="12449" citStr="Zbib et al., 2012" startWordPosition="2029" endWordPosition="2032">(with diacritization) to generate possible Arabic stems. The generated stems with their diacritics were checked against a large diacritized Arabic corpus containing more than 200 million diacritized words 4. If generated words contained the letters “v”, “}”, “*”, and “D”, we used the aforementioned letter substitutions. We retained words that exist in the large tweet corpus but not in the Aljazeera corpus. The list contained 8k surface forms. 5 Evaluation Setup Dataset: We performed dialect identification experiment for ARZ and MSA. For ARZ, we used the Egyptian side of the LDC2012T09 corpus (Zbib et al., 2012) 5. For MSA, we used the Arabic side of the English/Arabic parallel corpus from the International Workshop on Arabic Language Translation6 which consists of ,: 150k sentences. For testing, we constructed an evaluation set that is markedly different 3http://aljazeera.net 4http://www.sh.rewayat2.com 5We did not use the Arabic Online Commentary data (Zaidan et al., 2011) as annotations were often not reliable. 6https://wit3.fbk.eu/mt.php?release= 2013-01 from the training set. We crawled Arabic tweets from Twitter during March 2014 and selected those where user location was set to Egypt or a geog</context>
</contexts>
<marker>Zbib, Malchiodi, Devlin, Stallard, Matsoukas, Schwartz, Makhoul, Zaidan, CallisonBurch, 2012</marker>
<rawString>Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas, Richard Schwartz, John Makhoul, Omar F. Zaidan, Chris CallisonBurch. 2012. Machine translation of Arabic dialects. NAACL-2012, pages 49–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>Crossdomain feature selection for language identification. IJCNLP-2011,</title>
<date>2011</date>
<pages>553--561</pages>
<marker>Lui, Baldwin, 2011</marker>
<rawString>Marco Lui and Timothy Baldwin. 2011. Crossdomain feature selection for language identification. IJCNLP-2011, page 553–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
<author>Nikola Ljubesic</author>
</authors>
<title>Efficient discrimination between closely related languages.</title>
<date>2012</date>
<tech>COLING-2012,</tech>
<pages>2619--2634</pages>
<marker>Tiedemann, Ljubesic, 2012</marker>
<rawString>J¨org Tiedemann and Nikola Ljubesic. 2012. Efficient discrimination between closely related languages. COLING-2012, 2619–2634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<date>2001</date>
<booktitle>Comparing corpora. CL-01,</booktitle>
<pages>6--1</pages>
<marker>Kilgarriff, 2001</marker>
<rawString>Adam Kilgarriff. 2001. Comparing corpora. CL-01, 6(1).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.999615">
A Dependency Parser for Tweets
</title>
<author confidence="0.9822595">
Lingpeng Kong Nathan Schneider Swabha Swayamdipta
Archna Bhatia Chris Dyer Noah A. Smith
</author>
<affiliation confidence="0.875421333333333">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998774">
{lingpenk,nschneid,swabha,archna,cdyer,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.993903" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999906266666667">
We describe a new dependency parser for
English tweets, TWEEBOPARSER. The
parser builds on several contributions: new
syntactic annotations for a corpus of tweets
(TWEEBANK), with conventions informed
by the domain; adaptations to a statistical
parsing algorithm; and a new approach to
exploiting out-of-domain Penn Treebank
data. Our experiments show that the parser
achieves over 80% unlabeled attachment
accuracy on our new, high-quality test set
and measure the benefit of our contribu-
tions.
Our dataset and parser can be found at
http://www.ark.cs.cmu.edu/TweetNLP.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99879347826087">
In contrast to the edited, standardized language of
traditional publications such as news reports, social
media text closely represents language as it is used
by people in their everyday lives. These informal
texts, which account for ever larger proportions of
written content, are of considerable interest to re-
searchers, with applications such as sentiment anal-
ysis (Greene and Resnik, 2009; Kouloumpis et al.,
2011). However, their often nonstandard content
makes them challenging for traditional NLP tools.
Among the tools currently available for tweets are
a POS tagger (Gimpel et al., 2011; Owoputi et al.,
2013) and a named entity recognizer (Ritter et al.,
2011)—but not a parser.
Important steps have been taken. The English
Web Treebank (Bies et al., 2012) represents an
annotation effort on web text—which likely lies
somewhere between newspaper text and social me-
dia messages in formality and care of editing—that
was sufficient to support a shared task (Petrov and
McDonald, 2012). Foster et al. (2011b) annotated
a small test set of tweets to evaluate parsers trained
on the Penn Treebank (Marcus et al., 1993), aug-
mented using semi-supervision and in-domain data.
Others, such as Soni et al. (2014), have used exist-
ing Penn Treebank–trained models on tweets.
In this work, we argue that the Penn Treebank
approach to annotation—while well-matched to
edited genres like newswire—is poorly suited to
more informal genres. Our starting point is that
rapid, small-scale annotation efforts performed
by imperfectly-trained annotators should provide
enough evidence to train an effective parser. We
see this starting point as a necessity, given observa-
tions about the rapidly changing nature of tweets
(Eisenstein, 2013), the attested difficulties of do-
main adaptation for parsing (Dredze et al., 2007),
and the expense of creating Penn Treebank–style
annotations (Marcus et al., 1993).
This paper presents TWEEBOPARSER, the first
syntactic dependency parser designed explicitly for
English tweets. We developed this parser follow-
ing current best practices in empirical NLP: we
annotate a corpus (TWEEBANK) and train the pa-
rameters of a statistical parsing algorithm. Our
research contributions include:
</bodyText>
<listItem confidence="0.9928714">
• a survey of key challenges posed by syntactic
analysis of tweets (by humans or machines) and
decisions motivated by those challenges and by
our limited annotation-resource scenario (§2);
• our annotation process and quantitative mea-
sures of the quality of the annotations (§3);
• adaptations to a statistical dependency parsing
algorithm to make it fully compatible with the
above, and also to exploit information from out-
of-domain data cheaply and without a strong
commitment (§4); and
• an experimental analysis of the parser’s unla-
beled attachment accuracy—which surpasses
80%—and contributions of various important
components (§5).
</listItem>
<bodyText confidence="0.985405">
The dataset and parser can be found at http://www.
ark.cs.cmu.edu/TweetNLP.
</bodyText>
<page confidence="0.925132">
1001
</page>
<note confidence="0.939416">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1001–1012,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.94883" genericHeader="method">
2 Annotation Challenges
</sectionHeader>
<bodyText confidence="0.999993444444444">
Before describing our annotated corpus of tweets
(§3), we illustrate some of the challenges of syn-
tactic analysis they present. These challenges moti-
vate an approach to annotation that diverges signif-
icantly from conventional approaches to treebank-
ing. Figure 1 presents a single example illustrating
four of these: token selection, multiword expres-
sions, multiple roots, and structure within noun
phrases. We discuss each in turn.
</bodyText>
<subsectionHeader confidence="0.971308">
2.1 Token Selection
</subsectionHeader>
<bodyText confidence="0.973114666666667">
Many elements in tweets have no syntactic function.
These include, in many cases, hashtags, URLs, and
emoticons. For example:
RT @justinbieber : now Hailee get a twitter
The retweet discourse marker, username, and colon
should not, we argue, be included in the syntactic
analysis. By contrast, consider:
Got #college admissions questions ? Ask them
tonight during #CampusChat I’m looking
forward to advice from @collegevisit
http://bit.ly/cchOTk
Here, both the hashtags and the at-mentioned user-
name are syntactically part of the utterances, while
the punctuation and the hyperlink are not. In the
example of Figure 1, the unselected tokens include
several punctuation tokens and the final token #be-
lieber, which marks the topic of the tweet.
Typically, dependency parsing evaluations ig-
nore punctuation token attachment (Buchholz and
Marsi, 2006), and we believe it is a waste of an-
notator (and parser) time to decide how to attach
punctuation and other non-syntactic tokens. Ma
et al. (2014) recently proposed to treat punctua-
tion as context features rather than dependents, and
found that this led to state-of-the-art performance
in a transition-based parser. A small adaptation
to our graph-based parsing approach, described in
§4.2, allows a similar treatment.
Our approach to annotation (§3) forces annota-
tors to explicitly select tokens that have a syntactic
function. 75.6% tokens were selected by the anno-
tators. Against the annotators’ gold standard, we
found that a simple rule-based filter for usernames,
hashtags, punctuation, and retweet tokens achieves
95.2% (with gold-standard POS tags) and 95.1%
(with automatic POS tags) average accuracy in the
task of selecting tokens with a syntactic function
in a ten-fold cross-validation experiment. To take
context into account, we developed a first-order
sequence model and found that it achieves 97.4%
average accuracy (again, ten-fold cross-validated)
with either gold-standard or automatic POS tags.
Features include POS; shape features that recog-
nize the retweet marker, hashtags, usernames, and
hyperlinks; capitalization; and a binary feature
for tokens that include punctuation. We trained
the model using the structured perceptron (Collins,
2002).
</bodyText>
<subsectionHeader confidence="0.999356">
2.2 Multiword Expressions
</subsectionHeader>
<bodyText confidence="0.998641702702703">
We consider multiword expressions (MWEs) of
two kinds. The first, proper names, have been
widely modeled for information extraction pur-
poses, and even incorporated into parsing (Finkel
and Manning, 2009). (An example found in Fig-
ure 1 is LA Times.) The second, lexical idioms,
have been a “pain in the neck” for many years (Sag
et al., 2002) and have recently received shallow
treatment in NLP (Baldwin and Kim, 2010; Con-
stant and Sigogne, 2011; Schneider et al., 2014).
Constant et al. (2012), Green et al. (2012), Candito
and Constant (2014), and Le Roux et al. (2014)
considered MWEs in parsing. Figure 1 provides
LA Times and All the Rage as examples.
Penn Treebank–style syntactic analysis (and de-
pendency representations derived from it) does
not give first-class treatment to this phenomenon,
though there is precedent for marking multiword
lexical units and certain kinds of idiomatic relation-
ships (Hajiˇc et al., 2012; Abeillé et al., 2003).1
We argue that internal analysis of MWEs is not
critical for many downstream applications, and
therefore annotators should not expend energy on
developing and respecting conventions (or mak-
ing arbitrary decisions) within syntactically opaque
or idiosyncratic units. We therefore allow annota-
tors to decide to group words as explicit MWEs,
including: proper names (Justin Bieber, World
Series), noncompositional or entrenched nominal
compounds (belly button, grilled cheese), connec-
tives (as well as), prepositions (out of), adverbials
(so far), and idioms (giving up, make sure).
From an annotator’s perspective, a MWE func-
tions as a single node in the dependency parse,
with no internal structure. For idioms whose in-
ternal syntax is easily characterized, the parse can
be used to capture compositional structure, an at-
</bodyText>
<footnote confidence="0.997887">
1The popular Stanford typed dependencies (de Marneffe
and Manning, 2008) scheme includes a special dependency
type for multiwords, though this is only applied to a small list.
</footnote>
<page confidence="0.997514">
1002
</page>
<figure confidence="0.9977725">
OMG I ♥ the Biebs &amp; want to have his babies ! —&gt; LA Times : Teen Pop Star Heartthrob is All the Rage on Social Media ... #belieber
een
ROOT
NOUN PHRASE
INTERNAL STRUCTURE
he B
ROOT MULTIPLE ROOTS
to have his bab
ROOT
ROOT
COORD
MWE MWE
</figure>
<figureCaption confidence="0.999414">
Figure 1: Parse tree for a (constructed) example illustrating annotation challenges discussed in §2. Colors highlight token
</figureCaption>
<equation confidence="0.529977">
É #belieber
</equation>
<bodyText confidence="0.99573735">
selection (gray; §2.1), multiword expressions (blue; §2.2), multiple roots (red; §2.3), coordination (dotted arcs, green; §3.2), and
noun phrase internal structure (orange; §2.4). The internal structure of multiword expressions (dashed arcs below the sentence)
was predicted automatically by a parser, as described in §2.2.
tractive property from the perspective of semantic
processing.
To allow training a fairly conventional statisti-
cal dependency parser from these annotations, we
find it expedient to apply an automatic conversion
to the MWE annotations, in the spirit of Johnson
(1998). We apply an existing dependency parser,
the first-order TurboParser (Martins et al., 2009)
trained on the Penn Treebank, to parse each MWE
independently, assigning structures like those for
LA Times and All the Rage in Figure 1. Arcs
involving the MWE in the annotation are then re-
connected to the MWE-internal root, so that the re-
sulting tree respects the original tokenization. The
MWE-internal arcs are given a special label so that
the transformation can be reversed and MWEs re-
constructed from parser output.
</bodyText>
<subsectionHeader confidence="0.999272">
2.3 Multiple Roots
</subsectionHeader>
<bodyText confidence="0.999897428571429">
For news text such as that found in the Penn Tree-
bank, sentence segmentation is generally consid-
ered a very easy task (Reynar and Ratnaparkhi,
1997). Tweets, however, often contain multiple
sentences or fragments, which we call “utterances,”
each with its own syntactic root disconnected from
the others. The selected tokens in Figure 1 com-
prise four utterances.
Our approach to annotation allows multiple ut-
terances to emerge directly from the connectedness
properties of the graph implied by an annotator’s
decisions. Our parser allows multiple attachments
to the “wall” symbol, so that multi-rooted analyses
can be predicted.
</bodyText>
<subsectionHeader confidence="0.996229">
2.4 Noun Phrase Internal Structure
</subsectionHeader>
<bodyText confidence="0.9999196875">
A potentially important drawback of deriving de-
pendency structures from phrase-structure annota-
tions, as is typically done using the Penn Treebank,
is that flat annotations lead to loss of information.
This is especially notable for noun phrases in the
Penn Treebank (Vadas and Curran, 2007). Consider
Teen Pop Star Heartthrob in Figure 1; Penn Tree-
bank conventions would label this as a single NP
with four NN children and no internal structure. De-
pendency conversion tools would likely attach the
first three words in the NP to Heartthrob. Direct de-
pendency annotation (rather than phrase-structure
annotation followed by automatic conversion) al-
lows a richer treatment of such structures, which is
potentially important for semantic analysis (Vecchi
et al., 2013).
</bodyText>
<sectionHeader confidence="0.990654" genericHeader="method">
3 A Twitter Dependency Corpus
</sectionHeader>
<bodyText confidence="0.9992205">
In this section, we describe the TWEEBANK cor-
pus, highlighting data selection (§3.1), the annota-
tion process (§3.2), important convention choices
(§3.3), and measures of quality (§3.4).
</bodyText>
<subsectionHeader confidence="0.999546">
3.1 Data Selection
</subsectionHeader>
<bodyText confidence="0.999953578947368">
We added manual dependency parses to 929 tweets
(12,318 tokens) drawn from the POS-tagged Twit-
ter corpus of Owoputi et al. (2013), which are tok-
enized and contain manually annotated POS tags.
Owoputi et al.’s data consists of two parts. The
first, originally annotated by Gimpel et al. (2011),
consists of tweets sampled from a particular day,
October 27, 2010—this is known as OCT27. Due
to concerns about overfitting to phenomena specific
to that day (e.g., tweets about a particular sports
game), Owoputi et al. (2013) created a new set of
547 tweets (DAILY547) consisting of one random
English tweet per day from January 2011 through
June 2012.
Our corpus is drawn roughly equally from
OCT27 and DAILY547.2 Despite its obvious tem-
poral skew, there is no reason to believe this sample
is otherwise biased; our experiments in §5 suggest
that this property is important.
</bodyText>
<subsectionHeader confidence="0.999315">
3.2 Annotation
</subsectionHeader>
<bodyText confidence="0.9999458">
Unlike a typical treebanking project, which may
take years and involve thousands of person-hours
of work by linguists, most of TWEEBANK was built
in a day by two dozen annotators, most of whom
had only cursory training in the annotation scheme.
</bodyText>
<footnote confidence="0.894873">
2This results from a long-term goal to fully annotate both.
</footnote>
<page confidence="0.928397">
1003
</page>
<figure confidence="0.9358301">
(1) RT @FRIENDSHlP : Friendship is love without
kissing ...
Friendship &gt; is &lt; love &lt; without &lt; kissing
(2) bieber is an alien ! :O he went down to earth .
bieber &gt; is** &lt; alien &lt; an
he &gt; [went down]** &lt; to &lt; earth
(3) RT @YourFavWhiteGuy : Helppp meeeee . I’mmm
meltiiinngggg → http://twitpic.com/316cjg
Helppp** &lt; meeeee
I&apos;mmm** &lt; meltiiinngggg
</figure>
<figureCaption confidence="0.999957">
Figure 2: Examples of GFL annotations from the corpus.
</figureCaption>
<bodyText confidence="0.999982548387097">
Our annotators used the Graph Fragment Lan-
guage (GFL), a text-based notation that facilitates
keyboard entry of parses (Schneider et al., 2013). A
Python Flask web application allows the annotator
to validate and visualize each parse (Mordowanec
et al., 2014). Some examples are shown in Fig-
ure 2. Note that all of the challenges in §2 are
handled easily by GFL notation: “retweet” infor-
mation, punctuation, and a URL are not selected by
virtue of their exclusion from the GFL expression;
in (2) went down is annotated as a MWE using
GFL’s square bracket notation; in (3) the tokens
are grouped into two utterances whose roots are
marked by the ** symbol.
Schneider et al.’s GFL offers some additional fea-
tures, only some of which we made use of in this
project. One important feature allows an annotator
to leave the parse underspecified in some ways. We
allowed our annotators to make use of this feature;
however, we excluded from our training and test-
ing data any parse that was incomplete (i.e., any
parse that contained multiple disconnected frag-
ments with no explicit root, excluding unselected
tokens). Learning to parse from incomplete anno-
tations is a fascinating topic explored in the past
(Hwa, 2001; Pereira and Schabes, 1992) and, in the
case of tweets, left for future work.
An important feature of GFL that we did use is
special notation for coordination structures. For
the coordination structure in Figure 1, for example,
the notation is:
</bodyText>
<equation confidence="0.770687">
$a :: {T want} :: {&amp;}
</equation>
<bodyText confidence="0.999937727272727">
where $a creates a new node in the parse tree as it is
visualized for the annotator, and this new node at-
taches to the syntactic parent of the conjoined struc-
ture, avoiding the classic forced choice between
coordinator and conjunct as parent. For learning to
parse, we transform GFL’s coordination structures
into specially-labeled dependency parses collaps-
ing nodes like $a with the coordinator and labeling
the attachments specially for postprocessing, fol-
lowing Schneider et al. (2013). In our evaluation
(§5), these are treated like other attachments.
</bodyText>
<subsectionHeader confidence="0.999479">
3.3 Annotation Conventions
</subsectionHeader>
<bodyText confidence="0.999249739130435">
A wide range of dependency conventions are in use;
in many cases these are conversion conventions
specifying how dependency trees can be derived
from phrase-structure trees. For English, the most
popular are due to Yamada and Matsumoto (2003)
and de Marneffe and Manning (2008), known as
“Yamada-Matsumoto” (YM) and “Stanford” depen-
dencies, respectively. The main differences be-
tween them are in whether the auxiliary is the par-
ent of the main verb (or vice versa) and whether the
preposition or its argument heads a prepositional
phrase (Elming et al., 2013).
A full discussion of our annotation conventions
is out of scope. We largely followed the conven-
tions suggested by Schneider et al. (2013), which in
turn are close to those of YM. Auxiliary verbs are
parents of main verbs, and prepositions are parents
of their arguments. The key differences from YM
are in coordination structures (discussed in §3.2;
YM makes the first conjunct the head) and posses-
sive structures, in which the possessor is the child
of the clitic, which is the child of the semantic head,
e.g.,the &gt; king &gt; &apos;s &gt; horses.
</bodyText>
<subsectionHeader confidence="0.953234">
3.4 Intrinsic Quality
</subsectionHeader>
<bodyText confidence="0.999610444444444">
Our approach to developing this initial corpus of
syntactically annotated tweets was informed by an
aversion to making the perfect the enemy of the
good; that is, we sought enough data of sufficient
quality to build a usable parser within a relatively
short amount of time. If our research goals had
been to develop a replicable process for annotation,
more training and more quality control would have
been called for. Under our budgeted time and anno-
tator resources, this overhead was simply too costly.
Nonetheless, we performed a few analyses that give
a general picture of the quality of the annotations.
Inter-annotator agreement. 170 of the tweets
were annotated by multiple users. By the softCom-
Prec measure (Schneider et al., 2013),3 the agree-
ment rate on dependencies is above 90%.
Expert linguistic judgment. A linguist co-
author examined a stratified sample (balanced
</bodyText>
<footnote confidence="0.9923525">
3softComPrec is a generalization of attachment accuracy
that handles unselected tokens and MWEs.
</footnote>
<page confidence="0.995936">
1004
</page>
<bodyText confidence="0.999972111111111">
across annotators) of 60 annotations and rated their
quality on a 5-point scale. 30 annotations were
deemed to have “no obvious errors,” 15 only minor
errors, 3 a major error (i.e., clear violation of an-
notation guidelines),4 4 a major error and at least
one minor error, and 8 as containing multiple major
errors. Thus, 75% are judged as having no major
errors. We found this encouraging, considering that
this sample is skewed in favor of people who anno-
tated less (including many of the less experienced
and/or lower-proficiency annotators).
Pairwise ranking. For 170 of the doubly anno-
tated tweets, an experienced annotator examined
whether one or the other was markedly better. In
100 cases the two annotations were of comparable
quality (neither was obviously better) and did not
contain any obvious major errors. In only 7 pairs
did both of the annotations contain a serious error.
Qualitatively, we found several unsurprising
sources of error or disagreement, including em-
bedded/subordinate clauses, subject-auxiliary in-
version, predeterminers, and adverbial modifiers
following a modal/auxiliary verb and a main verb.
Clarification of the conventions, or even explicit
rule-based checking in the validation step, might
lead to quality improvements in further annotation
efforts.
</bodyText>
<sectionHeader confidence="0.972401" genericHeader="method">
4 Parsing Algorithm
</sectionHeader>
<bodyText confidence="0.9999824">
For parsing, we start with TurboParser, which is
open-source and has been found to perform well on
a range of parsing problems in different languages
(Martins et al., 2013; Kong and Smith, 2014). The
underlying model allows for flexible incorporation
of new features and changes to specification in the
output space. We briefly review the key ideas in
TurboParser (§4.1), then describe decoder modifi-
cations required for our problem (§4.2). We then
discuss features we added to TurboParser (§4.3).
</bodyText>
<subsectionHeader confidence="0.925539">
4.1 TurboParser
</subsectionHeader>
<bodyText confidence="0.995511583333333">
Let an input sentence be denoted by x and the set
of possible dependency parses for x be denoted by
Yx. A generic linear scoring function based on a
4What we deemed major errors included, for example,
an incorrect dependency relation between an auxiliary verb
and the main verb (like ima &gt; [have to]). Minor errors
included an incorrect attachment between two modifiers of
the same head, as in the &gt; only &gt; [grocery store]—the
correct annotation would have two attachments to a single
head,i.e.the &gt; [grocery store] &lt; only (or equivalent).
feature vector representation g is used in parsing
algorithms that seek to find:
</bodyText>
<equation confidence="0.9588735">
parse∗(x) = argmax wTg(x,y) (1)
y∈Yx
</equation>
<bodyText confidence="0.999929">
The score is parameterized by a vector w of
weights, which are learned from data (most com-
monly using MIRA, McDonald et al., 2005a).
The decomposition of the features into local
“parts” is a critical choice affecting the computa-
tional difficulty of solving Eq. 1. The most aggres-
sive decomposition leads to an “arc-factored” or
“first-order” model, which permits exact, efficient
solution of Eq. 1 using spanning tree algorithms
(McDonald et al., 2005b) or, with a projectivity
constraint, dynamic programming (Eisner, 1996).
Second- and third-order models have also been
introduced, typically relying on approximations,
since less-local features increase the computational
cost, sometimes to the point of NP-hardness (Mc-
Donald and Satta, 2007). TurboParser attacks the
parsing problem using a compact integer linear pro-
gramming (ILP) representation of Eq. 1 (Martins
et al., 2009), then employing alternating directions
dual decomposition (AD3; Martins et al., 2011).
This enables inclusion of second-order features
(e.g., on a word with its sibling or grandparent;
Carreras, 2007) and third-order features (e.g., a
word with its parent, grandparent, and a sibling, or
with its parent and two siblings; Koo and Collins,
2010).
For a collection of (possibly overlapping) parts
for input x, Sx (which includes the union of all
parts of all trees in Yx), we will use the following
notation. Let
</bodyText>
<equation confidence="0.942494">
g(x,y) = E fs(x,y), (2)
s∈Sx
</equation>
<bodyText confidence="0.999973625">
where fs only considers part s and is nonzero only
if s is present in y. In the ILP framework, each s
has a corresponding binary variable zs indicating
whether part s is included in the output. A col-
lection of constraints relating zs define the set of
feasible vectors z that correspond to valid outputs
and enfore agreement between parts that overlap.
Many different versions of these constraints have
been studied (Riedel and Clarke, 2006; Smith and
Eisner, 2008; Martins et al., 2009, 2010).
A key attraction of TurboParser is that many
overlapping parts can be handled, making use of
separate combinatorial algorithms for efficiently
handling subsets of constraints. For example, the
constraints that force z to encode a valid tree can
be exploited within the framework by making calls
</bodyText>
<page confidence="0.962865">
1005
</page>
<bodyText confidence="0.998882">
to classic arborescence algorithms (Chu and Liu,
1965; Edmonds, 1967). As a result, when describ-
ing modifications to TurboParser, we need only to
explain additional constraints and features imposed
on parts.
</bodyText>
<subsectionHeader confidence="0.98309">
4.2 Adapted Parse Parts
</subsectionHeader>
<bodyText confidence="0.9976398125">
The first collection of parts we adapt are simple
arcs, each consisting of an ordered pair of indices
of words in x; arc(p,c) corresponds to the attach-
ment of xc as a child of xp (iff zarc(p,c) =1). Our rep-
resentation explicitly excludes some tokens from
being part of the syntactic analysis (§2.1); to han-
dle this, we constrain zarc(i,j) = 0 whenever xi or xj
is excluded.
The implication is that excluded tokens are still
“visible” to feature functions that involve other
edges. For example, some conventional first-order
features consider the tokens occurring between a
parent and child. Even if a token plays no syntactic
role of its own, it might still be informative about
the syntactic relationships among other tokens. We
note three alternative methods:
</bodyText>
<listItem confidence="0.75852884">
1. We might remove all unselected tokens from
x before running the parser. In §5.6 we find
this method to fare 1.7–2.3% worse than our
modified decoding algorithm.
2. We might remove unselected tokens but use
them to define new features, so that they still
serve as evidence. This is the approach taken
by Ma et al. (2014) for punctuation. We judge
our simple modification to the decoding algo-
rithm to be more expedient, and leave the trans-
lation of existing context-word features into that
framework for future exploration.
3. We might incorporate the token selection deci-
sions into the parser, performing joint inference
for selection and parsing. The AD3 algorithm
within TurboParser is well-suited to this kind
of extension: z-variables for each token’s se-
lection could be added, and similar scores to
those of our token selection sequence model
(§2.1) could be integrated into parsing. Given,
however, that the sequence model achieves over
97% accuracy, and that perfect token selection
would gain only 0.1–1% in parsing accuracy (re-
ported in §5.5), we leave this option for future
work as well.
</listItem>
<bodyText confidence="0.999567714285714">
For first-order models, the above change is all
that is necessary. For second- and third-order
models, TurboParser makes use of head automata,
in particular “grand-sibling head automata” that
assign scores to word tuples of xg, its child xp,
and two of xp’s adjacent children, xc and x′c (Koo
et al., 2010). The second-order models in our
experiments include parts for sibling(p,c,c′) and
grandparent(p,c,g) and use the grand-sibling head
automaton to reason about these together. Au-
tomata for an unselected xp or xg, and transitions
that consider unselected tokens as children, are
eliminated. In order to allow the scores to depend
on unselected tokens between xc and x′c, we added
the binned counts of unselected tokens (mostly
punctuation) joint with the word form and POS
tag of xp and the POS tag of xc and x′c as features
scored in the sibling(p,c,c′) part. The changes dis-
cussed above comprise the totality of adaptations
we made to the TurboParser algorithm; we refer to
them as “parsing adaptations” in the experiments.
</bodyText>
<subsectionHeader confidence="0.996833">
4.3 Additional Features
</subsectionHeader>
<bodyText confidence="0.999702586206897">
Brown clusters. Owoputi et al. (2013) found that
Brown et al. (1992) clusters served as excellent fea-
tures in Twitter POS tagging. Others have found
them useful in parsing (Koo et al., 2008) and other
tasks (Turian et al., 2010). We therefore follow
Koo et al. in incorporating Brown clusters as fea-
tures, making use of the publicly available Twitter
clusters from Owoputi et al.5 We use 4 and 6 bit
cluster representations to create features wherever
POS tags are used, and full bit strings to create
features wherever words were used.
Penn Treebank features. A potential danger of
our choice to “start from scratch” in developing
a dependency parser for Twitter is that the result-
ing annotation conventions, data, and desired out-
put are very different from dependency parses de-
rived from the Penn Treebank. Indeed, Foster et al.
(2011a) took a very different approach, applying
Penn Treebank conventions in annotation of a test
dataset for evaluation of a parser trained using Penn
Treebank trees. In §5.4, we replicate, for depen-
dencies, their finding that a Penn Treebank–trained
parser is hard to beat on their dataset, which was
not designed to be topically representative of En-
glish Twitter. When we turn to a more realistic
dataset like ours, we find the performance of the
Penn Treebank–trained parser to be poor.
Nonetheless, it is hard to ignore such a large
amount of high-quality syntactic data. We there-
</bodyText>
<footnote confidence="0.936457">
5http://www.ark.cs.cmu.edu/TweetNLP/clusters/
50mpaths2
</footnote>
<page confidence="0.994323">
1006
</page>
<bodyText confidence="0.999917">
fore opted for a simple, stacking-inspired incor-
poration of Penn Treebank information into our
model.6 We define a feature on every candidate arc
whose value is the (quantized) score of the same arc
under a first-order model trained on the Penn Tree-
bank converted using head rules that are as close
as possible to our conventions (discussed in more
detail in §5.1). This lets a Penn Treebank model
literally “weigh in” on the parse for a tweet, and
lets the learning algorithm determine how much
consideration it deserves.
</bodyText>
<sectionHeader confidence="0.999381" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999664">
Our experiments quantify the contributions of vari-
ous components of our approach.
</bodyText>
<subsectionHeader confidence="0.95806">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.999948692307692">
We consider two test sets. The first, TEST-NEW,
consists of 201 tweets from our corpus annotated
by the most experienced of our annotators (one
of whom is a co-author of this work). Given very
limited data, we believe using the highest quality
data for measuring performance, and lower-quality
data for training, is a sensibly realistic choice.
Our second test set, TEST-FOSTER, is the dataset
annotated by Foster et al. (2011b), which consists
of 250 sentences. Recall that their corpus was
annotated with phrase structures according to Penn
Treebank conventions. Conversion to match our
annotation conventions was carried out as follows:
</bodyText>
<listItem confidence="0.995325083333333">
1. We used the PennConverter tool with head rule
options selected to approximate our annotation
conventions as closely as possible.7
2. An experienced annotator manually modified
the automatically converted trees by:
(a) Performing token selection (§2.1) to remove
the tokens which have no syntactic function.
(b) Grouping MWEs (§2.2). Here, most of the
MWEs are named entities such as Manch-
ester United.
(c) Attaching the roots of the utterance in tweets
to the “wall” symbol (§2.3).8
</listItem>
<bodyText confidence="0.920635">
6Stacking is a machine learning method where the predic-
tions of one model are used to create features for another. The
second model may be from a different family. Stacking has
been found successful for dependency parsing by Nivre and
McDonald (2008) and Martins et al. (2008). Johansson (2013)
describes further advances that use path features.
</bodyText>
<footnote confidence="0.991735166666667">
7http://nlp.cs.lth.se/software/treebank_
converter; run with -rightBranching=false
-coordStructure=prague -prepAsHead=true
-posAsHead=true -subAsHead=true -imAsHead=true
-whAsHead=false.
8This was infrequent; their annotations split most multi-
</footnote>
<table confidence="0.998634333333333">
TRAIN TEST-NEW TEST-FOSTER
tweets 717 201 &lt; 250†
unique tweets 569 201 &lt; 250†
tokens 9,310 2,839 2,841
selected tokens 7,015 2,158 2,366
types 3,566 1,461 1,230
utterances 1,473 429 337
multi-root tweets 398 123 60
MWEs 387 78 109
</table>
<tableCaption confidence="0.957595">
Table 1: Statistics of our datasets. (A tweet with k annotations
in the training set is counted k times for the totals of tokens,
utterances, etc.). †TEST-FOSTER contains 250 manually split
sentences. The number of tweets should be smaller but is not
recoverable from the data release.
</tableCaption>
<listItem confidence="0.78114075">
(d) Recovering the internal structure of the noun
phrases.
(e) Fixing a difference in conventions with re-
spect to subject-auxiliary inversion.9
</listItem>
<bodyText confidence="0.999576533333333">
We consider two training sets. TRAIN-NEW con-
sists of the remaining 717 tweets from our corpus
(§3) annotated by the rest of the annotators. Some
of these tweets have annotations from multiple an-
notators; 11 annotations for tweets that also oc-
curred in TEST-NEW were excluded. TRAIN-PTB
is the conventional training set from the Penn Tree-
bank (§2–21). The PennConverter tool was used
to extract dependencies, with head rule options se-
lected to approximate our annotation conventions
as closely as possible (see footnote 7). The result-
ing annotations lack the same attention to noun
phrase–internal structure (§2.4) and handle subject-
auxiliary inversions differently than our data. Part-
of-speech tags were coarsened to be compatible
with the Twitter POS tags, using the mappings spec-
ified by Gimpel et al. (2011).
Statistics for the in-domain datasets are given in
Table 1. As we can see in the table, more than half
of the tweets in our corpus have multiple utterances.
The out-of-vocabulary rate for our TRAIN/TEST-
NEW split is 33.7% by token and 62.5% by type;
for TRAIN/TEST-FOSTER it is 41.4% and 64.6%
respectively. These are much higher than the 2.5%
and 13.2% in the standard Penn Treebank split.
All evaluations here are on unlabeled attachment
F1 scores.10 Our parser provides labels for coordi-
nation structures and MWEs (§2), but we do not
present detailed evaluations of those due to space
constraints.
</bodyText>
<footnote confidence="0.8831686">
utterance tweets into separate sentence-instances.
9For example, in the sentence Is he driving, we attached
he to driving while PennConverter attaches it to Is.
10Because of token selection, precision and recall may not
be equal.
</footnote>
<page confidence="0.992017">
1007
</page>
<subsectionHeader confidence="0.993508">
5.2 Preprocessing
</subsectionHeader>
<bodyText confidence="0.99996725">
Because some of the tweets in our test set were
also in the training set of Owoputi et al. (2013),
we retrained their POS tagger on all the annotated
data they have minus the 201 tweets in our test
set. Its tagging accuracy was 92.8% and 88.7% on
TEST-NEW and TEST-FOSTER, respectively. The
token selection model (§2.1) achieves 97.4% on
TEST-NEW with gold or automatic POS tagging;
and on TEST-FOSTER, 99.0% and 99.5% with gold
and automatic POS tagging, respectively.
As noted in §4.3, Penn Treebank features were
developed using a first-order TurboParser trained
on TRAIN-PTB; Brown clusters were included in
computing these Penn Treebank features if they
were available in the parser to which the features
(i.e. Brown clusters) were added.
</bodyText>
<subsectionHeader confidence="0.999026">
5.3 Main Parser
</subsectionHeader>
<bodyText confidence="0.999930642857143">
The second-order TurboParser described in §4,
trained on TRAIN-NEW (default hyperparameter
values), achieves 80.9% unlabeled attachment ac-
curacy on TEST-NEW and 76.1% on TEST-FOSTER.
The experiments consider variations on this main
approach, which is the version released as TWEE-
BOPARSER.
The discrepancy between the two test sets is
easily explained: as noted in §3.1, the dataset
from which our tweets are drawn was designed
to be representative of English on Twitter. Fos-
ter et al. (2011b) selected tweets from Berming-
ham and Smeaton’s (2010) corpus, which uses fifty
predefined topics like politics, business, sports,
and entertainment—in short, topics not unlike
those found in the Penn Treebank. Relative to
the Penn Treebank training set, the by-type out-
of-vocabulary rates are 45.2% for TEST-NEW and
only 21.6% for TEST-FOSTER (cf. 13.2% for the
Penn Treebank test set).
Another mismatch is in the handling of utter-
ances. In our corpus, utterance segmentation
emerges from multi-rooted annotations (§2.3). Fos-
ter et al. (2011b) manually split each tweet into
utterances and treat those as separate instances in
their corpus, so that our model trained on often
multi-rooted tweets from TRAIN is being tested
only on single-rooted utterances.
</bodyText>
<subsectionHeader confidence="0.858464">
5.4 Experiment: Which Training Set?
</subsectionHeader>
<bodyText confidence="0.9995225">
We consider the direct use of TRAIN-PTB instead
of TRAIN-NEW. Table 2 reports the results on both
</bodyText>
<table confidence="0.9999308">
Unlabeled Attachment F1 (%)
mod. POS POS as-is
TEST-NEW
Baseline 73.0 73.5
+ Brown 73.7 73.3
+ Brown &amp; PA 72.9 73.1
TEST-FOSTER
Baseline 76.3 75.2
+ Brown 75.5 76.7
+ Brown &amp; PA 76.9 77.0
</table>
<tableCaption confidence="0.67887525">
Table 2: Performance of second-order TurboParser trained on
TRAIN-PTB, with various preprocessing options. The main
parser (§5.3) achieves 80.9% and 76.1% on the two test sets,
respectively; see §5.4 for discussion.
</tableCaption>
<bodyText confidence="0.974526470588235">
test sets, with various options. “Baseline” is off-
the-shelf second-order TurboParser. We consider
augmenting it with Brown cluster features (§4.3;
“+ Brown”) and then also with the parsing adapta-
tions of §4.2 (“+ Brown &amp; PA”). Another choice
is whether to modify the POS tags at test time; the
modified version (“mod. POS”) maps at-mentions
to pronoun, and hashtags and URLs to noun.
We note that comparing these scores to our main
parser (§5.3) conflates three very important inde-
pendent variables: the amount of training data
(39,832 Penn Treebank sentences vs. 1,473 Twitter
utterances), the annotation method, and the source
of the data. However, we are encouraged that, on
what we believe is the superior test set (TEST-NEW),
our overall approach obtains a 7.8% gain with an
order of magnitude less annotated data.
</bodyText>
<subsectionHeader confidence="0.94308">
5.5 Experiment: Effect of Preprocessing
</subsectionHeader>
<bodyText confidence="0.999783428571429">
Table 3 (second block, italicized) shows the per-
formance of the main parser on both test sets with
gold-standard and automatic POS tagging and to-
ken selection. On TEST-NEW, with either gold-
standard POS tags or gold-standard token selection,
performance increases by 1.1%; with both, it in-
creases by 2.3%. On TEST-FOSTER, token selec-
tion matters much less, but POS tagging accounts
for a drop of more than 6%. This is consistent with
Foster et al.’s finding: using a fine-grained Penn
Treebank–trained POS tagger (achieving around
84% accuracy on Twitter), they saw 5–8% improve-
ment in unlabeled dependency attachment accuracy
using gold-standard POS tags.
</bodyText>
<subsectionHeader confidence="0.966802">
5.6 Experiment: Ablations
</subsectionHeader>
<bodyText confidence="0.999907333333333">
We ablated each key element of our main parser—
PTB features, Brown features, second order fea-
tures and decoding, and the parsing adaptations of
</bodyText>
<page confidence="0.972609">
1008
</page>
<figure confidence="0.9999131875">
(a) TEST-NEW
(b) TEST-FOSTER
Unlabeled Attachment F1
0.82
0.78
0.76
0.8
First-Order
Second-Order
Unlabeled Attachment F1
0.76
0.74
0.72
0.7
First-Order
Second-Order
</figure>
<figureCaption confidence="0.999584">
Figure 3: Feature ablations; these charts present the same scores shown in Table 3 and more variants of the first-order model.
</figureCaption>
<table confidence="0.999953809523809">
Unlabeled Attachment F1 (%)
TEST-NEW TEST-FOSTER
Main parser 80.9 76.1
Gold POS and TS 83.2 82.8
Gold POS, automatic TS 82.0 82.3
Automatic POS, gold TS 82.0 76.2
Single ablations:
− PTB 80.2 72.6
− Brown 81.2 75.4
− 2nd order 80.1 75.6
− PA 79.2 73.7
Double ablations:
− PTB, − Brown 79.5 72.8
− PTB, − 2nd order 78.5 72.2
− PTB, − PA 77.4 69.6
− Brown, − 2nd order 80.7 74.5
− Brown, − PA 78.2 73.7
− 2nd order, − PA 77.7 73.5
Baselines:
Second order 76.5 70.4
First order 76.1 70.4
</table>
<tableCaption confidence="0.7456642">
Table 3: Effects of gold-standard POS tagging and token
selection (TS; §5.5) and of feature ablation (§5.6). The “base-
lines” are TurboParser without the parsing adaptations in §4.2
and without Penn Treebank or Brown features. The best result
in each column is bolded. See also Figure 3.
</tableCaption>
<bodyText confidence="0.999796764705882">
§4.2—as well as each pair of these. These condi-
tions use automatic POS tags and token selection.
The “− PA” condition, which ablates parsing adap-
tations, is accomplished by deleting punctuation
(in training and test data) and parsing using Turbo-
Parser’s existing algorithm.
Results are shown in Table 3. Further results
with first- and second-order TurboParsers are plot-
ted in Figure 3. Notably, a 2–3% gain is obtained by
modifying the parsing algorithm, and our stacking-
inspired use of Penn Treebank data contributes in
both cases, quite a lot on TEST-FOSTER (unsur-
prisingly given that test set’s similarity to the Penn
Treebank). More surprisingly, we find that Brown
cluster features do not consistently improve perfor-
mance, at least not as instantiated here, with our
small training set.
</bodyText>
<sectionHeader confidence="0.999561" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999804375">
We described TWEEBOPARSER, a dependency
parser for English tweets that achieves over 80%
unlabeled attachment score on a new, high-quality
test set. This is on par with state-of-the-art re-
ported results for news text in Turkish (77.6%;
Koo et al., 2010) and Arabic (81.1%; Martins
et al., 2011). Our contributions include impor-
tant steps taken to build the parser: a considera-
tion of the challenges of parsing tweets that in-
formed our annotation process, the resulting new
TWEEBANK corpus, adaptations to a statistical
parsing algorithm, a new approach to exploiting
data in a better-resourced domain (the Penn Tree-
bank), and experimental analysis of the decisions
we made. The dataset and parser can be found at
http://www.ark.cs.cmu.edu/TweetNLP.
</bodyText>
<sectionHeader confidence="0.997635" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.664418285714286">
The authors thank the anonymous reviewers
and André Martins, Yanchuan Sim, Wang Ling,
Michael Mordowanec, and Alexander Rush for
helpful feedback, as well as the annotators Waleed
Ammar, Jason Baldridge, David Bamman, Dallas
Card, Shay Cohen, Jesse Dodge, Jeffrey Flanigan,
Dan Garrette, Lori Levin, Wang Ling, Bill Mc-
Dowell, Michael Mordowanec, Brendan O’Connor,
Rohan Ramanath, Yanchuan Sim, Liang Sun, Sam
Thomson, and Dani Yogatama. This research was
supported in part by the U. S. Army Research Lab-
oratory and the U. S. Army Research Office under
contract/grant number W911NF-10-1-0533 and by
NSF grants IIS-1054319 and IIS-1352440.
</bodyText>
<page confidence="0.994228">
1009
</page>
<note confidence="0.7709205">
References sentation. In Proc. of COLING Workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
</note>
<reference confidence="0.997433637362637">
Anne Abeillé, Lionel Clément, and François Toussenel.
2003. Building a treebank for French. In Treebanks,
pages 165–187. Springer.
Timonthy Baldwin and Su Nam Kim. 2010. Multi-
word expressions. In Handbook of Natural Lan-
guage Processing, Second Edition. CRC Press, Tay-
lor and Francis Group.
Adam Bermingham and Alan F. Smeaton. 2010. Clas-
sifying sentiment in microblogs: Is brevity an advan-
tage? In Proc. of CIKM.
Ann Bies, Justin Mott, Colin Warner, and Seth
Kulick. 2012. English Web Treebank. Techni-
cal Report LDC2012T13, Linguistic Data Consor-
tium. URL http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2012T13.
Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467–479.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of CoNLL.
Marie Candito and Matthieu Constant. 2014. Strate-
gies for contiguous multiword expression analysis
and dependency parsing. In Proc. of ACL.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proc. of
EMNLP-CoNLL.
Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On shortest
arborescence of a directed graph. Scientia Sinica,
14(10):1396.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: theory and ex-
periments with perceptron algorithms. In Proc. of
EMNLP.
Matthieu Constant and Anthony Sigogne. 2011. MWU-
aware part-of-speech tagging with a CRF model and
lexical resources. In Proc. of the Workshop on Multi-
word Expressions: from Parsing and Generation to
the Real World.
Matthieu Constant, Anthony Sigogne, and Patrick Wa-
trin. 2012. Discriminative strategies to integrate mul-
tiword expression recognition and parsing. In Proc.
of ACL.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
Mark Dredze, John Blitzer, Partha Pratim Taluk-
dar, Kuzman Ganchev, Joao Graca, and Fernando
Pereira. 2007. Frustratingly hard domain adapta-
tion for dependency parsing. In Proc. of EMNLP-
CoNLL.
Jack Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards B,
71(233-240):160.
Jacob Eisenstein. 2013. What to do about bad language
on the internet. In Proc. of NAACL-HLT.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proc. of
COLING.
Jakob Elming, Anders Johannsen, Sigrid Klerke,
Emanuele Lapponi, Héctor Martínez Alonso, and
Anders Søgaard. 2013. Down-stream effects of tree-
to-dependency conversions. In Proc. of NAACL-
HLT.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Proc
ofACL-HLT.
Jennifer Foster, Özlem Qetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011a.
#hardtoparse: POS tagging and parsing the Twitter-
verse. In Proc. of AAAI Workshop on Analyzing Mi-
crotext.
Jennifer Foster, Özlem Qetino˘glu, Joachim Wagner,
Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and
Josef van Genabith. 2011b. From news to comment:
resources and benchmarks for parsing the language
of Web 2.0. In Proc. of IJCNLP.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: annotation, features, and experiments.
In Proc. of ACL-HLT.
Spence Green, Marie-Catherine de Marneffe, and
Christopher D. Manning. 2012. Parsing models for
identifying multiword expressions. Computational
Linguistics, 39(1):195–227.
Stephan Greene and Philip Resnik. 2009. Syntac-
tic packaging and implicit sentiment. In Proc. of
NAACL.
</reference>
<page confidence="0.740033">
1010
</page>
<reference confidence="0.999841612903226">
Jan Hajiˇc, Eva Hajiˇcová, Jarmila Panevová, Petr Sgall,
Silvie Cinková, Eva Fuˇcíková, Marie Mikulová,
Petr Pajas, Jan Popelka, Jiˇrí Semeck`y, Jana
Šindlerová, Jan Štˇepánek, Josef Toman, Zdeˇnka
Urešová, and Zdenˇek Žabokrtský. 2012. Prague
Czech-English Dependency Treebank 2.0. Techni-
cal Report LDC2012T08, Linguistic Data Consor-
tium. URL http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2012T08.
Rebecca Hwa. 2001. Learning Probabilistic Lexical-
ized Grammars for Natural Language Processing.
Ph.D. thesis, Harvard University.
Richard Johansson. 2013. Training parsers on incom-
patible treebanks. In Proc. of NAACL-HLT.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613–632.
Lingpeng Kong and Noah A. Smith. 2014. An empiri-
cal comparison of parsing methods for Stanford de-
pendencies. ArXiv:1404.4314.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In
Proc. ofACL.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proc. of ACL.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual de-
composition for parsing with non-projective head au-
tomata. In Proc. of EMNLP.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proc. of ICWSM.
Joseph Le Roux, Matthieu Constant, and Antoine
Rozenknop. 2014. Syntactic parsing and compound
recognition via dual decomposition: application to
French. In Proc. of COLING.
Ji Ma, Yue Zhang, and Jingbo Zhu. 2014. Punctua-
tion processing for projective dependency parsing.
In Proc. of ACL.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional linguistics, 19(2):313–330.
André F.T. Martins, Miguel Almeida, and Noah A.
Smith. 2013. Turning on the turbo: Fast third-order
non-projective turbo parsers. In Proc. of ACL.
André F.T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers. In
Proc. of EMNLP.
André F.T. Martins, Noah A. Smith, Pedro M.Q.
Aguiar, and Mário A.T. Figueiredo. 2011. Dual de-
composition with many overlapping components. In
Proc. of EMNLP.
André F.T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proc. of ACL-
IJCNLP.
André F.T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M.Q. Aguiar, and Mário A.T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proc. of EMNLP.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proc. of ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. of HLT-
EMNLP.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In Proc. of IWPT.
Michael T. Mordowanec, Nathan Schneider, Chris.
Dyer, and Noah A. Smith. 2014. Simplified depen-
dency annotations with GFL-Web. In Proc. of ACL,
demonstration track.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proc. of ACL-HLT.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proc. of NAACL-HLT.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proc. of ACL.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proc. ofANLP.
</reference>
<page confidence="0.797889">
1011
</page>
<reference confidence="0.999684875">
Sebastian Riedel and James Clarke. 2006. Incremen-
tal integer linear programming for non-projective de-
pendency parsing. In Proc. of EMNLP.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an ex-
perimental study. In Proc. of EMNLP.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Proc. of
CICLing.
Nathan Schneider, Emily Danchik, Chris Dyer, and
Noah A. Smith. 2014. Discriminative lexical se-
mantic segmentation with gaps: Running the MWE
gamut. Transactions of the Association for Compu-
tational Linguistics, 2:193–206.
Nathan Schneider, Brendan O’Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. In Proc. of the 7th Lin-
guistic Annotation Workshop and Interoperability
with Discourse.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proc. of EMNLP.
Sandeep Soni, Tanushree Mitra, Eric Gilbert, and Jacob
Eisenstein. 2014. Modeling factuality judgments in
social media text. In Proc. of ACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proc. of ACL.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proc. of
ACL.
Eva Maria Vecchi, Roberto Zamparelli, and Marco Ba-
roni. 2013. Studying the recursive behaviour of
adjectival modification with compositional distribu-
tional semantics. In Proc. of EMNLP.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proc. of IWPT.
</reference>
<page confidence="0.994898">
1012
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.524076">
<title confidence="0.999948">A Dependency Parser for Tweets</title>
<author confidence="0.948289">Lingpeng Kong Nathan Schneider Swabha</author>
<affiliation confidence="0.853459333333333">Archna Bhatia Chris Dyer Noah A. Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.958799">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999779">lingpenk@cs.cmu.edu</email>
<email confidence="0.999779">nschneid@cs.cmu.edu</email>
<email confidence="0.999779">swabha@cs.cmu.edu</email>
<email confidence="0.999779">archna@cs.cmu.edu</email>
<email confidence="0.999779">cdyer@cs.cmu.edu</email>
<email confidence="0.999779">nasmith@cs.cmu.edu</email>
<abstract confidence="0.999691857142857">We describe a new dependency parser for tweets, The parser builds on several contributions: new syntactic annotations for a corpus of tweets with conventions informed by the domain; adaptations to a statistical parsing algorithm; and a new approach to exploiting out-of-domain Penn Treebank data. Our experiments show that the parser achieves over 80% unlabeled attachment accuracy on our new, high-quality test set and measure the benefit of our contributions.</abstract>
<intro confidence="0.863693">Our dataset and parser can be found at</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abeillé</author>
<author>Lionel Clément</author>
<author>François Toussenel</author>
</authors>
<title>Building a treebank for French. In Treebanks,</title>
<date>2003</date>
<pages>165--187</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="7655" citStr="Abeillé et al., 2003" startWordPosition="1138" endWordPosition="1141">t al., 2002) and have recently received shallow treatment in NLP (Baldwin and Kim, 2010; Constant and Sigogne, 2011; Schneider et al., 2014). Constant et al. (2012), Green et al. (2012), Candito and Constant (2014), and Le Roux et al. (2014) considered MWEs in parsing. Figure 1 provides LA Times and All the Rage as examples. Penn Treebank–style syntactic analysis (and dependency representations derived from it) does not give first-class treatment to this phenomenon, though there is precedent for marking multiword lexical units and certain kinds of idiomatic relationships (Hajiˇc et al., 2012; Abeillé et al., 2003).1 We argue that internal analysis of MWEs is not critical for many downstream applications, and therefore annotators should not expend energy on developing and respecting conventions (or making arbitrary decisions) within syntactically opaque or idiosyncratic units. We therefore allow annotators to decide to group words as explicit MWEs, including: proper names (Justin Bieber, World Series), noncompositional or entrenched nominal compounds (belly button, grilled cheese), connectives (as well as), prepositions (out of), adverbials (so far), and idioms (giving up, make sure). From an annotator’</context>
</contexts>
<marker>Abeillé, Clément, Toussenel, 2003</marker>
<rawString>Anne Abeillé, Lionel Clément, and François Toussenel. 2003. Building a treebank for French. In Treebanks, pages 165–187. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timonthy Baldwin</author>
<author>Su Nam Kim</author>
</authors>
<title>Multiword expressions.</title>
<date>2010</date>
<booktitle>In Handbook of Natural Language Processing, Second Edition.</booktitle>
<publisher>CRC Press, Taylor and Francis Group.</publisher>
<contexts>
<context position="7121" citStr="Baldwin and Kim, 2010" startWordPosition="1053" endWordPosition="1056">shtags, usernames, and hyperlinks; capitalization; and a binary feature for tokens that include punctuation. We trained the model using the structured perceptron (Collins, 2002). 2.2 Multiword Expressions We consider multiword expressions (MWEs) of two kinds. The first, proper names, have been widely modeled for information extraction purposes, and even incorporated into parsing (Finkel and Manning, 2009). (An example found in Figure 1 is LA Times.) The second, lexical idioms, have been a “pain in the neck” for many years (Sag et al., 2002) and have recently received shallow treatment in NLP (Baldwin and Kim, 2010; Constant and Sigogne, 2011; Schneider et al., 2014). Constant et al. (2012), Green et al. (2012), Candito and Constant (2014), and Le Roux et al. (2014) considered MWEs in parsing. Figure 1 provides LA Times and All the Rage as examples. Penn Treebank–style syntactic analysis (and dependency representations derived from it) does not give first-class treatment to this phenomenon, though there is precedent for marking multiword lexical units and certain kinds of idiomatic relationships (Hajiˇc et al., 2012; Abeillé et al., 2003).1 We argue that internal analysis of MWEs is not critical for man</context>
</contexts>
<marker>Baldwin, Kim, 2010</marker>
<rawString>Timonthy Baldwin and Su Nam Kim. 2010. Multiword expressions. In Handbook of Natural Language Processing, Second Edition. CRC Press, Taylor and Francis Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Bermingham</author>
<author>Alan F Smeaton</author>
</authors>
<title>Classifying sentiment in microblogs: Is brevity an advantage?</title>
<date>2010</date>
<booktitle>In Proc. of CIKM.</booktitle>
<marker>Bermingham, Smeaton, 2010</marker>
<rawString>Adam Bermingham and Alan F. Smeaton. 2010. Classifying sentiment in microblogs: Is brevity an advantage? In Proc. of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Justin Mott</author>
<author>Colin Warner</author>
<author>Seth Kulick</author>
</authors>
<title>English Web Treebank.</title>
<date>2012</date>
<booktitle>Linguistic Data Consortium. URL http://www.ldc.upenn.edu/Catalog/ catalogEntry.jsp?catalogId=LDC2012T13.</booktitle>
<tech>Technical Report LDC2012T13,</tech>
<contexts>
<context position="1625" citStr="Bies et al., 2012" startWordPosition="230" endWordPosition="233">it is used by people in their everyday lives. These informal texts, which account for ever larger proportions of written content, are of considerable interest to researchers, with applications such as sentiment analysis (Greene and Resnik, 2009; Kouloumpis et al., 2011). However, their often nonstandard content makes them challenging for traditional NLP tools. Among the tools currently available for tweets are a POS tagger (Gimpel et al., 2011; Owoputi et al., 2013) and a named entity recognizer (Ritter et al., 2011)—but not a parser. Important steps have been taken. The English Web Treebank (Bies et al., 2012) represents an annotation effort on web text—which likely lies somewhere between newspaper text and social media messages in formality and care of editing—that was sufficient to support a shared task (Petrov and McDonald, 2012). Foster et al. (2011b) annotated a small test set of tweets to evaluate parsers trained on the Penn Treebank (Marcus et al., 1993), augmented using semi-supervision and in-domain data. Others, such as Soni et al. (2014), have used existing Penn Treebank–trained models on tweets. In this work, we argue that the Penn Treebank approach to annotation—while well-matched to e</context>
</contexts>
<marker>Bies, Mott, Warner, Kulick, 2012</marker>
<rawString>Ann Bies, Justin Mott, Colin Warner, and Seth Kulick. 2012. English Web Treebank. Technical Report LDC2012T13, Linguistic Data Consortium. URL http://www.ldc.upenn.edu/Catalog/ catalogEntry.jsp?catalogId=LDC2012T13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="25498" citStr="Brown et al. (1992)" startWordPosition="4012" endWordPosition="4015">cted xp or xg, and transitions that consider unselected tokens as children, are eliminated. In order to allow the scores to depend on unselected tokens between xc and x′c, we added the binned counts of unselected tokens (mostly punctuation) joint with the word form and POS tag of xp and the POS tag of xc and x′c as features scored in the sibling(p,c,c′) part. The changes discussed above comprise the totality of adaptations we made to the TurboParser algorithm; we refer to them as “parsing adaptations” in the experiments. 4.3 Additional Features Brown clusters. Owoputi et al. (2013) found that Brown et al. (1992) clusters served as excellent features in Twitter POS tagging. Others have found them useful in parsing (Koo et al., 2008) and other tasks (Turian et al., 2010). We therefore follow Koo et al. in incorporating Brown clusters as features, making use of the publicly available Twitter clusters from Owoputi et al.5 We use 4 and 6 bit cluster representations to create features wherever POS tags are used, and full bit strings to create features wherever words were used. Penn Treebank features. A potential danger of our choice to “start from scratch” in developing a dependency parser for Twitter is t</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="5310" citStr="Buchholz and Marsi, 2006" startWordPosition="777" endWordPosition="780">ould not, we argue, be included in the syntactic analysis. By contrast, consider: Got #college admissions questions ? Ask them tonight during #CampusChat I’m looking forward to advice from @collegevisit http://bit.ly/cchOTk Here, both the hashtags and the at-mentioned username are syntactically part of the utterances, while the punctuation and the hyperlink are not. In the example of Figure 1, the unselected tokens include several punctuation tokens and the final token #belieber, which marks the topic of the tweet. Typically, dependency parsing evaluations ignore punctuation token attachment (Buchholz and Marsi, 2006), and we believe it is a waste of annotator (and parser) time to decide how to attach punctuation and other non-syntactic tokens. Ma et al. (2014) recently proposed to treat punctuation as context features rather than dependents, and found that this led to state-of-the-art performance in a transition-based parser. A small adaptation to our graph-based parsing approach, described in §4.2, allows a similar treatment. Our approach to annotation (§3) forces annotators to explicitly select tokens that have a syntactic function. 75.6% tokens were selected by the annotators. Against the annotators’ g</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Candito</author>
<author>Matthieu Constant</author>
</authors>
<title>Strategies for contiguous multiword expression analysis and dependency parsing.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="7248" citStr="Candito and Constant (2014)" startWordPosition="1074" endWordPosition="1077">he model using the structured perceptron (Collins, 2002). 2.2 Multiword Expressions We consider multiword expressions (MWEs) of two kinds. The first, proper names, have been widely modeled for information extraction purposes, and even incorporated into parsing (Finkel and Manning, 2009). (An example found in Figure 1 is LA Times.) The second, lexical idioms, have been a “pain in the neck” for many years (Sag et al., 2002) and have recently received shallow treatment in NLP (Baldwin and Kim, 2010; Constant and Sigogne, 2011; Schneider et al., 2014). Constant et al. (2012), Green et al. (2012), Candito and Constant (2014), and Le Roux et al. (2014) considered MWEs in parsing. Figure 1 provides LA Times and All the Rage as examples. Penn Treebank–style syntactic analysis (and dependency representations derived from it) does not give first-class treatment to this phenomenon, though there is precedent for marking multiword lexical units and certain kinds of idiomatic relationships (Hajiˇc et al., 2012; Abeillé et al., 2003).1 We argue that internal analysis of MWEs is not critical for many downstream applications, and therefore annotators should not expend energy on developing and respecting conventions (or makin</context>
</contexts>
<marker>Candito, Constant, 2014</marker>
<rawString>Marie Candito and Matthieu Constant. 2014. Strategies for contiguous multiword expression analysis and dependency parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higherorder projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="21146" citStr="Carreras, 2007" startWordPosition="3296" endWordPosition="3297">ojectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word with its parent, grandparent, and a sibling, or with its parent and two siblings; Koo and Collins, 2010). For a collection of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all trees in Yx), we will use the following notation. Let g(x,y) = E fs(x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs indicating whether part s is included in the output. A collection of constraints relating zs define the set of feasible</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a higherorder projective dependency parser. In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoeng-Jin Chu</author>
<author>Tseng-Hong Liu</author>
</authors>
<title>On shortest arborescence of a directed graph.</title>
<date>1965</date>
<journal>Scientia Sinica,</journal>
<volume>14</volume>
<issue>10</issue>
<contexts>
<context position="22334" citStr="Chu and Liu, 1965" startWordPosition="3494" endWordPosition="3497">zs define the set of feasible vectors z that correspond to valid outputs and enfore agreement between parts that overlap. Many different versions of these constraints have been studied (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009, 2010). A key attraction of TurboParser is that many overlapping parts can be handled, making use of separate combinatorial algorithms for efficiently handling subsets of constraints. For example, the constraints that force z to encode a valid tree can be exploited within the framework by making calls 1005 to classic arborescence algorithms (Chu and Liu, 1965; Edmonds, 1967). As a result, when describing modifications to TurboParser, we need only to explain additional constraints and features imposed on parts. 4.2 Adapted Parse Parts The first collection of parts we adapt are simple arcs, each consisting of an ordered pair of indices of words in x; arc(p,c) corresponds to the attachment of xc as a child of xp (iff zarc(p,c) =1). Our representation explicitly excludes some tokens from being part of the syntactic analysis (§2.1); to handle this, we constrain zarc(i,j) = 0 whenever xi or xj is excluded. The implication is that excluded tokens are sti</context>
</contexts>
<marker>Chu, Liu, 1965</marker>
<rawString>Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On shortest arborescence of a directed graph. Scientia Sinica, 14(10):1396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for Hidden Markov Models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="6677" citStr="Collins, 2002" startWordPosition="982" endWordPosition="983">gs) and 95.1% (with automatic POS tags) average accuracy in the task of selecting tokens with a syntactic function in a ten-fold cross-validation experiment. To take context into account, we developed a first-order sequence model and found that it achieves 97.4% average accuracy (again, ten-fold cross-validated) with either gold-standard or automatic POS tags. Features include POS; shape features that recognize the retweet marker, hashtags, usernames, and hyperlinks; capitalization; and a binary feature for tokens that include punctuation. We trained the model using the structured perceptron (Collins, 2002). 2.2 Multiword Expressions We consider multiword expressions (MWEs) of two kinds. The first, proper names, have been widely modeled for information extraction purposes, and even incorporated into parsing (Finkel and Manning, 2009). (An example found in Figure 1 is LA Times.) The second, lexical idioms, have been a “pain in the neck” for many years (Sag et al., 2002) and have recently received shallow treatment in NLP (Baldwin and Kim, 2010; Constant and Sigogne, 2011; Schneider et al., 2014). Constant et al. (2012), Green et al. (2012), Candito and Constant (2014), and Le Roux et al. (2014) c</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for Hidden Markov Models: theory and experiments with perceptron algorithms. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthieu Constant</author>
<author>Anthony Sigogne</author>
</authors>
<title>MWUaware part-of-speech tagging with a CRF model and lexical resources.</title>
<date>2011</date>
<booktitle>In Proc. of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World.</booktitle>
<contexts>
<context position="7149" citStr="Constant and Sigogne, 2011" startWordPosition="1057" endWordPosition="1061">hyperlinks; capitalization; and a binary feature for tokens that include punctuation. We trained the model using the structured perceptron (Collins, 2002). 2.2 Multiword Expressions We consider multiword expressions (MWEs) of two kinds. The first, proper names, have been widely modeled for information extraction purposes, and even incorporated into parsing (Finkel and Manning, 2009). (An example found in Figure 1 is LA Times.) The second, lexical idioms, have been a “pain in the neck” for many years (Sag et al., 2002) and have recently received shallow treatment in NLP (Baldwin and Kim, 2010; Constant and Sigogne, 2011; Schneider et al., 2014). Constant et al. (2012), Green et al. (2012), Candito and Constant (2014), and Le Roux et al. (2014) considered MWEs in parsing. Figure 1 provides LA Times and All the Rage as examples. Penn Treebank–style syntactic analysis (and dependency representations derived from it) does not give first-class treatment to this phenomenon, though there is precedent for marking multiword lexical units and certain kinds of idiomatic relationships (Hajiˇc et al., 2012; Abeillé et al., 2003).1 We argue that internal analysis of MWEs is not critical for many downstream applications, a</context>
</contexts>
<marker>Constant, Sigogne, 2011</marker>
<rawString>Matthieu Constant and Anthony Sigogne. 2011. MWUaware part-of-speech tagging with a CRF model and lexical resources. In Proc. of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthieu Constant</author>
<author>Anthony Sigogne</author>
<author>Patrick Watrin</author>
</authors>
<title>Discriminative strategies to integrate multiword expression recognition and parsing.</title>
<date>2012</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="7198" citStr="Constant et al. (2012)" startWordPosition="1066" endWordPosition="1069">tokens that include punctuation. We trained the model using the structured perceptron (Collins, 2002). 2.2 Multiword Expressions We consider multiword expressions (MWEs) of two kinds. The first, proper names, have been widely modeled for information extraction purposes, and even incorporated into parsing (Finkel and Manning, 2009). (An example found in Figure 1 is LA Times.) The second, lexical idioms, have been a “pain in the neck” for many years (Sag et al., 2002) and have recently received shallow treatment in NLP (Baldwin and Kim, 2010; Constant and Sigogne, 2011; Schneider et al., 2014). Constant et al. (2012), Green et al. (2012), Candito and Constant (2014), and Le Roux et al. (2014) considered MWEs in parsing. Figure 1 provides LA Times and All the Rage as examples. Penn Treebank–style syntactic analysis (and dependency representations derived from it) does not give first-class treatment to this phenomenon, though there is precedent for marking multiword lexical units and certain kinds of idiomatic relationships (Hajiˇc et al., 2012; Abeillé et al., 2003).1 We argue that internal analysis of MWEs is not critical for many downstream applications, and therefore annotators should not expend energy </context>
</contexts>
<marker>Constant, Sigogne, Watrin, 2012</marker>
<rawString>Matthieu Constant, Anthony Sigogne, and Patrick Watrin. 2012. Discriminative strategies to integrate multiword expression recognition and parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford typed dependencies repre-</title>
<date>2008</date>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies repre-</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>John Blitzer</author>
</authors>
<title>Partha Pratim Talukdar, Kuzman Ganchev, Joao Graca, and Fernando Pereira.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLPCoNLL.</booktitle>
<marker>Dredze, Blitzer, 2007</marker>
<rawString>Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuzman Ganchev, Joao Graca, and Fernando Pereira. 2007. Frustratingly hard domain adaptation for dependency parsing. In Proc. of EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jack Edmonds</author>
</authors>
<title>Optimum branchings.</title>
<date>1967</date>
<journal>Journal of Research of the National Bureau of Standards B,</journal>
<pages>71--233</pages>
<contexts>
<context position="22350" citStr="Edmonds, 1967" startWordPosition="3498" endWordPosition="3499">f feasible vectors z that correspond to valid outputs and enfore agreement between parts that overlap. Many different versions of these constraints have been studied (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009, 2010). A key attraction of TurboParser is that many overlapping parts can be handled, making use of separate combinatorial algorithms for efficiently handling subsets of constraints. For example, the constraints that force z to encode a valid tree can be exploited within the framework by making calls 1005 to classic arborescence algorithms (Chu and Liu, 1965; Edmonds, 1967). As a result, when describing modifications to TurboParser, we need only to explain additional constraints and features imposed on parts. 4.2 Adapted Parse Parts The first collection of parts we adapt are simple arcs, each consisting of an ordered pair of indices of words in x; arc(p,c) corresponds to the attachment of xc as a child of xp (iff zarc(p,c) =1). Our representation explicitly excludes some tokens from being part of the syntactic analysis (§2.1); to handle this, we constrain zarc(i,j) = 0 whenever xi or xj is excluded. The implication is that excluded tokens are still “visible” to </context>
</contexts>
<marker>Edmonds, 1967</marker>
<rawString>Jack Edmonds. 1967. Optimum branchings. Journal of Research of the National Bureau of Standards B, 71(233-240):160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
</authors>
<title>What to do about bad language on the internet.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="2588" citStr="Eisenstein, 2013" startWordPosition="379" endWordPosition="380">t al., 1993), augmented using semi-supervision and in-domain data. Others, such as Soni et al. (2014), have used existing Penn Treebank–trained models on tweets. In this work, we argue that the Penn Treebank approach to annotation—while well-matched to edited genres like newswire—is poorly suited to more informal genres. Our starting point is that rapid, small-scale annotation efforts performed by imperfectly-trained annotators should provide enough evidence to train an effective parser. We see this starting point as a necessity, given observations about the rapidly changing nature of tweets (Eisenstein, 2013), the attested difficulties of domain adaptation for parsing (Dredze et al., 2007), and the expense of creating Penn Treebank–style annotations (Marcus et al., 1993). This paper presents TWEEBOPARSER, the first syntactic dependency parser designed explicitly for English tweets. We developed this parser following current best practices in empirical NLP: we annotate a corpus (TWEEBANK) and train the parameters of a statistical parsing algorithm. Our research contributions include: • a survey of key challenges posed by syntactic analysis of tweets (by humans or machines) and decisions motivated b</context>
</contexts>
<marker>Eisenstein, 2013</marker>
<rawString>Jacob Eisenstein. 2013. What to do about bad language on the internet. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="20588" citStr="Eisner, 1996" startWordPosition="3217" endWordPosition="3218"> g is used in parsing algorithms that seek to find: parse∗(x) = argmax wTg(x,y) (1) y∈Yx The score is parameterized by a vector w of weights, which are learned from data (most commonly using MIRA, McDonald et al., 2005a). The decomposition of the features into local “parts” is a critical choice affecting the computational difficulty of solving Eq. 1. The most aggressive decomposition leads to an “arc-factored” or “first-order” model, which permits exact, efficient solution of Eq. 1 using spanning tree algorithms (McDonald et al., 2005b) or, with a projectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word wi</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Elming</author>
<author>Anders Johannsen</author>
<author>Sigrid Klerke</author>
<author>Emanuele Lapponi</author>
<author>Héctor Martínez Alonso</author>
<author>Anders Søgaard</author>
</authors>
<title>Down-stream effects of treeto-dependency conversions.</title>
<date>2013</date>
<booktitle>In Proc. of NAACLHLT.</booktitle>
<contexts>
<context position="16041" citStr="Elming et al., 2013" startWordPosition="2487" endWordPosition="2490">e treated like other attachments. 3.3 Annotation Conventions A wide range of dependency conventions are in use; in many cases these are conversion conventions specifying how dependency trees can be derived from phrase-structure trees. For English, the most popular are due to Yamada and Matsumoto (2003) and de Marneffe and Manning (2008), known as “Yamada-Matsumoto” (YM) and “Stanford” dependencies, respectively. The main differences between them are in whether the auxiliary is the parent of the main verb (or vice versa) and whether the preposition or its argument heads a prepositional phrase (Elming et al., 2013). A full discussion of our annotation conventions is out of scope. We largely followed the conventions suggested by Schneider et al. (2013), which in turn are close to those of YM. Auxiliary verbs are parents of main verbs, and prepositions are parents of their arguments. The key differences from YM are in coordination structures (discussed in §3.2; YM makes the first conjunct the head) and possessive structures, in which the possessor is the child of the clitic, which is the child of the semantic head, e.g.,the &gt; king &gt; &apos;s &gt; horses. 3.4 Intrinsic Quality Our approach to developing this initia</context>
</contexts>
<marker>Elming, Johannsen, Klerke, Lapponi, Alonso, Søgaard, 2013</marker>
<rawString>Jakob Elming, Anders Johannsen, Sigrid Klerke, Emanuele Lapponi, Héctor Martínez Alonso, and Anders Søgaard. 2013. Down-stream effects of treeto-dependency conversions. In Proc. of NAACLHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint parsing and named entity recognition.</title>
<date>2009</date>
<booktitle>In Proc ofACL-HLT.</booktitle>
<contexts>
<context position="6908" citStr="Finkel and Manning, 2009" startWordPosition="1013" endWordPosition="1016">nce model and found that it achieves 97.4% average accuracy (again, ten-fold cross-validated) with either gold-standard or automatic POS tags. Features include POS; shape features that recognize the retweet marker, hashtags, usernames, and hyperlinks; capitalization; and a binary feature for tokens that include punctuation. We trained the model using the structured perceptron (Collins, 2002). 2.2 Multiword Expressions We consider multiword expressions (MWEs) of two kinds. The first, proper names, have been widely modeled for information extraction purposes, and even incorporated into parsing (Finkel and Manning, 2009). (An example found in Figure 1 is LA Times.) The second, lexical idioms, have been a “pain in the neck” for many years (Sag et al., 2002) and have recently received shallow treatment in NLP (Baldwin and Kim, 2010; Constant and Sigogne, 2011; Schneider et al., 2014). Constant et al. (2012), Green et al. (2012), Candito and Constant (2014), and Le Roux et al. (2014) considered MWEs in parsing. Figure 1 provides LA Times and All the Rage as examples. Penn Treebank–style syntactic analysis (and dependency representations derived from it) does not give first-class treatment to this phenomenon, tho</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2009. Joint parsing and named entity recognition. In Proc ofACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Özlem Qetinoglu</author>
<author>Joachim Wagner</author>
<author>Joseph Le Roux</author>
<author>Stephen Hogan</author>
<author>Joakim Nivre</author>
<author>Deirdre Hogan</author>
<author>Josef van Genabith</author>
</authors>
<title>hardtoparse: POS tagging and parsing the Twitterverse.</title>
<date>2011</date>
<booktitle>In Proc. of AAAI Workshop on Analyzing Microtext.</booktitle>
<marker>Foster, Qetinoglu, Wagner, Le Roux, Hogan, Nivre, Hogan, van Genabith, 2011</marker>
<rawString>Jennifer Foster, Özlem Qetinoglu, Joachim Wagner, Joseph Le Roux, Stephen Hogan, Joakim Nivre, Deirdre Hogan, and Josef van Genabith. 2011a. #hardtoparse: POS tagging and parsing the Twitterverse. In Proc. of AAAI Workshop on Analyzing Microtext.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Özlem Qetino˘glu</author>
<author>Joachim Wagner</author>
<author>Joseph Le Roux</author>
<author>Joakim Nivre</author>
<author>Deirdre Hogan</author>
<author>Josef van Genabith</author>
</authors>
<title>From news to comment: resources and benchmarks for parsing the language of Web 2.0.</title>
<date>2011</date>
<booktitle>In Proc. of IJCNLP.</booktitle>
<marker>Foster, Qetino˘glu, Wagner, Le Roux, Nivre, Hogan, van Genabith, 2011</marker>
<rawString>Jennifer Foster, Özlem Qetino˘glu, Joachim Wagner, Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and Josef van Genabith. 2011b. From news to comment: resources and benchmarks for parsing the language of Web 2.0. In Proc. of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for Twitter: annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proc. of ACL-HLT.</booktitle>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for Twitter: annotation, features, and experiments. In Proc. of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing models for identifying multiword expressions.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>1</issue>
<marker>Green, de Marneffe, Manning, 2012</marker>
<rawString>Spence Green, Marie-Catherine de Marneffe, and Christopher D. Manning. 2012. Parsing models for identifying multiword expressions. Computational Linguistics, 39(1):195–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Greene</author>
<author>Philip Resnik</author>
</authors>
<title>Syntactic packaging and implicit sentiment.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="1251" citStr="Greene and Resnik, 2009" startWordPosition="170" endWordPosition="173">t the parser achieves over 80% unlabeled attachment accuracy on our new, high-quality test set and measure the benefit of our contributions. Our dataset and parser can be found at http://www.ark.cs.cmu.edu/TweetNLP. 1 Introduction In contrast to the edited, standardized language of traditional publications such as news reports, social media text closely represents language as it is used by people in their everyday lives. These informal texts, which account for ever larger proportions of written content, are of considerable interest to researchers, with applications such as sentiment analysis (Greene and Resnik, 2009; Kouloumpis et al., 2011). However, their often nonstandard content makes them challenging for traditional NLP tools. Among the tools currently available for tweets are a POS tagger (Gimpel et al., 2011; Owoputi et al., 2013) and a named entity recognizer (Ritter et al., 2011)—but not a parser. Important steps have been taken. The English Web Treebank (Bies et al., 2012) represents an annotation effort on web text—which likely lies somewhere between newspaper text and social media messages in formality and care of editing—that was sufficient to support a shared task (Petrov and McDonald, 2012</context>
</contexts>
<marker>Greene, Resnik, 2009</marker>
<rawString>Stephan Greene and Philip Resnik. 2009. Syntactic packaging and implicit sentiment. In Proc. of NAACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Eva Hajiˇcová</author>
<author>Jarmila Panevová</author>
<author>Petr Sgall</author>
<author>Silvie Cinková</author>
<author>Eva Fuˇcíková</author>
<author>Marie Mikulová</author>
<author>Petr Pajas</author>
<author>Jan Popelka</author>
<author>Jiˇrí Semeck`y</author>
<author>Jana Šindlerová</author>
<author>Jan Štˇepánek</author>
<author>Josef Toman</author>
</authors>
<title>Zdeˇnka Urešová, and Zdenˇek Žabokrtský.</title>
<date>2012</date>
<booktitle>Prague Czech-English Dependency Treebank 2.0. Technical Report LDC2012T08, Linguistic Data Consortium. URL http://www.ldc.upenn.edu/Catalog/ catalogEntry.jsp?catalogId=LDC2012T08.</booktitle>
<marker>Hajiˇc, Hajiˇcová, Panevová, Sgall, Cinková, Fuˇcíková, Mikulová, Pajas, Popelka, Semeck`y, Šindlerová, Štˇepánek, Toman, 2012</marker>
<rawString>Jan Hajiˇc, Eva Hajiˇcová, Jarmila Panevová, Petr Sgall, Silvie Cinková, Eva Fuˇcíková, Marie Mikulová, Petr Pajas, Jan Popelka, Jiˇrí Semeck`y, Jana Šindlerová, Jan Štˇepánek, Josef Toman, Zdeˇnka Urešová, and Zdenˇek Žabokrtský. 2012. Prague Czech-English Dependency Treebank 2.0. Technical Report LDC2012T08, Linguistic Data Consortium. URL http://www.ldc.upenn.edu/Catalog/ catalogEntry.jsp?catalogId=LDC2012T08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Learning Probabilistic Lexicalized Grammars for Natural Language Processing.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="14631" citStr="Hwa, 2001" startWordPosition="2262" endWordPosition="2263">o utterances whose roots are marked by the ** symbol. Schneider et al.’s GFL offers some additional features, only some of which we made use of in this project. One important feature allows an annotator to leave the parse underspecified in some ways. We allowed our annotators to make use of this feature; however, we excluded from our training and testing data any parse that was incomplete (i.e., any parse that contained multiple disconnected fragments with no explicit root, excluding unselected tokens). Learning to parse from incomplete annotations is a fascinating topic explored in the past (Hwa, 2001; Pereira and Schabes, 1992) and, in the case of tweets, left for future work. An important feature of GFL that we did use is special notation for coordination structures. For the coordination structure in Figure 1, for example, the notation is: $a :: {T want} :: {&amp;} where $a creates a new node in the parse tree as it is visualized for the annotator, and this new node attaches to the syntactic parent of the conjoined structure, avoiding the classic forced choice between coordinator and conjunct as parent. For learning to parse, we transform GFL’s coordination structures into specially-labeled </context>
</contexts>
<marker>Hwa, 2001</marker>
<rawString>Rebecca Hwa. 2001. Learning Probabilistic Lexicalized Grammars for Natural Language Processing. Ph.D. thesis, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
</authors>
<title>Training parsers on incompatible treebanks.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="28957" citStr="Johansson (2013)" startWordPosition="4571" endWordPosition="4572">ually modified the automatically converted trees by: (a) Performing token selection (§2.1) to remove the tokens which have no syntactic function. (b) Grouping MWEs (§2.2). Here, most of the MWEs are named entities such as Manchester United. (c) Attaching the roots of the utterance in tweets to the “wall” symbol (§2.3).8 6Stacking is a machine learning method where the predictions of one model are used to create features for another. The second model may be from a different family. Stacking has been found successful for dependency parsing by Nivre and McDonald (2008) and Martins et al. (2008). Johansson (2013) describes further advances that use path features. 7http://nlp.cs.lth.se/software/treebank_ converter; run with -rightBranching=false -coordStructure=prague -prepAsHead=true -posAsHead=true -subAsHead=true -imAsHead=true -whAsHead=false. 8This was infrequent; their annotations split most multiTRAIN TEST-NEW TEST-FOSTER tweets 717 201 &lt; 250† unique tweets 569 201 &lt; 250† tokens 9,310 2,839 2,841 selected tokens 7,015 2,158 2,366 types 3,566 1,461 1,230 utterances 1,473 429 337 multi-root tweets 398 123 60 MWEs 387 78 109 Table 1: Statistics of our datasets. (A tweet with k annotations in the tr</context>
</contexts>
<marker>Johansson, 2013</marker>
<rawString>Richard Johansson. 2013. Training parsers on incompatible treebanks. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="9620" citStr="Johnson (1998)" startWordPosition="1443" endWordPosition="1444">light token É #belieber selection (gray; §2.1), multiword expressions (blue; §2.2), multiple roots (red; §2.3), coordination (dotted arcs, green; §3.2), and noun phrase internal structure (orange; §2.4). The internal structure of multiword expressions (dashed arcs below the sentence) was predicted automatically by a parser, as described in §2.2. tractive property from the perspective of semantic processing. To allow training a fairly conventional statistical dependency parser from these annotations, we find it expedient to apply an automatic conversion to the MWE annotations, in the spirit of Johnson (1998). We apply an existing dependency parser, the first-order TurboParser (Martins et al., 2009) trained on the Penn Treebank, to parse each MWE independently, assigning structures like those for LA Times and All the Rage in Figure 1. Arcs involving the MWE in the annotation are then reconnected to the MWE-internal root, so that the resulting tree respects the original tokenization. The MWE-internal arcs are given a special label so that the transformation can be reversed and MWEs reconstructed from parser output. 2.3 Multiple Roots For news text such as that found in the Penn Treebank, sentence s</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingpeng Kong</author>
<author>Noah A Smith</author>
</authors>
<title>An empirical comparison of parsing methods for Stanford dependencies.</title>
<date>2014</date>
<journal>ArXiv:1404.4314.</journal>
<contexts>
<context position="19086" citStr="Kong and Smith, 2014" startWordPosition="2974" endWordPosition="2977"> error. Qualitatively, we found several unsurprising sources of error or disagreement, including embedded/subordinate clauses, subject-auxiliary inversion, predeterminers, and adverbial modifiers following a modal/auxiliary verb and a main verb. Clarification of the conventions, or even explicit rule-based checking in the validation step, might lead to quality improvements in further annotation efforts. 4 Parsing Algorithm For parsing, we start with TurboParser, which is open-source and has been found to perform well on a range of parsing problems in different languages (Martins et al., 2013; Kong and Smith, 2014). The underlying model allows for flexible incorporation of new features and changes to specification in the output space. We briefly review the key ideas in TurboParser (§4.1), then describe decoder modifications required for our problem (§4.2). We then discuss features we added to TurboParser (§4.3). 4.1 TurboParser Let an input sentence be denoted by x and the set of possible dependency parses for x be denoted by Yx. A generic linear scoring function based on a 4What we deemed major errors included, for example, an incorrect dependency relation between an auxiliary verb and the main verb (l</context>
</contexts>
<marker>Kong, Smith, 2014</marker>
<rawString>Lingpeng Kong and Noah A. Smith. 2014. An empirical comparison of parsing methods for Stanford dependencies. ArXiv:1404.4314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="25620" citStr="Koo et al., 2008" startWordPosition="4033" endWordPosition="4036">depend on unselected tokens between xc and x′c, we added the binned counts of unselected tokens (mostly punctuation) joint with the word form and POS tag of xp and the POS tag of xc and x′c as features scored in the sibling(p,c,c′) part. The changes discussed above comprise the totality of adaptations we made to the TurboParser algorithm; we refer to them as “parsing adaptations” in the experiments. 4.3 Additional Features Brown clusters. Owoputi et al. (2013) found that Brown et al. (1992) clusters served as excellent features in Twitter POS tagging. Others have found them useful in parsing (Koo et al., 2008) and other tasks (Turian et al., 2010). We therefore follow Koo et al. in incorporating Brown clusters as features, making use of the publicly available Twitter clusters from Owoputi et al.5 We use 4 and 6 bit cluster representations to create features wherever POS tags are used, and full bit strings to create features wherever words were used. Penn Treebank features. A potential danger of our choice to “start from scratch” in developing a dependency parser for Twitter is that the resulting annotation conventions, data, and desired output are very different from dependency parses derived from </context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="21290" citStr="Koo and Collins, 2010" startWordPosition="3318" endWordPosition="3321"> approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word with its parent, grandparent, and a sibling, or with its parent and two siblings; Koo and Collins, 2010). For a collection of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all trees in Yx), we will use the following notation. Let g(x,y) = E fs(x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs indicating whether part s is included in the output. A collection of constraints relating zs define the set of feasible vectors z that correspond to valid outputs and enfore agreement between parts that overlap. Many different versions of these constraints have b</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="24682" citStr="Koo et al., 2010" startWordPosition="3880" endWordPosition="3883">milar scores to those of our token selection sequence model (§2.1) could be integrated into parsing. Given, however, that the sequence model achieves over 97% accuracy, and that perfect token selection would gain only 0.1–1% in parsing accuracy (reported in §5.5), we leave this option for future work as well. For first-order models, the above change is all that is necessary. For second- and third-order models, TurboParser makes use of head automata, in particular “grand-sibling head automata” that assign scores to word tuples of xg, its child xp, and two of xp’s adjacent children, xc and x′c (Koo et al., 2010). The second-order models in our experiments include parts for sibling(p,c,c′) and grandparent(p,c,g) and use the grand-sibling head automaton to reason about these together. Automata for an unselected xp or xg, and transitions that consider unselected tokens as children, are eliminated. In order to allow the scores to depend on unselected tokens between xc and x′c, we added the binned counts of unselected tokens (mostly punctuation) joint with the word form and POS tag of xp and the POS tag of xc and x′c as features scored in the sibling(p,c,c′) part. The changes discussed above comprise the </context>
<context position="37940" citStr="Koo et al., 2010" startWordPosition="6004" endWordPosition="6007">ng algorithm, and our stackinginspired use of Penn Treebank data contributes in both cases, quite a lot on TEST-FOSTER (unsurprisingly given that test set’s similarity to the Penn Treebank). More surprisingly, we find that Brown cluster features do not consistently improve performance, at least not as instantiated here, with our small training set. 6 Conclusion We described TWEEBOPARSER, a dependency parser for English tweets that achieves over 80% unlabeled attachment score on a new, high-quality test set. This is on par with state-of-the-art reported results for news text in Turkish (77.6%; Koo et al., 2010) and Arabic (81.1%; Martins et al., 2011). Our contributions include important steps taken to build the parser: a consideration of the challenges of parsing tweets that informed our annotation process, the resulting new TWEEBANK corpus, adaptations to a statistical parsing algorithm, a new approach to exploiting data in a better-resourced domain (the Penn Treebank), and experimental analysis of the decisions we made. The dataset and parser can be found at http://www.ark.cs.cmu.edu/TweetNLP. Acknowledgments The authors thank the anonymous reviewers and André Martins, Yanchuan Sim, Wang Ling, Mi</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efthymios Kouloumpis</author>
<author>Theresa Wilson</author>
<author>Johanna Moore</author>
</authors>
<title>Twitter sentiment analysis: The good the bad and the OMG!</title>
<date>2011</date>
<booktitle>In Proc. of ICWSM.</booktitle>
<contexts>
<context position="1277" citStr="Kouloumpis et al., 2011" startWordPosition="174" endWordPosition="177">r 80% unlabeled attachment accuracy on our new, high-quality test set and measure the benefit of our contributions. Our dataset and parser can be found at http://www.ark.cs.cmu.edu/TweetNLP. 1 Introduction In contrast to the edited, standardized language of traditional publications such as news reports, social media text closely represents language as it is used by people in their everyday lives. These informal texts, which account for ever larger proportions of written content, are of considerable interest to researchers, with applications such as sentiment analysis (Greene and Resnik, 2009; Kouloumpis et al., 2011). However, their often nonstandard content makes them challenging for traditional NLP tools. Among the tools currently available for tweets are a POS tagger (Gimpel et al., 2011; Owoputi et al., 2013) and a named entity recognizer (Ritter et al., 2011)—but not a parser. Important steps have been taken. The English Web Treebank (Bies et al., 2012) represents an annotation effort on web text—which likely lies somewhere between newspaper text and social media messages in formality and care of editing—that was sufficient to support a shared task (Petrov and McDonald, 2012). Foster et al. (2011b) a</context>
</contexts>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. 2011. Twitter sentiment analysis: The good the bad and the OMG! In Proc. of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Le Roux</author>
<author>Matthieu Constant</author>
<author>Antoine Rozenknop</author>
</authors>
<title>Syntactic parsing and compound recognition via dual decomposition: application to French.</title>
<date>2014</date>
<booktitle>In Proc. of COLING.</booktitle>
<marker>Le Roux, Constant, Rozenknop, 2014</marker>
<rawString>Joseph Le Roux, Matthieu Constant, and Antoine Rozenknop. 2014. Syntactic parsing and compound recognition via dual decomposition: application to French. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ji Ma</author>
<author>Yue Zhang</author>
<author>Jingbo Zhu</author>
</authors>
<title>Punctuation processing for projective dependency parsing.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="5456" citStr="Ma et al. (2014)" startWordPosition="804" endWordPosition="807">’m looking forward to advice from @collegevisit http://bit.ly/cchOTk Here, both the hashtags and the at-mentioned username are syntactically part of the utterances, while the punctuation and the hyperlink are not. In the example of Figure 1, the unselected tokens include several punctuation tokens and the final token #belieber, which marks the topic of the tweet. Typically, dependency parsing evaluations ignore punctuation token attachment (Buchholz and Marsi, 2006), and we believe it is a waste of annotator (and parser) time to decide how to attach punctuation and other non-syntactic tokens. Ma et al. (2014) recently proposed to treat punctuation as context features rather than dependents, and found that this led to state-of-the-art performance in a transition-based parser. A small adaptation to our graph-based parsing approach, described in §4.2, allows a similar treatment. Our approach to annotation (§3) forces annotators to explicitly select tokens that have a syntactic function. 75.6% tokens were selected by the annotators. Against the annotators’ gold standard, we found that a simple rule-based filter for usernames, hashtags, punctuation, and retweet tokens achieves 95.2% (with gold-standard</context>
<context position="23597" citStr="Ma et al. (2014)" startWordPosition="3706" endWordPosition="3709">other edges. For example, some conventional first-order features consider the tokens occurring between a parent and child. Even if a token plays no syntactic role of its own, it might still be informative about the syntactic relationships among other tokens. We note three alternative methods: 1. We might remove all unselected tokens from x before running the parser. In §5.6 we find this method to fare 1.7–2.3% worse than our modified decoding algorithm. 2. We might remove unselected tokens but use them to define new features, so that they still serve as evidence. This is the approach taken by Ma et al. (2014) for punctuation. We judge our simple modification to the decoding algorithm to be more expedient, and leave the translation of existing context-word features into that framework for future exploration. 3. We might incorporate the token selection decisions into the parser, performing joint inference for selection and parsing. The AD3 algorithm within TurboParser is well-suited to this kind of extension: z-variables for each token’s selection could be added, and similar scores to those of our token selection sequence model (§2.1) could be integrated into parsing. Given, however, that the sequen</context>
</contexts>
<marker>Ma, Zhang, Zhu, 2014</marker>
<rawString>Ji Ma, Yue Zhang, and Jingbo Zhu. 2014. Punctuation processing for projective dependency parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational linguistics,</title>
<date>1993</date>
<contexts>
<context position="1983" citStr="Marcus et al., 1993" startWordPosition="288" endWordPosition="291">tools. Among the tools currently available for tweets are a POS tagger (Gimpel et al., 2011; Owoputi et al., 2013) and a named entity recognizer (Ritter et al., 2011)—but not a parser. Important steps have been taken. The English Web Treebank (Bies et al., 2012) represents an annotation effort on web text—which likely lies somewhere between newspaper text and social media messages in formality and care of editing—that was sufficient to support a shared task (Petrov and McDonald, 2012). Foster et al. (2011b) annotated a small test set of tweets to evaluate parsers trained on the Penn Treebank (Marcus et al., 1993), augmented using semi-supervision and in-domain data. Others, such as Soni et al. (2014), have used existing Penn Treebank–trained models on tweets. In this work, we argue that the Penn Treebank approach to annotation—while well-matched to edited genres like newswire—is poorly suited to more informal genres. Our starting point is that rapid, small-scale annotation efforts performed by imperfectly-trained annotators should provide enough evidence to train an effective parser. We see this starting point as a necessity, given observations about the rapidly changing nature of tweets (Eisenstein, </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>André F T Martins</author>
<author>Miguel Almeida</author>
<author>Noah A Smith</author>
</authors>
<title>Turning on the turbo: Fast third-order non-projective turbo parsers.</title>
<date>2013</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="19063" citStr="Martins et al., 2013" startWordPosition="2970" endWordPosition="2973">ions contain a serious error. Qualitatively, we found several unsurprising sources of error or disagreement, including embedded/subordinate clauses, subject-auxiliary inversion, predeterminers, and adverbial modifiers following a modal/auxiliary verb and a main verb. Clarification of the conventions, or even explicit rule-based checking in the validation step, might lead to quality improvements in further annotation efforts. 4 Parsing Algorithm For parsing, we start with TurboParser, which is open-source and has been found to perform well on a range of parsing problems in different languages (Martins et al., 2013; Kong and Smith, 2014). The underlying model allows for flexible incorporation of new features and changes to specification in the output space. We briefly review the key ideas in TurboParser (§4.1), then describe decoder modifications required for our problem (§4.2). We then discuss features we added to TurboParser (§4.3). 4.1 TurboParser Let an input sentence be denoted by x and the set of possible dependency parses for x be denoted by Yx. A generic linear scoring function based on a 4What we deemed major errors included, for example, an incorrect dependency relation between an auxiliary ve</context>
</contexts>
<marker>Martins, Almeida, Smith, 2013</marker>
<rawString>André F.T. Martins, Miguel Almeida, and Noah A. Smith. 2013. Turning on the turbo: Fast third-order non-projective turbo parsers. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>André F T Martins</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Stacking dependency parsers.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="28939" citStr="Martins et al. (2008)" startWordPosition="4567" endWordPosition="4570">perienced annotator manually modified the automatically converted trees by: (a) Performing token selection (§2.1) to remove the tokens which have no syntactic function. (b) Grouping MWEs (§2.2). Here, most of the MWEs are named entities such as Manchester United. (c) Attaching the roots of the utterance in tweets to the “wall” symbol (§2.3).8 6Stacking is a machine learning method where the predictions of one model are used to create features for another. The second model may be from a different family. Stacking has been found successful for dependency parsing by Nivre and McDonald (2008) and Martins et al. (2008). Johansson (2013) describes further advances that use path features. 7http://nlp.cs.lth.se/software/treebank_ converter; run with -rightBranching=false -coordStructure=prague -prepAsHead=true -posAsHead=true -subAsHead=true -imAsHead=true -whAsHead=false. 8This was infrequent; their annotations split most multiTRAIN TEST-NEW TEST-FOSTER tweets 717 201 &lt; 250† unique tweets 569 201 &lt; 250† tokens 9,310 2,839 2,841 selected tokens 7,015 2,158 2,366 types 3,566 1,461 1,230 utterances 1,473 429 337 multi-root tweets 398 123 60 MWEs 387 78 109 Table 1: Statistics of our datasets. (A tweet with k ann</context>
</contexts>
<marker>Martins, Das, Smith, Xing, 2008</marker>
<rawString>André F.T. Martins, Dipanjan Das, Noah A. Smith, and Eric P. Xing. 2008. Stacking dependency parsers. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>André F T Martins</author>
<author>Noah A Smith</author>
<author>Pedro M Q Aguiar</author>
<author>Mário A T Figueiredo</author>
</authors>
<title>Dual decomposition with many overlapping components.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="21031" citStr="Martins et al., 2011" startWordPosition="3277" endWordPosition="3280">l, which permits exact, efficient solution of Eq. 1 using spanning tree algorithms (McDonald et al., 2005b) or, with a projectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word with its parent, grandparent, and a sibling, or with its parent and two siblings; Koo and Collins, 2010). For a collection of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all trees in Yx), we will use the following notation. Let g(x,y) = E fs(x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs indi</context>
<context position="37981" citStr="Martins et al., 2011" startWordPosition="6011" endWordPosition="6014">d use of Penn Treebank data contributes in both cases, quite a lot on TEST-FOSTER (unsurprisingly given that test set’s similarity to the Penn Treebank). More surprisingly, we find that Brown cluster features do not consistently improve performance, at least not as instantiated here, with our small training set. 6 Conclusion We described TWEEBOPARSER, a dependency parser for English tweets that achieves over 80% unlabeled attachment score on a new, high-quality test set. This is on par with state-of-the-art reported results for news text in Turkish (77.6%; Koo et al., 2010) and Arabic (81.1%; Martins et al., 2011). Our contributions include important steps taken to build the parser: a consideration of the challenges of parsing tweets that informed our annotation process, the resulting new TWEEBANK corpus, adaptations to a statistical parsing algorithm, a new approach to exploiting data in a better-resourced domain (the Penn Treebank), and experimental analysis of the decisions we made. The dataset and parser can be found at http://www.ark.cs.cmu.edu/TweetNLP. Acknowledgments The authors thank the anonymous reviewers and André Martins, Yanchuan Sim, Wang Ling, Michael Mordowanec, and Alexander Rush for </context>
</contexts>
<marker>Martins, Smith, Aguiar, Figueiredo, 2011</marker>
<rawString>André F.T. Martins, Noah A. Smith, Pedro M.Q. Aguiar, and Mário A.T. Figueiredo. 2011. Dual decomposition with many overlapping components. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>André F T Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proc. of ACLIJCNLP.</booktitle>
<contexts>
<context position="9712" citStr="Martins et al., 2009" startWordPosition="1454" endWordPosition="1457">ultiple roots (red; §2.3), coordination (dotted arcs, green; §3.2), and noun phrase internal structure (orange; §2.4). The internal structure of multiword expressions (dashed arcs below the sentence) was predicted automatically by a parser, as described in §2.2. tractive property from the perspective of semantic processing. To allow training a fairly conventional statistical dependency parser from these annotations, we find it expedient to apply an automatic conversion to the MWE annotations, in the spirit of Johnson (1998). We apply an existing dependency parser, the first-order TurboParser (Martins et al., 2009) trained on the Penn Treebank, to parse each MWE independently, assigning structures like those for LA Times and All the Rage in Figure 1. Arcs involving the MWE in the annotation are then reconnected to the MWE-internal root, so that the resulting tree respects the original tokenization. The MWE-internal arcs are given a special label so that the transformation can be reversed and MWEs reconstructed from parser output. 2.3 Multiple Roots For news text such as that found in the Penn Treebank, sentence segmentation is generally considered a very easy task (Reynar and Ratnaparkhi, 1997). Tweets,</context>
<context position="20945" citStr="Martins et al., 2009" startWordPosition="3266" endWordPosition="3269"> 1. The most aggressive decomposition leads to an “arc-factored” or “first-order” model, which permits exact, efficient solution of Eq. 1 using spanning tree algorithms (McDonald et al., 2005b) or, with a projectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word with its parent, grandparent, and a sibling, or with its parent and two siblings; Koo and Collins, 2010). For a collection of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all trees in Yx), we will use the following notation. Let g(x,y) = E fs(x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is </context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>André F.T. Martins, Noah A. Smith, and Eric P. Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proc. of ACLIJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>André F T Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
<author>Pedro M Q Aguiar</author>
<author>Mário A T Figueiredo</author>
</authors>
<title>Turbo parsers: Dependency parsing by approximate variational inference.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<marker>Martins, Smith, Xing, Aguiar, Figueiredo, 2010</marker>
<rawString>André F.T. Martins, Noah A. Smith, Eric P. Xing, Pedro M.Q. Aguiar, and Mário A.T. Figueiredo. 2010. Turbo parsers: Dependency parsing by approximate variational inference. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="20193" citStr="McDonald et al., 2005" startWordPosition="3157" endWordPosition="3160">ajor errors included, for example, an incorrect dependency relation between an auxiliary verb and the main verb (like ima &gt; [have to]). Minor errors included an incorrect attachment between two modifiers of the same head, as in the &gt; only &gt; [grocery store]—the correct annotation would have two attachments to a single head,i.e.the &gt; [grocery store] &lt; only (or equivalent). feature vector representation g is used in parsing algorithms that seek to find: parse∗(x) = argmax wTg(x,y) (1) y∈Yx The score is parameterized by a vector w of weights, which are learned from data (most commonly using MIRA, McDonald et al., 2005a). The decomposition of the features into local “parts” is a critical choice affecting the computational difficulty of solving Eq. 1. The most aggressive decomposition leads to an “arc-factored” or “first-order” model, which permits exact, efficient solution of Eq. 1 using spanning tree algorithms (McDonald et al., 2005b) or, with a projectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald a</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005a. Online large-margin training of dependency parsers. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proc. of HLTEMNLP.</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proc. of HLTEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Giorgio Satta</author>
</authors>
<title>On the complexity of non-projective data-driven dependency parsing.</title>
<date>2007</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="20808" citStr="McDonald and Satta, 2007" startWordPosition="3244" endWordPosition="3248"> al., 2005a). The decomposition of the features into local “parts” is a critical choice affecting the computational difficulty of solving Eq. 1. The most aggressive decomposition leads to an “arc-factored” or “first-order” model, which permits exact, efficient solution of Eq. 1 using spanning tree algorithms (McDonald et al., 2005b) or, with a projectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word with its parent, grandparent, and a sibling, or with its parent and two siblings; Koo and Collins, 2010). For a collection of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all trees</context>
</contexts>
<marker>McDonald, Satta, 2007</marker>
<rawString>Ryan McDonald and Giorgio Satta. 2007. On the complexity of non-projective data-driven dependency parsing. In Proc. of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>Simplified dependency annotations with GFL-Web.</title>
<date>2014</date>
<booktitle>In Proc. of ACL, demonstration track.</booktitle>
<marker>Dyer, Smith, 2014</marker>
<rawString>Michael T. Mordowanec, Nathan Schneider, Chris. Dyer, and Noah A. Smith. 2014. Simplified dependency annotations with GFL-Web. In Proc. of ACL, demonstration track.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-HLT.</booktitle>
<contexts>
<context position="28913" citStr="Nivre and McDonald (2008)" startWordPosition="4562" endWordPosition="4565">closely as possible.7 2. An experienced annotator manually modified the automatically converted trees by: (a) Performing token selection (§2.1) to remove the tokens which have no syntactic function. (b) Grouping MWEs (§2.2). Here, most of the MWEs are named entities such as Manchester United. (c) Attaching the roots of the utterance in tweets to the “wall” symbol (§2.3).8 6Stacking is a machine learning method where the predictions of one model are used to create features for another. The second model may be from a different family. Stacking has been found successful for dependency parsing by Nivre and McDonald (2008) and Martins et al. (2008). Johansson (2013) describes further advances that use path features. 7http://nlp.cs.lth.se/software/treebank_ converter; run with -rightBranching=false -coordStructure=prague -prepAsHead=true -posAsHead=true -subAsHead=true -imAsHead=true -whAsHead=false. 8This was infrequent; their annotations split most multiTRAIN TEST-NEW TEST-FOSTER tweets 717 201 &lt; 250† unique tweets 569 201 &lt; 250† tokens 9,310 2,839 2,841 selected tokens 7,015 2,158 2,366 types 3,566 1,461 1,230 utterances 1,473 429 337 multi-root tweets 398 123 60 MWEs 387 78 109 Table 1: Statistics of our dat</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proc. of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>Insideoutside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="14659" citStr="Pereira and Schabes, 1992" startWordPosition="2264" endWordPosition="2267">s whose roots are marked by the ** symbol. Schneider et al.’s GFL offers some additional features, only some of which we made use of in this project. One important feature allows an annotator to leave the parse underspecified in some ways. We allowed our annotators to make use of this feature; however, we excluded from our training and testing data any parse that was incomplete (i.e., any parse that contained multiple disconnected fragments with no explicit root, excluding unselected tokens). Learning to parse from incomplete annotations is a fascinating topic explored in the past (Hwa, 2001; Pereira and Schabes, 1992) and, in the case of tweets, left for future work. An important feature of GFL that we did use is special notation for coordination structures. For the coordination structure in Figure 1, for example, the notation is: $a :: {T want} :: {&amp;} where $a creates a new node in the parse tree as it is visualized for the annotator, and this new node attaches to the syntactic parent of the conjoined structure, avoiding the classic forced choice between coordinator and conjunct as parent. For learning to parse, we transform GFL’s coordination structures into specially-labeled dependency parses collapsing</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Fernando Pereira and Yves Schabes. 1992. Insideoutside reestimation from partially bracketed corpora. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<date>2012</date>
<booktitle>Overview of the 2012 Shared Task on Parsing the Web. In Notes of the First Workshop on Syntactic Analysis of NonCanonical Language.</booktitle>
<contexts>
<context position="1852" citStr="Petrov and McDonald, 2012" startWordPosition="265" endWordPosition="268"> (Greene and Resnik, 2009; Kouloumpis et al., 2011). However, their often nonstandard content makes them challenging for traditional NLP tools. Among the tools currently available for tweets are a POS tagger (Gimpel et al., 2011; Owoputi et al., 2013) and a named entity recognizer (Ritter et al., 2011)—but not a parser. Important steps have been taken. The English Web Treebank (Bies et al., 2012) represents an annotation effort on web text—which likely lies somewhere between newspaper text and social media messages in formality and care of editing—that was sufficient to support a shared task (Petrov and McDonald, 2012). Foster et al. (2011b) annotated a small test set of tweets to evaluate parsers trained on the Penn Treebank (Marcus et al., 1993), augmented using semi-supervision and in-domain data. Others, such as Soni et al. (2014), have used existing Penn Treebank–trained models on tweets. In this work, we argue that the Penn Treebank approach to annotation—while well-matched to edited genres like newswire—is poorly suited to more informal genres. Our starting point is that rapid, small-scale annotation efforts performed by imperfectly-trained annotators should provide enough evidence to train an effect</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 Shared Task on Parsing the Web. In Notes of the First Workshop on Syntactic Analysis of NonCanonical Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries.</title>
<date>1997</date>
<booktitle>In Proc. ofANLP.</booktitle>
<contexts>
<context position="10303" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="1554" endWordPosition="1557">der TurboParser (Martins et al., 2009) trained on the Penn Treebank, to parse each MWE independently, assigning structures like those for LA Times and All the Rage in Figure 1. Arcs involving the MWE in the annotation are then reconnected to the MWE-internal root, so that the resulting tree respects the original tokenization. The MWE-internal arcs are given a special label so that the transformation can be reversed and MWEs reconstructed from parser output. 2.3 Multiple Roots For news text such as that found in the Penn Treebank, sentence segmentation is generally considered a very easy task (Reynar and Ratnaparkhi, 1997). Tweets, however, often contain multiple sentences or fragments, which we call “utterances,” each with its own syntactic root disconnected from the others. The selected tokens in Figure 1 comprise four utterances. Our approach to annotation allows multiple utterances to emerge directly from the connectedness properties of the graph implied by an annotator’s decisions. Our parser allows multiple attachments to the “wall” symbol, so that multi-rooted analyses can be predicted. 2.4 Noun Phrase Internal Structure A potentially important drawback of deriving dependency structures from phrase-struc</context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proc. ofANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>James Clarke</author>
</authors>
<title>Incremental integer linear programming for non-projective dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="21926" citStr="Riedel and Clarke, 2006" startWordPosition="3430" endWordPosition="3433">ction of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all trees in Yx), we will use the following notation. Let g(x,y) = E fs(x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs indicating whether part s is included in the output. A collection of constraints relating zs define the set of feasible vectors z that correspond to valid outputs and enfore agreement between parts that overlap. Many different versions of these constraints have been studied (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009, 2010). A key attraction of TurboParser is that many overlapping parts can be handled, making use of separate combinatorial algorithms for efficiently handling subsets of constraints. For example, the constraints that force z to encode a valid tree can be exploited within the framework by making calls 1005 to classic arborescence algorithms (Chu and Liu, 1965; Edmonds, 1967). As a result, when describing modifications to TurboParser, we need only to explain additional constraints and features imposed on parts. 4.2 Adapted Parse Parts The first col</context>
</contexts>
<marker>Riedel, Clarke, 2006</marker>
<rawString>Sebastian Riedel and James Clarke. 2006. Incremental integer linear programming for non-projective dependency parsing. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1529" citStr="Ritter et al., 2011" startWordPosition="214" endWordPosition="217">f traditional publications such as news reports, social media text closely represents language as it is used by people in their everyday lives. These informal texts, which account for ever larger proportions of written content, are of considerable interest to researchers, with applications such as sentiment analysis (Greene and Resnik, 2009; Kouloumpis et al., 2011). However, their often nonstandard content makes them challenging for traditional NLP tools. Among the tools currently available for tweets are a POS tagger (Gimpel et al., 2011; Owoputi et al., 2013) and a named entity recognizer (Ritter et al., 2011)—but not a parser. Important steps have been taken. The English Web Treebank (Bies et al., 2012) represents an annotation effort on web text—which likely lies somewhere between newspaper text and social media messages in formality and care of editing—that was sufficient to support a shared task (Petrov and McDonald, 2012). Foster et al. (2011b) annotated a small test set of tweets to evaluate parsers trained on the Penn Treebank (Marcus et al., 1993), augmented using semi-supervision and in-domain data. Others, such as Soni et al. (2014), have used existing Penn Treebank–trained models on twee</context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: an experimental study. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multiword expressions: A pain in the neck for NLP.</title>
<date>2002</date>
<booktitle>In Proc. of CICLing.</booktitle>
<contexts>
<context position="7046" citStr="Sag et al., 2002" startWordPosition="1041" endWordPosition="1044">tures include POS; shape features that recognize the retweet marker, hashtags, usernames, and hyperlinks; capitalization; and a binary feature for tokens that include punctuation. We trained the model using the structured perceptron (Collins, 2002). 2.2 Multiword Expressions We consider multiword expressions (MWEs) of two kinds. The first, proper names, have been widely modeled for information extraction purposes, and even incorporated into parsing (Finkel and Manning, 2009). (An example found in Figure 1 is LA Times.) The second, lexical idioms, have been a “pain in the neck” for many years (Sag et al., 2002) and have recently received shallow treatment in NLP (Baldwin and Kim, 2010; Constant and Sigogne, 2011; Schneider et al., 2014). Constant et al. (2012), Green et al. (2012), Candito and Constant (2014), and Le Roux et al. (2014) considered MWEs in parsing. Figure 1 provides LA Times and All the Rage as examples. Penn Treebank–style syntactic analysis (and dependency representations derived from it) does not give first-class treatment to this phenomenon, though there is precedent for marking multiword lexical units and certain kinds of idiomatic relationships (Hajiˇc et al., 2012; Abeillé et a</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2002. Multiword expressions: A pain in the neck for NLP. In Proc. of CICLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Schneider</author>
<author>Emily Danchik</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>Discriminative lexical semantic segmentation with gaps: Running the MWE gamut.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--193</pages>
<contexts>
<context position="7174" citStr="Schneider et al., 2014" startWordPosition="1062" endWordPosition="1065">and a binary feature for tokens that include punctuation. We trained the model using the structured perceptron (Collins, 2002). 2.2 Multiword Expressions We consider multiword expressions (MWEs) of two kinds. The first, proper names, have been widely modeled for information extraction purposes, and even incorporated into parsing (Finkel and Manning, 2009). (An example found in Figure 1 is LA Times.) The second, lexical idioms, have been a “pain in the neck” for many years (Sag et al., 2002) and have recently received shallow treatment in NLP (Baldwin and Kim, 2010; Constant and Sigogne, 2011; Schneider et al., 2014). Constant et al. (2012), Green et al. (2012), Candito and Constant (2014), and Le Roux et al. (2014) considered MWEs in parsing. Figure 1 provides LA Times and All the Rage as examples. Penn Treebank–style syntactic analysis (and dependency representations derived from it) does not give first-class treatment to this phenomenon, though there is precedent for marking multiword lexical units and certain kinds of idiomatic relationships (Hajiˇc et al., 2012; Abeillé et al., 2003).1 We argue that internal analysis of MWEs is not critical for many downstream applications, and therefore annotators s</context>
</contexts>
<marker>Schneider, Danchik, Dyer, Smith, 2014</marker>
<rawString>Nathan Schneider, Emily Danchik, Chris Dyer, and Noah A. Smith. 2014. Discriminative lexical semantic segmentation with gaps: Running the MWE gamut. Transactions of the Association for Computational Linguistics, 2:193–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Naomi Saphra</author>
<author>David Bamman</author>
<author>Manaal Faruqui</author>
<author>Noah A Smith</author>
<author>Chris Dyer</author>
<author>Jason Baldridge</author>
</authors>
<title>A framework for (under)specifying dependency syntax without overloading annotators.</title>
<date>2013</date>
<booktitle>In Proc. of the 7th Linguistic Annotation Workshop and Interoperability with Discourse.</booktitle>
<marker>Schneider, O’Connor, Saphra, Bamman, Faruqui, Smith, Dyer, Baldridge, 2013</marker>
<rawString>Nathan Schneider, Brendan O’Connor, Naomi Saphra, David Bamman, Manaal Faruqui, Noah A. Smith, Chris Dyer, and Jason Baldridge. 2013. A framework for (under)specifying dependency syntax without overloading annotators. In Proc. of the 7th Linguistic Annotation Workshop and Interoperability with Discourse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="21950" citStr="Smith and Eisner, 2008" startWordPosition="3434" endWordPosition="3437">pping) parts for input x, Sx (which includes the union of all parts of all trees in Yx), we will use the following notation. Let g(x,y) = E fs(x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs indicating whether part s is included in the output. A collection of constraints relating zs define the set of feasible vectors z that correspond to valid outputs and enfore agreement between parts that overlap. Many different versions of these constraints have been studied (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009, 2010). A key attraction of TurboParser is that many overlapping parts can be handled, making use of separate combinatorial algorithms for efficiently handling subsets of constraints. For example, the constraints that force z to encode a valid tree can be exploited within the framework by making calls 1005 to classic arborescence algorithms (Chu and Liu, 1965; Edmonds, 1967). As a result, when describing modifications to TurboParser, we need only to explain additional constraints and features imposed on parts. 4.2 Adapted Parse Parts The first collection of parts we adap</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>David A. Smith and Jason Eisner. 2008. Dependency parsing by belief propagation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandeep Soni</author>
<author>Tanushree Mitra</author>
<author>Eric Gilbert</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Modeling factuality judgments in social media text.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2072" citStr="Soni et al. (2014)" startWordPosition="302" endWordPosition="305">; Owoputi et al., 2013) and a named entity recognizer (Ritter et al., 2011)—but not a parser. Important steps have been taken. The English Web Treebank (Bies et al., 2012) represents an annotation effort on web text—which likely lies somewhere between newspaper text and social media messages in formality and care of editing—that was sufficient to support a shared task (Petrov and McDonald, 2012). Foster et al. (2011b) annotated a small test set of tweets to evaluate parsers trained on the Penn Treebank (Marcus et al., 1993), augmented using semi-supervision and in-domain data. Others, such as Soni et al. (2014), have used existing Penn Treebank–trained models on tweets. In this work, we argue that the Penn Treebank approach to annotation—while well-matched to edited genres like newswire—is poorly suited to more informal genres. Our starting point is that rapid, small-scale annotation efforts performed by imperfectly-trained annotators should provide enough evidence to train an effective parser. We see this starting point as a necessity, given observations about the rapidly changing nature of tweets (Eisenstein, 2013), the attested difficulties of domain adaptation for parsing (Dredze et al., 2007), </context>
</contexts>
<marker>Soni, Mitra, Gilbert, Eisenstein, 2014</marker>
<rawString>Sandeep Soni, Tanushree Mitra, Eric Gilbert, and Jacob Eisenstein. 2014. Modeling factuality judgments in social media text. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="25658" citStr="Turian et al., 2010" startWordPosition="4040" endWordPosition="4043"> xc and x′c, we added the binned counts of unselected tokens (mostly punctuation) joint with the word form and POS tag of xp and the POS tag of xc and x′c as features scored in the sibling(p,c,c′) part. The changes discussed above comprise the totality of adaptations we made to the TurboParser algorithm; we refer to them as “parsing adaptations” in the experiments. 4.3 Additional Features Brown clusters. Owoputi et al. (2013) found that Brown et al. (1992) clusters served as excellent features in Twitter POS tagging. Others have found them useful in parsing (Koo et al., 2008) and other tasks (Turian et al., 2010). We therefore follow Koo et al. in incorporating Brown clusters as features, making use of the publicly available Twitter clusters from Owoputi et al.5 We use 4 and 6 bit cluster representations to create features wherever POS tags are used, and full bit strings to create features wherever words were used. Penn Treebank features. A potential danger of our choice to “start from scratch” in developing a dependency parser for Twitter is that the resulting annotation conventions, data, and desired output are very different from dependency parses derived from the Penn Treebank. Indeed, Foster et a</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James Curran</author>
</authors>
<title>Adding noun phrase structure to the Penn Treebank.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="11110" citStr="Vadas and Curran, 2007" startWordPosition="1675" endWordPosition="1678">re 1 comprise four utterances. Our approach to annotation allows multiple utterances to emerge directly from the connectedness properties of the graph implied by an annotator’s decisions. Our parser allows multiple attachments to the “wall” symbol, so that multi-rooted analyses can be predicted. 2.4 Noun Phrase Internal Structure A potentially important drawback of deriving dependency structures from phrase-structure annotations, as is typically done using the Penn Treebank, is that flat annotations lead to loss of information. This is especially notable for noun phrases in the Penn Treebank (Vadas and Curran, 2007). Consider Teen Pop Star Heartthrob in Figure 1; Penn Treebank conventions would label this as a single NP with four NN children and no internal structure. Dependency conversion tools would likely attach the first three words in the NP to Heartthrob. Direct dependency annotation (rather than phrase-structure annotation followed by automatic conversion) allows a richer treatment of such structures, which is potentially important for semantic analysis (Vecchi et al., 2013). 3 A Twitter Dependency Corpus In this section, we describe the TWEEBANK corpus, highlighting data selection (§3.1), the ann</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>David Vadas and James Curran. 2007. Adding noun phrase structure to the Penn Treebank. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Maria Vecchi</author>
<author>Roberto Zamparelli</author>
<author>Marco Baroni</author>
</authors>
<title>Studying the recursive behaviour of adjectival modification with compositional distributional semantics.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="11585" citStr="Vecchi et al., 2013" startWordPosition="1749" endWordPosition="1752">, is that flat annotations lead to loss of information. This is especially notable for noun phrases in the Penn Treebank (Vadas and Curran, 2007). Consider Teen Pop Star Heartthrob in Figure 1; Penn Treebank conventions would label this as a single NP with four NN children and no internal structure. Dependency conversion tools would likely attach the first three words in the NP to Heartthrob. Direct dependency annotation (rather than phrase-structure annotation followed by automatic conversion) allows a richer treatment of such structures, which is potentially important for semantic analysis (Vecchi et al., 2013). 3 A Twitter Dependency Corpus In this section, we describe the TWEEBANK corpus, highlighting data selection (§3.1), the annotation process (§3.2), important convention choices (§3.3), and measures of quality (§3.4). 3.1 Data Selection We added manual dependency parses to 929 tweets (12,318 tokens) drawn from the POS-tagged Twitter corpus of Owoputi et al. (2013), which are tokenized and contain manually annotated POS tags. Owoputi et al.’s data consists of two parts. The first, originally annotated by Gimpel et al. (2011), consists of tweets sampled from a particular day, October 27, 2010—th</context>
</contexts>
<marker>Vecchi, Zamparelli, Baroni, 2013</marker>
<rawString>Eva Maria Vecchi, Roberto Zamparelli, and Marco Baroni. 2013. Studying the recursive behaviour of adjectival modification with compositional distributional semantics. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="15724" citStr="Yamada and Matsumoto (2003)" startWordPosition="2435" endWordPosition="2438"> between coordinator and conjunct as parent. For learning to parse, we transform GFL’s coordination structures into specially-labeled dependency parses collapsing nodes like $a with the coordinator and labeling the attachments specially for postprocessing, following Schneider et al. (2013). In our evaluation (§5), these are treated like other attachments. 3.3 Annotation Conventions A wide range of dependency conventions are in use; in many cases these are conversion conventions specifying how dependency trees can be derived from phrase-structure trees. For English, the most popular are due to Yamada and Matsumoto (2003) and de Marneffe and Manning (2008), known as “Yamada-Matsumoto” (YM) and “Stanford” dependencies, respectively. The main differences between them are in whether the auxiliary is the parent of the main verb (or vice versa) and whether the preposition or its argument heads a prepositional phrase (Elming et al., 2013). A full discussion of our annotation conventions is out of scope. We largely followed the conventions suggested by Schneider et al. (2013), which in turn are close to those of YM. Auxiliary verbs are parents of main verbs, and prepositions are parents of their arguments. The key di</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proc. of IWPT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
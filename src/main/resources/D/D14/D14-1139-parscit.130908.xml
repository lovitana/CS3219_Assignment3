<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001173">
<title confidence="0.9862535">
Weakly-Supervised Learning with
Cost-Augmented Contrastive Estimation
</title>
<author confidence="0.994912">
Kevin Gimpel Mohit Bansal
</author>
<affiliation confidence="0.984552">
Toyota Technological Institute at Chicago, IL 60637, USA
</affiliation>
<email confidence="0.997369">
{kgimpel,mbansal}@ttic.edu
</email>
<sectionHeader confidence="0.994764" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999921777777778">
We generalize contrastive estimation in
two ways that permit adding more knowl-
edge to unsupervised learning. The first
allows the modeler to specify not only the
set of corrupted inputs for each observa-
tion, but also how bad each one is. The
second allows specifying structural prefer-
ences on the latent variable used to explain
the observations. They require setting ad-
ditional hyperparameters, which can be
problematic in unsupervised learning, so
we investigate new methods for unsuper-
vised model selection and system com-
bination. We instantiate these ideas for
part-of-speech induction without tag dic-
tionaries, improving over contrastive esti-
mation as well as strong benchmarks from
the PASCAL 2012 shared task.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999573">
Unsupervised NLP aims to discover useful struc-
ture in unannotated text. This structure might
be part-of-speech (POS) tag sequences (Merialdo,
1994), morphological segmentation (Creutz and
Lagus, 2005), or syntactic structure (Klein and
Manning, 2004), among others. Unsupervised
systems typically improve when researchers incor-
porate knowledge to bias learning to capture char-
acteristics of the desired structure.1
There are many successful examples of adding
knowledge to improve learning without labeled
examples, including: sparsity in POS tag distri-
butions (Johnson, 2007; Ravi and Knight, 2009;
Ganchev et al., 2010), short attachments for
dependency parsing (Smith and Eisner, 2006),
</bodyText>
<footnote confidence="0.8935652">
1We note that doing so strains the definition of the term
unsupervised. Hence we will use the term weakly-supervised
to refer to methods that do not explicitly train on labeled ex-
amples for the task of interest, but do use some form of task-
specific knowledge.
</footnote>
<bodyText confidence="0.999101048780488">
agreement of word alignment models (Liang et
al., 2006), power law effects in lexical distribu-
tions (Blunsom and Cohn, 2010; Blunsom and
Cohn, 2011), multilingual constraints (Smith and
Eisner, 2009; Ganchev et al., 2009; Snyder et al.,
2009; Das and Petrov, 2011), and orthographic
cues (Spitkovsky et al., 2010c; Spitkovsky et al.,
2011b), inter alia.
Contrastive estimation (CE; Smith and Eisner,
2005) is a general approach to weakly-supervised
learning with a particular way of incorporating
knowledge. CE increases the likelihood of the ob-
servations at the expense of those in a particular
neighborhood of each observation. The neighbor-
hood typically contains corrupted versions of the
observations. The latent structure is marginalized
out for both the observations and their corruptions;
the intent is to learn latent structure that helps to
explain why the observation was generated rather
than any of the corrupted alternatives.
In this paper, we present a new objective func-
tion for weakly-supervised learning that general-
izes CE by including two types of cost functions,
one on observations and one on output structures.
The first (§4) allows us to specify not only the set
of corrupted observations, but also how bad each
corruption was. We use n-gram language models
to measure the severity of each corruption.
The second (§5) allows us to specify prefer-
ences on desired output structures, regardless of
the input sentence. For POS tagging, we attempt
to learn language-independent tag frequencies by
computing counts from treebanks for 11 languages
not used in our POS induction experiments. For
example, we encourage tag sequences that contain
adjacent nouns and penalize those that contain ad-
jacent adpositions.
We consider several unsupervised ways to set
hyperparameters for these cost functions (§7), in-
cluding the recently-proposed log-likelihood esti-
mator of Bengio et al. (2013). We also circumvent
</bodyText>
<page confidence="0.963091">
1329
</page>
<note confidence="0.8996815">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1329–1341,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999841333333333">
hyperparameter selection via system combination,
developing a novel voting scheme for POS induc-
tion that aligns tag identifiers across runs.
We evaluate our approach, which we call cost-
augmented contrastive estimation (CCE), on
POS induction without tag dictionaries for five
languages from the PASCAL shared task (Gelling
et al., 2012). We find that CCE improves over both
standard CE as well as strong baselines from the
shared task. In particular, our final average accu-
racies are better than all entries in the shared task
that use the same number of tags.
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999976727272727">
Weakly-supervised techniques can be roughly cat-
egorized in terms of whether they influence the
model, the learning procedure, or explicitly target
the output structure. Examples abound in NLP;
we focus on those that have been applied to POS
tagging.
There have been many efforts at biasing
models, including features (Smith and Eisner,
2005a; Berg-Kirkpatrick et al., 2010), sparse
priors (Johnson, 2007; Goldwater and Griffiths,
2007; Toutanova and Johnson, 2007), sparsity
in tag transition distributions (Ravi and Knight,
2009), small models via minimum description
length criteria (Vaswani et al., 2010; Poon et al.,
2009), a one-tag-per-type constraint (Blunsom and
Cohn, 2011), and power law effects via Bayesian
nonparametrics (Van Gael et al., 2009; Blunsom
and Cohn, 2010; Blunsom and Cohn, 2011).
We focus below on efforts that induce bias into
the learning (§2.1) or more directly in the output
structure (§2.2), as they are more closely related
to our contributions in this paper.
</bodyText>
<subsectionHeader confidence="0.997485">
2.1 Biasing Learning
</subsectionHeader>
<bodyText confidence="0.999961714285714">
Some unsupervised methods do not change the
model or attempt to impose structural bias; rather,
they change the learning. This may involve op-
timizing a different objective function for the
same model, e.g., by switching from soft to hard
EM (Spitkovsky et al., 2010b). Or it may in-
volve changing the objective during learning via
annealing (Smith and Eisner, 2004) or more gen-
eral multi-objective techniques (Spitkovsky et al.,
2011a; Spitkovsky et al., 2013).
Other learning modifications relate to automatic
data selection, e.g., choosing examples for genera-
tive learning (Spitkovsky et al., 2010a) or automat-
ically generating negative examples for discrimi-
native unsupervised learning (Li et al., 2010; Xiao
et al., 2011).
CE does both, automatically generating nega-
tive examples and changing the objective function
to include them. Our observation cost function al-
ters CE’s objective function, sharpening the effec-
tive distribution of the negative examples.
</bodyText>
<subsectionHeader confidence="0.99418">
2.2 Structural Bias
</subsectionHeader>
<bodyText confidence="0.999972409090909">
Our output cost function is used to directly spec-
ify preferences on desired output structures. Sev-
eral others have had similar aims. For dependency
grammar induction, Smith and Eisner (2006) fa-
vored short attachments using a fixed-weight fea-
ture whose weight was optionally annealed during
learning. Their bias could be implemented as an
output cost function in our framework.
Posterior regularization (PR; Ganchev et al.,
2010) is a general framework for declaratively
specifying preferences on model outputs. Naseem
et al. (2010) proposed universal syntactic rules for
unsupervised dependency parsing and used them
in a PR regime; we use analogous universal tag
sequences in our cost function.
Our output cost is similar to posterior regular-
ization. The difference is that we specify pref-
erences via an arbitrary cost function on output
structures, while PR uses expectation constraints
on posteriors of the model. We compare to the PR
tag induction system of Grac¸a et al. (2011) in our
experiments, improving over it in several settings.
</bodyText>
<subsectionHeader confidence="0.999173">
2.3 Exploiting Resources
</subsectionHeader>
<bodyText confidence="0.999933">
Much of the work mentioned above also benefits
from leveraging existing resources. These may be
curated or crowdsourced resources like the Wik-
tionary (Li et al., 2012), or traditional annotated
treebanks for languages other than those under in-
vestigation (Cohen et al., 2011). In this paper, we
use tag statistics from treebanks for 11 languages
to impose our structural bias for a different set of
languages used in our POS induction experiments.
Substantial recent work has improved many
NLP tasks by leveraging multilingual or paral-
lel text (Cohen and Smith, 2009; Snyder et al.,
2009; Wang and Manning, 2014), including un-
supervised POS tagging (Naseem et al., 2009; Das
and Petrov, 2011; T¨ackstr¨om et al., 2013; Ganchev
and Das, 2013). This sort of multilingual guidance
could also be captured by particular output cost
functions, though we leave this to future work.
</bodyText>
<page confidence="0.993362">
1330
</page>
<sectionHeader confidence="0.95599" genericHeader="method">
3 Unsupervised Structure Learning
</sectionHeader>
<bodyText confidence="0.9999794">
We consider a structured unsupervised learning
setting. We use X to denote our set of possible
structured inputs, and for a particular x E X,
we use Y(x) to denote the set of valid structured
outputs for x. We are given a dataset of inputs
{x(i)}Ni=1. To map inputs to outputs, we start by
building a model of the joint probability distribu-
tion pθ(x, y). We use a log-linear parameteriza-
tion with feature vector f and weight vector θ:
where the sum in the denominator ranges over all
possible inputs and all valid outputs for them.
In this paper, we consider ways of learning the
parameters θ. Given θ, at test time we output a y
for a new x using, e.g., Viterbi or minimum Bayes
risk decoding; we use the latter in this paper.
</bodyText>
<subsectionHeader confidence="0.997851">
3.1 EM and Contrastive Estimation
</subsectionHeader>
<bodyText confidence="0.9997435">
We start by reviewing two ways of choosing
θ. The expectation-maximization algorithm (EM;
Dempster et al., 1977) finds a local optimum of
the marginal (log-)likelihood of the observations
{x(i)}Ni=1. The marginal log-likelihood is a sum
over all x(i) of the gain function γEM(x(i)):
</bodyText>
<equation confidence="0.9995823">
γEM(x(i)) =log X pθ(x(i), y)
y∈Y(x(i))
X= log exp θ&gt;f(x(i), y)
n o
y∈Y(x(i))
X− log exp θ&gt;f(x0, y0)
n o
x0∈X,y0∈Y(x0)
 |{z }
Z(θ)
</equation>
<bodyText confidence="0.99977352">
The difficulty is the final term, log Z(θ), which
requires summing over all possible inputs and
all valid outputs for them. This summation is
typically intractable for structured problems, and
may even diverge. For this reason, EM is typi-
cally only used to train log-linear model weights
when Z(θ) = 1, e.g., for hidden Markov models,
probabilistic context-free grammars, and models
composed of locally-normalized log-linear mod-
els (Berg-Kirkpatrick et al., 2010), among others.
There have been efforts at approximating the
summation over elements of X, whether by limit-
ing sequence length (Haghighi and Klein, 2006),
only summing over observations in the training
data (Riezler, 1999), restricting the observation
space based on the task (Dyer et al., 2011), or us-
ing Gibbs sampling to obtain an unbiased sample
of the full space (Della Pietra et al., 1997; Rosen-
feld, 1997).
Contrastive estimation (CE) addresses this chal-
lenge by using a neighborhood function N : X →
2X that generates a set of inputs that are “corrup-
tions” of an input x; N(x) always includes x. Us-
ing shorthand Ni for N(x(i)), CE corresponds to
maximizing the sum over inputs x(i) of the gain
</bodyText>
<equation confidence="0.9999498">
γCE(x(i))= log Pr(x(i)  |Ni)
P =log
y∈Y(x(i)) pθ(x(i), y)
P P y0∈Y(x0) pθ(x0, y0)
x0∈Ni
X= log n o
y∈Y(x(i)) exp θ&gt;f(x(i), y) −
X X n o
log exp θ&gt;f(x0, y0)
x0∈Ni y0∈Y(x0)
</equation>
<bodyText confidence="0.997422954545454">
Two log Z(θ) terms cancel out, leaving the sum-
mation over input/output pairs in the neighbor-
hood instead of the full summation over pairs.
Two desiderata govern the choice of N. One is
to make the summation over its elements computa-
tionally tractable. If N(x) = X for all x E X, we
obtain EM, so a smaller neighborhood typically
must be used in practice. The second considera-
tion is to target learning for the task of interest. For
POS tagging and dependency parsing, Smith and
Eisner (2005a, 2005b) used neighborhood func-
tions that corrupted the observations in systematic
ways, e.g., their TRANS1 neighborhood contains
the original sentence along with those that result
from transposing a single pair of adjacent words.
The intent was to force the learner to explain why
the given sentences were observed at the expense
of the corrupted sentences.
Next we present our modifications to con-
trastive estimation. Both can be viewed as adding
specialized cost functions that penalize some part
of the structured input/output pair.
</bodyText>
<sectionHeader confidence="0.970432" genericHeader="method">
4 Modeling Corruption Costs
</sectionHeader>
<bodyText confidence="0.999585571428572">
While CE allows us to specify a set of corrupted
x for each x(i) via the neighborhood function N,
it says nothing about how bad each corruption is.
The same type of corruption might be harmful in
one context and not harmful in another.
This fact was suggested as the reason why cer-
tain neighborhoods did not work as well for POS
</bodyText>
<equation confidence="0.932355">
exp {θ&gt;f(x, y)}
pθ(x, y) =
Px0∈X,y0∈Y(x0) exp Iθ&gt;f(x0, y0)}
</equation>
<page confidence="0.938635">
1331
</page>
<bodyText confidence="0.999912794117647">
tagging as others (Smith and Eisner, 2005a). One
poorly-performing neighborhood consisted of sen-
tences in which a single word of the original
was deleted. Deleting a single word in a sen-
tence might not harm grammaticality. By contrast,
neighborhoods that transpose adjacent words led
to better results. These kinds of corruptions are ex-
pected to be more frequently harmful, at least for
languages with relatively rigid word order. How-
ever, there may still be certain transpositions that
are benign, at least for grammaticality.
To address this, we introduce an observation
cost function A : X x X —* R≥0 that indicates
how much two observations differ. Using A, we
define the following gain function γCCE1(x(i)) =
The function A inflates the score of neighbor-
hood entries with larger differences from the ob-
served x(i). This gain function is inspired by ideas
from structured large-margin learning (Taskar et
al., 2003; Tsochantaridis et al., 2005), specifi-
cally softmax-margin (Povey et al., 2008; Gimpel
and Smith, 2010). Softmax-margin extends con-
ditional likelihood by allowing the user to specify
a cost function to give partial credit for structures
that are partially correct. Conditional likelihood,
by contrast, treats all incorrect structures equally.
While softmax-margin uses a cost function to
specify how two output structures differ, our gain
function γCCE1 uses a cost function A to specify
how two inputs differ. But the motivations are sim-
ilar: since poor structures have their scores artifi-
cially inflated by A, learning pays more attention
to them, choosing weights that penalize them more
than the lower-cost structures.
</bodyText>
<subsectionHeader confidence="0.989014">
4.1 Observation Cost Functions
</subsectionHeader>
<bodyText confidence="0.999897833333333">
What types of cost functions should we consider?
For efficient inference, we want to ensure that
A decomposes additively across parts of the cor-
rupted input x0 in the same way as the features; we
assume unigram and bigram features in this paper.
In addition, the choice of the observation cost
function A is tied to the choice of neighborhood
function. In our experiments, we use neighbor-
hoods that change the order of words in the obser-
vation but not the set of words. Our first cost func-
tion simply counts the number of novel bigrams
introduced when corrupting the original:
</bodyText>
<equation confidence="0.997311">
l ]
I xj−1xj �/ 2grams(x(i))
</equation>
<bodyText confidence="0.997701842105263">
where xj is the jth word of sentence x, x0 is
the start-of-sentence marker, x|x|+1 is the end-of-
sentence marker, 2grams(x) returns the set of bi-
grams in x, I[] returns 1 if its argument is true and
0 otherwise, and α is a constant to be tuned. We
call this cost function MATCH. Only x(i) (which
is always contained in Ni) is guaranteed to have
cost 0. In the TRANS1 neighborhood, corrupted
sequences will be penalized more if their transpo-
sitions occur in the middle of the sentence rather
than at the beginning or end.
We also consider a version that weights the in-
dicator by the negative log probability of the novel
bigram: ALM(x(i), x) =
where P(xj|xj−1) is obtained from a bigram lan-
guage model. Among novel bigrams in the cor-
ruption x, if the second word is highly surprising
conditioned on the first, the bigram will incur high
cost. We refer to ALM(x(i), x) as MATLM.
</bodyText>
<sectionHeader confidence="0.837692" genericHeader="method">
5 Expressing Structural Preferences
</sectionHeader>
<bodyText confidence="0.999964384615385">
Our second modification to CE allows us to spec-
ify structural preferences for outputs y. We first
note that there exist objective functions for su-
pervised structure prediction that never require
computing the feature vector for the true output
y(i). Examples include Bayes risk (Kaiser et al.,
2000; Povey and Woodland, 2002) and structured
ramp loss (Do et al., 2008). These two objec-
tives do, however, need to compute a cost func-
tion cost(y(i), y), which requires the true output
y(i). We start with the following form of struc-
tured ramp loss from Gimpel and Smith (2012),
transformed here to a gain function:
</bodyText>
<equation confidence="0.913666">
ax (θ&gt;f (x(i), y) − cost(y(i), y)) −
y&apos;∈Yax (θ&gt;f (x(i), y0) + cost(y(i), y0 )) (1)
</equation>
<bodyText confidence="0.999374">
Maximizing this gain function for supervised
learning corresponds to increasing the model score
</bodyText>
<equation confidence="0.998435230769231">
l ]
−log P(xj|xj−1)I xj−1xj �/ 2grams(x(i))
α
|x|+1 E
j=1
E { I
log exp θ&gt;f(x(i), y) −
y∈Y(x�i�)
E E { I
log exp θ&gt;f(x0, y0) + A(x(i), x0)
x&apos;∈Ni y&apos;∈Y(x&apos;)
AI(x(i), x) = α |x|+1 E
j=1
</equation>
<page confidence="0.836933">
1332
</page>
<bodyText confidence="0.988999277777778">
of outputs that have both high model score (θ&gt;f)
and low cost, while decreasing the model score of
outputs with high model score and high cost.
For unsupervised learning, we do not have y(i),
so we simply drop y(i) from the cost function. The
result is an output cost function 7r : Y —* R≥0
which captures our a priori knowledge about de-
sired output structures. The value of 7r(y) should
be large for outputs y that are far from the ideal.
In this paper, we consider POS induction and use
intrinsic evaluation; however, in a real-world sce-
nario, the output cost function could use signals
derived from the downstream task in which the
tags are being used.
Given 7r, we convert each max to a log P exp in
Eq. 1 and introduce the contrastive neighborhood
into the second term, defining our new gain func-
tion γCCE2(x(i)) =
</bodyText>
<table confidence="0.9989871">
tag unigram count cost
X 50783 3.83
NUM 174613 2.59
PRT 179131 2.57
ADV 330210 1.96
CONJ 436649 1.68
PRON 461880 1.62
DET 615284 1.33
ADJ 694685 1.21
ADP 906922 0.95
VERB 1018989 0.83
. 1042662 0.81
NOUN 2337234 0
tag bigram count cost
DET PRT 109 84.41
DET CONJ 518 68.82
NUM ADV 1587 57.63
NOUN NOUN 409828 2.09
DET NOUN 454980 1.04
NOUN . 504897 0
</table>
<tableCaption confidence="0.762583333333333">
Table 1: Counts and costs for universal tags based
on treebanks for 11 languages not used in POS in-
duction experiments.
</tableCaption>
<equation confidence="0.980942166666667">
X n o Where #(y) is the count of tag y in the treebank
log exp θ&gt;f(x(i), y) − 7r(y) − concatenation, the cost of y is
y∈Y(x(i))
X X n o �maxy0 #(y0) �
log exp θ&gt;f(x0,y0) + 7r(y0) u(y) = log #(y)
x0∈Ni y0∈Y(x0)
</equation>
<bodyText confidence="0.9926455">
Gimpel (2012) found that using such “softened”
versions of the ramp losses worked better than the
original versions (e.g., Eq. 1) when training ma-
chine translation systems.
</bodyText>
<subsectionHeader confidence="0.962393">
5.1 Output Cost Functions
</subsectionHeader>
<bodyText confidence="0.995506">
The output cost 7r should capture our desider-
ata about y for the task of interest. We con-
sider universal POS tag subsequences analogous
to the universal syntactic rules of Naseem et al.
(2010). In doing so, we use the universal tags of
Petrov et al. (2012): NOUN, VERB, ADJ (ad-
jective), ADV (adverb), PRON (pronoun), DET
(determiner), ADP (pre/postposition), NUM (nu-
meral), CONJ (conjunction), PRT (particle), ‘.’
(punctuation), and X (other).
We aimed for a set of rules that would be ro-
bust across languages. So, we used treebanks for
11 languages from the CoNLL 2006/2007 shared
tasks (Buchholz and Marsi, 2006; Nivre et al.,
2007) other than those used in our POS induc-
tion experiments. In particular, we used Arabic,
Bulgarian, Catalan, Czech, English, Spanish, Ger-
man, Hungarian, Italian, Japanese, and Turkish.
We replicated shorter treebanks a sufficient num-
ber of times until they were a similar size as the
largest treebank. Then we counted gold POS tag
unigrams and bigrams from the concatenation.
and, where #((y1, y2)) is the count of tag bigram
(y1, y2), the cost of (y1, y2) is
</bodyText>
<equation confidence="0.991217666666667">
u((y1,y2)) = 10Xlog maxhy0 1, y0 !
1,y0 2i #((y0 2))
#((y1, y2))
</equation>
<bodyText confidence="0.999913">
We use a multiplier of 10 in order to exaggerate
count differences among bigrams, which gener-
ally are closer together than unigram counts. In
Table 1, we show counts and costs for all tag uni-
grams and selected tag bigrams.2
Given these costs for individual tag unigrams
and bigrams, we use the following 7r function,
which we call UNIV:
</bodyText>
<equation confidence="0.996415">
7r(y) = Q |y|+1 X u(yj) + u((yj−1, yj))
j=1
</equation>
<bodyText confidence="0.971393857142857">
where Q is a constant to be tuned and yj is the
jth tag of y. We define y0 to be the beginning-
of-sentence marker and y|y|+1 to be the end-of-
sentence marker (which has unigram cost 0).
Many POS induction systems use one-tag-
per-type constraints (Blunsom and Cohn, 2011;
Gelling et al., 2012), which often lead to higher
</bodyText>
<footnote confidence="0.989766">
2The complete tag bigram list is provided in the supple-
mentary material.
</footnote>
<page confidence="0.747745">
1333
</page>
<equation confidence="0.9682413">
log exp n θT f (x(i), y) o − log X
VEY(x(i)) x0ENi
X n o
exp θTf(x�, y�) (2)
V0EY(x0)
XN
i=1
max
e
log X exp θTf(x(i), y) − π(y) − log
n o X
x0ENi
VEY(x(i))
X n o
exp θTf(x�, y�) + Δ(x(i), x�) + π(y�) (3)
V0EY(x0)
XN
i=1
max
e
</equation>
<figureCaption confidence="0.979618">
Figure 1: Contrastive estimation (Eq. 2) and cost-augmented contrastive estimation (Eq. 3). L2 regular-
ization terms (2 E|θ |1 θ2j) are not shown here but were used in our experiments.
</figureCaption>
<bodyText confidence="0.9981905">
accuracies even though the gold standard is not
constrained in this way. This constraint can be en-
coded as an output cost function, though it would
require approximate inference (Poon et al., 2009).
</bodyText>
<sectionHeader confidence="0.999224" genericHeader="method">
6 Cost-Augmented CE
</sectionHeader>
<bodyText confidence="0.999966153846154">
We extended the objective function underlying
CE by defining two new types of cost functions,
one on observations (§4) and one on outputs (§5).
We combine them into a single objective, which
we call cost-augmented contrastive estimation
(CCE), shown as Eq. 3 in Figure 1.
If the cost functions A and 7r factor in the same
way as the features f, then it is straightforward
to implement CCE atop an existing CE implemen-
tation. The additional terms in the cost functions
can be implemented as features with fixed weights
(albeit where the weight differs depending on the
context).
</bodyText>
<sectionHeader confidence="0.970273" genericHeader="method">
7 Model Selection
</sectionHeader>
<bodyText confidence="0.999511846153846">
Our modifications give increased flexibility, but
require setting new hyperparameters. In addition
to the choice of the cost functions, each has a
weight: α for A and Q for 7r. We need ways to
set these weights that do not require labeled data.
Smith and Eisner (2005a) chose the hyperpa-
rameter values that yielded the best CE objec-
tive on held-out development data. We use their
strategy, though we experiment with two others as
well.3 In particular, we estimate held-out data log-
likelihood via the method of Bengio et al. (2013)
and also consider ways of combining outputs from
multiple models.
</bodyText>
<subsectionHeader confidence="0.986033">
7.1 Estimating Held-Out Log-Likelihood
</subsectionHeader>
<bodyText confidence="0.998514097560976">
Bengio et al. (2013) recently proposed ways to
efficiently estimate held-out data log-likelihood
3When using their strategy for CCE, we compute the CE
criterion only, omitting the costs. We do so because the
weights of the cost terms can have a large impact on the mag-
nitude of the objective, making it difficult to do a fair com-
parison of models with different cost weights.
for generative models. They showed empirically
that a simple, biased version of their conserva-
tive sampling-based log-likelihood (CSL) estima-
tor can be useful for model selection.
The biased CSL requires a Markov chain on the
variables in the model (i.e., x and y) as well as
the ability to compute pθ(x|y). It generates con-
secutive samples of y from a Markov chain ini-
tialized at each x in a development set D, with
5 Markov chains run for each x. We compute
and sum pθ(x|yj) for each sampled yj, then sum
over all x in D. The result is a biased estimate for
the log-likelihood of D. Bengio et al. showed that
these biased estimates could give the same model
ranking as unbiased estimates, though more effi-
ciently. They also showed that taking the single,
initial sample from the 5 Markov chains resulted
in the same model ranking as using many samples
from each chain. We follow suit here.
Our Markov chain is a blocked Gibbs sam-
pler in which we alternate between sampling from
pθ(y|x) and pθ(x|y). Since we only use a sin-
gle sample from each Markov chain and initialize
each chain to x, this simply amounts to drawing 5
samples from pθ(y|x). To sample from pθ(y|x),
we use the exact algorithm obtained by running
the backward algorithm and then performing left-
to-right sampling of tags using the local features
and requisite backward terms to define the local
tag distributions.
We then compute pθ(x|y) for each sampled y.
If there are no features in f that look at more than
one word (which is the case with the features used
in our experiments), then this probability factors:
</bodyText>
<equation confidence="0.9980515">
pθ(x|y) = �|y|
k=1 pθ(xk|yk)
</equation>
<bodyText confidence="0.9992645">
This is easily computable assuming that we have
normalization constants Z(y) cached for each tag
y. To compute each Z(y), we sum over all words
observed in the training data (replacing some with
a special UNK token; see below). We can then
compute likelihoods for individual words and mul-
</bodyText>
<page confidence="0.976375">
1334
</page>
<bodyText confidence="0.956371">
tiply them across the words in the sentence to com-
pute pθ(x|y).
To summarize, we get a log-likelihood estimate
for development set D = {x(i)}|D|
</bodyText>
<equation confidence="0.900413">
i=1 by sampling 5
times from pθ(y|x(i)) for each x(i), getting sam-
ples {{y(i),j}S j=1}|D|
i=1, then we compute
�|D|
ES j=1 log pθ(x(i)|y(i),j)
i=1
</equation>
<bodyText confidence="0.99991875">
We used values of 5 ∈ {1,10,100}, finding that
the ranking of models was consistent across 5 val-
ues. We used 5 = 10 in all results reported below.
We note that this estimator was originally pre-
sented for generative models, and that (C)CE is
not a generative training criterion. It seeks to max-
imize the conditional probability of an observation
given its neighborhood. Nonetheless, when imple-
menting our log-likelihood estimator, we treat the
model as a generative model, computing the Z(y)
constants by summing over all words in the vocab-
ulary.
</bodyText>
<subsectionHeader confidence="0.993793">
7.2 System Combination
</subsectionHeader>
<bodyText confidence="0.999992153846154">
We can avoid choosing a single model by com-
bining the outputs of multiple models via system
combination. We decode test data by using poste-
rior decoding. To combine the outputs of multiple
models, we find the max-posterior tag under each
model, then choose the highest vote-getter, break-
ing ties arbitrarily.
However, when doing POS induction without a
tag dictionary, the tags are simply unique identi-
fiers and may not have consistent meaning across
runs. To address this, we propose a novel voting
scheme that is inspired by the widely-used 1-to-1
accuracy metric for POS induction (Haghighi and
Klein, 2006). This metric maps system tags to
gold tags to maximize accuracy with the constraint
that each gold tag is mapped to at most once. The
optimal mapping can be found by solving a maxi-
mum weighted bipartite matching problem.
We adapt this idea to map tags between two sys-
tems, rather than between system tags and gold
tags. Given k systems that we want to combine,
we choose one to be the backbone and map the re-
maining k − 1 systems’ outputs to the backbone.4
After mapping each system’s output to the back-
bone system, we perform simple majority voting
among all k systems. To choose the backbone, we
</bodyText>
<footnote confidence="0.6171325">
4We use the LEMON C++ toolkit (Dezs et al., 2011) to
solve the maximum weighted bipartite matching problems.
</footnote>
<bodyText confidence="0.999612727272727">
consider each of the k systems in turn as back-
bone and maximize the sum of the weights of the
weighted bipartite matching solutions found. This
is a heuristic that attempts to choose a backbone
that is similar to all other systems. We found
that highly-weighted matchings often led to high
POS tagging accuracy metrics. We call this vot-
ing scheme ALIGN. To see the benefit of ALIGN,
we also compare to a simple scheme (NA¨IVE) that
performs majority voting without any tag map-
ping.
</bodyText>
<sectionHeader confidence="0.99852" genericHeader="evaluation">
8 Experiments
</sectionHeader>
<bodyText confidence="0.999398214285714">
Task and Datasets We consider POS induction
without tag dictionaries using five freely-available
datasets from the PASCAL shared task (Gelling
et al., 2012).5 These include Danish (DA), using
the Copenhagen Dependency Treebank v2 (Buch-
Kromann et al., 2007); Dutch (NL), using the
Alpino treebank (Bouma et al., 2001); Por-
tuguese (PT), using the Floresta Sint´a(c)tica tree-
bank (Afonso et al., 2002); Slovene (SL), us-
ing the jos500k treebank (Erjavec et al., 2010);
and Swedish (SV), using the Talbanken tree-
bank (Nivre et al., 2006). We use their provided
training, development, and test sets.
Evaluation We fix the number of tags in our
models to 12, which matches the number of uni-
versal tags from Petrov et al. (2012). We use
both many-to-1 (M-1) and 1-to-1 (1-1) accuracy
as our evaluation metrics, using the universal tags
for the gold standard (which was done for the of-
ficial evaluation for the shared task).6 We note
that our 7r function assigns identities to tags (e.g.,
tag 1 is assumed to be NOUN), so we could use
actual tagging accuracy when training with the 7r
cost function. But we use M-1 and 1-1 accuracy
to enable easier comparison both among different
settings and to prior work.
Baselines From the shared task, we compare
to all entries that used 12 tags. These include
</bodyText>
<footnote confidence="0.974799">
5http://wiki.cs.ox.ac.uk/
InducingLinguisticStructure/SharedTask
6It is common to use a greedy algorithm to com-
pute 1-to-1 accuracy, e.g., as in the shared task scor-
ing script (http://www.dcs.shef.ac.uk/˜tcohn/
wils/eval.tar.gz), though the optimal mapping can
be computed efficiently via the maximum weighted bipartite
matching algorithm, as stated above. We use the shared task
scorer for all results here for ease of comparison. When we
instead evaluate using the optimal mapping, we find that ac-
curacies are usually only slightly higher than those found by
the greedy algorithm.
</footnote>
<page confidence="0.995104">
1335
</page>
<bodyText confidence="0.99932203030303">
BROWN clusters (Brown et al., 1992), clusters ob-
tained using the mkcls tool (Och, 1995), and the
featurized HMM with sparsity constraints trained
using posterior regularization (PR), described by
Grac¸a et al. (2011). The PR system achieved the
highest average 1-1 accuracy in the shared task.
We restrict our attention to systems that use 12
tags because the M-1 and 1-1 metrics are highly
dependent upon the number of hypothesized tags.
In general, using more tags leads to higher M-1
and lower 1-1 (Gelling et al., 2012). By keep-
ing the number of tags fixed, we hope to provide a
cleaner comparison among approaches.
We compare to two other baselines: an HMM
trained with 500 iterations of EM and an HMM
trained with 100 iterations of stepwise EM (Liang
and Klein, 2009). We used random initialization
as done by Liang and Klein: we set each param-
eter in each multinomial to exp11 + c}, where
c — U[0,1], then normalized to get probability
distributions. For stepwise EM, we used mini-
batch size 3 and stepsize reduction power 0.7.
For all models we trained, including both base-
lines and CCE, we used only the training data
during training and used the unannotated devel-
opment data for certain model selection criteria.
No labels were used except for final evaluation on
the test data. Therefore, we need a way to handle
unknown words in test data. When running EM
and stepwise EM, while reading in the final 10%
of sentences in the training set, we replace novel
words with the special token UNK. We then re-
place unknown words in test data with UNK.
</bodyText>
<subsectionHeader confidence="0.973059">
8.1 CCE Setup
</subsectionHeader>
<bodyText confidence="0.957066090909091">
Features We use standard indicator features on
tag-tag transitions and tag-word emissions, the
spelling features from Smith and Eisner (2005a),
and additional emission features based on Brown
clusters. The latter features are simply indicators
for tag-cluster pairs—analogous to tag-word emis-
sions in which the word is replaced by its Brown
cluster identifier. We run Brown clustering (Liang,
2005) on the POS training data for each language,
once with 12 clusters and once with 40, then add
tag-cluster emission features for each clustering
and one more for their conjunction.7
7To handle unknown words: for words that only appear
in the final 10% of training sentences, we replace them with
UNK when firing their tag-word emission features. We use
special Brown cluster identifiers reserved for UNK. But we
still use all spelling features derived from the actual word
Learning We solve Eq. 2 and Eq. 3 by running
LBFGS until convergence on the training data, up
to 100 iterations. We tag the test data with mini-
mum Bayes risk decoding and evaluate.
We use two neighborhood functions:
</bodyText>
<listItem confidence="0.9931746">
• TRANS1: the original sentence along with all
sentences that result from doing a single trans-
position of adjacent words.
• SHUFF10: the original sentence along with 10
random permutations of it.
</listItem>
<bodyText confidence="0.999720085714286">
We use L2 regularization, adding 2 �j 1 θ2j to
the objectives shown in Figure 1. We use a fixed
(untuned) C = 0.0001 for all experiments re-
ported below.8 We initialize each CE model by
sampling weights from N(0,1).
Cost Functions The cost functions A and π
have constants α and Q which balance their con-
tributions relative to the model score and must be
tuned. We consider the ways proposed in Sec-
tion 7, namely tuning based on the contrastive es-
timation criterion computed on development data
(CE), the log-likelihood estimate on development
data with S = 10 (LL), and our two system com-
bination algorithms: naive voting (NA¨IVE) and
aligned voting (ALIGN), both of which use as in-
put the 4 system outputs whose hyperparameters
led to the highest values for the CE criterion on
development data.
We used α E 13 x 10−4,10−3, 3 x
10−3, 0.01, 0.03, 0.1, 0.3} and Q E 13 x
10−6,10−5, 3 x 10−5,10−4, 3 x 10−4}. Setting
α = Q = 0 gives us CE, which we also compare
to. When using both MATLM and UNIV simul-
taneously, we first choose the best two α values
by the LL criterion and the best two Q values by
the CE criterion when using only those individual
costs. This gives us 4 pairs of values; we run ex-
periments with these pairs and choose the pair to
report using each of the model selection criteria.
For system combination, we use the 4 system out-
puts resulting from these 4 pairs.
For training bigram language models for the
MATLM cost, we use the language’s POS train-
ing data concatenated with its portion of the Eu-
roparl v7 corpus (Koehn, 2005) and the text of its
</bodyText>
<footnote confidence="0.9903336">
type. For unknown words at test time, we use the UNK emis-
sion feature, the Brown cluster features with the special UNK
cluster identifiers, and the word’s actual spelling features.
8In subsequent experiments we tried C ∈ {0.01, 0.001}
for the baseline CE setting and found minimal differences.
</footnote>
<page confidence="0.944064">
1336
</page>
<table confidence="0.996020083333333">
neigh- cost mod. DA NL 1-1 PT 1-1 SL 1-1 SV 1-1 avg
borhood sel. M-1 1-1 M-1 M-1 M-1 M-1 M-1 1-1
SHUFF10 none N/A 45.0 38.0 55.1 45.7 54.2 38.0 54.7 45.7 47.4 31.3 51.3 39.7
MATCH CE 48.9 31.5 56.5 46.4 54.2 37.7 55.9 46.8 48.9 33.8 52.9 39.2
LL 49.9 34.4 56.5 46.4 54.1 38.9 57.2 48.9 48.9 33.8 53.3 40.5
MATLM CE 49.1 34.3 59.6 50.4 53.6 37.1 55.0 46.2 48.8 33.1 53.2 40.2
LL 50.2 40.0 59.6 50.4 53.1 36.0 58.0 48.4 48.8 33.1 53.9 41.6
TRANS1 none N/A 58.5 42.7 62.5 49.5 70.7 43.8 58.6 46.1 58.7 53.8 61.8 47.2
MATCH CE 58.5 42.5 66.3 53.3 70.6 43.3 59.1 45.6 59.3 54.2 62.7 47.8
LL 58.8 42.8 66.3 53.3 70.6 43.3 60.3 43.7 59.8 54.9 63.1 47.6
MATLM CE 59.4 43.5 63.8 50.1 70.2 43.0 58.5 46.1 59.2 54.8 62.2 47.5
LL 58.7 42.8 66.5 60.4 70.5 43.6 59.1 47.7 59.2 54.8 62.8 49.9
</table>
<tableCaption confidence="0.987638">
Table 2: Results for observation cost functions. The CE baseline corresponds to rows where cost=“none”.
Other rows are CCE. Best score for each column and each neighborhood is bold.
</tableCaption>
<bodyText confidence="0.9990765">
Wikipedia. The word counts for the Wikipedias
used range from 18M for Slovene to 1.9B for
Dutch. We used modified Kneser-Ney smoothing
as implemented by SRILM (Stolcke, 2002).
</bodyText>
<subsectionHeader confidence="0.664853">
8.2 Results
</subsectionHeader>
<bodyText confidence="0.999921565217391">
We present two sets of results. First we compare
our MATCH and MATLM observation cost func-
tions for our two neighborhoods and two ways of
doing model selection. Then we do a broader com-
parison, comparing both types of costs and their
combination to our full set of baselines.
Observation Cost Functions In Table 2, we
show results for observation cost functions. We
note that the TRANS1 neighborhood works much
better than the SHUFF10 neighborhood, but we
find that using cost functions can close the gap in
certain cases, particularly for Dutch and Slovene
for which the SHUFF10 MATLM scores approach
or exceed the TRANS1 scores without a cost.
Since the SHUFF10 neighborhood exhibits
more diversity than TRANS1, we expect to see
larger gains from using observation cost functions.
We do in fact see larger gains in M-1, e.g., average
improvements are 1.6-2.6 for SHUFF10 and 0.4-
1.3 for TRANS1, though 1-1 gains are closer.
For TRANS1, while MATCH does reach a
slightly higher average M-1 than MATLM, the lat-
ter does much better in 1-1 (49.9 vs. 47.6 when
using LL for model selection). For SHUFF10,
MATLM consistently does better than MATCH.
Nonetheless, we suspect MATCH works as well as
it does because it at least differentiates the obser-
vation (which is always part of the neighborhood)
from the corruptions.
We find that the LL model selection criterion
consistently works better than the CE criterion for
model selection. When using LL model selection
and fixing the neighborhood, all average scores are
better than their CE baselines. For M-1, the aver-
age improvement is 1.0 to 2.6 points, and for 1-1
the average improvement ranges from 0.4 to 2.7.
We find the best overall performance when us-
ing MATLM with LL model selection with the
TRANS1 neighborhood, and we report this setting
in our subsequent experiments.
Output Cost Function Table 3 shows results
when using our UNIV output cost function, as well
as our full set of baselines. All (C)CE experiments
used the TRANS1 neighborhood.
We find that our contrastive estimation baseline
(cost=“none”) has a higher average M-1 (61.8)
than all results from the shared task, but its average
1-1 accuracy is lower than that reached by poste-
rior regularization, the best system in the shared
task according to 1-1. Using an observation cost
function increases both M-1 and 1-1: MATLM
yields an average 1-1 of 49.9, nearing the 50.1 of
PR while exceeding it in M-1 by nearly 2 points.
When using the UNIV cost function, we see
some variation in performance across model selec-
tion criteria, but we find improvements in both M-
1 and 1-1 accuracy under most settings. When do-
ing model selection via ALIGN voting, we roughly
match the average 1-1 of PR, and when using the
CE criterion, we beat it by 1 point on average (51.3
vs. 50.1).
Combined Costs When using the UNIV cost,
we find that model selection via CE works bet-
ter than LL. So for the combined costs, we took
the two best MATLM weights (α values) accord-
ing to LL and the two best UNIV weights (β val-
ues) according to CE and ran combined cost ex-
periments (MATCHLM+UNIV) with the four pairs
of hyperparameters. Then from among these four,
</bodyText>
<page confidence="0.977323">
1337
</page>
<table confidence="0.99899555">
system DA NL 1-1 PT 1-1 SL 1-1 SV 1-1 avg
M-1 1-1 M-1 M-1 M-1 M-1 M-1 1-1
HMM, EM 42.5 28.1 53.0 40.6 59.4 33.7 50.3 34.7 49.3 33.9 50.9 34.2
HMM, stepwise EM 51.7 38.2 61.6 45.2 66.5 46.7 53.6 35.7 55.3 39.6 57.7 41.1
BROWN 47.1 39.2 57.3 43.1 67.6 51.6 58.3 42.3 57.6 51.3 57.6 45.5
mkcls 53.1 44.2 63.0 54.1 68.1 46.3 50.4 40.6 57.3 43.6 58.4 45.8
posterior regularization 53.8 45.6 57.6 45.4 74.4 56.1 60.0 48.5 58.8 54.9 60.9 50.1
contrastive estimation
cost model sel.
none N/A 58.5 42.7 62.5 49.5 70.7 43.8 58.6 46.1 58.7 53.8 61.8 47.2
MATCH LL 58.8 42.8 66.3 53.3 70.6 43.3 60.3 43.7 59.8 54.9 63.1 47.6
MATLM LL 58.7 42.8 66.5 60.4 70.5 43.6 59.1 47.7 59.2 54.8 62.8 49.9
CE 59.7 45.6 60.6 51.1 70.0 62.7 60.9 44.1 57.1 52.8 61.7 51.3
UNIV LL 59.5 42.2 62.1 56.3 70.7 43.1 60.9 44.1 57.1 52.8 62.1 47.7
NA¨IVE 59.2 45.6 62.2 52.8 72.7 52.7 60.0 43.8 56.2 53.0 62.2 49.6
ALIGN 61.6 47.3 63.7 54.5 74.4 53.1 59.7 42.1 56.6 53.2 63.2 50.0
MATLM CE 59.8 45.7 60.4 48.4 70.0 62.8 52.9 45.0 59.4 54.9 60.5 51.4
LL 59.3 42.5 61.9 56.2 70.8 43.1 59.3 41.9 60.0 55.1 62.3 47.8
+ NA¨IVE 58.5 44.4 64.9 60.3 65.4 52.1 55.5 45.9 59.0 54.4 60.6 51.4
UNIV ALIGN 61.1 45.4 66.2 60.9 75.8 49.8 59.5 48.2 59.0 54.4 64.3 51.7
</table>
<tableCaption confidence="0.999172">
Table 3: Unsupervised POS tagging accuracies for five languages, showing results for three systems from
</tableCaption>
<bodyText confidence="0.9433036875">
the PASCAL shared task as well as three other baselines (EM, stepwise EM, and contrastive estimation).
All (C)CE results use the TRANS1 neighborhood. The best score in each column is bold.
we again chose results by CE, LL, and both voting
schemes.
The results are shown in the lower part of Ta-
ble 3. We find different trends in M-1 and 1-
1 depending on whether we use CE or LL for
model selection, which may be due to our lim-
ited hyperparameter search stemming from com-
putational constraints. However, by comparing
NA¨IVE to ALIGN, we see a consistent benefit
from aligning tags before voting, leading to our
highest average accuracies. In particular, using
MATCHLM+UNIV and ALIGN, we improve over
CE by 2.5 in M-1 and 4.5 in 1-1, also improving
over the best results from the shared task.
</bodyText>
<sectionHeader confidence="0.994744" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999974916666667">
We have shown how to modify contrastive estima-
tion to use additional sources of knowledge, both
in terms of observation and output cost functions.
We adapted a recently-proposed technique for es-
timating the log-likelihood of held-out data, find-
ing it to be effective as a model selection criterion
when using observation cost functions. We im-
proved tagging accuracy by using weak supervi-
sion in the form of universal tag frequencies. We
proposed a system combination method for POS
induction systems that consistently performs bet-
ter than naive voting and circumvents hyperpa-
rameter selection. We reported results on par with
or exceeding the best systems from the PASCAL
2012 shared task.
Contrastive estimation has been shown effective
for numerous NLP tasks, including dependency
grammar induction (Smith and Eisner, 2005b),
bilingual part-of-speech induction (Chen et al.,
2011), morphological segmentation (Poon et al.,
2009), and machine translation (Xiao et al., 2011).
The hope is that our contributions can benefit these
and other applications of weakly-supervised learn-
ing.
</bodyText>
<sectionHeader confidence="0.99756" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999941">
We thank the anonymous reviewers for their in-
sightful comments and Waleed Ammar, Chris
Dyer, David McAllester, Sasha Rush, Nathan
Schneider, Noah Smith, and John Wieting for
helpful discussions.
</bodyText>
<sectionHeader confidence="0.997955" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999530083333333">
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002.
Floresta sint´a(c)tica: a treebank for Portuguese. In
Proc. of LREC.
Y. Bengio, L. Yao, and K. Cho. 2013. Bounding
the test log-likelihood of generative models. arXiv
preprint arXiv:1311.6184.
T. Berg-Kirkpatrick, A. Bouchard-Cˆot´e, J. DeNero,
and D. Klein. 2010. Painless unsupervised learn-
ing with features. In Proc. of NAACL.
P. Blunsom and T. Cohn. 2010. Unsupervised induc-
tion of tree substitution grammars for dependency
parsing. In Proc. of EMNLP.
</reference>
<page confidence="0.926003">
1338
</page>
<reference confidence="0.999035524752475">
P. Blunsom and T. Cohn. 2011. A hierarchical Pitman-
Yor process HMM for unsupervised part of speech
induction. In Proc. of ACL.
G. Bouma, G. Van Noord, and R. Malouf. 2001.
Alpino: Wide-coverage computational analysis of
Dutch. Language and Computers, 37(1).
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della
Pietra, and J. C. Lai. 1992. Class-based N-gram
models of natural language. Computational Lin-
guistics, 18(4).
M. Buch-Kromann, J. Wedekind, and J. Elming.
2007. The Copenhagen Danish-English dependency
treebank v. 2.0. code.google.com/p/copenhagen-
dependency-treebank.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc.
of CoNLL.
D. Chen, C. Dyer, S. B. Cohen, and N. A. Smith. 2011.
Unsupervised bilingual POS tagging with Markov
random fields. In Proc. of the First Workshop on
Unsupervised Learning in NLP.
S. Cohen and N. A. Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsu-
pervised grammar induction. In Proc. of NAACL.
S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsu-
pervised structure prediction with non-parallel mul-
tilingual guidance. In Proc. of EMNLP.
M. Creutz and K. Lagus. 2005. Unsupervised mor-
pheme segmentation and morphology induction from
text corpora using Morfessor 1.0. Helsinki Univer-
sity of Technology.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projec-
tions. In Proc. of ACL.
S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997.
Inducing features of random fields. IEEE Trans.
Pattern Anal. Mach. Intell., 19(4).
A. Dempster, N. Laird, and D. Rubin. 1977. Maxi-
mum likelihood estimation from incomplete data via
the EM algorithm. Journal of the Royal Statistical
Society B, 39:1–38.
B. Dezs, A. J¨uttner, and P. Kov´acs. 2011. LEMON - an
open source C++ graph template library. Electron.
Notes Theor. Comput. Sci., 264(5).
C. B. Do, Q. Le, C. H. Teo, O. Chapelle, and A. Smola.
2008. Tighter bounds for structured estimation. In
Advances in NIPS.
C. Dyer, J. H. Clark, A. Lavie, and N. A. Smith. 2011.
Unsupervised word alignment with arbitrary fea-
tures. In Proc. of ACL.
T. Erjavec, D. Fiser, S. Krek, and N. Ledinek. 2010.
The JOS linguistically tagged corpus of Slovene. In
Proc. of LREC.
K. Ganchev and D. Das. 2013. Cross-lingual discrim-
inative learning of sequence models with posterior
regularization. In Proc. of EMNLP.
K. Ganchev, J. Gillenwater, and B. Taskar. 2009. De-
pendency grammar induction via bitext projection
constraints. In Proc. of ACL.
K. Ganchev, J. V. Grac¸a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search, 11.
D. Gelling, T. Cohn, P. Blunsom, and J. V. Grac¸a.
2012. The PASCAL challenge on grammar induc-
tion. In Proc. of NAACL-HLT Workshop on the In-
duction of Linguistic Structure.
K. Gimpel and N. A. Smith. 2010. Softmax-margin
CRFs: Training log-linear models with cost func-
tions. In Proc. of NAACL.
K. Gimpel and N. A. Smith. 2012. Structured ramp
loss minimization for machine translation. In Proc.
of NAACL.
K. Gimpel. 2012. Discriminative Feature-Rich Mod-
eling for Syntax-Based Machine Translation. Ph.D.
thesis, Carnegie Mellon University.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Proc. of ACL.
J. V. Grac¸a, K. Ganchev, L. Coheur, F. Pereira, and
B. Taskar. 2011. Controlling complexity in part-
of-speech induction. J. Artif. Int. Res., 41(2).
A. Haghighi and D. Klein. 2006. Prototype-driven
learning for sequence models. In Proc. of HLT-
NAACL.
M. Johnson. 2007. Why doesn’t EM find good HMM
POS-taggers? In Proc. of EMNLP-CoNLL.
J. Kaiser, B. Horvat, and Z. Kacic. 2000. A novel loss
function for the overall risk criterion based discrimi-
native training of HMM models. In Proc. of ICSLP.
D. Klein and C. D. Manning. 2004. Corpus-based
induction of syntactic structure: Models of depen-
dency and constituency. In Proc. of ACL.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In Proc. of MT Summit.
Z. Li, Z. Wang, S. Khudanpur, and J. Eisner. 2010.
Unsupervised discriminative language model train-
ing for machine translation using simulated confu-
sion sets. In Proc. of COLING.
S. Li, J. V. Grac¸a, and B. Taskar. 2012. Wiki-ly super-
vised part-of-speech tagging. In Proc. of EMNLP.
</reference>
<page confidence="0.884842">
1339
</page>
<reference confidence="0.999953168316831">
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proc. of NAACL.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. of HLT-NAACL.
P. Liang. 2005. Semi-supervised learning for natural
language. Master’s thesis, Massachusetts Institute
of Technology.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics, 20(2).
T. Naseem, B. Snyder, J. Eisenstein, and R. Barzilay.
2009. Multilingual part-of-speech tagging: Two un-
supervised approaches. JAIR, 36.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson.
2010. Using universal linguistic knowledge to guide
grammar induction. In Proc. of EMNLP.
J. Nivre, J. Nilsson, and J. Hall. 2006. Talbanken05: A
Swedish treebank with phrase structure and depen-
dency annotation. In Proc. of LREC.
J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of CoNLL.
F. J. Och. 1995. Maximum-likelihood-sch¨atzung
von wortkategorien mit verfahren der kombina-
torischen optimierung. Bachelor’s thesis (Studien-
arbeit), Friedrich-Alexander-Universit¨at Erlangen-
N¨urnburg, Germany.
S. Petrov, D. Das, and R. McDonald. 2012. A univer-
sal part-of-speech tagset. In Proc. of LREC.
H. Poon, C. Cherry, and K. Toutanova. 2009. Unsuper-
vised morphological segmentation with log-linear
models. In Proc. of HLT: NAACL.
D. Povey and P. C. Woodland. 2002. Minimum
phone error and I-smoothing for improved discrima-
tive training. In Proc. of ICASSP.
D. Povey, D. Kanevsky, B. Kingsbury, B. Ramabhad-
ran, G. Saon, and K. Visweswariah. 2008. Boosted
MMI for model and feature space discriminative
training. In Proc. of ICASSP.
S. Ravi and K. Knight. 2009. Minimized models for
unsupervised part-of-speech tagging. In Proc. of
ACL.
S. Riezler. 1999. Probabilistic Constraint Logic Pro-
gramming. Ph.D. thesis, Universit¨at T¨ubingen.
R. Rosenfeld. 1997. A whole sentence maximum en-
tropy language model. In Proc. of ASRU.
N. A. Smith and J. Eisner. 2004. Annealing techniques
for unsupervised statistical language learning. In
Proc. of ACL.
N. A. Smith and J. Eisner. 2005a. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proc. of ACL.
N. A. Smith and J. Eisner. 2005b. Guiding unsuper-
vised grammar induction using contrastive estima-
tion. In Proc. of IJCAI Workshop on Grammatical
Inference Applications.
N. A. Smith and J. Eisner. 2006. Annealing structural
bias in multilingual weighted grammar induction. In
Proc. of COLING-ACL.
D. A. Smith and J. Eisner. 2009. Parser adaptation
and projection with quasi-synchronous features. In
Proc. of EMNLP.
B. Snyder, T. Naseem, and R. Barzilay. 2009. Unsu-
pervised multilingual grammar induction. In Proc.
ofACL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a.
From Baby Steps to Leapfrog: How “Less is More”
in unsupervised dependency parsing. In Proc. of
NAACL-HLT.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D.
Manning. 2010b. Viterbi training improves unsu-
pervised dependency parsing. In Proc. of CoNLL.
V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010c.
Profiting from mark-up: Hyper-text annotations for
guided parsing. In Proc. of ACL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011a.
Lateen EM: Unsupervised training with multiple ob-
jectives, applied to dependency grammar induction.
In Proc. of EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b.
Punctuation: Making a point in unsupervised depen-
dency parsing. In Proc. of CoNLL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2013.
Breaking out of local optima with count transforms
and model recombination: A study in grammar in-
duction. In Proc. of EMNLP.
A. Stolcke. 2002. SRILM—an extensible language
modeling toolkit. In Proc. of ICSLP.
O. T¨ackstr¨om, D. Das, S. Petrov, R. McDonald, and
J. Nivre. 2013. Token and type constraints for cross-
lingual part-of-speech tagging. Transactions of the
Association for Computational Linguistics, 1.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-
margin Markov networks. In Advances in NIPS 16.
K. Toutanova and M. Johnson. 2007. A Bayesian
LDA-based model for semi-supervised part-of-
speech tagging. In Advances in NIPS.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. Journal ofMachine
Learning Research, 6.
</reference>
<page confidence="0.756329">
1340
</page>
<reference confidence="0.999878142857143">
J. Van Gael, A. Vlachos, and Z. Ghahramani. 2009.
The infinite HMM for unsupervised POS tagging.
In Proc. of EMNLP.
A. Vaswani, A. Pauls, and D. Chiang. 2010. Efficient
optimization of an MDL-inspired objective function
for unsupervised part-of-speech tagging. In Proc. of
ACL.
M. Wang and C. D. Manning. 2014. Cross-lingual
projected expectation regularization for weakly su-
pervised learning. Transactions of the Association
for Computational Linguistics, 2.
X. Xiao, Y. Liu, Q. Liu, and S. Lin. 2011. Fast gen-
eration of translation forest for large-scale SMT dis-
criminative training. In Proc. of EMNLP.
</reference>
<page confidence="0.992914">
1341
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.267568">
<title confidence="0.999788">Weakly-Supervised Learning Cost-Augmented Contrastive Estimation</title>
<author confidence="0.993815">Kevin Gimpel Mohit Bansal</author>
<address confidence="0.378208">Toyota Technological Institute at Chicago, IL 60637,</address>
<abstract confidence="0.97947752631579">We generalize contrastive estimation in two ways that permit adding more knowledge to unsupervised learning. The first allows the modeler to specify not only the corrupted inputs for each observabut also bad one is. The second allows specifying structural preferences on the latent variable used to explain the observations. They require setting additional hyperparameters, which can be problematic in unsupervised learning, so we investigate new methods for unsupervised model selection and system combination. We instantiate these ideas for part-of-speech induction without tag dictionaries, improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Afonso</author>
<author>E Bick</author>
<author>R Haber</author>
<author>D Santos</author>
</authors>
<title>Floresta sint´a(c)tica: a treebank for Portuguese.</title>
<date>2002</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="27525" citStr="Afonso et al., 2002" startWordPosition="4592" endWordPosition="4595">ten led to high POS tagging accuracy metrics. We call this voting scheme ALIGN. To see the benefit of ALIGN, we also compare to a simple scheme (NA¨IVE) that performs majority voting without any tag mapping. 8 Experiments Task and Datasets We consider POS induction without tag dictionaries using five freely-available datasets from the PASCAL shared task (Gelling et al., 2012).5 These include Danish (DA), using the Copenhagen Dependency Treebank v2 (BuchKromann et al., 2007); Dutch (NL), using the Alpino treebank (Bouma et al., 2001); Portuguese (PT), using the Floresta Sint´a(c)tica treebank (Afonso et al., 2002); Slovene (SL), using the jos500k treebank (Erjavec et al., 2010); and Swedish (SV), using the Talbanken treebank (Nivre et al., 2006). We use their provided training, development, and test sets. Evaluation We fix the number of tags in our models to 12, which matches the number of universal tags from Petrov et al. (2012). We use both many-to-1 (M-1) and 1-to-1 (1-1) accuracy as our evaluation metrics, using the universal tags for the gold standard (which was done for the official evaluation for the shared task).6 We note that our 7r function assigns identities to tags (e.g., tag 1 is assumed t</context>
</contexts>
<marker>Afonso, Bick, Haber, Santos, 2002</marker>
<rawString>S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. Floresta sint´a(c)tica: a treebank for Portuguese. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>L Yao</author>
<author>K Cho</author>
</authors>
<title>Bounding the test log-likelihood of generative models. arXiv preprint arXiv:1311.6184.</title>
<date>2013</date>
<contexts>
<context position="3772" citStr="Bengio et al. (2013)" startWordPosition="564" endWordPosition="567">to measure the severity of each corruption. The second (§5) allows us to specify preferences on desired output structures, regardless of the input sentence. For POS tagging, we attempt to learn language-independent tag frequencies by computing counts from treebanks for 11 languages not used in our POS induction experiments. For example, we encourage tag sequences that contain adjacent nouns and penalize those that contain adjacent adpositions. We consider several unsupervised ways to set hyperparameters for these cost functions (§7), including the recently-proposed log-likelihood estimator of Bengio et al. (2013). We also circumvent 1329 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1329–1341, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics hyperparameter selection via system combination, developing a novel voting scheme for POS induction that aligns tag identifiers across runs. We evaluate our approach, which we call costaugmented contrastive estimation (CCE), on POS induction without tag dictionaries for five languages from the PASCAL shared task (Gelling et al., 2012). We find that CCE improves over both stan</context>
<context position="22066" citStr="Bengio et al. (2013)" startWordPosition="3658" endWordPosition="3661">xed weights (albeit where the weight differs depending on the context). 7 Model Selection Our modifications give increased flexibility, but require setting new hyperparameters. In addition to the choice of the cost functions, each has a weight: α for A and Q for 7r. We need ways to set these weights that do not require labeled data. Smith and Eisner (2005a) chose the hyperparameter values that yielded the best CE objective on held-out development data. We use their strategy, though we experiment with two others as well.3 In particular, we estimate held-out data loglikelihood via the method of Bengio et al. (2013) and also consider ways of combining outputs from multiple models. 7.1 Estimating Held-Out Log-Likelihood Bengio et al. (2013) recently proposed ways to efficiently estimate held-out data log-likelihood 3When using their strategy for CCE, we compute the CE criterion only, omitting the costs. We do so because the weights of the cost terms can have a large impact on the magnitude of the objective, making it difficult to do a fair comparison of models with different cost weights. for generative models. They showed empirically that a simple, biased version of their conservative sampling-based log-</context>
</contexts>
<marker>Bengio, Yao, Cho, 2013</marker>
<rawString>Y. Bengio, L. Yao, and K. Cho. 2013. Bounding the test log-likelihood of generative models. arXiv preprint arXiv:1311.6184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Berg-Kirkpatrick</author>
<author>A Bouchard-Cˆot´e</author>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>T. Berg-Kirkpatrick, A. Bouchard-Cˆot´e, J. DeNero, and D. Klein. 2010. Painless unsupervised learning with features. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
</authors>
<title>Unsupervised induction of tree substitution grammars for dependency parsing.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1994" citStr="Blunsom and Cohn, 2010" startWordPosition="291" endWordPosition="294">f adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent struct</context>
<context position="5350" citStr="Blunsom and Cohn, 2010" startWordPosition="803" endWordPosition="806">structure. Examples abound in NLP; we focus on those that have been applied to POS tagging. There have been many efforts at biasing models, including features (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010), sparse priors (Johnson, 2007; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007), sparsity in tag transition distributions (Ravi and Knight, 2009), small models via minimum description length criteria (Vaswani et al., 2010; Poon et al., 2009), a one-tag-per-type constraint (Blunsom and Cohn, 2011), and power law effects via Bayesian nonparametrics (Van Gael et al., 2009; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve optimizing a different objective function for the same model, e.g., by switching from soft to hard EM (Spitkovsky et al., 2010b). Or it may involve changing the objective during learning via annealing (Smith and Eisner, 2004) o</context>
</contexts>
<marker>Blunsom, Cohn, 2010</marker>
<rawString>P. Blunsom and T. Cohn. 2010. Unsupervised induction of tree substitution grammars for dependency parsing. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
</authors>
<title>A hierarchical PitmanYor process HMM for unsupervised part of speech induction.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2019" citStr="Blunsom and Cohn, 2011" startWordPosition="295" endWordPosition="298">prove learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out f</context>
<context position="5252" citStr="Blunsom and Cohn, 2011" startWordPosition="787" endWordPosition="790">terms of whether they influence the model, the learning procedure, or explicitly target the output structure. Examples abound in NLP; we focus on those that have been applied to POS tagging. There have been many efforts at biasing models, including features (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010), sparse priors (Johnson, 2007; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007), sparsity in tag transition distributions (Ravi and Knight, 2009), small models via minimum description length criteria (Vaswani et al., 2010; Poon et al., 2009), a one-tag-per-type constraint (Blunsom and Cohn, 2011), and power law effects via Bayesian nonparametrics (Van Gael et al., 2009; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve optimizing a different objective function for the same model, e.g., by switching from soft to hard EM (Spitkovsky et al., 2010b).</context>
<context position="20180" citStr="Blunsom and Cohn, 2011" startWordPosition="3322" endWordPosition="3325"> in order to exaggerate count differences among bigrams, which generally are closer together than unigram counts. In Table 1, we show counts and costs for all tag unigrams and selected tag bigrams.2 Given these costs for individual tag unigrams and bigrams, we use the following 7r function, which we call UNIV: 7r(y) = Q |y|+1 X u(yj) + u((yj−1, yj)) j=1 where Q is a constant to be tuned and yj is the jth tag of y. We define y0 to be the beginningof-sentence marker and y|y|+1 to be the end-ofsentence marker (which has unigram cost 0). Many POS induction systems use one-tagper-type constraints (Blunsom and Cohn, 2011; Gelling et al., 2012), which often lead to higher 2The complete tag bigram list is provided in the supplementary material. 1333 log exp n θT f (x(i), y) o − log X VEY(x(i)) x0ENi X n o exp θTf(x�, y�) (2) V0EY(x0) XN i=1 max e log X exp θTf(x(i), y) − π(y) − log n o X x0ENi VEY(x(i)) X n o exp θTf(x�, y�) + Δ(x(i), x�) + π(y�) (3) V0EY(x0) XN i=1 max e Figure 1: Contrastive estimation (Eq. 2) and cost-augmented contrastive estimation (Eq. 3). L2 regularization terms (2 E|θ |1 θ2j) are not shown here but were used in our experiments. accuracies even though the gold standard is not constrained</context>
</contexts>
<marker>Blunsom, Cohn, 2011</marker>
<rawString>P. Blunsom and T. Cohn. 2011. A hierarchical PitmanYor process HMM for unsupervised part of speech induction. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouma</author>
<author>G Van Noord</author>
<author>R Malouf</author>
</authors>
<title>Alpino: Wide-coverage computational analysis of Dutch.</title>
<date>2001</date>
<journal>Language and Computers,</journal>
<volume>37</volume>
<issue>1</issue>
<marker>Bouma, Van Noord, Malouf, 2001</marker>
<rawString>G. Bouma, G. Van Noord, and R. Malouf. 2001. Alpino: Wide-coverage computational analysis of Dutch. Language and Computers, 37(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>P V deSouza</author>
<author>R L Mercer</author>
<author>V J Della Pietra</author>
<author>J C Lai</author>
</authors>
<title>Class-based N-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="29041" citStr="Brown et al., 1992" startWordPosition="4839" endWordPosition="4842">p://wiki.cs.ox.ac.uk/ InducingLinguisticStructure/SharedTask 6It is common to use a greedy algorithm to compute 1-to-1 accuracy, e.g., as in the shared task scoring script (http://www.dcs.shef.ac.uk/˜tcohn/ wils/eval.tar.gz), though the optimal mapping can be computed efficiently via the maximum weighted bipartite matching algorithm, as stated above. We use the shared task scorer for all results here for ease of comparison. When we instead evaluate using the optimal mapping, we find that accuracies are usually only slightly higher than those found by the greedy algorithm. 1335 BROWN clusters (Brown et al., 1992), clusters obtained using the mkcls tool (Och, 1995), and the featurized HMM with sparsity constraints trained using posterior regularization (PR), described by Grac¸a et al. (2011). The PR system achieved the highest average 1-1 accuracy in the shared task. We restrict our attention to systems that use 12 tags because the M-1 and 1-1 metrics are highly dependent upon the number of hypothesized tags. In general, using more tags leads to higher M-1 and lower 1-1 (Gelling et al., 2012). By keeping the number of tags fixed, we hope to provide a cleaner comparison among approaches. We compare to t</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della Pietra, and J. C. Lai. 1992. Class-based N-gram models of natural language. Computational Linguistics, 18(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Buch-Kromann</author>
<author>J Wedekind</author>
<author>J Elming</author>
</authors>
<title>The Copenhagen Danish-English dependency treebank v.</title>
<date>2007</date>
<note>2.0. code.google.com/p/copenhagendependency-treebank.</note>
<marker>Buch-Kromann, Wedekind, Elming, 2007</marker>
<rawString>M. Buch-Kromann, J. Wedekind, and J. Elming. 2007. The Copenhagen Danish-English dependency treebank v. 2.0. code.google.com/p/copenhagendependency-treebank.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="18988" citStr="Buchholz and Marsi, 2006" startWordPosition="3109" endWordPosition="3112"> Functions The output cost 7r should capture our desiderata about y for the task of interest. We consider universal POS tag subsequences analogous to the universal syntactic rules of Naseem et al. (2010). In doing so, we use the universal tags of Petrov et al. (2012): NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner), ADP (pre/postposition), NUM (numeral), CONJ (conjunction), PRT (particle), ‘.’ (punctuation), and X (other). We aimed for a set of rules that would be robust across languages. So, we used treebanks for 11 languages from the CoNLL 2006/2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) other than those used in our POS induction experiments. In particular, we used Arabic, Bulgarian, Catalan, Czech, English, Spanish, German, Hungarian, Italian, Japanese, and Turkish. We replicated shorter treebanks a sufficient number of times until they were a similar size as the largest treebank. Then we counted gold POS tag unigrams and bigrams from the concatenation. and, where #((y1, y2)) is the count of tag bigram (y1, y2), the cost of (y1, y2) is u((y1,y2)) = 10Xlog maxhy0 1, y0 ! 1,y0 2i #((y0 2)) #((y1, y2)) We use a multiplier of 10 in order to exaggerate count </context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chen</author>
<author>C Dyer</author>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Unsupervised bilingual POS tagging with Markov random fields.</title>
<date>2011</date>
<booktitle>In Proc. of the First Workshop on Unsupervised Learning in NLP.</booktitle>
<marker>Chen, Dyer, Cohen, Smith, 2011</marker>
<rawString>D. Chen, C. Dyer, S. B. Cohen, and N. A. Smith. 2011. Unsupervised bilingual POS tagging with Markov random fields. In Proc. of the First Workshop on Unsupervised Learning in NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cohen</author>
<author>N A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="8199" citStr="Cohen and Smith, 2009" startWordPosition="1248" endWordPosition="1251"> over it in several settings. 2.3 Exploiting Resources Much of the work mentioned above also benefits from leveraging existing resources. These may be curated or crowdsourced resources like the Wiktionary (Li et al., 2012), or traditional annotated treebanks for languages other than those under investigation (Cohen et al., 2011). In this paper, we use tag statistics from treebanks for 11 languages to impose our structural bias for a different set of languages used in our POS induction experiments. Substantial recent work has improved many NLP tasks by leveraging multilingual or parallel text (Cohen and Smith, 2009; Snyder et al., 2009; Wang and Manning, 2014), including unsupervised POS tagging (Naseem et al., 2009; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). This sort of multilingual guidance could also be captured by particular output cost functions, though we leave this to future work. 1330 3 Unsupervised Structure Learning We consider a structured unsupervised learning setting. We use X to denote our set of possible structured inputs, and for a particular x E X, we use Y(x) to denote the set of valid structured outputs for x. We are given a dataset of inputs {x(i)}Ni=1. </context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>S. Cohen and N. A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>D Das</author>
<author>N A Smith</author>
</authors>
<title>Unsupervised structure prediction with non-parallel multilingual guidance.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="7908" citStr="Cohen et al., 2011" startWordPosition="1200" endWordPosition="1203">to posterior regularization. The difference is that we specify preferences via an arbitrary cost function on output structures, while PR uses expectation constraints on posteriors of the model. We compare to the PR tag induction system of Grac¸a et al. (2011) in our experiments, improving over it in several settings. 2.3 Exploiting Resources Much of the work mentioned above also benefits from leveraging existing resources. These may be curated or crowdsourced resources like the Wiktionary (Li et al., 2012), or traditional annotated treebanks for languages other than those under investigation (Cohen et al., 2011). In this paper, we use tag statistics from treebanks for 11 languages to impose our structural bias for a different set of languages used in our POS induction experiments. Substantial recent work has improved many NLP tasks by leveraging multilingual or parallel text (Cohen and Smith, 2009; Snyder et al., 2009; Wang and Manning, 2014), including unsupervised POS tagging (Naseem et al., 2009; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). This sort of multilingual guidance could also be captured by particular output cost functions, though we leave this to future work. </context>
</contexts>
<marker>Cohen, Das, Smith, 2011</marker>
<rawString>S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsupervised structure prediction with non-parallel multilingual guidance. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Creutz</author>
<author>K Lagus</author>
</authors>
<title>Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0.</title>
<date>2005</date>
<institution>Helsinki University of Technology.</institution>
<contexts>
<context position="1121" citStr="Creutz and Lagus, 2005" startWordPosition="156" endWordPosition="159">ble used to explain the observations. They require setting additional hyperparameters, which can be problematic in unsupervised learning, so we investigate new methods for unsupervised model selection and system combination. We instantiate these ideas for part-of-speech induction without tag dictionaries, improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task. 1 Introduction Unsupervised NLP aims to discover useful structure in unannotated text. This structure might be part-of-speech (POS) tag sequences (Merialdo, 1994), morphological segmentation (Creutz and Lagus, 2005), or syntactic structure (Klein and Manning, 2004), among others. Unsupervised systems typically improve when researchers incorporate knowledge to bias learning to capture characteristics of the desired structure.1 There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supe</context>
</contexts>
<marker>Creutz, Lagus, 2005</marker>
<rawString>M. Creutz and K. Lagus. 2005. Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0. Helsinki University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>S Petrov</author>
</authors>
<title>Unsupervised part-ofspeech tagging with bilingual graph-based projections.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2135" citStr="Das and Petrov, 2011" startWordPosition="313" endWordPosition="316"> 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why th</context>
<context position="8324" citStr="Das and Petrov, 2011" startWordPosition="1269" endWordPosition="1272"> resources. These may be curated or crowdsourced resources like the Wiktionary (Li et al., 2012), or traditional annotated treebanks for languages other than those under investigation (Cohen et al., 2011). In this paper, we use tag statistics from treebanks for 11 languages to impose our structural bias for a different set of languages used in our POS induction experiments. Substantial recent work has improved many NLP tasks by leveraging multilingual or parallel text (Cohen and Smith, 2009; Snyder et al., 2009; Wang and Manning, 2014), including unsupervised POS tagging (Naseem et al., 2009; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). This sort of multilingual guidance could also be captured by particular output cost functions, though we leave this to future work. 1330 3 Unsupervised Structure Learning We consider a structured unsupervised learning setting. We use X to denote our set of possible structured inputs, and for a particular x E X, we use Y(x) to denote the set of valid structured outputs for x. We are given a dataset of inputs {x(i)}Ni=1. To map inputs to outputs, we start by building a model of the joint probability distribution pθ(x, y). We use a log-linear pa</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>D. Das and S. Petrov. 2011. Unsupervised part-ofspeech tagging with bilingual graph-based projections. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="10578" citStr="Pietra et al., 1997" startWordPosition="1647" endWordPosition="1650">cally only used to train log-linear model weights when Z(θ) = 1, e.g., for hidden Markov models, probabilistic context-free grammars, and models composed of locally-normalized log-linear models (Berg-Kirkpatrick et al., 2010), among others. There have been efforts at approximating the summation over elements of X, whether by limiting sequence length (Haghighi and Klein, 2006), only summing over observations in the training data (Riezler, 1999), restricting the observation space based on the task (Dyer et al., 2011), or using Gibbs sampling to obtain an unbiased sample of the full space (Della Pietra et al., 1997; Rosenfeld, 1997). Contrastive estimation (CE) addresses this challenge by using a neighborhood function N : X → 2X that generates a set of inputs that are “corruptions” of an input x; N(x) always includes x. Using shorthand Ni for N(x(i)), CE corresponds to maximizing the sum over inputs x(i) of the gain γCE(x(i))= log Pr(x(i) |Ni) P =log y∈Y(x(i)) pθ(x(i), y) P P y0∈Y(x0) pθ(x0, y0) x0∈Ni X= log n o y∈Y(x(i)) exp θ&gt;f(x(i), y) − X X n o log exp θ&gt;f(x0, y0) x0∈Ni y0∈Y(x0) Two log Z(θ) terms cancel out, leaving the summation over input/output pairs in the neighborhood instead of the full summa</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997. Inducing features of random fields. IEEE Trans. Pattern Anal. Mach. Intell., 19(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood estimation from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society B,</journal>
<pages>39--1</pages>
<contexts>
<context position="9421" citStr="Dempster et al., 1977" startWordPosition="1460" endWordPosition="1463">To map inputs to outputs, we start by building a model of the joint probability distribution pθ(x, y). We use a log-linear parameterization with feature vector f and weight vector θ: where the sum in the denominator ranges over all possible inputs and all valid outputs for them. In this paper, we consider ways of learning the parameters θ. Given θ, at test time we output a y for a new x using, e.g., Viterbi or minimum Bayes risk decoding; we use the latter in this paper. 3.1 EM and Contrastive Estimation We start by reviewing two ways of choosing θ. The expectation-maximization algorithm (EM; Dempster et al., 1977) finds a local optimum of the marginal (log-)likelihood of the observations {x(i)}Ni=1. The marginal log-likelihood is a sum over all x(i) of the gain function γEM(x(i)): γEM(x(i)) =log X pθ(x(i), y) y∈Y(x(i)) X= log exp θ&gt;f(x(i), y) n o y∈Y(x(i)) X− log exp θ&gt;f(x0, y0) n o x0∈X,y0∈Y(x0) |{z } Z(θ) The difficulty is the final term, log Z(θ), which requires summing over all possible inputs and all valid outputs for them. This summation is typically intractable for structured problems, and may even diverge. For this reason, EM is typically only used to train log-linear model weights when Z(θ) = </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likelihood estimation from incomplete data via the EM algorithm. Journal of the Royal Statistical Society B, 39:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dezs</author>
<author>A J¨uttner</author>
<author>P Kov´acs</author>
</authors>
<title>LEMON - an open source C++ graph template library.</title>
<date>2011</date>
<journal>Electron. Notes Theor. Comput. Sci.,</journal>
<volume>264</volume>
<issue>5</issue>
<marker>Dezs, J¨uttner, Kov´acs, 2011</marker>
<rawString>B. Dezs, A. J¨uttner, and P. Kov´acs. 2011. LEMON - an open source C++ graph template library. Electron. Notes Theor. Comput. Sci., 264(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C B Do</author>
<author>Q Le</author>
<author>C H Teo</author>
<author>O Chapelle</author>
<author>A Smola</author>
</authors>
<title>Tighter bounds for structured estimation.</title>
<date>2008</date>
<booktitle>In Advances in NIPS.</booktitle>
<contexts>
<context position="16065" citStr="Do et al., 2008" startWordPosition="2577" endWordPosition="2580">tained from a bigram language model. Among novel bigrams in the corruption x, if the second word is highly surprising conditioned on the first, the bigram will incur high cost. We refer to ALM(x(i), x) as MATLM. 5 Expressing Structural Preferences Our second modification to CE allows us to specify structural preferences for outputs y. We first note that there exist objective functions for supervised structure prediction that never require computing the feature vector for the true output y(i). Examples include Bayes risk (Kaiser et al., 2000; Povey and Woodland, 2002) and structured ramp loss (Do et al., 2008). These two objectives do, however, need to compute a cost function cost(y(i), y), which requires the true output y(i). We start with the following form of structured ramp loss from Gimpel and Smith (2012), transformed here to a gain function: ax (θ&gt;f (x(i), y) − cost(y(i), y)) − y&apos;∈Yax (θ&gt;f (x(i), y0) + cost(y(i), y0 )) (1) Maximizing this gain function for supervised learning corresponds to increasing the model score l ] −log P(xj|xj−1)I xj−1xj �/ 2grams(x(i)) α |x|+1 E j=1 E { I log exp θ&gt;f(x(i), y) − y∈Y(x�i�) E E { I log exp θ&gt;f(x0, y0) + A(x(i), x0) x&apos;∈Ni y&apos;∈Y(x&apos;) AI(x(i), x) = α |x|+1 E</context>
</contexts>
<marker>Do, Le, Teo, Chapelle, Smola, 2008</marker>
<rawString>C. B. Do, Q. Le, C. H. Teo, O. Chapelle, and A. Smola. 2008. Tighter bounds for structured estimation. In Advances in NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
<author>J H Clark</author>
<author>A Lavie</author>
<author>N A Smith</author>
</authors>
<title>Unsupervised word alignment with arbitrary features.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="10479" citStr="Dyer et al., 2011" startWordPosition="1628" endWordPosition="1631">s typically intractable for structured problems, and may even diverge. For this reason, EM is typically only used to train log-linear model weights when Z(θ) = 1, e.g., for hidden Markov models, probabilistic context-free grammars, and models composed of locally-normalized log-linear models (Berg-Kirkpatrick et al., 2010), among others. There have been efforts at approximating the summation over elements of X, whether by limiting sequence length (Haghighi and Klein, 2006), only summing over observations in the training data (Riezler, 1999), restricting the observation space based on the task (Dyer et al., 2011), or using Gibbs sampling to obtain an unbiased sample of the full space (Della Pietra et al., 1997; Rosenfeld, 1997). Contrastive estimation (CE) addresses this challenge by using a neighborhood function N : X → 2X that generates a set of inputs that are “corruptions” of an input x; N(x) always includes x. Using shorthand Ni for N(x(i)), CE corresponds to maximizing the sum over inputs x(i) of the gain γCE(x(i))= log Pr(x(i) |Ni) P =log y∈Y(x(i)) pθ(x(i), y) P P y0∈Y(x0) pθ(x0, y0) x0∈Ni X= log n o y∈Y(x(i)) exp θ&gt;f(x(i), y) − X X n o log exp θ&gt;f(x0, y0) x0∈Ni y0∈Y(x0) Two log Z(θ) terms canc</context>
</contexts>
<marker>Dyer, Clark, Lavie, Smith, 2011</marker>
<rawString>C. Dyer, J. H. Clark, A. Lavie, and N. A. Smith. 2011. Unsupervised word alignment with arbitrary features. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Erjavec</author>
<author>D Fiser</author>
<author>S Krek</author>
<author>N Ledinek</author>
</authors>
<title>The JOS linguistically tagged corpus of Slovene.</title>
<date>2010</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="27590" citStr="Erjavec et al., 2010" startWordPosition="4603" endWordPosition="4606">g scheme ALIGN. To see the benefit of ALIGN, we also compare to a simple scheme (NA¨IVE) that performs majority voting without any tag mapping. 8 Experiments Task and Datasets We consider POS induction without tag dictionaries using five freely-available datasets from the PASCAL shared task (Gelling et al., 2012).5 These include Danish (DA), using the Copenhagen Dependency Treebank v2 (BuchKromann et al., 2007); Dutch (NL), using the Alpino treebank (Bouma et al., 2001); Portuguese (PT), using the Floresta Sint´a(c)tica treebank (Afonso et al., 2002); Slovene (SL), using the jos500k treebank (Erjavec et al., 2010); and Swedish (SV), using the Talbanken treebank (Nivre et al., 2006). We use their provided training, development, and test sets. Evaluation We fix the number of tags in our models to 12, which matches the number of universal tags from Petrov et al. (2012). We use both many-to-1 (M-1) and 1-to-1 (1-1) accuracy as our evaluation metrics, using the universal tags for the gold standard (which was done for the official evaluation for the shared task).6 We note that our 7r function assigns identities to tags (e.g., tag 1 is assumed to be NOUN), so we could use actual tagging accuracy when training</context>
</contexts>
<marker>Erjavec, Fiser, Krek, Ledinek, 2010</marker>
<rawString>T. Erjavec, D. Fiser, S. Krek, and N. Ledinek. 2010. The JOS linguistically tagged corpus of Slovene. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ganchev</author>
<author>D Das</author>
</authors>
<title>Cross-lingual discriminative learning of sequence models with posterior regularization.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="8374" citStr="Ganchev and Das, 2013" startWordPosition="1277" endWordPosition="1280"> resources like the Wiktionary (Li et al., 2012), or traditional annotated treebanks for languages other than those under investigation (Cohen et al., 2011). In this paper, we use tag statistics from treebanks for 11 languages to impose our structural bias for a different set of languages used in our POS induction experiments. Substantial recent work has improved many NLP tasks by leveraging multilingual or parallel text (Cohen and Smith, 2009; Snyder et al., 2009; Wang and Manning, 2014), including unsupervised POS tagging (Naseem et al., 2009; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). This sort of multilingual guidance could also be captured by particular output cost functions, though we leave this to future work. 1330 3 Unsupervised Structure Learning We consider a structured unsupervised learning setting. We use X to denote our set of possible structured inputs, and for a particular x E X, we use Y(x) to denote the set of valid structured outputs for x. We are given a dataset of inputs {x(i)}Ni=1. To map inputs to outputs, we start by building a model of the joint probability distribution pθ(x, y). We use a log-linear parameterization with feature vector f and weight ve</context>
</contexts>
<marker>Ganchev, Das, 2013</marker>
<rawString>K. Ganchev and D. Das. 2013. Cross-lingual discriminative learning of sequence models with posterior regularization. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ganchev</author>
<author>J Gillenwater</author>
<author>B Taskar</author>
</authors>
<title>Dependency grammar induction via bitext projection constraints.</title>
<date>2009</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2091" citStr="Ganchev et al., 2009" startWordPosition="305" endWordPosition="308">tributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn l</context>
</contexts>
<marker>Ganchev, Gillenwater, Taskar, 2009</marker>
<rawString>K. Ganchev, J. Gillenwater, and B. Taskar. 2009. Dependency grammar induction via bitext projection constraints. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ganchev</author>
<author>J V Grac¸a</author>
<author>J Gillenwater</author>
<author>B Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>11</volume>
<marker>Ganchev, Grac¸a, Gillenwater, Taskar, 2010</marker>
<rawString>K. Ganchev, J. V. Grac¸a, J. Gillenwater, and B. Taskar. 2010. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gelling</author>
<author>T Cohn</author>
<author>P Blunsom</author>
<author>J V Grac¸a</author>
</authors>
<title>The PASCAL challenge on grammar induction.</title>
<date>2012</date>
<booktitle>In Proc. of NAACL-HLT Workshop on the Induction of Linguistic Structure.</booktitle>
<marker>Gelling, Cohn, Blunsom, Grac¸a, 2012</marker>
<rawString>D. Gelling, T. Cohn, P. Blunsom, and J. V. Grac¸a. 2012. The PASCAL challenge on grammar induction. In Proc. of NAACL-HLT Workshop on the Induction of Linguistic Structure.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Softmax-margin CRFs: Training log-linear models with cost functions.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="13528" citStr="Gimpel and Smith, 2010" startWordPosition="2148" endWordPosition="2151">th relatively rigid word order. However, there may still be certain transpositions that are benign, at least for grammaticality. To address this, we introduce an observation cost function A : X x X —* R≥0 that indicates how much two observations differ. Using A, we define the following gain function γCCE1(x(i)) = The function A inflates the score of neighborhood entries with larger differences from the observed x(i). This gain function is inspired by ideas from structured large-margin learning (Taskar et al., 2003; Tsochantaridis et al., 2005), specifically softmax-margin (Povey et al., 2008; Gimpel and Smith, 2010). Softmax-margin extends conditional likelihood by allowing the user to specify a cost function to give partial credit for structures that are partially correct. Conditional likelihood, by contrast, treats all incorrect structures equally. While softmax-margin uses a cost function to specify how two output structures differ, our gain function γCCE1 uses a cost function A to specify how two inputs differ. But the motivations are similar: since poor structures have their scores artificially inflated by A, learning pays more attention to them, choosing weights that penalize them more than the low</context>
</contexts>
<marker>Gimpel, Smith, 2010</marker>
<rawString>K. Gimpel and N. A. Smith. 2010. Softmax-margin CRFs: Training log-linear models with cost functions. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Structured ramp loss minimization for machine translation.</title>
<date>2012</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="16270" citStr="Gimpel and Smith (2012)" startWordPosition="2614" endWordPosition="2617"> x) as MATLM. 5 Expressing Structural Preferences Our second modification to CE allows us to specify structural preferences for outputs y. We first note that there exist objective functions for supervised structure prediction that never require computing the feature vector for the true output y(i). Examples include Bayes risk (Kaiser et al., 2000; Povey and Woodland, 2002) and structured ramp loss (Do et al., 2008). These two objectives do, however, need to compute a cost function cost(y(i), y), which requires the true output y(i). We start with the following form of structured ramp loss from Gimpel and Smith (2012), transformed here to a gain function: ax (θ&gt;f (x(i), y) − cost(y(i), y)) − y&apos;∈Yax (θ&gt;f (x(i), y0) + cost(y(i), y0 )) (1) Maximizing this gain function for supervised learning corresponds to increasing the model score l ] −log P(xj|xj−1)I xj−1xj �/ 2grams(x(i)) α |x|+1 E j=1 E { I log exp θ&gt;f(x(i), y) − y∈Y(x�i�) E E { I log exp θ&gt;f(x0, y0) + A(x(i), x0) x&apos;∈Ni y&apos;∈Y(x&apos;) AI(x(i), x) = α |x|+1 E j=1 1332 of outputs that have both high model score (θ&gt;f) and low cost, while decreasing the model score of outputs with high model score and high cost. For unsupervised learning, we do not have y(i), so </context>
</contexts>
<marker>Gimpel, Smith, 2012</marker>
<rawString>K. Gimpel and N. A. Smith. 2012. Structured ramp loss minimization for machine translation. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
</authors>
<title>Discriminative Feature-Rich Modeling for Syntax-Based Machine Translation.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="18189" citStr="Gimpel (2012)" startWordPosition="2979" endWordPosition="2980">96 CONJ 436649 1.68 PRON 461880 1.62 DET 615284 1.33 ADJ 694685 1.21 ADP 906922 0.95 VERB 1018989 0.83 . 1042662 0.81 NOUN 2337234 0 tag bigram count cost DET PRT 109 84.41 DET CONJ 518 68.82 NUM ADV 1587 57.63 NOUN NOUN 409828 2.09 DET NOUN 454980 1.04 NOUN . 504897 0 Table 1: Counts and costs for universal tags based on treebanks for 11 languages not used in POS induction experiments. X n o Where #(y) is the count of tag y in the treebank log exp θ&gt;f(x(i), y) − 7r(y) − concatenation, the cost of y is y∈Y(x(i)) X X n o �maxy0 #(y0) � log exp θ&gt;f(x0,y0) + 7r(y0) u(y) = log #(y) x0∈Ni y0∈Y(x0) Gimpel (2012) found that using such “softened” versions of the ramp losses worked better than the original versions (e.g., Eq. 1) when training machine translation systems. 5.1 Output Cost Functions The output cost 7r should capture our desiderata about y for the task of interest. We consider universal POS tag subsequences analogous to the universal syntactic rules of Naseem et al. (2010). In doing so, we use the universal tags of Petrov et al. (2012): NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner), ADP (pre/postposition), NUM (numeral), CONJ (conjunction), PRT (particle), ‘.’ </context>
</contexts>
<marker>Gimpel, 2012</marker>
<rawString>K. Gimpel. 2012. Discriminative Feature-Rich Modeling for Syntax-Based Machine Translation. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="5004" citStr="Goldwater and Griffiths, 2007" startWordPosition="752" endWordPosition="755">d CE as well as strong baselines from the shared task. In particular, our final average accuracies are better than all entries in the shared task that use the same number of tags. 2 Related Work Weakly-supervised techniques can be roughly categorized in terms of whether they influence the model, the learning procedure, or explicitly target the output structure. Examples abound in NLP; we focus on those that have been applied to POS tagging. There have been many efforts at biasing models, including features (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010), sparse priors (Johnson, 2007; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007), sparsity in tag transition distributions (Ravi and Knight, 2009), small models via minimum description length criteria (Vaswani et al., 2010; Poon et al., 2009), a one-tag-per-type constraint (Blunsom and Cohn, 2011), and power law effects via Bayesian nonparametrics (Van Gael et al., 2009; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised me</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>S. Goldwater and T. Griffiths. 2007. A fully Bayesian approach to unsupervised part-of-speech tagging. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J V Grac¸a</author>
<author>K Ganchev</author>
<author>L Coheur</author>
<author>F Pereira</author>
<author>B Taskar</author>
</authors>
<title>Controlling complexity in partof-speech induction.</title>
<date>2011</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>41</volume>
<issue>2</issue>
<marker>Grac¸a, Ganchev, Coheur, Pereira, Taskar, 2011</marker>
<rawString>J. V. Grac¸a, K. Ganchev, L. Coheur, F. Pereira, and B. Taskar. 2011. Controlling complexity in partof-speech induction. J. Artif. Int. Res., 41(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proc. of HLTNAACL.</booktitle>
<contexts>
<context position="10337" citStr="Haghighi and Klein, 2006" startWordPosition="1606" endWordPosition="1609">z } Z(θ) The difficulty is the final term, log Z(θ), which requires summing over all possible inputs and all valid outputs for them. This summation is typically intractable for structured problems, and may even diverge. For this reason, EM is typically only used to train log-linear model weights when Z(θ) = 1, e.g., for hidden Markov models, probabilistic context-free grammars, and models composed of locally-normalized log-linear models (Berg-Kirkpatrick et al., 2010), among others. There have been efforts at approximating the summation over elements of X, whether by limiting sequence length (Haghighi and Klein, 2006), only summing over observations in the training data (Riezler, 1999), restricting the observation space based on the task (Dyer et al., 2011), or using Gibbs sampling to obtain an unbiased sample of the full space (Della Pietra et al., 1997; Rosenfeld, 1997). Contrastive estimation (CE) addresses this challenge by using a neighborhood function N : X → 2X that generates a set of inputs that are “corruptions” of an input x; N(x) always includes x. Using shorthand Ni for N(x(i)), CE corresponds to maximizing the sum over inputs x(i) of the gain γCE(x(i))= log Pr(x(i) |Ni) P =log y∈Y(x(i)) pθ(x(i</context>
<context position="25923" citStr="Haghighi and Klein, 2006" startWordPosition="4316" endWordPosition="4319">em Combination We can avoid choosing a single model by combining the outputs of multiple models via system combination. We decode test data by using posterior decoding. To combine the outputs of multiple models, we find the max-posterior tag under each model, then choose the highest vote-getter, breaking ties arbitrarily. However, when doing POS induction without a tag dictionary, the tags are simply unique identifiers and may not have consistent meaning across runs. To address this, we propose a novel voting scheme that is inspired by the widely-used 1-to-1 accuracy metric for POS induction (Haghighi and Klein, 2006). This metric maps system tags to gold tags to maximize accuracy with the constraint that each gold tag is mapped to at most once. The optimal mapping can be found by solving a maximum weighted bipartite matching problem. We adapt this idea to map tags between two systems, rather than between system tags and gold tags. Given k systems that we want to combine, we choose one to be the backbone and map the remaining k − 1 systems’ outputs to the backbone.4 After mapping each system’s output to the backbone system, we perform simple majority voting among all k systems. To choose the backbone, we 4</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>A. Haghighi and D. Klein. 2006. Prototype-driven learning for sequence models. In Proc. of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1496" citStr="Johnson, 2007" startWordPosition="210" endWordPosition="211">SCAL 2012 shared task. 1 Introduction Unsupervised NLP aims to discover useful structure in unannotated text. This structure might be part-of-speech (POS) tag sequences (Merialdo, 1994), morphological segmentation (Creutz and Lagus, 2005), or syntactic structure (Klein and Manning, 2004), among others. Unsupervised systems typically improve when researchers incorporate knowledge to bias learning to capture characteristics of the desired structure.1 There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Sny</context>
<context position="4973" citStr="Johnson, 2007" startWordPosition="750" endWordPosition="751">er both standard CE as well as strong baselines from the shared task. In particular, our final average accuracies are better than all entries in the shared task that use the same number of tags. 2 Related Work Weakly-supervised techniques can be roughly categorized in terms of whether they influence the model, the learning procedure, or explicitly target the output structure. Examples abound in NLP; we focus on those that have been applied to POS tagging. There have been many efforts at biasing models, including features (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010), sparse priors (Johnson, 2007; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007), sparsity in tag transition distributions (Ravi and Knight, 2009), small models via minimum description length criteria (Vaswani et al., 2010; Poon et al., 2009), a one-tag-per-type constraint (Blunsom and Cohn, 2011), and power law effects via Bayesian nonparametrics (Van Gael et al., 2009; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasin</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>M. Johnson. 2007. Why doesn’t EM find good HMM POS-taggers? In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kaiser</author>
<author>B Horvat</author>
<author>Z Kacic</author>
</authors>
<title>A novel loss function for the overall risk criterion based discriminative training of HMM models.</title>
<date>2000</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="15995" citStr="Kaiser et al., 2000" startWordPosition="2565" endWordPosition="2568">og probability of the novel bigram: ALM(x(i), x) = where P(xj|xj−1) is obtained from a bigram language model. Among novel bigrams in the corruption x, if the second word is highly surprising conditioned on the first, the bigram will incur high cost. We refer to ALM(x(i), x) as MATLM. 5 Expressing Structural Preferences Our second modification to CE allows us to specify structural preferences for outputs y. We first note that there exist objective functions for supervised structure prediction that never require computing the feature vector for the true output y(i). Examples include Bayes risk (Kaiser et al., 2000; Povey and Woodland, 2002) and structured ramp loss (Do et al., 2008). These two objectives do, however, need to compute a cost function cost(y(i), y), which requires the true output y(i). We start with the following form of structured ramp loss from Gimpel and Smith (2012), transformed here to a gain function: ax (θ&gt;f (x(i), y) − cost(y(i), y)) − y&apos;∈Yax (θ&gt;f (x(i), y0) + cost(y(i), y0 )) (1) Maximizing this gain function for supervised learning corresponds to increasing the model score l ] −log P(xj|xj−1)I xj−1xj �/ 2grams(x(i)) α |x|+1 E j=1 E { I log exp θ&gt;f(x(i), y) − y∈Y(x�i�) E E { I lo</context>
</contexts>
<marker>Kaiser, Horvat, Kacic, 2000</marker>
<rawString>J. Kaiser, B. Horvat, and Z. Kacic. 2000. A novel loss function for the overall risk criterion based discriminative training of HMM models. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1171" citStr="Klein and Manning, 2004" startWordPosition="163" endWordPosition="166">e setting additional hyperparameters, which can be problematic in unsupervised learning, so we investigate new methods for unsupervised model selection and system combination. We instantiate these ideas for part-of-speech induction without tag dictionaries, improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task. 1 Introduction Unsupervised NLP aims to discover useful structure in unannotated text. This structure might be part-of-speech (POS) tag sequences (Merialdo, 1994), morphological segmentation (Creutz and Lagus, 2005), or syntactic structure (Klein and Manning, 2004), among others. Unsupervised systems typically improve when researchers incorporate knowledge to bias learning to capture characteristics of the desired structure.1 There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly </context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of MT Summit.</booktitle>
<contexts>
<context position="33400" citStr="Koehn, 2005" startWordPosition="5603" endWordPosition="5604"> CE, which we also compare to. When using both MATLM and UNIV simultaneously, we first choose the best two α values by the LL criterion and the best two Q values by the CE criterion when using only those individual costs. This gives us 4 pairs of values; we run experiments with these pairs and choose the pair to report using each of the model selection criteria. For system combination, we use the 4 system outputs resulting from these 4 pairs. For training bigram language models for the MATLM cost, we use the language’s POS training data concatenated with its portion of the Europarl v7 corpus (Koehn, 2005) and the text of its type. For unknown words at test time, we use the UNK emission feature, the Brown cluster features with the special UNK cluster identifiers, and the word’s actual spelling features. 8In subsequent experiments we tried C ∈ {0.01, 0.001} for the baseline CE setting and found minimal differences. 1336 neigh- cost mod. DA NL 1-1 PT 1-1 SL 1-1 SV 1-1 avg borhood sel. M-1 1-1 M-1 M-1 M-1 M-1 M-1 1-1 SHUFF10 none N/A 45.0 38.0 55.1 45.7 54.2 38.0 54.7 45.7 47.4 31.3 51.3 39.7 MATCH CE 48.9 31.5 56.5 46.4 54.2 37.7 55.9 46.8 48.9 33.8 52.9 39.2 LL 49.9 34.4 56.5 46.4 54.1 38.9 57.2</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proc. of MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Li</author>
<author>Z Wang</author>
<author>S Khudanpur</author>
<author>J Eisner</author>
</authors>
<title>Unsupervised discriminative language model training for machine translation using simulated confusion sets.</title>
<date>2010</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="6288" citStr="Li et al., 2010" startWordPosition="950" endWordPosition="953">ather, they change the learning. This may involve optimizing a different objective function for the same model, e.g., by switching from soft to hard EM (Spitkovsky et al., 2010b). Or it may involve changing the objective during learning via annealing (Smith and Eisner, 2004) or more general multi-objective techniques (Spitkovsky et al., 2011a; Spitkovsky et al., 2013). Other learning modifications relate to automatic data selection, e.g., choosing examples for generative learning (Spitkovsky et al., 2010a) or automatically generating negative examples for discriminative unsupervised learning (Li et al., 2010; Xiao et al., 2011). CE does both, automatically generating negative examples and changing the objective function to include them. Our observation cost function alters CE’s objective function, sharpening the effective distribution of the negative examples. 2.2 Structural Bias Our output cost function is used to directly specify preferences on desired output structures. Several others have had similar aims. For dependency grammar induction, Smith and Eisner (2006) favored short attachments using a fixed-weight feature whose weight was optionally annealed during learning. Their bias could be im</context>
</contexts>
<marker>Li, Wang, Khudanpur, Eisner, 2010</marker>
<rawString>Z. Li, Z. Wang, S. Khudanpur, and J. Eisner. 2010. Unsupervised discriminative language model training for machine translation using simulated confusion sets. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Li</author>
<author>J V Grac¸a</author>
<author>B Taskar</author>
</authors>
<title>Wiki-ly supervised part-of-speech tagging.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<marker>Li, Grac¸a, Taskar, 2012</marker>
<rawString>S. Li, J. V. Grac¸a, and B. Taskar. 2012. Wiki-ly supervised part-of-speech tagging. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>D Klein</author>
</authors>
<title>Online EM for unsupervised models.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="29779" citStr="Liang and Klein, 2009" startWordPosition="4965" endWordPosition="4968"> posterior regularization (PR), described by Grac¸a et al. (2011). The PR system achieved the highest average 1-1 accuracy in the shared task. We restrict our attention to systems that use 12 tags because the M-1 and 1-1 metrics are highly dependent upon the number of hypothesized tags. In general, using more tags leads to higher M-1 and lower 1-1 (Gelling et al., 2012). By keeping the number of tags fixed, we hope to provide a cleaner comparison among approaches. We compare to two other baselines: an HMM trained with 500 iterations of EM and an HMM trained with 100 iterations of stepwise EM (Liang and Klein, 2009). We used random initialization as done by Liang and Klein: we set each parameter in each multinomial to exp11 + c}, where c — U[0,1], then normalized to get probability distributions. For stepwise EM, we used minibatch size 3 and stepsize reduction power 0.7. For all models we trained, including both baselines and CCE, we used only the training data during training and used the unannotated development data for certain model selection criteria. No labels were used except for final evaluation on the test data. Therefore, we need a way to handle unknown words in test data. When running EM and st</context>
</contexts>
<marker>Liang, Klein, 2009</marker>
<rawString>P. Liang and D. Klein. 2009. Online EM for unsupervised models. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>B Taskar</author>
<author>D Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="1926" citStr="Liang et al., 2006" startWordPosition="280" endWordPosition="283">s of the desired structure.1 There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typicall</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>P. Liang, B. Taskar, and D. Klein. 2006. Alignment by agreement. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
</authors>
<title>Semi-supervised learning for natural language. Master’s thesis,</title>
<date>2005</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="30971" citStr="Liang, 2005" startWordPosition="5167" endWordPosition="5168">en running EM and stepwise EM, while reading in the final 10% of sentences in the training set, we replace novel words with the special token UNK. We then replace unknown words in test data with UNK. 8.1 CCE Setup Features We use standard indicator features on tag-tag transitions and tag-word emissions, the spelling features from Smith and Eisner (2005a), and additional emission features based on Brown clusters. The latter features are simply indicators for tag-cluster pairs—analogous to tag-word emissions in which the word is replaced by its Brown cluster identifier. We run Brown clustering (Liang, 2005) on the POS training data for each language, once with 12 clusters and once with 40, then add tag-cluster emission features for each clustering and one more for their conjunction.7 7To handle unknown words: for words that only appear in the final 10% of training sentences, we replace them with UNK when firing their tag-word emission features. We use special Brown cluster identifiers reserved for UNK. But we still use all spelling features derived from the actual word Learning We solve Eq. 2 and Eq. 3 by running LBFGS until convergence on the training data, up to 100 iterations. We tag the test</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>P. Liang. 2005. Semi-supervised learning for natural language. Master’s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="1068" citStr="Merialdo, 1994" startWordPosition="152" endWordPosition="153">ng structural preferences on the latent variable used to explain the observations. They require setting additional hyperparameters, which can be problematic in unsupervised learning, so we investigate new methods for unsupervised model selection and system combination. We instantiate these ideas for part-of-speech induction without tag dictionaries, improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task. 1 Introduction Unsupervised NLP aims to discover useful structure in unannotated text. This structure might be part-of-speech (POS) tag sequences (Merialdo, 1994), morphological segmentation (Creutz and Lagus, 2005), or syntactic structure (Klein and Manning, 2004), among others. Unsupervised systems typically improve when researchers incorporate knowledge to bias learning to capture characteristics of the desired structure.1 There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1We note that doing so strains the definition of the term</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>B. Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Naseem</author>
<author>B Snyder</author>
<author>J Eisenstein</author>
<author>R Barzilay</author>
</authors>
<title>Multilingual part-of-speech tagging: Two unsupervised approaches.</title>
<date>2009</date>
<journal>JAIR,</journal>
<volume>36</volume>
<contexts>
<context position="8302" citStr="Naseem et al., 2009" startWordPosition="1265" endWordPosition="1268">m leveraging existing resources. These may be curated or crowdsourced resources like the Wiktionary (Li et al., 2012), or traditional annotated treebanks for languages other than those under investigation (Cohen et al., 2011). In this paper, we use tag statistics from treebanks for 11 languages to impose our structural bias for a different set of languages used in our POS induction experiments. Substantial recent work has improved many NLP tasks by leveraging multilingual or parallel text (Cohen and Smith, 2009; Snyder et al., 2009; Wang and Manning, 2014), including unsupervised POS tagging (Naseem et al., 2009; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). This sort of multilingual guidance could also be captured by particular output cost functions, though we leave this to future work. 1330 3 Unsupervised Structure Learning We consider a structured unsupervised learning setting. We use X to denote our set of possible structured inputs, and for a particular x E X, we use Y(x) to denote the set of valid structured outputs for x. We are given a dataset of inputs {x(i)}Ni=1. To map inputs to outputs, we start by building a model of the joint probability distribution pθ(x, y). </context>
</contexts>
<marker>Naseem, Snyder, Eisenstein, Barzilay, 2009</marker>
<rawString>T. Naseem, B. Snyder, J. Eisenstein, and R. Barzilay. 2009. Multilingual part-of-speech tagging: Two unsupervised approaches. JAIR, 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Naseem</author>
<author>H Chen</author>
<author>R Barzilay</author>
<author>M Johnson</author>
</authors>
<title>Using universal linguistic knowledge to guide grammar induction.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="7097" citStr="Naseem et al. (2010)" startWordPosition="1072" endWordPosition="1075">ion, sharpening the effective distribution of the negative examples. 2.2 Structural Bias Our output cost function is used to directly specify preferences on desired output structures. Several others have had similar aims. For dependency grammar induction, Smith and Eisner (2006) favored short attachments using a fixed-weight feature whose weight was optionally annealed during learning. Their bias could be implemented as an output cost function in our framework. Posterior regularization (PR; Ganchev et al., 2010) is a general framework for declaratively specifying preferences on model outputs. Naseem et al. (2010) proposed universal syntactic rules for unsupervised dependency parsing and used them in a PR regime; we use analogous universal tag sequences in our cost function. Our output cost is similar to posterior regularization. The difference is that we specify preferences via an arbitrary cost function on output structures, while PR uses expectation constraints on posteriors of the model. We compare to the PR tag induction system of Grac¸a et al. (2011) in our experiments, improving over it in several settings. 2.3 Exploiting Resources Much of the work mentioned above also benefits from leveraging e</context>
<context position="18567" citStr="Naseem et al. (2010)" startWordPosition="3040" endWordPosition="3043">ction experiments. X n o Where #(y) is the count of tag y in the treebank log exp θ&gt;f(x(i), y) − 7r(y) − concatenation, the cost of y is y∈Y(x(i)) X X n o �maxy0 #(y0) � log exp θ&gt;f(x0,y0) + 7r(y0) u(y) = log #(y) x0∈Ni y0∈Y(x0) Gimpel (2012) found that using such “softened” versions of the ramp losses worked better than the original versions (e.g., Eq. 1) when training machine translation systems. 5.1 Output Cost Functions The output cost 7r should capture our desiderata about y for the task of interest. We consider universal POS tag subsequences analogous to the universal syntactic rules of Naseem et al. (2010). In doing so, we use the universal tags of Petrov et al. (2012): NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner), ADP (pre/postposition), NUM (numeral), CONJ (conjunction), PRT (particle), ‘.’ (punctuation), and X (other). We aimed for a set of rules that would be robust across languages. So, we used treebanks for 11 languages from the CoNLL 2006/2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) other than those used in our POS induction experiments. In particular, we used Arabic, Bulgarian, Catalan, Czech, English, Spanish, German, Hungarian, Italia</context>
</contexts>
<marker>Naseem, Chen, Barzilay, Johnson, 2010</marker>
<rawString>T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010. Using universal linguistic knowledge to guide grammar induction. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Nilsson</author>
<author>J Hall</author>
</authors>
<title>Talbanken05: A Swedish treebank with phrase structure and dependency annotation.</title>
<date>2006</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="27659" citStr="Nivre et al., 2006" startWordPosition="4615" endWordPosition="4618">e scheme (NA¨IVE) that performs majority voting without any tag mapping. 8 Experiments Task and Datasets We consider POS induction without tag dictionaries using five freely-available datasets from the PASCAL shared task (Gelling et al., 2012).5 These include Danish (DA), using the Copenhagen Dependency Treebank v2 (BuchKromann et al., 2007); Dutch (NL), using the Alpino treebank (Bouma et al., 2001); Portuguese (PT), using the Floresta Sint´a(c)tica treebank (Afonso et al., 2002); Slovene (SL), using the jos500k treebank (Erjavec et al., 2010); and Swedish (SV), using the Talbanken treebank (Nivre et al., 2006). We use their provided training, development, and test sets. Evaluation We fix the number of tags in our models to 12, which matches the number of universal tags from Petrov et al. (2012). We use both many-to-1 (M-1) and 1-to-1 (1-1) accuracy as our evaluation metrics, using the universal tags for the gold standard (which was done for the official evaluation for the shared task).6 We note that our 7r function assigns identities to tags (e.g., tag 1 is assumed to be NOUN), so we could use actual tagging accuracy when training with the 7r cost function. But we use M-1 and 1-1 accuracy to enable</context>
</contexts>
<marker>Nivre, Nilsson, Hall, 2006</marker>
<rawString>J. Nivre, J. Nilsson, and J. Hall. 2006. Talbanken05: A Swedish treebank with phrase structure and dependency annotation. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S K¨ubler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>shared task on dependency parsing.</title>
<date>2007</date>
<journal>The CoNLL</journal>
<booktitle>In Proc. of CoNLL.</booktitle>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Maximum-likelihood-sch¨atzung von wortkategorien mit verfahren der kombinatorischen optimierung. Bachelor’s thesis (Studienarbeit),</title>
<date>1995</date>
<location>Friedrich-Alexander-Universit¨at ErlangenN¨urnburg, Germany.</location>
<contexts>
<context position="29093" citStr="Och, 1995" startWordPosition="4850" endWordPosition="4851">6It is common to use a greedy algorithm to compute 1-to-1 accuracy, e.g., as in the shared task scoring script (http://www.dcs.shef.ac.uk/˜tcohn/ wils/eval.tar.gz), though the optimal mapping can be computed efficiently via the maximum weighted bipartite matching algorithm, as stated above. We use the shared task scorer for all results here for ease of comparison. When we instead evaluate using the optimal mapping, we find that accuracies are usually only slightly higher than those found by the greedy algorithm. 1335 BROWN clusters (Brown et al., 1992), clusters obtained using the mkcls tool (Och, 1995), and the featurized HMM with sparsity constraints trained using posterior regularization (PR), described by Grac¸a et al. (2011). The PR system achieved the highest average 1-1 accuracy in the shared task. We restrict our attention to systems that use 12 tags because the M-1 and 1-1 metrics are highly dependent upon the number of hypothesized tags. In general, using more tags leads to higher M-1 and lower 1-1 (Gelling et al., 2012). By keeping the number of tags fixed, we hope to provide a cleaner comparison among approaches. We compare to two other baselines: an HMM trained with 500 iteratio</context>
</contexts>
<marker>Och, 1995</marker>
<rawString>F. J. Och. 1995. Maximum-likelihood-sch¨atzung von wortkategorien mit verfahren der kombinatorischen optimierung. Bachelor’s thesis (Studienarbeit), Friedrich-Alexander-Universit¨at ErlangenN¨urnburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Das</author>
<author>R McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="18631" citStr="Petrov et al. (2012)" startWordPosition="3053" endWordPosition="3056"> treebank log exp θ&gt;f(x(i), y) − 7r(y) − concatenation, the cost of y is y∈Y(x(i)) X X n o �maxy0 #(y0) � log exp θ&gt;f(x0,y0) + 7r(y0) u(y) = log #(y) x0∈Ni y0∈Y(x0) Gimpel (2012) found that using such “softened” versions of the ramp losses worked better than the original versions (e.g., Eq. 1) when training machine translation systems. 5.1 Output Cost Functions The output cost 7r should capture our desiderata about y for the task of interest. We consider universal POS tag subsequences analogous to the universal syntactic rules of Naseem et al. (2010). In doing so, we use the universal tags of Petrov et al. (2012): NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner), ADP (pre/postposition), NUM (numeral), CONJ (conjunction), PRT (particle), ‘.’ (punctuation), and X (other). We aimed for a set of rules that would be robust across languages. So, we used treebanks for 11 languages from the CoNLL 2006/2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) other than those used in our POS induction experiments. In particular, we used Arabic, Bulgarian, Catalan, Czech, English, Spanish, German, Hungarian, Italian, Japanese, and Turkish. We replicated shorter treebanks a suff</context>
<context position="27847" citStr="Petrov et al. (2012)" startWordPosition="4649" endWordPosition="4652">asets from the PASCAL shared task (Gelling et al., 2012).5 These include Danish (DA), using the Copenhagen Dependency Treebank v2 (BuchKromann et al., 2007); Dutch (NL), using the Alpino treebank (Bouma et al., 2001); Portuguese (PT), using the Floresta Sint´a(c)tica treebank (Afonso et al., 2002); Slovene (SL), using the jos500k treebank (Erjavec et al., 2010); and Swedish (SV), using the Talbanken treebank (Nivre et al., 2006). We use their provided training, development, and test sets. Evaluation We fix the number of tags in our models to 12, which matches the number of universal tags from Petrov et al. (2012). We use both many-to-1 (M-1) and 1-to-1 (1-1) accuracy as our evaluation metrics, using the universal tags for the gold standard (which was done for the official evaluation for the shared task).6 We note that our 7r function assigns identities to tags (e.g., tag 1 is assumed to be NOUN), so we could use actual tagging accuracy when training with the 7r cost function. But we use M-1 and 1-1 accuracy to enable easier comparison both among different settings and to prior work. Baselines From the shared task, we compare to all entries that used 12 tags. These include 5http://wiki.cs.ox.ac.uk/ Ind</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>S. Petrov, D. Das, and R. McDonald. 2012. A universal part-of-speech tagset. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Poon</author>
<author>C Cherry</author>
<author>K Toutanova</author>
</authors>
<title>Unsupervised morphological segmentation with log-linear models.</title>
<date>2009</date>
<booktitle>In Proc. of HLT:</booktitle>
<publisher>NAACL.</publisher>
<contexts>
<context position="5196" citStr="Poon et al., 2009" startWordPosition="780" endWordPosition="783">upervised techniques can be roughly categorized in terms of whether they influence the model, the learning procedure, or explicitly target the output structure. Examples abound in NLP; we focus on those that have been applied to POS tagging. There have been many efforts at biasing models, including features (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010), sparse priors (Johnson, 2007; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007), sparsity in tag transition distributions (Ravi and Knight, 2009), small models via minimum description length criteria (Vaswani et al., 2010; Poon et al., 2009), a one-tag-per-type constraint (Blunsom and Cohn, 2011), and power law effects via Bayesian nonparametrics (Van Gael et al., 2009; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve optimizing a different objective function for the same model, e.g., by sw</context>
<context position="20918" citStr="Poon et al., 2009" startWordPosition="3463" endWordPosition="3466">ial. 1333 log exp n θT f (x(i), y) o − log X VEY(x(i)) x0ENi X n o exp θTf(x�, y�) (2) V0EY(x0) XN i=1 max e log X exp θTf(x(i), y) − π(y) − log n o X x0ENi VEY(x(i)) X n o exp θTf(x�, y�) + Δ(x(i), x�) + π(y�) (3) V0EY(x0) XN i=1 max e Figure 1: Contrastive estimation (Eq. 2) and cost-augmented contrastive estimation (Eq. 3). L2 regularization terms (2 E|θ |1 θ2j) are not shown here but were used in our experiments. accuracies even though the gold standard is not constrained in this way. This constraint can be encoded as an output cost function, though it would require approximate inference (Poon et al., 2009). 6 Cost-Augmented CE We extended the objective function underlying CE by defining two new types of cost functions, one on observations (§4) and one on outputs (§5). We combine them into a single objective, which we call cost-augmented contrastive estimation (CCE), shown as Eq. 3 in Figure 1. If the cost functions A and 7r factor in the same way as the features f, then it is straightforward to implement CCE atop an existing CE implementation. The additional terms in the cost functions can be implemented as features with fixed weights (albeit where the weight differs depending on the context). </context>
</contexts>
<marker>Poon, Cherry, Toutanova, 2009</marker>
<rawString>H. Poon, C. Cherry, and K. Toutanova. 2009. Unsupervised morphological segmentation with log-linear models. In Proc. of HLT: NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Povey</author>
<author>P C Woodland</author>
</authors>
<title>Minimum phone error and I-smoothing for improved discrimative training.</title>
<date>2002</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="16022" citStr="Povey and Woodland, 2002" startWordPosition="2569" endWordPosition="2572"> novel bigram: ALM(x(i), x) = where P(xj|xj−1) is obtained from a bigram language model. Among novel bigrams in the corruption x, if the second word is highly surprising conditioned on the first, the bigram will incur high cost. We refer to ALM(x(i), x) as MATLM. 5 Expressing Structural Preferences Our second modification to CE allows us to specify structural preferences for outputs y. We first note that there exist objective functions for supervised structure prediction that never require computing the feature vector for the true output y(i). Examples include Bayes risk (Kaiser et al., 2000; Povey and Woodland, 2002) and structured ramp loss (Do et al., 2008). These two objectives do, however, need to compute a cost function cost(y(i), y), which requires the true output y(i). We start with the following form of structured ramp loss from Gimpel and Smith (2012), transformed here to a gain function: ax (θ&gt;f (x(i), y) − cost(y(i), y)) − y&apos;∈Yax (θ&gt;f (x(i), y0) + cost(y(i), y0 )) (1) Maximizing this gain function for supervised learning corresponds to increasing the model score l ] −log P(xj|xj−1)I xj−1xj �/ 2grams(x(i)) α |x|+1 E j=1 E { I log exp θ&gt;f(x(i), y) − y∈Y(x�i�) E E { I log exp θ&gt;f(x0, y0) + A(x(i),</context>
</contexts>
<marker>Povey, Woodland, 2002</marker>
<rawString>D. Povey and P. C. Woodland. 2002. Minimum phone error and I-smoothing for improved discrimative training. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Povey</author>
<author>D Kanevsky</author>
<author>B Kingsbury</author>
<author>B Ramabhadran</author>
<author>G Saon</author>
<author>K Visweswariah</author>
</authors>
<title>Boosted MMI for model and feature space discriminative training.</title>
<date>2008</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="13503" citStr="Povey et al., 2008" startWordPosition="2144" endWordPosition="2147">ast for languages with relatively rigid word order. However, there may still be certain transpositions that are benign, at least for grammaticality. To address this, we introduce an observation cost function A : X x X —* R≥0 that indicates how much two observations differ. Using A, we define the following gain function γCCE1(x(i)) = The function A inflates the score of neighborhood entries with larger differences from the observed x(i). This gain function is inspired by ideas from structured large-margin learning (Taskar et al., 2003; Tsochantaridis et al., 2005), specifically softmax-margin (Povey et al., 2008; Gimpel and Smith, 2010). Softmax-margin extends conditional likelihood by allowing the user to specify a cost function to give partial credit for structures that are partially correct. Conditional likelihood, by contrast, treats all incorrect structures equally. While softmax-margin uses a cost function to specify how two output structures differ, our gain function γCCE1 uses a cost function A to specify how two inputs differ. But the motivations are similar: since poor structures have their scores artificially inflated by A, learning pays more attention to them, choosing weights that penali</context>
</contexts>
<marker>Povey, Kanevsky, Kingsbury, Ramabhadran, Saon, Visweswariah, 2008</marker>
<rawString>D. Povey, D. Kanevsky, B. Kingsbury, B. Ramabhadran, G. Saon, and K. Visweswariah. 2008. Boosted MMI for model and feature space discriminative training. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ravi</author>
<author>K Knight</author>
</authors>
<title>Minimized models for unsupervised part-of-speech tagging.</title>
<date>2009</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1519" citStr="Ravi and Knight, 2009" startWordPosition="212" endWordPosition="215">d task. 1 Introduction Unsupervised NLP aims to discover useful structure in unannotated text. This structure might be part-of-speech (POS) tag sequences (Merialdo, 1994), morphological segmentation (Creutz and Lagus, 2005), or syntactic structure (Klein and Manning, 2004), among others. Unsupervised systems typically improve when researchers incorporate knowledge to bias learning to capture characteristics of the desired structure.1 There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das a</context>
<context position="5100" citStr="Ravi and Knight, 2009" startWordPosition="765" endWordPosition="768">better than all entries in the shared task that use the same number of tags. 2 Related Work Weakly-supervised techniques can be roughly categorized in terms of whether they influence the model, the learning procedure, or explicitly target the output structure. Examples abound in NLP; we focus on those that have been applied to POS tagging. There have been many efforts at biasing models, including features (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010), sparse priors (Johnson, 2007; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007), sparsity in tag transition distributions (Ravi and Knight, 2009), small models via minimum description length criteria (Vaswani et al., 2010; Poon et al., 2009), a one-tag-per-type constraint (Blunsom and Cohn, 2011), and power law effects via Bayesian nonparametrics (Van Gael et al., 2009; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the lear</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>S. Ravi and K. Knight. 2009. Minimized models for unsupervised part-of-speech tagging. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
</authors>
<title>Probabilistic Constraint Logic Programming.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit¨at T¨ubingen.</institution>
<contexts>
<context position="10406" citStr="Riezler, 1999" startWordPosition="1618" endWordPosition="1619"> all possible inputs and all valid outputs for them. This summation is typically intractable for structured problems, and may even diverge. For this reason, EM is typically only used to train log-linear model weights when Z(θ) = 1, e.g., for hidden Markov models, probabilistic context-free grammars, and models composed of locally-normalized log-linear models (Berg-Kirkpatrick et al., 2010), among others. There have been efforts at approximating the summation over elements of X, whether by limiting sequence length (Haghighi and Klein, 2006), only summing over observations in the training data (Riezler, 1999), restricting the observation space based on the task (Dyer et al., 2011), or using Gibbs sampling to obtain an unbiased sample of the full space (Della Pietra et al., 1997; Rosenfeld, 1997). Contrastive estimation (CE) addresses this challenge by using a neighborhood function N : X → 2X that generates a set of inputs that are “corruptions” of an input x; N(x) always includes x. Using shorthand Ni for N(x(i)), CE corresponds to maximizing the sum over inputs x(i) of the gain γCE(x(i))= log Pr(x(i) |Ni) P =log y∈Y(x(i)) pθ(x(i), y) P P y0∈Y(x0) pθ(x0, y0) x0∈Ni X= log n o y∈Y(x(i)) exp θ&gt;f(x(i)</context>
</contexts>
<marker>Riezler, 1999</marker>
<rawString>S. Riezler. 1999. Probabilistic Constraint Logic Programming. Ph.D. thesis, Universit¨at T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A whole sentence maximum entropy language model.</title>
<date>1997</date>
<booktitle>In Proc. of ASRU.</booktitle>
<contexts>
<context position="10596" citStr="Rosenfeld, 1997" startWordPosition="1651" endWordPosition="1653">ain log-linear model weights when Z(θ) = 1, e.g., for hidden Markov models, probabilistic context-free grammars, and models composed of locally-normalized log-linear models (Berg-Kirkpatrick et al., 2010), among others. There have been efforts at approximating the summation over elements of X, whether by limiting sequence length (Haghighi and Klein, 2006), only summing over observations in the training data (Riezler, 1999), restricting the observation space based on the task (Dyer et al., 2011), or using Gibbs sampling to obtain an unbiased sample of the full space (Della Pietra et al., 1997; Rosenfeld, 1997). Contrastive estimation (CE) addresses this challenge by using a neighborhood function N : X → 2X that generates a set of inputs that are “corruptions” of an input x; N(x) always includes x. Using shorthand Ni for N(x(i)), CE corresponds to maximizing the sum over inputs x(i) of the gain γCE(x(i))= log Pr(x(i) |Ni) P =log y∈Y(x(i)) pθ(x(i), y) P P y0∈Y(x0) pθ(x0, y0) x0∈Ni X= log n o y∈Y(x(i)) exp θ&gt;f(x(i), y) − X X n o log exp θ&gt;f(x0, y0) x0∈Ni y0∈Y(x0) Two log Z(θ) terms cancel out, leaving the summation over input/output pairs in the neighborhood instead of the full summation over pairs. T</context>
</contexts>
<marker>Rosenfeld, 1997</marker>
<rawString>R. Rosenfeld. 1997. A whole sentence maximum entropy language model. In Proc. of ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Annealing techniques for unsupervised statistical language learning.</title>
<date>2004</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="5948" citStr="Smith and Eisner, 2004" startWordPosition="902" endWordPosition="905">9; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve optimizing a different objective function for the same model, e.g., by switching from soft to hard EM (Spitkovsky et al., 2010b). Or it may involve changing the objective during learning via annealing (Smith and Eisner, 2004) or more general multi-objective techniques (Spitkovsky et al., 2011a; Spitkovsky et al., 2013). Other learning modifications relate to automatic data selection, e.g., choosing examples for generative learning (Spitkovsky et al., 2010a) or automatically generating negative examples for discriminative unsupervised learning (Li et al., 2010; Xiao et al., 2011). CE does both, automatically generating negative examples and changing the objective function to include them. Our observation cost function alters CE’s objective function, sharpening the effective distribution of the negative examples. 2.</context>
</contexts>
<marker>Smith, Eisner, 2004</marker>
<rawString>N. A. Smith and J. Eisner. 2004. Annealing techniques for unsupervised statistical language learning. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2276" citStr="Smith and Eisner, 2005" startWordPosition="333" endWordPosition="336">ion of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of the corrupted alternatives. In this paper, we present a new objective function for weakly-supe</context>
<context position="4910" citStr="Smith and Eisner, 2005" startWordPosition="740" endWordPosition="743"> PASCAL shared task (Gelling et al., 2012). We find that CCE improves over both standard CE as well as strong baselines from the shared task. In particular, our final average accuracies are better than all entries in the shared task that use the same number of tags. 2 Related Work Weakly-supervised techniques can be roughly categorized in terms of whether they influence the model, the learning procedure, or explicitly target the output structure. Examples abound in NLP; we focus on those that have been applied to POS tagging. There have been many efforts at biasing models, including features (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010), sparse priors (Johnson, 2007; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007), sparsity in tag transition distributions (Ravi and Knight, 2009), small models via minimum description length criteria (Vaswani et al., 2010; Poon et al., 2009), a one-tag-per-type constraint (Blunsom and Cohn, 2011), and power law effects via Bayesian nonparametrics (Van Gael et al., 2009; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more</context>
<context position="11546" citStr="Smith and Eisner (2005" startWordPosition="1825" endWordPosition="1828">(x(i)) pθ(x(i), y) P P y0∈Y(x0) pθ(x0, y0) x0∈Ni X= log n o y∈Y(x(i)) exp θ&gt;f(x(i), y) − X X n o log exp θ&gt;f(x0, y0) x0∈Ni y0∈Y(x0) Two log Z(θ) terms cancel out, leaving the summation over input/output pairs in the neighborhood instead of the full summation over pairs. Two desiderata govern the choice of N. One is to make the summation over its elements computationally tractable. If N(x) = X for all x E X, we obtain EM, so a smaller neighborhood typically must be used in practice. The second consideration is to target learning for the task of interest. For POS tagging and dependency parsing, Smith and Eisner (2005a, 2005b) used neighborhood functions that corrupted the observations in systematic ways, e.g., their TRANS1 neighborhood contains the original sentence along with those that result from transposing a single pair of adjacent words. The intent was to force the learner to explain why the given sentences were observed at the expense of the corrupted sentences. Next we present our modifications to contrastive estimation. Both can be viewed as adding specialized cost functions that penalize some part of the structured input/output pair. 4 Modeling Corruption Costs While CE allows us to specify a se</context>
<context position="21803" citStr="Smith and Eisner (2005" startWordPosition="3614" endWordPosition="3617">), shown as Eq. 3 in Figure 1. If the cost functions A and 7r factor in the same way as the features f, then it is straightforward to implement CCE atop an existing CE implementation. The additional terms in the cost functions can be implemented as features with fixed weights (albeit where the weight differs depending on the context). 7 Model Selection Our modifications give increased flexibility, but require setting new hyperparameters. In addition to the choice of the cost functions, each has a weight: α for A and Q for 7r. We need ways to set these weights that do not require labeled data. Smith and Eisner (2005a) chose the hyperparameter values that yielded the best CE objective on held-out development data. We use their strategy, though we experiment with two others as well.3 In particular, we estimate held-out data loglikelihood via the method of Bengio et al. (2013) and also consider ways of combining outputs from multiple models. 7.1 Estimating Held-Out Log-Likelihood Bengio et al. (2013) recently proposed ways to efficiently estimate held-out data log-likelihood 3When using their strategy for CCE, we compute the CE criterion only, omitting the costs. We do so because the weights of the cost ter</context>
<context position="30713" citStr="Smith and Eisner (2005" startWordPosition="5127" endWordPosition="5130"> and CCE, we used only the training data during training and used the unannotated development data for certain model selection criteria. No labels were used except for final evaluation on the test data. Therefore, we need a way to handle unknown words in test data. When running EM and stepwise EM, while reading in the final 10% of sentences in the training set, we replace novel words with the special token UNK. We then replace unknown words in test data with UNK. 8.1 CCE Setup Features We use standard indicator features on tag-tag transitions and tag-word emissions, the spelling features from Smith and Eisner (2005a), and additional emission features based on Brown clusters. The latter features are simply indicators for tag-cluster pairs—analogous to tag-word emissions in which the word is replaced by its Brown cluster identifier. We run Brown clustering (Liang, 2005) on the POS training data for each language, once with 12 clusters and once with 40, then add tag-cluster emission features for each clustering and one more for their conjunction.7 7To handle unknown words: for words that only appear in the final 10% of training sentences, we replace them with UNK when firing their tag-word emission feature</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>N. A. Smith and J. Eisner. 2005a. Contrastive estimation: Training log-linear models on unlabeled data. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Guiding unsupervised grammar induction using contrastive estimation.</title>
<date>2005</date>
<booktitle>In Proc. of IJCAI Workshop on Grammatical Inference Applications.</booktitle>
<contexts>
<context position="2276" citStr="Smith and Eisner, 2005" startWordPosition="333" endWordPosition="336">ion of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of the corrupted alternatives. In this paper, we present a new objective function for weakly-supe</context>
<context position="4910" citStr="Smith and Eisner, 2005" startWordPosition="740" endWordPosition="743"> PASCAL shared task (Gelling et al., 2012). We find that CCE improves over both standard CE as well as strong baselines from the shared task. In particular, our final average accuracies are better than all entries in the shared task that use the same number of tags. 2 Related Work Weakly-supervised techniques can be roughly categorized in terms of whether they influence the model, the learning procedure, or explicitly target the output structure. Examples abound in NLP; we focus on those that have been applied to POS tagging. There have been many efforts at biasing models, including features (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010), sparse priors (Johnson, 2007; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007), sparsity in tag transition distributions (Ravi and Knight, 2009), small models via minimum description length criteria (Vaswani et al., 2010; Poon et al., 2009), a one-tag-per-type constraint (Blunsom and Cohn, 2011), and power law effects via Bayesian nonparametrics (Van Gael et al., 2009; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more</context>
<context position="11546" citStr="Smith and Eisner (2005" startWordPosition="1825" endWordPosition="1828">(x(i)) pθ(x(i), y) P P y0∈Y(x0) pθ(x0, y0) x0∈Ni X= log n o y∈Y(x(i)) exp θ&gt;f(x(i), y) − X X n o log exp θ&gt;f(x0, y0) x0∈Ni y0∈Y(x0) Two log Z(θ) terms cancel out, leaving the summation over input/output pairs in the neighborhood instead of the full summation over pairs. Two desiderata govern the choice of N. One is to make the summation over its elements computationally tractable. If N(x) = X for all x E X, we obtain EM, so a smaller neighborhood typically must be used in practice. The second consideration is to target learning for the task of interest. For POS tagging and dependency parsing, Smith and Eisner (2005a, 2005b) used neighborhood functions that corrupted the observations in systematic ways, e.g., their TRANS1 neighborhood contains the original sentence along with those that result from transposing a single pair of adjacent words. The intent was to force the learner to explain why the given sentences were observed at the expense of the corrupted sentences. Next we present our modifications to contrastive estimation. Both can be viewed as adding specialized cost functions that penalize some part of the structured input/output pair. 4 Modeling Corruption Costs While CE allows us to specify a se</context>
<context position="21803" citStr="Smith and Eisner (2005" startWordPosition="3614" endWordPosition="3617">), shown as Eq. 3 in Figure 1. If the cost functions A and 7r factor in the same way as the features f, then it is straightforward to implement CCE atop an existing CE implementation. The additional terms in the cost functions can be implemented as features with fixed weights (albeit where the weight differs depending on the context). 7 Model Selection Our modifications give increased flexibility, but require setting new hyperparameters. In addition to the choice of the cost functions, each has a weight: α for A and Q for 7r. We need ways to set these weights that do not require labeled data. Smith and Eisner (2005a) chose the hyperparameter values that yielded the best CE objective on held-out development data. We use their strategy, though we experiment with two others as well.3 In particular, we estimate held-out data loglikelihood via the method of Bengio et al. (2013) and also consider ways of combining outputs from multiple models. 7.1 Estimating Held-Out Log-Likelihood Bengio et al. (2013) recently proposed ways to efficiently estimate held-out data log-likelihood 3When using their strategy for CCE, we compute the CE criterion only, omitting the costs. We do so because the weights of the cost ter</context>
<context position="30713" citStr="Smith and Eisner (2005" startWordPosition="5127" endWordPosition="5130"> and CCE, we used only the training data during training and used the unannotated development data for certain model selection criteria. No labels were used except for final evaluation on the test data. Therefore, we need a way to handle unknown words in test data. When running EM and stepwise EM, while reading in the final 10% of sentences in the training set, we replace novel words with the special token UNK. We then replace unknown words in test data with UNK. 8.1 CCE Setup Features We use standard indicator features on tag-tag transitions and tag-word emissions, the spelling features from Smith and Eisner (2005a), and additional emission features based on Brown clusters. The latter features are simply indicators for tag-cluster pairs—analogous to tag-word emissions in which the word is replaced by its Brown cluster identifier. We run Brown clustering (Liang, 2005) on the POS training data for each language, once with 12 clusters and once with 40, then add tag-cluster emission features for each clustering and one more for their conjunction.7 7To handle unknown words: for words that only appear in the final 10% of training sentences, we replace them with UNK when firing their tag-word emission feature</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>N. A. Smith and J. Eisner. 2005b. Guiding unsupervised grammar induction using contrastive estimation. In Proc. of IJCAI Workshop on Grammatical Inference Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Annealing structural bias in multilingual weighted grammar induction.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL.</booktitle>
<contexts>
<context position="1609" citStr="Smith and Eisner, 2006" startWordPosition="225" endWordPosition="228"> text. This structure might be part-of-speech (POS) tag sequences (Merialdo, 1994), morphological segmentation (Creutz and Lagus, 2005), or syntactic structure (Klein and Manning, 2004), among others. Unsupervised systems typically improve when researchers incorporate knowledge to bias learning to capture characteristics of the desired structure.1 There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011</context>
<context position="6756" citStr="Smith and Eisner (2006)" startWordPosition="1021" endWordPosition="1024"> for generative learning (Spitkovsky et al., 2010a) or automatically generating negative examples for discriminative unsupervised learning (Li et al., 2010; Xiao et al., 2011). CE does both, automatically generating negative examples and changing the objective function to include them. Our observation cost function alters CE’s objective function, sharpening the effective distribution of the negative examples. 2.2 Structural Bias Our output cost function is used to directly specify preferences on desired output structures. Several others have had similar aims. For dependency grammar induction, Smith and Eisner (2006) favored short attachments using a fixed-weight feature whose weight was optionally annealed during learning. Their bias could be implemented as an output cost function in our framework. Posterior regularization (PR; Ganchev et al., 2010) is a general framework for declaratively specifying preferences on model outputs. Naseem et al. (2010) proposed universal syntactic rules for unsupervised dependency parsing and used them in a PR regime; we use analogous universal tag sequences in our cost function. Our output cost is similar to posterior regularization. The difference is that we specify pref</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>N. A. Smith and J. Eisner. 2006. Annealing structural bias in multilingual weighted grammar induction. In Proc. of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Parser adaptation and projection with quasi-synchronous features.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2069" citStr="Smith and Eisner, 2009" startWordPosition="301" endWordPosition="304"> sparsity in POS tag distributions (Johnson, 2007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; th</context>
</contexts>
<marker>Smith, Eisner, 2009</marker>
<rawString>D. A. Smith and J. Eisner. 2009. Parser adaptation and projection with quasi-synchronous features. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>T Naseem</author>
<author>R Barzilay</author>
</authors>
<title>Unsupervised multilingual grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="2112" citStr="Snyder et al., 2009" startWordPosition="309" endWordPosition="312">007; Ravi and Knight, 2009; Ganchev et al., 2010), short attachments for dependency parsing (Smith and Eisner, 2006), 1We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that </context>
<context position="8220" citStr="Snyder et al., 2009" startWordPosition="1252" endWordPosition="1255">tings. 2.3 Exploiting Resources Much of the work mentioned above also benefits from leveraging existing resources. These may be curated or crowdsourced resources like the Wiktionary (Li et al., 2012), or traditional annotated treebanks for languages other than those under investigation (Cohen et al., 2011). In this paper, we use tag statistics from treebanks for 11 languages to impose our structural bias for a different set of languages used in our POS induction experiments. Substantial recent work has improved many NLP tasks by leveraging multilingual or parallel text (Cohen and Smith, 2009; Snyder et al., 2009; Wang and Manning, 2014), including unsupervised POS tagging (Naseem et al., 2009; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). This sort of multilingual guidance could also be captured by particular output cost functions, though we leave this to future work. 1330 3 Unsupervised Structure Learning We consider a structured unsupervised learning setting. We use X to denote our set of possible structured inputs, and for a particular x E X, we use Y(x) to denote the set of valid structured outputs for x. We are given a dataset of inputs {x(i)}Ni=1. To map inputs to outp</context>
</contexts>
<marker>Snyder, Naseem, Barzilay, 2009</marker>
<rawString>B. Snyder, T. Naseem, and R. Barzilay. 2009. Unsupervised multilingual grammar induction. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>From Baby Steps to Leapfrog: How “Less is More” in unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="2183" citStr="Spitkovsky et al., 2010" startWordPosition="320" endWordPosition="323">s for dependency parsing (Smith and Eisner, 2006), 1We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of t</context>
<context position="5849" citStr="Spitkovsky et al., 2010" startWordPosition="886" endWordPosition="889">t (Blunsom and Cohn, 2011), and power law effects via Bayesian nonparametrics (Van Gael et al., 2009; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve optimizing a different objective function for the same model, e.g., by switching from soft to hard EM (Spitkovsky et al., 2010b). Or it may involve changing the objective during learning via annealing (Smith and Eisner, 2004) or more general multi-objective techniques (Spitkovsky et al., 2011a; Spitkovsky et al., 2013). Other learning modifications relate to automatic data selection, e.g., choosing examples for generative learning (Spitkovsky et al., 2010a) or automatically generating negative examples for discriminative unsupervised learning (Li et al., 2010; Xiao et al., 2011). CE does both, automatically generating negative examples and changing the objective function to include them. Our observation cost function</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2010</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a. From Baby Steps to Leapfrog: How “Less is More” in unsupervised dependency parsing. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Viterbi training improves unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="2183" citStr="Spitkovsky et al., 2010" startWordPosition="320" endWordPosition="323">s for dependency parsing (Smith and Eisner, 2006), 1We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of t</context>
<context position="5849" citStr="Spitkovsky et al., 2010" startWordPosition="886" endWordPosition="889">t (Blunsom and Cohn, 2011), and power law effects via Bayesian nonparametrics (Van Gael et al., 2009; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve optimizing a different objective function for the same model, e.g., by switching from soft to hard EM (Spitkovsky et al., 2010b). Or it may involve changing the objective during learning via annealing (Smith and Eisner, 2004) or more general multi-objective techniques (Spitkovsky et al., 2011a; Spitkovsky et al., 2013). Other learning modifications relate to automatic data selection, e.g., choosing examples for generative learning (Spitkovsky et al., 2010a) or automatically generating negative examples for discriminative unsupervised learning (Li et al., 2010; Xiao et al., 2011). CE does both, automatically generating negative examples and changing the objective function to include them. Our observation cost function</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, Manning, 2010</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Manning. 2010b. Viterbi training improves unsupervised dependency parsing. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>D Jurafsky</author>
<author>H Alshawi</author>
</authors>
<title>Profiting from mark-up: Hyper-text annotations for guided parsing.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2183" citStr="Spitkovsky et al., 2010" startWordPosition="320" endWordPosition="323">s for dependency parsing (Smith and Eisner, 2006), 1We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of t</context>
<context position="5849" citStr="Spitkovsky et al., 2010" startWordPosition="886" endWordPosition="889">t (Blunsom and Cohn, 2011), and power law effects via Bayesian nonparametrics (Van Gael et al., 2009; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve optimizing a different objective function for the same model, e.g., by switching from soft to hard EM (Spitkovsky et al., 2010b). Or it may involve changing the objective during learning via annealing (Smith and Eisner, 2004) or more general multi-objective techniques (Spitkovsky et al., 2011a; Spitkovsky et al., 2013). Other learning modifications relate to automatic data selection, e.g., choosing examples for generative learning (Spitkovsky et al., 2010a) or automatically generating negative examples for discriminative unsupervised learning (Li et al., 2010; Xiao et al., 2011). CE does both, automatically generating negative examples and changing the objective function to include them. Our observation cost function</context>
</contexts>
<marker>Spitkovsky, Jurafsky, Alshawi, 2010</marker>
<rawString>V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010c. Profiting from mark-up: Hyper-text annotations for guided parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>Lateen EM: Unsupervised training with multiple objectives, applied to dependency grammar induction.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2209" citStr="Spitkovsky et al., 2011" startWordPosition="324" endWordPosition="327">Smith and Eisner, 2006), 1We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of the corrupted alternatives.</context>
<context position="6016" citStr="Spitkovsky et al., 2011" startWordPosition="912" endWordPosition="915">n efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve optimizing a different objective function for the same model, e.g., by switching from soft to hard EM (Spitkovsky et al., 2010b). Or it may involve changing the objective during learning via annealing (Smith and Eisner, 2004) or more general multi-objective techniques (Spitkovsky et al., 2011a; Spitkovsky et al., 2013). Other learning modifications relate to automatic data selection, e.g., choosing examples for generative learning (Spitkovsky et al., 2010a) or automatically generating negative examples for discriminative unsupervised learning (Li et al., 2010; Xiao et al., 2011). CE does both, automatically generating negative examples and changing the objective function to include them. Our observation cost function alters CE’s objective function, sharpening the effective distribution of the negative examples. 2.2 Structural Bias Our output cost function is used to directly speci</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2011</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011a. Lateen EM: Unsupervised training with multiple objectives, applied to dependency grammar induction. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>Punctuation: Making a point in unsupervised dependency parsing.</title>
<date>2011</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="2209" citStr="Spitkovsky et al., 2011" startWordPosition="324" endWordPosition="327">Smith and Eisner, 2006), 1We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge. agreement of word alignment models (Liang et al., 2006), power law effects in lexical distributions (Blunsom and Cohn, 2010; Blunsom and Cohn, 2011), multilingual constraints (Smith and Eisner, 2009; Ganchev et al., 2009; Snyder et al., 2009; Das and Petrov, 2011), and orthographic cues (Spitkovsky et al., 2010c; Spitkovsky et al., 2011b), inter alia. Contrastive estimation (CE; Smith and Eisner, 2005) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the observations at the expense of those in a particular neighborhood of each observation. The neighborhood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of the corrupted alternatives.</context>
<context position="6016" citStr="Spitkovsky et al., 2011" startWordPosition="912" endWordPosition="915">n efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve optimizing a different objective function for the same model, e.g., by switching from soft to hard EM (Spitkovsky et al., 2010b). Or it may involve changing the objective during learning via annealing (Smith and Eisner, 2004) or more general multi-objective techniques (Spitkovsky et al., 2011a; Spitkovsky et al., 2013). Other learning modifications relate to automatic data selection, e.g., choosing examples for generative learning (Spitkovsky et al., 2010a) or automatically generating negative examples for discriminative unsupervised learning (Li et al., 2010; Xiao et al., 2011). CE does both, automatically generating negative examples and changing the objective function to include them. Our observation cost function alters CE’s objective function, sharpening the effective distribution of the negative examples. 2.2 Structural Bias Our output cost function is used to directly speci</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2011</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b. Punctuation: Making a point in unsupervised dependency parsing. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>Breaking out of local optima with count transforms and model recombination: A study in grammar induction.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="6043" citStr="Spitkovsky et al., 2013" startWordPosition="916" endWordPosition="919"> into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve optimizing a different objective function for the same model, e.g., by switching from soft to hard EM (Spitkovsky et al., 2010b). Or it may involve changing the objective during learning via annealing (Smith and Eisner, 2004) or more general multi-objective techniques (Spitkovsky et al., 2011a; Spitkovsky et al., 2013). Other learning modifications relate to automatic data selection, e.g., choosing examples for generative learning (Spitkovsky et al., 2010a) or automatically generating negative examples for discriminative unsupervised learning (Li et al., 2010; Xiao et al., 2011). CE does both, automatically generating negative examples and changing the objective function to include them. Our observation cost function alters CE’s objective function, sharpening the effective distribution of the negative examples. 2.2 Structural Bias Our output cost function is used to directly specify preferences on desired o</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2013</marker>
<rawString>V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2013. Breaking out of local optima with count transforms and model recombination: A study in grammar induction. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM—an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="34854" citStr="Stolcke, 2002" startWordPosition="5875" endWordPosition="5876">CE 58.5 42.5 66.3 53.3 70.6 43.3 59.1 45.6 59.3 54.2 62.7 47.8 LL 58.8 42.8 66.3 53.3 70.6 43.3 60.3 43.7 59.8 54.9 63.1 47.6 MATLM CE 59.4 43.5 63.8 50.1 70.2 43.0 58.5 46.1 59.2 54.8 62.2 47.5 LL 58.7 42.8 66.5 60.4 70.5 43.6 59.1 47.7 59.2 54.8 62.8 49.9 Table 2: Results for observation cost functions. The CE baseline corresponds to rows where cost=“none”. Other rows are CCE. Best score for each column and each neighborhood is bold. Wikipedia. The word counts for the Wikipedias used range from 18M for Slovene to 1.9B for Dutch. We used modified Kneser-Ney smoothing as implemented by SRILM (Stolcke, 2002). 8.2 Results We present two sets of results. First we compare our MATCH and MATLM observation cost functions for our two neighborhoods and two ways of doing model selection. Then we do a broader comparison, comparing both types of costs and their combination to our full set of baselines. Observation Cost Functions In Table 2, we show results for observation cost functions. We note that the TRANS1 neighborhood works much better than the SHUFF10 neighborhood, but we find that using cost functions can close the gap in certain cases, particularly for Dutch and Slovene for which the SHUFF10 MATLM </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM—an extensible language modeling toolkit. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O T¨ackstr¨om</author>
<author>D Das</author>
<author>S Petrov</author>
<author>R McDonald</author>
<author>J Nivre</author>
</authors>
<title>Token and type constraints for crosslingual part-of-speech tagging.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>1</volume>
<marker>T¨ackstr¨om, Das, Petrov, McDonald, Nivre, 2013</marker>
<rawString>O. T¨ackstr¨om, D. Das, S. Petrov, R. McDonald, and J. Nivre. 2013. Token and type constraints for crosslingual part-of-speech tagging. Transactions of the Association for Computational Linguistics, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Maxmargin Markov networks.</title>
<date>2003</date>
<booktitle>In Advances in NIPS 16.</booktitle>
<contexts>
<context position="13424" citStr="Taskar et al., 2003" startWordPosition="2133" endWordPosition="2136">ts. These kinds of corruptions are expected to be more frequently harmful, at least for languages with relatively rigid word order. However, there may still be certain transpositions that are benign, at least for grammaticality. To address this, we introduce an observation cost function A : X x X —* R≥0 that indicates how much two observations differ. Using A, we define the following gain function γCCE1(x(i)) = The function A inflates the score of neighborhood entries with larger differences from the observed x(i). This gain function is inspired by ideas from structured large-margin learning (Taskar et al., 2003; Tsochantaridis et al., 2005), specifically softmax-margin (Povey et al., 2008; Gimpel and Smith, 2010). Softmax-margin extends conditional likelihood by allowing the user to specify a cost function to give partial credit for structures that are partially correct. Conditional likelihood, by contrast, treats all incorrect structures equally. While softmax-margin uses a cost function to specify how two output structures differ, our gain function γCCE1 uses a cost function A to specify how two inputs differ. But the motivations are similar: since poor structures have their scores artificially in</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>B. Taskar, C. Guestrin, and D. Koller. 2003. Maxmargin Markov networks. In Advances in NIPS 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>M Johnson</author>
</authors>
<title>A Bayesian LDA-based model for semi-supervised part-ofspeech tagging.</title>
<date>2007</date>
<booktitle>In Advances in NIPS.</booktitle>
<contexts>
<context position="5034" citStr="Toutanova and Johnson, 2007" startWordPosition="756" endWordPosition="759">s from the shared task. In particular, our final average accuracies are better than all entries in the shared task that use the same number of tags. 2 Related Work Weakly-supervised techniques can be roughly categorized in terms of whether they influence the model, the learning procedure, or explicitly target the output structure. Examples abound in NLP; we focus on those that have been applied to POS tagging. There have been many efforts at biasing models, including features (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010), sparse priors (Johnson, 2007; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007), sparsity in tag transition distributions (Ravi and Knight, 2009), small models via minimum description length criteria (Vaswani et al., 2010; Poon et al., 2009), a one-tag-per-type constraint (Blunsom and Cohn, 2011), and power law effects via Bayesian nonparametrics (Van Gael et al., 2009; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model </context>
</contexts>
<marker>Toutanova, Johnson, 2007</marker>
<rawString>K. Toutanova and M. Johnson. 2007. A Bayesian LDA-based model for semi-supervised part-ofspeech tagging. In Advances in NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Joachims</author>
<author>T Hofmann</author>
<author>Y Altun</author>
</authors>
<title>Large margin methods for structured and interdependent output variables.</title>
<date>2005</date>
<journal>Journal ofMachine Learning Research,</journal>
<volume>6</volume>
<contexts>
<context position="13454" citStr="Tsochantaridis et al., 2005" startWordPosition="2137" endWordPosition="2140">rruptions are expected to be more frequently harmful, at least for languages with relatively rigid word order. However, there may still be certain transpositions that are benign, at least for grammaticality. To address this, we introduce an observation cost function A : X x X —* R≥0 that indicates how much two observations differ. Using A, we define the following gain function γCCE1(x(i)) = The function A inflates the score of neighborhood entries with larger differences from the observed x(i). This gain function is inspired by ideas from structured large-margin learning (Taskar et al., 2003; Tsochantaridis et al., 2005), specifically softmax-margin (Povey et al., 2008; Gimpel and Smith, 2010). Softmax-margin extends conditional likelihood by allowing the user to specify a cost function to give partial credit for structures that are partially correct. Conditional likelihood, by contrast, treats all incorrect structures equally. While softmax-margin uses a cost function to specify how two output structures differ, our gain function γCCE1 uses a cost function A to specify how two inputs differ. But the motivations are similar: since poor structures have their scores artificially inflated by A, learning pays mor</context>
</contexts>
<marker>Tsochantaridis, Joachims, Hofmann, Altun, 2005</marker>
<rawString>I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. 2005. Large margin methods for structured and interdependent output variables. Journal ofMachine Learning Research, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Van Gael</author>
<author>A Vlachos</author>
<author>Z Ghahramani</author>
</authors>
<title>The infinite HMM for unsupervised POS tagging.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<marker>Van Gael, Vlachos, Ghahramani, 2009</marker>
<rawString>J. Van Gael, A. Vlachos, and Z. Ghahramani. 2009. The infinite HMM for unsupervised POS tagging. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vaswani</author>
<author>A Pauls</author>
<author>D Chiang</author>
</authors>
<title>Efficient optimization of an MDL-inspired objective function for unsupervised part-of-speech tagging.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="5176" citStr="Vaswani et al., 2010" startWordPosition="776" endWordPosition="779"> Related Work Weakly-supervised techniques can be roughly categorized in terms of whether they influence the model, the learning procedure, or explicitly target the output structure. Examples abound in NLP; we focus on those that have been applied to POS tagging. There have been many efforts at biasing models, including features (Smith and Eisner, 2005a; Berg-Kirkpatrick et al., 2010), sparse priors (Johnson, 2007; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007), sparsity in tag transition distributions (Ravi and Knight, 2009), small models via minimum description length criteria (Vaswani et al., 2010; Poon et al., 2009), a one-tag-per-type constraint (Blunsom and Cohn, 2011), and power law effects via Bayesian nonparametrics (Van Gael et al., 2009; Blunsom and Cohn, 2010; Blunsom and Cohn, 2011). We focus below on efforts that induce bias into the learning (§2.1) or more directly in the output structure (§2.2), as they are more closely related to our contributions in this paper. 2.1 Biasing Learning Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve optimizing a different objective function for the sam</context>
</contexts>
<marker>Vaswani, Pauls, Chiang, 2010</marker>
<rawString>A. Vaswani, A. Pauls, and D. Chiang. 2010. Efficient optimization of an MDL-inspired objective function for unsupervised part-of-speech tagging. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wang</author>
<author>C D Manning</author>
</authors>
<title>Cross-lingual projected expectation regularization for weakly supervised learning.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>2</volume>
<contexts>
<context position="8245" citStr="Wang and Manning, 2014" startWordPosition="1256" endWordPosition="1259"> Resources Much of the work mentioned above also benefits from leveraging existing resources. These may be curated or crowdsourced resources like the Wiktionary (Li et al., 2012), or traditional annotated treebanks for languages other than those under investigation (Cohen et al., 2011). In this paper, we use tag statistics from treebanks for 11 languages to impose our structural bias for a different set of languages used in our POS induction experiments. Substantial recent work has improved many NLP tasks by leveraging multilingual or parallel text (Cohen and Smith, 2009; Snyder et al., 2009; Wang and Manning, 2014), including unsupervised POS tagging (Naseem et al., 2009; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). This sort of multilingual guidance could also be captured by particular output cost functions, though we leave this to future work. 1330 3 Unsupervised Structure Learning We consider a structured unsupervised learning setting. We use X to denote our set of possible structured inputs, and for a particular x E X, we use Y(x) to denote the set of valid structured outputs for x. We are given a dataset of inputs {x(i)}Ni=1. To map inputs to outputs, we start by building</context>
</contexts>
<marker>Wang, Manning, 2014</marker>
<rawString>M. Wang and C. D. Manning. 2014. Cross-lingual projected expectation regularization for weakly supervised learning. Transactions of the Association for Computational Linguistics, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Xiao</author>
<author>Y Liu</author>
<author>Q Liu</author>
<author>S Lin</author>
</authors>
<title>Fast generation of translation forest for large-scale SMT discriminative training.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="6308" citStr="Xiao et al., 2011" startWordPosition="954" endWordPosition="957">e the learning. This may involve optimizing a different objective function for the same model, e.g., by switching from soft to hard EM (Spitkovsky et al., 2010b). Or it may involve changing the objective during learning via annealing (Smith and Eisner, 2004) or more general multi-objective techniques (Spitkovsky et al., 2011a; Spitkovsky et al., 2013). Other learning modifications relate to automatic data selection, e.g., choosing examples for generative learning (Spitkovsky et al., 2010a) or automatically generating negative examples for discriminative unsupervised learning (Li et al., 2010; Xiao et al., 2011). CE does both, automatically generating negative examples and changing the objective function to include them. Our observation cost function alters CE’s objective function, sharpening the effective distribution of the negative examples. 2.2 Structural Bias Our output cost function is used to directly specify preferences on desired output structures. Several others have had similar aims. For dependency grammar induction, Smith and Eisner (2006) favored short attachments using a fixed-weight feature whose weight was optionally annealed during learning. Their bias could be implemented as an outp</context>
</contexts>
<marker>Xiao, Liu, Liu, Lin, 2011</marker>
<rawString>X. Xiao, Y. Liu, Q. Liu, and S. Lin. 2011. Fast generation of translation forest for large-scale SMT discriminative training. In Proc. of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.035730">
<title confidence="0.994899">
Elephant: Sequence Labeling for Word and Sentence Segmentation
</title>
<author confidence="0.999867">
Kilian Evang*, Valerio Basile*, Grzegorz Chrupała† and Johan Bos*
</author>
<affiliation confidence="0.9595645">
*University of Groningen, Oude Kijk in ’t Jatstraat 26, 9712 EK Groningen, The Netherlands
†Tilburg University, PO Box 90153, 5000 LE Tilburg, The Netherlands
</affiliation>
<email confidence="0.985519">
*{k.evang, v.basile, johan.bos}@rug.nl †g.chrupala@uvt.nl
</email>
<sectionHeader confidence="0.993705" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999525307692308">
Tokenization is widely regarded as a solved
problem due to the high accuracy that rule-
based tokenizers achieve. But rule-based
tokenizers are hard to maintain and their
rules language specific. We show that high-
accuracy word and sentence segmentation can
be achieved by using supervised sequence la-
beling on the character level combined with
unsupervised feature learning. We evalu-
ated our method on three languages and ob-
tained error rates of 0.27 %o (English), 0.35 %o
(Dutch) and 0.76 %o (Italian) for our best mod-
els.
</bodyText>
<sectionHeader confidence="0.625918" genericHeader="categories and subject descriptors">
1 An Elephant in the Room
</sectionHeader>
<bodyText confidence="0.980421357142857">
Tokenization, the task of segmenting a text into
words and sentences, is often regarded as a solved
problem in natural language processing (Dridan and
Oepen, 2012), probably because many corpora are
already in tokenized format. But like an elephant in
the living room, it is a problem that is impossible to
overlook whenever new raw datasets need to be pro-
cessed or when tokenization conventions are recon-
sidered. It is moreover an important problem, be-
cause any errors occurring early in the NLP pipeline
affect further analysis negatively. And even though
current tokenizers reach high performance, there are
three issues that we feel haven’t been addressed sat-
isfactorily so far:
</bodyText>
<listItem confidence="0.900956166666667">
• Most tokenizers are rule-based and therefore
hard to maintain and hard to adapt to new do-
mains and new languages (Silla Jr. and Kaest-
ner, 2004);
• Word and sentence segmentation are often seen
as separate tasks, but they obviously inform
each other and it could be advantageous to view
them as a combined task;
• Most tokenization methods provide no align-
ment between raw and tokenized text, which
makes mapping the tokenized version back
onto the actual source hard or impossible.
</listItem>
<bodyText confidence="0.9998694">
In short, we believe that regarding tokenization,
there is still room for improvement, in particular on
the methodological side of the task. We are partic-
ularly interested in the following questions: Can we
use supervised learning to avoid hand-crafting rules?
Can we use unsupervised feature learning to reduce
feature engineering effort and boost performance?
Can we use the same method across languages? Can
we combine word and sentence boundary detection
into one task?
</bodyText>
<sectionHeader confidence="0.999747" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999812636363636">
Usually the text segmentation task is split into word
tokenization and sentence boundary detection. Rule-
based systems for finding word and sentence bound-
aries often are variations on matching hand-coded
regular expressions (Grefenstette, 1999; Silla Jr. and
Kaestner, 2004; Jurafsky and Martin, 2008; Dridan
and Oepen, 2012).
Several unsupervised systems have been proposed
for sentence boundary detection. Kiss and Strunk
(2006) present a language-independent, unsuper-
vised approach and note that abbreviations form a
major source of ambiguity in sentence boundary
detection and use collocation detection to build a
high-accuracy abbreviation detector. The resulting
system reaches high accuracy, rivalling handcrafted
rule-based and supervised systems. A similar sys-
tem was proposed earlier by Mikheev (2002).
Existing supervised learning approaches for sen-
tence boundary detection use as features tokens pre-
ceding and following potential sentence boundary,
part of speech, capitalization information and lists
of abbreviations. Learning methods employed in
</bodyText>
<page confidence="0.918467">
1422
</page>
<bodyText confidence="0.947607235294118">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422–1426,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
these approaches include maximum entropy models
(Reynar and Ratnaparkhi, 1997) decision trees (Ri-
ley, 1989), and neural networks (Palmer and Hearst,
1997).
Closest to our work are approaches that present
token and sentence splitters using conditional ran-
dom fields (Tomanek et al., 2007; Fares et al., 2013).
However, these previous approaches consider tokens
(i.e. character sequences) as basic units for labeling,
whereas we consider single characters. As a con-
sequence, labeling is more resource-intensive, but it
also gives us more expressive power. In fact, our ap-
proach kills two birds with one stone, as it allows us
to integrate token and sentence boundaries detection
into one task.
</bodyText>
<sectionHeader confidence="0.998694" genericHeader="method">
3 Method
</sectionHeader>
<subsectionHeader confidence="0.998258">
3.1 IOB Tokenization
</subsectionHeader>
<bodyText confidence="0.99830452631579">
IOB tagging is widely used in tasks identifying
chunks of tokens. We use it to identify chunks of
characters. Characters outside of tokens are labeled
O, inside of tokens I. For characters at the beginning
of tokens, we use S at sentence boundaries, other-
wise T (for token). This scheme offers some nice
features, like allowing for discontinuous tokens (e.g.
hyphenated words at line breaks) and starting a new
token in the middle of a typographic word if the to-
kenization scheme requires it, as e.g. in did�n’t. An
example is given in Figure 1.
It didn’t matter if the faces were male,
SIOTIITIIOTIIIIIOTIOTIIOTIIIIOTIIIOTIIITO
female or those of children. Eighty-
TIIIIIOTIOTIIIIOTIOTIIIIIIITOSIIIIIIO
three percent of people in the 30-to-34
IIIIIOTIIIIIIOTIOTIIIIIOTIOTIIOTIIIIIIIO
year old age range gave correct responses.
TIIIOTIIOTIIOTIIIIOTIIIOTIIIIIIOTIIIIIIIIT
</bodyText>
<figureCaption confidence="0.999547">
Figure 1: Example of IOB-labeled characters
</figureCaption>
<subsectionHeader confidence="0.994456">
3.2 Datasets
</subsectionHeader>
<bodyText confidence="0.980390444444445">
In our experiments we use three datasets to compare
our method for different languages and for different
domains: manually checked English newswire texts
taken from the Groningen Meaning Bank, GMB
(Basile et al., 2012), Dutch newswire texts, com-
prising two days from January 2000 extracted from
the Twente News Corpus, TwNC (Ordelman et al.,
2007), and a random sample of Italian texts from the
PAIS A` corpus (Borghetti et al., 2011).
</bodyText>
<tableCaption confidence="0.998626">
Table 1: Datasets characteristics.
</tableCaption>
<table confidence="0.99806225">
Name Language Domain Sentences Tokens
GMB English Newswire 2,886 64,443
TNC Dutch Newswire 49,537 860,637
PAI Italian Web/various 42,674 869,095
</table>
<bodyText confidence="0.986125333333333">
The data was converted into IOB format by infer-
ring an alignment between the raw text and the seg-
mented text.
</bodyText>
<subsectionHeader confidence="0.999914">
3.3 Sequence labeling
</subsectionHeader>
<bodyText confidence="0.999998125">
We apply the Wapiti implementation (Lavergne et
al., 2010) of Conditional Random Fields (Lafferty
et al., 2001), using as features the output label of
each character, combined with 1) the character it-
self, 2) the output label on the previous character, 3)
characters and/or their Unicode categories from con-
text windows of varying sizes. For example, with a
context size of 3, in Figure 1, features for the E in
Eighty-three with the output label S would be E/S,
O/S, /S, i/S, Space/S, Lowercase/S. The intuition
is that the 31 existing Unicode categories can gen-
eralize across similar characters whereas character
features can identify specific contexts such as abbre-
viations or contractions (e.g. didn’t). The context
window sizes we use are 0, 1, 3, 5, 7, 9, 11 and 13,
centered around the focus character.
</bodyText>
<subsectionHeader confidence="0.981838">
3.4 Deep learning of features
</subsectionHeader>
<bodyText confidence="0.9998026875">
Automatically learned word embeddings have been
successfully used in NLP to reduce reliance on man-
ual feature engineering and boost performance. We
adapt this approach to the character level, and thus,
in addition to hand-crafted features we use text
representations induced in an unsupervised fashion
from character strings. A complete discussion of
our approach to learning text embeddings can be
found in (Chrupała, 2013). Here we provide a brief
overview.
Our representations correspond to the activation
of the hidden layer in a simple recurrent neural
(SRN) network (Elman, 1990; Elman, 1991), imple-
mented in a customized version of Mikolov (2010)’s
RNNLM toolkit. The network is sequentially pre-
sented with a large amount of raw text and learns to
</bodyText>
<page confidence="0.896844">
1423
</page>
<bodyText confidence="0.999991833333333">
predict the next character in the sequence. It uses the
units in the hidden layer to store a generalized rep-
resentation of the recent history. After training the
network on large amounts on unlabeled text, we run
it on the training and test data, and record the activa-
tion of the hidden layer at each position in the string
as it tries to predict the next character. The vector of
activations of the hidden layer provides additional
features used to train and run the CRF. For each of
the K = 10 most active units out of total J = 400
hidden units, we create features (f(1) ... f(K)) de-
fined as f(k) = 1 if sj(k) &gt; 0.5 and f(k) = 0 oth-
erwise, where sj(k) returns the activation of the kth
most active unit. For training the SRN only raw text
is necessary. We trained on the entire GMB 2.0.0
(2.5M characters), the portion of TwNC correspond-
ing to January 2000 (43M characters) and a sample
of the PAIS A` corpus (39M characters).
</bodyText>
<sectionHeader confidence="0.999732" genericHeader="evaluation">
4 Results and Evaluation
</sectionHeader>
<bodyText confidence="0.99999425">
In order to evaluate the quality of the tokenization
produced by our models we conducted several ex-
periments with different combinations of features
and context sizes. For these tests, the models are
trained on an 80% portion of the data sets and tested
on a 10% development set. Final results are obtained
on a 10% test set. We report both absolute number
of errors and error rates per thousand (‰).
</bodyText>
<subsectionHeader confidence="0.99109">
4.1 Feature sets
</subsectionHeader>
<bodyText confidence="0.999529375">
We experiment with two kinds of features at the
character level, namely Unicode categories (31 dif-
ferent ones), Unicode character codes, and a combi-
nation of them. Unicode categories are less sparse
than the character codes (there are 88, 134, and 502
unique characters for English, Dutch and Italian, re-
spectively), so the combination provide some gener-
alization over just character codes.
</bodyText>
<tableCaption confidence="0.931821">
Table 2: Error rates obtained with different feature sets.
Cat stands for Unicode category, Code for Unicode char-
acter code, and Cat-Code for a union of these features.
</tableCaption>
<table confidence="0.6202244">
Error rates per thousand (%o)
Feature set English Dutch Italian
Cat-9 45 (1.40) 1,403 (2.87) 1,548 (2.67)
Code-9 6 (0.19) 782 (1.60) 692 (1.20)
Cat-Code-9 8 (0.25) 774 (1.58) 657 (1.14)
</table>
<bodyText confidence="0.999529333333333">
From these results we see that categories alone
perform worse than only codes. For English there is
no gain from the combination over using only char-
acter codes. For Dutch and Italian there is an im-
provement, although it is only significant for Ital-
ian (p = 0.480 and p = 0.005 respectively, bino-
mial exact test). We use this feature combination in
the experiments that follow. Note that these models
are trained using a symmetrical context of 9 charac-
ters (four left and four right of the current character).
In the next section we show performance of models
with different window sizes.
</bodyText>
<subsectionHeader confidence="0.997675">
4.2 Context window
</subsectionHeader>
<bodyText confidence="0.9991105">
We run an experiment to evaluate how the size of the
context in the training phase impacts the classifica-
tion. In Table 4.2 we show the results for symmetri-
cal windows ranging in size from 1 to 13.
</bodyText>
<tableCaption confidence="0.999732">
Table 3: Using different context window sizes.
</tableCaption>
<table confidence="0.995867111111111">
Feature set Error rates per thousand (%o)
English Dutch Italian
Cat-Code-1 273 (8.51) 4,924 (10.06) 9,108 (15.86)
Cat-Code-3 118 (3.68) 3,525 (7.20) 2,013 (3.51)
Cat-Code-5 20 (0.62) 930 (1.90) 788 (1.37)
Cat-Code-7 10 (0.31) 778 (1.60) 667 (1.16)
Cat-Code-9 8 (0.25) 774 (1.58) 657 (1.14)
Cat-Code-11 9 (0.28) 761 (1.56) 692 (1.21)
Cat-Code-13 8 (0.25) 751 (1.54) 670 (1.17)
</table>
<subsectionHeader confidence="0.988211">
4.3 SRN features
</subsectionHeader>
<bodyText confidence="0.999928764705882">
We also tested the automatically learned features de-
rived from the activation of the hidden layer of an
SRN language model, as explained in Section 3.
We combined these features with character code and
Unicode category features in windows of different
sizes. The results of this test are shown in Table 4.
The first row shows the performance of SRN fea-
tures on their own. The following rows show the
combination of SRN features with the basic feature
sets of varying window size. It can be seen that aug-
menting the feature sets with SRN features results
in large reductions of error rates. The Cat-Code-1-
SRN setting has error rates comparable to Cat-Code-
9.
The addition of SRN features to the two best
previous models, Cat-Code-9 and Cat-Code-13, re-
duces the error rate by 83% resp. 81% for Dutch,
</bodyText>
<page confidence="0.989557">
1424
</page>
<bodyText confidence="0.999710833333333">
and by 24% resp. 26% for Italian. All these dif-
ferences are statistically significant according to the
binomial test (p &lt; 0.001). For English, there are too
few errors to detect a statistically significant effect
for Cat-Code-9 (p = 0.07), but for Cat-Code-13 we
find p = 0.016.
</bodyText>
<tableCaption confidence="0.9985125">
Table 4: Results obtained using different context window
sizes and addition of SRN features.
</tableCaption>
<table confidence="0.9968197">
Feature set Error rates per thousand (‰)
English Dutch Italian
SRN 24 (0.75) 276 (0.56) 738 (1.28)
Cat-Code-1-SRN 7 (0.21) 212 (0.43) 549 (0.96)
Cat-Code-3-SRN 4 (0.13) 165 (0.34) 507 (0.88)
Cat-Code-5-SRN 3 (0.10) 136 (0.28) 476 (0.83)
Cat-Code-7-SRN 1 (0.03) 111 (0.23) 497 (0.86)
Cat-Code-9-SRN 2 (0.06) 135 (0.28) 497 (0.86)
Cat-Code-11-SRN 2 (0.06) 132 (0.27) 468 (0.81)
Cat-Code-13-SRN 1 (0.03) 142 (0.29) 496 (0.86)
</table>
<bodyText confidence="0.999879">
In a final step, we selected the best models based
on the development sets (Cat-Code-7-SRN for En-
glish and Dutch, Cat-Code-11-SRN for Italian), and
checked their performance on the final test set. This
resulted in 10 errors (0.27 %o) for English (GMB
corpus), 199 errors (0.35 %o) for Dutch (TwNC cor-
pus), and 454 errors (0.76 %o) for Italian (PAIS A`
corpus).
</bodyText>
<sectionHeader confidence="0.998278" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999866684210526">
It is interesting to examine what kind of errors the
SRN features help avoid. In the English and Dutch
datasets many errors are caused by failure to rec-
ognize personal titles and initials or misparsing of
numbers. In the Italian data, a large fraction of er-
rors is due to verbs with clitics, which are written as
a single word, but treated as separate tokens. Table 5
shows examples of errors made by a simpler model
that are fixed by adding SRN features. Table 6 shows
the confusion matrices for the Cat-Code-7 and Cat-
Code-7-SRN sets on the Dutch data. The mistake
most improved by SRN features is T/I with 89% er-
ror reduction (see also Table 5). The is also the most
common remaining mistake.
A comparison with other approaches is hard be-
cause of the difference in datasets and task defini-
tion (combined word/sentence segmentation). Here
we just compare our results for sentence segmenta-
tion (sentence F1 score) with Punkt, a state-of-the-
</bodyText>
<tableCaption confidence="0.99848">
Table 5: Positive impact of SRN features.
</tableCaption>
<table confidence="0.999782388888889">
Cat-Code-7 Ms. Hughes will joi
Cat-Code-7-SRN SIIOSIIIIIOTIIIOTII
SIIOTIIIIIOTIIIOTII
Cat-Code-7 $ 3.9 trillion by t
Cat-Code-7-SRN TOTTIOTIIIIIIIOTIOT
TOTIIOTIIIIIIIOTIOT
Cat-Code-11 bleek 0,4 procent
Cat-Code-11-SRN OTIIIIOTTIOTIIIIIIO
OTIIIIOTIIOTIIIIIIO
Cat-Code-11 toebedeeld: 6,2. In
Cat-Code-11-SRN TIIIIIIIIITOTTITOSI
TIIIIIIIIITOTIITOSI
Cat-Code-11 prof. Teulings het
Cat-Code-11-SRN TIIITOSIIIIIIIOTIIO
TIIIIOTIIIIIIIOTIIO
Cat-Code-11 per costringerlo al
Cat-Code-11-SRN TIIOTIIIIIIIIIIIOTI
TIIOTIIIIIIIIITIOTI
</table>
<tableCaption confidence="0.989555">
Table 6: Confusion matrix for Dutch development set.
</tableCaption>
<table confidence="0.995762333333333">
Predicted, Cat-Code-7 Predicted, Cat-Code-7-SRN
Gold I O S T I O S T
I 328128 0 2 469 328546 0 0 53
O 0 75234 0 0 0 75234 0 0
S 4 0 4323 18 1 0 4332 12
T 252 0 33 80828 35 0 10 81068
</table>
<bodyText confidence="0.998242">
art sentence boundary detection system (Kiss and
Strunk, 2006). With its standard distributed mod-
els, Punkt achieves 98.51% on our English test set,
98.87% on Dutch and 98.34% on Italian, compared
with 100%, 99.54% and 99.51% for our system. Our
system benefits here from its ability to adapt to a new
domain with relatively little (but annotated) training
data.
</bodyText>
<sectionHeader confidence="0.969316" genericHeader="conclusions">
6 What Elephant?
</sectionHeader>
<bodyText confidence="0.999650111111111">
Word and sentence segmentation can be recast as a
combined tagging task. This way, tokenization is
cast as a supervised learning task, causing a shift of
labor from writing rules to manually correcting la-
bels. Learning this task with CRF achieves high ac-
curacy.1 Furthermore, our tagging method does not
lose the connection between original text and tokens.
In future work, we plan to broaden the scope of
this work to other steps in document preparation,
</bodyText>
<footnote confidence="0.969648">
1All software needed to replicate our experiments is
available at http://gmb.let.rug.nl/elephant/
experiments.php
</footnote>
<page confidence="0.991337">
1425
</page>
<bodyText confidence="0.999937166666667">
such as normalization of punctuation, and their in-
teraction with segmentation. We further plan to test
our method on a wider range of datasets, allowing a
more direct comparison with other approaches. Fi-
nally, we plan to explore the possibility of a statis-
tical universal segmentation model for mutliple lan-
guages and domains.
In a famous scene with a live elephant on stage,
the comedian Jimmy Durante was asked about it by
a policeman and surprisedly answered: “What ele-
phant?” We feel we can say the same now as far as
tokenization is concerned.
</bodyText>
<sectionHeader confidence="0.998944" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999578896551724">
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation (LREC 2012), pages 3196–3200, Istanbul,
Turkey.
Claudia Borghetti, Sara Castagnoli, and Marco Brunello.
2011. I testi del web: una proposta di classificazione
sulla base del corpus PAIS `A. In M. Cerruti, E. Corino,
and C. Onesti, editors, Formale e informale. La vari-
azione di registro nella comunicazione elettronica,
pages 147–170. Carocci, Roma.
Grzegorz Chrupała. 2013. Text segmentation with
character-level text embeddings. In ICML Workshop
on Deep Learning for Audio, Speech and Language
Processing, Atlanta, USA.
Rebecca Dridan and Stephan Oepen. 2012. Tokeniza-
tion: Returning to a long solved problem – a survey,
contrastive experiment, recommendations, and toolkit
–. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 2:
Short Papers), pages 378–382, Jeju Island, Korea. As-
sociation for Computational Linguistics.
Jeffrey L. Elman. 1990. Finding structure in time. Cog-
nitive science, 14(2):179–211.
Jeffrey L. Elman. 1991. Distributed representations,
simple recurrent networks, and grammatical structure.
Machine learning, 7(2):195–225.
Murhaf Fares, Stephan Oepen, and Zhang Yi. 2013. Ma-
chine learning for high-quality tokenization - replicat-
ing variable tokenization schemes. In A. Gelbukh, ed-
itor, CICLING 2013, volume 7816 of Lecture Notes in
Computer Science, pages 231–244, Berlin Heidelberg.
Springer-Verlag.
Gregory Grefenstette. 1999. Tokenization. In Hans van
Halteren, editor, Syntactic Wordclass Tagging, pages
117–133. Kluwer Academic Publishers, Dordrecht.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing. An Introduction to Natural
Language Processing, Computational Linguistics, and
Speech Recognition. Prentice Hall, 2nd edition.
Tibor Kiss and Jan Strunk. 2006. Unsupervised multi-
lingual sentence boundary detection. Computational
Linguistics, 32(4):485–525.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML-01, pages 282–289.
Thomas Lavergne, Olivier Capp´e, and Franc¸ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 504–513, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
Andrei Mikheev. 2002. Periods, capitalized words, etc.
Computational Linguistics, 28(3):289–318.
Tom´aˇs Mikolov, Martin Karafi´at, Luk´aˇs Burget, Jan
ˇCernock´y, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Interspeech.
Roeland Ordelman, Franciska de Jong, Arjan van Hessen,
and Hendri Hondorp. 2007. TwNC: a multifaceted
Dutch news corpus. ELRA Newsleter, 12(3/4):4–7.
David D. Palmer and Marti A. Hearst. 1997. Adap-
tive multilingual sentence boundary disambiguation.
Computational Linguistics, 23(2):241–267.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proceedings of the Fifth Conference
on Applied Natural Language Processing, pages 16–
19, Washington, DC, USA. Association for Computa-
tional Linguistics.
Michael D. Riley. 1989. Some applications of tree-based
modelling to speech and language. In Proceedings of
the workshop on Speech and Natural Language, HLT
’89, pages 339–352, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Carlos N. Silla Jr. and Celso A. A. Kaestner. 2004. An
analysis of sentence boundary detection systems for
English and Portuguese documents. In Fifth Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics, volume 2945 of Lecture
Notes in Computer Science, pages 135–141. Springer.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. Sentence and token splitting based on condi-
tional random fields. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 49–57, Melbourne, Australia.
</reference>
<page confidence="0.992559">
1426
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998257">Elephant: Sequence Labeling for Word and Sentence Segmentation</title>
<author confidence="0.985834">Valerio Grzegorz</author>
<author confidence="0.985834">Johan</author>
<address confidence="0.8220635">of Groningen, Oude Kijk in ’t Jatstraat 26, 9712 EK Groningen, The University, PO Box 90153, 5000 LE Tilburg, The</address>
<email confidence="0.950536">v.basile,</email>
<abstract confidence="0.937725150485436">Tokenization is widely regarded as a solved problem due to the high accuracy that rulebased tokenizers achieve. But rule-based tokenizers are hard to maintain and their rules language specific. We show that highaccuracy word and sentence segmentation can be achieved by using supervised sequence labeling on the character level combined with unsupervised feature learning. We evaluated our method on three languages and obtained error rates of 0.27 %o (English), 0.35 %o (Dutch) and 0.76 %o (Italian) for our best models. 1 An Elephant in the Room Tokenization, the task of segmenting a text into words and sentences, is often regarded as a solved problem in natural language processing (Dridan and Oepen, 2012), probably because many corpora are already in tokenized format. But like an elephant in the living room, it is a problem that is impossible to overlook whenever new raw datasets need to be processed or when tokenization conventions are reconsidered. It is moreover an important problem, because any errors occurring early in the NLP pipeline affect further analysis negatively. And even though current tokenizers reach high performance, there are three issues that we feel haven’t been addressed satisfactorily so far: • Most tokenizers are rule-based and therefore hard to maintain and hard to adapt to new domains and new languages (Silla Jr. and Kaestner, 2004); • Word and sentence segmentation are often seen as separate tasks, but they obviously inform each other and it could be advantageous to view them as a combined task; • Most tokenization methods provide no alignment between raw and tokenized text, which makes mapping the tokenized version back onto the actual source hard or impossible. In short, we believe that regarding tokenization, there is still room for improvement, in particular on the methodological side of the task. We are particularly interested in the following questions: Can we use supervised learning to avoid hand-crafting rules? Can we use unsupervised feature learning to reduce feature engineering effort and boost performance? Can we use the same method across languages? Can we combine word and sentence boundary detection into one task? 2 Related Work Usually the text segmentation task is split into word tokenization and sentence boundary detection. Rulebased systems for finding word and sentence boundaries often are variations on matching hand-coded regular expressions (Grefenstette, 1999; Silla Jr. and Kaestner, 2004; Jurafsky and Martin, 2008; Dridan and Oepen, 2012). Several unsupervised systems have been proposed for sentence boundary detection. Kiss and Strunk (2006) present a language-independent, unsupervised approach and note that abbreviations form a major source of ambiguity in sentence boundary detection and use collocation detection to build a high-accuracy abbreviation detector. The resulting system reaches high accuracy, rivalling handcrafted rule-based and supervised systems. A similar system was proposed earlier by Mikheev (2002). Existing supervised learning approaches for sentence boundary detection use as features tokens preceding and following potential sentence boundary, part of speech, capitalization information and lists of abbreviations. Learning methods employed in 1422 of the 2013 Conference on Empirical Methods in Natural Language pages 1422–1426, Washington, USA, 18-21 October 2013. Association for Computational Linguistics these approaches include maximum entropy models (Reynar and Ratnaparkhi, 1997) decision trees (Riley, 1989), and neural networks (Palmer and Hearst, 1997). Closest to our work are approaches that present token and sentence splitters using conditional random fields (Tomanek et al., 2007; Fares et al., 2013). However, these previous approaches consider tokens (i.e. character sequences) as basic units for labeling, whereas we consider single characters. As a consequence, labeling is more resource-intensive, but it also gives us more expressive power. In fact, our approach kills two birds with one stone, as it allows us to integrate token and sentence boundaries detection into one task. 3 Method 3.1 IOB Tokenization IOB tagging is widely used in tasks identifying chunks of tokens. We use it to identify chunks of characters. Characters outside of tokens are labeled O, inside of tokens I. For characters at the beginning of tokens, we use S at sentence boundaries, otherwise T (for token). This scheme offers some nice features, like allowing for discontinuous tokens (e.g. hyphenated words at line breaks) and starting a new token in the middle of a typographic word if the toscheme requires it, as e.g. in An example is given in Figure 1. It didn’t matter if the faces were male, SIOTIITIIOTIIIIIOTIOTIIOTIIIIOTIIIOTIIITO female or those of children. Eighty- TIIIIIOTIOTIIIIOTIOTIIIIIIITOSIIIIIIO three percent of people in the 30-to-34 IIIIIOTIIIIIIOTIOTIIIIIOTIOTIIOTIIIIIIIO year old age range gave correct responses. TIIIOTIIOTIIOTIIIIOTIIIOTIIIIIIOTIIIIIIIIT Figure 1: Example of IOB-labeled characters 3.2 Datasets In our experiments we use three datasets to compare our method for different languages and for different domains: manually checked English newswire texts taken from the Groningen Meaning Bank, GMB (Basile et al., 2012), Dutch newswire texts, comprising two days from January 2000 extracted from the Twente News Corpus, TwNC (Ordelman et al., 2007), and a random sample of Italian texts from the A`corpus (Borghetti et al., 2011). Table 1: Datasets characteristics. Name Language Domain Sentences Tokens GMB English Newswire 2,886 64,443 TNC Dutch Newswire 49,537 860,637 PAI Italian Web/various 42,674 869,095 The data was converted into IOB format by inferring an alignment between the raw text and the segmented text. 3.3 Sequence labeling We apply the Wapiti implementation (Lavergne et al., 2010) of Conditional Random Fields (Lafferty et al., 2001), using as features the output label of each character, combined with 1) the character itself, 2) the output label on the previous character, 3) characters and/or their Unicode categories from context windows of varying sizes. For example, with a context size of 3, in Figure 1, features for the E in the output label S would be E/S, O/S, /S, i/S, Space/S, Lowercase/S. The intuition is that the 31 existing Unicode categories can generalize across similar characters whereas character features can identify specific contexts such as abbreor contractions (e.g. The context window sizes we use are 0, 1, 3, 5, 7, 9, 11 and 13, centered around the focus character. 3.4 Deep learning of features Automatically learned word embeddings have been successfully used in NLP to reduce reliance on manual feature engineering and boost performance. We adapt this approach to the character level, and thus, in addition to hand-crafted features we use text representations induced in an unsupervised fashion from character strings. A complete discussion of our approach to learning text embeddings can be found in (Chrupała, 2013). Here we provide a brief overview. Our representations correspond to the activation of the hidden layer in a simple recurrent neural (SRN) network (Elman, 1990; Elman, 1991), implemented in a customized version of Mikolov (2010)’s RNNLM toolkit. The network is sequentially presented with a large amount of raw text and learns to 1423 predict the next character in the sequence. It uses the units in the hidden layer to store a generalized representation of the recent history. After training the network on large amounts on unlabeled text, we run it on the training and test data, and record the activation of the hidden layer at each position in the string as it tries to predict the next character. The vector of activations of the hidden layer provides additional features used to train and run the CRF. For each of 10 most active units out of total 400 units, we create features deas = 1 if and = 0 othwhere returns the activation of the most active unit. For training the SRN only raw text is necessary. We trained on the entire GMB 2.0.0 (2.5M characters), the portion of TwNC corresponding to January 2000 (43M characters) and a sample the PAIS A`corpus (39M characters). 4 Results and Evaluation In order to evaluate the quality of the tokenization produced by our models we conducted several experiments with different combinations of features and context sizes. For these tests, the models are trained on an 80% portion of the data sets and tested on a 10% development set. Final results are obtained on a 10% test set. We report both absolute number of errors and error rates per thousand (‰). 4.1 Feature sets We experiment with two kinds of features at the character level, namely Unicode categories (31 different ones), Unicode character codes, and a combination of them. Unicode categories are less sparse than the character codes (there are 88, 134, and 502 unique characters for English, Dutch and Italian, respectively), so the combination provide some generalization over just character codes. Table 2: Error rates obtained with different feature sets. Cat stands for Unicode category, Code for Unicode character code, and Cat-Code for a union of these features.</abstract>
<note confidence="0.908973">Error rates per thousand (%o) Feature set English Dutch Italian Cat-9 45 (1.40) 1,403 (2.87) 1,548 (2.67) Code-9 6 (0.19) 782 (1.60) 692 (1.20)</note>
<phone confidence="0.382073">Cat-Code-9 8 (0.25) 774 (1.58) 657 (1.14)</phone>
<abstract confidence="0.978670055555556">From these results we see that categories alone perform worse than only codes. For English there is no gain from the combination over using only character codes. For Dutch and Italian there is an improvement, although it is only significant for Italand respectively, binomial exact test). We use this feature combination in the experiments that follow. Note that these models are trained using a symmetrical context of 9 characters (four left and four right of the current character). In the next section we show performance of models with different window sizes. 4.2 Context window We run an experiment to evaluate how the size of the context in the training phase impacts the classification. In Table 4.2 we show the results for symmetrical windows ranging in size from 1 to 13. Table 3: Using different context window sizes.</abstract>
<note confidence="0.843573666666667">Feature set Error rates per thousand (%o) English Dutch Italian Cat-Code-1 273 (8.51) 4,924 (10.06) 9,108 (15.86) Cat-Code-3 118 (3.68) 3,525 (7.20) 2,013 (3.51) Cat-Code-5 20 (0.62) 930 (1.90) 788 (1.37) Cat-Code-7 10 (0.31) 778 (1.60) 667 (1.16) Cat-Code-9 8 (0.25) 774 (1.58) 657 (1.14) Cat-Code-11 9 (0.28) 761 (1.56) 692 (1.21) Cat-Code-13 8 (0.25) 751 (1.54) 670 (1.17)</note>
<abstract confidence="0.984831269230769">4.3 SRN features We also tested the automatically learned features derived from the activation of the hidden layer of an SRN language model, as explained in Section 3. We combined these features with character code and Unicode category features in windows of different sizes. The results of this test are shown in Table 4. The first row shows the performance of SRN features on their own. The following rows show the combination of SRN features with the basic feature sets of varying window size. It can be seen that augmenting the feature sets with SRN features results in large reductions of error rates. The Cat-Code-1- SRN setting has error rates comparable to Cat-Code- 9. The addition of SRN features to the two best previous models, Cat-Code-9 and Cat-Code-13, reduces the error rate by 83% resp. 81% for Dutch, 1424 and by 24% resp. 26% for Italian. All these differences are statistically significant according to the test &lt; For English, there are too few errors to detect a statistically significant effect Cat-Code-9 but for Cat-Code-13 we Table 4: Results obtained using different context window sizes and addition of SRN features.</abstract>
<title confidence="0.405054">Feature set Error rates per thousand (‰)</title>
<author confidence="0.619148">English Dutch Italian</author>
<note confidence="0.962726125">SRN 24 (0.75) 276 (0.56) 738 (1.28) Cat-Code-1-SRN 7 (0.21) 212 (0.43) 549 (0.96) Cat-Code-3-SRN 4 (0.13) 165 (0.34) 507 (0.88) Cat-Code-5-SRN 3 (0.10) 136 (0.28) 476 (0.83) Cat-Code-7-SRN 1 (0.03) 111 (0.23) 497 (0.86) Cat-Code-9-SRN 2 (0.06) 135 (0.28) 497 (0.86) Cat-Code-11-SRN 2 (0.06) 132 (0.27) 468 (0.81) Cat-Code-13-SRN 1 (0.03) 142 (0.29) 496 (0.86)</note>
<abstract confidence="0.985187714285714">In a final step, we selected the best models based on the development sets (Cat-Code-7-SRN for English and Dutch, Cat-Code-11-SRN for Italian), and checked their performance on the final test set. This resulted in 10 errors (0.27 %o) for English (GMB corpus), 199 errors (0.35 %o) for Dutch (TwNC corpus), and 454 errors (0.76 %o) for Italian (PAIS A` corpus). 5 Discussion It is interesting to examine what kind of errors the SRN features help avoid. In the English and Dutch datasets many errors are caused by failure to recognize personal titles and initials or misparsing of numbers. In the Italian data, a large fraction of errors is due to verbs with clitics, which are written as a single word, but treated as separate tokens. Table 5 shows examples of errors made by a simpler model that are fixed by adding SRN features. Table 6 shows the confusion matrices for the Cat-Code-7 and Cat- Code-7-SRN sets on the Dutch data. The mistake most improved by SRN features is T/I with 89% error reduction (see also Table 5). The is also the most common remaining mistake. A comparison with other approaches is hard because of the difference in datasets and task definition (combined word/sentence segmentation). Here we just compare our results for sentence segmenta- (sentence with Punkt, a state-of-the-</abstract>
<note confidence="0.845456769230769">Table 5: Positive impact of SRN features. Cat-Code-7 Cat-Code-7-SRN Ms. Hughes will Cat-Code-7 Cat-Code-7-SRN $ 3.9 trillion by Cat-Code-11 Cat-Code-11-SRN bleek 0,4 Cat-Code-11 Cat-Code-11-SRN toebedeeld: 6,2. In Cat-Code-11 Cat-Code-11-SRN prof. Teulings het Cat-Code-11 Cat-Code-11-SRN per costringerlo Table 6: Confusion matrix for Dutch development set. Predicted, Cat-Code-7 Predicted, Cat-Code-7-SRN Gold I O S T I O S T I 328128 0 2 469 328546 0 0 53 O 0 75234 0 0 0 75234 0 0 S 4 0 4323 18 1 0 4332 12</note>
<phone confidence="0.613489">T 252 0 33 80828 35 0 10 81068</phone>
<abstract confidence="0.995989117647059">art sentence boundary detection system (Kiss and Strunk, 2006). With its standard distributed models, Punkt achieves 98.51% on our English test set, 98.87% on Dutch and 98.34% on Italian, compared with 100%, 99.54% and 99.51% for our system. Our system benefits here from its ability to adapt to a new domain with relatively little (but annotated) training data. 6 What Elephant? Word and sentence segmentation can be recast as a combined tagging task. This way, tokenization is cast as a supervised learning task, causing a shift of labor from writing rules to manually correcting labels. Learning this task with CRF achieves high ac- Furthermore, our tagging method does not lose the connection between original text and tokens. In future work, we plan to broaden the scope of this work to other steps in document preparation, software needed to replicate our experiments is at experiments.php 1425 such as normalization of punctuation, and their interaction with segmentation. We further plan to test our method on a wider range of datasets, allowing a more direct comparison with other approaches. Finally, we plan to explore the possibility of a statistical universal segmentation model for mutliple languages and domains. In a famous scene with a live elephant on stage, the comedian Jimmy Durante was asked about it by a policeman and surprisedly answered: “What elephant?” We feel we can say the same now as far as tokenization is concerned.</abstract>
<note confidence="0.676108115384615">References Valerio Basile, Johan Bos, Kilian Evang, and Noortje Venhuizen. 2012. Developing a large semantically corpus. In of the Eight International Conference on Language Resources and (LREC pages 3196–3200, Istanbul, Turkey. Claudia Borghetti, Sara Castagnoli, and Marco Brunello. 2011. I testi del web: una proposta di classificazione sulla base del corpus PAIS `A. In M. Cerruti, E. Corino, C. Onesti, editors, e informale. La varidi registro nella comunicazione pages 147–170. Carocci, Roma. Grzegorz Chrupała. 2013. Text segmentation with text embeddings. In Workshop on Deep Learning for Audio, Speech and Language Atlanta, USA. Rebecca Dridan and Stephan Oepen. 2012. Tokenization: Returning to a long solved problem – a survey, contrastive experiment, recommendations, and toolkit In of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: pages 378–382, Jeju Island, Korea. Association for Computational Linguistics. L. Elman. 1990. Finding structure in time. Cog- 14(2):179–211.</note>
<author confidence="0.421254">Distributed representations</author>
<abstract confidence="0.9313755">simple recurrent networks, and grammatical structure. 7(2):195–225. Murhaf Fares, Stephan Oepen, and Zhang Yi. 2013. Machine learning for high-quality tokenization replicating variable tokenization schemes. In A. Gelbukh, edvolume 7816 of Notes in</abstract>
<note confidence="0.587746666666667">pages 231–244, Berlin Heidelberg. Springer-Verlag. Gregory Grefenstette. 1999. Tokenization. In Hans van editor, Wordclass pages 117–133. Kluwer Academic Publishers, Dordrecht. Jurafsky and James H. Martin. 2008.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Valerio Basile</author>
<author>Johan Bos</author>
<author>Kilian Evang</author>
<author>Noortje Venhuizen</author>
</authors>
<title>Developing a large semantically annotated corpus.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC 2012),</booktitle>
<pages>3196--3200</pages>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="5690" citStr="Basile et al., 2012" startWordPosition="856" endWordPosition="859">is given in Figure 1. It didn’t matter if the faces were male, SIOTIITIIOTIIIIIOTIOTIIOTIIIIOTIIIOTIIITO female or those of children. EightyTIIIIIOTIOTIIIIOTIOTIIIIIIITOSIIIIIIO three percent of people in the 30-to-34 IIIIIOTIIIIIIOTIOTIIIIIOTIOTIIOTIIIIIIIO year old age range gave correct responses. TIIIOTIIOTIIOTIIIIOTIIIOTIIIIIIOTIIIIIIIIT Figure 1: Example of IOB-labeled characters 3.2 Datasets In our experiments we use three datasets to compare our method for different languages and for different domains: manually checked English newswire texts taken from the Groningen Meaning Bank, GMB (Basile et al., 2012), Dutch newswire texts, comprising two days from January 2000 extracted from the Twente News Corpus, TwNC (Ordelman et al., 2007), and a random sample of Italian texts from the PAIS A` corpus (Borghetti et al., 2011). Table 1: Datasets characteristics. Name Language Domain Sentences Tokens GMB English Newswire 2,886 64,443 TNC Dutch Newswire 49,537 860,637 PAI Italian Web/various 42,674 869,095 The data was converted into IOB format by inferring an alignment between the raw text and the segmented text. 3.3 Sequence labeling We apply the Wapiti implementation (Lavergne et al., 2010) of Conditio</context>
</contexts>
<marker>Basile, Bos, Evang, Venhuizen, 2012</marker>
<rawString>Valerio Basile, Johan Bos, Kilian Evang, and Noortje Venhuizen. 2012. Developing a large semantically annotated corpus. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC 2012), pages 3196–3200, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Borghetti</author>
<author>Sara Castagnoli</author>
<author>Marco Brunello</author>
</authors>
<title>I testi del web: una proposta di classificazione sulla base del corpus PAIS `A.</title>
<date>2011</date>
<booktitle>Formale e informale. La variazione di registro nella comunicazione elettronica,</booktitle>
<pages>147--170</pages>
<editor>In M. Cerruti, E. Corino, and C. Onesti, editors,</editor>
<location>Carocci, Roma.</location>
<contexts>
<context position="5906" citStr="Borghetti et al., 2011" startWordPosition="893" endWordPosition="896">o-34 IIIIIOTIIIIIIOTIOTIIIIIOTIOTIIOTIIIIIIIO year old age range gave correct responses. TIIIOTIIOTIIOTIIIIOTIIIOTIIIIIIOTIIIIIIIIT Figure 1: Example of IOB-labeled characters 3.2 Datasets In our experiments we use three datasets to compare our method for different languages and for different domains: manually checked English newswire texts taken from the Groningen Meaning Bank, GMB (Basile et al., 2012), Dutch newswire texts, comprising two days from January 2000 extracted from the Twente News Corpus, TwNC (Ordelman et al., 2007), and a random sample of Italian texts from the PAIS A` corpus (Borghetti et al., 2011). Table 1: Datasets characteristics. Name Language Domain Sentences Tokens GMB English Newswire 2,886 64,443 TNC Dutch Newswire 49,537 860,637 PAI Italian Web/various 42,674 869,095 The data was converted into IOB format by inferring an alignment between the raw text and the segmented text. 3.3 Sequence labeling We apply the Wapiti implementation (Lavergne et al., 2010) of Conditional Random Fields (Lafferty et al., 2001), using as features the output label of each character, combined with 1) the character itself, 2) the output label on the previous character, 3) characters and/or their Unicod</context>
</contexts>
<marker>Borghetti, Castagnoli, Brunello, 2011</marker>
<rawString>Claudia Borghetti, Sara Castagnoli, and Marco Brunello. 2011. I testi del web: una proposta di classificazione sulla base del corpus PAIS `A. In M. Cerruti, E. Corino, and C. Onesti, editors, Formale e informale. La variazione di registro nella comunicazione elettronica, pages 147–170. Carocci, Roma.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Chrupała</author>
</authors>
<title>Text segmentation with character-level text embeddings.</title>
<date>2013</date>
<booktitle>In ICML Workshop on Deep Learning for Audio, Speech and Language Processing,</booktitle>
<location>Atlanta, USA.</location>
<contexts>
<context position="7484" citStr="Chrupała, 2013" startWordPosition="1147" endWordPosition="1148">texts such as abbreviations or contractions (e.g. didn’t). The context window sizes we use are 0, 1, 3, 5, 7, 9, 11 and 13, centered around the focus character. 3.4 Deep learning of features Automatically learned word embeddings have been successfully used in NLP to reduce reliance on manual feature engineering and boost performance. We adapt this approach to the character level, and thus, in addition to hand-crafted features we use text representations induced in an unsupervised fashion from character strings. A complete discussion of our approach to learning text embeddings can be found in (Chrupała, 2013). Here we provide a brief overview. Our representations correspond to the activation of the hidden layer in a simple recurrent neural (SRN) network (Elman, 1990; Elman, 1991), implemented in a customized version of Mikolov (2010)’s RNNLM toolkit. The network is sequentially presented with a large amount of raw text and learns to 1423 predict the next character in the sequence. It uses the units in the hidden layer to store a generalized representation of the recent history. After training the network on large amounts on unlabeled text, we run it on the training and test data, and record the ac</context>
</contexts>
<marker>Chrupała, 2013</marker>
<rawString>Grzegorz Chrupała. 2013. Text segmentation with character-level text embeddings. In ICML Workshop on Deep Learning for Audio, Speech and Language Processing, Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Dridan</author>
<author>Stephan Oepen</author>
</authors>
<title>Tokenization: Returning to a long solved problem – a survey, contrastive experiment, recommendations, and toolkit –.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>378--382</pages>
<institution>Jeju Island, Korea. Association for Computational Linguistics.</institution>
<contexts>
<context position="1066" citStr="Dridan and Oepen, 2012" startWordPosition="160" endWordPosition="163">sed tokenizers achieve. But rule-based tokenizers are hard to maintain and their rules language specific. We show that highaccuracy word and sentence segmentation can be achieved by using supervised sequence labeling on the character level combined with unsupervised feature learning. We evaluated our method on three languages and obtained error rates of 0.27 %o (English), 0.35 %o (Dutch) and 0.76 %o (Italian) for our best models. 1 An Elephant in the Room Tokenization, the task of segmenting a text into words and sentences, is often regarded as a solved problem in natural language processing (Dridan and Oepen, 2012), probably because many corpora are already in tokenized format. But like an elephant in the living room, it is a problem that is impossible to overlook whenever new raw datasets need to be processed or when tokenization conventions are reconsidered. It is moreover an important problem, because any errors occurring early in the NLP pipeline affect further analysis negatively. And even though current tokenizers reach high performance, there are three issues that we feel haven’t been addressed satisfactorily so far: • Most tokenizers are rule-based and therefore hard to maintain and hard to adap</context>
<context position="2883" citStr="Dridan and Oepen, 2012" startWordPosition="450" endWordPosition="453">uestions: Can we use supervised learning to avoid hand-crafting rules? Can we use unsupervised feature learning to reduce feature engineering effort and boost performance? Can we use the same method across languages? Can we combine word and sentence boundary detection into one task? 2 Related Work Usually the text segmentation task is split into word tokenization and sentence boundary detection. Rulebased systems for finding word and sentence boundaries often are variations on matching hand-coded regular expressions (Grefenstette, 1999; Silla Jr. and Kaestner, 2004; Jurafsky and Martin, 2008; Dridan and Oepen, 2012). Several unsupervised systems have been proposed for sentence boundary detection. Kiss and Strunk (2006) present a language-independent, unsupervised approach and note that abbreviations form a major source of ambiguity in sentence boundary detection and use collocation detection to build a high-accuracy abbreviation detector. The resulting system reaches high accuracy, rivalling handcrafted rule-based and supervised systems. A similar system was proposed earlier by Mikheev (2002). Existing supervised learning approaches for sentence boundary detection use as features tokens preceding and fol</context>
</contexts>
<marker>Dridan, Oepen, 2012</marker>
<rawString>Rebecca Dridan and Stephan Oepen. 2012. Tokenization: Returning to a long solved problem – a survey, contrastive experiment, recommendations, and toolkit –. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 378–382, Jeju Island, Korea. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive science,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="7644" citStr="Elman, 1990" startWordPosition="1172" endWordPosition="1173"> Deep learning of features Automatically learned word embeddings have been successfully used in NLP to reduce reliance on manual feature engineering and boost performance. We adapt this approach to the character level, and thus, in addition to hand-crafted features we use text representations induced in an unsupervised fashion from character strings. A complete discussion of our approach to learning text embeddings can be found in (Chrupała, 2013). Here we provide a brief overview. Our representations correspond to the activation of the hidden layer in a simple recurrent neural (SRN) network (Elman, 1990; Elman, 1991), implemented in a customized version of Mikolov (2010)’s RNNLM toolkit. The network is sequentially presented with a large amount of raw text and learns to 1423 predict the next character in the sequence. It uses the units in the hidden layer to store a generalized representation of the recent history. After training the network on large amounts on unlabeled text, we run it on the training and test data, and record the activation of the hidden layer at each position in the string as it tries to predict the next character. The vector of activations of the hidden layer provides ad</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffrey L. Elman. 1990. Finding structure in time. Cognitive science, 14(2):179–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Distributed representations, simple recurrent networks, and grammatical structure.</title>
<date>1991</date>
<booktitle>Machine learning,</booktitle>
<pages>7--2</pages>
<contexts>
<context position="7658" citStr="Elman, 1991" startWordPosition="1174" endWordPosition="1175">g of features Automatically learned word embeddings have been successfully used in NLP to reduce reliance on manual feature engineering and boost performance. We adapt this approach to the character level, and thus, in addition to hand-crafted features we use text representations induced in an unsupervised fashion from character strings. A complete discussion of our approach to learning text embeddings can be found in (Chrupała, 2013). Here we provide a brief overview. Our representations correspond to the activation of the hidden layer in a simple recurrent neural (SRN) network (Elman, 1990; Elman, 1991), implemented in a customized version of Mikolov (2010)’s RNNLM toolkit. The network is sequentially presented with a large amount of raw text and learns to 1423 predict the next character in the sequence. It uses the units in the hidden layer to store a generalized representation of the recent history. After training the network on large amounts on unlabeled text, we run it on the training and test data, and record the activation of the hidden layer at each position in the string as it tries to predict the next character. The vector of activations of the hidden layer provides additional featu</context>
</contexts>
<marker>Elman, 1991</marker>
<rawString>Jeffrey L. Elman. 1991. Distributed representations, simple recurrent networks, and grammatical structure. Machine learning, 7(2):195–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Murhaf Fares</author>
<author>Stephan Oepen</author>
<author>Zhang Yi</author>
</authors>
<title>Machine learning for high-quality tokenization - replicating variable tokenization schemes.</title>
<date>2013</date>
<booktitle>CICLING 2013,</booktitle>
<volume>7816</volume>
<pages>231--244</pages>
<editor>In A. Gelbukh, editor,</editor>
<publisher>Springer-Verlag.</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="4131" citStr="Fares et al., 2013" startWordPosition="622" endWordPosition="625">ndary, part of speech, capitalization information and lists of abbreviations. Learning methods employed in 1422 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422–1426, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics these approaches include maximum entropy models (Reynar and Ratnaparkhi, 1997) decision trees (Riley, 1989), and neural networks (Palmer and Hearst, 1997). Closest to our work are approaches that present token and sentence splitters using conditional random fields (Tomanek et al., 2007; Fares et al., 2013). However, these previous approaches consider tokens (i.e. character sequences) as basic units for labeling, whereas we consider single characters. As a consequence, labeling is more resource-intensive, but it also gives us more expressive power. In fact, our approach kills two birds with one stone, as it allows us to integrate token and sentence boundaries detection into one task. 3 Method 3.1 IOB Tokenization IOB tagging is widely used in tasks identifying chunks of tokens. We use it to identify chunks of characters. Characters outside of tokens are labeled O, inside of tokens I. For charact</context>
</contexts>
<marker>Fares, Oepen, Yi, 2013</marker>
<rawString>Murhaf Fares, Stephan Oepen, and Zhang Yi. 2013. Machine learning for high-quality tokenization - replicating variable tokenization schemes. In A. Gelbukh, editor, CICLING 2013, volume 7816 of Lecture Notes in Computer Science, pages 231–244, Berlin Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<date>1999</date>
<booktitle>Syntactic Wordclass Tagging,</booktitle>
<pages>117--133</pages>
<editor>Tokenization. In Hans van Halteren, editor,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="2801" citStr="Grefenstette, 1999" startWordPosition="439" endWordPosition="440">dological side of the task. We are particularly interested in the following questions: Can we use supervised learning to avoid hand-crafting rules? Can we use unsupervised feature learning to reduce feature engineering effort and boost performance? Can we use the same method across languages? Can we combine word and sentence boundary detection into one task? 2 Related Work Usually the text segmentation task is split into word tokenization and sentence boundary detection. Rulebased systems for finding word and sentence boundaries often are variations on matching hand-coded regular expressions (Grefenstette, 1999; Silla Jr. and Kaestner, 2004; Jurafsky and Martin, 2008; Dridan and Oepen, 2012). Several unsupervised systems have been proposed for sentence boundary detection. Kiss and Strunk (2006) present a language-independent, unsupervised approach and note that abbreviations form a major source of ambiguity in sentence boundary detection and use collocation detection to build a high-accuracy abbreviation detector. The resulting system reaches high accuracy, rivalling handcrafted rule-based and supervised systems. A similar system was proposed earlier by Mikheev (2002). Existing supervised learning a</context>
</contexts>
<marker>Grefenstette, 1999</marker>
<rawString>Gregory Grefenstette. 1999. Tokenization. In Hans van Halteren, editor, Syntactic Wordclass Tagging, pages 117–133. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and Language Processing. An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice Hall, 2nd edition.</title>
<date>2008</date>
<contexts>
<context position="2858" citStr="Jurafsky and Martin, 2008" startWordPosition="446" endWordPosition="449">terested in the following questions: Can we use supervised learning to avoid hand-crafting rules? Can we use unsupervised feature learning to reduce feature engineering effort and boost performance? Can we use the same method across languages? Can we combine word and sentence boundary detection into one task? 2 Related Work Usually the text segmentation task is split into word tokenization and sentence boundary detection. Rulebased systems for finding word and sentence boundaries often are variations on matching hand-coded regular expressions (Grefenstette, 1999; Silla Jr. and Kaestner, 2004; Jurafsky and Martin, 2008; Dridan and Oepen, 2012). Several unsupervised systems have been proposed for sentence boundary detection. Kiss and Strunk (2006) present a language-independent, unsupervised approach and note that abbreviations form a major source of ambiguity in sentence boundary detection and use collocation detection to build a high-accuracy abbreviation detector. The resulting system reaches high accuracy, rivalling handcrafted rule-based and supervised systems. A similar system was proposed earlier by Mikheev (2002). Existing supervised learning approaches for sentence boundary detection use as features</context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>Daniel Jurafsky and James H. Martin. 2008. Speech and Language Processing. An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice Hall, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tibor Kiss</author>
<author>Jan Strunk</author>
</authors>
<title>Unsupervised multilingual sentence boundary detection.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="2988" citStr="Kiss and Strunk (2006)" startWordPosition="464" endWordPosition="467">rning to reduce feature engineering effort and boost performance? Can we use the same method across languages? Can we combine word and sentence boundary detection into one task? 2 Related Work Usually the text segmentation task is split into word tokenization and sentence boundary detection. Rulebased systems for finding word and sentence boundaries often are variations on matching hand-coded regular expressions (Grefenstette, 1999; Silla Jr. and Kaestner, 2004; Jurafsky and Martin, 2008; Dridan and Oepen, 2012). Several unsupervised systems have been proposed for sentence boundary detection. Kiss and Strunk (2006) present a language-independent, unsupervised approach and note that abbreviations form a major source of ambiguity in sentence boundary detection and use collocation detection to build a high-accuracy abbreviation detector. The resulting system reaches high accuracy, rivalling handcrafted rule-based and supervised systems. A similar system was proposed earlier by Mikheev (2002). Existing supervised learning approaches for sentence boundary detection use as features tokens preceding and following potential sentence boundary, part of speech, capitalization information and lists of abbreviations</context>
<context position="14960" citStr="Kiss and Strunk, 2006" startWordPosition="2400" endWordPosition="2403">cent Cat-Code-11-SRN OTIIIIOTTIOTIIIIIIO OTIIIIOTIIOTIIIIIIO Cat-Code-11 toebedeeld: 6,2. In Cat-Code-11-SRN TIIIIIIIIITOTTITOSI TIIIIIIIIITOTIITOSI Cat-Code-11 prof. Teulings het Cat-Code-11-SRN TIIITOSIIIIIIIOTIIO TIIIIOTIIIIIIIOTIIO Cat-Code-11 per costringerlo al Cat-Code-11-SRN TIIOTIIIIIIIIIIIOTI TIIOTIIIIIIIIITIOTI Table 6: Confusion matrix for Dutch development set. Predicted, Cat-Code-7 Predicted, Cat-Code-7-SRN Gold I O S T I O S T I 328128 0 2 469 328546 0 0 53 O 0 75234 0 0 0 75234 0 0 S 4 0 4323 18 1 0 4332 12 T 252 0 33 80828 35 0 10 81068 art sentence boundary detection system (Kiss and Strunk, 2006). With its standard distributed models, Punkt achieves 98.51% on our English test set, 98.87% on Dutch and 98.34% on Italian, compared with 100%, 99.54% and 99.51% for our system. Our system benefits here from its ability to adapt to a new domain with relatively little (but annotated) training data. 6 What Elephant? Word and sentence segmentation can be recast as a combined tagging task. This way, tokenization is cast as a supervised learning task, causing a shift of labor from writing rules to manually correcting labels. Learning this task with CRF achieves high accuracy.1 Furthermore, our ta</context>
</contexts>
<marker>Kiss, Strunk, 2006</marker>
<rawString>Tibor Kiss and Jan Strunk. 2006. Unsupervised multilingual sentence boundary detection. Computational Linguistics, 32(4):485–525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML-01,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="6331" citStr="Lafferty et al., 2001" startWordPosition="958" endWordPosition="961">xts, comprising two days from January 2000 extracted from the Twente News Corpus, TwNC (Ordelman et al., 2007), and a random sample of Italian texts from the PAIS A` corpus (Borghetti et al., 2011). Table 1: Datasets characteristics. Name Language Domain Sentences Tokens GMB English Newswire 2,886 64,443 TNC Dutch Newswire 49,537 860,637 PAI Italian Web/various 42,674 869,095 The data was converted into IOB format by inferring an alignment between the raw text and the segmented text. 3.3 Sequence labeling We apply the Wapiti implementation (Lavergne et al., 2010) of Conditional Random Fields (Lafferty et al., 2001), using as features the output label of each character, combined with 1) the character itself, 2) the output label on the previous character, 3) characters and/or their Unicode categories from context windows of varying sizes. For example, with a context size of 3, in Figure 1, features for the E in Eighty-three with the output label S would be E/S, O/S, /S, i/S, Space/S, Lowercase/S. The intuition is that the 31 existing Unicode categories can generalize across similar characters whereas character features can identify specific contexts such as abbreviations or contractions (e.g. didn’t). The</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML-01, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lavergne</author>
<author>Olivier Capp´e</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Practical very large scale CRFs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>504--513</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>Lavergne, Capp´e, Yvon, 2010</marker>
<rawString>Thomas Lavergne, Olivier Capp´e, and Franc¸ois Yvon. 2010. Practical very large scale CRFs. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 504–513, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Mikheev</author>
</authors>
<title>Periods, capitalized words, etc.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="3369" citStr="Mikheev (2002)" startWordPosition="518" endWordPosition="519">-coded regular expressions (Grefenstette, 1999; Silla Jr. and Kaestner, 2004; Jurafsky and Martin, 2008; Dridan and Oepen, 2012). Several unsupervised systems have been proposed for sentence boundary detection. Kiss and Strunk (2006) present a language-independent, unsupervised approach and note that abbreviations form a major source of ambiguity in sentence boundary detection and use collocation detection to build a high-accuracy abbreviation detector. The resulting system reaches high accuracy, rivalling handcrafted rule-based and supervised systems. A similar system was proposed earlier by Mikheev (2002). Existing supervised learning approaches for sentence boundary detection use as features tokens preceding and following potential sentence boundary, part of speech, capitalization information and lists of abbreviations. Learning methods employed in 1422 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422–1426, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics these approaches include maximum entropy models (Reynar and Ratnaparkhi, 1997) decision trees (Riley, 1989), and neural networks (Palmer and Hear</context>
</contexts>
<marker>Mikheev, 2002</marker>
<rawString>Andrei Mikheev. 2002. Periods, capitalized words, etc. Computational Linguistics, 28(3):289–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´aˇs Mikolov</author>
<author>Martin Karafi´at</author>
<author>Luk´aˇs Burget</author>
<author>Jan ˇCernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In Interspeech.</booktitle>
<marker>Mikolov, Karafi´at, Burget, ˇCernock´y, Khudanpur, 2010</marker>
<rawString>Tom´aˇs Mikolov, Martin Karafi´at, Luk´aˇs Burget, Jan ˇCernock´y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roeland Ordelman</author>
<author>Franciska de Jong</author>
<author>Arjan van Hessen</author>
<author>Hendri Hondorp</author>
</authors>
<title>TwNC: a multifaceted Dutch news corpus.</title>
<date>2007</date>
<journal>ELRA Newsleter,</journal>
<pages>12--3</pages>
<marker>Ordelman, de Jong, van Hessen, Hondorp, 2007</marker>
<rawString>Roeland Ordelman, Franciska de Jong, Arjan van Hessen, and Hendri Hondorp. 2007. TwNC: a multifaceted Dutch news corpus. ELRA Newsleter, 12(3/4):4–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Palmer</author>
<author>Marti A Hearst</author>
</authors>
<title>Adaptive multilingual sentence boundary disambiguation.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="3978" citStr="Palmer and Hearst, 1997" startWordPosition="597" endWordPosition="600"> Mikheev (2002). Existing supervised learning approaches for sentence boundary detection use as features tokens preceding and following potential sentence boundary, part of speech, capitalization information and lists of abbreviations. Learning methods employed in 1422 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422–1426, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics these approaches include maximum entropy models (Reynar and Ratnaparkhi, 1997) decision trees (Riley, 1989), and neural networks (Palmer and Hearst, 1997). Closest to our work are approaches that present token and sentence splitters using conditional random fields (Tomanek et al., 2007; Fares et al., 2013). However, these previous approaches consider tokens (i.e. character sequences) as basic units for labeling, whereas we consider single characters. As a consequence, labeling is more resource-intensive, but it also gives us more expressive power. In fact, our approach kills two birds with one stone, as it allows us to integrate token and sentence boundaries detection into one task. 3 Method 3.1 IOB Tokenization IOB tagging is widely used in ta</context>
</contexts>
<marker>Palmer, Hearst, 1997</marker>
<rawString>David D. Palmer and Marti A. Hearst. 1997. Adaptive multilingual sentence boundary disambiguation. Computational Linguistics, 23(2):241–267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>16--19</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Washington, DC, USA.</location>
<contexts>
<context position="3902" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="585" endWordPosition="588">afted rule-based and supervised systems. A similar system was proposed earlier by Mikheev (2002). Existing supervised learning approaches for sentence boundary detection use as features tokens preceding and following potential sentence boundary, part of speech, capitalization information and lists of abbreviations. Learning methods employed in 1422 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422–1426, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics these approaches include maximum entropy models (Reynar and Ratnaparkhi, 1997) decision trees (Riley, 1989), and neural networks (Palmer and Hearst, 1997). Closest to our work are approaches that present token and sentence splitters using conditional random fields (Tomanek et al., 2007; Fares et al., 2013). However, these previous approaches consider tokens (i.e. character sequences) as basic units for labeling, whereas we consider single characters. As a consequence, labeling is more resource-intensive, but it also gives us more expressive power. In fact, our approach kills two birds with one stone, as it allows us to integrate token and sentence boundaries detection i</context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 16– 19, Washington, DC, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael D Riley</author>
</authors>
<title>Some applications of tree-based modelling to speech and language.</title>
<date>1989</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language, HLT ’89,</booktitle>
<pages>339--352</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3931" citStr="Riley, 1989" startWordPosition="591" endWordPosition="593">ilar system was proposed earlier by Mikheev (2002). Existing supervised learning approaches for sentence boundary detection use as features tokens preceding and following potential sentence boundary, part of speech, capitalization information and lists of abbreviations. Learning methods employed in 1422 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422–1426, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics these approaches include maximum entropy models (Reynar and Ratnaparkhi, 1997) decision trees (Riley, 1989), and neural networks (Palmer and Hearst, 1997). Closest to our work are approaches that present token and sentence splitters using conditional random fields (Tomanek et al., 2007; Fares et al., 2013). However, these previous approaches consider tokens (i.e. character sequences) as basic units for labeling, whereas we consider single characters. As a consequence, labeling is more resource-intensive, but it also gives us more expressive power. In fact, our approach kills two birds with one stone, as it allows us to integrate token and sentence boundaries detection into one task. 3 Method 3.1 IO</context>
</contexts>
<marker>Riley, 1989</marker>
<rawString>Michael D. Riley. 1989. Some applications of tree-based modelling to speech and language. In Proceedings of the workshop on Speech and Natural Language, HLT ’89, pages 339–352, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Celso A A Kaestner</author>
</authors>
<title>An analysis of sentence boundary detection systems for English and Portuguese documents.</title>
<date>2004</date>
<booktitle>In Fifth International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<volume>2945</volume>
<pages>135--141</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1731" citStr="Kaestner, 2004" startWordPosition="272" endWordPosition="274">enized format. But like an elephant in the living room, it is a problem that is impossible to overlook whenever new raw datasets need to be processed or when tokenization conventions are reconsidered. It is moreover an important problem, because any errors occurring early in the NLP pipeline affect further analysis negatively. And even though current tokenizers reach high performance, there are three issues that we feel haven’t been addressed satisfactorily so far: • Most tokenizers are rule-based and therefore hard to maintain and hard to adapt to new domains and new languages (Silla Jr. and Kaestner, 2004); • Word and sentence segmentation are often seen as separate tasks, but they obviously inform each other and it could be advantageous to view them as a combined task; • Most tokenization methods provide no alignment between raw and tokenized text, which makes mapping the tokenized version back onto the actual source hard or impossible. In short, we believe that regarding tokenization, there is still room for improvement, in particular on the methodological side of the task. We are particularly interested in the following questions: Can we use supervised learning to avoid hand-crafting rules? </context>
</contexts>
<marker>Kaestner, 2004</marker>
<rawString>Carlos N. Silla Jr. and Celso A. A. Kaestner. 2004. An analysis of sentence boundary detection systems for English and Portuguese documents. In Fifth International Conference on Intelligent Text Processing and Computational Linguistics, volume 2945 of Lecture Notes in Computer Science, pages 135–141. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Tomanek</author>
<author>Joachim Wermter</author>
<author>Udo Hahn</author>
</authors>
<title>Sentence and token splitting based on conditional random fields.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics,</booktitle>
<pages>49--57</pages>
<location>Melbourne, Australia.</location>
<contexts>
<context position="4110" citStr="Tomanek et al., 2007" startWordPosition="618" endWordPosition="621">potential sentence boundary, part of speech, capitalization information and lists of abbreviations. Learning methods employed in 1422 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422–1426, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics these approaches include maximum entropy models (Reynar and Ratnaparkhi, 1997) decision trees (Riley, 1989), and neural networks (Palmer and Hearst, 1997). Closest to our work are approaches that present token and sentence splitters using conditional random fields (Tomanek et al., 2007; Fares et al., 2013). However, these previous approaches consider tokens (i.e. character sequences) as basic units for labeling, whereas we consider single characters. As a consequence, labeling is more resource-intensive, but it also gives us more expressive power. In fact, our approach kills two birds with one stone, as it allows us to integrate token and sentence boundaries detection into one task. 3 Method 3.1 IOB Tokenization IOB tagging is widely used in tasks identifying chunks of tokens. We use it to identify chunks of characters. Characters outside of tokens are labeled O, inside of </context>
</contexts>
<marker>Tomanek, Wermter, Hahn, 2007</marker>
<rawString>Katrin Tomanek, Joachim Wermter, and Udo Hahn. 2007. Sentence and token splitting based on conditional random fields. In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics, pages 49–57, Melbourne, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000164">
<title confidence="0.978184">
Document Summarization via Guided Sentence Compression
</title>
<author confidence="0.998681">
Chen Li&apos;, Fei Liu2, Fuliang Weng2, Yang Liu&apos;
</author>
<affiliation confidence="0.947795666666667">
&apos; Computer Science Department, The University of Texas at Dallas
Richardson, Texas 75080, USA
2 Research and Technology Center, Robert Bosch LLC
</affiliation>
<address confidence="0.805087">
Palo Alto, California 94304, USA
</address>
<email confidence="0.991505">
{chenli,yangl@hlt.utdallas.edu}
{fei.liu, fuliang.weng@us.bosch.com}
</email>
<sectionHeader confidence="0.998544" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999841833333333">
Joint compression and summarization has
been used recently to generate high quality
summaries. However, such word-based joint
optimization is computationally expensive. In
this paper we adopt the ‘sentence compression
+ sentence selection’ pipeline approach for
compressive summarization, but propose to
perform summary guided compression, rather
than generic sentence-based compression. To
create an annotated corpus, the human anno-
tators were asked to compress sentences while
explicitly given the important summary words
in the sentences. Using this corpus, we train
a supervised sentence compression model us-
ing a set of word-, syntax-, and document-
level features. During summarization, we use
multiple compressed sentences in the inte-
ger linear programming framework to select
salient summary sentences. Our results on the
TAC 2008 and 2011 summarization data sets
show that by incorporating the guided sen-
tence compression model, our summarization
system can yield significant performance gain
as compared to the state-of-the-art.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960190476191">
Automatic summarization can be broadly divided
into two categories: extractive and abstractive sum-
marization. Extractive summarization focuses on
selecting the salient sentences from the document
collection and concatenating them to form a sum-
mary; while abstractive summarization is generally
considered more difficult, involving sophisticated
techniques for meaning representation, content plan-
ning, surface realization, etc., and the “true abstrac-
tive summarization remains a researcher’s dream”
(Radev et al., 2002).
There has been a surge of interest in recent
years on generating compressed document sum-
maries as a viable step towards abstractive sum-
marization. These compressive summaries often
contain more information than sentence-based ex-
tractive summaries since they can remove insignif-
icant sentence constituents and make space for more
salient information that is otherwise dropped due to
the summary length constraint. Two general strate-
gies have been used for compressive summarization.
One is a pipeline approach, where sentence-based
extractive summarization is followed or proceeded
by sentence compression (Knight and Marcu, 2000;
Lin, 2003; Zajic et al., 2007; Wang et al., 2013).
Another line of work uses joint compression and
summarization. They have been shown to achieve
promising performance (Daum´e, 2006; Martins and
Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali
and Hasan, 2012; Almeida and Martins, 2013; Qian
and Liu, 2013). One popular approach for such joint
compression and summarization is via integer lin-
ear programming (ILP). However, since words are
the units in the optimization framework, solving this
ILP problem can be expensive.
In this study, we use the pipeline compression
and summarization method because of its compu-
tational efficiency. Prior work using such pipeline
methods simply uses generic sentence-based com-
pression for each sentence in the documents, no mat-
ter whether compression is done before or after sum-
mary sentence extraction. We propose to use sum-
</bodyText>
<page confidence="0.974587">
490
</page>
<note confidence="0.7329065">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 490–500,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999946676470588">
mary guided compression combined with ILP-based
sentence selection for summarization in this paper.
We create a compression corpus for this purpose.
Using human summaries for a set of documents, we
identify salient words in the sentences. During anno-
tation, the human annotators are given these salient
words and asked to generate compressed sentences.
We expect such “guided” sentence compression is
beneficial for the pipeline compression and summa-
rization task. In addition, previous research on joint
modeling for compression and summarization sug-
gested that the labeled extraction and compression
data sets would be helpful for learning a better joint
model (Daum´e, 2006; Martins and Smith, 2009).
We hope that our work on this guided compression
will also be of benefit to the future joint modeling
studies.
Using our created compression data, we train
a supervised compression model using a variety
of word-, sentence-, and document-level features.
During summarization, we generate multiple com-
pression candidates for each sentence, and use the
ILP framework to select compressed summary sen-
tences. In addition, we also propose to apply a pre-
selection step to select some important sentences,
which can both speed up the summarization system
and improve performance. We evaluate our pro-
posed summarization approach on the TAC 2008
and 2011 data sets using the standard ROUGE met-
ric (Lin, 2004). Our results show that by incorporat-
ing a guided sentence compression model, our sum-
marization system can yield significant performance
gain as compared to the state-of-the-art reported re-
sults.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999263949152542">
Summarization research has seen great development
over the last fifty years (Nenkova and McKeown,
2011). Compared to the abstractive counterpart, ex-
tractive summarization has received considerable at-
tention due to its clear problem formulation – to ex-
tract a set of salient and non-redundant sentences
from the given document set. Both unsupervised and
supervised approaches have been explored for sen-
tence selection. The supervised approaches include
the Bayesian classifier (Kupiec et al., 1995), max-
imum entropy (Osborne, 2002), skip-chain condi-
tional random fields (CRF) (Galley, 2006), discrim-
inative reranking (Aker et al., 2010), among others.
The extractive summary sentence selection prob-
lem can also be formulated in an optimization
framework. Previous approaches include the inte-
ger linear programming (ILP) and submodular func-
tions, which are used to solve the optimization prob-
lem. In particular, Gillick et al. (2009) proposed
a concept-based ILP approach for summarization.
Li et al. (2013) improved it by using supervised
stragety to estimate concept weight in ILP frame-
work. In (Lin and Bilmes, 2010), the authors model
the sentence selection problem as maximizing a sub-
modular function under a budget constraint. A
greedy algorithm is proposed to efficiently approxi-
mate the solution to this NP-hard problem.
Compressive summarization receives increasing
attention in recent years, since it offers a viable
step towards abstractive summarization. The com-
pressed summaries can be generated through a joint
model of the sentence selection and compression
processes, or through a pipeline approach that in-
tegrates a generic sentence compression model with
a summary sentence pre-selection or post-selection
step.
Many studies explore the joint sentence compres-
sion and selection setting. Martins and Smith (2009)
jointly perform sentence extraction and compression
by solving an ILP problem; Berg-Kirkpatrick et al.
(2011) propose an approach to score the candidate
summaries according to a combined linear model
of extractive sentence selection and compression.
They train the model using a margin-based objec-
tive whose loss captures the final summary qual-
ity. Woodsend and Lapata (2012) present a method
where the summary’s informativeness, succinctness,
and grammaticality are learned separately from data
but optimized jointly using an ILP setup; Yoshikawa
et al. (2012) incorporate semantic role information
in the ILP model; Chali and Hasan (2012) investi-
gate three strategies in compressive summarization:
compression before extraction, after extraction, or
joint compression and extraction in one global op-
timization framework. These joint models offer a
promise for high quality summaries, but they often
have high computational cost. Qian and Liu (2013)
propose a graph-cut based method that improves the
speed of joint compression and summarization.
</bodyText>
<page confidence="0.998023">
491
</page>
<bodyText confidence="0.999981794871795">
The pipeline approach, where sentence-based ex-
tractive summarization is followed or proceeded by
sentence compression, is also popular. Knight and
Marcu (2000) utilize the noisy channel and deci-
sion tree method to perform sentence compression;
Lin (2003) shows that pure syntactic-based com-
pression may not improve the system performance;
Zajic et al. (2007) compare two sentence compres-
sion approaches for multi-document summarization,
including a ‘parse-and-trim’ and a noisy-channel ap-
proach; Galanis and Androutsopoulos (2010) use
the maximum entropy model to generate the candi-
date compressions by removing the branches from
the source sentences; Liu and Liu (2013) couple
the sentence compression and extraction approaches
for summarizing the spoken documents; Wang et al.
(2013) design a series of learning-based compres-
sion models built on parse trees, and integrate them
in query-focused multi-document summarization.
Prior studies often rely heavily on the generic sen-
tence compression approaches (McDonald, 2006;
Nomoto, 2007; Clarke and Lapata, 2008; Thadani
and McKeown, 2013) for compressing the sentences
in the documents, yet a generic compression system
may not be the best fit for the summarization pur-
pose.
In this paper, we adopt the pipeline-based com-
pressive summarization framework, but propose a
novel guided compression method that is catered to
the summarization task. We expect this approach
to take advantage of the efficient pipeline process-
ing while producing satisfying results as the joint
models. We train a supervised guided compression
model to produce n-best compressions for each sen-
tence, and use an ILP formulation to select the best
set of summary sentences. In addition, we pro-
pose to apply a sentence pre-selection step to fur-
ther accelerate the processing and enhance the per-
formance.
</bodyText>
<sectionHeader confidence="0.997325" genericHeader="method">
3 Guided Compression Corpus
</sectionHeader>
<bodyText confidence="0.999725833333333">
The goal of guided sentence compression is to create
compressed sentences that are grammatically cor-
rect and contain the important information that we
would like to preserve in the final summary. Fol-
lowing the compression literature (Clarke and Lap-
ata, 2008), the compression task is defined as a word
</bodyText>
<subsectionHeader confidence="0.989921">
Original Sentence:
</subsectionHeader>
<bodyText confidence="0.998187333333333">
The gas leak was contained Monday afternoon , nearly 18
hours after it was reported , Statoil spokesman Oeivind
Reinertsen said.
</bodyText>
<subsectionHeader confidence="0.895388">
Compression A:
</subsectionHeader>
<bodyText confidence="0.981751">
The gas leak was contained
</bodyText>
<subsectionHeader confidence="0.870456">
Compression B:
</subsectionHeader>
<bodyText confidence="0.948179">
The gas leak was contained Monday afternoon
</bodyText>
<subsectionHeader confidence="0.776299">
Compression C:
</subsectionHeader>
<bodyText confidence="0.884808">
The gas leak was contained nearly 18 hours after it was
</bodyText>
<subsubsectionHeader confidence="0.273875">
reported
</subsubsectionHeader>
<tableCaption confidence="0.999214">
Table 1: Example sentence and three compressions.
</tableCaption>
<bodyText confidence="0.999901151515152">
deletion problem, that is, the human annotators (and
also automatic compression systems) are allowed to
only remove words from the original sentence to
form a compression. The key difference between
our proposed guided compression with generic sen-
tence compression is that, we provide guidance to
the human compression process by specifying a set
of “important words” that we wish to keep for each
sentence. We expect this kind of summary oriented
compression would benefit the ultimate summariza-
tion task. Take the sentence shown in Table 1 as an
example. For generic sentence compression, there
may be multiple ‘good’ human compressions for this
sentence, such as those listed in the table. Without
guidance, a human annotator (or automatic system)
is likely to use option A or B; however, if “18 hours”
appears in the summary, then we want to provide this
guidance in the compression process, hence option
C may be the best compression choice. This guided
compression therefore avoids removing the salient
words that are important to the final summary.
To generate the guided compression corpus, we
use the TAC 2010 data set1 that was used for
the multi-document summarization task. There are
46 topics. Each has 10 news documents, and
also four human-created abstractive reference sum-
maries. Since annotating all the sentences in this
data set is time consuming and some sentences are
not very important for the summarization task, we
choose a set of sentences that are highly related to
the human abstracts for annotation. We compare
each sentence with the four human abstracts using
the ROUGE-2 metric (Lin, 2004), and the sentences
</bodyText>
<footnote confidence="0.988424">
1http://www.nist.gov/tac/2010/
</footnote>
<page confidence="0.991451">
492
</page>
<table confidence="0.687166117647059">
Original Sentence:
He said Vietnam veterans are presumed to have been ex-
posed to Agent Orange and veterans with any of the 10 dis-
eases is presumed to have contracted it from the exposure ,
without individual proof.
Guided Compression:
Vietnam veterans are presumed to have been exposed to
Agent Orange.
Original Sentence:
The province has limited the number of trees to be chopped
down in the forest area in northwest Yunnan and has stopped
building sugar factories in the Xishuangbanna region to
preserve the only tropical rain forest in the country located
there .
Guided Compression:
province has stopped building sugar factories in the
Xishuangbanna region to preserve tropical rain forest.
</table>
<tableCaption confidence="0.983032">
Table 2: Example original sentences and their guided
compressions. The “guiding words” are italicized and
marked in red.
</tableCaption>
<bodyText confidence="0.999690076923077">
with the highest scores are selected.
In annotation, human annotators are provided
with important ‘guiding words’ (highlighted in the
annotation interface) that we want to preserve in the
sentences. We calculate the word overlap between a
sentence and each of those sentences in the human
abstracts, and use a set of heuristic rules to deter-
mine the “guiding words” in a sentence: the longest
consecutive word overlaps (greater than 2 words) in
each sentence pair are first selected; the rest overlaps
that contain 2 or more words (excluding the stop-
words) are also selected. We suggest the human an-
notators to use their best judgment to keep the guid-
ing words as many as possible while compressing
the sentence.
We use the Amazon Mechanical Turk (AMT) for
data annotation2. In total, we select 1,150 sentences
from the TAC news documents. They are grouped
into about 230 human intelligence tasks (HITs) with
5 sentences in each HIT. A sentence was compressed
by 3 human annotatorsand we select the shortest
candidate as the goldstandard compression for each
sentence. In Table 2, we show two example sen-
tences, their guiding words (bold), and the human
compressions. The first example shows that giving
up some guiding words is acceptable, since more
</bodyText>
<footnote confidence="0.857222">
2http://www.mturk.com
</footnote>
<bodyText confidence="0.999972894736842">
unnecessary words will be included in order to ac-
commodate all the guiding words; the second ex-
ample shows that the guided compression can lead
to more aggressive word deletions since the con-
stituents that are not important to the summary will
be deleted even though they contain salient informa-
tion by themselves.
For our compression corpus, which contains
1,150 sentences and their guided compressions, the
average compression rate, as measured by the per-
centage of dropped words, is about 50%. This com-
pression ratio is higher compared to other generic
sentence compression corpora, in which the word
deletion rate ranges from 24% to 34% depending
on different text genres and annotation guidelines
(Clarke and Lapata, 2008; Liu and Liu, 2009). This
suggests that the annotators can remove words more
aggressively when they are provided with a limited
set of guiding words.
</bodyText>
<sectionHeader confidence="0.938248" genericHeader="method">
4 Summarization System
</sectionHeader>
<bodyText confidence="0.9999612">
Our summarization system consists of three key
components: we train a supervised guided compres-
sion model using our created compression data, with
a variety of features.then we use this model to gener-
ate n-best compressions for each sentence; we feed
the multiple compressed sentences to the ILP frame-
work to select the best summary sentences. In ad-
dition, we propose a sentence pre-selection step that
can both speed up the summarization system and im-
prove the performance.
</bodyText>
<subsectionHeader confidence="0.996422">
4.1 Guided Sentence Compression
</subsectionHeader>
<bodyText confidence="0.999822571428571">
Sentence compression has been explored in previous
studies using both supervised and unsupervised ap-
proaches, including the noisy-channel and decision
tree model (Knight and Marcu, 2000; Turner and
Charniak, 2005), discriminative learning (McDon-
ald, 2006), integer linear programming (Clarke and
Lapata, 2008; Thadani and McKeown, 2013), con-
ditional random fields (CRF) (Nomoto, 2007; Liu
and Liu, 2013), etc. In this paper, we employ the
CRF-based compression approach due to its proved
performance and its flexibility to integrate differ-
ent levels of discriminative features. Under this
framework, sentence compression is formulated as
a sequence labeling problem, where each word is
</bodyText>
<page confidence="0.996485">
493
</page>
<bodyText confidence="0.996633727272727">
labeled as either “0” (retained) or “1” (removed).
We develop different levels of features to capture
word-specific characteristics, sentence related infor-
mation, and document level importance. Most of the
features are extracted based only on the sentence to
be compressed. However, we introduce a few doc-
ument level features. These are designed to cap-
ture the word and sentence significance within the
given document collection and are thus expected to
be more summary related.
Word and sentence features:
</bodyText>
<listItem confidence="0.994848709677419">
• Word n-grams: identity of the current word
and two words before and after, as well as all
the bigrams and trigrams that can be formed by
the adjacent words and the current word.
• POS n-grams: same as the word n-grams, but
use the part-of-speech tags instead.
• Named entity tags: binary features represent-
ing whether the current word is a person, loca-
tion, or temporal expression. We use the Stan-
ford CoreNLP tools3 for named entity tagging.
• Stopwords: whether the current word is a stop-
word or not.
• Conjunction features: (1) conjunction of the
current word with its relative position in the
sentence; (2) conjunction of the NER tag with
its relative position.
• Syntactic features: We obtain the syntactic
parsing tree using the Berkeley Parser (Petrov
and Klein, 2007), then obtain the following fea-
tures: (1) the last sentence constituent tag in
the path from the root to the word; (2) depth:
length of the path starting from the root node
to the word; (3) normalized depth: depth di-
vided by the longest path in the parsing tree;
(4) whether the word is under an SBAR node;
(5) depth and normalized depth of the SBAR
node if the word is under an SBAR node;
• Dependency features: We employ the
Penn2Malt toolkit 4 to convert the parse re-
sult from the Berkeley parser to the depen-
dency parsing tree, and use these dependency
</listItem>
<footnote confidence="0.9995835">
3http://nlp.stanford.edu/software/corenlp.shtml
4http://stp.lingfil.uu.se/˜nivre/research/Penn2Malt.html
</footnote>
<bodyText confidence="0.9119678">
features: (1) dependency relations such as
‘AMOD’ (adjective modifier), ‘NMOD’ (noun
modifier), etc. (2) whether the word has a child,
left child, or right child in the dependency tree.
Document-level features:
</bodyText>
<listItem confidence="0.966930538461538">
• Sentence salience score: We use a simple re-
gression model to estimate a salience score for
each sentence (more details in Section 4.3),
which represents the importance of the sen-
tence in the document. This score is discretized
into four binary features according to the aver-
age sentence salience.
• Unigram document frequency: this is the
current word’s document frequency based on
the 10 documents associated with each topic.
• Bigram document frequency: document fre-
quency for the two bigrams, the current word
and its previous or next word.
</listItem>
<bodyText confidence="0.999158071428572">
Some of the above features were employed in re-
lated sentence compression studies (Nomoto, 2007;
Liu and Liu, 2013). In addition to these features, we
explored other related features, including the abso-
lute position of the current word, whether the word
appears in the corresponding topic title and descrip-
tions, conjunction of the syntactic tag with the tree
depth, etc.; however, these features did not lead to
improved performance. We train the CRF model
with the Pocket CRF toolkit5 using the guided com-
pression corpus collected in Section 3. During sum-
marization, we apply the model to a given sentence
to generate its n-best guided compressions and use
them in the following summarization step.
</bodyText>
<subsectionHeader confidence="0.99641">
4.2 Summary Sentence Selection
</subsectionHeader>
<bodyText confidence="0.999298375">
The sentence selection process is similar to the stan-
dard sentence-based extractive summarization, ex-
cept that the input to the selection module is a list
of compressed sentences in our work. Many extrac-
tive summarization approaches can be applied for
this purpose. In this work, we choose the integer
linear programming (ILP) method, specifically, the
concept-based ILP framework introduced in (Gillick
</bodyText>
<footnote confidence="0.979711">
5http://sourceforge.net/projects/pocket-crf-1/
</footnote>
<page confidence="0.997147">
494
</page>
<bodyText confidence="0.99994805882353">
et al., 2009), mainly because it yields best perfor-
mance in the TAC evaluation tasks. This ILP ap-
proach aims to extract sentences that can cover as
many important concepts as possible, while ensuring
the summary length is within a given constraint. We
follow the study in (Gillick et al., 2009) to use word
bi-grams as concepts, and assign a weight to each
bi-gram using its document frequency in the given
document collection for a test topic. Two differences
are between our ILP setup and that in (Gillick et al.,
2009). First, since we use multiple compressions
for one sentence, we need to introduce an additional
constraint: for each sentence, only one of the n-best
compressions may be included in the summary. Sec-
ond, we optimize a joint score of the concept cover-
age and the sentence salience. The formal ILP for-
mulation is shown below:
</bodyText>
<equation confidence="0.996939727272727">
�max �wici + �vj Sjk (1)
i j k
�S.t. Sjk G 1Vj (2)
k
SjkOcci jk G ci (3)
� SjkOcci jk �! ci (4)
jk
� ljkSjk G L (5)
jk
ci E {0, 11 Vi (6)
Sjk E 10, 11 Vj, k (7)
</equation>
<bodyText confidence="0.997583222222222">
where ci and Sjk are binary variables indicating the
presence of a concept and a sentence respectively;
Sjk denotes the kth candidate compression of the
jth sentence; wi represents the weight of the con-
cept; vj is the sentence salience score of the jth
sentence, predicted using a regression model (Sec-
tion 4.3), and all of its compressed candidates share
this value. (1) is the new objective function we use
that combines the coverage of the concepts and the
sentence salience scores. (2) represents our addi-
tional constraint, which requires that for each sen-
tence j, only one candidate compression will be cho-
sen. Occi jk represents the occurrence of concept i
in the sentence Sjk. Inequalities (3) and (4) associate
the sentences and the concepts. Constraint (5) con-
trols the summary length, as measured by the total
number of words in the summary. We use an open
source ILP solver6.
</bodyText>
<subsectionHeader confidence="0.99605">
4.3 Sentence Pre-selection
</subsectionHeader>
<bodyText confidence="0.999848482758621">
The above ILP method can offer an exact solution
to the defined objective function. However, ILP is
computationally expensive when the formulation in-
volves large quantities of variables, i.e, when we
have many sentences and a large number of candi-
date compressions for each sentence. We therefore
propose to apply a sentence pre-selection step be-
fore the compression. This kind of selection step
has been used in previous ILP-based summarization
systems (Berg-Kirkpatrick et al., 2011; Gillick et al.,
2009). In this work, we propose to use a simple su-
pervised support vector regression (SVR) model (Ng
et al., 2012) to predict a salience score for each sen-
tence and select the top ranked sentences for further
processing (compression and summarization).
To train the SVR model, the target value for each
sentence is the ROUGE-2 score between the sen-
tence and the four human abstracts (this same value
is used for sentence selection in corpus annotation
(Section 3)). We employ three commonly used fea-
tures: (1) sentence position in the document; (2) sen-
tence length as indicated by a binary feature: it takes
the value of 0 if the number of words in the sentence
is greater than 50 or less than 10, otherwise the fea-
ture value is 1; (3) interpolated n-gram document
frequency as introduced in (Ng et al., 2012), which
is a weighted linear combination of the document
frequency of the unigrams and bigrams contained in
the sentence:
</bodyText>
<equation confidence="0.9637595">
α E,,,,,ES DF(w.,) + (1 − α) E,,,CS DF(wb)
|S|
</equation>
<bodyText confidence="0.9998302">
where wu and wb represent the unigrams and bi-
grams contained in the sentence S; α is a balancing
factor; |S |denotes the number of words in the sen-
tence.
The SVR model was trained using the SVMlight
toolkit7. Using this model, we can predict a salience
score (Vj in Eq 1) for each sentence and only select
the top n sentences and supply them to the compres-
sion and summarization steps. In practice, using a
fixed n may not be a good choice since the number
</bodyText>
<footnote confidence="0.996562">
6http://www.gnu.org/software/glpk/
7http://svmlight.joachims.org/
</footnote>
<equation confidence="0.904514">
f(S) _
</equation>
<page confidence="0.994508">
495
</page>
<bodyText confidence="0.983256">
of sentences varies greatly for different topics. We
therefore set n heuristically based on the total num-
ber of sentences m for each topic: n=15 if m &gt; 150;
n=10 if m &lt; 100; n=0.1 * m otherwise.
</bodyText>
<sectionHeader confidence="0.993321" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.931145">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.99998912">
For our experiments, we use the standard TAC data
sets8, which have been used in the NIST competi-
tions and in other summarization studies. In par-
ticular, we used the TAC 2010 data set for creating
the guided compression corpus and training the SVR
pre-selection model, the TAC 2009 data set as devel-
opment set for parameter tuning, and the TAC 2008
and 2011 data sets as the test set for reporting the
final summarization results.
We compare our pipeline summarization sys-
tem against three recent studies, which have re-
ported some of the highest published results on this
task. Berg-Kirkpatrick et al. (2011) introduce a
joint model for sentence extraction and compres-
sion. The model is trained using a margin-based ob-
jective whose loss captures the end summary qual-
ity; Woodsend and Lapata (2012) learn individ-
ual summary aspects from data, e.g., informative-
ness, succinctness, grammaticality, stylistic writ-
ing conventions, and jointly optimize the outcome
in an integer linear programming framework. Ng
et al. (2012) exploit category-specific information
for multi-document summarization. In addition to
the three previous studies, we also report the best
achieved results in the TAC competitions.
</bodyText>
<subsectionHeader confidence="0.999643">
5.2 Summarization Results
</subsectionHeader>
<bodyText confidence="0.999966166666666">
In Table 3 and Table 4, we present the results of our
system and the aforementioned summarization stud-
ies. We use the ROUGE evaluation metrics (Lin,
2004), with R-2 measuring the bigram overlap be-
tween the system and reference summaries and R-
SU4 measuring the skip-bigram with the maximum
gap length of 4. “Our System” uses the pipeline
setting including the three components described in
Section 4. We use the SVR-based approach to pre-
select a set of sentences from the document set; these
sentences are further fed to the guided compression
module that produces n-best compressions for each
</bodyText>
<footnote confidence="0.925849">
8http://www.nist.gov/tac/data/index.html
</footnote>
<table confidence="0.999807428571429">
System R-2 R-SU4 CompR
TAC’08 Best System 11.03 13.96 n/a
(Berg-Kirkpatrick et al., 2011) 11.70 14.38 n/a
(Woodsend et al., 2012) 11.37 14.47 n/a
Our System 12.35† 15.27† 43.06%
Our System w/o Pre-selection 12.02 14.98 55.69%
Our System w/ Generic Comp 10.88 13.79 30.90%
</table>
<tableCaption confidence="0.739545666666667">
Table 3: Results on the TAC 2008 data set. “Our Sys-
tem” uses the SVR-based sentence pre-selection + guided
compression + ILP-based summary sentence selection.
“Our System w/ Generic Comp” uses the pre-selection +
generic compression + ILP summary sentence selection
setting. “CompR” represents the compression ratio, i.e.,
percentage of dropped words. † represents our system
outperforms the best previous result at the 95% signifi-
cance level.
</tableCaption>
<table confidence="0.999907">
System R-2 R-SU4 CompR
TAC’11 Best System 13.44 16.51 n/a
(Ng et al., 2012) 13.93 16.83 n/a
Our System 14.40 16.89 39.90%
Our System w/o Pre-selection 13.74 16.5 53.81%
Our System w/ Generic Comp 13.08 16.23 30.10%
</table>
<tableCaption confidence="0.9829035">
Table 4: Results on the TAC 2011 data set. The systems
use the same settings as for the TAC 2008 data set.
</tableCaption>
<bodyText confidence="0.999495">
sentence; the ILP-based framework is then used to
select the summary sentences from these compres-
sions.
We can see from the table that in general, our sys-
tem achieves considerably better results compared to
the state-of-the-art on both the TAC 2008 and 2011
data sets. On the TAC 2008 data set, our system out-
performs the best reported result at the 95% signifi-
cance level; on the TAC 2011 data set, our system
also yields considerable performance gain though
not exceed the 95% significance level. In the fol-
lowing, we show more detailed analysis to study the
effect of different system parameters.
With or without sentence pre-selection. First
we evaluate the impact of sentence pre-selection
step. In Table 3 and Table 4, we include the
results when this step is not used (“Our System
w/o Pre-selection”). That is, all of the sentences
in the documents (excluding those containing less
than 5 words) are compressed and used in the ILP-
</bodyText>
<page confidence="0.998153">
496
</page>
<bodyText confidence="0.999918628571428">
based summary sentence selection module. We can
see that although sentence pre-selection removes
some sentences from consideration in the later sum-
marization step, it actually significantly improves
system performance. In the TAC 2008 data set,
each topic contains averagely 210 sentences; while
the pre-selection step chooses 13 sentences among
them. These numbers are 185 and 12 for the TAC
2011 data set. Table 5 shows the average running
time of each topic in TAC 2011 data for the two sys-
tems, with or without the pre-selection step. Here
we fix the number of compressions to 100 in both
cases for fair comparison. We can see the selec-
tion step greatly accelerates the system processing.
When applying the pre-selection step, fewer sen-
tences are used in the compression and summariza-
tion, this means we are able to use more compres-
sion candidates for each sentence (considering the
complexity of ILP module). Using the TAC 2009
as development set, we tuned the number of can-
didate compressions generated for each sentence.
Without pre-selection, we used the 100-best candi-
dates generated from the compression model; with
pre-selection, we are able to increase the number
to 200-best candidate compressions and still main-
tain reasonable computational cost. These are the
numbers used in the results in Table 3 and 4. Us-
ing more compressions helps improve summariza-
tion performance. We also notice that the compres-
sion ratios are quite different when using sentence
pre-selection vs. not. This suggests that in the im-
portant sentences (those are kept after pre-selection),
there is more summary related information and thus
the compression model keeps more words in them
(lower compression ratio).
</bodyText>
<table confidence="0.9896224">
Compressed Number of Running
System
Sentences Compressions Time (sec)
w/o Pre-selection 185 100 3.9
w/ Pre-selection 12 100 0.85
</table>
<tableCaption confidence="0.997058">
Table 5: Average running time of our system, w/ or w/o
</tableCaption>
<bodyText confidence="0.969807660377359">
the sentence pre-selection step. Experiments conducted
on the TAC 2011 data set. Running time refers only to
the execution time of the ILP module for each topic.
Number of compression candidates. This pa-
rameter (denoted as n) also impacts system perfor-
mance. Figure 1 shows the R-2 scores of the two
systems (with and without the sentence pre-selection
step) when using different number of compressions
for each sentence. In general, we find that the R-2
scores do not change much when n is large enough.
For example, the ‘with pre-selection’ system can
achieve relatively stable R-2 scores on the TAC 2008
data set (ranging from 12.2 to 12.4) when m is
greater than 140; similarly, the R-2 scores on the
TAC 2011 data is over 14.2 when m is greater than
100. Without the pre-selection step, the scores are
less stable in regard to the changing of the m value,
since the large amount of sentences plus a high vol-
ume of the compression candidates may incur huge
computational cost to the ILP solver. This is also the
reason that in Figure 1, for the system without pre-
selection, we only vary n from 1 to 100. In general,
we also notice that given more compression candi-
dates, the R-2 score is still improving, as indicated
by Figure 1. The improved performance of ‘with
pre-selection’ over ‘without pre-selection’ is partly
because fewer sentences are used and thus we are
able to increase the number of compression candi-
dates for these sentences in the ILP sentence extrac-
tion module.
Quality of sentence compression training data.
In order to illustrate the contribution of our
summary-guided sentence compression component,
we train a generic sentence compression model
and use this in our compression and summariza-
tion pipeline. The generic compression model was
trained using the Edinburgh sentence compression
corpus (Clarke and Lapata, 2008), which contains
1370 sentences collected from news articles. This
data set has been widely used in other summariza-
tion studies (Martins and Smith, 2009). Each sen-
tence has 3 compressions and we choose the short-
est compression as the reference. The average com-
pression rate of this corpus is about 28%, lower than
that in our summary guided compression data. Note
that in generic sentence compression, we only use
those word and sentence features described in Sec-
tion 4.1, not the document-level features since they
are not available for the Edinburgh data set. Results
of our system using the generic compression model
(with sentence pre-selection) are shown in the last
row of Table 3 and Table 4. We can see that the sys-
tem with this generic compression model performs
</bodyText>
<page confidence="0.991179">
497
</page>
<figure confidence="0.999624">
0 20 40 60 80 100
# Compression Candidates
0 50 100 150 200 250
# Compression Candidates
</figure>
<figureCaption confidence="0.988617666666667">
Figure 1: R-2 scores of the two systems (without and
with the sentence pre-selection step) when using differ-
ent number of compressions for each sentence.
</figureCaption>
<bodyText confidence="0.851047833333333">
pact caused by the downstream summary sentence
selection module. As seen from Figure 2, increasing
the compression training data generally improves
summarization performance, although there are also
TAC 2011
fluctuations. When adding more training sentence
</bodyText>
<note confidence="0.674227">
TAC 2008
</note>
<bodyText confidence="0.7544605">
pairs, the system performance is likely to further in-
crease.
</bodyText>
<figureCaption confidence="0.99085">
Figure 2: ROUGE-2 scores when using different number
of sentences to train the guided compression model.
</figureCaption>
<figure confidence="0.993638119047619">
6 Conclusion and Future Work
15
14.5
14
13.5
13
12.5
12
11.5
11
TAC 2011
TAC 2008
12.5
12
11.5
11
10.5
200 400 600 800 10001200
# Sentence Pairs in the Training Set
TAC 2011
TAC 2008
ROUGE-2
ROUGE-2
51
21314
C C
2 2
0 0
0 1
8 1
ROUGE-2
14.5
13.5
12.5
11.5
15
14
13
12
11
TAC 2011
TAC 2008
</figure>
<bodyText confidence="0.996984279069767">
worse than ours, and is also inferior to the TAC best
performing system on both data sets, which signi-
fies the importance of our proposed summary guided
sentence compression approach. We can also see
there is a difference in the compression ratio in the
system generated compressions when using differ-
ent compression corpora to train the compression
models. The resulting compression ratio patterns are
consistent with those in the training data, that is, us-
ing our guided compression corpus our system com-
pressed sentences more aggressively.
Learning curve of guided compression. Since
we use a supervised compression model, we further
consider the relationship between the summarization
performance and the number of sentence pairs used
for training the guided compression model. In to-
tal, there are 1150 training sentence pairs in our cor-
pus. We incrementally add 100 sentence pairs each
time and plot the learning curve in Figure 2. In
the compression step, we generate only the 1-best
compression candidate in order to remove the im-
In this paper, we propose a pipeline summariza-
tion approach that combines a novel guided com-
pression model with ILP-based summary sentence
selection. We create a guided compression cor-
pus, where the human annotators were explicitly in-
formed about the important summary words during
the compression annotation. We then train a super-
vised compression model to capture the guided com-
pression process using a set of word-, sentence-, and
document-level features. We conduct experiments
on the TAC 2008 and 2011 summarization data sets
and show that by incorporating the guided sentence
compression model, our summarization system can
yield significant performance gain as compared to
the state-of-the-art. In future, we would like to
further explore the reinforcement relationship be-
tween keywords and summaries (Wan et al., 2007),
improve the readability of the sentences generated
from the guided compression system, and report re-
sults using multiple evaluation metrics (Nenkova et
al., 2007; Louis and Nenkova, 2012) as well as per-
forming human evaluations.
</bodyText>
<page confidence="0.99849">
498
</page>
<sectionHeader confidence="0.999213" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999714375">
Part of this work was done during the first au-
thor’s internship in Bosch Research and Technol-
ogy Center. The work is also partially supported
by NSF award IIS-0845484 and DARPA Contract
No. FA8750-13-2-0041. Any opinions, findings,
and conclusions or recommendations expressed are
those of the author and do not necessarily reflect the
views of the funding agencies.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999608633333333">
Ahmet Aker, Trevor Cohn, and Robert Gaizauskas. 2010.
Multi-document summarization using a* search and
discriminative training. In Proceedings of EMNLP.
Miguel B. Almeida and Andre F. T. Martins. 2013. Fast
and robust compressive summarization with dual de-
composition and multi-task learning. In Proceedings
of ACL.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL.
Yllias Chali and Sadid A. Hasan. 2012. On the effective-
ness of using sentence compression models for query-
focused multi-document summarization. In Proceed-
ings of COLING.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression an integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research.
Hal Daum´e. 2006. Practical structured learning tech-
niques for natural language processing. Ph.D. thesis,
University of Southern California.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance. In
Proceedings of EMNLP.
Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The icsi/utd
summarization system at tac 2009. In Proceedings of
TAC.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization - step one: Sentence compression.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings of
SIGIR.
Chen Li, Xian Qian, and Yang Liu. 2013. Using super-
vised bigram-based ilp for extractive summarization.
In Proceedings of ACL.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodular
functions. In Proceedings of NAACL.
Chin-Yew Lin. 2003. Improving summarization perfor-
mance by sentence compression - A pilot study. In
Proceeding of the Sixth International Workshop on In-
formation Retrieval with Asian Language.
Chin-Yew Lin. 2004. Rouge: a package for automatic
evaluation of summaries. In Proceedings ofACL.
Fei Liu and Yang Liu. 2009. From extractive to abstrac-
tive meeting summaries: Can it be done by sentence
compression? In Proceedings of ACL-IJCNLP.
Fei Liu and Yang Liu. 2013. Towards abstractive speech
summarization: Exploring unsupervised and super-
vised approaches for spoken utterance compression.
IEEE Transactions on Audio, Speech, and Language
Processing.
Annie Louis and Ani Nenkova. 2012. Automati-
cally assessing machine summary content with a gold-
standard. Computational Linguistics.
Andre F. T. Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction and
compression. In Proceedings of the ACL Workshop
on Integer Linear Programming for Natural Language
Processing.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL.
Ani Nenkova and Kathleen McKeown. 2011. Automatic
summarization. Foundations and Trends in Informa-
tion Retrieval.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. ACM Transactions on Speech and Language
Processing.
Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen Kan,
and Chew-Lim Tan. 2012. Exploiting category-
specific information for multi-document summariza-
tion. In Proceedings of COLING.
Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. Information
Processing and Management.
Miles Osborne. 2002. Using maximum entropy for sen-
tence extraction. In Proceedings of the ACL-02 Work-
shop on Automatic Summarization.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL.
</reference>
<page confidence="0.989597">
499
</page>
<reference confidence="0.999257966666667">
Xian Qian and Yang Liu. 2013. Fast joint compression
and summarization via graph cuts. In Proceedings of
EMNLP.
Dragomir R. Radev, Eduard Hovy, and Kathleen McKe-
own. 2002. Introduction to the special issue on sum-
marization. In Computational Linguistics.
Kapil Thadani and Kathleen McKeown. 2013. Sentence
compression with joint structural inference. In Pro-
ceedings of CoNLL.
Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In Proceedings of ACL.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. To-
wards an iterative reinforcement approach for simulta-
neous document summarization and keyword extrac-
tion. In Proceedings of ACL.
Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-
rian, and Claire Cardie. 2013. A sentence com-
pression based framework to query-focused multi-
document summarization. In Proceedings of ACL.
Kristian Woodsend and Mirella Lapata. 2012. Multiple
aspect summarization using integer linear program-
ming. In Proceedings of EMNLP-CoNLL.
Katsumasa Yoshikawa, Tsutomu Hirao, Ryu Iida, and
Manabu Okumura. 2012. Sentence compression with
semantic role constraints. In Proceedings of ACL.
David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization
tasks. In Information Processing and Management.
</reference>
<page confidence="0.9956">
500
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.707826">
<title confidence="0.99914">Document Summarization via Guided Sentence Compression</title>
<author confidence="0.998347">Fei Fuliang Yang</author>
<affiliation confidence="0.985939">Science Department, The University of Texas at</affiliation>
<address confidence="0.903604333333333">Richardson, Texas 75080, 2Research and Technology Center, Robert Bosch Palo Alto, California 94304,</address>
<abstract confidence="0.99839752">Joint compression and summarization has been used recently to generate high quality summaries. However, such word-based joint optimization is computationally expensive. In this paper we adopt the ‘sentence compression + sentence selection’ pipeline approach for compressive summarization, but propose to guided rather than generic sentence-based compression. To create an annotated corpus, the human annotators were asked to compress sentences while explicitly given the important summary words in the sentences. Using this corpus, we train a supervised sentence compression model using a set of word-, syntax-, and documentlevel features. During summarization, we use multiple compressed sentences in the integer linear programming framework to select salient summary sentences. Our results on the TAC 2008 and 2011 summarization data sets show that by incorporating the guided sentence compression model, our summarization system can yield significant performance gain as compared to the state-of-the-art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ahmet Aker</author>
<author>Trevor Cohn</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Multi-document summarization using a* search and discriminative training.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5874" citStr="Aker et al., 2010" startWordPosition="853" endWordPosition="856">earch has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation – to extract a set of salient and non-redundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. The supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain conditional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the authors model the sentence selection problem as maximizing a submodular function under a budget constraint. A gr</context>
</contexts>
<marker>Aker, Cohn, Gaizauskas, 2010</marker>
<rawString>Ahmet Aker, Trevor Cohn, and Robert Gaizauskas. 2010. Multi-document summarization using a* search and discriminative training. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miguel B Almeida</author>
<author>Andre F T Martins</author>
</authors>
<title>Fast and robust compressive summarization with dual decomposition and multi-task learning.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2846" citStr="Almeida and Martins, 2013" startWordPosition="394" endWordPosition="397">constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before or after summary sentence extraction. We propose to use sum490 Proceedings of the</context>
</contexts>
<marker>Almeida, Martins, 2013</marker>
<rawString>Miguel B. Almeida and Andre F. T. Martins. 2013. Fast and robust compressive summarization with dual decomposition and multi-task learning. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Gillick</author>
<author>Dan Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2796" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="386" endWordPosition="389">ummaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before or after summary sentence extra</context>
<context position="7176" citStr="Berg-Kirkpatrick et al. (2011)" startWordPosition="1046" endWordPosition="1049">is NP-hard problem. Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline approach that integrates a generic sentence compression model with a summary sentence pre-selection or post-selection step. Many studies explore the joint sentence compression and selection setting. Martins and Smith (2009) jointly perform sentence extraction and compression by solving an ILP problem; Berg-Kirkpatrick et al. (2011) propose an approach to score the candidate summaries according to a combined linear model of extractive sentence selection and compression. They train the model using a margin-based objective whose loss captures the final summary quality. Woodsend and Lapata (2012) present a method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup; Yoshikawa et al. (2012) incorporate semantic role information in the ILP model; Chali and Hasan (2012) investigate three strategies in compressive summarization: compressi</context>
<context position="22810" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="3544" endWordPosition="3547">trols the summary length, as measured by the total number of words in the summary. We use an open source ILP solver6. 4.3 Sentence Pre-selection The above ILP method can offer an exact solution to the defined objective function. However, ILP is computationally expensive when the formulation involves large quantities of variables, i.e, when we have many sentences and a large number of candidate compressions for each sentence. We therefore propose to apply a sentence pre-selection step before the compression. This kind of selection step has been used in previous ILP-based summarization systems (Berg-Kirkpatrick et al., 2011; Gillick et al., 2009). In this work, we propose to use a simple supervised support vector regression (SVR) model (Ng et al., 2012) to predict a salience score for each sentence and select the top ranked sentences for further processing (compression and summarization). To train the SVR model, the target value for each sentence is the ROUGE-2 score between the sentence and the four human abstracts (this same value is used for sentence selection in corpus annotation (Section 3)). We employ three commonly used features: (1) sentence position in the document; (2) sentence length as indicated by a</context>
<context position="25189" citStr="Berg-Kirkpatrick et al. (2011)" startWordPosition="3959" endWordPosition="3962">.1 Experimental Setup For our experiments, we use the standard TAC data sets8, which have been used in the NIST competitions and in other summarization studies. In particular, we used the TAC 2010 data set for creating the guided compression corpus and training the SVR pre-selection model, the TAC 2009 data set as development set for parameter tuning, and the TAC 2008 and 2011 data sets as the test set for reporting the final summarization results. We compare our pipeline summarization system against three recent studies, which have reported some of the highest published results on this task. Berg-Kirkpatrick et al. (2011) introduce a joint model for sentence extraction and compression. The model is trained using a margin-based objective whose loss captures the end summary quality; Woodsend and Lapata (2012) learn individual summary aspects from data, e.g., informativeness, succinctness, grammaticality, stylistic writing conventions, and jointly optimize the outcome in an integer linear programming framework. Ng et al. (2012) exploit category-specific information for multi-document summarization. In addition to the three previous studies, we also report the best achieved results in the TAC competitions. 5.2 Sum</context>
<context position="26531" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="4162" endWordPosition="4165">tion studies. We use the ROUGE evaluation metrics (Lin, 2004), with R-2 measuring the bigram overlap between the system and reference summaries and RSU4 measuring the skip-bigram with the maximum gap length of 4. “Our System” uses the pipeline setting including the three components described in Section 4. We use the SVR-based approach to preselect a set of sentences from the document set; these sentences are further fed to the guided compression module that produces n-best compressions for each 8http://www.nist.gov/tac/data/index.html System R-2 R-SU4 CompR TAC’08 Best System 11.03 13.96 n/a (Berg-Kirkpatrick et al., 2011) 11.70 14.38 n/a (Woodsend et al., 2012) 11.37 14.47 n/a Our System 12.35† 15.27† 43.06% Our System w/o Pre-selection 12.02 14.98 55.69% Our System w/ Generic Comp 10.88 13.79 30.90% Table 3: Results on the TAC 2008 data set. “Our System” uses the SVR-based sentence pre-selection + guided compression + ILP-based summary sentence selection. “Our System w/ Generic Comp” uses the pre-selection + generic compression + ILP summary sentence selection setting. “CompR” represents the compression ratio, i.e., percentage of dropped words. † represents our system outperforms the best previous result at t</context>
</contexts>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yllias Chali</author>
<author>Sadid A Hasan</author>
</authors>
<title>On the effectiveness of using sentence compression models for queryfocused multi-document summarization.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="2819" citStr="Chali and Hasan, 2012" startWordPosition="390" endWordPosition="393">insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before or after summary sentence extraction. We propose to us</context>
<context position="7707" citStr="Chali and Hasan (2012)" startWordPosition="1125" endWordPosition="1128">ntence extraction and compression by solving an ILP problem; Berg-Kirkpatrick et al. (2011) propose an approach to score the candidate summaries according to a combined linear model of extractive sentence selection and compression. They train the model using a margin-based objective whose loss captures the final summary quality. Woodsend and Lapata (2012) present a method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup; Yoshikawa et al. (2012) incorporate semantic role information in the ILP model; Chali and Hasan (2012) investigate three strategies in compressive summarization: compression before extraction, after extraction, or joint compression and extraction in one global optimization framework. These joint models offer a promise for high quality summaries, but they often have high computational cost. Qian and Liu (2013) propose a graph-cut based method that improves the speed of joint compression and summarization. 491 The pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression, is also popular. Knight and Marcu (2000) utilize the noisy channel an</context>
</contexts>
<marker>Chali, Hasan, 2012</marker>
<rawString>Yllias Chali and Sadid A. Hasan. 2012. On the effectiveness of using sentence compression models for queryfocused multi-document summarization. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Global inference for sentence compression an integer linear programming approach.</title>
<date>2008</date>
<journal>Journal of Artificial Intelligence Research.</journal>
<contexts>
<context position="9180" citStr="Clarke and Lapata, 2008" startWordPosition="1336" endWordPosition="1339">ncluding a ‘parse-and-trim’ and a noisy-channel approach; Galanis and Androutsopoulos (2010) use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences; Liu and Liu (2013) couple the sentence compression and extraction approaches for summarizing the spoken documents; Wang et al. (2013) design a series of learning-based compression models built on parse trees, and integrate them in query-focused multi-document summarization. Prior studies often rely heavily on the generic sentence compression approaches (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2008; Thadani and McKeown, 2013) for compressing the sentences in the documents, yet a generic compression system may not be the best fit for the summarization purpose. In this paper, we adopt the pipeline-based compressive summarization framework, but propose a novel guided compression method that is catered to the summarization task. We expect this approach to take advantage of the efficient pipeline processing while producing satisfying results as the joint models. We train a supervised guided compression model to produce n-best compressions for each sentence, and use an ILP formulation to sele</context>
<context position="15146" citStr="Clarke and Lapata, 2008" startWordPosition="2294" endWordPosition="2297">ows that the guided compression can lead to more aggressive word deletions since the constituents that are not important to the summary will be deleted even though they contain salient information by themselves. For our compression corpus, which contains 1,150 sentences and their guided compressions, the average compression rate, as measured by the percentage of dropped words, is about 50%. This compression ratio is higher compared to other generic sentence compression corpora, in which the word deletion rate ranges from 24% to 34% depending on different text genres and annotation guidelines (Clarke and Lapata, 2008; Liu and Liu, 2009). This suggests that the annotators can remove words more aggressively when they are provided with a limited set of guiding words. 4 Summarization System Our summarization system consists of three key components: we train a supervised guided compression model using our created compression data, with a variety of features.then we use this model to generate n-best compressions for each sentence; we feed the multiple compressed sentences to the ILP framework to select the best summary sentences. In addition, we propose a sentence pre-selection step that can both speed up the s</context>
<context position="32153" citStr="Clarke and Lapata, 2008" startWordPosition="5090" endWordPosition="5093">y Figure 1. The improved performance of ‘with pre-selection’ over ‘without pre-selection’ is partly because fewer sentences are used and thus we are able to increase the number of compression candidates for these sentences in the ILP sentence extraction module. Quality of sentence compression training data. In order to illustrate the contribution of our summary-guided sentence compression component, we train a generic sentence compression model and use this in our compression and summarization pipeline. The generic compression model was trained using the Edinburgh sentence compression corpus (Clarke and Lapata, 2008), which contains 1370 sentences collected from news articles. This data set has been widely used in other summarization studies (Martins and Smith, 2009). Each sentence has 3 compressions and we choose the shortest compression as the reference. The average compression rate of this corpus is about 28%, lower than that in our summary guided compression data. Note that in generic sentence compression, we only use those word and sentence features described in Section 4.1, not the document-level features since they are not available for the Edinburgh data set. Results of our system using the generi</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>James Clarke and Mirella Lapata. 2008. Global inference for sentence compression an integer linear programming approach. Journal of Artificial Intelligence Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Practical structured learning techniques for natural language processing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Southern California.</institution>
<marker>Daum´e, 2006</marker>
<rawString>Hal Daum´e. 2006. Practical structured learning techniques for natural language processing. Ph.D. thesis, University of Southern California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitrios Galanis</author>
<author>Ion Androutsopoulos</author>
</authors>
<title>An extractive supervised two-stage method for sentence compression.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8649" citStr="Galanis and Androutsopoulos (2010)" startWordPosition="1258" endWordPosition="1261">3) propose a graph-cut based method that improves the speed of joint compression and summarization. 491 The pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression, is also popular. Knight and Marcu (2000) utilize the noisy channel and decision tree method to perform sentence compression; Lin (2003) shows that pure syntactic-based compression may not improve the system performance; Zajic et al. (2007) compare two sentence compression approaches for multi-document summarization, including a ‘parse-and-trim’ and a noisy-channel approach; Galanis and Androutsopoulos (2010) use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences; Liu and Liu (2013) couple the sentence compression and extraction approaches for summarizing the spoken documents; Wang et al. (2013) design a series of learning-based compression models built on parse trees, and integrate them in query-focused multi-document summarization. Prior studies often rely heavily on the generic sentence compression approaches (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2008; Thadani and McKeown, 2013) for compressing the sentences in the doc</context>
</contexts>
<marker>Galanis, Androutsopoulos, 2010</marker>
<rawString>Dimitrios Galanis and Ion Androutsopoulos. 2010. An extractive supervised two-stage method for sentence compression. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
</authors>
<title>A skip-chain conditional random field for ranking meeting utterances by importance.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5828" citStr="Galley, 2006" startWordPosition="848" endWordPosition="849">results. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation – to extract a set of salient and non-redundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. The supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain conditional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the authors model the sentence selection problem as maximizing a submo</context>
</contexts>
<marker>Galley, 2006</marker>
<rawString>Michel Galley. 2006. A skip-chain conditional random field for ranking meeting utterances by importance. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
<author>Dilek Hakkani-Tur</author>
<author>Berndt Bohnet</author>
<author>Yang Liu</author>
<author>Shasha Xie</author>
</authors>
<title>The icsi/utd summarization system at tac</title>
<date>2009</date>
<booktitle>In Proceedings of TAC.</booktitle>
<contexts>
<context position="6170" citStr="Gillick et al. (2009)" startWordPosition="898" endWordPosition="901"> given document set. Both unsupervised and supervised approaches have been explored for sentence selection. The supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain conditional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the authors model the sentence selection problem as maximizing a submodular function under a budget constraint. A greedy algorithm is proposed to efficiently approximate the solution to this NP-hard problem. Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of</context>
<context position="20700" citStr="Gillick et al., 2009" startWordPosition="3174" endWordPosition="3177">selection module is a list of compressed sentences in our work. Many extractive summarization approaches can be applied for this purpose. In this work, we choose the integer linear programming (ILP) method, specifically, the concept-based ILP framework introduced in (Gillick 5http://sourceforge.net/projects/pocket-crf-1/ 494 et al., 2009), mainly because it yields best performance in the TAC evaluation tasks. This ILP approach aims to extract sentences that can cover as many important concepts as possible, while ensuring the summary length is within a given constraint. We follow the study in (Gillick et al., 2009) to use word bi-grams as concepts, and assign a weight to each bi-gram using its document frequency in the given document collection for a test topic. Two differences are between our ILP setup and that in (Gillick et al., 2009). First, since we use multiple compressions for one sentence, we need to introduce an additional constraint: for each sentence, only one of the n-best compressions may be included in the summary. Second, we optimize a joint score of the concept coverage and the sentence salience. The formal ILP formulation is shown below: �max �wici + �vj Sjk (1) i j k �S.t. Sjk G 1Vj (2</context>
<context position="22833" citStr="Gillick et al., 2009" startWordPosition="3548" endWordPosition="3551">asured by the total number of words in the summary. We use an open source ILP solver6. 4.3 Sentence Pre-selection The above ILP method can offer an exact solution to the defined objective function. However, ILP is computationally expensive when the formulation involves large quantities of variables, i.e, when we have many sentences and a large number of candidate compressions for each sentence. We therefore propose to apply a sentence pre-selection step before the compression. This kind of selection step has been used in previous ILP-based summarization systems (Berg-Kirkpatrick et al., 2011; Gillick et al., 2009). In this work, we propose to use a simple supervised support vector regression (SVR) model (Ng et al., 2012) to predict a salience score for each sentence and select the top ranked sentences for further processing (compression and summarization). To train the SVR model, the target value for each sentence is the ROUGE-2 score between the sentence and the four human abstracts (this same value is used for sentence selection in corpus annotation (Section 3)). We employ three commonly used features: (1) sentence position in the document; (2) sentence length as indicated by a binary feature: it tak</context>
</contexts>
<marker>Gillick, Favre, Hakkani-Tur, Bohnet, Liu, Xie, 2009</marker>
<rawString>Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, Berndt Bohnet, Yang Liu, and Shasha Xie. 2009. The icsi/utd summarization system at tac 2009. In Proceedings of TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistics-based summarization - step one: Sentence compression.</title>
<date>2000</date>
<contexts>
<context position="2557" citStr="Knight and Marcu, 2000" startWordPosition="349" endWordPosition="352">has been a surge of interest in recent years on generating compressed document summaries as a viable step towards abstractive summarization. These compressive summaries often contain more information than sentence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method </context>
<context position="8278" citStr="Knight and Marcu (2000)" startWordPosition="1206" endWordPosition="1209">information in the ILP model; Chali and Hasan (2012) investigate three strategies in compressive summarization: compression before extraction, after extraction, or joint compression and extraction in one global optimization framework. These joint models offer a promise for high quality summaries, but they often have high computational cost. Qian and Liu (2013) propose a graph-cut based method that improves the speed of joint compression and summarization. 491 The pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression, is also popular. Knight and Marcu (2000) utilize the noisy channel and decision tree method to perform sentence compression; Lin (2003) shows that pure syntactic-based compression may not improve the system performance; Zajic et al. (2007) compare two sentence compression approaches for multi-document summarization, including a ‘parse-and-trim’ and a noisy-channel approach; Galanis and Androutsopoulos (2010) use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences; Liu and Liu (2013) couple the sentence compression and extraction approaches for summarizing the spoken doc</context>
<context position="16012" citStr="Knight and Marcu, 2000" startWordPosition="2429" endWordPosition="2432">supervised guided compression model using our created compression data, with a variety of features.then we use this model to generate n-best compressions for each sentence; we feed the multiple compressed sentences to the ILP framework to select the best summary sentences. In addition, we propose a sentence pre-selection step that can both speed up the summarization system and improve the performance. 4.1 Guided Sentence Compression Sentence compression has been explored in previous studies using both supervised and unsupervised approaches, including the noisy-channel and decision tree model (Knight and Marcu, 2000; Turner and Charniak, 2005), discriminative learning (McDonald, 2006), integer linear programming (Clarke and Lapata, 2008; Thadani and McKeown, 2013), conditional random fields (CRF) (Nomoto, 2007; Liu and Liu, 2013), etc. In this paper, we employ the CRF-based compression approach due to its proved performance and its flexibility to integrate different levels of discriminative features. Under this framework, sentence compression is formulated as a sequence labeling problem, where each word is 493 labeled as either “0” (retained) or “1” (removed). We develop different levels of features to c</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statistics-based summarization - step one: Sentence compression.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="5736" citStr="Kupiec et al., 1995" startWordPosition="833" endWordPosition="836">ization system can yield significant performance gain as compared to the state-of-the-art reported results. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation – to extract a set of salient and non-redundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. The supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain conditional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (L</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Li</author>
<author>Xian Qian</author>
<author>Yang Liu</author>
</authors>
<title>Using supervised bigram-based ilp for extractive summarization.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6244" citStr="Li et al. (2013)" startWordPosition="909" endWordPosition="912">ored for sentence selection. The supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain conditional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the authors model the sentence selection problem as maximizing a submodular function under a budget constraint. A greedy algorithm is proposed to efficiently approximate the solution to this NP-hard problem. Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline a</context>
</contexts>
<marker>Li, Qian, Liu, 2013</marker>
<rawString>Chen Li, Xian Qian, and Yang Liu. 2013. Using supervised bigram-based ilp for extractive summarization. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>Multi-document summarization via budgeted maximization of submodular functions.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="6356" citStr="Lin and Bilmes, 2010" startWordPosition="928" endWordPosition="931">), maximum entropy (Osborne, 2002), skip-chain conditional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the authors model the sentence selection problem as maximizing a submodular function under a budget constraint. A greedy algorithm is proposed to efficiently approximate the solution to this NP-hard problem. Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline approach that integrates a generic sentence compression model with a summary sentence pre-selection or post-selec</context>
</contexts>
<marker>Lin, Bilmes, 2010</marker>
<rawString>Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Improving summarization performance by sentence compression - A pilot study.</title>
<date>2003</date>
<booktitle>In Proceeding of the Sixth International Workshop on Information Retrieval with Asian Language.</booktitle>
<contexts>
<context position="2568" citStr="Lin, 2003" startWordPosition="353" endWordPosition="354">rest in recent years on generating compressed document summaries as a viable step towards abstractive summarization. These compressive summaries often contain more information than sentence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of </context>
<context position="8373" citStr="Lin (2003)" startWordPosition="1223" endWordPosition="1224">on: compression before extraction, after extraction, or joint compression and extraction in one global optimization framework. These joint models offer a promise for high quality summaries, but they often have high computational cost. Qian and Liu (2013) propose a graph-cut based method that improves the speed of joint compression and summarization. 491 The pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression, is also popular. Knight and Marcu (2000) utilize the noisy channel and decision tree method to perform sentence compression; Lin (2003) shows that pure syntactic-based compression may not improve the system performance; Zajic et al. (2007) compare two sentence compression approaches for multi-document summarization, including a ‘parse-and-trim’ and a noisy-channel approach; Galanis and Androutsopoulos (2010) use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences; Liu and Liu (2013) couple the sentence compression and extraction approaches for summarizing the spoken documents; Wang et al. (2013) design a series of learning-based compression models built on parse </context>
</contexts>
<marker>Lin, 2003</marker>
<rawString>Chin-Yew Lin. 2003. Improving summarization performance by sentence compression - A pilot study. In Proceeding of the Sixth International Workshop on Information Retrieval with Asian Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: a package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="5028" citStr="Lin, 2004" startWordPosition="730" endWordPosition="731">oint modeling studies. Using our created compression data, we train a supervised compression model using a variety of word-, sentence-, and document-level features. During summarization, we generate multiple compression candidates for each sentence, and use the ILP framework to select compressed summary sentences. In addition, we also propose to apply a preselection step to select some important sentences, which can both speed up the summarization system and improve performance. We evaluate our proposed summarization approach on the TAC 2008 and 2011 data sets using the standard ROUGE metric (Lin, 2004). Our results show that by incorporating a guided sentence compression model, our summarization system can yield significant performance gain as compared to the state-of-the-art reported results. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation – to extract a set of salient and non-redundant sentences from the given document set. Both unsupervised and supervised approaches have been expl</context>
<context position="12275" citStr="Lin, 2004" startWordPosition="1836" endWordPosition="1837">words that are important to the final summary. To generate the guided compression corpus, we use the TAC 2010 data set1 that was used for the multi-document summarization task. There are 46 topics. Each has 10 news documents, and also four human-created abstractive reference summaries. Since annotating all the sentences in this data set is time consuming and some sentences are not very important for the summarization task, we choose a set of sentences that are highly related to the human abstracts for annotation. We compare each sentence with the four human abstracts using the ROUGE-2 metric (Lin, 2004), and the sentences 1http://www.nist.gov/tac/2010/ 492 Original Sentence: He said Vietnam veterans are presumed to have been exposed to Agent Orange and veterans with any of the 10 diseases is presumed to have contracted it from the exposure , without individual proof. Guided Compression: Vietnam veterans are presumed to have been exposed to Agent Orange. Original Sentence: The province has limited the number of trees to be chopped down in the forest area in northwest Yunnan and has stopped building sugar factories in the Xishuangbanna region to preserve the only tropical rain forest in the co</context>
<context position="25962" citStr="Lin, 2004" startWordPosition="4077" endWordPosition="4078">Woodsend and Lapata (2012) learn individual summary aspects from data, e.g., informativeness, succinctness, grammaticality, stylistic writing conventions, and jointly optimize the outcome in an integer linear programming framework. Ng et al. (2012) exploit category-specific information for multi-document summarization. In addition to the three previous studies, we also report the best achieved results in the TAC competitions. 5.2 Summarization Results In Table 3 and Table 4, we present the results of our system and the aforementioned summarization studies. We use the ROUGE evaluation metrics (Lin, 2004), with R-2 measuring the bigram overlap between the system and reference summaries and RSU4 measuring the skip-bigram with the maximum gap length of 4. “Our System” uses the pipeline setting including the three components described in Section 4. We use the SVR-based approach to preselect a set of sentences from the document set; these sentences are further fed to the guided compression module that produces n-best compressions for each 8http://www.nist.gov/tac/data/index.html System R-2 R-SU4 CompR TAC’08 Best System 11.03 13.96 n/a (Berg-Kirkpatrick et al., 2011) 11.70 14.38 n/a (Woodsend et a</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: a package for automatic evaluation of summaries. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Yang Liu</author>
</authors>
<title>From extractive to abstractive meeting summaries: Can it be done by sentence compression?</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP.</booktitle>
<contexts>
<context position="15166" citStr="Liu and Liu, 2009" startWordPosition="2298" endWordPosition="2301">ession can lead to more aggressive word deletions since the constituents that are not important to the summary will be deleted even though they contain salient information by themselves. For our compression corpus, which contains 1,150 sentences and their guided compressions, the average compression rate, as measured by the percentage of dropped words, is about 50%. This compression ratio is higher compared to other generic sentence compression corpora, in which the word deletion rate ranges from 24% to 34% depending on different text genres and annotation guidelines (Clarke and Lapata, 2008; Liu and Liu, 2009). This suggests that the annotators can remove words more aggressively when they are provided with a limited set of guiding words. 4 Summarization System Our summarization system consists of three key components: we train a supervised guided compression model using our created compression data, with a variety of features.then we use this model to generate n-best compressions for each sentence; we feed the multiple compressed sentences to the ILP framework to select the best summary sentences. In addition, we propose a sentence pre-selection step that can both speed up the summarization system </context>
</contexts>
<marker>Liu, Liu, 2009</marker>
<rawString>Fei Liu and Yang Liu. 2009. From extractive to abstractive meeting summaries: Can it be done by sentence compression? In Proceedings of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Yang Liu</author>
</authors>
<title>Towards abstractive speech summarization: Exploring unsupervised and supervised approaches for spoken utterance compression.</title>
<date>2013</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing.</journal>
<contexts>
<context position="8789" citStr="Liu and Liu (2013)" startWordPosition="1281" endWordPosition="1284">ve summarization is followed or proceeded by sentence compression, is also popular. Knight and Marcu (2000) utilize the noisy channel and decision tree method to perform sentence compression; Lin (2003) shows that pure syntactic-based compression may not improve the system performance; Zajic et al. (2007) compare two sentence compression approaches for multi-document summarization, including a ‘parse-and-trim’ and a noisy-channel approach; Galanis and Androutsopoulos (2010) use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences; Liu and Liu (2013) couple the sentence compression and extraction approaches for summarizing the spoken documents; Wang et al. (2013) design a series of learning-based compression models built on parse trees, and integrate them in query-focused multi-document summarization. Prior studies often rely heavily on the generic sentence compression approaches (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2008; Thadani and McKeown, 2013) for compressing the sentences in the documents, yet a generic compression system may not be the best fit for the summarization purpose. In this paper, we adopt the pipeline-based c</context>
<context position="16230" citStr="Liu and Liu, 2013" startWordPosition="2460" endWordPosition="2463">ILP framework to select the best summary sentences. In addition, we propose a sentence pre-selection step that can both speed up the summarization system and improve the performance. 4.1 Guided Sentence Compression Sentence compression has been explored in previous studies using both supervised and unsupervised approaches, including the noisy-channel and decision tree model (Knight and Marcu, 2000; Turner and Charniak, 2005), discriminative learning (McDonald, 2006), integer linear programming (Clarke and Lapata, 2008; Thadani and McKeown, 2013), conditional random fields (CRF) (Nomoto, 2007; Liu and Liu, 2013), etc. In this paper, we employ the CRF-based compression approach due to its proved performance and its flexibility to integrate different levels of discriminative features. Under this framework, sentence compression is formulated as a sequence labeling problem, where each word is 493 labeled as either “0” (retained) or “1” (removed). We develop different levels of features to capture word-specific characteristics, sentence related information, and document level importance. Most of the features are extracted based only on the sentence to be compressed. However, we introduce a few document le</context>
<context position="19334" citStr="Liu and Liu, 2013" startWordPosition="2961" endWordPosition="2964"> regression model to estimate a salience score for each sentence (more details in Section 4.3), which represents the importance of the sentence in the document. This score is discretized into four binary features according to the average sentence salience. • Unigram document frequency: this is the current word’s document frequency based on the 10 documents associated with each topic. • Bigram document frequency: document frequency for the two bigrams, the current word and its previous or next word. Some of the above features were employed in related sentence compression studies (Nomoto, 2007; Liu and Liu, 2013). In addition to these features, we explored other related features, including the absolute position of the current word, whether the word appears in the corresponding topic title and descriptions, conjunction of the syntactic tag with the tree depth, etc.; however, these features did not lead to improved performance. We train the CRF model with the Pocket CRF toolkit5 using the guided compression corpus collected in Section 3. During summarization, we apply the model to a given sentence to generate its n-best guided compressions and use them in the following summarization step. 4.2 Summary Se</context>
</contexts>
<marker>Liu, Liu, 2013</marker>
<rawString>Fei Liu and Yang Liu. 2013. Towards abstractive speech summarization: Exploring unsupervised and supervised approaches for spoken utterance compression. IEEE Transactions on Audio, Speech, and Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatically assessing machine summary content with a goldstandard. Computational Linguistics.</title>
<date>2012</date>
<marker>Louis, Nenkova, 2012</marker>
<rawString>Annie Louis and Ani Nenkova. 2012. Automatically assessing machine summary content with a goldstandard. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre F T Martins</author>
<author>Noah A Smith</author>
</authors>
<title>Summarization with a joint model for sentence extraction and compression.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL Workshop on Integer Linear Programming for Natural Language Processing.</booktitle>
<contexts>
<context position="2765" citStr="Martins and Smith, 2009" startWordPosition="382" endWordPosition="385">ntence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before </context>
<context position="4328" citStr="Martins and Smith, 2009" startWordPosition="616" endWordPosition="619">zation in this paper. We create a compression corpus for this purpose. Using human summaries for a set of documents, we identify salient words in the sentences. During annotation, the human annotators are given these salient words and asked to generate compressed sentences. We expect such “guided” sentence compression is beneficial for the pipeline compression and summarization task. In addition, previous research on joint modeling for compression and summarization suggested that the labeled extraction and compression data sets would be helpful for learning a better joint model (Daum´e, 2006; Martins and Smith, 2009). We hope that our work on this guided compression will also be of benefit to the future joint modeling studies. Using our created compression data, we train a supervised compression model using a variety of word-, sentence-, and document-level features. During summarization, we generate multiple compression candidates for each sentence, and use the ILP framework to select compressed summary sentences. In addition, we also propose to apply a preselection step to select some important sentences, which can both speed up the summarization system and improve performance. We evaluate our proposed s</context>
<context position="7066" citStr="Martins and Smith (2009)" startWordPosition="1031" endWordPosition="1034"> under a budget constraint. A greedy algorithm is proposed to efficiently approximate the solution to this NP-hard problem. Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline approach that integrates a generic sentence compression model with a summary sentence pre-selection or post-selection step. Many studies explore the joint sentence compression and selection setting. Martins and Smith (2009) jointly perform sentence extraction and compression by solving an ILP problem; Berg-Kirkpatrick et al. (2011) propose an approach to score the candidate summaries according to a combined linear model of extractive sentence selection and compression. They train the model using a margin-based objective whose loss captures the final summary quality. Woodsend and Lapata (2012) present a method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup; Yoshikawa et al. (2012) incorporate semantic role information</context>
<context position="32306" citStr="Martins and Smith, 2009" startWordPosition="5114" endWordPosition="5117"> to increase the number of compression candidates for these sentences in the ILP sentence extraction module. Quality of sentence compression training data. In order to illustrate the contribution of our summary-guided sentence compression component, we train a generic sentence compression model and use this in our compression and summarization pipeline. The generic compression model was trained using the Edinburgh sentence compression corpus (Clarke and Lapata, 2008), which contains 1370 sentences collected from news articles. This data set has been widely used in other summarization studies (Martins and Smith, 2009). Each sentence has 3 compressions and we choose the shortest compression as the reference. The average compression rate of this corpus is about 28%, lower than that in our summary guided compression data. Note that in generic sentence compression, we only use those word and sentence features described in Section 4.1, not the document-level features since they are not available for the Edinburgh data set. Results of our system using the generic compression model (with sentence pre-selection) are shown in the last row of Table 3 and Table 4. We can see that the system with this generic compress</context>
</contexts>
<marker>Martins, Smith, 2009</marker>
<rawString>Andre F. T. Martins and Noah A. Smith. 2009. Summarization with a joint model for sentence extraction and compression. In Proceedings of the ACL Workshop on Integer Linear Programming for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="9141" citStr="McDonald, 2006" startWordPosition="1332" endWordPosition="1333">ulti-document summarization, including a ‘parse-and-trim’ and a noisy-channel approach; Galanis and Androutsopoulos (2010) use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences; Liu and Liu (2013) couple the sentence compression and extraction approaches for summarizing the spoken documents; Wang et al. (2013) design a series of learning-based compression models built on parse trees, and integrate them in query-focused multi-document summarization. Prior studies often rely heavily on the generic sentence compression approaches (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2008; Thadani and McKeown, 2013) for compressing the sentences in the documents, yet a generic compression system may not be the best fit for the summarization purpose. In this paper, we adopt the pipeline-based compressive summarization framework, but propose a novel guided compression method that is catered to the summarization task. We expect this approach to take advantage of the efficient pipeline processing while producing satisfying results as the joint models. We train a supervised guided compression model to produce n-best compressions for each sente</context>
<context position="16082" citStr="McDonald, 2006" startWordPosition="2439" endWordPosition="2441">a variety of features.then we use this model to generate n-best compressions for each sentence; we feed the multiple compressed sentences to the ILP framework to select the best summary sentences. In addition, we propose a sentence pre-selection step that can both speed up the summarization system and improve the performance. 4.1 Guided Sentence Compression Sentence compression has been explored in previous studies using both supervised and unsupervised approaches, including the noisy-channel and decision tree model (Knight and Marcu, 2000; Turner and Charniak, 2005), discriminative learning (McDonald, 2006), integer linear programming (Clarke and Lapata, 2008; Thadani and McKeown, 2013), conditional random fields (CRF) (Nomoto, 2007; Liu and Liu, 2013), etc. In this paper, we employ the CRF-based compression approach due to its proved performance and its flexibility to integrate different levels of discriminative features. Under this framework, sentence compression is formulated as a sequence labeling problem, where each word is 493 labeled as either “0” (retained) or “1” (removed). We develop different levels of features to capture word-specific characteristics, sentence related information, an</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<title>Automatic summarization. Foundations and Trends in Information Retrieval.</title>
<date>2011</date>
<contexts>
<context position="5342" citStr="Nenkova and McKeown, 2011" startWordPosition="775" endWordPosition="778">ummary sentences. In addition, we also propose to apply a preselection step to select some important sentences, which can both speed up the summarization system and improve performance. We evaluate our proposed summarization approach on the TAC 2008 and 2011 data sets using the standard ROUGE metric (Lin, 2004). Our results show that by incorporating a guided sentence compression model, our summarization system can yield significant performance gain as compared to the state-of-the-art reported results. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation – to extract a set of salient and non-redundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. The supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain conditional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem ca</context>
</contexts>
<marker>Nenkova, McKeown, 2011</marker>
<rawString>Ani Nenkova and Kathleen McKeown. 2011. Automatic summarization. Foundations and Trends in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
<author>Kathleen McKeown</author>
</authors>
<title>The pyramid method: Incorporating human content selection variation in summarization evaluation.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing.</journal>
<marker>Nenkova, Passonneau, McKeown, 2007</marker>
<rawString>Ani Nenkova, Rebecca Passonneau, and Kathleen McKeown. 2007. The pyramid method: Incorporating human content selection variation in summarization evaluation. ACM Transactions on Speech and Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun-Ping Ng</author>
<author>Praveen Bysani</author>
<author>Ziheng Lin</author>
<author>Min-Yen Kan</author>
<author>Chew-Lim Tan</author>
</authors>
<title>Exploiting categoryspecific information for multi-document summarization.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="22942" citStr="Ng et al., 2012" startWordPosition="3568" endWordPosition="3571">The above ILP method can offer an exact solution to the defined objective function. However, ILP is computationally expensive when the formulation involves large quantities of variables, i.e, when we have many sentences and a large number of candidate compressions for each sentence. We therefore propose to apply a sentence pre-selection step before the compression. This kind of selection step has been used in previous ILP-based summarization systems (Berg-Kirkpatrick et al., 2011; Gillick et al., 2009). In this work, we propose to use a simple supervised support vector regression (SVR) model (Ng et al., 2012) to predict a salience score for each sentence and select the top ranked sentences for further processing (compression and summarization). To train the SVR model, the target value for each sentence is the ROUGE-2 score between the sentence and the four human abstracts (this same value is used for sentence selection in corpus annotation (Section 3)). We employ three commonly used features: (1) sentence position in the document; (2) sentence length as indicated by a binary feature: it takes the value of 0 if the number of words in the sentence is greater than 50 or less than 10, otherwise the fe</context>
<context position="25600" citStr="Ng et al. (2012)" startWordPosition="4021" endWordPosition="4024">inal summarization results. We compare our pipeline summarization system against three recent studies, which have reported some of the highest published results on this task. Berg-Kirkpatrick et al. (2011) introduce a joint model for sentence extraction and compression. The model is trained using a margin-based objective whose loss captures the end summary quality; Woodsend and Lapata (2012) learn individual summary aspects from data, e.g., informativeness, succinctness, grammaticality, stylistic writing conventions, and jointly optimize the outcome in an integer linear programming framework. Ng et al. (2012) exploit category-specific information for multi-document summarization. In addition to the three previous studies, we also report the best achieved results in the TAC competitions. 5.2 Summarization Results In Table 3 and Table 4, we present the results of our system and the aforementioned summarization studies. We use the ROUGE evaluation metrics (Lin, 2004), with R-2 measuring the bigram overlap between the system and reference summaries and RSU4 measuring the skip-bigram with the maximum gap length of 4. “Our System” uses the pipeline setting including the three components described in Sec</context>
<context position="27233" citStr="Ng et al., 2012" startWordPosition="4273" endWordPosition="4276">6% Our System w/o Pre-selection 12.02 14.98 55.69% Our System w/ Generic Comp 10.88 13.79 30.90% Table 3: Results on the TAC 2008 data set. “Our System” uses the SVR-based sentence pre-selection + guided compression + ILP-based summary sentence selection. “Our System w/ Generic Comp” uses the pre-selection + generic compression + ILP summary sentence selection setting. “CompR” represents the compression ratio, i.e., percentage of dropped words. † represents our system outperforms the best previous result at the 95% significance level. System R-2 R-SU4 CompR TAC’11 Best System 13.44 16.51 n/a (Ng et al., 2012) 13.93 16.83 n/a Our System 14.40 16.89 39.90% Our System w/o Pre-selection 13.74 16.5 53.81% Our System w/ Generic Comp 13.08 16.23 30.10% Table 4: Results on the TAC 2011 data set. The systems use the same settings as for the TAC 2008 data set. sentence; the ILP-based framework is then used to select the summary sentences from these compressions. We can see from the table that in general, our system achieves considerably better results compared to the state-of-the-art on both the TAC 2008 and 2011 data sets. On the TAC 2008 data set, our system outperforms the best reported result at the 95%</context>
</contexts>
<marker>Ng, Bysani, Lin, Kan, Tan, 2012</marker>
<rawString>Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen Kan, and Chew-Lim Tan. 2012. Exploiting categoryspecific information for multi-document summarization. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadashi Nomoto</author>
</authors>
<title>Discriminative sentence compression with conditional random fields. Information Processing and Management.</title>
<date>2007</date>
<contexts>
<context position="9155" citStr="Nomoto, 2007" startWordPosition="1334" endWordPosition="1335">mmarization, including a ‘parse-and-trim’ and a noisy-channel approach; Galanis and Androutsopoulos (2010) use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences; Liu and Liu (2013) couple the sentence compression and extraction approaches for summarizing the spoken documents; Wang et al. (2013) design a series of learning-based compression models built on parse trees, and integrate them in query-focused multi-document summarization. Prior studies often rely heavily on the generic sentence compression approaches (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2008; Thadani and McKeown, 2013) for compressing the sentences in the documents, yet a generic compression system may not be the best fit for the summarization purpose. In this paper, we adopt the pipeline-based compressive summarization framework, but propose a novel guided compression method that is catered to the summarization task. We expect this approach to take advantage of the efficient pipeline processing while producing satisfying results as the joint models. We train a supervised guided compression model to produce n-best compressions for each sentence, and use a</context>
<context position="16210" citStr="Nomoto, 2007" startWordPosition="2458" endWordPosition="2459">tences to the ILP framework to select the best summary sentences. In addition, we propose a sentence pre-selection step that can both speed up the summarization system and improve the performance. 4.1 Guided Sentence Compression Sentence compression has been explored in previous studies using both supervised and unsupervised approaches, including the noisy-channel and decision tree model (Knight and Marcu, 2000; Turner and Charniak, 2005), discriminative learning (McDonald, 2006), integer linear programming (Clarke and Lapata, 2008; Thadani and McKeown, 2013), conditional random fields (CRF) (Nomoto, 2007; Liu and Liu, 2013), etc. In this paper, we employ the CRF-based compression approach due to its proved performance and its flexibility to integrate different levels of discriminative features. Under this framework, sentence compression is formulated as a sequence labeling problem, where each word is 493 labeled as either “0” (retained) or “1” (removed). We develop different levels of features to capture word-specific characteristics, sentence related information, and document level importance. Most of the features are extracted based only on the sentence to be compressed. However, we introdu</context>
<context position="19314" citStr="Nomoto, 2007" startWordPosition="2959" endWordPosition="2960">e use a simple regression model to estimate a salience score for each sentence (more details in Section 4.3), which represents the importance of the sentence in the document. This score is discretized into four binary features according to the average sentence salience. • Unigram document frequency: this is the current word’s document frequency based on the 10 documents associated with each topic. • Bigram document frequency: document frequency for the two bigrams, the current word and its previous or next word. Some of the above features were employed in related sentence compression studies (Nomoto, 2007; Liu and Liu, 2013). In addition to these features, we explored other related features, including the absolute position of the current word, whether the word appears in the corresponding topic title and descriptions, conjunction of the syntactic tag with the tree depth, etc.; however, these features did not lead to improved performance. We train the CRF model with the Pocket CRF toolkit5 using the guided compression corpus collected in Section 3. During summarization, we apply the model to a given sentence to generate its n-best guided compressions and use them in the following summarization </context>
</contexts>
<marker>Nomoto, 2007</marker>
<rawString>Tadashi Nomoto. 2007. Discriminative sentence compression with conditional random fields. Information Processing and Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Osborne</author>
</authors>
<title>Using maximum entropy for sentence extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Automatic Summarization.</booktitle>
<contexts>
<context position="5769" citStr="Osborne, 2002" startWordPosition="840" endWordPosition="841">rformance gain as compared to the state-of-the-art reported results. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation – to extract a set of salient and non-redundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. The supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain conditional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the authors</context>
</contexts>
<marker>Osborne, 2002</marker>
<rawString>Miles Osborne. 2002. Using maximum entropy for sentence extraction. In Proceedings of the ACL-02 Workshop on Automatic Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of HLTNAACL.</booktitle>
<contexts>
<context position="17801" citStr="Petrov and Klein, 2007" startWordPosition="2715" endWordPosition="2718">rds and the current word. • POS n-grams: same as the word n-grams, but use the part-of-speech tags instead. • Named entity tags: binary features representing whether the current word is a person, location, or temporal expression. We use the Stanford CoreNLP tools3 for named entity tagging. • Stopwords: whether the current word is a stopword or not. • Conjunction features: (1) conjunction of the current word with its relative position in the sentence; (2) conjunction of the NER tag with its relative position. • Syntactic features: We obtain the syntactic parsing tree using the Berkeley Parser (Petrov and Klein, 2007), then obtain the following features: (1) the last sentence constituent tag in the path from the root to the word; (2) depth: length of the path starting from the root node to the word; (3) normalized depth: depth divided by the longest path in the parsing tree; (4) whether the word is under an SBAR node; (5) depth and normalized depth of the SBAR node if the word is under an SBAR node; • Dependency features: We employ the Penn2Malt toolkit 4 to convert the parse result from the Berkeley parser to the dependency parsing tree, and use these dependency 3http://nlp.stanford.edu/software/corenlp.s</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Qian</author>
<author>Yang Liu</author>
</authors>
<title>Fast joint compression and summarization via graph cuts.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2867" citStr="Qian and Liu, 2013" startWordPosition="398" endWordPosition="401"> for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before or after summary sentence extraction. We propose to use sum490 Proceedings of the 2013 Conference on E</context>
<context position="8017" citStr="Qian and Liu (2013)" startWordPosition="1169" endWordPosition="1172"> summary quality. Woodsend and Lapata (2012) present a method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup; Yoshikawa et al. (2012) incorporate semantic role information in the ILP model; Chali and Hasan (2012) investigate three strategies in compressive summarization: compression before extraction, after extraction, or joint compression and extraction in one global optimization framework. These joint models offer a promise for high quality summaries, but they often have high computational cost. Qian and Liu (2013) propose a graph-cut based method that improves the speed of joint compression and summarization. 491 The pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression, is also popular. Knight and Marcu (2000) utilize the noisy channel and decision tree method to perform sentence compression; Lin (2003) shows that pure syntactic-based compression may not improve the system performance; Zajic et al. (2007) compare two sentence compression approaches for multi-document summarization, including a ‘parse-and-trim’ and a noisy-channel approach; Ga</context>
</contexts>
<marker>Qian, Liu, 2013</marker>
<rawString>Xian Qian and Yang Liu. 2013. Fast joint compression and summarization via graph cuts. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Eduard Hovy</author>
<author>Kathleen McKeown</author>
</authors>
<title>Introduction to the special issue on summarization.</title>
<date>2002</date>
<booktitle>In Computational Linguistics.</booktitle>
<contexts>
<context position="1927" citStr="Radev et al., 2002" startWordPosition="258" endWordPosition="261">n system can yield significant performance gain as compared to the state-of-the-art. 1 Introduction Automatic summarization can be broadly divided into two categories: extractive and abstractive summarization. Extractive summarization focuses on selecting the salient sentences from the document collection and concatenating them to form a summary; while abstractive summarization is generally considered more difficult, involving sophisticated techniques for meaning representation, content planning, surface realization, etc., and the “true abstractive summarization remains a researcher’s dream” (Radev et al., 2002). There has been a surge of interest in recent years on generating compressed document summaries as a viable step towards abstractive summarization. These compressive summaries often contain more information than sentence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compr</context>
</contexts>
<marker>Radev, Hovy, McKeown, 2002</marker>
<rawString>Dragomir R. Radev, Eduard Hovy, and Kathleen McKeown. 2002. Introduction to the special issue on summarization. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kapil Thadani</author>
<author>Kathleen McKeown</author>
</authors>
<title>Sentence compression with joint structural inference.</title>
<date>2013</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="9208" citStr="Thadani and McKeown, 2013" startWordPosition="1340" endWordPosition="1343">m’ and a noisy-channel approach; Galanis and Androutsopoulos (2010) use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences; Liu and Liu (2013) couple the sentence compression and extraction approaches for summarizing the spoken documents; Wang et al. (2013) design a series of learning-based compression models built on parse trees, and integrate them in query-focused multi-document summarization. Prior studies often rely heavily on the generic sentence compression approaches (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2008; Thadani and McKeown, 2013) for compressing the sentences in the documents, yet a generic compression system may not be the best fit for the summarization purpose. In this paper, we adopt the pipeline-based compressive summarization framework, but propose a novel guided compression method that is catered to the summarization task. We expect this approach to take advantage of the efficient pipeline processing while producing satisfying results as the joint models. We train a supervised guided compression model to produce n-best compressions for each sentence, and use an ILP formulation to select the best set of summary s</context>
<context position="16163" citStr="Thadani and McKeown, 2013" startWordPosition="2449" endWordPosition="2452">ssions for each sentence; we feed the multiple compressed sentences to the ILP framework to select the best summary sentences. In addition, we propose a sentence pre-selection step that can both speed up the summarization system and improve the performance. 4.1 Guided Sentence Compression Sentence compression has been explored in previous studies using both supervised and unsupervised approaches, including the noisy-channel and decision tree model (Knight and Marcu, 2000; Turner and Charniak, 2005), discriminative learning (McDonald, 2006), integer linear programming (Clarke and Lapata, 2008; Thadani and McKeown, 2013), conditional random fields (CRF) (Nomoto, 2007; Liu and Liu, 2013), etc. In this paper, we employ the CRF-based compression approach due to its proved performance and its flexibility to integrate different levels of discriminative features. Under this framework, sentence compression is formulated as a sequence labeling problem, where each word is 493 labeled as either “0” (retained) or “1” (removed). We develop different levels of features to capture word-specific characteristics, sentence related information, and document level importance. Most of the features are extracted based only on the</context>
</contexts>
<marker>Thadani, McKeown, 2013</marker>
<rawString>Kapil Thadani and Kathleen McKeown. 2013. Sentence compression with joint structural inference. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenine Turner</author>
<author>Eugene Charniak</author>
</authors>
<title>Supervised and unsupervised learning for sentence compression.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="16040" citStr="Turner and Charniak, 2005" startWordPosition="2433" endWordPosition="2436">ssion model using our created compression data, with a variety of features.then we use this model to generate n-best compressions for each sentence; we feed the multiple compressed sentences to the ILP framework to select the best summary sentences. In addition, we propose a sentence pre-selection step that can both speed up the summarization system and improve the performance. 4.1 Guided Sentence Compression Sentence compression has been explored in previous studies using both supervised and unsupervised approaches, including the noisy-channel and decision tree model (Knight and Marcu, 2000; Turner and Charniak, 2005), discriminative learning (McDonald, 2006), integer linear programming (Clarke and Lapata, 2008; Thadani and McKeown, 2013), conditional random fields (CRF) (Nomoto, 2007; Liu and Liu, 2013), etc. In this paper, we employ the CRF-based compression approach due to its proved performance and its flexibility to integrate different levels of discriminative features. Under this framework, sentence compression is formulated as a sequence labeling problem, where each word is 493 labeled as either “0” (retained) or “1” (removed). We develop different levels of features to capture word-specific charact</context>
</contexts>
<marker>Turner, Charniak, 2005</marker>
<rawString>Jenine Turner and Eugene Charniak. 2005. Supervised and unsupervised learning for sentence compression. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
<author>Jianguo Xiao</author>
</authors>
<title>Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="35756" citStr="Wan et al., 2007" startWordPosition="5686" endWordPosition="5689">tators were explicitly informed about the important summary words during the compression annotation. We then train a supervised compression model to capture the guided compression process using a set of word-, sentence-, and document-level features. We conduct experiments on the TAC 2008 and 2011 summarization data sets and show that by incorporating the guided sentence compression model, our summarization system can yield significant performance gain as compared to the state-of-the-art. In future, we would like to further explore the reinforcement relationship between keywords and summaries (Wan et al., 2007), improve the readability of the sentences generated from the guided compression system, and report results using multiple evaluation metrics (Nenkova et al., 2007; Louis and Nenkova, 2012) as well as performing human evaluations. 498 Acknowledgments Part of this work was done during the first author’s internship in Bosch Research and Technology Center. The work is also partially supported by NSF award IIS-0845484 and DARPA Contract No. FA8750-13-2-0041. Any opinions, findings, and conclusions or recommendations expressed are those of the author and do not necessarily reflect the views of the </context>
</contexts>
<marker>Wan, Yang, Xiao, 2007</marker>
<rawString>Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu Wang</author>
<author>Hema Raghavan</author>
<author>Vittorio Castelli</author>
<author>Radu Florian</author>
<author>Claire Cardie</author>
</authors>
<title>A sentence compression based framework to query-focused multidocument summarization.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2608" citStr="Wang et al., 2013" startWordPosition="359" endWordPosition="362">ing compressed document summaries as a viable step towards abstractive summarization. These compressive summaries often contain more information than sentence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work</context>
<context position="8904" citStr="Wang et al. (2013)" startWordPosition="1297" endWordPosition="1300"> the noisy channel and decision tree method to perform sentence compression; Lin (2003) shows that pure syntactic-based compression may not improve the system performance; Zajic et al. (2007) compare two sentence compression approaches for multi-document summarization, including a ‘parse-and-trim’ and a noisy-channel approach; Galanis and Androutsopoulos (2010) use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences; Liu and Liu (2013) couple the sentence compression and extraction approaches for summarizing the spoken documents; Wang et al. (2013) design a series of learning-based compression models built on parse trees, and integrate them in query-focused multi-document summarization. Prior studies often rely heavily on the generic sentence compression approaches (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2008; Thadani and McKeown, 2013) for compressing the sentences in the documents, yet a generic compression system may not be the best fit for the summarization purpose. In this paper, we adopt the pipeline-based compressive summarization framework, but propose a novel guided compression method that is catered to the summarizat</context>
</contexts>
<marker>Wang, Raghavan, Castelli, Florian, Cardie, 2013</marker>
<rawString>Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Florian, and Claire Cardie. 2013. A sentence compression based framework to query-focused multidocument summarization. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Multiple aspect summarization using integer linear programming.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="7442" citStr="Woodsend and Lapata (2012)" startWordPosition="1087" endWordPosition="1090">ses, or through a pipeline approach that integrates a generic sentence compression model with a summary sentence pre-selection or post-selection step. Many studies explore the joint sentence compression and selection setting. Martins and Smith (2009) jointly perform sentence extraction and compression by solving an ILP problem; Berg-Kirkpatrick et al. (2011) propose an approach to score the candidate summaries according to a combined linear model of extractive sentence selection and compression. They train the model using a margin-based objective whose loss captures the final summary quality. Woodsend and Lapata (2012) present a method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup; Yoshikawa et al. (2012) incorporate semantic role information in the ILP model; Chali and Hasan (2012) investigate three strategies in compressive summarization: compression before extraction, after extraction, or joint compression and extraction in one global optimization framework. These joint models offer a promise for high quality summaries, but they often have high computational cost. Qian and Liu (2013) propose a graph-cut base</context>
<context position="25378" citStr="Woodsend and Lapata (2012)" startWordPosition="3990" endWordPosition="3993">010 data set for creating the guided compression corpus and training the SVR pre-selection model, the TAC 2009 data set as development set for parameter tuning, and the TAC 2008 and 2011 data sets as the test set for reporting the final summarization results. We compare our pipeline summarization system against three recent studies, which have reported some of the highest published results on this task. Berg-Kirkpatrick et al. (2011) introduce a joint model for sentence extraction and compression. The model is trained using a margin-based objective whose loss captures the end summary quality; Woodsend and Lapata (2012) learn individual summary aspects from data, e.g., informativeness, succinctness, grammaticality, stylistic writing conventions, and jointly optimize the outcome in an integer linear programming framework. Ng et al. (2012) exploit category-specific information for multi-document summarization. In addition to the three previous studies, we also report the best achieved results in the TAC competitions. 5.2 Summarization Results In Table 3 and Table 4, we present the results of our system and the aforementioned summarization studies. We use the ROUGE evaluation metrics (Lin, 2004), with R-2 measu</context>
</contexts>
<marker>Woodsend, Lapata, 2012</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2012. Multiple aspect summarization using integer linear programming. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsumasa Yoshikawa</author>
<author>Tsutomu Hirao</author>
<author>Ryu Iida</author>
<author>Manabu Okumura</author>
</authors>
<title>Sentence compression with semantic role constraints.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="7628" citStr="Yoshikawa et al. (2012)" startWordPosition="1113" endWordPosition="1116">e compression and selection setting. Martins and Smith (2009) jointly perform sentence extraction and compression by solving an ILP problem; Berg-Kirkpatrick et al. (2011) propose an approach to score the candidate summaries according to a combined linear model of extractive sentence selection and compression. They train the model using a margin-based objective whose loss captures the final summary quality. Woodsend and Lapata (2012) present a method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup; Yoshikawa et al. (2012) incorporate semantic role information in the ILP model; Chali and Hasan (2012) investigate three strategies in compressive summarization: compression before extraction, after extraction, or joint compression and extraction in one global optimization framework. These joint models offer a promise for high quality summaries, but they often have high computational cost. Qian and Liu (2013) propose a graph-cut based method that improves the speed of joint compression and summarization. 491 The pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence com</context>
</contexts>
<marker>Yoshikawa, Hirao, Iida, Okumura, 2012</marker>
<rawString>Katsumasa Yoshikawa, Tsutomu Hirao, Ryu Iida, and Manabu Okumura. 2012. Sentence compression with semantic role constraints. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Zajic</author>
<author>Bonnie J Dorr</author>
<author>Jimmy Lin</author>
<author>Richard Schwartz</author>
</authors>
<title>Multi-candidate reduction: Sentence compression as a tool for document summarization tasks.</title>
<date>2007</date>
<booktitle>In Information Processing and Management.</booktitle>
<contexts>
<context position="2588" citStr="Zajic et al., 2007" startWordPosition="355" endWordPosition="358">ent years on generating compressed document summaries as a viable step towards abstractive summarization. These compressive summaries often contain more information than sentence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational ef</context>
<context position="8477" citStr="Zajic et al. (2007)" startWordPosition="1237" endWordPosition="1240"> global optimization framework. These joint models offer a promise for high quality summaries, but they often have high computational cost. Qian and Liu (2013) propose a graph-cut based method that improves the speed of joint compression and summarization. 491 The pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression, is also popular. Knight and Marcu (2000) utilize the noisy channel and decision tree method to perform sentence compression; Lin (2003) shows that pure syntactic-based compression may not improve the system performance; Zajic et al. (2007) compare two sentence compression approaches for multi-document summarization, including a ‘parse-and-trim’ and a noisy-channel approach; Galanis and Androutsopoulos (2010) use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences; Liu and Liu (2013) couple the sentence compression and extraction approaches for summarizing the spoken documents; Wang et al. (2013) design a series of learning-based compression models built on parse trees, and integrate them in query-focused multi-document summarization. Prior studies often rely heavil</context>
</contexts>
<marker>Zajic, Dorr, Lin, Schwartz, 2007</marker>
<rawString>David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard Schwartz. 2007. Multi-candidate reduction: Sentence compression as a tool for document summarization tasks. In Information Processing and Management.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
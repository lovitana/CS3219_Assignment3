<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.969892">
Max-Margin Synchronous Grammar Induction for Machine Translation
</title>
<author confidence="0.94972">
Xinyan Xiao and Deyi Xiong*
</author>
<affiliation confidence="0.831125">
School of Computer Science and Technology
Soochow University
Suzhou 215006, China
</affiliation>
<email confidence="0.996097">
xyxiao.cn@gmail.com, dyxiong@suda.edu.cn
</email>
<sectionHeader confidence="0.997373" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99714375">
Traditional synchronous grammar induction
estimates parameters by maximizing likeli-
hood, which only has a loose relation to trans-
lation quality. Alternatively, we propose a
max-margin estimation approach to discrim-
inatively inducing synchronous grammars for
machine translation, which directly optimizes
translation quality measured by BLEU. In
the max-margin estimation of parameters, we
only need to calculate Viterbi translations.
This further facilitates the incorporation of
various non-local features that are defined on
the target side. We test the effectiveness of our
max-margin estimation framework on a com-
petitive hierarchical phrase-based system. Ex-
periments show that our max-margin method
significantly outperforms the traditional two-
step pipeline for synchronous rule extraction
by 1.3 BLEU points and is also better than pre-
vious max-likelihood estimation method.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987907782608696">
Synchronous grammar induction, which refers to
the process of learning translation rules from bilin-
gual corpus, still remains an open problem in sta-
tistical machine translation (SMT). Although state-
of-the-art SMT systems model the translation pro-
cess based on synchronous grammars (including
bilingual phrases), most of them still learn trans-
lation rules via a pipeline with word-based heuris-
tics (Koehn et al., 2003). This pipeline first builds
word alignments using heuristic combination strate-
gies, then heuristically extracts rules that are consis-
tent with word alignments. Such heuristic pipeline
*Corresponding author
is not elegant theoretically. It brings an undesirable
gap that separates modeling and learning in an SMT
system.
Therefore, researchers have proposed alternative
approaches to learning synchronous grammars di-
rectly from sentence pairs without word alignments,
via generative models (Marcu and Wong, 2002;
Cherry and Lin, 2007; Zhang et al., 2008; DeNero
et al., 2008; Blunsom et al., 2009; Cohn and Blun-
som, 2009; Neubig et al., 2011; Levenberg et al.,
2012) or discriminative models (Xiao et al., 2012).
Theoretically, these approaches describe how sen-
tence pairs are generated by applying sequences of
synchronous rules in an elegant way. However, they
learn synchronous grammars by maximizing likeli-
hood,1 which only has a loose relation to transla-
tion quality (He and Deng, 2012). Moreover, gen-
erative models are normally hard to be extended to
incorporate useful features, and the discriminative
synchronous grammar induction model proposed by
Xiao et al. (2012) only incorporates local features
defined on parse trees of the source language. Non-
local features, which encode information from parse
trees of the target language, have never been ex-
ploited before due to the computational complexity
of normalization in max-likelihood estimation.
Consequently, we would like to learn syn-
chronous grammars in a discriminative way that can
directly maximize the end-to-end translation quality
measured by BLEU (Papineni et al., 2002), and is
also able to incorporate non-local features from tar-
get parse trees.
We thus propose a max-margin estimation method
</bodyText>
<footnote confidence="0.680124">
&apos;More precisely, the discriminative model by Xiao et al.
(2012) maximizes conditional likelihood.
</footnote>
<page confidence="0.946584">
255
</page>
<note confidence="0.735718">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 255–264,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999916131578947">
to discriminatively induce synchronous grammar di-
rectly from sentence pairs without word alignments.
We try to maximize the margin between a reference
translation and a candidate translation with transla-
tion errors that are measured by BLEU. The more
serious the translation errors, the larger the margin.
In this way, our max-margin method is able to learn
synchronous grammars according to their translation
performance. We further incorporate various non-
local features defined on target parse trees. We ef-
ficiently calculate the non-local feature values of a
translation over its exponential derivation space us-
ing the inside-outside algorithm. Because our max-
margin estimation optimizes feature weights only by
the feature values of Viterbi and reference transla-
tions, we are able to efficiently perform optimization
even with non-local features.
We apply the proposed max-margin estimation
method to learn synchronous grammars for a hi-
erarchical phrase-based translation system (Chiang,
2007) which typically produces state-of-the-art per-
formance. With non-local features defined on tar-
get parse trees, our max-margin method significantly
outperforms the baseline that uses synchronous
rules learned from the traditional pipeline by 1.3
BLEU points on large-scale Chinese-English bilin-
gual training data.
The remainder of this paper is organized as fol-
lows. Section 2 presents the discriminative syn-
chronous grammar induction model with the non-
local features. In Section 3, we elaborate our max-
margin estimation method which is able to directly
optimize BLEU, and discuss how we induce gram-
mar rules. Local and non-local features are de-
scribed in Section 4. Finally, in Section 5, we verify
the effectiveness of our method through experiments
by comparing it against both the traditional pipeline
and max-likelihood estimation method.
</bodyText>
<sectionHeader confidence="0.941838" genericHeader="introduction">
2 Discriminative Model with Non-local
Features
</sectionHeader>
<bodyText confidence="0.999324">
Let S denotes the set of all strings in a source lan-
guage. Given a source sentence s E S, T (s) denotes
all candidate translations in the target language that
can be generated by a synchronous grammar G. A
translation t E T (s) is generated by a sequence of
translation steps (r1, ..., r&apos;), where we apply a syn-
</bodyText>
<equation confidence="0.958917">
r1: ( yu shalong =:&gt;. with Sharon)
r2: ( X juxing huitan =:&gt;. held a talk X )
r3: ( bushi X =:&gt;. Bush X )
</equation>
<figureCaption confidence="0.895360666666667">
Figure 1: A derivation of a sentence pair represented by
a synchronous tree. The above and below part are the
parses in the source language side and the target language
side respectively. Left subscript of a node X denotes the
source span, while right subscript denotes the target span.
A dashed line denotes an alignment from a source span
to a target span. The annotation for a dashed line cor-
responds to the rewriting rule used in the corresponding
step of the derivation.
chronous rule r E G in one step. We refer to such
a sequence of translation steps as a derivation (See
Figure 1) and denote it as d E D(s), where D(s)
</figureCaption>
<bodyText confidence="0.998965833333333">
represents the derivation space of a source sentence.
Given an input source sentence s, we output a pair
(t, d) in SMT. Thus, we study the triple (s, t, d) in
SMT.
In our discriminative model, we calculate the
value of a triple (s, t, d) according to the following
</bodyText>
<equation confidence="0.741233">
scoring function:
f(s, t, d) = θT b(s, t, d) (1)
</equation>
<bodyText confidence="0.992788714285714">
where θ E O is a feature weight vector, and b is the
feature function.
There are exponential outputs in SMT. Therefore
it is necessary to factorize the feature function in or-
der to perform efficient calculation over the SMT
output space using dynamic programming. We de-
compose the feature function of a triple (s, t, d) into
</bodyText>
<figure confidence="0.9993956">
[0,5]
[1,5]
[1,3]
ᐳӰ о ⋉嗉 Ѯ㹼 Պ䈸
0 1 2 3 4 5
bushi yu shalong juxing huitan
0 1 2 3 4 5 6
[4,6]
[1,6]
[0,6]
</figure>
<page confidence="0.937615">
256
</page>
<figureCaption confidence="0.7506445">
Figure 2: Example features for the derivation in Figure 1.
Shaded nodes denote information encoded in the feature.
</figureCaption>
<bodyText confidence="0.656688">
a sum of values of each synchronous rule in the
derivation d.
</bodyText>
<equation confidence="0.920170333333333">
ϕ(r, s, t) (2)
 |{z }
non-local
</equation>
<bodyText confidence="0.999663535714285">
Our feature functions include both local and non-
local features. A feature is a local feature if and
only if it can be factored among the translation steps
in a derivation. In other words, the value of a lo-
cal feature for ⟨s, t, d⟩ can be calculated as a sum of
local scores in each translation step, and the calcula-
tion of each local score only requires to look at the
rule used in corresponding step and the input sen-
tence. Otherwise, the feature is a non-local feature.
Our discriminative model allows to incorporate non-
local features that are defined on target translations.
For example, a rule feature in Figure 2(a), which
indicates the application of a specific rule in a
derivation, is a local feature. A source span bound-
ary feature in Figure 2(b) that is defined on the
source parse tree is also a local feature. However,
a target span boundary feature in Figure 2(c), which
assesses the target parse structure, is a non-local fea-
ture. According to Figure 1, the span is parsed in
step r2, but it also depends on the translation bound-
ary word “held” generated in previous step r1. We
will describe the details of both local and non-local
features that we use in Section 4.
Non-local features enable us to model the target
parse structure in a derivation. However, it is com-
putationally expensive to calculate the expected val-
ues of non-local features over D(s), as non-local
features require to record states of target boundary
</bodyText>
<listItem confidence="0.8123618">
s, S, S s is a sentence in a source language;
S means source training sentences;
S denotes all the possible sentences;
t, T, T symbols for the target language that
similar to s, S, S;
</listItem>
<tableCaption confidence="0.977162666666667">
d, D derivation and derivation space;
D(s) space of derivations for
a source sentence;
D(s, t) space of derivations for
a source sentence with its translation;
H(s) hypergraph that represents D(s);
H(s, t) hypergraph that represents D(s, t);
Table 1: Notations in this paper. We give an abstract of
related notations for clarity.
</tableCaption>
<bodyText confidence="0.999760647058824">
words and result in an extremely large number of
states during dynamic programming. Fortunately,
when integrating out derivations over the derivation
space D(s, t) of a source sentence and its transla-
tion, we can efficiently calculate the non-local fea-
tures. Because all derivations in D(s, t) share the
same translation, there is no need to maintain states
for target boundary words. We will discuss this com-
putational problem in details in Section 3.3. In the
proposed max-margin estimation described in next
section, we only need to integrate out derivation
for a Viterbi translation and a reference translation
when updating feature weights. Therefore, the de-
fined non-local features allow us to not only explore
useful knowledge on the target parse trees, but also
compute them efficiently over D(s, t) during max-
margin estimation.
</bodyText>
<sectionHeader confidence="0.997295" genericHeader="method">
3 Max-Margin Estimation
</sectionHeader>
<bodyText confidence="0.999851846153846">
In this section, we describe how we use a parallel
training corpus {S, T} _ {(s(i), t(i))}Ni_1 to esti-
mate feature weights θ, which contain parameters of
the induced synchronous grammars and the defined
non-local features.
We choose the parameters that maximize the
translation quality measured by BLEU using the
max-margin estimation (Taskar et al., 2004). Mar-
gin refers to the difference of the model score be-
tween a reference translation t(i) and a candidate
translation t. We hope that the worse the transla-
tion quality of t, the larger the margin between t
and t(i). In this way, we penalize larger translation
</bodyText>
<figure confidence="0.988515">
Ѯ㹼
Պ䈸
о
[1,5]
[1,6]
Պ䈸
∑ ϕ(r, s) ∑
-b(s, t, d) _  |{z } +
rEd local rEd
</figure>
<page confidence="0.979219">
257
</page>
<bodyText confidence="0.9985145">
errors more severely than smaller ones. This intu-
ition is expressed by the following equation.
</bodyText>
<equation confidence="0.977057">
min 2110112
1 (3)
s.t. f(s(i), t(i)) − f(s(i), t) &gt; cost(t(i), t)
Vt E T (s(i))
</equation>
<bodyText confidence="0.999321571428571">
Here, f(s, t) is the feature function of a translation,
and cost function cost(t(i), t) measures the trans-
lation errors of a candidate translation t comparing
with a reference translation t(i). We define the cost
function via the widely-used translation evaluation
metric BLEU. We use the smoothed sentence level
BLEU-4 (Lin and Och, 2004) here:
</bodyText>
<equation confidence="0.97599">
cost(t(i), t) = 1 − BLEU-4(t(i), t) (4)
</equation>
<bodyText confidence="0.999950714285714">
In Section 3.1, we will discuss how we use the
scoring function f(s, t, d) to calculate f(s, t). Then
in Section 3.2, we recast the equation (3) as an un-
constrained empirical loss minimization problem,
and describe the learning algorithm for optimizing
0 and inducing G. Finally, we give the details of
inference for the learning algorithm in Section 3.3.
</bodyText>
<subsectionHeader confidence="0.995698">
3.1 Integrate Out Derivation by Averaging
</subsectionHeader>
<bodyText confidence="0.982537785714286">
Although we only model the triple (s, t, d) in the
equation (1), it’s necessary to calculate the scoring
function f(s, t) of a translation by integrating out
the variable of derivation as derivation is not ob-
served in the training data.
We use an averaging computation over all possi-
ble derivations of a translation D(s, t). We call this
an average derivation based estimation:
The “average derivation” can be considered as the
geometric central point in the space D(s, t).
Another possible way to deal with the latent
derivation is max-derivation, which uses the max-
operator over D(s, t). The max derivation method
sets f(s, t) as maxdED(s,t) f(s, t, d). It is often
adopted in traditional SMT systems. Nevertheless,
we instead use average-derivation for two reasons.2
2Imagine that H(s, t) in the Algorithm 1 is replaced by a
maximum derivation in H(s, t).
First, as a translation has an exponential number of
derivations, finding the max derivation of a refer-
ence translation for learning is nontrivial (Chiang et
al., 2009). Second, the max derivation estimation
will result in a low rule coverage, as rules in a max
derivation only covers a small fraction of rules in
the D(s, t). Because rule coverage is important in
synchronous grammar induction, we would like to
explore the entire derivation space using the average
operator.
</bodyText>
<subsectionHeader confidence="0.999883">
3.2 Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.999909">
We reformulate the equation (3) as an unconstrained
empirical loss minimization problem as follows:
</bodyText>
<equation confidence="0.97186975">
∑N L(s(i), t(i), 0) (6)
A
min 2 110112 + 1
N n=1
</equation>
<bodyText confidence="0.993338">
Where A denotes the regularization strength for
L2-norm. The loss function of a sentence pair
L(s(i), t(i), 0) is a convex hinge loss function de-
noted by:
</bodyText>
<equation confidence="0.999114">
max{0, −f(s(i), t(i)) (7)
( )
+ max f(s(i),t) + cost(t(i),t) 1
tET (s��))
</equation>
<bodyText confidence="0.9999374">
According to the second max-operator in the
hinge loss function, the optimization towards BLEU
is expressed by cost-augmented inference. Cost-
augmented inference finds a translation that has a
maximum model score augmented with cost.
</bodyText>
<equation confidence="0.979769">
( )
t� = max f(s(i), t) + cost(t(i), t) (8)
tET (s��))
</equation>
<bodyText confidence="0.9999831">
We applied the Pegasos algorithm for the op-
timization of equation (6) (Shalev-Shwartz et al.,
2007). This is an online algorithm, which alternates
between stochastic gradient descent steps and pro-
jection steps. When the loss function is non-zero, it
updates weights according to the sub-gradient of the
hinge loss function. Using the average scoring func-
tion in the equation (5), the sub-gradient of hinge
loss function for a sentence pair is the difference of
average feature values between a Viterbi translation
</bodyText>
<equation confidence="0.737846739130435">
1
f(s, t) =
∑ f(s, t, d) (5)
|D(s, t)|
dED(s,t)
258
Algorithm 1 UPDATE(s, t, θ, G) ◃ One step in online algorithm. s, t are short for s(i), t(i) here
1: R(s, t) +— BIPARSE(s, t, B) &gt; Build hypergraph of reference translation
2: G +— G + R(s, t) &gt; Discover rules from R(s, t)
3:t, d� +— arg max⟨t′,d′⟩∈D(s) f(s, t′, d′) + cost(t, t′) &gt; Find Viterbi translation
4: R(s, t) +— BIPARSE(s, t, B) &gt; Build hypergraph of Viterbi translation
5: if f(s, t) &lt; f(s, t) + cost(t, t) then
6: B +— (1 − 77λ)B + 77 x ae (R(s, t), R(s, t)) &gt; Update B by gradient ae and learning rate 77
√
7: B +— min 11, 1�11011 } � B
X &gt; Projection by scaling
8: return G, B
∂L
slation.
1
∑b(s(i), t(i), d)
|D(s ,
(i)t(i))|
</equation>
<bodyText confidence="0.908487">
nally, we update the feature weights using the sum
of these gradients.
nce
</bodyText>
<equation confidence="0.995079333333333">
dED(s(z),t(z))
1 ∑ 4&apos;(s(i), t, d) (9)
|D(s(i),�t) |dED(s(&apos;&apos;),ˆt)
</equation>
<bodyText confidence="0.98420453125">
Algorithm 1 shows the procedure of one step in
the online optimization algorithm. The procedure
discovers rules and updates weights in an online
fashion. In the procedure, we first biparse the sen-
tence pair to construct a synchronous hypergraph of
a reference translation (line 1). In the biparsing al-
gorithm, synchronous rules for constructing hyper-
edges are not required to be in G, but can be any
rules that follow the form defined in Chiang (2007).
Thus, the biparsing algorithm can discover new rules
that are not in G. Then we collect the translation
rules discovered in the hypergraph of the reference
translation (line 2), which are rules indicated by hy-
peredges in the hypergraph. We then calculate the
Viterbi translation according to the scoring function
and cost function (see Section 3.3) (line 3), and build
the synchronous hypergraph for the Viterbi transla-
tion (line 4). Finally, we update weights according to
the Pegasos algorithm (line 5). The sub-gradient is
calculated based on the hypergraph of Viterbi trans-
lation and reference translation.
In practice, in order to process the data in a paral-
lel manner, we use a larger step size of 1000 for the
learning algorithm. In each step of our online opti-
mization algorithm, we first biparse 1000 reference
sentence pairs in parallel. Then, we collect grammar
rules from the generated reference hypergraphs. Af-
ter that, we compute the gradients of 1000 sentence
pairs in parallel, by calculating feature weights over
reference hypergraphs and Viterbi hypergraphs. Fi-
and a reference tran
∂θ �
</bodyText>
<subsectionHeader confidence="0.810085">
3.3 Infere
</subsectionHeader>
<bodyText confidence="0.981789411764706">
There are two parts that need to be calculated in
the learning algorithm: finding acost-augmented
Viterbi translation according to the scoring func-
tion and cost function (Equation 8), and constructing
synchronous hypergraphs for the Viterbi and refer-
ence translation so as to discover rules and calculate
average feature values in Equation (9). Following
the traditional decoding procedure, we resort to the
cube-pruning based algorithm for approximation.
To find the Viterbi translation, we run the tra-
ditional translation decoding algorithm (Chiang,
2007) to get the best derivation. Then we use
the translation yielded by the best derivation as the
Viterbi translation. In order to obtain the BLEU
score in the cost function, we need to calculate the
ngram precision. It is calculated in a way similar to
the calculation of the ngram language model. The
computation of BLEU-4 requires to record 3 bound-
ary words in both the left and right side during dy-
namic programming. Therefore, even when we use
a language model whose order is less than 4, we still
expands the states to record 3 boundary words so as
to calculate the cost measured by BLEU.
We build synchronous hypergraphs using the
cube-pruning based biparsing algorithm (Xiao et al.,
2012). Algorithm 2 shows the procedure. Using
a chart, the biparsing algorithm constructs k-best
alignments for every source word (lines 1-5) and k-
best hyperedges for every source span (lines 6-13)
from the bottom up. Thus, a synchronous hyper-
graph is generated during the construction of the
chart. More specifically, for a source span
, it first
creates cubes G for all source parses γ that are in-
</bodyText>
<page confidence="0.984416">
259
</page>
<bodyText confidence="0.434278">
Algorithm 2 BIPARSE(s, t, θ) ◃ (Xiao et al., 2012)
</bodyText>
<listItem confidence="0.9565391875">
D Create k-best alignments for each source word
1: for i ← 1,.., |s |do
2: for j ← 1,.., |t |do
3: Lj ← {E, tj} &gt; si aligns to tj or not
4: L ← ⟨L1, ...,L|t|⟩
5: chart[s, i] ← KBEST(L,⊗,θ)
D Create k-best hyperedges for each source span
6: H ← ∅
7: for h ← 1,.., |s |do &gt; h is the size of span
8: for all i, j s.t. j − i = h do
9: L ← ∅
10: for y inferable from chart do
11: L ← L + ⟨chart[y1], ..., chart[y|7|]⟩
12: chart[X, i, j] ← KBEST(L,⊗,θ)
13: H ← H + chart[X, i, j] &gt; save hyperedges
14: return H
</listItem>
<bodyText confidence="0.999856684210526">
ferable from the chart (lines 9-11). Here γi is a par-
tial source parse that covers either a single source
word or a span of source words. Then it uses the
cube pruning algorithm to keep the top k derivations
among all partial derivations that share the same
source span [i, j] (line 12). Notably, this biparsing
algorithm does not require specific translation rules
as input. Instead, it is able to discover new syn-
chronous grammar rules when constructing a syn-
chronous hypergraph: extracting each hyperedge in
the hypergraph as a synchronous rule.
Based on the biparsing algorithm, we are able to
construct the reference hypergraph x(s(i), t(i)) and
Viterbi hypergraph x(s(i), t). By the reference hy-
pergraph, we collect new synchronous translation
rules and record them in the grammar G. We also
calculate the average feature values of hypergraphs
using the inside-outside algorithm (Li et al., 2009),
so as to compute the gradients.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.9999845">
One advantage of the discriminative method is that
it enables us to incorporate arbitrary features. As
shown in Section 2, our model incorporates both lo-
cal and non-local features.
</bodyText>
<subsectionHeader confidence="0.993294">
4.1 Local Features
</subsectionHeader>
<bodyText confidence="0.999844809523809">
Rule features We associate each rule with an indi-
cator feature. Each indicator feature counts the num-
ber of times that a rule appears in a derivation. In
this way, we are able to learn a weight for every rule
according to the entire structure of sentence.
Word association features Lexicalized features
are widely used in traditional SMT systems. Here
we adopt two lexical weights called noisy-or fea-
tures (Zens and Ney, 2004). The noisy-or feature
is estimated by word translation probabilities output
by GIZA++. We set the initial weight of these two
lexical scores with equivalent positive values. The
lexical weights enable our system to score and rank
the hyperedges at the beginning. Although word
alignment features are used, we do not constrain the
derivation space of a sentence pair by prefixed word
alignment, and do not require any heuristic align-
ment combination strategy.
Length feature We integrate the length of target
translation that is used in traditional SMT system as
our feature.
Source span boundary features We use this kind
of feature to assess the source parse tree in a deriva-
tion. Previous work (Xiong et al., 2010) has shown
the importance of phrase boundary features for
translation. Actually, this kind of feature is a good
cue for deciding the boundary where a rule is to be
learnt. Following Taskar et al. (2004), for a bispan
[i, j, k, l] in a derivation, we define the feature tem-
plates that indicates the boundaries of a span by its
beginning and end words: {B : si+1; E : sj; BE :
si+1, sj}.
Source span orientation features Orientation
features are only used for those spans that are swap-
ping. In Figure 1, the translation of source span [1, 3]
is swapping with that of span [4, 5] by r2, thus ori-
entation feature for span [1, 3] is activated. We also
define three feature templates for a swapping span
similar to the boundary features: {B : si+1; E :
sj; BE : si+1, sj}. In practice, we add a prefix to
the orientation features so as to distinguish these fea-
tures from the boundary features.
</bodyText>
<subsectionHeader confidence="0.867003">
4.2 Non-local Features
</subsectionHeader>
<bodyText confidence="0.9999946">
Target span boundary features We also want to
assess the target tree structure in a derivation. We
define these features in a way similar to source span
boundary features. For a bispan [i, j, k, l] in a deriva-
tion, we define the feature templates that indicates
</bodyText>
<page confidence="0.991421">
260
</page>
<table confidence="0.9997834">
System Grammar Size MT03 MT04 MT05 Avg.
Moses 302.5M 34.26 36.56 32.69 34.50
Baseline 77.8M 33.83 35.81 33.23 34.29
Max-margin 59.4M 34.62 37.14 34.00 35.25
+Sparse feature 35.48 37.31 34.07 35.62
</table>
<tableCaption confidence="0.9735285">
Table 2: Experiment results. Baseline is an in-house implementation of hierarchical phrase based system. Moses
denotes the implementation of hierarchical phrased-model in Moses (Koehn et al., 2007). +Sparsefeature means
that those sparse features used in the grammar induction are also used during decoding. The improvement of max-
margin over Baseline is statistically significant (p &lt; 0.01).
</tableCaption>
<bodyText confidence="0.907853916666667">
target span boundary as: {B : tk+1; E : tl; BE :
tk+1, tl}.
Target span orientation features Similar target
orientation features are used for a swapping span
[i, j, k, l] with feature templates {B : tk+1; E :
tl; BE : tk+1, tl}.
Relative position features Following Blunsom
and Cohn (Blunsom and Cohn, 2006), we integrate
features indicating the closeness to the alignment
matrix diagonal. For an aligned word pair with
source position i and target position j, the value of
this feature is  |i
</bodyText>
<equation confidence="0.988558">
|s |− j
</equation>
<bodyText confidence="0.99929375">
|t||. As this feature depends
on the length of the target sentence, it is a non-local
feature.
Language model We also incorporate an ngram
language model which is an important component
in SMT. For efficiency, we use a 3-gram language
model trained on the target side of our training data
during the induction of synchronous grammars.
</bodyText>
<sectionHeader confidence="0.996333" genericHeader="method">
5 Experiment
</sectionHeader>
<bodyText confidence="0.9998997">
In this section, we present our experiments on the
NIST Chinese-to-English translation tasks. We first
compare our max-margin based method with the tra-
ditional pipeline on a large bitext which contains
1.1 million sentences. We then present a detailed
comparison on a smaller dataset, in order to analyze
the effectiveness of max-margin estimation compar-
ing with the max likelihood estimation (Xiao et al.,
2012), and also the effectiveness of the non-local
features that are defined on the target side.
</bodyText>
<subsectionHeader confidence="0.988138">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.999919">
The baseline system is the hierarchical phrase based
system (Chiang, 2007). We used a bilingual corpus
that contains 1.1M sentences (44.6 million words)
of up to length 40 from the LDC data.3 Our 5-gram
language model was trained by SRILM toolkit (Stol-
cke, 2002). The monolingual training data includes
the Xinhua section of the English Gigaword corpus
and the English side of the entire LDC data (432 mil-
lion words).
We used the NIST 2002 (MT02) as our develop-
ment set, and the NIST 2003-2005 (MT03-05) as the
test set. Case-insensitive NIST BLEU-4 (Papineni
et al., 2002) is used to measure translation perfor-
mance, and also the cost function in the max-margin
estimation. Statistical significance in BLEU differ-
ences was tested by paired bootstrap re-sampling
(Koehn, 2004). We used minimum error rate train-
ing (MERT) (Och, 2003) to optimize feature weights
for the traditional log-linear model.
We used the same decoder as the baseline system
in all estimation methods. Without special explana-
tion, we used the same features as those in the tra-
ditional pipeline: forward and backward translation
probabilities, forward and backward lexical weights,
count of extracted rules, count of glue rules, length
of translation, and language model. For the lexical
weights we used the noisy-or in all configurations
including the baseline system. For the discrimina-
tive grammar induction, rule translation probabili-
ties were calculated using the expectations of rules
in the synchronous hypergraphs of sentence pairs.
As our max-margin synchronous grammar induc-
tion is trained on the entire bitext, it is necessary to
load all the rules into the memory during training.
To control the size of rule table, we used Viterbi-
</bodyText>
<footnote confidence="0.996556666666667">
3Including LDC2002E18, LDC2003E07, LDC2003E14,
LDC2004T07, LDC2005T06 and Hansards portion of
LDC2004T08.
</footnote>
<page confidence="0.984709">
261
</page>
<table confidence="0.9998844">
System Feature Function MT03 MT04 MT05 Avg.
Baseline — 31.76 33.08 31.06 31.96
Max-likelihood local 32.84 34.54 31.61 33.00
local 32.97 34.92 31.99 33.29
Max-margin local,non-local 33.27 34.83 32.32 33.47
</table>
<tableCaption confidence="0.969700666666667">
Table 3: Comparison of Max-margin and Max-likelihood estimation on a smaller corpus. For max-margin method, we
present two results according to the usages of non-local features. The max-margin with non-local features significantly
outperforms the Baseline (p &lt; 0.01) and also the max-likelihood estimation (p &lt; 0.05).
</tableCaption>
<bodyText confidence="0.9993995">
pruning (Huang, 2008) when collecting rules as
shown in line 2 of optimization procedure in Section
3.2. Furthermore, we aggressively discarded those
large rules (The number of source symbols or the
number of target symbols are more than two) that
occur only in one sentence. Whenever the learning
algorithm processes 50K sentences, we performed
this discarding operation for large rules.
</bodyText>
<subsectionHeader confidence="0.999666">
5.2 Result on Large Dataset
</subsectionHeader>
<bodyText confidence="0.999928482758621">
Table 2 shows the translation results. Our method
induces 59.4 million synchronous rules, which are
76.3% of the grammar size of baseline. Note that
Moses allows the boundary words of a phrase to be
unaligned, while our baseline constraints the initial
phrase to be tightly consistent with word alignment.
Therefore, Moses extract a much larger rule table
than that of our baseline.
With fewer translation rules, our method obtains
an average improvement of +0.96 BLEU points on
the three test sets over the Baseline. As the differ-
ence between the baseline and our max-margin syn-
chronous grammar induction model only lies in the
grammar, this result clearly denotes that our learnt
grammar does outperform the grammar extracted by
the traditional two-step pipeline.
We also incorporate the sparse features during de-
coding in a way similar to Xiao et al. (2012) and
Dyer et al. (2011). In order to optimize these sparse
features with the dense features by MERT, we group
features of the same type into one coarse “summary
feature”, and get three such features including: rule,
phrase-boundary and phrase orientation features. In
this way, we rescale the weights of the three “sum-
mary features” with the 8 dense features by MERT.
We achieve a further improvement of +0.37 BLEU
points. Therefore, our training algorithm is able to
learn the useful information encoded by the sparse
features for translation.
</bodyText>
<subsectionHeader confidence="0.9997665">
5.3 Comparison of Estimation Objective and
Non-Local Feature
</subsectionHeader>
<bodyText confidence="0.999960733333333">
We want to investigate whether the max-margin esti-
mation is able to outperform the max-likelihood es-
timation method (Xiao et al., 2012). Therefore we
carried out experiments to compare them directly.
As the max-margin method is able to use non-local
features, we compare two settings of features for the
max-margin method. One uses only local features,
the other uses both local and non-local features. Be-
cause the training procedure need to run on the entire
corpus, which is time consuming, we therefore use
a smaller corpus containing 50K sentences from the
entire bitext for comparison.
Table 3 shows the results. When using only local
features, the max-margin method consistently out-
performs the max-likelihood method in all three test
sets. This clearly shows the advantage of learning
grammars by optimizing BLEU over likelihood.
When incorporating the non-local features into
the max-margin method, we achieve further im-
provement against the max-margin method with-
out non-local features. With non-local features,
our max-margin estimation method outperforms the
baseline by 1.5 BLEU points, and is better than
the max-likelihood estimation by 0.5 BLEU points.
Based on these results, we believe that non-local fea-
tures, which encode information from target parse
structures, are helpful for grammar induction. This
further confirms the advance of the max-margin es-
timation, as it provides us a convenient way to use
non-local features.
</bodyText>
<page confidence="0.994315">
262
</page>
<sectionHeader confidence="0.999966" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999987297297297">
As the synchronous grammar is the key compo-
nent in SMT systems, researchers have proposed
various methods to improve the quality of gram-
mars. In addition to the generative and discrimina-
tive models introduced in Section 1, researchers also
have made efforts on word alignment and grammar
weight rescoring.
The first line is to modify word alignment by ex-
ploring information of syntactic structures (May and
Knight, 2007; DeNero and Klein, 2010; Pauls et
al., 2010; Burkett et al., 2010; Riesa et al., 2011).
Such syntactic information is combined with word
alignment via a discriminative framework. These
methods prefer word alignments that are consistent
with syntactic structure alignments. However, la-
beled word alignment data are required in order to
learn the discriminative model.
Yet another line is to rescore the weights of trans-
lation rules. This line of work tries to improve the
relative frequency estimation used in the traditional
pipeline. They rescore the weights or probabilities
of extracted rules. The rescoring is done by using
the similar latent log-linear model as ours (Blun-
som et al., 2008; K¨a¨ari¨ainen, 2009; He and Deng,
2012), or incorporating various features using la-
beled word aligned bilingual data (Huang and Xi-
ang, 2010). However, in rescoring, translation rules
are still extracted by the heuristic two-step pipeline.
Therefore these previous work still suffers from the
inelegance problem of the traditional pipeline.
Our work also relates to the discriminative train-
ing (Och, 2003; Watanabe et al., 2007; Chiang et al.,
2009; Xiao et al., 2011; Gimpel and Smith, 2012)
that has been widely used in SMT systems. Notably,
these discriminative training methods are not used to
learn grammar. Instead, they assume that grammar
are extracted by the traditional two-step pipeline.
</bodyText>
<sectionHeader confidence="0.999292" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999551">
In this paper we have presented a max-margin esti-
mation for discriminative synchronous grammar in-
duction. By associating the margin with the transla-
tion quality, we directly learn translation rules that
optimize the translation performance measured by
BLEU. Max-margin estimation also provides us a
convenient way to incorporate non-local features.
Experiment results validate the effectiveness of opti-
mizing parameters by BLEU, and the importance of
incorporating non-local features defined on the tar-
get language. These results confirm the advantage of
our max-margin estimation framework as it can both
optimize BLEU and incorporate non-local features.
Feature engineering is very important for discrim-
inative models. Researchers have proposed various
types of features for machine translation, which are
often estimated from word alignments. We would
like to investigate whether further improvement can
be achieved by incorporating such features, espe-
cially the context model (Shen et al., 2009) in the
future. Because our proposed model is quite general,
we are also interested in applying this method to
induce linguistically motivated synchronous gram-
mars for syntax-based SMT.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999638333333333">
The first author was partially supported by 863
State Key Project (No. 2011AA01A207) and
National Key Technology R&amp;D Program (No.
2012BAH39B03). We are grateful to the anony-
mous reviewers for their insightful comments. We
also thank Yi Lin for her invaluable feedback.
</bodyText>
<sectionHeader confidence="0.999082" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998104238095238">
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Prob. ACL 2006, July.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. ACL 2008.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proc. ACL 2009.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchronized
grammars. In Proc. NAACL 2010.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In Proc. SSST 2007, NAACL-HLT Workshop on Syntax
and Structure in Statistical Translation, April.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proc. NAACL 2009.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
</reference>
<page confidence="0.982079">
263
</page>
<reference confidence="0.999437188679246">
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model
of syntax-directed tree to string grammar induction. In
Proc. EMNLP 2009.
John DeNero and Dan Klein. 2010. Discriminative mod-
eling of extraction sets for machine translation. In
Proc. ACL 2010.
John DeNero, Alexandre Bouchard-Cˆot´e, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proc. EMNLP 2008.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The cmu-ark german-english
translation system. In Proc. WMT 2011.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proc. NAACL 2012.
Xiaodong He and Li Deng. 2012. Maximum expected
bleu training of phrase and lexicon translation models.
In Proc. ACL 2012.
Fei Huang and Bing Xiang. 2010. Feature-rich discrimi-
native phrase rescoring for smt. In Proc. Coling 2010.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. ACL 2008.
Matti K¨a¨ari¨ainen. 2009. Sinuhe – statistical machine
translation using a globally trained conditional expo-
nential family translation model. In Proc. EMNLP
2009.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
ACL 2007 (demonstration session).
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP
2004.
Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012.
A bayesian model for learning scfgs with discontigu-
ous rules. In Proc. EMNLP 2012. Association for
Computational Linguistics, July.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proc. ACL 2009.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In Pro. Coling 2004.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. EMNLP 2002.
Jonathan May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In Proc.
EMNLP 2007.
Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shin-
suke Mori, and Tatsuya Kawahara. 2011. An unsuper-
vised model for joint phrase alignment and extraction.
In Proc. ACL 2011.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. ACL 2002.
Adam Pauls, Dan Klein, David Chiang, and Kevin
Knight. 2010. Unsupervised syntactic alignment with
inversion transduction grammars. In Proc. NAACL
2010.
Jason Riesa, Ann Irvine, and Daniel Marcu. 2011.
Feature-rich language-independent syntax-based
alignment for statistical machine translation. In Proc.
EMNLP 2011.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.
2007. Pegasos: Primal estimated sub-gradient solver
for svm. In Proc. ICML 2007.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proc. EMNLP 2009.
Andreas Stolcke. 2002. Srilm – an extensible language
modeling toolkit.
Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and
Christopher Manning. 2004. Max-margin parsing. In
Proc. EMNLP 2004.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proc. EMNLP-CoNLL
2007.
Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun Lin.
2011. Fast generation of translation forest for large-
scale smt discriminative training. In Proc. EMNLP
2011.
Xinyan Xiao, Deyi Xiong, Yang Liu, Qun Liu, and
Shouxun Lin. 2012. Unsupervised discriminative in-
duction of synchronous grammar for machine transla-
tion. In Proc. Coling 2012.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learn-
ing translation boundaries for phrase-based decoding.
In Proc. NAACL2010.
Richard Zens and Hermann Ney. 2004. Improvements in
phrase-based statistical machine translation. In Prob.
NAACL 2004.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
Proc. ACL 2008.
</reference>
<page confidence="0.998209">
264
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.724169">
<title confidence="0.999835">Max-Margin Synchronous Grammar Induction for Machine Translation</title>
<author confidence="0.992836">Xiao</author>
<affiliation confidence="0.8828995">School of Computer Science and Soochow</affiliation>
<address confidence="0.963216">Suzhou 215006, China</address>
<email confidence="0.982787">xyxiao.cn@gmail.com,dyxiong@suda.edu.cn</email>
<abstract confidence="0.99830780952381">Traditional synchronous grammar induction estimates parameters by maximizing likelihood, which only has a loose relation to translation quality. Alternatively, we propose a max-margin estimation approach to discriminatively inducing synchronous grammars for machine translation, which directly optimizes quality measured by In the max-margin estimation of parameters, we only need to calculate Viterbi translations. This further facilitates the incorporation of various non-local features that are defined on the target side. We test the effectiveness of our max-margin estimation framework on a competitive hierarchical phrase-based system. Experiments show that our max-margin method significantly outperforms the traditional twostep pipeline for synchronous rule extraction 1.3 and is also better than previous max-likelihood estimation method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Discriminative word alignment with conditional random fields.</title>
<date>2006</date>
<booktitle>In Prob. ACL</booktitle>
<contexts>
<context position="23462" citStr="Blunsom and Cohn, 2006" startWordPosition="3932" endWordPosition="3935">rchical phrase based system. Moses denotes the implementation of hierarchical phrased-model in Moses (Koehn et al., 2007). +Sparsefeature means that those sparse features used in the grammar induction are also used during decoding. The improvement of maxmargin over Baseline is statistically significant (p &lt; 0.01). target span boundary as: {B : tk+1; E : tl; BE : tk+1, tl}. Target span orientation features Similar target orientation features are used for a swapping span [i, j, k, l] with feature templates {B : tk+1; E : tl; BE : tk+1, tl}. Relative position features Following Blunsom and Cohn (Blunsom and Cohn, 2006), we integrate features indicating the closeness to the alignment matrix diagonal. For an aligned word pair with source position i and target position j, the value of this feature is |i |s |− j |t||. As this feature depends on the length of the target sentence, it is a non-local feature. Language model We also incorporate an ngram language model which is an important component in SMT. For efficiency, we use a 3-gram language model trained on the target side of our training data during the induction of synchronous grammars. 5 Experiment In this section, we present our experiments on the NIST Ch</context>
</contexts>
<marker>Blunsom, Cohn, 2006</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2006. Discriminative word alignment with conditional random fields. In Prob. ACL 2006, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="31324" citStr="Blunsom et al., 2008" startWordPosition="5172" endWordPosition="5176">et al., 2011). Such syntactic information is combined with word alignment via a discriminative framework. These methods prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. Yet another line is to rescore the weights of translation rules. This line of work tries to improve the relative frequency estimation used in the traditional pipeline. They rescore the weights or probabilities of extracted rules. The rescoring is done by using the similar latent log-linear model as ours (Blunsom et al., 2008; K¨a¨ari¨ainen, 2009; He and Deng, 2012), or incorporating various features using labeled word aligned bilingual data (Huang and Xiang, 2010). However, in rescoring, translation rules are still extracted by the heuristic two-step pipeline. Therefore these previous work still suffers from the inelegance problem of the traditional pipeline. Our work also relates to the discriminative training (Och, 2003; Watanabe et al., 2007; Chiang et al., 2009; Xiao et al., 2011; Gimpel and Smith, 2012) that has been widely used in SMT systems. Notably, these discriminative training methods are not used to l</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proc. ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Chris Dyer</author>
<author>Miles Osborne</author>
</authors>
<title>A gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="2134" citStr="Blunsom et al., 2009" startWordPosition="294" endWordPosition="297">ics (Koehn et al., 2003). This pipeline first builds word alignments using heuristic combination strategies, then heuristically extracts rules that are consistent with word alignments. Such heuristic pipeline *Corresponding author is not elegant theoretically. It brings an undesirable gap that separates modeling and learning in an SMT system. Therefore, researchers have proposed alternative approaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012) or discriminative models (Xiao et al., 2012). Theoretically, these approaches describe how sentence pairs are generated by applying sequences of synchronous rules in an elegant way. However, they learn synchronous grammars by maximizing likelihood,1 which only has a loose relation to translation quality (He and Deng, 2012). Moreover, generative models are normally hard to be extended to incorporate useful features, and the discriminative synchronous grammar induction model proposed by Xiao et al. (2012) only incorporates lo</context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009. A gibbs sampler for phrasal synchronous grammar induction. In Proc. ACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Joint parsing and alignment with weakly synchronized grammars.</title>
<date>2010</date>
<booktitle>In Proc. NAACL</booktitle>
<contexts>
<context position="30696" citStr="Burkett et al., 2010" startWordPosition="5075" endWordPosition="5078">er confirms the advance of the max-margin estimation, as it provides us a convenient way to use non-local features. 262 6 Related Work As the synchronous grammar is the key component in SMT systems, researchers have proposed various methods to improve the quality of grammars. In addition to the generative and discriminative models introduced in Section 1, researchers also have made efforts on word alignment and grammar weight rescoring. The first line is to modify word alignment by exploring information of syntactic structures (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011). Such syntactic information is combined with word alignment via a discriminative framework. These methods prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. Yet another line is to rescore the weights of translation rules. This line of work tries to improve the relative frequency estimation used in the traditional pipeline. They rescore the weights or probabilities of extracted rules. The rescoring is done by using the similar latent log-linear model a</context>
</contexts>
<marker>Burkett, Blitzer, Klein, 2010</marker>
<rawString>David Burkett, John Blitzer, and Dan Klein. 2010. Joint parsing and alignment with weakly synchronized grammars. In Proc. NAACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Inversion transduction grammar for joint phrasal translation modeling.</title>
<date>2007</date>
<booktitle>In Proc. SSST 2007, NAACL-HLT Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<contexts>
<context position="2071" citStr="Cherry and Lin, 2007" startWordPosition="282" endWordPosition="285"> learn translation rules via a pipeline with word-based heuristics (Koehn et al., 2003). This pipeline first builds word alignments using heuristic combination strategies, then heuristically extracts rules that are consistent with word alignments. Such heuristic pipeline *Corresponding author is not elegant theoretically. It brings an undesirable gap that separates modeling and learning in an SMT system. Therefore, researchers have proposed alternative approaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012) or discriminative models (Xiao et al., 2012). Theoretically, these approaches describe how sentence pairs are generated by applying sequences of synchronous rules in an elegant way. However, they learn synchronous grammars by maximizing likelihood,1 which only has a loose relation to translation quality (He and Deng, 2012). Moreover, generative models are normally hard to be extended to incorporate useful features, and the discriminative synchronous grammar indu</context>
</contexts>
<marker>Cherry, Lin, 2007</marker>
<rawString>Colin Cherry and Dekang Lin. 2007. Inversion transduction grammar for joint phrasal translation modeling. In Proc. SSST 2007, NAACL-HLT Workshop on Syntax and Structure in Statistical Translation, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. NAACL</booktitle>
<contexts>
<context position="12982" citStr="Chiang et al., 2009" startWordPosition="2114" endWordPosition="2117">be considered as the geometric central point in the space D(s, t). Another possible way to deal with the latent derivation is max-derivation, which uses the maxoperator over D(s, t). The max derivation method sets f(s, t) as maxdED(s,t) f(s, t, d). It is often adopted in traditional SMT systems. Nevertheless, we instead use average-derivation for two reasons.2 2Imagine that H(s, t) in the Algorithm 1 is replaced by a maximum derivation in H(s, t). First, as a translation has an exponential number of derivations, finding the max derivation of a reference translation for learning is nontrivial (Chiang et al., 2009). Second, the max derivation estimation will result in a low rule coverage, as rules in a max derivation only covers a small fraction of rules in the D(s, t). Because rule coverage is important in synchronous grammar induction, we would like to explore the entire derivation space using the average operator. 3.2 Learning Algorithm We reformulate the equation (3) as an unconstrained empirical loss minimization problem as follows: ∑N L(s(i), t(i), 0) (6) A min 2 110112 + 1 N n=1 Where A denotes the regularization strength for L2-norm. The loss function of a sentence pair L(s(i), t(i), 0) is a con</context>
<context position="31773" citStr="Chiang et al., 2009" startWordPosition="5241" endWordPosition="5244">l pipeline. They rescore the weights or probabilities of extracted rules. The rescoring is done by using the similar latent log-linear model as ours (Blunsom et al., 2008; K¨a¨ari¨ainen, 2009; He and Deng, 2012), or incorporating various features using labeled word aligned bilingual data (Huang and Xiang, 2010). However, in rescoring, translation rules are still extracted by the heuristic two-step pipeline. Therefore these previous work still suffers from the inelegance problem of the traditional pipeline. Our work also relates to the discriminative training (Och, 2003; Watanabe et al., 2007; Chiang et al., 2009; Xiao et al., 2011; Gimpel and Smith, 2012) that has been widely used in SMT systems. Notably, these discriminative training methods are not used to learn grammar. Instead, they assume that grammar are extracted by the traditional two-step pipeline. 7 Conclusion In this paper we have presented a max-margin estimation for discriminative synchronous grammar induction. By associating the margin with the translation quality, we directly learn translation rules that optimize the translation performance measured by BLEU. Max-margin estimation also provides us a convenient way to incorporate non-loc</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proc. NAACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="4601" citStr="Chiang, 2007" startWordPosition="660" endWordPosition="661">ding to their translation performance. We further incorporate various nonlocal features defined on target parse trees. We efficiently calculate the non-local feature values of a translation over its exponential derivation space using the inside-outside algorithm. Because our maxmargin estimation optimizes feature weights only by the feature values of Viterbi and reference translations, we are able to efficiently perform optimization even with non-local features. We apply the proposed max-margin estimation method to learn synchronous grammars for a hierarchical phrase-based translation system (Chiang, 2007) which typically produces state-of-the-art performance. With non-local features defined on target parse trees, our max-margin method significantly outperforms the baseline that uses synchronous rules learned from the traditional pipeline by 1.3 BLEU points on large-scale Chinese-English bilingual training data. The remainder of this paper is organized as follows. Section 2 presents the discriminative synchronous grammar induction model with the nonlocal features. In Section 3, we elaborate our maxmargin estimation method which is able to directly optimize BLEU, and discuss how we induce gramma</context>
<context position="15777" citStr="Chiang (2007)" startWordPosition="2614" endWordPosition="2615">, d) |D(s , (i)t(i))| nally, we update the feature weights using the sum of these gradients. nce dED(s(z),t(z)) 1 ∑ 4&apos;(s(i), t, d) (9) |D(s(i),�t) |dED(s(&apos;&apos;),ˆt) Algorithm 1 shows the procedure of one step in the online optimization algorithm. The procedure discovers rules and updates weights in an online fashion. In the procedure, we first biparse the sentence pair to construct a synchronous hypergraph of a reference translation (line 1). In the biparsing algorithm, synchronous rules for constructing hyperedges are not required to be in G, but can be any rules that follow the form defined in Chiang (2007). Thus, the biparsing algorithm can discover new rules that are not in G. Then we collect the translation rules discovered in the hypergraph of the reference translation (line 2), which are rules indicated by hyperedges in the hypergraph. We then calculate the Viterbi translation according to the scoring function and cost function (see Section 3.3) (line 3), and build the synchronous hypergraph for the Viterbi translation (line 4). Finally, we update weights according to the Pegasos algorithm (line 5). The sub-gradient is calculated based on the hypergraph of Viterbi translation and reference </context>
<context position="17452" citStr="Chiang, 2007" startWordPosition="2876" endWordPosition="2877">rbi hypergraphs. Fiand a reference tran ∂θ � 3.3 Infere There are two parts that need to be calculated in the learning algorithm: finding acost-augmented Viterbi translation according to the scoring function and cost function (Equation 8), and constructing synchronous hypergraphs for the Viterbi and reference translation so as to discover rules and calculate average feature values in Equation (9). Following the traditional decoding procedure, we resort to the cube-pruning based algorithm for approximation. To find the Viterbi translation, we run the traditional translation decoding algorithm (Chiang, 2007) to get the best derivation. Then we use the translation yielded by the best derivation as the Viterbi translation. In order to obtain the BLEU score in the cost function, we need to calculate the ngram precision. It is calculated in a way similar to the calculation of the ngram language model. The computation of BLEU-4 requires to record 3 boundary words in both the left and right side during dynamic programming. Therefore, even when we use a language model whose order is less than 4, we still expands the states to record 3 boundary words so as to calculate the cost measured by BLEU. We build</context>
<context position="24592" citStr="Chiang, 2007" startWordPosition="4118" endWordPosition="4119">s grammars. 5 Experiment In this section, we present our experiments on the NIST Chinese-to-English translation tasks. We first compare our max-margin based method with the traditional pipeline on a large bitext which contains 1.1 million sentences. We then present a detailed comparison on a smaller dataset, in order to analyze the effectiveness of max-margin estimation comparing with the max likelihood estimation (Xiao et al., 2012), and also the effectiveness of the non-local features that are defined on the target side. 5.1 Setup The baseline system is the hierarchical phrase based system (Chiang, 2007). We used a bilingual corpus that contains 1.1M sentences (44.6 million words) of up to length 40 from the LDC data.3 Our 5-gram language model was trained by SRILM toolkit (Stolcke, 2002). The monolingual training data includes the Xinhua section of the English Gigaword corpus and the English side of the entire LDC data (432 million words). We used the NIST 2002 (MT02) as our development set, and the NIST 2003-2005 (MT03-05) as the test set. Case-insensitive NIST BLEU-4 (Papineni et al., 2002) is used to measure translation performance, and also the cost function in the max-margin estimation.</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
</authors>
<title>A Bayesian model of syntax-directed tree to string grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. EMNLP</booktitle>
<contexts>
<context position="2158" citStr="Cohn and Blunsom, 2009" startWordPosition="298" endWordPosition="302">3). This pipeline first builds word alignments using heuristic combination strategies, then heuristically extracts rules that are consistent with word alignments. Such heuristic pipeline *Corresponding author is not elegant theoretically. It brings an undesirable gap that separates modeling and learning in an SMT system. Therefore, researchers have proposed alternative approaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012) or discriminative models (Xiao et al., 2012). Theoretically, these approaches describe how sentence pairs are generated by applying sequences of synchronous rules in an elegant way. However, they learn synchronous grammars by maximizing likelihood,1 which only has a loose relation to translation quality (He and Deng, 2012). Moreover, generative models are normally hard to be extended to incorporate useful features, and the discriminative synchronous grammar induction model proposed by Xiao et al. (2012) only incorporates local features defined on </context>
</contexts>
<marker>Cohn, Blunsom, 2009</marker>
<rawString>Trevor Cohn and Phil Blunsom. 2009. A Bayesian model of syntax-directed tree to string grammar induction. In Proc. EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Discriminative modeling of extraction sets for machine translation.</title>
<date>2010</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="30654" citStr="DeNero and Klein, 2010" startWordPosition="5067" endWordPosition="5070">re helpful for grammar induction. This further confirms the advance of the max-margin estimation, as it provides us a convenient way to use non-local features. 262 6 Related Work As the synchronous grammar is the key component in SMT systems, researchers have proposed various methods to improve the quality of grammars. In addition to the generative and discriminative models introduced in Section 1, researchers also have made efforts on word alignment and grammar weight rescoring. The first line is to modify word alignment by exploring information of syntactic structures (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011). Such syntactic information is combined with word alignment via a discriminative framework. These methods prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. Yet another line is to rescore the weights of translation rules. This line of work tries to improve the relative frequency estimation used in the traditional pipeline. They rescore the weights or probabilities of extracted rules. The rescoring is done by u</context>
</contexts>
<marker>DeNero, Klein, 2010</marker>
<rawString>John DeNero and Dan Klein. 2010. Discriminative modeling of extraction sets for machine translation. In Proc. ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP</booktitle>
<marker>DeNero, Bouchard-Cˆot´e, Klein, 2008</marker>
<rawString>John DeNero, Alexandre Bouchard-Cˆot´e, and Dan Klein. 2008. Sampling alignment structure under a Bayesian translation model. In Proc. EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Jonathan H Clark</author>
<author>Noah A Smith</author>
</authors>
<title>The cmu-ark german-english translation system.</title>
<date>2011</date>
<booktitle>In Proc. WMT</booktitle>
<contexts>
<context position="28165" citStr="Dyer et al. (2011)" startWordPosition="4677" endWordPosition="4680"> tightly consistent with word alignment. Therefore, Moses extract a much larger rule table than that of our baseline. With fewer translation rules, our method obtains an average improvement of +0.96 BLEU points on the three test sets over the Baseline. As the difference between the baseline and our max-margin synchronous grammar induction model only lies in the grammar, this result clearly denotes that our learnt grammar does outperform the grammar extracted by the traditional two-step pipeline. We also incorporate the sparse features during decoding in a way similar to Xiao et al. (2012) and Dyer et al. (2011). In order to optimize these sparse features with the dense features by MERT, we group features of the same type into one coarse “summary feature”, and get three such features including: rule, phrase-boundary and phrase orientation features. In this way, we rescale the weights of the three “summary features” with the 8 dense features by MERT. We achieve a further improvement of +0.37 BLEU points. Therefore, our training algorithm is able to learn the useful information encoded by the sparse features for translation. 5.3 Comparison of Estimation Objective and Non-Local Feature We want to invest</context>
</contexts>
<marker>Dyer, Gimpel, Clark, Smith, 2011</marker>
<rawString>Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and Noah A. Smith. 2011. The cmu-ark german-english translation system. In Proc. WMT 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Structured ramp loss minimization for machine translation.</title>
<date>2012</date>
<booktitle>In Proc. NAACL</booktitle>
<contexts>
<context position="31817" citStr="Gimpel and Smith, 2012" startWordPosition="5249" endWordPosition="5252">probabilities of extracted rules. The rescoring is done by using the similar latent log-linear model as ours (Blunsom et al., 2008; K¨a¨ari¨ainen, 2009; He and Deng, 2012), or incorporating various features using labeled word aligned bilingual data (Huang and Xiang, 2010). However, in rescoring, translation rules are still extracted by the heuristic two-step pipeline. Therefore these previous work still suffers from the inelegance problem of the traditional pipeline. Our work also relates to the discriminative training (Och, 2003; Watanabe et al., 2007; Chiang et al., 2009; Xiao et al., 2011; Gimpel and Smith, 2012) that has been widely used in SMT systems. Notably, these discriminative training methods are not used to learn grammar. Instead, they assume that grammar are extracted by the traditional two-step pipeline. 7 Conclusion In this paper we have presented a max-margin estimation for discriminative synchronous grammar induction. By associating the margin with the translation quality, we directly learn translation rules that optimize the translation performance measured by BLEU. Max-margin estimation also provides us a convenient way to incorporate non-local features. Experiment results validate the</context>
</contexts>
<marker>Gimpel, Smith, 2012</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2012. Structured ramp loss minimization for machine translation. In Proc. NAACL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Li Deng</author>
</authors>
<title>Maximum expected bleu training of phrase and lexicon translation models.</title>
<date>2012</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="2529" citStr="He and Deng, 2012" startWordPosition="357" endWordPosition="360">roaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012) or discriminative models (Xiao et al., 2012). Theoretically, these approaches describe how sentence pairs are generated by applying sequences of synchronous rules in an elegant way. However, they learn synchronous grammars by maximizing likelihood,1 which only has a loose relation to translation quality (He and Deng, 2012). Moreover, generative models are normally hard to be extended to incorporate useful features, and the discriminative synchronous grammar induction model proposed by Xiao et al. (2012) only incorporates local features defined on parse trees of the source language. Nonlocal features, which encode information from parse trees of the target language, have never been exploited before due to the computational complexity of normalization in max-likelihood estimation. Consequently, we would like to learn synchronous grammars in a discriminative way that can directly maximize the end-to-end translatio</context>
<context position="31365" citStr="He and Deng, 2012" startWordPosition="5179" endWordPosition="5182">s combined with word alignment via a discriminative framework. These methods prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. Yet another line is to rescore the weights of translation rules. This line of work tries to improve the relative frequency estimation used in the traditional pipeline. They rescore the weights or probabilities of extracted rules. The rescoring is done by using the similar latent log-linear model as ours (Blunsom et al., 2008; K¨a¨ari¨ainen, 2009; He and Deng, 2012), or incorporating various features using labeled word aligned bilingual data (Huang and Xiang, 2010). However, in rescoring, translation rules are still extracted by the heuristic two-step pipeline. Therefore these previous work still suffers from the inelegance problem of the traditional pipeline. Our work also relates to the discriminative training (Och, 2003; Watanabe et al., 2007; Chiang et al., 2009; Xiao et al., 2011; Gimpel and Smith, 2012) that has been widely used in SMT systems. Notably, these discriminative training methods are not used to learn grammar. Instead, they assume that g</context>
</contexts>
<marker>He, Deng, 2012</marker>
<rawString>Xiaodong He and Li Deng. 2012. Maximum expected bleu training of phrase and lexicon translation models. In Proc. ACL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Bing Xiang</author>
</authors>
<title>Feature-rich discriminative phrase rescoring for smt.</title>
<date>2010</date>
<booktitle>In Proc. Coling</booktitle>
<contexts>
<context position="31466" citStr="Huang and Xiang, 2010" startWordPosition="5194" endWordPosition="5198">ts that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. Yet another line is to rescore the weights of translation rules. This line of work tries to improve the relative frequency estimation used in the traditional pipeline. They rescore the weights or probabilities of extracted rules. The rescoring is done by using the similar latent log-linear model as ours (Blunsom et al., 2008; K¨a¨ari¨ainen, 2009; He and Deng, 2012), or incorporating various features using labeled word aligned bilingual data (Huang and Xiang, 2010). However, in rescoring, translation rules are still extracted by the heuristic two-step pipeline. Therefore these previous work still suffers from the inelegance problem of the traditional pipeline. Our work also relates to the discriminative training (Och, 2003; Watanabe et al., 2007; Chiang et al., 2009; Xiao et al., 2011; Gimpel and Smith, 2012) that has been widely used in SMT systems. Notably, these discriminative training methods are not used to learn grammar. Instead, they assume that grammar are extracted by the traditional two-step pipeline. 7 Conclusion In this paper we have present</context>
</contexts>
<marker>Huang, Xiang, 2010</marker>
<rawString>Fei Huang and Bing Xiang. 2010. Feature-rich discriminative phrase rescoring for smt. In Proc. Coling 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="26886" citStr="Huang, 2008" startWordPosition="4472" endWordPosition="4473">E14, LDC2004T07, LDC2005T06 and Hansards portion of LDC2004T08. 261 System Feature Function MT03 MT04 MT05 Avg. Baseline — 31.76 33.08 31.06 31.96 Max-likelihood local 32.84 34.54 31.61 33.00 local 32.97 34.92 31.99 33.29 Max-margin local,non-local 33.27 34.83 32.32 33.47 Table 3: Comparison of Max-margin and Max-likelihood estimation on a smaller corpus. For max-margin method, we present two results according to the usages of non-local features. The max-margin with non-local features significantly outperforms the Baseline (p &lt; 0.01) and also the max-likelihood estimation (p &lt; 0.05). pruning (Huang, 2008) when collecting rules as shown in line 2 of optimization procedure in Section 3.2. Furthermore, we aggressively discarded those large rules (The number of source symbols or the number of target symbols are more than two) that occur only in one sentence. Whenever the learning algorithm processes 50K sentences, we performed this discarding operation for large rules. 5.2 Result on Large Dataset Table 2 shows the translation results. Our method induces 59.4 million synchronous rules, which are 76.3% of the grammar size of baseline. Note that Moses allows the boundary words of a phrase to be unali</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proc. ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matti K¨a¨ari¨ainen</author>
</authors>
<title>Sinuhe – statistical machine translation using a globally trained conditional exponential family translation model.</title>
<date>2009</date>
<booktitle>In Proc. EMNLP</booktitle>
<marker>K¨a¨ari¨ainen, 2009</marker>
<rawString>Matti K¨a¨ari¨ainen. 2009. Sinuhe – statistical machine translation using a globally trained conditional exponential family translation model. In Proc. EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL</booktitle>
<contexts>
<context position="1538" citStr="Koehn et al., 2003" startWordPosition="208" endWordPosition="211">in method significantly outperforms the traditional twostep pipeline for synchronous rule extraction by 1.3 BLEU points and is also better than previous max-likelihood estimation method. 1 Introduction Synchronous grammar induction, which refers to the process of learning translation rules from bilingual corpus, still remains an open problem in statistical machine translation (SMT). Although stateof-the-art SMT systems model the translation process based on synchronous grammars (including bilingual phrases), most of them still learn translation rules via a pipeline with word-based heuristics (Koehn et al., 2003). This pipeline first builds word alignments using heuristic combination strategies, then heuristically extracts rules that are consistent with word alignments. Such heuristic pipeline *Corresponding author is not elegant theoretically. It brings an undesirable gap that separates modeling and learning in an SMT system. Therefore, researchers have proposed alternative approaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Co</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. HLT-NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ACL</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="22960" citStr="Koehn et al., 2007" startWordPosition="3847" endWordPosition="3850">t to assess the target tree structure in a derivation. We define these features in a way similar to source span boundary features. For a bispan [i, j, k, l] in a derivation, we define the feature templates that indicates 260 System Grammar Size MT03 MT04 MT05 Avg. Moses 302.5M 34.26 36.56 32.69 34.50 Baseline 77.8M 33.83 35.81 33.23 34.29 Max-margin 59.4M 34.62 37.14 34.00 35.25 +Sparse feature 35.48 37.31 34.07 35.62 Table 2: Experiment results. Baseline is an in-house implementation of hierarchical phrase based system. Moses denotes the implementation of hierarchical phrased-model in Moses (Koehn et al., 2007). +Sparsefeature means that those sparse features used in the grammar induction are also used during decoding. The improvement of maxmargin over Baseline is statistically significant (p &lt; 0.01). target span boundary as: {B : tk+1; E : tl; BE : tk+1, tl}. Target span orientation features Similar target orientation features are used for a swapping span [i, j, k, l] with feature templates {B : tk+1; E : tl; BE : tk+1, tl}. Relative position features Following Blunsom and Cohn (Blunsom and Cohn, 2006), we integrate features indicating the closeness to the alignment matrix diagonal. For an aligned </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ACL 2007 (demonstration session).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. EMNLP</booktitle>
<contexts>
<context position="25294" citStr="Koehn, 2004" startWordPosition="4234" endWordPosition="4235">ngth 40 from the LDC data.3 Our 5-gram language model was trained by SRILM toolkit (Stolcke, 2002). The monolingual training data includes the Xinhua section of the English Gigaword corpus and the English side of the entire LDC data (432 million words). We used the NIST 2002 (MT02) as our development set, and the NIST 2003-2005 (MT03-05) as the test set. Case-insensitive NIST BLEU-4 (Papineni et al., 2002) is used to measure translation performance, and also the cost function in the max-margin estimation. Statistical significance in BLEU differences was tested by paired bootstrap re-sampling (Koehn, 2004). We used minimum error rate training (MERT) (Och, 2003) to optimize feature weights for the traditional log-linear model. We used the same decoder as the baseline system in all estimation methods. Without special explanation, we used the same features as those in the traditional pipeline: forward and backward translation probabilities, forward and backward lexical weights, count of extracted rules, count of glue rules, length of translation, and language model. For the lexical weights we used the noisy-or in all configurations including the baseline system. For the discriminative grammar indu</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Chris Dyer</author>
<author>Phil Blunsom</author>
</authors>
<title>A bayesian model for learning scfgs with discontiguous rules.</title>
<date>2012</date>
<booktitle>In Proc. EMNLP 2012. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="2204" citStr="Levenberg et al., 2012" startWordPosition="307" endWordPosition="310">s using heuristic combination strategies, then heuristically extracts rules that are consistent with word alignments. Such heuristic pipeline *Corresponding author is not elegant theoretically. It brings an undesirable gap that separates modeling and learning in an SMT system. Therefore, researchers have proposed alternative approaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012) or discriminative models (Xiao et al., 2012). Theoretically, these approaches describe how sentence pairs are generated by applying sequences of synchronous rules in an elegant way. However, they learn synchronous grammars by maximizing likelihood,1 which only has a loose relation to translation quality (He and Deng, 2012). Moreover, generative models are normally hard to be extended to incorporate useful features, and the discriminative synchronous grammar induction model proposed by Xiao et al. (2012) only incorporates local features defined on parse trees of the source language. Nonlocal f</context>
</contexts>
<marker>Levenberg, Dyer, Blunsom, 2012</marker>
<rawString>Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012. A bayesian model for learning scfgs with discontiguous rules. In Proc. EMNLP 2012. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Variational decoding for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="20001" citStr="Li et al., 2009" startWordPosition="3342" endWordPosition="3345">. Notably, this biparsing algorithm does not require specific translation rules as input. Instead, it is able to discover new synchronous grammar rules when constructing a synchronous hypergraph: extracting each hyperedge in the hypergraph as a synchronous rule. Based on the biparsing algorithm, we are able to construct the reference hypergraph x(s(i), t(i)) and Viterbi hypergraph x(s(i), t). By the reference hypergraph, we collect new synchronous translation rules and record them in the grammar G. We also calculate the average feature values of hypergraphs using the inside-outside algorithm (Li et al., 2009), so as to compute the gradients. 4 Features One advantage of the discriminative method is that it enables us to incorporate arbitrary features. As shown in Section 2, our model incorporates both local and non-local features. 4.1 Local Features Rule features We associate each rule with an indicator feature. Each indicator feature counts the number of times that a rule appears in a derivation. In this way, we are able to learn a weight for every rule according to the entire structure of sentence. Word association features Lexicalized features are widely used in traditional SMT systems. Here we </context>
</contexts>
<marker>Li, Eisner, Khudanpur, 2009</marker>
<rawString>Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009. Variational decoding for statistical machine translation. In Proc. ACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Orange: a method for evaluating automatic evaluation metrics for machine translation. In Pro. Coling</title>
<date>2004</date>
<contexts>
<context position="11510" citStr="Lin and Och, 2004" startWordPosition="1866" endWordPosition="1869"> penalize larger translation Ѯ㹼 Պ䈸 о [1,5] [1,6] Պ䈸 ∑ ϕ(r, s) ∑ -b(s, t, d) _ |{z } + rEd local rEd 257 errors more severely than smaller ones. This intuition is expressed by the following equation. min 2110112 1 (3) s.t. f(s(i), t(i)) − f(s(i), t) &gt; cost(t(i), t) Vt E T (s(i)) Here, f(s, t) is the feature function of a translation, and cost function cost(t(i), t) measures the translation errors of a candidate translation t comparing with a reference translation t(i). We define the cost function via the widely-used translation evaluation metric BLEU. We use the smoothed sentence level BLEU-4 (Lin and Och, 2004) here: cost(t(i), t) = 1 − BLEU-4(t(i), t) (4) In Section 3.1, we will discuss how we use the scoring function f(s, t, d) to calculate f(s, t). Then in Section 3.2, we recast the equation (3) as an unconstrained empirical loss minimization problem, and describe the learning algorithm for optimizing 0 and inducing G. Finally, we give the details of inference for the learning algorithm in Section 3.3. 3.1 Integrate Out Derivation by Averaging Although we only model the triple (s, t, d) in the equation (1), it’s necessary to calculate the scoring function f(s, t) of a translation by integrating o</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Orange: a method for evaluating automatic evaluation metrics for machine translation. In Pro. Coling 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP</booktitle>
<contexts>
<context position="2049" citStr="Marcu and Wong, 2002" startWordPosition="278" endWordPosition="281">s), most of them still learn translation rules via a pipeline with word-based heuristics (Koehn et al., 2003). This pipeline first builds word alignments using heuristic combination strategies, then heuristically extracts rules that are consistent with word alignments. Such heuristic pipeline *Corresponding author is not elegant theoretically. It brings an undesirable gap that separates modeling and learning in an SMT system. Therefore, researchers have proposed alternative approaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012) or discriminative models (Xiao et al., 2012). Theoretically, these approaches describe how sentence pairs are generated by applying sequences of synchronous rules in an elegant way. However, they learn synchronous grammars by maximizing likelihood,1 which only has a loose relation to translation quality (He and Deng, 2012). Moreover, generative models are normally hard to be extended to incorporate useful features, and the discriminative sy</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proc. EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan May</author>
<author>Kevin Knight</author>
</authors>
<title>Syntactic realignment models for machine translation.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP</booktitle>
<contexts>
<context position="30630" citStr="May and Knight, 2007" startWordPosition="5063" endWordPosition="5066">et parse structures, are helpful for grammar induction. This further confirms the advance of the max-margin estimation, as it provides us a convenient way to use non-local features. 262 6 Related Work As the synchronous grammar is the key component in SMT systems, researchers have proposed various methods to improve the quality of grammars. In addition to the generative and discriminative models introduced in Section 1, researchers also have made efforts on word alignment and grammar weight rescoring. The first line is to modify word alignment by exploring information of syntactic structures (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011). Such syntactic information is combined with word alignment via a discriminative framework. These methods prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. Yet another line is to rescore the weights of translation rules. This line of work tries to improve the relative frequency estimation used in the traditional pipeline. They rescore the weights or probabilities of extracted rules. Th</context>
</contexts>
<marker>May, Knight, 2007</marker>
<rawString>Jonathan May and Kevin Knight. 2007. Syntactic realignment models for machine translation. In Proc. EMNLP 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
<author>Shinsuke Mori</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>An unsupervised model for joint phrase alignment and extraction.</title>
<date>2011</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="2179" citStr="Neubig et al., 2011" startWordPosition="303" endWordPosition="306">builds word alignments using heuristic combination strategies, then heuristically extracts rules that are consistent with word alignments. Such heuristic pipeline *Corresponding author is not elegant theoretically. It brings an undesirable gap that separates modeling and learning in an SMT system. Therefore, researchers have proposed alternative approaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012) or discriminative models (Xiao et al., 2012). Theoretically, these approaches describe how sentence pairs are generated by applying sequences of synchronous rules in an elegant way. However, they learn synchronous grammars by maximizing likelihood,1 which only has a loose relation to translation quality (He and Deng, 2012). Moreover, generative models are normally hard to be extended to incorporate useful features, and the discriminative synchronous grammar induction model proposed by Xiao et al. (2012) only incorporates local features defined on parse trees of the so</context>
</contexts>
<marker>Neubig, Watanabe, Sumita, Mori, Kawahara, 2011</marker>
<rawString>Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shinsuke Mori, and Tatsuya Kawahara. 2011. An unsupervised model for joint phrase alignment and extraction. In Proc. ACL 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="25350" citStr="Och, 2003" startWordPosition="4244" endWordPosition="4245">trained by SRILM toolkit (Stolcke, 2002). The monolingual training data includes the Xinhua section of the English Gigaword corpus and the English side of the entire LDC data (432 million words). We used the NIST 2002 (MT02) as our development set, and the NIST 2003-2005 (MT03-05) as the test set. Case-insensitive NIST BLEU-4 (Papineni et al., 2002) is used to measure translation performance, and also the cost function in the max-margin estimation. Statistical significance in BLEU differences was tested by paired bootstrap re-sampling (Koehn, 2004). We used minimum error rate training (MERT) (Och, 2003) to optimize feature weights for the traditional log-linear model. We used the same decoder as the baseline system in all estimation methods. Without special explanation, we used the same features as those in the traditional pipeline: forward and backward translation probabilities, forward and backward lexical weights, count of extracted rules, count of glue rules, length of translation, and language model. For the lexical weights we used the noisy-or in all configurations including the baseline system. For the discriminative grammar induction, rule translation probabilities were calculated us</context>
<context position="31729" citStr="Och, 2003" startWordPosition="5235" endWordPosition="5236"> estimation used in the traditional pipeline. They rescore the weights or probabilities of extracted rules. The rescoring is done by using the similar latent log-linear model as ours (Blunsom et al., 2008; K¨a¨ari¨ainen, 2009; He and Deng, 2012), or incorporating various features using labeled word aligned bilingual data (Huang and Xiang, 2010). However, in rescoring, translation rules are still extracted by the heuristic two-step pipeline. Therefore these previous work still suffers from the inelegance problem of the traditional pipeline. Our work also relates to the discriminative training (Och, 2003; Watanabe et al., 2007; Chiang et al., 2009; Xiao et al., 2011; Gimpel and Smith, 2012) that has been widely used in SMT systems. Notably, these discriminative training methods are not used to learn grammar. Instead, they assume that grammar are extracted by the traditional two-step pipeline. 7 Conclusion In this paper we have presented a max-margin estimation for discriminative synchronous grammar induction. By associating the margin with the translation quality, we directly learn translation rules that optimize the translation performance measured by BLEU. Max-margin estimation also provide</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="3179" citStr="Papineni et al., 2002" startWordPosition="453" endWordPosition="456">s are normally hard to be extended to incorporate useful features, and the discriminative synchronous grammar induction model proposed by Xiao et al. (2012) only incorporates local features defined on parse trees of the source language. Nonlocal features, which encode information from parse trees of the target language, have never been exploited before due to the computational complexity of normalization in max-likelihood estimation. Consequently, we would like to learn synchronous grammars in a discriminative way that can directly maximize the end-to-end translation quality measured by BLEU (Papineni et al., 2002), and is also able to incorporate non-local features from target parse trees. We thus propose a max-margin estimation method &apos;More precisely, the discriminative model by Xiao et al. (2012) maximizes conditional likelihood. 255 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 255–264, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics to discriminatively induce synchronous grammar directly from sentence pairs without word alignments. We try to maximize the margin between a reference translation and a candid</context>
<context position="25091" citStr="Papineni et al., 2002" startWordPosition="4202" endWordPosition="4205">s that are defined on the target side. 5.1 Setup The baseline system is the hierarchical phrase based system (Chiang, 2007). We used a bilingual corpus that contains 1.1M sentences (44.6 million words) of up to length 40 from the LDC data.3 Our 5-gram language model was trained by SRILM toolkit (Stolcke, 2002). The monolingual training data includes the Xinhua section of the English Gigaword corpus and the English side of the entire LDC data (432 million words). We used the NIST 2002 (MT02) as our development set, and the NIST 2003-2005 (MT03-05) as the test set. Case-insensitive NIST BLEU-4 (Papineni et al., 2002) is used to measure translation performance, and also the cost function in the max-margin estimation. Statistical significance in BLEU differences was tested by paired bootstrap re-sampling (Koehn, 2004). We used minimum error rate training (MERT) (Och, 2003) to optimize feature weights for the traditional log-linear model. We used the same decoder as the baseline system in all estimation methods. Without special explanation, we used the same features as those in the traditional pipeline: forward and backward translation probabilities, forward and backward lexical weights, count of extracted r</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
<author>David Chiang</author>
<author>Kevin Knight</author>
</authors>
<title>Unsupervised syntactic alignment with inversion transduction grammars.</title>
<date>2010</date>
<booktitle>In Proc. NAACL</booktitle>
<contexts>
<context position="30674" citStr="Pauls et al., 2010" startWordPosition="5071" endWordPosition="5074">nduction. This further confirms the advance of the max-margin estimation, as it provides us a convenient way to use non-local features. 262 6 Related Work As the synchronous grammar is the key component in SMT systems, researchers have proposed various methods to improve the quality of grammars. In addition to the generative and discriminative models introduced in Section 1, researchers also have made efforts on word alignment and grammar weight rescoring. The first line is to modify word alignment by exploring information of syntactic structures (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011). Such syntactic information is combined with word alignment via a discriminative framework. These methods prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. Yet another line is to rescore the weights of translation rules. This line of work tries to improve the relative frequency estimation used in the traditional pipeline. They rescore the weights or probabilities of extracted rules. The rescoring is done by using the similar lat</context>
</contexts>
<marker>Pauls, Klein, Chiang, Knight, 2010</marker>
<rawString>Adam Pauls, Dan Klein, David Chiang, and Kevin Knight. 2010. Unsupervised syntactic alignment with inversion transduction grammars. In Proc. NAACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Riesa</author>
<author>Ann Irvine</author>
<author>Daniel Marcu</author>
</authors>
<title>Feature-rich language-independent syntax-based alignment for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proc. EMNLP</booktitle>
<contexts>
<context position="30717" citStr="Riesa et al., 2011" startWordPosition="5079" endWordPosition="5082">e of the max-margin estimation, as it provides us a convenient way to use non-local features. 262 6 Related Work As the synchronous grammar is the key component in SMT systems, researchers have proposed various methods to improve the quality of grammars. In addition to the generative and discriminative models introduced in Section 1, researchers also have made efforts on word alignment and grammar weight rescoring. The first line is to modify word alignment by exploring information of syntactic structures (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011). Such syntactic information is combined with word alignment via a discriminative framework. These methods prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. Yet another line is to rescore the weights of translation rules. This line of work tries to improve the relative frequency estimation used in the traditional pipeline. They rescore the weights or probabilities of extracted rules. The rescoring is done by using the similar latent log-linear model as ours (Blunsom et al</context>
</contexts>
<marker>Riesa, Irvine, Marcu, 2011</marker>
<rawString>Jason Riesa, Ann Irvine, and Daniel Marcu. 2011. Feature-rich language-independent syntax-based alignment for statistical machine translation. In Proc. EMNLP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
<author>Nathan Srebro</author>
</authors>
<title>Pegasos: Primal estimated sub-gradient solver for svm.</title>
<date>2007</date>
<booktitle>In Proc. ICML</booktitle>
<contexts>
<context position="14079" citStr="Shalev-Shwartz et al., 2007" startWordPosition="2299" endWordPosition="2302">2 + 1 N n=1 Where A denotes the regularization strength for L2-norm. The loss function of a sentence pair L(s(i), t(i), 0) is a convex hinge loss function denoted by: max{0, −f(s(i), t(i)) (7) ( ) + max f(s(i),t) + cost(t(i),t) 1 tET (s��)) According to the second max-operator in the hinge loss function, the optimization towards BLEU is expressed by cost-augmented inference. Costaugmented inference finds a translation that has a maximum model score augmented with cost. ( ) t� = max f(s(i), t) + cost(t(i), t) (8) tET (s��)) We applied the Pegasos algorithm for the optimization of equation (6) (Shalev-Shwartz et al., 2007). This is an online algorithm, which alternates between stochastic gradient descent steps and projection steps. When the loss function is non-zero, it updates weights according to the sub-gradient of the hinge loss function. Using the average scoring function in the equation (5), the sub-gradient of hinge loss function for a sentence pair is the difference of average feature values between a Viterbi translation 1 f(s, t) = ∑ f(s, t, d) (5) |D(s, t)| dED(s,t) 258 Algorithm 1 UPDATE(s, t, θ, G) ◃ One step in online algorithm. s, t are short for s(i), t(i) here 1: R(s, t) +— BIPARSE(s, t, B) &gt; Bu</context>
</contexts>
<marker>Shalev-Shwartz, Singer, Srebro, 2007</marker>
<rawString>Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. 2007. Pegasos: Primal estimated sub-gradient solver for svm. In Proc. ICML 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Ralph Weischedel</author>
</authors>
<title>Effective use of linguistic and contextual information for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. EMNLP</booktitle>
<marker>Shen, Xu, Zhang, Matsoukas, Weischedel, 2009</marker>
<rawString>Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas, and Ralph Weischedel. 2009. Effective use of linguistic and contextual information for statistical machine translation. In Proc. EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm – an extensible language modeling toolkit.</title>
<date>2002</date>
<contexts>
<context position="24780" citStr="Stolcke, 2002" startWordPosition="4150" endWordPosition="4152">peline on a large bitext which contains 1.1 million sentences. We then present a detailed comparison on a smaller dataset, in order to analyze the effectiveness of max-margin estimation comparing with the max likelihood estimation (Xiao et al., 2012), and also the effectiveness of the non-local features that are defined on the target side. 5.1 Setup The baseline system is the hierarchical phrase based system (Chiang, 2007). We used a bilingual corpus that contains 1.1M sentences (44.6 million words) of up to length 40 from the LDC data.3 Our 5-gram language model was trained by SRILM toolkit (Stolcke, 2002). The monolingual training data includes the Xinhua section of the English Gigaword corpus and the English side of the entire LDC data (432 million words). We used the NIST 2002 (MT02) as our development set, and the NIST 2003-2005 (MT03-05) as the test set. Case-insensitive NIST BLEU-4 (Papineni et al., 2002) is used to measure translation performance, and also the cost function in the max-margin estimation. Statistical significance in BLEU differences was tested by paired bootstrap re-sampling (Koehn, 2004). We used minimum error rate training (MERT) (Och, 2003) to optimize feature weights f</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm – an extensible language modeling toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Dan Klein</author>
<author>Mike Collins</author>
<author>Daphne Koller</author>
<author>Christopher Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In Proc. EMNLP</booktitle>
<contexts>
<context position="10661" citStr="Taskar et al., 2004" startWordPosition="1712" endWordPosition="1715">rence translation when updating feature weights. Therefore, the defined non-local features allow us to not only explore useful knowledge on the target parse trees, but also compute them efficiently over D(s, t) during maxmargin estimation. 3 Max-Margin Estimation In this section, we describe how we use a parallel training corpus {S, T} _ {(s(i), t(i))}Ni_1 to estimate feature weights θ, which contain parameters of the induced synchronous grammars and the defined non-local features. We choose the parameters that maximize the translation quality measured by BLEU using the max-margin estimation (Taskar et al., 2004). Margin refers to the difference of the model score between a reference translation t(i) and a candidate translation t. We hope that the worse the translation quality of t, the larger the margin between t and t(i). In this way, we penalize larger translation Ѯ㹼 Պ䈸 о [1,5] [1,6] Պ䈸 ∑ ϕ(r, s) ∑ -b(s, t, d) _ |{z } + rEd local rEd 257 errors more severely than smaller ones. This intuition is expressed by the following equation. min 2110112 1 (3) s.t. f(s(i), t(i)) − f(s(i), t) &gt; cost(t(i), t) Vt E T (s(i)) Here, f(s, t) is the feature function of a translation, and cost function cost(t(i), t) me</context>
<context position="21591" citStr="Taskar et al. (2004)" startWordPosition="3607" endWordPosition="3610">e used, we do not constrain the derivation space of a sentence pair by prefixed word alignment, and do not require any heuristic alignment combination strategy. Length feature We integrate the length of target translation that is used in traditional SMT system as our feature. Source span boundary features We use this kind of feature to assess the source parse tree in a derivation. Previous work (Xiong et al., 2010) has shown the importance of phrase boundary features for translation. Actually, this kind of feature is a good cue for deciding the boundary where a rule is to be learnt. Following Taskar et al. (2004), for a bispan [i, j, k, l] in a derivation, we define the feature templates that indicates the boundaries of a span by its beginning and end words: {B : si+1; E : sj; BE : si+1, sj}. Source span orientation features Orientation features are only used for those spans that are swapping. In Figure 1, the translation of source span [1, 3] is swapping with that of span [4, 5] by r2, thus orientation feature for span [1, 3] is activated. We also define three feature templates for a swapping span similar to the boundary features: {B : si+1; E : sj; BE : si+1, sj}. In practice, we add a prefix to the</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and Christopher Manning. 2004. Max-margin parsing. In Proc. EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP-CoNLL</booktitle>
<contexts>
<context position="31752" citStr="Watanabe et al., 2007" startWordPosition="5237" endWordPosition="5240"> used in the traditional pipeline. They rescore the weights or probabilities of extracted rules. The rescoring is done by using the similar latent log-linear model as ours (Blunsom et al., 2008; K¨a¨ari¨ainen, 2009; He and Deng, 2012), or incorporating various features using labeled word aligned bilingual data (Huang and Xiang, 2010). However, in rescoring, translation rules are still extracted by the heuristic two-step pipeline. Therefore these previous work still suffers from the inelegance problem of the traditional pipeline. Our work also relates to the discriminative training (Och, 2003; Watanabe et al., 2007; Chiang et al., 2009; Xiao et al., 2011; Gimpel and Smith, 2012) that has been widely used in SMT systems. Notably, these discriminative training methods are not used to learn grammar. Instead, they assume that grammar are extracted by the traditional two-step pipeline. 7 Conclusion In this paper we have presented a max-margin estimation for discriminative synchronous grammar induction. By associating the margin with the translation quality, we directly learn translation rules that optimize the translation performance measured by BLEU. Max-margin estimation also provides us a convenient way t</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proc. EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinyan Xiao</author>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Fast generation of translation forest for largescale smt discriminative training.</title>
<date>2011</date>
<booktitle>In Proc. EMNLP</booktitle>
<contexts>
<context position="31792" citStr="Xiao et al., 2011" startWordPosition="5245" endWordPosition="5248">ore the weights or probabilities of extracted rules. The rescoring is done by using the similar latent log-linear model as ours (Blunsom et al., 2008; K¨a¨ari¨ainen, 2009; He and Deng, 2012), or incorporating various features using labeled word aligned bilingual data (Huang and Xiang, 2010). However, in rescoring, translation rules are still extracted by the heuristic two-step pipeline. Therefore these previous work still suffers from the inelegance problem of the traditional pipeline. Our work also relates to the discriminative training (Och, 2003; Watanabe et al., 2007; Chiang et al., 2009; Xiao et al., 2011; Gimpel and Smith, 2012) that has been widely used in SMT systems. Notably, these discriminative training methods are not used to learn grammar. Instead, they assume that grammar are extracted by the traditional two-step pipeline. 7 Conclusion In this paper we have presented a max-margin estimation for discriminative synchronous grammar induction. By associating the margin with the translation quality, we directly learn translation rules that optimize the translation performance measured by BLEU. Max-margin estimation also provides us a convenient way to incorporate non-local features. Experi</context>
</contexts>
<marker>Xiao, Liu, Liu, Lin, 2011</marker>
<rawString>Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun Lin. 2011. Fast generation of translation forest for largescale smt discriminative training. In Proc. EMNLP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinyan Xiao</author>
<author>Deyi Xiong</author>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Unsupervised discriminative induction of synchronous grammar for machine translation.</title>
<date>2012</date>
<booktitle>In Proc. Coling</booktitle>
<contexts>
<context position="2249" citStr="Xiao et al., 2012" startWordPosition="314" endWordPosition="317">ristically extracts rules that are consistent with word alignments. Such heuristic pipeline *Corresponding author is not elegant theoretically. It brings an undesirable gap that separates modeling and learning in an SMT system. Therefore, researchers have proposed alternative approaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012) or discriminative models (Xiao et al., 2012). Theoretically, these approaches describe how sentence pairs are generated by applying sequences of synchronous rules in an elegant way. However, they learn synchronous grammars by maximizing likelihood,1 which only has a loose relation to translation quality (He and Deng, 2012). Moreover, generative models are normally hard to be extended to incorporate useful features, and the discriminative synchronous grammar induction model proposed by Xiao et al. (2012) only incorporates local features defined on parse trees of the source language. Nonlocal features, which encode information from parse </context>
<context position="18145" citStr="Xiao et al., 2012" startWordPosition="2995" endWordPosition="2998">t derivation as the Viterbi translation. In order to obtain the BLEU score in the cost function, we need to calculate the ngram precision. It is calculated in a way similar to the calculation of the ngram language model. The computation of BLEU-4 requires to record 3 boundary words in both the left and right side during dynamic programming. Therefore, even when we use a language model whose order is less than 4, we still expands the states to record 3 boundary words so as to calculate the cost measured by BLEU. We build synchronous hypergraphs using the cube-pruning based biparsing algorithm (Xiao et al., 2012). Algorithm 2 shows the procedure. Using a chart, the biparsing algorithm constructs k-best alignments for every source word (lines 1-5) and kbest hyperedges for every source span (lines 6-13) from the bottom up. Thus, a synchronous hypergraph is generated during the construction of the chart. More specifically, for a source span , it first creates cubes G for all source parses γ that are in259 Algorithm 2 BIPARSE(s, t, θ) ◃ (Xiao et al., 2012) D Create k-best alignments for each source word 1: for i ← 1,.., |s |do 2: for j ← 1,.., |t |do 3: Lj ← {E, tj} &gt; si aligns to tj or not 4: L ← ⟨L1, ..</context>
<context position="24416" citStr="Xiao et al., 2012" startWordPosition="4088" endWordPosition="4091">guage model which is an important component in SMT. For efficiency, we use a 3-gram language model trained on the target side of our training data during the induction of synchronous grammars. 5 Experiment In this section, we present our experiments on the NIST Chinese-to-English translation tasks. We first compare our max-margin based method with the traditional pipeline on a large bitext which contains 1.1 million sentences. We then present a detailed comparison on a smaller dataset, in order to analyze the effectiveness of max-margin estimation comparing with the max likelihood estimation (Xiao et al., 2012), and also the effectiveness of the non-local features that are defined on the target side. 5.1 Setup The baseline system is the hierarchical phrase based system (Chiang, 2007). We used a bilingual corpus that contains 1.1M sentences (44.6 million words) of up to length 40 from the LDC data.3 Our 5-gram language model was trained by SRILM toolkit (Stolcke, 2002). The monolingual training data includes the Xinhua section of the English Gigaword corpus and the English side of the entire LDC data (432 million words). We used the NIST 2002 (MT02) as our development set, and the NIST 2003-2005 (MT0</context>
<context position="28142" citStr="Xiao et al. (2012)" startWordPosition="4672" endWordPosition="4675">he initial phrase to be tightly consistent with word alignment. Therefore, Moses extract a much larger rule table than that of our baseline. With fewer translation rules, our method obtains an average improvement of +0.96 BLEU points on the three test sets over the Baseline. As the difference between the baseline and our max-margin synchronous grammar induction model only lies in the grammar, this result clearly denotes that our learnt grammar does outperform the grammar extracted by the traditional two-step pipeline. We also incorporate the sparse features during decoding in a way similar to Xiao et al. (2012) and Dyer et al. (2011). In order to optimize these sparse features with the dense features by MERT, we group features of the same type into one coarse “summary feature”, and get three such features including: rule, phrase-boundary and phrase orientation features. In this way, we rescale the weights of the three “summary features” with the 8 dense features by MERT. We achieve a further improvement of +0.37 BLEU points. Therefore, our training algorithm is able to learn the useful information encoded by the sparse features for translation. 5.3 Comparison of Estimation Objective and Non-Local Fe</context>
</contexts>
<marker>Xiao, Xiong, Liu, Liu, Lin, 2012</marker>
<rawString>Xinyan Xiao, Deyi Xiong, Yang Liu, Qun Liu, and Shouxun Lin. 2012. Unsupervised discriminative induction of synchronous grammar for machine translation. In Proc. Coling 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Learning translation boundaries for phrase-based decoding. In</title>
<date>2010</date>
<booktitle>Proc. NAACL2010.</booktitle>
<contexts>
<context position="21389" citStr="Xiong et al., 2010" startWordPosition="3572" endWordPosition="3575"> initial weight of these two lexical scores with equivalent positive values. The lexical weights enable our system to score and rank the hyperedges at the beginning. Although word alignment features are used, we do not constrain the derivation space of a sentence pair by prefixed word alignment, and do not require any heuristic alignment combination strategy. Length feature We integrate the length of target translation that is used in traditional SMT system as our feature. Source span boundary features We use this kind of feature to assess the source parse tree in a derivation. Previous work (Xiong et al., 2010) has shown the importance of phrase boundary features for translation. Actually, this kind of feature is a good cue for deciding the boundary where a rule is to be learnt. Following Taskar et al. (2004), for a bispan [i, j, k, l] in a derivation, we define the feature templates that indicates the boundaries of a span by its beginning and end words: {B : si+1; E : sj; BE : si+1, sj}. Source span orientation features Orientation features are only used for those spans that are swapping. In Figure 1, the translation of source span [1, 3] is swapping with that of span [4, 5] by r2, thus orientation</context>
</contexts>
<marker>Xiong, Zhang, Li, 2010</marker>
<rawString>Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learning translation boundaries for phrase-based decoding. In Proc. NAACL2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Improvements in phrase-based statistical machine translation.</title>
<date>2004</date>
<booktitle>In Prob. NAACL</booktitle>
<contexts>
<context position="20672" citStr="Zens and Ney, 2004" startWordPosition="3455" endWordPosition="3458">antage of the discriminative method is that it enables us to incorporate arbitrary features. As shown in Section 2, our model incorporates both local and non-local features. 4.1 Local Features Rule features We associate each rule with an indicator feature. Each indicator feature counts the number of times that a rule appears in a derivation. In this way, we are able to learn a weight for every rule according to the entire structure of sentence. Word association features Lexicalized features are widely used in traditional SMT systems. Here we adopt two lexical weights called noisy-or features (Zens and Ney, 2004). The noisy-or feature is estimated by word translation probabilities output by GIZA++. We set the initial weight of these two lexical scores with equivalent positive values. The lexical weights enable our system to score and rank the hyperedges at the beginning. Although word alignment features are used, we do not constrain the derivation space of a sentence pair by prefixed word alignment, and do not require any heuristic alignment combination strategy. Length feature We integrate the length of target translation that is used in traditional SMT system as our feature. Source span boundary fea</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>Richard Zens and Hermann Ney. 2004. Improvements in phrase-based statistical machine translation. In Prob. NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Chris Quirk</author>
<author>Robert C Moore</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of noncompositional phrases with synchronous parsing.</title>
<date>2008</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="2091" citStr="Zhang et al., 2008" startWordPosition="286" endWordPosition="289">es via a pipeline with word-based heuristics (Koehn et al., 2003). This pipeline first builds word alignments using heuristic combination strategies, then heuristically extracts rules that are consistent with word alignments. Such heuristic pipeline *Corresponding author is not elegant theoretically. It brings an undesirable gap that separates modeling and learning in an SMT system. Therefore, researchers have proposed alternative approaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012) or discriminative models (Xiao et al., 2012). Theoretically, these approaches describe how sentence pairs are generated by applying sequences of synchronous rules in an elegant way. However, they learn synchronous grammars by maximizing likelihood,1 which only has a loose relation to translation quality (He and Deng, 2012). Moreover, generative models are normally hard to be extended to incorporate useful features, and the discriminative synchronous grammar induction model proposed</context>
</contexts>
<marker>Zhang, Quirk, Moore, Gildea, 2008</marker>
<rawString>Hao Zhang, Chris Quirk, Robert C. Moore, and Daniel Gildea. 2008. Bayesian learning of noncompositional phrases with synchronous parsing. In Proc. ACL 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
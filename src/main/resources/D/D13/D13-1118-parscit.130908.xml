<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.96195">
Improvements to the Bayesian Topic N-gram Models
</title>
<author confidence="0.854959">
Hiroshi Noji†$ Daichi Mochihashi†* Yusuke Miyao†$
</author>
<email confidence="0.824088">
noji@nii.ac.jp daichi@ism.ac.jp yusuke@nii.ac.jp
</email>
<affiliation confidence="0.993763333333333">
†Graduate University for Advanced Studies
$National Institute of Informatics, Tokyo, Japan
*The Institute of Statistical Mathematics, Tokyo, Japan
</affiliation>
<sectionHeader confidence="0.989262" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999530625">
One of the language phenomena that n-gram
language model fails to capture is the topic in-
formation of a given situation. We advance the
previous study of the Bayesian topic language
model by Wallach (2006) in two directions:
one, investigating new priors to alleviate the
sparseness problem caused by dividing all n-
grams into exclusive topics, and two, develop-
ing a novel Gibbs sampler that enables moving
multiple n-grams across different documents
to another topic. Our blocked sampler can
efficiently search for higher probability space
even with higher order n-grams. In terms of
modeling assumption, we found it is effective
to assign a topic to only some parts of a docu-
ment.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992725862069">
N-gram language model is still ubiquitous in NLP,
but due to its simplicity it fails to capture some im-
portant aspects of language, such as difference of
word usage in different situations, sentence level
syntactic correctness, and so on. Toward language
model that can consider such a more global con-
text, many extensions have been proposed from
lexical pattern adaptation, e.g., adding cache (Je-
linek et al., 1991) or topic information (Gildea and
Hofmann, 1999; Wallach, 2006), to grammaticality
aware models (Pauls and Klein, 2012).
Topic language models are important for use in
e.g., unsupervised language model adaptation: we
want a language model that can adapt to the do-
main or topic of the current situation (e.g., a doc-
ument in SMT or a conversation in ASR) automat-
ically and select the appropriate words using both
topic and syntactic context. Wallach (2006) is one
such model, which generate each word based on lo-
cal context and global topic information to capture
the difference of lexical usage among different top-
ics.
However, Wallach’s experiments were limited to
bigrams, a toy setting for language models, and ex-
periments with higher-order n-grams have not yet
been sufficiently studied, which we investigate in
this paper. In particular, we point out the two funda-
mental problems caused when extending Wallach’s
model to a higher-order: sparseness caused by di-
viding all n-grams into exclusive topics, and local
minima caused by the deep hierarchy of the model.
On resolving these problems, we make several con-
tributions to both computational linguistics and ma-
chine learning.
To address the first problem, we investigate incor-
porating a global language model for ease of sparse-
ness, along with some priors on a suffix tree to cap-
ture the difference of topicality for each context,
which include an unsupervised extension of the dou-
bly hierarchical Pitman-Yor language model (Wood
and Teh, 2009), a Bayesian generative model for su-
pervised language model adaptation. For the sec-
ond inference problem, we develop a novel blocked
Gibbs sampler. When the number of topics is K
and vocabulary size is V , n-gram topic model has
O(KVn) parameters, which grow exponentially to
n, making the local minima problem even more se-
vere. Our sampler resolves this problem by moving
many customers in the hierarchical Chinese restau-
rant process at a time.
We evaluate various models by incremental cal-
culation of test document perplexity on 3 types of
corpora having different size and diversity. By com-
bining the proposed prior and the sampling method,
our Bayesian model achieve much higher accura-
cies than the naive extension of Wallach (2006) and
shows results competitive with the unigram rescal-
ing (Gildea and Hofmann, 1999), which require
</bodyText>
<page confidence="0.930271">
1180
</page>
<note confidence="0.73473">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1180–1190,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.972959">
huge computational cost at prediction, with much
faster prediction time.
</bodyText>
<sectionHeader confidence="0.993325" genericHeader="method">
2 Basic Models
</sectionHeader>
<bodyText confidence="0.998754333333333">
All models presented in this paper are based on the
Bayesian n-gram language model, the hierarchical
Pitman-Yor process language model (HPYLM). In
the following, we first introduce the HPYLM, and
then discuss the topic model extension of Wallach
(2006) with HPYLM.
</bodyText>
<subsectionHeader confidence="0.609225">
2.1 HPYLM
</subsectionHeader>
<bodyText confidence="0.999812888888889">
Let us first define some notations. W is a vocabulary
set, V = |W  |is the size of that set, and u, v, w E W
represent the word type.
The HPYLM is a Bayesian treatment of the n-
gram language model. The generative story starts
with the unigram word distribution Gφ, which is
a V-dimensional multinomial where Gφ(w) repre-
sents the probability of word w. The model first
generates this distribution from the PYP as Gφ —
PYP(a, b, G0), where G0 is a V-dimensional uni-
form distribution (G0(u) = V1;Vu E W) and
acts as a prior for Gφ and a, b are hyperparameters
called discount and concentration, respectively. It
then generates all bigram distributions {GuIuEW as
Gu — PYP(a, b, Gφ). Given this distributions, it
successively generates 3-gram distributions Guv —
PYP(a, b, Gu) for all (u, v) E W2 pairs, which
encode a natural assumption that contexts having
common suffix have similar word distributions. For
example, two contexts “he is” and “she is”, which
share the suffix “is”, are generated from the same
(bigram) distribution Gig, so they would have simi-
lar word distributions. This process continues until
the context length reaches n — 1 where n is a pre-
specified n-gram order (if n = 3, the above example
is a complete process). We often generalize this pro-
cess using two contexts h and h&apos; as
</bodyText>
<equation confidence="0.931711">
Gh — PYP(a, b, Gh,), (1)
</equation>
<bodyText confidence="0.996278333333333">
where h = ah&apos;, in which a is a leftmost word of h.
We are interested in the posterior word distribu-
tion following a context h. Our training corpus w
is a collection of n-grams, from which we can cal-
culate the posterior p(w|h, w), which is often ex-
plained with the Chinese restaurant process (CRP):
</bodyText>
<equation confidence="0.993970666666667">
chw � athw
p(w|h, w) = +
ch· + b
</equation>
<bodyText confidence="0.9874977">
(2)
where chw is an observed count of n-gram hw called
customers, while thw is a hidden variable called ta-
bles. ch· and th· represents marginal counts: ch· =
Ew chw and th· = Ew thw. This form is very
similar to the well-known Kneser-Ney smoothing,
and actually the Kneser-Ney can be understood as a
heuristic approximation of the HPYLM. This char-
acteristic enables us to build the state-of-the-art lan-
guage model into a more complex generative model.
</bodyText>
<subsectionHeader confidence="0.931151">
2.2 Wallach (2006) with HPYLM
</subsectionHeader>
<bodyText confidence="0.989650115384615">
Wallach (2006) is a generative model for a docu-
ment collection that combines the topic model with
a Bayesian n-gram language model. The latent
Dirichlet allocation (LDA) (Blei et al., 2003) is the
most basic topic model, which generates each word
in a document based on a unigram word distribution
defined by a topic allocated to that word. The bi-
gram topic model of Wallach (2006) simply replaces
this unigram word distribution (a multinomial) for
each topic with a bigram word distribution 1. In
other words, ordinary LDA generates word condi-
tioning only on the latent topic, whereas the bigram
topic model generates conditioning on both the la-
tent topic and the previous word, as in the bigram
language model. Extending this model with a higher
order n-gram is trivial; all we have to do is to replace
the bigram language model for each topic with an n-
gram language model.
The formal description of the generative story of
this n-gram topic model is as follows. First, for
each topic k E 1, · · · , K, where K is the num-
ber of topics, the model generates an n-gram lan-
guage model Gkh.2 These n-gram models are gen-
erated by the PYP, so Gkh — PYP(a, b, Gkh,) holds.
The model then generate a document collection. For
each document j E 1, · · · , D, it generates a K-
</bodyText>
<footnote confidence="0.580739625">
1This is the model called prior 2 in Wallach (2006); it con-
sistently outperformed the other prior. Wallach used the Dirich-
let language model as each topic, but we only explore the model
with HPYLM because its superiority to the Dirichlet language
model has been well studied (Teh, 2006b).
2We sometimes denote Gh to represent a language model of
topic k, not a specific multinomial for some context h, depend-
ing on the context.
</footnote>
<equation confidence="0.957788">
ath· + bp(w|h&apos; &apos; w),
ch· + b
</equation>
<page confidence="0.955064">
1181
</page>
<bodyText confidence="0.9997978">
dimensional topic distribution Bj by a Dirichlet dis-
tribution Dir(α) where α = (a1, a2, · · · , aK) is a
prior. Finally, for each word position i ∈ 1, · · · , Nj
where Nj is the number of words in document j, i-
th word’s topic assignment zji is chosen according
</bodyText>
<equation confidence="0.704172">
to Bj, then a word type wji is generated from Gz�i
h�i
</equation>
<bodyText confidence="0.998049">
where hji is the last n − 1 words preceding wji. We
can summarize this process as follows:
</bodyText>
<listItem confidence="0.670237">
1. Generate topics:
</listItem>
<equation confidence="0.9964452">
For each h ∈ 0, {W}, · · · , {W}n−1:
For each k ∈ 1, · · · , K:
Gh ∼ PYP(a, b, Gh,)
2. Generate corpora:
For each document j ∈ 1, · · · D:
Bj ∼ Dir(α)
For each word position i ∈ 1, · · · , Nj:
zji ∼ Bj
zj i
wji∼Ghji
</equation>
<sectionHeader confidence="0.998839" genericHeader="method">
3 Extended Models
</sectionHeader>
<bodyText confidence="0.999971173913043">
One serious drawback of the n-gram topic model
presented in the previous section is sparseness. At
inference, as in LDA, we assign each n-gram a topic,
resulting in an exclusive clustering of n-grams in
the corpora. Roughly speaking, when the number
of topics is K and the number of all n-grams in the
training corpus is N, a language model of topic k,
Gkh is learned using only about O(N/K) instances
of the n-grams assigned the topic k, making each
Gkh much sparser and unreliable distribution.
One way to alleviate this problem is to place an-
other n-gram model, say G0h, which is shared with
all topic-specific n-gram models {Gkh}Kk=1. How-
ever, what is the best way to use this special distribu-
tion? We explore two different approaches to incor-
porate this distribution in the model presented in the
previous section. In one model, the HIERARCHICAL
model, G0h is used as a prior for all other n-gram
models, where G0h exploits global statistics across
all topics {Gkh}. In the other model, the SWITCH-
ING model, no statistics are shared across G0h and
{Gkh}, but some words are directly generated from
G0h regardless of the topic distribution.
</bodyText>
<subsectionHeader confidence="0.958405">
3.1 HIERARCHICAL Model
</subsectionHeader>
<bodyText confidence="0.999637333333333">
Informally, what we want to do is to establish hier-
archies among the global G0 h and other topics {Gkh}.
In Bayesian formalism, we can explain this using an
</bodyText>
<figureCaption confidence="0.8367154">
Figure 1: Variable dependencies of the HIERARCHICAL
model. {u, v} are word types, k is a topic and each Gh
is a multinomial word distribution. For example, Gam„
represents a word distribution following the context uv
in topic 2.
</figureCaption>
<bodyText confidence="0.9850665">
abstract distribution F as Gkh ∼ F(G0h). The prob-
lem here is making the appropriate choice for the
distribution F. Each topic word distribution already
has hierarchies among n − 1-gram and n-gram con-
texts as Gkh ∼ PYP(a, b, Gkh,). A natural solution
to this problem is the doubly hierarchical Pitman-
Yor process (DHPYP) proposed in Wood and Teh
(2009). Using this distribution, the new generative
process of Gkh is
Gkh ∼ PYP(a, b, AGkh, + (1 − A)G0h), (3)
where A is a new hyperparameter that determines
mixture weight. The dependencies among G0h and
{Gkh} are shown in Figure 1. Note that the genera-
tive process of G0h is the same as the HPYLM (1).
Let us clarify the DHPYP usage differences be-
tween our model and the previous work of Wood and
Teh (2009). A key difference is the problem setting:
Wood and Teh (2009) is aimed at the supervised
adaptation of a language model for a specific do-
main, whereas our goal is unsupervised adaptation.
In Wood and Teh (2009), each Gkh fork ∈ 1, 2, · · ·
corresponds to a language model of a specific do-
main and the training corpus for each k is pre-
specified and fixed. For ease of data sparseness of
domain-specific corpora, latent model G0h exploits
shared statistics among Gkh for k = 1, 2, · · · . In con-
trast, with our model, each Gkhis a topic, so it must
perform the clustering of n-grams in addition to ex-
</bodyText>
<figure confidence="0.978926">
・・・
・・・
・・・
・・・
・・・
・・・
</figure>
<page confidence="0.988838">
1182
</page>
<bodyText confidence="0.9998888">
ploiting the latent G0h. This makes inference harder
and requires more careful design of λ.
Modeling of λ We can better understand the role
of λ in (3) by considering the posterior predictive
form corresponds to (2), which is written as
</bodyText>
<equation confidence="0.9911872">
k k
p(w|h,k,w) =chck+ bw+
h·
(4)
q(w|h, k, w) = λp(w|h&apos;, k, w) + (1 − λ)p(w|h, 0, w),
</equation>
<bodyText confidence="0.999961">
where c, t with superscript k corresponds to the
count existing in topic k. This shows us that λ de-
termines the back-off behavior: which probability
we should take into account: the shorter context of
the same topic Gkh, or the full context of the global
model G0h. Wood and Teh (2009) shares this vari-
able across all contexts of the same length, for each
k, but this assumption may not be the best. For ex-
ample, after the context “in order”, we can predict
the word “to” or “that”, and this tendency is unaf-
fected by the topic. We call this property of context
the topicality and say that “in order” has weak topi-
cality. Therefore, we place λ as a distinct value for
each context h, which we share across all topics. We
designate this λ determined by h λh in the follow-
ing. Moreover, similar contexts may have similar
values of λh. For example, the two contexts “of the”
and “in the”, which share the suffix “the”, both have
a strong topicality3. We encode this assumption by
placing hierarchical Beta distributions on the suffix
tree across all topics:
</bodyText>
<equation confidence="0.867906">
λh ∼ Beta(γλh,,γ(1 − λh,)) = DP(γ, λh,), (5)
</equation>
<bodyText confidence="0.985573846153846">
where DP is the hierarchical Dirichlet process (Teh
et al., 2006), which has only two atoms in {0,1} and
γ is a concentration parameter. As in HPYLM, we
place a uniform prior λ0 = 1/2 on the base distribu-
tion of the top node (λφ ∼ DP(γ, λ0)).
Having generated the topic component of the
model, the corpus generating process is the same as
the previous model because we only change the gen-
erating process of Gkh for k = 1, · · · , K.
3These words can be used very differently depending on the
context. For example, in a teen story, “in the room” or “in the
school” seems more dominant than “in the corpora” or “in the
topic”, which is likely to appear in this paper.
</bodyText>
<subsectionHeader confidence="0.964066">
3.2 SWITCHING Model
</subsectionHeader>
<bodyText confidence="0.999524411764706">
Our second extension also exploits the global G0h, al-
beit differently than the HIERARCHICAL model. In
this model, the relationship of G0 h to the other {Gkh}
is flat, not hierarchical: G0h is a special topic that can
generate a word. The model first generates each lan-
guage model of k = 0, 1, 2, · · · , K independently
as Gkh ∼ PYP(a, b, Gkh,). When generating a word,
it first determines whether to use global model G0h
or topic model {Gkh}Kk=1. Here, we use the λh in-
troduced above in a similar way: the probability of
selecting k = 0 for the next word is determined by
the previous context. This assumption seems natu-
ral; we expect the G0h to mainly generate common n-
grams, and the topicality of each context determines
how common that n-gram might be. The complete
generative process of this model is written as fol-
lows:
</bodyText>
<listItem confidence="0.725643">
1. Generate topics:
</listItem>
<equation confidence="0.974109076923077">
For each h ∈ φ, {V }, · · · , {V }n−1:
λh ∼ DP(γ, λ&apos; h)
For each k ∈ 0, · · · , K:
Gkh ∼ PYP(a, b, Gkh,)
2. Generate corpora:
For each document j ∈ 1, · · · D:
θj ∼ Dir(α)
For each word position i ∈ 1, · · · , Nj:
lji ∼ Bern(λhji)
If lji 0: zji = 0
If lji = 1: zji ∼ θj
wji ∼ Gzji
hji
</equation>
<bodyText confidence="0.9998725">
The difference between the two models is their
usage of the global model G0h. For a better under-
standing of this, we provide a comparison of their
graphical models in Figure 2.
</bodyText>
<sectionHeader confidence="0.999802" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.999987666666667">
For posterior inference, we use the collapsed Gibbs
sampler. In our models, all the latent variables are
{Gkh, λh, θj, z, O}, where z is the set of topic assign-
ments and O = {a, b, γ, α} are hyperparameters,
which are treated later. We collapse all multinomials
in the model, i.e., {Gkh, λh, θj}, in which Gkh and λh
are replaced with the Chinese restaurant process of
PYP and DP respectively. Given the training corpus
w, the target posterior distribution is p(z, S|w, O),
where S is the set of seating arrangements of all
restaurants. To distinguish the two types of restau-
rant, in the following, we refer the restaurant to indi-
</bodyText>
<figure confidence="0.95244025">
atkh· + b q(w|h, k, w),
ckh· + b
1183
(a) HIERARCHICAL (b) SWITCHING
</figure>
<figureCaption confidence="0.99927">
Figure 2: Graphical model representations of our two models in the case of a 3-gram model. Edges that only exist in
one model are colored.
</figureCaption>
<bodyText confidence="0.997662142857143">
cate the collapsed state of Gkh (PYP), while we refer
the restaurant of λh to indicates the collapsed state
of λh (DP). We present two different types of sam-
pler: a token-based sampler and a table-based sam-
pler. For both samplers, we first explain in the case
of our basic model (Section 2.2), and later discuss
some notes on our extended models.
</bodyText>
<subsectionHeader confidence="0.992603">
4.1 Token-based Sampler
</subsectionHeader>
<bodyText confidence="0.999971285714286">
The token-based sampler is almost identical to
the collapsed sampler of the LDA (Griffiths and
Steyvers, 2004). At each iteration, we consider the
following conditional distribution of zji given all
other topic assignments z−ji and S−ji, which is the
set of seating arrangements with a customer corre-
sponds to wji removed, as
</bodyText>
<equation confidence="0.453530833333333">
p(zji|z−ji, S−ji) a p(zji|z−ji)p(wji|zji, hji, S−ji),
(6)
where p(wji|zji, hji, S−ji) =
ck hw − atk hw + atk h· + b p(wji|zji, hji, S−ji) (7)
h· + b h· + b
ck ck
</equation>
<bodyText confidence="0.8526425">
is a predictive word probability under the topic zji,
and
</bodyText>
<equation confidence="0.994669333333333">
njk + αk
−ji
p(zji|z−ji) = Nj − 1 + Ek0 αk0 ,(8)
</equation>
<bodyText confidence="0.9919995625">
where n−ji
jk is the number of words that is assigned
topic k in document j excluding wji, which is the
same as the LDA. Given the sampled topic zji, we
update the language model of topic zji, by adding
customer wji to the restaurant specified by zji and
context hji. See Teh (2006a) for details of these cus-
tomer operations.
HIERARCHICAL Adding customer operation is
slightly changed: When a new table is added to a
restaurant, we must track the label l E f0, 1g indi-
cating the parent restaurant of that table, and add the
customer corresponding to l to the restaurant of λh.
See Wood and Teh (2009) for details of this opera-
tion.
SWITCHING We replace p(zji|z−ji) with
</bodyText>
<equation confidence="0.898853">
p(zji|z−ji) =
p(lji = 0|hji) (zji = 0)
n−ji
jk +αk
p(lji = 1|hji) �Ek,40 njkji+Ek0 αk0 (zji 0)
,
(9)
</equation>
<bodyText confidence="0.9965328">
where p(lji|hji) is a predictive of lji given by the
CRP of λhji. We need not assign a label to a new
table, but rather we always add a customer to the
restaurant of λh according to whether the sampled
topic is 0 or not.
</bodyText>
<subsectionHeader confidence="0.984826">
4.2 Table-based Sampler
</subsectionHeader>
<bodyText confidence="0.916388142857143">
One problem with the token-based sampler is that
the seating arrangement of the internal restaurant
would never be changed unless a new table is cre-
ated (or an old table is removed) in its child restau-
rant. This probability is very low, particularly in
the restaurants of shallow depth (e.g., unigram or
{
</bodyText>
<page confidence="0.794579">
1184
</page>
<figure confidence="0.818597">
Construct a block
</figure>
<figureCaption confidence="0.984986142857143">
Figure 3: Transition of the state of restaurants in the
table-based sampler when the number of topics is 2.
{u, v, w} are word types. Each box represents a restau-
rant where the type in the upper-right corner indicates the
context. In this case, we can change the topic of the three
3-grams (vvw, vvw, uvw) in some documents from 1 to
2 at the same time.
</figureCaption>
<bodyText confidence="0.841336117647059">
bigram restaurants) because these restaurants have
a larger number of customers and tables than those
of deep depth, leading to get stack in undesirable
local minima. For example, imagine a table in
the restaurant of context “hidden” (depth is 2) and
some topic, served “unit”. This table is connected
to tables in its child restaurants corresponding to
some 3-grams (e.g., “of hidden unit” or “train hid-
den unit”), whereas similar n-grams, such as those
of “of hidden units” or “train hidden units” might
be gathered in another topic, but collecting these n-
grams into the same topic might be difficult under
the token-based sampler. The table-based sampler
moves those different n-grams having common suf-
fixes jointly into another topic.
Figure 3 shows a transition of state by the table-
based sampler and Algorithm 4.2 depicts a high-
level description of one iteration. First, we select
a table in a restaurant, which is shown with a dotted
line in the figure. Next, we descend the tree to col-
lect the tables connected to the selected table, which
are pointed by arrows. Because this connection can-
not be preserved in common data structures for a
restaurant described in Teh (2006a) or Blunsom et
al. (2009), we select the child tables randomly. This
is correct because customers in CRP are exchange-
Algorithm 1 Table-based sampler
for all table in all restaurants do
Remove a customer from the parent restaurant.
Construct a block of seating arrangement S by de-
scending the tree recursively.
Sample topic assignment zS — p(zS|S, S−S, z−S).
Move S to sampled topic, and add a customer to the
parent restaurant of the first selected table.
</bodyText>
<subsectionHeader confidence="0.739285">
end for
</subsectionHeader>
<bodyText confidence="0.9998455">
able, so we can restore the parent-child relations ar-
bitrarily. We continue this process recursively until
reaching the leaf nodes, obtaining a block of seat-
ing arrangement S. After calculating the conditional
distribution, we sample new topic assignment for
this block. Finally, we move this block to the sam-
pled topic, which potentially changes the topic of
many words across different documents, which are
connected to customers in a block at leaf nodes (this
connection is also arbitrary).
Conditional distribution Let zs be the block of
topic assignments connected to S and zs be a vari-
able indicating the topic assignment. Thanks to the
exchangeability of all customers and tables in one
restaurant (Teh, 2006a), we can imagine that cus-
tomers and tables in S have been added to the restau-
rants last. We are interested in the following condi-
tional distribution: (conditioning O is omitted)
</bodyText>
<equation confidence="0.947272">
p(zS = k&apos;|S, S−S, z−S) a p(S|S−S, k&apos;)p(zS = k&apos;|z−S),
</equation>
<bodyText confidence="0.998302666666667">
where p(S|S−s, k0) is a product of customers’ ac-
tions moving to another topic, which can be decom-
posed as:
</bodyText>
<equation confidence="0.995003444444444">
p(S|S−�, k0) = p(w|k0, h) 11 p(s|k0) (10)
s∈s
p(s|k0) = IIi=0 (b+a(ths� s)+i)) rIjsi1(7−a) (11)
(b+ck0(−s)
hsw· )cs·
jli=O (b+a(ths s)+i))
a (12)
(b+ck0(−s)
hsw· )cs· .
</equation>
<bodyText confidence="0.9336937">
Let us define some notations used above. Each
s E S is a part of seating arrangements in a restau-
rant, there being ts tables, i-th of which with csi
customers, with hs as the corresponding context. A
restaurant of context h and topic k has thw tables
served dish w, i-th of which with chwi customers.
Superscripts −s indicate excluding the contribution
: customer
: table
Move the block to the sampled topic
</bodyText>
<page confidence="0.88748">
1185
</page>
<bodyText confidence="0.997968428571429">
of customers in s, and xn = x(x+1) · · · (x+n—1)
is the ascending factorial. In (10) p(w|k0, h) is the
parent distribution of the first selected table, and
the other p(s|k0) is the seating arrangement of cus-
tomers. The likelihood for changing topic assign-
ments across documents must also be considered,
which is p(zS = k0|z−S) and decomposed as:
</bodyText>
<equation confidence="0.997428666666667">
(n�S
jk� +αk�)nj(S)
(N S+Ek αk)nj (S) (13)
</equation>
<bodyText confidence="0.999914142857143">
where nj(5) is the number of word tokens con-
nected with 5 in document j.
HIERARCHICAL We skip tables on restaurants of
k = 0, because these tables are all from other topics
and we cannot construct a block. The effects of A
can be ignored because these are shared by all topics.
SWITCHING In the SWITCHING, p(zS = k0|z−S)
cannot be calculated in a closed form because
p(hi|hji) in (9) would be changed dynamically
when adding customers. This problem is the same
one addressed by Blunsom and Cohn (2011), and we
follow the same approximation in which, when we
calculate the probability, we fractionally add tables
and customers recursively.
</bodyText>
<subsectionHeader confidence="0.998241">
4.3 Inference of Hyperparameters
</subsectionHeader>
<bodyText confidence="0.998017666666667">
We also place a prior on each hyperparameter and
sample value from the posterior distribution for ev-
ery iteration. As in Teh (2006a), we set different
values of a and b for each depth of PYP, but share
across all topics and sample values with an auxiliary
variable method. We also set different value of -y for
each depth, on which we place Gamma(1,1). We
make the topic prior α asymmetric: α = Qα0; Q —
Gamma(1,1), α0 — Dir(1).
</bodyText>
<sectionHeader confidence="0.99997" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999737888888889">
HMM-LDA (Griffiths et al., 2005) is a composite
model of HMM and LDA that assumes the words
in a document are generated by HMM, where only
one state has a document-specific topic distribution.
Our SWITCHING model can be understood as a lex-
ical extension of HMM-LDA. It models the topical-
ity by context-specific binary random variables, not
by hidden states. Other n-gram topic models have
focused mainly on information retrieval. Wang et
</bodyText>
<table confidence="0.8151098">
min. training set test set
Corpus appear # types # docs # tokens # docs # tokens
Brown 4 19,759 470 1,157,225 30 70,795
NIPS 4 22,705 1500 5,088,786 50 167,730
BNC 10 33,071 6,162 12,783,130 100 202,994
</table>
<tableCaption confidence="0.9941924">
Table 1: Corpus statistics after the pre-processing: We
replace words appearing less than min.appear times in
training + test documents, or appearing only in a test set
with an unknown token. All numbers are replaced with
#, while punctuations are remained.
</tableCaption>
<bodyText confidence="0.999222466666667">
al. (2007) is a topic model on automatically seg-
mented chunks. Lindsey et al. (2012) extended this
model with the hierarchical Pitman-Yor prior. They
also used switching variables, but for a different pur-
pose: to determine the segmenting points. They treat
these variables completely independently, while our
model employs a hierarchical prior to share statisti-
cal strength among similar contexts.
Our primary interest is language model adapta-
tion, which has been studied mainly in the area of
speech processing. Conventionally, this adaptation
has relied on a heuristic combination of two sep-
arately trained models: an n-gram model p(w|h)
and a topic model p(w|d). The unigram rescal-
ing, which is a product model of these two mod-
els, perform better than more simpler models such
as linear interpolation (Gildea and Hofmann, 1999).
There are also some extensions to this method (Tam
and Schultz, 2009; Huang and Renals, 2008), but
these methods have one major drawback: at predic-
tion, the rescaling-based method requires normaliza-
tion across vocabulary at each word, which prohibits
use on applications requiring dynamic (incremental)
adaptation, e.g., settings where we have to update
the topic distribution as new inputs come in. Tam
and Schultz (2005) studied on this incremental set-
tings, but they employ an interpolation. The practi-
cal interest here is whether our Bayesian models can
rival the rescaling-based method in terms of predic-
tion power. We evaluate this in the next section.
</bodyText>
<sectionHeader confidence="0.998855" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.993057">
6.1 Settings
</subsectionHeader>
<bodyText confidence="0.999780666666667">
We test the effectiveness of presented models and
the blocked sampling method on unsupervised lan-
guage model adaptation settings. Specifically we
</bodyText>
<equation confidence="0.9394335">
p(zS = k0|z−S) = TT
1j1
</equation>
<page confidence="0.977843">
1186
</page>
<figure confidence="0.997697813559322">
negative log-likelihood
8.1e+06
7.9e+06
7.7e+06
7.5e+06
7.3e+06
negative log-likelihood
3.7e+07
3.5e+07
3.3e+07
3.1e+07
2.9e+07
negative log-likelihood
9.2
8.9
8.6e+07
8.3e+07
8.0e+07
3-gram HPYTM
3-gram HPYTM
4-gram HPYTM
4-gram HPYTM
0 2 4 6 8 0 8 16 24 32 0 15 30 45 60
time (hr.) time (hr.) time (hr.)
(a) Brown (b) NIPS (c) BNC
test perplexity
245
240
235
230
225
220
215
210
205
test perplexity
125
120
115
110
105
100
test perplexity
190
180
170
160
150
140
130
HPYLM
HPYTM
HPYTM
RESCALING
SWITCHING
HIERARCHICAL
10 50 100 10 50 100 10 50 100
# topics # topics # topics
(d) Brown (e) NIPS (f) BNC
</figure>
<figureCaption confidence="0.999480333333333">
Figure 4: (a)–(c): Comparison of negative log-likelihoods at training of HPYTM (K = 50). Lower is better. HPYTM
is trained on both token- and table-based samplers, while HPYTMtoke,,, is trained only on the token-based sampler.
(d)–(f): Test perplexity of various 3-gram models as a function of number of topics on each corpus.
</figureCaption>
<bodyText confidence="0.999970225806452">
concentrate on the dynamic adaptation: We update
the posterior of language model given previously ob-
served contexts, which might be decoded transcripts
at that point in ASR or MT.
We use three corpora: the Brown, BNC and NIPS.
The Brown and BNC are balanced corpora that con-
sist of documents of several genres from news to
romance. The Brown corpus comprises 15 cate-
gories. We selected two documents from each cate-
gory for the test set, and use other 470 documents for
the training set. For the NIPS, we randomly select
1,500 papers for training and 50 papers for testing.
For BNC, we first randomly selected 400 documents
from a written corpus and then split each document
into smaller documents every 100 sentences, leading
to 6,262 documents, from which we randomly se-
lected 100 documents for testing, and other are used
for training. See Table 1 for the pre-processing of
unknown types and the resulting corpus statistics.
For comparison, besides our proposed HIERAR-
CHICAL and SWITCHING models, we prepare vari-
ous models for baseline. HPYLM is a n-gram lan-
guage model without any topics. We call the model
without the global Gh introduced in Section 2.2
HPYTM. To see the effect of the table-based sam-
pler, we also prepare HPYTMtoken, which is trained
only on the token-based sampler. RESCALING is
the unigram rescaling. This is a product model of
an n-gram model p(w|h) and a topic model p(w|d),
where we learn each model separately and then com-
bine them by:
</bodyText>
<equation confidence="0.9691825">
p
p(w |h, d) ∝ (pp(I ))) p(w |h) (14)
</equation>
<bodyText confidence="0.999083">
We set Q in (14) to 0.7, which we tuned with the
Brown corpus.
</bodyText>
<subsectionHeader confidence="0.999849">
6.2 Effects of Table-based Sampler
</subsectionHeader>
<bodyText confidence="0.999988285714286">
We first evaluate the effects of our blocked sam-
pler at training. For simplicity, we concentrate on
the HPYTM with K = 50. Table 4(a)–(c) shows
negative likelihoods of the model during training.
On all corpora, the model with the table-based sam-
pler reached the higher probability space with much
faster speed on both 3-gram and 4-gram models.
</bodyText>
<subsectionHeader confidence="0.999646">
6.3 Perplexity Results
</subsectionHeader>
<bodyText confidence="0.999686466666667">
Training For burn-in, we ran the sampler as fol-
lows: For HPYLM, we ran 100 Gibbs iterations. For
RESCALING, we ran 900 iterations on LDA and 100
iterations on HPYLM. For all other models, we ran
500 iterations of the Gibbs; HPYTMtoken is trained
only on the token-based sampler, while for other
models, the table-based sampler is performed after
the token-based sampler.
Evaluation We have to adapt to the topic dis-
tribution of unseen documents incrementally. Al-
though previous works have employed incremental
EM (Gildea and Hofmann, 1999; Tam and Schultz,
2005) because their inference is EM/VB-based, we
use the left-to-right method (Wallach et al., 2009),
which is a kind of particle filter updating the poste-
rior topic distribution of a test document. We set the
number of particles to 10 and resampled each parti-
cle every 10 words for all experiments. To get the
final perplexity, after burn-in, we sampled 10 sam-
ples every 10 iterations of Gibbs, calculated a test
perplexity for each sample, and averaged the results.
Comparison of 3-grams Figure 4(d)–(f) shows
perplexities when varying the number of top-
ics. Generally, compared to the HPYTMtoken, the
HPYTM got much perplexity gains, which again
confirm the effectiveness of our blocked sampler.
Both our proposed models, the HIERARCHICAL and
the SWITCHING, got better performances than the
HPYTM, which does not place the global model
Gh. Our SWITCHING model consistently performed
the best. The HIERARCHICAL performed somewhat
worse than the RESCALING when K become large,
but the SWITCHING outperformed that.
Comparison of 4-grams and beyond We sum-
marize the results with higher order n-grams in Ta-
ble 2, where we also show the time for prediction.
We fixed the number of topics K = 100 because
we saw that all models but HPYTMtoken performed
best at K = 100 when n = 3. Generally, the
results are consistent with those of n = 3. The
models with n = ∞ indicate a model extension
using the Bayesian variable-order language model
(Mochihashi and Sumita, 2008), which can naturally
be integrated with our generative models. By this
extension, we can prune unnecessary nodes stochas-
</bodyText>
<table confidence="0.999653909090909">
Model n NIPS BNC
PPL time PPL time
HPYLM 4 117.2 59 169.2 74
HPYLM ∞ 117.9 61 173.1 59
RESCALING 4 101.4 19009 130.3 36323
HPYTM 4 107.0 1004 133.1 980
HPYTM ∞ 107.2 1346 133.6 1232
HIERARCHICAL 4 106.3 1038 129.0 993
HIERARCHICAL ∞ 105.7 1337 129.3 1001
SWITCHING 4 100.0 1059 125.5 991
SWITCHING ∞ 100.4 1369 125.7 1006
</table>
<tableCaption confidence="0.982461666666667">
Table 2: Comparison of perplexity and the time require
for prediction (in seconds). The number of topics is fixed
to 100 on all topic-based models.
</tableCaption>
<bodyText confidence="0.999891">
tically during training. We can see that this ∞-
gram did not hurt performances, but the sampled
model get much more compact; in BNC, the number
of nodes of the SWITCHING with 4-gram is about
7.9M, while the one with ∞-gram is about 3.9M.
Note that our models require no explicit normaliza-
tion, thereby drastically reducing the time for pre-
diction compared to the RESCALING. This differ-
ence is especially remarkable when the vocabulary
size becomes large.
We can see that our SWITCHING performed con-
sistently better than the HIERARCHICAL. One rea-
son for this result might be the mismatch of pre-
diction of the topic distribution in the HIERARCHI-
CAL. The HIERARCHICAL must allocate some (not
global) topics to every word in a document, so even
the words to which the SWITCHING might allocate
the global topic (mainly function words; see below)
must be allocated to some other topics, causing a
mismatch of allocations of topic.
</bodyText>
<subsectionHeader confidence="0.991912">
6.4 Qualitative Results
</subsectionHeader>
<bodyText confidence="0.999874909090909">
To observe the behavior in which the SWITCHING
allocates some words to the global topic, in Figure
5, we show the posterior of allocating the topic 0
or not at each word in a part of the NIPS training
corpus. We can see that the model elegantly identi-
fied content and function words, learning the topic
distribution appropriately using only semantic con-
texts. These same results in the HIERARCHICAL are
presented in Table 3, where we show some relations
between Ah and context h. Contexts that might be
likely to precede nouns have a higher value of Ah,
</bodyText>
<page confidence="0.978699">
1188
</page>
<bodyText confidence="0.999055">
there has been much recent work on measuring image statistics
and on learning probability distributions on images. we observe
that the mapping from images to statistics is many-to-one and
show it can be quantified by a phase space factor.
</bodyText>
<figureCaption confidence="0.8847">
Figure 5: The posterior for assigning topic 0 or not in
NIPS by the ∞-gram SWITCHING. Darker words indi-
cate a higher probability of not being assigned topic 0.
</figureCaption>
<bodyText confidence="0.7269305">
Ah h
in spite, were unable, a sort, on behalf, . regardless
assumed it, rand mines, plans was, other excersises
that the, the existing, the new, their own, and spatial
</bodyText>
<tableCaption confidence="0.9874825">
Table 3: Some contexts h for various values of Ah in-
duced by the 3-gram HIERARCHICAL in BNC.
</tableCaption>
<bodyText confidence="0.999775285714286">
while prefixes of idioms have a lower value. The ∞-
gram extension gives us the posterior of n-gram or-
der p(n|h), which can be used to calculate the proba-
bility of a word ordering composing a phrase in topic
k as p(w, n|k, h) ∝ p(n|h)p(w|k, n, h). In Table
4, we show some higher probability topic-specific
phrases from the model trained on the NIPS.
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999993">
We have presented modeling and algorithmic con-
tributions to the existing Bayesian n-gram topic
model. We explored two different priors to incor-
porate a global model, and found the effectiveness
of the flat structured model. We developed a novel
blocked Gibbs move for these types of models to ac-
celerate inference. We believe that this Gibbs op-
eration can be incorporated with other models hav-
ing a similar hierarchical structure. Empirically, we
demonstrate that by a careful model design and effi-
cient inference, a well-defined Bayesian model can
rival the conventional heuristics.
</bodyText>
<sectionHeader confidence="0.999477" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9992812">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. The Journal of Ma-
chine Learning Research, 3:993–1022.
Phil Blunsom and Trevor Cohn. 2011. A hierarchi-
cal pitman-yor process hmm for unsupervised part of
speech induction. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 865–874,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
</reference>
<table confidence="0.98685175">
0 46
according to &amp;quot; support vectors
&amp;quot; ( # ) in high dimentional
&amp;quot; section # as decision function
techniques such as set of # observations
&amp;quot; ( b ) original data set
83 89
the hierarchical mixtures &amp;quot; linear discriminant
the rbf units images per class
the gating networks multi-class classification
grown hme &amp;quot; decision boundaries
the modular architecture references per class
</table>
<tableCaption confidence="0.983463">
Table 4: Topical phrases from NIPS induced by the ∞-
</tableCaption>
<reference confidence="0.997659225">
gram SWITCHING model. &amp;quot; is a symbol for the beginning
of a sentence and # represents a number.
Phil Blunsom, Trevor Cohn, Sharon Goldwater, and Mark
Johnson. 2009. A note on the implementation of hi-
erarchical dirichlet processes. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
337–340, Suntec, Singapore, August. Association for
Computational Linguistics.
Daniel Gildea and Thomas Hofmann. 1999. Topic-based
language models using em. In In Proceedings of EU-
ROSPEECH, pages 2167–2170.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America,
101(Suppl 1):5228–5235.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In In Advances in Neural Information Pro-
cessing Systems 17, pages 537–544. MIT Press.
Songfang Huang and Steve Renals. 2008. Unsupervised
language model adaptation based on topic and role in-
formation in multiparty meetings. In in Proc. Inter-
speech08, pages 833–836.
F. Jelinek, B. Merialdo, S. Roukos, and M. Strauss. 1991.
A dynamic language model for speech recognition. In
Proceedings of the workshop on Speech and Natural
Language, HLT ’91, pages 293–295, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Robert Lindsey, William Headden, and Michael Stipice-
vic. 2012. A phrase-discovering topic model using hi-
erarchical pitman-yor processes. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 214–222, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Daichi Mochihashi and Eiichiro Sumita. 2008. The infi-
nite markov model. In J.C. Platt, D. Koller, Y. Singer,
and S. Roweis, editors, Advances in Neural Informa-
tion Processing Systems 20, pages 1017–1024. MIT
Press, Cambridge, MA.
</reference>
<figure confidence="0.981370333333333">
0.0–0.1
0.5–0.6
0.9–1.0
</figure>
<page confidence="0.976311">
1189
</page>
<reference confidence="0.999283659574468">
Adam Pauls and Dan Klein. 2012. Large-scale syntac-
tic language modeling with treelets. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics: Long Papers - Volume 1, pages
959–968. Association for Computational Linguistics.
Yik-Cheung Tam and Tanja Schultz. 2005. Dynamic lan-
guage model adaptation using variational bayes infer-
ence. In INTERSPEECH, pages 5–8.
Yik-Cheung Tam and Tanja Schultz. 2009. Correlated
bigram lsa for unsupervised language model adapta-
tion. In D. Koller, D. Schuurmans, Y. Bengio, and
L. Bottou, editors, Advances in Neural Information
Processing Systems 21, pages 1633–1640.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566–1581.
Yee Whye Teh. 2006a. A Bayesian Interpretation of
Interpolated Kneser-Ney. NUS School of Computing
Technical Report TRA2/06.
Yee Whye Teh. 2006b. A hierarchical bayesian language
model based on pitman-yor processes. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 985–
992, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluation methods for
topic models. In Proceedings of the 26th Annual In-
ternational Conference on Machine Learning, ICML
’09, pages 1105–1112, New York, NY, USA. ACM.
Hanna M. Wallach. 2006. Topic modeling: beyond bag-
of-words. In Proceedings of the 23rd international
conference on Machine learning, ICML ’06, pages
977–984.
Xuerui Wang, Andrew McCallum, and Xing Wei. 2007.
Topical n-grams: Phrase and topic discovery, with an
application to information retrieval. In Proceedings
of the 2007 Seventh IEEE International Conference on
Data Mining, ICDM ’07, pages 697–702, Washington,
DC, USA. IEEE Computer Society.
Frank Wood and Yee Whye Teh. 2009. A hierarchi-
cal nonparametric Bayesian approach to statistical lan-
guage model domain adaptation. In Proceedings of the
International Conference on Artificial Intelligence and
Statistics, volume 12.
</reference>
<page confidence="0.992032">
1190
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.512165">
<title confidence="0.990513">to the Bayesian Topic Models</title>
<author confidence="0.920661">nojinii ac jp daichiism ac jp yusukenii ac jp</author>
<affiliation confidence="0.920145">University for Advanced Institute of Informatics, Tokyo, Institute of Statistical Mathematics, Tokyo, Japan</affiliation>
<abstract confidence="0.983838529411765">One of the language phenomena that n-gram language model fails to capture is the topic information of a given situation. We advance the previous study of the Bayesian topic language model by Wallach (2006) in two directions: one, investigating new priors to alleviate the sparseness problem caused by dividing all ngrams into exclusive topics, and two, developing a novel Gibbs sampler that enables moving multiple n-grams across different documents to another topic. Our blocked sampler can efficiently search for higher probability space even with higher order n-grams. In terms of modeling assumption, we found it is effective to assign a topic to only some parts of a document.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="6644" citStr="Blei et al., 2003" startWordPosition="1091" endWordPosition="1094">d customers, while thw is a hidden variable called tables. ch· and th· represents marginal counts: ch· = Ew chw and th· = Ew thw. This form is very similar to the well-known Kneser-Ney smoothing, and actually the Kneser-Ney can be understood as a heuristic approximation of the HPYLM. This characteristic enables us to build the state-of-the-art language model into a more complex generative model. 2.2 Wallach (2006) with HPYLM Wallach (2006) is a generative model for a document collection that combines the topic model with a Bayesian n-gram language model. The latent Dirichlet allocation (LDA) (Blei et al., 2003) is the most basic topic model, which generates each word in a document based on a unigram word distribution defined by a topic allocated to that word. The bigram topic model of Wallach (2006) simply replaces this unigram word distribution (a multinomial) for each topic with a bigram word distribution 1. In other words, ordinary LDA generates word conditioning only on the latent topic, whereas the bigram topic model generates conditioning on both the latent topic and the previous word, as in the bigram language model. Extending this model with a higher order n-gram is trivial; all we have to d</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. The Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>A hierarchical pitman-yor process hmm for unsupervised part of speech induction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>865--874</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="22885" citStr="Blunsom and Cohn (2011)" startWordPosition="4073" endWordPosition="4076"> documents must also be considered, which is p(zS = k0|z−S) and decomposed as: (n�S jk� +αk�)nj(S) (N S+Ek αk)nj (S) (13) where nj(5) is the number of word tokens connected with 5 in document j. HIERARCHICAL We skip tables on restaurants of k = 0, because these tables are all from other topics and we cannot construct a block. The effects of A can be ignored because these are shared by all topics. SWITCHING In the SWITCHING, p(zS = k0|z−S) cannot be calculated in a closed form because p(hi|hji) in (9) would be changed dynamically when adding customers. This problem is the same one addressed by Blunsom and Cohn (2011), and we follow the same approximation in which, when we calculate the probability, we fractionally add tables and customers recursively. 4.3 Inference of Hyperparameters We also place a prior on each hyperparameter and sample value from the posterior distribution for every iteration. As in Teh (2006a), we set different values of a and b for each depth of PYP, but share across all topics and sample values with an auxiliary variable method. We also set different value of -y for each depth, on which we place Gamma(1,1). We make the topic prior α asymmetric: α = Qα0; Q — Gamma(1,1), α0 — Dir(1). </context>
</contexts>
<marker>Blunsom, Cohn, 2011</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2011. A hierarchical pitman-yor process hmm for unsupervised part of speech induction. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 865–874, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>gram SWITCHING model</author>
</authors>
<title>is a symbol for the beginning of a sentence and # represents a number.</title>
<marker>model, </marker>
<rawString>gram SWITCHING model. &amp;quot; is a symbol for the beginning of a sentence and # represents a number.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Sharon Goldwater</author>
<author>Mark Johnson</author>
</authors>
<title>A note on the implementation of hierarchical dirichlet processes.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>337--340</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="19919" citStr="Blunsom et al. (2009)" startWordPosition="3561" endWordPosition="3564">ght be difficult under the token-based sampler. The table-based sampler moves those different n-grams having common suffixes jointly into another topic. Figure 3 shows a transition of state by the tablebased sampler and Algorithm 4.2 depicts a highlevel description of one iteration. First, we select a table in a restaurant, which is shown with a dotted line in the figure. Next, we descend the tree to collect the tables connected to the selected table, which are pointed by arrows. Because this connection cannot be preserved in common data structures for a restaurant described in Teh (2006a) or Blunsom et al. (2009), we select the child tables randomly. This is correct because customers in CRP are exchangeAlgorithm 1 Table-based sampler for all table in all restaurants do Remove a customer from the parent restaurant. Construct a block of seating arrangement S by descending the tree recursively. Sample topic assignment zS — p(zS|S, S−S, z−S). Move S to sampled topic, and add a customer to the parent restaurant of the first selected table. end for able, so we can restore the parent-child relations arbitrarily. We continue this process recursively until reaching the leaf nodes, obtaining a block of seating </context>
</contexts>
<marker>Blunsom, Cohn, Goldwater, Johnson, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Sharon Goldwater, and Mark Johnson. 2009. A note on the implementation of hierarchical dirichlet processes. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 337–340, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Thomas Hofmann</author>
</authors>
<title>Topic-based language models using em. In</title>
<date>1999</date>
<booktitle>In Proceedings of EUROSPEECH,</booktitle>
<pages>2167--2170</pages>
<contexts>
<context position="1464" citStr="Gildea and Hofmann, 1999" startWordPosition="220" endWordPosition="223">ace even with higher order n-grams. In terms of modeling assumption, we found it is effective to assign a topic to only some parts of a document. 1 Introduction N-gram language model is still ubiquitous in NLP, but due to its simplicity it fails to capture some important aspects of language, such as difference of word usage in different situations, sentence level syntactic correctness, and so on. Toward language model that can consider such a more global context, many extensions have been proposed from lexical pattern adaptation, e.g., adding cache (Jelinek et al., 1991) or topic information (Gildea and Hofmann, 1999; Wallach, 2006), to grammaticality aware models (Pauls and Klein, 2012). Topic language models are important for use in e.g., unsupervised language model adaptation: we want a language model that can adapt to the domain or topic of the current situation (e.g., a document in SMT or a conversation in ASR) automatically and select the appropriate words using both topic and syntactic context. Wallach (2006) is one such model, which generate each word based on local context and global topic information to capture the difference of lexical usage among different topics. However, Wallach’s experiment</context>
<context position="3736" citStr="Gildea and Hofmann, 1999" startWordPosition="590" endWordPosition="593">ize is V , n-gram topic model has O(KVn) parameters, which grow exponentially to n, making the local minima problem even more severe. Our sampler resolves this problem by moving many customers in the hierarchical Chinese restaurant process at a time. We evaluate various models by incremental calculation of test document perplexity on 3 types of corpora having different size and diversity. By combining the proposed prior and the sampling method, our Bayesian model achieve much higher accuracies than the naive extension of Wallach (2006) and shows results competitive with the unigram rescaling (Gildea and Hofmann, 1999), which require 1180 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1180–1190, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics huge computational cost at prediction, with much faster prediction time. 2 Basic Models All models presented in this paper are based on the Bayesian n-gram language model, the hierarchical Pitman-Yor process language model (HPYLM). In the following, we first introduce the HPYLM, and then discuss the topic model extension of Wallach (2006) with HPYLM. 2.1 HPYLM Let us first def</context>
<context position="25229" citStr="Gildea and Hofmann, 1999" startWordPosition="4462" endWordPosition="4465">ose: to determine the segmenting points. They treat these variables completely independently, while our model employs a hierarchical prior to share statistical strength among similar contexts. Our primary interest is language model adaptation, which has been studied mainly in the area of speech processing. Conventionally, this adaptation has relied on a heuristic combination of two separately trained models: an n-gram model p(w|h) and a topic model p(w|d). The unigram rescaling, which is a product model of these two models, perform better than more simpler models such as linear interpolation (Gildea and Hofmann, 1999). There are also some extensions to this method (Tam and Schultz, 2009; Huang and Renals, 2008), but these methods have one major drawback: at prediction, the rescaling-based method requires normalization across vocabulary at each word, which prohibits use on applications requiring dynamic (incremental) adaptation, e.g., settings where we have to update the topic distribution as new inputs come in. Tam and Schultz (2005) studied on this incremental settings, but they employ an interpolation. The practical interest here is whether our Bayesian models can rival the rescaling-based method in term</context>
<context position="29528" citStr="Gildea and Hofmann, 1999" startWordPosition="5189" endWordPosition="5192">space with much faster speed on both 3-gram and 4-gram models. 6.3 Perplexity Results Training For burn-in, we ran the sampler as follows: For HPYLM, we ran 100 Gibbs iterations. For RESCALING, we ran 900 iterations on LDA and 100 iterations on HPYLM. For all other models, we ran 500 iterations of the Gibbs; HPYTMtoken is trained only on the token-based sampler, while for other models, the table-based sampler is performed after the token-based sampler. Evaluation We have to adapt to the topic distribution of unseen documents incrementally. Although previous works have employed incremental EM (Gildea and Hofmann, 1999; Tam and Schultz, 2005) because their inference is EM/VB-based, we use the left-to-right method (Wallach et al., 2009), which is a kind of particle filter updating the posterior topic distribution of a test document. We set the number of particles to 10 and resampled each particle every 10 words for all experiments. To get the final perplexity, after burn-in, we sampled 10 samples every 10 iterations of Gibbs, calculated a test perplexity for each sample, and averaged the results. Comparison of 3-grams Figure 4(d)–(f) shows perplexities when varying the number of topics. Generally, compared t</context>
</contexts>
<marker>Gildea, Hofmann, 1999</marker>
<rawString>Daniel Gildea and Thomas Hofmann. 1999. Topic-based language models using em. In In Proceedings of EUROSPEECH, pages 2167–2170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America, 101(Suppl</booktitle>
<pages>1--5228</pages>
<contexts>
<context position="16528" citStr="Griffiths and Steyvers, 2004" startWordPosition="2951" endWordPosition="2954">WITCHING Figure 2: Graphical model representations of our two models in the case of a 3-gram model. Edges that only exist in one model are colored. cate the collapsed state of Gkh (PYP), while we refer the restaurant of λh to indicates the collapsed state of λh (DP). We present two different types of sampler: a token-based sampler and a table-based sampler. For both samplers, we first explain in the case of our basic model (Section 2.2), and later discuss some notes on our extended models. 4.1 Token-based Sampler The token-based sampler is almost identical to the collapsed sampler of the LDA (Griffiths and Steyvers, 2004). At each iteration, we consider the following conditional distribution of zji given all other topic assignments z−ji and S−ji, which is the set of seating arrangements with a customer corresponds to wji removed, as p(zji|z−ji, S−ji) a p(zji|z−ji)p(wji|zji, hji, S−ji), (6) where p(wji|zji, hji, S−ji) = ck hw − atk hw + atk h· + b p(wji|zji, hji, S−ji) (7) h· + b h· + b ck ck is a predictive word probability under the topic zji, and njk + αk −ji p(zji|z−ji) = Nj − 1 + Ek0 αk0 ,(8) where n−ji jk is the number of words that is assigned topic k in document j excluding wji, which is the same as the</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences of the United States of America, 101(Suppl 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>David M Blei</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2005</date>
<booktitle>In In Advances in Neural Information Processing Systems 17,</booktitle>
<pages>537--544</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="23532" citStr="Griffiths et al., 2005" startWordPosition="4185" endWordPosition="4188"> approximation in which, when we calculate the probability, we fractionally add tables and customers recursively. 4.3 Inference of Hyperparameters We also place a prior on each hyperparameter and sample value from the posterior distribution for every iteration. As in Teh (2006a), we set different values of a and b for each depth of PYP, but share across all topics and sample values with an auxiliary variable method. We also set different value of -y for each depth, on which we place Gamma(1,1). We make the topic prior α asymmetric: α = Qα0; Q — Gamma(1,1), α0 — Dir(1). 5 Related Work HMM-LDA (Griffiths et al., 2005) is a composite model of HMM and LDA that assumes the words in a document are generated by HMM, where only one state has a document-specific topic distribution. Our SWITCHING model can be understood as a lexical extension of HMM-LDA. It models the topicality by context-specific binary random variables, not by hidden states. Other n-gram topic models have focused mainly on information retrieval. Wang et min. training set test set Corpus appear # types # docs # tokens # docs # tokens Brown 4 19,759 470 1,157,225 30 70,795 NIPS 4 22,705 1500 5,088,786 50 167,730 BNC 10 33,071 6,162 12,783,130 100</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2005</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, David M. Blei, and Joshua B. Tenenbaum. 2005. Integrating topics and syntax. In In Advances in Neural Information Processing Systems 17, pages 537–544. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Songfang Huang</author>
<author>Steve Renals</author>
</authors>
<title>Unsupervised language model adaptation based on topic and role information in multiparty meetings.</title>
<date>2008</date>
<booktitle>In in Proc. Interspeech08,</booktitle>
<pages>833--836</pages>
<contexts>
<context position="25324" citStr="Huang and Renals, 2008" startWordPosition="4478" endWordPosition="4481">le our model employs a hierarchical prior to share statistical strength among similar contexts. Our primary interest is language model adaptation, which has been studied mainly in the area of speech processing. Conventionally, this adaptation has relied on a heuristic combination of two separately trained models: an n-gram model p(w|h) and a topic model p(w|d). The unigram rescaling, which is a product model of these two models, perform better than more simpler models such as linear interpolation (Gildea and Hofmann, 1999). There are also some extensions to this method (Tam and Schultz, 2009; Huang and Renals, 2008), but these methods have one major drawback: at prediction, the rescaling-based method requires normalization across vocabulary at each word, which prohibits use on applications requiring dynamic (incremental) adaptation, e.g., settings where we have to update the topic distribution as new inputs come in. Tam and Schultz (2005) studied on this incremental settings, but they employ an interpolation. The practical interest here is whether our Bayesian models can rival the rescaling-based method in terms of prediction power. We evaluate this in the next section. 6 Experiments 6.1 Settings We test</context>
</contexts>
<marker>Huang, Renals, 2008</marker>
<rawString>Songfang Huang and Steve Renals. 2008. Unsupervised language model adaptation based on topic and role information in multiparty meetings. In in Proc. Interspeech08, pages 833–836.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>B Merialdo</author>
<author>S Roukos</author>
<author>M Strauss</author>
</authors>
<title>A dynamic language model for speech recognition.</title>
<date>1991</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language, HLT ’91,</booktitle>
<pages>293--295</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1417" citStr="Jelinek et al., 1991" startWordPosition="212" endWordPosition="216">efficiently search for higher probability space even with higher order n-grams. In terms of modeling assumption, we found it is effective to assign a topic to only some parts of a document. 1 Introduction N-gram language model is still ubiquitous in NLP, but due to its simplicity it fails to capture some important aspects of language, such as difference of word usage in different situations, sentence level syntactic correctness, and so on. Toward language model that can consider such a more global context, many extensions have been proposed from lexical pattern adaptation, e.g., adding cache (Jelinek et al., 1991) or topic information (Gildea and Hofmann, 1999; Wallach, 2006), to grammaticality aware models (Pauls and Klein, 2012). Topic language models are important for use in e.g., unsupervised language model adaptation: we want a language model that can adapt to the domain or topic of the current situation (e.g., a document in SMT or a conversation in ASR) automatically and select the appropriate words using both topic and syntactic context. Wallach (2006) is one such model, which generate each word based on local context and global topic information to capture the difference of lexical usage among </context>
</contexts>
<marker>Jelinek, Merialdo, Roukos, Strauss, 1991</marker>
<rawString>F. Jelinek, B. Merialdo, S. Roukos, and M. Strauss. 1991. A dynamic language model for speech recognition. In Proceedings of the workshop on Speech and Natural Language, HLT ’91, pages 293–295, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Lindsey</author>
<author>William Headden</author>
<author>Michael Stipicevic</author>
</authors>
<title>A phrase-discovering topic model using hierarchical pitman-yor processes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>214--222</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="24483" citStr="Lindsey et al. (2012)" startWordPosition="4346" endWordPosition="4349">r n-gram topic models have focused mainly on information retrieval. Wang et min. training set test set Corpus appear # types # docs # tokens # docs # tokens Brown 4 19,759 470 1,157,225 30 70,795 NIPS 4 22,705 1500 5,088,786 50 167,730 BNC 10 33,071 6,162 12,783,130 100 202,994 Table 1: Corpus statistics after the pre-processing: We replace words appearing less than min.appear times in training + test documents, or appearing only in a test set with an unknown token. All numbers are replaced with #, while punctuations are remained. al. (2007) is a topic model on automatically segmented chunks. Lindsey et al. (2012) extended this model with the hierarchical Pitman-Yor prior. They also used switching variables, but for a different purpose: to determine the segmenting points. They treat these variables completely independently, while our model employs a hierarchical prior to share statistical strength among similar contexts. Our primary interest is language model adaptation, which has been studied mainly in the area of speech processing. Conventionally, this adaptation has relied on a heuristic combination of two separately trained models: an n-gram model p(w|h) and a topic model p(w|d). The unigram rescal</context>
</contexts>
<marker>Lindsey, Headden, Stipicevic, 2012</marker>
<rawString>Robert Lindsey, William Headden, and Michael Stipicevic. 2012. A phrase-discovering topic model using hierarchical pitman-yor processes. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 214–222, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daichi Mochihashi</author>
<author>Eiichiro Sumita</author>
</authors>
<title>The infinite markov model.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems 20,</booktitle>
<pages>1017--1024</pages>
<editor>In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="31009" citStr="Mochihashi and Sumita, 2008" startWordPosition="5436" endWordPosition="5439">del Gh. Our SWITCHING model consistently performed the best. The HIERARCHICAL performed somewhat worse than the RESCALING when K become large, but the SWITCHING outperformed that. Comparison of 4-grams and beyond We summarize the results with higher order n-grams in Table 2, where we also show the time for prediction. We fixed the number of topics K = 100 because we saw that all models but HPYTMtoken performed best at K = 100 when n = 3. Generally, the results are consistent with those of n = 3. The models with n = ∞ indicate a model extension using the Bayesian variable-order language model (Mochihashi and Sumita, 2008), which can naturally be integrated with our generative models. By this extension, we can prune unnecessary nodes stochasModel n NIPS BNC PPL time PPL time HPYLM 4 117.2 59 169.2 74 HPYLM ∞ 117.9 61 173.1 59 RESCALING 4 101.4 19009 130.3 36323 HPYTM 4 107.0 1004 133.1 980 HPYTM ∞ 107.2 1346 133.6 1232 HIERARCHICAL 4 106.3 1038 129.0 993 HIERARCHICAL ∞ 105.7 1337 129.3 1001 SWITCHING 4 100.0 1059 125.5 991 SWITCHING ∞ 100.4 1369 125.7 1006 Table 2: Comparison of perplexity and the time require for prediction (in seconds). The number of topics is fixed to 100 on all topic-based models. tically d</context>
</contexts>
<marker>Mochihashi, Sumita, 2008</marker>
<rawString>Daichi Mochihashi and Eiichiro Sumita. 2008. The infinite markov model. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 1017–1024. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Large-scale syntactic language modeling with treelets.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers -</booktitle>
<volume>1</volume>
<pages>959--968</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1536" citStr="Pauls and Klein, 2012" startWordPosition="230" endWordPosition="233">und it is effective to assign a topic to only some parts of a document. 1 Introduction N-gram language model is still ubiquitous in NLP, but due to its simplicity it fails to capture some important aspects of language, such as difference of word usage in different situations, sentence level syntactic correctness, and so on. Toward language model that can consider such a more global context, many extensions have been proposed from lexical pattern adaptation, e.g., adding cache (Jelinek et al., 1991) or topic information (Gildea and Hofmann, 1999; Wallach, 2006), to grammaticality aware models (Pauls and Klein, 2012). Topic language models are important for use in e.g., unsupervised language model adaptation: we want a language model that can adapt to the domain or topic of the current situation (e.g., a document in SMT or a conversation in ASR) automatically and select the appropriate words using both topic and syntactic context. Wallach (2006) is one such model, which generate each word based on local context and global topic information to capture the difference of lexical usage among different topics. However, Wallach’s experiments were limited to bigrams, a toy setting for language models, and experi</context>
</contexts>
<marker>Pauls, Klein, 2012</marker>
<rawString>Adam Pauls and Dan Klein. 2012. Large-scale syntactic language modeling with treelets. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, pages 959–968. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yik-Cheung Tam</author>
<author>Tanja Schultz</author>
</authors>
<title>Dynamic language model adaptation using variational bayes inference.</title>
<date>2005</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>5--8</pages>
<contexts>
<context position="25653" citStr="Tam and Schultz (2005)" startWordPosition="4527" endWordPosition="4530"> p(w|h) and a topic model p(w|d). The unigram rescaling, which is a product model of these two models, perform better than more simpler models such as linear interpolation (Gildea and Hofmann, 1999). There are also some extensions to this method (Tam and Schultz, 2009; Huang and Renals, 2008), but these methods have one major drawback: at prediction, the rescaling-based method requires normalization across vocabulary at each word, which prohibits use on applications requiring dynamic (incremental) adaptation, e.g., settings where we have to update the topic distribution as new inputs come in. Tam and Schultz (2005) studied on this incremental settings, but they employ an interpolation. The practical interest here is whether our Bayesian models can rival the rescaling-based method in terms of prediction power. We evaluate this in the next section. 6 Experiments 6.1 Settings We test the effectiveness of presented models and the blocked sampling method on unsupervised language model adaptation settings. Specifically we p(zS = k0|z−S) = TT 1j1 1186 negative log-likelihood 8.1e+06 7.9e+06 7.7e+06 7.5e+06 7.3e+06 negative log-likelihood 3.7e+07 3.5e+07 3.3e+07 3.1e+07 2.9e+07 negative log-likelihood 9.2 8.9 8</context>
<context position="29552" citStr="Tam and Schultz, 2005" startWordPosition="5193" endWordPosition="5196">ed on both 3-gram and 4-gram models. 6.3 Perplexity Results Training For burn-in, we ran the sampler as follows: For HPYLM, we ran 100 Gibbs iterations. For RESCALING, we ran 900 iterations on LDA and 100 iterations on HPYLM. For all other models, we ran 500 iterations of the Gibbs; HPYTMtoken is trained only on the token-based sampler, while for other models, the table-based sampler is performed after the token-based sampler. Evaluation We have to adapt to the topic distribution of unseen documents incrementally. Although previous works have employed incremental EM (Gildea and Hofmann, 1999; Tam and Schultz, 2005) because their inference is EM/VB-based, we use the left-to-right method (Wallach et al., 2009), which is a kind of particle filter updating the posterior topic distribution of a test document. We set the number of particles to 10 and resampled each particle every 10 words for all experiments. To get the final perplexity, after burn-in, we sampled 10 samples every 10 iterations of Gibbs, calculated a test perplexity for each sample, and averaged the results. Comparison of 3-grams Figure 4(d)–(f) shows perplexities when varying the number of topics. Generally, compared to the HPYTMtoken, the HP</context>
</contexts>
<marker>Tam, Schultz, 2005</marker>
<rawString>Yik-Cheung Tam and Tanja Schultz. 2005. Dynamic language model adaptation using variational bayes inference. In INTERSPEECH, pages 5–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yik-Cheung Tam</author>
<author>Tanja Schultz</author>
</authors>
<title>Correlated bigram lsa for unsupervised language model adaptation. In</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems 21,</booktitle>
<pages>1633--1640</pages>
<editor>D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors,</editor>
<contexts>
<context position="25299" citStr="Tam and Schultz, 2009" startWordPosition="4474" endWordPosition="4477">tely independently, while our model employs a hierarchical prior to share statistical strength among similar contexts. Our primary interest is language model adaptation, which has been studied mainly in the area of speech processing. Conventionally, this adaptation has relied on a heuristic combination of two separately trained models: an n-gram model p(w|h) and a topic model p(w|d). The unigram rescaling, which is a product model of these two models, perform better than more simpler models such as linear interpolation (Gildea and Hofmann, 1999). There are also some extensions to this method (Tam and Schultz, 2009; Huang and Renals, 2008), but these methods have one major drawback: at prediction, the rescaling-based method requires normalization across vocabulary at each word, which prohibits use on applications requiring dynamic (incremental) adaptation, e.g., settings where we have to update the topic distribution as new inputs come in. Tam and Schultz (2005) studied on this incremental settings, but they employ an interpolation. The practical interest here is whether our Bayesian models can rival the rescaling-based method in terms of prediction power. We evaluate this in the next section. 6 Experim</context>
</contexts>
<marker>Tam, Schultz, 2009</marker>
<rawString>Yik-Cheung Tam and Tanja Schultz. 2009. Correlated bigram lsa for unsupervised language model adaptation. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1633–1640.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="13268" citStr="Teh et al., 2006" startWordPosition="2325" endWordPosition="2328">ty of context the topicality and say that “in order” has weak topicality. Therefore, we place λ as a distinct value for each context h, which we share across all topics. We designate this λ determined by h λh in the following. Moreover, similar contexts may have similar values of λh. For example, the two contexts “of the” and “in the”, which share the suffix “the”, both have a strong topicality3. We encode this assumption by placing hierarchical Beta distributions on the suffix tree across all topics: λh ∼ Beta(γλh,,γ(1 − λh,)) = DP(γ, λh,), (5) where DP is the hierarchical Dirichlet process (Teh et al., 2006), which has only two atoms in {0,1} and γ is a concentration parameter. As in HPYLM, we place a uniform prior λ0 = 1/2 on the base distribution of the top node (λφ ∼ DP(γ, λ0)). Having generated the topic component of the model, the corpus generating process is the same as the previous model because we only change the generating process of Gkh for k = 1, · · · , K. 3These words can be used very differently depending on the context. For example, in a teen story, “in the room” or “in the school” seems more dominant than “in the corpora” or “in the topic”, which is likely to appear in this paper.</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A Bayesian Interpretation of Interpolated Kneser-Ney.</title>
<date>2006</date>
<journal>NUS School of Computing</journal>
<tech>Technical Report TRA2/06.</tech>
<contexts>
<context position="8006" citStr="Teh, 2006" startWordPosition="1344" endWordPosition="1345">c model is as follows. First, for each topic k E 1, · · · , K, where K is the number of topics, the model generates an n-gram language model Gkh.2 These n-gram models are generated by the PYP, so Gkh — PYP(a, b, Gkh,) holds. The model then generate a document collection. For each document j E 1, · · · , D, it generates a K1This is the model called prior 2 in Wallach (2006); it consistently outperformed the other prior. Wallach used the Dirichlet language model as each topic, but we only explore the model with HPYLM because its superiority to the Dirichlet language model has been well studied (Teh, 2006b). 2We sometimes denote Gh to represent a language model of topic k, not a specific multinomial for some context h, depending on the context. ath· + bp(w|h&apos; &apos; w), ch· + b 1181 dimensional topic distribution Bj by a Dirichlet distribution Dir(α) where α = (a1, a2, · · · , aK) is a prior. Finally, for each word position i ∈ 1, · · · , Nj where Nj is the number of words in document j, ith word’s topic assignment zji is chosen according to Bj, then a word type wji is generated from Gz�i h�i where hji is the last n − 1 words preceding wji. We can summarize this process as follows: 1. Generate topi</context>
<context position="17294" citStr="Teh (2006" startWordPosition="3099" endWordPosition="3100">rrangements with a customer corresponds to wji removed, as p(zji|z−ji, S−ji) a p(zji|z−ji)p(wji|zji, hji, S−ji), (6) where p(wji|zji, hji, S−ji) = ck hw − atk hw + atk h· + b p(wji|zji, hji, S−ji) (7) h· + b h· + b ck ck is a predictive word probability under the topic zji, and njk + αk −ji p(zji|z−ji) = Nj − 1 + Ek0 αk0 ,(8) where n−ji jk is the number of words that is assigned topic k in document j excluding wji, which is the same as the LDA. Given the sampled topic zji, we update the language model of topic zji, by adding customer wji to the restaurant specified by zji and context hji. See Teh (2006a) for details of these customer operations. HIERARCHICAL Adding customer operation is slightly changed: When a new table is added to a restaurant, we must track the label l E f0, 1g indicating the parent restaurant of that table, and add the customer corresponding to l to the restaurant of λh. See Wood and Teh (2009) for details of this operation. SWITCHING We replace p(zji|z−ji) with p(zji|z−ji) = p(lji = 0|hji) (zji = 0) n−ji jk +αk p(lji = 1|hji) �Ek,40 njkji+Ek0 αk0 (zji 0) , (9) where p(lji|hji) is a predictive of lji given by the CRP of λhji. We need not assign a label to a new table, b</context>
<context position="19892" citStr="Teh (2006" startWordPosition="3558" endWordPosition="3559">e same topic might be difficult under the token-based sampler. The table-based sampler moves those different n-grams having common suffixes jointly into another topic. Figure 3 shows a transition of state by the tablebased sampler and Algorithm 4.2 depicts a highlevel description of one iteration. First, we select a table in a restaurant, which is shown with a dotted line in the figure. Next, we descend the tree to collect the tables connected to the selected table, which are pointed by arrows. Because this connection cannot be preserved in common data structures for a restaurant described in Teh (2006a) or Blunsom et al. (2009), we select the child tables randomly. This is correct because customers in CRP are exchangeAlgorithm 1 Table-based sampler for all table in all restaurants do Remove a customer from the parent restaurant. Construct a block of seating arrangement S by descending the tree recursively. Sample topic assignment zS — p(zS|S, S−S, z−S). Move S to sampled topic, and add a customer to the parent restaurant of the first selected table. end for able, so we can restore the parent-child relations arbitrarily. We continue this process recursively until reaching the leaf nodes, ob</context>
<context position="23186" citStr="Teh (2006" startWordPosition="4122" endWordPosition="4123">uct a block. The effects of A can be ignored because these are shared by all topics. SWITCHING In the SWITCHING, p(zS = k0|z−S) cannot be calculated in a closed form because p(hi|hji) in (9) would be changed dynamically when adding customers. This problem is the same one addressed by Blunsom and Cohn (2011), and we follow the same approximation in which, when we calculate the probability, we fractionally add tables and customers recursively. 4.3 Inference of Hyperparameters We also place a prior on each hyperparameter and sample value from the posterior distribution for every iteration. As in Teh (2006a), we set different values of a and b for each depth of PYP, but share across all topics and sample values with an auxiliary variable method. We also set different value of -y for each depth, on which we place Gamma(1,1). We make the topic prior α asymmetric: α = Qα0; Q — Gamma(1,1), α0 — Dir(1). 5 Related Work HMM-LDA (Griffiths et al., 2005) is a composite model of HMM and LDA that assumes the words in a document are generated by HMM, where only one state has a document-specific topic distribution. Our SWITCHING model can be understood as a lexical extension of HMM-LDA. It models the topica</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006a. A Bayesian Interpretation of Interpolated Kneser-Ney. NUS School of Computing Technical Report TRA2/06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical bayesian language model based on pitman-yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>985--992</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="8006" citStr="Teh, 2006" startWordPosition="1344" endWordPosition="1345">c model is as follows. First, for each topic k E 1, · · · , K, where K is the number of topics, the model generates an n-gram language model Gkh.2 These n-gram models are generated by the PYP, so Gkh — PYP(a, b, Gkh,) holds. The model then generate a document collection. For each document j E 1, · · · , D, it generates a K1This is the model called prior 2 in Wallach (2006); it consistently outperformed the other prior. Wallach used the Dirichlet language model as each topic, but we only explore the model with HPYLM because its superiority to the Dirichlet language model has been well studied (Teh, 2006b). 2We sometimes denote Gh to represent a language model of topic k, not a specific multinomial for some context h, depending on the context. ath· + bp(w|h&apos; &apos; w), ch· + b 1181 dimensional topic distribution Bj by a Dirichlet distribution Dir(α) where α = (a1, a2, · · · , aK) is a prior. Finally, for each word position i ∈ 1, · · · , Nj where Nj is the number of words in document j, ith word’s topic assignment zji is chosen according to Bj, then a word type wji is generated from Gz�i h�i where hji is the last n − 1 words preceding wji. We can summarize this process as follows: 1. Generate topi</context>
<context position="17294" citStr="Teh (2006" startWordPosition="3099" endWordPosition="3100">rrangements with a customer corresponds to wji removed, as p(zji|z−ji, S−ji) a p(zji|z−ji)p(wji|zji, hji, S−ji), (6) where p(wji|zji, hji, S−ji) = ck hw − atk hw + atk h· + b p(wji|zji, hji, S−ji) (7) h· + b h· + b ck ck is a predictive word probability under the topic zji, and njk + αk −ji p(zji|z−ji) = Nj − 1 + Ek0 αk0 ,(8) where n−ji jk is the number of words that is assigned topic k in document j excluding wji, which is the same as the LDA. Given the sampled topic zji, we update the language model of topic zji, by adding customer wji to the restaurant specified by zji and context hji. See Teh (2006a) for details of these customer operations. HIERARCHICAL Adding customer operation is slightly changed: When a new table is added to a restaurant, we must track the label l E f0, 1g indicating the parent restaurant of that table, and add the customer corresponding to l to the restaurant of λh. See Wood and Teh (2009) for details of this operation. SWITCHING We replace p(zji|z−ji) with p(zji|z−ji) = p(lji = 0|hji) (zji = 0) n−ji jk +αk p(lji = 1|hji) �Ek,40 njkji+Ek0 αk0 (zji 0) , (9) where p(lji|hji) is a predictive of lji given by the CRP of λhji. We need not assign a label to a new table, b</context>
<context position="19892" citStr="Teh (2006" startWordPosition="3558" endWordPosition="3559">e same topic might be difficult under the token-based sampler. The table-based sampler moves those different n-grams having common suffixes jointly into another topic. Figure 3 shows a transition of state by the tablebased sampler and Algorithm 4.2 depicts a highlevel description of one iteration. First, we select a table in a restaurant, which is shown with a dotted line in the figure. Next, we descend the tree to collect the tables connected to the selected table, which are pointed by arrows. Because this connection cannot be preserved in common data structures for a restaurant described in Teh (2006a) or Blunsom et al. (2009), we select the child tables randomly. This is correct because customers in CRP are exchangeAlgorithm 1 Table-based sampler for all table in all restaurants do Remove a customer from the parent restaurant. Construct a block of seating arrangement S by descending the tree recursively. Sample topic assignment zS — p(zS|S, S−S, z−S). Move S to sampled topic, and add a customer to the parent restaurant of the first selected table. end for able, so we can restore the parent-child relations arbitrarily. We continue this process recursively until reaching the leaf nodes, ob</context>
<context position="23186" citStr="Teh (2006" startWordPosition="4122" endWordPosition="4123">uct a block. The effects of A can be ignored because these are shared by all topics. SWITCHING In the SWITCHING, p(zS = k0|z−S) cannot be calculated in a closed form because p(hi|hji) in (9) would be changed dynamically when adding customers. This problem is the same one addressed by Blunsom and Cohn (2011), and we follow the same approximation in which, when we calculate the probability, we fractionally add tables and customers recursively. 4.3 Inference of Hyperparameters We also place a prior on each hyperparameter and sample value from the posterior distribution for every iteration. As in Teh (2006a), we set different values of a and b for each depth of PYP, but share across all topics and sample values with an auxiliary variable method. We also set different value of -y for each depth, on which we place Gamma(1,1). We make the topic prior α asymmetric: α = Qα0; Q — Gamma(1,1), α0 — Dir(1). 5 Related Work HMM-LDA (Griffiths et al., 2005) is a composite model of HMM and LDA that assumes the words in a document are generated by HMM, where only one state has a document-specific topic distribution. Our SWITCHING model can be understood as a lexical extension of HMM-LDA. It models the topica</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006b. A hierarchical bayesian language model based on pitman-yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 985– 992, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
<author>Iain Murray</author>
<author>Ruslan Salakhutdinov</author>
<author>David Mimno</author>
</authors>
<title>Evaluation methods for topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09,</booktitle>
<pages>1105--1112</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="29647" citStr="Wallach et al., 2009" startWordPosition="5207" endWordPosition="5210">pler as follows: For HPYLM, we ran 100 Gibbs iterations. For RESCALING, we ran 900 iterations on LDA and 100 iterations on HPYLM. For all other models, we ran 500 iterations of the Gibbs; HPYTMtoken is trained only on the token-based sampler, while for other models, the table-based sampler is performed after the token-based sampler. Evaluation We have to adapt to the topic distribution of unseen documents incrementally. Although previous works have employed incremental EM (Gildea and Hofmann, 1999; Tam and Schultz, 2005) because their inference is EM/VB-based, we use the left-to-right method (Wallach et al., 2009), which is a kind of particle filter updating the posterior topic distribution of a test document. We set the number of particles to 10 and resampled each particle every 10 words for all experiments. To get the final perplexity, after burn-in, we sampled 10 samples every 10 iterations of Gibbs, calculated a test perplexity for each sample, and averaged the results. Comparison of 3-grams Figure 4(d)–(f) shows perplexities when varying the number of topics. Generally, compared to the HPYTMtoken, the HPYTM got much perplexity gains, which again confirm the effectiveness of our blocked sampler. Bo</context>
</contexts>
<marker>Wallach, Murray, Salakhutdinov, Mimno, 2009</marker>
<rawString>Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov, and David Mimno. 2009. Evaluation methods for topic models. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pages 1105–1112, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
</authors>
<title>Topic modeling: beyond bagof-words.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd international conference on Machine learning, ICML ’06,</booktitle>
<pages>977--984</pages>
<contexts>
<context position="1480" citStr="Wallach, 2006" startWordPosition="224" endWordPosition="225"> n-grams. In terms of modeling assumption, we found it is effective to assign a topic to only some parts of a document. 1 Introduction N-gram language model is still ubiquitous in NLP, but due to its simplicity it fails to capture some important aspects of language, such as difference of word usage in different situations, sentence level syntactic correctness, and so on. Toward language model that can consider such a more global context, many extensions have been proposed from lexical pattern adaptation, e.g., adding cache (Jelinek et al., 1991) or topic information (Gildea and Hofmann, 1999; Wallach, 2006), to grammaticality aware models (Pauls and Klein, 2012). Topic language models are important for use in e.g., unsupervised language model adaptation: we want a language model that can adapt to the domain or topic of the current situation (e.g., a document in SMT or a conversation in ASR) automatically and select the appropriate words using both topic and syntactic context. Wallach (2006) is one such model, which generate each word based on local context and global topic information to capture the difference of lexical usage among different topics. However, Wallach’s experiments were limited t</context>
<context position="3652" citStr="Wallach (2006)" startWordPosition="579" endWordPosition="580">el blocked Gibbs sampler. When the number of topics is K and vocabulary size is V , n-gram topic model has O(KVn) parameters, which grow exponentially to n, making the local minima problem even more severe. Our sampler resolves this problem by moving many customers in the hierarchical Chinese restaurant process at a time. We evaluate various models by incremental calculation of test document perplexity on 3 types of corpora having different size and diversity. By combining the proposed prior and the sampling method, our Bayesian model achieve much higher accuracies than the naive extension of Wallach (2006) and shows results competitive with the unigram rescaling (Gildea and Hofmann, 1999), which require 1180 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1180–1190, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics huge computational cost at prediction, with much faster prediction time. 2 Basic Models All models presented in this paper are based on the Bayesian n-gram language model, the hierarchical Pitman-Yor process language model (HPYLM). In the following, we first introduce the HPYLM, and then discus</context>
<context position="6443" citStr="Wallach (2006)" startWordPosition="1060" endWordPosition="1061"> we can calculate the posterior p(w|h, w), which is often explained with the Chinese restaurant process (CRP): chw � athw p(w|h, w) = + ch· + b (2) where chw is an observed count of n-gram hw called customers, while thw is a hidden variable called tables. ch· and th· represents marginal counts: ch· = Ew chw and th· = Ew thw. This form is very similar to the well-known Kneser-Ney smoothing, and actually the Kneser-Ney can be understood as a heuristic approximation of the HPYLM. This characteristic enables us to build the state-of-the-art language model into a more complex generative model. 2.2 Wallach (2006) with HPYLM Wallach (2006) is a generative model for a document collection that combines the topic model with a Bayesian n-gram language model. The latent Dirichlet allocation (LDA) (Blei et al., 2003) is the most basic topic model, which generates each word in a document based on a unigram word distribution defined by a topic allocated to that word. The bigram topic model of Wallach (2006) simply replaces this unigram word distribution (a multinomial) for each topic with a bigram word distribution 1. In other words, ordinary LDA generates word conditioning only on the latent topic, whereas th</context>
<context position="7772" citStr="Wallach (2006)" startWordPosition="1305" endWordPosition="1306">ge model. Extending this model with a higher order n-gram is trivial; all we have to do is to replace the bigram language model for each topic with an ngram language model. The formal description of the generative story of this n-gram topic model is as follows. First, for each topic k E 1, · · · , K, where K is the number of topics, the model generates an n-gram language model Gkh.2 These n-gram models are generated by the PYP, so Gkh — PYP(a, b, Gkh,) holds. The model then generate a document collection. For each document j E 1, · · · , D, it generates a K1This is the model called prior 2 in Wallach (2006); it consistently outperformed the other prior. Wallach used the Dirichlet language model as each topic, but we only explore the model with HPYLM because its superiority to the Dirichlet language model has been well studied (Teh, 2006b). 2We sometimes denote Gh to represent a language model of topic k, not a specific multinomial for some context h, depending on the context. ath· + bp(w|h&apos; &apos; w), ch· + b 1181 dimensional topic distribution Bj by a Dirichlet distribution Dir(α) where α = (a1, a2, · · · , aK) is a prior. Finally, for each word position i ∈ 1, · · · , Nj where Nj is the number of w</context>
</contexts>
<marker>Wallach, 2006</marker>
<rawString>Hanna M. Wallach. 2006. Topic modeling: beyond bagof-words. In Proceedings of the 23rd international conference on Machine learning, ICML ’06, pages 977–984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuerui Wang</author>
<author>Andrew McCallum</author>
<author>Xing Wei</author>
</authors>
<title>Topical n-grams: Phrase and topic discovery, with an application to information retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Seventh IEEE International Conference on Data Mining, ICDM ’07,</booktitle>
<pages>697--702</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<marker>Wang, McCallum, Wei, 2007</marker>
<rawString>Xuerui Wang, Andrew McCallum, and Xing Wei. 2007. Topical n-grams: Phrase and topic discovery, with an application to information retrieval. In Proceedings of the 2007 Seventh IEEE International Conference on Data Mining, ICDM ’07, pages 697–702, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Wood</author>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical nonparametric Bayesian approach to statistical language model domain adaptation.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference on Artificial Intelligence and Statistics,</booktitle>
<volume>12</volume>
<contexts>
<context position="2916" citStr="Wood and Teh, 2009" startWordPosition="455" endWordPosition="458">ems caused when extending Wallach’s model to a higher-order: sparseness caused by dividing all n-grams into exclusive topics, and local minima caused by the deep hierarchy of the model. On resolving these problems, we make several contributions to both computational linguistics and machine learning. To address the first problem, we investigate incorporating a global language model for ease of sparseness, along with some priors on a suffix tree to capture the difference of topicality for each context, which include an unsupervised extension of the doubly hierarchical Pitman-Yor language model (Wood and Teh, 2009), a Bayesian generative model for supervised language model adaptation. For the second inference problem, we develop a novel blocked Gibbs sampler. When the number of topics is K and vocabulary size is V , n-gram topic model has O(KVn) parameters, which grow exponentially to n, making the local minima problem even more severe. Our sampler resolves this problem by moving many customers in the hierarchical Chinese restaurant process at a time. We evaluate various models by incremental calculation of test document perplexity on 3 types of corpora having different size and diversity. By combining </context>
<context position="10748" citStr="Wood and Teh (2009)" startWordPosition="1855" endWordPosition="1858">formalism, we can explain this using an Figure 1: Variable dependencies of the HIERARCHICAL model. {u, v} are word types, k is a topic and each Gh is a multinomial word distribution. For example, Gam„ represents a word distribution following the context uv in topic 2. abstract distribution F as Gkh ∼ F(G0h). The problem here is making the appropriate choice for the distribution F. Each topic word distribution already has hierarchies among n − 1-gram and n-gram contexts as Gkh ∼ PYP(a, b, Gkh,). A natural solution to this problem is the doubly hierarchical PitmanYor process (DHPYP) proposed in Wood and Teh (2009). Using this distribution, the new generative process of Gkh is Gkh ∼ PYP(a, b, AGkh, + (1 − A)G0h), (3) where A is a new hyperparameter that determines mixture weight. The dependencies among G0h and {Gkh} are shown in Figure 1. Note that the generative process of G0h is the same as the HPYLM (1). Let us clarify the DHPYP usage differences between our model and the previous work of Wood and Teh (2009). A key difference is the problem setting: Wood and Teh (2009) is aimed at the supervised adaptation of a language model for a specific domain, whereas our goal is unsupervised adaptation. In Wood</context>
<context position="12388" citStr="Wood and Teh (2009)" startWordPosition="2166" endWordPosition="2169">・ ・・・ 1182 ploiting the latent G0h. This makes inference harder and requires more careful design of λ. Modeling of λ We can better understand the role of λ in (3) by considering the posterior predictive form corresponds to (2), which is written as k k p(w|h,k,w) =chck+ bw+ h· (4) q(w|h, k, w) = λp(w|h&apos;, k, w) + (1 − λ)p(w|h, 0, w), where c, t with superscript k corresponds to the count existing in topic k. This shows us that λ determines the back-off behavior: which probability we should take into account: the shorter context of the same topic Gkh, or the full context of the global model G0h. Wood and Teh (2009) shares this variable across all contexts of the same length, for each k, but this assumption may not be the best. For example, after the context “in order”, we can predict the word “to” or “that”, and this tendency is unaffected by the topic. We call this property of context the topicality and say that “in order” has weak topicality. Therefore, we place λ as a distinct value for each context h, which we share across all topics. We designate this λ determined by h λh in the following. Moreover, similar contexts may have similar values of λh. For example, the two contexts “of the” and “in the”,</context>
<context position="17613" citStr="Wood and Teh (2009)" startWordPosition="3154" endWordPosition="3157">− 1 + Ek0 αk0 ,(8) where n−ji jk is the number of words that is assigned topic k in document j excluding wji, which is the same as the LDA. Given the sampled topic zji, we update the language model of topic zji, by adding customer wji to the restaurant specified by zji and context hji. See Teh (2006a) for details of these customer operations. HIERARCHICAL Adding customer operation is slightly changed: When a new table is added to a restaurant, we must track the label l E f0, 1g indicating the parent restaurant of that table, and add the customer corresponding to l to the restaurant of λh. See Wood and Teh (2009) for details of this operation. SWITCHING We replace p(zji|z−ji) with p(zji|z−ji) = p(lji = 0|hji) (zji = 0) n−ji jk +αk p(lji = 1|hji) �Ek,40 njkji+Ek0 αk0 (zji 0) , (9) where p(lji|hji) is a predictive of lji given by the CRP of λhji. We need not assign a label to a new table, but rather we always add a customer to the restaurant of λh according to whether the sampled topic is 0 or not. 4.2 Table-based Sampler One problem with the token-based sampler is that the seating arrangement of the internal restaurant would never be changed unless a new table is created (or an old table is removed) in</context>
</contexts>
<marker>Wood, Teh, 2009</marker>
<rawString>Frank Wood and Yee Whye Teh. 2009. A hierarchical nonparametric Bayesian approach to statistical language model domain adaptation. In Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 12.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000081">
<title confidence="0.986066">
Multi-Relational Latent Semantic Analysis
</title>
<author confidence="0.996674">
Kai-Wei Chang* Wen-tau Yih Christopher Meek
</author>
<affiliation confidence="0.867262">
University of Illinois Microsoft Research
Urbana, IL 61801, USA Redmond, WA 98052, USA
</affiliation>
<email confidence="0.998919">
kchang10@illinois.edu {scottyih,meek}@microsoft.com
</email>
<sectionHeader confidence="0.995636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999844473684211">
We present Multi-Relational Latent Seman-
tic Analysis (MRLSA) which generalizes La-
tent Semantic Analysis (LSA). MRLSA pro-
vides an elegant approach to combining mul-
tiple relations between words by construct-
ing a 3-way tensor. Similar to LSA, a low-
rank approximation of the tensor is derived
using a tensor decomposition. Each word in
the vocabulary is thus represented by a vec-
tor in the latent semantic space and each re-
lation is captured by a latent square matrix.
The degree of two words having a specific
relation can then be measured through sim-
ple linear algebraic operations. We demon-
strate that by integrating multiple relations
from both homogeneous and heterogeneous
information sources, MRLSA achieves state-
of-the-art performance on existing benchmark
datasets for two relations, antonymy and is-a.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999326717391304">
Continuous semantic space representations have
proven successful in a wide variety of NLP and IR
applications, such as document clustering (Xu et al.,
2003) and cross-lingual document retrieval (Dumais
et al., 1997; Platt et al., 2010) at the document level
and sentential semantics (Guo and Diab, 2012; Guo
and Diab, 2013) and syntactic parsing (Socher et
al., 2013) at the sentence level. Such representa-
tions also play an important role in applications for
lexical semantics, such as word sense disambigua-
tion (Boyd-Graber et al., 2007), measuring word
*Work conducted while interning at Microsoft Research.
similarity (Deerwester et al., 1990) and relational
similarity (Turney, 2006; Zhila et al., 2013; Mikolov
et al., 2013). In many of these applications, La-
tent Semantic Analysis (LSA) (Deerwester et al.,
1990) has been widely used, serving as a fundamen-
tal component or as a strong baseline.
LSA operates by mapping text objects, typically
documents and words, to a latent semantic space.
The proximity of the vectors in this space implies
that the original text objects are semantically re-
lated. However, one well-known limitation of LSA
is that it is unable to differentiate fine-grained re-
lations. For instance, when applied to lexical se-
mantics, synonyms and antonyms may both be as-
signed high similarity scores (Landauer and Laham,
1998; Landauer, 2002). Asymmetric relations like
hyponyms and hypernyms also cannot be differenti-
ated. Although there exists some recent work, such
as PILSA which tries to overcome this weakness
of LSA by introducing the notion of polarity (Yih
et al., 2012). This extension, however, can only
handle two opposing relations (e.g., synonyms and
antonyms), leaving open the challenge of encoding
multiple relations.
In this paper, we propose Multi-Relational Latent
Semantic Analysis (MRLSA), which strictly gener-
alizes LSA to incorporate information of multiple
relations concurrently. Similar to LSA or PILSA
when applied to lexical semantics, each word is still
mapped to a vector in the latent space. However,
when measuring whether two words have a specific
relation (e.g., antonymy or is-a), the word vectors
will be mapped to a new space according to the rela-
tion where the degree of having this relation will be
</bodyText>
<page confidence="0.965161">
1602
</page>
<note confidence="0.732791">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1602–1612,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999864266666667">
judged by cosine similarity. The raw data construc-
tion in MRLSA is straightforward and similar to the
document-term matrix in LSA. However, instead of
using one matrix to capture all relations, we extend
the representation to a 3-way tensor. Each slice cor-
responds to the document-term matrix in the original
LSA design but for a specific relation. Analogous to
LSA, the whole linear transformation mapping is de-
rived through tensor decomposition, which provides
a low-rank approximation of the original tensor. As
a result, previously unseen relations between two
words can be discovered, and the information en-
coded in other relations can influence the construc-
tion of the latent representations, and thus poten-
tially improves the overall quality. In addition, the
information in different slices can come from het-
erogeneous sources (conceptually similar to (Riedel
et al., 2013)), which not only improves the model,
but also extends the word coverage in a reliable way.
We provide empirical evidence that MRLSA is ef-
fective using two different word relations: antonymy
and is-a. We use the benchmark GRE test of closest-
opposites (Mohammad et al., 2008) to show that
MRLSA performs comparably to PILSA, which was
the pervious state-of-the-art approach on this prob-
lem, when given the same amount of information. In
addition, when other words and relations are avail-
able, potentially from additional resources, MRLSA
is able to outperform previous methods significantly.
We use the is-a relation to demonstrate that MRLSA
is capable of handling asymmetric relations. We
take the list of word pairs from the Class-Inclusion
(i.e., is-a) relations in SemEval-2012 Task 2 (Jur-
gens et al., 2012), and use our model to measure the
degree of two words have this relation. The mea-
sures derived from our model correlate with human
judgement better than the best system that partici-
pated in the task.
The rest of this paper is organized as follows. We
first survey some related work in Section 2, followed
by a more detailed description of LSA and PILSA
in Section 3. Our proposed model, MRLSA, is pre-
sented in Section 4. Section 5 presents our experi-
mental results. Finally, Section 6 concludes the pa-
per.
</bodyText>
<sectionHeader confidence="0.999339" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999751391304348">
MRLSA can be viewed as a model that derives gen-
eral continuous space representations for capturing
lexical semantics, with the help of tensor decompo-
sition techniques. We highlight some recent work
related to our approach.
The most commonly used continuous space rep-
resentation of text is arguably the vector space
model (VSM) (Turney and Pantel, 2010). In this
representation, each text object can be represented
by a high-dimensional sparse vector, such as a
term-vector or a document-vector that denotes the
statistics of term occurrences (Salton et al., 1975)
in a large corpus. The text can also be repre-
sented by a low-dimensional dense vector derived
by linear projection models like latent semantic
analysis (LSA) (Deerwester et al., 1990), by dis-
criminative learning methods like Siamese neural
networks (Yih et al., 2011), recurrent neural net-
works (Mikolov et al., 2013) and recursive neu-
ral networks (Socher et al., 2011), or by graphical
models such as probabilistic latent semantic anal-
ysis (PLSA) (Hofmann, 1999) and latent Dirichlet
allocation (LDA) (Blei et al., 2003). As a general-
ization of LSA, MRLSA is also a linear projection
model. However, while the words are represented
by vectors as well, multiple relations between words
are captured separately by matrices.
In the context of lexical semantics, VSMs provide
a natural way of measuring semantic word related-
ness by computing the distance between the cor-
responding vectors, which has been a standard ap-
proach (Agirre et al., 2009; Reisinger and Mooney,
2010; Yih and Qazvinian, 2012). These approaches
do not apply directly to the problem of modeling
other types of relations. Existing methods that do
handle multiple relations often use a model com-
bination scheme to integrate signals from various
types of information sources. For instance, mor-
phological variations discovered from the Google
n-gram corpus have been combined with informa-
tion from thesauri and vector-based word related-
ness models for detecting antonyms (Mohammad et
al., 2008). An alternative approach proposed by Tur-
ney (2008) that handles synonyms, antonyms and
associations is to use a uniform approach by first
reducing the problem to determining whether two
</bodyText>
<page confidence="0.971709">
1603
</page>
<bodyText confidence="0.9999632">
pairs of words can be analogous, and then predicting
it using a supervised model with features based on
the frequencies of patterns in the corpus. Similarly,
to measure whether two word pairs have the same
relation, Zhila et al. (2013) proposed to combine het-
erogeneous models, which achieved state-of-the-art
performance. In comparison, MRLSA models mul-
tiple lexical relations holistically. The degree that
two words having a particular relation is estimated
using the same linear function of the corresponding
vectors and matrix.
Tensor decomposition generalizes matrix factor-
ization and has been applied to several NLP applica-
tions recently. For example, Cohen et al. (2013) pro-
posed an approximation algorithm for PCFG pars-
ing that relies on Kruskal decomposition. Van de
Cruys et al. (2013) modeled the composition of
subject-verb-object triples using Tucker decompo-
sition, which results in a better similarity measure
for transitive phrases. Similar to this construction
but used in the community-based question answer-
ing (CQA) scenario, Qiu et al. (2013) represented
triples of question title, question content and answer
as a tensor and applied 3-mode SVD to derive latent
semantic representations for question matching. The
construction of MRLSA bears some resemblance to
the work that use tensors to capture triples. How-
ever, our goal of modeling different relations for lex-
ical semantics is very different from the intended us-
age of tensor decomposition in the existing work.
</bodyText>
<sectionHeader confidence="0.958421" genericHeader="method">
3 Latent Semantic Analysis
</sectionHeader>
<bodyText confidence="0.987170133333333">
Latent Semantic Analysis (LSA) (Deerwester et al.,
1990) is a widely used continuous vector space
model that maps words and documents into a low
dimensional space. LSA consists of two main steps.
First, taking a collection of d documents that con-
tains words from a vocabulary list of size n, it first
constructs a d x n document-term matrix W to en-
code the occurrence information of a word in a docu-
ment. For instance, in its simplest form, the element
Wjj can be the term frequency of the j-th word in
the i-th document. In practice, a weighting scheme
that better captures the importance of a word in the
document, such as TFxIDF (Salton et al., 1975),
is often used instead. Notice that “document” here
simply means a group of words and has been applied
</bodyText>
<figureCaption confidence="0.7706735">
Figure 1: SVD applied to a dxn document-term ma-
trix W. The rank-k approximation, X, is the mul-
</figureCaption>
<bodyText confidence="0.996598727272727">
tiplication of U, E and VT, where U and V are
d x k and n x k orthonormal matrices and E is a
k x k diagonal matrix. The column vectors of VT
multiplied by the singular values E represent words
in the latent semantic space.
to various texts including news articles, sentences
and bags of words. Once the matrix is constructed,
the second step is to apply singular value decom-
position (SVD) to W in order to derive a low-rank
approximation. To have a rank-k approximation, X
is the reconstruction matrix of W, defined as
</bodyText>
<equation confidence="0.999485">
W Pz� X = UEVT (1)
</equation>
<bodyText confidence="0.999991916666667">
where the dimensions of U and V are d x k and
n x k, respectively, and E is a k x k diagonal ma-
trix. In addition, the columns in U and V are or-
thonormal and the elements in E are the singular
values and are conventionally reverse-ordered. Fig-
ure 1 illustrates this decomposition.
LSA can be used to compute the similarity be-
tween two documents or two words in the latent
space. For instance, to compare the u-th and v-th
words in the vocabulary, one can compute the co-
sine similarity of the u-th and v-th column vectors
of X, the reconstruction matrix of W. In contrast to
a direct lexical matching via the columns of W, the
similarity measure computed as a result of the SVD
may have a nonzero similarity score even if these
two words do not co-occur in any documents. This
is due to the fact that those words can share some
latent components.
An alternative view of using LSA is to treat the
column vectors of EVT as a representation of the
words in a new k-dimensional latent space. This
comes from the observation that the inner product
of every two column vectors in X is the inner prod-
uct of the corresponding column vectors of EVT,
</bodyText>
<equation confidence="0.56913175">
W
X
= U
VT
</equation>
<page confidence="0.922814">
1604
</page>
<bodyText confidence="0.91708975">
joyfulness 1 1 -1 0 0 0 0
gladden 1 1 0 -1 0 0 0
sad -1 0 1 1 0 0 0
anger 0 0 0 0 1 0 0
</bodyText>
<figureCaption confidence="0.721706">
Figure 2: The matrix construction of PILSA. The
vocabulary is {joy, gladden, sorrow, sadden, anger,
emotion, feeling} and target words are {joyfulness,
</figureCaption>
<bodyText confidence="0.6850704">
gladden, sad, anger}. For ease of presentation,
we show the numbers with 0-1 values instead of
TFxIDF scores. The polarity (i.e., sign) indicates
whether the term in the vocabulary is a synonym or
antonym of the target word.
</bodyText>
<equation confidence="0.9072598">
which can be derived from the equations below.
XTX = (UEVT)T(UEVT)
= VEUTUEVT (E is diagonal)
= VE2VT (Columns of U are orthonormal)
= (EVT )T (EVT) (2)
</equation>
<bodyText confidence="0.999147">
Thus, the semantic relatedness between the i-th and
j-th words can be computed by cosine similarity1:
</bodyText>
<equation confidence="0.801765">
cos(X:�i, X:�j) (3)
</equation>
<bodyText confidence="0.999897083333333">
When used to compare words, one well-known
limitation of LSA is that the score captures the gen-
eral notion of semantic similarity, and is unable
to distinguish fine-grained word relations, such as
antonyms (Landauer and Laham, 1998; Landauer,
2002). This is due to the fact that the raw matrix rep-
resentation only records the occurrences of words in
documents without knowing the specific relation be-
tween the word and document. To address this issue,
Yih et al. (2012) proposed a polarity inducing latent
semantic analysis model recently, which we intro-
duce next.
</bodyText>
<footnote confidence="0.9942095">
1Cosine similarity is equivalent to the inner product of the
normalized vectors.
</footnote>
<subsectionHeader confidence="0.995573">
3.1 Polarity Inducing Latent Semantic
Analysis
</subsectionHeader>
<bodyText confidence="0.99999296969697">
In order to distinguish antonyms from synonyms,
the polarity inducing LSA (PILSA) model (Yih et
al., 2012) takes a thesaurus as input. Synonyms and
antonyms of the same target word are grouped to-
gether as a “document” and a document-term matrix
is constructed accordingly as done in LSA. Because
each word in a group belongs to either one of the two
opposite relations, synonymy and antonymy, the po-
larity information is induced by flipping the signs of
antonyms. While the absolute value of each element
in the matrix is still the same TFxIDF score, the
elements that correspond to the antonyms become
negative.
This design has an intriguing effect. When com-
paring two words using the cosine similarity (or sim-
ply inner product) of their corresponding column
vectors in the matrix, the score of a synonym pair
remains positive, but the score of an antonym pair
becomes negative. Figure 2 illustrates this design
using a simplified matrix as example.
Once the matrix is constructed, PILSA applies
SVD as done in LSA, which generalizes the model
to go beyond lexical matching. The sign of the co-
sine score of the column vectors of any two words
indicates whether they are close to synonyms or to
antonyms and the absolute value reflects the degree
of the relation. When all the column vectors are nor-
malized to unit vectors, it can also be viewed as syn-
onyms are clustered together and antonyms lie on
the opposite sides of a unit sphere. Although PILSA
successfully extends LSA to handle not just one sin-
gle occurrence relation, the extension is limited to
encoding two opposing relations
</bodyText>
<sectionHeader confidence="0.842728" genericHeader="method">
4 Multi-Relational Latent Semantic
Analysis
</sectionHeader>
<bodyText confidence="0.999991222222222">
The fundamental reason why it is difficult to handle
multiple relations is due to the 2-dimensional ma-
trix representation. In order to overcome this, we
encode the raw data in a 3-way tensor. Each slice
captures a particular relation and is in the format of
the document-term matrix in LSA. Just as in LSA,
where the low-rank approximation by SVD helps
generalize the representation and discover unseen
relations, we apply a tensor decomposition method,
</bodyText>
<page confidence="0.610414">
1605
</page>
<bodyText confidence="0.9986055">
joyfulness 1 1 0 0 0 0 0 joyfulness 0 0 1 0 0 0 0 joyfulness 0 0 0 0 0 1 1
gladden 1 1 0 0 0 0 0 gladden 0 0 0 1 0 0 0 gladden 0 0 0 0 0 0 0
sad 0 0 1 1 0 0 0 sad 1 0 0 0 0 0 0 sad 0 0 0 0 0 1 1
anger 0 0 0 0 1 0 0 anger 0 0 0 0 0 0 0 anger 0 0 0 0 0 1 1
</bodyText>
<figure confidence="0.985968">
(a) Synonym layer (b) Antonym layer (c) Hypernym layer
</figure>
<figureCaption confidence="0.999112">
Figure 3: The three slices of MRLSA raw tensor W for an example with vocabulary {joy, gladden, sorrow,
</figureCaption>
<bodyText confidence="0.7347496">
sadden, anger, emotion, feeling} and target words {joyfulness, gladden, sad, anger}. Figures 3(a), 3(b), 3(c)
show the matrices W:,:,syn, W:,:,ant, W:,:,hyper, respectively. Rows represent documents (see definition in
text), and columns represent words. For ease of presentation, we show numbers with 0-1 values instead of
TFxIDF scores.
the Tucker decomposition, to the tensor.
</bodyText>
<subsectionHeader confidence="0.9775485">
4.1 Representing Multi-Relational Data in
Tensors
</subsectionHeader>
<bodyText confidence="0.9998755">
A tensor is simply a multi-dimensional array. In this
work, we use a 3-way tensor W to encode multi-
ple word relations. An element of W is denoted
by WZ,,,k using its indices, and W:,:,k represents
the k-th slice of W (a slice of a 3-way tensor is
a matrix, obtained by fixing the third index). Fol-
lowing (Kolda and Bader, 2009), a fiber of a ten-
sor W:,,,k is a vector, which is a high order analog
of a matrix row or column.
When constructing the raw tensor W in MRLSA,
each slice is analogous to the document-term ma-
trix in LSA, but created based on the data of a par-
ticular relation, such as synonyms. With a slight
abuse of notation, we sometimes use the value rather
than index when there is no confusion. For in-
stance, W:,“word»,k represents the fiber correspond-
ing to the “word” in slice k, and W:,:,syn refers to
the slice that encodes the synonymy relation. Below
we use an example to compare this construction to
the raw matrix in PILSA, and discuss how it extends
LSA.
Suppose we are interested in representing two re-
lations, synonymy and antonymy. The raw tensor in
MRLSA would then consist of two slices, W:,:,syn
and W:,:,ant, to encode synonyms and antonyms of
target words from a knowledge source (e.g., a the-
saurus). Each row in W:,:,syn represents the syn-
onyms of a target word, and the corresponding
row in W:,:,ant encodes its antonyms. Figures 3(a)
and 3(b) illustrate an example, where “joy”, ”glad-
den” are synonyms of the target word “joyfulness”
and “sorrow” is its antonym. Therefore, the values
of the corresponding entries are 1. Notice that the
matrix W&apos; = W:,:,syn − W:,:,ant is identical to the
PILSA raw matrix. We can extend the construction
above to enable MRLSA to utilize other semantic
relations (e.g., hypernymy) by adding a slice cor-
responding to each relation of interest. Fig. 3(c)
demonstrates how to add another slice W:,:,hyper to
the tensor for encoding hypernyms.
</bodyText>
<subsectionHeader confidence="0.923523">
4.2 Tensor Decomposition
</subsectionHeader>
<bodyText confidence="0.999888733333333">
The MRLSA raw tensor encodes relations in one or
more data resources, such as thesauri. However, the
knowledge from a thesaurus is usually noisy and in-
complete. In this section, we derive a low-rank ap-
proximation of the tensor to generalize the knowl-
edge. This step is analogous to the rank-k approxi-
mation in LSA.
Various tensor decomposition methods have been
proposed in literature. Among them, Tucker decom-
position (Tucker, 1966) is recognized as a multi-
dimensional extension of SVD and has been widely
used in many applications. An illustration of this
method is in Fig. 4(a). In Tucker decomposition,
a d x n x m tensor W is decomposed into four
components !g, U, V, T. A low-rank approximation
</bodyText>
<page confidence="0.961854">
1606
</page>
<figure confidence="0.999837166666667">
(a) Tucker Tensor Decomposition
(b) Our Reformulation
T
VT
= U
W
X
=
U
S
VT
S:,:,1
</figure>
<figureCaption confidence="0.8494905">
Figure 4: Fig. 4(a) illustrates the Tucker tensor decomposition method which factors a 3-way tensor W to
three orthogonal matrices, U, V, T, and a core tensor G. We further apply a n-mode matrix product on the
core tensor G with T. Consequently, each slice of the resulted core tensor S (a square matrix) captures a
semantic relation type, and each column of VT is a vector representing a word.
</figureCaption>
<equation confidence="0.774872">
X of W is defined by
Wi,j,k ≈ Xi,j,k
Gr1,r2,r3Ui,r1Vj,r2Tk,r3,
</equation>
<bodyText confidence="0.999879823529412">
where G is a core tensor with dimensions R1 ×R2 ×
R3 and U, V, T are orthogonal matrices with di-
mensions d × R1, n × R2, m × R3, respectively.
The rank parameters R1 ≤ d, R2 ≤ n, R3 ≤ m are
given as input to the algorithm. In MRLSA, m (the
number of relations) is usually small, while d and n
are typically large (often in the scale of hundreds
of thousands). Therefore, we choose R1 = R2 = T,
T « d, n and R3 = m, where T is typically less than
1000.
To make the analogy to SVD clear, we rewrite the
results of Tucker decomposition by performing a n-
mode matrix product over the core tensor G with the
matrix T. This produces a tensor S where each slice
is a linear combination of the slices of G with coeffi-
cients given by T (see (Kolda and Bader, 2009) for
detail). That is, we have
</bodyText>
<equation confidence="0.746777">
S:,:,k = �m Tt,kG:,:,t, ∀k.
t=1
</equation>
<bodyText confidence="0.946433333333333">
An illustration is shown in Fig. 4(b), Then, a
straightforward calculation shows that k-th slice of
tensor W is approximated by
</bodyText>
<equation confidence="0.901945">
W:,:,k ≈ X:,:,k = US:,:,kVT. (4)
</equation>
<bodyText confidence="0.999681125">
Comparing Eq. (4) to Eq. (1), one can observe
that matrices U and V play similar roles here, and
each slice of the core tensor S is analogous to Σ.
However, the square matrix G:,:,k is not necessary
to be diagonal. As in SVD, the column vectors
of G:,:,kVT (capture both word and relation infor-
mation) behave similarly to the column vectors of
the original tensor slice W:,:,k.
</bodyText>
<subsectionHeader confidence="0.999393">
4.3 Measuring the Degrees of Word Relations
</subsectionHeader>
<bodyText confidence="0.9999156">
In principle, the raw information in the input ten-
sor W can be used for computing lexical similarity
using the cosine score between the column vectors
for two words from the same slice of the tensor. To
measure the degree of other relations, however, our
approach requires one to specify a pivot slice. The
key role of the pivot slice is to expand the lexical
coverage of the relation of interest to additional lexi-
cal entries and, for this reason, the pivot slice should
be chosen to capture the equivalence of the lexical
entries. In this paper, we use the synonymy relation
as our pivot slice. First we consider measuring the
degree of a relation rel holding between the i-th and
j-th words using the raw tensor W, which can be
computed as
</bodyText>
<equation confidence="0.8923375">
(5)
cos (W:,i,syn, W:,j,rel) .
</equation>
<bodyText confidence="0.999455428571429">
This measurement can be motivated from the logical
rule: syn(wordi, target) ∧ rel(target, wordj) →
rel(wordi, wordj), where the pivot relation syn ex-
pands the coverage of the relation of interest rel.
Turning to the use of the tensor decomposition,
we use a similar derivation to Eq. (3), and measure
the degree of relation rel between two words by
</bodyText>
<equation confidence="0.9972492">
cos (S:,:,synVTi,:, S:,:,relVT ) . (6)
j,:
R1 R2 R3
= � �
r1=1 r2=1 r3=1
</equation>
<page confidence="0.910256">
1607
</page>
<bodyText confidence="0.999975916666667">
For instance, the degree of antonymy between
“joy” and “sorrow” is measured by the co-
sine similarity between the respective fibers
cos(X,“joy”,syn, X,“sorrow”,ant). We can encode both
symmetric relations (e.g., antonymy and synonymy)
and asymmetric relations (e.g., hypernymy and
hyponymy) in the same tensor representation. For a
symmetric relation, we use both cos(X,i,syn, X,j,rel)
and cos(X,j,syn, X,i,rel) and measure the degree of
a symmetric relation by the average of these two
cosine similarity scores. However, for asymmetric
relations, we use only cos(X,i,syn, X,j,rel).
</bodyText>
<sectionHeader confidence="0.999387" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99993775">
We evaluate MRLSA on two tasks: answering the
closest-opposite GRE questions and measuring de-
grees of various class-inclusion (i.e., is-a) relations.
In both tasks, we design the experiments to empir-
ically validate the following claims. When encod-
ing two opposite relations from the same source,
MRLSA performs comparably to PILSA. However,
MRLSA generalizes LSA to model multiple rela-
tions, which could be obtained from both homoge-
neous and heterogeneous data sources. As a result,
the performance of a target task can be further im-
proved.
</bodyText>
<subsectionHeader confidence="0.952205">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999928058823529">
We construct the raw tensors to encode a particular
relation in each slice based on two data sources.
Encarta The Encarta thesaurus is developed by
Bloomsbury Publishing Plc2. For each target word,
it provides a list of synonyms and antonyms. We
use the same version of the thesaurus as in (Yih et
al., 2012), which contains about 47k words and a
vocabulary list of approximately 50k words.
WordNet We use four types of relations from
WordNet: synonymy, antonymy, hypernymy and
hyponymy. The number of target words and the
size of the vocabulary in our version are 117,791
and 149,400, respectively. WordNet has better vo-
cabulary coverage, but fewer antonym pairs. For
instance, the WordNet antonym slice contains only
46,945 nonzero entries, while the Encarta antonym
slice has 129,733.
</bodyText>
<footnote confidence="0.911397">
2http://www.bloomsbury.com
</footnote>
<bodyText confidence="0.999941166666667">
We apply a memory-efficient Tucker decomposi-
tion algorithm (Kolda and Sun, 2008) implemented
in tensor toolbox v2.5 (Bader et al., 2012)3 to factor
the tensor. The largest tensor considered in this pa-
per can be decomposed in about 3 hours using less
than 4GB of memory with a commodity PC.
</bodyText>
<subsectionHeader confidence="0.999684">
5.2 Answering GRE Antonym Questions
</subsectionHeader>
<bodyText confidence="0.998431484848485">
The first task is to answer the closest-opposite ques-
tions from the GRE test provided by Mohammad et
al. (2008)4. Each question in this test consists of
a target word and five candidate words, where the
goal is to pick the candidate word that has the most
opposite meaning to the target word. In order to
have a fair comparison, we use the same data split
as in (Mohammad et al., 2008), with 162 questions
used for the development set and 950 for test. Fol-
lowing (Mohammad et al., 2008; Yih et al., 2012),
we report the results in precision (accuracy of the
questions that the system attempts to answer), re-
call (percentage of the questions answered correctly
over all questions) and F1 (the harmonic mean of
precision and recall).
We tune two sets of parameters using the devel-
opment set: (1) the rank parameter T in the tensor
decomposition and (2) the scaling factors of differ-
ent slices of the tensor. The rank parameter spec-
ifies the number of dimensions of the latent space.
In the experiments, We pick the best value of T from
1100, 200, 300, 500, 750, 10001. The scaling factors
adjust the values of each slice of the tensor. The el-
ements of each slice are multiplied by the scaling
factor before factorization. This is important be-
cause Tucker decomposition minimizes the recon-
struction error (the Frobenius norm of the residual
tensor). As a result, the slice with a larger range of
values becomes more influential to U and V. In this
work, we fix W:,:,ant, and search for the scaling fac-
tor of W:,:,syn in 10.25,0.5, 1, 2,41 and the factors
of W:,:,hyper and W:,:,hypo in 10.0625, 0.125, 0.251.
Table 1 summarizes the results of training
</bodyText>
<footnote confidence="0.725441714285714">
3http://www.sandia.gov/˜tgkolda/
TensorToolbox. The Tucker decomposition involves
performing SVD on a large matrix. We modify the MATLAB
code of tensor toolbox to use the built-in svd function instead
of svds. This modification reduces both the running time and
memory usage.
4http://www.saifmohammad.com
</footnote>
<page confidence="0.956089">
1608
</page>
<table confidence="0.999122916666667">
Dev. Set Fi Test Set Fi
Prec. Rec. Prec. Rec.
WordNet Lookup 0.40 0.40 0.40 0.42 0.41 0.42
WordNet RawTensor 0.42 0.41 0.42 0.42 0.41 0.42
WordNet PILSA 0.63 0.62 0.62 0.60 0.60 0.60
WordNet MRLSA:Syn+Ant 0.63 0.62 0.62 0.59 0.58 0.59
WordNet MRLSA:4-layers 0.66 0.65 0.65 0.61 0.59 0.60
Encarta Lookup 0.65 0.61 0.63 0.61 0.56 0.59
Encarta RawTensor 0.67 0.64 0.65 0.62 0.57 0.59
Encarta PILSA 0.86 0.81 0.84 0.81 0.74 0.77
Encarta MRLSA:Syn+Ant 0.87 0.82 0.84 0.82 0.74 0.78
MRLSA:WordNet+Encarta 0.88 0.85 0.87 0.81 0.77 0.79
</table>
<tableCaption confidence="0.998891">
Table 1: GRE antonym test results of models based on Encarta and WordNet data in precision, recall and Fl.
</tableCaption>
<bodyText confidence="0.98949274137931">
RawTensor evaluates the performance of the tensor with 2 slices encoding synonyms and antonyms be-
fore decomposition (see Eq. (5)), which is comparable to checking the original data directly (Lookup).
MRLSA:Syn+Ant applies Tucker decomposition to the raw tensor and measures the degree of antonymy
using Eq. (6). The result is similar to that of PILSA (see Sec. 3.1). MRLSA:4-layers adds hypernyms and
hyponyms from WordNet; MRLSA:WordNet+Encarta consists of synonyms/antonyms from Encarta and hy-
pernyms/hyponyms from WordNet, where the target words are aligned using the synonymy relations. Both
models demonstrate the advantage of encoding more relations, from either the same or different resources.
MRLSA using two different corpora, Encarta and
WordNet. The performance of the MRLSA raw
tensor is close to that of looking up the thesaurus.
This indicates the tensor representation is able to
capture the word relations explicitly described in
the thesaurus. After conducting tensor decomposi-
tion, MRLSA:Syn+Ant achieves similar results to
PILSA. This confirms our claim that when giv-
ing the same among of information, MRLSA per-
forms at least comparably to PILSA. However, the
true power of MRLSA is its ability to incorpo-
rate other semantic relations to boost the perfor-
mance of the target task. For example, when
we add the hypernymy and hyponymy relations to
the tensor, these class-inclusion relations provide a
weak signal to help resolve antonymy. We sus-
pect that this is due to the fact that antonyms typ-
ically share the same properties but only have the
opposite meaning on one particular semantic di-
mension. For instance, the antonyms “sadness”
and “happiness” are different forms of emotion.
When two words are hyponyms of a target word,
the likelihood that they are antonyms should thus
be increased. We show that the target relations
and these auxiliary semantic relations can be col-
lected from the same data source (e.g., WordNet
MRLSA:4-layers) or from multiple, heterogeneous
sources (e.g., MRLSA:WordNET+Encarta). In both
cases, the performance of the model improves as
more relations are incorporated. Moreover, our ex-
periments show that adding the hypernym and hy-
ponym layers from WordNet improves modeling
antonym relations based on the Encarta thesaurus.
This suggests that the weak signal from a resource
with a large vocabulary (e.g., WordNet) can help
predict relations between out-of-vocabulary words
and thus improve the recall.
To better understand the model, we examine the
top antonyms for three question words from the
GRE test. The lists below show antonyms and their
MRLSA scores for each of the GRE question words
as determined by the MRLSA:WordNET+Encarta
model. Antonyms that can be found directly in the
Encarta thesaurus are in italics.
inanimate alive (0.91), living (0.90), bodily (0.90), in-
the-flesh (0.89), incarnate (0.89)
alleviate exacerbate (0.68), make-worse (0.67), in-
flame (0.66), amplify (0.65), stir-up (0.64)
relish detest (0.33), abhor (0.33), abominate (0.33), de-
spise (0.33), loathe (0.31)
We can see that from these examples, MRLSA not
</bodyText>
<page confidence="0.978253">
1609
</page>
<table confidence="0.99695075">
Dev. 1b (Functional) Test 1d (Plural) Avg.
1a (Taxonomic) 1c (Singular)
WordNet Lookup 52.9 34.5 41.4 34.3 36.7
WordNet RawTensor 51.0 38.3 50.0 42.1 43.5
WordNet MRLSA:Syn+Hypony 55.8 41.7 (43.2) 51.0 (51.4) 37.5 (44.4) 43.4 (46.3)
WordNet MRLSA:4-layers 52.9 51.5 (53.9) 51.9 (60.0) 43.5 (50.5) 49.0 (54.8)
MRLSA:WordNet+Encarta 62.1 55.3 (58.7) 57.1 (65.7) 48.6 (53.7) 55.8 (60.1)
UTDNB (Rink and Harabagiu, 2012) - 38.3 36.7 28.2 34.4
</table>
<tableCaption confidence="0.997599">
Table 2: Results of measuring the class-inclusion (is-a) relations in MaxDiff accuracy (see text for de-
</tableCaption>
<bodyText confidence="0.93067">
tail). RawTensor has synonym and hyponym slices and measures the degree of is-a relation using Eq. (5).
MRLSA:Syn+Hypo factors the raw tensor and judges the relation by Eq. (6). The constructions of
MRLSA:4-layers and MRLSA:WordNet+Encarta are the same as in Sec. 5.2 (see the caption of Table 1
for detail). For MRLSA models, numbers shown in the parentheses are the results when parameters are
tuned using the test sets. UTDNB is the results of the best performing system in SemEval-2012 Task 2.
only preserves the antonyms in the thesaurus, but
also discovers additional ones, such as exacerbate
and inflame for “alleviate”. Another interesting find-
ing is that while the scores are useful in ranking
the candidate words, they might not be comparable
across different question words. This could be an
issue for some applications, which need to make a
binary decision on whether two words are antonyms.
</bodyText>
<subsectionHeader confidence="0.998799">
5.3 Measuring degrees of Is-A relations
</subsectionHeader>
<bodyText confidence="0.999955444444445">
We evaluate MRLSA using the class-inclusion por-
tion of SemEval-2012 Task 2 data (Jurgens et al.,
2012). Here the goal is to measure the degree
of two words having the is-a relation. Five an-
notated datasets are provided for different subcate-
gories of this relation: 1a-taxonomic, 1b-functional,
1c-singular, 1d-plural, 1e-class individual. We omit
1e because it focuses on real world entities (e.g.,
queen:Elizabeth, river:Nile), which are not included
in WordNet.
Each dataset contains about 100 questions based
on approximately 40 word pairs. The question con-
sists of 4 randomly chosen word pairs and asks the
best and worst pairs that exemplify the specific is-a
relation. The performance is measured by the av-
erage prediction accuracy, also called the MaxDiff
accuracy (Louviere and Woodworth, 1991).
Because the questions are generated from the
same set of word pairs, these questions are not mutu-
ally independent. Therefore, it is not proper to split
the data of each subcategory into the development
and test sets. Alternatively, we follow the setting
of SemEval-2012 Task 2 and use the first subcat-
egory (1a-taxonomy) to tune the model and eval-
uate its performance based on the results on other
datasets. Since the models are tuned and tested on
different types of subcategories, they might not be
the optimal ones when evaluated on the test sets.
Therefore, we show results using the best parame-
ters tuned on the development set and those tuned on
the test set, where the latter suggests a performance
upper-bound. Besides the rank parameter, we tune
the scaling factors of the synonym, hypernym and
hyponym slices from 14, 16, 641. The scaling factor
of the antonym slice is fixed to 1.
Table 2 shows the performance in MaxDiff accu-
racy. Results show that even the raw tensor repre-
sentation (RawTensor) performs better than Word-
Net lookup. We suspect that this is because the
tensor representation can capture the fact that the
hyponyms of a word are usually synonymous to
each other. By performing Tucker decomposition
on the raw Tensor, MRLSA achieves better per-
formance. MRLSA:4-layers further leverages the
information from antonyms and hypernyms and
thus improves the model. As we notice in the
GRE antonym test, models based on the Encarta
thesaurus perform better in predicting antonyms.
Therefore, it is interesting to check if combining
synonyms and antonyms from Encarta helps. As
a result, MRLSA:WordNet+Encarta improves over
MRLSA:4-layers significantly. This demonstrates
again that MRLSA can leverage knowledge stored in
heterogeneous resources. Notably, MRLSA outper-
</bodyText>
<page confidence="0.958759">
1610
</page>
<bodyText confidence="0.957247523809524">
forms the best system participated in the SemEval-
2012 task with a large margin, with a difference of
21.4 in MaxDiff accuracy.
Next we examine the top words that have the is-
a relation relative to three question words from the
task. The lists below show the hyponyms and their
respective MRLSA scores for each of the question
words as determined by MRLSA:4-layers.
bird ostrich (0.75), gamecock (0.75), nighthawk (0.75),
amazon (0.74), parrot (0.74)
automobile minivan (0.48), wagon (0.48), taxi (0.46),
minicab (0.45), gypsy cab (0.45)
vegetable buttercrunch (0.61), yellow turnip (0.61), ro-
maine (0.61), chipotle (0.61), chilli (0.61)
Although the model in general does a good job
finding hyponyms, we observe that some suggested
words, such as buttercrunch (a mild lettuce) vs.
“vegetable”, do not seem intuitive (e.g., compared to
carrot). Having one additional slice to capture the
general term co-occurrence relation may help im-
prove the model in this respect.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999983">
In this paper, we propose Multi-Relational Latent
Semantic Analysis (MRLSA) which generalizes La-
tent Semantic Analysis (LSA) for lexical seman-
tics. MRLSA models multiple word relations by
leveraging a 3-way tensor, where each slice cap-
tures one particular relation. A low-rank approx-
imation of the tensor is then derived using a ten-
sor decomposition. Consequently, words in the vo-
cabulary are represented by vectors in the latent se-
mantic space, and each relation is captured by a
latent square matrix. Given two words, MRLSA
not only can measure their degree of having a spe-
cific relation, but also can discover unknown rela-
tions between them. These advantages have been
demonstrated in our experiments. By encoding re-
lations from both homogeneous or heterogeneous
data sources, MRLSA achieves state-of-the-art per-
formance on existing benchmark datasets for two re-
lations, antonymy and is-a.
For future work, we plan to explore directions that
aim for improving both the quality and word cover-
age of the model. For instance, the knowledge en-
coded by MRLSA can be enriched by adding more
relations from a variety of linguistic resources, in-
cluding the co-occurrence relations from large cor-
pora. On model refinement, we notice that MRLSA
can be viewed as a 3-layer neural network without
applying the sigmoid function. Following the strat-
egy of using Siamese neural networks to enhance
PILSA (Yih et al., 2012), training MRLSA with a
multi-task discriminative learning setting can be a
promising approach as well.
</bodyText>
<sectionHeader confidence="0.997402" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.992279">
We thank Geoff Zweig for valuable discussions and
the anonymous reviewers for their comments.
</bodyText>
<sectionHeader confidence="0.998944" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997313513513513">
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas¸ca
and A. Soroa. 2009. A study on similarity and re-
latedness using distributional and WordNet-based ap-
proaches. In NAACL ’09, pages 19–27.
Brett W. Bader, Tamara G. Kolda, et al. 2012. Matlab
tensor toolbox version 2.5. Available online, January.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet allocation. Jour-
nal of Machine Learning Research, 3:993–1022.
Jordan L Boyd-Graber, David M Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambiguation.
In EMNLP-CoNLL, pages 1024–1033.
Shay B. Cohen, Giorgio Satta, and Michael Collins.
2013. Approximate PCFG parsing using tensor de-
composition. In NAACL-HLT 2013, pages 487–496.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society for Information
Science, 41(96).
S. Dumais, T. Letsche, M. Littman, and T. Landauer.
1997. Automatic cross-language retrieval using latent
semantic indexing. In AAAI-97 Spring Symposium Se-
ries: Cross-Language Text and Speech Retrieval.
Weiwei Guo and Mona Diab. 2012. Modeling sentences
in the latent space. In ACL 2012, pages 864–872.
Weiwei Guo and Mona Diab. 2013. Improving lexical
semantics for sentential semantics: Modeling selec-
tional preference and similar words in a latent variable
model. In NAACL-HLT 2013, pages 739–745.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of Uncertainty in Artificial
Intelligence, pages 289–296.
D. Jurgens, S. Mohammad, P. Turney, and K. Holyoak.
2012. SemEval-2012 Task 2: Measuring degrees of
relational similarity. In Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
2012), pages 356–364.
</reference>
<page confidence="0.811375">
1611
</page>
<reference confidence="0.997937257142857">
Tamara G. Kolda and Brett W. Bader. 2009. Ten-
sor decompositions and applications. SIAM Review,
51(3):455–500, September.
Tamara G. Kolda and Jimeng Sun. 2008. Scalable ten-
sor decompositions for multi-aspect data mining. In
ICDM 2008, pages 363–372.
T. Landauer and D. Laham. 1998. Learning human-
like knowledge by singular value decomposition: A
progress report. In NIPS 1998.
T. Landauer. 2002. On the computational basis of learn-
ing and cognition: Arguments from lsa. Psychology of
Learning and Motivation, 41:43–84.
Jordan J. Louviere and G. G. Woodworth. 1991. Best-
worst scaling: A model for the largest difference judg-
ments. Technical report, University of Alberta.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space word
representations. In NAACL-HLT 2013.
Saif Mohammad, Bonnie Dorr, and Graeme Hirst. 2008.
Computing word pair antonymy. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
John Platt, Kristina Toutanova, and Wen-tau Yih. 2010.
Translingual document representations from discrimi-
native projections. In Proceedings of EMNLP, pages
251–261.
Xipeng Qiu, Le Tian, and Xuanjing Huang. 2013. Latent
semantic tensor indexing for community-based ques-
tion answering. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 434–439, Sofia,
Bulgaria, August. Association for Computational Lin-
guistics.
Joseph Reisinger and Raymond J. Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Proceedings of HLT-NAACL, pages 109–117.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
NAACL-HLT 2013, pages 74–84.
Bryan Rink and Sanda Harabagiu. 2012. UTD: Deter-
mining relational similarity using lexical patterns. In
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 413–418,
Montr´eal, Canada, 7-8 June. Association for Compu-
tational Linguistics.
G. Salton, A. Wong, and C. S. Yang. 1975. A Vector
Space Model for Automatic Indexing. Communica-
tions of the ACM, 18(11).
Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,
and Christopher D. Manning. 2011. Parsing natural
scenes and natural language with recursive neural net-
works. In ICML ’11.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing with compositional
vector grammars. In Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Ledyard R Tucker. 1966. Some mathematical
notes on three-mode factor analysis. Psychometrika,
31(3):279–311.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal ofArtificial Intelligence Research, 37(1):141–
188.
P. D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379–416.
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2013. A tensor-based factorization model of se-
mantic compositionality. In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 1142–1151, Atlanta, Geor-
gia, June. Association for Computational Linguistics.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Document
clustering based on non-negative matrix factorization.
In Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in in-
formaion retrieval, pages 267–273, New York, NY,
USA. ACM.
Wen-tau Yih and Vahed Qazvinian. 2012. Measur-
ing word relatedness using heterogeneous vector space
models. In Proceedings of NAACL-HLT, pages 616–
620, Montr´eal, Canada, June.
Wen-tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning discriminative
projections for text similarity measures. In Proceed-
ings of the Fifteenth Conference on Computational
Natural Language Learning, pages 247–256, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Po-
larity inducing latent semantic analysis. In Proceed-
ings of NAACL-HLT, pages 1212–1222, Jeju Island,
Korea, July.
Alisa Zhila, Wen-tau Yih, Christopher Meek, Geoffrey
Zweig, and Tomas Mikolov. 2013. Combining het-
erogeneous models for measuring relational similar-
ity. In Proceedings of the 2013 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 1000–1009, Atlanta, Georgia, June. Asso-
ciation for Computational Linguistics.
</reference>
<page confidence="0.994352">
1612
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.840194">
<title confidence="0.999815">Multi-Relational Latent Semantic Analysis</title>
<author confidence="0.999837">Yih Christopher Meek</author>
<affiliation confidence="0.999974">University of Illinois Microsoft Research</affiliation>
<address confidence="0.997832">Urbana, IL 61801, USA Redmond, WA 98052, USA</address>
<abstract confidence="0.9919948">We present Multi-Relational Latent Semantic Analysis (MRLSA) which generalizes Latent Semantic Analysis (LSA). MRLSA provides an elegant approach to combining multiple relations between words by constructing a 3-way tensor. Similar to LSA, a lowrank approximation of the tensor is derived using a tensor decomposition. Each word in the vocabulary is thus represented by a vector in the latent semantic space and each relation is captured by a latent square matrix. The degree of two words having a specific relation can then be measured through simple linear algebraic operations. We demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources, MRLSA achieves stateof-the-art performance on existing benchmark for two relations,</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>E Alfonseca</author>
<author>K Hall</author>
<author>J Kravalova</author>
<author>M Pas¸ca</author>
<author>A Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and WordNet-based approaches.</title>
<date>2009</date>
<booktitle>In NAACL ’09,</booktitle>
<pages>pages</pages>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas¸ca and A. Soroa. 2009. A study on similarity and relatedness using distributional and WordNet-based approaches. In NAACL ’09, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brett W Bader</author>
<author>Tamara G Kolda</author>
</authors>
<title>Matlab tensor toolbox version 2.5. Available online,</title>
<date>2012</date>
<marker>Bader, Kolda, 2012</marker>
<rawString>Brett W. Bader, Tamara G. Kolda, et al. 2012. Matlab tensor toolbox version 2.5. Available online, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
<author>John Lafferty</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="6826" citStr="Blei et al., 2003" startWordPosition="1065" endWordPosition="1068">ector or a document-vector that denotes the statistics of term occurrences (Salton et al., 1975) in a large corpus. The text can also be represented by a low-dimensional dense vector derived by linear projection models like latent semantic analysis (LSA) (Deerwester et al., 1990), by discriminative learning methods like Siamese neural networks (Yih et al., 2011), recurrent neural networks (Mikolov et al., 2013) and recursive neural networks (Socher et al., 2011), or by graphical models such as probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003). As a generalization of LSA, MRLSA is also a linear projection model. However, while the words are represented by vectors as well, multiple relations between words are captured separately by matrices. In the context of lexical semantics, VSMs provide a natural way of measuring semantic word relatedness by computing the distance between the corresponding vectors, which has been a standard approach (Agirre et al., 2009; Reisinger and Mooney, 2010; Yih and Qazvinian, 2012). These approaches do not apply directly to the problem of modeling other types of relations. Existing methods that do handle</context>
</contexts>
<marker>Blei, Ng, Jordan, Lafferty, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, Michael I. Jordan, and John Lafferty. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan L Boyd-Graber</author>
<author>David M Blei</author>
<author>Xiaojin Zhu</author>
</authors>
<title>A topic model for word sense disambiguation. In EMNLP-CoNLL,</title>
<date>2007</date>
<pages>1024--1033</pages>
<contexts>
<context position="1596" citStr="Boyd-Graber et al., 2007" startWordPosition="236" endWordPosition="239">e on existing benchmark datasets for two relations, antonymy and is-a. 1 Introduction Continuous semantic space representations have proven successful in a wide variety of NLP and IR applications, such as document clustering (Xu et al., 2003) and cross-lingual document retrieval (Dumais et al., 1997; Platt et al., 2010) at the document level and sentential semantics (Guo and Diab, 2012; Guo and Diab, 2013) and syntactic parsing (Socher et al., 2013) at the sentence level. Such representations also play an important role in applications for lexical semantics, such as word sense disambiguation (Boyd-Graber et al., 2007), measuring word *Work conducted while interning at Microsoft Research. similarity (Deerwester et al., 1990) and relational similarity (Turney, 2006; Zhila et al., 2013; Mikolov et al., 2013). In many of these applications, Latent Semantic Analysis (LSA) (Deerwester et al., 1990) has been widely used, serving as a fundamental component or as a strong baseline. LSA operates by mapping text objects, typically documents and words, to a latent semantic space. The proximity of the vectors in this space implies that the original text objects are semantically related. However, one well-known limitati</context>
</contexts>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>Jordan L Boyd-Graber, David M Blei, and Xiaojin Zhu. 2007. A topic model for word sense disambiguation. In EMNLP-CoNLL, pages 1024–1033.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Giorgio Satta</author>
<author>Michael Collins</author>
</authors>
<title>Approximate PCFG parsing using tensor decomposition.</title>
<date>2013</date>
<booktitle>In NAACL-HLT 2013,</booktitle>
<pages>487--496</pages>
<contexts>
<context position="8633" citStr="Cohen et al. (2013)" startWordPosition="1345" endWordPosition="1348">ing a supervised model with features based on the frequencies of patterns in the corpus. Similarly, to measure whether two word pairs have the same relation, Zhila et al. (2013) proposed to combine heterogeneous models, which achieved state-of-the-art performance. In comparison, MRLSA models multiple lexical relations holistically. The degree that two words having a particular relation is estimated using the same linear function of the corresponding vectors and matrix. Tensor decomposition generalizes matrix factorization and has been applied to several NLP applications recently. For example, Cohen et al. (2013) proposed an approximation algorithm for PCFG parsing that relies on Kruskal decomposition. Van de Cruys et al. (2013) modeled the composition of subject-verb-object triples using Tucker decomposition, which results in a better similarity measure for transitive phrases. Similar to this construction but used in the community-based question answering (CQA) scenario, Qiu et al. (2013) represented triples of question title, question content and answer as a tensor and applied 3-mode SVD to derive latent semantic representations for question matching. The construction of MRLSA bears some resemblance</context>
</contexts>
<marker>Cohen, Satta, Collins, 2013</marker>
<rawString>Shay B. Cohen, Giorgio Satta, and Michael Collins. 2013. Approximate PCFG parsing using tensor decomposition. In NAACL-HLT 2013, pages 487–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S Dumais</author>
<author>G Furnas</author>
<author>T Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>96</issue>
<contexts>
<context position="1704" citStr="Deerwester et al., 1990" startWordPosition="250" endWordPosition="253">ce representations have proven successful in a wide variety of NLP and IR applications, such as document clustering (Xu et al., 2003) and cross-lingual document retrieval (Dumais et al., 1997; Platt et al., 2010) at the document level and sentential semantics (Guo and Diab, 2012; Guo and Diab, 2013) and syntactic parsing (Socher et al., 2013) at the sentence level. Such representations also play an important role in applications for lexical semantics, such as word sense disambiguation (Boyd-Graber et al., 2007), measuring word *Work conducted while interning at Microsoft Research. similarity (Deerwester et al., 1990) and relational similarity (Turney, 2006; Zhila et al., 2013; Mikolov et al., 2013). In many of these applications, Latent Semantic Analysis (LSA) (Deerwester et al., 1990) has been widely used, serving as a fundamental component or as a strong baseline. LSA operates by mapping text objects, typically documents and words, to a latent semantic space. The proximity of the vectors in this space implies that the original text objects are semantically related. However, one well-known limitation of LSA is that it is unable to differentiate fine-grained relations. For instance, when applied to lexica</context>
<context position="6488" citStr="Deerwester et al., 1990" startWordPosition="1012" endWordPosition="1015">the help of tensor decomposition techniques. We highlight some recent work related to our approach. The most commonly used continuous space representation of text is arguably the vector space model (VSM) (Turney and Pantel, 2010). In this representation, each text object can be represented by a high-dimensional sparse vector, such as a term-vector or a document-vector that denotes the statistics of term occurrences (Salton et al., 1975) in a large corpus. The text can also be represented by a low-dimensional dense vector derived by linear projection models like latent semantic analysis (LSA) (Deerwester et al., 1990), by discriminative learning methods like Siamese neural networks (Yih et al., 2011), recurrent neural networks (Mikolov et al., 2013) and recursive neural networks (Socher et al., 2011), or by graphical models such as probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003). As a generalization of LSA, MRLSA is also a linear projection model. However, while the words are represented by vectors as well, multiple relations between words are captured separately by matrices. In the context of lexical semantics, VSMs provide a natural </context>
<context position="9526" citStr="Deerwester et al., 1990" startWordPosition="1482" endWordPosition="1485">. Similar to this construction but used in the community-based question answering (CQA) scenario, Qiu et al. (2013) represented triples of question title, question content and answer as a tensor and applied 3-mode SVD to derive latent semantic representations for question matching. The construction of MRLSA bears some resemblance to the work that use tensors to capture triples. However, our goal of modeling different relations for lexical semantics is very different from the intended usage of tensor decomposition in the existing work. 3 Latent Semantic Analysis Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is a widely used continuous vector space model that maps words and documents into a low dimensional space. LSA consists of two main steps. First, taking a collection of d documents that contains words from a vocabulary list of size n, it first constructs a d x n document-term matrix W to encode the occurrence information of a word in a document. For instance, in its simplest form, the element Wjj can be the term frequency of the j-th word in the i-th document. In practice, a weighting scheme that better captures the importance of a word in the document, such as TFxIDF (Salton et al., 1975), i</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(96).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dumais</author>
<author>T Letsche</author>
<author>M Littman</author>
<author>T Landauer</author>
</authors>
<title>Automatic cross-language retrieval using latent semantic indexing.</title>
<date>1997</date>
<booktitle>In AAAI-97 Spring Symposium Series: Cross-Language Text and Speech Retrieval.</booktitle>
<contexts>
<context position="1271" citStr="Dumais et al., 1997" startWordPosition="183" endWordPosition="186">elation is captured by a latent square matrix. The degree of two words having a specific relation can then be measured through simple linear algebraic operations. We demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources, MRLSA achieves stateof-the-art performance on existing benchmark datasets for two relations, antonymy and is-a. 1 Introduction Continuous semantic space representations have proven successful in a wide variety of NLP and IR applications, such as document clustering (Xu et al., 2003) and cross-lingual document retrieval (Dumais et al., 1997; Platt et al., 2010) at the document level and sentential semantics (Guo and Diab, 2012; Guo and Diab, 2013) and syntactic parsing (Socher et al., 2013) at the sentence level. Such representations also play an important role in applications for lexical semantics, such as word sense disambiguation (Boyd-Graber et al., 2007), measuring word *Work conducted while interning at Microsoft Research. similarity (Deerwester et al., 1990) and relational similarity (Turney, 2006; Zhila et al., 2013; Mikolov et al., 2013). In many of these applications, Latent Semantic Analysis (LSA) (Deerwester et al., </context>
</contexts>
<marker>Dumais, Letsche, Littman, Landauer, 1997</marker>
<rawString>S. Dumais, T. Letsche, M. Littman, and T. Landauer. 1997. Automatic cross-language retrieval using latent semantic indexing. In AAAI-97 Spring Symposium Series: Cross-Language Text and Speech Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Modeling sentences in the latent space.</title>
<date>2012</date>
<booktitle>In ACL 2012,</booktitle>
<pages>864--872</pages>
<contexts>
<context position="1359" citStr="Guo and Diab, 2012" startWordPosition="198" endWordPosition="201">relation can then be measured through simple linear algebraic operations. We demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources, MRLSA achieves stateof-the-art performance on existing benchmark datasets for two relations, antonymy and is-a. 1 Introduction Continuous semantic space representations have proven successful in a wide variety of NLP and IR applications, such as document clustering (Xu et al., 2003) and cross-lingual document retrieval (Dumais et al., 1997; Platt et al., 2010) at the document level and sentential semantics (Guo and Diab, 2012; Guo and Diab, 2013) and syntactic parsing (Socher et al., 2013) at the sentence level. Such representations also play an important role in applications for lexical semantics, such as word sense disambiguation (Boyd-Graber et al., 2007), measuring word *Work conducted while interning at Microsoft Research. similarity (Deerwester et al., 1990) and relational similarity (Turney, 2006; Zhila et al., 2013; Mikolov et al., 2013). In many of these applications, Latent Semantic Analysis (LSA) (Deerwester et al., 1990) has been widely used, serving as a fundamental component or as a strong baseline. </context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012. Modeling sentences in the latent space. In ACL 2012, pages 864–872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Improving lexical semantics for sentential semantics: Modeling selectional preference and similar words in a latent variable model.</title>
<date>2013</date>
<booktitle>In NAACL-HLT 2013,</booktitle>
<pages>739--745</pages>
<contexts>
<context position="1380" citStr="Guo and Diab, 2013" startWordPosition="202" endWordPosition="205"> measured through simple linear algebraic operations. We demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources, MRLSA achieves stateof-the-art performance on existing benchmark datasets for two relations, antonymy and is-a. 1 Introduction Continuous semantic space representations have proven successful in a wide variety of NLP and IR applications, such as document clustering (Xu et al., 2003) and cross-lingual document retrieval (Dumais et al., 1997; Platt et al., 2010) at the document level and sentential semantics (Guo and Diab, 2012; Guo and Diab, 2013) and syntactic parsing (Socher et al., 2013) at the sentence level. Such representations also play an important role in applications for lexical semantics, such as word sense disambiguation (Boyd-Graber et al., 2007), measuring word *Work conducted while interning at Microsoft Research. similarity (Deerwester et al., 1990) and relational similarity (Turney, 2006; Zhila et al., 2013; Mikolov et al., 2013). In many of these applications, Latent Semantic Analysis (LSA) (Deerwester et al., 1990) has been widely used, serving as a fundamental component or as a strong baseline. LSA operates by mappi</context>
</contexts>
<marker>Guo, Diab, 2013</marker>
<rawString>Weiwei Guo and Mona Diab. 2013. Improving lexical semantics for sentential semantics: Modeling selectional preference and similar words in a latent variable model. In NAACL-HLT 2013, pages 739–745.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic analysis.</title>
<date>1999</date>
<booktitle>In Proceedings of Uncertainty in Artificial Intelligence,</booktitle>
<pages>289--296</pages>
<contexts>
<context position="6768" citStr="Hofmann, 1999" startWordPosition="1058" endWordPosition="1059"> by a high-dimensional sparse vector, such as a term-vector or a document-vector that denotes the statistics of term occurrences (Salton et al., 1975) in a large corpus. The text can also be represented by a low-dimensional dense vector derived by linear projection models like latent semantic analysis (LSA) (Deerwester et al., 1990), by discriminative learning methods like Siamese neural networks (Yih et al., 2011), recurrent neural networks (Mikolov et al., 2013) and recursive neural networks (Socher et al., 2011), or by graphical models such as probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003). As a generalization of LSA, MRLSA is also a linear projection model. However, while the words are represented by vectors as well, multiple relations between words are captured separately by matrices. In the context of lexical semantics, VSMs provide a natural way of measuring semantic word relatedness by computing the distance between the corresponding vectors, which has been a standard approach (Agirre et al., 2009; Reisinger and Mooney, 2010; Yih and Qazvinian, 2012). These approaches do not apply directly to the problem of modeling</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic analysis. In Proceedings of Uncertainty in Artificial Intelligence, pages 289–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurgens</author>
<author>S Mohammad</author>
<author>P Turney</author>
<author>K Holyoak</author>
</authors>
<title>SemEval-2012 Task 2: Measuring degrees of relational similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>356--364</pages>
<contexts>
<context position="5219" citStr="Jurgens et al., 2012" startWordPosition="799" endWordPosition="803"> and is-a. We use the benchmark GRE test of closestopposites (Mohammad et al., 2008) to show that MRLSA performs comparably to PILSA, which was the pervious state-of-the-art approach on this problem, when given the same amount of information. In addition, when other words and relations are available, potentially from additional resources, MRLSA is able to outperform previous methods significantly. We use the is-a relation to demonstrate that MRLSA is capable of handling asymmetric relations. We take the list of word pairs from the Class-Inclusion (i.e., is-a) relations in SemEval-2012 Task 2 (Jurgens et al., 2012), and use our model to measure the degree of two words have this relation. The measures derived from our model correlate with human judgement better than the best system that participated in the task. The rest of this paper is organized as follows. We first survey some related work in Section 2, followed by a more detailed description of LSA and PILSA in Section 3. Our proposed model, MRLSA, is presented in Section 4. Section 5 presents our experimental results. Finally, Section 6 concludes the paper. 2 Related Work MRLSA can be viewed as a model that derives general continuous space represent</context>
<context position="31745" citStr="Jurgens et al., 2012" startWordPosition="5306" endWordPosition="5309">DNB is the results of the best performing system in SemEval-2012 Task 2. only preserves the antonyms in the thesaurus, but also discovers additional ones, such as exacerbate and inflame for “alleviate”. Another interesting finding is that while the scores are useful in ranking the candidate words, they might not be comparable across different question words. This could be an issue for some applications, which need to make a binary decision on whether two words are antonyms. 5.3 Measuring degrees of Is-A relations We evaluate MRLSA using the class-inclusion portion of SemEval-2012 Task 2 data (Jurgens et al., 2012). Here the goal is to measure the degree of two words having the is-a relation. Five annotated datasets are provided for different subcategories of this relation: 1a-taxonomic, 1b-functional, 1c-singular, 1d-plural, 1e-class individual. We omit 1e because it focuses on real world entities (e.g., queen:Elizabeth, river:Nile), which are not included in WordNet. Each dataset contains about 100 questions based on approximately 40 word pairs. The question consists of 4 randomly chosen word pairs and asks the best and worst pairs that exemplify the specific is-a relation. The performance is measured</context>
</contexts>
<marker>Jurgens, Mohammad, Turney, Holyoak, 2012</marker>
<rawString>D. Jurgens, S. Mohammad, P. Turney, and K. Holyoak. 2012. SemEval-2012 Task 2: Measuring degrees of relational similarity. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 356–364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara G Kolda</author>
<author>Brett W Bader</author>
</authors>
<title>Tensor decompositions and applications.</title>
<date>2009</date>
<journal>SIAM Review,</journal>
<volume>51</volume>
<issue>3</issue>
<contexts>
<context position="16709" citStr="Kolda and Bader, 2009" startWordPosition="2796" endWordPosition="2799">yn, W:,:,ant, W:,:,hyper, respectively. Rows represent documents (see definition in text), and columns represent words. For ease of presentation, we show numbers with 0-1 values instead of TFxIDF scores. the Tucker decomposition, to the tensor. 4.1 Representing Multi-Relational Data in Tensors A tensor is simply a multi-dimensional array. In this work, we use a 3-way tensor W to encode multiple word relations. An element of W is denoted by WZ,,,k using its indices, and W:,:,k represents the k-th slice of W (a slice of a 3-way tensor is a matrix, obtained by fixing the third index). Following (Kolda and Bader, 2009), a fiber of a tensor W:,,,k is a vector, which is a high order analog of a matrix row or column. When constructing the raw tensor W in MRLSA, each slice is analogous to the document-term matrix in LSA, but created based on the data of a particular relation, such as synonyms. With a slight abuse of notation, we sometimes use the value rather than index when there is no confusion. For instance, W:,“word»,k represents the fiber corresponding to the “word” in slice k, and W:,:,syn refers to the slice that encodes the synonymy relation. Below we use an example to compare this construction to the r</context>
<context position="20316" citStr="Kolda and Bader, 2009" startWordPosition="3449" endWordPosition="3452">pectively. The rank parameters R1 ≤ d, R2 ≤ n, R3 ≤ m are given as input to the algorithm. In MRLSA, m (the number of relations) is usually small, while d and n are typically large (often in the scale of hundreds of thousands). Therefore, we choose R1 = R2 = T, T « d, n and R3 = m, where T is typically less than 1000. To make the analogy to SVD clear, we rewrite the results of Tucker decomposition by performing a nmode matrix product over the core tensor G with the matrix T. This produces a tensor S where each slice is a linear combination of the slices of G with coefficients given by T (see (Kolda and Bader, 2009) for detail). That is, we have S:,:,k = �m Tt,kG:,:,t, ∀k. t=1 An illustration is shown in Fig. 4(b), Then, a straightforward calculation shows that k-th slice of tensor W is approximated by W:,:,k ≈ X:,:,k = US:,:,kVT. (4) Comparing Eq. (4) to Eq. (1), one can observe that matrices U and V play similar roles here, and each slice of the core tensor S is analogous to Σ. However, the square matrix G:,:,k is not necessary to be diagonal. As in SVD, the column vectors of G:,:,kVT (capture both word and relation information) behave similarly to the column vectors of the original tensor slice W:,:,k</context>
</contexts>
<marker>Kolda, Bader, 2009</marker>
<rawString>Tamara G. Kolda and Brett W. Bader. 2009. Tensor decompositions and applications. SIAM Review, 51(3):455–500, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara G Kolda</author>
<author>Jimeng Sun</author>
</authors>
<title>Scalable tensor decompositions for multi-aspect data mining.</title>
<date>2008</date>
<booktitle>In ICDM</booktitle>
<pages>363--372</pages>
<contexts>
<context position="24218" citStr="Kolda and Sun, 2008" startWordPosition="4091" endWordPosition="4094">hesaurus as in (Yih et al., 2012), which contains about 47k words and a vocabulary list of approximately 50k words. WordNet We use four types of relations from WordNet: synonymy, antonymy, hypernymy and hyponymy. The number of target words and the size of the vocabulary in our version are 117,791 and 149,400, respectively. WordNet has better vocabulary coverage, but fewer antonym pairs. For instance, the WordNet antonym slice contains only 46,945 nonzero entries, while the Encarta antonym slice has 129,733. 2http://www.bloomsbury.com We apply a memory-efficient Tucker decomposition algorithm (Kolda and Sun, 2008) implemented in tensor toolbox v2.5 (Bader et al., 2012)3 to factor the tensor. The largest tensor considered in this paper can be decomposed in about 3 hours using less than 4GB of memory with a commodity PC. 5.2 Answering GRE Antonym Questions The first task is to answer the closest-opposite questions from the GRE test provided by Mohammad et al. (2008)4. Each question in this test consists of a target word and five candidate words, where the goal is to pick the candidate word that has the most opposite meaning to the target word. In order to have a fair comparison, we use the same data spli</context>
</contexts>
<marker>Kolda, Sun, 2008</marker>
<rawString>Tamara G. Kolda and Jimeng Sun. 2008. Scalable tensor decompositions for multi-aspect data mining. In ICDM 2008, pages 363–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Landauer</author>
<author>D Laham</author>
</authors>
<title>Learning humanlike knowledge by singular value decomposition: A progress report.</title>
<date>1998</date>
<booktitle>In NIPS</booktitle>
<contexts>
<context position="2408" citStr="Landauer and Laham, 1998" startWordPosition="364" endWordPosition="367">13). In many of these applications, Latent Semantic Analysis (LSA) (Deerwester et al., 1990) has been widely used, serving as a fundamental component or as a strong baseline. LSA operates by mapping text objects, typically documents and words, to a latent semantic space. The proximity of the vectors in this space implies that the original text objects are semantically related. However, one well-known limitation of LSA is that it is unable to differentiate fine-grained relations. For instance, when applied to lexical semantics, synonyms and antonyms may both be assigned high similarity scores (Landauer and Laham, 1998; Landauer, 2002). Asymmetric relations like hyponyms and hypernyms also cannot be differentiated. Although there exists some recent work, such as PILSA which tries to overcome this weakness of LSA by introducing the notion of polarity (Yih et al., 2012). This extension, however, can only handle two opposing relations (e.g., synonyms and antonyms), leaving open the challenge of encoding multiple relations. In this paper, we propose Multi-Relational Latent Semantic Analysis (MRLSA), which strictly generalizes LSA to incorporate information of multiple relations concurrently. Similar to LSA or P</context>
<context position="12986" citStr="Landauer and Laham, 1998" startWordPosition="2128" endWordPosition="2131">s. The polarity (i.e., sign) indicates whether the term in the vocabulary is a synonym or antonym of the target word. which can be derived from the equations below. XTX = (UEVT)T(UEVT) = VEUTUEVT (E is diagonal) = VE2VT (Columns of U are orthonormal) = (EVT )T (EVT) (2) Thus, the semantic relatedness between the i-th and j-th words can be computed by cosine similarity1: cos(X:�i, X:�j) (3) When used to compare words, one well-known limitation of LSA is that the score captures the general notion of semantic similarity, and is unable to distinguish fine-grained word relations, such as antonyms (Landauer and Laham, 1998; Landauer, 2002). This is due to the fact that the raw matrix representation only records the occurrences of words in documents without knowing the specific relation between the word and document. To address this issue, Yih et al. (2012) proposed a polarity inducing latent semantic analysis model recently, which we introduce next. 1Cosine similarity is equivalent to the inner product of the normalized vectors. 3.1 Polarity Inducing Latent Semantic Analysis In order to distinguish antonyms from synonyms, the polarity inducing LSA (PILSA) model (Yih et al., 2012) takes a thesaurus as input. Syn</context>
</contexts>
<marker>Landauer, Laham, 1998</marker>
<rawString>T. Landauer and D. Laham. 1998. Learning humanlike knowledge by singular value decomposition: A progress report. In NIPS 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Landauer</author>
</authors>
<title>On the computational basis of learning and cognition: Arguments from lsa.</title>
<date>2002</date>
<booktitle>Psychology of Learning and Motivation,</booktitle>
<pages>41--43</pages>
<contexts>
<context position="2425" citStr="Landauer, 2002" startWordPosition="368" endWordPosition="369">ications, Latent Semantic Analysis (LSA) (Deerwester et al., 1990) has been widely used, serving as a fundamental component or as a strong baseline. LSA operates by mapping text objects, typically documents and words, to a latent semantic space. The proximity of the vectors in this space implies that the original text objects are semantically related. However, one well-known limitation of LSA is that it is unable to differentiate fine-grained relations. For instance, when applied to lexical semantics, synonyms and antonyms may both be assigned high similarity scores (Landauer and Laham, 1998; Landauer, 2002). Asymmetric relations like hyponyms and hypernyms also cannot be differentiated. Although there exists some recent work, such as PILSA which tries to overcome this weakness of LSA by introducing the notion of polarity (Yih et al., 2012). This extension, however, can only handle two opposing relations (e.g., synonyms and antonyms), leaving open the challenge of encoding multiple relations. In this paper, we propose Multi-Relational Latent Semantic Analysis (MRLSA), which strictly generalizes LSA to incorporate information of multiple relations concurrently. Similar to LSA or PILSA when applied</context>
<context position="13003" citStr="Landauer, 2002" startWordPosition="2132" endWordPosition="2133">n) indicates whether the term in the vocabulary is a synonym or antonym of the target word. which can be derived from the equations below. XTX = (UEVT)T(UEVT) = VEUTUEVT (E is diagonal) = VE2VT (Columns of U are orthonormal) = (EVT )T (EVT) (2) Thus, the semantic relatedness between the i-th and j-th words can be computed by cosine similarity1: cos(X:�i, X:�j) (3) When used to compare words, one well-known limitation of LSA is that the score captures the general notion of semantic similarity, and is unable to distinguish fine-grained word relations, such as antonyms (Landauer and Laham, 1998; Landauer, 2002). This is due to the fact that the raw matrix representation only records the occurrences of words in documents without knowing the specific relation between the word and document. To address this issue, Yih et al. (2012) proposed a polarity inducing latent semantic analysis model recently, which we introduce next. 1Cosine similarity is equivalent to the inner product of the normalized vectors. 3.1 Polarity Inducing Latent Semantic Analysis In order to distinguish antonyms from synonyms, the polarity inducing LSA (PILSA) model (Yih et al., 2012) takes a thesaurus as input. Synonyms and antonym</context>
</contexts>
<marker>Landauer, 2002</marker>
<rawString>T. Landauer. 2002. On the computational basis of learning and cognition: Arguments from lsa. Psychology of Learning and Motivation, 41:43–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan J Louviere</author>
<author>G G Woodworth</author>
</authors>
<title>Bestworst scaling: A model for the largest difference judgments.</title>
<date>1991</date>
<tech>Technical report,</tech>
<institution>University of Alberta.</institution>
<contexts>
<context position="32445" citStr="Louviere and Woodworth, 1991" startWordPosition="5413" endWordPosition="5416">elation. Five annotated datasets are provided for different subcategories of this relation: 1a-taxonomic, 1b-functional, 1c-singular, 1d-plural, 1e-class individual. We omit 1e because it focuses on real world entities (e.g., queen:Elizabeth, river:Nile), which are not included in WordNet. Each dataset contains about 100 questions based on approximately 40 word pairs. The question consists of 4 randomly chosen word pairs and asks the best and worst pairs that exemplify the specific is-a relation. The performance is measured by the average prediction accuracy, also called the MaxDiff accuracy (Louviere and Woodworth, 1991). Because the questions are generated from the same set of word pairs, these questions are not mutually independent. Therefore, it is not proper to split the data of each subcategory into the development and test sets. Alternatively, we follow the setting of SemEval-2012 Task 2 and use the first subcategory (1a-taxonomy) to tune the model and evaluate its performance based on the results on other datasets. Since the models are tuned and tested on different types of subcategories, they might not be the optimal ones when evaluated on the test sets. Therefore, we show results using the best param</context>
</contexts>
<marker>Louviere, Woodworth, 1991</marker>
<rawString>Jordan J. Louviere and G. G. Woodworth. 1991. Bestworst scaling: A model for the largest difference judgments. Technical report, University of Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In NAACL-HLT</booktitle>
<contexts>
<context position="1787" citStr="Mikolov et al., 2013" startWordPosition="263" endWordPosition="266">, such as document clustering (Xu et al., 2003) and cross-lingual document retrieval (Dumais et al., 1997; Platt et al., 2010) at the document level and sentential semantics (Guo and Diab, 2012; Guo and Diab, 2013) and syntactic parsing (Socher et al., 2013) at the sentence level. Such representations also play an important role in applications for lexical semantics, such as word sense disambiguation (Boyd-Graber et al., 2007), measuring word *Work conducted while interning at Microsoft Research. similarity (Deerwester et al., 1990) and relational similarity (Turney, 2006; Zhila et al., 2013; Mikolov et al., 2013). In many of these applications, Latent Semantic Analysis (LSA) (Deerwester et al., 1990) has been widely used, serving as a fundamental component or as a strong baseline. LSA operates by mapping text objects, typically documents and words, to a latent semantic space. The proximity of the vectors in this space implies that the original text objects are semantically related. However, one well-known limitation of LSA is that it is unable to differentiate fine-grained relations. For instance, when applied to lexical semantics, synonyms and antonyms may both be assigned high similarity scores (Lan</context>
<context position="6622" citStr="Mikolov et al., 2013" startWordPosition="1033" endWordPosition="1036">ce representation of text is arguably the vector space model (VSM) (Turney and Pantel, 2010). In this representation, each text object can be represented by a high-dimensional sparse vector, such as a term-vector or a document-vector that denotes the statistics of term occurrences (Salton et al., 1975) in a large corpus. The text can also be represented by a low-dimensional dense vector derived by linear projection models like latent semantic analysis (LSA) (Deerwester et al., 1990), by discriminative learning methods like Siamese neural networks (Yih et al., 2011), recurrent neural networks (Mikolov et al., 2013) and recursive neural networks (Socher et al., 2011), or by graphical models such as probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003). As a generalization of LSA, MRLSA is also a linear projection model. However, while the words are represented by vectors as well, multiple relations between words are captured separately by matrices. In the context of lexical semantics, VSMs provide a natural way of measuring semantic word relatedness by computing the distance between the corresponding vectors, which has been a standard appr</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In NAACL-HLT 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Bonnie Dorr</author>
<author>Graeme Hirst</author>
</authors>
<title>Computing word pair antonymy.</title>
<date>2008</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="4682" citStr="Mohammad et al., 2008" startWordPosition="716" endWordPosition="719">ly unseen relations between two words can be discovered, and the information encoded in other relations can influence the construction of the latent representations, and thus potentially improves the overall quality. In addition, the information in different slices can come from heterogeneous sources (conceptually similar to (Riedel et al., 2013)), which not only improves the model, but also extends the word coverage in a reliable way. We provide empirical evidence that MRLSA is effective using two different word relations: antonymy and is-a. We use the benchmark GRE test of closestopposites (Mohammad et al., 2008) to show that MRLSA performs comparably to PILSA, which was the pervious state-of-the-art approach on this problem, when given the same amount of information. In addition, when other words and relations are available, potentially from additional resources, MRLSA is able to outperform previous methods significantly. We use the is-a relation to demonstrate that MRLSA is capable of handling asymmetric relations. We take the list of word pairs from the Class-Inclusion (i.e., is-a) relations in SemEval-2012 Task 2 (Jurgens et al., 2012), and use our model to measure the degree of two words have thi</context>
<context position="7764" citStr="Mohammad et al., 2008" startWordPosition="1212" endWordPosition="1215">g the distance between the corresponding vectors, which has been a standard approach (Agirre et al., 2009; Reisinger and Mooney, 2010; Yih and Qazvinian, 2012). These approaches do not apply directly to the problem of modeling other types of relations. Existing methods that do handle multiple relations often use a model combination scheme to integrate signals from various types of information sources. For instance, morphological variations discovered from the Google n-gram corpus have been combined with information from thesauri and vector-based word relatedness models for detecting antonyms (Mohammad et al., 2008). An alternative approach proposed by Turney (2008) that handles synonyms, antonyms and associations is to use a uniform approach by first reducing the problem to determining whether two 1603 pairs of words can be analogous, and then predicting it using a supervised model with features based on the frequencies of patterns in the corpus. Similarly, to measure whether two word pairs have the same relation, Zhila et al. (2013) proposed to combine heterogeneous models, which achieved state-of-the-art performance. In comparison, MRLSA models multiple lexical relations holistically. The degree that </context>
<context position="24575" citStr="Mohammad et al. (2008)" startWordPosition="4154" endWordPosition="4157">coverage, but fewer antonym pairs. For instance, the WordNet antonym slice contains only 46,945 nonzero entries, while the Encarta antonym slice has 129,733. 2http://www.bloomsbury.com We apply a memory-efficient Tucker decomposition algorithm (Kolda and Sun, 2008) implemented in tensor toolbox v2.5 (Bader et al., 2012)3 to factor the tensor. The largest tensor considered in this paper can be decomposed in about 3 hours using less than 4GB of memory with a commodity PC. 5.2 Answering GRE Antonym Questions The first task is to answer the closest-opposite questions from the GRE test provided by Mohammad et al. (2008)4. Each question in this test consists of a target word and five candidate words, where the goal is to pick the candidate word that has the most opposite meaning to the target word. In order to have a fair comparison, we use the same data split as in (Mohammad et al., 2008), with 162 questions used for the development set and 950 for test. Following (Mohammad et al., 2008; Yih et al., 2012), we report the results in precision (accuracy of the questions that the system attempts to answer), recall (percentage of the questions answered correctly over all questions) and F1 (the harmonic mean of pr</context>
</contexts>
<marker>Mohammad, Dorr, Hirst, 2008</marker>
<rawString>Saif Mohammad, Bonnie Dorr, and Graeme Hirst. 2008. Computing word pair antonymy. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Platt</author>
<author>Kristina Toutanova</author>
<author>Wen-tau Yih</author>
</authors>
<title>Translingual document representations from discriminative projections.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>251--261</pages>
<contexts>
<context position="1292" citStr="Platt et al., 2010" startWordPosition="187" endWordPosition="190">y a latent square matrix. The degree of two words having a specific relation can then be measured through simple linear algebraic operations. We demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources, MRLSA achieves stateof-the-art performance on existing benchmark datasets for two relations, antonymy and is-a. 1 Introduction Continuous semantic space representations have proven successful in a wide variety of NLP and IR applications, such as document clustering (Xu et al., 2003) and cross-lingual document retrieval (Dumais et al., 1997; Platt et al., 2010) at the document level and sentential semantics (Guo and Diab, 2012; Guo and Diab, 2013) and syntactic parsing (Socher et al., 2013) at the sentence level. Such representations also play an important role in applications for lexical semantics, such as word sense disambiguation (Boyd-Graber et al., 2007), measuring word *Work conducted while interning at Microsoft Research. similarity (Deerwester et al., 1990) and relational similarity (Turney, 2006; Zhila et al., 2013; Mikolov et al., 2013). In many of these applications, Latent Semantic Analysis (LSA) (Deerwester et al., 1990) has been widely</context>
</contexts>
<marker>Platt, Toutanova, Yih, 2010</marker>
<rawString>John Platt, Kristina Toutanova, and Wen-tau Yih. 2010. Translingual document representations from discriminative projections. In Proceedings of EMNLP, pages 251–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xipeng Qiu</author>
<author>Le Tian</author>
<author>Xuanjing Huang</author>
</authors>
<title>Latent semantic tensor indexing for community-based question answering.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>434--439</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Qiu, Le Tian, Huang, 2013</marker>
<rawString>Xipeng Qiu, Le Tian, and Xuanjing Huang. 2013. Latent semantic tensor indexing for community-based question answering. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 434–439, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>Multiprototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>109--117</pages>
<contexts>
<context position="7275" citStr="Reisinger and Mooney, 2010" startWordPosition="1138" endWordPosition="1141">rks (Socher et al., 2011), or by graphical models such as probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003). As a generalization of LSA, MRLSA is also a linear projection model. However, while the words are represented by vectors as well, multiple relations between words are captured separately by matrices. In the context of lexical semantics, VSMs provide a natural way of measuring semantic word relatedness by computing the distance between the corresponding vectors, which has been a standard approach (Agirre et al., 2009; Reisinger and Mooney, 2010; Yih and Qazvinian, 2012). These approaches do not apply directly to the problem of modeling other types of relations. Existing methods that do handle multiple relations often use a model combination scheme to integrate signals from various types of information sources. For instance, morphological variations discovered from the Google n-gram corpus have been combined with information from thesauri and vector-based word relatedness models for detecting antonyms (Mohammad et al., 2008). An alternative approach proposed by Turney (2008) that handles synonyms, antonyms and associations is to use </context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J. Mooney. 2010. Multiprototype vector-space models of word meaning. In Proceedings of HLT-NAACL, pages 109–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
<author>Benjamin M Marlin</author>
</authors>
<title>Relation extraction with matrix factorization and universal schemas.</title>
<date>2013</date>
<booktitle>In NAACL-HLT 2013,</booktitle>
<pages>74--84</pages>
<contexts>
<context position="4408" citStr="Riedel et al., 2013" startWordPosition="670" endWordPosition="673">nds to the document-term matrix in the original LSA design but for a specific relation. Analogous to LSA, the whole linear transformation mapping is derived through tensor decomposition, which provides a low-rank approximation of the original tensor. As a result, previously unseen relations between two words can be discovered, and the information encoded in other relations can influence the construction of the latent representations, and thus potentially improves the overall quality. In addition, the information in different slices can come from heterogeneous sources (conceptually similar to (Riedel et al., 2013)), which not only improves the model, but also extends the word coverage in a reliable way. We provide empirical evidence that MRLSA is effective using two different word relations: antonymy and is-a. We use the benchmark GRE test of closestopposites (Mohammad et al., 2008) to show that MRLSA performs comparably to PILSA, which was the pervious state-of-the-art approach on this problem, when given the same amount of information. In addition, when other words and relations are available, potentially from additional resources, MRLSA is able to outperform previous methods significantly. We use th</context>
</contexts>
<marker>Riedel, Yao, McCallum, Marlin, 2013</marker>
<rawString>Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M. Marlin. 2013. Relation extraction with matrix factorization and universal schemas. In NAACL-HLT 2013, pages 74–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Rink</author>
<author>Sanda Harabagiu</author>
</authors>
<title>UTD: Determining relational similarity using lexical patterns.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>413--418</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="30573" citStr="Rink and Harabagiu, 2012" startWordPosition="5115" endWordPosition="5118">0.68), make-worse (0.67), inflame (0.66), amplify (0.65), stir-up (0.64) relish detest (0.33), abhor (0.33), abominate (0.33), despise (0.33), loathe (0.31) We can see that from these examples, MRLSA not 1609 Dev. 1b (Functional) Test 1d (Plural) Avg. 1a (Taxonomic) 1c (Singular) WordNet Lookup 52.9 34.5 41.4 34.3 36.7 WordNet RawTensor 51.0 38.3 50.0 42.1 43.5 WordNet MRLSA:Syn+Hypony 55.8 41.7 (43.2) 51.0 (51.4) 37.5 (44.4) 43.4 (46.3) WordNet MRLSA:4-layers 52.9 51.5 (53.9) 51.9 (60.0) 43.5 (50.5) 49.0 (54.8) MRLSA:WordNet+Encarta 62.1 55.3 (58.7) 57.1 (65.7) 48.6 (53.7) 55.8 (60.1) UTDNB (Rink and Harabagiu, 2012) - 38.3 36.7 28.2 34.4 Table 2: Results of measuring the class-inclusion (is-a) relations in MaxDiff accuracy (see text for detail). RawTensor has synonym and hyponym slices and measures the degree of is-a relation using Eq. (5). MRLSA:Syn+Hypo factors the raw tensor and judges the relation by Eq. (6). The constructions of MRLSA:4-layers and MRLSA:WordNet+Encarta are the same as in Sec. 5.2 (see the caption of Table 1 for detail). For MRLSA models, numbers shown in the parentheses are the results when parameters are tuned using the test sets. UTDNB is the results of the best performing system </context>
</contexts>
<marker>Rink, Harabagiu, 2012</marker>
<rawString>Bryan Rink and Sanda Harabagiu. 2012. UTD: Determining relational similarity using lexical patterns. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 413–418, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A Vector Space Model for Automatic Indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="6304" citStr="Salton et al., 1975" startWordPosition="982" endWordPosition="985">. Finally, Section 6 concludes the paper. 2 Related Work MRLSA can be viewed as a model that derives general continuous space representations for capturing lexical semantics, with the help of tensor decomposition techniques. We highlight some recent work related to our approach. The most commonly used continuous space representation of text is arguably the vector space model (VSM) (Turney and Pantel, 2010). In this representation, each text object can be represented by a high-dimensional sparse vector, such as a term-vector or a document-vector that denotes the statistics of term occurrences (Salton et al., 1975) in a large corpus. The text can also be represented by a low-dimensional dense vector derived by linear projection models like latent semantic analysis (LSA) (Deerwester et al., 1990), by discriminative learning methods like Siamese neural networks (Yih et al., 2011), recurrent neural networks (Mikolov et al., 2013) and recursive neural networks (Socher et al., 2011), or by graphical models such as probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003). As a generalization of LSA, MRLSA is also a linear projection model. However</context>
<context position="10123" citStr="Salton et al., 1975" startWordPosition="1592" endWordPosition="1595">eerwester et al., 1990) is a widely used continuous vector space model that maps words and documents into a low dimensional space. LSA consists of two main steps. First, taking a collection of d documents that contains words from a vocabulary list of size n, it first constructs a d x n document-term matrix W to encode the occurrence information of a word in a document. For instance, in its simplest form, the element Wjj can be the term frequency of the j-th word in the i-th document. In practice, a weighting scheme that better captures the importance of a word in the document, such as TFxIDF (Salton et al., 1975), is often used instead. Notice that “document” here simply means a group of words and has been applied Figure 1: SVD applied to a dxn document-term matrix W. The rank-k approximation, X, is the multiplication of U, E and VT, where U and V are d x k and n x k orthonormal matrices and E is a k x k diagonal matrix. The column vectors of VT multiplied by the singular values E represent words in the latent semantic space. to various texts including news articles, sentences and bags of words. Once the matrix is constructed, the second step is to apply singular value decomposition (SVD) to W in orde</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton, A. Wong, and C. S. Yang. 1975. A Vector Space Model for Automatic Indexing. Communications of the ACM, 18(11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff Chiung-Yu Lin</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In ICML ’11.</booktitle>
<contexts>
<context position="6674" citStr="Socher et al., 2011" startWordPosition="1042" endWordPosition="1045">e model (VSM) (Turney and Pantel, 2010). In this representation, each text object can be represented by a high-dimensional sparse vector, such as a term-vector or a document-vector that denotes the statistics of term occurrences (Salton et al., 1975) in a large corpus. The text can also be represented by a low-dimensional dense vector derived by linear projection models like latent semantic analysis (LSA) (Deerwester et al., 1990), by discriminative learning methods like Siamese neural networks (Yih et al., 2011), recurrent neural networks (Mikolov et al., 2013) and recursive neural networks (Socher et al., 2011), or by graphical models such as probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003). As a generalization of LSA, MRLSA is also a linear projection model. However, while the words are represented by vectors as well, multiple relations between words are captured separately by matrices. In the context of lexical semantics, VSMs provide a natural way of measuring semantic word relatedness by computing the distance between the corresponding vectors, which has been a standard approach (Agirre et al., 2009; Reisinger and Mooney, 201</context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng, and Christopher D. Manning. 2011. Parsing natural scenes and natural language with recursive neural networks. In ICML ’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1424" citStr="Socher et al., 2013" startWordPosition="209" endWordPosition="212">perations. We demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources, MRLSA achieves stateof-the-art performance on existing benchmark datasets for two relations, antonymy and is-a. 1 Introduction Continuous semantic space representations have proven successful in a wide variety of NLP and IR applications, such as document clustering (Xu et al., 2003) and cross-lingual document retrieval (Dumais et al., 1997; Platt et al., 2010) at the document level and sentential semantics (Guo and Diab, 2012; Guo and Diab, 2013) and syntactic parsing (Socher et al., 2013) at the sentence level. Such representations also play an important role in applications for lexical semantics, such as word sense disambiguation (Boyd-Graber et al., 2007), measuring word *Work conducted while interning at Microsoft Research. similarity (Deerwester et al., 1990) and relational similarity (Turney, 2006; Zhila et al., 2013; Mikolov et al., 2013). In many of these applications, Latent Semantic Analysis (LSA) (Deerwester et al., 1990) has been widely used, serving as a fundamental component or as a strong baseline. LSA operates by mapping text objects, typically documents and wor</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. Parsing with compositional vector grammars. In Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ledyard R Tucker</author>
</authors>
<title>Some mathematical notes on three-mode factor analysis.</title>
<date>1966</date>
<journal>Psychometrika,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="18748" citStr="Tucker, 1966" startWordPosition="3146" endWordPosition="3147">dding a slice corresponding to each relation of interest. Fig. 3(c) demonstrates how to add another slice W:,:,hyper to the tensor for encoding hypernyms. 4.2 Tensor Decomposition The MRLSA raw tensor encodes relations in one or more data resources, such as thesauri. However, the knowledge from a thesaurus is usually noisy and incomplete. In this section, we derive a low-rank approximation of the tensor to generalize the knowledge. This step is analogous to the rank-k approximation in LSA. Various tensor decomposition methods have been proposed in literature. Among them, Tucker decomposition (Tucker, 1966) is recognized as a multidimensional extension of SVD and has been widely used in many applications. An illustration of this method is in Fig. 4(a). In Tucker decomposition, a d x n x m tensor W is decomposed into four components !g, U, V, T. A low-rank approximation 1606 (a) Tucker Tensor Decomposition (b) Our Reformulation T VT = U W X = U S VT S:,:,1 Figure 4: Fig. 4(a) illustrates the Tucker tensor decomposition method which factors a 3-way tensor W to three orthogonal matrices, U, V, T, and a core tensor G. We further apply a n-mode matrix product on the core tensor G with T. Consequently</context>
</contexts>
<marker>Tucker, 1966</marker>
<rawString>Ledyard R Tucker. 1966. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3):279–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<pages>188</pages>
<contexts>
<context position="6093" citStr="Turney and Pantel, 2010" startWordPosition="950" endWordPosition="953">e first survey some related work in Section 2, followed by a more detailed description of LSA and PILSA in Section 3. Our proposed model, MRLSA, is presented in Section 4. Section 5 presents our experimental results. Finally, Section 6 concludes the paper. 2 Related Work MRLSA can be viewed as a model that derives general continuous space representations for capturing lexical semantics, with the help of tensor decomposition techniques. We highlight some recent work related to our approach. The most commonly used continuous space representation of text is arguably the vector space model (VSM) (Turney and Pantel, 2010). In this representation, each text object can be represented by a high-dimensional sparse vector, such as a term-vector or a document-vector that denotes the statistics of term occurrences (Salton et al., 1975) in a large corpus. The text can also be represented by a low-dimensional dense vector derived by linear projection models like latent semantic analysis (LSA) (Deerwester et al., 1990), by discriminative learning methods like Siamese neural networks (Yih et al., 2011), recurrent neural networks (Mikolov et al., 2013) and recursive neural networks (Socher et al., 2011), or by graphical m</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal ofArtificial Intelligence Research, 37(1):141– 188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="1744" citStr="Turney, 2006" startWordPosition="257" endWordPosition="258">variety of NLP and IR applications, such as document clustering (Xu et al., 2003) and cross-lingual document retrieval (Dumais et al., 1997; Platt et al., 2010) at the document level and sentential semantics (Guo and Diab, 2012; Guo and Diab, 2013) and syntactic parsing (Socher et al., 2013) at the sentence level. Such representations also play an important role in applications for lexical semantics, such as word sense disambiguation (Boyd-Graber et al., 2007), measuring word *Work conducted while interning at Microsoft Research. similarity (Deerwester et al., 1990) and relational similarity (Turney, 2006; Zhila et al., 2013; Mikolov et al., 2013). In many of these applications, Latent Semantic Analysis (LSA) (Deerwester et al., 1990) has been widely used, serving as a fundamental component or as a strong baseline. LSA operates by mapping text objects, typically documents and words, to a latent semantic space. The proximity of the vectors in this space implies that the original text objects are semantically related. However, one well-known limitation of LSA is that it is unable to differentiate fine-grained relations. For instance, when applied to lexical semantics, synonyms and antonyms may b</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>P. D. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>A uniform approach to analogies, synonyms, antonyms, and associations.</title>
<date>2008</date>
<booktitle>In International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="7815" citStr="Turney (2008)" startWordPosition="1221" endWordPosition="1223">been a standard approach (Agirre et al., 2009; Reisinger and Mooney, 2010; Yih and Qazvinian, 2012). These approaches do not apply directly to the problem of modeling other types of relations. Existing methods that do handle multiple relations often use a model combination scheme to integrate signals from various types of information sources. For instance, morphological variations discovered from the Google n-gram corpus have been combined with information from thesauri and vector-based word relatedness models for detecting antonyms (Mohammad et al., 2008). An alternative approach proposed by Turney (2008) that handles synonyms, antonyms and associations is to use a uniform approach by first reducing the problem to determining whether two 1603 pairs of words can be analogous, and then predicting it using a supervised model with features based on the frequencies of patterns in the corpus. Similarly, to measure whether two word pairs have the same relation, Zhila et al. (2013) proposed to combine heterogeneous models, which achieved state-of-the-art performance. In comparison, MRLSA models multiple lexical relations holistically. The degree that two words having a particular relation is estimated</context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter Turney. 2008. A uniform approach to analogies, synonyms, antonyms, and associations. In International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
<author>Thierry Poibeau</author>
<author>Anna Korhonen</author>
</authors>
<title>A tensor-based factorization model of semantic compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1142--1151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<marker>Van de Cruys, Poibeau, Korhonen, 2013</marker>
<rawString>Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen. 2013. A tensor-based factorization model of semantic compositionality. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1142–1151, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Xin Liu</author>
<author>Yihong Gong</author>
</authors>
<title>Document clustering based on non-negative matrix factorization.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,</booktitle>
<pages>267--273</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1213" citStr="Xu et al., 2003" startWordPosition="175" endWordPosition="178">ted by a vector in the latent semantic space and each relation is captured by a latent square matrix. The degree of two words having a specific relation can then be measured through simple linear algebraic operations. We demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources, MRLSA achieves stateof-the-art performance on existing benchmark datasets for two relations, antonymy and is-a. 1 Introduction Continuous semantic space representations have proven successful in a wide variety of NLP and IR applications, such as document clustering (Xu et al., 2003) and cross-lingual document retrieval (Dumais et al., 1997; Platt et al., 2010) at the document level and sentential semantics (Guo and Diab, 2012; Guo and Diab, 2013) and syntactic parsing (Socher et al., 2013) at the sentence level. Such representations also play an important role in applications for lexical semantics, such as word sense disambiguation (Boyd-Graber et al., 2007), measuring word *Work conducted while interning at Microsoft Research. similarity (Deerwester et al., 1990) and relational similarity (Turney, 2006; Zhila et al., 2013; Mikolov et al., 2013). In many of these applica</context>
</contexts>
<marker>Xu, Liu, Gong, 2003</marker>
<rawString>Wei Xu, Xin Liu, and Yihong Gong. 2003. Document clustering based on non-negative matrix factorization. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 267–273, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Vahed Qazvinian</author>
</authors>
<title>Measuring word relatedness using heterogeneous vector space models.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>616--620</pages>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="7301" citStr="Yih and Qazvinian, 2012" startWordPosition="1142" endWordPosition="1145">r by graphical models such as probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003). As a generalization of LSA, MRLSA is also a linear projection model. However, while the words are represented by vectors as well, multiple relations between words are captured separately by matrices. In the context of lexical semantics, VSMs provide a natural way of measuring semantic word relatedness by computing the distance between the corresponding vectors, which has been a standard approach (Agirre et al., 2009; Reisinger and Mooney, 2010; Yih and Qazvinian, 2012). These approaches do not apply directly to the problem of modeling other types of relations. Existing methods that do handle multiple relations often use a model combination scheme to integrate signals from various types of information sources. For instance, morphological variations discovered from the Google n-gram corpus have been combined with information from thesauri and vector-based word relatedness models for detecting antonyms (Mohammad et al., 2008). An alternative approach proposed by Turney (2008) that handles synonyms, antonyms and associations is to use a uniform approach by firs</context>
</contexts>
<marker>Yih, Qazvinian, 2012</marker>
<rawString>Wen-tau Yih and Vahed Qazvinian. 2012. Measuring word relatedness using heterogeneous vector space models. In Proceedings of NAACL-HLT, pages 616– 620, Montr´eal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Kristina Toutanova</author>
<author>John C Platt</author>
<author>Christopher Meek</author>
</authors>
<title>Learning discriminative projections for text similarity measures.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>247--256</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="6572" citStr="Yih et al., 2011" startWordPosition="1025" endWordPosition="1028">pproach. The most commonly used continuous space representation of text is arguably the vector space model (VSM) (Turney and Pantel, 2010). In this representation, each text object can be represented by a high-dimensional sparse vector, such as a term-vector or a document-vector that denotes the statistics of term occurrences (Salton et al., 1975) in a large corpus. The text can also be represented by a low-dimensional dense vector derived by linear projection models like latent semantic analysis (LSA) (Deerwester et al., 1990), by discriminative learning methods like Siamese neural networks (Yih et al., 2011), recurrent neural networks (Mikolov et al., 2013) and recursive neural networks (Socher et al., 2011), or by graphical models such as probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003). As a generalization of LSA, MRLSA is also a linear projection model. However, while the words are represented by vectors as well, multiple relations between words are captured separately by matrices. In the context of lexical semantics, VSMs provide a natural way of measuring semantic word relatedness by computing the distance between the cor</context>
</contexts>
<marker>Yih, Toutanova, Platt, Meek, 2011</marker>
<rawString>Wen-tau Yih, Kristina Toutanova, John C. Platt, and Christopher Meek. 2011. Learning discriminative projections for text similarity measures. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 247–256, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
<author>John Platt</author>
</authors>
<title>Polarity inducing latent semantic analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>1212--1222</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="2662" citStr="Yih et al., 2012" startWordPosition="404" endWordPosition="407">tic space. The proximity of the vectors in this space implies that the original text objects are semantically related. However, one well-known limitation of LSA is that it is unable to differentiate fine-grained relations. For instance, when applied to lexical semantics, synonyms and antonyms may both be assigned high similarity scores (Landauer and Laham, 1998; Landauer, 2002). Asymmetric relations like hyponyms and hypernyms also cannot be differentiated. Although there exists some recent work, such as PILSA which tries to overcome this weakness of LSA by introducing the notion of polarity (Yih et al., 2012). This extension, however, can only handle two opposing relations (e.g., synonyms and antonyms), leaving open the challenge of encoding multiple relations. In this paper, we propose Multi-Relational Latent Semantic Analysis (MRLSA), which strictly generalizes LSA to incorporate information of multiple relations concurrently. Similar to LSA or PILSA when applied to lexical semantics, each word is still mapped to a vector in the latent space. However, when measuring whether two words have a specific relation (e.g., antonymy or is-a), the word vectors will be mapped to a new space according to th</context>
<context position="13224" citStr="Yih et al. (2012)" startWordPosition="2169" endWordPosition="2172">rmal) = (EVT )T (EVT) (2) Thus, the semantic relatedness between the i-th and j-th words can be computed by cosine similarity1: cos(X:�i, X:�j) (3) When used to compare words, one well-known limitation of LSA is that the score captures the general notion of semantic similarity, and is unable to distinguish fine-grained word relations, such as antonyms (Landauer and Laham, 1998; Landauer, 2002). This is due to the fact that the raw matrix representation only records the occurrences of words in documents without knowing the specific relation between the word and document. To address this issue, Yih et al. (2012) proposed a polarity inducing latent semantic analysis model recently, which we introduce next. 1Cosine similarity is equivalent to the inner product of the normalized vectors. 3.1 Polarity Inducing Latent Semantic Analysis In order to distinguish antonyms from synonyms, the polarity inducing LSA (PILSA) model (Yih et al., 2012) takes a thesaurus as input. Synonyms and antonyms of the same target word are grouped together as a “document” and a document-term matrix is constructed accordingly as done in LSA. Because each word in a group belongs to either one of the two opposite relations, synony</context>
<context position="23631" citStr="Yih et al., 2012" startWordPosition="4004" endWordPosition="4007">opposite relations from the same source, MRLSA performs comparably to PILSA. However, MRLSA generalizes LSA to model multiple relations, which could be obtained from both homogeneous and heterogeneous data sources. As a result, the performance of a target task can be further improved. 5.1 Experimental Setup We construct the raw tensors to encode a particular relation in each slice based on two data sources. Encarta The Encarta thesaurus is developed by Bloomsbury Publishing Plc2. For each target word, it provides a list of synonyms and antonyms. We use the same version of the thesaurus as in (Yih et al., 2012), which contains about 47k words and a vocabulary list of approximately 50k words. WordNet We use four types of relations from WordNet: synonymy, antonymy, hypernymy and hyponymy. The number of target words and the size of the vocabulary in our version are 117,791 and 149,400, respectively. WordNet has better vocabulary coverage, but fewer antonym pairs. For instance, the WordNet antonym slice contains only 46,945 nonzero entries, while the Encarta antonym slice has 129,733. 2http://www.bloomsbury.com We apply a memory-efficient Tucker decomposition algorithm (Kolda and Sun, 2008) implemented </context>
<context position="24968" citStr="Yih et al., 2012" startWordPosition="4228" endWordPosition="4231">composed in about 3 hours using less than 4GB of memory with a commodity PC. 5.2 Answering GRE Antonym Questions The first task is to answer the closest-opposite questions from the GRE test provided by Mohammad et al. (2008)4. Each question in this test consists of a target word and five candidate words, where the goal is to pick the candidate word that has the most opposite meaning to the target word. In order to have a fair comparison, we use the same data split as in (Mohammad et al., 2008), with 162 questions used for the development set and 950 for test. Following (Mohammad et al., 2008; Yih et al., 2012), we report the results in precision (accuracy of the questions that the system attempts to answer), recall (percentage of the questions answered correctly over all questions) and F1 (the harmonic mean of precision and recall). We tune two sets of parameters using the development set: (1) the rank parameter T in the tensor decomposition and (2) the scaling factors of different slices of the tensor. The rank parameter specifies the number of dimensions of the latent space. In the experiments, We pick the best value of T from 1100, 200, 300, 500, 750, 10001. The scaling factors adjust the values</context>
</contexts>
<marker>Yih, Zweig, Platt, 2012</marker>
<rawString>Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Polarity inducing latent semantic analysis. In Proceedings of NAACL-HLT, pages 1212–1222, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alisa Zhila</author>
<author>Wen-tau Yih</author>
<author>Christopher Meek</author>
<author>Geoffrey Zweig</author>
<author>Tomas Mikolov</author>
</authors>
<title>Combining heterogeneous models for measuring relational similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1000--1009</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="1764" citStr="Zhila et al., 2013" startWordPosition="259" endWordPosition="262"> and IR applications, such as document clustering (Xu et al., 2003) and cross-lingual document retrieval (Dumais et al., 1997; Platt et al., 2010) at the document level and sentential semantics (Guo and Diab, 2012; Guo and Diab, 2013) and syntactic parsing (Socher et al., 2013) at the sentence level. Such representations also play an important role in applications for lexical semantics, such as word sense disambiguation (Boyd-Graber et al., 2007), measuring word *Work conducted while interning at Microsoft Research. similarity (Deerwester et al., 1990) and relational similarity (Turney, 2006; Zhila et al., 2013; Mikolov et al., 2013). In many of these applications, Latent Semantic Analysis (LSA) (Deerwester et al., 1990) has been widely used, serving as a fundamental component or as a strong baseline. LSA operates by mapping text objects, typically documents and words, to a latent semantic space. The proximity of the vectors in this space implies that the original text objects are semantically related. However, one well-known limitation of LSA is that it is unable to differentiate fine-grained relations. For instance, when applied to lexical semantics, synonyms and antonyms may both be assigned high</context>
<context position="8191" citStr="Zhila et al. (2013)" startWordPosition="1282" endWordPosition="1285">al variations discovered from the Google n-gram corpus have been combined with information from thesauri and vector-based word relatedness models for detecting antonyms (Mohammad et al., 2008). An alternative approach proposed by Turney (2008) that handles synonyms, antonyms and associations is to use a uniform approach by first reducing the problem to determining whether two 1603 pairs of words can be analogous, and then predicting it using a supervised model with features based on the frequencies of patterns in the corpus. Similarly, to measure whether two word pairs have the same relation, Zhila et al. (2013) proposed to combine heterogeneous models, which achieved state-of-the-art performance. In comparison, MRLSA models multiple lexical relations holistically. The degree that two words having a particular relation is estimated using the same linear function of the corresponding vectors and matrix. Tensor decomposition generalizes matrix factorization and has been applied to several NLP applications recently. For example, Cohen et al. (2013) proposed an approximation algorithm for PCFG parsing that relies on Kruskal decomposition. Van de Cruys et al. (2013) modeled the composition of subject-verb</context>
</contexts>
<marker>Zhila, Yih, Meek, Zweig, Mikolov, 2013</marker>
<rawString>Alisa Zhila, Wen-tau Yih, Christopher Meek, Geoffrey Zweig, and Tomas Mikolov. 2013. Combining heterogeneous models for measuring relational similarity. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1000–1009, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
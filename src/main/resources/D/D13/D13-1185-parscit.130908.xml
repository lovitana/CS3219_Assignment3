<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.796492">
Event Schema Induction with a Probabilistic Entity-Driven Model
</title>
<author confidence="0.742303">
Nathanael Chambers
</author>
<affiliation confidence="0.607105">
United States Naval Academy
</affiliation>
<address confidence="0.781145">
Annapolis, MD 21402
</address>
<email confidence="0.99853">
nchamber@usna.edu
</email>
<sectionHeader confidence="0.99665" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999894047619047">
Event schema induction is the task of learning
high-level representations of complex events
(e.g., a bombing) and their entity roles (e.g.,
perpetrator and victim) from unlabeled text.
Event schemas have important connections to
early NLP research on frames and scripts,
as well as modern applications like template
extraction. Recent research suggests event
schemas can be learned from raw text. In-
spired by a pipelined learner based on named
entity coreference, this paper presents the first
generative model for schema induction that in-
tegrates coreference chains into learning. Our
generative model is conceptually simpler than
the pipelined approach and requires far less
training data. It also provides an interesting
contrast with a recent HMM-based model. We
evaluate on a common dataset for template
schema extraction. Our generative model
matches the pipeline’s performance, and out-
performs the HMM by 7 F1 points (20%).
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99915552">
Early research in language understanding focused
on high-level semantic representations to drive their
models. Many proposals, such as frames and scripts,
used rich event schemas to model the situations de-
scribed in text. While the field has since focused on
more shallow approaches, recent work on schema
induction shows that event schemas might be learn-
able from raw text. This paper continues the trend,
addressing the question, can event schemas be in-
duced from raw text without prior knowledge? We
present a new generative model for event schemas,
and it produces state-of-the-art induction results, in-
cluding a 7 F1 point gain over a different generative
proposal developed in parallel with this work.
Event schemas are unique from most work in in-
formation extraction (IE). Current relation discovery
(Banko et al., 2007a; Carlson et al., 2010b) focuses
on atomic facts and relations. Event schemas build
relations into coherent event structures, often called
templates in IE. For instance, an election template
jointly connects that obama won a presidential elec-
tion with romney was the defeated, the election oc-
curred in 2012, and the popular vote was 50-48. The
entities in these relations fill specific semantic roles,
as in this template schema:
</bodyText>
<sectionHeader confidence="0.781814" genericHeader="introduction">
Template Schema for Elections
</sectionHeader>
<keyword confidence="0.798408333333333">
(events: nominate, vote, elect, win, declare, concede)
Date: Timestamp
Winner: Person
Loser: Person
Position: Occupation
Vote: Number
</keyword>
<bodyText confidence="0.999615642857143">
Traditionally, template extractors assume fore-
knowledge of the event schemas. They know a Win-
ner exists, and research focuses on supervised learn-
ing to extract winners from text. This paper focuses
on the other side of the supervision spectrum. The
learner receives no human input, and it first induces
a schema before extracting instances of it.
Our proposed model contributes to a growing
line of research in schema induction. The majority
of previous work relies on ad-hoc clustering algo-
rithms (Filatova et al., 2006; Sekine, 2006; Cham-
bers and Jurafsky, 2011). Chambers and Jurafsky
is a pipelined approach, learning events first, and
later learning syntactic patterns as fillers. It requires
</bodyText>
<page confidence="0.93969">
1797
</page>
<note confidence="0.7313905">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.998813714285714">
several ad-hoc metrics and parameters, and it lacks
the benefits of a formal model. However, central to
their algorithm is the use of coreferring entity men-
tions to knit events and entities together into an event
schema. We adapt this entity-driven approach to a
single model that requires fewer parameters and far
less training data. Further, experiments show state-
of-the-art performance.
Other research conducted at the time of this pa-
per also proposes a generative model for schema in-
duction (Cheung et al., 2013). Theirs is not entity-
based, but instead uses a sequence model (HMM-
based) of verb clauses. These two papers thus pro-
vide a unique opportunity to compare two very dif-
ferent views of document structure. One is entity-
driven, modeling an entity’s role by its coreference
chain. The other is clause-driven, classifying indi-
vidual clauses based on text sequence. Each model
makes unique assumptions, providing an interest-
ing contrast. Our entity model outperforms by 7 F1
points on a common extraction task.
The rest of the paper describes in detail our
main contributions: (1) the first entity-based gen-
erative model for schema induction, (2) a direct
pipeline/formal model comparison, (3) results im-
proving state-of-the-art performance by 20%, and
(4) schema induction from the smallest amount of
training data to date.
</bodyText>
<sectionHeader confidence="0.996137" genericHeader="method">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999882553846154">
Unsupervised learning for information extraction
usually learns binary relations and atomic facts.
Models can learn relations like Person is married to
Person without labeled data (Banko et al., 2007b), or
rely on seed examples for ontology induction (dog is
a mammal) and attribute extraction (dogs have tails)
(Carlson et al., 2010b; Carlson et al., 2010a; Huang
and Riloff, 2010; Durme and Pasca, 2008). These do
not typically capture the deeper connections mod-
eled by event schemas.
Algorithms that do focus on event schema extrac-
tion typically require both the schemas and labeled
corpora, such as rule-based approaches (Chinchor
et al., 1993; Rau et al., 1992) and modern super-
vised classifiers (Freitag, 1998; Chieu et al., 2003;
Bunescu and Mooney, 2004; Patwardhan and Riloff,
2009; Huang and Riloff, 2011). Classifiers rely on
the labeled examples’ surrounding context for fea-
tures (Maslennikov and Chua, 2007). Weakly su-
pervised learning removes some of the need for la-
beled data, but most still require the event schemas.
One common approach is to begin with unlabeled,
but clustered event-specific documents, and extract
common word patterns as extractors (Riloff and
Schmelzenbach, 1998; Sudo et al., 2003; Riloff et
al., 2005; Filatova et al., 2006; Patwardhan and
Riloff, 2007; Chen et al., 2011). Bootstrapping with
seed examples of known slot fillers has been shown
to be effective (Yangarber et al., 2000; Surdeanu et
al., 2006).
Shinyama and Sekine (2006) presented unre-
stricted relation discovery to discover relations in
unlabeled documents. Their algorithm used redun-
dant documents (e.g., all describe Hurricane Ivan)
to observe repeated proper nouns. The approach re-
quires many documents about the exact same event
instance, and relations are binary (not schemas) over
repeated named entities. Our model instead learns
schemas from documents with mixed topics that
don’t describe the same event, so repeated proper
nouns are less helpful.
Chen et al. (2011) perform relation extraction
with no supervision on earthquake and finance do-
mains. Theirs is a generative model that represents
relations as predicate/argument pairs. As with oth-
ers, training data is pre-clustered by event type and
there is no schema connection between relations.
This paper builds the most on Chambers and Ju-
rafsky (2011). They learned event schemas with a
three-stage clustering algorithm that included a re-
quirement to retrieve extra training data. This paper
removes many of these complexities. We present
a formal model that uniquely models coreference
chains. Advantages include a joint clustering of
events and entities, and a formal probabilistic inter-
pretation of the resulting schemas. We achieve better
performance, and do so with far less training data.
Cheung et al. (2013) is most related as a genera-
tive formulation of schema induction. They propose
an HMM-based model over latent event variables,
where each variable generates the observed clauses.
Latent schema variables generate the event vari-
ables (in the spirit of preliminary work by O’Connor
(2012)). There is no notion of an entity, so learning
uses text mentions and relies on the local HMM win-
</bodyText>
<page confidence="0.98929">
1798
</page>
<bodyText confidence="0.6255355">
message: id dev-muc3-0112 (bellcore, mitre)
incident: date 10 mar 89
incident: location peru: huanuco, ambo (town)
incident: type bombing
incident: stage accomplished
incident: instrument explosive: ”-”
perp: individual ”shining path members”
perp: organization ”shining path”
</bodyText>
<figureCaption confidence="0.994844">
Figure 1: A subset of the slots in a MUC-4 template.
</figureCaption>
<bodyText confidence="0.999901666666667">
dow for event transitions. Their model was created
in parallel with our work, and provides a nice con-
trast in both approach and results. Ours outperforms
their model by 20% on a MUC-4 evaluation.
In summary, this paper extends most previous
work on event schema induction by removing the
supervision. Of the recent ‘unsupervised’ work, we
present the first entity-driven generative model, and
we experiment on a mixed-domain corpus.
</bodyText>
<sectionHeader confidence="0.989379" genericHeader="method">
3 Dataset: The MUC-4 Corpus
</sectionHeader>
<bodyText confidence="0.999546833333333">
The corpus from the Message Understanding Con-
ference (MUC-4) serves as the challenge text (Sund-
heim, 1991), and will ground discussion of our
model. MUC-4 is also used by the closest previ-
ous work. It contains Latin American newswire
about terrorism events, and it provides a set of
hand-constructed event schemas that are tradition-
ally called template schemas. It also maps labeled
templates to the text, providing a dataset for tem-
plate extraction evaluations. Until very recently,
only extraction has been evaluated. We too evalu-
ate our model through extraction, but we also com-
pare our learned schemas to the hand-created tem-
plate schemas. An example of a filled in MUC-4
template is given in Figure 1.
The MUC-4 corpus defines six template types:
Attack, Kidnapping, Bombing, Arson, Robbery,
and Forced Work Stoppage. Documents are often
labeled with more than one template and type. Many
include multiple events at different times in different
locations. The corpus is particularly challenging be-
cause template schemas are inter-mixed and entities
can play multiple roles across instances.
The training corpus contains 1300 documents,
733 of which are labeled with at least one schema.
567 documents are not labeled with any schemas.
These unlabeled documents are articles that report
on non-specific political events and speeches. They
make the corpus particularly challenging. The de-
velopment and test sets each contain 200 documents.
</bodyText>
<sectionHeader confidence="0.984025" genericHeader="method">
4 A Generative Model for Event Schemas
</sectionHeader>
<bodyText confidence="0.9999634">
This paper’s model is an entity-based approach, sim-
ilar in motivation to Haghighi and Klein (2010) and
the pipelined induction of Chambers and Jurafsky
(2011). Coreference resolution guides the learning
by providing a set of pre-resolved entities. Each
entity receives a schema role label, so it allows all
mentions of the entity to inform that role choice.
This important constraint links coreferring mentions
to the same schema role, and distinguishes our ap-
proach from others (Cheung et al., 2013).
</bodyText>
<subsectionHeader confidence="0.961125">
4.1 Illustration
</subsectionHeader>
<bodyText confidence="0.999735888888889">
The model represents a document as a set of enti-
ties. An entity is a set of entity mentions clustered
by coreference resolution. We will use the following
two sentences for illustration:
A truck bomb exploded near the embassy.
Three militia planted it, and then they fled.
This text contains five entity mentions. A perfect
coreference resolution system will resolve these five
mentions into three entities:
</bodyText>
<subsectionHeader confidence="0.988459">
Entity Mentions Entities Roles
</subsectionHeader>
<bodyText confidence="0.975351818181818">
a truck bomb (a truck bomb, it) Instrument
the embassy (the embassy) Target
three militia (three militia, they) Perpetrator
it
they
The schema roles, or template slots, are the type
of target knowledge we want to learn. Each en-
tity will be labeled with both a slot variable s and
a template variable t (e.g., the s=perpetrator of a
t=bombing). The lexical context of the entity men-
tions guides the learning model to this end.
</bodyText>
<subsectionHeader confidence="0.967722">
4.2 Definitions
</subsectionHeader>
<bodyText confidence="0.986383">
A document d E D is represented as a set of entities
Ed. Each entity e E Ed is a triple: e = (h, M, F)
</bodyText>
<footnote confidence="0.613915">
1. he is the canonical word for the entity (typically
the first mention’s head word)
</footnote>
<page confidence="0.993059">
1799
</page>
<subsectionHeader confidence="0.713386">
Text
</subsectionHeader>
<bodyText confidence="0.967489">
A truck bomb exploded near the embassy.
Three militia planted it, and then they fled.
</bodyText>
<subsectionHeader confidence="0.623134">
Entity Representation
</subsectionHeader>
<equation confidence="0.998188625">
entity 1: h = bomb, F = {PHYS-OBJ},
M = { (p=explode, d=subject-explode)
(p=plant, d=object-plant) }
entity 2: h = militia, F = {PERSON, ORG},
M = { (p=plant, d=subject-plant),
(p=flee, subject-flee) }
entity 3: h = embassy, F = {PHYS-OBJ, ORG},
M = { (p=explode, d=prep near-explode) }
</equation>
<figureCaption confidence="0.989449">
Figure 2: Example text mapped to our entities.
</figureCaption>
<listItem confidence="0.778032">
2. Me is a set of entity mentions m E Me. Each
mention is a pair m = (p, d): the predicate, and
the typed dependency from the predicate to the
mention (e.g., push and subject-push).
3. Fe is a set of binary entity features. This paper
only uses named entity types as features, but
generalizes to other features as well.
</listItem>
<bodyText confidence="0.999222111111111">
A document is thus reduced to its entities, their
grammatical contexts, and entity features. Figure 2
continues our example using this formulation. he is
chosen to be e’s longest non-pronoun mention m E
Me. Mentions are labeled with NER and WordNet
synsets to create an entity’s features Fe C {Person,
Org, Loc, Event, Time, Object, Other}. We use the
Stanford NLP toolkit to parse, extract typed depen-
dencies, label with NER, and run coreference.
</bodyText>
<subsectionHeader confidence="0.999611">
4.3 The Generative Models
</subsectionHeader>
<bodyText confidence="0.999951642857143">
Similar to topics in LDA, each document d in our
model has a corresponding multinomial over schema
types θd, drawn from a Dirichlet. For each entity in
the document, a hidden variable t is drawn accord-
ing to θd. These t variables represent the high level
schema types, such as bombing or kidnapping. The
predicates associated with each of the entity’s men-
tions are then drawn from the schema’s multinomial
over predicates Pt. The variable t also generates
a hidden variable s from its distribution over slots,
such as perpetrator and victim. Finally, the entity’s
canonical head word is generated from βs, all entity
mentions’ typed dependencies from δs, and named
entity types from γs.
The most important characteristic of this model
is the separation of event words from the lexical
properties of specific entity mentions. The schema
type variables t only model the distribution of event
words (bomb, plant, defuse), but the slot variables
s model the syntax (subject-bomb, subject-plant,
object-arrest) and entity words (suspect, terrorist,
man). This allows the high-level schemas to first se-
lect predicates, and then forces predicate arguments
to prefer slots that are in the parent schema type.
Formally, a document d receives a labeling Zd
where each entity e E Ed is labeled Zd,e = (t, s)
with a schema type t and a slot s. The joint distribu-
tion of a document and labeling is then as follows:
</bodyText>
<equation confidence="0.999611333333334">
P(d, Zd) = 11 P(t|θ) x P(s|t)
eEEd
11 x P(he|s)
eEEd
P(f|s)
P(dM|s) * P(pM|t) (1)
</equation>
<bodyText confidence="0.99716125">
The plate diagram for the model is given in Fig-
ure 3. The darker circles correspond to the observed
entity components in Figure 2. We assume the fol-
lowing generative process for a document d:
</bodyText>
<table confidence="0.7788901">
Generate θd from Dir(α)
for each schema type t = 1...m do
Generate Pt from Dir(η)
for each slot st = 1...k do
Generate βs from Dir(µ)
Generate γs from Dir(ν)
Generate δs from Dir(ϕ)
for each entity e E Ed do
Generate schema type t from Multinomial(θd)
Generate slot s from UniformDist(k)
</table>
<bodyText confidence="0.877445222222222">
Generate head word h from Multinomial(βs)
for each mention m E Me do
Generate predicate token p from Multinomial(Pt)
Generate typed dependency d from Multinomial(δs)
for each entity type i = 1..|Fe |do
Generate entity type f from Multinomial(γs)
The number of schema types m and the number
of slots per schema k are chosen based on training
set performance.
</bodyText>
<figure confidence="0.99874438">
11 x
eEEd
11
fEFe
11 x
eEEd
11
MEMe
1800
ENTITY HEAD
h
ENTITY FEATURES ENTITY MENTIONS
s
f
F
e
�
d
NAMED ENTITIES
P
DOCUMENTS
M
h
R
e
s
b
k
d
M
E
D
R
h
s
b
k
d
e
t
P
|T|
p
M
E
D
E
D
P
kxmk
</figure>
<figureCaption confidence="0.9938245">
Figure 3: The full plate diagram for the event schema
model. Hyper-parameters are omitted for readability.
</figureCaption>
<subsectionHeader confidence="0.589516">
The Flat Relation Model
</subsectionHeader>
<bodyText confidence="0.999980380952381">
We also experiment with a Flat Relation Model that
removes the hidden t variables, ignoring schema
types. Figure 4 visually compares this flat model
with the full model. We found that the predicate
distribution Pt hurts performance in a flat model.
Predicates are more informative at the higher level,
but less so for slots where syntax is more important.
We thus removed Pt from the model, and everything
else remains the same. This flat model now learns
a large set of k slots 5 that aren’t connected by a
high-level schema variable. Each slot s ∈ 5 has a
corresponding triple of multinomials (h, M, F) sim-
ilar to above: (1) a multinomial over the head men-
tions 0s, (2) a multinomial over the grammatical re-
lations of the entity mentions Ss, and (3) a multino-
mial over the entity features -ys. For each entity in
a document, a hidden slot s ∈ 5 is first drawn from
O, and then the observed entity (h, M, F) is drawn
according to the multinomials (0s, -ys, Ss). We later
evaluate this flat model to show the benefit of added
schema structure.
</bodyText>
<subsectionHeader confidence="0.676827">
4.4 Inference
</subsectionHeader>
<bodyText confidence="0.994188">
We use collapsed Gibbs sampling for inference,
sampling the latent variables te,d and se,d in se-
</bodyText>
<figureCaption confidence="0.992732333333333">
Figure 4: Simplified plate diagrams comparing the flat
relation model to the full template model. The observed
f ∈ F variables are not included for clarity.
</figureCaption>
<bodyText confidence="0.99855472">
quence conditioned on a full setting of all the other
variables (Griffiths and Steyvers, 2004). Initial pa-
rameter values are set by randomly setting t and s
variables from the uniform distribution over schema
types and slots, then computing the other parameter
values based on these initial settings. The hyperpa-
rameters for the dirichlet distributions were chosen
from a small grid search (see Experiments).
Beyond standard inference, we added one con-
straint to the model that favors grammatical distri-
butions Ss that do not contain conflicts. The subject
and direct object of a verb should not both receive
high probability mass under the same schema slot
Ss. For instance, the victim of a kidnapping should
not favor both the subject and object of a single verb.
Semantic roles should (typically) select one syntac-
tic slot, so this constraint encourages that behavior.
During sampling of se,d, we use a penalty factor A
to make conflicting relations less likely. Formally,
P(se,d = s|0, he, Fe, Me) = A iff there exists an
m ∈ Me such that P(m|as) &lt; P(inv(m)|as) and
P(inv(m)|as) &gt; 0.1, where inv(m) = object if
m = subject and vice versa. Otherwise, the proba-
bility is computed as normal. We normalize the dis-
tributions after penalties are computed.
</bodyText>
<subsectionHeader confidence="0.965308">
4.5 Entity Extraction for Template Filling
</subsectionHeader>
<bodyText confidence="0.999922">
Inducing event schemas is only one benefit of the
model. The learned model can also extract spe-
cific instances of the learned schemas without ad-
</bodyText>
<equation confidence="0.9638365">
R Y s
m
</equation>
<page confidence="0.898247">
1801
</page>
<bodyText confidence="0.999989733333333">
ditional complexity. To evaluate the effectiveness of
the model, we apply the model to perform standard
template extraction on MUC-4. Previous MUC-4
induction required an extraction algorithm separate
from induction because induction created hard clus-
ters (Chambers and Jurafsky, 2011). Cluster scores
don’t have a natural interpretation, so extraction re-
quired several parameters/thresholds to tune. Our
model instead simply relies on model inference.
We run inference as described above and each en-
tity receives a template label te,d and a template slot
label se,d. These labels are the extractions, and it re-
quires no other parameters. The model thus requires
far less machinery than a pipeline, and the exper-
iments below further show that this simpler model
outperforms the pipeline.
Beyond parameters, the question of “irrelevant”
documents is a concern in MUC-4. Approximately
half the corpus are documents that are not labeled
with a template, so past algorithms required extra
processing stages to filter out these irrelevant doc-
uments. Patwardhan and Riloff (2009) and Cham-
bers and Jurafsky (2011) make initial decisions as to
whether they should extract or not from a document.
Huang and Riloff (2011) use a genre detector for this
problem. Even the generative HMM-based model of
Cheung et al. (Cheung et al., 2013) requires an ex-
tra filtering parameter. Our formal model is unique
in not requiring additional effort. Ours is the only
approach that doesn’t require document filtering.
</bodyText>
<sectionHeader confidence="0.994753" genericHeader="method">
5 Evaluation Setup
</sectionHeader>
<bodyText confidence="0.9999885">
Evaluating on MUC-4 has a diverse history that
complicates comparison. The following balances
comparison against previous work and enables fu-
ture comparison to our results.
</bodyText>
<subsectionHeader confidence="0.993833">
5.1 Template Schema Slots
</subsectionHeader>
<bodyText confidence="0.999894666666667">
Most systems do not evaluate performance on all
MUC-4 template slots. They instead focus on four
main slots, ignoring the parameterized slots that in-
volve deeper reasoning (such as ‘stage of execution’
and ‘effect of incident’). The four slots and example
entity fillers are shown here:
</bodyText>
<table confidence="0.350738">
Perpetrator: Shining Path members
Victim: Sergio Horna
</table>
<tableCaption confidence="0.4815685">
Target: public facilities
Instrument: explosives
</tableCaption>
<bodyText confidence="0.999896307692308">
We also focus only on these four slots. We merged
MUC’s two perpetrator slots (individuals and orgs)
into one gold Perpetrator. Previous work has both
split the two and merged the two. We merge them
because the distinction between an individual and
an organization is often subtle and not practically
important to analysts. This is also consistent with
the most recent event schema induction in Chambers
and Jurafsky (2011) and Cheung et al. (2013).
One peculiarity in MUC-4 is that some templates
are labeled as optional (i.e., all its slots are optional),
and some required templates contain optional slots
(i.e., a subset of slots are optional). We ignore
both optional templates and specific optional slots
when computing recall, as in previous work (Pat-
wardhan and Riloff, 2007; Patwardhan and Riloff,
2009; Chambers and Jurafsky, 2011).
Comparison between the extracted strings and the
gold template strings uses head word scoring. We
do not use gold parses for the text, so head words
are defined simply as the rightmost word in the noun
phrase. The exception is when the extracted phrase
is of the form “A of B”, then the rightmost word in
“A” is used as the head. This is again consistent with
previous work1. The standard evaluation metrics are
precision, recall, and F1 score.
</bodyText>
<subsectionHeader confidence="0.999717">
5.2 Mapping Learned Slots
</subsectionHeader>
<bodyText confidence="0.999272">
Induced schemas need to map to gold schemas be-
fore evaluation. Which learned slots correspond to
MUC-4 slots? There are two methods of mapping.
The first ignores the schema type variables t, and
simply finds the best performing s variable for each
gold template slot2. We call this the slot-only map-
ping evaluation. The second approach is to map each
template variable t to the best gold template type g,
and limit the slot mapping so that only the slots un-
der t can map to slots under g. We call this the tem-
plate mapping evaluation. The slot-only mapping
can result in higher scores since it is not constrained
to preserve schema structure in the mapping.
Chambers and Jurafsky (2011) used template
mapping in their evaluation. Cheung et al. (2013)
used slot-only mapping. We run both evaluations in
this paper and separately compare both.
</bodyText>
<footnote confidence="0.999255">
1Personal communications with Patwardhan and Riloff
2bombing-victim is a template slot distinct from kidnap-
victim. Both need to be mapped.
</footnote>
<page confidence="0.998191">
1802
</page>
<sectionHeader confidence="0.99862" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.996912714285714">
We use the Stanford CoreNLP toolkit for text pro-
cessing and parsing. We developed the models on
the 1300 document MUC-4 training set. We then
learned once on the entire 1700 training/dev/test set,
and report extraction numbers from the inferred la-
bels on the 200 document test set. Each experiment
was repeated 10 times. Reported numbers are aver-
aged across these runs.
There are two structure variables for the model:
the number of schema types and the number of slots
under each type. We searched for the optimal values
on the training set before evaluating on test. The
hyperparameters for all evaluations were set to α =
n = p = v = 1, co = .1 based on a grid search.
</bodyText>
<subsectionHeader confidence="0.997858">
6.1 Template Schema Induction
</subsectionHeader>
<bodyText confidence="0.9997097">
The first evaluation compares the learned schemas
to the gold schemas in MUC-4.
Since most previous work assumes this knowl-
edge ahead of time, we align our schemas with the
main MUC-4 template types to measure quality. We
inspected the learned event schemas that mapped to
MUC-4 schemas based on the template mapping ex-
traction evaluation.
Figure 5 shows some of the learned distribu-
tions for two mapped schemas: kidnappings and
bombings. The predicate distribution for each event
schema is shown, as well as the top 5 head words
and grammatical relations for each slot. The words
and events that were jointly learned in these exam-
ples appear quite accurate. The bombing and kidnap
schemas learned all of the equivalent MUC-4 gold
slots. Interestingly, our model also learned Loca-
tions and Times as important entities that appear in
the text. These entities are not traditionally included
in the MUC-4 extraction task.
Figure 6 lists the MUC-4 slots that we did and
did not learn for the four most prevelant types. We
report 71% recall, with almost all errors due to the
model’s failure to learn about arsons. Arson tem-
plates only occur in 40 articles, much less than the
200 bombing and over 400 attack. We show below
that overall extraction performs well despite this.
The learned distributions for Attack end up extract-
ing Arson perpetrators and Arson victims in the ac-
tual extraction evaluation.
</bodyText>
<table confidence="0.854613857142857">
Bomb Kidnap Attack Arson
Perpetrator ✓ ✓ ✓ x
Victim ✓ ✓ ✓ ✓
Target ✓ - ✓ x
Instrument ✓ - x x
Location ✓ ✓ ✓ ✓
Date/Time ✓ ✓ ✓ x
</table>
<figureCaption confidence="0.966474333333333">
Figure 6: The MUC-4 gold slots that were learned. The
bottom two are not in the traditional evaluation, but were
learned by our model nonetheless.
</figureCaption>
<table confidence="0.9658085">
Evaluation: Template Mapping
Prec Recall F1
C &amp; J 2011 .48 .25 .33
Formal Template Model .42 .27 .33
</table>
<tableCaption confidence="0.985021333333333">
Table 1: MUC-4 extraction with template mapping. A
learned schema first maps to a gold MUC template.
Learned slots can then only map to slots in that template.
</tableCaption>
<subsectionHeader confidence="0.999808">
6.2 Extraction Experiments
</subsectionHeader>
<bodyText confidence="0.999922384615385">
We now present the full extraction experiment that
is traditionally used for evaluating MUC-4 per-
formance. Although our learned schemas closely
match gold schemas, extraction depends on how
well the model can extract from diverse lexical con-
texts. We ran inference on the full training and test
sets, and used the inferred labels as schema labels.
These labels were mapped and evaluated against the
gold MUC-4 labels as discussed in Section 5.
Performance is compared to two state-of-the-art
induction systems. Since these previous two mod-
els used different methods to map their learned
schemas, we compare separately. Table 1 shows the
template mapping evaluation with Chambers and Ju-
rafsky (C&amp;J). Table 2 shows the slot-only mapping
evaluation with Cheung et al.
Our model achieves an F1 score comparable to
C&amp;J, and 20% higher than Cheung et al. Part of the
greater increase over Cheung et al. is the mapping
difference. For each MUC-4 type, such as bombing,
any four learned slots can map to the four MUC-
4 bombing slots. There is no constraint that the
learned slots must come from the same schema type.
The more strict template mapping (Table 1) ensures
that entire schema types are mapped together, and it
reduces our performance from .41 to .33.
</bodyText>
<page confidence="0.956771">
1803
</page>
<table confidence="0.984736">
Kidnapping Entities
Victim (Person 88%) Perpetrator (Person 62%, Org 30%) Date (TimeDate 89%)
businessman object-kidnap guerrilla subject-kidnap TIME tmod-kidnap
citizen object-release ELN subject-hold February prep on-kidnap
Soares prep of-kidnapping group subject-attack hours tmod-release
Kent possessive-release extraditables subject-demand morning prep on-release
hostage object-found man subject-announce night tmod-take
Bombing Entities
Victim (Person 86%, Location 8%) Physical Target (Object 65%, Event 42%) Instrument (Event 56%, Object 39%)
person object-kill building object-destroy bomb subject-explode
guerrilla object-wound office object-damage explosion subject-occur
soldier subject-die explosive object-use attack object-cause
man subject-blow up station and-office charge object-place
civilian subject-try vehicle prep of-number device subject-destroy
</table>
<figureCaption confidence="0.672983">
Figure 5: Select distributions for two learned events. Left columns are head word distributions 0, right columns are
syntactic relation distributions S, and entity types in parentheses are the learned y. Most probable words are shown.
</figureCaption>
<table confidence="0.9783748">
Evaluation: Slot-Only Mapping
Prec Recall F1
Cheung et al. 2013 .32 .37 .34
Flat Relation Model .26 .45 .33
Formal Template Model .41 .41 .41
</table>
<tableCaption confidence="0.9747195">
Table 2: MUC-4 extraction with slot-only mapping. Any
learned slot is allowed to map to any gold slot.
</tableCaption>
<table confidence="0.992652833333333">
Entity Role Performance
Prec Recall F1
Perpetrator .40 .20 .26
Victim .42 .31 .34
Target .38 .28 .31
Instrument .57 .39 .45
</table>
<tableCaption confidence="0.994585">
Table 3: Results for each MUC-4 template slot using the
template-mapping evaluation.
</tableCaption>
<bodyText confidence="0.998878615384615">
The macro-level F1 scores can be broken down
into individual slot performance. Table 3 shows
these results ranging from .26 to .45. The Instrument
role proves easiest to learn, consistent with C&amp;J.
A large portion of MUC-4 includes irrelevant
documents. Cheung et al. (2013) evaluated their
model without irrelevant documents in the test set
that to see how performance is affected. We com-
pare against their numbers in Table 4. Results are
closer now with ours outperforming .46 to .43 F1.
This suggests that the HMM-based approach stum-
bles more on spurious documents, but performs bet-
ter on relevant ones.
</bodyText>
<table confidence="0.8925005">
Gold Document Evaluation
Prec Recall F1
Cheung et al. 2013 .41 .44 .43
Formal Template Model .49 .43 .46
</table>
<tableCaption confidence="0.922902666666667">
Table 4: Full MUC-4 extraction with gold document clas-
sification. These results ignore false positives extracted
from “irrelevant” documents in the test set.
</tableCaption>
<subsectionHeader confidence="0.999087">
6.3 Model Ablation
</subsectionHeader>
<bodyText confidence="0.9998742">
Table 2 shows that the flat relation model (no latent
type variables t) is inferior to the full schema model.
F1 drops 20% without the explicit modeling of both
schema types t and their entity slots s. The entity
features Fe are less important. Experiments with-
out them show a slight drop in performance (2 F1
points), small enough that they could be removed for
efficiency. However, it is extremely useful to learn
slots with NER labels like Person or Location.
Finally, we experimented without the sub-
ject/object constraint (Section 4.4). Performance
drops 5-10% depending on the number of schemas
learned. Anecdotally, it merges too many schema
slots that should be separate. We recommend using
this constraint as it has little impact on CPU time.
</bodyText>
<subsectionHeader confidence="0.93135">
6.4 Extension: Reduce Training Size
</subsectionHeader>
<bodyText confidence="0.9999605">
One of the main benefits of this generative model
appears to be the reduction in training data. The
pipelined approach in C&amp;J required an information
retrieval stage to bring in hundreds of other docu-
</bodyText>
<page confidence="0.989793">
1804
</page>
<bodyText confidence="0.999971647058824">
ments from an external corpus. This paper’s genera-
tive model doesn’t require such a stage.
We thus attempted to induce and extract event
schemas from just the 200 test set documents, with
no training or development data. We repeated this
experiment 30 times and averaged the results, setting
the number of templates t = 20 and slots s = 10 as
in the main experiment. The resulting F1 score for
the template-mapping evaluation fell to 0.27 from
the full data experiment of 0.33 F1. Adding more
training documents in another experiment did not
significantly increase performance over 0.27 until
all training and development documents were in-
cluded. This could be explained by the develop-
ment set being more similar to the test set than train-
ing. We did not investigate further to prevent over-
experimentation on test.
</bodyText>
<sectionHeader confidence="0.99966" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999992481481482">
Our model is one of the first generative formula-
tions of schema induction. It produces state-of-the-
art performance on a traditional extraction task, and
performs with less training data as well as a more
complex pipelined approach. Further, our unique
entity-driven approach outperforms an HMM-based
model developed in parallel to this work.
Our entity-driven proposal is strongly influenced
by the ideas in the pipeline model of Chambers and
Jurafsky (2011). Coreference chains have been used
in a variety of learning tasks, such as narrative learn-
ing and summarization. Here we are the first to show
how it can be used for schema induction in a proba-
bilistic model, connecting predicates across a docu-
ment in a way that is otherwise difficult to represent.
The models perform similarly, but ours also includes
significant benefits like a reduction in complexity,
reproducibility, and a large reduction in training data
requirements.
This paper also implies that learning and ex-
traction need not be independent algorithms. Our
model’s inference procedure to learn schemas is the
same one that labels text for extraction. C&amp;J re-
quired 3-4 separate pipelined steps. Cheung et al.
(2013) required specific cutoffs for document classi-
fication before extraction. Not only does our model
perform well, but it does so without these steps.
Highlighted here are key differences between this
proposal and the HMM-based model of Cheung et
al. (2013). One of the HMM strengths is the in-
clusion of sequence-based knowledge. Each slot la-
bel is influenced by the previous label in the text,
encouraging syntactic arguments of a predicate to
choose the same schema. This knowledge is only
loosely present in our document distribution 0. Che-
ung et al. also include a hidden event variable be-
tween the template and slot variables. Our model
collapses this event variable and makes fewer depen-
dency assumptions. This difference requires further
investigation as it is unclear if it provides valuable
information, or too much complexity.
We also note a warning for future work on proper
evaluation methodology. This task is particularly
difficult to compare to other models due to its
combination of both induction and then extraction.
There are many ways to map induced schemas to
gold answers, and this paper illustrates how ex-
traction performance is significantly affected by the
choice. We suggest the template-mapping evalua-
tion to preserve learned structure.
Finally, these induced results are far behind su-
pervised learning (Huang and Riloff, 2011). There
is ample room for improvement and future research
in event schema induction.
</bodyText>
<sectionHeader confidence="0.99756" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999988363636364">
This work was partially supported by a grant from
the Office of Naval Research. It was also sup-
ported, in part, by the Johns Hopkins Human Lan-
guage Technology Center of Excellence. Any opin-
ions, findings, and conclusions or recommendations
expressed in this material are those of the author.
Thanks to Eric Wang for his insights into Bayesian
modeling, Brendan O’Connor for his efforts on nor-
malizing MUC-4 evaluation details, Frank Ferraro
and Benjamin Van Durme for helpful conversations,
and to the reviewers for insightful feedback.
</bodyText>
<page confidence="0.990111">
1805
</page>
<sectionHeader confidence="0.98281" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999798352380952">
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007a. Learning
relations from the web. In Proceedings of the Interna-
tional Joint Conferences on Artificial Intelligence (IJ-
CAI).
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007b. Open in-
formation extraction from the web. In Proceedings of
the International Joint Conferences on Artificial Intel-
ligence (IJCAI).
Razvan Bunescu and Raymond Mooney. 2004. Collec-
tive information extraction with relational markov net-
works. In Proceedings of the Association for Compu-
tational Linguistics (ACL), pages 438–445.
Andrew Carlson, J. Betteridge, B. Kisiel, B. Settles,
E.R. Hruschka Jr., and T.M. Mitchell. 2010a. To-
ward an architecture for never-ending language learn-
ing. In Proceedings of the Conference on Artificial
Intelligence (AAAI).
Andrew Carlson, J. Betteridge, R.C. Wang, E.R. Hr-
uschka Jr., and T.M. Mitchell. 2010b. Coupled semi-
supervised learning for information extraction. In Pro-
ceedings of the ACM International Conference on Web
Search and Data Mining (WSDM).
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of the Association for Computational Lin-
guistics.
Harr Chen, Edward Benson, Tahira Naseem, and Regina
Barzilay. 2011. In-domain relation discovery with
meta-constraints via posterior regularization. In Pro-
ceedings of the Association for Computational Lin-
guistics (ACL).
Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
Proceedings of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies.
Hai Leong Chieu, Hwee Tou Ng, and Yoong Keok Lee.
2003. Closing the gap: Learning-based information
extraction rivaling knowledge-engineering methods.
In Proceedings of the Association for Computational
Linguistics (ACL).
Nancy Chinchor, David Lewis, and Lynette Hirschman.
1993. Evaluating message understanding systems: an
analysis of the third message understanding confer-
ence. Computational Linguistics, 19:3:409–449.
Benjamin Van Durme and Marius Pasca. 2008. Finding
cars, goddesses and enzymes: Parametrizable acquisi-
tion of labeled instances for open-domain information
extraction. In Proceedings of the 23rd Annual Con-
ference on Artificial Intelligence (AAAI-2008), pages
1243–1248.
Elena Filatova, Vasileios Hatzivassiloglou, and Kathleen
McKeown. 2006. Automatic creation of domain tem-
plates. In Proceedings of the Association for Compu-
tational Linguistics (ACL).
Dayne Freitag. 1998. Toward general-purpose learning
for information extraction. In Proceedings of the As-
sociation for Computational Linguistics (ACL), pages
404–408.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. In Proceedings of the National Academy of
Sciences of the United States of America, pages 5228–
5235.
Aria Haghighi and Dan Klein. 2010. An entity-level ap-
proach to information extraction. In Proceedings of
the Association for Computational Linguistics (ACL).
Ruihong Huang and Ellen Riloff. 2010. Inducing
domain-specific semantic class taggers from (almost)
nothing. In Proceedings of the Association for Com-
putational Linguistics (ACL).
Ruihong Huang and Ellen Riloff. 2011. Peeling back the
layers: Detecting event role fillers in secondary con-
texts. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics (ACL).
Mstislav Maslennikov and Tat-Seng Chua. 2007. Auto-
matic acquisition of domain knowledge for informa-
tion extraction. In Proceedings of the Association for
Computational Linguistics (ACL).
Brendan O’Connor. 2012. Learning frames from text
with an unsupervised latent variable model. Technical
report, Carnegie Mellon University.
Siddharth Patwardhan and Ellen Riloff. 2007. Effective
ie with semantic affinity patterns and relevant regions.
In Proceedings of the Conference on Empirical Meth-
ods on Natural Language Processing (EMNLP).
Siddharth Patwardhan and Ellen Riloff. 2009. A unified
model of phrasal and sentential evidence for informa-
tion extraction. In Proceedings of the Conference on
Empirical Methods on Natural Language Processing
(EMNLP).
Lisa Rau, George Krupka, Paul Jacobs, Ira Sider, and
Lois Childs. 1992. Ge nltoolset: Muc-4 test results
and analysis. In Proceedings of the Message Under-
standing Conference (MUC-4), pages 94–99.
Ellen Riloff and Mark Schmelzenbach. 1998. An em-
pirical approach to conceptual case frame acquisition.
In Proceedings of the Sixth Workshop on Very Large
Corpora.
Ellen Riloff, Janyce Wiebe, and William Phillips. 2005.
Exploiting subjectivity classification to improve infor-
mation extraction. In Proceedings of AAAI-05.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proceedings of the Joint Conference of the
</reference>
<page confidence="0.831645">
1806
</page>
<reference confidence="0.9994444">
International Committee on Computational Linguis-
tics and the Association for Computational Linguis-
tics, pages 731–738.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
ie using unrestricted relation discovery. In Proceed-
ings of NAACL.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic ie pattern acquisition. In Pro-
ceedings of the Association for Computational Lin-
guistics (ACL), pages 224–231.
Beth M. Sundheim. 1991. Third message understand-
ing evaluation and conference (muc-3): Phase 1 status
report. In Proceedings of the Message Understanding
Conference.
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2006.
A hybrid approach for the acquisition of information
extraction patterns. In Proceedings of the EACL Work-
shop on Adaptive Text Extraction and Mining.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisi-
tion of domain knowledge for information extraction.
In Proceedings of the 18th International Conference
on Computational Linguistics (COLING), pages 940–
946.
</reference>
<page confidence="0.994198">
1807
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.346260">
<title confidence="0.902696">Event Schema Induction with a Probabilistic Entity-Driven Model</title>
<author confidence="0.71282">Nathanael</author>
<affiliation confidence="0.880205">United States Naval</affiliation>
<address confidence="0.628148">Annapolis, MD</address>
<email confidence="0.99481">nchamber@usna.edu</email>
<abstract confidence="0.988758818181818">Event schema induction is the task of learning high-level representations of complex events a and their entity roles (e.g., from unlabeled text. Event schemas have important connections to early NLP research on frames and scripts, as well as modern applications like template extraction. Recent research suggests event schemas can be learned from raw text. Inspired by a pipelined learner based on named entity coreference, this paper presents the first generative model for schema induction that integrates coreference chains into learning. Our generative model is conceptually simpler than the pipelined approach and requires far less training data. It also provides an interesting contrast with a recent HMM-based model. We evaluate on a common dataset for template schema extraction. Our generative model matches the pipeline’s performance, and outperforms the HMM by 7 F1 points (20%).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Learning relations from the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Joint Conferences on Artificial Intelligence (IJCAI).</booktitle>
<contexts>
<context position="1930" citStr="Banko et al., 2007" startWordPosition="289" endWordPosition="292">ibed in text. While the field has since focused on more shallow approaches, recent work on schema induction shows that event schemas might be learnable from raw text. This paper continues the trend, addressing the question, can event schemas be induced from raw text without prior knowledge? We present a new generative model for event schemas, and it produces state-of-the-art induction results, including a 7 F1 point gain over a different generative proposal developed in parallel with this work. Event schemas are unique from most work in information extraction (IE). Current relation discovery (Banko et al., 2007a; Carlson et al., 2010b) focuses on atomic facts and relations. Event schemas build relations into coherent event structures, often called templates in IE. For instance, an election template jointly connects that obama won a presidential election with romney was the defeated, the election occurred in 2012, and the popular vote was 50-48. The entities in these relations fill specific semantic roles, as in this template schema: Template Schema for Elections (events: nominate, vote, elect, win, declare, concede) Date: Timestamp Winner: Person Loser: Person Position: Occupation Vote: Number Tradi</context>
<context position="4975" citStr="Banko et al., 2007" startWordPosition="760" endWordPosition="763">sting contrast. Our entity model outperforms by 7 F1 points on a common extraction task. The rest of the paper describes in detail our main contributions: (1) the first entity-based generative model for schema induction, (2) a direct pipeline/formal model comparison, (3) results improving state-of-the-art performance by 20%, and (4) schema induction from the smallest amount of training data to date. 2 Previous Work Unsupervised learning for information extraction usually learns binary relations and atomic facts. Models can learn relations like Person is married to Person without labeled data (Banko et al., 2007b), or rely on seed examples for ontology induction (dog is a mammal) and attribute extraction (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang a</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007a. Learning relations from the web. In Proceedings of the International Joint Conferences on Artificial Intelligence (IJCAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Joint Conferences on Artificial Intelligence (IJCAI).</booktitle>
<contexts>
<context position="1930" citStr="Banko et al., 2007" startWordPosition="289" endWordPosition="292">ibed in text. While the field has since focused on more shallow approaches, recent work on schema induction shows that event schemas might be learnable from raw text. This paper continues the trend, addressing the question, can event schemas be induced from raw text without prior knowledge? We present a new generative model for event schemas, and it produces state-of-the-art induction results, including a 7 F1 point gain over a different generative proposal developed in parallel with this work. Event schemas are unique from most work in information extraction (IE). Current relation discovery (Banko et al., 2007a; Carlson et al., 2010b) focuses on atomic facts and relations. Event schemas build relations into coherent event structures, often called templates in IE. For instance, an election template jointly connects that obama won a presidential election with romney was the defeated, the election occurred in 2012, and the popular vote was 50-48. The entities in these relations fill specific semantic roles, as in this template schema: Template Schema for Elections (events: nominate, vote, elect, win, declare, concede) Date: Timestamp Winner: Person Loser: Person Position: Occupation Vote: Number Tradi</context>
<context position="4975" citStr="Banko et al., 2007" startWordPosition="760" endWordPosition="763">sting contrast. Our entity model outperforms by 7 F1 points on a common extraction task. The rest of the paper describes in detail our main contributions: (1) the first entity-based generative model for schema induction, (2) a direct pipeline/formal model comparison, (3) results improving state-of-the-art performance by 20%, and (4) schema induction from the smallest amount of training data to date. 2 Previous Work Unsupervised learning for information extraction usually learns binary relations and atomic facts. Models can learn relations like Person is married to Person without labeled data (Banko et al., 2007b), or rely on seed examples for ontology induction (dog is a mammal) and attribute extraction (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang a</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007b. Open information extraction from the web. In Proceedings of the International Joint Conferences on Artificial Intelligence (IJCAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>Collective information extraction with relational markov networks.</title>
<date>2004</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>438--445</pages>
<contexts>
<context position="5537" citStr="Bunescu and Mooney, 2004" startWordPosition="851" endWordPosition="854"> is married to Person without labeled data (Banko et al., 2007b), or rely on seed examples for ontology induction (dog is a mammal) and attribute extraction (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot </context>
</contexts>
<marker>Bunescu, Mooney, 2004</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2004. Collective information extraction with relational markov networks. In Proceedings of the Association for Computational Linguistics (ACL), pages 438–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>J Betteridge</author>
<author>B Kisiel</author>
<author>B Settles</author>
<author>E R Hruschka Jr</author>
<author>T M Mitchell</author>
</authors>
<title>Toward an architecture for never-ending language learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="1953" citStr="Carlson et al., 2010" startWordPosition="293" endWordPosition="296">he field has since focused on more shallow approaches, recent work on schema induction shows that event schemas might be learnable from raw text. This paper continues the trend, addressing the question, can event schemas be induced from raw text without prior knowledge? We present a new generative model for event schemas, and it produces state-of-the-art induction results, including a 7 F1 point gain over a different generative proposal developed in parallel with this work. Event schemas are unique from most work in information extraction (IE). Current relation discovery (Banko et al., 2007a; Carlson et al., 2010b) focuses on atomic facts and relations. Event schemas build relations into coherent event structures, often called templates in IE. For instance, an election template jointly connects that obama won a presidential election with romney was the defeated, the election occurred in 2012, and the popular vote was 50-48. The entities in these relations fill specific semantic roles, as in this template schema: Template Schema for Elections (events: nominate, vote, elect, win, declare, concede) Date: Timestamp Winner: Person Loser: Person Position: Occupation Vote: Number Traditionally, template extr</context>
<context position="5109" citStr="Carlson et al., 2010" startWordPosition="782" endWordPosition="785">ur main contributions: (1) the first entity-based generative model for schema induction, (2) a direct pipeline/formal model comparison, (3) results improving state-of-the-art performance by 20%, and (4) schema induction from the smallest amount of training data to date. 2 Previous Work Unsupervised learning for information extraction usually learns binary relations and atomic facts. Models can learn relations like Person is married to Person without labeled data (Banko et al., 2007b), or rely on seed examples for ontology induction (dog is a mammal) and attribute extraction (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supe</context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, J. Betteridge, B. Kisiel, B. Settles, E.R. Hruschka Jr., and T.M. Mitchell. 2010a. Toward an architecture for never-ending language learning. In Proceedings of the Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>J Betteridge</author>
<author>R C Wang</author>
<author>E R Hruschka Jr</author>
<author>T M Mitchell</author>
</authors>
<title>Coupled semisupervised learning for information extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM).</booktitle>
<contexts>
<context position="1953" citStr="Carlson et al., 2010" startWordPosition="293" endWordPosition="296">he field has since focused on more shallow approaches, recent work on schema induction shows that event schemas might be learnable from raw text. This paper continues the trend, addressing the question, can event schemas be induced from raw text without prior knowledge? We present a new generative model for event schemas, and it produces state-of-the-art induction results, including a 7 F1 point gain over a different generative proposal developed in parallel with this work. Event schemas are unique from most work in information extraction (IE). Current relation discovery (Banko et al., 2007a; Carlson et al., 2010b) focuses on atomic facts and relations. Event schemas build relations into coherent event structures, often called templates in IE. For instance, an election template jointly connects that obama won a presidential election with romney was the defeated, the election occurred in 2012, and the popular vote was 50-48. The entities in these relations fill specific semantic roles, as in this template schema: Template Schema for Elections (events: nominate, vote, elect, win, declare, concede) Date: Timestamp Winner: Person Loser: Person Position: Occupation Vote: Number Traditionally, template extr</context>
<context position="5109" citStr="Carlson et al., 2010" startWordPosition="782" endWordPosition="785">ur main contributions: (1) the first entity-based generative model for schema induction, (2) a direct pipeline/formal model comparison, (3) results improving state-of-the-art performance by 20%, and (4) schema induction from the smallest amount of training data to date. 2 Previous Work Unsupervised learning for information extraction usually learns binary relations and atomic facts. Models can learn relations like Person is married to Person without labeled data (Banko et al., 2007b), or rely on seed examples for ontology induction (dog is a mammal) and attribute extraction (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supe</context>
</contexts>
<marker>Carlson, Betteridge, Wang, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, J. Betteridge, R.C. Wang, E.R. Hruschka Jr., and T.M. Mitchell. 2010b. Coupled semisupervised learning for information extraction. In Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Templatebased information extraction without the templates.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3089" citStr="Chambers and Jurafsky, 2011" startWordPosition="468" endWordPosition="472"> Winner: Person Loser: Person Position: Occupation Vote: Number Traditionally, template extractors assume foreknowledge of the event schemas. They know a Winner exists, and research focuses on supervised learning to extract winners from text. This paper focuses on the other side of the supervision spectrum. The learner receives no human input, and it first induces a schema before extracting instances of it. Our proposed model contributes to a growing line of research in schema induction. The majority of previous work relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). Chambers and Jurafsky is a pipelined approach, learning events first, and later learning syntactic patterns as fillers. It requires 1797 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics several ad-hoc metrics and parameters, and it lacks the benefits of a formal model. However, central to their algorithm is the use of coreferring entity mentions to knit events and entities together into an event schema. We adapt this entity-driven approach to </context>
<context position="7097" citStr="Chambers and Jurafsky (2011)" startWordPosition="1091" endWordPosition="1095">res many documents about the exact same event instance, and relations are binary (not schemas) over repeated named entities. Our model instead learns schemas from documents with mixed topics that don’t describe the same event, so repeated proper nouns are less helpful. Chen et al. (2011) perform relation extraction with no supervision on earthquake and finance domains. Theirs is a generative model that represents relations as predicate/argument pairs. As with others, training data is pre-clustered by event type and there is no schema connection between relations. This paper builds the most on Chambers and Jurafsky (2011). They learned event schemas with a three-stage clustering algorithm that included a requirement to retrieve extra training data. This paper removes many of these complexities. We present a formal model that uniquely models coreference chains. Advantages include a joint clustering of events and entities, and a formal probabilistic interpretation of the resulting schemas. We achieve better performance, and do so with far less training data. Cheung et al. (2013) is most related as a generative formulation of schema induction. They propose an HMM-based model over latent event variables, where eac</context>
<context position="10380" citStr="Chambers and Jurafsky (2011)" startWordPosition="1605" endWordPosition="1608">late schemas are inter-mixed and entities can play multiple roles across instances. The training corpus contains 1300 documents, 733 of which are labeled with at least one schema. 567 documents are not labeled with any schemas. These unlabeled documents are articles that report on non-specific political events and speeches. They make the corpus particularly challenging. The development and test sets each contain 200 documents. 4 A Generative Model for Event Schemas This paper’s model is an entity-based approach, similar in motivation to Haghighi and Klein (2010) and the pipelined induction of Chambers and Jurafsky (2011). Coreference resolution guides the learning by providing a set of pre-resolved entities. Each entity receives a schema role label, so it allows all mentions of the entity to inform that role choice. This important constraint links coreferring mentions to the same schema role, and distinguishes our approach from others (Cheung et al., 2013). 4.1 Illustration The model represents a document as a set of entities. An entity is a set of entity mentions clustered by coreference resolution. We will use the following two sentences for illustration: A truck bomb exploded near the embassy. Three militi</context>
<context position="18748" citStr="Chambers and Jurafsky, 2011" startWordPosition="3061" endWordPosition="3064"> if m = subject and vice versa. Otherwise, the probability is computed as normal. We normalize the distributions after penalties are computed. 4.5 Entity Extraction for Template Filling Inducing event schemas is only one benefit of the model. The learned model can also extract specific instances of the learned schemas without adR Y s m 1801 ditional complexity. To evaluate the effectiveness of the model, we apply the model to perform standard template extraction on MUC-4. Previous MUC-4 induction required an extraction algorithm separate from induction because induction created hard clusters (Chambers and Jurafsky, 2011). Cluster scores don’t have a natural interpretation, so extraction required several parameters/thresholds to tune. Our model instead simply relies on model inference. We run inference as described above and each entity receives a template label te,d and a template slot label se,d. These labels are the extractions, and it requires no other parameters. The model thus requires far less machinery than a pipeline, and the experiments below further show that this simpler model outperforms the pipeline. Beyond parameters, the question of “irrelevant” documents is a concern in MUC-4. Approximately ha</context>
<context position="20990" citStr="Chambers and Jurafsky (2011)" startWordPosition="3413" endWordPosition="3416">e of execution’ and ‘effect of incident’). The four slots and example entity fillers are shown here: Perpetrator: Shining Path members Victim: Sergio Horna Target: public facilities Instrument: explosives We also focus only on these four slots. We merged MUC’s two perpetrator slots (individuals and orgs) into one gold Perpetrator. Previous work has both split the two and merged the two. We merge them because the distinction between an individual and an organization is often subtle and not practically important to analysts. This is also consistent with the most recent event schema induction in Chambers and Jurafsky (2011) and Cheung et al. (2013). One peculiarity in MUC-4 is that some templates are labeled as optional (i.e., all its slots are optional), and some required templates contain optional slots (i.e., a subset of slots are optional). We ignore both optional templates and specific optional slots when computing recall, as in previous work (Patwardhan and Riloff, 2007; Patwardhan and Riloff, 2009; Chambers and Jurafsky, 2011). Comparison between the extracted strings and the gold template strings uses head word scoring. We do not use gold parses for the text, so head words are defined simply as the right</context>
<context position="22567" citStr="Chambers and Jurafsky (2011)" startWordPosition="3683" endWordPosition="3686">ion. Which learned slots correspond to MUC-4 slots? There are two methods of mapping. The first ignores the schema type variables t, and simply finds the best performing s variable for each gold template slot2. We call this the slot-only mapping evaluation. The second approach is to map each template variable t to the best gold template type g, and limit the slot mapping so that only the slots under t can map to slots under g. We call this the template mapping evaluation. The slot-only mapping can result in higher scores since it is not constrained to preserve schema structure in the mapping. Chambers and Jurafsky (2011) used template mapping in their evaluation. Cheung et al. (2013) used slot-only mapping. We run both evaluations in this paper and separately compare both. 1Personal communications with Patwardhan and Riloff 2bombing-victim is a template slot distinct from kidnapvictim. Both need to be mapped. 1802 6 Experiments We use the Stanford CoreNLP toolkit for text processing and parsing. We developed the models on the 1300 document MUC-4 training set. We then learned once on the entire 1700 training/dev/test set, and report extraction numbers from the inferred labels on the 200 document test set. Each</context>
<context position="31532" citStr="Chambers and Jurafsky (2011)" startWordPosition="5138" endWordPosition="5141">be explained by the development set being more similar to the test set than training. We did not investigate further to prevent overexperimentation on test. 7 Discussion Our model is one of the first generative formulations of schema induction. It produces state-of-theart performance on a traditional extraction task, and performs with less training data as well as a more complex pipelined approach. Further, our unique entity-driven approach outperforms an HMM-based model developed in parallel to this work. Our entity-driven proposal is strongly influenced by the ideas in the pipeline model of Chambers and Jurafsky (2011). Coreference chains have been used in a variety of learning tasks, such as narrative learning and summarization. Here we are the first to show how it can be used for schema induction in a probabilistic model, connecting predicates across a document in a way that is otherwise difficult to represent. The models perform similarly, but ours also includes significant benefits like a reduction in complexity, reproducibility, and a large reduction in training data requirements. This paper also implies that learning and extraction need not be independent algorithms. Our model’s inference procedure to</context>
</contexts>
<marker>Chambers, Jurafsky, 2011</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2011. Templatebased information extraction without the templates. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harr Chen</author>
<author>Edward Benson</author>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
</authors>
<title>In-domain relation discovery with meta-constraints via posterior regularization.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="6088" citStr="Chen et al., 2011" startWordPosition="937" endWordPosition="940">ifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber et al., 2000; Surdeanu et al., 2006). Shinyama and Sekine (2006) presented unrestricted relation discovery to discover relations in unlabeled documents. Their algorithm used redundant documents (e.g., all describe Hurricane Ivan) to observe repeated proper nouns. The approach requires many documents about the exact same event instance, and relations are binary (not schemas) over repeated named entities. Our model instead learns schemas from documents with mixed topics that don’t describe the same</context>
</contexts>
<marker>Chen, Benson, Naseem, Barzilay, 2011</marker>
<rawString>Harr Chen, Edward Benson, Tahira Naseem, and Regina Barzilay. 2011. In-domain relation discovery with meta-constraints via posterior regularization. In Proceedings of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackie Chi Kit Cheung</author>
<author>Hoifung Poon</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Probabilistic frame induction.</title>
<date>2013</date>
<booktitle>In Proceedings of the North American Chapter of the Association</booktitle>
<contexts>
<context position="3944" citStr="Cheung et al., 2013" startWordPosition="598" endWordPosition="601">97–1807, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics several ad-hoc metrics and parameters, and it lacks the benefits of a formal model. However, central to their algorithm is the use of coreferring entity mentions to knit events and entities together into an event schema. We adapt this entity-driven approach to a single model that requires fewer parameters and far less training data. Further, experiments show stateof-the-art performance. Other research conducted at the time of this paper also proposes a generative model for schema induction (Cheung et al., 2013). Theirs is not entitybased, but instead uses a sequence model (HMMbased) of verb clauses. These two papers thus provide a unique opportunity to compare two very different views of document structure. One is entitydriven, modeling an entity’s role by its coreference chain. The other is clause-driven, classifying individual clauses based on text sequence. Each model makes unique assumptions, providing an interesting contrast. Our entity model outperforms by 7 F1 points on a common extraction task. The rest of the paper describes in detail our main contributions: (1) the first entity-based gener</context>
<context position="7561" citStr="Cheung et al. (2013)" startWordPosition="1163" endWordPosition="1166">training data is pre-clustered by event type and there is no schema connection between relations. This paper builds the most on Chambers and Jurafsky (2011). They learned event schemas with a three-stage clustering algorithm that included a requirement to retrieve extra training data. This paper removes many of these complexities. We present a formal model that uniquely models coreference chains. Advantages include a joint clustering of events and entities, and a formal probabilistic interpretation of the resulting schemas. We achieve better performance, and do so with far less training data. Cheung et al. (2013) is most related as a generative formulation of schema induction. They propose an HMM-based model over latent event variables, where each variable generates the observed clauses. Latent schema variables generate the event variables (in the spirit of preliminary work by O’Connor (2012)). There is no notion of an entity, so learning uses text mentions and relies on the local HMM win1798 message: id dev-muc3-0112 (bellcore, mitre) incident: date 10 mar 89 incident: location peru: huanuco, ambo (town) incident: type bombing incident: stage accomplished incident: instrument explosive: ”-” perp: ind</context>
<context position="10722" citStr="Cheung et al., 2013" startWordPosition="1659" endWordPosition="1662">ticularly challenging. The development and test sets each contain 200 documents. 4 A Generative Model for Event Schemas This paper’s model is an entity-based approach, similar in motivation to Haghighi and Klein (2010) and the pipelined induction of Chambers and Jurafsky (2011). Coreference resolution guides the learning by providing a set of pre-resolved entities. Each entity receives a schema role label, so it allows all mentions of the entity to inform that role choice. This important constraint links coreferring mentions to the same schema role, and distinguishes our approach from others (Cheung et al., 2013). 4.1 Illustration The model represents a document as a set of entities. An entity is a set of entity mentions clustered by coreference resolution. We will use the following two sentences for illustration: A truck bomb exploded near the embassy. Three militia planted it, and then they fled. This text contains five entity mentions. A perfect coreference resolution system will resolve these five mentions into three entities: Entity Mentions Entities Roles a truck bomb (a truck bomb, it) Instrument the embassy (the embassy) Target three militia (three militia, they) Perpetrator it they The schema</context>
<context position="19788" citStr="Cheung et al., 2013" startWordPosition="3228" endWordPosition="3231">e experiments below further show that this simpler model outperforms the pipeline. Beyond parameters, the question of “irrelevant” documents is a concern in MUC-4. Approximately half the corpus are documents that are not labeled with a template, so past algorithms required extra processing stages to filter out these irrelevant documents. Patwardhan and Riloff (2009) and Chambers and Jurafsky (2011) make initial decisions as to whether they should extract or not from a document. Huang and Riloff (2011) use a genre detector for this problem. Even the generative HMM-based model of Cheung et al. (Cheung et al., 2013) requires an extra filtering parameter. Our formal model is unique in not requiring additional effort. Ours is the only approach that doesn’t require document filtering. 5 Evaluation Setup Evaluating on MUC-4 has a diverse history that complicates comparison. The following balances comparison against previous work and enables future comparison to our results. 5.1 Template Schema Slots Most systems do not evaluate performance on all MUC-4 template slots. They instead focus on four main slots, ignoring the parameterized slots that involve deeper reasoning (such as ‘stage of execution’ and ‘effec</context>
<context position="21015" citStr="Cheung et al. (2013)" startWordPosition="3418" endWordPosition="3421">cident’). The four slots and example entity fillers are shown here: Perpetrator: Shining Path members Victim: Sergio Horna Target: public facilities Instrument: explosives We also focus only on these four slots. We merged MUC’s two perpetrator slots (individuals and orgs) into one gold Perpetrator. Previous work has both split the two and merged the two. We merge them because the distinction between an individual and an organization is often subtle and not practically important to analysts. This is also consistent with the most recent event schema induction in Chambers and Jurafsky (2011) and Cheung et al. (2013). One peculiarity in MUC-4 is that some templates are labeled as optional (i.e., all its slots are optional), and some required templates contain optional slots (i.e., a subset of slots are optional). We ignore both optional templates and specific optional slots when computing recall, as in previous work (Patwardhan and Riloff, 2007; Patwardhan and Riloff, 2009; Chambers and Jurafsky, 2011). Comparison between the extracted strings and the gold template strings uses head word scoring. We do not use gold parses for the text, so head words are defined simply as the rightmost word in the noun phr</context>
<context position="22631" citStr="Cheung et al. (2013)" startWordPosition="3693" endWordPosition="3696">s of mapping. The first ignores the schema type variables t, and simply finds the best performing s variable for each gold template slot2. We call this the slot-only mapping evaluation. The second approach is to map each template variable t to the best gold template type g, and limit the slot mapping so that only the slots under t can map to slots under g. We call this the template mapping evaluation. The slot-only mapping can result in higher scores since it is not constrained to preserve schema structure in the mapping. Chambers and Jurafsky (2011) used template mapping in their evaluation. Cheung et al. (2013) used slot-only mapping. We run both evaluations in this paper and separately compare both. 1Personal communications with Patwardhan and Riloff 2bombing-victim is a template slot distinct from kidnapvictim. Both need to be mapped. 1802 6 Experiments We use the Stanford CoreNLP toolkit for text processing and parsing. We developed the models on the 1300 document MUC-4 training set. We then learned once on the entire 1700 training/dev/test set, and report extraction numbers from the inferred labels on the 200 document test set. Each experiment was repeated 10 times. Reported numbers are averaged</context>
<context position="27974" citStr="Cheung et al. 2013" startWordPosition="4552" endWordPosition="4555"> person object-kill building object-destroy bomb subject-explode guerrilla object-wound office object-damage explosion subject-occur soldier subject-die explosive object-use attack object-cause man subject-blow up station and-office charge object-place civilian subject-try vehicle prep of-number device subject-destroy Figure 5: Select distributions for two learned events. Left columns are head word distributions 0, right columns are syntactic relation distributions S, and entity types in parentheses are the learned y. Most probable words are shown. Evaluation: Slot-Only Mapping Prec Recall F1 Cheung et al. 2013 .32 .37 .34 Flat Relation Model .26 .45 .33 Formal Template Model .41 .41 .41 Table 2: MUC-4 extraction with slot-only mapping. Any learned slot is allowed to map to any gold slot. Entity Role Performance Prec Recall F1 Perpetrator .40 .20 .26 Victim .42 .31 .34 Target .38 .28 .31 Instrument .57 .39 .45 Table 3: Results for each MUC-4 template slot using the template-mapping evaluation. The macro-level F1 scores can be broken down into individual slot performance. Table 3 shows these results ranging from .26 to .45. The Instrument role proves easiest to learn, consistent with C&amp;J. A large por</context>
<context position="32259" citStr="Cheung et al. (2013)" startWordPosition="5256" endWordPosition="5259">ation. Here we are the first to show how it can be used for schema induction in a probabilistic model, connecting predicates across a document in a way that is otherwise difficult to represent. The models perform similarly, but ours also includes significant benefits like a reduction in complexity, reproducibility, and a large reduction in training data requirements. This paper also implies that learning and extraction need not be independent algorithms. Our model’s inference procedure to learn schemas is the same one that labels text for extraction. C&amp;J required 3-4 separate pipelined steps. Cheung et al. (2013) required specific cutoffs for document classification before extraction. Not only does our model perform well, but it does so without these steps. Highlighted here are key differences between this proposal and the HMM-based model of Cheung et al. (2013). One of the HMM strengths is the inclusion of sequence-based knowledge. Each slot label is influenced by the previous label in the text, encouraging syntactic arguments of a predicate to choose the same schema. This knowledge is only loosely present in our document distribution 0. Cheung et al. also include a hidden event variable between the </context>
</contexts>
<marker>Cheung, Poon, Vanderwende, 2013</marker>
<rawString>Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Vanderwende. 2013. Probabilistic frame induction. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Hwee Tou Ng</author>
<author>Yoong Keok Lee</author>
</authors>
<title>Closing the gap: Learning-based information extraction rivaling knowledge-engineering methods.</title>
<date>2003</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5511" citStr="Chieu et al., 2003" startWordPosition="847" endWordPosition="850">elations like Person is married to Person without labeled data (Banko et al., 2007b), or rely on seed examples for ontology induction (dog is a mammal) and attribute extraction (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with se</context>
</contexts>
<marker>Chieu, Ng, Lee, 2003</marker>
<rawString>Hai Leong Chieu, Hwee Tou Ng, and Yoong Keok Lee. 2003. Closing the gap: Learning-based information extraction rivaling knowledge-engineering methods. In Proceedings of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
<author>David Lewis</author>
<author>Lynette Hirschman</author>
</authors>
<title>Evaluating message understanding systems: an analysis of the third message understanding conference. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="5423" citStr="Chinchor et al., 1993" startWordPosition="832" endWordPosition="835">information extraction usually learns binary relations and atomic facts. Models can learn relations like Person is married to Person without labeled data (Banko et al., 2007b), or rely on seed examples for ontology induction (dog is a mammal) and attribute extraction (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filat</context>
</contexts>
<marker>Chinchor, Lewis, Hirschman, 1993</marker>
<rawString>Nancy Chinchor, David Lewis, and Lynette Hirschman. 1993. Evaluating message understanding systems: an analysis of the third message understanding conference. Computational Linguistics, 19:3:409–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Marius Pasca</author>
</authors>
<title>Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 23rd Annual Conference on Artificial Intelligence (AAAI-2008),</booktitle>
<pages>1243--1248</pages>
<marker>Van Durme, Pasca, 2008</marker>
<rawString>Benjamin Van Durme and Marius Pasca. 2008. Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction. In Proceedings of the 23rd Annual Conference on Artificial Intelligence (AAAI-2008), pages 1243–1248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen McKeown</author>
</authors>
<title>Automatic creation of domain templates.</title>
<date>2006</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="3045" citStr="Filatova et al., 2006" startWordPosition="462" endWordPosition="465">in, declare, concede) Date: Timestamp Winner: Person Loser: Person Position: Occupation Vote: Number Traditionally, template extractors assume foreknowledge of the event schemas. They know a Winner exists, and research focuses on supervised learning to extract winners from text. This paper focuses on the other side of the supervision spectrum. The learner receives no human input, and it first induces a schema before extracting instances of it. Our proposed model contributes to a growing line of research in schema induction. The majority of previous work relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). Chambers and Jurafsky is a pipelined approach, learning events first, and later learning syntactic patterns as fillers. It requires 1797 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics several ad-hoc metrics and parameters, and it lacks the benefits of a formal model. However, central to their algorithm is the use of coreferring entity mentions to knit events and entities together into an event sche</context>
<context position="6039" citStr="Filatova et al., 2006" startWordPosition="929" endWordPosition="932"> 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber et al., 2000; Surdeanu et al., 2006). Shinyama and Sekine (2006) presented unrestricted relation discovery to discover relations in unlabeled documents. Their algorithm used redundant documents (e.g., all describe Hurricane Ivan) to observe repeated proper nouns. The approach requires many documents about the exact same event instance, and relations are binary (not schemas) over repeated named entities. Our model instead learns schemas from documen</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, McKeown, 2006</marker>
<rawString>Elena Filatova, Vasileios Hatzivassiloglou, and Kathleen McKeown. 2006. Automatic creation of domain templates. In Proceedings of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
</authors>
<title>Toward general-purpose learning for information extraction.</title>
<date>1998</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>404--408</pages>
<contexts>
<context position="5491" citStr="Freitag, 1998" startWordPosition="845" endWordPosition="846">els can learn relations like Person is married to Person without labeled data (Banko et al., 2007b), or rely on seed examples for ontology induction (dog is a mammal) and attribute extraction (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). B</context>
</contexts>
<marker>Freitag, 1998</marker>
<rawString>Dayne Freitag. 1998. Toward general-purpose learning for information extraction. In Proceedings of the Association for Computational Linguistics (ACL), pages 404–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Academy of Sciences of the United States of America,</booktitle>
<pages>5228--5235</pages>
<contexts>
<context position="17103" citStr="Griffiths and Steyvers, 2004" startWordPosition="2790" endWordPosition="2793">ntity features -ys. For each entity in a document, a hidden slot s ∈ 5 is first drawn from O, and then the observed entity (h, M, F) is drawn according to the multinomials (0s, -ys, Ss). We later evaluate this flat model to show the benefit of added schema structure. 4.4 Inference We use collapsed Gibbs sampling for inference, sampling the latent variables te,d and se,d in seFigure 4: Simplified plate diagrams comparing the flat relation model to the full template model. The observed f ∈ F variables are not included for clarity. quence conditioned on a full setting of all the other variables (Griffiths and Steyvers, 2004). Initial parameter values are set by randomly setting t and s variables from the uniform distribution over schema types and slots, then computing the other parameter values based on these initial settings. The hyperparameters for the dirichlet distributions were chosen from a small grid search (see Experiments). Beyond standard inference, we added one constraint to the model that favors grammatical distributions Ss that do not contain conflicts. The subject and direct object of a verb should not both receive high probability mass under the same schema slot Ss. For instance, the victim of a ki</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. L. Griffiths and M. Steyvers. 2004. Finding scientific topics. In Proceedings of the National Academy of Sciences of the United States of America, pages 5228– 5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>An entity-level approach to information extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="10320" citStr="Haghighi and Klein (2010)" startWordPosition="1596" endWordPosition="1599">ions. The corpus is particularly challenging because template schemas are inter-mixed and entities can play multiple roles across instances. The training corpus contains 1300 documents, 733 of which are labeled with at least one schema. 567 documents are not labeled with any schemas. These unlabeled documents are articles that report on non-specific political events and speeches. They make the corpus particularly challenging. The development and test sets each contain 200 documents. 4 A Generative Model for Event Schemas This paper’s model is an entity-based approach, similar in motivation to Haghighi and Klein (2010) and the pipelined induction of Chambers and Jurafsky (2011). Coreference resolution guides the learning by providing a set of pre-resolved entities. Each entity receives a schema role label, so it allows all mentions of the entity to inform that role choice. This important constraint links coreferring mentions to the same schema role, and distinguishes our approach from others (Cheung et al., 2013). 4.1 Illustration The model represents a document as a set of entities. An entity is a set of entity mentions clustered by coreference resolution. We will use the following two sentences for illust</context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Aria Haghighi and Dan Klein. 2010. An entity-level approach to information extraction. In Proceedings of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruihong Huang</author>
<author>Ellen Riloff</author>
</authors>
<title>Inducing domain-specific semantic class taggers from (almost) nothing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5157" citStr="Huang and Riloff, 2010" startWordPosition="790" endWordPosition="793">sed generative model for schema induction, (2) a direct pipeline/formal model comparison, (3) results improving state-of-the-art performance by 20%, and (4) schema induction from the smallest amount of training data to date. 2 Previous Work Unsupervised learning for information extraction usually learns binary relations and atomic facts. Models can learn relations like Person is married to Person without labeled data (Banko et al., 2007b), or rely on seed examples for ontology induction (dog is a mammal) and attribute extraction (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for lab</context>
</contexts>
<marker>Huang, Riloff, 2010</marker>
<rawString>Ruihong Huang and Ellen Riloff. 2010. Inducing domain-specific semantic class taggers from (almost) nothing. In Proceedings of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruihong Huang</author>
<author>Ellen Riloff</author>
</authors>
<title>Peeling back the layers: Detecting event role fillers in secondary contexts.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5591" citStr="Huang and Riloff, 2011" startWordPosition="859" endWordPosition="862">., 2007b), or rely on seed examples for ontology induction (dog is a mammal) and attribute extraction (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber et a</context>
<context position="19674" citStr="Huang and Riloff (2011)" startWordPosition="3208" endWordPosition="3211"> extractions, and it requires no other parameters. The model thus requires far less machinery than a pipeline, and the experiments below further show that this simpler model outperforms the pipeline. Beyond parameters, the question of “irrelevant” documents is a concern in MUC-4. Approximately half the corpus are documents that are not labeled with a template, so past algorithms required extra processing stages to filter out these irrelevant documents. Patwardhan and Riloff (2009) and Chambers and Jurafsky (2011) make initial decisions as to whether they should extract or not from a document. Huang and Riloff (2011) use a genre detector for this problem. Even the generative HMM-based model of Cheung et al. (Cheung et al., 2013) requires an extra filtering parameter. Our formal model is unique in not requiring additional effort. Ours is the only approach that doesn’t require document filtering. 5 Evaluation Setup Evaluating on MUC-4 has a diverse history that complicates comparison. The following balances comparison against previous work and enables future comparison to our results. 5.1 Template Schema Slots Most systems do not evaluate performance on all MUC-4 template slots. They instead focus on four m</context>
<context position="33610" citStr="Huang and Riloff, 2011" startWordPosition="5471" endWordPosition="5474">uires further investigation as it is unclear if it provides valuable information, or too much complexity. We also note a warning for future work on proper evaluation methodology. This task is particularly difficult to compare to other models due to its combination of both induction and then extraction. There are many ways to map induced schemas to gold answers, and this paper illustrates how extraction performance is significantly affected by the choice. We suggest the template-mapping evaluation to preserve learned structure. Finally, these induced results are far behind supervised learning (Huang and Riloff, 2011). There is ample room for improvement and future research in event schema induction. Acknowledgments This work was partially supported by a grant from the Office of Naval Research. It was also supported, in part, by the Johns Hopkins Human Language Technology Center of Excellence. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author. Thanks to Eric Wang for his insights into Bayesian modeling, Brendan O’Connor for his efforts on normalizing MUC-4 evaluation details, Frank Ferraro and Benjamin Van Durme for helpful conversations, and to t</context>
</contexts>
<marker>Huang, Riloff, 2011</marker>
<rawString>Ruihong Huang and Ellen Riloff. 2011. Peeling back the layers: Detecting event role fillers in secondary contexts. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mstislav Maslennikov</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Automatic acquisition of domain knowledge for information extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5696" citStr="Maslennikov and Chua, 2007" startWordPosition="874" endWordPosition="877"> (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber et al., 2000; Surdeanu et al., 2006). Shinyama and Sekine (2006) presented unrestricted relation discovery to</context>
</contexts>
<marker>Maslennikov, Chua, 2007</marker>
<rawString>Mstislav Maslennikov and Tat-Seng Chua. 2007. Automatic acquisition of domain knowledge for information extraction. In Proceedings of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
</authors>
<title>Learning frames from text with an unsupervised latent variable model.</title>
<date>2012</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University.</institution>
<marker>O’Connor, 2012</marker>
<rawString>Brendan O’Connor. 2012. Learning frames from text with an unsupervised latent variable model. Technical report, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ellen Riloff</author>
</authors>
<title>Effective ie with semantic affinity patterns and relevant regions.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="6068" citStr="Patwardhan and Riloff, 2007" startWordPosition="933" endWordPosition="936">) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber et al., 2000; Surdeanu et al., 2006). Shinyama and Sekine (2006) presented unrestricted relation discovery to discover relations in unlabeled documents. Their algorithm used redundant documents (e.g., all describe Hurricane Ivan) to observe repeated proper nouns. The approach requires many documents about the exact same event instance, and relations are binary (not schemas) over repeated named entities. Our model instead learns schemas from documents with mixed topics that don</context>
<context position="21349" citStr="Patwardhan and Riloff, 2007" startWordPosition="3470" endWordPosition="3474">t the two and merged the two. We merge them because the distinction between an individual and an organization is often subtle and not practically important to analysts. This is also consistent with the most recent event schema induction in Chambers and Jurafsky (2011) and Cheung et al. (2013). One peculiarity in MUC-4 is that some templates are labeled as optional (i.e., all its slots are optional), and some required templates contain optional slots (i.e., a subset of slots are optional). We ignore both optional templates and specific optional slots when computing recall, as in previous work (Patwardhan and Riloff, 2007; Patwardhan and Riloff, 2009; Chambers and Jurafsky, 2011). Comparison between the extracted strings and the gold template strings uses head word scoring. We do not use gold parses for the text, so head words are defined simply as the rightmost word in the noun phrase. The exception is when the extracted phrase is of the form “A of B”, then the rightmost word in “A” is used as the head. This is again consistent with previous work1. The standard evaluation metrics are precision, recall, and F1 score. 5.2 Mapping Learned Slots Induced schemas need to map to gold schemas before evaluation. Which</context>
</contexts>
<marker>Patwardhan, Riloff, 2007</marker>
<rawString>Siddharth Patwardhan and Ellen Riloff. 2007. Effective ie with semantic affinity patterns and relevant regions. In Proceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ellen Riloff</author>
</authors>
<title>A unified model of phrasal and sentential evidence for information extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="5566" citStr="Patwardhan and Riloff, 2009" startWordPosition="855" endWordPosition="858">out labeled data (Banko et al., 2007b), or rely on seed examples for ontology induction (dog is a mammal) and attribute extraction (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be </context>
<context position="19536" citStr="Patwardhan and Riloff (2009)" startWordPosition="3184" endWordPosition="3187">erence. We run inference as described above and each entity receives a template label te,d and a template slot label se,d. These labels are the extractions, and it requires no other parameters. The model thus requires far less machinery than a pipeline, and the experiments below further show that this simpler model outperforms the pipeline. Beyond parameters, the question of “irrelevant” documents is a concern in MUC-4. Approximately half the corpus are documents that are not labeled with a template, so past algorithms required extra processing stages to filter out these irrelevant documents. Patwardhan and Riloff (2009) and Chambers and Jurafsky (2011) make initial decisions as to whether they should extract or not from a document. Huang and Riloff (2011) use a genre detector for this problem. Even the generative HMM-based model of Cheung et al. (Cheung et al., 2013) requires an extra filtering parameter. Our formal model is unique in not requiring additional effort. Ours is the only approach that doesn’t require document filtering. 5 Evaluation Setup Evaluating on MUC-4 has a diverse history that complicates comparison. The following balances comparison against previous work and enables future comparison to</context>
<context position="21378" citStr="Patwardhan and Riloff, 2009" startWordPosition="3475" endWordPosition="3478"> We merge them because the distinction between an individual and an organization is often subtle and not practically important to analysts. This is also consistent with the most recent event schema induction in Chambers and Jurafsky (2011) and Cheung et al. (2013). One peculiarity in MUC-4 is that some templates are labeled as optional (i.e., all its slots are optional), and some required templates contain optional slots (i.e., a subset of slots are optional). We ignore both optional templates and specific optional slots when computing recall, as in previous work (Patwardhan and Riloff, 2007; Patwardhan and Riloff, 2009; Chambers and Jurafsky, 2011). Comparison between the extracted strings and the gold template strings uses head word scoring. We do not use gold parses for the text, so head words are defined simply as the rightmost word in the noun phrase. The exception is when the extracted phrase is of the form “A of B”, then the rightmost word in “A” is used as the head. This is again consistent with previous work1. The standard evaluation metrics are precision, recall, and F1 score. 5.2 Mapping Learned Slots Induced schemas need to map to gold schemas before evaluation. Which learned slots correspond to </context>
</contexts>
<marker>Patwardhan, Riloff, 2009</marker>
<rawString>Siddharth Patwardhan and Ellen Riloff. 2009. A unified model of phrasal and sentential evidence for information extraction. In Proceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa Rau</author>
<author>George Krupka</author>
<author>Paul Jacobs</author>
<author>Ira Sider</author>
<author>Lois Childs</author>
</authors>
<title>Ge nltoolset: Muc-4 test results and analysis.</title>
<date>1992</date>
<booktitle>In Proceedings of the Message Understanding Conference (MUC-4),</booktitle>
<pages>94--99</pages>
<contexts>
<context position="5442" citStr="Rau et al., 1992" startWordPosition="836" endWordPosition="839">usually learns binary relations and atomic facts. Models can learn relations like Person is married to Person without labeled data (Banko et al., 2007b), or rely on seed examples for ontology induction (dog is a mammal) and attribute extraction (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; P</context>
</contexts>
<marker>Rau, Krupka, Jacobs, Sider, Childs, 1992</marker>
<rawString>Lisa Rau, George Krupka, Paul Jacobs, Ira Sider, and Lois Childs. 1992. Ge nltoolset: Muc-4 test results and analysis. In Proceedings of the Message Understanding Conference (MUC-4), pages 94–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Mark Schmelzenbach</author>
</authors>
<title>An empirical approach to conceptual case frame acquisition.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="5976" citStr="Riloff and Schmelzenbach, 1998" startWordPosition="917" endWordPosition="920">mas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber et al., 2000; Surdeanu et al., 2006). Shinyama and Sekine (2006) presented unrestricted relation discovery to discover relations in unlabeled documents. Their algorithm used redundant documents (e.g., all describe Hurricane Ivan) to observe repeated proper nouns. The approach requires many documents about the exact same event instance, and relations are binary (not schemas) over repeate</context>
</contexts>
<marker>Riloff, Schmelzenbach, 1998</marker>
<rawString>Ellen Riloff and Mark Schmelzenbach. 1998. An empirical approach to conceptual case frame acquisition. In Proceedings of the Sixth Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
<author>William Phillips</author>
</authors>
<title>Exploiting subjectivity classification to improve information extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of AAAI-05.</booktitle>
<contexts>
<context position="6016" citStr="Riloff et al., 2005" startWordPosition="925" endWordPosition="928">hes (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber et al., 2000; Surdeanu et al., 2006). Shinyama and Sekine (2006) presented unrestricted relation discovery to discover relations in unlabeled documents. Their algorithm used redundant documents (e.g., all describe Hurricane Ivan) to observe repeated proper nouns. The approach requires many documents about the exact same event instance, and relations are binary (not schemas) over repeated named entities. Our model instead lear</context>
</contexts>
<marker>Riloff, Wiebe, Phillips, 2005</marker>
<rawString>Ellen Riloff, Janyce Wiebe, and William Phillips. 2005. Exploiting subjectivity classification to improve information extraction. In Proceedings of AAAI-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>On-demand information extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics,</booktitle>
<pages>731--738</pages>
<contexts>
<context position="3059" citStr="Sekine, 2006" startWordPosition="466" endWordPosition="467">ate: Timestamp Winner: Person Loser: Person Position: Occupation Vote: Number Traditionally, template extractors assume foreknowledge of the event schemas. They know a Winner exists, and research focuses on supervised learning to extract winners from text. This paper focuses on the other side of the supervision spectrum. The learner receives no human input, and it first induces a schema before extracting instances of it. Our proposed model contributes to a growing line of research in schema induction. The majority of previous work relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). Chambers and Jurafsky is a pipelined approach, learning events first, and later learning syntactic patterns as fillers. It requires 1797 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics several ad-hoc metrics and parameters, and it lacks the benefits of a formal model. However, central to their algorithm is the use of coreferring entity mentions to knit events and entities together into an event schema. We adapt t</context>
<context position="6251" citStr="Sekine (2006)" startWordPosition="965" endWordPosition="966">rounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber et al., 2000; Surdeanu et al., 2006). Shinyama and Sekine (2006) presented unrestricted relation discovery to discover relations in unlabeled documents. Their algorithm used redundant documents (e.g., all describe Hurricane Ivan) to observe repeated proper nouns. The approach requires many documents about the exact same event instance, and relations are binary (not schemas) over repeated named entities. Our model instead learns schemas from documents with mixed topics that don’t describe the same event, so repeated proper nouns are less helpful. Chen et al. (2011) perform relation extraction with no supervision on earthquake and finance domains. Theirs is </context>
</contexts>
<marker>Sekine, 2006</marker>
<rawString>Satoshi Sekine. 2006. On-demand information extraction. In Proceedings of the Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics, pages 731–738.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Preemptive ie using unrestricted relation discovery.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="6251" citStr="Shinyama and Sekine (2006)" startWordPosition="963" endWordPosition="966">examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber et al., 2000; Surdeanu et al., 2006). Shinyama and Sekine (2006) presented unrestricted relation discovery to discover relations in unlabeled documents. Their algorithm used redundant documents (e.g., all describe Hurricane Ivan) to observe repeated proper nouns. The approach requires many documents about the exact same event instance, and relations are binary (not schemas) over repeated named entities. Our model instead learns schemas from documents with mixed topics that don’t describe the same event, so repeated proper nouns are less helpful. Chen et al. (2011) perform relation extraction with no supervision on earthquake and finance domains. Theirs is </context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive ie using unrestricted relation discovery. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoshi Sudo</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>An improved extraction pattern representation model for automatic ie pattern acquisition.</title>
<date>2003</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>224--231</pages>
<contexts>
<context position="5995" citStr="Sudo et al., 2003" startWordPosition="921" endWordPosition="924"> rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber et al., 2000; Surdeanu et al., 2006). Shinyama and Sekine (2006) presented unrestricted relation discovery to discover relations in unlabeled documents. Their algorithm used redundant documents (e.g., all describe Hurricane Ivan) to observe repeated proper nouns. The approach requires many documents about the exact same event instance, and relations are binary (not schemas) over repeated named entities. O</context>
</contexts>
<marker>Sudo, Sekine, Grishman, 2003</marker>
<rawString>Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman. 2003. An improved extraction pattern representation model for automatic ie pattern acquisition. In Proceedings of the Association for Computational Linguistics (ACL), pages 224–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth M Sundheim</author>
</authors>
<title>Third message understanding evaluation and conference (muc-3): Phase 1 status report.</title>
<date>1991</date>
<booktitle>In Proceedings of the Message Understanding Conference.</booktitle>
<contexts>
<context position="8846" citStr="Sundheim, 1991" startWordPosition="1365" endWordPosition="1367">e 1: A subset of the slots in a MUC-4 template. dow for event transitions. Their model was created in parallel with our work, and provides a nice contrast in both approach and results. Ours outperforms their model by 20% on a MUC-4 evaluation. In summary, this paper extends most previous work on event schema induction by removing the supervision. Of the recent ‘unsupervised’ work, we present the first entity-driven generative model, and we experiment on a mixed-domain corpus. 3 Dataset: The MUC-4 Corpus The corpus from the Message Understanding Conference (MUC-4) serves as the challenge text (Sundheim, 1991), and will ground discussion of our model. MUC-4 is also used by the closest previous work. It contains Latin American newswire about terrorism events, and it provides a set of hand-constructed event schemas that are traditionally called template schemas. It also maps labeled templates to the text, providing a dataset for template extraction evaluations. Until very recently, only extraction has been evaluated. We too evaluate our model through extraction, but we also compare our learned schemas to the hand-created template schemas. An example of a filled in MUC-4 template is given in Figure 1.</context>
</contexts>
<marker>Sundheim, 1991</marker>
<rawString>Beth M. Sundheim. 1991. Third message understanding evaluation and conference (muc-3): Phase 1 status report. In Proceedings of the Message Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Jordi Turmo</author>
<author>Alicia Ageno</author>
</authors>
<title>A hybrid approach for the acquisition of information extraction patterns.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL Workshop on Adaptive Text Extraction and Mining.</booktitle>
<contexts>
<context position="6223" citStr="Surdeanu et al., 2006" startWordPosition="959" endWordPosition="962">ers rely on the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber et al., 2000; Surdeanu et al., 2006). Shinyama and Sekine (2006) presented unrestricted relation discovery to discover relations in unlabeled documents. Their algorithm used redundant documents (e.g., all describe Hurricane Ivan) to observe repeated proper nouns. The approach requires many documents about the exact same event instance, and relations are binary (not schemas) over repeated named entities. Our model instead learns schemas from documents with mixed topics that don’t describe the same event, so repeated proper nouns are less helpful. Chen et al. (2011) perform relation extraction with no supervision on earthquake and</context>
</contexts>
<marker>Surdeanu, Turmo, Ageno, 2006</marker>
<rawString>Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2006. A hybrid approach for the acquisition of information extraction patterns. In Proceedings of the EACL Workshop on Adaptive Text Extraction and Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
<author>Ralph Grishman</author>
<author>Pasi Tapanainen</author>
<author>Silja Huttunen</author>
</authors>
<title>Automatic acquisition of domain knowledge for information extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>940--946</pages>
<contexts>
<context position="6199" citStr="Yangarber et al., 2000" startWordPosition="955" endWordPosition="958"> Riloff, 2011). Classifiers rely on the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber et al., 2000; Surdeanu et al., 2006). Shinyama and Sekine (2006) presented unrestricted relation discovery to discover relations in unlabeled documents. Their algorithm used redundant documents (e.g., all describe Hurricane Ivan) to observe repeated proper nouns. The approach requires many documents about the exact same event instance, and relations are binary (not schemas) over repeated named entities. Our model instead learns schemas from documents with mixed topics that don’t describe the same event, so repeated proper nouns are less helpful. Chen et al. (2011) perform relation extraction with no super</context>
</contexts>
<marker>Yangarber, Grishman, Tapanainen, Huttunen, 2000</marker>
<rawString>Roman Yangarber, Ralph Grishman, Pasi Tapanainen, and Silja Huttunen. 2000. Automatic acquisition of domain knowledge for information extraction. In Proceedings of the 18th International Conference on Computational Linguistics (COLING), pages 940– 946.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
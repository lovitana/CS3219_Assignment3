<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.987458">
Easy Victories and Uphill Battles in Coreference Resolution
</title>
<author confidence="0.998741">
Greg Durrett and Dan Klein
</author>
<affiliation confidence="0.99876">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.999385">
{gdurrett,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.996677" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999756192307692">
Classical coreference systems encode various
syntactic, discourse, and semantic phenomena
explicitly, using heterogenous features com-
puted from hand-crafted heuristics. In con-
trast, we present a state-of-the-art coreference
system that captures such phenomena implic-
itly, with a small number of homogeneous
feature templates examining shallow proper-
ties of mentions. Surprisingly, our features
are actually more effective than the corre-
sponding hand-engineered ones at modeling
these key linguistic phenomena, allowing us
to win “easy victories” without crafted heuris-
tics. These features are successful on syntax
and discourse; however, they do not model
semantic compatibility well, nor do we see
gains from experiments with shallow seman-
tic features from the literature, suggesting that
this approach to semantics is an “uphill bat-
tle.” Nonetheless, our final system1 outper-
forms the Stanford system (Lee et al. (2011),
the winner of the CoNLL 2011 shared task)
by 3.5% absolute on the CoNLL metric and
outperforms the IMS system (Bj¨orkelund and
Farkas (2012), the best publicly available En-
glish coreference system) by 1.9% absolute.
</bodyText>
<sectionHeader confidence="0.999162" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999739">
Coreference resolution is a multi-faceted task: hu-
mans resolve references by exploiting contextual
and grammatical clues, as well as semantic infor-
mation and world knowledge, so capturing each of
</bodyText>
<footnote confidence="0.977973">
1The Berkeley Coreference Resolution System is available
athttp://nlp.cs.berkeley.edu.
</footnote>
<bodyText confidence="0.999798764705882">
these will be necessary for an automatic system to
fully solve the problem. Acknowledging this com-
plexity, coreference systems, either learning-based
(Bengtson and Roth, 2008; Stoyanov et al., 2010;
Haghighi and Klein, 2010; Rahman and Ng, 2011b)
or rule-based (Haghighi and Klein, 2009; Lee et
al., 2011), draw on diverse information sources and
complex heuristics to resolve pronouns, model dis-
course, determine anaphoricity, and identify seman-
tically compatible mentions. However, this leads to
systems with many heterogenous parts that can be
difficult to interpret or modify.
We build a learning-based, mention-synchronous
coreference system that aims to use the simplest pos-
sible set of features to tackle the various aspects
of coreference resolution. Though they arise from
a small number of simple templates, our features
are numerous, which works to our advantage: we
can both implicitly model important linguistic ef-
fects and capture other patterns in the data that are
not easily teased out by hand. As a result, our data-
driven, homogeneous feature set is able to achieve
high performance despite only using surface-level
document characteristics and shallow syntactic in-
formation. We win “easy victories” without design-
ing features and heuristics explicitly targeting par-
ticular phenomena.
Though our approach is successful at modeling
syntax, we find semantics to be a much more chal-
lenging aspect of coreference. Our base system
uses only two recall-oriented features on nominal
and proper mentions: head match and exact string
match. Building on these features, we critically eval-
uate several classes of semantic features which intu-
</bodyText>
<page confidence="0.949928">
1971
</page>
<note confidence="0.731466">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1971–1982,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.99988052631579">
itively should prove useful but have had mixed re-
sults in the literature, and we observe that they are
ineffective for our system. However, these features
are beneficial when gold mentions are provided to
our system, leading us to conclude that the large
number of system mentions extracted by most coref-
erence systems (Lee et al., 2011; Fernandes et al.,
2012) means that weak indicators cannot overcome
the bias against making coreference links. Capturing
semantic information in this shallow way is an “up-
hill battle” due to this structural property of corefer-
ence resolution.
Nevertheless, using a simple architecture and fea-
ture set, our final system outperforms the two best
publicly available English coreference systems, the
Stanford system (Lee et al., 2011) and the IMS sys-
tem (Bj¨orkelund and Farkas, 2012), by wide mar-
gins: 3.5% absolute and 1.9% absolute, respectively,
on the CoNLL metric.
</bodyText>
<sectionHeader confidence="0.997666" genericHeader="introduction">
2 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999746272727273">
Throughout this work, we use the datasets from the
CoNLL 2011 shared task2 (Pradhan et al., 2011),
which is derived from the OntoNotes corpus (Hovy
et al., 2006). When applicable, we use the standard
automatic parses and NER tags for each document.
All experiments use system mentions except where
otherwise indicated. For each experiment, we report
MUC (Vilain et al., 1995), B3 (Bagga and Baldwin,
1998), and CEAFe (Luo, 2005), as well as their av-
erage, the CoNLL metric. All metrics are computed
using version 5 of the official CoNLL scorer.3
</bodyText>
<sectionHeader confidence="0.995595" genericHeader="method">
3 A Mention-Synchronous Framework
</sectionHeader>
<bodyText confidence="0.996855142857143">
We first present the basic architecture of our corefer-
ence system, independent of a feature set. Unlike bi-
nary classification-based coreference systems where
independent binary decisions are made about each
pair (Soon et al., 2001; Bengtson and Roth, 2008;
Versley et al., 2008; Stoyanov et al., 2010), we use a
log-linear model to select at most one antecedent for
</bodyText>
<footnote confidence="0.988195">
2This dataset is identical to the English portion of the
CoNLL 2012 data, except for the absence of a small pivot text.
3Note that this version of the scorer implements a modified
version of B3, described in Cai and Strube (2010), that was used
for the CoNLL shared tasks. The implementation of CEAF,
is also not exactly as described in Luo et al. (2004), but for
completeness we include this metric as well.
</footnote>
<bodyText confidence="0.999693083333333">
each mention or determine that it begins a new clus-
ter (Denis and Baldridge, 2008). In this mention-
ranking or mention-synchronous framework, fea-
tures examine single mentions to evaluate whether
or not they are anaphoric and pairs of mentions to
evaluate whether or not they corefer. While other
work has used this framework as a starting point
for entity-level systems (Luo et al., 2004; Rahman
and Ng, 2009; Haghighi and Klein, 2010; Durrett et
al., 2013), we will show that a mention-synchronous
approach is sufficient to get state-of-the-art perfor-
mance on its own.
</bodyText>
<subsectionHeader confidence="0.999694">
3.1 Mention Detection
</subsectionHeader>
<bodyText confidence="0.99998985">
Our system first identifies a set of predicted men-
tions from text annotated with parses and named en-
tity tags. We extract three distinct types of mentions:
proper mentions from all named entity chunks ex-
cept for those labeled as QUANTITY, CARDINAL, or
PERCENT, pronominal mentions from single words
tagged with PRP or PRP$, and nominal mentions
from all other maximal NP projections. These basic
rules are similar to those of Lee et al. (2011), except
that their system uses an additional set of filtering
rules designed to discard instances of pleonastic it,
partitives, certain quantified noun phrases, and other
spurious mentions. In contrast to this highly engi-
neered approach and to systems which use a trained
classifier to compute anaphoricity separately (Rah-
man and Ng, 2009; Bj¨orkelund and Farkas, 2012),
we aim for the highest possible recall of gold men-
tions with a low-complexity method, leaving us with
a large number of spurious system mentions that we
will have to reject later.
</bodyText>
<subsectionHeader confidence="0.996246">
3.2 Coreference Model
</subsectionHeader>
<bodyText confidence="0.99969475">
Figure 1 shows the mention-ranking architecture
that serves as the backbone of our coreference sys-
tem. Assume we have extracted n mentions from
a document x, where x denotes the surface proper-
ties of a document and any precomputed informa-
tion. The ith mention in a document has an asso-
ciated random variable ai taking values in the set
11, ... , i−1, NEW}; this variable specifies mention
i’s selected antecedent or indicates that it begins a
new coreference chain. A setting of the ai, denoted
by a = (a1, ..., an), implies a unique set of corefer-
ence chains C that serve as our system output.
</bodyText>
<page confidence="0.988735">
1972
</page>
<figure confidence="0.998275928571428">
1+—
2+—
3+—
NEW
a4
Wrong Link
False New
Correct
Correct
1+— False Anaphor
1+— Correct 2+— False Anaphor
NEW Correct NEW False New NEW Correct
a1 a2 a3
[Voters]1 agree when [they]1 are given a [chance]2 to decide if [they]1 ...
</figure>
<figureCaption confidence="0.99780375">
Figure 1: The basic structure of our coreference model. The ith mention in a document has i possible antecedence
choices: link to one of the i − 1 preceding mentions or begin a new cluster. We place a distribution over these choices
with a log-linear model. Structurally different kinds of errors are weighted differently to optimize for final coreference
loss functions; error types are shown corresponding to the decisions for each mention.
</figureCaption>
<bodyText confidence="0.895634">
We use a log linear model of the conditional dis-
tribution P(a|x) as follows:
</bodyText>
<equation confidence="0.67256">
!wTf(i, ai, x)
</equation>
<bodyText confidence="0.999841071428571">
where f(i, ai, x) is a feature function that examines
the coreference decision ai for mention i with doc-
ument context x. When ai = NEW, the features
fired indicate the suitability of the given mention to
be anaphoric or not; when ai = j for some j, the
features express aspects of the pairwise linkage, and
can examine any relevant attributes of the anaphor
i or the antecedent j, since information about each
mention is contained in x.
Inference in this model is efficient: because
log P(a|x) decomposes linearly over mentions, we
can compute ai = arg maxaz P(ai|x) separately
for each mention and return the set of coreference
chains implied by these decisions.
</bodyText>
<subsectionHeader confidence="0.999708">
3.3 Learning
</subsectionHeader>
<bodyText confidence="0.99644132">
During learning, we optimize for conditional log-
likelihood augmented with a parameterized loss
function (Durrett et al., 2013). The main compli-
cating factor in this process is that the supervision
in coreference consists of a gold clustering C* de-
fined over gold mentions. This is problematic for
two reasons: first, because the clustering is defined
over gold mentions rather than our system mentions,
and second, because a clustering does not specify a
full antecedent structure of the sort our model pro-
duces. We can address the first of these problems
by imputing singleton clusters for mentions that do
not appear in the gold standard; our system will then
simply learn to put spurious mentions in their own
clusters. Singletons are always removed before eval-
uation because the OntoNotes corpus does not anno-
tate them, so in this way we can neatly dispose of
spurious mentions. To address the lack of explicit
antecedents in C*, we simply sum over all possible
antecedent structures licensed by the gold clusters.
Formally, we will maximize the conditional log-
likelihood of the set A(C*) of antecedent vectors
a for a document that are consistent with the gold
annotation.4 Consistency for an antecedent choice
ai under gold clusters C* is defined as follows:
</bodyText>
<listItem confidence="0.997128">
1. If ai = j, ai is consistent iff mentions i and j
are present in C* and are in the same cluster.
2. If ai = NEW, ai is consistent off mention i is
not present in C*, or it is present in C* and has
no gold antecedents, or it is present in C* and
none of its gold antecedents are among the set
of system predicted mentions.
</listItem>
<bodyText confidence="0.9715185">
Given t training examples of the form (xk, C*k),
we write the following likelihood function:
</bodyText>
<equation confidence="0.993244">
log X P&apos;(a|xk) + AJJwJJ1
(aEA(Ck*)
</equation>
<bodyText confidence="0.724867">
where P&apos;(a|xk) a P(a|xk) exp(l(a, C*k)) with
l(a, C*) being a real-valued loss function. The loss
4Because of this marginalization over latent antecedent
choices, our objective is non-convex.
</bodyText>
<equation confidence="0.99571075">
P(a|x) a exp Xn
i=1
e(w) = Xt
k=1
</equation>
<page confidence="0.84216">
1973
</page>
<bodyText confidence="0.99972725">
here plays an analogous role to the loss in struc-
tured max-margin objectives; incorporating it into a
conditional likelihood objective is a technique called
softmax-margin (Gimpel and Smith, 2010).
Our loss function l(a, C*) is a weighted linear
combination of three error types, examples of which
are shown in Figure 1. A false anaphor (FA) error
occurs when az is chosen to be anaphoric when it
should start a new cluster. A false new (FN) error oc-
curs in the opposite case, when az wrongly indicates
a new cluster when it should be anaphoric. Finally,
a wrong link (WL) error occurs when the antecedent
chosen for az is the wrong antecedent (but az is in-
deed anaphoric). Our final parameterized loss func-
tion is a weighted sum of the counts of these three
error types:
</bodyText>
<equation confidence="0.754719">
l(a, C*) = αFAFA(a, C*) + αFNFN(a, C*) + αWLWL(a, C*)
</equation>
<bodyText confidence="0.999782444444444">
where FA(a, C*) gives the number of false anaphor
errors in prediction a with gold chains C* (FN and
WL are analogous). By setting αFA low and αFN
high relative to αWL, we can counterbalance the
high number of singleton mentions and bias the sys-
tem towards making more coreference linkages. We
set (αFA, αFN, αWL) = (0.1, 3.0, 1.0) and A =
0.001 and optimize the objective using AdaGrad
(Duchi et al., 2011).
</bodyText>
<sectionHeader confidence="0.942485" genericHeader="method">
4 Easy Victories from Surface Features
</sectionHeader>
<bodyText confidence="0.999986285714286">
Our primary goal with this work is to show that a
high-performance coreference system is attainable
with a small number of feature templates that use
only surface-level information sources. These fea-
tures will be general-purpose and capture linguistic
effects to the point where standard heuristic-driven
features are no longer needed in our system.
</bodyText>
<subsectionHeader confidence="0.999734">
4.1 SURFACE Features and Conjunctions
</subsectionHeader>
<bodyText confidence="0.998545">
Our SURFACE feature set only considers the follow-
ing properties of mentions and mention pairs:
</bodyText>
<listItem confidence="0.99948525">
• Mention type (nominal, proper, or pronominal)
• The complete string of a mention
• The semantic head of a mention
• The first word and last word of each mention
</listItem>
<table confidence="0.99849235">
Feature name Count
Features on the current mention
[ANAPHORIC] + [HEAD WORD] 41371
[ANAPHORIC] + [FIRST WORD] 18991
[ANAPHORIC] + [LAST WORD] 19184
[ANAPHORIC] + [PRECEDING WORD] 54605
[ANAPHORIC] + [FOLLOWING WORD] 57239
[ANAPHORIC] + [LENGTH] 4304
Features on the antecedent
[ANTECEDENT HEAD WORD] 57383
[ANTECEDENT FIRST WORD] 24239
[ANTECEDENT LAST WORD] 23819
[ANTECEDENT PRECEDING WORD] 53421
[ANTECEDENT FOLLOWING WORD] 55718
[ANTECEDENT LENGTH] 4620
Features on the pair
[EXACT STRING MATCH (T/F)] 47
[HEAD MATCH (T/F)] 46
[SENTENCE DISTANCE, CAPPED AT 10] 2037
[MENTION DISTANCE, CAPPED AT 10] 1680
</table>
<tableCaption confidence="0.7777025">
Table 1: Our SURFACE feature set, which exploits a
small number of surface-level mention properties. Fea-
</tableCaption>
<bodyText confidence="0.9495828">
ture counts for each template are computed over the train-
ing set, and include features generated by our conjunction
scheme (not explicitly shown in the table; see Figure 2),
which yields large numbers of features at varying levels
of expressivity.
</bodyText>
<listItem confidence="0.9582104">
• The word immediately preceding and the word
immediately following a mention
• Mention length, in words
• Two distance measures between mentions
(number of sentences and number of mentions)
</listItem>
<bodyText confidence="0.9996085">
Table 1 shows the SURFACE feature set. Features
that look only at the current mention fire on all de-
cisions (az = j or az = NEW), whereas features
that look at the antecedent in any way (the latter
two groups of features) only fire on pairwise link-
ages (az =� NEW).
Two conjunctions of each feature are also in-
cluded: first with the “type” of the mention be-
ing resolved (either NOMINAL, PROPER, or, if it is
pronominal, the citation form of the pronoun), and
then additionally with the antecedent type (only if
the feature is over a pairwise link). This conjunc-
tion process is shown in Figure 2. Note that features
that just examine the antecedent will end up with
</bodyText>
<page confidence="0.97015">
1974
</page>
<figure confidence="0.965527">
a2
[Voters]1 generally agree when [they]1 ...
</figure>
<figureCaption confidence="0.946706666666667">
Figure 2: Demonstration of the conjunction scheme we
use. Each feature on anaphoricity is conjoined with the
type (NOMINAL, PROPER, or the citation form if it is a
pronoun) of the mention being resolved. Each feature on
a mention pair is additionally conjoined with the types of
the current and antecedent mentions.
</figureCaption>
<bodyText confidence="0.999981380952381">
conjunctions that examine properties of the current
mention as well, as shown with the ANT. HEAD fea-
ture in the figure.
Finally, we found it beneficial for our lexical indi-
cator features to only fire on words occurring at least
20 times in the training set; for rare words, we use
the part of speech of the word instead.
The performance of our system is shown in Ta-
ble 2. We contrast our performance with that of
the Stanford system (Lee et al. (2011), the winner
of the CoNLL 2011 shared task) and the IMS sys-
tem (Bj¨orkelund and Farkas (2012), the best publicly
available English coreference system). Despite its
simplicity, our SURFACE system is sufficient to out-
perform these sophisticated systems: the Stanford
system uses a cascade of ten rule-based sieves each
of which has customized heuristics, and the IMS
system uses a similarly long pipeline consisting of
a learned referentiality classifier followed by multi-
ple resolvers, which are run in sequence and rely on
the outputs of the previous resolvers as features.
</bodyText>
<subsectionHeader confidence="0.988699">
4.2 Data-Driven versus Heuristic-Driven
Features
</subsectionHeader>
<bodyText confidence="0.9998796">
Why are the SURFACE features sufficient to give
high coreference performance, when they do not
make apparent reference to important linguistic phe-
nomena? The main reason is that they actually do
capture the same phenomena as standard corefer-
</bodyText>
<table confidence="0.99715725">
MUC B3 CEAFe Avg.
STANFORD 60.46 65.48 47.07 57.67
IMS 62.15 65.57 46.66 58.13
SURFACE 64.39 66.78 49.00 60.06
</table>
<tableCaption confidence="0.94732325">
Table 2: Results for our SURFACE system, the STAN-
FORD system, and the IMS system on the CoNLL 2011
development set. Complete results are shown in Ta-
ble 7. Despite using limited information sources, our sys-
tem is able to substantially outperform the other two, the
two best publicly-available English coreference systems.
Bolded values are significant with p &lt; 0.05 according to
a bootstrap resampling test.
</tableCaption>
<bodyText confidence="0.998063454545455">
ence features, just implicitly. For example, rather
than having rules targeting person, number, gender,
or animacy of mentions, we use conjunctions with
pronoun identity, which contains this information.
Rather than explicitly writing a feature targeting def-
initeness, our indicators on the first word of a men-
tion will capture this and other effects. And finally,
rather than targeting centering theory (Grosz et al.,
1995) with rule-based features identifying syntac-
tic positions (Stoyanov et al., 2010; Haghighi and
Klein, 2010), our features on word context can iden-
tify configurational clues like whether a mention
is preceded or followed by a verb, and therefore
whether it is likely in subject or object position.5
Not only are data-driven features able to capture
the same phenomena as heuristic-driven features,
but they do so at a finer level of granularity, and can
therefore model more patterns in the data. To con-
trast these two types of features, we experiment with
three ablated versions of our system, where we re-
place data-driven features with their heuristic-driven
counterparts:
</bodyText>
<listItem confidence="0.971478333333333">
1. Instead of using an indicator on the first word
of a mention (1STWORD), we instead fire
a feature based on that mention’s manually-
computed definiteness (DEF).
2. Instead of conjoining features on pronominal-
pronominal linkages with the citation form of
</listItem>
<bodyText confidence="0.9879325">
5Heuristic-driven approaches were historically more appro-
priate, since past coreference corpora such as MUC and ACE
were smaller and therefore more prone to overfitting feature-
rich models. However, the OntoNotes dataset contains thou-
sands of documents, so having support for features is less of a
concern.
</bodyText>
<table confidence="0.98937655">
ANT. HEAD = Voters
ANT. HEAD = Voters ∧ [they]
ANT. HEAD = Voters ∧ [they] ∧ NOM
MENT DIST = 1
MENT DIST = 1 ∧ [they]
MENT DIST = 1 ∧ [they] ∧ NOM
1⟵
NEW
NEW ∧ LEN = 1
NEW ∧ LEN = 1 ∧ [they]
1975
MUC B3 CEAFe Avg.
SURFACE 64.39 66.78 49.00 60.06
−1STWORD 63.32 66.22 47.89 59.14
+DEF−1STWORD 63.79 66.46 48.35 59.53
−PRONCONJ 59.97 63.46 47.94 57.12
+AGR−PRONCONJ 63.54 66.10 48.72 59.45
−CONTEXT 60.88 64.66 47.60 57.71
+POSN−CONTEXT 62.45 65.44 48.08 58.65
+DEF+AGR+POSN 64.55 66.93 48.94 60.14
</table>
<tableCaption confidence="0.997419">
Table 3: CoNLL metric scores on the development set,
</tableCaption>
<figureCaption confidence="0.464403125">
for the three different ablations and replacement features
described in Section 4.2. Feature types are described in
the text; + indicates inclusion of that feature class, − in-
dicates exclusion. Each individual shallow indicator ap-
pears to do as well at capturing its target phenomenon as
the hand-engineered features, while capturing other infor-
mation as well. Moreover, the hand-engineered features
give no benefit over the SURFACE system.
</figureCaption>
<bodyText confidence="0.907179038461538">
each pronoun (PRONCONJ), we only conjoin
with a PRONOUN indicator and add features
targeting the person, number, gender, and an-
imacy of the two pronouns (AGR).
3. Instead of using our context features on the
preceding and following word (CONTEXT), we
use manual determinations of when mentions
are in subject, direct object, indirect objection,
or oblique position (POSN).
All rules for computing person, number, gender, an-
imacy, definiteness, and syntactic position are taken
from the system of Lee et al. (2011).
Table 3 shows each of the target ablations, as well
as the SURFACE system with the DEF, AGR, and
POSN features added. While the heuristic-driven
feature always help over the corresponding ablated
system, they cannot do the work of the fine-grained
data-driven features. Most tellingly, though, none of
the heuristic-driven features give statistically signifi-
cant improvements on top of the data-driven features
we have already included, indicating that we are at
the point of diminishing returns on modeling those
specific phenomena. While this does not preclude
further engineering to take better advantage of other
syntactic constraints, our simple features represent
an “easy victory” on this subtask.
</bodyText>
<sectionHeader confidence="0.931806" genericHeader="method">
5 Uphill Battles on Semantics
</sectionHeader>
<bodyText confidence="0.999812692307692">
In Section 4, we gave a simple set of features that
yielded a high-performance coreference system; this
high performance is possible because features tar-
geting only superficial properties in a fine-grained
way can actually model complex linguistic con-
straints. However, while our existing features cap-
ture syntactic and discourse-level phenomena sur-
prisingly well, they are not effective at capturing se-
mantic phenomena like type compatibility. We will
show that due to structural aspects of the coreference
resolution problem, even a combination of several
shallow semantic features from the literature fails to
adequately model semantics.
</bodyText>
<subsectionHeader confidence="0.996472">
5.1 Analysis of the SURFACE System
</subsectionHeader>
<bodyText confidence="0.9999628">
What can the SURFACE system resolve correctly,
and what errors does it still make? To answer this
question, we will split mentions into several cate-
gories based on their observable properties and the
gold standard coreference information, and exam-
ine our system’s accuracy on each mention subclass
in order to more thoroughly characterize its perfor-
mance.6 These categories represent important dis-
tinctions in terms of the difficulty of mention reso-
lution for our system.
We first split mentions into three categories by
their status in the gold standard: singleton (unanno-
tated in the OntoNotes corpus), starting a new entity
with at least two mentions, or anaphoric. It is impor-
tant to note that while singletons and mentions start-
ing new entities are outwardly similar in that they
have no antecedents, and the prediction should be
the same in either case (NEW), we treat them as dis-
tinct because the factors that impact the coreference
decision differ in the two cases. Mentions that start
new clusters are semantically similar to anaphoric
mentions, but may be marked by heaviness or by a
tendency to be named entities, whereas singletons
may be generic or temporal NPs which might be
thought of as coreferent in a loose sense, but are not
</bodyText>
<footnote confidence="0.570102">
6This method of analysis is similar to that undertaken in
Stoyanov et al. (2009) and Rahman and Ng (2011b), though
we split our mentions along different axes, and can simply eval-
uate on accuracy because our decisions do not directly imply
multiple links, as they do in binary classification-based systems
(Stoyanov et al., 2009) or in entity-mention models (Rahman
and Ng, 2011b).
</footnote>
<page confidence="0.976266">
1976
</page>
<table confidence="0.9997794">
Nominal/Proper Pronominal
1st w/head 2nd+ w/head
Singleton 99.7% 18.1K 85.5% 7.3K 66.5% 1.7K
Starts Entity 98.7% 2.1K 78.9% 0.7K 48.5% 0.3K
Anaphoric 7.9% 0.9K 75.5% 3.9K 72.0% 4.4K
</table>
<tableCaption confidence="0.60934">
Table 4: Analysis of our SURFACE system on the de-
velopment set. We characterize each predicted mention
by its status in the gold standard (singleton, starting a
new entity, or anaphoric), its type (pronominal or nom-
inal/proper), and by whether its head has appeared as the
head of a previous mention. Each cell shows our sys-
tem’s accuracy on that mention class as well as the size
of the class. The biggest weakness of our system appears
to be its inability to resolve anaphoric mentions with new
heads (bottom-left cell).
</tableCaption>
<bodyText confidence="0.999609482758621">
included in the OntoNotes dataset due to choices in
the annotation standard.
Second, we divide mentions by their type,
pronominal versus nominal/proper; we then further
subdivide nominals and propers based on whether or
not the head word of the mention has appeared as the
head of a previous mention in the document.
Table 4 shows the results of our analysis. In
each cell, we show the fraction of mentions that
we correctly resolve (i.e., for which we make an
antecedence decision consistent with the gold stan-
dard), as well as the total number of mentions falling
into that cell. First, we observe that there are a sur-
prisingly large number of singleton mentions with
misleading head matches to previous mentions (of-
ten recurring temporal nouns phrases, like July).
The features in our system targeting anaphoricity are
useful for exactly this reason: the more bad head
matches we can rule out based on other criteria, the
more strongly we can rely on head match to make
correct linkages.
Our system is most noticeably poor at resolving
anaphoric mentions whose heads have not appeared
before. The fact that exact match and head match
are our only recall-oriented features on nominals
and propers is starkly apparent here: when we can-
not rely on head match, as is true for this mention
class, we only resolve 7.9% of anaphoric mentions
correctly.7 Many of the mentions in this category
</bodyText>
<footnote confidence="0.713223">
7There are an additional 346 anaphoric nominal/proper men-
tions in the 2nd+ category whose heads only appeared previ-
ously as part of a different cluster; we only resolve 1.7% of
</footnote>
<bodyText confidence="0.999298">
can only be correctly resolved by exploiting world
knowledge, so we will need to include features that
capture this knowledge in some fashion.
</bodyText>
<subsectionHeader confidence="0.997997">
5.2 Incorporating Shallow Semantics
</subsectionHeader>
<bodyText confidence="0.999866">
As we were able to incorporate syntax with shal-
low features, so too might we hope to incorporate
semantics. However, the semantic information con-
tained even in a coreference corpus of thousands
of documents is insufficient to generalize to unseen
data,8 so system designers have turned to exter-
nal resources such as semantic classes derived from
WordNet (Soon et al., 2001), WordNet hypernymy
or synonymy (Stoyanov et al., 2010), semantic simi-
larity computed from online resources (Ponzetto and
Strube, 2006), named entity type features, gender
and number match using the dataset of Bergsma and
Lin (2006), and features from unsupervised clus-
ters (Hendrickx and Daelemans, 2007; Durrett et al.,
2013). In this section, we consider the following
subset of these information sources:
</bodyText>
<listItem confidence="0.9227385">
• WordNet hypernymy and synonymy
• Number and gender data for nominals and
propers from Bergsma and Lin (2006)
• Named entity types
• Latent clusters computed from English Giga-
word (Graff et al., 2007), where a latent cluster
label generates each nominal head (excluding
pronouns) and a conjunction of its verbal gov-
</listItem>
<bodyText confidence="0.942997166666667">
ernor and semantic role, if any (Durrett et al.,
2013). We use twenty clusters, which include
clusters like president and leader (things which
announce).
Together, we call these the SEM features. We
show results from this expansion of the feature set in
Table 5. When using system mentions, the improve-
ments are not statistically significant on every met-
ric, and are quite marginal given that these features
add information that is intuitively central to corefer-
ence and otherwise unavailable to the system. We
explore the reasons behind this in the next section.
</bodyText>
<footnote confidence="0.86803575">
these extremely tricky cases correctly.
8We experimented with bilexical features on head pairs, but
they did not give statistically significant improvements over the
SURFACE features.
</footnote>
<page confidence="0.979319">
1977
</page>
<table confidence="0.9993222">
MUC B3 CEAFe Avg.
SURFACE 64.39 66.78 49.00 60.06
SURFACE+SEM 64.70 67.27 49.28 60.42
SURFACE (G) 82.80 74.10 68.33 75.08
SURFACE+SEM (G) 84.49 75.65 69.89 76.68
</table>
<tableCaption confidence="0.994426">
Table 5: CoNLL metric scores on the development set
</tableCaption>
<bodyText confidence="0.954513666666667">
for our SEM features when added on top of our SURFACE
features. We experiment on both system mentions and
gold mentions. Surprisingly, despite the fact that absolute
performance numbers are much higher on gold mentions
and there is less room for improvement, the semantic fea-
tures help much more than they do on system mentions.
</bodyText>
<subsectionHeader confidence="0.999894">
5.3 Analysis of Semantic Features
</subsectionHeader>
<bodyText confidence="0.970011277777778">
The main reason that weak semantic cues are not
more effective is the small fraction of positive coref-
erence links present in the training data. From Ta-
ble 4, the number of annotated coreferent spans in
the OntoNotes data is about a factor of five smaller
than the number of system mentions.9 This both
means that most NPs are not coreferent, and for
those that are, choosing the correct links is much
more difficult because of the large number of pos-
sible antecedents. Even head match, which is gen-
erally considered a high-precision indicator (Lee et
al., 2011), would introduce many spurious corefer-
ence arcs if applied too liberally (see Table 4).
In light of this fact, a system needs very strong
evidence to overcome the default hypothesis that a
mention is not coreferent, and a weak indicator will
have such a high “false positive” rate that it cannot
be relied on (given high weight, this feature would
do more harm than good, by introducing many false
linkages).
To confirm this intuition, we show in the bot-
tom part of Table 5 results when we apply these se-
mantic features on top of our SURFACE system on
gold mentions, where there are no singletons. In the
gold mention setting, we see that the semantic fea-
tures give a consistent improvement on every metric.
Moreover, if we look at a breakdown of errors, the
main improvement the semantic features give us is
on resolution of anaphoric nominals with no head
9This observation is more general than just our system: the
majority of coreference systems, including the winners of the
CoNLL shared tasks (Lee et al., 2011; Fernandes et al., 2012),
opt for high mention recall and resolve a relatively large number
of system mentions.
match: accuracy on the 1601 mentions that fall into
this category improves from 28.0% to 37.9%. On
predicted mentions, by contrast, this category only
improves from 7.9% to 12.2%, a much smaller ab-
solute improvement and one that comes at the ex-
pense of performance on most other resolution class.
The one class that does not get worse, singleton pro-
nouns, actually improves by a similar 4% margin,
indicating that roughly half of the gains we observe
are not even necessarily a result of our features do-
ing what they were designed to do.
Our weak cues do yield some small gains, so there
is hope that better weak indicators of semantic com-
patibility could prove more useful. However, while
extremely high-precision approaches with carefully
engineered features have been shown to be suc-
cessful (Rahman and Ng, 2011a; Bansal and Klein,
2012; Recasens et al., 2013a), we conclude that cap-
turing semantics in a data-driven, shallow manner
remains an uphill battle.
</bodyText>
<sectionHeader confidence="0.896534" genericHeader="method">
6 FINAL System and Results
</sectionHeader>
<bodyText confidence="0.999801916666667">
While semantic features ended up giving only
marginal benefit, we have demonstrated that nev-
ertheless our SURFACE system is a state-of-the-art
English coreference system. However, there remain
a few natural features that we omitted in order to
keep the system as simple as possible, since they
were orthogonal to the discussion of data-driven
versus heuristic-driven features and do not target
world knowledge. Before giving final results, we
will present a small set of additional features that
consider four additional mention properties beyond
those in Section 4.1:
</bodyText>
<listItem confidence="0.998323714285714">
• Whether two mentions are nested
• Ancestry of each mention head: the depen-
dency parent and grandparent POS tags and arc
directions (shown in Figure 3)
• The speaker of each mention
• Number and gender of each mention as deter-
mined by Bergsma and Lin (2006)
</listItem>
<bodyText confidence="0.999728">
The specific additional features we use are shown
in Table 6. Note that unlike in Section 5, we use
the number and gender information only on the an-
tecedent. Due to our conjunction scheme, both this
</bodyText>
<page confidence="0.966233">
1978
</page>
<figure confidence="0.9952158">
president R TO VBD
R
ROOT
... sent it to the [president] ... [President Obama] signed ...
VBD PRP TO DET NN NNP NNP VBD
</figure>
<figureCaption confidence="0.9969735">
Figure 3: Demonstration of the ancestry extraction pro-
cess. These features capture more sophisticated configu-
rational information than our context word features do: in
this example, president is in a characteristic indirect ob-
ject position based on its dependency parents, and Obama
is the subject of the main verb of the sentence.
</figureCaption>
<bodyText confidence="0.999931416666667">
semantic information and the speaker information
can apply in a fine-grained way to different pro-
nouns, and can therefore improve pronoun resolu-
tion substantially; however, these features generally
only improve pronoun resolution.
Full results for our SURFACE and FINAL feature
sets are shown in Table 7. Again, we compare to Lee
et al. (2011) and Bj¨orkelund and Farkas (2012).10
Despite our system’s emphasis on one-pass resolu-
tion with as simple a feature set as possible, we are
able to outperform even these sophisticated systems
by a wide margin.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.997888285714286">
Many of the individual features we employ in the FI-
NAL feature set have appeared in other coreference
systems (Bj¨orkelund and Nugues, 2011; Rahman
and Ng, 2011b; Fernandes et al., 2012). However,
other authors have often emphasized bilexical fea-
tures on head pairs, whereas our features are heavily
monolexical. For feature conjunctions, other authors
have exploited three classes (Lee et al., 2011) or au-
tomatically learned conjunction schemes (Fernandes
et al., 2012; Lassalle and Denis, 2013), but to our
knowledge we are the first to do fine-grained mod-
eling of every pronoun. Inclusion of a hierarchy of
10Discrepancies between scores here and those printed in
Pradhan et al. (2012) arise from two sources: improvements
to the system of Lee et al. (2011) since the first CoNLL shared
task, and a fix to the scoring of B3 in the official scorer since
results of the two CoNLL shared tasks were released. Unfor-
tunately, because of this bug in the scoring program, direct
comparison to the printed results of the other highest-scoring
English systems, Fernandes et al. (2012) and Martschat et al.
(2012), is impossible.
</bodyText>
<table confidence="0.971757142857143">
Feature name Count
Features of the SURFACE system 418704
Features on the current mention
[ANAPHORIC] + [CURRENT ANCESTRY] 46047
Features on the antecedent
[ANTECEDENT ANCESTRY] 53874
[ANTECEDENT GENDER] 338
[ANTECEDENT NUMBER] 290
Features on the pair
[HEAD CONTAINED (T/F)] 136
[EXACT STRING CONTAINED (T/F)] 133
[NESTED (T/F)] 355
[DOC TYPE] + [SAME SPEAKER (T/F)] 437
[CURRENT ANCESTRY] + [ANT. ANCESTRY] 2555359
</table>
<tableCaption confidence="0.995445">
Table 6: FINAL feature set; note that this includes the
</tableCaption>
<bodyText confidence="0.965378625">
SURFACE feature set. As with the features of the SUR-
FACE system, two conjoined variants of each feature
are included: first with the type of the current mention
(NOMINAL, PROPER, or the citation form of the pro-
noun), then with the types of both mentions in the pair.
These conjunctions allow antecedent features on gender
and number to impact pronoun resolution, and they al-
low speaker match to capture effects like I and you being
coreferent when the speakers differ.
features with regularization also means that we or-
ganically get distinctions among different mention
types without having to choose a level of granularity
a priori, unlike the distinct classifiers employed by
Denis and Baldridge (2008).
In terms of architecture, many coreference sys-
tems operate in a pipelined fashion, making par-
tial decisions about coreference or pruning arcs
before full resolution. Some systems use sepa-
rate rule-based and learning-based passes (Chen
and Ng, 2012; Fernandes et al., 2012), a series
of learning-based passes (Bj¨orkelund and Farkas,
2012), or referentiality classifiers that prune the set
of mentions before resolution (Rahman and Ng,
2009; Bj¨orkelund and Farkas, 2012; Recasens et
al., 2013b). By contrast, our system resolves all
mentions in one pass and does not need pruning:
the SURFACE system can train in less than two
hours without any subsampling of coreference arcs,
and rule-based pruning of coreference arcs actually
causes our system to perform less well, since our
features can learn valuable information from these
negative examples.
</bodyText>
<figure confidence="0.325055">
Obama L VBD ROOT
</figure>
<page confidence="0.67277">
1979
</page>
<table confidence="0.999646545454545">
Prec. MUC F1 Prec. B3 F1 Prec. CEAFe F1 Avg.
Rec. Rec. Rec. F1
CoNLL 2011 Development Set
STANFORD 61.62 59.34 60.46 74.05 58.70 65.48 45.98 48.22 47.07 57.67
IMS 66.67 58.20 62.15 77.60 56.77 65.57 42.92 51.11 46.66 58.13
SURFACE* 68.42 60.80 64.39 76.57 59.21 66.78 45.30 53.36 49.00 60.06
FINAL* 68.97 63.47 66.10 76.58 62.06 68.56 47.32 53.19 50.09 61.58
CoNLL 2011 Test Set
STANFORD 60.91 62.13 61.51 70.61 57.31 63.27 45.79 44.56 45.17 56.65
IMS 68.15 61.60 64.71 75.97 56.39 64.73 42.30 48.88 45.35 58.26
FINAL* 66.81 66.04 66.43 71.07 61.89 66.16 47.37 48.22 47.79 60.13
</table>
<tableCaption confidence="0.96593875">
Table 7: CoNLL metric scores for our systems on the CoNLL development and blind test sets, compared to the results
of Lee et al. (2011) (STANFORD) and Bj¨orkelund and Farkas (2012) (IMS). Starred systems are contributions of this
work. Bolded F1 values represent statistically significant improvements over other systems with p &lt; 0.05 using a
bootstrap resampling test. Metric values reflect version 5 of the CoNLL scorer.
</tableCaption>
<sectionHeader confidence="0.995368" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999926625">
We have presented a coreference system that uses a
simple, homogeneous set of features in a discrim-
inative learning framework to achieve high perfor-
mance. Large numbers of lexicalized, data-driven
features implicitly model linguistic phenomena such
as definiteness and centering, obviating the need for
heuristic-driven rules explicitly targeting these same
phenomena. Additional semantic features give only
slight benefit beyond head match because they do
not provide strong enough signals of coreference to
improve performance in the system mention setting;
modeling semantic similarity still requires complex
outside information and deep heuristics.
Our system, the Berkeley Coreference
Resolution System, is publicly available at
http://nlp.cs.berkeley.edu.
</bodyText>
<sectionHeader confidence="0.99698" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999949166666667">
This work was partially supported by BBN un-
der DARPA contract HR0011-12-C-0014 and by
an NSF fellowship for the first author. Thanks
to Sameer Pradhan for helpful discussions regard-
ing the CoNLL scoring program, and thanks to the
anonymous reviewers for their insightful comments.
</bodyText>
<sectionHeader confidence="0.99847" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995477282051282">
Amit Bagga and Breck Baldwin. 1998. Algorithms for
Scoring Coreference Chains. In Proceedings of the
Conference on Language Resources and Evaluation
Workshop on Linguistics Coreference.
Mohit Bansal and Dan Klein. 2012. Coreference Seman-
tics from Web Features. In Proceedings of the Associ-
ation for Computational Linguistics.
Eric Bengtson and Dan Roth. 2008. Understanding the
Value of Features for Coreference Resolution. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
Path-Based Pronoun Resolution. In Proceedings of the
Conference on Computational Linguistics and the As-
sociation for Computational Linguistics.
Anders Bj¨orkelund and Rich´ard Farkas. 2012. Data-
driven Multilingual Coreference Resolution using Re-
solver Stacking. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language Pro-
ceedings and Conference on Computational Natural
Language Learning - Shared Task.
Anders Bj¨orkelund and Pierre Nugues. 2011. Exploring
Lexicalized Features for Coreference Resolution. In
Proceedings of the Conference on Computational Nat-
ural Language Learning: Shared Task.
Jie Cai and Michael Strube. 2010. Evaluation Metrics for
End-to-End Coreference Resolution Systems. In Pro-
ceedings of the Special Interest Group on Discourse
and Dialogue.
Chen Chen and Vincent Ng. 2012. Combining the Best
of Two Worlds: A Hybrid Approach to Multilingual
Coreference Resolution. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Proceedings and Conference on Computational
Natural Language Learning - Shared Task.
Pascal Denis and Jason Baldridge. 2008. Specialized
Models and Ranking for Coreference Resolution. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
</reference>
<page confidence="0.929703">
1980
</page>
<reference confidence="0.999691886792453">
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. Journal of Machine
Learning Research, 12:2121–2159, July.
Greg Durrett, David Hall, and Dan Klein. 2013. Decen-
tralized Entity-Level Modeling for Coreference Reso-
lution. In Proceedings of the Association for Compu-
tational Linguistics.
Eraldo Rezende Fernandes, Cicero Nogueira dos Santos,
and Ruy Luiz Milidi´u. 2012. Latent Structure Per-
ceptron with Feature Induction for Unrestricted Coref-
erence Resolution. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Proceedings and Conference on Computational Nat-
ural Language Learning - Shared Task.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-
Margin CRFs: Training Log-Linear Models with Cost
Functions. In Proceedings of the North American
Chapter for the Association for Computational Lin-
guistics.
David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.
2007. English Gigaword Third Edition. Linguistic
Data Consortium, Catalog Number LDC2007T07.
Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi.
1995. Centering: A Framework for Modeling the Lo-
cal Coherence of Discourse. Computational Linguis-
tics, 21(2):203–225, June.
Aria Haghighi and Dan Klein. 2009. Simple Coreference
Resolution with Rich Syntactic and Semantic Features.
In Proceedings of Empirical Methods in Natural Lan-
guage Processing.
Aria Haghighi and Dan Klein. 2010. Coreference Res-
olution in a Modular, Entity-Centered Model. In Pro-
ceedings of the North American Chapter of the Asso-
ciation for Computational Linguistics.
Iris Hendrickx and Walter Daelemans, 2007. Adding Se-
mantic Information: Unsupervised Clusters for Coref-
erence Resolution.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% Solution. In Proceedings of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Short Papers.
Emmanuel Lassalle and Pascal Denis. 2013. Improving
Pairwise Coreference Models Through Feature Space
Hierarchy Learning. In Proceedings of the Association
for Computational Linguistics.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford’s Multi-Pass Sieve Coreference Resolution
System at the CoNLL-2011 Shared Task. In Proceed-
ings of the Conference on Computational Natural Lan-
guage Learning: Shared Task.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A
Mention-Synchronous Coreference Resolution Algo-
rithm Based on the Bell Tree. In Proceedings of the
Association for Computational Linguistics.
Xiaoqiang Luo. 2005. On Coreference Resolution Per-
formance Metrics. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Sebastian Martschat, Jie Cai, Samuel Broscheit, ´Eva
M´ujdricza-Maydt, and Michael Strube. 2012. A
Multigraph Model for Coreference Resolution. In
Proceedings of the Joint Conference on Empirical
Methods in Natural Language Proceedings and Con-
ference on Computational Natural Language Learning
- Shared Task.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting Semantic Role Labeling, WordNet and
Wikipedia for Coreference Resolution. In Proceed-
ings of the North American Chapter of the Association
of Computational Linguistics.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes. In Proceedings of
the Conference on Computational Natural Language
Learning: Shared Task.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling Multilingual Unre-
stricted Coreference in OntoNotes. In Joint Confer-
ence on EMNLP and CoNLL - Shared Task.
Altaf Rahman and Vincent Ng. 2009. Supervised Mod-
els for Coreference Resolution. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Altaf Rahman and Vincent Ng. 2011a. Coreference
Resolution with World Knowledge. In Proceedings
of the Association for Computational Linguistics: Hu-
man Language Technologies.
Altaf Rahman and Vincent Ng. 2011b. Narrowing
the Modeling Gap: A Cluster-Ranking Approach to
Coreference Resolution. Journal of Artificial Intelli-
gence Research, 40(1):469–521, January.
Marta Recasens, Matthew Can, and Daniel Jurafsky.
2013a. Same Referent, Different Words: Unsuper-
vised Mining of Opaque Coreferent Mentions. In Pro-
ceedings of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies.
Marta Recasens, Marie-Catherine de Marneffe, and
Christopher Potts. 2013b. The Life and Death of Dis-
course Entities: Identifying Singleton Mentions. In
</reference>
<page confidence="0.868661">
1981
</page>
<reference confidence="0.999111481481481">
Proceedings of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A Machine Learning Approach to Coref-
erence Resolution of Noun Phrases. Computational
Linguistics, 27(4):521–544, December.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in Noun Phrase
Coreference Resolution: Making Sense of the State-
of-the-Art. In Proceedings of the Association for Com-
putational Linguistics.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. Coref-
erence Resolution with Reconcile. In Proceedings of
the Association for Computational Linguistics: Short
Papers.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith, Xi-
aofeng Yang, and Alessandro Moschitti. 2008. BART:
A Modular Toolkit for Coreference Resolution. In
Proceedings of the Association for Computational Lin-
guistics: Demo Session.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A Model-
Theoretic Coreference Scoring Scheme. In Proceed-
ings of the Conference on Message Understanding.
</reference>
<page confidence="0.995911">
1982
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.511995">
<title confidence="0.996848">Easy Victories and Uphill Battles in Coreference Resolution</title>
<author confidence="0.964852">Durrett</author>
<affiliation confidence="0.9999105">Computer Science University of California,</affiliation>
<abstract confidence="0.99855748">Classical coreference systems encode various syntactic, discourse, and semantic phenomena explicitly, using heterogenous features computed from hand-crafted heuristics. In contrast, we present a state-of-the-art coreference system that captures such phenomena implicitly, with a small number of homogeneous feature templates examining shallow properties of mentions. Surprisingly, our features are actually more effective than the corresponding hand-engineered ones at modeling these key linguistic phenomena, allowing us to win “easy victories” without crafted heuristics. These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow semantic features from the literature, suggesting that this approach to semantics is an “uphill bat- Nonetheless, our final outperforms the Stanford system (Lee et al. (2011), the winner of the CoNLL 2011 shared task) by 3.5% absolute on the CoNLL metric and outperforms the IMS system (Bj¨orkelund and</abstract>
<note confidence="0.638488">Farkas (2012), the best publicly available English coreference system) by 1.9% absolute.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for Scoring Coreference Chains.</title>
<date>1998</date>
<booktitle>In Proceedings of the Conference on Language Resources and Evaluation Workshop on Linguistics Coreference.</booktitle>
<contexts>
<context position="4810" citStr="Bagga and Baldwin, 1998" startWordPosition="715" endWordPosition="718">h coreference systems, the Stanford system (Lee et al., 2011) and the IMS system (Bj¨orkelund and Farkas, 2012), by wide margins: 3.5% absolute and 1.9% absolute, respectively, on the CoNLL metric. 2 Experimental Setup Throughout this work, we use the datasets from the CoNLL 2011 shared task2 (Pradhan et al., 2011), which is derived from the OntoNotes corpus (Hovy et al., 2006). When applicable, we use the standard automatic parses and NER tags for each document. All experiments use system mentions except where otherwise indicated. For each experiment, we report MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average, the CoNLL metric. All metrics are computed using version 5 of the official CoNLL scorer.3 3 A Mention-Synchronous Framework We first present the basic architecture of our coreference system, independent of a feature set. Unlike binary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al., 2001; Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2This dataset is identical to the English portion of the Co</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for Scoring Coreference Chains. In Proceedings of the Conference on Language Resources and Evaluation Workshop on Linguistics Coreference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Dan Klein</author>
</authors>
<title>Coreference Semantics from Web Features.</title>
<date>2012</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="30906" citStr="Bansal and Klein, 2012" startWordPosition="5042" endWordPosition="5045">nd one that comes at the expense of performance on most other resolution class. The one class that does not get worse, singleton pronouns, actually improves by a similar 4% margin, indicating that roughly half of the gains we observe are not even necessarily a result of our features doing what they were designed to do. Our weak cues do yield some small gains, so there is hope that better weak indicators of semantic compatibility could prove more useful. However, while extremely high-precision approaches with carefully engineered features have been shown to be successful (Rahman and Ng, 2011a; Bansal and Klein, 2012; Recasens et al., 2013a), we conclude that capturing semantics in a data-driven, shallow manner remains an uphill battle. 6 FINAL System and Results While semantic features ended up giving only marginal benefit, we have demonstrated that nevertheless our SURFACE system is a state-of-the-art English coreference system. However, there remain a few natural features that we omitted in order to keep the system as simple as possible, since they were orthogonal to the discussion of data-driven versus heuristic-driven features and do not target world knowledge. Before giving final results, we will pr</context>
</contexts>
<marker>Bansal, Klein, 2012</marker>
<rawString>Mohit Bansal and Dan Klein. 2012. Coreference Semantics from Web Features. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Bengtson</author>
<author>Dan Roth</author>
</authors>
<title>Understanding the Value of Features for Coreference Resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1801" citStr="Bengtson and Roth, 2008" startWordPosition="249" endWordPosition="252"> CoNLL metric and outperforms the IMS system (Bj¨orkelund and Farkas (2012), the best publicly available English coreference system) by 1.9% absolute. 1 Introduction Coreference resolution is a multi-faceted task: humans resolve references by exploiting contextual and grammatical clues, as well as semantic information and world knowledge, so capturing each of 1The Berkeley Coreference Resolution System is available athttp://nlp.cs.berkeley.edu. these will be necessary for an automatic system to fully solve the problem. Acknowledging this complexity, coreference systems, either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010; Rahman and Ng, 2011b) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011), draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many heterogenous parts that can be difficult to interpret or modify. We build a learning-based, mention-synchronous coreference system that aims to use the simplest possible set of features to tackle the various aspects of coreference resolution. Though they ari</context>
<context position="5240" citStr="Bengtson and Roth, 2008" startWordPosition="783" endWordPosition="786">rses and NER tags for each document. All experiments use system mentions except where otherwise indicated. For each experiment, we report MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average, the CoNLL metric. All metrics are computed using version 5 of the official CoNLL scorer.3 3 A Mention-Synchronous Framework We first present the basic architecture of our coreference system, independent of a feature set. Unlike binary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al., 2001; Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2This dataset is identical to the English portion of the CoNLL 2012 data, except for the absence of a small pivot text. 3Note that this version of the scorer implements a modified version of B3, described in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAF, is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. each mention or determine that it begins a new cluster (Denis and Baldridge, 200</context>
</contexts>
<marker>Bengtson, Roth, 2008</marker>
<rawString>Eric Bengtson and Dan Roth. 2008. Understanding the Value of Features for Coreference Resolution. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
</authors>
<title>Bootstrapping Path-Based Pronoun Resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Computational Linguistics and the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="26552" citStr="Bergsma and Lin (2006)" startWordPosition="4315" endWordPosition="4318">hallow Semantics As we were able to incorporate syntax with shallow features, so too might we hope to incorporate semantics. However, the semantic information contained even in a coreference corpus of thousands of documents is insufficient to generalize to unseen data,8 so system designers have turned to external resources such as semantic classes derived from WordNet (Soon et al., 2001), WordNet hypernymy or synonymy (Stoyanov et al., 2010), semantic similarity computed from online resources (Ponzetto and Strube, 2006), named entity type features, gender and number match using the dataset of Bergsma and Lin (2006), and features from unsupervised clusters (Hendrickx and Daelemans, 2007; Durrett et al., 2013). In this section, we consider the following subset of these information sources: • WordNet hypernymy and synonymy • Number and gender data for nominals and propers from Bergsma and Lin (2006) • Named entity types • Latent clusters computed from English Gigaword (Graff et al., 2007), where a latent cluster label generates each nominal head (excluding pronouns) and a conjunction of its verbal governor and semantic role, if any (Durrett et al., 2013). We use twenty clusters, which include clusters like</context>
<context position="31883" citStr="Bergsma and Lin (2006)" startWordPosition="5200" endWordPosition="5203">l features that we omitted in order to keep the system as simple as possible, since they were orthogonal to the discussion of data-driven versus heuristic-driven features and do not target world knowledge. Before giving final results, we will present a small set of additional features that consider four additional mention properties beyond those in Section 4.1: • Whether two mentions are nested • Ancestry of each mention head: the dependency parent and grandparent POS tags and arc directions (shown in Figure 3) • The speaker of each mention • Number and gender of each mention as determined by Bergsma and Lin (2006) The specific additional features we use are shown in Table 6. Note that unlike in Section 5, we use the number and gender information only on the antecedent. Due to our conjunction scheme, both this 1978 president R TO VBD R ROOT ... sent it to the [president] ... [President Obama] signed ... VBD PRP TO DET NN NNP NNP VBD Figure 3: Demonstration of the ancestry extraction process. These features capture more sophisticated configurational information than our context word features do: in this example, president is in a characteristic indirect object position based on its dependency parents, an</context>
</contexts>
<marker>Bergsma, Lin, 2006</marker>
<rawString>Shane Bergsma and Dekang Lin. 2006. Bootstrapping Path-Based Pronoun Resolution. In Proceedings of the Conference on Computational Linguistics and the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Rich´ard Farkas</author>
</authors>
<title>Datadriven Multilingual Coreference Resolution using Resolver Stacking.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Proceedings and Conference on Computational Natural Language Learning - Shared Task.</booktitle>
<marker>Bj¨orkelund, Farkas, 2012</marker>
<rawString>Anders Bj¨orkelund and Rich´ard Farkas. 2012. Datadriven Multilingual Coreference Resolution using Resolver Stacking. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Proceedings and Conference on Computational Natural Language Learning - Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Pierre Nugues</author>
</authors>
<title>Exploring Lexicalized Features for Coreference Resolution.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning: Shared Task.</booktitle>
<marker>Bj¨orkelund, Nugues, 2011</marker>
<rawString>Anders Bj¨orkelund and Pierre Nugues. 2011. Exploring Lexicalized Features for Coreference Resolution. In Proceedings of the Conference on Computational Natural Language Learning: Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Cai</author>
<author>Michael Strube</author>
</authors>
<title>Evaluation Metrics for End-to-End Coreference Resolution Systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the Special Interest Group on Discourse and Dialogue.</booktitle>
<contexts>
<context position="5580" citStr="Cai and Strube (2010)" startWordPosition="844" endWordPosition="847">ion-Synchronous Framework We first present the basic architecture of our coreference system, independent of a feature set. Unlike binary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al., 2001; Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2This dataset is identical to the English portion of the CoNLL 2012 data, except for the absence of a small pivot text. 3Note that this version of the scorer implements a modified version of B3, described in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAF, is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention-synchronous framework, features examine single mentions to evaluate whether or not they are anaphoric and pairs of mentions to evaluate whether or not they corefer. While other work has used this framework as a starting point for entity-level systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi an</context>
</contexts>
<marker>Cai, Strube, 2010</marker>
<rawString>Jie Cai and Michael Strube. 2010. Evaluation Metrics for End-to-End Coreference Resolution Systems. In Proceedings of the Special Interest Group on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Chen</author>
<author>Vincent Ng</author>
</authors>
<title>Combining the Best of Two Worlds: A Hybrid Approach to Multilingual Coreference Resolution.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Proceedings and Conference on Computational Natural Language Learning - Shared Task.</booktitle>
<contexts>
<context position="35656" citStr="Chen and Ng, 2012" startWordPosition="5812" endWordPosition="5815">impact pronoun resolution, and they allow speaker match to capture effects like I and you being coreferent when the speakers differ. features with regularization also means that we organically get distinctions among different mention types without having to choose a level of granularity a priori, unlike the distinct classifiers employed by Denis and Baldridge (2008). In terms of architecture, many coreference systems operate in a pipelined fashion, making partial decisions about coreference or pruning arcs before full resolution. Some systems use separate rule-based and learning-based passes (Chen and Ng, 2012; Fernandes et al., 2012), a series of learning-based passes (Bj¨orkelund and Farkas, 2012), or referentiality classifiers that prune the set of mentions before resolution (Rahman and Ng, 2009; Bj¨orkelund and Farkas, 2012; Recasens et al., 2013b). By contrast, our system resolves all mentions in one pass and does not need pruning: the SURFACE system can train in less than two hours without any subsampling of coreference arcs, and rule-based pruning of coreference arcs actually causes our system to perform less well, since our features can learn valuable information from these negative example</context>
</contexts>
<marker>Chen, Ng, 2012</marker>
<rawString>Chen Chen and Vincent Ng. 2012. Combining the Best of Two Worlds: A Hybrid Approach to Multilingual Coreference Resolution. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Proceedings and Conference on Computational Natural Language Learning - Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Specialized Models and Ranking for Coreference Resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5842" citStr="Denis and Baldridge, 2008" startWordPosition="891" endWordPosition="894"> Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2This dataset is identical to the English portion of the CoNLL 2012 data, except for the absence of a small pivot text. 3Note that this version of the scorer implements a modified version of B3, described in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAF, is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention-synchronous framework, features examine single mentions to evaluate whether or not they are anaphoric and pairs of mentions to evaluate whether or not they corefer. While other work has used this framework as a starting point for entity-level systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Durrett et al., 2013), we will show that a mention-synchronous approach is sufficient to get state-of-the-art performance on its own. 3.1 Mention Detection Our system first identifies a set of predicted mentions from text annotated with parses and</context>
<context position="35407" citStr="Denis and Baldridge (2008)" startWordPosition="5774" endWordPosition="5777"> variants of each feature are included: first with the type of the current mention (NOMINAL, PROPER, or the citation form of the pronoun), then with the types of both mentions in the pair. These conjunctions allow antecedent features on gender and number to impact pronoun resolution, and they allow speaker match to capture effects like I and you being coreferent when the speakers differ. features with regularization also means that we organically get distinctions among different mention types without having to choose a level of granularity a priori, unlike the distinct classifiers employed by Denis and Baldridge (2008). In terms of architecture, many coreference systems operate in a pipelined fashion, making partial decisions about coreference or pruning arcs before full resolution. Some systems use separate rule-based and learning-based passes (Chen and Ng, 2012; Fernandes et al., 2012), a series of learning-based passes (Bj¨orkelund and Farkas, 2012), or referentiality classifiers that prune the set of mentions before resolution (Rahman and Ng, 2009; Bj¨orkelund and Farkas, 2012; Recasens et al., 2013b). By contrast, our system resolves all mentions in one pass and does not need pruning: the SURFACE syste</context>
</contexts>
<marker>Denis, Baldridge, 2008</marker>
<rawString>Pascal Denis and Jason Baldridge. 2008. Specialized Models and Ranking for Coreference Resolution. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="12588" citStr="Duchi et al., 2011" startWordPosition="2050" endWordPosition="2053">or az is the wrong antecedent (but az is indeed anaphoric). Our final parameterized loss function is a weighted sum of the counts of these three error types: l(a, C*) = αFAFA(a, C*) + αFNFN(a, C*) + αWLWL(a, C*) where FA(a, C*) gives the number of false anaphor errors in prediction a with gold chains C* (FN and WL are analogous). By setting αFA low and αFN high relative to αWL, we can counterbalance the high number of singleton mentions and bias the system towards making more coreference linkages. We set (αFA, αFN, αWL) = (0.1, 3.0, 1.0) and A = 0.001 and optimize the objective using AdaGrad (Duchi et al., 2011). 4 Easy Victories from Surface Features Our primary goal with this work is to show that a high-performance coreference system is attainable with a small number of feature templates that use only surface-level information sources. These features will be general-purpose and capture linguistic effects to the point where standard heuristic-driven features are no longer needed in our system. 4.1 SURFACE Features and Conjunctions Our SURFACE feature set only considers the following properties of mentions and mention pairs: • Mention type (nominal, proper, or pronominal) • The complete string of a m</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12:2121–2159, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>David Hall</author>
<author>Dan Klein</author>
</authors>
<title>Decentralized Entity-Level Modeling for Coreference Resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6216" citStr="Durrett et al., 2013" startWordPosition="952" endWordPosition="955"> for the CoNLL shared tasks. The implementation of CEAF, is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention-synchronous framework, features examine single mentions to evaluate whether or not they are anaphoric and pairs of mentions to evaluate whether or not they corefer. While other work has used this framework as a starting point for entity-level systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Durrett et al., 2013), we will show that a mention-synchronous approach is sufficient to get state-of-the-art performance on its own. 3.1 Mention Detection Our system first identifies a set of predicted mentions from text annotated with parses and named entity tags. We extract three distinct types of mentions: proper mentions from all named entity chunks except for those labeled as QUANTITY, CARDINAL, or PERCENT, pronominal mentions from single words tagged with PRP or PRP$, and nominal mentions from all other maximal NP projections. These basic rules are similar to those of Lee et al. (2011), except that their sy</context>
<context position="9533" citStr="Durrett et al., 2013" startWordPosition="1512" endWordPosition="1515"> the given mention to be anaphoric or not; when ai = j for some j, the features express aspects of the pairwise linkage, and can examine any relevant attributes of the anaphor i or the antecedent j, since information about each mention is contained in x. Inference in this model is efficient: because log P(a|x) decomposes linearly over mentions, we can compute ai = arg maxaz P(ai|x) separately for each mention and return the set of coreference chains implied by these decisions. 3.3 Learning During learning, we optimize for conditional loglikelihood augmented with a parameterized loss function (Durrett et al., 2013). The main complicating factor in this process is that the supervision in coreference consists of a gold clustering C* defined over gold mentions. This is problematic for two reasons: first, because the clustering is defined over gold mentions rather than our system mentions, and second, because a clustering does not specify a full antecedent structure of the sort our model produces. We can address the first of these problems by imputing singleton clusters for mentions that do not appear in the gold standard; our system will then simply learn to put spurious mentions in their own clusters. Sin</context>
<context position="26647" citStr="Durrett et al., 2013" startWordPosition="4329" endWordPosition="4332">pe to incorporate semantics. However, the semantic information contained even in a coreference corpus of thousands of documents is insufficient to generalize to unseen data,8 so system designers have turned to external resources such as semantic classes derived from WordNet (Soon et al., 2001), WordNet hypernymy or synonymy (Stoyanov et al., 2010), semantic similarity computed from online resources (Ponzetto and Strube, 2006), named entity type features, gender and number match using the dataset of Bergsma and Lin (2006), and features from unsupervised clusters (Hendrickx and Daelemans, 2007; Durrett et al., 2013). In this section, we consider the following subset of these information sources: • WordNet hypernymy and synonymy • Number and gender data for nominals and propers from Bergsma and Lin (2006) • Named entity types • Latent clusters computed from English Gigaword (Graff et al., 2007), where a latent cluster label generates each nominal head (excluding pronouns) and a conjunction of its verbal governor and semantic role, if any (Durrett et al., 2013). We use twenty clusters, which include clusters like president and leader (things which announce). Together, we call these the SEM features. We sho</context>
</contexts>
<marker>Durrett, Hall, Klein, 2013</marker>
<rawString>Greg Durrett, David Hall, and Dan Klein. 2013. Decentralized Entity-Level Modeling for Coreference Resolution. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<title>Eraldo Rezende Fernandes, Cicero Nogueira dos Santos, and Ruy Luiz Milidi´u.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Proceedings and Conference on Computational Natural Language Learning - Shared Task.</booktitle>
<contexts>
<context position="1253" citStr="(2012)" startWordPosition="177" endWordPosition="177">red ones at modeling these key linguistic phenomena, allowing us to win “easy victories” without crafted heuristics. These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow semantic features from the literature, suggesting that this approach to semantics is an “uphill battle.” Nonetheless, our final system1 outperforms the Stanford system (Lee et al. (2011), the winner of the CoNLL 2011 shared task) by 3.5% absolute on the CoNLL metric and outperforms the IMS system (Bj¨orkelund and Farkas (2012), the best publicly available English coreference system) by 1.9% absolute. 1 Introduction Coreference resolution is a multi-faceted task: humans resolve references by exploiting contextual and grammatical clues, as well as semantic information and world knowledge, so capturing each of 1The Berkeley Coreference Resolution System is available athttp://nlp.cs.berkeley.edu. these will be necessary for an automatic system to fully solve the problem. Acknowledging this complexity, coreference systems, either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010; R</context>
<context position="16002" citStr="(2012)" startWordPosition="2621" endWordPosition="2621">joined with the types of the current and antecedent mentions. conjunctions that examine properties of the current mention as well, as shown with the ANT. HEAD feature in the figure. Finally, we found it beneficial for our lexical indicator features to only fire on words occurring at least 20 times in the training set; for rare words, we use the part of speech of the word instead. The performance of our system is shown in Table 2. We contrast our performance with that of the Stanford system (Lee et al. (2011), the winner of the CoNLL 2011 shared task) and the IMS system (Bj¨orkelund and Farkas (2012), the best publicly available English coreference system). Despite its simplicity, our SURFACE system is sufficient to outperform these sophisticated systems: the Stanford system uses a cascade of ten rule-based sieves each of which has customized heuristics, and the IMS system uses a similarly long pipeline consisting of a learned referentiality classifier followed by multiple resolvers, which are run in sequence and rely on the outputs of the previous resolvers as features. 4.2 Data-Driven versus Heuristic-Driven Features Why are the SURFACE features sufficient to give high coreference perfo</context>
<context position="32917" citStr="(2012)" startWordPosition="5376" endWordPosition="5376">onfigurational information than our context word features do: in this example, president is in a characteristic indirect object position based on its dependency parents, and Obama is the subject of the main verb of the sentence. semantic information and the speaker information can apply in a fine-grained way to different pronouns, and can therefore improve pronoun resolution substantially; however, these features generally only improve pronoun resolution. Full results for our SURFACE and FINAL feature sets are shown in Table 7. Again, we compare to Lee et al. (2011) and Bj¨orkelund and Farkas (2012).10 Despite our system’s emphasis on one-pass resolution with as simple a feature set as possible, we are able to outperform even these sophisticated systems by a wide margin. 7 Related Work Many of the individual features we employ in the FINAL feature set have appeared in other coreference systems (Bj¨orkelund and Nugues, 2011; Rahman and Ng, 2011b; Fernandes et al., 2012). However, other authors have often emphasized bilexical features on head pairs, whereas our features are heavily monolexical. For feature conjunctions, other authors have exploited three classes (Lee et al., 2011) or autom</context>
<context position="34186" citStr="(2012)" startWordPosition="5584" endWordPosition="5584">assalle and Denis, 2013), but to our knowledge we are the first to do fine-grained modeling of every pronoun. Inclusion of a hierarchy of 10Discrepancies between scores here and those printed in Pradhan et al. (2012) arise from two sources: improvements to the system of Lee et al. (2011) since the first CoNLL shared task, and a fix to the scoring of B3 in the official scorer since results of the two CoNLL shared tasks were released. Unfortunately, because of this bug in the scoring program, direct comparison to the printed results of the other highest-scoring English systems, Fernandes et al. (2012) and Martschat et al. (2012), is impossible. Feature name Count Features of the SURFACE system 418704 Features on the current mention [ANAPHORIC] + [CURRENT ANCESTRY] 46047 Features on the antecedent [ANTECEDENT ANCESTRY] 53874 [ANTECEDENT GENDER] 338 [ANTECEDENT NUMBER] 290 Features on the pair [HEAD CONTAINED (T/F)] 136 [EXACT STRING CONTAINED (T/F)] 133 [NESTED (T/F)] 355 [DOC TYPE] + [SAME SPEAKER (T/F)] 437 [CURRENT ANCESTRY] + [ANT. ANCESTRY] 2555359 Table 6: FINAL feature set; note that this includes the SURFACE feature set. As with the features of the SURFACE system, two conjoined vari</context>
<context position="37040" citStr="(2012)" startWordPosition="6042" endWordPosition="6042">57.67 IMS 66.67 58.20 62.15 77.60 56.77 65.57 42.92 51.11 46.66 58.13 SURFACE* 68.42 60.80 64.39 76.57 59.21 66.78 45.30 53.36 49.00 60.06 FINAL* 68.97 63.47 66.10 76.58 62.06 68.56 47.32 53.19 50.09 61.58 CoNLL 2011 Test Set STANFORD 60.91 62.13 61.51 70.61 57.31 63.27 45.79 44.56 45.17 56.65 IMS 68.15 61.60 64.71 75.97 56.39 64.73 42.30 48.88 45.35 58.26 FINAL* 66.81 66.04 66.43 71.07 61.89 66.16 47.37 48.22 47.79 60.13 Table 7: CoNLL metric scores for our systems on the CoNLL development and blind test sets, compared to the results of Lee et al. (2011) (STANFORD) and Bj¨orkelund and Farkas (2012) (IMS). Starred systems are contributions of this work. Bolded F1 values represent statistically significant improvements over other systems with p &lt; 0.05 using a bootstrap resampling test. Metric values reflect version 5 of the CoNLL scorer. 8 Conclusion We have presented a coreference system that uses a simple, homogeneous set of features in a discriminative learning framework to achieve high performance. Large numbers of lexicalized, data-driven features implicitly model linguistic phenomena such as definiteness and centering, obviating the need for heuristic-driven rules explicitly targeti</context>
</contexts>
<marker>2012</marker>
<rawString>Eraldo Rezende Fernandes, Cicero Nogueira dos Santos, and Ruy Luiz Milidi´u. 2012. Latent Structure Perceptron with Feature Induction for Unrestricted Coreference Resolution. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Proceedings and Conference on Computational Natural Language Learning - Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>SoftmaxMargin CRFs: Training Log-Linear Models with Cost Functions.</title>
<date>2010</date>
<booktitle>In Proceedings of the North American Chapter for the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11551" citStr="Gimpel and Smith, 2010" startWordPosition="1859" endWordPosition="1862">ts gold antecedents are among the set of system predicted mentions. Given t training examples of the form (xk, C*k), we write the following likelihood function: log X P&apos;(a|xk) + AJJwJJ1 (aEA(Ck*) where P&apos;(a|xk) a P(a|xk) exp(l(a, C*k)) with l(a, C*) being a real-valued loss function. The loss 4Because of this marginalization over latent antecedent choices, our objective is non-convex. P(a|x) a exp Xn i=1 e(w) = Xt k=1 1973 here plays an analogous role to the loss in structured max-margin objectives; incorporating it into a conditional likelihood objective is a technique called softmax-margin (Gimpel and Smith, 2010). Our loss function l(a, C*) is a weighted linear combination of three error types, examples of which are shown in Figure 1. A false anaphor (FA) error occurs when az is chosen to be anaphoric when it should start a new cluster. A false new (FN) error occurs in the opposite case, when az wrongly indicates a new cluster when it should be anaphoric. Finally, a wrong link (WL) error occurs when the antecedent chosen for az is the wrong antecedent (but az is indeed anaphoric). Our final parameterized loss function is a weighted sum of the counts of these three error types: l(a, C*) = αFAFA(a, C*) </context>
</contexts>
<marker>Gimpel, Smith, 2010</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2010. SoftmaxMargin CRFs: Training Log-Linear Models with Cost Functions. In Proceedings of the North American Chapter for the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<date>2007</date>
<booktitle>English Gigaword Third Edition. Linguistic Data Consortium, Catalog Number LDC2007T07.</booktitle>
<contexts>
<context position="26930" citStr="Graff et al., 2007" startWordPosition="4376" endWordPosition="4379">l., 2001), WordNet hypernymy or synonymy (Stoyanov et al., 2010), semantic similarity computed from online resources (Ponzetto and Strube, 2006), named entity type features, gender and number match using the dataset of Bergsma and Lin (2006), and features from unsupervised clusters (Hendrickx and Daelemans, 2007; Durrett et al., 2013). In this section, we consider the following subset of these information sources: • WordNet hypernymy and synonymy • Number and gender data for nominals and propers from Bergsma and Lin (2006) • Named entity types • Latent clusters computed from English Gigaword (Graff et al., 2007), where a latent cluster label generates each nominal head (excluding pronouns) and a conjunction of its verbal governor and semantic role, if any (Durrett et al., 2013). We use twenty clusters, which include clusters like president and leader (things which announce). Together, we call these the SEM features. We show results from this expansion of the feature set in Table 5. When using system mentions, the improvements are not statistically significant on every metric, and are quite marginal given that these features add information that is intuitively central to coreference and otherwise unav</context>
</contexts>
<marker>Graff, Kong, Chen, Maeda, 2007</marker>
<rawString>David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2007. English Gigaword Third Edition. Linguistic Data Consortium, Catalog Number LDC2007T07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Scott Weinstein</author>
<author>Aravind K Joshi</author>
</authors>
<title>Centering: A Framework for Modeling the Local Coherence of Discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="17715" citStr="Grosz et al., 1995" startWordPosition="2883" endWordPosition="2886">able to substantially outperform the other two, the two best publicly-available English coreference systems. Bolded values are significant with p &lt; 0.05 according to a bootstrap resampling test. ence features, just implicitly. For example, rather than having rules targeting person, number, gender, or animacy of mentions, we use conjunctions with pronoun identity, which contains this information. Rather than explicitly writing a feature targeting definiteness, our indicators on the first word of a mention will capture this and other effects. And finally, rather than targeting centering theory (Grosz et al., 1995) with rule-based features identifying syntactic positions (Stoyanov et al., 2010; Haghighi and Klein, 2010), our features on word context can identify configurational clues like whether a mention is preceded or followed by a verb, and therefore whether it is likely in subject or object position.5 Not only are data-driven features able to capture the same phenomena as heuristic-driven features, but they do so at a finer level of granularity, and can therefore model more patterns in the data. To contrast these two types of features, we experiment with three ablated versions of our system, where </context>
</contexts>
<marker>Grosz, Weinstein, Joshi, 1995</marker>
<rawString>Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi. 1995. Centering: A Framework for Modeling the Local Coherence of Discourse. Computational Linguistics, 21(2):203–225, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Simple Coreference Resolution with Rich Syntactic and Semantic Features.</title>
<date>2009</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1913" citStr="Haghighi and Klein, 2009" startWordPosition="267" endWordPosition="270">h coreference system) by 1.9% absolute. 1 Introduction Coreference resolution is a multi-faceted task: humans resolve references by exploiting contextual and grammatical clues, as well as semantic information and world knowledge, so capturing each of 1The Berkeley Coreference Resolution System is available athttp://nlp.cs.berkeley.edu. these will be necessary for an automatic system to fully solve the problem. Acknowledging this complexity, coreference systems, either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010; Rahman and Ng, 2011b) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011), draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many heterogenous parts that can be difficult to interpret or modify. We build a learning-based, mention-synchronous coreference system that aims to use the simplest possible set of features to tackle the various aspects of coreference resolution. Though they arise from a small number of simple templates, our features are numerous, which works to our advantage: we can both</context>
</contexts>
<marker>Haghighi, Klein, 2009</marker>
<rawString>Aria Haghighi and Dan Klein. 2009. Simple Coreference Resolution with Rich Syntactic and Semantic Features. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coreference Resolution in a Modular, Entity-Centered Model.</title>
<date>2010</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1850" citStr="Haghighi and Klein, 2010" startWordPosition="257" endWordPosition="260">¨orkelund and Farkas (2012), the best publicly available English coreference system) by 1.9% absolute. 1 Introduction Coreference resolution is a multi-faceted task: humans resolve references by exploiting contextual and grammatical clues, as well as semantic information and world knowledge, so capturing each of 1The Berkeley Coreference Resolution System is available athttp://nlp.cs.berkeley.edu. these will be necessary for an automatic system to fully solve the problem. Acknowledging this complexity, coreference systems, either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010; Rahman and Ng, 2011b) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011), draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many heterogenous parts that can be difficult to interpret or modify. We build a learning-based, mention-synchronous coreference system that aims to use the simplest possible set of features to tackle the various aspects of coreference resolution. Though they arise from a small number of simple templates, our f</context>
<context position="6193" citStr="Haghighi and Klein, 2010" startWordPosition="948" endWordPosition="951">rube (2010), that was used for the CoNLL shared tasks. The implementation of CEAF, is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention-synchronous framework, features examine single mentions to evaluate whether or not they are anaphoric and pairs of mentions to evaluate whether or not they corefer. While other work has used this framework as a starting point for entity-level systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Durrett et al., 2013), we will show that a mention-synchronous approach is sufficient to get state-of-the-art performance on its own. 3.1 Mention Detection Our system first identifies a set of predicted mentions from text annotated with parses and named entity tags. We extract three distinct types of mentions: proper mentions from all named entity chunks except for those labeled as QUANTITY, CARDINAL, or PERCENT, pronominal mentions from single words tagged with PRP or PRP$, and nominal mentions from all other maximal NP projections. These basic rules are similar to those of Lee et al. (2011</context>
<context position="17822" citStr="Haghighi and Klein, 2010" startWordPosition="2898" endWordPosition="2901">ystems. Bolded values are significant with p &lt; 0.05 according to a bootstrap resampling test. ence features, just implicitly. For example, rather than having rules targeting person, number, gender, or animacy of mentions, we use conjunctions with pronoun identity, which contains this information. Rather than explicitly writing a feature targeting definiteness, our indicators on the first word of a mention will capture this and other effects. And finally, rather than targeting centering theory (Grosz et al., 1995) with rule-based features identifying syntactic positions (Stoyanov et al., 2010; Haghighi and Klein, 2010), our features on word context can identify configurational clues like whether a mention is preceded or followed by a verb, and therefore whether it is likely in subject or object position.5 Not only are data-driven features able to capture the same phenomena as heuristic-driven features, but they do so at a finer level of granularity, and can therefore model more patterns in the data. To contrast these two types of features, we experiment with three ablated versions of our system, where we replace data-driven features with their heuristic-driven counterparts: 1. Instead of using an indicator </context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Aria Haghighi and Dan Klein. 2010. Coreference Resolution in a Modular, Entity-Centered Model. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iris Hendrickx</author>
<author>Walter Daelemans</author>
</authors>
<title>Adding Semantic Information: Unsupervised Clusters for Coreference Resolution.</title>
<date>2007</date>
<contexts>
<context position="26624" citStr="Hendrickx and Daelemans, 2007" startWordPosition="4325" endWordPosition="4328">ow features, so too might we hope to incorporate semantics. However, the semantic information contained even in a coreference corpus of thousands of documents is insufficient to generalize to unseen data,8 so system designers have turned to external resources such as semantic classes derived from WordNet (Soon et al., 2001), WordNet hypernymy or synonymy (Stoyanov et al., 2010), semantic similarity computed from online resources (Ponzetto and Strube, 2006), named entity type features, gender and number match using the dataset of Bergsma and Lin (2006), and features from unsupervised clusters (Hendrickx and Daelemans, 2007; Durrett et al., 2013). In this section, we consider the following subset of these information sources: • WordNet hypernymy and synonymy • Number and gender data for nominals and propers from Bergsma and Lin (2006) • Named entity types • Latent clusters computed from English Gigaword (Graff et al., 2007), where a latent cluster label generates each nominal head (excluding pronouns) and a conjunction of its verbal governor and semantic role, if any (Durrett et al., 2013). We use twenty clusters, which include clusters like president and leader (things which announce). Together, we call these t</context>
</contexts>
<marker>Hendrickx, Daelemans, 2007</marker>
<rawString>Iris Hendrickx and Walter Daelemans, 2007. Adding Semantic Information: Unsupervised Clusters for Coreference Resolution.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>OntoNotes: The 90% Solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics: Short Papers.</booktitle>
<contexts>
<context position="4566" citStr="Hovy et al., 2006" startWordPosition="677" endWordPosition="680">nformation in this shallow way is an “uphill battle” due to this structural property of coreference resolution. Nevertheless, using a simple architecture and feature set, our final system outperforms the two best publicly available English coreference systems, the Stanford system (Lee et al., 2011) and the IMS system (Bj¨orkelund and Farkas, 2012), by wide margins: 3.5% absolute and 1.9% absolute, respectively, on the CoNLL metric. 2 Experimental Setup Throughout this work, we use the datasets from the CoNLL 2011 shared task2 (Pradhan et al., 2011), which is derived from the OntoNotes corpus (Hovy et al., 2006). When applicable, we use the standard automatic parses and NER tags for each document. All experiments use system mentions except where otherwise indicated. For each experiment, we report MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average, the CoNLL metric. All metrics are computed using version 5 of the official CoNLL scorer.3 3 A Mention-Synchronous Framework We first present the basic architecture of our coreference system, independent of a feature set. Unlike binary classification-based coreference systems where independent binary deci</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: The 90% Solution. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Lassalle</author>
<author>Pascal Denis</author>
</authors>
<title>Improving Pairwise Coreference Models Through Feature Space Hierarchy Learning.</title>
<date>2013</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="33604" citStr="Lassalle and Denis, 2013" startWordPosition="5481" endWordPosition="5484">as simple a feature set as possible, we are able to outperform even these sophisticated systems by a wide margin. 7 Related Work Many of the individual features we employ in the FINAL feature set have appeared in other coreference systems (Bj¨orkelund and Nugues, 2011; Rahman and Ng, 2011b; Fernandes et al., 2012). However, other authors have often emphasized bilexical features on head pairs, whereas our features are heavily monolexical. For feature conjunctions, other authors have exploited three classes (Lee et al., 2011) or automatically learned conjunction schemes (Fernandes et al., 2012; Lassalle and Denis, 2013), but to our knowledge we are the first to do fine-grained modeling of every pronoun. Inclusion of a hierarchy of 10Discrepancies between scores here and those printed in Pradhan et al. (2012) arise from two sources: improvements to the system of Lee et al. (2011) since the first CoNLL shared task, and a fix to the scoring of B3 in the official scorer since results of the two CoNLL shared tasks were released. Unfortunately, because of this bug in the scoring program, direct comparison to the printed results of the other highest-scoring English systems, Fernandes et al. (2012) and Martschat et </context>
</contexts>
<marker>Lassalle, Denis, 2013</marker>
<rawString>Emmanuel Lassalle and Pascal Denis. 2013. Improving Pairwise Coreference Models Through Feature Space Hierarchy Learning. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanford’s Multi-Pass Sieve Coreference Resolution System at the CoNLL-2011 Shared Task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning: Shared Task.</booktitle>
<contexts>
<context position="1111" citStr="Lee et al. (2011)" startWordPosition="150" endWordPosition="153">us feature templates examining shallow properties of mentions. Surprisingly, our features are actually more effective than the corresponding hand-engineered ones at modeling these key linguistic phenomena, allowing us to win “easy victories” without crafted heuristics. These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow semantic features from the literature, suggesting that this approach to semantics is an “uphill battle.” Nonetheless, our final system1 outperforms the Stanford system (Lee et al. (2011), the winner of the CoNLL 2011 shared task) by 3.5% absolute on the CoNLL metric and outperforms the IMS system (Bj¨orkelund and Farkas (2012), the best publicly available English coreference system) by 1.9% absolute. 1 Introduction Coreference resolution is a multi-faceted task: humans resolve references by exploiting contextual and grammatical clues, as well as semantic information and world knowledge, so capturing each of 1The Berkeley Coreference Resolution System is available athttp://nlp.cs.berkeley.edu. these will be necessary for an automatic system to fully solve the problem. Acknowle</context>
<context position="3816" citStr="Lee et al., 2011" startWordPosition="558" endWordPosition="561">res, we critically evaluate several classes of semantic features which intu1971 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1971–1982, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics itively should prove useful but have had mixed results in the literature, and we observe that they are ineffective for our system. However, these features are beneficial when gold mentions are provided to our system, leading us to conclude that the large number of system mentions extracted by most coreference systems (Lee et al., 2011; Fernandes et al., 2012) means that weak indicators cannot overcome the bias against making coreference links. Capturing semantic information in this shallow way is an “uphill battle” due to this structural property of coreference resolution. Nevertheless, using a simple architecture and feature set, our final system outperforms the two best publicly available English coreference systems, the Stanford system (Lee et al., 2011) and the IMS system (Bj¨orkelund and Farkas, 2012), by wide margins: 3.5% absolute and 1.9% absolute, respectively, on the CoNLL metric. 2 Experimental Setup Throughout </context>
<context position="6794" citStr="Lee et al. (2011)" startWordPosition="1047" endWordPosition="1050"> and Klein, 2010; Durrett et al., 2013), we will show that a mention-synchronous approach is sufficient to get state-of-the-art performance on its own. 3.1 Mention Detection Our system first identifies a set of predicted mentions from text annotated with parses and named entity tags. We extract three distinct types of mentions: proper mentions from all named entity chunks except for those labeled as QUANTITY, CARDINAL, or PERCENT, pronominal mentions from single words tagged with PRP or PRP$, and nominal mentions from all other maximal NP projections. These basic rules are similar to those of Lee et al. (2011), except that their system uses an additional set of filtering rules designed to discard instances of pleonastic it, partitives, certain quantified noun phrases, and other spurious mentions. In contrast to this highly engineered approach and to systems which use a trained classifier to compute anaphoricity separately (Rahman and Ng, 2009; Bj¨orkelund and Farkas, 2012), we aim for the highest possible recall of gold mentions with a low-complexity method, leaving us with a large number of spurious system mentions that we will have to reject later. 3.2 Coreference Model Figure 1 shows the mention</context>
<context position="15909" citStr="Lee et al. (2011)" startWordPosition="2601" endWordPosition="2604">rm if it is a pronoun) of the mention being resolved. Each feature on a mention pair is additionally conjoined with the types of the current and antecedent mentions. conjunctions that examine properties of the current mention as well, as shown with the ANT. HEAD feature in the figure. Finally, we found it beneficial for our lexical indicator features to only fire on words occurring at least 20 times in the training set; for rare words, we use the part of speech of the word instead. The performance of our system is shown in Table 2. We contrast our performance with that of the Stanford system (Lee et al. (2011), the winner of the CoNLL 2011 shared task) and the IMS system (Bj¨orkelund and Farkas (2012), the best publicly available English coreference system). Despite its simplicity, our SURFACE system is sufficient to outperform these sophisticated systems: the Stanford system uses a cascade of ten rule-based sieves each of which has customized heuristics, and the IMS system uses a similarly long pipeline consisting of a learned referentiality classifier followed by multiple resolvers, which are run in sequence and rely on the outputs of the previous resolvers as features. 4.2 Data-Driven versus Heu</context>
<context position="20454" citStr="Lee et al. (2011)" startWordPosition="3329" endWordPosition="3332">er information as well. Moreover, the hand-engineered features give no benefit over the SURFACE system. each pronoun (PRONCONJ), we only conjoin with a PRONOUN indicator and add features targeting the person, number, gender, and animacy of the two pronouns (AGR). 3. Instead of using our context features on the preceding and following word (CONTEXT), we use manual determinations of when mentions are in subject, direct object, indirect objection, or oblique position (POSN). All rules for computing person, number, gender, animacy, definiteness, and syntactic position are taken from the system of Lee et al. (2011). Table 3 shows each of the target ablations, as well as the SURFACE system with the DEF, AGR, and POSN features added. While the heuristic-driven feature always help over the corresponding ablated system, they cannot do the work of the fine-grained data-driven features. Most tellingly, though, none of the heuristic-driven features give statistically significant improvements on top of the data-driven features we have already included, indicating that we are at the point of diminishing returns on modeling those specific phenomena. While this does not preclude further engineering to take better </context>
<context position="28937" citStr="Lee et al., 2011" startWordPosition="4705" endWordPosition="4708"> on system mentions. 5.3 Analysis of Semantic Features The main reason that weak semantic cues are not more effective is the small fraction of positive coreference links present in the training data. From Table 4, the number of annotated coreferent spans in the OntoNotes data is about a factor of five smaller than the number of system mentions.9 This both means that most NPs are not coreferent, and for those that are, choosing the correct links is much more difficult because of the large number of possible antecedents. Even head match, which is generally considered a high-precision indicator (Lee et al., 2011), would introduce many spurious coreference arcs if applied too liberally (see Table 4). In light of this fact, a system needs very strong evidence to overcome the default hypothesis that a mention is not coreferent, and a weak indicator will have such a high “false positive” rate that it cannot be relied on (given high weight, this feature would do more harm than good, by introducing many false linkages). To confirm this intuition, we show in the bottom part of Table 5 results when we apply these semantic features on top of our SURFACE system on gold mentions, where there are no singletons. I</context>
<context position="32883" citStr="Lee et al. (2011)" startWordPosition="5368" endWordPosition="5371">. These features capture more sophisticated configurational information than our context word features do: in this example, president is in a characteristic indirect object position based on its dependency parents, and Obama is the subject of the main verb of the sentence. semantic information and the speaker information can apply in a fine-grained way to different pronouns, and can therefore improve pronoun resolution substantially; however, these features generally only improve pronoun resolution. Full results for our SURFACE and FINAL feature sets are shown in Table 7. Again, we compare to Lee et al. (2011) and Bj¨orkelund and Farkas (2012).10 Despite our system’s emphasis on one-pass resolution with as simple a feature set as possible, we are able to outperform even these sophisticated systems by a wide margin. 7 Related Work Many of the individual features we employ in the FINAL feature set have appeared in other coreference systems (Bj¨orkelund and Nugues, 2011; Rahman and Ng, 2011b; Fernandes et al., 2012). However, other authors have often emphasized bilexical features on head pairs, whereas our features are heavily monolexical. For feature conjunctions, other authors have exploited three c</context>
<context position="36995" citStr="Lee et al. (2011)" startWordPosition="6033" endWordPosition="6036">D 61.62 59.34 60.46 74.05 58.70 65.48 45.98 48.22 47.07 57.67 IMS 66.67 58.20 62.15 77.60 56.77 65.57 42.92 51.11 46.66 58.13 SURFACE* 68.42 60.80 64.39 76.57 59.21 66.78 45.30 53.36 49.00 60.06 FINAL* 68.97 63.47 66.10 76.58 62.06 68.56 47.32 53.19 50.09 61.58 CoNLL 2011 Test Set STANFORD 60.91 62.13 61.51 70.61 57.31 63.27 45.79 44.56 45.17 56.65 IMS 68.15 61.60 64.71 75.97 56.39 64.73 42.30 48.88 45.35 58.26 FINAL* 66.81 66.04 66.43 71.07 61.89 66.16 47.37 48.22 47.79 60.13 Table 7: CoNLL metric scores for our systems on the CoNLL development and blind test sets, compared to the results of Lee et al. (2011) (STANFORD) and Bj¨orkelund and Farkas (2012) (IMS). Starred systems are contributions of this work. Bolded F1 values represent statistically significant improvements over other systems with p &lt; 0.05 using a bootstrap resampling test. Metric values reflect version 5 of the CoNLL scorer. 8 Conclusion We have presented a coreference system that uses a simple, homogeneous set of features in a discriminative learning framework to achieve high performance. Large numbers of lexicalized, data-driven features implicitly model linguistic phenomena such as definiteness and centering, obviating the need </context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s Multi-Pass Sieve Coreference Resolution System at the CoNLL-2011 Shared Task. In Proceedings of the Conference on Computational Natural Language Learning: Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Salim Roukos</author>
</authors>
<title>A Mention-Synchronous Coreference Resolution Algorithm Based on the Bell Tree.</title>
<date>2004</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5705" citStr="Luo et al. (2004)" startWordPosition="867" endWordPosition="870"> binary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al., 2001; Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2This dataset is identical to the English portion of the CoNLL 2012 data, except for the absence of a small pivot text. 3Note that this version of the scorer implements a modified version of B3, described in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAF, is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention-synchronous framework, features examine single mentions to evaluate whether or not they are anaphoric and pairs of mentions to evaluate whether or not they corefer. While other work has used this framework as a starting point for entity-level systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Durrett et al., 2013), we will show that a mention-synchronous approach is sufficient to get state-of-the-art </context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla, and Salim Roukos. 2004. A Mention-Synchronous Coreference Resolution Algorithm Based on the Bell Tree. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On Coreference Resolution Performance Metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="4833" citStr="Luo, 2005" startWordPosition="721" endWordPosition="722">ystem (Lee et al., 2011) and the IMS system (Bj¨orkelund and Farkas, 2012), by wide margins: 3.5% absolute and 1.9% absolute, respectively, on the CoNLL metric. 2 Experimental Setup Throughout this work, we use the datasets from the CoNLL 2011 shared task2 (Pradhan et al., 2011), which is derived from the OntoNotes corpus (Hovy et al., 2006). When applicable, we use the standard automatic parses and NER tags for each document. All experiments use system mentions except where otherwise indicated. For each experiment, we report MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average, the CoNLL metric. All metrics are computed using version 5 of the official CoNLL scorer.3 3 A Mention-Synchronous Framework We first present the basic architecture of our coreference system, independent of a feature set. Unlike binary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al., 2001; Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2This dataset is identical to the English portion of the CoNLL 2012 data, except f</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On Coreference Resolution Performance Metrics. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Martschat</author>
<author>Jie Cai</author>
<author>Samuel Broscheit</author>
<author>´Eva M´ujdricza-Maydt</author>
<author>Michael Strube</author>
</authors>
<title>A Multigraph Model for Coreference Resolution.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Proceedings and Conference on Computational Natural Language Learning - Shared Task.</booktitle>
<marker>Martschat, Cai, Broscheit, M´ujdricza-Maydt, Strube, 2012</marker>
<rawString>Sebastian Martschat, Jie Cai, Samuel Broscheit, ´Eva M´ujdricza-Maydt, and Michael Strube. 2012. A Multigraph Model for Coreference Resolution. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Proceedings and Conference on Computational Natural Language Learning - Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Exploiting Semantic Role Labeling, WordNet and Wikipedia for Coreference Resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the North American Chapter of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="26455" citStr="Ponzetto and Strube, 2006" startWordPosition="4299" endWordPosition="4302"> so we will need to include features that capture this knowledge in some fashion. 5.2 Incorporating Shallow Semantics As we were able to incorporate syntax with shallow features, so too might we hope to incorporate semantics. However, the semantic information contained even in a coreference corpus of thousands of documents is insufficient to generalize to unseen data,8 so system designers have turned to external resources such as semantic classes derived from WordNet (Soon et al., 2001), WordNet hypernymy or synonymy (Stoyanov et al., 2010), semantic similarity computed from online resources (Ponzetto and Strube, 2006), named entity type features, gender and number match using the dataset of Bergsma and Lin (2006), and features from unsupervised clusters (Hendrickx and Daelemans, 2007; Durrett et al., 2013). In this section, we consider the following subset of these information sources: • WordNet hypernymy and synonymy • Number and gender data for nominals and propers from Bergsma and Lin (2006) • Named entity types • Latent clusters computed from English Gigaword (Graff et al., 2007), where a latent cluster label generates each nominal head (excluding pronouns) and a conjunction of its verbal governor and </context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2006. Exploiting Semantic Role Labeling, WordNet and Wikipedia for Coreference Resolution. In Proceedings of the North American Chapter of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Ralph Weischedel</author>
<author>Nianwen Xue</author>
</authors>
<title>CoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNotes.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning: Shared Task.</booktitle>
<contexts>
<context position="4502" citStr="Pradhan et al., 2011" startWordPosition="666" endWordPosition="669">ome the bias against making coreference links. Capturing semantic information in this shallow way is an “uphill battle” due to this structural property of coreference resolution. Nevertheless, using a simple architecture and feature set, our final system outperforms the two best publicly available English coreference systems, the Stanford system (Lee et al., 2011) and the IMS system (Bj¨orkelund and Farkas, 2012), by wide margins: 3.5% absolute and 1.9% absolute, respectively, on the CoNLL metric. 2 Experimental Setup Throughout this work, we use the datasets from the CoNLL 2011 shared task2 (Pradhan et al., 2011), which is derived from the OntoNotes corpus (Hovy et al., 2006). When applicable, we use the standard automatic parses and NER tags for each document. All experiments use system mentions except where otherwise indicated. For each experiment, we report MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average, the CoNLL metric. All metrics are computed using version 5 of the official CoNLL scorer.3 3 A Mention-Synchronous Framework We first present the basic architecture of our coreference system, independent of a feature set. Unlike binary classi</context>
</contexts>
<marker>Pradhan, Ramshaw, Marcus, Palmer, Weischedel, Xue, 2011</marker>
<rawString>Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha Palmer, Ralph Weischedel, and Nianwen Xue. 2011. CoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNotes. In Proceedings of the Conference on Computational Natural Language Learning: Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Alessandro Moschitti</author>
<author>Nianwen Xue</author>
<author>Olga Uryupina</author>
<author>Yuchen Zhang</author>
</authors>
<title>CoNLL2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes.</title>
<date>2012</date>
<booktitle>In Joint Conference on EMNLP and CoNLL - Shared Task.</booktitle>
<contexts>
<context position="33796" citStr="Pradhan et al. (2012)" startWordPosition="5514" endWordPosition="5517">have appeared in other coreference systems (Bj¨orkelund and Nugues, 2011; Rahman and Ng, 2011b; Fernandes et al., 2012). However, other authors have often emphasized bilexical features on head pairs, whereas our features are heavily monolexical. For feature conjunctions, other authors have exploited three classes (Lee et al., 2011) or automatically learned conjunction schemes (Fernandes et al., 2012; Lassalle and Denis, 2013), but to our knowledge we are the first to do fine-grained modeling of every pronoun. Inclusion of a hierarchy of 10Discrepancies between scores here and those printed in Pradhan et al. (2012) arise from two sources: improvements to the system of Lee et al. (2011) since the first CoNLL shared task, and a fix to the scoring of B3 in the official scorer since results of the two CoNLL shared tasks were released. Unfortunately, because of this bug in the scoring program, direct comparison to the printed results of the other highest-scoring English systems, Fernandes et al. (2012) and Martschat et al. (2012), is impossible. Feature name Count Features of the SURFACE system 418704 Features on the current mention [ANAPHORIC] + [CURRENT ANCESTRY] 46047 Features on the antecedent [ANTECEDEN</context>
</contexts>
<marker>Pradhan, Moschitti, Xue, Uryupina, Zhang, 2012</marker>
<rawString>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. CoNLL2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes. In Joint Conference on EMNLP and CoNLL - Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Supervised Models for Coreference Resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="6167" citStr="Rahman and Ng, 2009" startWordPosition="944" endWordPosition="947">scribed in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAF, is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention-synchronous framework, features examine single mentions to evaluate whether or not they are anaphoric and pairs of mentions to evaluate whether or not they corefer. While other work has used this framework as a starting point for entity-level systems (Luo et al., 2004; Rahman and Ng, 2009; Haghighi and Klein, 2010; Durrett et al., 2013), we will show that a mention-synchronous approach is sufficient to get state-of-the-art performance on its own. 3.1 Mention Detection Our system first identifies a set of predicted mentions from text annotated with parses and named entity tags. We extract three distinct types of mentions: proper mentions from all named entity chunks except for those labeled as QUANTITY, CARDINAL, or PERCENT, pronominal mentions from single words tagged with PRP or PRP$, and nominal mentions from all other maximal NP projections. These basic rules are similar to</context>
<context position="35848" citStr="Rahman and Ng, 2009" startWordPosition="5840" endWordPosition="5843">ly get distinctions among different mention types without having to choose a level of granularity a priori, unlike the distinct classifiers employed by Denis and Baldridge (2008). In terms of architecture, many coreference systems operate in a pipelined fashion, making partial decisions about coreference or pruning arcs before full resolution. Some systems use separate rule-based and learning-based passes (Chen and Ng, 2012; Fernandes et al., 2012), a series of learning-based passes (Bj¨orkelund and Farkas, 2012), or referentiality classifiers that prune the set of mentions before resolution (Rahman and Ng, 2009; Bj¨orkelund and Farkas, 2012; Recasens et al., 2013b). By contrast, our system resolves all mentions in one pass and does not need pruning: the SURFACE system can train in less than two hours without any subsampling of coreference arcs, and rule-based pruning of coreference arcs actually causes our system to perform less well, since our features can learn valuable information from these negative examples. Obama L VBD ROOT 1979 Prec. MUC F1 Prec. B3 F1 Prec. CEAFe F1 Avg. Rec. Rec. Rec. F1 CoNLL 2011 Development Set STANFORD 61.62 59.34 60.46 74.05 58.70 65.48 45.98 48.22 47.07 57.67 IMS 66.6</context>
</contexts>
<marker>Rahman, Ng, 2009</marker>
<rawString>Altaf Rahman and Vincent Ng. 2009. Supervised Models for Coreference Resolution. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Coreference Resolution with World Knowledge.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics: Human Language Technologies.</booktitle>
<contexts>
<context position="1871" citStr="Rahman and Ng, 2011" startWordPosition="261" endWordPosition="264">), the best publicly available English coreference system) by 1.9% absolute. 1 Introduction Coreference resolution is a multi-faceted task: humans resolve references by exploiting contextual and grammatical clues, as well as semantic information and world knowledge, so capturing each of 1The Berkeley Coreference Resolution System is available athttp://nlp.cs.berkeley.edu. these will be necessary for an automatic system to fully solve the problem. Acknowledging this complexity, coreference systems, either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010; Rahman and Ng, 2011b) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011), draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many heterogenous parts that can be difficult to interpret or modify. We build a learning-based, mention-synchronous coreference system that aims to use the simplest possible set of features to tackle the various aspects of coreference resolution. Though they arise from a small number of simple templates, our features are numerous,</context>
<context position="23216" citStr="Rahman and Ng (2011" startWordPosition="3766" endWordPosition="3769">arting new entities are outwardly similar in that they have no antecedents, and the prediction should be the same in either case (NEW), we treat them as distinct because the factors that impact the coreference decision differ in the two cases. Mentions that start new clusters are semantically similar to anaphoric mentions, but may be marked by heaviness or by a tendency to be named entities, whereas singletons may be generic or temporal NPs which might be thought of as coreferent in a loose sense, but are not 6This method of analysis is similar to that undertaken in Stoyanov et al. (2009) and Rahman and Ng (2011b), though we split our mentions along different axes, and can simply evaluate on accuracy because our decisions do not directly imply multiple links, as they do in binary classification-based systems (Stoyanov et al., 2009) or in entity-mention models (Rahman and Ng, 2011b). 1976 Nominal/Proper Pronominal 1st w/head 2nd+ w/head Singleton 99.7% 18.1K 85.5% 7.3K 66.5% 1.7K Starts Entity 98.7% 2.1K 78.9% 0.7K 48.5% 0.3K Anaphoric 7.9% 0.9K 75.5% 3.9K 72.0% 4.4K Table 4: Analysis of our SURFACE system on the development set. We characterize each predicted mention by its status in the gold standar</context>
<context position="30881" citStr="Rahman and Ng, 2011" startWordPosition="5038" endWordPosition="5041">absolute improvement and one that comes at the expense of performance on most other resolution class. The one class that does not get worse, singleton pronouns, actually improves by a similar 4% margin, indicating that roughly half of the gains we observe are not even necessarily a result of our features doing what they were designed to do. Our weak cues do yield some small gains, so there is hope that better weak indicators of semantic compatibility could prove more useful. However, while extremely high-precision approaches with carefully engineered features have been shown to be successful (Rahman and Ng, 2011a; Bansal and Klein, 2012; Recasens et al., 2013a), we conclude that capturing semantics in a data-driven, shallow manner remains an uphill battle. 6 FINAL System and Results While semantic features ended up giving only marginal benefit, we have demonstrated that nevertheless our SURFACE system is a state-of-the-art English coreference system. However, there remain a few natural features that we omitted in order to keep the system as simple as possible, since they were orthogonal to the discussion of data-driven versus heuristic-driven features and do not target world knowledge. Before giving </context>
<context position="33268" citStr="Rahman and Ng, 2011" startWordPosition="5432" endWordPosition="5435">an therefore improve pronoun resolution substantially; however, these features generally only improve pronoun resolution. Full results for our SURFACE and FINAL feature sets are shown in Table 7. Again, we compare to Lee et al. (2011) and Bj¨orkelund and Farkas (2012).10 Despite our system’s emphasis on one-pass resolution with as simple a feature set as possible, we are able to outperform even these sophisticated systems by a wide margin. 7 Related Work Many of the individual features we employ in the FINAL feature set have appeared in other coreference systems (Bj¨orkelund and Nugues, 2011; Rahman and Ng, 2011b; Fernandes et al., 2012). However, other authors have often emphasized bilexical features on head pairs, whereas our features are heavily monolexical. For feature conjunctions, other authors have exploited three classes (Lee et al., 2011) or automatically learned conjunction schemes (Fernandes et al., 2012; Lassalle and Denis, 2013), but to our knowledge we are the first to do fine-grained modeling of every pronoun. Inclusion of a hierarchy of 10Discrepancies between scores here and those printed in Pradhan et al. (2012) arise from two sources: improvements to the system of Lee et al. (2011)</context>
</contexts>
<marker>Rahman, Ng, 2011</marker>
<rawString>Altaf Rahman and Vincent Ng. 2011a. Coreference Resolution with World Knowledge. In Proceedings of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Narrowing the Modeling Gap: A Cluster-Ranking Approach to Coreference Resolution.</title>
<date>2011</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="1871" citStr="Rahman and Ng, 2011" startWordPosition="261" endWordPosition="264">), the best publicly available English coreference system) by 1.9% absolute. 1 Introduction Coreference resolution is a multi-faceted task: humans resolve references by exploiting contextual and grammatical clues, as well as semantic information and world knowledge, so capturing each of 1The Berkeley Coreference Resolution System is available athttp://nlp.cs.berkeley.edu. these will be necessary for an automatic system to fully solve the problem. Acknowledging this complexity, coreference systems, either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010; Rahman and Ng, 2011b) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011), draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many heterogenous parts that can be difficult to interpret or modify. We build a learning-based, mention-synchronous coreference system that aims to use the simplest possible set of features to tackle the various aspects of coreference resolution. Though they arise from a small number of simple templates, our features are numerous,</context>
<context position="23216" citStr="Rahman and Ng (2011" startWordPosition="3766" endWordPosition="3769">arting new entities are outwardly similar in that they have no antecedents, and the prediction should be the same in either case (NEW), we treat them as distinct because the factors that impact the coreference decision differ in the two cases. Mentions that start new clusters are semantically similar to anaphoric mentions, but may be marked by heaviness or by a tendency to be named entities, whereas singletons may be generic or temporal NPs which might be thought of as coreferent in a loose sense, but are not 6This method of analysis is similar to that undertaken in Stoyanov et al. (2009) and Rahman and Ng (2011b), though we split our mentions along different axes, and can simply evaluate on accuracy because our decisions do not directly imply multiple links, as they do in binary classification-based systems (Stoyanov et al., 2009) or in entity-mention models (Rahman and Ng, 2011b). 1976 Nominal/Proper Pronominal 1st w/head 2nd+ w/head Singleton 99.7% 18.1K 85.5% 7.3K 66.5% 1.7K Starts Entity 98.7% 2.1K 78.9% 0.7K 48.5% 0.3K Anaphoric 7.9% 0.9K 75.5% 3.9K 72.0% 4.4K Table 4: Analysis of our SURFACE system on the development set. We characterize each predicted mention by its status in the gold standar</context>
<context position="30881" citStr="Rahman and Ng, 2011" startWordPosition="5038" endWordPosition="5041">absolute improvement and one that comes at the expense of performance on most other resolution class. The one class that does not get worse, singleton pronouns, actually improves by a similar 4% margin, indicating that roughly half of the gains we observe are not even necessarily a result of our features doing what they were designed to do. Our weak cues do yield some small gains, so there is hope that better weak indicators of semantic compatibility could prove more useful. However, while extremely high-precision approaches with carefully engineered features have been shown to be successful (Rahman and Ng, 2011a; Bansal and Klein, 2012; Recasens et al., 2013a), we conclude that capturing semantics in a data-driven, shallow manner remains an uphill battle. 6 FINAL System and Results While semantic features ended up giving only marginal benefit, we have demonstrated that nevertheless our SURFACE system is a state-of-the-art English coreference system. However, there remain a few natural features that we omitted in order to keep the system as simple as possible, since they were orthogonal to the discussion of data-driven versus heuristic-driven features and do not target world knowledge. Before giving </context>
<context position="33268" citStr="Rahman and Ng, 2011" startWordPosition="5432" endWordPosition="5435">an therefore improve pronoun resolution substantially; however, these features generally only improve pronoun resolution. Full results for our SURFACE and FINAL feature sets are shown in Table 7. Again, we compare to Lee et al. (2011) and Bj¨orkelund and Farkas (2012).10 Despite our system’s emphasis on one-pass resolution with as simple a feature set as possible, we are able to outperform even these sophisticated systems by a wide margin. 7 Related Work Many of the individual features we employ in the FINAL feature set have appeared in other coreference systems (Bj¨orkelund and Nugues, 2011; Rahman and Ng, 2011b; Fernandes et al., 2012). However, other authors have often emphasized bilexical features on head pairs, whereas our features are heavily monolexical. For feature conjunctions, other authors have exploited three classes (Lee et al., 2011) or automatically learned conjunction schemes (Fernandes et al., 2012; Lassalle and Denis, 2013), but to our knowledge we are the first to do fine-grained modeling of every pronoun. Inclusion of a hierarchy of 10Discrepancies between scores here and those printed in Pradhan et al. (2012) arise from two sources: improvements to the system of Lee et al. (2011)</context>
</contexts>
<marker>Rahman, Ng, 2011</marker>
<rawString>Altaf Rahman and Vincent Ng. 2011b. Narrowing the Modeling Gap: A Cluster-Ranking Approach to Coreference Resolution. Journal of Artificial Intelligence Research, 40(1):469–521, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Matthew Can</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Same Referent, Different Words: Unsupervised Mining of Opaque Coreferent Mentions.</title>
<date>2013</date>
<booktitle>In Proceedings of the North American Chapter of the Association</booktitle>
<contexts>
<context position="30929" citStr="Recasens et al., 2013" startWordPosition="5046" endWordPosition="5049"> expense of performance on most other resolution class. The one class that does not get worse, singleton pronouns, actually improves by a similar 4% margin, indicating that roughly half of the gains we observe are not even necessarily a result of our features doing what they were designed to do. Our weak cues do yield some small gains, so there is hope that better weak indicators of semantic compatibility could prove more useful. However, while extremely high-precision approaches with carefully engineered features have been shown to be successful (Rahman and Ng, 2011a; Bansal and Klein, 2012; Recasens et al., 2013a), we conclude that capturing semantics in a data-driven, shallow manner remains an uphill battle. 6 FINAL System and Results While semantic features ended up giving only marginal benefit, we have demonstrated that nevertheless our SURFACE system is a state-of-the-art English coreference system. However, there remain a few natural features that we omitted in order to keep the system as simple as possible, since they were orthogonal to the discussion of data-driven versus heuristic-driven features and do not target world knowledge. Before giving final results, we will present a small set of ad</context>
<context position="35901" citStr="Recasens et al., 2013" startWordPosition="5848" endWordPosition="5851">ithout having to choose a level of granularity a priori, unlike the distinct classifiers employed by Denis and Baldridge (2008). In terms of architecture, many coreference systems operate in a pipelined fashion, making partial decisions about coreference or pruning arcs before full resolution. Some systems use separate rule-based and learning-based passes (Chen and Ng, 2012; Fernandes et al., 2012), a series of learning-based passes (Bj¨orkelund and Farkas, 2012), or referentiality classifiers that prune the set of mentions before resolution (Rahman and Ng, 2009; Bj¨orkelund and Farkas, 2012; Recasens et al., 2013b). By contrast, our system resolves all mentions in one pass and does not need pruning: the SURFACE system can train in less than two hours without any subsampling of coreference arcs, and rule-based pruning of coreference arcs actually causes our system to perform less well, since our features can learn valuable information from these negative examples. Obama L VBD ROOT 1979 Prec. MUC F1 Prec. B3 F1 Prec. CEAFe F1 Avg. Rec. Rec. Rec. F1 CoNLL 2011 Development Set STANFORD 61.62 59.34 60.46 74.05 58.70 65.48 45.98 48.22 47.07 57.67 IMS 66.67 58.20 62.15 77.60 56.77 65.57 42.92 51.11 46.66 58.</context>
</contexts>
<marker>Recasens, Can, Jurafsky, 2013</marker>
<rawString>Marta Recasens, Matthew Can, and Daniel Jurafsky. 2013a. Same Referent, Different Words: Unsupervised Mining of Opaque Coreferent Mentions. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher Potts</author>
</authors>
<title>The Life and Death of Discourse Entities: Identifying Singleton Mentions.</title>
<date>2013</date>
<booktitle>In Proceedings of the North American Chapter of</booktitle>
<marker>Recasens, de Marneffe, Potts, 2013</marker>
<rawString>Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts. 2013b. The Life and Death of Discourse Entities: Identifying Singleton Mentions. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A Machine Learning Approach to Coreference Resolution of Noun Phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="5215" citStr="Soon et al., 2001" startWordPosition="779" endWordPosition="782">andard automatic parses and NER tags for each document. All experiments use system mentions except where otherwise indicated. For each experiment, we report MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average, the CoNLL metric. All metrics are computed using version 5 of the official CoNLL scorer.3 3 A Mention-Synchronous Framework We first present the basic architecture of our coreference system, independent of a feature set. Unlike binary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al., 2001; Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2This dataset is identical to the English portion of the CoNLL 2012 data, except for the absence of a small pivot text. 3Note that this version of the scorer implements a modified version of B3, described in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAF, is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. each mention or determine that it begins a new cluster </context>
<context position="26320" citStr="Soon et al., 2001" startWordPosition="4280" endWordPosition="4283">reviously as part of a different cluster; we only resolve 1.7% of can only be correctly resolved by exploiting world knowledge, so we will need to include features that capture this knowledge in some fashion. 5.2 Incorporating Shallow Semantics As we were able to incorporate syntax with shallow features, so too might we hope to incorporate semantics. However, the semantic information contained even in a coreference corpus of thousands of documents is insufficient to generalize to unseen data,8 so system designers have turned to external resources such as semantic classes derived from WordNet (Soon et al., 2001), WordNet hypernymy or synonymy (Stoyanov et al., 2010), semantic similarity computed from online resources (Ponzetto and Strube, 2006), named entity type features, gender and number match using the dataset of Bergsma and Lin (2006), and features from unsupervised clusters (Hendrickx and Daelemans, 2007; Durrett et al., 2013). In this section, we consider the following subset of these information sources: • WordNet hypernymy and synonymy • Number and gender data for nominals and propers from Bergsma and Lin (2006) • Named entity types • Latent clusters computed from English Gigaword (Graff et </context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A Machine Learning Approach to Coreference Resolution of Noun Phrases. Computational Linguistics, 27(4):521–544, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Nathan Gilbert</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
</authors>
<title>Conundrums in Noun Phrase Coreference Resolution: Making Sense of the Stateof-the-Art.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="23192" citStr="Stoyanov et al. (2009)" startWordPosition="3761" endWordPosition="3764"> singletons and mentions starting new entities are outwardly similar in that they have no antecedents, and the prediction should be the same in either case (NEW), we treat them as distinct because the factors that impact the coreference decision differ in the two cases. Mentions that start new clusters are semantically similar to anaphoric mentions, but may be marked by heaviness or by a tendency to be named entities, whereas singletons may be generic or temporal NPs which might be thought of as coreferent in a loose sense, but are not 6This method of analysis is similar to that undertaken in Stoyanov et al. (2009) and Rahman and Ng (2011b), though we split our mentions along different axes, and can simply evaluate on accuracy because our decisions do not directly imply multiple links, as they do in binary classification-based systems (Stoyanov et al., 2009) or in entity-mention models (Rahman and Ng, 2011b). 1976 Nominal/Proper Pronominal 1st w/head 2nd+ w/head Singleton 99.7% 18.1K 85.5% 7.3K 66.5% 1.7K Starts Entity 98.7% 2.1K 78.9% 0.7K 48.5% 0.3K Anaphoric 7.9% 0.9K 75.5% 3.9K 72.0% 4.4K Table 4: Analysis of our SURFACE system on the development set. We characterize each predicted mention by its st</context>
</contexts>
<marker>Stoyanov, Gilbert, Cardie, Riloff, 2009</marker>
<rawString>Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and Ellen Riloff. 2009. Conundrums in Noun Phrase Coreference Resolution: Making Sense of the Stateof-the-Art. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Claire Cardie</author>
<author>Nathan Gilbert</author>
<author>Ellen Riloff</author>
<author>David Buttler</author>
<author>David Hysom</author>
</authors>
<title>Coreference Resolution with Reconcile.</title>
<date>2010</date>
<booktitle>In Proceedings of the Association for Computational Linguistics: Short Papers.</booktitle>
<contexts>
<context position="1824" citStr="Stoyanov et al., 2010" startWordPosition="253" endWordPosition="256">orms the IMS system (Bj¨orkelund and Farkas (2012), the best publicly available English coreference system) by 1.9% absolute. 1 Introduction Coreference resolution is a multi-faceted task: humans resolve references by exploiting contextual and grammatical clues, as well as semantic information and world knowledge, so capturing each of 1The Berkeley Coreference Resolution System is available athttp://nlp.cs.berkeley.edu. these will be necessary for an automatic system to fully solve the problem. Acknowledging this complexity, coreference systems, either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010; Rahman and Ng, 2011b) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011), draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many heterogenous parts that can be difficult to interpret or modify. We build a learning-based, mention-synchronous coreference system that aims to use the simplest possible set of features to tackle the various aspects of coreference resolution. Though they arise from a small number </context>
<context position="5286" citStr="Stoyanov et al., 2010" startWordPosition="791" endWordPosition="794">ments use system mentions except where otherwise indicated. For each experiment, we report MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average, the CoNLL metric. All metrics are computed using version 5 of the official CoNLL scorer.3 3 A Mention-Synchronous Framework We first present the basic architecture of our coreference system, independent of a feature set. Unlike binary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al., 2001; Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2This dataset is identical to the English portion of the CoNLL 2012 data, except for the absence of a small pivot text. 3Note that this version of the scorer implements a modified version of B3, described in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAF, is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionranking or mention-synchron</context>
<context position="17795" citStr="Stoyanov et al., 2010" startWordPosition="2894" endWordPosition="2897">e English coreference systems. Bolded values are significant with p &lt; 0.05 according to a bootstrap resampling test. ence features, just implicitly. For example, rather than having rules targeting person, number, gender, or animacy of mentions, we use conjunctions with pronoun identity, which contains this information. Rather than explicitly writing a feature targeting definiteness, our indicators on the first word of a mention will capture this and other effects. And finally, rather than targeting centering theory (Grosz et al., 1995) with rule-based features identifying syntactic positions (Stoyanov et al., 2010; Haghighi and Klein, 2010), our features on word context can identify configurational clues like whether a mention is preceded or followed by a verb, and therefore whether it is likely in subject or object position.5 Not only are data-driven features able to capture the same phenomena as heuristic-driven features, but they do so at a finer level of granularity, and can therefore model more patterns in the data. To contrast these two types of features, we experiment with three ablated versions of our system, where we replace data-driven features with their heuristic-driven counterparts: 1. Ins</context>
<context position="26375" citStr="Stoyanov et al., 2010" startWordPosition="4288" endWordPosition="4291">esolve 1.7% of can only be correctly resolved by exploiting world knowledge, so we will need to include features that capture this knowledge in some fashion. 5.2 Incorporating Shallow Semantics As we were able to incorporate syntax with shallow features, so too might we hope to incorporate semantics. However, the semantic information contained even in a coreference corpus of thousands of documents is insufficient to generalize to unseen data,8 so system designers have turned to external resources such as semantic classes derived from WordNet (Soon et al., 2001), WordNet hypernymy or synonymy (Stoyanov et al., 2010), semantic similarity computed from online resources (Ponzetto and Strube, 2006), named entity type features, gender and number match using the dataset of Bergsma and Lin (2006), and features from unsupervised clusters (Hendrickx and Daelemans, 2007; Durrett et al., 2013). In this section, we consider the following subset of these information sources: • WordNet hypernymy and synonymy • Number and gender data for nominals and propers from Bergsma and Lin (2006) • Named entity types • Latent clusters computed from English Gigaword (Graff et al., 2007), where a latent cluster label generates each</context>
</contexts>
<marker>Stoyanov, Cardie, Gilbert, Riloff, Buttler, Hysom, 2010</marker>
<rawString>Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom. 2010. Coreference Resolution with Reconcile. In Proceedings of the Association for Computational Linguistics: Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannick Versley</author>
<author>Simone Paolo Ponzetto</author>
<author>Massimo Poesio</author>
<author>Vladimir Eidelman</author>
<author>Alan Jern</author>
<author>Jason Smith</author>
</authors>
<title>Xiaofeng Yang, and Alessandro Moschitti.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics: Demo Session.</booktitle>
<contexts>
<context position="5262" citStr="Versley et al., 2008" startWordPosition="787" endWordPosition="790">h document. All experiments use system mentions except where otherwise indicated. For each experiment, we report MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average, the CoNLL metric. All metrics are computed using version 5 of the official CoNLL scorer.3 3 A Mention-Synchronous Framework We first present the basic architecture of our coreference system, independent of a feature set. Unlike binary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al., 2001; Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2This dataset is identical to the English portion of the CoNLL 2012 data, except for the absence of a small pivot text. 3Note that this version of the scorer implements a modified version of B3, described in Cai and Strube (2010), that was used for the CoNLL shared tasks. The implementation of CEAF, is also not exactly as described in Luo et al. (2004), but for completeness we include this metric as well. each mention or determine that it begins a new cluster (Denis and Baldridge, 2008). In this mentionran</context>
</contexts>
<marker>Versley, Ponzetto, Poesio, Eidelman, Jern, Smith, 2008</marker>
<rawString>Yannick Versley, Simone Paolo Ponzetto, Massimo Poesio, Vladimir Eidelman, Alan Jern, Jason Smith, Xiaofeng Yang, and Alessandro Moschitti. 2008. BART: A Modular Toolkit for Coreference Resolution. In Proceedings of the Association for Computational Linguistics: Demo Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A ModelTheoretic Coreference Scoring Scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the Conference on Message Understanding.</booktitle>
<contexts>
<context position="4780" citStr="Vilain et al., 1995" startWordPosition="710" endWordPosition="713"> publicly available English coreference systems, the Stanford system (Lee et al., 2011) and the IMS system (Bj¨orkelund and Farkas, 2012), by wide margins: 3.5% absolute and 1.9% absolute, respectively, on the CoNLL metric. 2 Experimental Setup Throughout this work, we use the datasets from the CoNLL 2011 shared task2 (Pradhan et al., 2011), which is derived from the OntoNotes corpus (Hovy et al., 2006). When applicable, we use the standard automatic parses and NER tags for each document. All experiments use system mentions except where otherwise indicated. For each experiment, we report MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005), as well as their average, the CoNLL metric. All metrics are computed using version 5 of the official CoNLL scorer.3 3 A Mention-Synchronous Framework We first present the basic architecture of our coreference system, independent of a feature set. Unlike binary classification-based coreference systems where independent binary decisions are made about each pair (Soon et al., 2001; Bengtson and Roth, 2008; Versley et al., 2008; Stoyanov et al., 2010), we use a log-linear model to select at most one antecedent for 2This dataset is identical to</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A ModelTheoretic Coreference Scoring Scheme. In Proceedings of the Conference on Message Understanding.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
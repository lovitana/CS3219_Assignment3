<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006589">
<title confidence="0.877883">
Learning to rank lexical substitutions
</title>
<author confidence="0.805712">
Gy¨orgy Szarvas1 R´obert Busa-Fekete2 Eyke H¨ullermeier
</author>
<affiliation confidence="0.721946">
Amazon Inc. University of Marburg
</affiliation>
<address confidence="0.317324">
szarvasg@amazon.com Hans-Meerwein-Str., 35032 Marburg, Germany
</address>
<email confidence="0.6266755">
busarobi@mathematik.uni-marburg.de
eyke@mathematik.uni-marburg.de
</email>
<sectionHeader confidence="0.987771" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998636875">
The problem to replace a word with a syn-
onym that fits well in its sentential context
is known as the lexical substitution task. In
this paper, we tackle this task as a supervised
ranking problem. Given a dataset of target
words, their sentential contexts and the poten-
tial substitutions for the target words, the goal
is to train a model that accurately ranks the
candidate substitutions based on their contex-
tual fitness. As a key contribution, we cus-
tomize and evaluate several learning-to-rank
models to the lexical substitution task, includ-
ing classification-based and regression-based
approaches. On two datasets widely used for
lexical substitution, our best models signifi-
cantly advance the state-of-the-art.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99944175">
The task to generate lexical substitutions in context
(McCarthy and Navigli, 2007), i.e., to replace words
in a sentence without changing its meaning, has be-
come an increasingly popular research topic. This
task is used, e.g. to evaluate semantic models with
regard to their accuracy in modeling word meaning
in context (Erk and Pad´o, 2010). Moreover, it pro-
vides a basis of NLP applications in many fields,
including linguistic steganography (Topkara et al.,
2006; Chang and Clark, 2010), semantic text simi-
larity (Agirre et al., 2012) and plagiarism detection
(Gipp et al., 2011). While closely related to WSD,
</bodyText>
<footnote confidence="0.6372196">
1Work was done while working at RGAI of the Hungarian
Acad. Sci. and University of Szeged.
2 R. Busa-Fekete is on leave from the Research Group on
Artificial Intelligence of the Hungarian Academy of Sciences
and University of Szeged.
</footnote>
<bodyText confidence="0.999844303030303">
lexical substitution does not rely on explicitly de-
fined sense inventories (Dagan et al., 2006): the pos-
sible substitutions reflect all conceivable senses of
the word, and the correct sense has to be ascertained
to provide an accurate substitution.
While a few lexical sample datasets (McCarthy
and Navigli, 2007; Biemann, 2012) with human-
provided substitutions exist and can be used to
evaluate different lexical paraphrasing approaches,
a practically useful system must also be able to
rephrase unseen words, i.e., any word for which
a list of synonyms is provided. Correspondingly,
unsupervised and knowledge-based approaches that
are not directly dependent on any training material,
prevailed in the SemEval 2007 shared task on En-
glish Lexical Substitution and dominated follow-up
work. The only supervised approach is limited to
the combination of several knowledge-based lexi-
cal substitution models based on different underly-
ing lexicons (Sinha and Mihalcea, 2009).3
A recent work by Szarvas et al. (2013) de-
scribes a tailor-made supervised system based on
delexicalized features that – unlike earlier super-
vised approaches, and similar to unsupervised and
knowledge-based methods proposed for this task –
is able to generalize to an open vocabulary. For
each target word to paraphrase, they first compute
a set of substitution candidates using WordNet: all
synonyms from all of the target word’s WordNet
synsets, together with the words from synsets in
similar to, entailment and also see relation to these
synsets are considered as potential substitutions.
Each candidate then constitutes a training (or test)
</bodyText>
<footnote confidence="0.898109666666667">
3Another notable example for supervised lexical substitu-
tion is Biemann (2012), but this is a lexical sample system ap-
plicable only to the target words of the training datasets.
</footnote>
<page confidence="0.895311">
1926
</page>
<bodyText confidence="0.9734855">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1926–1932,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
example, and these instances are characterized using
non-lexical features from heterogeneous evidence
such as lexical-semantic resources and distributional
similarity, n-gram counts and shallow syntactic fea-
tures computed on large, unannotated background
corpora. The goal is then i) to predict how well
a particular candidate fits in the original context,
and ii) given these predictions for each of the can-
didates, to correctly order the elements of the candi-
date set according to their contextual fitness. That is,
a model is successful if it prioritizes plausible substi-
tutions ahead of less likely synonyms (given the con-
text). This model is able to generate paraphrases for
target words not contained in the training material.
This favorable property is achieved using only such
features (e.g. local n-gram frequencies in context)
that are meaningfully comparable across the differ-
ent target words and candidate substitutions they are
computed from. More importantly, their model also
provides superior ranking results compared to state
of the art unsupervised and knowledge based ap-
proaches and therefore it defines the current state of
the art for open vocabulary lexical substitution.
Motivated by the findings of Szarvas et al. (2013),
we address lexical substitution as a supervised learn-
ing problem, and go beyond their approach from
a methodological point of view. Our experiments
show that the performance on the lexical substitution
task is strongly influenced by the way in which this
task is formalized as a machine learning problem
(i.e., as binary or multi-class classification or regres-
sion) and by the learning method used to solve this
problem. As a result, we are able to report the best
performances on this task for two standard datasets.
</bodyText>
<sectionHeader confidence="0.999621" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999775916666667">
Previous approaches to lexical substitution often
seek to automatically generate a set of candidate
substitutions for each target word first, and to rank
the elements of this set of candidates afterward (Has-
san et al., 2007; Giuliano et al., 2007; Martinez et
al., 2007; Yuret, 2007). Alternatively, the candidate
set can be defined by all human-suggested substi-
tutions for the given target word in all of its con-
texts; then, the focus is just on the ranking problem
(Erk and Pad´o, 2010; Thater et al., 2010; Dinu and
Lapata, 2010; Thater et al., 2011). While only the
former approach qualifies as a full-fledged substitu-
tion system for arbitrary, previously unseen target
words, the latter simplifies the comparison of se-
mantic ranking models, as the ranking step is not
burdened with the shortcomings of automatically
generated substitution candidates.
As mentioned before, Szarvas et al. (2013) re-
cently formalized the lexical substitution problem as
a supervised learning task, using delexicalized fea-
tures. This non-lexical feature representation makes
different target word/substitution pairs in different
contexts4 directly comparable. Thus, it becomes
possible to learn an all-words system that is appli-
cable to unseen words, using supervised methods,
which provides superior ranking accuracy to unsu-
pervised and knowledge based models.
In this work, we build on the problem formu-
lation and the features proposed by Szarvas et
al. (2013) while largely extending their machine
learning methodology. We customize and experi-
ment with several different learning-to-rank models,
which are better tailored for this task. As our experi-
ments show, this contribution leads to further signif-
icant improvements in modeling the semantics of a
text and in end-system accuracy.
</bodyText>
<sectionHeader confidence="0.989729" genericHeader="method">
3 Datasets and experimental setup
</sectionHeader>
<bodyText confidence="0.993318842105263">
Here we introduce the datasets, experimental setup
and evaluation measures used in our experiments.
Since space restrictions prohibit a comprehensive
exposition, we only provide the most essential in-
formation and refer to Szarvas et al. (2013), whose
experimental setup we adopted, for further details.
Datasets. We use two prominent datasets for lex-
ical substitution. The LexSub dataset introduced
in the Lexical Substitution task at Semeval 2007
(McCarthy and Navigli, 2007)5 contains 2002 sen-
tences for a total of 201 target words (from all
parts of speech), and lexical substitutions assigned
(to each target word and sentence pair) by 5 na-
tive speaker annotators. The second dataset, TWSI
(Biemann, 2012)6, consists of 24,647 sentences for
a total of 1,012 target nouns, and lexical substitu-
4E.g., bright substituted with intelligent in “He was bright
and independent and proud” and side for part in “Find someone
who can compose the biblical side”.
</bodyText>
<footnote confidence="0.995416">
5http://nlp.cs.swarthmore.edu/
semeval/tasks/task10/data.shtml
6http://www.ukp.tu-darmstadt.de/data/
lexical-resources/twsi-lexical-substitutions/
</footnote>
<page confidence="0.994746">
1927
</page>
<bodyText confidence="0.998816722891567">
tions for each target word in context resulting from
a crowdsourced annotation process.
For each sentence in each dataset, the annotators
provided as many substitutions for the target word
as they found appropriate in the context. Each sub-
stitution is then labeled by the number of annotators
who listed that word as a good lexical substitution.
Experimental setup and Evaluation. On both
datasets, we conduct experiments using a 10-fold
cross validation process, and evaluate all learning al-
gorithms on the same train/test splits. The datasets
are randomly split into 10 equal-sized folds on the
target word level, such that all examples for a par-
ticular target word fall into either the training or
the test set, but never both. This way, we make
sure to evaluate the models on target words not seen
during training, thereby mimicking an open vocab-
ulary paraphrasing system: at testing time, para-
phrases are ranked for unseen target words, simi-
larly as the models would rank paraphrases for any
words (not necessarily contained in the dataset). For
algorithms with tunable parameters, we further di-
vide the training sets into a training and a validation
part to find the best parameter settings. For evalua-
tion, we use Generalized Average Precision (GAP)
(Kishida, 2005) and Precision at 1 (P@1), i.e., the
percentage of correct paraphrases at rank 1.
Features. In all experiments, we used the features
described in Szarvas et al. (2013), implemented pre-
cisely as proposed by the original work.
Each (sentence, target word, substitution)
triplet represents an instance, and the feature values
are computed from the sentence context, the target
word and the substitution word. The features used
fall into four major categories.
The most important features describe the syntag-
matic coherence of the substitution in context, mea-
sured as local n-gram frequencies obtained from
web data. The frequency for a 1-5gram context with
the substitution word is computed and normalized
with respect to either 1) the frequency of the origi-
nal context (with the target word) or 2) the sum of
frequencies observed for all possible substitutions.
A third feature computes similar frequencies for the
substitution and the target word observed in the lo-
cal context (as part of a conjunctive phrase).
A second group of features describe the (non-
positional, i.e. non-local) distributional similarity of
the target and its candidate substitution in terms
of sentence level co-occurrence statistics collected
from newspaper texts: 1) How many words from the
sentence appear in the top 1000 salient words listed
for the candidate substitution in a distributional the-
saurus, 2) how similar the top K salient words lists
are for the candidate and the target word, 3) how
similar the 2nd order distributional profiles are for
candidate and target, etc. All these features are care-
fully normalized so that values compare well accross
different words and contexts.
Another set of features capture the properties of
the target and candidate word in WordNet, such as
their 1) number of senses, 2) how frequent senses
are synonymous and 3) the lowest common ancestor
(and all synsets up) for the candidate and target word
in the WordNet hierarchy (represented as a nominal
feature, by the ID of these synsets).
Lastly a group of features capture shallow syntac-
tic patterns of the target word and its local context in
the form of 1) part of speech patterns (trigrams) in
a sliding window around the target word using main
POS categories, i.e. only the first letter of the Penn
Treebank codes, and 2) the detailed POS code of the
candidate word assigned by a POS tagger.
We omit a mathematically precise description of
these features for space reasons and refer the reader
to Szarvas et al. (2013) for a more formal and
detailed description of the feature functions. Im-
portantly, these delexicalized features are numeri-
cally comparable across the different target words
and candidate substitutions they are computed from.
This property enables the models to generalize over
the words in the datasets and thus enables a super-
vised, all-words lexical substitution system.
</bodyText>
<sectionHeader confidence="0.999962" genericHeader="method">
4 Learning-to-Rank methods
</sectionHeader>
<bodyText confidence="0.999608727272727">
Machine learning methods for ranking are tradition-
ally classified into three categories. In the point-
wise approach, a model is trained that maps in-
stances (in this case candidate substitutions in a con-
text) to scores indicating their relevance or fitness;
to this end, one typically applies standard regression
techniques, which essentially look at individual in-
stances in isolation (i.e., independent of any other
instances in the training or test set). To predict a
ranking of a set of query instances, these are sim-
ply sorted by their predicted scores (Li et al., 2007).
</bodyText>
<page confidence="0.978321">
1928
</page>
<bodyText confidence="0.99998075">
The pairwise approach trains models that are able
to compare pairs of instances. By marking such a
pair as positive if the first instance is preferred to the
second one, and as negative otherwise, the problem
can formally be reduced to a binary classification
task (Freund et al., 2003). Finally, in the listwise ap-
proach, tailor-made learning methods are used that
directly optimize the ranking performance with re-
spect to a global evaluation metric, i.e., a measure
that evaluates the ranking of a complete set of query
instances (Valizadegan et al., 2009).
Below we give a brief overview of the methods in-
cluded in our experiments. We used the implementa-
tions provided by the MultiBoost (Benbouzid et al.,
2012), RankSVM and RankLib packages.7 For a de-
tailed description, we refer to the original literature.
</bodyText>
<subsectionHeader confidence="0.746668">
4.1 MAXENT
</subsectionHeader>
<bodyText confidence="0.999998">
The ranking model proposed by Szarvas et al. (2013)
was used as a baseline. This is a pointwise approach
based on a maximum entropy classifier, in which
the ranking task is cast as a binary classification
problem, namely to discriminate good (label &gt; 0)
from bad substitutions. The actual label values for
good substitutions were used for weighting the train-
ing examples. The underlying MaxEnt model was
trained until convergence, i.e., there was no hyper-
parameter to be tuned. For a new target/substitution
pair, the classifier delivers an estimation of the pos-
terior probability for being a good substitution. The
ranking is then produced by sorting the candidates
in decreasing order according to this probability.
</bodyText>
<sectionHeader confidence="0.826033" genericHeader="method">
4.2 EXPENS
</sectionHeader>
<bodyText confidence="0.999729636363636">
EXPENS (Busa-Fekete et al., 2013) is a point-
wise method with listwise meta-learning step
that exploits an ensemble of multi-class classi-
fiers. It consists of three steps. First, AD-
ABOOST.MH (Schapire and Singer, 1999) classi-
fiers with several different weak learners (Busa-
Fekete et al., 2011; K´egl and Busa-Fekete, 2009)
are trained to predict the level of relevance (qual-
ity) of a substitution (i.e., the number of annotators
who proposed the candidate for that particular con-
text). Second, the classifiers are calibrated to obtain
</bodyText>
<footnote confidence="0.89252125">
7RankLib is available at http://people.cs.umass.
edu/˜vdang/ranklib.html. We extended the imple-
mentation of the LAMBDAMART algorithm in this package to
compute the gradients of and optimize for the GAP measure.
</footnote>
<bodyText confidence="0.999975533333333">
an accurate posterior distribution; to this end, several
calibration techniques, such as Platt scaling (Platt,
2000), are used to obtain a diverse pool of calibrated
classifiers. Note that this step takes advantage of
the ordinal structure of the underlying scale of rel-
evance levels, which is an important difference to
MAXENT. Third, the posteriors of these calibrated
classifiers are additively combined, with the weight
of each model being exponentially proportional to
its GAP score (on the validation set). This method
has two hyperparameters: the number of boosting it-
erations T and the scaling factor in the exponential
weighting scheme c. We select T and c from the in-
tervals [100, 2000] and [0, 100], with step sizes 100
and 10, respectively.
</bodyText>
<subsectionHeader confidence="0.948071">
4.3 RANKBOOST
</subsectionHeader>
<bodyText confidence="0.999944454545455">
RANKBOOST (Freund et al., 2003) is a pairwise
boosting approach. The objective function is the
rank loss (as opposed to ADABOOST, which opti-
mizes the exponential loss). In each boosting it-
eration, the weak classifier is chosen by maximiz-
ing the weighted rank loss. For the weak learner,
we used the decision stump described in (Freund et
al., 2003), which is able to optimize the rank loss
in an efficient way. The only hyperparameter of
RANKBOOST to be tuned is the number of iterations
that we selected from the interval [1, 1000].
</bodyText>
<subsectionHeader confidence="0.920696">
4.4 RANKSVM
</subsectionHeader>
<bodyText confidence="0.999978888888889">
RANKSVM (Joachims, 2006) is a pairwise method
based on support vector machines, which formulates
the ranking task as binary classification of pairs of
instances. We used a linear kernel, because the opti-
mization using non-linear kernels cannot be done in
a reasonable time. The tolerance level of the op-
timization was set to 0.001 and the regularization
parameter was validated in the interval [10−6,104]
with a logarithmically increasing step size.
</bodyText>
<subsectionHeader confidence="0.92944">
4.5 LAMBDAMART
</subsectionHeader>
<bodyText confidence="0.999904875">
LAMBDAMART (Wu et al., 2010) is a listwise
method based on the gradient boosted regression
trees by Friedman (1999). The ordinal labels are
learned directly by the boosted regression trees
whose parameters are tuned by using a gradient-
based optimization method. The gradient of parame-
ters is calculated based on the evaluation metric used
(in this case GAP). We tuned the number of boosting
</bodyText>
<page confidence="0.990614">
1929
</page>
<table confidence="0.999879928571429">
Database LexSub TWSI
Candidates WN Gold WN Gold
GAP
MaxEnt 43.8 52.4 36.6 47.2
ExpEns 44.3 53.5 37.8 49.7
RankBoost 44.0 51.4 37.0 47.8
RankSVM 43.3 51.8 35.5 45.2
LambdaMART 45.5 55.0 37.8 50.1
P@1
MaxEnt 40.2 57.7 32.4 49.5
ExpEns 39.8 58.5 33.8 53.2
RankBoost 40.7 55.2 33.1 50.8
RankSVM 40.3 51.7 33.2 45.1
LambdaMART 40.8 60.2 33.1 53.6
</table>
<tableCaption confidence="0.998714">
Table 1: GAP and p@1 values, with significant improve-
ments over the performance of MaxEnt marked in bold.
</tableCaption>
<table confidence="0.99966675">
System GAP
Erk and Pad´o (2010) 38.6
Dinu and Lapata (2010) 42.9
Thater et al. (2010) 46.0
Thater et al. (2011) 51.7
Szarvas et al. (2013) 52.4
EXPENS 53.5
LAMBDAMART 55.0
</table>
<tableCaption confidence="0.9922315">
Table 2: Comparison to previous studies (dataset LexSub,
candidates Gold).
</tableCaption>
<bodyText confidence="0.97408">
iterations in the interval [10,1000] and the number
of tree leaves in 18, 16, 321.
</bodyText>
<sectionHeader confidence="0.998108" genericHeader="evaluation">
5 Results and discussion
</sectionHeader>
<bodyText confidence="0.999987488372093">
Our results using the above learning methods are
summarized in Table 1. As can be seen, the two
methods that exploit the cardinal structure of the
label set (relevance degrees), namely EXPENS and
LAMBDAMART, consistently outperform the base-
line taken from Szarvas et al. (2013) – the only ex-
ception is the p@1 score for EXPENS on the Semeval
Lexical Substitution dataset and the candidate sub-
stitutions extracted from WordNet. The improve-
ments are significant (using paired t-test, p &lt; 0.01)
for 3 out of 4 settings for EXPENS and in all settings
for LAMBDAMART. In particular, the results of
LAMBDAMART are so far the best scores that have
been reported for the best studied setting, i.e. the
LexSub dataset using substitution candidates taken
from the gold standard (see Table 2).
We suppose that the relatively good results
achieved by the LAMBDAMART and EXPENS
methods are due to that, first, it seems crucial to
properly model and exploit the ordinal nature of
the annotations (number of annotators who sug-
gested a given word as a good paraphrase) pro-
vided by the datasets. Second, the RANKBOOST and
RANKSVM are less complex methods than the EX-
PENS and LAMBDAMART. The RANKSVM is the
least complex method from the pool of learning-to-
rank methods we applied, since it is a simple lin-
ear model. The RANKBOOST is a boosted decision
stump where, in each boosting iteration, the stump
is found by maximizing the weighted exponential
rank loss. On the other hand, both the EXPENS
and LAMBDAMART make use of tree learners in the
ensemble classifier they produce. We believe that
overfitting is not an issue in a learning task like the
LexSub task: most features are relatively weak pre-
dictors on their own, and we can learn from a large
number of data points (2000 sentences with an av-
erage set size of 20, about 40K data points for the
smallest dataset and setting). Rather, as our results
show, less complex models tend to underfit the data.
Therefore we believe that more complex models can
achieve a better performance, of course with an in-
creased computational cost.
</bodyText>
<sectionHeader confidence="0.947623" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999974647058824">
In this paper, we customized and applied some rela-
tively novel algorithms from the field of learning-to-
rank for ranking lexical substitutions in context. In
turn, we achieved significant improvements on the
two prominent datasets for lexical substitution.
Our results indicate that an exploitation of the or-
dinal structure of the labels in the datasets can lead to
considerable gains in terms of both ranking quality
(GAP) and precision at 1 (p@1). This observation
is supported both for the theoretically simpler point-
wise learning approach and for the most powerful
listwise approach. On the other hand, the pairwise
methods that cannot naturally exploit this property,
did not provide a consistent improvement over the
baseline. In the future, we plan to investigate this
finding in the context of other, similar ranking prob-
lems in Natural Language Processing.
</bodyText>
<sectionHeader confidence="0.770227" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.989759666666667">
This work was supported by the German Research
Foundation (DFG) as part of the Priority Programme
1527.
</bodyText>
<page confidence="0.991203">
1930
</page>
<sectionHeader confidence="0.988721" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999519586538462">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics – Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385–393,
Montr´eal, Canada.
D. Benbouzid, R. Busa-Fekete, N. Casagrande, F.-D.
Collin, and B. K´egl. 2012. MultiBoost: a multi-
purpose boosting package. Journal of Machine Learn-
ing Research, 13:549–553.
Chris Biemann. 2012. Creating a System for Lexi-
cal Substitutions from Scratch using Crowdsourcing.
Language Resources and Evaluation: Special Issue
on Collaboratively Constructed Language Resources,
46(2).
R. Busa-Fekete, B. K´egl, T. ´Eltet˝o, and Gy. Szarvas.
2011. Ranking by calibrated AdaBoost. In (JMLR
W&amp;CP), volume 14, pages 37–48.
R. Busa-Fekete, B. K´egl, T. ´Eltet˝o, and Gy. Szarvas.
2013. Tune and mix: learning to rank using ensembles
of calibrated multi-class classifiers. Machine Learn-
ing, 93(2–3):261–292.
Ching-Yun Chang and Stephen Clark. 2010. Practi-
cal linguistic steganography using contextual synonym
substitution and vertex colour coding. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1194–1203, Cam-
bridge, MA.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct word
sense matching for lexical substitution. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 449–456, Sydney, Australia.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162–1172, Cambridge,
MA.
Katrin Erk and Sebastian Pad´o. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of the ACL 2010 Conference Short Papers, pages 92–
97, Uppsala, Sweden.
Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. 2003.
An efficient boosting algorithm for combining prefer-
ences. Journal of Machine Learning Research, 4:933–
969.
J. Friedman. 1999. Greedy function approximation: a
gradient boosting machine. Technical report, Dept. of
Statistics, Stanford University.
Bela Gipp, Norman Meuschke, and Joeran Beel. 2011.
Comparative Evaluation of Text- and Citation-based
Plagiarism Detection Approaches using GuttenPlag.
In Proceedings of 11th ACM/IEEE-CS Joint Confer-
ence on Digital Libraries (JCDL’11), pages 255–258,
Ottawa, Canada. ACM New York, NY, USA. Avail-
able at http://sciplore.org/pub/.
Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.
2007. FBK-irst: Lexical substitution task exploit-
ing domain and syntagmatic coherence. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 145–148, Prague,
Czech Republic.
Samer Hassan, Andras Csomai, Carmen Banea, Ravi
Sinha, and Rada Mihalcea. 2007. UNT: SubFinder:
Combining knowledge sources for automatic lexical
substitution. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 410–413, Prague, Czech Republic.
T. Joachims. 2006. Training linear svms in linear time.
In Proceedings of the ACM Conference on Knowledge
Discovery and Data Mining (KDD).
B. K´egl and R. Busa-Fekete. 2009. Boosting products of
base classifiers. In International Conference on Ma-
chine Learning, volume 26, pages 497–504, Montreal,
Canada.
Kazuaki Kishida. 2005. Property of Average Precision
and Its Generalization: An Examination of Evaluation
Indicator for Information Retrieval Experiments. NII
technical report. National Institute of Informatics.
P. Li, C. Burges, and Q. Wu. 2007. McRank: Learning to
rank using multiple classification and gradient boost-
ing. In Advances in Neural Information Processing
Systems, volume 19, pages 897–904. The MIT Press.
David Martinez, Su Nam Kim, and Timothy Bald-
win. 2007. MELB-MKB: Lexical substitution system
based on relatives in context. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 237–240, Prague, Czech
Republic.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 48–53,
Prague, Czech Republic.
J. Platt. 2000. Probabilistic outputs for support vec-
tor machines and comparison to regularized likelihood
methods. In A.J. Smola, P. Bartlett, B. Schoelkopf,
and D. Schuurmans, editors, Advances in Large Mar-
gin Classifiers, pages 61–74. MIT Press.
</reference>
<page confidence="0.869904">
1931
</page>
<reference confidence="0.999469441860465">
R. E. Schapire and Y. Singer. 1999. Improved boosting
algorithms using confidence-rated predictions. Ma-
chine Learning, 37(3):297–336.
Ravi Sinha and Rada Mihalcea. 2009. Combining lex-
ical resources for contextual synonym expansion. In
Proceedings of the International Conference RANLP-
2009, pages 404–410, Borovets, Bulgaria.
Gy¨orgy Szarvas, Chris Biemann, and Iryna Gurevych.
2013. Supervised all-words lexical substitution us-
ing delexicalized features. In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT 2013), June.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 948–957, Uppsala,
Sweden.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and effec-
tive vector model. In Proceedings of the Fifth Interna-
tional Joint Conference on Natural Language Process-
ing : IJCNLP 2011, pages 1134–1143, Chiang Mai,
Thailand. MP, ISSN 978-974-466-564-5.
Umut Topkara, Mercan Topkara, and Mikhail J. Atal-
lah. 2006. The hiding virtues of ambiguity: quan-
tifiably resilient watermarking of natural language text
through synonym substitutions. In Proceedings of the
8th workshop on Multimedia and security, pages 164–
174, New York, NY, USA. ACM.
H. Valizadegan, R. Jin, R. Zhang, and J. Mao. 2009.
Learning to rank by optimizing NDCG measure. In
Advances in Neural Information Processing Systems
22, pages 1883–1891.
Q. Wu, C. J. C. Burges, K. M. Svore, and J. Gao. 2010.
Adapting boosting for information retrieval measures.
Inf. Retr., 13(3):254–270.
Deniz Yuret. 2007. Ku: Word sense disambiguation by
substitution. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 207–214, Prague, Czech Republic, June.
Association for Computational Linguistics.
</reference>
<page confidence="0.995852">
1932
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.660594">
<title confidence="0.998638">Learning to rank lexical substitutions</title>
<author confidence="0.940462">Eyke H¨ullermeier</author>
<affiliation confidence="0.989699">szarvasg@amazon.com University of</affiliation>
<address confidence="0.781488">Hans-Meerwein-Str., 35032 Marburg,</address>
<email confidence="0.998653">eyke@mathematik.uni-marburg.de</email>
<abstract confidence="0.993949764705882">The problem to replace a word with a synonym that fits well in its sentential context is known as the lexical substitution task. In this paper, we tackle this task as a supervised ranking problem. Given a dataset of target words, their sentential contexts and the potential substitutions for the target words, the goal is to train a model that accurately ranks the candidate substitutions based on their contextual fitness. As a key contribution, we customize and evaluate several learning-to-rank models to the lexical substitution task, including classification-based and regression-based approaches. On two datasets widely used for lexical substitution, our best models significantly advance the state-of-the-art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>385--393</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="1536" citStr="Agirre et al., 2012" startWordPosition="220" endWordPosition="223">tution, our best models significantly advance the state-of-the-art. 1 Introduction The task to generate lexical substitutions in context (McCarthy and Navigli, 2007), i.e., to replace words in a sentence without changing its meaning, has become an increasingly popular research topic. This task is used, e.g. to evaluate semantic models with regard to their accuracy in modeling word meaning in context (Erk and Pad´o, 2010). Moreover, it provides a basis of NLP applications in many fields, including linguistic steganography (Topkara et al., 2006; Chang and Clark, 2010), semantic text similarity (Agirre et al., 2012) and plagiarism detection (Gipp et al., 2011). While closely related to WSD, 1Work was done while working at RGAI of the Hungarian Acad. Sci. and University of Szeged. 2 R. Busa-Fekete is on leave from the Research Group on Artificial Intelligence of the Hungarian Academy of Sciences and University of Szeged. lexical substitution does not rely on explicitly defined sense inventories (Dagan et al., 2006): the possible substitutions reflect all conceivable senses of the word, and the correct sense has to be ascertained to provide an accurate substitution. While a few lexical sample datasets (McC</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385–393, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Benbouzid</author>
<author>R Busa-Fekete</author>
<author>N Casagrande</author>
<author>F-D Collin</author>
<author>B K´egl</author>
</authors>
<title>MultiBoost: a multipurpose boosting package.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>13--549</pages>
<marker>Benbouzid, Busa-Fekete, Casagrande, Collin, K´egl, 2012</marker>
<rawString>D. Benbouzid, R. Busa-Fekete, N. Casagrande, F.-D. Collin, and B. K´egl. 2012. MultiBoost: a multipurpose boosting package. Journal of Machine Learning Research, 13:549–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Creating a System for Lexical Substitutions from Scratch using Crowdsourcing. Language Resources and Evaluation: Special Issue on Collaboratively Constructed Language Resources,</title>
<date>2012</date>
<volume>46</volume>
<issue>2</issue>
<contexts>
<context position="2175" citStr="Biemann, 2012" startWordPosition="324" endWordPosition="325"> (Gipp et al., 2011). While closely related to WSD, 1Work was done while working at RGAI of the Hungarian Acad. Sci. and University of Szeged. 2 R. Busa-Fekete is on leave from the Research Group on Artificial Intelligence of the Hungarian Academy of Sciences and University of Szeged. lexical substitution does not rely on explicitly defined sense inventories (Dagan et al., 2006): the possible substitutions reflect all conceivable senses of the word, and the correct sense has to be ascertained to provide an accurate substitution. While a few lexical sample datasets (McCarthy and Navigli, 2007; Biemann, 2012) with humanprovided substitutions exist and can be used to evaluate different lexical paraphrasing approaches, a practically useful system must also be able to rephrase unseen words, i.e., any word for which a list of synonyms is provided. Correspondingly, unsupervised and knowledge-based approaches that are not directly dependent on any training material, prevailed in the SemEval 2007 shared task on English Lexical Substitution and dominated follow-up work. The only supervised approach is limited to the combination of several knowledge-based lexical substitution models based on different unde</context>
<context position="3543" citStr="Biemann (2012)" startWordPosition="530" endWordPosition="531">res that – unlike earlier supervised approaches, and similar to unsupervised and knowledge-based methods proposed for this task – is able to generalize to an open vocabulary. For each target word to paraphrase, they first compute a set of substitution candidates using WordNet: all synonyms from all of the target word’s WordNet synsets, together with the words from synsets in similar to, entailment and also see relation to these synsets are considered as potential substitutions. Each candidate then constitutes a training (or test) 3Another notable example for supervised lexical substitution is Biemann (2012), but this is a lexical sample system applicable only to the target words of the training datasets. 1926 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1926–1932, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics example, and these instances are characterized using non-lexical features from heterogeneous evidence such as lexical-semantic resources and distributional similarity, n-gram counts and shallow syntactic features computed on large, unannotated background corpora. The goal is then i) to predict </context>
<context position="8143" citStr="Biemann, 2012" startWordPosition="1238" endWordPosition="1239">nts. Since space restrictions prohibit a comprehensive exposition, we only provide the most essential information and refer to Szarvas et al. (2013), whose experimental setup we adopted, for further details. Datasets. We use two prominent datasets for lexical substitution. The LexSub dataset introduced in the Lexical Substitution task at Semeval 2007 (McCarthy and Navigli, 2007)5 contains 2002 sentences for a total of 201 target words (from all parts of speech), and lexical substitutions assigned (to each target word and sentence pair) by 5 native speaker annotators. The second dataset, TWSI (Biemann, 2012)6, consists of 24,647 sentences for a total of 1,012 target nouns, and lexical substitu4E.g., bright substituted with intelligent in “He was bright and independent and proud” and side for part in “Find someone who can compose the biblical side”. 5http://nlp.cs.swarthmore.edu/ semeval/tasks/task10/data.shtml 6http://www.ukp.tu-darmstadt.de/data/ lexical-resources/twsi-lexical-substitutions/ 1927 tions for each target word in context resulting from a crowdsourced annotation process. For each sentence in each dataset, the annotators provided as many substitutions for the target word as they found</context>
</contexts>
<marker>Biemann, 2012</marker>
<rawString>Chris Biemann. 2012. Creating a System for Lexical Substitutions from Scratch using Crowdsourcing. Language Resources and Evaluation: Special Issue on Collaboratively Constructed Language Resources, 46(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Szarvas</author>
</authors>
<title>Ranking by calibrated AdaBoost.</title>
<date>2011</date>
<booktitle>In (JMLR W&amp;CP),</booktitle>
<volume>14</volume>
<pages>37--48</pages>
<marker>Szarvas, 2011</marker>
<rawString>R. Busa-Fekete, B. K´egl, T. ´Eltet˝o, and Gy. Szarvas. 2011. Ranking by calibrated AdaBoost. In (JMLR W&amp;CP), volume 14, pages 37–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Szarvas</author>
</authors>
<title>Tune and mix: learning to rank using ensembles of calibrated multi-class classifiers.</title>
<date>2013</date>
<booktitle>Machine Learning,</booktitle>
<pages>93--2</pages>
<marker>Szarvas, 2013</marker>
<rawString>R. Busa-Fekete, B. K´egl, T. ´Eltet˝o, and Gy. Szarvas. 2013. Tune and mix: learning to rank using ensembles of calibrated multi-class classifiers. Machine Learning, 93(2–3):261–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ching-Yun Chang</author>
<author>Stephen Clark</author>
</authors>
<title>Practical linguistic steganography using contextual synonym substitution and vertex colour coding.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1194--1203</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="1488" citStr="Chang and Clark, 2010" startWordPosition="212" endWordPosition="215">es. On two datasets widely used for lexical substitution, our best models significantly advance the state-of-the-art. 1 Introduction The task to generate lexical substitutions in context (McCarthy and Navigli, 2007), i.e., to replace words in a sentence without changing its meaning, has become an increasingly popular research topic. This task is used, e.g. to evaluate semantic models with regard to their accuracy in modeling word meaning in context (Erk and Pad´o, 2010). Moreover, it provides a basis of NLP applications in many fields, including linguistic steganography (Topkara et al., 2006; Chang and Clark, 2010), semantic text similarity (Agirre et al., 2012) and plagiarism detection (Gipp et al., 2011). While closely related to WSD, 1Work was done while working at RGAI of the Hungarian Acad. Sci. and University of Szeged. 2 R. Busa-Fekete is on leave from the Research Group on Artificial Intelligence of the Hungarian Academy of Sciences and University of Szeged. lexical substitution does not rely on explicitly defined sense inventories (Dagan et al., 2006): the possible substitutions reflect all conceivable senses of the word, and the correct sense has to be ascertained to provide an accurate substi</context>
</contexts>
<marker>Chang, Clark, 2010</marker>
<rawString>Ching-Yun Chang and Stephen Clark. 2010. Practical linguistic steganography using contextual synonym substitution and vertex colour coding. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1194–1203, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Alfio Gliozzo</author>
<author>Efrat Marmorshtein</author>
<author>Carlo Strapparava</author>
</authors>
<title>Direct word sense matching for lexical substitution.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>449--456</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1942" citStr="Dagan et al., 2006" startWordPosition="286" endWordPosition="289"> and Pad´o, 2010). Moreover, it provides a basis of NLP applications in many fields, including linguistic steganography (Topkara et al., 2006; Chang and Clark, 2010), semantic text similarity (Agirre et al., 2012) and plagiarism detection (Gipp et al., 2011). While closely related to WSD, 1Work was done while working at RGAI of the Hungarian Acad. Sci. and University of Szeged. 2 R. Busa-Fekete is on leave from the Research Group on Artificial Intelligence of the Hungarian Academy of Sciences and University of Szeged. lexical substitution does not rely on explicitly defined sense inventories (Dagan et al., 2006): the possible substitutions reflect all conceivable senses of the word, and the correct sense has to be ascertained to provide an accurate substitution. While a few lexical sample datasets (McCarthy and Navigli, 2007; Biemann, 2012) with humanprovided substitutions exist and can be used to evaluate different lexical paraphrasing approaches, a practically useful system must also be able to rephrase unseen words, i.e., any word for which a list of synonyms is provided. Correspondingly, unsupervised and knowledge-based approaches that are not directly dependent on any training material, prevaile</context>
</contexts>
<marker>Dagan, Glickman, Gliozzo, Marmorshtein, Strapparava, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Marmorshtein, and Carlo Strapparava. 2006. Direct word sense matching for lexical substitution. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 449–456, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Measuring distributional similarity in context.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1162--1172</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="6158" citStr="Dinu and Lapata, 2010" startWordPosition="940" endWordPosition="943">le to report the best performances on this task for two standard datasets. 2 Related work Previous approaches to lexical substitution often seek to automatically generate a set of candidate substitutions for each target word first, and to rank the elements of this set of candidates afterward (Hassan et al., 2007; Giuliano et al., 2007; Martinez et al., 2007; Yuret, 2007). Alternatively, the candidate set can be defined by all human-suggested substitutions for the given target word in all of its contexts; then, the focus is just on the ranking problem (Erk and Pad´o, 2010; Thater et al., 2010; Dinu and Lapata, 2010; Thater et al., 2011). While only the former approach qualifies as a full-fledged substitution system for arbitrary, previously unseen target words, the latter simplifies the comparison of semantic ranking models, as the ranking step is not burdened with the shortcomings of automatically generated substitution candidates. As mentioned before, Szarvas et al. (2013) recently formalized the lexical substitution problem as a supervised learning task, using delexicalized features. This non-lexical feature representation makes different target word/substitution pairs in different contexts4 directly</context>
<context position="18263" citStr="Dinu and Lapata (2010)" startWordPosition="2859" endWordPosition="2862">ameters is calculated based on the evaluation metric used (in this case GAP). We tuned the number of boosting 1929 Database LexSub TWSI Candidates WN Gold WN Gold GAP MaxEnt 43.8 52.4 36.6 47.2 ExpEns 44.3 53.5 37.8 49.7 RankBoost 44.0 51.4 37.0 47.8 RankSVM 43.3 51.8 35.5 45.2 LambdaMART 45.5 55.0 37.8 50.1 P@1 MaxEnt 40.2 57.7 32.4 49.5 ExpEns 39.8 58.5 33.8 53.2 RankBoost 40.7 55.2 33.1 50.8 RankSVM 40.3 51.7 33.2 45.1 LambdaMART 40.8 60.2 33.1 53.6 Table 1: GAP and p@1 values, with significant improvements over the performance of MaxEnt marked in bold. System GAP Erk and Pad´o (2010) 38.6 Dinu and Lapata (2010) 42.9 Thater et al. (2010) 46.0 Thater et al. (2011) 51.7 Szarvas et al. (2013) 52.4 EXPENS 53.5 LAMBDAMART 55.0 Table 2: Comparison to previous studies (dataset LexSub, candidates Gold). iterations in the interval [10,1000] and the number of tree leaves in 18, 16, 321. 5 Results and discussion Our results using the above learning methods are summarized in Table 1. As can be seen, the two methods that exploit the cardinal structure of the label set (relevance degrees), namely EXPENS and LAMBDAMART, consistently outperform the baseline taken from Szarvas et al. (2013) – the only exception is th</context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010. Measuring distributional similarity in context. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1162–1172, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>Exemplar-based models for word meaning in context.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>92--97</pages>
<location>Uppsala,</location>
<marker>Erk, Pad´o, 2010</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2010. Exemplar-based models for word meaning in context. In Proceedings of the ACL 2010 Conference Short Papers, pages 92– 97, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Iyer</author>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>An efficient boosting algorithm for combining preferences.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>4</volume>
<pages>969</pages>
<contexts>
<context position="13571" citStr="Freund et al., 2003" startWordPosition="2099" endWordPosition="2102">r fitness; to this end, one typically applies standard regression techniques, which essentially look at individual instances in isolation (i.e., independent of any other instances in the training or test set). To predict a ranking of a set of query instances, these are simply sorted by their predicted scores (Li et al., 2007). 1928 The pairwise approach trains models that are able to compare pairs of instances. By marking such a pair as positive if the first instance is preferred to the second one, and as negative otherwise, the problem can formally be reduced to a binary classification task (Freund et al., 2003). Finally, in the listwise approach, tailor-made learning methods are used that directly optimize the ranking performance with respect to a global evaluation metric, i.e., a measure that evaluates the ranking of a complete set of query instances (Valizadegan et al., 2009). Below we give a brief overview of the methods included in our experiments. We used the implementations provided by the MultiBoost (Benbouzid et al., 2012), RankSVM and RankLib packages.7 For a detailed description, we refer to the original literature. 4.1 MAXENT The ranking model proposed by Szarvas et al. (2013) was used as</context>
<context position="16380" citStr="Freund et al., 2003" startWordPosition="2544" endWordPosition="2547"> Note that this step takes advantage of the ordinal structure of the underlying scale of relevance levels, which is an important difference to MAXENT. Third, the posteriors of these calibrated classifiers are additively combined, with the weight of each model being exponentially proportional to its GAP score (on the validation set). This method has two hyperparameters: the number of boosting iterations T and the scaling factor in the exponential weighting scheme c. We select T and c from the intervals [100, 2000] and [0, 100], with step sizes 100 and 10, respectively. 4.3 RANKBOOST RANKBOOST (Freund et al., 2003) is a pairwise boosting approach. The objective function is the rank loss (as opposed to ADABOOST, which optimizes the exponential loss). In each boosting iteration, the weak classifier is chosen by maximizing the weighted rank loss. For the weak learner, we used the decision stump described in (Freund et al., 2003), which is able to optimize the rank loss in an efficient way. The only hyperparameter of RANKBOOST to be tuned is the number of iterations that we selected from the interval [1, 1000]. 4.4 RANKSVM RANKSVM (Joachims, 2006) is a pairwise method based on support vector machines, which</context>
</contexts>
<marker>Freund, Iyer, Schapire, Singer, 2003</marker>
<rawString>Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. 2003. An efficient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4:933– 969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Friedman</author>
</authors>
<title>Greedy function approximation: a gradient boosting machine.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Dept. of Statistics, Stanford University.</institution>
<contexts>
<context position="17475" citStr="Friedman (1999)" startWordPosition="2726" endWordPosition="2727">he interval [1, 1000]. 4.4 RANKSVM RANKSVM (Joachims, 2006) is a pairwise method based on support vector machines, which formulates the ranking task as binary classification of pairs of instances. We used a linear kernel, because the optimization using non-linear kernels cannot be done in a reasonable time. The tolerance level of the optimization was set to 0.001 and the regularization parameter was validated in the interval [10−6,104] with a logarithmically increasing step size. 4.5 LAMBDAMART LAMBDAMART (Wu et al., 2010) is a listwise method based on the gradient boosted regression trees by Friedman (1999). The ordinal labels are learned directly by the boosted regression trees whose parameters are tuned by using a gradientbased optimization method. The gradient of parameters is calculated based on the evaluation metric used (in this case GAP). We tuned the number of boosting 1929 Database LexSub TWSI Candidates WN Gold WN Gold GAP MaxEnt 43.8 52.4 36.6 47.2 ExpEns 44.3 53.5 37.8 49.7 RankBoost 44.0 51.4 37.0 47.8 RankSVM 43.3 51.8 35.5 45.2 LambdaMART 45.5 55.0 37.8 50.1 P@1 MaxEnt 40.2 57.7 32.4 49.5 ExpEns 39.8 58.5 33.8 53.2 RankBoost 40.7 55.2 33.1 50.8 RankSVM 40.3 51.7 33.2 45.1 LambdaMA</context>
</contexts>
<marker>Friedman, 1999</marker>
<rawString>J. Friedman. 1999. Greedy function approximation: a gradient boosting machine. Technical report, Dept. of Statistics, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bela Gipp</author>
<author>Norman Meuschke</author>
<author>Joeran Beel</author>
</authors>
<title>Comparative Evaluation of Text- and Citation-based Plagiarism Detection Approaches using GuttenPlag.</title>
<date>2011</date>
<booktitle>In Proceedings of 11th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL’11),</booktitle>
<pages>255--258</pages>
<publisher>ACM</publisher>
<location>Ottawa,</location>
<note>Available at http://sciplore.org/pub/.</note>
<contexts>
<context position="1581" citStr="Gipp et al., 2011" startWordPosition="227" endWordPosition="230">he state-of-the-art. 1 Introduction The task to generate lexical substitutions in context (McCarthy and Navigli, 2007), i.e., to replace words in a sentence without changing its meaning, has become an increasingly popular research topic. This task is used, e.g. to evaluate semantic models with regard to their accuracy in modeling word meaning in context (Erk and Pad´o, 2010). Moreover, it provides a basis of NLP applications in many fields, including linguistic steganography (Topkara et al., 2006; Chang and Clark, 2010), semantic text similarity (Agirre et al., 2012) and plagiarism detection (Gipp et al., 2011). While closely related to WSD, 1Work was done while working at RGAI of the Hungarian Acad. Sci. and University of Szeged. 2 R. Busa-Fekete is on leave from the Research Group on Artificial Intelligence of the Hungarian Academy of Sciences and University of Szeged. lexical substitution does not rely on explicitly defined sense inventories (Dagan et al., 2006): the possible substitutions reflect all conceivable senses of the word, and the correct sense has to be ascertained to provide an accurate substitution. While a few lexical sample datasets (McCarthy and Navigli, 2007; Biemann, 2012) with </context>
</contexts>
<marker>Gipp, Meuschke, Beel, 2011</marker>
<rawString>Bela Gipp, Norman Meuschke, and Joeran Beel. 2011. Comparative Evaluation of Text- and Citation-based Plagiarism Detection Approaches using GuttenPlag. In Proceedings of 11th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL’11), pages 255–258, Ottawa, Canada. ACM New York, NY, USA. Available at http://sciplore.org/pub/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alfio Gliozzo</author>
<author>Carlo Strapparava</author>
</authors>
<title>FBK-irst: Lexical substitution task exploiting domain and syntagmatic coherence.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>145--148</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5873" citStr="Giuliano et al., 2007" startWordPosition="890" endWordPosition="893">the performance on the lexical substitution task is strongly influenced by the way in which this task is formalized as a machine learning problem (i.e., as binary or multi-class classification or regression) and by the learning method used to solve this problem. As a result, we are able to report the best performances on this task for two standard datasets. 2 Related work Previous approaches to lexical substitution often seek to automatically generate a set of candidate substitutions for each target word first, and to rank the elements of this set of candidates afterward (Hassan et al., 2007; Giuliano et al., 2007; Martinez et al., 2007; Yuret, 2007). Alternatively, the candidate set can be defined by all human-suggested substitutions for the given target word in all of its contexts; then, the focus is just on the ranking problem (Erk and Pad´o, 2010; Thater et al., 2010; Dinu and Lapata, 2010; Thater et al., 2011). While only the former approach qualifies as a full-fledged substitution system for arbitrary, previously unseen target words, the latter simplifies the comparison of semantic ranking models, as the ranking step is not burdened with the shortcomings of automatically generated substitution ca</context>
</contexts>
<marker>Giuliano, Gliozzo, Strapparava, 2007</marker>
<rawString>Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava. 2007. FBK-irst: Lexical substitution task exploiting domain and syntagmatic coherence. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 145–148, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samer Hassan</author>
<author>Andras Csomai</author>
<author>Carmen Banea</author>
<author>Ravi Sinha</author>
<author>Rada Mihalcea</author>
</authors>
<title>UNT: SubFinder: Combining knowledge sources for automatic lexical substitution.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007),</booktitle>
<pages>410--413</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5850" citStr="Hassan et al., 2007" startWordPosition="885" endWordPosition="889">xperiments show that the performance on the lexical substitution task is strongly influenced by the way in which this task is formalized as a machine learning problem (i.e., as binary or multi-class classification or regression) and by the learning method used to solve this problem. As a result, we are able to report the best performances on this task for two standard datasets. 2 Related work Previous approaches to lexical substitution often seek to automatically generate a set of candidate substitutions for each target word first, and to rank the elements of this set of candidates afterward (Hassan et al., 2007; Giuliano et al., 2007; Martinez et al., 2007; Yuret, 2007). Alternatively, the candidate set can be defined by all human-suggested substitutions for the given target word in all of its contexts; then, the focus is just on the ranking problem (Erk and Pad´o, 2010; Thater et al., 2010; Dinu and Lapata, 2010; Thater et al., 2011). While only the former approach qualifies as a full-fledged substitution system for arbitrary, previously unseen target words, the latter simplifies the comparison of semantic ranking models, as the ranking step is not burdened with the shortcomings of automatically ge</context>
</contexts>
<marker>Hassan, Csomai, Banea, Sinha, Mihalcea, 2007</marker>
<rawString>Samer Hassan, Andras Csomai, Carmen Banea, Ravi Sinha, and Rada Mihalcea. 2007. UNT: SubFinder: Combining knowledge sources for automatic lexical substitution. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007), pages 410–413, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Training linear svms in linear time.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD).</booktitle>
<contexts>
<context position="16919" citStr="Joachims, 2006" startWordPosition="2638" endWordPosition="2639">izes 100 and 10, respectively. 4.3 RANKBOOST RANKBOOST (Freund et al., 2003) is a pairwise boosting approach. The objective function is the rank loss (as opposed to ADABOOST, which optimizes the exponential loss). In each boosting iteration, the weak classifier is chosen by maximizing the weighted rank loss. For the weak learner, we used the decision stump described in (Freund et al., 2003), which is able to optimize the rank loss in an efficient way. The only hyperparameter of RANKBOOST to be tuned is the number of iterations that we selected from the interval [1, 1000]. 4.4 RANKSVM RANKSVM (Joachims, 2006) is a pairwise method based on support vector machines, which formulates the ranking task as binary classification of pairs of instances. We used a linear kernel, because the optimization using non-linear kernels cannot be done in a reasonable time. The tolerance level of the optimization was set to 0.001 and the regularization parameter was validated in the interval [10−6,104] with a logarithmically increasing step size. 4.5 LAMBDAMART LAMBDAMART (Wu et al., 2010) is a listwise method based on the gradient boosted regression trees by Friedman (1999). The ordinal labels are learned directly by</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>T. Joachims. 2006. Training linear svms in linear time. In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B K´egl</author>
<author>R Busa-Fekete</author>
</authors>
<title>Boosting products of base classifiers.</title>
<date>2009</date>
<booktitle>In International Conference on Machine Learning,</booktitle>
<volume>26</volume>
<pages>497--504</pages>
<location>Montreal, Canada.</location>
<marker>K´egl, Busa-Fekete, 2009</marker>
<rawString>B. K´egl and R. Busa-Fekete. 2009. Boosting products of base classifiers. In International Conference on Machine Learning, volume 26, pages 497–504, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuaki Kishida</author>
</authors>
<title>Property of Average Precision and Its Generalization: An Examination of Evaluation Indicator for Information Retrieval Experiments. NII technical report.</title>
<date>2005</date>
<institution>National Institute of Informatics.</institution>
<contexts>
<context position="9813" citStr="Kishida, 2005" startWordPosition="1490" endWordPosition="1491">rget word fall into either the training or the test set, but never both. This way, we make sure to evaluate the models on target words not seen during training, thereby mimicking an open vocabulary paraphrasing system: at testing time, paraphrases are ranked for unseen target words, similarly as the models would rank paraphrases for any words (not necessarily contained in the dataset). For algorithms with tunable parameters, we further divide the training sets into a training and a validation part to find the best parameter settings. For evaluation, we use Generalized Average Precision (GAP) (Kishida, 2005) and Precision at 1 (P@1), i.e., the percentage of correct paraphrases at rank 1. Features. In all experiments, we used the features described in Szarvas et al. (2013), implemented precisely as proposed by the original work. Each (sentence, target word, substitution) triplet represents an instance, and the feature values are computed from the sentence context, the target word and the substitution word. The features used fall into four major categories. The most important features describe the syntagmatic coherence of the substitution in context, measured as local n-gram frequencies obtained fr</context>
</contexts>
<marker>Kishida, 2005</marker>
<rawString>Kazuaki Kishida. 2005. Property of Average Precision and Its Generalization: An Examination of Evaluation Indicator for Information Retrieval Experiments. NII technical report. National Institute of Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Li</author>
<author>C Burges</author>
<author>Q Wu</author>
</authors>
<title>McRank: Learning to rank using multiple classification and gradient boosting.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>19</volume>
<pages>897--904</pages>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="13278" citStr="Li et al., 2007" startWordPosition="2049" endWordPosition="2052">titution system. 4 Learning-to-Rank methods Machine learning methods for ranking are traditionally classified into three categories. In the pointwise approach, a model is trained that maps instances (in this case candidate substitutions in a context) to scores indicating their relevance or fitness; to this end, one typically applies standard regression techniques, which essentially look at individual instances in isolation (i.e., independent of any other instances in the training or test set). To predict a ranking of a set of query instances, these are simply sorted by their predicted scores (Li et al., 2007). 1928 The pairwise approach trains models that are able to compare pairs of instances. By marking such a pair as positive if the first instance is preferred to the second one, and as negative otherwise, the problem can formally be reduced to a binary classification task (Freund et al., 2003). Finally, in the listwise approach, tailor-made learning methods are used that directly optimize the ranking performance with respect to a global evaluation metric, i.e., a measure that evaluates the ranking of a complete set of query instances (Valizadegan et al., 2009). Below we give a brief overview of</context>
</contexts>
<marker>Li, Burges, Wu, 2007</marker>
<rawString>P. Li, C. Burges, and Q. Wu. 2007. McRank: Learning to rank using multiple classification and gradient boosting. In Advances in Neural Information Processing Systems, volume 19, pages 897–904. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Martinez</author>
<author>Su Nam Kim</author>
<author>Timothy Baldwin</author>
</authors>
<title>MELB-MKB: Lexical substitution system based on relatives in context.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>237--240</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5896" citStr="Martinez et al., 2007" startWordPosition="894" endWordPosition="897">lexical substitution task is strongly influenced by the way in which this task is formalized as a machine learning problem (i.e., as binary or multi-class classification or regression) and by the learning method used to solve this problem. As a result, we are able to report the best performances on this task for two standard datasets. 2 Related work Previous approaches to lexical substitution often seek to automatically generate a set of candidate substitutions for each target word first, and to rank the elements of this set of candidates afterward (Hassan et al., 2007; Giuliano et al., 2007; Martinez et al., 2007; Yuret, 2007). Alternatively, the candidate set can be defined by all human-suggested substitutions for the given target word in all of its contexts; then, the focus is just on the ranking problem (Erk and Pad´o, 2010; Thater et al., 2010; Dinu and Lapata, 2010; Thater et al., 2011). While only the former approach qualifies as a full-fledged substitution system for arbitrary, previously unseen target words, the latter simplifies the comparison of semantic ranking models, as the ranking step is not burdened with the shortcomings of automatically generated substitution candidates. As mentioned </context>
</contexts>
<marker>Martinez, Kim, Baldwin, 2007</marker>
<rawString>David Martinez, Su Nam Kim, and Timothy Baldwin. 2007. MELB-MKB: Lexical substitution system based on relatives in context. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 237–240, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>Semeval2007 task 10: English lexical substitution task.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>48--53</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1081" citStr="McCarthy and Navigli, 2007" startWordPosition="146" endWordPosition="149"> problem. Given a dataset of target words, their sentential contexts and the potential substitutions for the target words, the goal is to train a model that accurately ranks the candidate substitutions based on their contextual fitness. As a key contribution, we customize and evaluate several learning-to-rank models to the lexical substitution task, including classification-based and regression-based approaches. On two datasets widely used for lexical substitution, our best models significantly advance the state-of-the-art. 1 Introduction The task to generate lexical substitutions in context (McCarthy and Navigli, 2007), i.e., to replace words in a sentence without changing its meaning, has become an increasingly popular research topic. This task is used, e.g. to evaluate semantic models with regard to their accuracy in modeling word meaning in context (Erk and Pad´o, 2010). Moreover, it provides a basis of NLP applications in many fields, including linguistic steganography (Topkara et al., 2006; Chang and Clark, 2010), semantic text similarity (Agirre et al., 2012) and plagiarism detection (Gipp et al., 2011). While closely related to WSD, 1Work was done while working at RGAI of the Hungarian Acad. Sci. and</context>
<context position="7910" citStr="McCarthy and Navigli, 2007" startWordPosition="1197" endWordPosition="1200">ntribution leads to further significant improvements in modeling the semantics of a text and in end-system accuracy. 3 Datasets and experimental setup Here we introduce the datasets, experimental setup and evaluation measures used in our experiments. Since space restrictions prohibit a comprehensive exposition, we only provide the most essential information and refer to Szarvas et al. (2013), whose experimental setup we adopted, for further details. Datasets. We use two prominent datasets for lexical substitution. The LexSub dataset introduced in the Lexical Substitution task at Semeval 2007 (McCarthy and Navigli, 2007)5 contains 2002 sentences for a total of 201 target words (from all parts of speech), and lexical substitutions assigned (to each target word and sentence pair) by 5 native speaker annotators. The second dataset, TWSI (Biemann, 2012)6, consists of 24,647 sentences for a total of 1,012 target nouns, and lexical substitu4E.g., bright substituted with intelligent in “He was bright and independent and proud” and side for part in “Find someone who can compose the biblical side”. 5http://nlp.cs.swarthmore.edu/ semeval/tasks/task10/data.shtml 6http://www.ukp.tu-darmstadt.de/data/ lexical-resources/tw</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2007. Semeval2007 task 10: English lexical substitution task. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 48–53, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparison to regularized likelihood methods.</title>
<date>2000</date>
<booktitle>Advances in Large Margin Classifiers,</booktitle>
<pages>61--74</pages>
<editor>In A.J. Smola, P. Bartlett, B. Schoelkopf, and D. Schuurmans, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="15698" citStr="Platt, 2000" startWordPosition="2434" endWordPosition="2435">ent weak learners (BusaFekete et al., 2011; K´egl and Busa-Fekete, 2009) are trained to predict the level of relevance (quality) of a substitution (i.e., the number of annotators who proposed the candidate for that particular context). Second, the classifiers are calibrated to obtain 7RankLib is available at http://people.cs.umass. edu/˜vdang/ranklib.html. We extended the implementation of the LAMBDAMART algorithm in this package to compute the gradients of and optimize for the GAP measure. an accurate posterior distribution; to this end, several calibration techniques, such as Platt scaling (Platt, 2000), are used to obtain a diverse pool of calibrated classifiers. Note that this step takes advantage of the ordinal structure of the underlying scale of relevance levels, which is an important difference to MAXENT. Third, the posteriors of these calibrated classifiers are additively combined, with the weight of each model being exponentially proportional to its GAP score (on the validation set). This method has two hyperparameters: the number of boosting iterations T and the scaling factor in the exponential weighting scheme c. We select T and c from the intervals [100, 2000] and [0, 100], with </context>
</contexts>
<marker>Platt, 2000</marker>
<rawString>J. Platt. 2000. Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. In A.J. Smola, P. Bartlett, B. Schoelkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers, pages 61–74. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>Improved boosting algorithms using confidence-rated predictions.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="15054" citStr="Schapire and Singer, 1999" startWordPosition="2337" endWordPosition="2340">ions were used for weighting the training examples. The underlying MaxEnt model was trained until convergence, i.e., there was no hyperparameter to be tuned. For a new target/substitution pair, the classifier delivers an estimation of the posterior probability for being a good substitution. The ranking is then produced by sorting the candidates in decreasing order according to this probability. 4.2 EXPENS EXPENS (Busa-Fekete et al., 2013) is a pointwise method with listwise meta-learning step that exploits an ensemble of multi-class classifiers. It consists of three steps. First, ADABOOST.MH (Schapire and Singer, 1999) classifiers with several different weak learners (BusaFekete et al., 2011; K´egl and Busa-Fekete, 2009) are trained to predict the level of relevance (quality) of a substitution (i.e., the number of annotators who proposed the candidate for that particular context). Second, the classifiers are calibrated to obtain 7RankLib is available at http://people.cs.umass. edu/˜vdang/ranklib.html. We extended the implementation of the LAMBDAMART algorithm in this package to compute the gradients of and optimize for the GAP measure. an accurate posterior distribution; to this end, several calibration tec</context>
</contexts>
<marker>Schapire, Singer, 1999</marker>
<rawString>R. E. Schapire and Y. Singer. 1999. Improved boosting algorithms using confidence-rated predictions. Machine Learning, 37(3):297–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Sinha</author>
<author>Rada Mihalcea</author>
</authors>
<title>Combining lexical resources for contextual synonym expansion.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference RANLP2009,</booktitle>
<pages>404--410</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="2817" citStr="Sinha and Mihalcea, 2009" startWordPosition="416" endWordPosition="419">ed substitutions exist and can be used to evaluate different lexical paraphrasing approaches, a practically useful system must also be able to rephrase unseen words, i.e., any word for which a list of synonyms is provided. Correspondingly, unsupervised and knowledge-based approaches that are not directly dependent on any training material, prevailed in the SemEval 2007 shared task on English Lexical Substitution and dominated follow-up work. The only supervised approach is limited to the combination of several knowledge-based lexical substitution models based on different underlying lexicons (Sinha and Mihalcea, 2009).3 A recent work by Szarvas et al. (2013) describes a tailor-made supervised system based on delexicalized features that – unlike earlier supervised approaches, and similar to unsupervised and knowledge-based methods proposed for this task – is able to generalize to an open vocabulary. For each target word to paraphrase, they first compute a set of substitution candidates using WordNet: all synonyms from all of the target word’s WordNet synsets, together with the words from synsets in similar to, entailment and also see relation to these synsets are considered as potential substitutions. Each </context>
</contexts>
<marker>Sinha, Mihalcea, 2009</marker>
<rawString>Ravi Sinha and Rada Mihalcea. 2009. Combining lexical resources for contextual synonym expansion. In Proceedings of the International Conference RANLP2009, pages 404–410, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gy¨orgy Szarvas</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
</authors>
<title>Supervised all-words lexical substitution using delexicalized features.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT</booktitle>
<contexts>
<context position="2858" citStr="Szarvas et al. (2013)" startWordPosition="424" endWordPosition="427">luate different lexical paraphrasing approaches, a practically useful system must also be able to rephrase unseen words, i.e., any word for which a list of synonyms is provided. Correspondingly, unsupervised and knowledge-based approaches that are not directly dependent on any training material, prevailed in the SemEval 2007 shared task on English Lexical Substitution and dominated follow-up work. The only supervised approach is limited to the combination of several knowledge-based lexical substitution models based on different underlying lexicons (Sinha and Mihalcea, 2009).3 A recent work by Szarvas et al. (2013) describes a tailor-made supervised system based on delexicalized features that – unlike earlier supervised approaches, and similar to unsupervised and knowledge-based methods proposed for this task – is able to generalize to an open vocabulary. For each target word to paraphrase, they first compute a set of substitution candidates using WordNet: all synonyms from all of the target word’s WordNet synsets, together with the words from synsets in similar to, entailment and also see relation to these synsets are considered as potential substitutions. Each candidate then constitutes a training (or</context>
<context position="5092" citStr="Szarvas et al. (2013)" startWordPosition="761" endWordPosition="764">context). This model is able to generate paraphrases for target words not contained in the training material. This favorable property is achieved using only such features (e.g. local n-gram frequencies in context) that are meaningfully comparable across the different target words and candidate substitutions they are computed from. More importantly, their model also provides superior ranking results compared to state of the art unsupervised and knowledge based approaches and therefore it defines the current state of the art for open vocabulary lexical substitution. Motivated by the findings of Szarvas et al. (2013), we address lexical substitution as a supervised learning problem, and go beyond their approach from a methodological point of view. Our experiments show that the performance on the lexical substitution task is strongly influenced by the way in which this task is formalized as a machine learning problem (i.e., as binary or multi-class classification or regression) and by the learning method used to solve this problem. As a result, we are able to report the best performances on this task for two standard datasets. 2 Related work Previous approaches to lexical substitution often seek to automat</context>
<context position="6525" citStr="Szarvas et al. (2013)" startWordPosition="994" endWordPosition="997"> 2007). Alternatively, the candidate set can be defined by all human-suggested substitutions for the given target word in all of its contexts; then, the focus is just on the ranking problem (Erk and Pad´o, 2010; Thater et al., 2010; Dinu and Lapata, 2010; Thater et al., 2011). While only the former approach qualifies as a full-fledged substitution system for arbitrary, previously unseen target words, the latter simplifies the comparison of semantic ranking models, as the ranking step is not burdened with the shortcomings of automatically generated substitution candidates. As mentioned before, Szarvas et al. (2013) recently formalized the lexical substitution problem as a supervised learning task, using delexicalized features. This non-lexical feature representation makes different target word/substitution pairs in different contexts4 directly comparable. Thus, it becomes possible to learn an all-words system that is applicable to unseen words, using supervised methods, which provides superior ranking accuracy to unsupervised and knowledge based models. In this work, we build on the problem formulation and the features proposed by Szarvas et al. (2013) while largely extending their machine learning meth</context>
<context position="9980" citStr="Szarvas et al. (2013)" startWordPosition="1516" endWordPosition="1519"> thereby mimicking an open vocabulary paraphrasing system: at testing time, paraphrases are ranked for unseen target words, similarly as the models would rank paraphrases for any words (not necessarily contained in the dataset). For algorithms with tunable parameters, we further divide the training sets into a training and a validation part to find the best parameter settings. For evaluation, we use Generalized Average Precision (GAP) (Kishida, 2005) and Precision at 1 (P@1), i.e., the percentage of correct paraphrases at rank 1. Features. In all experiments, we used the features described in Szarvas et al. (2013), implemented precisely as proposed by the original work. Each (sentence, target word, substitution) triplet represents an instance, and the feature values are computed from the sentence context, the target word and the substitution word. The features used fall into four major categories. The most important features describe the syntagmatic coherence of the substitution in context, measured as local n-gram frequencies obtained from web data. The frequency for a 1-5gram context with the substitution word is computed and normalized with respect to either 1) the frequency of the original context </context>
<context position="12306" citStr="Szarvas et al. (2013)" startWordPosition="1897" endWordPosition="1900">l synsets up) for the candidate and target word in the WordNet hierarchy (represented as a nominal feature, by the ID of these synsets). Lastly a group of features capture shallow syntactic patterns of the target word and its local context in the form of 1) part of speech patterns (trigrams) in a sliding window around the target word using main POS categories, i.e. only the first letter of the Penn Treebank codes, and 2) the detailed POS code of the candidate word assigned by a POS tagger. We omit a mathematically precise description of these features for space reasons and refer the reader to Szarvas et al. (2013) for a more formal and detailed description of the feature functions. Importantly, these delexicalized features are numerically comparable across the different target words and candidate substitutions they are computed from. This property enables the models to generalize over the words in the datasets and thus enables a supervised, all-words lexical substitution system. 4 Learning-to-Rank methods Machine learning methods for ranking are traditionally classified into three categories. In the pointwise approach, a model is trained that maps instances (in this case candidate substitutions in a co</context>
<context position="14159" citStr="Szarvas et al. (2013)" startWordPosition="2195" endWordPosition="2198">cation task (Freund et al., 2003). Finally, in the listwise approach, tailor-made learning methods are used that directly optimize the ranking performance with respect to a global evaluation metric, i.e., a measure that evaluates the ranking of a complete set of query instances (Valizadegan et al., 2009). Below we give a brief overview of the methods included in our experiments. We used the implementations provided by the MultiBoost (Benbouzid et al., 2012), RankSVM and RankLib packages.7 For a detailed description, we refer to the original literature. 4.1 MAXENT The ranking model proposed by Szarvas et al. (2013) was used as a baseline. This is a pointwise approach based on a maximum entropy classifier, in which the ranking task is cast as a binary classification problem, namely to discriminate good (label &gt; 0) from bad substitutions. The actual label values for good substitutions were used for weighting the training examples. The underlying MaxEnt model was trained until convergence, i.e., there was no hyperparameter to be tuned. For a new target/substitution pair, the classifier delivers an estimation of the posterior probability for being a good substitution. The ranking is then produced by sorting</context>
<context position="18342" citStr="Szarvas et al. (2013)" startWordPosition="2874" endWordPosition="2877"> tuned the number of boosting 1929 Database LexSub TWSI Candidates WN Gold WN Gold GAP MaxEnt 43.8 52.4 36.6 47.2 ExpEns 44.3 53.5 37.8 49.7 RankBoost 44.0 51.4 37.0 47.8 RankSVM 43.3 51.8 35.5 45.2 LambdaMART 45.5 55.0 37.8 50.1 P@1 MaxEnt 40.2 57.7 32.4 49.5 ExpEns 39.8 58.5 33.8 53.2 RankBoost 40.7 55.2 33.1 50.8 RankSVM 40.3 51.7 33.2 45.1 LambdaMART 40.8 60.2 33.1 53.6 Table 1: GAP and p@1 values, with significant improvements over the performance of MaxEnt marked in bold. System GAP Erk and Pad´o (2010) 38.6 Dinu and Lapata (2010) 42.9 Thater et al. (2010) 46.0 Thater et al. (2011) 51.7 Szarvas et al. (2013) 52.4 EXPENS 53.5 LAMBDAMART 55.0 Table 2: Comparison to previous studies (dataset LexSub, candidates Gold). iterations in the interval [10,1000] and the number of tree leaves in 18, 16, 321. 5 Results and discussion Our results using the above learning methods are summarized in Table 1. As can be seen, the two methods that exploit the cardinal structure of the label set (relevance degrees), namely EXPENS and LAMBDAMART, consistently outperform the baseline taken from Szarvas et al. (2013) – the only exception is the p@1 score for EXPENS on the Semeval Lexical Substitution dataset and the cand</context>
</contexts>
<marker>Szarvas, Biemann, Gurevych, 2013</marker>
<rawString>Gy¨orgy Szarvas, Chris Biemann, and Iryna Gurevych. 2013. Supervised all-words lexical substitution using delexicalized features. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Contextualizing semantic representations using syntactically enriched vector models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>948--957</pages>
<location>Uppsala,</location>
<marker>Thater, F¨urstenau, Pinkal, 2010</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2010. Contextualizing semantic representations using syntactically enriched vector models. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948–957, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Word meaning in context: A simple and effective vector model.</title>
<date>2011</date>
<journal>Chiang Mai, Thailand. MP, ISSN</journal>
<booktitle>In Proceedings of the Fifth International Joint Conference on Natural Language Processing : IJCNLP 2011,</booktitle>
<pages>1134--1143</pages>
<marker>Thater, F¨urstenau, Pinkal, 2011</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2011. Word meaning in context: A simple and effective vector model. In Proceedings of the Fifth International Joint Conference on Natural Language Processing : IJCNLP 2011, pages 1134–1143, Chiang Mai, Thailand. MP, ISSN 978-974-466-564-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Umut Topkara</author>
<author>Mercan Topkara</author>
<author>Mikhail J Atallah</author>
</authors>
<title>The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 8th workshop on Multimedia and security,</booktitle>
<pages>164--174</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1464" citStr="Topkara et al., 2006" startWordPosition="208" endWordPosition="211">ression-based approaches. On two datasets widely used for lexical substitution, our best models significantly advance the state-of-the-art. 1 Introduction The task to generate lexical substitutions in context (McCarthy and Navigli, 2007), i.e., to replace words in a sentence without changing its meaning, has become an increasingly popular research topic. This task is used, e.g. to evaluate semantic models with regard to their accuracy in modeling word meaning in context (Erk and Pad´o, 2010). Moreover, it provides a basis of NLP applications in many fields, including linguistic steganography (Topkara et al., 2006; Chang and Clark, 2010), semantic text similarity (Agirre et al., 2012) and plagiarism detection (Gipp et al., 2011). While closely related to WSD, 1Work was done while working at RGAI of the Hungarian Acad. Sci. and University of Szeged. 2 R. Busa-Fekete is on leave from the Research Group on Artificial Intelligence of the Hungarian Academy of Sciences and University of Szeged. lexical substitution does not rely on explicitly defined sense inventories (Dagan et al., 2006): the possible substitutions reflect all conceivable senses of the word, and the correct sense has to be ascertained to pr</context>
</contexts>
<marker>Topkara, Topkara, Atallah, 2006</marker>
<rawString>Umut Topkara, Mercan Topkara, and Mikhail J. Atallah. 2006. The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions. In Proceedings of the 8th workshop on Multimedia and security, pages 164– 174, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Valizadegan</author>
<author>R Jin</author>
<author>R Zhang</author>
<author>J Mao</author>
</authors>
<title>Learning to rank by optimizing NDCG measure.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 22,</booktitle>
<pages>1883--1891</pages>
<contexts>
<context position="13843" citStr="Valizadegan et al., 2009" startWordPosition="2142" endWordPosition="2145">re simply sorted by their predicted scores (Li et al., 2007). 1928 The pairwise approach trains models that are able to compare pairs of instances. By marking such a pair as positive if the first instance is preferred to the second one, and as negative otherwise, the problem can formally be reduced to a binary classification task (Freund et al., 2003). Finally, in the listwise approach, tailor-made learning methods are used that directly optimize the ranking performance with respect to a global evaluation metric, i.e., a measure that evaluates the ranking of a complete set of query instances (Valizadegan et al., 2009). Below we give a brief overview of the methods included in our experiments. We used the implementations provided by the MultiBoost (Benbouzid et al., 2012), RankSVM and RankLib packages.7 For a detailed description, we refer to the original literature. 4.1 MAXENT The ranking model proposed by Szarvas et al. (2013) was used as a baseline. This is a pointwise approach based on a maximum entropy classifier, in which the ranking task is cast as a binary classification problem, namely to discriminate good (label &gt; 0) from bad substitutions. The actual label values for good substitutions were used </context>
</contexts>
<marker>Valizadegan, Jin, Zhang, Mao, 2009</marker>
<rawString>H. Valizadegan, R. Jin, R. Zhang, and J. Mao. 2009. Learning to rank by optimizing NDCG measure. In Advances in Neural Information Processing Systems 22, pages 1883–1891.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Wu</author>
<author>C J C Burges</author>
<author>K M Svore</author>
<author>J Gao</author>
</authors>
<title>Adapting boosting for information retrieval measures.</title>
<date>2010</date>
<journal>Inf. Retr.,</journal>
<volume>13</volume>
<issue>3</issue>
<contexts>
<context position="17388" citStr="Wu et al., 2010" startWordPosition="2710" endWordPosition="2713">erparameter of RANKBOOST to be tuned is the number of iterations that we selected from the interval [1, 1000]. 4.4 RANKSVM RANKSVM (Joachims, 2006) is a pairwise method based on support vector machines, which formulates the ranking task as binary classification of pairs of instances. We used a linear kernel, because the optimization using non-linear kernels cannot be done in a reasonable time. The tolerance level of the optimization was set to 0.001 and the regularization parameter was validated in the interval [10−6,104] with a logarithmically increasing step size. 4.5 LAMBDAMART LAMBDAMART (Wu et al., 2010) is a listwise method based on the gradient boosted regression trees by Friedman (1999). The ordinal labels are learned directly by the boosted regression trees whose parameters are tuned by using a gradientbased optimization method. The gradient of parameters is calculated based on the evaluation metric used (in this case GAP). We tuned the number of boosting 1929 Database LexSub TWSI Candidates WN Gold WN Gold GAP MaxEnt 43.8 52.4 36.6 47.2 ExpEns 44.3 53.5 37.8 49.7 RankBoost 44.0 51.4 37.0 47.8 RankSVM 43.3 51.8 35.5 45.2 LambdaMART 45.5 55.0 37.8 50.1 P@1 MaxEnt 40.2 57.7 32.4 49.5 ExpEns</context>
</contexts>
<marker>Wu, Burges, Svore, Gao, 2010</marker>
<rawString>Q. Wu, C. J. C. Burges, K. M. Svore, and J. Gao. 2010. Adapting boosting for information retrieval measures. Inf. Retr., 13(3):254–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
</authors>
<title>Ku: Word sense disambiguation by substitution.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007),</booktitle>
<pages>207--214</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5910" citStr="Yuret, 2007" startWordPosition="898" endWordPosition="899">sk is strongly influenced by the way in which this task is formalized as a machine learning problem (i.e., as binary or multi-class classification or regression) and by the learning method used to solve this problem. As a result, we are able to report the best performances on this task for two standard datasets. 2 Related work Previous approaches to lexical substitution often seek to automatically generate a set of candidate substitutions for each target word first, and to rank the elements of this set of candidates afterward (Hassan et al., 2007; Giuliano et al., 2007; Martinez et al., 2007; Yuret, 2007). Alternatively, the candidate set can be defined by all human-suggested substitutions for the given target word in all of its contexts; then, the focus is just on the ranking problem (Erk and Pad´o, 2010; Thater et al., 2010; Dinu and Lapata, 2010; Thater et al., 2011). While only the former approach qualifies as a full-fledged substitution system for arbitrary, previously unseen target words, the latter simplifies the comparison of semantic ranking models, as the ranking step is not burdened with the shortcomings of automatically generated substitution candidates. As mentioned before, Szarva</context>
</contexts>
<marker>Yuret, 2007</marker>
<rawString>Deniz Yuret. 2007. Ku: Word sense disambiguation by substitution. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007), pages 207–214, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
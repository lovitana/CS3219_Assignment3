<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.9279815">
Japanese Zero Reference Resolution
Considering Exophora and Author/Reader Mentions
</title>
<author confidence="0.988227">
Masatsugu Hangyo Daisuke Kawahara Sadao Kurohashi
</author>
<affiliation confidence="0.992829">
Graduate School of Informatics, Kyoto University
</affiliation>
<address confidence="0.8041385">
Yoshida-honmachi, Sakyo-ku
Kyoto, 606-8501, Japan
</address>
<email confidence="0.999685">
{hangyo,dk,kuro}@nlp.ist.i.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.99667" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999837842105263">
In Japanese, zero references often occur and
many of them are categorized into zero ex-
ophora, in which a referent is not mentioned in
the document. However, previous studies have
focused on only zero endophora, in which
a referent explicitly appears. We present a
zero reference resolution model considering
zero exophora and author/reader of a docu-
ment. To deal with zero exophora, our model
adds pseudo entities corresponding to zero
exophora to candidate referents of zero pro-
nouns. In addition, we automatically detect
mentions that refer to the author and reader of
a document by using lexico-syntactic patterns.
We represent their particular behavior in a dis-
course as a feature vector of a machine learn-
ing model. The experimental results demon-
strate the effectiveness of our model for not
only zero exophora but also zero endophora.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.971630902439024">
Zero reference resolution is the task of detecting and
identifying omitted arguments of a predicate. Since
the arguments are often omitted in Japanese, zero
reference resolution is essential in a wide range of
Japanese natural language processing (NLP) appli-
cations such as information retrieval and machine
translation.
For example, in example (1) , the accusative argu-
ment of the predicate “ A­�iT” (eat) is omitted.&apos;
The omitted argument is called a zero pronoun. In
this example, the zero pronoun refers to “���”
(pasta).
Zero reference resolution is divided into two sub-
tasks: zero pronoun detection and referent identifi-
cation. Zero pronoun detection is the task that de-
tects omitted zero pronouns from a document. In
example (1), this task detects that there are the zero
pronouns in the accusative and nominative cases of
“ A­�iT” (eat) and there is no zero pronoun in
the dative case of “ A­�iT”. Referent identifica-
tion is the task that identifies the referent of a zero
pronoun. In example (1), this task identifies that the
referent of the zero pronoun in the accusative case of
“ A­�iT” is “���” (pasta). These two subtasks
are often resolved simultaneously and our proposed
model is a unified model.
Many previous studies (Imamura et al., 2009;
Sasano et al., 2008; Sasano and Kurohashi, 2011)
have treated only zero endophora, which is a phe-
nomenon that a referent is mentioned in a document,
such as “���” (pasta) in example (1). However,
zero exophora, which is a phenomenon that a ref-
erent does not appear in a document, often occurs in
Japanese when a referent is an author or reader of a
document or an indefinite pronoun. For example, in
example (1), the referent of the zero pronoun of the
nominative case of “ A­�iT” (eat) is the author of
&apos;In this paper, we use the following abbreviations: NOM
(nominative), ABL(ablative), ACC (accusative), DAT (dative),
ALL (allative), GEN (genitive), CMI (comitative), CNJ (con-
junction), INS(instrumental) and TOP (topic marker).
</bodyText>
<equation confidence="0.6027342">
(1) パスタが ffきで 毎日 (0 ガ)
pasta-NOM like everyday (0-NOM)
(0 ヲ) 食べます。
(0-ACC) eat
(Liking pasta, (0) eats (0) every day)
</equation>
<page confidence="0.983911">
924
</page>
<note confidence="0.834286916666667">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 924–934,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
Zero pronoun Referent Example
in the document
Zero endophora Exist Exist 僕はカフェが好きで毎日 (カフェ ニ)通っている。
(I like cafes and go (to a cafe) everyday.)
Zero exophora Exist Not exist 私がメリットを ([reader] ニ)
説明させていただきます。
(I would like to explain the advantage (to [reader]).)
No zero reference Not exist Not exist あなたはリラックスタイムが (×ニ)過ごせる。
(You can have a relaxing time.)
*There is no dative case.
</note>
<tableCaption confidence="0.998982">
Table 1: Examples of zero endophora, zero exophora and no zero reference.
</tableCaption>
<bodyText confidence="0.9761135">
the document, but the author is not mentioned ex-
plicitly.
</bodyText>
<equation confidence="0.9719246">
(2) 最近は パソコンで 動画を
recently PC-INS movie-ACC
([unspecified:person] ガ) 見れる。
([unspecified:person]-NOM) can see
(Recently, (people) can see movies by a PC.)
</equation>
<bodyText confidence="0.977448763157895">
Similarly, in example (2), the referent of the zero
pronoun of the nominative case of “A*16” (can
see) is an unspecified person.2
Most previous studies have neglected zero ex-
ophora, as though a zero pronoun does not exist in
a sentence. However, such a rough approximation
has impeded the zero reference resolution research.
In Table 1, in “zero exophora,” the dative case of
the predicate has the zero pronoun, but in “no zero
reference,” the dative case of the predicate does not
have a zero pronoun. Treating them with no dis-
tinction causes a decrease in accuracy of machine
learning-based zero pronoun detection due to a gap
between the valency of a predicate and observed ar-
guments of the predicate. In this work, to deal with
zero exophora explicitly, we provide pseudo entities
such as [author], [reader] and [unspecified:person]
as candidate referents of zero pronouns.
In the referent identification, selectional prefer-
ences of a predicate (Sasano et al., 2008; Sasano and
Kurohashi, 2011) and contextual information (Iida
et al., 2006) have been widely used. The author and
reader (A/R) of a document have not been used for
contextual clues because the A/R rarely appear in
the discourse in corpora based on newspaper arti-
cles, which are main targets of the previous studies.
However, in other domain documents such as blog
2In the following examples, omitted arguments are put in
parentheses and exophoric referents are put in square brackets.
articles and shopping sites, the A/R often appear in
the discourse. The A/R tend to be omitted and there
are many clues for the referent identification about
the A/R such as honorific expressions and modal-
ity expressions. Therefore, it is important to deal
with the A/R of a document explicitly for the refer-
ent identification.
The A/R appear as not only the exophora but also
the endophora.
</bodyText>
<equation confidence="0.9285653">
(3) 僕 author は 京都に (僕ガ)
I-TOP Kyoto-DAT (I-NOM)
行こうと 思っています。
will go have thought
(I have thought (I) will go to Kyoto.)
皆さん reader は どこに 行きたいか
you all-TOP where-DAT want to go
(皆さんガ) (僕ニ) 教えてください。
(you all-NOM) (I-DAT) let me know
(Please let (me) know where do you want to go.)
</equation>
<bodyText confidence="0.99793875">
In example (3), “M” (I), which is explicitly men-
tioned in the document, is the author of the docu-
ment and “��/v” (you all) is the reader. In this pa-
per, we call these expressions, which refer to the au-
thor and reader, author mention and reader men-
tion. We treat them explicitly to improve the per-
formance of zero reference resolution. Since the
A/R are mentioned as various expressions besides
personal pronouns in Japanese, it is difficult to de-
tect the A/R mentions based merely on lexical in-
formation. In this work, we automatically detect
the A/R mentions by using a learning-to-rank al-
gorithm(Herbrich et al., 1998; Joachims, 2002) that
uses lexico-syntactic patterns as features.
Once the A/R mentions can be detected, their in-
formation is useful for the referent identification.
</bodyText>
<page confidence="0.997495">
925
</page>
<bodyText confidence="0.999947653846154">
The A/R mentions have both a property of the dis-
course element mentioned in a document and a prop-
erty of the zero exophoric A/R. In the first sentence
of example (3), it can be estimated that the referent
of the zero pronoun of the nominative case of “行こ
う” (will go) from a contextual clue that “僕” (I) is
the topic of this sentence and a syntactic clues that “
僕” (I) depends on “思っています” (have thought)
over the predicate “行こう” (will go).3 Such con-
textual clues can be available only for the discourse
entities that are mentioned explicitly. On the other
hand, in the second sentence, since “教えてくださ
い” (let me know) is a request form, it can be as-
sumed that the referent of the zero pronoun of the
nominative case is “僕” (I), which is the author,
and the one of the dative case is “皆様” (you all),
which is the reader. The clues such as request forms,
honorific expressions and modality expressions are
available for the author and reader. In this work, to
represent such aspect of the A/R mentions, both the
endophora and exophora features are given to them.
In this paper, we propose a zero reference reso-
lution model considering the zero exophora and the
author/reader mentions, which resolves the zero ref-
erence as a part of a predicate-argument structure
analysis.
</bodyText>
<sectionHeader confidence="0.999732" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99993305882353">
Several approaches to Japanese zero reference reso-
lution have been proposed.
Iida et al. (2006) proposed a zero reference resolu-
tion model that uses the syntactic relations between
a zero pronoun and a candidate referent as a feature.
They deal with zero exophora by judging that a zero
pronoun does not have anaphoricity. However, the
information of zero pronoun existences is given and
thus they did not address zero pronoun detection.
Zero reference resolution has been tackled as a
part of predicate-argument structure analysis. Ima-
mura et al. (2009) proposed a predicate-argument
structure analysis model based on a log-linear model
that simultaneously conducts zero endophora resolu-
tion. They assumed a particular candidate referent,
NULL, and when the analyzer selected this refer-
ent, the analyzer outputs “zero exophora or no zero
</bodyText>
<footnote confidence="0.752928333333333">
3Since “IR” (I) depends on “��Z J:T” (have thought),
the relation between “IR” (I) and “TT=7” (will go) is the zero
reference.
</footnote>
<bodyText confidence="0.999727923076923">
pronoun,” in which they are treated without distinc-
tion. Sasano et al. (2008) proposed a probabilis-
tic predicate-argument structure analysis model in-
cluding zero endophora resolution by using wide-
coverage case frames constructed from a web cor-
pus. Sasano and Kurohashi (2011) extended the
Sasano et al. (2008)’s model by focusing on zero en-
dophora. Their model is based on a log-linear model
that uses case frame information and the location of
a candidate referent as features. In their work, zero
exophora is not treated and they assumed that a zero
pronoun is absent when there is no referent in a doc-
ument.
For languages other than Japanese, zero pronoun
resolution methods have been proposed for Chinese,
Portuguese, Spanish and other languages. In Chi-
nese, Kong and Zhou (2010) proposed tree-kernel
based models for three subtasks: zero pronoun de-
tection, anaphoricity decision and referent selection.
In Portuguese and Spanish, only a subject word is
omitted and zero pronoun resolution has been tack-
led as a part of coreference resolution. Poesio et
al. (2010) and Rello et al. (2012) detected omitted
subjects and made a decision whether the omitted
subject has anaphoricity or not as preprocessing of
coreference resolution systems.
</bodyText>
<sectionHeader confidence="0.999138" genericHeader="method">
3 Baseline Model
</sectionHeader>
<bodyText confidence="0.999922294117647">
In this section, we describe a baseline zero refer-
ence resolution system. In our model, the zero refer-
ence resolution is conducted as a part of predicate-
argument structure (PAS) analysis. The PAS con-
sists of a case frame and an alignment between case
slots and referents. The case frames are constructed
for each meaning of a predicate. Each case frame
describes surface cases that each predicate has (case
slot) and words that can fill each case slot (exam-
ple). In this study, the case frames are constructed
from 6.9 billion Web sentences by using Kawahara
and Kurohashi (2006a)’s method.
The baseline model does not treat zero exophora
as the previous studies. The baseline model analyzes
a document in the following procedure in the same
way as the previous study (Sasano and Kurohashi,
2011).4
</bodyText>
<footnote confidence="0.9727045">
4For learning, the previous study used a log-linear model,
but we use a learning-to-rank model. In our preliminary exper-
</footnote>
<page confidence="0.994298">
926
</page>
<figure confidence="0.996293904761905">
� �
行きます。
go
(I like a curry shop in Kyoto station and often go to the shop.)
今日は 皆さんに (カレー屋ヲ) 紹介します。
Today-TOP you all-DAT (curry shop-ACC) will introduce
(Today, I will introduce (the shop) to you.)
Discourse entities ✏
✓{ 京都駅 (Kyoto station)}, { カレー屋 (curry shop), その店 (the shop)}, { 今日 (today)},
{ 皆さん (you all)}
✒ ✑
✓ Candidate predicate-argument structures of “紹介します” in the baseline model ✏
[1-1] case frame:[紹介する (1)], { NOM:Null, ACC:Null, DAT:皆さん, TIME:今日 }
[1-2] case frame:[紹介する (1)], { NOM:Null, ACC:カレー屋, DAT:皆さん, TIME:今日 }
[1-3] case frame:[紹介する (1)], { NOM:京都駅, ACC:カレー屋, DAT:皆さん, TIME:今日 }
...
[2-1] case frame:[紹介する (2)], { NOM:Null, ACC:Null, DAT:皆さん, TIME:今日 }
[2-2] case frame:[紹介する (2)], { NOM:Null, ACC:カレー屋, DAT:皆さん, TIME:今日 }
.
..
� ✒ ✑ �
</figure>
<figureCaption confidence="0.99961">
Figure 1: Examples of discourse entities and predicate-argument structures
</figureCaption>
<figure confidence="0.9673895">
京都駅に ある
Kyoto station-DAT stand
curry shop-NOM like
カレー屋が 好きで、
その店に よく
the shop often
</figure>
<listItem confidence="0.996975818181818">
1. Parse the input document and recognize named
entities.
2. Resolve coreferential relations and set dis-
course entities.
3. Analyze the predicate-argument structure for
each predicate using the following steps:
(a) Generate candidate predicate-argument
structures.
(b) Calculate the score of each predicate-
argument structure and select the one with
the highest score.
</listItem>
<bodyText confidence="0.999941404761905">
We illustrate the details of the above procedure.
First, we describe how to set the discourse entities
in step 2. In our model, we treat referents of a zero
pronoun using a unit called discourse entity, which
is what mentions in a coreference chain are bound
into. In Figure 1, we treat “カレー屋” (curry shop)
and “その店” (the shop), which are in a coreference
chain, as one discourse entity. In Figure 1, the dis-
course entity { カレー屋, その店 } is selected for
the referent of the accusative case of the predicate “
紹介します” (will introduce).
Next, we illustrate the PAS analysis in step 3. In
step 3a, possible combinations of the case frame
(cf) and the alignment (a) between case slots and
iment of the baseline model, there is little difference between
the results of these methods.
discourse entities are listed. First, one case frame is
selected from case frames for the predicate. Next,
overt arguments, which have dependency relations
with the predicate, are aligned to a case slot of the
case frame. Finally, each of zero pronouns of re-
maining case slots is assigned to a discourse entity
or is not assigned to any discourse entities. The case
slot whose zero pronoun is not assigned to any dis-
course entities corresponds to the case that does not
have a zero pronoun. In Figure 1, we show the ex-
amples of candidate PASs. In these examples, [紹介
する (1)] and [紹介する (2)] are case frames corre-
sponding to each meaning of “紹介する”. Referents
of each case slot are actually selected from discourse
entities but are explained as a representative word
for illustration. “Null” indicates that a case slot is
not assigned to any discourse entities. Since align-
ments between case slots and discourse entities of
the PAS [1-2] and [2-2] are the same but their case
frames are different, we deal with them as discrete
PASs. In this case, however, the results of zero ref-
erence resolution are the same.
We represent each PAS as a feature vector, which
is described in section 3.1, and calculate a score of
each PAS with the learned weights. Finally, the sys-
tem outputs the PAS with the highest score.
</bodyText>
<page confidence="0.990353">
927
</page>
<table confidence="0.9998472">
Type Value Description
Case Log Probabilities that {words, categories and named entity types} of e is assigned to c of cf
frame
Log Generative probabilities of {words, categories and named entity types} of e
Log PMIs between {words, categories and named entity types} of e and c of cf
Log Max of PMIs between {words, categories and named entity types} of e and c of cf
Log Probability that c of cf is assigned to any words
Log Ratio of examples of c to ones of cf
Binary c of cf is {adjacent and obligate} case
Predicate Binary Modality types of p
Binary Honorific expressions of p
Binary Tenses of p
Binary p is potential form
Binary Modifier of p (predicate, noun and end of sentence)
Binary p is {dynamic and stative} verb
Context Binary Named entity types of e
Integer Number of mentions about e in t
Integer Number of mentions about e {before and after} p in t
Binary e is mentioned with post position “b:” in a target sentence
Binary Sentence distances between e and p
Binary Location categories of e (Sasano and Kurohashi, 2011)
Binary e is mentioned at head of a target sentence
Binary e is mentioned with post position {“b:” and “が” } at head of a target sentence
Binary e is mentioned at head of the first sentence
Binary e is mentioned with post position “b:” at head of the first sentence
Binary e is mentioned at end of the first sentence
Binary e is mentioned with copula at end of the first sentence
Binary e is mentioned with noun phrase stop at end of the first sentence
Binary Salience score of e is larger than 1 (Sasano and Kurohashi, 2011)
other Binary c is assigned
</table>
<tableCaption confidence="0.999629">
Table 2: The features of Oassigned(cf, c — e, p, t)
</tableCaption>
<subsectionHeader confidence="0.99612">
3.1 Feature Representation of
Predicate-Argument Structure
</subsectionHeader>
<bodyText confidence="0.996537823529412">
When text t and target predicate p are given and PAS
(cf, a) is chosen, we represent a feature vector of the
PAS as φ(cf, a, p, t). φ(cf, a, p, t) consists of a fea-
ture vector φovert-PAS(cf, a, p, t) and feature vec-
tors φ(cf, c/e, p, t). Where φovert-PAS(cf, a, p, t)
corresponds to alignment between case slots and
overt (not omitted) arguments and φ(cf, c/e, p, t)
represents that a case slot c is assigned to a discourse
entity e. If a case slot is assigned to an overt entity,
φ(cf, c/e, p, t) is set to a zero vector.
Each feature vector φ(cf, c/e, p, t) consists
of φA(cf, c/e, p, t) and φNA(cf, c/Null, p, t).
φA(cf, c/e, p, t) becomes active when the case
slot c is assigned to the discourse entity e and
φNA(cf, c/Null, p, t) becomes active when the
case slot c is not assigned to any discourse entities.
For example, the PAS [1-2] in Figure 1 is repre-
</bodyText>
<equation confidence="0.961006333333334">
sented as:
(φovert-PAS(紹介する (1), {NOM:Null, ACC:Null,
NOM:皆さん, TIME:今日 }),00A,
φNA(紹介する (1),NOM/Null),
φA(紹介する (1),ACC/カレー屋),
00NA,00A,00NA). 5
</equation>
<bodyText confidence="0.999231307692308">
In our feature representation, the second and third
terms correspond to the nominative case, the forth
and fifth ones correspond to the accusative and the
sixth and seventh ones correspond to the dative
case.
We present the details of φovert-PAS(cf, a, p, t),
φA(cf, c/e, p, t) and φNA(cf, c/Null, p, t). We use
a score of the probabilistic PAS analysis (Kawahara
and Kurohashi, 2006b) to φovert-PAS(cf, a, p, t).
We list the features of φA(cf, c/e, p, t) in Table 2
and the features of φNA(cf, c/Null, p, t) in Table
SIn the following example, p and t are sometimes omitted
and 00is 0 vector that has the same dimension as 0.
</bodyText>
<page confidence="0.987279">
928
</page>
<table confidence="0.975562285714285">
Description
Probability that c of cf is
not assigned
Ratio of number of examples
of c to ones of cf
cofcf is
{adjacent and obligate} case
</table>
<tableCaption confidence="0.99968">
Table 3: The features of ONA(cf, c/Null, p, t)
</tableCaption>
<sectionHeader confidence="0.54585" genericHeader="method">
3.
</sectionHeader>
<subsectionHeader confidence="0.999365">
3.2 Weight Learning
</subsectionHeader>
<bodyText confidence="0.999623333333333">
In the previous section, we defined the feature vec-
tor 0(cf, a, p, t), which represents a PAS. In this
section, we illustrate the learning method of the
weight vector corresponding to the feature vector.
The weight vector is learned by using a learning-to-
rank algorithm.
In a corpus, gold-standard alignments a* are man-
ually annotated but case frames are not annotated.
Since the case frames are constructed for each mean-
ing, some of them are unsuitable for a usage of a
prdicate in a context. If training data includes PASs
(cf, a*) whose cf is such case a frame as correct
instances, these are harmful for training. Hence,
we treat a case frame cf* which is selected by a
heuristic method as a correct case frame and remove
(cf, a*) which has other cf.
In particular, we make ranking data for the learn-
ing for each target predicate p in the following steps.
</bodyText>
<listItem confidence="0.970763375">
1. List possible PASs (cf, a) for predicate p.
2. Calculate a probabilistic zero reference resolu-
tion score for each (cf, a*) and define the one
with highest score as (cf*, a*).
3. Remove (cf, a*) except (cf*, a*) from the
learning instance.
4. Make ranking data that (cf*, a*) has a higher
rank than other (cf, a).
</listItem>
<bodyText confidence="0.999777">
In the above steps, we make ranking data for each
predicate and use the ranking data collected from all
target predicates as training data.
</bodyText>
<sectionHeader confidence="0.99792" genericHeader="method">
4 Corpus
</sectionHeader>
<bodyText confidence="0.999961391304348">
In this work, we use Diverse Document Leads Cor-
pus (DDLC) (Hangyo et al., 2012) for experiments.
In DDLC, documents collected from the web are
annotated with morpheme, syntax, named entity,
coreference, PAS and A/R mention. Morpheme,
syntax, named entity, coreference and PAS are an-
notated on the basis of Kyoto University Text Cor-
pus (Kawahara et al., 2002). The PAS annotation in-
cludes zero reference information and the exophora
referents are defined as five elements, [author],
[reader], [US(unspecified):person], [US:matter] and
[US:situation]. The A/R mentions are annotated
to head phrases of compound nouns when the A/R
mentions consist of compound nouns. If the A/R
is mentioned by multiple expressions, only one of
them is annotated with the A/R mention tag and all
of these mentions are linked by a coreference chain.
In other words, the A/R mentions are annotated to
discourse entities. In the web site of an organiza-
tion such as a company, the site administrator often
writes the document on behalf of the organization.
In such a case, the organization is annotated as the
author.
</bodyText>
<sectionHeader confidence="0.995649" genericHeader="method">
5 Author/Reader Mention Detection
</sectionHeader>
<bodyText confidence="0.9997388">
A/R mentions, which refer to A/R of a document,
have different properties from other discourse enti-
ties. The A/R are mentioned as very various expres-
sions such as personal pronouns, proper expressions
and role expressions.
</bodyText>
<figure confidence="0.683163">
(4) `/vtZ-15tJ:, _&apos;u-)&apos;--Aon
Hello project team-GEN
7+ author Z_5J_.
am Umetsuji
(Hello, I’m Umetsuji on the project team.)
(5) REffi W�_x N IVI author Z
problem-NOM exist to moderator
430�,tくtZtt &amp;quot;.
</figure>
<bodyText confidence="0.971536">
let me know
(Please let me know if there are any problems.)
In example (4), the author is mentioned as “OLP”
(Umetsuji), which is the name of the author, and in
example (5), the author is mentioned as “fITIA”
(moderator), which expresses the status of the au-
thor. Likewise, the reader is sometimes mentioned
as “���” (customer) and others. However, since
such expressions often refer to someone other than
the A/R, whether an expression indicates the A/R of
a document depends on the context of the document.
In English and other languages, the A/R mentions
can be detected from coreference information be-
cause it can be assumed that the expression that has
</bodyText>
<figure confidence="0.93255125">
Type Value
Case frame Log
Log
Binary
</figure>
<page confidence="0.994191">
929
</page>
<bodyText confidence="0.999757083333333">
a coreference relation with first or second personal
pronoun is the A/R mention. However, since the
A/R tend to be omitted and personal pronouns are
rarely used in Japanese, it is difficult to detect the
A/R mentions from coreference information. Be-
cause of these reasons, it is difficult to detect which
discourse entity is the A/R mention from lexical in-
formation of the entities. In this study, the A/R men-
tions are detected from lexico-syntactic (LS) pat-
terns in the document. We use a learning-to-rank
algorithm to detect A/R mentions by using the LS
patterns as features.
</bodyText>
<subsectionHeader confidence="0.936999">
5.1 Author/Reader Detection Model
</subsectionHeader>
<bodyText confidence="0.999984678571429">
We use a learning-to-rank method for detecting A/R
mentions. This method learns the ranking that en-
tities of the A/R mentions have a higher rank than
other discourse entities. Here, it is an important
point that there are no A/R mentions in some doc-
uments. The documents in which the A/R mentions
do not appear are classified into two types. The first
type is a document that the A/R do not appear in
the discourse of the document such as newspaper ar-
ticles and novels. The second type is a document
that the A/R appear in the discourse but all of their
mentions are omitted. For example, in Figure 1, the
author appears in the discourse (e.g. the nominative
argument of “like”) but is not mentioned explicitly.
We introduce two pseudo entities corresponding to
these types. The first pseudo entity “no A/R men-
tion (discourse)” represents the document that the
A/R do not appear in the discourse. It is considered
that the document that the A/R do not appear have
characteristics of writing style such that honorific
expressions and request expressions are rarely used.
This pseudo entity is represented as a document vec-
tor that consists of LS pattern features of the whole
document, which reflect a writing style of a doc-
ument. The second pseudo entity “no A/R men-
tion (omitted)” represents the document in which all
mentions of the A/R are omitted and this pseudo en-
tity is represented as 0 vector. Since a decision score
of this pseudo entity is allways 0, discourse entities
whose score is lower than the score of this pseudo
entity can be treated as a negative example in a bi-
nary classification.
When there are A/R mentions in a document, we
make ranking data where the discourse entity of
the A/R mention has a higher rank than other dis-
course entities and “no A/R mention” pseudo enti-
ties. When the A/R do not appear in the discourse,
we make ranking data where “no A/R mention (dis-
course)” has a higher rank than all discourse enti-
ties and “no A/R mention (omitted)”. When the A/R
appear in the discourse but all mentions are omit-
ted, we make ranking data where “no A/R mention
(omitted)” has a higher rank than all discourse en-
tities and “no A/R mention (discourse)”. We judge
that the A/R appear in the discourse if the A/R ap-
pear as a referent of zero reference in gold-standard
PASs and this judgment is used only in the training
phase. After making the ranking data for each doc-
ument, all of the ranking data are merged and the
merged data is fed into the learning-to-rank model.
For the A/R mention detection, we calculate the
score of all discourse entities and the pseudo entities
and select the discourse entity with the highest score
to the A/R mention. If any “no A/R mention” have
the highest score, we decide that there are no A/R
mentions in the document.
</bodyText>
<subsectionHeader confidence="0.988411">
5.2 Lexico-Syntactic Patterns
</subsectionHeader>
<bodyText confidence="0.999957458333333">
For each discourse entity, phrases of the discourse
entity, its parent and their dependency relations are
used to make LS patterns that represent the discourse
entity. When a discourse entity is mentioned multi-
ple times, the phrases of all mentions are used to
make the LS patterns. LS patterns of phrases are
made by generalizing these phrases on various lev-
els (types). LS patterns of dependencies are made
from combining the LS patterns of phrases.
Table 4 lists generalization types. On the word
type, we make a phrase LS pattern by generalizing
each content word and jointing them. For example, a
LS pattern of the phrase “6TC6t” generalized on the
&lt;representative form&gt; is “IR6t”. The word+ type
is the same as word except all content words are gen-
eralized on the &lt;part of speech and conjugation&gt;.
For example, a LS pattern of the dependency rela-
tion “)�R�6t --� k-_,k” generalized on the &lt;named
entity&gt; is “NE:PERSON+6t —* verb:past”. We also
use the LS patterns of generalized individual mor-
phemes. On the phrase type, each phrase is gener-
alized according to the information assigned to the
phrase and all content words are generalized on the
&lt;part of speech and conjugation&gt; if the information
</bodyText>
<page confidence="0.989699">
930
</page>
<table confidence="0.9999235">
Unit Type Example (original phrase)
word &lt;no generalization&gt; 僕は (僕は)
&lt;original form&gt; 走った (走る)
&lt;representative form&gt; 僕は (ぼくは)
&lt;part of speech and conjugation&gt; verb:past (走った)
word+ &lt;category&gt; Category:PERSON+は (僕は)
&lt;named entity&gt; NE:PERSON+は (太郎は)
&lt;first person pronoun&gt; FirstPersonPronoun+は (僕は)
&lt;second person pronoun&gt; SecondPersonPronoun+に (あなたに)
phrase &lt;modality&gt; modality:request (お問い合わせください)
&lt;honorific expression&gt; honorific:modest (お送りします)
&lt;attached words&gt; ください (お問い合わせください)
</table>
<tableCaption confidence="0.999867">
Table 4: Generalization types of the LS patterns
</tableCaption>
<bodyText confidence="0.9982815">
is not assigned to the phrase.
For “no A/R mention (discourse)” instance, the
above features of all mentions, including verbs and
adjectives, and their dependencies in the document
are gathered and used as the features representing
the instance.
</bodyText>
<sectionHeader confidence="0.924226" genericHeader="method">
6 Zero Reference Resolution Considering
</sectionHeader>
<subsectionHeader confidence="0.607968">
Exophora and Author/Reader Mentions
</subsectionHeader>
<bodyText confidence="0.999170857142857">
In this section, we describe the zero reference reso-
lution system that considers the zero exophora and
the A/R mentions. The proposed model resolves
zero reference as a part of the PAS analysis based
on the baseline model.
The proposed model analyzes the PASs in the fol-
lowing steps:
</bodyText>
<listItem confidence="0.958090222222222">
1. Parse the input document and recognize named
entities.
2. Resolve coreferential relations and set dis-
course entities.
3. Detect the A/R mentions of the document.
4. Set pseudo entities from the estimated A/R
mentions.
5. Analyze the PAS for each predicate using the
same procedure as the baseline model.
</listItem>
<bodyText confidence="0.999643">
The differences form baseline model are the estima-
tion of the A/R mentions in step 3 and the setting of
pseudo entities in step 4.
</bodyText>
<subsectionHeader confidence="0.9997315">
6.1 Pseudo Entities and Author/Reader
Mentions for Zero Exophora
</subsectionHeader>
<bodyText confidence="0.999715">
In the baseline model, referents of zero pronouns
are selected form discourse entities. The proposed
model adds pseudo entities([author], [reader],
[US.-person] (unspecified:person) and [US.-others]
(unspecified:others)6) to deal with zero exophora.
When the A/R mentions appear in a document,
the A/R pseudo entities raise an issue. The zero en-
dophora are given priority to zero exophora. In other
words, the A/R mentions are selected to the referents
in preference to pseudo entities when there are A/R
mentions. Therefore, when the system estimates that
A/R mentions appear, the A/R pseudo entities are
not created.
In the PAS analysis, referents are selected from
discourse entities and the pseudo entities. A zero
reference is the zero exophora when a case slot is
assigned to pseudo entities. Candidate PASs of “紹
介します” in Figure 1 are shown in Figure 2.
</bodyText>
<subsectionHeader confidence="0.999793">
6.2 Feature Representation of Predicate
Argument Structure
</subsectionHeader>
<bodyText confidence="0.886086333333333">
In the same way as the baseline model, the
proposed model represents a PAS as a fea-
ture vector that consists of the feature vector
φovert-PAS(cf, a, p, t) and the feature vectors
φ(cf, c/e, p, t). The difference from the baseline
model is a composition of φA(cf,c/e,p,t). In the
proposed model, each φA(cf, c/e) is composed of
vectors, φdiscourse(cf, c/e), φ[author](cf, c/e),
φ[reader](cf,c/e), φ[�S:person](cf,c/e),
φ[�S:others](cf,c/e) and φ..ax(cf,c/e). Their
contents and dimensions are the same and similar to
φA(cf, c/e) of the baseline model the except for the
</bodyText>
<footnote confidence="0.8723325">
6We merge [US.-matter] and [US.-situation] because of the
small amount of [US.-situation] in the corpus.
</footnote>
<page confidence="0.994645">
931
</page>
<figure confidence="0.997776636363636">
��
[1-1] case frame:[紹介する (1)], { NOM:[author], ACC:Null, DAT:皆さん reader, TIME:今日 }
[1-2] case frame:[紹介する (1)], { NOM:[US:person], ACC:Null, DAT:皆さん reader, TIME:今日 }
[1-3] case frame:[紹介する (1)], { NOM:[author], ACC:カレー屋, DAT:皆さん reader, TIME:今日 }
[1-4] case frame:[紹介する (1)], { NOM:京都駅, ACC:カレー屋, DAT:皆さん reader, TIME:今日 }
[1-5] case frame:[紹介する (1)], { NOM:[author], ACC:[US:others], DAT:皆さん reader, TIME:今日 }
...
[2-1] case frame:[紹介する (2)], { NOM:[author], ACC:Null, DAT:皆さん reader, TIME:今日 }
[2-2] case frame:[紹介する (2)], { NOM:[US:person], ACC:Null, DAT:皆さん reader, TIME:今日 }
...
� �
</figure>
<figureCaption confidence="0.999036">
Figure 2: Candidate predicate-argument structures of “紹介します” in the proposed model
</figureCaption>
<table confidence="0.998035222222222">
Expressions Categories
author 私 (I), 我々 (we), 俺 (I), 僕 (I), PERSON, ORGANIZATION
当社 (our company), 弊社 (our company), 当店 (our shop)
reader PERSON
あなた (you), 客 (customer), 君 (you), 皆様 (you all),
皆さん (you all), 方 (person), 方々 (people)
US:person 人 (person), 人々 (people) PERSON
US:others もの (thing),状況 (situation) all categories except
PERSON and ORGANIZATION
</table>
<tableCaption confidence="0.999453">
Table 5: Expressions and categories for pseudo entities
</tableCaption>
<bodyText confidence="0.997682">
addition of a few features described in section 6.3.
Odiscourse corresponds to the discourse entities,
which are mentioned explicitly and becomes active
when e is a discourse entity including the A/R men-
tions. Odiscourse is the same as OA of the base-
line model and the difference is explained in section
6.3. O[author] and φ[reader] become active when e is
[author]/[reader] or the discourse entity correspond-
ing to the A/R mention. In particular, when e is
the discourse entity corresponding to the A/R men-
tion, both Odiscourse and O[author]/O[reader] become
active. This representation gives the A/R mentions
the properties of the discourse entity and the A/R.
O[US:person] and O[US:others] become active when e
is [US.-person] and [US.-others].
</bodyText>
<subsubsectionHeader confidence="0.779409">
Because O[author], O[reader], O[US:person] and
</subsubsectionHeader>
<bodyText confidence="0.999872">
O[US:others] correspond to the pseudo entities, which
are not mentioned explicitly, we cannot use word in-
formation such as expressions and categories. We
assume that the pseudo entities have expressions and
categories shown in Table 5 and use these to cal-
culate case frame features. Finally, O..ax consists
of the highest value of correspondent feature of the
above feature vectors.
</bodyText>
<subsectionHeader confidence="0.99043">
6.3 Author/Reader Mention Score
</subsectionHeader>
<bodyText confidence="0.9999315">
We add A/R mention score features to the feature
vector OA(cf, c/e, p, t) described in Table 2. The
A/R mention scores are the discriminant function
scores of the A/R mention detection. When e is the
A/R mention, we set the A/R mention score to the
feature.
</bodyText>
<sectionHeader confidence="0.999909" genericHeader="method">
7 Experiments
</sectionHeader>
<subsectionHeader confidence="0.994572">
7.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999971857142857">
We used 1,000 documents from DDLC and per-
formed 5-fold cross-validation. 1,440 zero en-
dophora and 1,935 zero exophora are annotated in
these documents. 258 documens are annotated with
author mentions and 105 documens are annotated
with reader mentions. We used gold-standard (man-
ually annotated) morphemes, named entities, depen-
dency structures and coreference relations to focus
on the A/R detection and the zero reference resolu-
tion. We used SVMrank7 for the learning-to-rank
method of the A/R detection and the PAS analysis.
The categories of words are given by the morpho-
logical analyzer JUMAN8. Named entities and pred-
icate features (e.g., honorific expressions, modality)
</bodyText>
<footnote confidence="0.999968">
7http://www.cs.cornell.edu/people/tj/svm light/svm rank.html
8http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
</footnote>
<page confidence="0.98883">
932
</page>
<figure confidence="0.992712761904762">
Wrong
Correct
Gold
112
704
Exist
None
-standard
6
38
140
-
Recall Precision F1
Baseline 0.269 0.377 0.314
Proposed model 0.282 0.448 0.346
(estimate) 0.388 0.522 0.445
Proposed model
(gold-standard)
System output
Exist
None
</figure>
<tableCaption confidence="0.997472">
Table 6: Result of the author mention detection
System output
Table 7: Result of the reader mention detection
</tableCaption>
<bodyText confidence="0.773984">
are given by the syntactic parser KNP.9
</bodyText>
<sectionHeader confidence="0.7707985" genericHeader="evaluation">
7.2 Results of Author/Reader Mention
Detection
</sectionHeader>
<bodyText confidence="0.999967714285714">
We show the results of the author and reader men-
tion detection in Table 6 and Table 7. In these tables,
“exist” indicates numbers of documents in which the
A/R mentions are manually annotated or our system
estimated that some discourse entities are A/R men-
tions. From these results, the A/R mentions includ-
ing “none” can be predicted to accuracies of approx-
imately 80%. On the other hand, the recalls are not
particularly high: the recall of author is 140/258 and
the recall of reader is 56/105. This is because the
documents in which the A/R do not appear are more
than the ones in which the A/R appear and the sys-
tem prefers to output “no author/reader mention” as
the result of training.
</bodyText>
<subsectionHeader confidence="0.973112">
7.3 Results of Zero Reference Resolution
</subsectionHeader>
<bodyText confidence="0.999542153846154">
We show the results of zero reference resolution
in Table 8 and Table 9. The difference between
the baseline and the proposed model is statistically
significant (p &lt; 0.05) from the McNemar’s test.
In Table 8, we evaluate only the zero endophora
for comparison to the baseline model, which deals
with only the zero endophora. “Proposed model
(estimate)” shows the result of the proposed model
which estimated the A/R mentions and “Proposed
model (gold-standard)” shows the result of the pro-
posed model which is given the A/R mentions of
gold-standard from the corpus.
From Table 8, considering the zero exophora and
</bodyText>
<footnote confidence="0.667305">
9http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP
</footnote>
<tableCaption confidence="0.997179">
Table 8: Results of zero endophora resolution
</tableCaption>
<table confidence="0.998490166666667">
Recall Precision F1
Baseline 0.115 0.377 0.176
Proposed model 0.317 0.411 0.358
(estimate) 0.377 0.485 0.424
Proposed model
(gold-standard)
</table>
<tableCaption confidence="0.999788">
Table 9: Results of zero reference resolution
</tableCaption>
<bodyText confidence="0.999822888888889">
the A/R mentions improves accuracy of zero en-
dophora resolution as well as zero reference reso-
lution including zero exophora.
From Table 8 and Table 9, the proposed model
given the gold-standard A/R mentions achieves ex-
traordinarily high accuracies. This result indicates
that improvement of the A/R mention detection im-
proves the accuracy of zero reference resolution in
the proposed model.
</bodyText>
<sectionHeader confidence="0.998219" genericHeader="acknowledgments">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999955285714286">
This paper presented a zero reference resolution
model considering exophora and author/reader men-
tions. In the experiments, our proposed model
achieves higher accuracy than the baseline model.
As future work, we plan to improve the au-
thor/reader detection model to improve the zero ref-
erence resolution.
</bodyText>
<sectionHeader confidence="0.99898" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998545571428571">
Masatsugu Hangyo, Daisuke Kawahara, and Sadao Kuro-
hashi. 2012. Building a diverse document leads
corpus annotated with semantic relations. In Pro-
ceedings of the 26th Pacific Asia Conference on Lan-
guage, Information, and Computation, pages 535–
544, Bali,Indonesia, November. Faculty of Computer
Science, Universitas Indonesia.
Ralf Herbrich, Thore Graepel, Peter Bollmann-Sdorra,
and Klaus Obermayer. 1998. Learning preference re-
lations for information retrieval. In ICML-98 Work-
shop: text categorization and machine learning, pages
80–84.
Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Ex-
ploiting syntactic patterns as clues in zero-anaphora
</reference>
<figure confidence="0.9968152">
Exist
Correct Wrong
Gold Exist 56 2 47
-standard None - 23 872
None
</figure>
<page confidence="0.991006">
933
</page>
<reference confidence="0.999925735294118">
resolution. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 625–632, Sydney, Australia, July.
Association for Computational Linguistics.
Kenji Imamura, Kuniko Saito, and Tomoko Izumi. 2009.
Discriminative approach to predicate-argument struc-
ture analysis with zero-anaphora resolution. In Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short
Papers, pages 85–88, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 133–142.
ACM.
Daisuke Kawahara and Sadao Kurohashi. 2006a.
Case frame compilation from the web using high-
performance computing. In Proceedings of the 5th
International Conference on Language Resources and
Evaluation, pages 1344–1347.
Daisuke Kawahara and Sadao Kurohashi. 2006b. A
fully-lexicalized probabilistic model for japanese syn-
tactic and case structure analysis. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 176–183, New York
City, USA, June. Association for Computational Lin-
guistics.
Daisuke Kawahara, Sadao Kurohashi, and Koiti Hasida.
2002. Construction of a japanese relevance-tagged
corpus. In Proc. of The Third International Confer-
ence on Language Resources Evaluation, May.
Fang Kong and Guodong Zhou. 2010. A tree kernel-
based unified framework for chinese zero anaphora
resolution. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 882–891, Cambridge, MA, October. Associa-
tion for Computational Linguistics.
Massimo Poesio, Olga Uryupina, and Yannick Versley.
2010. Creating a coreference resolution system for
italian. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC’10), Valletta, Malta, may. European Language
Resources Association (ELRA).
Luz Rello, Ricardo Baeza-Yates, and Ruslan Mitkov.
2012. Elliphant: Improved automatic detection of zero
subjects and impersonal constructions in spanish. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 706–715. Association for Computational
Linguistics.
Ryohei Sasano and Sadao Kurohashi. 2011. A dis-
criminative approach to japanese zero anaphora res-
olution with large-scale lexicalized case frames. In
Proceedings of 5th International Joint Conference on
Natural Language Processing, pages 758–766, Chiang
Mai, Thailand, November. Asian Federation of Natu-
ral Language Processing.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2008. A fully-lexicalized probabilistic model
for japanese zero anaphora resolution. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 769–776,
Manchester, UK, August. Coling 2008 Organizing
Committee.
</reference>
<page confidence="0.998502">
934
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.722307">
<title confidence="0.9923335">Japanese Zero Reference Considering Exophora and Author/Reader Mentions</title>
<author confidence="0.995352">Masatsugu Hangyo Daisuke Kawahara Sadao Kurohashi</author>
<affiliation confidence="0.8784565">Graduate School of Informatics, Kyoto Yoshida-honmachi,</affiliation>
<address confidence="0.933644">Kyoto, 606-8501,</address>
<abstract confidence="0.99957315">In Japanese, zero references often occur and many of them are categorized into zero exophora, in which a referent is not mentioned in the document. However, previous studies have focused on only zero endophora, in which a referent explicitly appears. We present a zero reference resolution model considering zero exophora and author/reader of a document. To deal with zero exophora, our model adds pseudo entities corresponding to zero exophora to candidate referents of zero pronouns. In addition, we automatically detect mentions that refer to the author and reader of a document by using lexico-syntactic patterns. We represent their particular behavior in a discourse as a feature vector of a machine learning model. The experimental results demonstrate the effectiveness of our model for not only zero exophora but also zero endophora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Masatsugu Hangyo</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Building a diverse document leads corpus annotated with semantic relations.</title>
<date>2012</date>
<booktitle>In Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation,</booktitle>
<pages>535--544</pages>
<institution>Faculty of Computer Science, Universitas Indonesia.</institution>
<location>Bali,Indonesia,</location>
<contexts>
<context position="19803" citStr="Hangyo et al., 2012" startWordPosition="3345" endWordPosition="3348">g data for the learning for each target predicate p in the following steps. 1. List possible PASs (cf, a) for predicate p. 2. Calculate a probabilistic zero reference resolution score for each (cf, a*) and define the one with highest score as (cf*, a*). 3. Remove (cf, a*) except (cf*, a*) from the learning instance. 4. Make ranking data that (cf*, a*) has a higher rank than other (cf, a). In the above steps, we make ranking data for each predicate and use the ranking data collected from all target predicates as training data. 4 Corpus In this work, we use Diverse Document Leads Corpus (DDLC) (Hangyo et al., 2012) for experiments. In DDLC, documents collected from the web are annotated with morpheme, syntax, named entity, coreference, PAS and A/R mention. Morpheme, syntax, named entity, coreference and PAS are annotated on the basis of Kyoto University Text Corpus (Kawahara et al., 2002). The PAS annotation includes zero reference information and the exophora referents are defined as five elements, [author], [reader], [US(unspecified):person], [US:matter] and [US:situation]. The A/R mentions are annotated to head phrases of compound nouns when the A/R mentions consist of compound nouns. If the A/R is m</context>
</contexts>
<marker>Hangyo, Kawahara, Kurohashi, 2012</marker>
<rawString>Masatsugu Hangyo, Daisuke Kawahara, and Sadao Kurohashi. 2012. Building a diverse document leads corpus annotated with semantic relations. In Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation, pages 535– 544, Bali,Indonesia, November. Faculty of Computer Science, Universitas Indonesia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Herbrich</author>
<author>Thore Graepel</author>
<author>Peter Bollmann-Sdorra</author>
<author>Klaus Obermayer</author>
</authors>
<title>Learning preference relations for information retrieval.</title>
<date>1998</date>
<booktitle>In ICML-98 Workshop: text categorization and machine learning,</booktitle>
<pages>80--84</pages>
<contexts>
<context position="6863" citStr="Herbrich et al., 1998" startWordPosition="1108" endWordPosition="1112">) In example (3), “M” (I), which is explicitly mentioned in the document, is the author of the document and “��/v” (you all) is the reader. In this paper, we call these expressions, which refer to the author and reader, author mention and reader mention. We treat them explicitly to improve the performance of zero reference resolution. Since the A/R are mentioned as various expressions besides personal pronouns in Japanese, it is difficult to detect the A/R mentions based merely on lexical information. In this work, we automatically detect the A/R mentions by using a learning-to-rank algorithm(Herbrich et al., 1998; Joachims, 2002) that uses lexico-syntactic patterns as features. Once the A/R mentions can be detected, their information is useful for the referent identification. 925 The A/R mentions have both a property of the discourse element mentioned in a document and a property of the zero exophoric A/R. In the first sentence of example (3), it can be estimated that the referent of the zero pronoun of the nominative case of “行こ う” (will go) from a contextual clue that “僕” (I) is the topic of this sentence and a syntactic clues that “ 僕” (I) depends on “思っています” (have thought) over the predicate “行こう”</context>
</contexts>
<marker>Herbrich, Graepel, Bollmann-Sdorra, Obermayer, 1998</marker>
<rawString>Ralf Herbrich, Thore Graepel, Peter Bollmann-Sdorra, and Klaus Obermayer. 1998. Learning preference relations for information retrieval. In ICML-98 Workshop: text categorization and machine learning, pages 80–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryu Iida</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Exploiting syntactic patterns as clues in zero-anaphora resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>625--632</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="5162" citStr="Iida et al., 2006" startWordPosition="813" endWordPosition="816"> reference,” the dative case of the predicate does not have a zero pronoun. Treating them with no distinction causes a decrease in accuracy of machine learning-based zero pronoun detection due to a gap between the valency of a predicate and observed arguments of the predicate. In this work, to deal with zero exophora explicitly, we provide pseudo entities such as [author], [reader] and [unspecified:person] as candidate referents of zero pronouns. In the referent identification, selectional preferences of a predicate (Sasano et al., 2008; Sasano and Kurohashi, 2011) and contextual information (Iida et al., 2006) have been widely used. The author and reader (A/R) of a document have not been used for contextual clues because the A/R rarely appear in the discourse in corpora based on newspaper articles, which are main targets of the previous studies. However, in other domain documents such as blog 2In the following examples, omitted arguments are put in parentheses and exophoric referents are put in square brackets. articles and shopping sites, the A/R often appear in the discourse. The A/R tend to be omitted and there are many clues for the referent identification about the A/R such as honorific expres</context>
<context position="8415" citStr="Iida et al. (2006)" startWordPosition="1380" endWordPosition="1383"> the dative case is “皆様” (you all), which is the reader. The clues such as request forms, honorific expressions and modality expressions are available for the author and reader. In this work, to represent such aspect of the A/R mentions, both the endophora and exophora features are given to them. In this paper, we propose a zero reference resolution model considering the zero exophora and the author/reader mentions, which resolves the zero reference as a part of a predicate-argument structure analysis. 2 Related Work Several approaches to Japanese zero reference resolution have been proposed. Iida et al. (2006) proposed a zero reference resolution model that uses the syntactic relations between a zero pronoun and a candidate referent as a feature. They deal with zero exophora by judging that a zero pronoun does not have anaphoricity. However, the information of zero pronoun existences is given and thus they did not address zero pronoun detection. Zero reference resolution has been tackled as a part of predicate-argument structure analysis. Imamura et al. (2009) proposed a predicate-argument structure analysis model based on a log-linear model that simultaneously conducts zero endophora resolution. T</context>
</contexts>
<marker>Iida, Inui, Matsumoto, 2006</marker>
<rawString>Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Exploiting syntactic patterns as clues in zero-anaphora resolution. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 625–632, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Imamura</author>
<author>Kuniko Saito</author>
<author>Tomoko Izumi</author>
</authors>
<title>Discriminative approach to predicate-argument structure analysis with zero-anaphora resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>85--88</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="2395" citStr="Imamura et al., 2009" startWordPosition="370" endWordPosition="373">o pronoun detection is the task that detects omitted zero pronouns from a document. In example (1), this task detects that there are the zero pronouns in the accusative and nominative cases of “ A�iT” (eat) and there is no zero pronoun in the dative case of “ A�iT”. Referent identification is the task that identifies the referent of a zero pronoun. In example (1), this task identifies that the referent of the zero pronoun in the accusative case of “ A�iT” is “���” (pasta). These two subtasks are often resolved simultaneously and our proposed model is a unified model. Many previous studies (Imamura et al., 2009; Sasano et al., 2008; Sasano and Kurohashi, 2011) have treated only zero endophora, which is a phenomenon that a referent is mentioned in a document, such as “���” (pasta) in example (1). However, zero exophora, which is a phenomenon that a referent does not appear in a document, often occurs in Japanese when a referent is an author or reader of a document or an indefinite pronoun. For example, in example (1), the referent of the zero pronoun of the nominative case of “ A�iT” (eat) is the author of &apos;In this paper, we use the following abbreviations: NOM (nominative), ABL(ablative), ACC (accu</context>
<context position="8874" citStr="Imamura et al. (2009)" startWordPosition="1453" endWordPosition="1457"> a part of a predicate-argument structure analysis. 2 Related Work Several approaches to Japanese zero reference resolution have been proposed. Iida et al. (2006) proposed a zero reference resolution model that uses the syntactic relations between a zero pronoun and a candidate referent as a feature. They deal with zero exophora by judging that a zero pronoun does not have anaphoricity. However, the information of zero pronoun existences is given and thus they did not address zero pronoun detection. Zero reference resolution has been tackled as a part of predicate-argument structure analysis. Imamura et al. (2009) proposed a predicate-argument structure analysis model based on a log-linear model that simultaneously conducts zero endophora resolution. They assumed a particular candidate referent, NULL, and when the analyzer selected this referent, the analyzer outputs “zero exophora or no zero 3Since “IR” (I) depends on “��Z J:T” (have thought), the relation between “IR” (I) and “TT=7” (will go) is the zero reference. pronoun,” in which they are treated without distinction. Sasano et al. (2008) proposed a probabilistic predicate-argument structure analysis model including zero endophora resolution by us</context>
</contexts>
<marker>Imamura, Saito, Izumi, 2009</marker>
<rawString>Kenji Imamura, Kuniko Saito, and Tomoko Izumi. 2009. Discriminative approach to predicate-argument structure analysis with zero-anaphora resolution. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 85–88, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>133--142</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6880" citStr="Joachims, 2002" startWordPosition="1113" endWordPosition="1114">I), which is explicitly mentioned in the document, is the author of the document and “��/v” (you all) is the reader. In this paper, we call these expressions, which refer to the author and reader, author mention and reader mention. We treat them explicitly to improve the performance of zero reference resolution. Since the A/R are mentioned as various expressions besides personal pronouns in Japanese, it is difficult to detect the A/R mentions based merely on lexical information. In this work, we automatically detect the A/R mentions by using a learning-to-rank algorithm(Herbrich et al., 1998; Joachims, 2002) that uses lexico-syntactic patterns as features. Once the A/R mentions can be detected, their information is useful for the referent identification. 925 The A/R mentions have both a property of the discourse element mentioned in a document and a property of the zero exophoric A/R. In the first sentence of example (3), it can be estimated that the referent of the zero pronoun of the nominative case of “行こ う” (will go) from a contextual clue that “僕” (I) is the topic of this sentence and a syntactic clues that “ 僕” (I) depends on “思っています” (have thought) over the predicate “行こう” (will go).3 Such</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 133–142. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Case frame compilation from the web using highperformance computing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation,</booktitle>
<pages>1344--1347</pages>
<contexts>
<context position="11124" citStr="Kawahara and Kurohashi (2006" startWordPosition="1818" endWordPosition="1821"> coreference resolution systems. 3 Baseline Model In this section, we describe a baseline zero reference resolution system. In our model, the zero reference resolution is conducted as a part of predicateargument structure (PAS) analysis. The PAS consists of a case frame and an alignment between case slots and referents. The case frames are constructed for each meaning of a predicate. Each case frame describes surface cases that each predicate has (case slot) and words that can fill each case slot (example). In this study, the case frames are constructed from 6.9 billion Web sentences by using Kawahara and Kurohashi (2006a)’s method. The baseline model does not treat zero exophora as the previous studies. The baseline model analyzes a document in the following procedure in the same way as the previous study (Sasano and Kurohashi, 2011).4 4For learning, the previous study used a log-linear model, but we use a learning-to-rank model. In our preliminary exper926 � � 行きます。 go (I like a curry shop in Kyoto station and often go to the shop.) 今日は 皆さんに (カレー屋ヲ) 紹介します。 Today-TOP you all-DAT (curry shop-ACC) will introduce (Today, I will introduce (the shop) to you.) Discourse entities ✏ ✓{ 京都駅 (Kyoto station)}, { カレー屋 (</context>
<context position="17943" citStr="Kawahara and Kurohashi, 2006" startWordPosition="3004" endWordPosition="3007">se slot c is not assigned to any discourse entities. For example, the PAS [1, 2] in Figure 1 is represented as: (φovert-PAS(紹介する (1), {NOM:Null, ACC:Null, NOM:皆さん, TIME:今日 }),00A, φNA(紹介する (1),NOM/Null), φA(紹介する (1),ACC/カレー屋), 00NA,00A,00NA). 5 In our feature representation, the second and third terms correspond to the nominative case, the forth and fifth ones correspond to the accusative and the sixth and seventh ones correspond to the dative case. We present the details of φovert-PAS(cf, a, p, t), φA(cf, c/e, p, t) and φNA(cf, c/Null, p, t). We use a score of the probabilistic PAS analysis (Kawahara and Kurohashi, 2006b) to φovert-PAS(cf, a, p, t). We list the features of φA(cf, c/e, p, t) in Table 2 and the features of φNA(cf, c/Null, p, t) in Table SIn the following example, p and t are sometimes omitted and 00is 0 vector that has the same dimension as 0. 928 Description Probability that c of cf is not assigned Ratio of number of examples of c to ones of cf cofcf is {adjacent and obligate} case Table 3: The features of ONA(cf, c/Null, p, t) 3. 3.2 Weight Learning In the previous section, we defined the feature vector 0(cf, a, p, t), which represents a PAS. In this section, we illustrate the learning metho</context>
</contexts>
<marker>Kawahara, Kurohashi, 2006</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2006a. Case frame compilation from the web using highperformance computing. In Proceedings of the 5th International Conference on Language Resources and Evaluation, pages 1344–1347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A fully-lexicalized probabilistic model for japanese syntactic and case structure analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>176--183</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="11124" citStr="Kawahara and Kurohashi (2006" startWordPosition="1818" endWordPosition="1821"> coreference resolution systems. 3 Baseline Model In this section, we describe a baseline zero reference resolution system. In our model, the zero reference resolution is conducted as a part of predicateargument structure (PAS) analysis. The PAS consists of a case frame and an alignment between case slots and referents. The case frames are constructed for each meaning of a predicate. Each case frame describes surface cases that each predicate has (case slot) and words that can fill each case slot (example). In this study, the case frames are constructed from 6.9 billion Web sentences by using Kawahara and Kurohashi (2006a)’s method. The baseline model does not treat zero exophora as the previous studies. The baseline model analyzes a document in the following procedure in the same way as the previous study (Sasano and Kurohashi, 2011).4 4For learning, the previous study used a log-linear model, but we use a learning-to-rank model. In our preliminary exper926 � � 行きます。 go (I like a curry shop in Kyoto station and often go to the shop.) 今日は 皆さんに (カレー屋ヲ) 紹介します。 Today-TOP you all-DAT (curry shop-ACC) will introduce (Today, I will introduce (the shop) to you.) Discourse entities ✏ ✓{ 京都駅 (Kyoto station)}, { カレー屋 (</context>
<context position="17943" citStr="Kawahara and Kurohashi, 2006" startWordPosition="3004" endWordPosition="3007">se slot c is not assigned to any discourse entities. For example, the PAS [1, 2] in Figure 1 is represented as: (φovert-PAS(紹介する (1), {NOM:Null, ACC:Null, NOM:皆さん, TIME:今日 }),00A, φNA(紹介する (1),NOM/Null), φA(紹介する (1),ACC/カレー屋), 00NA,00A,00NA). 5 In our feature representation, the second and third terms correspond to the nominative case, the forth and fifth ones correspond to the accusative and the sixth and seventh ones correspond to the dative case. We present the details of φovert-PAS(cf, a, p, t), φA(cf, c/e, p, t) and φNA(cf, c/Null, p, t). We use a score of the probabilistic PAS analysis (Kawahara and Kurohashi, 2006b) to φovert-PAS(cf, a, p, t). We list the features of φA(cf, c/e, p, t) in Table 2 and the features of φNA(cf, c/Null, p, t) in Table SIn the following example, p and t are sometimes omitted and 00is 0 vector that has the same dimension as 0. 928 Description Probability that c of cf is not assigned Ratio of number of examples of c to ones of cf cofcf is {adjacent and obligate} case Table 3: The features of ONA(cf, c/Null, p, t) 3. 3.2 Weight Learning In the previous section, we defined the feature vector 0(cf, a, p, t), which represents a PAS. In this section, we illustrate the learning metho</context>
</contexts>
<marker>Kawahara, Kurohashi, 2006</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2006b. A fully-lexicalized probabilistic model for japanese syntactic and case structure analysis. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 176–183, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
<author>Koiti Hasida</author>
</authors>
<title>Construction of a japanese relevance-tagged corpus.</title>
<date>2002</date>
<booktitle>In Proc. of The Third International Conference on Language Resources Evaluation,</booktitle>
<contexts>
<context position="20082" citStr="Kawahara et al., 2002" startWordPosition="3389" endWordPosition="3392">pt (cf*, a*) from the learning instance. 4. Make ranking data that (cf*, a*) has a higher rank than other (cf, a). In the above steps, we make ranking data for each predicate and use the ranking data collected from all target predicates as training data. 4 Corpus In this work, we use Diverse Document Leads Corpus (DDLC) (Hangyo et al., 2012) for experiments. In DDLC, documents collected from the web are annotated with morpheme, syntax, named entity, coreference, PAS and A/R mention. Morpheme, syntax, named entity, coreference and PAS are annotated on the basis of Kyoto University Text Corpus (Kawahara et al., 2002). The PAS annotation includes zero reference information and the exophora referents are defined as five elements, [author], [reader], [US(unspecified):person], [US:matter] and [US:situation]. The A/R mentions are annotated to head phrases of compound nouns when the A/R mentions consist of compound nouns. If the A/R is mentioned by multiple expressions, only one of them is annotated with the A/R mention tag and all of these mentions are linked by a coreference chain. In other words, the A/R mentions are annotated to discourse entities. In the web site of an organization such as a company, the s</context>
</contexts>
<marker>Kawahara, Kurohashi, Hasida, 2002</marker>
<rawString>Daisuke Kawahara, Sadao Kurohashi, and Koiti Hasida. 2002. Construction of a japanese relevance-tagged corpus. In Proc. of The Third International Conference on Language Resources Evaluation, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fang Kong</author>
<author>Guodong Zhou</author>
</authors>
<title>A tree kernelbased unified framework for chinese zero anaphora resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>882--891</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="10069" citStr="Kong and Zhou (2010)" startWordPosition="1646" endWordPosition="1649">ndophora resolution by using widecoverage case frames constructed from a web corpus. Sasano and Kurohashi (2011) extended the Sasano et al. (2008)’s model by focusing on zero endophora. Their model is based on a log-linear model that uses case frame information and the location of a candidate referent as features. In their work, zero exophora is not treated and they assumed that a zero pronoun is absent when there is no referent in a document. For languages other than Japanese, zero pronoun resolution methods have been proposed for Chinese, Portuguese, Spanish and other languages. In Chinese, Kong and Zhou (2010) proposed tree-kernel based models for three subtasks: zero pronoun detection, anaphoricity decision and referent selection. In Portuguese and Spanish, only a subject word is omitted and zero pronoun resolution has been tackled as a part of coreference resolution. Poesio et al. (2010) and Rello et al. (2012) detected omitted subjects and made a decision whether the omitted subject has anaphoricity or not as preprocessing of coreference resolution systems. 3 Baseline Model In this section, we describe a baseline zero reference resolution system. In our model, the zero reference resolution is co</context>
</contexts>
<marker>Kong, Zhou, 2010</marker>
<rawString>Fang Kong and Guodong Zhou. 2010. A tree kernelbased unified framework for chinese zero anaphora resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 882–891, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Massimo Poesio</author>
<author>Olga Uryupina</author>
<author>Yannick Versley</author>
</authors>
<title>Creating a coreference resolution system for italian.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<contexts>
<context position="10354" citStr="Poesio et al. (2010)" startWordPosition="1690" endWordPosition="1693">andidate referent as features. In their work, zero exophora is not treated and they assumed that a zero pronoun is absent when there is no referent in a document. For languages other than Japanese, zero pronoun resolution methods have been proposed for Chinese, Portuguese, Spanish and other languages. In Chinese, Kong and Zhou (2010) proposed tree-kernel based models for three subtasks: zero pronoun detection, anaphoricity decision and referent selection. In Portuguese and Spanish, only a subject word is omitted and zero pronoun resolution has been tackled as a part of coreference resolution. Poesio et al. (2010) and Rello et al. (2012) detected omitted subjects and made a decision whether the omitted subject has anaphoricity or not as preprocessing of coreference resolution systems. 3 Baseline Model In this section, we describe a baseline zero reference resolution system. In our model, the zero reference resolution is conducted as a part of predicateargument structure (PAS) analysis. The PAS consists of a case frame and an alignment between case slots and referents. The case frames are constructed for each meaning of a predicate. Each case frame describes surface cases that each predicate has (case s</context>
</contexts>
<marker>Poesio, Uryupina, Versley, 2010</marker>
<rawString>Massimo Poesio, Olga Uryupina, and Yannick Versley. 2010. Creating a coreference resolution system for italian. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luz Rello</author>
<author>Ricardo Baeza-Yates</author>
<author>Ruslan Mitkov</author>
</authors>
<title>Elliphant: Improved automatic detection of zero subjects and impersonal constructions in spanish.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>706--715</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10378" citStr="Rello et al. (2012)" startWordPosition="1695" endWordPosition="1698">ures. In their work, zero exophora is not treated and they assumed that a zero pronoun is absent when there is no referent in a document. For languages other than Japanese, zero pronoun resolution methods have been proposed for Chinese, Portuguese, Spanish and other languages. In Chinese, Kong and Zhou (2010) proposed tree-kernel based models for three subtasks: zero pronoun detection, anaphoricity decision and referent selection. In Portuguese and Spanish, only a subject word is omitted and zero pronoun resolution has been tackled as a part of coreference resolution. Poesio et al. (2010) and Rello et al. (2012) detected omitted subjects and made a decision whether the omitted subject has anaphoricity or not as preprocessing of coreference resolution systems. 3 Baseline Model In this section, we describe a baseline zero reference resolution system. In our model, the zero reference resolution is conducted as a part of predicateargument structure (PAS) analysis. The PAS consists of a case frame and an alignment between case slots and referents. The case frames are constructed for each meaning of a predicate. Each case frame describes surface cases that each predicate has (case slot) and words that can </context>
</contexts>
<marker>Rello, Baeza-Yates, Mitkov, 2012</marker>
<rawString>Luz Rello, Ricardo Baeza-Yates, and Ruslan Mitkov. 2012. Elliphant: Improved automatic detection of zero subjects and impersonal constructions in spanish. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 706–715. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A discriminative approach to japanese zero anaphora resolution with large-scale lexicalized case frames.</title>
<date>2011</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>758--766</pages>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="2445" citStr="Sasano and Kurohashi, 2011" startWordPosition="378" endWordPosition="381">s omitted zero pronouns from a document. In example (1), this task detects that there are the zero pronouns in the accusative and nominative cases of “ A�iT” (eat) and there is no zero pronoun in the dative case of “ A�iT”. Referent identification is the task that identifies the referent of a zero pronoun. In example (1), this task identifies that the referent of the zero pronoun in the accusative case of “ A�iT” is “���” (pasta). These two subtasks are often resolved simultaneously and our proposed model is a unified model. Many previous studies (Imamura et al., 2009; Sasano et al., 2008; Sasano and Kurohashi, 2011) have treated only zero endophora, which is a phenomenon that a referent is mentioned in a document, such as “���” (pasta) in example (1). However, zero exophora, which is a phenomenon that a referent does not appear in a document, often occurs in Japanese when a referent is an author or reader of a document or an indefinite pronoun. For example, in example (1), the referent of the zero pronoun of the nominative case of “ A�iT” (eat) is the author of &apos;In this paper, we use the following abbreviations: NOM (nominative), ABL(ablative), ACC (accusative), DAT (dative), ALL (allative), GEN (geniti</context>
<context position="5115" citStr="Sasano and Kurohashi, 2011" startWordPosition="806" endWordPosition="809">e of the predicate has the zero pronoun, but in “no zero reference,” the dative case of the predicate does not have a zero pronoun. Treating them with no distinction causes a decrease in accuracy of machine learning-based zero pronoun detection due to a gap between the valency of a predicate and observed arguments of the predicate. In this work, to deal with zero exophora explicitly, we provide pseudo entities such as [author], [reader] and [unspecified:person] as candidate referents of zero pronouns. In the referent identification, selectional preferences of a predicate (Sasano et al., 2008; Sasano and Kurohashi, 2011) and contextual information (Iida et al., 2006) have been widely used. The author and reader (A/R) of a document have not been used for contextual clues because the A/R rarely appear in the discourse in corpora based on newspaper articles, which are main targets of the previous studies. However, in other domain documents such as blog 2In the following examples, omitted arguments are put in parentheses and exophoric referents are put in square brackets. articles and shopping sites, the A/R often appear in the discourse. The A/R tend to be omitted and there are many clues for the referent identi</context>
<context position="9561" citStr="Sasano and Kurohashi (2011)" startWordPosition="1559" endWordPosition="1562">d on a log-linear model that simultaneously conducts zero endophora resolution. They assumed a particular candidate referent, NULL, and when the analyzer selected this referent, the analyzer outputs “zero exophora or no zero 3Since “IR” (I) depends on “��Z J:T” (have thought), the relation between “IR” (I) and “TT=7” (will go) is the zero reference. pronoun,” in which they are treated without distinction. Sasano et al. (2008) proposed a probabilistic predicate-argument structure analysis model including zero endophora resolution by using widecoverage case frames constructed from a web corpus. Sasano and Kurohashi (2011) extended the Sasano et al. (2008)’s model by focusing on zero endophora. Their model is based on a log-linear model that uses case frame information and the location of a candidate referent as features. In their work, zero exophora is not treated and they assumed that a zero pronoun is absent when there is no referent in a document. For languages other than Japanese, zero pronoun resolution methods have been proposed for Chinese, Portuguese, Spanish and other languages. In Chinese, Kong and Zhou (2010) proposed tree-kernel based models for three subtasks: zero pronoun detection, anaphoricity </context>
<context position="11342" citStr="Sasano and Kurohashi, 2011" startWordPosition="1853" endWordPosition="1856">re (PAS) analysis. The PAS consists of a case frame and an alignment between case slots and referents. The case frames are constructed for each meaning of a predicate. Each case frame describes surface cases that each predicate has (case slot) and words that can fill each case slot (example). In this study, the case frames are constructed from 6.9 billion Web sentences by using Kawahara and Kurohashi (2006a)’s method. The baseline model does not treat zero exophora as the previous studies. The baseline model analyzes a document in the following procedure in the same way as the previous study (Sasano and Kurohashi, 2011).4 4For learning, the previous study used a log-linear model, but we use a learning-to-rank model. In our preliminary exper926 � � 行きます。 go (I like a curry shop in Kyoto station and often go to the shop.) 今日は 皆さんに (カレー屋ヲ) 紹介します。 Today-TOP you all-DAT (curry shop-ACC) will introduce (Today, I will introduce (the shop) to you.) Discourse entities ✏ ✓{ 京都駅 (Kyoto station)}, { カレー屋 (curry shop), その店 (the shop)}, { 今日 (today)}, { 皆さん (you all)} ✒ ✑ ✓ Candidate predicate-argument structures of “紹介します” in the baseline model ✏ [1-1] case frame:[紹介する (1)], { NOM:Null, ACC:Null, DAT:皆さん, TIME:今日 } [1, 2</context>
<context position="15890" citStr="Sasano and Kurohashi, 2011" startWordPosition="2633" endWordPosition="2636">d to any words Log Ratio of examples of c to ones of cf Binary c of cf is {adjacent and obligate} case Predicate Binary Modality types of p Binary Honorific expressions of p Binary Tenses of p Binary p is potential form Binary Modifier of p (predicate, noun and end of sentence) Binary p is {dynamic and stative} verb Context Binary Named entity types of e Integer Number of mentions about e in t Integer Number of mentions about e {before and after} p in t Binary e is mentioned with post position “b:” in a target sentence Binary Sentence distances between e and p Binary Location categories of e (Sasano and Kurohashi, 2011) Binary e is mentioned at head of a target sentence Binary e is mentioned with post position {“b:” and “が” } at head of a target sentence Binary e is mentioned at head of the first sentence Binary e is mentioned with post position “b:” at head of the first sentence Binary e is mentioned at end of the first sentence Binary e is mentioned with copula at end of the first sentence Binary e is mentioned with noun phrase stop at end of the first sentence Binary Salience score of e is larger than 1 (Sasano and Kurohashi, 2011) other Binary c is assigned Table 2: The features of Oassigned(cf, c — e, p</context>
</contexts>
<marker>Sasano, Kurohashi, 2011</marker>
<rawString>Ryohei Sasano and Sadao Kurohashi. 2011. A discriminative approach to japanese zero anaphora resolution with large-scale lexicalized case frames. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 758–766, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A fully-lexicalized probabilistic model for japanese zero anaphora resolution.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>769--776</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="2416" citStr="Sasano et al., 2008" startWordPosition="374" endWordPosition="377"> the task that detects omitted zero pronouns from a document. In example (1), this task detects that there are the zero pronouns in the accusative and nominative cases of “ A�iT” (eat) and there is no zero pronoun in the dative case of “ A�iT”. Referent identification is the task that identifies the referent of a zero pronoun. In example (1), this task identifies that the referent of the zero pronoun in the accusative case of “ A�iT” is “���” (pasta). These two subtasks are often resolved simultaneously and our proposed model is a unified model. Many previous studies (Imamura et al., 2009; Sasano et al., 2008; Sasano and Kurohashi, 2011) have treated only zero endophora, which is a phenomenon that a referent is mentioned in a document, such as “���” (pasta) in example (1). However, zero exophora, which is a phenomenon that a referent does not appear in a document, often occurs in Japanese when a referent is an author or reader of a document or an indefinite pronoun. For example, in example (1), the referent of the zero pronoun of the nominative case of “ A�iT” (eat) is the author of &apos;In this paper, we use the following abbreviations: NOM (nominative), ABL(ablative), ACC (accusative), DAT (dative)</context>
<context position="5086" citStr="Sasano et al., 2008" startWordPosition="802" endWordPosition="805">hora,” the dative case of the predicate has the zero pronoun, but in “no zero reference,” the dative case of the predicate does not have a zero pronoun. Treating them with no distinction causes a decrease in accuracy of machine learning-based zero pronoun detection due to a gap between the valency of a predicate and observed arguments of the predicate. In this work, to deal with zero exophora explicitly, we provide pseudo entities such as [author], [reader] and [unspecified:person] as candidate referents of zero pronouns. In the referent identification, selectional preferences of a predicate (Sasano et al., 2008; Sasano and Kurohashi, 2011) and contextual information (Iida et al., 2006) have been widely used. The author and reader (A/R) of a document have not been used for contextual clues because the A/R rarely appear in the discourse in corpora based on newspaper articles, which are main targets of the previous studies. However, in other domain documents such as blog 2In the following examples, omitted arguments are put in parentheses and exophoric referents are put in square brackets. articles and shopping sites, the A/R often appear in the discourse. The A/R tend to be omitted and there are many </context>
<context position="9363" citStr="Sasano et al. (2008)" startWordPosition="1530" endWordPosition="1533">oun detection. Zero reference resolution has been tackled as a part of predicate-argument structure analysis. Imamura et al. (2009) proposed a predicate-argument structure analysis model based on a log-linear model that simultaneously conducts zero endophora resolution. They assumed a particular candidate referent, NULL, and when the analyzer selected this referent, the analyzer outputs “zero exophora or no zero 3Since “IR” (I) depends on “��Z J:T” (have thought), the relation between “IR” (I) and “TT=7” (will go) is the zero reference. pronoun,” in which they are treated without distinction. Sasano et al. (2008) proposed a probabilistic predicate-argument structure analysis model including zero endophora resolution by using widecoverage case frames constructed from a web corpus. Sasano and Kurohashi (2011) extended the Sasano et al. (2008)’s model by focusing on zero endophora. Their model is based on a log-linear model that uses case frame information and the location of a candidate referent as features. In their work, zero exophora is not treated and they assumed that a zero pronoun is absent when there is no referent in a document. For languages other than Japanese, zero pronoun resolution methods</context>
</contexts>
<marker>Sasano, Kawahara, Kurohashi, 2008</marker>
<rawString>Ryohei Sasano, Daisuke Kawahara, and Sadao Kurohashi. 2008. A fully-lexicalized probabilistic model for japanese zero anaphora resolution. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 769–776, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
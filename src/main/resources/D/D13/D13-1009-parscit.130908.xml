<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000605">
<title confidence="0.986762">
Question Difficulty Estimation in Community Question Answering Services∗
</title>
<author confidence="0.998105">
Jing Liu† Quan Wang$ Chin-Yew Lin♯ Hsiao-Wuen Hon♯†Harbin Institute of Technology, Harbin 150001, P.R.China
</author>
<affiliation confidence="0.9406505">
$Peking University, Beijing 100871, P.R.China
♯Microsoft Research Asia, Beijing 100080, P.R.China
</affiliation>
<email confidence="0.997343">
jliu@ir.hit.edu.cn quanwang1012@gmail.com {cyl,hon}@microsoft.com
</email>
<sectionHeader confidence="0.993872" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999803615384615">
In this paper, we address the problem of
estimating question difficulty in community
question answering services. We propose a
competition-based model for estimating ques-
tion difficulty by leveraging pairwise compar-
isons between questions and users. Our ex-
perimental results show that our model sig-
nificantly outperforms a PageRank-based ap-
proach. Most importantly, our analysis shows
that the text of question descriptions reflects
the question difficulty. This implies the pos-
sibility of predicting question difficulty from
the text of question descriptions.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.927317388888889">
In recent years, community question answering (C-
QA) services such as Stackoverflowl and Yahoo!
Answers have seen rapid growth. A great deal
of research effort has been conducted on CQA, in-
cluding: (1) question search (Xue et al., 2008; Du-
an et al., 2008; Suryanto et al., 2009; Zhou et al.,
2011; Cao et al., 2010; Zhang et al., 2012; Ji et
al., 2012); (2) answer quality estimation (Jeon et al.,
2006; Agichtein et al., 2008; Bian et al., 2009; Liu
et al., 2008); (3) user expertise estimation (Jurczyk
and Agichtein, 2007; Zhang et al., 2007; Bouguessa
et al., 2008; Pal and Konstan, 2010; Liu et al., 2011);
and (4) question routing (Zhou et al., 2009; Li and
King, 2010; Li et al., 2011).
*This work was done when Jing Liu and Quan Wang were
visiting students at Microsoft Research Asia. Quan Wang is
currently affiliated with Institute of Information Engineering,
Chinese Academy of Sciences.
</bodyText>
<footnote confidence="0.8175815">
lhttp://stackoverflow.com
zhttp://answers.yahoo.com
</footnote>
<bodyText confidence="0.999978">
However, less attention has been paid to question
difficulty estimation in CQA. Question difficulty es-
timation can benefit many applications: (1) Experts
are usually under time constraints. We do not want
to bore experts by routing every question (including
both easy and hard ones) to them. Assigning ques-
tions to experts by matching question difficulty with
expertise level, not just question topic, will make
better use of the experts’ time and expertise (Ack-
erman and McDonald, 1996). (2) Nam et al. (2009)
found that winning the point awards offered by the
reputation system is a driving factor in user partici-
pation in CQA. Question difficulty estimation would
be helpful in designing a better incentive mechanis-
m by assigning higher point awards to more diffi-
cult questions. (3) Question difficulty estimation can
help analyze user behavior in CQA, since users may
make strategic choices when encountering questions
of different difficulty levels.
To the best of our knowledge, not much research
has been conducted on the problem of estimating
question difficulty in CQA. The most relevant work
is a PageRank-based approach proposed by Yang et
al. (2008) to estimate task difficulty in crowdsourc-
ing contest services. Their key idea is to construct
a graph of tasks: creating an edge from a task t1 to
a task t2 when a user u wins task t1 but loses task
t2, implying that task t2 is likely to be more diffi-
cult than task t1. Then the standard PageRank al-
gorithm is employed on the task graph to estimate
PageRank score (i.e., difficulty score) of each task.
This approach implicitly assumes that task difficulty
is the only factor affecting the outcomes of competi-
tions (i.e. the best answer). However, the outcomes
of competitions depend on both the difficulty levels
of tasks and the expertise levels of competitors (i.e.
</bodyText>
<page confidence="0.996419">
85
</page>
<bodyText confidence="0.9091845">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 85–90,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
other answerers).
Inspired by Liu et al. (2011), we propose a
competition-based approach which jointly models
question difficulty and user expertise level. Our ap-
proach is based on two intuitive assumptions: (1)
given a question answering thread, the difficulty s-
core of the question is higher than the expertise score
of the asker, but lower than that of the best answerer;
</bodyText>
<listItem confidence="0.95588296">
(2) the expertise score of the best answerer is higher
than that of the asker as well as all other answer-
ers. Given the two assumptions, we can determine
the question difficulty score and user expertise score
through pairwise comparisons between (1) a ques-
tion and an asker, (2) a question and a best answerer,
(3) a best answerer and an asker, and (4) a best an-
swerer and all other non-best answerers.
The main contributions of this paper are:
• We propose a competition-based approach to es-
timate question difficulty (Sec. 2). Our model signif-
icantly outperforms the PageRank-based approach
(Yang et al., 2008) for estimating question difficulty
on the data of Stack Overflow (Sec. 3.2).
• Additionally, we calibrate question difficulty s-
cores across two CQA services to verify the effec-
tiveness of our model (Sec. 3.3).
• Most importantly, we demonstrate that different
words or tags in the question descriptions indicate
question difficulty levels. This implies the possibil-
ity of predicting question difficulty purely from the
text of question descriptions (Sec. 3.4).
best answer ub correctly responds to question q that
asker ua does not know.
• The expertise score of the best answerer ub is
</listItem>
<bodyText confidence="0.955376783333334">
higher than that of asker ua and all answerers in S.
This is straightforward since the best answerer ub
solves question q better than asker ua and all non-
best answerers in S.
Let’s view question q as a pseudo user uq. Tak-
ing a competitive viewpoint, each pairwise compar-
ison can be viewed as a two-player competition with
one winner and one loser, including (1) one compe-
tition between pseudo user uq and asker ua, (2) one
competition between pseudo user uq and the best
answerer ub, (3) one competition between the best
answerer ub and asker ua, and (4) |S |competitions
between the best answerer ub and all non-best an-
swers in S. Additionally, pseudo user uq wins the
first competition and the best answerer ub wins all
remaining (|S |+ 2) competitions.
Hence, the problem of estimating the question d-
ifficulty score (and the user expertise score) is cast
as a problem of learning the relative skills of play-
ers from the win-loss results of the generated two-
player competitions. Formally, let Q denote the set
of all questions in one category (or topic), and Rq de-
note the set of all two-player competitions generated
from question q ∈ Q, i.e., Rq = {(ua ≺ uq), (uq ≺
ub), (ua ≺ ub), (uol ≺ ub), ··· , (uo|S |≺ ub)},
where j ≺ i means that user i beats user j in the
competition. Define
2 Competition based Question Difficulty ∪ Rq (1)
Estimation R =
qE-0
CQA is a virtual community where people can ask
questions and seek opinions from others. Formally,
when an asker ua posts a question q, there will be
several answerers to answer her question. One an-
swer among the received ones will be selected as the
best answer by the asker ua or voted by the com-
munity. The user who provides the best answer is
called the best answerer ub, and we denote the set of
all non-best answerers as S = {uol, · · · , uom}. As-
suming that question difficulty scores and user ex-
pertise scores are expressed on the same scale, we
make the following two assumptions:
• The difficulty score of question q is higher than
the expertise score of asker ua, but lower than that
of the best answerer ub. This is intuitive since the
as the set of all two-player competitions. Our prob-
lem is then to learn the relative skills of players from
R. The learned skills of the pseudo question users
are question difficulty scores, and the learned skills
of all other users are their expertise scores.
TrueSkill In this paper, we follow (Liu et al.,
2011) and apply TrueSkill to learn the relative skill-
s of players from the set of generated competitions
R (Equ. 1). TrueSkill (Herbrich et al., 2007) is a
Bayesian skill rating model that is developed for es-
timating the relative skill levels of players in games.
In this paper, we present a two-player version of
TrueSkill with no-draw.
TrueSkill assumes that the practical performance
of each player in a game follows a normal distribu-
</bodyText>
<page confidence="0.979962">
86
</page>
<bodyText confidence="0.999986333333333">
tion N(µ, σ2), where µ means the skill level of the
player and σ means the uncertainty of the estimated
skill level. Basically, TrueSkill learns the skill lev-
els of players by leveraging Bayes’ theorem. Giv-
en the current estimated skill levels of two players
(priori probability) and the outcome of a new game
between them (likelihood), TrueSkill model updates
its estimation of player skill levels (posterior prob-
ability). TrueSkill updates the skill level µ and the
uncertainty σ intuitively: (a) if the outcome of a new
competition is expected, i.e. the player with higher
skill level wins the game, it will cause small updates
in skill level µ and uncertainty σ; (b) if the outcome
of a new competition is unexpected, i.e. the player
with lower skill level wins the game, it will cause
large updates in skill level µ and uncertainty σ. Ac-
cording to these intuitions, the equations to update
the skill level µ and uncertainty σ are as follows:
</bodyText>
<equation confidence="0.996762285714286">
( t )
· vc, ε , (3)
c
[ ( t )]
1 − σ2
σ2 winner
winner = σ2 winner · c2 · w c, ε ,
c
(4)
[ (t )]
1 − σ2
σ2 loser = σ2 loser
loser · c2 · w c, ε , (5)
c
</equation>
<bodyText confidence="0.999985555555555">
where t = µwinner − µloser and c2 = 2β2 +
σ2 winner +σ2loser. Here, ε is a parameter representing
the probability of a draw in one game, and v(t, ε)
and w(t, ε) are weighting factors for skill level µ
and standard deviation σ respectively. Please refer
to (Herbrich et al., 2007) for more details. In this
paper, we set the initial values of the skill level µ
and the standard deviation σ of each player the same
as the default values used in (Herbrich et al., 2007).
</bodyText>
<sectionHeader confidence="0.999604" genericHeader="introduction">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997146">
3.1 Data Set
</subsectionHeader>
<bodyText confidence="0.999793666666667">
In this paper, we use Stack Overflow (SO) for our
experiments. We obtained a publicly available da-
ta seta of SO between July 31, 2008 and August 1,
2012. SO contains questions with various topics,
such as programming, mathematics, and English. In
this paper, we use SO C++ programming (SO/CPP)
</bodyText>
<footnote confidence="0.9169295">
3http://blog.stackoverflow.com/category/
cc-wiki-dump/
</footnote>
<bodyText confidence="0.996303">
and mathematics (SO/Math) questions for our main
experiments. Additionally, we use the data of Math
Overflows (MO) for calibrating question difficulty
scores across communities (Sec. 3.3). The statistics
of these data sets are shown in Table 1.
</bodyText>
<table confidence="0.99519175">
SO/CPP SO/Math MO
# of questions 122,012 51,174 27,333
# of answers 357,632 94,488 65,966
# of users 67,819 16,961 12,064
</table>
<tableCaption confidence="0.999963">
Table 1: The statistics of the data sets.
</tableCaption>
<bodyText confidence="0.999943115384615">
To evaluate the effectiveness of our proposed
model for estimating question difficulty scores, we
randomly sampled 300 question pairs from both
SO/CPP and SO/Math, and we asked experts to
compare the difficulty of every pair. We had two
graduate students majoring in computer science an-
notate the SO/CPP question pairs, and two gradu-
ate students majoring in mathematics annotate the
SO/Math question pairs. When annotating each
question pair, only the titles, descriptions, and tags
of the questions were shown, and other information
(e.g. users, answers, etc.) was excluded. Given each
pair of questions (q1 and q2), the annotators were
asked to give one of four labels: (1) q1 ≻ q2, which
means that the difficulty of q1 was higher than q2;
(2) q1 ≺ q2, which means that the difficulty of q1
was lower than q2; (3) q1 = q2, which means that
the difficulty of q1 was equal to q2; (4) Unknown,
which means that the annotator could not make a
decision. The agreements between annotators on
both SO/CPP (kappa value = 0.741) and SO/Math
(kappa value = 0.873) were substantial. When eval-
uating models, we only kept the pairs that annotators
had given the same labels. There were 260 SO/CPP
question pairs and 280 SO/Math question pairs re-
maining.
</bodyText>
<subsectionHeader confidence="0.999967">
3.2 Accuracy of Question Difficulty Estimation
</subsectionHeader>
<bodyText confidence="0.932905666666667">
We employ a standard evaluation metric for infor-
mation retrieval: accuracy (Acc), defined as follows:
.
the total number of pairwise comparisons
We use the PageRank-based approach proposed
by Yang et al. (2008) as a baseline. As described in
</bodyText>
<footnote confidence="0.9989315">
4http://math.stackexchange.com
5http://mathoverflow.net
</footnote>
<figure confidence="0.952404727272727">
Qwinner
c
µwinner = µwinner +
v ( t, a)
c c (2)
σ2 loser
c
µloser = µloser
the number of correct pairwise comparisons
Acc =
87
0.09
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0
Bins of question difficulty scores
</figure>
<figureCaption confidence="0.9715015">
Figure 1: The distributions of calibrated question d-
ifficulty scores of MO and SO/Math.
</figureCaption>
<bodyText confidence="0.988511588235294">
Sec. 1, this is the most relevant method for our prob-
lem. Table 2 gives the accuracy of the baseline and
our Competition-based approach on SO/CPP and
SO/Math. From the results, we can see that (1) the
proposed Competition-based approach significant-
ly outperformed the PageRank-based approach on
both data sets; (2) PageRank-based approach only
achieved a similar performance as randomly guess-
ing. This is because the PageRank-based approach
only models the outcomes of competitions affected
by question difficulty. However, the outcomes of
competitions depend on both the question difficulty
levels and the expertise levels of competitors. Our
Competition-based approach considers both these
factors for modeling the competitions. The exper-
imental results demonstrate the advantage of our ap-
proach.
</bodyText>
<table confidence="0.994775666666667">
Acc@SO/CPP Acc@SO/Math
PageRank 50.38% 48.93%
Competition 66.54% 71.79%
</table>
<tableCaption confidence="0.99923">
Table 2: Accuracy on SO/CPP and SO/Math.
</tableCaption>
<subsectionHeader confidence="0.992677">
3.3 Calibrating Question Difficulty across
CQA Services
</subsectionHeader>
<bodyText confidence="0.999512888888889">
Both MO and SO/Math are CQA services for asking
mathematics questions. However, these two services
are designed for different audiences, and they have
different types of questions. MO’s primary goal
is asking and answering research level mathemat-
ics questions6. In contrast, SO/Math is for people
studying mathematics at any level in related field-
s7. Usually, the community members in MO are
not interested in basic mathematics questions. If
</bodyText>
<footnote confidence="0.995707">
6http://mathoverflow.net/faq
7http://area51.stackexchange.com/
proposals/3355/mathematics
</footnote>
<bodyText confidence="0.99981925">
a posted question is too elementary, someone will
suggest moving it to SO/Math. Similarly, if a post-
ed question is advanced, the community members in
SO/Math will recommend moving it to MO. Hence,
it is expected that the ratio of difficult questions in
MO is higher than SO/Math. In this section, we ex-
amine whether our competition-based model can i-
dentify such differences.
We first calibrate the estimated question difficul-
ty scores across these two services on a same scale.
The key idea is to link the users who participate in
both services. In both MO and SO/Math, users can
specify their home pages. We assume that if a us-
er u1 on MO and a user u2 on SO/Math have the
same home page URL, they should be linked as one
natural person in the real world. We successfully
linked 633 users. They provided 18,196 answers in
SO/Math among which 10, 993 (60.41%) were se-
lected as the best answers. In contrast, they provided
8, 044 answers in MO among which 3, 215 (39.97%)
were selected as the best answers. This shows that
these users reflect more competitive contests in MO.
After the common users are linked, we have a joint
data set of MO and SO/Math. Then, we can calibrate
the estimated question difficulty scores across the
two services by performing the competition-based
model on the joint data set. Figure 1 shows the dis-
tributions of the calibrated question difficulty scores
of MO and SO/Math on the same scale. As expect-
ed, we observed that the ratio of difficult question-
s in MO was higher than SO/Math. Additionally,
these two distributions were significantly differen-
t (Kolmogorov-Smirnov Test, p-value &lt; 0.05). This
demonstrates that our competition-based model suc-
cessfully identified the difference between questions
on two CQA services.
</bodyText>
<subsectionHeader confidence="0.998962">
3.4 Analysis on the Question Descriptions
</subsectionHeader>
<bodyText confidence="0.972084583333333">
In this section, we analyze the text of question de-
scriptions on the scale of question difficulty scores
estimated by the competition model.
Micro Level We first examine the frequency dis-
tributions of individual words over the question d-
ifficulty scores. Figure 3 shows the examples of
four words in SO/CPP. We observe that the words
’list’ and ’array’ have the lowest mean of difficul-
ty scores, compared to the words ’virtual’ and ’gcc’.
This is reasonable, since ’list’ and ’array’ are related
The ratio of questions in each
bin of question difficulty
</bodyText>
<page confidence="0.977627882352941">
1
7
13
19
25
31
37
43
49
55
61
67
73
79
85
91
97
</page>
<note confidence="0.3204125">
MO
SO/Math
</note>
<page confidence="0.693036">
88
</page>
<figure confidence="0.991956">
(a) Easy questions (b) Normal questions (c) Hard questions
</figure>
<figureCaption confidence="0.996015333333333">
Figure 2: Tag clouds on SO/Math questions with different difficulty levels
Figure 3: The frequency distributions of words on
the scale of question difficulty scores (SO/CPP).
</figureCaption>
<bodyText confidence="0.996176390243902">
to basic concepts in programming language, while
’virtual’ and ’gcc’ are related to more advanced top-
ics. It can be observed that the order of the means of
the difficulty scores of these words are well aligned
to our learning process.
Macro Level We evenly split the range of ques-
tion difficulty scores into n buckets, and we grouped
the questions into the n buckets according to which
bucket their difficulty scores were in. Then, we had
n question buckets and each bucket corresponded to
a word distribution of questions. Let variable X de-
note the distance between the difficulty scores in two
question buckets (which is the difference between
the average difficulty scores of questions in the two
buckets), and variable Y denote the Jensen-Shannon
distance between word distributions in two question
buckets. We examined the correlation between vari-
able X and variable Y . The experimental results
showed that the correlation between these two vari-
ables were strongly positive. Specifically, the cor-
relation coefficient on SO/CPP was 0.8129 and on
SO/Math was 0.7412. In other words, when the dis-
tance between the difficulty scores of two buckets
become larger, the two word distributions in the two
buckets become less similar, and vice versa.
We further visualized the word distribution in
each question bucket. We set n as 3, and we had
three question buckets: (1) easy questions; (2) nor-
mal questions; and (3) hard questions. Figure 3.4
plots the tag clouds of SO/Math questions in the
three buckets. The size of tags is proportional to
the frequency of tags in each bucket. We observed
that (1) the tag ’homework’ and ’calculus’ become s-
maller from easy questions to hard questions; (2) the
tag ’set-theory’ becomes larger. These observations
also reflect our learning process.
The above experimental results show that differ-
ent words or tags of question descriptions reflect the
question difficulty levels. This implies the possibil-
ity of predicting question difficulty purely from the
text of question descriptions.
</bodyText>
<sectionHeader confidence="0.995608" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999964727272727">
In this paper, we address the problem of estimating
question difficulty in CQA services. Our proposed
competition-based model for estimating question
difficulty significantly outperforms the PageRank-
based approach. Most importantly, our analysis
shows that the text of question descriptions reflect-
s the question difficulty. In the future, we would
like to explore predicting question difficulty from
the text of question descriptions. We also will inves-
tigate non-technical areas, where there might be no
strongly distinct notion of experts and non-experts.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999790333333333">
We would like to thank Yunbo Cao and Jie Cai for
their valuable suggestions for this paper, and the
anonymous reviewers for their helpful comments.
</bodyText>
<figure confidence="0.9993823125">
1 2 3 4 5 6 7 8 9 10 11 12
Bins of the clustion difficulty scores
The number of questions
(normalized)
0.35
0.25
0.15
0.05
0.3
0.2
0.1
0
array
Virtual
gcc
list
</figure>
<page confidence="0.996859">
89
</page>
<sectionHeader confidence="0.990043" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99989327631579">
M.S. Ackerman and D.W. McDonald. 1996. Answer
garden 2: merging organizational memory with col-
laborative help. In Proceedings of CSCW.
E. Agichtein, C. Castillo, D. Donato, A. Gionis, and
G. Mishne. 2008. Finding high-quality content in so-
cial media. In Proceedings of WSDM.
J. Bian, Y. Liu, D. Zhou, E. Agichtein, and H. Zha. 2009.
Learning to recognize reliable users and content in so-
cial media with coupled mutual reinforcement. In Pro-
ceedings of WWW.
M. Bouguessa, B. Dumoulin, and S. Wang. 2008. Iden-
tifying authoritative actors in question-answering fo-
rums: the case of yahoo! answers. In Proceeding of
SIGKDD.
Xin Cao, Gao Cong, Bin Cui, and Christian S Jensen.
2010. A generalized framework of exploring category
information for question retrieval in community ques-
tion answer archives. In Proceedings of WWW.
H. Duan, Y. Cao, C.Y. Lin, and Y. Yu. 2008. Searching
questions by identifying question topic and question
focus. In Proceedings of ACL.
R. Herbrich, T. Minka, and T. Graepel. 2007. Trueskil-
l: A bayesian skill rating system. In Proceedings of
NIPS.
J. Jeon, W.B. Croft, J.H. Lee, and S. Park. 2006. A
framework to predict the quality of answers with non-
textual features. In Proceedings of SIGIR.
Zongcheng Ji, Fei Xu, Bin Wang, and Ben He. 2012.
Question-answer topic model for question retrieval in
community question answering. In Proceedings of
CIKM.
P. Jurczyk and E. Agichtein. 2007. Discovering au-
thorities in question answer communities by using link
analysis. In Proceedings of CIKM.
B. Li and I. King. 2010. Routing questions to appropriate
answerers in community question answering services.
In Proceedings of CIKM.
B. Li, I. King, and M.R. Lyu. 2011. Question routing in
community question answering: putting category in its
place. In Proceedings of CIKM.
Y. Liu, J. Bian, and E. Agichtein. 2008. Predicting in-
formation seeker satisfaction in community question
answering. In Proceedings of SIGIR.
J. Liu, Y.I. Song, and C.Y. Lin. 2011. Competition-based
user expertise score estimation. In Proceedings of SI-
GIR.
K.K. Nam, M.S. Ackerman, and L.A. Adamic. 2009.
Questions in, knowledge in?: a study of naver’s ques-
tion answering community. In Proceedings of CHI.
A. Pal and J.A. Konstan. 2010. Expert identification in
community question answering: exploring question s-
election bias. In Proceedings of CIKM.
M.A. Suryanto, E.P. Lim, A. Sun, and R.H.L. Chiang.
2009. Quality-aware collaborative question answer-
ing: methods and evaluation. In Proceedings of WSD-
M.
Xiaobing Xue, Jiwoon Jeon, and W Bruce Croft. 2008.
Retrieval models for question and answer archives. In
Proceedings of SIGIR.
Jiang Yang, Lada Adamic, and Mark Ackerman. 2008.
Competing to share expertise: the taskcn knowledge
sharing community. In Proceedings of ICWSM.
J. Zhang, M.S. Ackerman, and L. Adamic. 2007. Ex-
pertise networks in online communities: structure and
algorithms. In Proceedings of WWW.
Weinan Zhang, Zhaoyan Ming, Yu Zhang, Liqiang Nie,
Ting Liu, and Tat-Seng Chua. 2012. The use of depen-
dency relation graph to enhance the term weighting in
question retrieval. In Proceedings of COLING.
Y. Zhou, G. Cong, B. Cui, C.S. Jensen, and J. Yao. 2009.
Routing questions to the right users in online commu-
nities. In Proceedings of ICDE.
Guangyou Zhou, Li Cai, Jun Zhao, and Kang Liu. 2011.
Phrase-based translation model for question retrieval
in community question answer archives. In Proceed-
ings of ACL.
</reference>
<page confidence="0.998636">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.578644">
<title confidence="0.84813">Difficulty Estimation in Community Question Answering</title>
<affiliation confidence="0.94375">Institute of Technology, Harbin 150001,</affiliation>
<address confidence="0.832875">University, Beijing 100871, Research Asia, Beijing 100080,</address>
<email confidence="0.997237">quanwang1012@gmail.com</email>
<abstract confidence="0.999332642857143">In this paper, we address the problem of estimating question difficulty in community question answering services. We propose a competition-based model for estimating question difficulty by leveraging pairwise comparisons between questions and users. Our experimental results show that our model significantly outperforms a PageRank-based approach. Most importantly, our analysis shows that the text of question descriptions reflects the question difficulty. This implies the possibility of predicting question difficulty from the text of question descriptions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M S Ackerman</author>
<author>D W McDonald</author>
</authors>
<title>Answer garden 2: merging organizational memory with collaborative help.</title>
<date>1996</date>
<booktitle>In Proceedings of CSCW.</booktitle>
<contexts>
<context position="2367" citStr="Ackerman and McDonald, 1996" startWordPosition="348" endWordPosition="352">ently affiliated with Institute of Information Engineering, Chinese Academy of Sciences. lhttp://stackoverflow.com zhttp://answers.yahoo.com However, less attention has been paid to question difficulty estimation in CQA. Question difficulty estimation can benefit many applications: (1) Experts are usually under time constraints. We do not want to bore experts by routing every question (including both easy and hard ones) to them. Assigning questions to experts by matching question difficulty with expertise level, not just question topic, will make better use of the experts’ time and expertise (Ackerman and McDonald, 1996). (2) Nam et al. (2009) found that winning the point awards offered by the reputation system is a driving factor in user participation in CQA. Question difficulty estimation would be helpful in designing a better incentive mechanism by assigning higher point awards to more difficult questions. (3) Question difficulty estimation can help analyze user behavior in CQA, since users may make strategic choices when encountering questions of different difficulty levels. To the best of our knowledge, not much research has been conducted on the problem of estimating question difficulty in CQA. The most</context>
</contexts>
<marker>Ackerman, McDonald, 1996</marker>
<rawString>M.S. Ackerman and D.W. McDonald. 1996. Answer garden 2: merging organizational memory with collaborative help. In Proceedings of CSCW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>C Castillo</author>
<author>D Donato</author>
<author>A Gionis</author>
<author>G Mishne</author>
</authors>
<title>Finding high-quality content in social media.</title>
<date>2008</date>
<booktitle>In Proceedings of WSDM.</booktitle>
<contexts>
<context position="1355" citStr="Agichtein et al., 2008" startWordPosition="191" endWordPosition="194">shows that the text of question descriptions reflects the question difficulty. This implies the possibility of predicting question difficulty from the text of question descriptions. 1 Introduction In recent years, community question answering (CQA) services such as Stackoverflowl and Yahoo! Answers have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). *This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Information Engineering, Chinese Academy of Sciences. lhttp://stackoverflow.com zhttp://answers.yahoo.com However, less attention has been paid to question difficulty estimation in </context>
</contexts>
<marker>Agichtein, Castillo, Donato, Gionis, Mishne, 2008</marker>
<rawString>E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G. Mishne. 2008. Finding high-quality content in social media. In Proceedings of WSDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bian</author>
<author>Y Liu</author>
<author>D Zhou</author>
<author>E Agichtein</author>
<author>H Zha</author>
</authors>
<title>Learning to recognize reliable users and content in social media with coupled mutual reinforcement.</title>
<date>2009</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="1374" citStr="Bian et al., 2009" startWordPosition="195" endWordPosition="198">uestion descriptions reflects the question difficulty. This implies the possibility of predicting question difficulty from the text of question descriptions. 1 Introduction In recent years, community question answering (CQA) services such as Stackoverflowl and Yahoo! Answers have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). *This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Information Engineering, Chinese Academy of Sciences. lhttp://stackoverflow.com zhttp://answers.yahoo.com However, less attention has been paid to question difficulty estimation in CQA. Question diffi</context>
</contexts>
<marker>Bian, Liu, Zhou, Agichtein, Zha, 2009</marker>
<rawString>J. Bian, Y. Liu, D. Zhou, E. Agichtein, and H. Zha. 2009. Learning to recognize reliable users and content in social media with coupled mutual reinforcement. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bouguessa</author>
<author>B Dumoulin</author>
<author>S Wang</author>
</authors>
<title>Identifying authoritative actors in question-answering forums: the case of yahoo! answers.</title>
<date>2008</date>
<booktitle>In Proceeding of SIGKDD.</booktitle>
<contexts>
<context position="1497" citStr="Bouguessa et al., 2008" startWordPosition="215" endWordPosition="218">from the text of question descriptions. 1 Introduction In recent years, community question answering (CQA) services such as Stackoverflowl and Yahoo! Answers have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). *This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Information Engineering, Chinese Academy of Sciences. lhttp://stackoverflow.com zhttp://answers.yahoo.com However, less attention has been paid to question difficulty estimation in CQA. Question difficulty estimation can benefit many applications: (1) Experts are usually under time constraints. We do not want to bore expe</context>
</contexts>
<marker>Bouguessa, Dumoulin, Wang, 2008</marker>
<rawString>M. Bouguessa, B. Dumoulin, and S. Wang. 2008. Identifying authoritative actors in question-answering forums: the case of yahoo! answers. In Proceeding of SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Cao</author>
<author>Gao Cong</author>
<author>Bin Cui</author>
<author>Christian S Jensen</author>
</authors>
<title>A generalized framework of exploring category information for question retrieval in community question answer archives.</title>
<date>2010</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="1243" citStr="Cao et al., 2010" startWordPosition="171" endWordPosition="174">s show that our model significantly outperforms a PageRank-based approach. Most importantly, our analysis shows that the text of question descriptions reflects the question difficulty. This implies the possibility of predicting question difficulty from the text of question descriptions. 1 Introduction In recent years, community question answering (CQA) services such as Stackoverflowl and Yahoo! Answers have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). *This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Information Engineering, Chinese Academy of Sciences. lhttp://stackov</context>
</contexts>
<marker>Cao, Cong, Cui, Jensen, 2010</marker>
<rawString>Xin Cao, Gao Cong, Bin Cui, and Christian S Jensen. 2010. A generalized framework of exploring category information for question retrieval in community question answer archives. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Duan</author>
<author>Y Cao</author>
<author>C Y Lin</author>
<author>Y Yu</author>
</authors>
<title>Searching questions by identifying question topic and question focus.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1183" citStr="Duan et al., 2008" startWordPosition="158" endWordPosition="162">parisons between questions and users. Our experimental results show that our model significantly outperforms a PageRank-based approach. Most importantly, our analysis shows that the text of question descriptions reflects the question difficulty. This implies the possibility of predicting question difficulty from the text of question descriptions. 1 Introduction In recent years, community question answering (CQA) services such as Stackoverflowl and Yahoo! Answers have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). *This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Informati</context>
</contexts>
<marker>Duan, Cao, Lin, Yu, 2008</marker>
<rawString>H. Duan, Y. Cao, C.Y. Lin, and Y. Yu. 2008. Searching questions by identifying question topic and question focus. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Herbrich</author>
<author>T Minka</author>
<author>T Graepel</author>
</authors>
<title>Trueskill: A bayesian skill rating system.</title>
<date>2007</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="8050" citStr="Herbrich et al., 2007" startWordPosition="1332" endWordPosition="1335">ons: • The difficulty score of question q is higher than the expertise score of asker ua, but lower than that of the best answerer ub. This is intuitive since the as the set of all two-player competitions. Our problem is then to learn the relative skills of players from R. The learned skills of the pseudo question users are question difficulty scores, and the learned skills of all other users are their expertise scores. TrueSkill In this paper, we follow (Liu et al., 2011) and apply TrueSkill to learn the relative skills of players from the set of generated competitions R (Equ. 1). TrueSkill (Herbrich et al., 2007) is a Bayesian skill rating model that is developed for estimating the relative skill levels of players in games. In this paper, we present a two-player version of TrueSkill with no-draw. TrueSkill assumes that the practical performance of each player in a game follows a normal distribu86 tion N(µ, σ2), where µ means the skill level of the player and σ means the uncertainty of the estimated skill level. Basically, TrueSkill learns the skill levels of players by leveraging Bayes’ theorem. Given the current estimated skill levels of two players (priori probability) and the outcome of a new game </context>
<context position="9718" citStr="Herbrich et al., 2007" startWordPosition="1649" endWordPosition="1652">evel wins the game, it will cause large updates in skill level µ and uncertainty σ. According to these intuitions, the equations to update the skill level µ and uncertainty σ are as follows: ( t ) · vc, ε , (3) c [ ( t )] 1 − σ2 σ2 winner winner = σ2 winner · c2 · w c, ε , c (4) [ (t )] 1 − σ2 σ2 loser = σ2 loser loser · c2 · w c, ε , (5) c where t = µwinner − µloser and c2 = 2β2 + σ2 winner +σ2loser. Here, ε is a parameter representing the probability of a draw in one game, and v(t, ε) and w(t, ε) are weighting factors for skill level µ and standard deviation σ respectively. Please refer to (Herbrich et al., 2007) for more details. In this paper, we set the initial values of the skill level µ and the standard deviation σ of each player the same as the default values used in (Herbrich et al., 2007). 3 Experiments 3.1 Data Set In this paper, we use Stack Overflow (SO) for our experiments. We obtained a publicly available data seta of SO between July 31, 2008 and August 1, 2012. SO contains questions with various topics, such as programming, mathematics, and English. In this paper, we use SO C++ programming (SO/CPP) 3http://blog.stackoverflow.com/category/ cc-wiki-dump/ and mathematics (SO/Math) questions</context>
</contexts>
<marker>Herbrich, Minka, Graepel, 2007</marker>
<rawString>R. Herbrich, T. Minka, and T. Graepel. 2007. Trueskill: A bayesian skill rating system. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jeon</author>
<author>W B Croft</author>
<author>J H Lee</author>
<author>S Park</author>
</authors>
<title>A framework to predict the quality of answers with nontextual features.</title>
<date>2006</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="1331" citStr="Jeon et al., 2006" startWordPosition="187" endWordPosition="190">ntly, our analysis shows that the text of question descriptions reflects the question difficulty. This implies the possibility of predicting question difficulty from the text of question descriptions. 1 Introduction In recent years, community question answering (CQA) services such as Stackoverflowl and Yahoo! Answers have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). *This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Information Engineering, Chinese Academy of Sciences. lhttp://stackoverflow.com zhttp://answers.yahoo.com However, less attention has been paid to question d</context>
</contexts>
<marker>Jeon, Croft, Lee, Park, 2006</marker>
<rawString>J. Jeon, W.B. Croft, J.H. Lee, and S. Park. 2006. A framework to predict the quality of answers with nontextual features. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zongcheng Ji</author>
<author>Fei Xu</author>
<author>Bin Wang</author>
<author>Ben He</author>
</authors>
<title>Question-answer topic model for question retrieval in community question answering.</title>
<date>2012</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<contexts>
<context position="1281" citStr="Ji et al., 2012" startWordPosition="179" endWordPosition="182">tperforms a PageRank-based approach. Most importantly, our analysis shows that the text of question descriptions reflects the question difficulty. This implies the possibility of predicting question difficulty from the text of question descriptions. 1 Introduction In recent years, community question answering (CQA) services such as Stackoverflowl and Yahoo! Answers have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). *This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Information Engineering, Chinese Academy of Sciences. lhttp://stackoverflow.com zhttp://answers.yahoo.com H</context>
</contexts>
<marker>Ji, Xu, Wang, He, 2012</marker>
<rawString>Zongcheng Ji, Fei Xu, Bin Wang, and Ben He. 2012. Question-answer topic model for question retrieval in community question answering. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jurczyk</author>
<author>E Agichtein</author>
</authors>
<title>Discovering authorities in question answer communities by using link analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<contexts>
<context position="1453" citStr="Jurczyk and Agichtein, 2007" startWordPosition="207" endWordPosition="210">he possibility of predicting question difficulty from the text of question descriptions. 1 Introduction In recent years, community question answering (CQA) services such as Stackoverflowl and Yahoo! Answers have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). *This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Information Engineering, Chinese Academy of Sciences. lhttp://stackoverflow.com zhttp://answers.yahoo.com However, less attention has been paid to question difficulty estimation in CQA. Question difficulty estimation can benefit many applications: (1) Experts are usually under t</context>
</contexts>
<marker>Jurczyk, Agichtein, 2007</marker>
<rawString>P. Jurczyk and E. Agichtein. 2007. Discovering authorities in question answer communities by using link analysis. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Li</author>
<author>I King</author>
</authors>
<title>Routing questions to appropriate answerers in community question answering services.</title>
<date>2010</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<contexts>
<context position="1603" citStr="Li and King, 2010" startWordPosition="235" endWordPosition="238">ices such as Stackoverflowl and Yahoo! Answers have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). *This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Information Engineering, Chinese Academy of Sciences. lhttp://stackoverflow.com zhttp://answers.yahoo.com However, less attention has been paid to question difficulty estimation in CQA. Question difficulty estimation can benefit many applications: (1) Experts are usually under time constraints. We do not want to bore experts by routing every question (including both easy and hard ones) to them. Assigning questions to experts </context>
</contexts>
<marker>Li, King, 2010</marker>
<rawString>B. Li and I. King. 2010. Routing questions to appropriate answerers in community question answering services. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Li</author>
<author>I King</author>
<author>M R Lyu</author>
</authors>
<title>Question routing in community question answering: putting category in its place.</title>
<date>2011</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<contexts>
<context position="1621" citStr="Li et al., 2011" startWordPosition="239" endWordPosition="242">verflowl and Yahoo! Answers have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). *This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Information Engineering, Chinese Academy of Sciences. lhttp://stackoverflow.com zhttp://answers.yahoo.com However, less attention has been paid to question difficulty estimation in CQA. Question difficulty estimation can benefit many applications: (1) Experts are usually under time constraints. We do not want to bore experts by routing every question (including both easy and hard ones) to them. Assigning questions to experts by matching questi</context>
</contexts>
<marker>Li, King, Lyu, 2011</marker>
<rawString>B. Li, I. King, and M.R. Lyu. 2011. Question routing in community question answering: putting category in its place. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>J Bian</author>
<author>E Agichtein</author>
</authors>
<title>Predicting information seeker satisfaction in community question answering.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="1393" citStr="Liu et al., 2008" startWordPosition="199" endWordPosition="202">s reflects the question difficulty. This implies the possibility of predicting question difficulty from the text of question descriptions. 1 Introduction In recent years, community question answering (CQA) services such as Stackoverflowl and Yahoo! Answers have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). *This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Information Engineering, Chinese Academy of Sciences. lhttp://stackoverflow.com zhttp://answers.yahoo.com However, less attention has been paid to question difficulty estimation in CQA. Question difficulty estimation ca</context>
</contexts>
<marker>Liu, Bian, Agichtein, 2008</marker>
<rawString>Y. Liu, J. Bian, and E. Agichtein. 2008. Predicting information seeker satisfaction in community question answering. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Liu</author>
<author>Y I Song</author>
<author>C Y Lin</author>
</authors>
<title>Competition-based user expertise score estimation.</title>
<date>2011</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="1539" citStr="Liu et al., 2011" startWordPosition="223" endWordPosition="226">duction In recent years, community question answering (CQA) services such as Stackoverflowl and Yahoo! Answers have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). *This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Information Engineering, Chinese Academy of Sciences. lhttp://stackoverflow.com zhttp://answers.yahoo.com However, less attention has been paid to question difficulty estimation in CQA. Question difficulty estimation can benefit many applications: (1) Experts are usually under time constraints. We do not want to bore experts by routing every question (including b</context>
<context position="3957" citStr="Liu et al. (2011)" startWordPosition="609" endWordPosition="612">lgorithm is employed on the task graph to estimate PageRank score (i.e., difficulty score) of each task. This approach implicitly assumes that task difficulty is the only factor affecting the outcomes of competitions (i.e. the best answer). However, the outcomes of competitions depend on both the difficulty levels of tasks and the expertise levels of competitors (i.e. 85 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 85–90, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics other answerers). Inspired by Liu et al. (2011), we propose a competition-based approach which jointly models question difficulty and user expertise level. Our approach is based on two intuitive assumptions: (1) given a question answering thread, the difficulty score of the question is higher than the expertise score of the asker, but lower than that of the best answerer; (2) the expertise score of the best answerer is higher than that of the asker as well as all other answerers. Given the two assumptions, we can determine the question difficulty score and user expertise score through pairwise comparisons between (1) a question and an aske</context>
<context position="7905" citStr="Liu et al., 2011" startWordPosition="1307" endWordPosition="1310">uom}. Assuming that question difficulty scores and user expertise scores are expressed on the same scale, we make the following two assumptions: • The difficulty score of question q is higher than the expertise score of asker ua, but lower than that of the best answerer ub. This is intuitive since the as the set of all two-player competitions. Our problem is then to learn the relative skills of players from R. The learned skills of the pseudo question users are question difficulty scores, and the learned skills of all other users are their expertise scores. TrueSkill In this paper, we follow (Liu et al., 2011) and apply TrueSkill to learn the relative skills of players from the set of generated competitions R (Equ. 1). TrueSkill (Herbrich et al., 2007) is a Bayesian skill rating model that is developed for estimating the relative skill levels of players in games. In this paper, we present a two-player version of TrueSkill with no-draw. TrueSkill assumes that the practical performance of each player in a game follows a normal distribu86 tion N(µ, σ2), where µ means the skill level of the player and σ means the uncertainty of the estimated skill level. Basically, TrueSkill learns the skill levels of </context>
</contexts>
<marker>Liu, Song, Lin, 2011</marker>
<rawString>J. Liu, Y.I. Song, and C.Y. Lin. 2011. Competition-based user expertise score estimation. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K K Nam</author>
<author>M S Ackerman</author>
<author>L A Adamic</author>
</authors>
<title>Questions in, knowledge in?: a study of naver’s question answering community.</title>
<date>2009</date>
<booktitle>In Proceedings of CHI.</booktitle>
<contexts>
<context position="2390" citStr="Nam et al. (2009)" startWordPosition="354" endWordPosition="357"> Information Engineering, Chinese Academy of Sciences. lhttp://stackoverflow.com zhttp://answers.yahoo.com However, less attention has been paid to question difficulty estimation in CQA. Question difficulty estimation can benefit many applications: (1) Experts are usually under time constraints. We do not want to bore experts by routing every question (including both easy and hard ones) to them. Assigning questions to experts by matching question difficulty with expertise level, not just question topic, will make better use of the experts’ time and expertise (Ackerman and McDonald, 1996). (2) Nam et al. (2009) found that winning the point awards offered by the reputation system is a driving factor in user participation in CQA. Question difficulty estimation would be helpful in designing a better incentive mechanism by assigning higher point awards to more difficult questions. (3) Question difficulty estimation can help analyze user behavior in CQA, since users may make strategic choices when encountering questions of different difficulty levels. To the best of our knowledge, not much research has been conducted on the problem of estimating question difficulty in CQA. The most relevant work is a Pag</context>
</contexts>
<marker>Nam, Ackerman, Adamic, 2009</marker>
<rawString>K.K. Nam, M.S. Ackerman, and L.A. Adamic. 2009. Questions in, knowledge in?: a study of naver’s question answering community. In Proceedings of CHI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pal</author>
<author>J A Konstan</author>
</authors>
<title>Expert identification in community question answering: exploring question selection bias.</title>
<date>2010</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<contexts>
<context position="1520" citStr="Pal and Konstan, 2010" startWordPosition="219" endWordPosition="222">n descriptions. 1 Introduction In recent years, community question answering (CQA) services such as Stackoverflowl and Yahoo! Answers have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). *This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Information Engineering, Chinese Academy of Sciences. lhttp://stackoverflow.com zhttp://answers.yahoo.com However, less attention has been paid to question difficulty estimation in CQA. Question difficulty estimation can benefit many applications: (1) Experts are usually under time constraints. We do not want to bore experts by routing every qu</context>
</contexts>
<marker>Pal, Konstan, 2010</marker>
<rawString>A. Pal and J.A. Konstan. 2010. Expert identification in community question answering: exploring question selection bias. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Suryanto</author>
<author>E P Lim</author>
<author>A Sun</author>
<author>R H L Chiang</author>
</authors>
<title>Quality-aware collaborative question answering: methods and evaluation.</title>
<date>2009</date>
<booktitle>In Proceedings of WSDM.</booktitle>
<contexts>
<context position="1206" citStr="Suryanto et al., 2009" startWordPosition="163" endWordPosition="166">estions and users. Our experimental results show that our model significantly outperforms a PageRank-based approach. Most importantly, our analysis shows that the text of question descriptions reflects the question difficulty. This implies the possibility of predicting question difficulty from the text of question descriptions. 1 Introduction In recent years, community question answering (CQA) services such as Stackoverflowl and Yahoo! Answers have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). *This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Information Engineering, Chinese</context>
</contexts>
<marker>Suryanto, Lim, Sun, Chiang, 2009</marker>
<rawString>M.A. Suryanto, E.P. Lim, A. Sun, and R.H.L. Chiang. 2009. Quality-aware collaborative question answering: methods and evaluation. In Proceedings of WSDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaobing Xue</author>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
</authors>
<title>Retrieval models for question and answer archives.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="1164" citStr="Xue et al., 2008" startWordPosition="154" endWordPosition="157">aging pairwise comparisons between questions and users. Our experimental results show that our model significantly outperforms a PageRank-based approach. Most importantly, our analysis shows that the text of question descriptions reflects the question difficulty. This implies the possibility of predicting question difficulty from the text of question descriptions. 1 Introduction In recent years, community question answering (CQA) services such as Stackoverflowl and Yahoo! Answers have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). *This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Ins</context>
</contexts>
<marker>Xue, Jeon, Croft, 2008</marker>
<rawString>Xiaobing Xue, Jiwoon Jeon, and W Bruce Croft. 2008. Retrieval models for question and answer archives. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Yang</author>
<author>Lada Adamic</author>
<author>Mark Ackerman</author>
</authors>
<title>Competing to share expertise: the taskcn knowledge sharing community.</title>
<date>2008</date>
<booktitle>In Proceedings of ICWSM.</booktitle>
<contexts>
<context position="3041" citStr="Yang et al. (2008)" startWordPosition="457" endWordPosition="460">rds offered by the reputation system is a driving factor in user participation in CQA. Question difficulty estimation would be helpful in designing a better incentive mechanism by assigning higher point awards to more difficult questions. (3) Question difficulty estimation can help analyze user behavior in CQA, since users may make strategic choices when encountering questions of different difficulty levels. To the best of our knowledge, not much research has been conducted on the problem of estimating question difficulty in CQA. The most relevant work is a PageRank-based approach proposed by Yang et al. (2008) to estimate task difficulty in crowdsourcing contest services. Their key idea is to construct a graph of tasks: creating an edge from a task t1 to a task t2 when a user u wins task t1 but loses task t2, implying that task t2 is likely to be more difficult than task t1. Then the standard PageRank algorithm is employed on the task graph to estimate PageRank score (i.e., difficulty score) of each task. This approach implicitly assumes that task difficulty is the only factor affecting the outcomes of competitions (i.e. the best answer). However, the outcomes of competitions depend on both the dif</context>
<context position="4897" citStr="Yang et al., 2008" startWordPosition="767" endWordPosition="770">erer; (2) the expertise score of the best answerer is higher than that of the asker as well as all other answerers. Given the two assumptions, we can determine the question difficulty score and user expertise score through pairwise comparisons between (1) a question and an asker, (2) a question and a best answerer, (3) a best answerer and an asker, and (4) a best answerer and all other non-best answerers. The main contributions of this paper are: • We propose a competition-based approach to estimate question difficulty (Sec. 2). Our model significantly outperforms the PageRank-based approach (Yang et al., 2008) for estimating question difficulty on the data of Stack Overflow (Sec. 3.2). • Additionally, we calibrate question difficulty scores across two CQA services to verify the effectiveness of our model (Sec. 3.3). • Most importantly, we demonstrate that different words or tags in the question descriptions indicate question difficulty levels. This implies the possibility of predicting question difficulty purely from the text of question descriptions (Sec. 3.4). best answer ub correctly responds to question q that asker ua does not know. • The expertise score of the best answerer ub is higher than </context>
<context position="12193" citStr="Yang et al. (2008)" startWordPosition="2058" endWordPosition="2061">) Unknown, which means that the annotator could not make a decision. The agreements between annotators on both SO/CPP (kappa value = 0.741) and SO/Math (kappa value = 0.873) were substantial. When evaluating models, we only kept the pairs that annotators had given the same labels. There were 260 SO/CPP question pairs and 280 SO/Math question pairs remaining. 3.2 Accuracy of Question Difficulty Estimation We employ a standard evaluation metric for information retrieval: accuracy (Acc), defined as follows: . the total number of pairwise comparisons We use the PageRank-based approach proposed by Yang et al. (2008) as a baseline. As described in 4http://math.stackexchange.com 5http://mathoverflow.net Qwinner c µwinner = µwinner + v ( t, a) c c (2) σ2 loser c µloser = µloser the number of correct pairwise comparisons Acc = 87 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0 Bins of question difficulty scores Figure 1: The distributions of calibrated question difficulty scores of MO and SO/Math. Sec. 1, this is the most relevant method for our problem. Table 2 gives the accuracy of the baseline and our Competition-based approach on SO/CPP and SO/Math. From the results, we can see that (1) the proposed Compe</context>
</contexts>
<marker>Yang, Adamic, Ackerman, 2008</marker>
<rawString>Jiang Yang, Lada Adamic, and Mark Ackerman. 2008. Competing to share expertise: the taskcn knowledge sharing community. In Proceedings of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhang</author>
<author>M S Ackerman</author>
<author>L Adamic</author>
</authors>
<title>Expertise networks in online communities: structure and algorithms.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="1473" citStr="Zhang et al., 2007" startWordPosition="211" endWordPosition="214">question difficulty from the text of question descriptions. 1 Introduction In recent years, community question answering (CQA) services such as Stackoverflowl and Yahoo! Answers have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). *This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Information Engineering, Chinese Academy of Sciences. lhttp://stackoverflow.com zhttp://answers.yahoo.com However, less attention has been paid to question difficulty estimation in CQA. Question difficulty estimation can benefit many applications: (1) Experts are usually under time constraints. We </context>
</contexts>
<marker>Zhang, Ackerman, Adamic, 2007</marker>
<rawString>J. Zhang, M.S. Ackerman, and L. Adamic. 2007. Expertise networks in online communities: structure and algorithms. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weinan Zhang</author>
<author>Zhaoyan Ming</author>
<author>Yu Zhang</author>
<author>Liqiang Nie</author>
<author>Ting Liu</author>
<author>Tat-Seng Chua</author>
</authors>
<title>The use of dependency relation graph to enhance the term weighting in question retrieval.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="1263" citStr="Zhang et al., 2012" startWordPosition="175" endWordPosition="178">del significantly outperforms a PageRank-based approach. Most importantly, our analysis shows that the text of question descriptions reflects the question difficulty. This implies the possibility of predicting question difficulty from the text of question descriptions. 1 Introduction In recent years, community question answering (CQA) services such as Stackoverflowl and Yahoo! Answers have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). *This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Information Engineering, Chinese Academy of Sciences. lhttp://stackoverflow.com zhttp://a</context>
</contexts>
<marker>Zhang, Ming, Zhang, Nie, Liu, Chua, 2012</marker>
<rawString>Weinan Zhang, Zhaoyan Ming, Yu Zhang, Liqiang Nie, Ting Liu, and Tat-Seng Chua. 2012. The use of dependency relation graph to enhance the term weighting in question retrieval. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhou</author>
<author>G Cong</author>
<author>B Cui</author>
<author>C S Jensen</author>
<author>J Yao</author>
</authors>
<title>Routing questions to the right users in online communities.</title>
<date>2009</date>
<booktitle>In Proceedings of ICDE.</booktitle>
<contexts>
<context position="1584" citStr="Zhou et al., 2009" startWordPosition="231" endWordPosition="234">nswering (CQA) services such as Stackoverflowl and Yahoo! Answers have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). *This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Information Engineering, Chinese Academy of Sciences. lhttp://stackoverflow.com zhttp://answers.yahoo.com However, less attention has been paid to question difficulty estimation in CQA. Question difficulty estimation can benefit many applications: (1) Experts are usually under time constraints. We do not want to bore experts by routing every question (including both easy and hard ones) to them. Assigning qu</context>
</contexts>
<marker>Zhou, Cong, Cui, Jensen, Yao, 2009</marker>
<rawString>Y. Zhou, G. Cong, B. Cui, C.S. Jensen, and J. Yao. 2009. Routing questions to the right users in online communities. In Proceedings of ICDE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangyou Zhou</author>
<author>Li Cai</author>
<author>Jun Zhao</author>
<author>Kang Liu</author>
</authors>
<title>Phrase-based translation model for question retrieval in community question answer archives.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1225" citStr="Zhou et al., 2011" startWordPosition="167" endWordPosition="170">experimental results show that our model significantly outperforms a PageRank-based approach. Most importantly, our analysis shows that the text of question descriptions reflects the question difficulty. This implies the possibility of predicting question difficulty from the text of question descriptions. 1 Introduction In recent years, community question answering (CQA) services such as Stackoverflowl and Yahoo! Answers have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012); (2) answer quality estimation (Jeon et al., 2006; Agichtein et al., 2008; Bian et al., 2009; Liu et al., 2008); (3) user expertise estimation (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; Liu et al., 2011); and (4) question routing (Zhou et al., 2009; Li and King, 2010; Li et al., 2011). *This work was done when Jing Liu and Quan Wang were visiting students at Microsoft Research Asia. Quan Wang is currently affiliated with Institute of Information Engineering, Chinese Academy of Science</context>
</contexts>
<marker>Zhou, Cai, Zhao, Liu, 2011</marker>
<rawString>Guangyou Zhou, Li Cai, Jun Zhao, and Kang Liu. 2011. Phrase-based translation model for question retrieval in community question answer archives. In Proceedings of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
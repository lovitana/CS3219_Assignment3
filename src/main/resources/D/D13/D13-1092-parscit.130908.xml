<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001612">
<title confidence="0.896394">
Implicit Feature Detection via a Constrained Topic Model and SVM
</title>
<author confidence="0.996431">
Wei Wang ,* Hua Xu * and Xiaoqiu Huang †
</author>
<affiliation confidence="0.9930166">
*State Key Laboratory of Intelligent Technology and Systems,
Tsinghua National Laboratory for Information Science and Technology,
Department of Computer Science and Technology, Tsinghua University,
Beijing 100084, China
†Beijing University of Posts and Telecommunications, Beijing 100876, China
</affiliation>
<email confidence="0.989475">
ww880412@gmail.com, xuhua@tsinghua.edu.cn, alexalexhxqhxq@gmail.com
</email>
<sectionHeader confidence="0.996269" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9656405">
Implicit feature detection, also known as im-
plicit feature identification, is an essential as-
pect of feature-specific opinion mining but
previous works have often ignored it. We
think, based on the explicit sentences, sever-
al Support Vector Machine (SVM) classifier-
s can be established to do this task. Never-
theless, we believe it is possible to do bet-
ter by using a constrained topic model instead
of traditional attribute selection methods. Ex-
periments show that this method outperforms
the traditional attribute selection methods by
a large margin and the detection task can be
completed better.
</bodyText>
<sectionHeader confidence="0.998597" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997237739130435">
Feature-specific opinion mining has been well de-
fined by Ding and Liu(2008). Example 1 is a cell
phone review in which two features are mentioned.
Example 1 This cell phone is fashion in appear-
ance, and it is also very cheap.
If a feature appears in a review directly, it is called
an explicit feature. If a feature is only implied, it is
called an implicitfeature. In Example 1, appearance
is an explicit feature while price is an implicit fea-
ture, which is implied by cheap. Furthermore, an ex-
plicit sentence is defined as a sentence containing at
least one explicit feature, and an implicit sentence is
the sentence only containing implicit features. Thus,
the first sentence is an explicit sentence, while the
second is an implicit one.
This paper proposes an approach for implicit fea-
ture detection based on SVM and Topic Model(TM).
The Topic Model, which incorporated into con-
straints based on the pre-defined product feature,
is established to extract the training attributes for
SVM. In the end, several SVM classifiers are con-
structed to train the selected attributes and utilized
to detect the implicit features.
</bodyText>
<sectionHeader confidence="0.999791" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999891">
The definition of implicit feature comes from Liu
et al. (2005)’s work. Su et al. (2006) used Point-
wise Mutual Information (PMI) based semantic as-
sociation analysis to identify implicit features, but
no quantitative experimental results were provided.
Hai et al. (2011) used co-occurrence association rule
mining to identify implicit features. However, they
only dealt with opinion words and neglected the
facts. Therefore, in this paper, both the opinions and
facts will be taken into account.
Blei et al. (2003) proposed the original LDA us-
ing EM estimation. Griffiths and Steyvers (2004)
applied Gibbs sampling to estimate LDA’s parame-
ters. Since the inception of these works, many vari-
ations have been proposed. For example, LDA has
previously been used to construct attributes for clas-
sification; it often acts to reduce data dimension(Blei
and Jordan, 2003; Fei-Fei and Perona, 2005; Quel-
has et al., 2005). Here, we modify LDA and adopt it
to select the training attributes for SVM.
</bodyText>
<sectionHeader confidence="0.99754" genericHeader="method">
3 Model Design
</sectionHeader>
<subsectionHeader confidence="0.999767">
3.1 Introduction to LDA
</subsectionHeader>
<bodyText confidence="0.9996205">
We briefly introduce LDA, following the notation
of Griffiths(Griffiths and Steyvers, 2004). Given D
</bodyText>
<page confidence="0.989405">
903
</page>
<bodyText confidence="0.888828125">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 903–907,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
documents expressed over W unique words and T
topics, LDA outputs the document-topic distribution
θ and topic-word distribution cp, both of which can
be obtained with Gibbs Sampling. For this scheme,
the core process is the topic updating for each word
in each document according to Equation 1.
</bodyText>
<equation confidence="0.994441">
P(zi = j|z−i, w, α, O) =
n−i,j + O
(wi)
( Ew′ n(w,j + WO)
</equation>
<bodyText confidence="0.999679333333333">
where zi = j represents the assignment of the ith
word in a document to topic j, z−i represents all
the topic assignments excluding the ith word. n(w′)
</bodyText>
<equation confidence="0.527165">
j
is the number of instances of word w′ assigned to
topic j and n(di)
</equation>
<bodyText confidence="0.998429888888889">
j is the number of words from doc-
ument di assigned to topic j, the −i notation sig-
nifies that the counts are taken omitting the value
of zi. Furthermore, α and O are hyper-parameters
for the document-topic and topic-word Dirichlet dis-
tributions, respectively. After N iterations of Gibbs
sampling for all words in all documents, the distri-
bution θ and cp are finally estimated using Equations
2 and 3.
</bodyText>
<equation confidence="0.999559428571429">
n(wi) + O
(wi) = j (2)
′
j EWw′ new) + WO
n(di) + α
θ(di) = j(3)
ETn(di) j+ Tα
</equation>
<subsectionHeader confidence="0.997988">
3.2 Framework
</subsectionHeader>
<bodyText confidence="0.998471111111111">
Algorithm 1 summarizes the main steps. When a
specific product and the reviews are provided, the
explicit sentences and corresponding features are
extracted(Line 1) by word segmentation, part-of-
speech(POS) tagging and synonyms feature cluster-
ing. Then the prior knowledge are drawn from the
explicit sentences automatically and integrated in-
to the constrained topic model((Line 3 - Line 5).
The word clusters are chosen as the training at-
</bodyText>
<construct confidence="0.39382575">
tributes(Line 6). Finally, several SVM classifier-
s are generated and applied to detect implicit fea-
tures(Line 7 - Line 12).
Algorithm 1 Implicit Feature Detection
</construct>
<listItem confidence="0.962334333333333">
1: ES ← extract explicit sentence set
2: NES ← non-explicit sentence set
3: CS ← constraint set from ES
4: CPK ← correlation prior knowledge from ES
5: ETM ← ConstrainedTopicModel(T,ES,CS,CPK)
6: TA ← select training attributes from ETM
7: for each fi in feature clusters do
8: TDi ← GenerateTrainingData(TAi,ES)
9: Ci ← BuildClassificationModelBySVM(TDi)
10: PRi ← positive result of Classify(Ci,NES)
11: the feature of sentence in PRi ← fi
12: end for
</listItem>
<subsectionHeader confidence="0.9778445">
3.3 Prior Knowledge Extraction and
Incorporation
</subsectionHeader>
<bodyText confidence="0.998085333333333">
It is obvious that the pre-existing knowledge can as-
sist to produce better and more significant clusters.
In our work, we use a constrained topic model to s-
elect attributes for each product features. Each topic
is first pre-defined a product feature. Then two type-
s of prior knowledge, which are derived from the
pre-defined product features, are extracted automat-
ically and incorporated: must-link/cannot-link and
correlation prior knowledge.
</bodyText>
<subsectionHeader confidence="0.822861">
3.3.1 Must-link and Cannot-link
</subsectionHeader>
<bodyText confidence="0.983541772727273">
Must-link: It specifies that two data instances
must be in the same cluster. Here is the must-link
from an observation: as ”cheap” to ”price”, some
words must be associated with a feature. In order
to mine these words, we compute the co-occurrence
degree by frequency*PMI(f,w), whose formula is as
Pf�w
following: Pf&amp;w � lo�2 Pf Pw , where P is the proba-
bility of subscript occurrence in explicit sentences,
f is the feature, w is the word, and f&amp;w means
the co-occurrence of f and w. A higher value of
frequency*PMI signifies that w often indicates f.
For a feature fi, the top five words and fi consti-
tute must-links. For example, the co-occurrence of
”price” and ”cheap” is very high, then the must-link
between ”price” and ”cheap” can be identified.
Cannot-link: It specifies that two data instances
cannot be in the same cluster. If a word and a fea-
ture never co-occur in our corpus, we assume them
to form a cannot-link. For example, the word low-
cost has never co-occurred with the product feature
screen, so they constitute a cannot-link in our cor-
</bodyText>
<equation confidence="0.9627982">
n−i,j + α
(di) (1)
ET n(di) + Tα
j −i,j
)
</equation>
<page confidence="0.980041">
904
</page>
<bodyText confidence="0.998167625">
pus.
In this paper, the pre-defined process, must-link,
and cannot-link are derived from Andrzejewski and
Zhu (2009)’s work, all must-links and cannot-links
are incorporated our constrained topic model. We
multiply an indicator function 6(wi, zj), which rep-
resents a hard constraint, to the Equation 1 as the
final probability for topic updating (see Equation 4).
</bodyText>
<equation confidence="0.999132142857143">
P(zi = j|z−i, w, α, β) =
n(wi3
+ N
∑W)(
w′ n(w′)
−i,j + Wβ
(4)
</equation>
<bodyText confidence="0.9998395">
As illustrated by Equations 1 and 4, 6(wi, zj),
which represents intervention or help from pre-
existing knowledge of must-links and cannot-links,
plays a key role in this study. In the topic updating
for each word in each document, we assume that the
current word is wi and its linked feature topic set is
Z(wi), then for the current topic zj, 6(wi, zj) is cal-
culated as follows:
</bodyText>
<listItem confidence="0.998195571428571">
1. If wi is constrained by must-links and the
linked feature belongs to Z(wi), 6(wi, zj|zj ∈
Z(wi)) = 1 and 6(wi, zj|zj ∈/ Z(wi)) = 0.
2. If wi is constrained by cannot-links and the
linked feature belongs to Z(wi), 6(wi, zj|zj ∈
Z(wi)) = 0 and 6(wi, zj|zj ∈/ Z(wi)) = 1.
3. In other cases, 6(wi, zj|j = 1, ... , T) = 1.
</listItem>
<subsectionHeader confidence="0.604175">
3.3.2 Correlation Prior Knowledge
</subsectionHeader>
<bodyText confidence="0.9958035">
In view of the explicit product feature of each top-
ic, the association of the word and the feature to
topic-word distribution should be taken into accoun-
t. Therefore, Equation 2 is revised as the following:
</bodyText>
<equation confidence="0.9999375">
ϕ(wi) = (1 + Cwi,j)(njwi)) + β (5 )
∑w′ (1 + Cw′,j)(njw′)) + Wβ
</equation>
<bodyText confidence="0.999286">
where Cw′ j reflects the correlation of w′ with the
topic j, which is centered on the product feature fzj.
The basic idea is to determine the association of w′
and fzj, if they have the high relevance, Cw′,j should
be set as a positive number. Otherwise, if we can
determine w′ and fzj are irrelevant, Cw′,j should be
set as a positive number. In this paper, we attempt
to using PMI or dependency relation to judge the
relevance. For word w′ and feature fzj:
</bodyText>
<listItem confidence="0.9660665">
1. Dependency relation judgement: If w′ as par-
ent node in the syntax tree mainly co-occurs
with fzj, Cw′,j will be set positive. If w′ mainly
co-occurs with several features including fzj,
Cw′,j will be set negative. Otherwise, Cw′,j
will be set 0.
2. PMI judgement: If w′ mainly co-occurs with
fzj and PMI(w′, fzj) is greater than the giv-
en value, Cw′,j will be set positive. Otherwise,
Cw′,j will be set negative.
</listItem>
<subsectionHeader confidence="0.989911">
3.4 Attribute Selection
</subsectionHeader>
<bodyText confidence="0.999945965517241">
Some words, such as ”good”, can modify sever-
al product features and should be removed. In the
result of run once, if a word appears in the topics
which relates to different features, it is defined as a
conflicting word. If a term is thought to describe
several features or indicate no features, it is defined
as a noise word.
When each topic has been pre-allocated, we run
the explicit topic model 100 times. If a word turns
into a conflicting word Tcw times(Tcw is set to 20),
we assume that it is a noise word. Then the noise
word collection is obtained and applied to filter the
explicit sentences. Actually, here 100 is just an esti-
mated number. And for Tcw, when it is between 15
and 25, the result is same, and when it exceeds 25,
the result does not change a lot. The most important
part to filter noise words is the correlation compu-
tation. So the experiment can work well with only
estimated parameters.
Next, By integrating pre-existing knowledge, the
explicit topic model, which runs Titer times, sever-
s as attribute selection for SVM. In every result for
each topic cluster, we remove the least four prob-
able of word groups and merge the results by the
pre-defined product feature. For a feature, if a word
appears in its topic words more than Titer ∗ tratio
times, it is selected as one of the training attributes
for the feature. In the end, if an attribute associates
with different features, it is deleted.
</bodyText>
<equation confidence="0.71372475">
6(wi, zj)(
n−i,j + α
(di)
∑j n(di + Tα)
</equation>
<page confidence="0.82575">
905
</page>
<figure confidence="0.999829514285714">
(a) SVM based on traditional attribute
selection method
(b) our constrained topic model by
different trG.tio (Titer = 20)
(c) our constrained topic model by
different Titer (trG.tio = 0.1)
20 30 40 50 60 70 80 90 100
Attribute Factor Number
ChiSquare
GainRatio
InfoGain
TM
TM+must
TM+cannot
TM+must+cannot
TM+syntactic
TM+must+cannot+syntactic
TM+PMI
TM+must+cannot+PMI
TM+correlation knowledge(PMI+syntactic)
TM+must+cannot+correlation knowledge
0.1 0.2 0.3 0.4 0.5
t ratio
10 20 30 40 50
T iter
TM
TM+must
TM+cannot
TM+must+cannot
TM+syntactic
TM+must+cannot+syntactic
TM+PMI
TM+must+cannot+PMI
TM+correlation knowledge(PMI+syntactic)
TM+must+cannot+correlation knowledge
</figure>
<figureCaption confidence="0.999984">
Figure 1: Performance of different cases
</figureCaption>
<subsectionHeader confidence="0.99639">
3.5 Implicit Feature Detection via SVM
</subsectionHeader>
<bodyText confidence="0.999961375">
After completing attribute selection, vector space
model(VSM) is applied to the selected attributes on
the explicit sentences. For each feature fi, a SVM
classifier Ci is adopted. In train-set, the positive cas-
es are the explicit sentences of fi, and the negative
cases are the other explicit sentences. For a non-
explicit sentence, if the classification result of Ci is
positive, it is an implicit sentence which implies fi.
</bodyText>
<sectionHeader confidence="0.854058" genericHeader="evaluation">
4 Evaluation of Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.998751">
4.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.999977857142857">
There has no standard data set yet, we crawled the
experiment data, which included reviews about a
cellphone, from a famous Chinese shopping web-
site&apos;. The data contains 14218 sentences. The fea-
ture of each sentence was manually annotated by
two research assistants. A handful of sentences
which were annotated inconsistently were deleted.
Table 1 depicts the data set which is evaluated. Other
features were ignored because of their rare appear-
ance.
Here are some explanations: (1)The sentences
containing several explicit features were not added
to the train-set. (2) A tiny number of sentences con-
tain both explicit and implicit features, and they can
only be regarded as explicit sentences. (3) The train-
ing set contains 3140 explicit sentences, the test set
contains 7043 non-explicit sentences and more than
5500 sentences have no feature. (4) According to
the ratio among the explicit sentences(6:1:2:3:1:2),
it is reasonable that the most suitable number of top-
ics should be 14. For example, the ratio of the prod-
</bodyText>
<footnote confidence="0.425786">
lhttp://www.360buy.com/
</footnote>
<tableCaption confidence="0.999058">
Table 1: Experiment data
</tableCaption>
<table confidence="0.990277571428571">
Features Explicit Implicit Total
screen 1165 244 1409
quality 199 83 282
battery 456 205 661
price 627 561 1188
appearance 224 167 391
software 469 129 598
</table>
<bodyText confidence="0.999903222222222">
uct feature screen is 6, so we can assign the feature
to topic 0,1,2,3,4,5. In our experiment, the perfor-
mance of algorithm 1 is evaluated using F-measure.
(5) Although the size of dataset is limited, out pro-
posed is based on the constraint-based topic model,
which has been widely used in different NLP field-
s. So, our approach can generalize well in different
datasets. Of course, more high quality data will be
collected to do the experiment in the future.
</bodyText>
<subsectionHeader confidence="0.98377">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999856833333333">
Figure 1a depicts the performance of using tradi-
tional attribute selection methods on SVM. Using
X2 test on SVM can achieve the best performance,
which is about 66.7%. In our constrained topic
model, we use different TiteT and tTatio. We con-
ducted experiments by incorporating different types
prior knowledge. From Figure 1b and 1c, we con-
clude that: (1)All these methods perform much bet-
ter than the traditional feature selection methods, the
improvements are more than 6%. (2)The reason for
the little improvement of must-links is that the top-
ic clusters have already obtained these linked word-
</bodyText>
<page confidence="0.994768">
906
</page>
<bodyText confidence="0.999896888888889">
s. (3)All the pre-existing knowledge performs best
and shows 3% improvement over non prior knowl-
edge. (4)Different types of prior knowledge have
different impact on the stabilities of different pa-
rameters. (5)As we have expected, by combing al-
l prior knowledge, the best performance can reach
77.78%. Furthermore, as tratio or Titer changes,
our constrained topic model incorporating all prior
knowledge look like very stable.
</bodyText>
<sectionHeader confidence="0.999197" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999841666666667">
In this paper, we adopt a constrained topic model
incorporating prior knowledge to select attribute for
SVM classifiers to detect implicit features. Exper-
iments show this method outperforms the attribute
feature selection methods and detect implicit fea-
tures better.
</bodyText>
<sectionHeader confidence="0.999671" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.950266">
This work is supported by National Natural Science
Foundation of China (Grant No: 61175110) and Na-
tional Basic Research Program of China (973 Pro-
gram, Grant No: 2012CB316305).
</bodyText>
<sectionHeader confidence="0.994443" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999884565217391">
David Andrzejewski and Xiaojin Zhu. 2009. Laten-
t dirichlet allocation with topic-in-set knowledge. In
Proceedings of the NAACL HLT 2009 Workshop on
Semi-Supervised Learning for Natural Language Pro-
cessing, pages 43–48. Association for Computational
Linguistics.
D.M. Blei and M.I. Jordan. 2003. Modeling annotated
data. In Proceedings of the 26th annual international
ACM SIGIR conference on Research and development
in informaion retrieval, pages 127–134. ACM.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Laten-
t dirichlet allocation. the Journal of machine Learning
research, 3:993–1022.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
Proceedings of the international conference on Web
search and web data mining, WSDM ’08, pages 231–
240, New York, NY, USA. ACM.
L. Fei-Fei and P. Perona. 2005. A bayesian hierarchical
model for learning natural scene categories. In Com-
puter Vision and Pattern Recognition, 2005. CVPR
2005. IEEE Computer Society Conference on, vol-
ume 2, pages 524–531. IEEE.
T.L. Griffiths and M. Steyvers. 2004. Finding scientif-
ic topics. Proceedings of the National Academy of
Sciences of the United States of America, 101(Suppl
1):5228–5235.
Z. Hai, K. Chang, and J. Kim. 2011. Implicit feature
identification via co-occurrence association rule min-
ing. Computational Linguistics and Intelligent Text
Processing, pages 393–404.
B. Liu, M. Hu, and J. Cheng. 2005. Opinion observer:
analyzing and comparing opinions on the web. In Pro-
ceedings of the 14th international conference on World
Wide Web, pages 342–351. ACM.
P. Quelhas, F. Monay, J.M. Odobez, D. Gatica-Perez,
T. Tuytelaars, and L. Van Gool. 2005. Modeling
scenes with local descriptors and latent aspects. In
Computer Vision, 2005. ICCV 2005. Tenth IEEE In-
ternational Conference on, volume 1, pages 883–890.
IEEE.
Q. Su, K. Xiang, H. Wang, B. Sun, and S. Yu. 2006. Us-
ing pointwise mutual information to identify implicit
features in customer reviews. Computer Processing of
Oriental Languages. Beyond the Orient: The Research
Challenges Ahead, pages 22–30.
</reference>
<page confidence="0.997531">
907
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.613237">
<title confidence="0.999203">Implicit Feature Detection via a Constrained Topic Model and SVM</title>
<author confidence="0.994985">Wang Xu Huang</author>
<affiliation confidence="0.962058666666667">Key Laboratory of Intelligent Technology and Tsinghua National Laboratory for Information Science and Department of Computer Science and Technology, Tsinghua</affiliation>
<address confidence="0.801809">Beijing 100084,</address>
<affiliation confidence="0.909997">University of Posts and Telecommunications, Beijing 100876,</affiliation>
<email confidence="0.971373">ww880412@gmail.com,xuhua@tsinghua.edu.cn,alexalexhxqhxq@gmail.com</email>
<abstract confidence="0.997470866666667">Implicit feature detection, also known as implicit feature identification, is an essential aspect of feature-specific opinion mining but previous works have often ignored it. We think, based on the explicit sentences, several Support Vector Machine (SVM) classifiers can be established to do this task. Nevertheless, we believe it is possible to do better by using a constrained topic model instead of traditional attribute selection methods. Experiments show that this method outperforms the traditional attribute selection methods by a large margin and the detection task can be completed better.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Andrzejewski</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Latent dirichlet allocation with topic-in-set knowledge.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing,</booktitle>
<pages>43--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7428" citStr="Andrzejewski and Zhu (2009)" startWordPosition="1210" endWordPosition="1213">and fi constitute must-links. For example, the co-occurrence of ”price” and ”cheap” is very high, then the must-link between ”price” and ”cheap” can be identified. Cannot-link: It specifies that two data instances cannot be in the same cluster. If a word and a feature never co-occur in our corpus, we assume them to form a cannot-link. For example, the word lowcost has never co-occurred with the product feature screen, so they constitute a cannot-link in our corn−i,j + α (di) (1) ET n(di) + Tα j −i,j ) 904 pus. In this paper, the pre-defined process, must-link, and cannot-link are derived from Andrzejewski and Zhu (2009)’s work, all must-links and cannot-links are incorporated our constrained topic model. We multiply an indicator function 6(wi, zj), which represents a hard constraint, to the Equation 1 as the final probability for topic updating (see Equation 4). P(zi = j|z−i, w, α, β) = n(wi3 + N ∑W)( w′ n(w′) −i,j + Wβ (4) As illustrated by Equations 1 and 4, 6(wi, zj), which represents intervention or help from preexisting knowledge of must-links and cannot-links, plays a key role in this study. In the topic updating for each word in each document, we assume that the current word is wi and its linked featu</context>
</contexts>
<marker>Andrzejewski, Zhu, 2009</marker>
<rawString>David Andrzejewski and Xiaojin Zhu. 2009. Latent dirichlet allocation with topic-in-set knowledge. In Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing, pages 43–48. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>M I Jordan</author>
</authors>
<title>Modeling annotated data.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,</booktitle>
<pages>127--134</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3092" citStr="Blei and Jordan, 2003" startWordPosition="479" endWordPosition="482"> provided. Hai et al. (2011) used co-occurrence association rule mining to identify implicit features. However, they only dealt with opinion words and neglected the facts. Therefore, in this paper, both the opinions and facts will be taken into account. Blei et al. (2003) proposed the original LDA using EM estimation. Griffiths and Steyvers (2004) applied Gibbs sampling to estimate LDA’s parameters. Since the inception of these works, many variations have been proposed. For example, LDA has previously been used to construct attributes for classification; it often acts to reduce data dimension(Blei and Jordan, 2003; Fei-Fei and Perona, 2005; Quelhas et al., 2005). Here, we modify LDA and adopt it to select the training attributes for SVM. 3 Model Design 3.1 Introduction to LDA We briefly introduce LDA, following the notation of Griffiths(Griffiths and Steyvers, 2004). Given D 903 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 903–907, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics documents expressed over W unique words and T topics, LDA outputs the document-topic distribution θ and topic-word distribution cp,</context>
</contexts>
<marker>Blei, Jordan, 2003</marker>
<rawString>D.M. Blei and M.I. Jordan. 2003. Modeling annotated data. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 127–134. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="2743" citStr="Blei et al. (2003)" startWordPosition="424" endWordPosition="427">onstructed to train the selected attributes and utilized to detect the implicit features. 2 Related Work The definition of implicit feature comes from Liu et al. (2005)’s work. Su et al. (2006) used Pointwise Mutual Information (PMI) based semantic association analysis to identify implicit features, but no quantitative experimental results were provided. Hai et al. (2011) used co-occurrence association rule mining to identify implicit features. However, they only dealt with opinion words and neglected the facts. Therefore, in this paper, both the opinions and facts will be taken into account. Blei et al. (2003) proposed the original LDA using EM estimation. Griffiths and Steyvers (2004) applied Gibbs sampling to estimate LDA’s parameters. Since the inception of these works, many variations have been proposed. For example, LDA has previously been used to construct attributes for classification; it often acts to reduce data dimension(Blei and Jordan, 2003; Fei-Fei and Perona, 2005; Quelhas et al., 2005). Here, we modify LDA and adopt it to select the training attributes for SVM. 3 Model Design 3.1 Introduction to LDA We briefly introduce LDA, following the notation of Griffiths(Griffiths and Steyvers,</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaowen Ding</author>
<author>Bing Liu</author>
<author>Philip S Yu</author>
</authors>
<title>A holistic lexicon-based approach to opinion mining.</title>
<date>2008</date>
<booktitle>In Proceedings of the international conference on Web search and web data mining, WSDM ’08,</booktitle>
<pages>231--240</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Ding, Liu, Yu, 2008</marker>
<rawString>Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A holistic lexicon-based approach to opinion mining. In Proceedings of the international conference on Web search and web data mining, WSDM ’08, pages 231– 240, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Fei-Fei</author>
<author>P Perona</author>
</authors>
<title>A bayesian hierarchical model for learning natural scene categories.</title>
<date>2005</date>
<booktitle>In Computer Vision and Pattern Recognition,</booktitle>
<volume>2</volume>
<pages>524--531</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3118" citStr="Fei-Fei and Perona, 2005" startWordPosition="483" endWordPosition="486">2011) used co-occurrence association rule mining to identify implicit features. However, they only dealt with opinion words and neglected the facts. Therefore, in this paper, both the opinions and facts will be taken into account. Blei et al. (2003) proposed the original LDA using EM estimation. Griffiths and Steyvers (2004) applied Gibbs sampling to estimate LDA’s parameters. Since the inception of these works, many variations have been proposed. For example, LDA has previously been used to construct attributes for classification; it often acts to reduce data dimension(Blei and Jordan, 2003; Fei-Fei and Perona, 2005; Quelhas et al., 2005). Here, we modify LDA and adopt it to select the training attributes for SVM. 3 Model Design 3.1 Introduction to LDA We briefly introduce LDA, following the notation of Griffiths(Griffiths and Steyvers, 2004). Given D 903 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 903–907, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics documents expressed over W unique words and T topics, LDA outputs the document-topic distribution θ and topic-word distribution cp, both of which can be obta</context>
</contexts>
<marker>Fei-Fei, Perona, 2005</marker>
<rawString>L. Fei-Fei and P. Perona. 2005. A bayesian hierarchical model for learning natural scene categories. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 2, pages 524–531. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America, 101(Suppl</booktitle>
<pages>1--5228</pages>
<contexts>
<context position="2820" citStr="Griffiths and Steyvers (2004)" startWordPosition="436" endWordPosition="439"> the implicit features. 2 Related Work The definition of implicit feature comes from Liu et al. (2005)’s work. Su et al. (2006) used Pointwise Mutual Information (PMI) based semantic association analysis to identify implicit features, but no quantitative experimental results were provided. Hai et al. (2011) used co-occurrence association rule mining to identify implicit features. However, they only dealt with opinion words and neglected the facts. Therefore, in this paper, both the opinions and facts will be taken into account. Blei et al. (2003) proposed the original LDA using EM estimation. Griffiths and Steyvers (2004) applied Gibbs sampling to estimate LDA’s parameters. Since the inception of these works, many variations have been proposed. For example, LDA has previously been used to construct attributes for classification; it often acts to reduce data dimension(Blei and Jordan, 2003; Fei-Fei and Perona, 2005; Quelhas et al., 2005). Here, we modify LDA and adopt it to select the training attributes for SVM. 3 Model Design 3.1 Introduction to LDA We briefly introduce LDA, following the notation of Griffiths(Griffiths and Steyvers, 2004). Given D 903 Proceedings of the 2013 Conference on Empirical Methods i</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T.L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences of the United States of America, 101(Suppl 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Hai</author>
<author>K Chang</author>
<author>J Kim</author>
</authors>
<title>Implicit feature identification via co-occurrence association rule mining.</title>
<date>2011</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>393--404</pages>
<contexts>
<context position="2499" citStr="Hai et al. (2011)" startWordPosition="386" endWordPosition="389">ure detection based on SVM and Topic Model(TM). The Topic Model, which incorporated into constraints based on the pre-defined product feature, is established to extract the training attributes for SVM. In the end, several SVM classifiers are constructed to train the selected attributes and utilized to detect the implicit features. 2 Related Work The definition of implicit feature comes from Liu et al. (2005)’s work. Su et al. (2006) used Pointwise Mutual Information (PMI) based semantic association analysis to identify implicit features, but no quantitative experimental results were provided. Hai et al. (2011) used co-occurrence association rule mining to identify implicit features. However, they only dealt with opinion words and neglected the facts. Therefore, in this paper, both the opinions and facts will be taken into account. Blei et al. (2003) proposed the original LDA using EM estimation. Griffiths and Steyvers (2004) applied Gibbs sampling to estimate LDA’s parameters. Since the inception of these works, many variations have been proposed. For example, LDA has previously been used to construct attributes for classification; it often acts to reduce data dimension(Blei and Jordan, 2003; Fei-F</context>
</contexts>
<marker>Hai, Chang, Kim, 2011</marker>
<rawString>Z. Hai, K. Chang, and J. Kim. 2011. Implicit feature identification via co-occurrence association rule mining. Computational Linguistics and Intelligent Text Processing, pages 393–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
<author>M Hu</author>
<author>J Cheng</author>
</authors>
<title>Opinion observer: analyzing and comparing opinions on the web.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th international conference on World Wide Web,</booktitle>
<pages>342--351</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2293" citStr="Liu et al. (2005)" startWordPosition="355" endWordPosition="358"> implicit sentence is the sentence only containing implicit features. Thus, the first sentence is an explicit sentence, while the second is an implicit one. This paper proposes an approach for implicit feature detection based on SVM and Topic Model(TM). The Topic Model, which incorporated into constraints based on the pre-defined product feature, is established to extract the training attributes for SVM. In the end, several SVM classifiers are constructed to train the selected attributes and utilized to detect the implicit features. 2 Related Work The definition of implicit feature comes from Liu et al. (2005)’s work. Su et al. (2006) used Pointwise Mutual Information (PMI) based semantic association analysis to identify implicit features, but no quantitative experimental results were provided. Hai et al. (2011) used co-occurrence association rule mining to identify implicit features. However, they only dealt with opinion words and neglected the facts. Therefore, in this paper, both the opinions and facts will be taken into account. Blei et al. (2003) proposed the original LDA using EM estimation. Griffiths and Steyvers (2004) applied Gibbs sampling to estimate LDA’s parameters. Since the inception</context>
</contexts>
<marker>Liu, Hu, Cheng, 2005</marker>
<rawString>B. Liu, M. Hu, and J. Cheng. 2005. Opinion observer: analyzing and comparing opinions on the web. In Proceedings of the 14th international conference on World Wide Web, pages 342–351. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Quelhas</author>
<author>F Monay</author>
<author>J M Odobez</author>
<author>D Gatica-Perez</author>
<author>T Tuytelaars</author>
<author>L Van Gool</author>
</authors>
<title>Modeling scenes with local descriptors and latent aspects.</title>
<date>2005</date>
<booktitle>In Computer Vision,</booktitle>
<volume>1</volume>
<pages>883--890</pages>
<publisher>IEEE.</publisher>
<marker>Quelhas, Monay, Odobez, Gatica-Perez, Tuytelaars, Van Gool, 2005</marker>
<rawString>P. Quelhas, F. Monay, J.M. Odobez, D. Gatica-Perez, T. Tuytelaars, and L. Van Gool. 2005. Modeling scenes with local descriptors and latent aspects. In Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on, volume 1, pages 883–890. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Su</author>
<author>K Xiang</author>
<author>H Wang</author>
<author>B Sun</author>
<author>S Yu</author>
</authors>
<title>Using pointwise mutual information to identify implicit features in customer reviews. Computer Processing of Oriental Languages. Beyond the Orient: The Research Challenges Ahead,</title>
<date>2006</date>
<pages>22--30</pages>
<contexts>
<context position="2318" citStr="Su et al. (2006)" startWordPosition="360" endWordPosition="363">sentence only containing implicit features. Thus, the first sentence is an explicit sentence, while the second is an implicit one. This paper proposes an approach for implicit feature detection based on SVM and Topic Model(TM). The Topic Model, which incorporated into constraints based on the pre-defined product feature, is established to extract the training attributes for SVM. In the end, several SVM classifiers are constructed to train the selected attributes and utilized to detect the implicit features. 2 Related Work The definition of implicit feature comes from Liu et al. (2005)’s work. Su et al. (2006) used Pointwise Mutual Information (PMI) based semantic association analysis to identify implicit features, but no quantitative experimental results were provided. Hai et al. (2011) used co-occurrence association rule mining to identify implicit features. However, they only dealt with opinion words and neglected the facts. Therefore, in this paper, both the opinions and facts will be taken into account. Blei et al. (2003) proposed the original LDA using EM estimation. Griffiths and Steyvers (2004) applied Gibbs sampling to estimate LDA’s parameters. Since the inception of these works, many var</context>
</contexts>
<marker>Su, Xiang, Wang, Sun, Yu, 2006</marker>
<rawString>Q. Su, K. Xiang, H. Wang, B. Sun, and S. Yu. 2006. Using pointwise mutual information to identify implicit features in customer reviews. Computer Processing of Oriental Languages. Beyond the Orient: The Research Challenges Ahead, pages 22–30.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
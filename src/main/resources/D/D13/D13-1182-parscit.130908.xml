<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000043">
<title confidence="0.998492">
A Generative Joint, Additive, Sequential Model
of Topics and Speech Acts in Patient-Doctor Communication
</title>
<author confidence="0.995553">
Byron C. Wallace†, Thomas A. Trikalinos†, M. Barton Laws†,
</author>
<affiliation confidence="0.990471">
Ira B. Wilson† and Eugene Charniak‡†Dept. of Health Services, Policy &amp; Practice, Brown University, Providence, RI
‡Dept. of Computer Science, Brown University, Providence, RI
</affiliation>
<address confidence="0.486299">
{byron wallace, thomas trikalinos, michael barton laws
</address>
<email confidence="0.87477">
ira wilson, eugene charniak}@brown.edu
</email>
<sectionHeader confidence="0.994934" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.922968666666667">
We develop a novel generative model of con-
versation that jointly captures both the top-
ical content and the speech act type asso-
ciated with each utterance. Our model ex-
presses both token emission and state tran-
sition probabilities as log-linear functions of
separate components corresponding to topics
and speech acts (and their interactions). We
apply this model to a dataset comprising anno-
tated patient-physician visits and show that the
proposed joint approach outperforms a base-
line univariate model.
Role Utterance Topic Speech act
D Let me just write down some
of these issues here so I get
them straight in my mind.
P Doctor you ain’t got to tell me
nuttin’.
P I’m in very good hands when
I’m around you.
P If push comes to a shove, you
open the window and throw
me out.
D I wanted to ask you, too - Biomedical Conv. Mgmt.
</bodyText>
<construct confidence="0.9098316">
D you know you had that Biomedical Ask Q.
colonic polyp -
D - is it two years from now that Biomedical Ask Q.
they’re going to be doing the
repeat?
</construct>
<figure confidence="0.721777857142857">
P Yeah. Biomedical Conv. Mgmt.
Logistics Commissive
Socializing Directive
Socializing Give Info.
Socializing Humor/Levity
D We’ll do the repeat coloscopy Biomedical Give Info.
in about two years.
</figure>
<sectionHeader confidence="0.986825" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999665947368421">
Communication involves at least two aspects: the
words one says and the acts one performs in saying
them. Examples of the latter include asking ques-
tions, issuing commands, and so on. These are re-
ferred to as speech acts under the sociolinguistic the-
ory of Austin (1955), which was further developed
by Searle (1969; 1985). Recognizing speech acts is
crucial to understanding communication because a
speaker’s meaning is only partially captured by the
words they use; much of their intent is expressed im-
plicitly via speech acts (Searle, 1969).
On this view, conversational utterances can be as-
signed both a topic and a speech act. The former
describes the subject matter of what was said and
the latter captures the “social act” (e.g., promising)
performed by saying it. For example, the utterance
“Obama won the election” is topically political and
is an example of an information giving speech act.
“Did Obama win the election?”, meanwhile, belongs
</bodyText>
<tableCaption confidence="0.947677">
Table 1: An excerpt from a patient-doctor interaction,
annotated with topic and speech act codes. The D and
P roles denote doctor and patient, respectively. Conv.
Mgmt. abbreviates conversation management; Ask Q. ab-
breviates ask question.
</tableCaption>
<bodyText confidence="0.998318">
to the same topic but is a question. Both aspects are
necessary to understand conversation.
Previous computational work on speech acts –
which we review in Section 6 – has modeled them
in isolation (Perrault and Allen, 1980; Stolcke et al.,
1998; Stolcke et al., 2000; Kim et al., 2010), i.e.,
independent of topical content. But a richer model
would account for both speech acts and the contex-
tualizing topic of each utterance. To this end, we de-
velop a novel joint, generative model of topics and
speech acts.
We focus on physician-patient communication as
a motivating domain. This is of interest because
</bodyText>
<page confidence="0.921751">
1765
</page>
<note confidence="0.7292535">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1765–1775,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999952738095238">
it is widely appreciated that effective communica-
tion is an integral part of clinical practice (Irwin and
Richardson, 2006; Makoul, 2001; Teutsch, 2003).
We provide an excerpt of a conversation between a
patient and their doctor annotated with topics and
speech acts in Table 1. Such annotations can provide
substantive insights into how doctors communicate
with patients (Ong et al., 1995).
A concrete example of this is the use of topic
and speech act codes to assess the efficacy of an
intervention meant to influence physician-patient
communication regarding adherence to antiretrovi-
ral (ARV) medication (Wilson et al., 2010). To
measure the effect of the intervention, investigators
performed a randomized control trial in which they
quantified change in communication patterns by tal-
lying the number of information giving speech acts
that fell under the ARV adherence topic. Without
assigning both topics and speech acts to utterances,
this analysis would not have been possible.
In this work, we develop a novel component-
based generative model for bivariate, sequentially
structured problems. Our approach extends the re-
cently proposed Sparse Additive Generative (SAGE)
model (Eisenstein et al., 2011) and similar recently
developed additive models (Paul and Dredze, 2012;
Paul et al., 2013) to the case of supervised sequen-
tial tasks to capture the joint conditional influence
of topics and speech acts, both with respect to token
generation and state transitions. For brevity, we refer
to this generative Joint, Additive, Sequential model
as JAS. In contrast to previous work on speech acts,
JAS provides a single, coherent generative model of
conversations. And because it is component-based,
this model provides a flexible framework for analyz-
ing communication patterns. We demonstrate that
JAS outperforms a generative univariate baseline in
topic/speech act prediction. Further, we automati-
cally reproduce an analysis of the aforementioned
randomized control trial, and in doing so show that
JAS reproduces the results more faithfully than a
univariate approach.
</bodyText>
<sectionHeader confidence="0.981888" genericHeader="method">
2 The Markov-Multinomial Model
</sectionHeader>
<bodyText confidence="0.999269">
We begin by considering a baseline generative ap-
proach to modeling topics and speech acts indepen-
dently. This simple approach was used by Stolcke et
al. (2000) to model speech acts. It accounts for only
a single output at each time point yt E Y, and hence
here we model topics and speech acts independently.
A straight-forward (albeit naive) alternative
would be to treat the Cartesian product of topics
and speech acts as a single output space on which
emissions and transitions are conditioned, but this
space is too large and sparse for this approach to be
practicable. We note that the fully coupled HMM
(Brand et al., 1997) suffers from a similar exponen-
tial output state problem. The related factorial HMM
(FHMM) (Ghahramani and Jordan, 1997; Van Gael
et al., 2008), meanwhile, imposes unwarranted (in
our case) independence assumptions with respect to
state transitions along parallel chains, does not obvi-
ously lend itself to discrete observations (typically
Gaussians are assumed), and does not scale well
enough (in terms of training time) to be feasible for
our application.
The Markov-Multinomial (MM) comprises two
components; transitions and emissions. The former
is modeled by making a first-order Markov assump-
tion, specifically:
</bodyText>
<equation confidence="0.999157">
P(yt|y0, ..., yt−1) = P(yt|yt−1) = Ayt−,,yt (1)
</equation>
<bodyText confidence="0.99699425">
Emissions can be modeled via a multinomial
that captures the conditional probabilities of to-
kens given labels. Denoting an utterance (an utter-
ance comprises the words corresponding to a single
speech act; see Section 4) at time t by ut and its la-
bel by yt, and making the standard naive assumption
that words are generated independently conditioned
on a label, we have:
</bodyText>
<equation confidence="0.9696805">
P(ut|yt) = 11 P(w|yt) = 11 Tyt,w (2)
wEut wEut
</equation>
<bodyText confidence="0.999878125">
Both sets of parameters (the A’s and the -r’s) can be
estimated straight-forwardly using maximum like-
lihood (i.e., using observed counts). We can use
Viterbi decoding (Rabiner and Juang, 1986) to make
predictions for new sequences, as usual. To make
both topic and speech act predictions, we simply in-
duce models for each and make predictions indepen-
dently.
</bodyText>
<sectionHeader confidence="0.952275" genericHeader="method">
3 JAS: A Joint, Additive, Sequential Model
</sectionHeader>
<bodyText confidence="0.9553385">
An obvious shortcoming of the simple MM model
outlined above is that it treats topics and speech acts
</bodyText>
<page confidence="0.985105">
1766
</page>
<bodyText confidence="0.998867372093024">
as statistically independent. They are not (as con-
firmed at statistical significance p &lt; .001 using a
χ2 test). One would prefer a more expressive model
that conditions topic and speech act transitions as
well as the production of utterances jointly on both
the current topic and the current speech act.
More specifically, we would like a model that re-
flects the assumption that some latent intent gives
rise to both the topic and the speech act associated
with an utterance. This is consistent with Searle’s
(1969) notion of perlocutionary effects; one per-
forms speech acts with the aim of getting someone
to do something. Intent gives rise to the current
topic and speech act, and the current intent affects
the next; this induces a correlation between adjacent
topics and speech acts. This conceptual model is de-
picted graphically in the left-half of Figure 1.
The latent intent may be, e.g., to encourage a pa-
tient to take their medication more regularly. In our
application the topical content may be ARV adher-
ence and the type of speech act would be selected
by the provider (presumably to maximize the likeli-
hood of patient adherence). For example, she may
opt to urge imperatively (“You really need to take
your medicine”) or to implore with a question (“Will
you please remember to take your medicine?”). Be-
cause we have no way of explicitly modeling intent
(it is never observed), we instead rely on variables
for which we have annotations (i.e., the topics and
speech acts; see Figure 1). We next describe the
model in more detail.
We refer to the topic set by Y, the speech act
set by S and the vocabulary as W. We denote the
(log of the) background probability of word w by
θw, and we will denote components corresponding
to deviations from θw due to a specific topic (speech
act) by ηyw (ηsw). Further, we include the component
ηy,s
w to capture interaction effects between topics and
speech acts. We assume that the conditional proba-
bility of word w belonging to an utterance ut with
corresponding topic yt and speech act st is log-linear
with respect to these components, i.e.:
</bodyText>
<equation confidence="0.954378125">
1
P(w|yt, st) = exp{θw+ηyt w +ηst w +ηst,yt
w } (3)
Zw
Where Zw is a normalizing term (implicitly condi-
tioned on yt and st) defined as:
�Zw = exp{θw0 + ηytw0 + ηstw0 + ηst,yt
w0∈W w0 } (4)
</equation>
<bodyText confidence="0.999937">
We make the standard naive assumption that words
are generated independently, given the topic and
speech act of the utterance to which they belong:
</bodyText>
<equation confidence="0.9659795">
11 P(ut|yt, st) = P(w|yt, st) (5)
w∈ut
</equation>
<bodyText confidence="0.999990952380952">
The per-token emission probability just described
falls under the additive generative family of models
recently proposed by Eisenstein et al. (2011). How-
ever, in addition to conditional token emission prob-
abilities, here we need also to model the transition
probabilities such that the likelihood of transition-
ing to topic yt (and to speech act st) reflects both the
previous topic and the previous speech act, captur-
ing the dependencies illustrated in Figure 1. To this
end, we model topic and speech act transition proba-
bilities as log-linear functions of the preceding topic
and speech act.
We denote log of the background topic frequen-
cies by TrY, and components capturing the influence
of transitioning to topic yt due to the preceding topic
and speech act by σyt−1,yt and σst−1,yt respectively.
We also include a component σ(yt−1,st−1),yt that cor-
responds to the interaction effect on topic transi-
tion probability due to the preceding topic/speech
act pair. We then model the topic transition prob-
ability (given the preceding states) as:
</bodyText>
<equation confidence="0.946040285714286">
P(yt|yt−1,st−1) =
1 exp{πY yt +σyt−1,yt +σst−1,yt +σ(yt−1,st−1),yt}
Zy
Where Zy is a normalizing term for the topic transi-
tions (implicitly conditioned on st−1, yt−1):
�Zy = exp{πYy0+σyt−1,y0+σst−1,y0+σ(yt−1,st−1),y0}
y0∈Y
</equation>
<bodyText confidence="0.9999592">
Similarly, denoting by 7rS log-transformed speech
act background frequencies, and including analo-
gous components as above that correspond to the in-
fluence of the preceding topic, speech act and their
interaction on transitioning into speech act st, we
</bodyText>
<page confidence="0.958072">
1767
</page>
<figure confidence="0.999257444444445">
Topict-1 Speech Actt-1
Utterancet-1
Intentt-1 Intentt
Topict Speech Actt
Utterancet
Topict-1 Speech Actt-1
Utterancet-1
Topict Speech Actt
Utterancet
</figure>
<figureCaption confidence="0.9455025">
Figure 1: The generative story of utterances, depicted graphically. On the left we show our motivating conceptualiza-
tion: a latent intent gives rise to both the topic and speech acts; these, in turn, jointly induce a distribution over words
and transitions. On the right we show our operationalization of this concept. For clarity, we have denoted arrows
capturing influence due to topics with dotted lines.
</figureCaption>
<equation confidence="0.939831545454546">
have:
P(st|st−1, yt−1) _
1 S
exp{7rS +est-
Z t 1,St +Olyt−1,St +0,(yt-1,st-1),st}
s (8)
Where Zs is a normalizing constant for speech acts
analogous to Equation 7. Putting things together:
P(yt, st|st−1, yt−1, ut) _
P(ut|yt, st) · P(yt|yt−1, st−1) · P(st|st−1, yt−1)
(9)
</equation>
<bodyText confidence="0.999927884615385">
As implied by Figure 1, this model assumes that the
topic and speech act at time t are conditionally in-
dependent given the preceding topic and speech act
(yt−1 and st−1). This is intuitively agreeable be-
cause time intervenes as a blocking factor; condi-
tioning the current topic on the current speech act
(or vice versa) would contradict the fact that these
occur simultaneously. Instead, the correlation is in-
duced by the preceding topic/speech act pair. (That
said, this is still a simplifying assumption, as one
may instead choose to model speech act selection as
conditional on topic (Traum and Larsson, 2003).)
Predictions can again be made via Viterbi de-
coding (Rabiner and Juang, 1986) over a matrix of
pairs of joint topic/speech act states. The strategy of
modeling (additive) components allows JAS to avoid
problems due to sparsity in this large output space.
Model parameters can be estimated using stan-
dard optimization techniques. We fix the ‘back-
ground’ frequencies 0, 7rY, 7rS to the log of the
corresponding observed proportions of words, top-
ics and speech acts, respectively. For the remaining
parameters, one can use descent-based optimization
methods. The partial derivative for the topic-to-topic
transition component Qy,y0 with respect to the likeli-
hood, for example, is:
</bodyText>
<equation confidence="0.949108">
C(y,s),y0 − P(y&apos;|y, s)C(y,s),* (10)
</equation>
<bodyText confidence="0.999980307692308">
Where C(y,s),y0 denotes the observed count of tran-
sitions from topic/speech act pair (y, s) to y&apos;, and
C(y,s),* denotes the total number of observed transi-
tions out of this pair. The term P(y&apos;|y, s) is with
respect to the current parameter estimates and is
defined in Equation 6. The partial derivatives for
the other component parameters (both transition and
emission) are analogous. We use a Newton opti-
mization method similar to the approach outlined by
Eisenstein et al. (2011).1 We assess convergence
by calculating predictive performance on a held-out
portion (5%) of the training dataset at each step,
halting the descent when this declines.
</bodyText>
<sectionHeader confidence="0.997717" genericHeader="method">
4 Dataset
</sectionHeader>
<bodyText confidence="0.9987498">
We use a corpus of patient-provider visits annotated
with Generalized Medical Interaction Anaylsis Sys-
tem (GMIAS) codes. The GMIAS has been used
to: characterize interaction processes in physician-
patient communication about ARV adherence in the
</bodyText>
<footnote confidence="0.606691">
1With the exception that we do not explicitly model the dis-
tribution over component variances.
</footnote>
<figure confidence="0.988501333333333">
a _1:
sES
auy,y0
</figure>
<page confidence="0.9171">
1768
</page>
<table confidence="0.999198704918033">
Topic; Speech act Count (prevalence)
ARV Adherence; Ask Q 2939 (0.013)
ARV Adherence; Commissive 245 (0.001)
ARV Adherence; Continuation 328 (0.001)
ARV Adherence; Conv. Management 4298 (0.018)
ARV Adherence; Directive 1650 (0.007)
ARV Adherence; Empathy 111 (0.000)
ARV Adherence; Give Information 12796 (0.055)
ARV Adherence; Humor/Levity 46 (0.000)
ARV Adherence; Missing/other 977 (0.004)
ARV Adherence; Social-Ritual 15 (0.000)
Biomedical; Ask Q 13753 (0.059)
Biomedical; Commissive 1049 (0.005)
Biomedical; Continuation 1005 (0.004)
Biomedical; Conv. Management 17611 (0.076)
Biomedical; Directive 4617 (0.020)
Biomedical; Empathy 423 (0.002)
Biomedical; Give Information 54231 (0.233)
Biomedical; Humor/Levity 255 (0.001)
Biomedical; Missing/other 4426 (0.019)
Biomedical; Social-Ritual 119 (0.001)
Logistics; Ask Q 5517 (0.024)
Logistics; Commissive 2308 (0.010)
Logistics; Continuation 435 (0.002)
Logistics; Conv. Management 9672 (0.042)
Logistics; Directive 5148 (0.022)
Logistics; Empathy 100 (0.000)
Logistics; Give Information 23351 (0.101)
Logistics; Humor/Levity 135 (0.001)
Logistics; Missing/other 2732 (0.012)
Logistics; Social-Ritual 285 (0.001)
Missing/other; Ask Q 820 (0.004)
Missing/other; Commissive 70 (0.000)
Missing/other; Continuation 1173 (0.005)
Missing/other; Conv. Management 1605 (0.007)
Missing/other; Directive 523 (0.002)
Missing/other; Empathy 48 (0.000)
Missing/other; Give Information 3994 (0.017)
Missing/other; Humor/Levity 27 (0.000)
Missing/other; Missing/other 12103 (0.052)
Missing/other; Social-Ritual 69 (0.000)
Psycho-Social; Ask Q 2933 (0.013)
Psycho-Social; Commissive 164 (0.001)
Psycho-Social; Continuation 208 (0.001)
Psycho-Social; Conv. Management 4433 (0.019)
Psycho-Social; Directive 787 (0.003)
Psycho-Social; Empathy 262 (0.001)
Psycho-Social; Give Information 15521 (0.067)
Psycho-Social; Humor/Levity 63 (0.000)
Psycho-Social; Missing/other 1199 (0.005)
Psycho-Social; Social-Ritual 36 (0.000)
Socializing; Ask Q 1283 (0.006)
Socializing; Commissive 79 (0.000)
Socializing; Continuation 85 (0.000)
Socializing; Conv. Management 2166 (0.009)
Socializing; Directive 222 (0.001)
Socializing; Empathy 73 (0.000)
Socializing; Give Information 8981 (0.039)
Socializing; Humor/Levity 306 (0.001)
Socializing; Missing/other 849 (0.004)
Socializing; Social-Ritual 1685 (0.007)
</table>
<tableCaption confidence="0.997941">
Table 2: Topic/speech act pairs and their counts.
</tableCaption>
<bodyText confidence="0.9999205">
context of an intervention trial (Wilson et al., 2010);
analyze communication about sexual risk behavior
(Laws et al., 2011a); elucidate the association of
visit length with constructs of patient-centeredness
(Laws et al., 2011b); and to describe provider-
patient communication regarding ARV adherence
compared with communication about other issues
(Laws et al., 2012). GMIAS annotation is described
at length elsewhere,2 but we summarize it here for
completeness.
GMIAS segments conversation into utterances.
An utterance is here defined as a single completed
speech act. Previous coding systems have simply
defined an utterance as conveying a single thought
(Roter and Larson, 2002) or any independent or un-
restrictive dependent clause of a sentence (Ford and
Ford, 1995). Stolcke et al. (2000) followed Meteer
et al. (1995) in using “sentence-level units”. These
definitions provide helpful guidance to coders, but
many speech acts are poorly formed grammatically,
and cannot be described as a “clause”. Further, some
speech acts cannot be said to convey a “thought” (or
sentence) at all, but rather are pre-syntactical (e.g.,
interjections and non-lexical utterances like laugh-
ter). In any case, most natural segmentations of con-
versations probably largely agree with intuition, and
are not likely to differ substantially.
The model we develop in this work assumes that
transcripts have been manually segmented. While
this comes at some cost, segmenting is still much
cheaper than annotating transcripts. Manually an-
notating a single visit with GMIAS codes takes 2-
4 hours and must be performed by someone with
substantive domain expertise. By contrast, segment-
ing transcripts into utterances takes at most 1/4th of
the time as annotation and can be done by a less
highly-skilled individual. That said, in future work
we hope to explore incorporating automatic segmen-
tation methods (Galley et al., 2003; Eisenstein and
Barzilay, 2008) into our approach.
Each utterance is assigned a single topic code and
a single speech act code. Inter-rater agreement has
been observed to be relatively high for this task:
Kappa between three trained annotators and a ref-
erence annotation ranged from 0.89 to 1.0 for top-
ics and 0.81 to 0.95 for speech acts. We next de-
</bodyText>
<footnote confidence="0.945516">
2https://sites.google.com/a/brown.edu/m-barton-laws/home/gmias
</footnote>
<page confidence="0.993384">
1769
</page>
<bodyText confidence="0.999144111111111">
scribe the topics and speech acts we consider in
more detail; Table 2 enumerates all pairs of these
and their respective counts in the corpus. We note
that GMIAS defines a hierarchy of both topic and
speech act codes, but here we only attempt to cap-
ture the highest level codes in these hierarchies.
Topics comprise six major categories: ARV
adherence, biomedical, logistics, missing/other,
psycho-social and socializing. Antiretroviral (ARV)
adherence applies to utterances that address ARV
medication usage. Biomedical utterances subsume
clinical observations and diagnostic conclusions.
Utterances that concern the business of conducting a
physical examination fall under logistics. The miss-
ing/other topic covers a few cases, including utter-
ances that are effectively outside of the GMIAS uni-
verse and inaudible utterances; however we note that
missing/other is a topic explicitly assigned by hu-
man annotators. The psycho-social topic includes
such issues as substance abuse, recovery, employ-
ment and relationships. Finally, socializing refers to
casual conversation unrelated to the business of the
medical visit, and to social rituals such as greetings.
There are 10 speech acts:3 ask question, commis-
sive, continuation, conversation management, direc-
tive, empathy, give information, humor/levity, miss-
ing/other, and social-ritual. Ask question is self-
explanatory. Utterances in which the speaker makes
a promise or resolves to take action are commissives.
A continuation refers to the completion of a previ-
ously interrupted speech act (these are rare). Con-
versation management describes utterances that fa-
cilitate turn-taking or guide discussion (‘talk about
talk’). Directives refer to statements that look to
control or influence the behavior of the interlocutor.
Utterances that express responses to emotions, con-
cerns or feelings are coded under empathy. Com-
munication of (purported) facts falls under give in-
formation. Humor/levity captures jokes and jovial
conversation. Missing/other is the same as for top-
ics. Finally, social-ritual utterances represent for-
malities (e.g., “thank you”).
The corpus we use includes 360 GMIAS anno-
tated patient-provider interactions (median length:
605 utterances). This data originated as part of
</bodyText>
<footnote confidence="0.7517995">
3These are high-level speech acts; technically each consti-
tutes a category of speech act types.
</footnote>
<bodyText confidence="0.999824625">
a study designed to assess the role of the patient-
provider relationship in explaining racial/ethnic dis-
parities in HIV care. Study subjects were HIV care
providers and their patients at four US care sites.
The group responsible for the data are awaiting a
decision from the institutional review board (IRB)
regarding whether we can make this data publicly
available in some form.
</bodyText>
<sectionHeader confidence="0.830153" genericHeader="method">
5 Experimental Results
</sectionHeader>
<figure confidence="0.993961666666667">
0.26
0.25
0.24
average F-score 0.23
0.22
0.21
0.20
0.19
Markov-Multinomial Joint Additive Sequential
</figure>
<figureCaption confidence="0.737736">
Figure 2: Mean F-scores across all topic/speech act pairs
for the Markov-Multinomial (MM; left) and the proposed
Joint Additive Sequential (JAS; right) models. The thick
</figureCaption>
<bodyText confidence="0.981478111111111">
black line shows the mean difference over ten different
folds; the thin grey lines describe per-fold differences.
The proposed JAS model outperforms the baseline MM
model for all folds
Our evaluation includes two parts.4 First, we
perform standard cross-validation over the afore-
mentioned 360 annotated interactions, evaluating F-
measure for each topic/speech act pair. Second,
we look to automatically reproduce an analysis of
</bodyText>
<footnote confidence="0.997161">
4Source code at: https://github.com/bwallace/JAS; unfortu-
nately we do not yet have permission to post the data.
</footnote>
<page confidence="0.993536">
1770
</page>
<bodyText confidence="0.999811708333333">
a randomized control trial that assessed the efficacy
of an intervention meant to alter physician-patient
communication. We show that JAS outperforms the
baseline approach with respect to both tasks.
We emphasize that while we are here compar-
ing predictive performance, we are specifically in-
terested in fully generative models of conversations
due to the longer-term applications we have in mind.
We would like, e.g., to use this model to assess the
variation in communicative approaches across dif-
ferent doctors, and generative models are more nat-
urally amenable to answering such exploratory ques-
tions. Indeed, perhaps the main strength of the ad-
ditive component based sequential model we have
proposed here is that it will allow us to easily in-
corporate physician-specific parameters that capture
deviations in provider speech act and/or topic tran-
sition patterns. Further, we may soon have access
to many unannotated transcripts, and we would like
to learn from these; generative approaches allow
straight-forward exploitation of unlabeled data. For
these reasons, we did not experiment with discrim-
inative models, e.g., Dynamic Conditional Random
Fields (DCRFs) (Sutton et al., 2007) for this work.
</bodyText>
<subsectionHeader confidence="0.948852">
5.1 Cross-fold Validation
</subsectionHeader>
<bodyText confidence="0.99995006122449">
Our aim is to measure model performance in terms
of correctly identifying both the topic and speech act
corresponding to each utterance. We quantify this
via the F-score calculated for each topic/speech act
pair that is observed at least once. One can see in
Table 2 that many such pairs have low prevalence;
this can result in undefined F-scores (e.g., when no
utterances are assigned to a given pair). In this case,
it is reasonable to treat these as zero values, as is
commonly done (Forman and Scholz, 2010). This
penalizes models when they completely fail to iden-
tify an entire class of utterances.
We first report macro-averages, that is, averages
of the individual topic/speech act pair F-scores.
Figure 2 displays the macro-averaged F-score for
each of the 10 folds (grey lines connect folds)
and the average of these (thick black line). The
JAS model achieves an average macro-averaged F-
score of .234 versus the .207 achieved by base-
line Markov-Multinomial (MM) model; JAS outper-
forms MM on every fold.
For a more granular picture, Figure 3 displays av-
erage F-score differences with respect to every indi-
vidual topic and speech act pair for which this differ-
ence was non-zero. This is the (signed) difference of
the F-score achieved using JAS minus that achieved
using the MM model; black lines thus correspond
to pairs for which JAS outperformed MM, and red
lines to pairs for which MM outperformed JAS. The
latter achieves an improvement of &gt;_ .05 for 10
pairs, and results in an F-score of &gt; .02 below that
attained by MM only once.
The relatively low F-scores for the metrics quanti-
fying performance with respect to the cross of topic
and speech act codes belie relatively good over-
all (marginal) predictive performance. That is, we
achieve much better performance with respect to
metrics that measure topic and speech act predic-
tions independently of one another. This is due to the
very large output space under consideration (see Ta-
ble 2). Specifically, averaged over ten runs, the MM
model achieves a marginal mean topic F-score of
.667 and marginal mean speech act F-score of .516.
JAS begets a marginal mean topic F-score of .661
and a marginal mean speech act F-score of .544;
hence the JAS model incurs an F-score loss of .006
(a 0.9% decrease) with respect to marginal topic
code prediction, but improves the marginal speech
act F-score by .028 (a 5.4% increase).
</bodyText>
<subsectionHeader confidence="0.999688">
5.2 (Re-)Analysis of Randomized Control Trial
</subsectionHeader>
<bodyText confidence="0.999991947368421">
We also evaluated performance by tallying model
predictions over 116 held-out cases collected from
a randomized, cross-over study of an intervention
aimed at improving physicians knowledge of pa-
tients anti-retroviral (ARV) adherence (Wilson et al.,
2010). The intervention was a report given to the
physician before a routine office visit that contained
information regarding the patients ARV usage and
their beliefs about ARV therapy. To explore the ef-
ficacy of this intervention, 58 paired (116 total) au-
dio recorded visits were annotated with GMIAS; 58
correspond to visits before which the provider was
not provided with the report (control cases), while
the other 58 correspond to visits before which they
were (intervention cases).
Wilson et al. (2010) demonstrated that the in-
tervention indeed increased adherence-related dia-
logue, and specifically the number of information
giving speech acts performed by the physician un-
</bodyText>
<page confidence="0.994716">
1771
</page>
<figureCaption confidence="0.974705666666667">
Figure 3: Average difference in F-scores corresponding to specific topic/speech act pairs, sorted by magnitude. Black
lines (extending rightward) represent pairs for which JAS outperforms the baseline model; red lines (leftward) are
pairs for which baseline performs better.
</figureCaption>
<figure confidence="0.9971976">
JAS
True
control
MM
intervention
10 (4, 28)
23 (14, 40)
control
13 (5, 33)
intervention
23 (11, 39)
intervention
27 (16, 44)
control
12 (5, 28)
</figure>
<tableCaption confidence="0.99046">
Table 3: Utterance counts {Median (25th, 75th per-
</tableCaption>
<bodyText confidence="0.985266833333333">
centile)} for the ARV/information giving topic and speech
act pair. We show the ‘gold standard’ (True) tallies,
which were assigned by humans, and the counts taken
using the two models, MM and JAS. The JAS model pre-
dictions are closer to the true numbers.
derneath this topic. We attempted to reproduce this
finding using automated rather manual annotations.
To this end, we trained MM and JAS models over
the aforementioned 360 annotated visits and then
used this model to generate topic and speech act
code predictions for the utterances comprising the
116 held-out visits used for the analysis (these were
not part of the training set). We then assessed the
direction and magnitude of the change in the num-
ber of ARV adherence/information giving utterances
in the paired control versus intervention cases. We
compared the results for this analysis calculated us-
ing the true (manually assigned) codes to the results
calculated using the predicted codes.
Following the original analysis (Wilson et
al., 2010), we report the median number of
ARV/information giving utterances and correspond-
ing 25th and 75th percentiles over the 58 control and
intervention visits, as counted using the true (hu-
man) annotations and using the codes predicted by
the MM and JAS models. These are reported in Ta-
ble 3. The JAS model predictions better match the
true labels in all except one case (the lower 25th for
the controls, for which it predicts the same number
as the MM model).
</bodyText>
<sectionHeader confidence="0.9999" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.999986692307692">
There is a relatively long history of research into
modeling conversational speech acts in computa-
tional linguistics. Perrault and Allen (1980) con-
ducted pioneering work on computationally formal-
izing speech acts, though their work pre-dates statis-
tical NLP and is therefore not directly relevant to the
present work.
Stolcke et al. (2000; 1998) proposed a probabilis-
tic approach to modeling conversational speech acts
based on the Hidden Markov Model (HMM) (Ra-
biner and Juang, 1986). They were interested in
modeling an unrestricted set of conversations, and
did not impose a hierarchy on the speech acts; they
</bodyText>
<page confidence="0.984481">
1772
</page>
<bodyText confidence="0.999857155555556">
therefore enumerated many more speech acts (42)
than we do in the present work (recall that we use 10
‘high-level’ speech acts).5 Their model has served
as the baseline approach in the present work. Stol-
cke et al. also considered jointly performing speech
recognition and speech act classification.
Others have investigated visual structures of
patient-provider interactions to qualitatively assess
communication in care. Specifically, (Cretchley et
al., 2010) leveraged concept maps to explore conver-
sations between people with schizophrenia and their
carers. Briefly, this approach allowed them to (qual-
itatively) identify two distinct conversational strate-
gies used by care-takers and their patients. Angus et
al. (Angus et al., 2012) presented a similar approach
in which they used text visualization software to ex-
plore patterns of (inferred) topics in consultations.
Another thread of research has investigated classi-
fying speech acts in emails into one of a small set of
“email speech acts”, e.g., request, propose, commit
(Cohen et al., 2004; Goldstein et al., 2006). Cohen et
al. (2004) demonstrated that good performance can
be achieved for this task via existing text classifica-
tion technologies. Elsewhere, researchers have ex-
plored automatically inferring “speech acts” in vari-
ous other online social mediums, including message
board posts (Qadir and Riloff, 2011), Wikipedia talk
pages (Ferschke et al., 2012) and Twitter (Zhang et
al., 2012).
A separate line of inquiry concerns classifying di-
alogue acts in chat. Researchers have attempted di-
alogue act classification both for 1-on-1 (Kim et al.,
2010) and multi-party (Kim et al., 2012; Clark and
Popescu-Belis, 2004) online chats. Ang et al. (2005)
considered the task of jointly segmenting and clas-
sifying utterances comprising multiparty meetings,
while Hsueh and Moore (2006) proposed analogous
methods for topic segmentation and labeling (other
works on topic segmentation include (Galley et al.,
2003) and (Eisenstein and Barzilay, 2008)). Incor-
porating such segmentation methods into the pro-
posed model (rather than relying on inputs to be
manually segmented beforehand) would be a natu-
ral extension of this work.
Additive component models of text have recently
</bodyText>
<footnote confidence="0.5160355">
5We note that only 8 of the 42 speech acts appeared with
greater than 1% frequency in Stolcke et al.’s corpus.
</footnote>
<bodyText confidence="0.99683525">
gained traction (Eisenstein et al., 2011; Paul, 2012;
Paul and Dredze, 2012; Paul et al., 2013). To our
knowledge, this is the first extension of supervised
additive component models to a sequential task.6
</bodyText>
<sectionHeader confidence="0.987705" genericHeader="conclusions">
7 Conclusions and Future Directions
</sectionHeader>
<bodyText confidence="0.999942228571428">
We have proposed a novel Joint, Additive, Sequen-
tial (JAS) model of conversational topics and speech
acts. In contrast to previous approaches to mod-
eling conversational exchanges, this model factors
both the current topic and the current speech act into
token emission and state transition probabilities. We
demonstrated that this model consistently outper-
forms a univariate generative baseline that treats
speech acts and topics independently. Furthermore,
we showed JAS can automatically re-produce the
analysis of a randomized control trial designed to as-
sess the efficacy of an intervention to alter physician
communication habits with high-fidelity.
The generative component-based framework we
have introduced in this work provides a means of
exploring factors in patient-physician communica-
tion. One limitation of the model we have pre-
sented is that it makes several simplifying assump-
tions around dialogue. For example, we have ig-
nored non-linearities and ‘back-channels’ in con-
versation, and we have ignored differences across
physicians with respect to communication styles.
Going forward, we hope to address these limita-
tions. We also plan on extending this model to in-
vestigate qualitative questions surrounding patient-
physician communication quantitatively. For exam-
ple, we are interested in investigating how communi-
cation varies across hospitals and physicians. To ex-
plore this, we can add additional components to the
transition probability terms corresponding to differ-
ent hospitals and doctors. Ultimately, we would like
to correlate patterns in physician communication (as
gleaned from the model) with objective, measured
health outcomes (e.g., patient satisfaction and adher-
ence to ARVs).
</bodyText>
<footnote confidence="0.965174">
6Though Paul (2012) recently proposed ‘mixed-
membership’ Markov models for unsupervised conversation
modeling.
</footnote>
<page confidence="0.986595">
1773
</page>
<sectionHeader confidence="0.996466" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999937888888889">
The authors thank members of the Brown Labora-
tory for Linguistic Information Processing (BLLIP)
and Kevin Small for providing helpful feedback on
earlier versions of this work. We also thank the three
anonymous EMNLP reviewers for insightful com-
ments. This work was partially supported by the Na-
tional Institute of Mental Health (2 K24MH092242,
R34MH089279 and R01MH083595) and by NIDA
(R01DA015679).
</bodyText>
<sectionHeader confidence="0.997806" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999747355555555">
Jeremy Ang, Yang Liu, and Elizabeth Shriberg. 2005.
Automatic dialog act segmentation and classification
in multiparty meetings. In Proc. ICASSP, volume 1,
pages 1061–1064.
Daniel Angus, Bernadette Watson, Andrew Smith, Cindy
Gallois, and Janet Wiles. 2012. Visualising con-
versation structure across time: Insights into effective
doctor-patient consultations. PloS one, 7(6).
John Langshaw Austin. 1955. How to do things with
words, volume 88. Harvard University Press.
Matthew Brand, Nuria Oliver, and Alex Pentland. 1997.
Coupled hidden Markov models for complex action
recognition. In Computer Vision and Pattern Recog-
nition, 1997. Proceedings., 1997 IEEE Computer So-
ciety Conference on, pages 994–999. IEEE.
Alexander Clark and Andrei Popescu-Belis. 2004.
Multi-level dialogue act tags. In Proc. SIGdial, pages
163–170.
William W Cohen, Vitor R Carvalho, and Tom M
Mitchell. 2004. Learning to classify email into speech
acts. In Proceedings of EMNLP, volume 4. sn.
Julia Cretchley, Cindy Gallois, Helen Chenery, and An-
drew Smith. 2010. Conversations between carers
and people with schizophrenia: a qualitative analy-
sis using leximancer. Qualitative Health Research,
20(12):1611–1628.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 334–343. Association for
Computational Linguistics.
J. Eisenstein, A. Ahmed, and E.P. Xing. 2011. Sparse
additive generative models of text. In Proceedings of
ICML, pages 1041–1048.
Oliver Ferschke, Iryna Gurevych, and Yevgen Chebotar.
2012. Behind the article: Recognizing dialog acts in
wikipedia talk pages. In Proceedings of the 13th Con-
ference of the European Chapter of the Association for
Computational Linguistics (EACL 2012), pages 777–
786. Citeseer.
Jeffrey D Ford and Laurie W Ford. 1995. The role of
conversations in producing intentional change in or-
ganizations. Academy of Management Review, pages
541–570.
George Forman and Martin Scholz. 2010. Apples-to-
apples in cross-validation studies: pitfalls in classifier
performance measurement. volume 12, pages 49–57.
ACM.
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation of
multi-party conversation. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics, pages 562–569. Association for Compu-
tational Linguistics.
Zoubin Ghahramani and Michael I Jordan. 1997. Facto-
rial hidden Markov models. Machine learning, 29(2-
3):245–273.
Jade Goldstein, Andrew Kwasinski, Paul Kingsbury,
R Sabin, and Albert McDowell. 2006. Annotating
subsets of the enron email corpus. In Proceedings of
the Third Conference on Email and Anti-Spam. Cite-
seer.
P-Y Hsueh and Johanna D Moore. 2006. Automatic
topic segmentation and labeling in multiparty dia-
logue. In Spoken Language Technology Workshop,
2006. IEEE, pages 98–101. IEEE.
Richard S Irwin and Naomi D Richardson. 2006.
Patient-focused careusing the right tools. CHEST
Journal, 130(1 suppl):73S–82S.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2010. Classifying dialogue acts in one-on-one
live chats. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 862–871. Association for Computational Lin-
guistics.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2012. Classifying dialogue acts in multi-party
live chats.
Michael Barton Laws, Ylisabyth S Bradshaw, Steven A
Safren, Mary Catherine Beach, Yoojin Lee, William
Rogers, and Ira B Wilson. 2011a. Discussion of sex-
ual risk behavior in HIV care is infrequent and appears
ineffectual: a mixed methods study. AIDS and Behav-
ior, 15(4):812–822.
Michael Barton Laws, Lauren Epstein, Yoojin Lee,
William Rogers, Mary Catherine Beach, and Ira B
Wilson. 2011b. The association of visit length and
measures of patient-centered communication in HIV
care: A mixed methods study. Patient Education and
Counseling, 85(3):e183–e188.
</reference>
<page confidence="0.868399">
1774
</page>
<reference confidence="0.999365144444444">
Michael Barton Laws, Mary Catherine Beach, Yoojin
Lee, William H Rogers, Somnath Saha, P Todd Ko-
rthuis, Victoria Sharp, and Ira B Wilson. 2012.
Provider-patient adherence dialogue in HIV care: re-
sults of a multisite study. AIDS and Behavior, pages
1–12.
Gregory Makoul. 2001. Essential elements of communi-
cation in medical encounters: the kalamazoo consen-
sus statement. Academic Medicine, 76(4):390–393.
Marie W Meteer, Ann A Taylor, Robert MacIntyre, and
Rukmini Iyer. 1995. Dysfluency annotation stylebook
for the switchboard corpus. University of Pennsylva-
nia.
Lucille ML Ong, Johanna CJM De Haes, Alaysia M
Hoos, and Frits B Lammes. 1995. Doctor-patient
communication: a review of the literature. Social sci-
ence &amp; medicine, 40(7):903–918.
Michael Paul and Mark Dredze. 2012. Factorial lda:
Sparse multi-dimensional text models. In Advances
in Neural Information Processing Systems 25, pages
2591–2599.
Michael J. Paul, Byron C. Wallace, and Mark Dredze.
2013. What affects patient (dis)satisfaction? analyz-
ing online doctor ratings with a joint topic-sentiment
model. In AAAI Workshop on Expanding the Bound-
aries of Health Informatics Using AI (HIAI).
Michael J Paul. 2012. Mixed membership Markov mod-
els for unsupervised conversation modeling. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 94–104.
Association for Computational Linguistics.
C Raymond Perrault and James F Allen. 1980. A plan-
based analysis of indirect speech acts. Computational
Linguistics, 6(3-4):167–182.
Ashequl Qadir and Ellen Riloff. 2011. Classifying sen-
tences as speech acts in message board posts. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 748–758. Asso-
ciation for Computational Linguistics.
Lawrence Rabiner and B Juang. 1986. An introduction
to hidden Markov models. ASSP Magazine, IEEE,
3(1):4–16.
Debra Roter and Susan Larson. 2002. The roter in-
teraction analysis system (rias): utility and flexibility
for analysis of medical interactions. Patient education
and counseling, 46(4):243–251.
John R Searle. 1969. Speech acts: An essay in the phi-
losophy of language. Cambridge university press.
John R Searle. 1985. Expression and meaning: Studies
in the theory of speech acts. Cambridge University
Press.
Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates,
Noah Coccaro, Daniel Jurafsky, Rachel Martin, Marie
Meteer, Klaus Ries, Paul Taylor, and Carol Van Ess-
Dykema. 1998. Dialog act modeling for conversa-
tional speech. In AAAI Spring Symposium on Apply-
ing Machine Learning to Discourse Processing, pages
98–105.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational linguistics, 26(3):339–373.
Charles Sutton, Andrew McCallum, and Khashayar Ro-
hanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling and
segmenting sequence data. The Journal of Machine
Learning Research, 8:693–723.
Carol Teutsch. 2003. Patient-doctor communication.
The medical clinics of North America, 87(5):1115.
David R Traum and Staffan Larsson. 2003. The infor-
mation state approach to dialogue management. In
Current and new directions in discourse and dialogue,
pages 325–353. Springer.
Jurgen Van Gael, Yee Whye Teh, and Zoubin Ghahra-
mani. 2008. The infinite factorial hidden Markov
model. In Neural Information Processing Systems,
volume 21.
Ira B Wilson, M Barton Laws, Steven A Safren, Yoo-
jin Lee, Minyi Lu, William Coady, Paul R Skolnik,
and William H Rogers. 2010. Provider-focused inter-
vention increases adherence-related dialogue, but does
not improve antiretroviral therapy adherence in per-
sons with HIV. Journal of acquired immune deficiency
syndromes, 53(3):338.
Renxian Zhang, Dehong Gao, and Wenjie Li. 2012. To-
wards scalable speech act recognition in twitter: Tack-
ling insufficient training data. EACL 2012, page 18.
</reference>
<page confidence="0.991326">
1775
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.131551">
<title confidence="0.9949315">A Generative Joint, Additive, Sequential of Topics and Speech Acts in Patient-Doctor Communication</title>
<author confidence="0.999418">C Thomas A M Barton</author>
<affiliation confidence="0.9470095">B. of Health Services, Policy &amp; Practice, Brown University, Providence, of Computer Science, Brown University, Providence,</affiliation>
<address confidence="0.523378">wallace, thomas trikalinos, michael barton wilson, eugene</address>
<abstract confidence="0.993532612903226">We develop a novel generative model of conversation that jointly captures both the topical content and the speech act type associated with each utterance. Our model expresses both token emission and state transition probabilities as log-linear functions of separate components corresponding to topics and speech acts (and their interactions). We apply this model to a dataset comprising annotated patient-physician visits and show that the proposed joint approach outperforms a baseline univariate model. Role Utterance Topic Speech act me just write down of these issues here so I get them straight in my mind. you ain’t got to tell nuttin’. in very good hands I’m around you. push comes to a shove, open the window and throw me out. wanted to ask you, too - Conv. Mgmt. know you had that Ask colonic polyp is it two years from now that Ask they’re going to be doing the repeat? Conv. Mgmt.</abstract>
<title confidence="0.83466575">Logistics Commissive Socializing Directive Socializing Give Info. Socializing Humor/Levity</title>
<abstract confidence="0.637039">do the repeat coloscopy Give Info. in about two years.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jeremy Ang</author>
<author>Yang Liu</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Automatic dialog act segmentation and classification in multiparty meetings.</title>
<date>2005</date>
<booktitle>In Proc. ICASSP,</booktitle>
<volume>1</volume>
<pages>1061--1064</pages>
<contexts>
<context position="32274" citStr="Ang et al. (2005)" startWordPosition="5018" endWordPosition="5021">onstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods into the proposed model (rather than relying on inputs to be manually segmented beforehand) would be a natural extension of this work. Additive component models of text have recently 5We note that only 8 of the 42 speech acts appeared with greater than 1% frequen</context>
</contexts>
<marker>Ang, Liu, Shriberg, 2005</marker>
<rawString>Jeremy Ang, Yang Liu, and Elizabeth Shriberg. 2005. Automatic dialog act segmentation and classification in multiparty meetings. In Proc. ICASSP, volume 1, pages 1061–1064.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Angus</author>
<author>Bernadette Watson</author>
<author>Andrew Smith</author>
<author>Cindy Gallois</author>
<author>Janet Wiles</author>
</authors>
<title>Visualising conversation structure across time: Insights into effective doctor-patient consultations.</title>
<date>2012</date>
<journal>PloS one,</journal>
<volume>7</volume>
<issue>6</issue>
<contexts>
<context position="31295" citStr="Angus et al., 2012" startWordPosition="4865" endWordPosition="4868">ech acts).5 Their model has served as the baseline approach in the present work. Stolcke et al. also considered jointly performing speech recognition and speech act classification. Others have investigated visual structures of patient-provider interactions to qualitatively assess communication in care. Specifically, (Cretchley et al., 2010) leveraged concept maps to explore conversations between people with schizophrenia and their carers. Briefly, this approach allowed them to (qualitatively) identify two distinct conversational strategies used by care-takers and their patients. Angus et al. (Angus et al., 2012) presented a similar approach in which they used text visualization software to explore patterns of (inferred) topics in consultations. Another thread of research has investigated classifying speech acts in emails into one of a small set of “email speech acts”, e.g., request, propose, commit (Cohen et al., 2004; Goldstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including mess</context>
</contexts>
<marker>Angus, Watson, Smith, Gallois, Wiles, 2012</marker>
<rawString>Daniel Angus, Bernadette Watson, Andrew Smith, Cindy Gallois, and Janet Wiles. 2012. Visualising conversation structure across time: Insights into effective doctor-patient consultations. PloS one, 7(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Langshaw Austin</author>
</authors>
<title>How to do things with words, volume 88.</title>
<date>1955</date>
<publisher>Harvard University Press.</publisher>
<contexts>
<context position="1901" citStr="Austin (1955)" startWordPosition="312" endWordPosition="313">ad that Biomedical Ask Q. colonic polyp - D - is it two years from now that Biomedical Ask Q. they’re going to be doing the repeat? P Yeah. Biomedical Conv. Mgmt. Logistics Commissive Socializing Directive Socializing Give Info. Socializing Humor/Levity D We’ll do the repeat coloscopy Biomedical Give Info. in about two years. 1 Introduction Communication involves at least two aspects: the words one says and the acts one performs in saying them. Examples of the latter include asking questions, issuing commands, and so on. These are referred to as speech acts under the sociolinguistic theory of Austin (1955), which was further developed by Searle (1969; 1985). Recognizing speech acts is crucial to understanding communication because a speaker’s meaning is only partially captured by the words they use; much of their intent is expressed implicitly via speech acts (Searle, 1969). On this view, conversational utterances can be assigned both a topic and a speech act. The former describes the subject matter of what was said and the latter captures the “social act” (e.g., promising) performed by saying it. For example, the utterance “Obama won the election” is topically political and is an example of an</context>
</contexts>
<marker>Austin, 1955</marker>
<rawString>John Langshaw Austin. 1955. How to do things with words, volume 88. Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Brand</author>
<author>Nuria Oliver</author>
<author>Alex Pentland</author>
</authors>
<title>Coupled hidden Markov models for complex action recognition.</title>
<date>1997</date>
<booktitle>In Computer Vision and Pattern Recognition,</booktitle>
<pages>994--999</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6370" citStr="Brand et al., 1997" startWordPosition="1011" endWordPosition="1014">by considering a baseline generative approach to modeling topics and speech acts independently. This simple approach was used by Stolcke et al. (2000) to model speech acts. It accounts for only a single output at each time point yt E Y, and hence here we model topics and speech acts independently. A straight-forward (albeit naive) alternative would be to treat the Cartesian product of topics and speech acts as a single output space on which emissions and transitions are conditioned, but this space is too large and sparse for this approach to be practicable. We note that the fully coupled HMM (Brand et al., 1997) suffers from a similar exponential output state problem. The related factorial HMM (FHMM) (Ghahramani and Jordan, 1997; Van Gael et al., 2008), meanwhile, imposes unwarranted (in our case) independence assumptions with respect to state transitions along parallel chains, does not obviously lend itself to discrete observations (typically Gaussians are assumed), and does not scale well enough (in terms of training time) to be feasible for our application. The Markov-Multinomial (MM) comprises two components; transitions and emissions. The former is modeled by making a first-order Markov assumpti</context>
</contexts>
<marker>Brand, Oliver, Pentland, 1997</marker>
<rawString>Matthew Brand, Nuria Oliver, and Alex Pentland. 1997. Coupled hidden Markov models for complex action recognition. In Computer Vision and Pattern Recognition, 1997. Proceedings., 1997 IEEE Computer Society Conference on, pages 994–999. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
<author>Andrei Popescu-Belis</author>
</authors>
<title>Multi-level dialogue act tags.</title>
<date>2004</date>
<booktitle>In Proc. SIGdial,</booktitle>
<pages>163--170</pages>
<contexts>
<context position="32242" citStr="Clark and Popescu-Belis, 2004" startWordPosition="5012" endWordPosition="5015">dstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods into the proposed model (rather than relying on inputs to be manually segmented beforehand) would be a natural extension of this work. Additive component models of text have recently 5We note that only 8 of the 42 speech acts appea</context>
</contexts>
<marker>Clark, Popescu-Belis, 2004</marker>
<rawString>Alexander Clark and Andrei Popescu-Belis. 2004. Multi-level dialogue act tags. In Proc. SIGdial, pages 163–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Vitor R Carvalho</author>
<author>Tom M Mitchell</author>
</authors>
<title>Learning to classify email into speech acts.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<volume>4</volume>
<pages>sn.</pages>
<contexts>
<context position="31607" citStr="Cohen et al., 2004" startWordPosition="4915" endWordPosition="4918">lly, (Cretchley et al., 2010) leveraged concept maps to explore conversations between people with schizophrenia and their carers. Briefly, this approach allowed them to (qualitatively) identify two distinct conversational strategies used by care-takers and their patients. Angus et al. (Angus et al., 2012) presented a similar approach in which they used text visualization software to explore patterns of (inferred) topics in consultations. Another thread of research has investigated classifying speech acts in emails into one of a small set of “email speech acts”, e.g., request, propose, commit (Cohen et al., 2004; Goldstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2</context>
</contexts>
<marker>Cohen, Carvalho, Mitchell, 2004</marker>
<rawString>William W Cohen, Vitor R Carvalho, and Tom M Mitchell. 2004. Learning to classify email into speech acts. In Proceedings of EMNLP, volume 4. sn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Cretchley</author>
<author>Cindy Gallois</author>
<author>Helen Chenery</author>
<author>Andrew Smith</author>
</authors>
<title>Conversations between carers and people with schizophrenia: a qualitative analysis using leximancer.</title>
<date>2010</date>
<journal>Qualitative Health Research,</journal>
<volume>20</volume>
<issue>12</issue>
<contexts>
<context position="31018" citStr="Cretchley et al., 2010" startWordPosition="4824" endWordPosition="4827">MM) (Rabiner and Juang, 1986). They were interested in modeling an unrestricted set of conversations, and did not impose a hierarchy on the speech acts; they 1772 therefore enumerated many more speech acts (42) than we do in the present work (recall that we use 10 ‘high-level’ speech acts).5 Their model has served as the baseline approach in the present work. Stolcke et al. also considered jointly performing speech recognition and speech act classification. Others have investigated visual structures of patient-provider interactions to qualitatively assess communication in care. Specifically, (Cretchley et al., 2010) leveraged concept maps to explore conversations between people with schizophrenia and their carers. Briefly, this approach allowed them to (qualitatively) identify two distinct conversational strategies used by care-takers and their patients. Angus et al. (Angus et al., 2012) presented a similar approach in which they used text visualization software to explore patterns of (inferred) topics in consultations. Another thread of research has investigated classifying speech acts in emails into one of a small set of “email speech acts”, e.g., request, propose, commit (Cohen et al., 2004; Goldstein</context>
</contexts>
<marker>Cretchley, Gallois, Chenery, Smith, 2010</marker>
<rawString>Julia Cretchley, Cindy Gallois, Helen Chenery, and Andrew Smith. 2010. Conversations between carers and people with schizophrenia: a qualitative analysis using leximancer. Qualitative Health Research, 20(12):1611–1628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Bayesian unsupervised topic segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>334--343</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19449" citStr="Eisenstein and Barzilay, 2008" startWordPosition="3015" endWordPosition="3018">er substantially. The model we develop in this work assumes that transcripts have been manually segmented. While this comes at some cost, segmenting is still much cheaper than annotating transcripts. Manually annotating a single visit with GMIAS codes takes 2- 4 hours and must be performed by someone with substantive domain expertise. By contrast, segmenting transcripts into utterances takes at most 1/4th of the time as annotation and can be done by a less highly-skilled individual. That said, in future work we hope to explore incorporating automatic segmentation methods (Galley et al., 2003; Eisenstein and Barzilay, 2008) into our approach. Each utterance is assigned a single topic code and a single speech act code. Inter-rater agreement has been observed to be relatively high for this task: Kappa between three trained annotators and a reference annotation ranged from 0.89 to 1.0 for topics and 0.81 to 0.95 for speech acts. We next de2https://sites.google.com/a/brown.edu/m-barton-laws/home/gmias 1769 scribe the topics and speech acts we consider in more detail; Table 2 enumerates all pairs of these and their respective counts in the corpus. We note that GMIAS defines a hierarchy of both topic and speech act co</context>
<context position="32568" citStr="Eisenstein and Barzilay, 2008" startWordPosition="5059" endWordPosition="5062">ikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods into the proposed model (rather than relying on inputs to be manually segmented beforehand) would be a natural extension of this work. Additive component models of text have recently 5We note that only 8 of the 42 speech acts appeared with greater than 1% frequency in Stolcke et al.’s corpus. gained traction (Eisenstein et al., 2011; Paul, 2012; Paul and Dredze, 2012; Paul et al., 2013). To our knowledge, this is the first extension of supervised additive component models to a sequential task.6 7 Conclusions and Future Directions We have proposed a no</context>
</contexts>
<marker>Eisenstein, Barzilay, 2008</marker>
<rawString>Jacob Eisenstein and Regina Barzilay. 2008. Bayesian unsupervised topic segmentation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 334–343. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>A Ahmed</author>
<author>E P Xing</author>
</authors>
<title>Sparse additive generative models of text.</title>
<date>2011</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>1041--1048</pages>
<contexts>
<context position="4843" citStr="Eisenstein et al., 2011" startWordPosition="768" endWordPosition="771">oviral (ARV) medication (Wilson et al., 2010). To measure the effect of the intervention, investigators performed a randomized control trial in which they quantified change in communication patterns by tallying the number of information giving speech acts that fell under the ARV adherence topic. Without assigning both topics and speech acts to utterances, this analysis would not have been possible. In this work, we develop a novel componentbased generative model for bivariate, sequentially structured problems. Our approach extends the recently proposed Sparse Additive Generative (SAGE) model (Eisenstein et al., 2011) and similar recently developed additive models (Paul and Dredze, 2012; Paul et al., 2013) to the case of supervised sequential tasks to capture the joint conditional influence of topics and speech acts, both with respect to token generation and state transitions. For brevity, we refer to this generative Joint, Additive, Sequential model as JAS. In contrast to previous work on speech acts, JAS provides a single, coherent generative model of conversations. And because it is component-based, this model provides a flexible framework for analyzing communication patterns. We demonstrate that JAS ou</context>
<context position="10579" citStr="Eisenstein et al. (2011)" startWordPosition="1723" endWordPosition="1726">with corresponding topic yt and speech act st is log-linear with respect to these components, i.e.: 1 P(w|yt, st) = exp{θw+ηyt w +ηst w +ηst,yt w } (3) Zw Where Zw is a normalizing term (implicitly conditioned on yt and st) defined as: �Zw = exp{θw0 + ηytw0 + ηstw0 + ηst,yt w0∈W w0 } (4) We make the standard naive assumption that words are generated independently, given the topic and speech act of the utterance to which they belong: 11 P(ut|yt, st) = P(w|yt, st) (5) w∈ut The per-token emission probability just described falls under the additive generative family of models recently proposed by Eisenstein et al. (2011). However, in addition to conditional token emission probabilities, here we need also to model the transition probabilities such that the likelihood of transitioning to topic yt (and to speech act st) reflects both the previous topic and the previous speech act, capturing the dependencies illustrated in Figure 1. To this end, we model topic and speech act transition probabilities as log-linear functions of the preceding topic and speech act. We denote log of the background topic frequencies by TrY, and components capturing the influence of transitioning to topic yt due to the preceding topic a</context>
<context position="14591" citStr="Eisenstein et al. (2011)" startWordPosition="2357" endWordPosition="2360">ivative for the topic-to-topic transition component Qy,y0 with respect to the likelihood, for example, is: C(y,s),y0 − P(y&apos;|y, s)C(y,s),* (10) Where C(y,s),y0 denotes the observed count of transitions from topic/speech act pair (y, s) to y&apos;, and C(y,s),* denotes the total number of observed transitions out of this pair. The term P(y&apos;|y, s) is with respect to the current parameter estimates and is defined in Equation 6. The partial derivatives for the other component parameters (both transition and emission) are analogous. We use a Newton optimization method similar to the approach outlined by Eisenstein et al. (2011).1 We assess convergence by calculating predictive performance on a held-out portion (5%) of the training dataset at each step, halting the descent when this declines. 4 Dataset We use a corpus of patient-provider visits annotated with Generalized Medical Interaction Anaylsis System (GMIAS) codes. The GMIAS has been used to: characterize interaction processes in physicianpatient communication about ARV adherence in the 1With the exception that we do not explicitly model the distribution over component variances. a _1: sES auy,y0 1768 Topic; Speech act Count (prevalence) ARV Adherence; Ask Q 29</context>
<context position="32945" citStr="Eisenstein et al., 2011" startWordPosition="5122" endWordPosition="5125">lassifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods into the proposed model (rather than relying on inputs to be manually segmented beforehand) would be a natural extension of this work. Additive component models of text have recently 5We note that only 8 of the 42 speech acts appeared with greater than 1% frequency in Stolcke et al.’s corpus. gained traction (Eisenstein et al., 2011; Paul, 2012; Paul and Dredze, 2012; Paul et al., 2013). To our knowledge, this is the first extension of supervised additive component models to a sequential task.6 7 Conclusions and Future Directions We have proposed a novel Joint, Additive, Sequential (JAS) model of conversational topics and speech acts. In contrast to previous approaches to modeling conversational exchanges, this model factors both the current topic and the current speech act into token emission and state transition probabilities. We demonstrated that this model consistently outperforms a univariate generative baseline tha</context>
</contexts>
<marker>Eisenstein, Ahmed, Xing, 2011</marker>
<rawString>J. Eisenstein, A. Ahmed, and E.P. Xing. 2011. Sparse additive generative models of text. In Proceedings of ICML, pages 1041–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Ferschke</author>
<author>Iryna Gurevych</author>
<author>Yevgen Chebotar</author>
</authors>
<title>Behind the article: Recognizing dialog acts in wikipedia talk pages.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2012),</booktitle>
<pages>777--786</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="31981" citStr="Ferschke et al., 2012" startWordPosition="4970" endWordPosition="4973">ion software to explore patterns of (inferred) topics in consultations. Another thread of research has investigated classifying speech acts in emails into one of a small set of “email speech acts”, e.g., request, propose, commit (Cohen et al., 2004; Goldstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporat</context>
</contexts>
<marker>Ferschke, Gurevych, Chebotar, 2012</marker>
<rawString>Oliver Ferschke, Iryna Gurevych, and Yevgen Chebotar. 2012. Behind the article: Recognizing dialog acts in wikipedia talk pages. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2012), pages 777– 786. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey D Ford</author>
<author>Laurie W Ford</author>
</authors>
<title>The role of conversations in producing intentional change in organizations. Academy of Management Review,</title>
<date>1995</date>
<pages>541--570</pages>
<contexts>
<context position="18283" citStr="Ford and Ford, 1995" startWordPosition="2834" endWordPosition="2837"> of visit length with constructs of patient-centeredness (Laws et al., 2011b); and to describe providerpatient communication regarding ARV adherence compared with communication about other issues (Laws et al., 2012). GMIAS annotation is described at length elsewhere,2 but we summarize it here for completeness. GMIAS segments conversation into utterances. An utterance is here defined as a single completed speech act. Previous coding systems have simply defined an utterance as conveying a single thought (Roter and Larson, 2002) or any independent or unrestrictive dependent clause of a sentence (Ford and Ford, 1995). Stolcke et al. (2000) followed Meteer et al. (1995) in using “sentence-level units”. These definitions provide helpful guidance to coders, but many speech acts are poorly formed grammatically, and cannot be described as a “clause”. Further, some speech acts cannot be said to convey a “thought” (or sentence) at all, but rather are pre-syntactical (e.g., interjections and non-lexical utterances like laughter). In any case, most natural segmentations of conversations probably largely agree with intuition, and are not likely to differ substantially. The model we develop in this work assumes that</context>
</contexts>
<marker>Ford, Ford, 1995</marker>
<rawString>Jeffrey D Ford and Laurie W Ford. 1995. The role of conversations in producing intentional change in organizations. Academy of Management Review, pages 541–570.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Forman</author>
<author>Martin Scholz</author>
</authors>
<title>Apples-toapples in cross-validation studies: pitfalls in classifier performance measurement.</title>
<date>2010</date>
<volume>12</volume>
<pages>49--57</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="25136" citStr="Forman and Scholz, 2010" startWordPosition="3876" endWordPosition="3879">, e.g., Dynamic Conditional Random Fields (DCRFs) (Sutton et al., 2007) for this work. 5.1 Cross-fold Validation Our aim is to measure model performance in terms of correctly identifying both the topic and speech act corresponding to each utterance. We quantify this via the F-score calculated for each topic/speech act pair that is observed at least once. One can see in Table 2 that many such pairs have low prevalence; this can result in undefined F-scores (e.g., when no utterances are assigned to a given pair). In this case, it is reasonable to treat these as zero values, as is commonly done (Forman and Scholz, 2010). This penalizes models when they completely fail to identify an entire class of utterances. We first report macro-averages, that is, averages of the individual topic/speech act pair F-scores. Figure 2 displays the macro-averaged F-score for each of the 10 folds (grey lines connect folds) and the average of these (thick black line). The JAS model achieves an average macro-averaged Fscore of .234 versus the .207 achieved by baseline Markov-Multinomial (MM) model; JAS outperforms MM on every fold. For a more granular picture, Figure 3 displays average F-score differences with respect to every in</context>
</contexts>
<marker>Forman, Scholz, 2010</marker>
<rawString>George Forman and Martin Scholz. 2010. Apples-toapples in cross-validation studies: pitfalls in classifier performance measurement. volume 12, pages 49–57. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
<author>Eric Fosler-Lussier</author>
<author>Hongyan Jing</author>
</authors>
<title>Discourse segmentation of multi-party conversation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>562--569</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19417" citStr="Galley et al., 2003" startWordPosition="3011" endWordPosition="3014">re not likely to differ substantially. The model we develop in this work assumes that transcripts have been manually segmented. While this comes at some cost, segmenting is still much cheaper than annotating transcripts. Manually annotating a single visit with GMIAS codes takes 2- 4 hours and must be performed by someone with substantive domain expertise. By contrast, segmenting transcripts into utterances takes at most 1/4th of the time as annotation and can be done by a less highly-skilled individual. That said, in future work we hope to explore incorporating automatic segmentation methods (Galley et al., 2003; Eisenstein and Barzilay, 2008) into our approach. Each utterance is assigned a single topic code and a single speech act code. Inter-rater agreement has been observed to be relatively high for this task: Kappa between three trained annotators and a reference annotation ranged from 0.89 to 1.0 for topics and 0.81 to 0.95 for speech acts. We next de2https://sites.google.com/a/brown.edu/m-barton-laws/home/gmias 1769 scribe the topics and speech acts we consider in more detail; Table 2 enumerates all pairs of these and their respective counts in the corpus. We note that GMIAS defines a hierarchy</context>
<context position="32532" citStr="Galley et al., 2003" startWordPosition="5054" endWordPosition="5057">Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods into the proposed model (rather than relying on inputs to be manually segmented beforehand) would be a natural extension of this work. Additive component models of text have recently 5We note that only 8 of the 42 speech acts appeared with greater than 1% frequency in Stolcke et al.’s corpus. gained traction (Eisenstein et al., 2011; Paul, 2012; Paul and Dredze, 2012; Paul et al., 2013). To our knowledge, this is the first extension of supervised additive component models to a sequential task.6 7 Conclusions and Fut</context>
</contexts>
<marker>Galley, McKeown, Fosler-Lussier, Jing, 2003</marker>
<rawString>Michel Galley, Kathleen McKeown, Eric Fosler-Lussier, and Hongyan Jing. 2003. Discourse segmentation of multi-party conversation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 562–569. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoubin Ghahramani</author>
<author>Michael I Jordan</author>
</authors>
<date>1997</date>
<booktitle>Factorial hidden Markov models. Machine learning,</booktitle>
<pages>29--2</pages>
<contexts>
<context position="6489" citStr="Ghahramani and Jordan, 1997" startWordPosition="1029" endWordPosition="1032">roach was used by Stolcke et al. (2000) to model speech acts. It accounts for only a single output at each time point yt E Y, and hence here we model topics and speech acts independently. A straight-forward (albeit naive) alternative would be to treat the Cartesian product of topics and speech acts as a single output space on which emissions and transitions are conditioned, but this space is too large and sparse for this approach to be practicable. We note that the fully coupled HMM (Brand et al., 1997) suffers from a similar exponential output state problem. The related factorial HMM (FHMM) (Ghahramani and Jordan, 1997; Van Gael et al., 2008), meanwhile, imposes unwarranted (in our case) independence assumptions with respect to state transitions along parallel chains, does not obviously lend itself to discrete observations (typically Gaussians are assumed), and does not scale well enough (in terms of training time) to be feasible for our application. The Markov-Multinomial (MM) comprises two components; transitions and emissions. The former is modeled by making a first-order Markov assumption, specifically: P(yt|y0, ..., yt−1) = P(yt|yt−1) = Ayt−,,yt (1) Emissions can be modeled via a multinomial that captu</context>
</contexts>
<marker>Ghahramani, Jordan, 1997</marker>
<rawString>Zoubin Ghahramani and Michael I Jordan. 1997. Factorial hidden Markov models. Machine learning, 29(2-3):245–273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Andrew Kwasinski</author>
<author>Paul Kingsbury</author>
<author>R Sabin</author>
<author>Albert McDowell</author>
</authors>
<title>Annotating subsets of the enron email corpus.</title>
<date>2006</date>
<booktitle>In Proceedings of the Third Conference on Email and Anti-Spam. Citeseer.</booktitle>
<contexts>
<context position="31632" citStr="Goldstein et al., 2006" startWordPosition="4919" endWordPosition="4922">l., 2010) leveraged concept maps to explore conversations between people with schizophrenia and their carers. Briefly, this approach allowed them to (qualitatively) identify two distinct conversational strategies used by care-takers and their patients. Angus et al. (Angus et al., 2012) presented a similar approach in which they used text visualization software to explore patterns of (inferred) topics in consultations. Another thread of research has investigated classifying speech acts in emails into one of a small set of “email speech acts”, e.g., request, propose, commit (Cohen et al., 2004; Goldstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Be</context>
</contexts>
<marker>Goldstein, Kwasinski, Kingsbury, Sabin, McDowell, 2006</marker>
<rawString>Jade Goldstein, Andrew Kwasinski, Paul Kingsbury, R Sabin, and Albert McDowell. 2006. Annotating subsets of the enron email corpus. In Proceedings of the Third Conference on Email and Anti-Spam. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P-Y Hsueh</author>
<author>Johanna D Moore</author>
</authors>
<title>Automatic topic segmentation and labeling in multiparty dialogue.</title>
<date>2006</date>
<booktitle>In Spoken Language Technology Workshop,</booktitle>
<pages>98--101</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="32404" citStr="Hsueh and Moore (2006)" startWordPosition="5036" endWordPosition="5039">rchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods into the proposed model (rather than relying on inputs to be manually segmented beforehand) would be a natural extension of this work. Additive component models of text have recently 5We note that only 8 of the 42 speech acts appeared with greater than 1% frequency in Stolcke et al.’s corpus. gained traction (Eisenstein et al., 2011; Paul, 2012; Paul and Dredze, 2012; Paul et al., 2013). To</context>
</contexts>
<marker>Hsueh, Moore, 2006</marker>
<rawString>P-Y Hsueh and Johanna D Moore. 2006. Automatic topic segmentation and labeling in multiparty dialogue. In Spoken Language Technology Workshop, 2006. IEEE, pages 98–101. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard S Irwin</author>
<author>Naomi D Richardson</author>
</authors>
<title>Patient-focused careusing the right tools.</title>
<date>2006</date>
<journal>CHEST Journal,</journal>
<volume>130</volume>
<issue>1</issue>
<pages>73--82</pages>
<contexts>
<context position="3757" citStr="Irwin and Richardson, 2006" startWordPosition="603" endWordPosition="606">topical content. But a richer model would account for both speech acts and the contextualizing topic of each utterance. To this end, we develop a novel joint, generative model of topics and speech acts. We focus on physician-patient communication as a motivating domain. This is of interest because 1765 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1765–1775, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics it is widely appreciated that effective communication is an integral part of clinical practice (Irwin and Richardson, 2006; Makoul, 2001; Teutsch, 2003). We provide an excerpt of a conversation between a patient and their doctor annotated with topics and speech acts in Table 1. Such annotations can provide substantive insights into how doctors communicate with patients (Ong et al., 1995). A concrete example of this is the use of topic and speech act codes to assess the efficacy of an intervention meant to influence physician-patient communication regarding adherence to antiretroviral (ARV) medication (Wilson et al., 2010). To measure the effect of the intervention, investigators performed a randomized control tri</context>
</contexts>
<marker>Irwin, Richardson, 2006</marker>
<rawString>Richard S Irwin and Naomi D Richardson. 2006. Patient-focused careusing the right tools. CHEST Journal, 130(1 suppl):73S–82S.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Lawrence Cavedon</author>
<author>Timothy Baldwin</author>
</authors>
<title>Classifying dialogue acts in one-on-one live chats.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>862--871</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3108" citStr="Kim et al., 2010" startWordPosition="505" endWordPosition="508">mple of an information giving speech act. “Did Obama win the election?”, meanwhile, belongs Table 1: An excerpt from a patient-doctor interaction, annotated with topic and speech act codes. The D and P roles denote doctor and patient, respectively. Conv. Mgmt. abbreviates conversation management; Ask Q. abbreviates ask question. to the same topic but is a question. Both aspects are necessary to understand conversation. Previous computational work on speech acts – which we review in Section 6 – has modeled them in isolation (Perrault and Allen, 1980; Stolcke et al., 1998; Stolcke et al., 2000; Kim et al., 2010), i.e., independent of topical content. But a richer model would account for both speech acts and the contextualizing topic of each utterance. To this end, we develop a novel joint, generative model of topics and speech acts. We focus on physician-patient communication as a motivating domain. This is of interest because 1765 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1765–1775, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics it is widely appreciated that effective communication is an integral part</context>
<context position="32176" citStr="Kim et al., 2010" startWordPosition="5002" endWordPosition="5005">g., request, propose, commit (Cohen et al., 2004; Goldstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods into the proposed model (rather than relying on inputs to be manually segmented beforehand) would be a natural extension of this work. Additive component models of t</context>
</contexts>
<marker>Kim, Cavedon, Baldwin, 2010</marker>
<rawString>Su Nam Kim, Lawrence Cavedon, and Timothy Baldwin. 2010. Classifying dialogue acts in one-on-one live chats. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 862–871. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Lawrence Cavedon</author>
<author>Timothy Baldwin</author>
</authors>
<title>Classifying dialogue acts in multi-party live chats.</title>
<date>2012</date>
<contexts>
<context position="32210" citStr="Kim et al., 2012" startWordPosition="5008" endWordPosition="5011"> et al., 2004; Goldstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods into the proposed model (rather than relying on inputs to be manually segmented beforehand) would be a natural extension of this work. Additive component models of text have recently 5We note that on</context>
</contexts>
<marker>Kim, Cavedon, Baldwin, 2012</marker>
<rawString>Su Nam Kim, Lawrence Cavedon, and Timothy Baldwin. 2012. Classifying dialogue acts in multi-party live chats.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Barton Laws</author>
<author>Ylisabyth S Bradshaw</author>
<author>Steven A Safren</author>
<author>Mary Catherine Beach</author>
<author>Yoojin Lee</author>
<author>William Rogers</author>
<author>Ira B Wilson</author>
</authors>
<title>Discussion of sexual risk behavior in HIV care is infrequent and appears ineffectual: a mixed methods study.</title>
<date>2011</date>
<journal>AIDS and Behavior,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="17634" citStr="Laws et al., 2011" startWordPosition="2739" endWordPosition="2742">l; Missing/other 1199 (0.005) Psycho-Social; Social-Ritual 36 (0.000) Socializing; Ask Q 1283 (0.006) Socializing; Commissive 79 (0.000) Socializing; Continuation 85 (0.000) Socializing; Conv. Management 2166 (0.009) Socializing; Directive 222 (0.001) Socializing; Empathy 73 (0.000) Socializing; Give Information 8981 (0.039) Socializing; Humor/Levity 306 (0.001) Socializing; Missing/other 849 (0.004) Socializing; Social-Ritual 1685 (0.007) Table 2: Topic/speech act pairs and their counts. context of an intervention trial (Wilson et al., 2010); analyze communication about sexual risk behavior (Laws et al., 2011a); elucidate the association of visit length with constructs of patient-centeredness (Laws et al., 2011b); and to describe providerpatient communication regarding ARV adherence compared with communication about other issues (Laws et al., 2012). GMIAS annotation is described at length elsewhere,2 but we summarize it here for completeness. GMIAS segments conversation into utterances. An utterance is here defined as a single completed speech act. Previous coding systems have simply defined an utterance as conveying a single thought (Roter and Larson, 2002) or any independent or unrestrictive dep</context>
</contexts>
<marker>Laws, Bradshaw, Safren, Beach, Lee, Rogers, Wilson, 2011</marker>
<rawString>Michael Barton Laws, Ylisabyth S Bradshaw, Steven A Safren, Mary Catherine Beach, Yoojin Lee, William Rogers, and Ira B Wilson. 2011a. Discussion of sexual risk behavior in HIV care is infrequent and appears ineffectual: a mixed methods study. AIDS and Behavior, 15(4):812–822.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Barton Laws</author>
<author>Lauren Epstein</author>
<author>Yoojin Lee</author>
<author>William Rogers</author>
<author>Mary Catherine Beach</author>
<author>Ira B Wilson</author>
</authors>
<title>The association of visit length and measures of patient-centered communication in HIV care: A mixed methods study. Patient Education and Counseling,</title>
<date>2011</date>
<contexts>
<context position="17634" citStr="Laws et al., 2011" startWordPosition="2739" endWordPosition="2742">l; Missing/other 1199 (0.005) Psycho-Social; Social-Ritual 36 (0.000) Socializing; Ask Q 1283 (0.006) Socializing; Commissive 79 (0.000) Socializing; Continuation 85 (0.000) Socializing; Conv. Management 2166 (0.009) Socializing; Directive 222 (0.001) Socializing; Empathy 73 (0.000) Socializing; Give Information 8981 (0.039) Socializing; Humor/Levity 306 (0.001) Socializing; Missing/other 849 (0.004) Socializing; Social-Ritual 1685 (0.007) Table 2: Topic/speech act pairs and their counts. context of an intervention trial (Wilson et al., 2010); analyze communication about sexual risk behavior (Laws et al., 2011a); elucidate the association of visit length with constructs of patient-centeredness (Laws et al., 2011b); and to describe providerpatient communication regarding ARV adherence compared with communication about other issues (Laws et al., 2012). GMIAS annotation is described at length elsewhere,2 but we summarize it here for completeness. GMIAS segments conversation into utterances. An utterance is here defined as a single completed speech act. Previous coding systems have simply defined an utterance as conveying a single thought (Roter and Larson, 2002) or any independent or unrestrictive dep</context>
</contexts>
<marker>Laws, Epstein, Lee, Rogers, Beach, Wilson, 2011</marker>
<rawString>Michael Barton Laws, Lauren Epstein, Yoojin Lee, William Rogers, Mary Catherine Beach, and Ira B Wilson. 2011b. The association of visit length and measures of patient-centered communication in HIV care: A mixed methods study. Patient Education and Counseling, 85(3):e183–e188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Barton Laws</author>
<author>Mary Catherine Beach</author>
<author>Yoojin Lee</author>
<author>William H Rogers</author>
<author>Somnath Saha</author>
<author>P Todd Korthuis</author>
<author>Victoria Sharp</author>
<author>Ira B Wilson</author>
</authors>
<title>Provider-patient adherence dialogue in HIV care: results of a multisite study. AIDS and Behavior,</title>
<date>2012</date>
<pages>1--12</pages>
<contexts>
<context position="17878" citStr="Laws et al., 2012" startWordPosition="2772" endWordPosition="2775"> (0.001) Socializing; Empathy 73 (0.000) Socializing; Give Information 8981 (0.039) Socializing; Humor/Levity 306 (0.001) Socializing; Missing/other 849 (0.004) Socializing; Social-Ritual 1685 (0.007) Table 2: Topic/speech act pairs and their counts. context of an intervention trial (Wilson et al., 2010); analyze communication about sexual risk behavior (Laws et al., 2011a); elucidate the association of visit length with constructs of patient-centeredness (Laws et al., 2011b); and to describe providerpatient communication regarding ARV adherence compared with communication about other issues (Laws et al., 2012). GMIAS annotation is described at length elsewhere,2 but we summarize it here for completeness. GMIAS segments conversation into utterances. An utterance is here defined as a single completed speech act. Previous coding systems have simply defined an utterance as conveying a single thought (Roter and Larson, 2002) or any independent or unrestrictive dependent clause of a sentence (Ford and Ford, 1995). Stolcke et al. (2000) followed Meteer et al. (1995) in using “sentence-level units”. These definitions provide helpful guidance to coders, but many speech acts are poorly formed grammatically, </context>
</contexts>
<marker>Laws, Beach, Lee, Rogers, Saha, Korthuis, Sharp, Wilson, 2012</marker>
<rawString>Michael Barton Laws, Mary Catherine Beach, Yoojin Lee, William H Rogers, Somnath Saha, P Todd Korthuis, Victoria Sharp, and Ira B Wilson. 2012. Provider-patient adherence dialogue in HIV care: results of a multisite study. AIDS and Behavior, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Makoul</author>
</authors>
<title>Essential elements of communication in medical encounters: the kalamazoo consensus statement.</title>
<date>2001</date>
<journal>Academic Medicine,</journal>
<volume>76</volume>
<issue>4</issue>
<contexts>
<context position="3771" citStr="Makoul, 2001" startWordPosition="607" endWordPosition="608">r model would account for both speech acts and the contextualizing topic of each utterance. To this end, we develop a novel joint, generative model of topics and speech acts. We focus on physician-patient communication as a motivating domain. This is of interest because 1765 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1765–1775, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics it is widely appreciated that effective communication is an integral part of clinical practice (Irwin and Richardson, 2006; Makoul, 2001; Teutsch, 2003). We provide an excerpt of a conversation between a patient and their doctor annotated with topics and speech acts in Table 1. Such annotations can provide substantive insights into how doctors communicate with patients (Ong et al., 1995). A concrete example of this is the use of topic and speech act codes to assess the efficacy of an intervention meant to influence physician-patient communication regarding adherence to antiretroviral (ARV) medication (Wilson et al., 2010). To measure the effect of the intervention, investigators performed a randomized control trial in which th</context>
</contexts>
<marker>Makoul, 2001</marker>
<rawString>Gregory Makoul. 2001. Essential elements of communication in medical encounters: the kalamazoo consensus statement. Academic Medicine, 76(4):390–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie W Meteer</author>
<author>Ann A Taylor</author>
<author>Robert MacIntyre</author>
<author>Rukmini Iyer</author>
</authors>
<title>Dysfluency annotation stylebook for the switchboard corpus.</title>
<date>1995</date>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="18336" citStr="Meteer et al. (1995)" startWordPosition="2843" endWordPosition="2846">ess (Laws et al., 2011b); and to describe providerpatient communication regarding ARV adherence compared with communication about other issues (Laws et al., 2012). GMIAS annotation is described at length elsewhere,2 but we summarize it here for completeness. GMIAS segments conversation into utterances. An utterance is here defined as a single completed speech act. Previous coding systems have simply defined an utterance as conveying a single thought (Roter and Larson, 2002) or any independent or unrestrictive dependent clause of a sentence (Ford and Ford, 1995). Stolcke et al. (2000) followed Meteer et al. (1995) in using “sentence-level units”. These definitions provide helpful guidance to coders, but many speech acts are poorly formed grammatically, and cannot be described as a “clause”. Further, some speech acts cannot be said to convey a “thought” (or sentence) at all, but rather are pre-syntactical (e.g., interjections and non-lexical utterances like laughter). In any case, most natural segmentations of conversations probably largely agree with intuition, and are not likely to differ substantially. The model we develop in this work assumes that transcripts have been manually segmented. While this</context>
</contexts>
<marker>Meteer, Taylor, MacIntyre, Iyer, 1995</marker>
<rawString>Marie W Meteer, Ann A Taylor, Robert MacIntyre, and Rukmini Iyer. 1995. Dysfluency annotation stylebook for the switchboard corpus. University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucille ML Ong</author>
<author>Johanna CJM De Haes</author>
<author>Alaysia M Hoos</author>
<author>Frits B Lammes</author>
</authors>
<title>Doctor-patient communication: a review of the literature.</title>
<date>1995</date>
<journal>Social science &amp; medicine,</journal>
<volume>40</volume>
<issue>7</issue>
<marker>Ong, De Haes, Hoos, Lammes, 1995</marker>
<rawString>Lucille ML Ong, Johanna CJM De Haes, Alaysia M Hoos, and Frits B Lammes. 1995. Doctor-patient communication: a review of the literature. Social science &amp; medicine, 40(7):903–918.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Mark Dredze</author>
</authors>
<title>Factorial lda: Sparse multi-dimensional text models.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems 25,</booktitle>
<pages>2591--2599</pages>
<contexts>
<context position="4913" citStr="Paul and Dredze, 2012" startWordPosition="778" endWordPosition="781">he intervention, investigators performed a randomized control trial in which they quantified change in communication patterns by tallying the number of information giving speech acts that fell under the ARV adherence topic. Without assigning both topics and speech acts to utterances, this analysis would not have been possible. In this work, we develop a novel componentbased generative model for bivariate, sequentially structured problems. Our approach extends the recently proposed Sparse Additive Generative (SAGE) model (Eisenstein et al., 2011) and similar recently developed additive models (Paul and Dredze, 2012; Paul et al., 2013) to the case of supervised sequential tasks to capture the joint conditional influence of topics and speech acts, both with respect to token generation and state transitions. For brevity, we refer to this generative Joint, Additive, Sequential model as JAS. In contrast to previous work on speech acts, JAS provides a single, coherent generative model of conversations. And because it is component-based, this model provides a flexible framework for analyzing communication patterns. We demonstrate that JAS outperforms a generative univariate baseline in topic/speech act predict</context>
<context position="32980" citStr="Paul and Dredze, 2012" startWordPosition="5128" endWordPosition="5131">iparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods into the proposed model (rather than relying on inputs to be manually segmented beforehand) would be a natural extension of this work. Additive component models of text have recently 5We note that only 8 of the 42 speech acts appeared with greater than 1% frequency in Stolcke et al.’s corpus. gained traction (Eisenstein et al., 2011; Paul, 2012; Paul and Dredze, 2012; Paul et al., 2013). To our knowledge, this is the first extension of supervised additive component models to a sequential task.6 7 Conclusions and Future Directions We have proposed a novel Joint, Additive, Sequential (JAS) model of conversational topics and speech acts. In contrast to previous approaches to modeling conversational exchanges, this model factors both the current topic and the current speech act into token emission and state transition probabilities. We demonstrated that this model consistently outperforms a univariate generative baseline that treats speech acts and topics ind</context>
</contexts>
<marker>Paul, Dredze, 2012</marker>
<rawString>Michael Paul and Mark Dredze. 2012. Factorial lda: Sparse multi-dimensional text models. In Advances in Neural Information Processing Systems 25, pages 2591–2599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Paul</author>
<author>Byron C Wallace</author>
<author>Mark Dredze</author>
</authors>
<title>What affects patient (dis)satisfaction? analyzing online doctor ratings with a joint topic-sentiment model.</title>
<date>2013</date>
<booktitle>In AAAI Workshop on Expanding the Boundaries of Health Informatics Using AI (HIAI).</booktitle>
<contexts>
<context position="4933" citStr="Paul et al., 2013" startWordPosition="782" endWordPosition="785">igators performed a randomized control trial in which they quantified change in communication patterns by tallying the number of information giving speech acts that fell under the ARV adherence topic. Without assigning both topics and speech acts to utterances, this analysis would not have been possible. In this work, we develop a novel componentbased generative model for bivariate, sequentially structured problems. Our approach extends the recently proposed Sparse Additive Generative (SAGE) model (Eisenstein et al., 2011) and similar recently developed additive models (Paul and Dredze, 2012; Paul et al., 2013) to the case of supervised sequential tasks to capture the joint conditional influence of topics and speech acts, both with respect to token generation and state transitions. For brevity, we refer to this generative Joint, Additive, Sequential model as JAS. In contrast to previous work on speech acts, JAS provides a single, coherent generative model of conversations. And because it is component-based, this model provides a flexible framework for analyzing communication patterns. We demonstrate that JAS outperforms a generative univariate baseline in topic/speech act prediction. Further, we aut</context>
<context position="33000" citStr="Paul et al., 2013" startWordPosition="5132" endWordPosition="5135">Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods into the proposed model (rather than relying on inputs to be manually segmented beforehand) would be a natural extension of this work. Additive component models of text have recently 5We note that only 8 of the 42 speech acts appeared with greater than 1% frequency in Stolcke et al.’s corpus. gained traction (Eisenstein et al., 2011; Paul, 2012; Paul and Dredze, 2012; Paul et al., 2013). To our knowledge, this is the first extension of supervised additive component models to a sequential task.6 7 Conclusions and Future Directions We have proposed a novel Joint, Additive, Sequential (JAS) model of conversational topics and speech acts. In contrast to previous approaches to modeling conversational exchanges, this model factors both the current topic and the current speech act into token emission and state transition probabilities. We demonstrated that this model consistently outperforms a univariate generative baseline that treats speech acts and topics independently. Furtherm</context>
</contexts>
<marker>Paul, Wallace, Dredze, 2013</marker>
<rawString>Michael J. Paul, Byron C. Wallace, and Mark Dredze. 2013. What affects patient (dis)satisfaction? analyzing online doctor ratings with a joint topic-sentiment model. In AAAI Workshop on Expanding the Boundaries of Health Informatics Using AI (HIAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Paul</author>
</authors>
<title>Mixed membership Markov models for unsupervised conversation modeling.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>94--104</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="32957" citStr="Paul, 2012" startWordPosition="5126" endWordPosition="5127">prising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods into the proposed model (rather than relying on inputs to be manually segmented beforehand) would be a natural extension of this work. Additive component models of text have recently 5We note that only 8 of the 42 speech acts appeared with greater than 1% frequency in Stolcke et al.’s corpus. gained traction (Eisenstein et al., 2011; Paul, 2012; Paul and Dredze, 2012; Paul et al., 2013). To our knowledge, this is the first extension of supervised additive component models to a sequential task.6 7 Conclusions and Future Directions We have proposed a novel Joint, Additive, Sequential (JAS) model of conversational topics and speech acts. In contrast to previous approaches to modeling conversational exchanges, this model factors both the current topic and the current speech act into token emission and state transition probabilities. We demonstrated that this model consistently outperforms a univariate generative baseline that treats spe</context>
</contexts>
<marker>Paul, 2012</marker>
<rawString>Michael J Paul. 2012. Mixed membership Markov models for unsupervised conversation modeling. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 94–104. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Raymond Perrault</author>
<author>James F Allen</author>
</authors>
<title>A planbased analysis of indirect speech acts.</title>
<date>1980</date>
<journal>Computational Linguistics,</journal>
<pages>6--3</pages>
<contexts>
<context position="3045" citStr="Perrault and Allen, 1980" startWordPosition="493" endWordPosition="496">tterance “Obama won the election” is topically political and is an example of an information giving speech act. “Did Obama win the election?”, meanwhile, belongs Table 1: An excerpt from a patient-doctor interaction, annotated with topic and speech act codes. The D and P roles denote doctor and patient, respectively. Conv. Mgmt. abbreviates conversation management; Ask Q. abbreviates ask question. to the same topic but is a question. Both aspects are necessary to understand conversation. Previous computational work on speech acts – which we review in Section 6 – has modeled them in isolation (Perrault and Allen, 1980; Stolcke et al., 1998; Stolcke et al., 2000; Kim et al., 2010), i.e., independent of topical content. But a richer model would account for both speech acts and the contextualizing topic of each utterance. To this end, we develop a novel joint, generative model of topics and speech acts. We focus on physician-patient communication as a motivating domain. This is of interest because 1765 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1765–1775, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics it is wide</context>
<context position="30084" citStr="Perrault and Allen (1980)" startWordPosition="4681" endWordPosition="4684">al., 2010), we report the median number of ARV/information giving utterances and corresponding 25th and 75th percentiles over the 58 control and intervention visits, as counted using the true (human) annotations and using the codes predicted by the MM and JAS models. These are reported in Table 3. The JAS model predictions better match the true labels in all except one case (the lower 25th for the controls, for which it predicts the same number as the MM model). 6 Related work There is a relatively long history of research into modeling conversational speech acts in computational linguistics. Perrault and Allen (1980) conducted pioneering work on computationally formalizing speech acts, though their work pre-dates statistical NLP and is therefore not directly relevant to the present work. Stolcke et al. (2000; 1998) proposed a probabilistic approach to modeling conversational speech acts based on the Hidden Markov Model (HMM) (Rabiner and Juang, 1986). They were interested in modeling an unrestricted set of conversations, and did not impose a hierarchy on the speech acts; they 1772 therefore enumerated many more speech acts (42) than we do in the present work (recall that we use 10 ‘high-level’ speech acts</context>
</contexts>
<marker>Perrault, Allen, 1980</marker>
<rawString>C Raymond Perrault and James F Allen. 1980. A planbased analysis of indirect speech acts. Computational Linguistics, 6(3-4):167–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashequl Qadir</author>
<author>Ellen Riloff</author>
</authors>
<title>Classifying sentences as speech acts in message board posts.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>748--758</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="31935" citStr="Qadir and Riloff, 2011" startWordPosition="4963" endWordPosition="4966">lar approach in which they used text visualization software to explore patterns of (inferred) topics in consultations. Another thread of research has investigated classifying speech acts in emails into one of a small set of “email speech acts”, e.g., request, propose, commit (Cohen et al., 2004; Goldstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) an</context>
</contexts>
<marker>Qadir, Riloff, 2011</marker>
<rawString>Ashequl Qadir and Ellen Riloff. 2011. Classifying sentences as speech acts in message board posts. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 748–758. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Rabiner</author>
<author>B Juang</author>
</authors>
<title>An introduction to hidden Markov models.</title>
<date>1986</date>
<journal>ASSP Magazine, IEEE,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="7645" citStr="Rabiner and Juang, 1986" startWordPosition="1212" endWordPosition="1215">= Ayt−,,yt (1) Emissions can be modeled via a multinomial that captures the conditional probabilities of tokens given labels. Denoting an utterance (an utterance comprises the words corresponding to a single speech act; see Section 4) at time t by ut and its label by yt, and making the standard naive assumption that words are generated independently conditioned on a label, we have: P(ut|yt) = 11 P(w|yt) = 11 Tyt,w (2) wEut wEut Both sets of parameters (the A’s and the -r’s) can be estimated straight-forwardly using maximum likelihood (i.e., using observed counts). We can use Viterbi decoding (Rabiner and Juang, 1986) to make predictions for new sequences, as usual. To make both topic and speech act predictions, we simply induce models for each and make predictions independently. 3 JAS: A Joint, Additive, Sequential Model An obvious shortcoming of the simple MM model outlined above is that it treats topics and speech acts 1766 as statistically independent. They are not (as confirmed at statistical significance p &lt; .001 using a χ2 test). One would prefer a more expressive model that conditions topic and speech act transitions as well as the production of utterances jointly on both the current topic and the </context>
<context position="13473" citStr="Rabiner and Juang, 1986" startWordPosition="2180" endWordPosition="2183">h act at time t are conditionally independent given the preceding topic and speech act (yt−1 and st−1). This is intuitively agreeable because time intervenes as a blocking factor; conditioning the current topic on the current speech act (or vice versa) would contradict the fact that these occur simultaneously. Instead, the correlation is induced by the preceding topic/speech act pair. (That said, this is still a simplifying assumption, as one may instead choose to model speech act selection as conditional on topic (Traum and Larsson, 2003).) Predictions can again be made via Viterbi decoding (Rabiner and Juang, 1986) over a matrix of pairs of joint topic/speech act states. The strategy of modeling (additive) components allows JAS to avoid problems due to sparsity in this large output space. Model parameters can be estimated using standard optimization techniques. We fix the ‘background’ frequencies 0, 7rY, 7rS to the log of the corresponding observed proportions of words, topics and speech acts, respectively. For the remaining parameters, one can use descent-based optimization methods. The partial derivative for the topic-to-topic transition component Qy,y0 with respect to the likelihood, for example, is:</context>
<context position="30424" citStr="Rabiner and Juang, 1986" startWordPosition="4734" endWordPosition="4738"> true labels in all except one case (the lower 25th for the controls, for which it predicts the same number as the MM model). 6 Related work There is a relatively long history of research into modeling conversational speech acts in computational linguistics. Perrault and Allen (1980) conducted pioneering work on computationally formalizing speech acts, though their work pre-dates statistical NLP and is therefore not directly relevant to the present work. Stolcke et al. (2000; 1998) proposed a probabilistic approach to modeling conversational speech acts based on the Hidden Markov Model (HMM) (Rabiner and Juang, 1986). They were interested in modeling an unrestricted set of conversations, and did not impose a hierarchy on the speech acts; they 1772 therefore enumerated many more speech acts (42) than we do in the present work (recall that we use 10 ‘high-level’ speech acts).5 Their model has served as the baseline approach in the present work. Stolcke et al. also considered jointly performing speech recognition and speech act classification. Others have investigated visual structures of patient-provider interactions to qualitatively assess communication in care. Specifically, (Cretchley et al., 2010) lever</context>
</contexts>
<marker>Rabiner, Juang, 1986</marker>
<rawString>Lawrence Rabiner and B Juang. 1986. An introduction to hidden Markov models. ASSP Magazine, IEEE, 3(1):4–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Debra Roter</author>
<author>Susan Larson</author>
</authors>
<title>The roter interaction analysis system (rias): utility and flexibility for analysis of medical interactions. Patient education and counseling,</title>
<date>2002</date>
<pages>46--4</pages>
<contexts>
<context position="18194" citStr="Roter and Larson, 2002" startWordPosition="2819" endWordPosition="2822">yze communication about sexual risk behavior (Laws et al., 2011a); elucidate the association of visit length with constructs of patient-centeredness (Laws et al., 2011b); and to describe providerpatient communication regarding ARV adherence compared with communication about other issues (Laws et al., 2012). GMIAS annotation is described at length elsewhere,2 but we summarize it here for completeness. GMIAS segments conversation into utterances. An utterance is here defined as a single completed speech act. Previous coding systems have simply defined an utterance as conveying a single thought (Roter and Larson, 2002) or any independent or unrestrictive dependent clause of a sentence (Ford and Ford, 1995). Stolcke et al. (2000) followed Meteer et al. (1995) in using “sentence-level units”. These definitions provide helpful guidance to coders, but many speech acts are poorly formed grammatically, and cannot be described as a “clause”. Further, some speech acts cannot be said to convey a “thought” (or sentence) at all, but rather are pre-syntactical (e.g., interjections and non-lexical utterances like laughter). In any case, most natural segmentations of conversations probably largely agree with intuition, a</context>
</contexts>
<marker>Roter, Larson, 2002</marker>
<rawString>Debra Roter and Susan Larson. 2002. The roter interaction analysis system (rias): utility and flexibility for analysis of medical interactions. Patient education and counseling, 46(4):243–251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Searle</author>
</authors>
<title>Speech acts: An essay in the philosophy of language. Cambridge university press.</title>
<date>1969</date>
<contexts>
<context position="1946" citStr="Searle (1969" startWordPosition="319" endWordPosition="320">is it two years from now that Biomedical Ask Q. they’re going to be doing the repeat? P Yeah. Biomedical Conv. Mgmt. Logistics Commissive Socializing Directive Socializing Give Info. Socializing Humor/Levity D We’ll do the repeat coloscopy Biomedical Give Info. in about two years. 1 Introduction Communication involves at least two aspects: the words one says and the acts one performs in saying them. Examples of the latter include asking questions, issuing commands, and so on. These are referred to as speech acts under the sociolinguistic theory of Austin (1955), which was further developed by Searle (1969; 1985). Recognizing speech acts is crucial to understanding communication because a speaker’s meaning is only partially captured by the words they use; much of their intent is expressed implicitly via speech acts (Searle, 1969). On this view, conversational utterances can be assigned both a topic and a speech act. The former describes the subject matter of what was said and the latter captures the “social act” (e.g., promising) performed by saying it. For example, the utterance “Obama won the election” is topically political and is an example of an information giving speech act. “Did Obama wi</context>
</contexts>
<marker>Searle, 1969</marker>
<rawString>John R Searle. 1969. Speech acts: An essay in the philosophy of language. Cambridge university press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Searle</author>
</authors>
<title>Expression and meaning: Studies in the theory of speech acts.</title>
<date>1985</date>
<publisher>Cambridge University Press.</publisher>
<marker>Searle, 1985</marker>
<rawString>John R Searle. 1985. Expression and meaning: Studies in the theory of speech acts. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Noah Coccaro</author>
<author>Daniel Jurafsky</author>
<author>Rachel Martin</author>
<author>Marie Meteer</author>
<author>Klaus Ries</author>
<author>Paul Taylor</author>
<author>Carol Van EssDykema</author>
</authors>
<title>Dialog act modeling for conversational speech.</title>
<date>1998</date>
<booktitle>In AAAI Spring Symposium on Applying Machine Learning to Discourse Processing,</booktitle>
<pages>98--105</pages>
<marker>Stolcke, Shriberg, Bates, Coccaro, Jurafsky, Martin, Meteer, Ries, Taylor, Van EssDykema, 1998</marker>
<rawString>Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates, Noah Coccaro, Daniel Jurafsky, Rachel Martin, Marie Meteer, Klaus Ries, Paul Taylor, and Carol Van EssDykema. 1998. Dialog act modeling for conversational speech. In AAAI Spring Symposium on Applying Machine Learning to Discourse Processing, pages 98–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Klaus Ries</author>
<author>Noah Coccaro</author>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Daniel Jurafsky</author>
<author>Paul Taylor</author>
<author>Rachel Martin</author>
<author>Carol Van Ess-Dykema</author>
<author>Marie Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational linguistics,</journal>
<pages>26--3</pages>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van Ess-Dykema, Meteer, 2000</marker>
<rawString>Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema, and Marie Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational linguistics, 26(3):339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
<author>Khashayar Rohanimanesh</author>
</authors>
<title>Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data.</title>
<date>2007</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>8--693</pages>
<contexts>
<context position="24583" citStr="Sutton et al., 2007" startWordPosition="3781" endWordPosition="3784">nswering such exploratory questions. Indeed, perhaps the main strength of the additive component based sequential model we have proposed here is that it will allow us to easily incorporate physician-specific parameters that capture deviations in provider speech act and/or topic transition patterns. Further, we may soon have access to many unannotated transcripts, and we would like to learn from these; generative approaches allow straight-forward exploitation of unlabeled data. For these reasons, we did not experiment with discriminative models, e.g., Dynamic Conditional Random Fields (DCRFs) (Sutton et al., 2007) for this work. 5.1 Cross-fold Validation Our aim is to measure model performance in terms of correctly identifying both the topic and speech act corresponding to each utterance. We quantify this via the F-score calculated for each topic/speech act pair that is observed at least once. One can see in Table 2 that many such pairs have low prevalence; this can result in undefined F-scores (e.g., when no utterances are assigned to a given pair). In this case, it is reasonable to treat these as zero values, as is commonly done (Forman and Scholz, 2010). This penalizes models when they completely fa</context>
</contexts>
<marker>Sutton, McCallum, Rohanimanesh, 2007</marker>
<rawString>Charles Sutton, Andrew McCallum, and Khashayar Rohanimanesh. 2007. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. The Journal of Machine Learning Research, 8:693–723.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Teutsch</author>
</authors>
<title>Patient-doctor communication. The medical clinics of</title>
<date>2003</date>
<journal>North America,</journal>
<volume>87</volume>
<issue>5</issue>
<contexts>
<context position="3787" citStr="Teutsch, 2003" startWordPosition="609" endWordPosition="610">account for both speech acts and the contextualizing topic of each utterance. To this end, we develop a novel joint, generative model of topics and speech acts. We focus on physician-patient communication as a motivating domain. This is of interest because 1765 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1765–1775, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics it is widely appreciated that effective communication is an integral part of clinical practice (Irwin and Richardson, 2006; Makoul, 2001; Teutsch, 2003). We provide an excerpt of a conversation between a patient and their doctor annotated with topics and speech acts in Table 1. Such annotations can provide substantive insights into how doctors communicate with patients (Ong et al., 1995). A concrete example of this is the use of topic and speech act codes to assess the efficacy of an intervention meant to influence physician-patient communication regarding adherence to antiretroviral (ARV) medication (Wilson et al., 2010). To measure the effect of the intervention, investigators performed a randomized control trial in which they quantified ch</context>
</contexts>
<marker>Teutsch, 2003</marker>
<rawString>Carol Teutsch. 2003. Patient-doctor communication. The medical clinics of North America, 87(5):1115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
<author>Staffan Larsson</author>
</authors>
<title>The information state approach to dialogue management.</title>
<date>2003</date>
<booktitle>In Current and new directions in discourse and dialogue,</booktitle>
<pages>325--353</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="13394" citStr="Traum and Larsson, 2003" startWordPosition="2167" endWordPosition="2170">, yt−1) (9) As implied by Figure 1, this model assumes that the topic and speech act at time t are conditionally independent given the preceding topic and speech act (yt−1 and st−1). This is intuitively agreeable because time intervenes as a blocking factor; conditioning the current topic on the current speech act (or vice versa) would contradict the fact that these occur simultaneously. Instead, the correlation is induced by the preceding topic/speech act pair. (That said, this is still a simplifying assumption, as one may instead choose to model speech act selection as conditional on topic (Traum and Larsson, 2003).) Predictions can again be made via Viterbi decoding (Rabiner and Juang, 1986) over a matrix of pairs of joint topic/speech act states. The strategy of modeling (additive) components allows JAS to avoid problems due to sparsity in this large output space. Model parameters can be estimated using standard optimization techniques. We fix the ‘background’ frequencies 0, 7rY, 7rS to the log of the corresponding observed proportions of words, topics and speech acts, respectively. For the remaining parameters, one can use descent-based optimization methods. The partial derivative for the topic-to-to</context>
</contexts>
<marker>Traum, Larsson, 2003</marker>
<rawString>David R Traum and Staffan Larsson. 2003. The information state approach to dialogue management. In Current and new directions in discourse and dialogue, pages 325–353. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Van Gael</author>
<author>Yee Whye Teh</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>The infinite factorial hidden Markov model.</title>
<date>2008</date>
<booktitle>In Neural Information Processing Systems,</booktitle>
<volume>21</volume>
<marker>Van Gael, Teh, Ghahramani, 2008</marker>
<rawString>Jurgen Van Gael, Yee Whye Teh, and Zoubin Ghahramani. 2008. The infinite factorial hidden Markov model. In Neural Information Processing Systems, volume 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ira B Wilson</author>
<author>M Barton Laws</author>
<author>Steven A Safren</author>
<author>Yoojin Lee</author>
<author>Minyi Lu</author>
<author>William Coady</author>
<author>Paul R Skolnik</author>
<author>William H Rogers</author>
</authors>
<title>Provider-focused intervention increases adherence-related dialogue, but does not improve antiretroviral therapy adherence in persons with HIV. Journal of acquired immune deficiency syndromes,</title>
<date>2010</date>
<pages>53--3</pages>
<contexts>
<context position="4264" citStr="Wilson et al., 2010" startWordPosition="682" endWordPosition="685">ely appreciated that effective communication is an integral part of clinical practice (Irwin and Richardson, 2006; Makoul, 2001; Teutsch, 2003). We provide an excerpt of a conversation between a patient and their doctor annotated with topics and speech acts in Table 1. Such annotations can provide substantive insights into how doctors communicate with patients (Ong et al., 1995). A concrete example of this is the use of topic and speech act codes to assess the efficacy of an intervention meant to influence physician-patient communication regarding adherence to antiretroviral (ARV) medication (Wilson et al., 2010). To measure the effect of the intervention, investigators performed a randomized control trial in which they quantified change in communication patterns by tallying the number of information giving speech acts that fell under the ARV adherence topic. Without assigning both topics and speech acts to utterances, this analysis would not have been possible. In this work, we develop a novel componentbased generative model for bivariate, sequentially structured problems. Our approach extends the recently proposed Sparse Additive Generative (SAGE) model (Eisenstein et al., 2011) and similar recently</context>
<context position="17565" citStr="Wilson et al., 2010" startWordPosition="2729" endWordPosition="2732">mation 15521 (0.067) Psycho-Social; Humor/Levity 63 (0.000) Psycho-Social; Missing/other 1199 (0.005) Psycho-Social; Social-Ritual 36 (0.000) Socializing; Ask Q 1283 (0.006) Socializing; Commissive 79 (0.000) Socializing; Continuation 85 (0.000) Socializing; Conv. Management 2166 (0.009) Socializing; Directive 222 (0.001) Socializing; Empathy 73 (0.000) Socializing; Give Information 8981 (0.039) Socializing; Humor/Levity 306 (0.001) Socializing; Missing/other 849 (0.004) Socializing; Social-Ritual 1685 (0.007) Table 2: Topic/speech act pairs and their counts. context of an intervention trial (Wilson et al., 2010); analyze communication about sexual risk behavior (Laws et al., 2011a); elucidate the association of visit length with constructs of patient-centeredness (Laws et al., 2011b); and to describe providerpatient communication regarding ARV adherence compared with communication about other issues (Laws et al., 2012). GMIAS annotation is described at length elsewhere,2 but we summarize it here for completeness. GMIAS segments conversation into utterances. An utterance is here defined as a single completed speech act. Previous coding systems have simply defined an utterance as conveying a single tho</context>
<context position="27311" citStr="Wilson et al., 2010" startWordPosition="4235" endWordPosition="4238"> mean speech act F-score of .516. JAS begets a marginal mean topic F-score of .661 and a marginal mean speech act F-score of .544; hence the JAS model incurs an F-score loss of .006 (a 0.9% decrease) with respect to marginal topic code prediction, but improves the marginal speech act F-score by .028 (a 5.4% increase). 5.2 (Re-)Analysis of Randomized Control Trial We also evaluated performance by tallying model predictions over 116 held-out cases collected from a randomized, cross-over study of an intervention aimed at improving physicians knowledge of patients anti-retroviral (ARV) adherence (Wilson et al., 2010). The intervention was a report given to the physician before a routine office visit that contained information regarding the patients ARV usage and their beliefs about ARV therapy. To explore the efficacy of this intervention, 58 paired (116 total) audio recorded visits were annotated with GMIAS; 58 correspond to visits before which the provider was not provided with the report (control cases), while the other 58 correspond to visits before which they were (intervention cases). Wilson et al. (2010) demonstrated that the intervention indeed increased adherence-related dialogue, and specificall</context>
<context position="29469" citStr="Wilson et al., 2010" startWordPosition="4577" endWordPosition="4580"> JAS models over the aforementioned 360 annotated visits and then used this model to generate topic and speech act code predictions for the utterances comprising the 116 held-out visits used for the analysis (these were not part of the training set). We then assessed the direction and magnitude of the change in the number of ARV adherence/information giving utterances in the paired control versus intervention cases. We compared the results for this analysis calculated using the true (manually assigned) codes to the results calculated using the predicted codes. Following the original analysis (Wilson et al., 2010), we report the median number of ARV/information giving utterances and corresponding 25th and 75th percentiles over the 58 control and intervention visits, as counted using the true (human) annotations and using the codes predicted by the MM and JAS models. These are reported in Table 3. The JAS model predictions better match the true labels in all except one case (the lower 25th for the controls, for which it predicts the same number as the MM model). 6 Related work There is a relatively long history of research into modeling conversational speech acts in computational linguistics. Perrault a</context>
</contexts>
<marker>Wilson, Laws, Safren, Lee, Lu, Coady, Skolnik, Rogers, 2010</marker>
<rawString>Ira B Wilson, M Barton Laws, Steven A Safren, Yoojin Lee, Minyi Lu, William Coady, Paul R Skolnik, and William H Rogers. 2010. Provider-focused intervention increases adherence-related dialogue, but does not improve antiretroviral therapy adherence in persons with HIV. Journal of acquired immune deficiency syndromes, 53(3):338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Renxian Zhang</author>
<author>Dehong Gao</author>
<author>Wenjie Li</author>
</authors>
<title>Towards scalable speech act recognition in twitter: Tackling insufficient training data. EACL</title>
<date>2012</date>
<pages>18</pages>
<contexts>
<context position="32014" citStr="Zhang et al., 2012" startWordPosition="4976" endWordPosition="4979">(inferred) topics in consultations. Another thread of research has investigated classifying speech acts in emails into one of a small set of “email speech acts”, e.g., request, propose, commit (Cohen et al., 2004; Goldstein et al., 2006). Cohen et al. (2004) demonstrated that good performance can be achieved for this task via existing text classification technologies. Elsewhere, researchers have explored automatically inferring “speech acts” in various other online social mediums, including message board posts (Qadir and Riloff, 2011), Wikipedia talk pages (Ferschke et al., 2012) and Twitter (Zhang et al., 2012). A separate line of inquiry concerns classifying dialogue acts in chat. Researchers have attempted dialogue act classification both for 1-on-1 (Kim et al., 2010) and multi-party (Kim et al., 2012; Clark and Popescu-Belis, 2004) online chats. Ang et al. (2005) considered the task of jointly segmenting and classifying utterances comprising multiparty meetings, while Hsueh and Moore (2006) proposed analogous methods for topic segmentation and labeling (other works on topic segmentation include (Galley et al., 2003) and (Eisenstein and Barzilay, 2008)). Incorporating such segmentation methods int</context>
</contexts>
<marker>Zhang, Gao, Li, 2012</marker>
<rawString>Renxian Zhang, Dehong Gao, and Wenjie Li. 2012. Towards scalable speech act recognition in twitter: Tackling insufficient training data. EACL 2012, page 18.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
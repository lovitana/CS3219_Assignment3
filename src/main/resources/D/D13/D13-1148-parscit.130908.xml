<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001005">
<title confidence="0.968158">
Naive Bayes Word Sense Induction
</title>
<author confidence="0.996369">
Do Kook Choe Eugene Charniak
</author>
<affiliation confidence="0.99827">
Brown University Brown University
</affiliation>
<address confidence="0.893545">
Providence, RI Providence, RI
</address>
<email confidence="0.998848">
dc65@cs.brown.edu ec@cs.brown.edu
</email>
<sectionHeader confidence="0.980064" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995729571428572">
We introduce an extended naive Bayes model
for word sense induction (WSI) and apply it to
a WSI task. The extended model incorporates
the idea the words closer to the target word are
more relevant in predicting its sense. The pro-
posed model is very simple yet effective when
evaluated on SemEval-2010 WSI data.
</bodyText>
<sectionHeader confidence="0.995231" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999876703703704">
The task of word sense induction (WSI) is to find
clusters of tokens of an ambiguous word in an un-
labeled corpus that have the same sense. For in-
stance, given a target word “crane,” a good WSI sys-
tem should find a cluster of tokens referring to avian
cranes and another referring to mechanical cranes.
We believe that neighboring words contain enough
information that these clusters can be found from
plain texts.
WSI is related to word sense disambiguation
(WSD). In a WSD task, a system learns a sense clas-
sifier in a supervised manner from a sense-labeled
corpus. The performance of the learned classifier
is measured on some unseen data. WSD systems
perform better than WSI systems, but building la-
beled data can be prohibitively expensive. In addi-
tion, WSD systems are not suitable for newly cre-
ated words, new senses of existing words, or domain-
specific words. On the other hand, WSI systems can
learn new senses of words directly from texts because
these programs do not rely on a predefined set of
senses.
In Section 2 we describe relevant previous work. In
Section 3 and 4 we introduce the naive Bayes model
for WSI and inference schemes for the model. In Sec-
tion 5 we evaluate the model on SemEval-2010 data.
In Section 6 we conclude.
</bodyText>
<sectionHeader confidence="0.998956" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999643576923077">
Yarowsky (1995) introduces a semi-supervised
bootstrapping algorithm with two assumptions
that rivals supervised algorithms: one-sense-per-
collocation and one-sense-per-discourse. But this
algorithm cannot easily be scaled up because for
any new ambiguous word humans need to pick
a few seed words, which initialize the algorithm.
In order to automate the semi-supervised system,
Eisner and Karakos (2005) propose an unsupervised
bootstrapping algorithm. Their system tries many
different seeds for bootstrapping and chooses the
“best” classifier at the end. Eisner and Karakos&apos;s
algorithm is limited in that their system is designed
for disambiguating words that have only 2 senses.
Bayesian WSI systems have been developed by
several authors. Brody and Lapata (2009) apply
Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) to WSI. They run a topic modeling algorithm
on texts with some fixed number of topics that
correspond to senses and induce a cluster by finding
target words assigned to the same topic. Their
system is evaluated on SemEval-2007 noun data
(Agirre and Soroa, 2007). Lau et al. (2012) apply
a nonparametric model, Hierarchical Dirichlet Pro-
cesses (HDP), to SemEval-2010 data (Manandhar et
al., 2010).
</bodyText>
<sectionHeader confidence="0.994086" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.9998712">
Following Yarowsky (1995), we assume that a word
in a document has one sense. Multiple occurrences
of a word in a document refer to the same object
or concept. The naive Bayes model is well suited
for this one-sense-per-document assumption. Each
document has one topic corresponding to the sense of
the target word that needs disambiguation. Context
words in a document are drawn from the conditional
distribution of words given the sense. Context words
are assumed to be independent from each other given
</bodyText>
<page confidence="0.889382">
1433
</page>
<bodyText confidence="0.665520333333333">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1433–1437,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
the sense, which is far from being true yet effective.
</bodyText>
<subsectionHeader confidence="0.991865">
3.1 Naive Bayes
</subsectionHeader>
<bodyText confidence="0.999827">
The naive Bayes model assumes that every word in a
document is generated independently from the con-
ditional distribution of words given a sense, p(w|s).
The mathematical definition of the naive Bayes
model is as follows:
</bodyText>
<equation confidence="0.9991098">
p(w) = X p(s, w) = X p(s)p(w|s)
s s
X= Y
s p(s) p(w|s), (1)
w
</equation>
<bodyText confidence="0.999579">
where w is a vector of words in the document. With
the model, a new document can be easily labeled
using the following classifier:
</bodyText>
<equation confidence="0.997071333333333">
Y
p(s) p(w|s), (2)
w
</equation>
<bodyText confidence="0.999688142857143">
where s&apos; is the label of the new document. In con-
trast to LDA-like models, it is easy to construct
the closed form classifier from the model. The pa-
rameters of the model, p(s) and p(w|s), can be
learned by maximizing the probability of the corpus,
p(d) = Qd p(d) = Qw p(w) where d is a vector of
documents and d = w.
</bodyText>
<subsectionHeader confidence="0.9988">
3.2 Distance Incorporated Naive Bayes
</subsectionHeader>
<bodyText confidence="0.9999865">
Intuitively, context words near a target word are
more indicative of its sense than ones that are far-
ther away. To account for this intuition, we propose
a more sophisticated model that uses the distance
between a context word and a target word. Before
introducing the new model, we define a probability
distribution, f(w|s), that incorporates distances as
follows:
</bodyText>
<equation confidence="0.9873745">
p(w|s)l(w)
f(w|s) = Pw EW p(w&apos;|s)l(w) ,(3)
</equation>
<bodyText confidence="0.9868102">
where l(w) = 1
dist(w)= . W is a set of types in the cor-
pus. x is a tunable parameter that takes nonnegative
real values. With the new probability distribution,
the model and the classifier become:
</bodyText>
<equation confidence="0.99627">
X Y
p(w) = p(s) f(w|s) (4)
s w
Y
p(s) f(w|s), (5)
w
</equation>
<bodyText confidence="0.999905714285714">
where f(w|s) replaces p(w|s). The naive Bayes
model is a special case; set x = 0. The new model
puts more weight on context words that are close
to the target word. The distribution of words that
are farther away approaches the uniform distribu-
tion. l(w) smoothes the distribution more as x be-
comes larger.
</bodyText>
<sectionHeader confidence="0.995437" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.999878">
Given the generative model, we employ two inference
algorithms to learn the sense distribution and word
distributions given a sense. Expectation Maximiza-
tion (EM) is a natural choice for the naive Bayes
(Dempster et al., 1977). When initialized with ran-
dom parameters, EM gets stuck at local maxima. To
avoid local maxima, we use a Gibbs sampler for the
plain naive Bayes to learn parameters that initialize
EM.
</bodyText>
<sectionHeader confidence="0.99673" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.910047">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999973875">
We evaluate the model on SemEval-2010 WSI task
data (Manandhar et al., 2010). The task has 100
target words, 50 nouns and 50 verbs. For each target
word, there are training and test documents. Table
1 have details. The training and test data are plain
texts without sense tags. For evaluation, the inferred
sense labels are compared with human annotations.
To tune some parameters we use the trial data of
</bodyText>
<table confidence="0.9987785">
Training Testing Senses (#)
All 879807 8915 3.79
Nouns 716945 5285 4.46
Verbs 162862 3630 3.12
</table>
<tableCaption confidence="0.999592">
Table 1: Details of SemEval-2010 data
</tableCaption>
<bodyText confidence="0.99981025">
SemEval-2010. The trial data consists of training
and test portions of 4 verbs. On average there are
137 documents for each target word in the training
part of the trial data.
</bodyText>
<subsectionHeader confidence="0.995954">
5.2 Task
</subsectionHeader>
<bodyText confidence="0.999947214285714">
Participants induce clusters from the training data
and use them to label the test data. Resources other
than NLP tools for morphology and syntax such as
lemmatizer, POS-tagger, and parser are not allowed.
Tuning parameters and inducing clusters are only
allowed during the training phase. After training,
participants submit their sense-labeled test data to
organizers.
LDA models are not compatible with the scoring
rules for the SemEval-2010 competition, and that is
the work against which we most want to compare.
These rules require that training be done strictly be-
fore the testing is done. Note however that LDA re-
quires learning the mixture weights of topics for each
</bodyText>
<equation confidence="0.820205">
s&apos; = argmax
s
s&apos; = argmax
s
</equation>
<page confidence="0.908095">
1434
</page>
<bodyText confidence="0.999303">
individual document p(topic  |document). These are,
of course, learned during training. But the docu-
ments in the testing corpus have never been seen
before, so clearly their topic mixture weights are not
learned during training, and thus not learned at all.
The way to overcome this is by training on both
train and test documents, but this is exactly what
SemEval-2010 forbids.
</bodyText>
<subsectionHeader confidence="0.989779">
5.3 Implementation Details
</subsectionHeader>
<bodyText confidence="0.999974769230769">
The documents are tokenized and stemmed by
Stanford tokenizer and stemmer. Stop words and
punctuation in the training and test data are
discarded. Words that occur at most 10 times are
discarded from the training data. Context words
within a window of 50 about a target word are used
to construct a bag-of-words.
When a target word appears more than once
in a document, the distance between that target
word and a context word is ambiguous. We define
this distance to be minimum distance between a
context word and an instance of the target word.
For example, the word “chip” appears 3 times. For
</bodyText>
<listItem confidence="0.835019666666667">
· · · of memory chips . Currently, chips are pro-
duced by shining light through a mask to produce
an image on the chip , much as · · ·
</listItem>
<tableCaption confidence="0.391762">
Example 1: an excerpt from “chip” test data
</tableCaption>
<bodyText confidence="0.999720647058823">
a context word, e.g., “shining” there are three pos-
sible distances: 8 away from the first “chip,” 4 away
from the second “chip” and 11 away from the last
“chip.” We set the distance of “shining” from the
target to 4.
We model each target word individually. We set α,
a Dirichlet prior for senses, to 0.02 and 0, a Dirichlet
prior for contextual words, to 0.1 for the Gibbs sam-
pler as in Brody and Lapata (2009). We initialize
EM with parameters learned from the sampler. We
run EM until the likehood changes less than 1%. We
run the sampler 2000 iterations including 1000 itera-
tions of burn-in: 10 samples at an interval of 100 are
averaged. For comparison, we also evaluate EM with
random initialization. All reported scores (described
in Section 5.4) are averaged over ten different runs
of the program.1
</bodyText>
<subsubsectionHeader confidence="0.799351">
5.3.1 Tuning Parameters
</subsubsectionHeader>
<bodyText confidence="0.99993325">
Two parameters, the number of senses and x of
the function l(w), need to be determined before run-
ning the program. To find a good setting we do grid
search on the trial data with the number of senses
</bodyText>
<footnote confidence="0.7042215">
&apos;Code used for experiments is available for download at
http://cs.brown.edu/~dc65/.
</footnote>
<bodyText confidence="0.999885416666667">
ranging from 2 to 5 and x ranging from 0 to 1.1 with
an interval 0.1. Due to the small size of the training
portion of the trial data, words that occur once are
thrown out in the training portion. All the other pa-
rameters are as described in Section 5.3. We choose
(4, 0.4), which achieves the highest supervised recall.
See Table 2 for the performance of the model with
various parameter settings. With a fixed value of x,
a column is nearly unimodal in the number of senses
and vice versa. x = 0 is not optimal and there is
some noticeable difference between scores with opti-
mal x and scores with x = 0.
</bodyText>
<subsectionHeader confidence="0.931342">
5.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.999942125">
We compare our system to other WSI systems and
discuss two metrics for unsupervised evaluation (V-
Measure, paired F-Score) and one metric for super-
vised evaluation (supervised recall). We refer to the
true group of tokens as a gold class and to an induced
group of tokens as a cluster. We refer to the model
learned with the sampler and EM as NB, and to the
model learned with EM only as NB0.
</bodyText>
<subsectionHeader confidence="0.2522275">
5.4.1 Short Descriptions of Other WSI
Systems Evaluated on SemEval-2010
</subsectionHeader>
<bodyText confidence="0.999929666666667">
The baseline assigns every instance of a target
word with the most frequent sense (MFS). UoY runs
a clustering algorithm on a graph with words as
nodes and co-occurrences between words as edges
(Korkontzelos and Manandhar, 2010). Hermit ap-
proximates co-occurrence space with Random Index-
ing and applies a hybrid of k-means and Hierarchical
Agglomerate Clustering to co-occurrence space (Ju-
rgens and Stevens, 2010). NMFtiib factors a matrix
using nonnegative matrix factorization and runs a
clustering algorithm on test instances represented by
factors (Van de Cruys et al., 2011).
</bodyText>
<subsectionHeader confidence="0.586653">
5.4.2 V-Measure
</subsectionHeader>
<bodyText confidence="0.999893875">
V-Measure computes the quality of induced clus-
ters as the harmonic mean of two values, homo-
geneity and completeness. Homogeneity measures
whether instances of a cluster belong to a single gold
class. Completeness measures whether instances of a
gold class belong to a cluster. V-Measure is between
0 and 1; higher is better. See Table 3 for details of
V-Measure evaluation (#cl is the number of induced
clusters).
With respect to V-Measure, NB performs much
better than NB0. This holds for paired F-Score and
supervised recall evaluations. The sampler improves
the log-likelihood of NB by 3.8% on average (4.8%
on nouns and 2.9% on verbs).
Pedersen (2010) points out that it is possible to
increase the V-Measure of bad models by increasing
</bodyText>
<page confidence="0.970831">
1435
</page>
<table confidence="0.9975684">
#s \ x 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1
2 74.73 74.76 74.41 74.57 74.06 74.07 74.18 74.33 74.14 74.22 74.15 74.52
3 74.60 74.71 75.21 75.46 75.21 75.57 75.61 75.32 75.53 75.56 74.98 74.79
4 74.52 75.06 74.97 75.14 76.02 75.51 75.74 75.51 75.59 75.51 75.37 75.35
5 73.40 73.88 74.93 75.13 74.79 74.68 74.71 74.49 75.11 74.94 74.86 75.25
</table>
<tableCaption confidence="0.9657165">
Table 2: Performance of the model with various parameters: supervised recall on the trial data. The best value from
each row is bold-faced. The scores are averaged over 100 runs.
</tableCaption>
<table confidence="0.999896">
VM(%) all nouns verbs #cl
NB 18.0 23.7 9.9 3.42
NB0 14.9 19.0 9.0 3.77
Hermit 16.2 16.7 15.6 10.78
UoY 15.7 20.6 8.5 11.54
NMFlib 11.8 13.5 9.4 4.80
MFS 0.0 0.0 0.0 1.00
</table>
<tableCaption confidence="0.99995">
Table 3: Unsupervised evaluation: V-Measure
</tableCaption>
<bodyText confidence="0.999756857142857">
the number of clusters. But increasing the number
of clusters harms paired F-Score, which results in
bad supervised recalls. NB attains a very high V-
Measure with few induced clusters, which indicates
that those clusters are high quality. Other systems
use more induced clusters but fail to attain the V-
Measure of NB.
</bodyText>
<subsubsectionHeader confidence="0.829186">
5.4.3 Paired F-Score
</subsubsectionHeader>
<bodyText confidence="0.9997285">
Paired F-Score is the harmonic mean of paired re-
call and paired precision. Paired recall is fraction of
pairs belonging to the same gold class that belong
to the same cluster. Paired precision is fraction of
pairs belonging to the same cluster that belong to
the same class. See Table 4 for details of paired F-
Score evaluation.
As with V-Measure, it is possible to attain a high
paired F-Score by producing only one cluster. The
baseline, MFS, attains 100% paired recall, which to-
gether with the poor performance of WSI systems
makes its paired F-Score difficult to beat. V-Measure
and paired F-Score are meaningful when systems
produce about the same numbers of clusters as the
numbers of classes and attain high scores on these
metrics.
</bodyText>
<table confidence="0.998561">
FS(%) all nouns verbs #cl
MFS 63.5 57.0 72.7 1.00
NB 52.9 52.5 53.5 3.42
NB0 46.8 47.4 46.0 3.77
UoY 49.8 38.2 66.6 11.54
NMFlib 45.3 42.2 49.8 4.80
Hermit 26.7 24.4 30.1 10.78
</table>
<tableCaption confidence="0.999856">
Table 4: Unsupervised evaluation: paired F-Score
</tableCaption>
<subsubsectionHeader confidence="0.76825">
5.4.4 Supervised Recall
</subsubsectionHeader>
<bodyText confidence="0.998541">
For the supervised task, the test data is split into
two groups: one for mapping clusters to classes and
the other for standard WSD evaluation. 2 differ-
ent split schemes (80% mapping, 20% evaluation and
60% mapping, 40% evaluation) are evaluated. 5 ran-
dom splits are averaged for each split scheme. Map-
ping is induced automatically by the program pro-
vided by organizers. See Table 5 for details of super-
vised recall evaluation (#s is the average number of
classes mapped from clusters).2
</bodyText>
<table confidence="0.998429857142857">
SR(%) all nouns verbs #s
NB 65.4 62.6 69.5 1.72
NB0 63.5 59.8 69.0 1.76
NMFlib 62.6 57.3 70.2 1.82
UoY 62.4 59.4 66.8 1.51
MFS 58.7 53.2 66.6 1.00
Hermit 58.3 53.6 65.3 2.06
</table>
<tableCaption confidence="0.9917825">
Table 5: Supervised evaluation: supervised recall, 80%
mapping and 20% evaluation
</tableCaption>
<bodyText confidence="0.9999313">
Overall our system performs better than other sys-
tems with respect to supervised recall. When a sys-
tem has higher V-Measure and paired F-Score on
nouns than another system, it achieves a higher su-
pervised recall on nouns too. However, this behav-
ior is not observed on verbs. For example, NB has
higher V-Measure and paired F-Score on verbs than
NMFlib but NB attains a lower supervised recall on
verbs than NMFlib. It is difficult to see which verbs
clusters are better than some other clusters.
</bodyText>
<sectionHeader confidence="0.999376" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9982405">
Of the four SemEval-2010 evaluation metrics, and
restricting ourselves to systems obeying the evalua-
tion conditions for that competition, our new model
achieves new best results on three. The exception is
paired F-Score. As we note earlier, this metric tends
to assign very high scores when every word receives
only one sense, and our model is bested by the base-
line system that does exactly that.
</bodyText>
<footnote confidence="0.565087">
260-40 split is omitted here due to almost identical result.
</footnote>
<page confidence="0.987737">
1436
</page>
<bodyText confidence="0.999961">
If we loosen possible comparison systems, the
LDA/HDP model of Lau et al. (2012) achieves supe-
rior numbers to ours for the two supervised metrics,
but at the expense of requiring LDA type processing
on the test data, something that the SemEval or-
ganizers ruled out, presumably with the reasonable
idea that such processing would not be feasible in
the real world. More generally, their system assigns
many senses (about 10) to each word, and thus no-
doubt does poorly on the paired F-Score (they do not
report results on V-Measure and paired F-Score).
</bodyText>
<sectionHeader confidence="0.997615" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999620920634921">
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 7–12. Asso-
ciation for Computational Linguistics.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003.
Latent dirichlet allocation. the Journal of machine
Learning research, 3:993–1022.
Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, EACL &apos;09, pages 103–111,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Arthur P Dempster, Nan M Laird, and Donald B Ru-
bin. 1977. Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), pages 1–38.
Jason Eisner and Damianos Karakos. 2005. Bootstrap-
ping without the boot. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, HLT &apos;05, pages
395–402, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
David Jurgens and Keith Stevens. 2010. Hermit: Flex-
ible clustering for the semeval-2 wsi task. In Pro-
ceedings of the 5th international workshop on semantic
evaluation, pages 359–362. Association for Computa-
tional Linguistics.
Ioannis Korkontzelos and Suresh Manandhar. 2010. Uoy:
Graphs of unambiguous vertices for word sense induc-
tion and disambiguation. In Proceedings of the 5th
international workshop on semantic evaluation, pages
355–358. Association for Computational Linguistics.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense induc-
tion for novel sense detection. In Proceedings of the
13th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 591–601.
Association for Computational Linguistics.
Suresh Manandhar, Ioannis P Klapaftis, Dmitriy Dli-
gach, and Sameer S Pradhan. 2010. Semeval-2010
task 14: Word sense induction &amp; disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 63–68. Association for Com-
putational Linguistics.
Ted Pedersen. 2010. Duluth-wsi: Senseclusters applied
to the sense induction task of semeval-2. In Proceed-
ings of the 5th international workshop on semantic
evaluation, pages 363–366. Association for Computa-
tional Linguistics.
Tim Van de Cruys, Marianna Apidianaki, et al. 2011.
Latent semantic word sense induction and disambigua-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies (ACL/HLT), pages 1476–
1485.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceedings
of the 33rd annual meeting on Association for Compu-
tational Linguistics, ACL &apos;95, pages 189–196, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.993901">
1437
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.828289">
<title confidence="0.999866">Naive Bayes Word Sense Induction</title>
<author confidence="0.9993">Do Kook Choe Eugene Charniak</author>
<affiliation confidence="0.999993">Brown University Brown University</affiliation>
<address confidence="0.944777">Providence, RI Providence, RI</address>
<email confidence="0.993525">dc65@cs.brown.eduec@cs.brown.edu</email>
<abstract confidence="0.98454175">We introduce an extended naive Bayes model for word sense induction (WSI) and apply it to a WSI task. The extended model incorporates the idea the words closer to the target word are more relevant in predicting its sense. The proposed model is very simple yet effective when evaluated on SemEval-2010 WSI data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Semeval-2007 task 02: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>7--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2844" citStr="Agirre and Soroa, 2007" startWordPosition="459" endWordPosition="462"> tries many different seeds for bootstrapping and chooses the “best” classifier at the end. Eisner and Karakos&apos;s algorithm is limited in that their system is designed for disambiguating words that have only 2 senses. Bayesian WSI systems have been developed by several authors. Brody and Lapata (2009) apply Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to WSI. They run a topic modeling algorithm on texts with some fixed number of topics that correspond to senses and induce a cluster by finding target words assigned to the same topic. Their system is evaluated on SemEval-2007 noun data (Agirre and Soroa, 2007). Lau et al. (2012) apply a nonparametric model, Hierarchical Dirichlet Processes (HDP), to SemEval-2010 data (Manandhar et al., 2010). 3 Model Following Yarowsky (1995), we assume that a word in a document has one sense. Multiple occurrences of a word in a document refer to the same object or concept. The naive Bayes model is well suited for this one-sense-per-document assumption. Each document has one topic corresponding to the sense of the target word that needs disambiguation. Context words in a document are drawn from the conditional distribution of words given the sense. Context words ar</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task 02: Evaluating word sense induction and discrimination systems. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 7–12. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="2582" citStr="Blei et al., 2003" startWordPosition="414" endWordPosition="417">ly be scaled up because for any new ambiguous word humans need to pick a few seed words, which initialize the algorithm. In order to automate the semi-supervised system, Eisner and Karakos (2005) propose an unsupervised bootstrapping algorithm. Their system tries many different seeds for bootstrapping and chooses the “best” classifier at the end. Eisner and Karakos&apos;s algorithm is limited in that their system is designed for disambiguating words that have only 2 senses. Bayesian WSI systems have been developed by several authors. Brody and Lapata (2009) apply Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to WSI. They run a topic modeling algorithm on texts with some fixed number of topics that correspond to senses and induce a cluster by finding target words assigned to the same topic. Their system is evaluated on SemEval-2007 noun data (Agirre and Soroa, 2007). Lau et al. (2012) apply a nonparametric model, Hierarchical Dirichlet Processes (HDP), to SemEval-2010 data (Manandhar et al., 2010). 3 Model Following Yarowsky (1995), we assume that a word in a document has one sense. Multiple occurrences of a word in a document refer to the same object or concept. The naive Bayes model is well suit</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Mirella Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL &apos;09,</booktitle>
<pages>103--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2522" citStr="Brody and Lapata (2009)" startWordPosition="405" endWordPosition="408">ation and one-sense-per-discourse. But this algorithm cannot easily be scaled up because for any new ambiguous word humans need to pick a few seed words, which initialize the algorithm. In order to automate the semi-supervised system, Eisner and Karakos (2005) propose an unsupervised bootstrapping algorithm. Their system tries many different seeds for bootstrapping and chooses the “best” classifier at the end. Eisner and Karakos&apos;s algorithm is limited in that their system is designed for disambiguating words that have only 2 senses. Bayesian WSI systems have been developed by several authors. Brody and Lapata (2009) apply Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to WSI. They run a topic modeling algorithm on texts with some fixed number of topics that correspond to senses and induce a cluster by finding target words assigned to the same topic. Their system is evaluated on SemEval-2007 noun data (Agirre and Soroa, 2007). Lau et al. (2012) apply a nonparametric model, Hierarchical Dirichlet Processes (HDP), to SemEval-2010 data (Manandhar et al., 2010). 3 Model Following Yarowsky (1995), we assume that a word in a document has one sense. Multiple occurrences of a word in a document refer to th</context>
<context position="8996" citStr="Brody and Lapata (2009)" startWordPosition="1528" endWordPosition="1531">e word “chip” appears 3 times. For · · · of memory chips . Currently, chips are produced by shining light through a mask to produce an image on the chip , much as · · · Example 1: an excerpt from “chip” test data a context word, e.g., “shining” there are three possible distances: 8 away from the first “chip,” 4 away from the second “chip” and 11 away from the last “chip.” We set the distance of “shining” from the target to 4. We model each target word individually. We set α, a Dirichlet prior for senses, to 0.02 and 0, a Dirichlet prior for contextual words, to 0.1 for the Gibbs sampler as in Brody and Lapata (2009). We initialize EM with parameters learned from the sampler. We run EM until the likehood changes less than 1%. We run the sampler 2000 iterations including 1000 iterations of burn-in: 10 samples at an interval of 100 are averaged. For comparison, we also evaluate EM with random initialization. All reported scores (described in Section 5.4) are averaged over ten different runs of the program.1 5.3.1 Tuning Parameters Two parameters, the number of senses and x of the function l(w), need to be determined before running the program. To find a good setting we do grid search on the trial data with </context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL &apos;09, pages 103–111, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<pages>1--38</pages>
<contexts>
<context position="5764" citStr="Dempster et al., 1977" startWordPosition="964" endWordPosition="967">d the classifier become: X Y p(w) = p(s) f(w|s) (4) s w Y p(s) f(w|s), (5) w where f(w|s) replaces p(w|s). The naive Bayes model is a special case; set x = 0. The new model puts more weight on context words that are close to the target word. The distribution of words that are farther away approaches the uniform distribution. l(w) smoothes the distribution more as x becomes larger. 4 Inference Given the generative model, we employ two inference algorithms to learn the sense distribution and word distributions given a sense. Expectation Maximization (EM) is a natural choice for the naive Bayes (Dempster et al., 1977). When initialized with random parameters, EM gets stuck at local maxima. To avoid local maxima, we use a Gibbs sampler for the plain naive Bayes to learn parameters that initialize EM. 5 Experiments 5.1 Data We evaluate the model on SemEval-2010 WSI task data (Manandhar et al., 2010). The task has 100 target words, 50 nouns and 50 verbs. For each target word, there are training and test documents. Table 1 have details. The training and test data are plain texts without sense tags. For evaluation, the inferred sense labels are compared with human annotations. To tune some parameters we use the</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur P Dempster, Nan M Laird, and Donald B Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society. Series B (Methodological), pages 1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Damianos Karakos</author>
</authors>
<title>Bootstrapping without the boot.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05,</booktitle>
<pages>395--402</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2159" citStr="Eisner and Karakos (2005)" startWordPosition="351" endWordPosition="354">relevant previous work. In Section 3 and 4 we introduce the naive Bayes model for WSI and inference schemes for the model. In Section 5 we evaluate the model on SemEval-2010 data. In Section 6 we conclude. 2 Related Work Yarowsky (1995) introduces a semi-supervised bootstrapping algorithm with two assumptions that rivals supervised algorithms: one-sense-percollocation and one-sense-per-discourse. But this algorithm cannot easily be scaled up because for any new ambiguous word humans need to pick a few seed words, which initialize the algorithm. In order to automate the semi-supervised system, Eisner and Karakos (2005) propose an unsupervised bootstrapping algorithm. Their system tries many different seeds for bootstrapping and chooses the “best” classifier at the end. Eisner and Karakos&apos;s algorithm is limited in that their system is designed for disambiguating words that have only 2 senses. Bayesian WSI systems have been developed by several authors. Brody and Lapata (2009) apply Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to WSI. They run a topic modeling algorithm on texts with some fixed number of topics that correspond to senses and induce a cluster by finding target words assigned to the sam</context>
</contexts>
<marker>Eisner, Karakos, 2005</marker>
<rawString>Jason Eisner and Damianos Karakos. 2005. Bootstrapping without the boot. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05, pages 395–402, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Keith Stevens</author>
</authors>
<title>Hermit: Flexible clustering for the semeval-2 wsi task.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th international workshop on semantic evaluation,</booktitle>
<pages>359--362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11199" citStr="Jurgens and Stevens, 2010" startWordPosition="1910" endWordPosition="1914">ced group of tokens as a cluster. We refer to the model learned with the sampler and EM as NB, and to the model learned with EM only as NB0. 5.4.1 Short Descriptions of Other WSI Systems Evaluated on SemEval-2010 The baseline assigns every instance of a target word with the most frequent sense (MFS). UoY runs a clustering algorithm on a graph with words as nodes and co-occurrences between words as edges (Korkontzelos and Manandhar, 2010). Hermit approximates co-occurrence space with Random Indexing and applies a hybrid of k-means and Hierarchical Agglomerate Clustering to co-occurrence space (Jurgens and Stevens, 2010). NMFtiib factors a matrix using nonnegative matrix factorization and runs a clustering algorithm on test instances represented by factors (Van de Cruys et al., 2011). 5.4.2 V-Measure V-Measure computes the quality of induced clusters as the harmonic mean of two values, homogeneity and completeness. Homogeneity measures whether instances of a cluster belong to a single gold class. Completeness measures whether instances of a gold class belong to a cluster. V-Measure is between 0 and 1; higher is better. See Table 3 for details of V-Measure evaluation (#cl is the number of induced clusters). Wi</context>
</contexts>
<marker>Jurgens, Stevens, 2010</marker>
<rawString>David Jurgens and Keith Stevens. 2010. Hermit: Flexible clustering for the semeval-2 wsi task. In Proceedings of the 5th international workshop on semantic evaluation, pages 359–362. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Korkontzelos</author>
<author>Suresh Manandhar</author>
</authors>
<title>Uoy: Graphs of unambiguous vertices for word sense induction and disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th international workshop on semantic evaluation,</booktitle>
<pages>355--358</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11014" citStr="Korkontzelos and Manandhar, 2010" startWordPosition="1884" endWordPosition="1887">metrics for unsupervised evaluation (VMeasure, paired F-Score) and one metric for supervised evaluation (supervised recall). We refer to the true group of tokens as a gold class and to an induced group of tokens as a cluster. We refer to the model learned with the sampler and EM as NB, and to the model learned with EM only as NB0. 5.4.1 Short Descriptions of Other WSI Systems Evaluated on SemEval-2010 The baseline assigns every instance of a target word with the most frequent sense (MFS). UoY runs a clustering algorithm on a graph with words as nodes and co-occurrences between words as edges (Korkontzelos and Manandhar, 2010). Hermit approximates co-occurrence space with Random Indexing and applies a hybrid of k-means and Hierarchical Agglomerate Clustering to co-occurrence space (Jurgens and Stevens, 2010). NMFtiib factors a matrix using nonnegative matrix factorization and runs a clustering algorithm on test instances represented by factors (Van de Cruys et al., 2011). 5.4.2 V-Measure V-Measure computes the quality of induced clusters as the harmonic mean of two values, homogeneity and completeness. Homogeneity measures whether instances of a cluster belong to a single gold class. Completeness measures whether i</context>
</contexts>
<marker>Korkontzelos, Manandhar, 2010</marker>
<rawString>Ioannis Korkontzelos and Suresh Manandhar. 2010. Uoy: Graphs of unambiguous vertices for word sense induction and disambiguation. In Proceedings of the 5th international workshop on semantic evaluation, pages 355–358. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>Paul Cook</author>
<author>Diana McCarthy</author>
<author>David Newman</author>
<author>Timothy Baldwin</author>
</authors>
<title>Word sense induction for novel sense detection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>591--601</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2863" citStr="Lau et al. (2012)" startWordPosition="463" endWordPosition="466">ds for bootstrapping and chooses the “best” classifier at the end. Eisner and Karakos&apos;s algorithm is limited in that their system is designed for disambiguating words that have only 2 senses. Bayesian WSI systems have been developed by several authors. Brody and Lapata (2009) apply Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to WSI. They run a topic modeling algorithm on texts with some fixed number of topics that correspond to senses and induce a cluster by finding target words assigned to the same topic. Their system is evaluated on SemEval-2007 noun data (Agirre and Soroa, 2007). Lau et al. (2012) apply a nonparametric model, Hierarchical Dirichlet Processes (HDP), to SemEval-2010 data (Manandhar et al., 2010). 3 Model Following Yarowsky (1995), we assume that a word in a document has one sense. Multiple occurrences of a word in a document refer to the same object or concept. The naive Bayes model is well suited for this one-sense-per-document assumption. Each document has one topic corresponding to the sense of the target word that needs disambiguation. Context words in a document are drawn from the conditional distribution of words given the sense. Context words are assumed to be ind</context>
</contexts>
<marker>Lau, Cook, McCarthy, Newman, Baldwin, 2012</marker>
<rawString>Jey Han Lau, Paul Cook, Diana McCarthy, David Newman, and Timothy Baldwin. 2012. Word sense induction for novel sense detection. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 591–601. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suresh Manandhar</author>
<author>Ioannis P Klapaftis</author>
<author>Dmitriy Dligach</author>
<author>Sameer S Pradhan</author>
</authors>
<title>Semeval-2010 task 14: Word sense induction &amp; disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>63--68</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2978" citStr="Manandhar et al., 2010" startWordPosition="479" endWordPosition="482">in that their system is designed for disambiguating words that have only 2 senses. Bayesian WSI systems have been developed by several authors. Brody and Lapata (2009) apply Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to WSI. They run a topic modeling algorithm on texts with some fixed number of topics that correspond to senses and induce a cluster by finding target words assigned to the same topic. Their system is evaluated on SemEval-2007 noun data (Agirre and Soroa, 2007). Lau et al. (2012) apply a nonparametric model, Hierarchical Dirichlet Processes (HDP), to SemEval-2010 data (Manandhar et al., 2010). 3 Model Following Yarowsky (1995), we assume that a word in a document has one sense. Multiple occurrences of a word in a document refer to the same object or concept. The naive Bayes model is well suited for this one-sense-per-document assumption. Each document has one topic corresponding to the sense of the target word that needs disambiguation. Context words in a document are drawn from the conditional distribution of words given the sense. Context words are assumed to be independent from each other given 1433 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Pro</context>
<context position="6049" citStr="Manandhar et al., 2010" startWordPosition="1013" endWordPosition="1016"> away approaches the uniform distribution. l(w) smoothes the distribution more as x becomes larger. 4 Inference Given the generative model, we employ two inference algorithms to learn the sense distribution and word distributions given a sense. Expectation Maximization (EM) is a natural choice for the naive Bayes (Dempster et al., 1977). When initialized with random parameters, EM gets stuck at local maxima. To avoid local maxima, we use a Gibbs sampler for the plain naive Bayes to learn parameters that initialize EM. 5 Experiments 5.1 Data We evaluate the model on SemEval-2010 WSI task data (Manandhar et al., 2010). The task has 100 target words, 50 nouns and 50 verbs. For each target word, there are training and test documents. Table 1 have details. The training and test data are plain texts without sense tags. For evaluation, the inferred sense labels are compared with human annotations. To tune some parameters we use the trial data of Training Testing Senses (#) All 879807 8915 3.79 Nouns 716945 5285 4.46 Verbs 162862 3630 3.12 Table 1: Details of SemEval-2010 data SemEval-2010. The trial data consists of training and test portions of 4 verbs. On average there are 137 documents for each target word i</context>
</contexts>
<marker>Manandhar, Klapaftis, Dligach, Pradhan, 2010</marker>
<rawString>Suresh Manandhar, Ioannis P Klapaftis, Dmitriy Dligach, and Sameer S Pradhan. 2010. Semeval-2010 task 14: Word sense induction &amp; disambiguation. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 63–68. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Duluth-wsi: Senseclusters applied to the sense induction task of semeval-2.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th international workshop on semantic evaluation,</booktitle>
<pages>363--366</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12038" citStr="Pedersen (2010)" startWordPosition="2047" endWordPosition="2048">d clusters as the harmonic mean of two values, homogeneity and completeness. Homogeneity measures whether instances of a cluster belong to a single gold class. Completeness measures whether instances of a gold class belong to a cluster. V-Measure is between 0 and 1; higher is better. See Table 3 for details of V-Measure evaluation (#cl is the number of induced clusters). With respect to V-Measure, NB performs much better than NB0. This holds for paired F-Score and supervised recall evaluations. The sampler improves the log-likelihood of NB by 3.8% on average (4.8% on nouns and 2.9% on verbs). Pedersen (2010) points out that it is possible to increase the V-Measure of bad models by increasing 1435 #s \ x 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 2 74.73 74.76 74.41 74.57 74.06 74.07 74.18 74.33 74.14 74.22 74.15 74.52 3 74.60 74.71 75.21 75.46 75.21 75.57 75.61 75.32 75.53 75.56 74.98 74.79 4 74.52 75.06 74.97 75.14 76.02 75.51 75.74 75.51 75.59 75.51 75.37 75.35 5 73.40 73.88 74.93 75.13 74.79 74.68 74.71 74.49 75.11 74.94 74.86 75.25 Table 2: Performance of the model with various parameters: supervised recall on the trial data. The best value from each row is bold-faced. The scores are ave</context>
</contexts>
<marker>Pedersen, 2010</marker>
<rawString>Ted Pedersen. 2010. Duluth-wsi: Senseclusters applied to the sense induction task of semeval-2. In Proceedings of the 5th international workshop on semantic evaluation, pages 363–366. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
<author>Marianna Apidianaki</author>
</authors>
<title>Latent semantic word sense induction and disambiguation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL/HLT),</booktitle>
<pages>1476--1485</pages>
<marker>Van de Cruys, Apidianaki, 2011</marker>
<rawString>Tim Van de Cruys, Marianna Apidianaki, et al. 2011. Latent semantic word sense induction and disambiguation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL/HLT), pages 1476– 1485.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, ACL &apos;95,</booktitle>
<pages>189--196</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1770" citStr="Yarowsky (1995)" startWordPosition="300" endWordPosition="301">m better than WSI systems, but building labeled data can be prohibitively expensive. In addition, WSD systems are not suitable for newly created words, new senses of existing words, or domainspecific words. On the other hand, WSI systems can learn new senses of words directly from texts because these programs do not rely on a predefined set of senses. In Section 2 we describe relevant previous work. In Section 3 and 4 we introduce the naive Bayes model for WSI and inference schemes for the model. In Section 5 we evaluate the model on SemEval-2010 data. In Section 6 we conclude. 2 Related Work Yarowsky (1995) introduces a semi-supervised bootstrapping algorithm with two assumptions that rivals supervised algorithms: one-sense-percollocation and one-sense-per-discourse. But this algorithm cannot easily be scaled up because for any new ambiguous word humans need to pick a few seed words, which initialize the algorithm. In order to automate the semi-supervised system, Eisner and Karakos (2005) propose an unsupervised bootstrapping algorithm. Their system tries many different seeds for bootstrapping and chooses the “best” classifier at the end. Eisner and Karakos&apos;s algorithm is limited in that their s</context>
<context position="3013" citStr="Yarowsky (1995)" startWordPosition="486" endWordPosition="487">iguating words that have only 2 senses. Bayesian WSI systems have been developed by several authors. Brody and Lapata (2009) apply Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to WSI. They run a topic modeling algorithm on texts with some fixed number of topics that correspond to senses and induce a cluster by finding target words assigned to the same topic. Their system is evaluated on SemEval-2007 noun data (Agirre and Soroa, 2007). Lau et al. (2012) apply a nonparametric model, Hierarchical Dirichlet Processes (HDP), to SemEval-2010 data (Manandhar et al., 2010). 3 Model Following Yarowsky (1995), we assume that a word in a document has one sense. Multiple occurrences of a word in a document refer to the same object or concept. The naive Bayes model is well suited for this one-sense-per-document assumption. Each document has one topic corresponding to the sense of the target word that needs disambiguation. Context words in a document are drawn from the conditional distribution of words given the sense. Context words are assumed to be independent from each other given 1433 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1433–1437, Seattle, </context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, ACL &apos;95, pages 189–196, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
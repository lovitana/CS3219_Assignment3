<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9988315">
Improving Pivot-Based Statistical Machine Translation
Using Random Walk
</title>
<author confidence="0.9937885">
Xiaoning Zhu1*, Zhongjun He2, Hua Wu2, Haifeng Wang2,
Conghui Zhu1, and Tiejun Zhao1
</author>
<affiliation confidence="0.8083705">
Harbin Institute of Technology, Harbin, China1
Baidu Inc., Beijing, China2
</affiliation>
<email confidence="0.9557415">
{xnzhu, chzhu, tjzhao}@mtlab.hit.edu.cn
{hezhongjun,wu_hua,wanghaifeng}@baidu.com
</email>
<sectionHeader confidence="0.996513" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999896666666667">
This paper proposes a novel approach that uti-
lizes a machine learning method to improve
pivot-based statistical machine translation
(SMT). For language pairs with few bilingual
data, a possible solution in pivot-based SMT
using another language as a &amp;quot;bridge&amp;quot; to gen-
erate source-target translation. However, one
of the weaknesses is that some useful source-
target translations cannot be generated if the
corresponding source phrase and target phrase
connect to different pivot phrases. To allevi-
ate the problem, we utilize Markov random
walks to connect possible translation phrases
between source and target language. Experi-
mental results on European Parliament data,
spoken language data and web data show that
our method leads to significant improvements
on all the tasks over the baseline system.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999784894736842">
Statistical machine translation (SMT) uses bilin-
gual corpora to build translation models. The
amount and the quality of the bilingual data
strongly affect the performance of SMT systems.
For resource-rich language pairs, such as Chinese-
English, it is easy to collect large amounts of bi-
lingual corpus. However, for resource-poor lan-
guage pairs, such as Chinese-Spanish, it is difficult
to build a high-performance SMT system with the
small scale bilingual data available.
The pivot language approach, which performs
translation through a third language, provides a
possible solution to the problem. The triangulation
method (Wu and Wang, 2007; Cohn and Lapata,
2007) is a representative work for pivot-based ma-
chine translation. With a triangulation pivot ap-
proach, a source-target phrase table can be
obtained by combining the source-pivot phrase
table and the pivot-target phrase table. However,
one of the weaknesses is that some corresponding
source and target phrase pairs cannot be generated,
because they are connected to different pivot
phrases (Cui et al., 2013). As illustrated in Figure
1, since there is no direct translation between “很
可口 henkekou” and “really delicious”, the trian-
gulation method is unable to establish a relation
between “很可口 henkekou” and the two Spanish
phrases.
To solve this problem, we apply a Markov ran-
dom walk method to pivot-based SMT system.
Random walk has been widely used. For example,
Brin and Page (1998) used random walk to dis-
cover potential relations between queries and doc-
uments for link analysis in information retrieval.
Analogous to link analysis, the aim of pivot-based
translation is to discover potential translations be-
tween source and target language via the pivot
language.
</bodyText>
<footnote confidence="0.633277">
* This work was done when the first author was visiting Baidu.
</footnote>
<page confidence="0.926799">
524
</page>
<note confidence="0.786507">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 524–534,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.989695111111111">
49011�
feichanghaochi
fP,_WTF1
henkekou
Chinese English Spanish
really delicious
very tasty
realmente delicioso
muy delicioso
</figure>
<figureCaption confidence="0.99996">
Figure 1: An example of random walk on phrase table. The dashed line indicates an implicit relation
</figureCaption>
<bodyText confidence="0.9781383">
in the phrase table.
The goal of this paper is to extend the previous
triangulation approach by exploring implicit trans-
lation relations using random walk method. We
evaluated our approach in several translation tasks,
including translations between European lan-
guages; Chinese-Spanish spoken language transla-
tion and Chinese-Japanese translation with English
as the pivot language. Experimental results show
that our approach achieves significant improve-
ments over the conventional pivot-based method,
triangulation method.
The remainder of this paper is organized as fol-
lows. In section 2, we describe the related work.
We review the triangulation method for pivot-
based machine translation in section 3. Section 4
describes the random walk models. In section 5
and section 6, we describe the experiments and
analyze the performance, respectively. Section 7
gives a conclusion of the paper.
</bodyText>
<sectionHeader confidence="0.999796" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998140761904762">
Several methods have been proposed for pivot-
based translation. Typically, they can be classified
into 3 kinds of methods:
Transfer Method: Within the transfer frame-
work (Utiyama and Isahara, 2007; Wang et al.,
2008; Costa-jussà et al., 2011), a source sentence
is first translated to n pivot sentences via a source-
pivot translation system, and then each pivot sen-
tence is translated to m target sentences via a piv-
ot-target translation system. At each step (source
to pivot and pivot to target), multiple translation
outputs will be generated, thus a minimum Bayes-
risk system combination method is often used to
select the optimal sentence (González-Rubio et al.,
2011; Duh et al., 2011). A problem with the trans-
fer method is that it needs to decode twice. On one
hand, the time cost is doubled; on the other hand,
the translation error of the source-pivot translation
system will be transferred to the pivot-target trans-
lation.
Synthetic Method: A synthetic method creates
a synthetic source-target corpus using source-pivot
translation model or pivot-target translation model
(Utiyama et al., 2008; Wu and Wang, 2009). For
example, we can translate each pivot sentence in
the pivot-target corpus to source language with a
pivot-source model, and then combine the translat-
ed source sentence with the target sentence to ob-
tain a synthetic source-target corpus, and vice
versa. However, it is difficult to build a high quali-
ty translation system with a corpus created by a
machine translation system.
Triangulation Method: The triangulation
method obtains source-target model by combining
source-pivot and pivot-target translation models
(Wu and Wang, 2007; Cohn and Lapata 2007),
which has been shown to work better than the oth-
er pivot approaches (Utiyama and Isahara, 2007).
As we mentioned earlier, the weakness of triangu-
lation is that the corresponding source and target
phrase pairs cannot be connected in the case that
they connect to different pivot phrases.
</bodyText>
<sectionHeader confidence="0.980737" genericHeader="method">
3 The Triangulation Method
</sectionHeader>
<bodyText confidence="0.999714888888889">
In this section, we review the triangulation method
for pivot-based translation.
With the two additional bilingual corpora, the
source-pivot and pivot-target translation models
can be trained. Thus, a pivot model can be ob-
tained by merging these two models. In the trans-
lation model, the phrase translation probability and
the lexical weight are language dependent, which
will be introduced in the next two sub-sections.
</bodyText>
<page confidence="0.996506">
525
</page>
<subsectionHeader confidence="0.996736">
3.1 Phrase Translation Probability
</subsectionHeader>
<bodyText confidence="0.999906142857143">
The triangulation method assumes that there exist
translations between phrases s and phrase p in
source and pivot languages, and between phrase
p and phrase t in pivot and target languages.
The phrase translation probability φ between
source and target languages is determined by the
following model:
</bodyText>
<equation confidence="0.994790444444444">
φ (  |)
s t = ∑ φ (  |, ) (  |)
s p t p t
φ
p (1)
∑ φ φ
(  |) (  |)
s p p t
p
</equation>
<subsectionHeader confidence="0.99835">
3.2 Lexical Weight
</subsectionHeader>
<bodyText confidence="0.874186555555556">
Given a phrase pair (s, t ) and a word alignment
a between the source word positions i =1, ... , n
and the target word positions j = 0,1, ..., m , the
lexical weight of phrase pair (s, t ) can be calcu-
lated with the following formula (Koehn et al.
2003) :
In formula 2, the lexical translation probability
distribution ω(s  |t) between source word s and
target word t can be estimated with formula 3.
</bodyText>
<equation confidence="0.987716333333333">
ω (s  |t) — count(s, t) E(3)
—
, count(s&amp;quot; t)
</equation>
<bodyText confidence="0.9874625">
Thus the alignment a between the source
phrase s and target phrase t via pivot phrase p
is needed for computing the lexical weight. The
alignment a can be obtained as follows:
</bodyText>
<equation confidence="0.583919">
a = {(s, t) |∃ p : (s, p)∈ a1 &amp; (p, t)∈ a2 } (4)
</equation>
<bodyText confidence="0.994569">
where a1 and a2 indicate the word alignment be-
tween the phrase pair (s, p) and (p, t ) , respec-
tively.
The triangulation method requires that both the
source and target phrases connect to the same piv-
ot phrase. Otherwise, the source-target phrase pair
cannot be discovered. As a result, some useful
translation relations will be lost. In order to allevi-
ate this problem, we propose a random walk model,
to discover the implicit relations among the source,
pivot and target phrases.
</bodyText>
<sectionHeader confidence="0.871327" genericHeader="method">
4 Random Walks on Translation Graph
</sectionHeader>
<bodyText confidence="0.997108483870968">
For phrase-based SMT, all source-target phrase
pairs are stored in a phrase table. In our random
walk approach, we first build a translation graph
according to the phrase table. A translation graph
contains two types of nodes: source phrase and
target phrase. A source phrase s and a target
phrase t are connected if exists a phrase pair
(s, t ) in the phrase table. The edge can be
weighted according to translation probabilities or
alignments in the phrase table. For the pivot-based
translation, the translation graph can be derived
from the source-pivot phrase table and pivot-target
phrase table.
Our random walk model is inspired by two
works (Szummer and Jaakkola, 2002; Craswell
and Szummer,2007). The general process of ran-
dom walk can be described as follows:
Let G = (V, E) be a directed graph with n ver-
tices and m edges. For a vertex v ∈ V, Γ(v) de-
notes the set of neighbors of v in G . A random
walk on G follows the following process: start at
a vertex v0 , chose and walk along a random
neighbor v1 , with v1 ∈Γ (v0) . At the second step,
start from v1 and chose a random neighbor v2, and
so on.
Let S be the set of source phrases, and P be the
set of pivot phrases. Then the nodes V are the un-
ion of S and P. The edges E correspond to the rela-
tions between phrase pairs.
Let R represent the binary relations between
source phrases and pivot phrases. Then the 1-step
</bodyText>
<equation confidence="0.842052142857143">
translation Rik from node i to node k can be direct-
ly obtained in the phrase table.
Define operator ⊗ to denote the calculation of
relation R. Then 2-step translation Rij from node i
to node j can be obtained with the following for-
mula.
Rij = Rik Rkj⊗ (4)
</equation>
<bodyText confidence="0.937074">
We use Rt|0(k  |i) to denote a t-step translation
relation from node i to node k. In order to calculate
the translation relations efficiently, we use a ma-
trix A to represent the graph. A t step translation
probability can be denoted with the following for-
mula.
</bodyText>
<equation confidence="0.948505769230769">
n 1
(  |, )
s t a = ∏ ∑ω(  |)
s t
i j
{  |( , ) }
j i j a
∈
i=1 ∀ ( ,
i j
pξ
)∈a
(2)
</equation>
<page confidence="0.956797">
526
</page>
<figureCaption confidence="0.988961">
Figure 2: Framework of random walk based pivot translation. The ST phrase table was generated by combin-
ing SP and PT phrase table through triangulation method. The phrase table with superscript “’” means that it
was enlarged by random walk.
Figure 3: Some possible decoding processes of random walk based pivot approach. The □ stands for the
source phrase (S); the ○ represents the pivot phrase (P) and the ◇ stands for the target phrase (T).
</figureCaption>
<figure confidence="0.958940875">
Pivot without
random walk
Pivot with
random walk
random walk
ST
Phrase Table
S’T’
Phrase Table
PT
Phrase Table
random walk
P’T’
Phrase Table
SP
Phrase Table
S’P’
Phrase Table
(a) Pivot without (b) Random walk on (c) Random walk on (d) Random walk on
random walk source-pivot side pivot-target side both sides
S P T
S P T
S P T
S P T
</figure>
<equation confidence="0.723514">
Pt|0 (k  |i) = [At ]ik (5)
</equation>
<bodyText confidence="0.959485">
where A is a matrix whose i,k-th element is Rik .
</bodyText>
<subsectionHeader confidence="0.998127">
4.1 Framework of Random Walk Approach
</subsectionHeader>
<bodyText confidence="0.999930892857143">
The overall framework of random walk for pivot-
based machine translation is shown in Figure 2.
Before using random walk model, we have two
phrase tables: source-pivot phrase table (SP phrase
table) and pivot-target phrase table (PT phrase ta-
ble). After applying the random walk approach, we
can achieve two extended phrase table: extended
source-pivot phrase table (S’P’ phrase table) and
extended pivot-target phrase table (P’T’ phrase
table). The goal of pivot-based SMT is to get a
source-target phrase table (ST phrase table) via SP
phrase table and PT phrase table.
Our random walk was applied on SP phrase ta-
ble or PT phrase table separately. In next 2 sub-
sections, we will explain how the phrase transla-
tion probabilities and lexical weight are obtained
with random walk model on the phrase table.
Figure 3 shows some possible decoding pro-
cesses of random walk based pivot approach. In
figure 3-a, the possible source-target phrase pair
can be obtained directly via a pivot phrase, so it
does not need a random walk model. In figure 3-b
and figure 3-c, one candidate source-target phrase
pair can be obtained by random walks on source-
pivot side or pivot-target side. Figure 3-d shows
that the possible source-target can only by ob-
tained by random walks on source-pivot side and
pivot-target side.
</bodyText>
<subsectionHeader confidence="0.997604">
4.2 Phrase Translation Probabilities
</subsectionHeader>
<bodyText confidence="0.999237333333333">
For the translation probabilities, the binary relation
R is the translation probabilities in the phrase table.
The operator ⊗ is multiplication. According to
formula 5, the random walk sums up the probabili-
ties of all paths of length t between the node i and
k.
</bodyText>
<page confidence="0.965623">
527
</page>
<figure confidence="0.982199">
step 1
step 2
step 3
请 您 填写 这个 表格
could you fill out this form
could you fill out this form
please fill out this form
请 填写 这 张 表
请 填写 这 张 表
</figure>
<figureCaption confidence="0.999851">
Figure 4: An example of word alignment induction with 3 steps random walks
</figureCaption>
<bodyText confidence="0.99518575">
Take source-to-pivot phrase graph as an exam-
ple; denote matrix A contains s+p nodes (s source
phrases and p pivot phrases) to represent the trans-
lation graph.
</bodyText>
<equation confidence="0.9737506">
A g
=     (6)
ij s p s p
( ) ( )
+ × +
</equation>
<bodyText confidence="0.916451222222222">
where gij is the i,j-th elements of matrix A.
We can split the matrix A into 4 sub-matrixes:
= 0s×s Asp
A Aps 0p×p 
where the sub-matrix Asp = [pikJs×p represents the
translation probabilities from source to pivot lan-
guage, and Aps represents the similar meaning.
Take 3 steps walks as an example:
Step1:
</bodyText>
<figure confidence="0.939452157894737">
 0 A 
s s
× sp
A A
=  
0
 ps p p
× 
Step2:
×
A A 0 
2 sp ps s p
×
A =  
0 A A
×
 p s
× ps sp 
Step3:
</figure>
<bodyText confidence="0.9795024">
analogous to paraphrasing (Bannard and
Callison-Burch, 2005). For the example shown
in figure 1 as an example, the hidden relation
between “很可口 henkekou” and “非常好吃
feichanghaochi” can be found through Step 2.
</bodyText>
<listItem confidence="0.548658545454545">
3. The third step describes the following proce-
dure: S-P-S’-P’. An extended source-pivot
phrase table is generated by 3-step random
walks. Compared with the initial phrase table
in Step1, although the number of phrases is
not increased, the relations between phrase
pairs are increased and more translation rules
can be obtained. Still for the example in Fig-
ure 1 , the hidden relation between “很可口
henkekou” and “really delicious” can be gen-
erated in Step 3.
</listItem>
<subsectionHeader confidence="0.987989">
4.3 Lexical Weights
</subsectionHeader>
<bodyText confidence="0.8282128">

×
For the 3 steps example, each step performs a
translation process in the form of matrix’s self-
multiplication.
</bodyText>
<listItem confidence="0.950176375">
1. The first step means the translation from
source language to pivot language. The matrix
A is derived from the phrase table directly and
each element in the graph indicates a transla-
tion rule in the phrase table.
2. The second step demonstrates a procedure: S-
P-S’. With 2 steps random walks, we can find
the synonymous phrases, and this procedure is
</listItem>
<bodyText confidence="0.9990597">
To build a translation graph, the two sets of phrase
translation probabilities are represented in the
phrase tables. However, the two lexical weights
are not presented in the graph directly. To deal
with this, we should conduct a word alignment
random walk model to obtain a new alignment a
after t steps. For the computation of lexical
weights, the relation R can be expressed as the
word alignment in the phrase table. The operator
⊗ can be induced with the following formula.
</bodyText>
<equation confidence="0.979445">
a = {(x, y) |∃p : (x, z) ∈ a1 &amp; (z, y) ∈ a2 } (8)
</equation>
<bodyText confidence="0.993685">
where a1 and a2 represent the word alignment
information inside the phrase pairs (x, y) and
(y, z) respectively. An example of word
alignment inducing is shown in Figure 4. With a
new word alignment, the two lexical weights can
be calculated by formula 2 and formula 3.
</bodyText>
<figure confidence="0.8544562">
Aps × Asp
A3 = 0s×s
A ×A ×A 0
Asp×
psspps
p p
(7)
528
5 Experiments guages. Table 1 and Table 2 summarized the train-
ing data.
</figure>
<subsectionHeader confidence="0.733956">
5.1 Translation System and Evaluation Met-
ric
</subsectionHeader>
<bodyText confidence="0.9999625">
In our experiments, the word alignment was ob-
tained by GIZA++ (Och and Ney, 2000) and the
heuristics “grow-diag-final” refinement rule.
(Koehn et al., 2003). Our translation system is an
in-house phrase-based system using a log-linear
framework including a phrase translation model, a
language model, a lexicalized reordering model, a
word penalty model and a phrase penalty model,
which is analogous to Moses (Koehn et al., 2007).
The baseline system is the triangulation method
based pivot approach (Wu and Wang, 2007).
To evaluate the translation quality, we used
BLEU (Papineni et al., 2002) as our evaluation
metric. The statistical significance using 95% con-
fidence intervals were measured with paired boot-
strap resampling (Koehn, 2004).
</bodyText>
<subsectionHeader confidence="0.9864895">
5.2 Experiments on Europarl
5.2.1. Data sets
</subsectionHeader>
<bodyText confidence="0.999978045454545">
We mainly test our approach on Europarl1 corpus,
which is a multi-lingual corpus including 21 Euro-
pean languages. Due to the size of the data, we
only select 11 languages which were added to
Europarl from 04/1996 or 01/1997, including Dan-
ish (da), German (de), Greek (el), English (en),
Spanish (es), Finnish (fi), French (fr), Italian (it)
Dutch (nl) Portuguese (pt) and Swedish (sv). In
order to avoid a trilingual scenario, we split the
training corpus into 2 parts by the year of the data:
the data released in odd years were used for train-
ing source-pivot model and the data released in
even years were used for training pivot-target
model.
We perform our experiments on different trans-
lation directions and via different pivot languages.
As a most widely used language in the world
(Mydans, 2011), English was used as the pivot
language for granted when carrying out experi-
ments on different translation directions. For trans-
lating Portuguese to Swedish, we also tried to
perform our experiments via different pivot lan-
</bodyText>
<footnote confidence="0.86489">
1 http://www.statmt.org/europarl/
</footnote>
<table confidence="0.927624578947368">
Language Sentence Language Sentence
Pairs Pairs # Pairs Pairs #
(src-pvt) (pvt-tgt)
da-en 974,189 en-da 953,002
de-en 983,411 en-de 905,167
el-en 609,315 en-el 596,331
es-en 968,527 en-es 961,782
fi-en 998,429 en-fi 903,689
fr-en 989,652 en-fr 974,637
it-en 934,448 en-it 938,573
nl-en 982,696 en-nl 971,379
pt-en 967,816 en-pt 960,214
sv-en 960,631 en-sv 869,254
Table1. Training data for experiments using English as
the pivot language. For source-pivot (src-pvt; xx-en)
model training, the data of odd years were used. Instead
the data of even years were used for pivot-target (pvt-
src; en-xx) model training.
Language Sentence Language Sentence
Pairs Pairs # Pairs Pairs #
(src-pvt) (pvt-tgt)
pt-da 941,876 da-sv 865,020
pt-de 939,932 de-sv 814,678
pt-el 591,429 el-sv 558,765
pt-es 934,783 es-sv 827,964
pt-fi 950,588 fi-sv 872,182
pt-fr 954,637 fr-sv 860,272
pt-it 900,185 it-sv 813,000
pt-nl 945,997 nl-sv 864,675
Table2. Training data for experiments via different piv-
ot languages. For source-pivot (src-pvt; pt-xx) model
training, the data of odd years were used. Instead the
data of even years were used for pivot-target (pvt-src;
xx-sv) model training.
Test Set Sentence # Reference #
WMT06 2,000 1
WMT07 2,000 1
WMT08 2,000 1
</table>
<tableCaption confidence="0.991902">
Table3. Statistics of test sets.
</tableCaption>
<page confidence="0.962496">
529
</page>
<table confidence="0.999714090909091">
TGT da de el es fi fr it nl pt sv
SRC
Baseline da - 19.83 20.46 27.59 14.76 24.11 20.49 22.26 24.38 28.33
RW 20.15* 21.02* 28.29* 15.63* 24.71* 20.82* 22.57* 24.88* 28.87*
Baseline de 23.35 - 19.83 26.21 12.72 22.43 18.82 23.74 23.05 21.17
RW 23.69* 20.05 26.70* 13.57* 22.78* 19.32* 24.11* 23.35* 21.27
Baseline el 23.24 18.12 - 32.28 13.31 27.35 23.19 20.80 27.62 22.70
RW 23.82* 18.49* 32.48 14.08* 27.67* 23.63* 21.26* 27.86 23.15*
Baseline es 25.34 19.67 27.24 - 13.93 32.91 27.67 22.37 34.73 24.83
RW 26.07* 20.17* 27.52 14.61* 33.16 27.92 22.85* 34.93 25.50*
Baseline fi 18.29 13.20 14.72 20.17 - 17.52 14.76 15.50 17.30 16.63
RW 18.63* 13.40 15.00* 20.48* 17.84* 15.01 16.04* 17.68* 16.79
Baseline fr 25.67 20.02 26.58 37.50 13.90 - 28.51 22.65 33.81 24.64
RW 26.51* 20.45* 26.75 37.80* 14.75* 28.71 23.33* 33.93 25.59*
Baseline it 22.63 17.81 24.24 34.36 13.20 30.16 - 21.37 30.84 22.12
RW 23.27* 18.40* 24.66* 35.42* 14.11* 30.48* 21.81* 30.92* 22.64*
Baseline nl 22.49 19.86 18.56 24.69 11.96 21.48 18.36 - 21.71 19.83
RW 22.76 20.45* 19.10* 25.19* 12.63* 22.05* 18.67* 22.13* 22.17*
Baseline pt 24.08 19.11 25.30 36.59 13.33 32.47 28.08 21.52 - 22.90
RW 25.29* 19.83* 26.20* 37.13* 14.21* 32.78* 28.44* 22.46* 23.90*
Baseline sv 31.24 20.26 22.06 29.21 15.39 25.63 21.25 22.30 25.60 -
RW 31.75* 20.74* 22.59* 29.87* 16.13* 26.18* 21.81* 22.62* 26.09*
</table>
<tableCaption confidence="0.9541155">
Table4. Experimental results on Europarl with different translation directions (BLEU% on WMT08).
RW=Random Walk. * indicates the results are significantly better than the baseline (p&lt;0.05).
</tableCaption>
<bodyText confidence="0.999619">
Several test sets have been released for the
Europarl corpus. In our experiments, we used
WMT20062, WMT20073 and WMT20084 as our
test data. The original test data includes 4 lan-
guages and extended versions with 11 languages
of these test sets are available by the EuroMatrix5
project. Table 3 shows the test sets.
</bodyText>
<sectionHeader confidence="0.879241" genericHeader="method">
5.2.2. Experiments on Different Translation
Directions
</sectionHeader>
<bodyText confidence="0.999682">
We build 180 pivot translation systems6 (including
90 baseline systems and 90 random walk based
systems) using 10 source/target languages and 1
pivot language (English).
The baseline system was built following the tra-
ditional triangulation pivot approach. Table 4 lists
the results on Europarl training data. Limited by
</bodyText>
<footnote confidence="0.946939666666667">
2 http://www.statmt.org/wmt06/shared-task/
3 http://www.statmt.org/wmt07/shared-task.html
4 http://www.statmt.org/wmt08/shared-task.html
5 http://matrix.statmt.org/test_sets/list
6 Given N languages, a total of N*(N-1) SMT systems should
be build to cover the translation between each language.
</footnote>
<bodyText confidence="0.99869475">
the length of the paper, we only show the results
on WMT08, the tendency of the results on
WMT06 and WMT07 is similar to WMT08.
Several observations can be made from the table.
</bodyText>
<listItem confidence="0.773426666666667">
1. In all 90 language pairs, our method achieves
general improvements over the baseline system.
2. Among 90 language pairs, random walk
based approach is significantly better than the
baseline system in 75 language pairs.
3. The improvements of our approach are not
</listItem>
<bodyText confidence="0.970123461538462">
equal in different translation directions. The im-
provement ranges from 0.06 (it-es) to 1.21 (pt-da).
One possible reason is that the performance is re-
lated with the source and target language. For ex-
ample, when using Finnish as the target language,
the improvement is significant over the baseline.
This may be caused by the great divergence be-
tween Uralic language (Finnish) and Indo-
European language (the other European language
in Table4). From the table we can find that the
translation between languages in different lan-
guage family is worse than that in some language
family. But our random walk approach can im-
</bodyText>
<page confidence="0.988771">
530
</page>
<bodyText confidence="0.990041253968254">
prove the performance of translations between dif-
ferent language families.
5.2.3. Experiments via Different Pivot Lan-
guages
In addition to using English as the pivot language,
we also try some other languages as the pivot
language. In this sub-section, experiments were
carried out from translating Portuguese to Swedish
via different pivot languages.
Table 5 summarizes the BLEU% scores of dif-
ferent pivot language when translating from Por-
tuguese to Swedish. Similar to Table 4, our
approach still achieves general improvements over
the baseline system even if the pivot language has
been changed. From the table we can see that for
most of the pivot language, the random walk based
approach gains more than 1 BLEU score over the
baseline. But when using Finnish as the pivot lan-
guage, the improvement is only 0.02 BLEU scores
on WMT08. This phenomenon shows that the piv-
ot language can also influence the performance of
random walk approach. One possible reason for
the poor performance of using Finnish as the pivot
language is that Finnish belongs to Uralic lan-
guage family, and the other languages belong to
Indo-European family. The divergence between
different language families led to a poor perfor-
mance. Thus how to select a best pivot language is
our future work.
The problem with random walk is that it will
lead to a larger phrase table with noises. In this
sub-section, a pre-pruning (before random walk)
and a post-pruning (after random walk) method
were introduced to deal with this problem.
We used a naive pruning method which selects
the top N phrase pairs in the phrase table. In our
experiments, we set N to 20. For pre-pruning, we
prune the SP phrase table and PT phrase table be-
fore applying random walks. Post-pruning means
that we prune the ST phrase table after random
walks. For the baseline system, we also apply a
pruning method before combine the SP and PT
phrase table. We test our pruning method on pt-en-
sv translation task. Table 6 shows the results.
With a pre- and post-pruning method, the ran-
dom walk approach is able to achieve further im-
provements. Our approach achieved BLEU scores
of 25.11, 24.69 and 24.34 on WMT06, WMT07
and WMT08 respectively, which is much better
than the baseline and the random walk approach
with pruning. Moreover, the size of the phrase
table is about half of the no-pruning method.
When adopting a post-pruning method, the per-
formance of translation did not improved signifi-
cantly over the pre-pruning, but the scale of the
phrase table dropped to 69M, which is only about
2 times larger than the triangulation method.
Phrase table pruning is a key work to improve
the performance of random walk. We plan to ex-
plore more approaches for phrase table pruning.
E.g. using significance test (Johnson et al., 2007)
or monolingual key phrases (He et al., 2009) to
filter the phrase table.
</bodyText>
<table confidence="0.999815142857143">
trans WMT WMT WMT
language 06 07 08
Baseline pt-da-sv 23.40 22.80 23.75*
RW 24.21*
22.4924.47*
Baseline pt-de-sv 22.72 22.21 22.35*
RW 23.26*
21.7623.12*
Baseline pt-el-sv 22.53 22.19 22.40*
RW 23.22*
21.3723.75*
Baseline pt-en-sv 23.54 23.24 23.90*
RW 24.22*
22.9024.66*
Baseline 23.58 23.37 22.80
RW pt-es-sv 24.65* 24.10* 23.77*
Baseline pt-fi-sv 21.06 20.06 20.28
RW 20.42*
20.2621.17
Baseline pt-fr-sv 23.55 23.09 23.96*
RW 24.15*
22.8924.75*
Baseline pt-it-sv 23.65 22.96 24.02*
RW 24.18*
22.7924.74*
Baseline pt-nl-sv 21.87 21.83 22.29*
RW 22.76*
21.3623.06*
</table>
<tableCaption confidence="0.93826525">
Table5. Experimental results on translating from Portu-
guese to Swedish via different pivot language.
RW=Random Walk. * indicates the results are signifi-
cantly better than the baseline (p&lt;0.05).
</tableCaption>
<table confidence="0.999927444444444">
WMT WMT WMT Phrase
06 07 08 Pairs #
Baseline 23.54 23.24 22.90 46M
+pruning 24.05 23.70 23.59 32M
* * *
RW 24.66 24.22 23.90 215M
+pre-pruning 25.11 24.69 24.34 109M
+post-pruning 25.19 24.79 24.41 69M
* * *
</table>
<tableCaption confidence="0.910798">
Table6. Results of Phrase Table Filtering
</tableCaption>
<page confidence="0.996669">
531
</page>
<bodyText confidence="0.967344714285714">
Chinese to Japanese via English with web crawled
data.
All the training data were crawled on the web.
The scale of Chinese-English and English-
Japanese is 10 million respectively. The test set
was built in house with 1,000 sentences and 4 ref-
erences.
</bodyText>
<table confidence="0.928917142857143">
System BLEU% phrase pairs #
Baseline 28.76 4.5G
+pruning 28.90 273M
RW 29.13 46G
+pre-pruning 29.44 11G
+post-pruning 29.51* 3.4G
Table10. Results on Web Data
</table>
<tableCaption confidence="0.62041225">
Table 10 lists the results on web data. From the
table we can find that the random walk model can
achieve an absolute improvement of 0.75 percent-
ages points on BLEU over the baseline.
</tableCaption>
<bodyText confidence="0.999919166666667">
In this subsection, the training data contains
parallel sentences with different domains. And the
two training corpora (Chinese-English and Eng-
lish-Japanese) are typically very different. It
means that our random walk approach is robust in
the realistic scenario.
</bodyText>
<subsectionHeader confidence="0.99748">
5.3 Experiments on Spoken Language
</subsectionHeader>
<bodyText confidence="0.999466263157895">
The European languages show various degrees of
similarity to one another. In this sub-section, we
consider translation from Chinese to Spanish with
English as the pivot language. Chinese belongs to
Sino-Tibetan Languages and English/Spanish be-
longs to Indo-European Languages, the gap be-
tween two languages is wide.
A pivot task was included in IWSLT 2008 in
which the participants need to translate Chinese to
Spanish via English. A Chinese-English and an
English-Spanish data were supplied to carry out
the experiments. The entire training corpus was
tokenized and lowercased. Table 7 and Table 8
summarize the training data and test data.
Table 9 shows the similar tendency with Table 4.
The random walk models achieved BLEU% scores
32.09, which achieved an absolute improvement of
2.08 percentages points on BLEU over the base-
line.
</bodyText>
<table confidence="0.999216">
Corpus Sentence Source Target
pair # word # word #
CE 20,000 135,518 182,793
ES 19,972 153,178 147,560
</table>
<tableCaption confidence="0.998183">
Table 7: Training Data of IWSLT2008
</tableCaption>
<sectionHeader confidence="0.974295" genericHeader="method">
6 Discussions
</sectionHeader>
<table confidence="0.994616">
Test Set Sentence # Reference #
IWSLT08 507 16
Table8. Test Data of IWSLT2008
System BLEU% phrase pairs #
Baseline 30.01 143,790
+pruning 30.25 108,407
RW 31.57 2,760,439
+pre-pruning 31.99 1,845,648
+post-pruning 32.09* 1,514,694
</table>
<tableCaption confidence="0.737361">
Table9. Results on IWSLT2008
</tableCaption>
<subsectionHeader confidence="0.864525">
5.4 Experiments on Web Data
</subsectionHeader>
<bodyText confidence="0.9999534">
The setting with Europarl data is quite artificial as
the training data for directly translating between
source and target actually exists in the original
data sets. The IWSLT data set is too small to rep-
resent the real scenario. Thus we try our experi-
ment on a more realistic scenario: translating from
The random walk approach mainly improves the
performance of pivot translation in two aspects:
reduces the OOVs and provides more hypothesis
phrases for decoding.
</bodyText>
<subsectionHeader confidence="0.858126">
6.1 OOV
</subsectionHeader>
<bodyText confidence="0.999939461538462">
Out-of-vocabulary (OOV 7 ) terms cause serious
problems for machine translation systems (Zhang
et al., 2005). The random walk model can reduce
the OOVs. As illustrated in Figure 1, the Chinese
phrase “很可口henkekou” cannot be connected to
any Spanish phrase, thus it is a OOV term.
We count the OOVs when decoding with trian-
gulation model and random walk model on
IWSLT2008 data. The statistics shows that when
using triangulation model, there are 11% OOVs
when using triangulation model, compared with
9.6% when using random walk model. Less OOV
often lead to a better result.
</bodyText>
<page confidence="0.8600515">
7 OOV refer to phrases here.
532
</page>
<subsectionHeader confidence="0.99583">
6.2 Hypothesis Phrases
</subsectionHeader>
<bodyText confidence="0.999650666666667">
To illustrate how the random walk method helps
improve the performance of machine translation,
we illustrate an example as follows:
</bodyText>
<table confidence="0.39430675">
- Source: 我 想 要 枕头
wo xiang yao zhentou
- Baseline trans: Quiero almohada
- Random Walk trans: Quiero una almohada
</table>
<bodyText confidence="0.999728090909091">
For translating a Chinese sentence “我想要枕头
wo xiang yao zhentou” to Spanish, we can get two
candidate translations. In this case, the random
walk translation is better than the baseline system.
The key phrase in this sentence is “枕头 zhentou”,
figure 5 shows the extension process. In this case,
the article “a” is hidden in the source-pivot phrase
table. The same situation often occurs in articles
and prepositions. Random walk is able to discover
the hidden relations (hypothesis phrases) among
source, pivot and target phrases.
</bodyText>
<figure confidence="0.9698655">
枕头 pillow
zhentou
个枕头 a pillow
ge zhentou
</figure>
<figureCaption confidence="0.9886375">
Figure 5: Phrase extension process. The dotted line
indicates an implicit relation in the phrase table.
</figureCaption>
<sectionHeader confidence="0.976214" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999725625">
In this paper, we proposed a random walk method
to improve pivot-based statistical machine transla-
tion. The random walk method can find implicit
relations between phrases in the source and target
languages. Therefore, more source-target phrase
pairs can be obtained than conventional pivot-
based method. Experimental results show that our
method achieves significant improvements over
the baseline on Europarl corpus, spoken language
data and the web data.
A critical problem in the approach is the noise
that may bring in. In this paper, we used a simple
filtering to reduce the noise. Although the filtering
method is effective, other method may work better.
In the future, we plan to explore more approaches
for phrase table pruning.
</bodyText>
<sectionHeader confidence="0.99547" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999964222222222">
We would like to thank Jianyun Nie, Muyun Yang
and Lemao Liu for insightful discussions, and
three anonymous reviewers for many invaluable
comments and suggestions to improve our paper.
This work is supported by National Natural Sci-
ence Foundation of China (61100093), and the
Key Project of the National High Technology Re-
search and Development Program of China
(2011AA01A207).
</bodyText>
<sectionHeader confidence="0.997843" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999669594594595">
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics, pages
597-604
Sergey Brin and Lawrence Page. 1998. The Anatomy of
a Large-Scale Hypertextual Web Search Engine. In
Proceedings of the Seventh International World
Wide Web Conference
Trevor Cohn and Mirella Lapata. 2007. Machine Trans-
lation by Triangulation: Make Effective Use of Mul-
ti-Parallel Corpora. In Proceedings of 45th Annual
Meeting of the Association for Computational Lin-
guistics, pages 828-735.
Marta R. Costa-jussà, Carlos Henríquez, and Rafael E.
Banchs. 2011. Enhancing Scarce-Resource Language
Translation through Pivot Combinations. In Proceed-
ings of the 5th International Joint Conference on
Natural Language Processing, pages 1361-1365
Nick Craswell and Martin Szummer. 2007. Random
Walks on the Click Graph. In Proceedings of the
30th annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 239-246
Yiming Cui, Conghui Zhu, Xiaoning Zhu, Tiejun Zhao
and Dequan Zheng. 2013. Phrase Table Combination
Deficiency Analyses in Pivot-based SMT. In Pro-
ceedings of 18th International Conference on Appli-
cation of Natural Language to Information Systems,
pages 355-358.
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime
Tsukada and Masaaki Nagata. 2011. Generalized
Minimum Bayes Risk System Combination. In Pro-
ceedings of the 5th International Joint Conference
on Natural Language Processing, pages 1356–1360
Jesús González-Rubio, Alfons Juan and Francisco
Casacuberta. 2011. Minimum Bayes-risk System
</reference>
<figure confidence="0.876526333333333">
almohada
una
almohada
</figure>
<page confidence="0.993043">
533
</page>
<reference confidence="0.999884123595506">
Combination. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1268–1277
Zhongjun He, Yao Meng, Yajuan Lü, Hao Yu and Qun
Liu. 2009. Reducing SMT Rule Table with Mono-
lingual Key Phrase. In Proceedings of the ACL-
IJCILP 2009 Conference Short Papers, pages 121-
124
Howard Johnson, Joel Martin, George Foster, and Ro-
land Kuhn. 2007. Improving translation quality by
discarding most of the phrase table. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Iatural Language Processing and Computational
Iatural Language Learning, pages 967–975.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In HLT-IAACL:
Human Language Technology Conference of the
Iorth American Chapter of the Association for
Computational Linguistics, pages 127-133
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of the
2004 Conference on Empirical Methods in Iatural
Language Processing (EMILP), pages 388–395.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit X, pages 79-86.
Philipp Koehn, Hieu Hoang, Alexanda Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Rich-
ard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics, demon-
stration session, pages 177–180.
Franz Josef Och and Hermann Ney. 2000. A compari-
son of alignment models for statistical machine
translation. In Proceedings of the 18th International
Conference on Computational Linguistics, pages
1086–1090
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computation Linguistics, pages 311-319
Karl Pearson. 1905. The Problem of the Random Walk.
Iature, 27(1865):294
Mydans, Seth. 2011. Across cultures, English is the
word. Iew York Times.
Martin Szummer and Tommi Jaakkola. 2002. Partially
Labeled Classification with Markov Random Walks.
In Advances in Ieural Information Processing Sys-
tems, pages 945-952
Kristina Toutanova, Christopher D. Manning and An-
drew Y. Ng. 2004. Learning Random Walk Models
for Inducting Word Dependency Distributions. In
Proceedings of the 21st International Conference on
Machine Learning.
Masao Utiyama and Hitoshi Isahara. 2007. A Compari-
son of Pivot Methods for Phrase-Based Statistical
Machine Translation. In Proceedings of Human
Language Technology: the Conference of the Iorth
American Chapter of the Association for Computa-
tional Linguistics, pages 484-491
Masao Utiyama, Andrew Finch, Hideo Okuma, Michael
Paul, Hailong Cao, Hirofumi Yamamoto, Keiji Ya-
suda, and Eiichiro Sumita. 2008. The NICT/ATR
speech Translation System for IWSLT 2008. In Pro-
ceedings of the International Workshop on Spoken
Language Translation, pages 77-84
Haifeng Wang, Hua Wu, Xiaoguang Hu, Zhanyi Liu,
Jianfeng Li, Dengjun Ren, and Zhengyu Niu. 2008.
The TCH Machine Translation System for IWSLT
2008. In Proceedings of the International Workshop
on Spoken Language Translation, pages 124-131
Hua Wu and Haifeng Wang. 2007. Pivot Language Ap-
proach for Phrase-Based Statistical Machine Transla-
tion. In Proceedings of 45th Annual Meeting of the
Association for Computational Linguistics, pages
856-863.
Hua Wu and Haifeng Wang. 2009. Revisiting Pivot
Language Approach for Machine Translation. In
Proceedings of the 47th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 4th
IJCILP of the AFILP, pages 154-162
Ying Zhang, Fei Huang, Stephan Vogel. 2005. Mining
translations of OOV terms from the web through
cross-lingual query expansion. In Proceedings of the
27th ACM SIGIR. pages 524-525
</reference>
<page confidence="0.998328">
534
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.672727">
<title confidence="0.976674">Improving Pivot-Based Statistical Machine Using Random Walk</title>
<author confidence="0.975708">Zhongjun Hua Haifeng</author>
<affiliation confidence="0.89411">and Tiejun Institute of Technology, Harbin,</affiliation>
<address confidence="0.939733">Inc., Beijing,</address>
<email confidence="0.9806825">xnzhu@baidu.com</email>
<email confidence="0.9806825">chzhu@baidu.com</email>
<email confidence="0.9806825">{hezhongjun@baidu.com</email>
<email confidence="0.9806825">wu_hua@baidu.com</email>
<email confidence="0.9806825">wanghaifeng@baidu.com</email>
<abstract confidence="0.999455631578947">This paper proposes a novel approach that utilizes a machine learning method to improve pivot-based statistical machine translation (SMT). For language pairs with few bilingual data, a possible solution in pivot-based SMT using another language as a &amp;quot;bridge&amp;quot; to generate source-target translation. However, one of the weaknesses is that some useful sourcetarget translations cannot be generated if the corresponding source phrase and target phrase connect to different pivot phrases. To alleviate the problem, we utilize Markov random walks to connect possible translation phrases between source and target language. Experimental results on European Parliament data, spoken language data and web data show that our method leads to significant improvements on all the tasks over the baseline system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with Bilingual Parallel Corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>597--604</pages>
<contexts>
<context position="13683" citStr="Bannard and Callison-Burch, 2005" startWordPosition="2359" endWordPosition="2362">denote matrix A contains s+p nodes (s source phrases and p pivot phrases) to represent the translation graph. A g =     (6) ij s p s p ( ) ( ) + × + where gij is the i,j-th elements of matrix A. We can split the matrix A into 4 sub-matrixes: = 0s×s Asp A Aps 0p×p  where the sub-matrix Asp = [pikJs×p represents the translation probabilities from source to pivot language, and Aps represents the similar meaning. Take 3 steps walks as an example: Step1:  0 A  s s × sp A A =   0  ps p p ×  Step2: × A A 0  2 sp ps s p × A =   0 A A ×  p s × ps sp  Step3: analogous to paraphrasing (Bannard and Callison-Burch, 2005). For the example shown in figure 1 as an example, the hidden relation between “很可口 henkekou” and “非常好吃 feichanghaochi” can be found through Step 2. 3. The third step describes the following procedure: S-P-S’-P’. An extended source-pivot phrase table is generated by 3-step random walks. Compared with the initial phrase table in Step1, although the number of phrases is not increased, the relations between phrase pairs are increased and more translation rules can be obtained. Still for the example in Figure 1 , the hidden relation between “很可口 henkekou” and “really delicious” can be generated in</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with Bilingual Parallel Corpora. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 597-604</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The Anatomy of a Large-Scale Hypertextual Web Search Engine.</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventh International World Wide Web Conference</booktitle>
<contexts>
<context position="2588" citStr="Brin and Page (1998)" startWordPosition="382" endWordPosition="385">rce-pivot phrase table and the pivot-target phrase table. However, one of the weaknesses is that some corresponding source and target phrase pairs cannot be generated, because they are connected to different pivot phrases (Cui et al., 2013). As illustrated in Figure 1, since there is no direct translation between “很 可口 henkekou” and “really delicious”, the triangulation method is unable to establish a relation between “很可口 henkekou” and the two Spanish phrases. To solve this problem, we apply a Markov random walk method to pivot-based SMT system. Random walk has been widely used. For example, Brin and Page (1998) used random walk to discover potential relations between queries and documents for link analysis in information retrieval. Analogous to link analysis, the aim of pivot-based translation is to discover potential translations between source and target language via the pivot language. * This work was done when the first author was visiting Baidu. 524 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 524–534, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 49011� feichanghaochi fP,_WTF1 henkekou Chinese Eng</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The Anatomy of a Large-Scale Hypertextual Web Search Engine. In Proceedings of the Seventh International World Wide Web Conference</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Machine Translation by Triangulation: Make Effective Use of Multi-Parallel Corpora.</title>
<date>2007</date>
<booktitle>In Proceedings of 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>828--735</pages>
<contexts>
<context position="1803" citStr="Cohn and Lapata, 2007" startWordPosition="256" endWordPosition="259">ilingual corpora to build translation models. The amount and the quality of the bilingual data strongly affect the performance of SMT systems. For resource-rich language pairs, such as ChineseEnglish, it is easy to collect large amounts of bilingual corpus. However, for resource-poor language pairs, such as Chinese-Spanish, it is difficult to build a high-performance SMT system with the small scale bilingual data available. The pivot language approach, which performs translation through a third language, provides a possible solution to the problem. The triangulation method (Wu and Wang, 2007; Cohn and Lapata, 2007) is a representative work for pivot-based machine translation. With a triangulation pivot approach, a source-target phrase table can be obtained by combining the source-pivot phrase table and the pivot-target phrase table. However, one of the weaknesses is that some corresponding source and target phrase pairs cannot be generated, because they are connected to different pivot phrases (Cui et al., 2013). As illustrated in Figure 1, since there is no direct translation between “很 可口 henkekou” and “really delicious”, the triangulation method is unable to establish a relation between “很可口 henkekou</context>
<context position="5949" citStr="Cohn and Lapata 2007" startWordPosition="895" endWordPosition="898">et translation model (Utiyama et al., 2008; Wu and Wang, 2009). For example, we can translate each pivot sentence in the pivot-target corpus to source language with a pivot-source model, and then combine the translated source sentence with the target sentence to obtain a synthetic source-target corpus, and vice versa. However, it is difficult to build a high quality translation system with a corpus created by a machine translation system. Triangulation Method: The triangulation method obtains source-target model by combining source-pivot and pivot-target translation models (Wu and Wang, 2007; Cohn and Lapata 2007), which has been shown to work better than the other pivot approaches (Utiyama and Isahara, 2007). As we mentioned earlier, the weakness of triangulation is that the corresponding source and target phrase pairs cannot be connected in the case that they connect to different pivot phrases. 3 The Triangulation Method In this section, we review the triangulation method for pivot-based translation. With the two additional bilingual corpora, the source-pivot and pivot-target translation models can be trained. Thus, a pivot model can be obtained by merging these two models. In the translation model, </context>
</contexts>
<marker>Cohn, Lapata, 2007</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2007. Machine Translation by Triangulation: Make Effective Use of Multi-Parallel Corpora. In Proceedings of 45th Annual Meeting of the Association for Computational Linguistics, pages 828-735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta R Costa-jussà</author>
<author>Carlos Henríquez</author>
<author>Rafael E Banchs</author>
</authors>
<title>Enhancing Scarce-Resource Language Translation through Pivot Combinations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1361--1365</pages>
<contexts>
<context position="4511" citStr="Costa-jussà et al., 2011" startWordPosition="668" endWordPosition="671">. The remainder of this paper is organized as follows. In section 2, we describe the related work. We review the triangulation method for pivotbased machine translation in section 3. Section 4 describes the random walk models. In section 5 and section 6, we describe the experiments and analyze the performance, respectively. Section 7 gives a conclusion of the paper. 2 Related Work Several methods have been proposed for pivotbased translation. Typically, they can be classified into 3 kinds of methods: Transfer Method: Within the transfer framework (Utiyama and Isahara, 2007; Wang et al., 2008; Costa-jussà et al., 2011), a source sentence is first translated to n pivot sentences via a sourcepivot translation system, and then each pivot sentence is translated to m target sentences via a pivot-target translation system. At each step (source to pivot and pivot to target), multiple translation outputs will be generated, thus a minimum Bayesrisk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). A problem with the transfer method is that it needs to decode twice. On one hand, the time cost is doubled; on the other hand, the translation error of </context>
</contexts>
<marker>Costa-jussà, Henríquez, Banchs, 2011</marker>
<rawString>Marta R. Costa-jussà, Carlos Henríquez, and Rafael E. Banchs. 2011. Enhancing Scarce-Resource Language Translation through Pivot Combinations. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1361-1365</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Craswell</author>
<author>Martin Szummer</author>
</authors>
<title>Random Walks on the Click Graph.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>239--246</pages>
<marker>Craswell, Szummer, 2007</marker>
<rawString>Nick Craswell and Martin Szummer. 2007. Random Walks on the Click Graph. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 239-246</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Cui</author>
<author>Conghui Zhu</author>
</authors>
<title>Xiaoning Zhu, Tiejun Zhao and Dequan Zheng.</title>
<date>2013</date>
<booktitle>In Proceedings of 18th International Conference on Application of Natural Language to Information Systems,</booktitle>
<pages>355--358</pages>
<marker>Cui, Zhu, 2013</marker>
<rawString>Yiming Cui, Conghui Zhu, Xiaoning Zhu, Tiejun Zhao and Dequan Zheng. 2013. Phrase Table Combination Deficiency Analyses in Pivot-based SMT. In Proceedings of 18th International Conference on Application of Natural Language to Information Systems, pages 355-358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
</authors>
<title>Katsuhito Sudoh, Xianchao Wu, Hajime Tsukada and</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1356--1360</pages>
<marker>Duh, 2011</marker>
<rawString>Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime Tsukada and Masaaki Nagata. 2011. Generalized Minimum Bayes Risk System Combination. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1356–1360</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesús González-Rubio</author>
<author>Alfons Juan</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Minimum Bayes-risk System Combination.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1268--1277</pages>
<contexts>
<context position="4938" citStr="González-Rubio et al., 2011" startWordPosition="738" endWordPosition="741">votbased translation. Typically, they can be classified into 3 kinds of methods: Transfer Method: Within the transfer framework (Utiyama and Isahara, 2007; Wang et al., 2008; Costa-jussà et al., 2011), a source sentence is first translated to n pivot sentences via a sourcepivot translation system, and then each pivot sentence is translated to m target sentences via a pivot-target translation system. At each step (source to pivot and pivot to target), multiple translation outputs will be generated, thus a minimum Bayesrisk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). A problem with the transfer method is that it needs to decode twice. On one hand, the time cost is doubled; on the other hand, the translation error of the source-pivot translation system will be transferred to the pivot-target translation. Synthetic Method: A synthetic method creates a synthetic source-target corpus using source-pivot translation model or pivot-target translation model (Utiyama et al., 2008; Wu and Wang, 2009). For example, we can translate each pivot sentence in the pivot-target corpus to source language with a pivot-source model, and then combine the tr</context>
</contexts>
<marker>González-Rubio, Juan, Casacuberta, 2011</marker>
<rawString>Jesús González-Rubio, Alfons Juan and Francisco Casacuberta. 2011. Minimum Bayes-risk System Combination. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1268–1277</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongjun He</author>
<author>Yao Meng</author>
<author>Yajuan Lü</author>
<author>Hao Yu</author>
<author>Qun Liu</author>
</authors>
<title>Reducing SMT Rule Table with Monolingual Key Phrase.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACLIJCILP 2009 Conference Short Papers,</booktitle>
<pages>121--124</pages>
<contexts>
<context position="25292" citStr="He et al., 2009" startWordPosition="4263" endWordPosition="4266">ter than the baseline and the random walk approach with pruning. Moreover, the size of the phrase table is about half of the no-pruning method. When adopting a post-pruning method, the performance of translation did not improved significantly over the pre-pruning, but the scale of the phrase table dropped to 69M, which is only about 2 times larger than the triangulation method. Phrase table pruning is a key work to improve the performance of random walk. We plan to explore more approaches for phrase table pruning. E.g. using significance test (Johnson et al., 2007) or monolingual key phrases (He et al., 2009) to filter the phrase table. trans WMT WMT WMT language 06 07 08 Baseline pt-da-sv 23.40 22.80 23.75* RW 24.21* 22.4924.47* Baseline pt-de-sv 22.72 22.21 22.35* RW 23.26* 21.7623.12* Baseline pt-el-sv 22.53 22.19 22.40* RW 23.22* 21.3723.75* Baseline pt-en-sv 23.54 23.24 23.90* RW 24.22* 22.9024.66* Baseline 23.58 23.37 22.80 RW pt-es-sv 24.65* 24.10* 23.77* Baseline pt-fi-sv 21.06 20.06 20.28 RW 20.42* 20.2621.17 Baseline pt-fr-sv 23.55 23.09 23.96* RW 24.15* 22.8924.75* Baseline pt-it-sv 23.65 22.96 24.02* RW 24.18* 22.7924.74* Baseline pt-nl-sv 21.87 21.83 22.29* RW 22.76* 21.3623.06* Table</context>
</contexts>
<marker>He, Meng, Lü, Yu, Liu, 2009</marker>
<rawString>Zhongjun He, Yao Meng, Yajuan Lü, Hao Yu and Qun Liu. 2009. Reducing SMT Rule Table with Monolingual Key Phrase. In Proceedings of the ACLIJCILP 2009 Conference Short Papers, pages 121-124</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard Johnson</author>
<author>Joel Martin</author>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Improving translation quality by discarding most of the phrase table.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Iatural Language Processing and Computational Iatural Language Learning,</booktitle>
<pages>967--975</pages>
<contexts>
<context position="25247" citStr="Johnson et al., 2007" startWordPosition="4255" endWordPosition="4258">6, WMT07 and WMT08 respectively, which is much better than the baseline and the random walk approach with pruning. Moreover, the size of the phrase table is about half of the no-pruning method. When adopting a post-pruning method, the performance of translation did not improved significantly over the pre-pruning, but the scale of the phrase table dropped to 69M, which is only about 2 times larger than the triangulation method. Phrase table pruning is a key work to improve the performance of random walk. We plan to explore more approaches for phrase table pruning. E.g. using significance test (Johnson et al., 2007) or monolingual key phrases (He et al., 2009) to filter the phrase table. trans WMT WMT WMT language 06 07 08 Baseline pt-da-sv 23.40 22.80 23.75* RW 24.21* 22.4924.47* Baseline pt-de-sv 22.72 22.21 22.35* RW 23.26* 21.7623.12* Baseline pt-el-sv 22.53 22.19 22.40* RW 23.22* 21.3723.75* Baseline pt-en-sv 23.54 23.24 23.90* RW 24.22* 22.9024.66* Baseline 23.58 23.37 22.80 RW pt-es-sv 24.65* 24.10* 23.77* Baseline pt-fi-sv 21.06 20.06 20.28 RW 20.42* 20.2621.17 Baseline pt-fr-sv 23.55 23.09 23.96* RW 24.15* 22.8924.75* Baseline pt-it-sv 23.65 22.96 24.02* RW 24.18* 22.7924.74* Baseline pt-nl-sv 2</context>
</contexts>
<marker>Johnson, Martin, Foster, Kuhn, 2007</marker>
<rawString>Howard Johnson, Joel Martin, George Foster, and Roland Kuhn. 2007. Improving translation quality by discarding most of the phrase table. In Proceedings of the 2007 Joint Conference on Empirical Methods in Iatural Language Processing and Computational Iatural Language Learning, pages 967–975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In HLT-IAACL: Human Language Technology Conference of the Iorth American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="7371" citStr="Koehn et al. 2003" startWordPosition="1153" endWordPosition="1156">umes that there exist translations between phrases s and phrase p in source and pivot languages, and between phrase p and phrase t in pivot and target languages. The phrase translation probability φ between source and target languages is determined by the following model: φ ( |) s t = ∑ φ ( |, ) ( |) s p t p t φ p (1) ∑ φ φ ( |) ( |) s p p t p 3.2 Lexical Weight Given a phrase pair (s, t ) and a word alignment a between the source word positions i =1, ... , n and the target word positions j = 0,1, ..., m , the lexical weight of phrase pair (s, t ) can be calculated with the following formula (Koehn et al. 2003) : In formula 2, the lexical translation probability distribution ω(s |t) between source word s and target word t can be estimated with formula 3. ω (s |t) — count(s, t) E(3) — , count(s&amp;quot; t) Thus the alignment a between the source phrase s and target phrase t via pivot phrase p is needed for computing the lexical weight. The alignment a can be obtained as follows: a = {(s, t) |∃ p : (s, p)∈ a1 &amp; (p, t)∈ a2 } (4) where a1 and a2 indicate the word alignment between the phrase pair (s, p) and (p, t ) , respectively. The triangulation method requires that both the source and target phrases connect</context>
<context position="15905" citStr="Koehn et al., 2003" startWordPosition="2753" endWordPosition="2756"> &amp; (z, y) ∈ a2 } (8) where a1 and a2 represent the word alignment information inside the phrase pairs (x, y) and (y, z) respectively. An example of word alignment inducing is shown in Figure 4. With a new word alignment, the two lexical weights can be calculated by formula 2 and formula 3. Aps × Asp A3 = 0s×s A ×A ×A 0 Asp× psspps p p (7) 528 5 Experiments guages. Table 1 and Table 2 summarized the training data. 5.1 Translation System and Evaluation Metric In our experiments, the word alignment was obtained by GIZA++ (Och and Ney, 2000) and the heuristics “grow-diag-final” refinement rule. (Koehn et al., 2003). Our translation system is an in-house phrase-based system using a log-linear framework including a phrase translation model, a language model, a lexicalized reordering model, a word penalty model and a phrase penalty model, which is analogous to Moses (Koehn et al., 2007). The baseline system is the triangulation method based pivot approach (Wu and Wang, 2007). To evaluate the translation quality, we used BLEU (Papineni et al., 2002) as our evaluation metric. The statistical significance using 95% confidence intervals were measured with paired bootstrap resampling (Koehn, 2004). 5.2 Experime</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In HLT-IAACL: Human Language Technology Conference of the Iorth American Chapter of the Association for Computational Linguistics, pages 127-133</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Iatural Language Processing (EMILP),</booktitle>
<pages>388--395</pages>
<contexts>
<context position="16491" citStr="Koehn, 2004" startWordPosition="2844" endWordPosition="2845">rule. (Koehn et al., 2003). Our translation system is an in-house phrase-based system using a log-linear framework including a phrase translation model, a language model, a lexicalized reordering model, a word penalty model and a phrase penalty model, which is analogous to Moses (Koehn et al., 2007). The baseline system is the triangulation method based pivot approach (Wu and Wang, 2007). To evaluate the translation quality, we used BLEU (Papineni et al., 2002) as our evaluation metric. The statistical significance using 95% confidence intervals were measured with paired bootstrap resampling (Koehn, 2004). 5.2 Experiments on Europarl 5.2.1. Data sets We mainly test our approach on Europarl1 corpus, which is a multi-lingual corpus including 21 European languages. Due to the size of the data, we only select 11 languages which were added to Europarl from 04/1996 or 01/1997, including Danish (da), German (de), Greek (el), English (en), Spanish (es), Finnish (fi), French (fr), Italian (it) Dutch (nl) Portuguese (pt) and Swedish (sv). In order to avoid a trilingual scenario, we split the training corpus into 2 parts by the year of the data: the data released in odd years were used for training sourc</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Iatural Language Processing (EMILP), pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit X,</booktitle>
<pages>79--86</pages>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Proceedings of MT Summit X, pages 79-86.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexanda Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, demonstration session,</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="16179" citStr="Koehn et al., 2007" startWordPosition="2795" endWordPosition="2798">2 and formula 3. Aps × Asp A3 = 0s×s A ×A ×A 0 Asp× psspps p p (7) 528 5 Experiments guages. Table 1 and Table 2 summarized the training data. 5.1 Translation System and Evaluation Metric In our experiments, the word alignment was obtained by GIZA++ (Och and Ney, 2000) and the heuristics “grow-diag-final” refinement rule. (Koehn et al., 2003). Our translation system is an in-house phrase-based system using a log-linear framework including a phrase translation model, a language model, a lexicalized reordering model, a word penalty model and a phrase penalty model, which is analogous to Moses (Koehn et al., 2007). The baseline system is the triangulation method based pivot approach (Wu and Wang, 2007). To evaluate the translation quality, we used BLEU (Papineni et al., 2002) as our evaluation metric. The statistical significance using 95% confidence intervals were measured with paired bootstrap resampling (Koehn, 2004). 5.2 Experiments on Europarl 5.2.1. Data sets We mainly test our approach on Europarl1 corpus, which is a multi-lingual corpus including 21 European languages. Due to the size of the data, we only select 11 languages which were added to Europarl from 04/1996 or 01/1997, including Danish</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexanda Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, demonstration session, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A comparison of alignment models for statistical machine translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>1086--1090</pages>
<contexts>
<context position="15830" citStr="Och and Ney, 2000" startWordPosition="2743" endWordPosition="2746">⊗ can be induced with the following formula. a = {(x, y) |∃p : (x, z) ∈ a1 &amp; (z, y) ∈ a2 } (8) where a1 and a2 represent the word alignment information inside the phrase pairs (x, y) and (y, z) respectively. An example of word alignment inducing is shown in Figure 4. With a new word alignment, the two lexical weights can be calculated by formula 2 and formula 3. Aps × Asp A3 = 0s×s A ×A ×A 0 Asp× psspps p p (7) 528 5 Experiments guages. Table 1 and Table 2 summarized the training data. 5.1 Translation System and Evaluation Metric In our experiments, the word alignment was obtained by GIZA++ (Och and Ney, 2000) and the heuristics “grow-diag-final” refinement rule. (Koehn et al., 2003). Our translation system is an in-house phrase-based system using a log-linear framework including a phrase translation model, a language model, a lexicalized reordering model, a word penalty model and a phrase penalty model, which is analogous to Moses (Koehn et al., 2007). The baseline system is the triangulation method based pivot approach (Wu and Wang, 2007). To evaluate the translation quality, we used BLEU (Papineni et al., 2002) as our evaluation metric. The statistical significance using 95% confidence intervals</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. A comparison of alignment models for statistical machine translation. In Proceedings of the 18th International Conference on Computational Linguistics, pages 1086–1090</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computation Linguistics,</booktitle>
<pages>311--319</pages>
<contexts>
<context position="16344" citStr="Papineni et al., 2002" startWordPosition="2821" endWordPosition="2824">stem and Evaluation Metric In our experiments, the word alignment was obtained by GIZA++ (Och and Ney, 2000) and the heuristics “grow-diag-final” refinement rule. (Koehn et al., 2003). Our translation system is an in-house phrase-based system using a log-linear framework including a phrase translation model, a language model, a lexicalized reordering model, a word penalty model and a phrase penalty model, which is analogous to Moses (Koehn et al., 2007). The baseline system is the triangulation method based pivot approach (Wu and Wang, 2007). To evaluate the translation quality, we used BLEU (Papineni et al., 2002) as our evaluation metric. The statistical significance using 95% confidence intervals were measured with paired bootstrap resampling (Koehn, 2004). 5.2 Experiments on Europarl 5.2.1. Data sets We mainly test our approach on Europarl1 corpus, which is a multi-lingual corpus including 21 European languages. Due to the size of the data, we only select 11 languages which were added to Europarl from 04/1996 or 01/1997, including Danish (da), German (de), Greek (el), English (en), Spanish (es), Finnish (fi), French (fr), Italian (it) Dutch (nl) Portuguese (pt) and Swedish (sv). In order to avoid a </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computation Linguistics, pages 311-319</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Pearson</author>
</authors>
<date>1905</date>
<booktitle>The Problem of the Random Walk. Iature,</booktitle>
<volume>27</volume>
<issue>1865</issue>
<marker>Pearson, 1905</marker>
<rawString>Karl Pearson. 1905. The Problem of the Random Walk. Iature, 27(1865):294</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seth Mydans</author>
</authors>
<title>Across cultures, English is the word. Iew York Times.</title>
<date>2011</date>
<contexts>
<context position="17340" citStr="Mydans, 2011" startWordPosition="2987" endWordPosition="2988">o Europarl from 04/1996 or 01/1997, including Danish (da), German (de), Greek (el), English (en), Spanish (es), Finnish (fi), French (fr), Italian (it) Dutch (nl) Portuguese (pt) and Swedish (sv). In order to avoid a trilingual scenario, we split the training corpus into 2 parts by the year of the data: the data released in odd years were used for training source-pivot model and the data released in even years were used for training pivot-target model. We perform our experiments on different translation directions and via different pivot languages. As a most widely used language in the world (Mydans, 2011), English was used as the pivot language for granted when carrying out experiments on different translation directions. For translating Portuguese to Swedish, we also tried to perform our experiments via different pivot lan1 http://www.statmt.org/europarl/ Language Sentence Language Sentence Pairs Pairs # Pairs Pairs # (src-pvt) (pvt-tgt) da-en 974,189 en-da 953,002 de-en 983,411 en-de 905,167 el-en 609,315 en-el 596,331 es-en 968,527 en-es 961,782 fi-en 998,429 en-fi 903,689 fr-en 989,652 en-fr 974,637 it-en 934,448 en-it 938,573 nl-en 982,696 en-nl 971,379 pt-en 967,816 en-pt 960,214 sv-en 9</context>
</contexts>
<marker>Mydans, 2011</marker>
<rawString>Mydans, Seth. 2011. Across cultures, English is the word. Iew York Times.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Szummer</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Partially Labeled Classification with Markov Random Walks.</title>
<date>2002</date>
<booktitle>In Advances in Ieural Information Processing Systems,</booktitle>
<pages>945--952</pages>
<contexts>
<context position="8981" citStr="Szummer and Jaakkola, 2002" startWordPosition="1438" endWordPosition="1441">rs are stored in a phrase table. In our random walk approach, we first build a translation graph according to the phrase table. A translation graph contains two types of nodes: source phrase and target phrase. A source phrase s and a target phrase t are connected if exists a phrase pair (s, t ) in the phrase table. The edge can be weighted according to translation probabilities or alignments in the phrase table. For the pivot-based translation, the translation graph can be derived from the source-pivot phrase table and pivot-target phrase table. Our random walk model is inspired by two works (Szummer and Jaakkola, 2002; Craswell and Szummer,2007). The general process of random walk can be described as follows: Let G = (V, E) be a directed graph with n vertices and m edges. For a vertex v ∈ V, Γ(v) denotes the set of neighbors of v in G . A random walk on G follows the following process: start at a vertex v0 , chose and walk along a random neighbor v1 , with v1 ∈Γ (v0) . At the second step, start from v1 and chose a random neighbor v2, and so on. Let S be the set of source phrases, and P be the set of pivot phrases. Then the nodes V are the union of S and P. The edges E correspond to the relations between ph</context>
</contexts>
<marker>Szummer, Jaakkola, 2002</marker>
<rawString>Martin Szummer and Tommi Jaakkola. 2002. Partially Labeled Classification with Markov Random Walks. In Advances in Ieural Information Processing Systems, pages 945-952</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning Random Walk Models for Inducting Word Dependency Distributions.</title>
<date>2004</date>
<booktitle>In Proceedings of the 21st International Conference on Machine Learning.</booktitle>
<marker>Toutanova, Manning, Ng, 2004</marker>
<rawString>Kristina Toutanova, Christopher D. Manning and Andrew Y. Ng. 2004. Learning Random Walk Models for Inducting Word Dependency Distributions. In Proceedings of the 21st International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A Comparison of Pivot Methods for Phrase-Based Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technology: the Conference of the Iorth American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>484--491</pages>
<contexts>
<context position="4465" citStr="Utiyama and Isahara, 2007" startWordPosition="660" endWordPosition="663">ional pivot-based method, triangulation method. The remainder of this paper is organized as follows. In section 2, we describe the related work. We review the triangulation method for pivotbased machine translation in section 3. Section 4 describes the random walk models. In section 5 and section 6, we describe the experiments and analyze the performance, respectively. Section 7 gives a conclusion of the paper. 2 Related Work Several methods have been proposed for pivotbased translation. Typically, they can be classified into 3 kinds of methods: Transfer Method: Within the transfer framework (Utiyama and Isahara, 2007; Wang et al., 2008; Costa-jussà et al., 2011), a source sentence is first translated to n pivot sentences via a sourcepivot translation system, and then each pivot sentence is translated to m target sentences via a pivot-target translation system. At each step (source to pivot and pivot to target), multiple translation outputs will be generated, thus a minimum Bayesrisk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). A problem with the transfer method is that it needs to decode twice. On one hand, the time cost is doubled</context>
<context position="6046" citStr="Utiyama and Isahara, 2007" startWordPosition="912" endWordPosition="915">e each pivot sentence in the pivot-target corpus to source language with a pivot-source model, and then combine the translated source sentence with the target sentence to obtain a synthetic source-target corpus, and vice versa. However, it is difficult to build a high quality translation system with a corpus created by a machine translation system. Triangulation Method: The triangulation method obtains source-target model by combining source-pivot and pivot-target translation models (Wu and Wang, 2007; Cohn and Lapata 2007), which has been shown to work better than the other pivot approaches (Utiyama and Isahara, 2007). As we mentioned earlier, the weakness of triangulation is that the corresponding source and target phrase pairs cannot be connected in the case that they connect to different pivot phrases. 3 The Triangulation Method In this section, we review the triangulation method for pivot-based translation. With the two additional bilingual corpora, the source-pivot and pivot-target translation models can be trained. Thus, a pivot model can be obtained by merging these two models. In the translation model, the phrase translation probability and the lexical weight are language dependent, which will be i</context>
</contexts>
<marker>Utiyama, Isahara, 2007</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2007. A Comparison of Pivot Methods for Phrase-Based Statistical Machine Translation. In Proceedings of Human Language Technology: the Conference of the Iorth American Chapter of the Association for Computational Linguistics, pages 484-491</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Andrew Finch</author>
<author>Hideo Okuma</author>
<author>Michael Paul</author>
<author>Hailong Cao</author>
<author>Hirofumi Yamamoto</author>
<author>Keiji Yasuda</author>
<author>Eiichiro Sumita</author>
</authors>
<date>2008</date>
<booktitle>The NICT/ATR speech Translation System for IWSLT</booktitle>
<pages>77--84</pages>
<contexts>
<context position="5370" citStr="Utiyama et al., 2008" startWordPosition="806" endWordPosition="809">vot to target), multiple translation outputs will be generated, thus a minimum Bayesrisk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). A problem with the transfer method is that it needs to decode twice. On one hand, the time cost is doubled; on the other hand, the translation error of the source-pivot translation system will be transferred to the pivot-target translation. Synthetic Method: A synthetic method creates a synthetic source-target corpus using source-pivot translation model or pivot-target translation model (Utiyama et al., 2008; Wu and Wang, 2009). For example, we can translate each pivot sentence in the pivot-target corpus to source language with a pivot-source model, and then combine the translated source sentence with the target sentence to obtain a synthetic source-target corpus, and vice versa. However, it is difficult to build a high quality translation system with a corpus created by a machine translation system. Triangulation Method: The triangulation method obtains source-target model by combining source-pivot and pivot-target translation models (Wu and Wang, 2007; Cohn and Lapata 2007), which has been show</context>
</contexts>
<marker>Utiyama, Finch, Okuma, Paul, Cao, Yamamoto, Yasuda, Sumita, 2008</marker>
<rawString>Masao Utiyama, Andrew Finch, Hideo Okuma, Michael Paul, Hailong Cao, Hirofumi Yamamoto, Keiji Yasuda, and Eiichiro Sumita. 2008. The NICT/ATR speech Translation System for IWSLT 2008. In Proceedings of the International Workshop on Spoken Language Translation, pages 77-84</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haifeng Wang</author>
<author>Hua Wu</author>
<author>Xiaoguang Hu</author>
<author>Zhanyi Liu</author>
<author>Jianfeng Li</author>
<author>Dengjun Ren</author>
<author>Zhengyu Niu</author>
</authors>
<date>2008</date>
<booktitle>The TCH Machine Translation System for IWSLT</booktitle>
<pages>124--131</pages>
<contexts>
<context position="4484" citStr="Wang et al., 2008" startWordPosition="664" endWordPosition="667">riangulation method. The remainder of this paper is organized as follows. In section 2, we describe the related work. We review the triangulation method for pivotbased machine translation in section 3. Section 4 describes the random walk models. In section 5 and section 6, we describe the experiments and analyze the performance, respectively. Section 7 gives a conclusion of the paper. 2 Related Work Several methods have been proposed for pivotbased translation. Typically, they can be classified into 3 kinds of methods: Transfer Method: Within the transfer framework (Utiyama and Isahara, 2007; Wang et al., 2008; Costa-jussà et al., 2011), a source sentence is first translated to n pivot sentences via a sourcepivot translation system, and then each pivot sentence is translated to m target sentences via a pivot-target translation system. At each step (source to pivot and pivot to target), multiple translation outputs will be generated, thus a minimum Bayesrisk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). A problem with the transfer method is that it needs to decode twice. On one hand, the time cost is doubled; on the other hand</context>
</contexts>
<marker>Wang, Wu, Hu, Liu, Li, Ren, Niu, 2008</marker>
<rawString>Haifeng Wang, Hua Wu, Xiaoguang Hu, Zhanyi Liu, Jianfeng Li, Dengjun Ren, and Zhengyu Niu. 2008. The TCH Machine Translation System for IWSLT 2008. In Proceedings of the International Workshop on Spoken Language Translation, pages 124-131</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
</authors>
<title>Pivot Language Approach for Phrase-Based Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>856--863</pages>
<contexts>
<context position="1779" citStr="Wu and Wang, 2007" startWordPosition="252" endWordPosition="255">lation (SMT) uses bilingual corpora to build translation models. The amount and the quality of the bilingual data strongly affect the performance of SMT systems. For resource-rich language pairs, such as ChineseEnglish, it is easy to collect large amounts of bilingual corpus. However, for resource-poor language pairs, such as Chinese-Spanish, it is difficult to build a high-performance SMT system with the small scale bilingual data available. The pivot language approach, which performs translation through a third language, provides a possible solution to the problem. The triangulation method (Wu and Wang, 2007; Cohn and Lapata, 2007) is a representative work for pivot-based machine translation. With a triangulation pivot approach, a source-target phrase table can be obtained by combining the source-pivot phrase table and the pivot-target phrase table. However, one of the weaknesses is that some corresponding source and target phrase pairs cannot be generated, because they are connected to different pivot phrases (Cui et al., 2013). As illustrated in Figure 1, since there is no direct translation between “很 可口 henkekou” and “really delicious”, the triangulation method is unable to establish a relati</context>
<context position="5926" citStr="Wu and Wang, 2007" startWordPosition="891" endWordPosition="894">model or pivot-target translation model (Utiyama et al., 2008; Wu and Wang, 2009). For example, we can translate each pivot sentence in the pivot-target corpus to source language with a pivot-source model, and then combine the translated source sentence with the target sentence to obtain a synthetic source-target corpus, and vice versa. However, it is difficult to build a high quality translation system with a corpus created by a machine translation system. Triangulation Method: The triangulation method obtains source-target model by combining source-pivot and pivot-target translation models (Wu and Wang, 2007; Cohn and Lapata 2007), which has been shown to work better than the other pivot approaches (Utiyama and Isahara, 2007). As we mentioned earlier, the weakness of triangulation is that the corresponding source and target phrase pairs cannot be connected in the case that they connect to different pivot phrases. 3 The Triangulation Method In this section, we review the triangulation method for pivot-based translation. With the two additional bilingual corpora, the source-pivot and pivot-target translation models can be trained. Thus, a pivot model can be obtained by merging these two models. In </context>
<context position="16269" citStr="Wu and Wang, 2007" startWordPosition="2809" endWordPosition="2812">s. Table 1 and Table 2 summarized the training data. 5.1 Translation System and Evaluation Metric In our experiments, the word alignment was obtained by GIZA++ (Och and Ney, 2000) and the heuristics “grow-diag-final” refinement rule. (Koehn et al., 2003). Our translation system is an in-house phrase-based system using a log-linear framework including a phrase translation model, a language model, a lexicalized reordering model, a word penalty model and a phrase penalty model, which is analogous to Moses (Koehn et al., 2007). The baseline system is the triangulation method based pivot approach (Wu and Wang, 2007). To evaluate the translation quality, we used BLEU (Papineni et al., 2002) as our evaluation metric. The statistical significance using 95% confidence intervals were measured with paired bootstrap resampling (Koehn, 2004). 5.2 Experiments on Europarl 5.2.1. Data sets We mainly test our approach on Europarl1 corpus, which is a multi-lingual corpus including 21 European languages. Due to the size of the data, we only select 11 languages which were added to Europarl from 04/1996 or 01/1997, including Danish (da), German (de), Greek (el), English (en), Spanish (es), Finnish (fi), French (fr), Ita</context>
</contexts>
<marker>Wu, Wang, 2007</marker>
<rawString>Hua Wu and Haifeng Wang. 2007. Pivot Language Approach for Phrase-Based Statistical Machine Translation. In Proceedings of 45th Annual Meeting of the Association for Computational Linguistics, pages 856-863.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
</authors>
<title>Revisiting Pivot Language Approach for Machine Translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th IJCILP of the AFILP,</booktitle>
<pages>154--162</pages>
<contexts>
<context position="5390" citStr="Wu and Wang, 2009" startWordPosition="810" endWordPosition="813">le translation outputs will be generated, thus a minimum Bayesrisk system combination method is often used to select the optimal sentence (González-Rubio et al., 2011; Duh et al., 2011). A problem with the transfer method is that it needs to decode twice. On one hand, the time cost is doubled; on the other hand, the translation error of the source-pivot translation system will be transferred to the pivot-target translation. Synthetic Method: A synthetic method creates a synthetic source-target corpus using source-pivot translation model or pivot-target translation model (Utiyama et al., 2008; Wu and Wang, 2009). For example, we can translate each pivot sentence in the pivot-target corpus to source language with a pivot-source model, and then combine the translated source sentence with the target sentence to obtain a synthetic source-target corpus, and vice versa. However, it is difficult to build a high quality translation system with a corpus created by a machine translation system. Triangulation Method: The triangulation method obtains source-target model by combining source-pivot and pivot-target translation models (Wu and Wang, 2007; Cohn and Lapata 2007), which has been shown to work better tha</context>
</contexts>
<marker>Wu, Wang, 2009</marker>
<rawString>Hua Wu and Haifeng Wang. 2009. Revisiting Pivot Language Approach for Machine Translation. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th IJCILP of the AFILP, pages 154-162</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Fei Huang</author>
<author>Stephan Vogel</author>
</authors>
<title>Mining translations of OOV terms from the web through cross-lingual query expansion.</title>
<date>2005</date>
<booktitle>In Proceedings of the 27th ACM SIGIR.</booktitle>
<pages>524--525</pages>
<contexts>
<context position="29086" citStr="Zhang et al., 2005" startWordPosition="4864" endWordPosition="4867">WSLT2008 5.4 Experiments on Web Data The setting with Europarl data is quite artificial as the training data for directly translating between source and target actually exists in the original data sets. The IWSLT data set is too small to represent the real scenario. Thus we try our experiment on a more realistic scenario: translating from The random walk approach mainly improves the performance of pivot translation in two aspects: reduces the OOVs and provides more hypothesis phrases for decoding. 6.1 OOV Out-of-vocabulary (OOV 7 ) terms cause serious problems for machine translation systems (Zhang et al., 2005). The random walk model can reduce the OOVs. As illustrated in Figure 1, the Chinese phrase “很可口henkekou” cannot be connected to any Spanish phrase, thus it is a OOV term. We count the OOVs when decoding with triangulation model and random walk model on IWSLT2008 data. The statistics shows that when using triangulation model, there are 11% OOVs when using triangulation model, compared with 9.6% when using random walk model. Less OOV often lead to a better result. 7 OOV refer to phrases here. 532 6.2 Hypothesis Phrases To illustrate how the random walk method helps improve the performance of ma</context>
</contexts>
<marker>Zhang, Huang, Vogel, 2005</marker>
<rawString>Ying Zhang, Fei Huang, Stephan Vogel. 2005. Mining translations of OOV terms from the web through cross-lingual query expansion. In Proceedings of the 27th ACM SIGIR. pages 524-525</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
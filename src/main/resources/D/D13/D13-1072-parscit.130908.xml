<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993069">
Exploiting language models for visual recognition
</title>
<author confidence="0.993458">
Dieu-Thu Le Jasper Uijlings Raffaella Bernardi
</author>
<affiliation confidence="0.99562">
DISI, University of Trento DISI, University of Trento DISI, University of Trento
</affiliation>
<address confidence="0.638461">
Povo, 38123, Italy Povo, 38123, Italy Povo, 38123, Italy
</address>
<email confidence="0.996274">
dle@disi.unitn.it jrr@disi.unitn.it bernardi@disi.unitn.it
</email>
<sectionHeader confidence="0.996618" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999215">
The problem of learning language models
from large text corpora has been widely stud-
ied within the computational linguistic com-
munity. However, little is known about the
performance of these language models when
applied to the computer vision domain. In this
work, we compare representative models: a
window-based model, a topic model, a distri-
butional memory and a commonsense knowl-
edge database, ConceptNet, in two visual
recognition scenarios: human action recog-
nition and object prediction. We examine
whether the knowledge extracted from texts
through these models are compatible to the
knowledge represented in images. We de-
termine the usefulness of different language
models in aiding the two visual recognition
tasks. The study shows that the language
models built from general text corpora can be
used instead of expensive annotated images
and even outperform the image model when
testing on a big general dataset.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99976525">
Computational linguistics have created many tools
for automatic knowledge acquisition which have
been successfully applied in many tasks inside the
language domain, such as question answering, ma-
chine translation, semantic web, etc. In this paper
we ask whether such knowledge generalizes to the
observed reality outside the language domain, where
we use well-known image datasets as a proxy for ob-
served reality.
In particular, we aim to determine which language
model yields knowledge that is most suitable for use
in Computer Vision. Therefore we test a variety of
language models and a linguistically mined knowl-
edge base within two computer vision scenarios:
Human action recognition : Recognizing
&lt;subject, verb, object&gt; triples based on
objects (e.g., car, horse) and scenes (the place
that the actions occur, e.g., countryside, forest,
office) recognized in images. In this scenario,
we only consider images with human actions
so the “human” subject is always present.
Objects in context : Predicting the most likely
identity of an object given its context as ex-
pressed in terms of co-occurring objects.
Computer vision can greatly benefit from natural
language processing as learning from images re-
quires a prohibitively expensive annotation effort. A
major goal of natural language processing is to ob-
tain general knowledge from text and in this paper
we test which model provides the best knowledge
for use in the visual domain.
Within the two visual scenarios, we compare three
state-of-the-art language models and a knowledge
base: (1) A window-based model, which counts
co-occurrence frequencies within a fixed window;
(2) R-LDA (S´eaghdha, 2010), an extension of LDA
that enables generation of joint probabilities; (3)
TypeDM (Baroni and Lenci, 2010), a strong Distri-
butional Memory model; (4) ConceptNet (Speer and
Havasi, 2013), an automatically generated semantic
graph containing concepts with their relations.
We test the language models in two ways: (1) We
directly compare the statistics of the linguistic mod-
els with statistics extracted from the visual domain.
</bodyText>
<page confidence="0.971634">
769
</page>
<note confidence="0.7330335">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 769–779,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.998741875">
(2) We compare the linguistic models inside the two
computer vision applications, leading to a direct es-
timation of their usefulness.
To summarize, our main research questions are:
(1) Is the knowledge from language compatible with
the knowledge from vision? (2) Can the knowl-
edge extracted from language help in computer vi-
sion scenarios?
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999983767123288">
Using high level knowledge to aid image under-
standing has become a recent interest in the com-
puter vision community. Objects, actions and scenes
are detected and localized in images using low-
level features. This detection and localization pro-
cess is guided by reasoning and knowledge. Such
knowledge is employed to disambiguate locations
between objects in (Gupta and Davis, 2008). From
the defined relationships between nouns (e.g., above,
below, brighter, smaller), the system constrains
which region in an image corresponds to which ob-
ject/noun. Similarly, (Srikanth et al., 2005) ex-
ploit ontologies extracted from WordNet to asso-
ciate words and images and image regions. (Yu
et al., 2011) employ relations between scenes and
objects introducing an active model to recognize
scenes through objects. The reasoning knowledge
limits the detector to search for an object within a
particular region rather than on the whole image.
Language models have also been employed to
generate descriptive sentences for images. (Ushiku
et al., 2012) introduce an online learning method for
multi-keyphrase estimation to generate a sentence
using a grammar model to describe an image. Simi-
larly, from objects and scenes detected in an image,
(Yang et al., 2011) estimated a sentence structure to
generate a sentence description composed of a noun,
verb, scene and preposition.
The studies most similar to ours are (Teo et al.,
2012) and (Lampert et al., 2009). In (Teo et al.,
2012), the Gigaword corpus is used to extract rela-
tionships between tools and actions (e.g., knife - cut,
cup - drink) by counting their co-occurences. These
relationships are used to constrain and select the
most plausible actions within a predefined set of ac-
tions in cooking videos. Instead of using this knowl-
edge as a guidance during recognition, we compare
different language models and build a general frame-
work that is able to detect unseen actions through
their components (verb - object - scene), hence our
method does not limit the number of actions in im-
ages. (Lampert et al., 2009) use attributes of nouns
(e.g., an animal: white, eat fish, water, etc.). They
can detect animals without having seen training ex-
amples by manually defining the attributes of the tar-
get animal. In this work, rather than relying on man-
ual definitions, our aim is to find the best language
models built automatically from available corpora to
extract relations from natural language.
Currently, human action recognition is popular
and mostly studied in video using the Bag-of-Visual-
Words method (Delaitre et al., 2010; Everts et al.,
2013; Kuehne et al., 2012; Reddy and Shah, 2012;
Wang et al., 2013). In this method one extracts small
local visual patches of, say, 24 by 24 pixels by 10
frames at every 12th pixel at every 5th frame. For
each patch local gradients or local movement (opti-
cal flow) histograms are calculated. Then these local
visual features are mapped to abstract, predefined
“visual words”, previously obtained using k-means
clustering on a set of random features. While results
are good, there are two main drawbacks with this
approach. First of all, human actions are semantic
and more naturally recognized through their compo-
nents (human, objects, scene) rather than through a
bag of local gradient/motion patterns. Hence we use
a component-based method for human action recog-
nition. Second, the number of possible human ac-
tions is huge (the number of objects times the num-
ber of verbs). Obtaining annotated visual examples
for each action is therefore prohibitively expensive.
So we learn from language models how components
combine into human actions.
</bodyText>
<sectionHeader confidence="0.958835" genericHeader="method">
3 Two Visual Recognition Scenarios
</sectionHeader>
<bodyText confidence="0.9999445">
We now describe the two computer vision scenarios:
human action recognition and objects in context.
</bodyText>
<subsectionHeader confidence="0.999068">
3.1 Human Action Recognition
</subsectionHeader>
<bodyText confidence="0.9987736">
We want to identify a human action, defined as a
&lt;subject, verb, object&gt; triple. We do this by recog-
nizing the human, the object, and the scene and then
determine the most likely verb based on these com-
ponents. Scenes are only used here as features for
</bodyText>
<page confidence="0.990938">
770
</page>
<bodyText confidence="0.9998439">
predicting/disambiguating the human action and the
final task is to define the human action triple. As in
most work in human action recognition, we simplify
the problem by considering only images in which
human actions occur. This means that a human is
always present, leaving the problem of predicting
the verb given the object and the scene. While this
may seem like a strong assumption, the possibility
of having no action in the image at all is largely un-
explored in computer vision due to its difficulty.
</bodyText>
<figureCaption confidence="0.99034625">
Figure 1: Human action suggestion: based on the objects
and scenes recognized in an image, the system suggests
the most plausible actions. The action models provide the
relationships between objects - scenes - verbs
</figureCaption>
<subsectionHeader confidence="0.723135">
3.1.1 Human action recognition framework
</subsectionHeader>
<bodyText confidence="0.9998968">
Our general action recognition framework is pre-
sented in Figure 1. Given an image, an object recog-
nizer will predict the probability of each object (e.g,
bike, horse) presented in that image. Furthermore,
a scene recognizer will provide the probabilities of
each scene (e.g., countryside, suburb, forest) given
the image. The action model is composed of the
conditional probabilities that relate verbs, objects
and scenes, which have been learned from training
images or language corpora. Given the object and
scene probabilities recognized in the image, the ac-
tion model will guide the action prediction process
and finally, the system will suggest the most proper
actions (e.g., ride horse, drive car). We will now de-
scribe each component in detail:
</bodyText>
<listItem confidence="0.938828">
• Object and scene recognizers: To train the ob-
ject recognizer, we use a set of images where ob-
jects have been annotated with bounding boxes (to
</listItem>
<bodyText confidence="0.925075928571429">
specify objects’ locations). We follow the state-
of-the-art method of (Uijlings et al., 2013). The
method is based on multiple hierarchical segmen-
tations to sample a limited set of high quality ob-
ject locations in terms of bounding boxes. A Bag-
of-Visual-Words method (Uijlings et al., 2010) is
applied to these boxes to localize and recognize
objects. For the scene recognizer, we trained the
same Bag-of-Visual-Words method on complete
images on a dataset annotated with 15 scenes. In
both cases we use Support Vector Machines to
learn the object/scene models. We use Platt’s sig-
moid function to obtain the final conditional prob-
abilities P(oj|I) and P(sk|I).
</bodyText>
<listItem confidence="0.8391826">
• Action models: The model captures the relation-
ship between Object - Scene, Verb - Scene and
Verb - Object: the probability of an object given a
scene P(oj|sk), a verb given an object P(vi|oj),
and a verb given a scene P (vi|sk). In one exper-
iment we learn the probabilities from the train-
ing images, where each image has been anno-
tated with an object, a verb (of an action) and a
scene. All three probabilities are computed using
frequency counts in the training set, for example:
</listItem>
<equation confidence="0.895852">
P(oj|sk) = #images having oj, sk (1)
</equation>
<bodyText confidence="0.9194055">
#images having sk
We aim to replace this learning from annotated
training images, which are expensive to obtain,
with learning from language corpora. The de-
tails of how to extract the probability distributions
from language models are explained in section 4.
</bodyText>
<subsectionHeader confidence="0.59648">
3.1.2 Component integration
</subsectionHeader>
<bodyText confidence="0.9990947">
To combine these components in the framework,
we use an energy-based model (Lecun et al., 2006)
visualized in Figure 2, which includes the image I
(an observed variable) and object O, scene S, and
verb V . This energy-based formulation allows us to
set different weights for energies which come from
disparate sources (i.e. language and vision) using
Gibbs measure.
Now given an image I, we can compute the score
function S(aij; I) of an action aij as:
</bodyText>
<page confidence="0.505697">
1
</page>
<figure confidence="0.9575488">
S(aij; I) = S(vi, oj; I) = Z exp _ E
FEF
Input
images
Language
torpors
Training
Images
Object
Recognizer
Training
Images
Objaasoan. v.ro-eran.
car
bus
horse
���
ction mde!s
varb-Objsn
countryside
suburb
forest
���
Scene
Recognizer
Training
Images
Suggested actions
nde horse
dd. car
dr. bus
���
�
ij
EF
</figure>
<page confidence="0.74603">
771
</page>
<figureCaption confidence="0.998467">
Figure 2: An energy-based model for action recognition
</figureCaption>
<bodyText confidence="0.999962666666667">
where we define each energy function EijF to give
lower energies to correct answers and higher ener-
gies to incorrect ones, with S is the set of all scenes:
</bodyText>
<equation confidence="0.992092545454546">
EijF1(O, I) = −wF1 log P(oj|I) (2)
�
Eij
F2(S, I) = −wF2 log
SES
(3)
EijF3(V, O) = −wF3 log P(vi|oj) (4)
� P(vi|S) × P(S|I)
Eij (5)
F4(V, S) = −wF4 log
SES
</equation>
<bodyText confidence="0.999853142857143">
Let Pi be the position of the correct action in the
ranked list of predicted actions for a certain image
Ii. The ranked list is sorted in the order of the score
S. We evaluate human action recognition in terms of
this position average over all images, which we call
Average Ranking (AR). Therefore we use Average
Ranking as our loss-function:
</bodyText>
<equation confidence="0.981013">
N
1
L(wF) = ARN = N
i=0
</equation>
<bodyText confidence="0.9571305">
Training the energy model involves finding the fac-
tors wF that minimizes the loss:
</bodyText>
<equation confidence="0.996764">
wF = argmin L(wF) (7)
wF
</equation>
<bodyText confidence="0.999987333333333">
As we have only four parameters to learn in our
energy model, we do this by performing an ex-
haustive search and cross validation. We require
</bodyText>
<equation confidence="0.922951333333333">
wF ∈ {0.0, 0.1, 0.2, ..., 0.9} and set the constraint
E
FEF wF = 1. We note that the factor graph for-
</equation>
<bodyText confidence="0.999984">
mulation of our framework would allow us to use
more advanced learning algorithms. We plan to look
into this once the model becomes more complex by
adding, for example, information about the position
of the objects and the human.
</bodyText>
<subsectionHeader confidence="0.733406">
3.1.3 Dataset
</subsectionHeader>
<bodyText confidence="0.99999275">
Recently, researchers have released many im-
age action datasets such as the 7 everyday ac-
tions (Delaitre et al., 2010), the Stanford 40 action
dataset (Yao et al., 2011), the PASCAL action clas-
sification competition (Everingham et al., 2012), and
the 89 action dataset (Le et al., 2013). The 89 action
dataset was originally created for the recognition of
20 objects. Afterwards also actions were annotated.
Therefore, the actions occurring with these objects
are mostly unbiased, unlike in other action datasets.
Hence we choose to use the 89 action dataset.
In the 89 action dataset, every image has been an-
notated with human actions, where each action is
composed of a verb and an object. We addition-
ally annotated every image with one of the 15 scenes
from the 15 scene dataset (Lazebnik et al., 2006).
</bodyText>
<subsectionHeader confidence="0.99705">
3.2 Objects in Context
</subsectionHeader>
<bodyText confidence="0.999872608695652">
Our other computer vision scenario is about objects
in context. Context is useful in visual recognition for
two reasons: Firstly, context can significantly reduce
the number of possible object categories simplifying
the problem. Secondly, when the object appearance
is inconclusive for its identity, context can be used
for disambiguation. For example, a grey rectangle
on a desk may be recognized as a pen, while a grey
rectangle on a table may be recognized as a knife.
As the recognition systems are not always reliable,
the use of context can greatly improve results.
For this scenario we choose a theoretical setting
in which we want to predict the identity of one ob-
ject given that the identities of all other objects in
the image are known. We believe that our main con-
clusions on the linguistic models will transfer to a
practical computer vision application where visual
recognition systems predict the object identities.
Formally, we can describe this scenario as fol-
lows: Given an image I with N objects O =
{o1, o2, · · · , oN}, we want to predict the identity of
object oi given all other objects O \ oi. In this paper
we use a Naive Bayes assumption, leading to:
</bodyText>
<equation confidence="0.9995845">
P (O \ oi|oi) × P (oi)
P (oi|O \ oi) = P (O \ oi)
≈ P(oi) × rI P(oj|oi). (8)
ojEd\oi
P(oj|S) × P(S|I)
Pi. (6)
</equation>
<page confidence="0.975861">
772
</page>
<bodyText confidence="0.997315666666667">
In this scenario, we need conditional relations
P(oj|oi) and priors. We obtain these from language
data or from images directly.
</bodyText>
<subsectionHeader confidence="0.459906">
3.2.1 Dataset
</subsectionHeader>
<bodyText confidence="0.99996915">
For the objects in context scenario, we use the
SUN object dataset (Xiao et al., 2010), which con-
tains more than 16 thousand images, more than
79,000 objects whose locations are annotated us-
ing polygons. The dataset has been annotated by
various people who could choose their own object
categories, leading to duplicate categories such as
“building” and “buildings”, “person” and “person
walking”. Furthermore, for some images large parts
are not annotated leading to an incomplete context.
We therefore cleaned the object categories (mapping
from around 7,500 objects to over 700 unique object
categories) and considered only images whose con-
tent was sufficiently annotated.
In our experiments, we used the predefined train-
ing and testing parts of the SUN dataset and obtained
around 4,500 images for learning the object relations
and 10,600 images for testing the object prediction.
We obtain conditional probabilities P(oj|oi) from
frequency counts.
</bodyText>
<sectionHeader confidence="0.996041" genericHeader="method">
4 Language models &amp; distribution
extraction
</sectionHeader>
<bodyText confidence="0.998033166666667">
To extract probability distributions from texts, we
use ConceptNet, the Window2 and 20 model,
TypeDM and R-LDA. We will now describe
how we estimate the four conditional probabilities
P(V |O), P(V |S), P(O|S), P(O|O) needed in the
two visual scenarios for each language model.
</bodyText>
<subsectionHeader confidence="0.958331">
4.1 ConceptNet
</subsectionHeader>
<bodyText confidence="0.996957833333334">
ConceptNet (Speer and Havasi, 2013) is a large se-
mantic graph containing concepts and relations be-
tween them. It includes everyday basic, cultural
and scientific knowledge, which have been automat-
ically extracted from Internet using predefined rules.
In this work we use the most current version, Con-
ceptNet 5. As it was mined from free text using
rules, the database has uncontrolled vocabulary and
contains many false/nonesense statements.
To extract relations from ConceptNet5, we first
examine all relations in the database and define those
that are relevant to our scenarios (Figure 3). For
</bodyText>
<figureCaption confidence="0.998787">
Figure 3: List of relations in ConceptNet
</figureCaption>
<bodyText confidence="0.999821375">
example, for the conditional probability of objects
given scenes, relations such as “At Location”, “Lo-
cated Near” are extracted. For the human action
recognition scenario, we used a list of 19 objects,
15 scenes and around 5 thousand verbs for comput-
ing P(V |O),P(O|S),P(V |S). For the objects in
context scenario, we used 700 objects for comput-
ing P(O|O). Examples of relations extracted from
ConceptNet are illustrated in Table 1, such as: Oil -
Located near - Car, Horse - Related to - Zebra. From
these relations, we define the four conditional prob-
abilities using their frequency counts. For example,
to compute the conditional probability of an object
given a scene P (oi|sj), we extract all triples having
the form &lt;object, rel, scene&gt;, where “rel” can be
“AtLocation”, “LocatedNear”, etc.
</bodyText>
<equation confidence="0.892197666666667">
f req(&lt; oi, rel, sj &gt;)
P(oi  |sj) = Eo EO f req&lt; om, rel, s� &gt; (9)
,
</equation>
<subsectionHeader confidence="0.836596">
4.2 Window model
</subsectionHeader>
<bodyText confidence="0.999199466666667">
One of the most famous and basic statistical model is
based on counting co-occurrences within a window
of fixed width, which follows the tradition of hy-
perspace analogue to language (Lund and Burgess,
1996). We took the Window2, 20 models which
have been built in (Bruni et al., 2012) using the
ukWaC (1.9B tokens) and Wackypedia (820M to-
kens). As the Window2 model only looks at 2 words
on the left and right of the current one, it reflects
the relationships between words occurring near each
other, while the Window20 searches for a broader
view of how words are related to each other. The
weights of each pairs of words are calculated using
the Local Mutual Information (LMI). To compute
the conditional probabilities, we use the LMI scor-
</bodyText>
<page confidence="0.988849">
773
</page>
<table confidence="0.99900075">
LocatedNear RelatedTo UsedFor AtLocation
oil car seatbelt car horse zebra plant garden bottle store liquid horse race bus city car city
chair your bottom chair school horse pony sheep baa boat fish table eat off of bike street dog city
plant everywhere muzzle dog plant green sheep cloud dog companionship chair rest bird countryside dog street
trailer car dog bark bone boat ship cow bull horse riding bus travel car street chair city
salt table horse cowboy chair table horse riding chair sitting table eat meal cat store bus city
stool table carriage horse dog wolf sheep farm chair sit on boat travel car street chair store
pasture cow horse fence dog cat cow milk car transportation bottle hold liquid car street bicycle store
cat dog whisker cat sheep lamb table desk sheep wool boat float on water bird forest chair store
horse zebra desk chair sheep wool cat feline table put thing on table eat at car city bottle store
cat household train railroad dog a wolf dog canine boat travel on water cat catch mouse table kitchen chair office
horsehair horse sheep wool cat dog plant flower chair sit cow milk chair office chair city
</table>
<tableCaption confidence="0.999736">
Table 1: Examples of relations extracted from ConceptNet 5
</tableCaption>
<bodyText confidence="0.489678">
ing function provided by the models, for example:
</bodyText>
<equation confidence="0.484563666666667">
P(viloj) = LMIvi,oj
(10)
Ev„ EV LMIv.,oj
</equation>
<subsectionHeader confidence="0.955494">
4.3 Distributional Memory
</subsectionHeader>
<bodyText confidence="0.999933857142857">
Distributional Memory (Baroni and Lenci, 2010)
(DM) is a multi-purpose framework for semantic
modeling. This model is more complex than the
Window models because it exploits different de-
grees of lexicalization for each relation. Distribu-
tional information is extracted as a set of weighted
&lt;word-link-word&gt; tuples obtained from a depen-
dency parse of corpora. In the Window model the
relation between each word pair is decided by their
co-occurences within a sliding window, while in DM
this relation is defined by distributional properties of
the two words. These distributional properties are
based on a syntactic relation or lexico-syntactic pat-
tern that links the two words. For example, the tu-
ple &lt;marine, use, bomb&gt; encodes that marine co-
occurs with bomb in the corpus, and the word “use”
specifies the type of the syntagmatic link.
Distributional Memory contains three different
models, corresponding to different ways to con-
struct the weighted structure through the “link”. The
first model, LexDM is the most heavily lexicalized
model with the most variety of links, whereas the
DepDM has the minimum degree of lexicalization,
thus having the smallest number of links. TypeDM,
which was reported to achieve the best performance
in different tasks including selectional preferences,
is laying somewhere in the middle of the other two
models. It shares the same lexical information as in
LexDM but use a different scoring function, which
focuses on the variety of surface forms, rather than
the frequency of a link. Hence we choose the best
model, TypeDM, to learn the relationships between
verbs, objects and scenes. As in the window model,
we compute conditional probabilities using the LMI
scores provided by the model (Equation 10).
</bodyText>
<subsectionHeader confidence="0.979573">
4.4 R-LDA
</subsectionHeader>
<bodyText confidence="0.999812428571429">
To model the relationships between verbs, objects
and scenes, we adapt the R-LDA model (S´eaghdha,
2010) (ROOTH-LDA), which has been used for the
selectional preference task in order to obtain con-
ditional probabilities of two words. Each relation m
of &lt; w1, w2 &gt; is generated by picking up a distribu-
tion over topics, then both elements of the relation m
share the same topic assignment zm, which keep two
different w1-topic and w2-topic distributions sharing
the same topic (Figure 4). The models are estimated
by Gibbs sampling following (Heinrich, 2004). It is
also noted that these models are generative, hence
they also predict the probabilities of tuples that do
not occur in the corpus.
</bodyText>
<figureCaption confidence="0.98883">
Figure 4: Generative graphical model of R-LDA: model-
ing the relations between two words
</figureCaption>
<bodyText confidence="0.999845375">
To model the relations between objects and verbs,
we follow the data preparation in (Le et al., 2013),
using the British National Corpus (BNC) which has
been preprocessed and parsed using TreeTagger and
Maltparser. Verbs are heads of sentences while ob-
jects are either direct or indirect objects related to
those verbs by the parser. For the relations between
verbs and scenes, we consider also verbs as heads
</bodyText>
<figure confidence="0.956183111111111">
SZ
. .
a
w2
w
tpk
(Pk
Y
R
</figure>
<page confidence="0.991694">
774
</page>
<table confidence="0.948613214285714">
Topic 8: Topic 14: Topic 0: Topic 54:
Noun Verb Noun Verb Noun Noun Noun Noun
people 0.0208 have 0.157 year 0.0154 win 0.109 attention 0.0172 study 0.01 decision 0.02 case .0176
job 0.0167 work 0.108 Cup 0.0093 have 0.099 model 0.0147 research 0.0123 view 0.0244 fact .0096
work 0.0156 make 0.0262 team 0.0086 beat 0 study 0.014 work 0.0111 question 0.018 question .0096
class 0.014 take 0.0244 race 0.00772 take 0.025 role 0.0139 chapter 0.0085 issue 0.0124 law .0096
worker 0.0123 find 0.0194 season 0.0062 lose 0.0211 account 0.013 problem 0.0075 evidence 0.0104 decision .0092
staff 0.0111 pay 0.0156 time 0.006 run 0.0152 analysis 0.0123 issue 0.006 point 0.0099 time .0067
group 0.0089 say 0.0146 world 0.0058 finish 0.0135 aspect 0.012 system 0.0065 reason 0.0096 issue .0062
way 0.0086 get 0.0136 game 0.0055 make 0.0127 problem 0.0106 area 0.006 statement 0.0086 evidence .00617
service 0.008 leave 0.0114 champion 0.0053 lead 0.0122 effect 0.0105 process 0.006 doubt 0.008 interest .0059
company 0.0076 run 0.0102 seat 0.0049 follow 0.0098 pattern 0.0103 policy 0.0055 attention 0.0076 point .0058
day 0.0069 come 0.0101 match 0.0049 qualify 0.008 issue 0.0102 theory 0.00551 matter 0.00738 judge .0055
number 0.00615 help 0.0088 place 0.0047 compete 0.0074 range 0.0095 way 0.0053 policy 0.006 statement .005
</table>
<tableCaption confidence="0.958218">
Table 2: Random R-LDA topics with the relations between Noun-Verb (first 2 columns) and between Noun-Noun (last
2 columns)
</tableCaption>
<bodyText confidence="0.995220333333333">
of sentences while scenes are all nouns occurring in
the same sentence. For the relations between objects
and scenes as well as objects and objects, we use all
nouns to capture a general model1. The statistics of
the BNC corpus with their corresponding relations
are reported in Table 3.
</bodyText>
<table confidence="0.996138">
#Relations #Tokens
Verb - Object 3.3M 6.7M
Noun - Noun 19.8M 39.7M
Verb - Noun 83.4M 166.8M
</table>
<tableCaption confidence="0.997661">
Table 3: The statistics of the dataset used for estimating
R-LDA models for each relation type
</tableCaption>
<bodyText confidence="0.9994338">
Samples of topics extracted through R-LDA are
illustrated in Table 3. It shows that Noun and Object
share many similar terms in the same topic while
Noun and Verb sharing the same topics tend to go
often together (e.g., win, cup, beat, race).
</bodyText>
<sectionHeader confidence="0.999365" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.856756555555556">
In this section we want to answer our two main re-
search questions: (1) Is knowledge from language
compatible with knowledge from vision? (2) Can
we use knowledge extracted from language in com-
puter vision scenarios?
5.1 Is knowledge from language and vision
compatible?
In this section we compare statistics mined from
texts with those mined from visual sources. Ideally,
</bodyText>
<footnote confidence="0.927852">
1Different from objects and verbs, which can be defined ex-
plicitly from the parsed corpora, scenes can only be defined
from more restrained rules (e.g., followed by some preposi-
tions), so here we take all nouns to have the most general model.
</footnote>
<table confidence="0.999327833333333">
Chi statistics P(V|O) P(V|S) P(O|S)
R-LDA 17.8 11.6 11.9
Window2 11.6 11.4 32.6
Window20 11.7 11.4 23.7
TypeDM 11.5 13.3 23.2
ConceptNet 17.5 11.5 34.4
</table>
<tableCaption confidence="0.9874425">
Table 4: X2 distance for relations between verbs, objects,
scenes from different language models to image data
</tableCaption>
<bodyText confidence="0.999455833333333">
we want statistics from the language models to fol-
low those of the image model, even though not all
statistics from images can be reliably measured due
to insufficient data. Therefore, we measure how well
the estimated language models fit the estimated vi-
sual distributions using the the X2-distance:
</bodyText>
<equation confidence="0.990444333333333">
(PIi − PLi)2
PIi
(11)
</equation>
<bodyText confidence="0.999507214285714">
where PI and PL are the probability distribution ob-
tained from the image data and language models re-
spectively.
For the conditional probabilities P(V |O),
P(V |S), and P(O|S) we compare language models
with image statistics extracted from the 89 human
action dataset. Table 4 shows the results. For the re-
lations between verb and scene P(V |S), there is not
much fluctuation among different language models.
For objects and scenes P(O|S), R-LDA is closest
to the image model. This is because R-LDA is good
at measuring contextual and indirect relations by
design, which is the case for object-scene relations.
This also explains why TypeDM and Window20 are
</bodyText>
<equation confidence="0.987233666666667">
N
X2 =
i=1
</equation>
<page confidence="0.992579">
775
</page>
<bodyText confidence="0.963198892857143">
further away from the image model, followed by
the Window2 model. Instead, human actions are
found in language as the relation between verbs and
their direct linguistic objects. Indeed, TypeDM is
closest to the image model for P(V |O) as it makes
explicit use of this linguistic link. The Window2
and 20 models are almost as close to the image
model for P(VIO), while R-LDA is considerably
further away due to its contextual nature. Finally,
ConceptNet is the furthest away from the image
model. To conclude, TypeDM is best for modelling
direct verb-object relations, while R-LDA is better
at capturing the more contextual object-scene
relations.
To look closer at the difference between the statis-
tics obtained from the image and language data,
we give an example of the conditional probabil-
ities of an object given a scene P(OIS) in Fig-
ure 5. We see that the distribution extracted from
language (TypeDM) is much smoother and contains
more relations than the image model since it has
been trained on general and large text corpora. The
��
distribution from image data on the other hand is
��
more sparse and tailored to this specific dataset. For
��
example, given a “store”, the probability that there
</bodyText>
<equation confidence="0.609365">
�
is a “table” is 1, given “highway”, the probability of
��
</equation>
<bodyText confidence="0.993617333333333">
a “car” is also 1 in the image dataset, while the high-
est conditional probability of the language model is
only less than 60%.
</bodyText>
<figure confidence="0.544961">
R-LDA TypeDM Window20 Window2 ConceptNet
</figure>
<figureCaption confidence="0.963145333333333">
Figure 6: χ2-distances between the tested language mod-
els and the image model for conditional probabilities of
objects P(O|O).
</figureCaption>
<bodyText confidence="0.9999436875">
For the relations between objects and objects, we
use the SUN dataset, which is much bigger and more
general than the action dataset. As shown in Fig-
ure 6, R-LDA is most similar to the image model,
closely followed by TypeDM and the Window20
model. All these three models are good at captur-
ing broad contextual relations. The Window2 model
has a significantly larger distance to the image model
as it captures a narrow context of 2 words, which is
apparently not enough to find co-occurences of ob-
jects. ConceptNet is the most inconsistent with this
image data since not enough objects and their re-
lations are extracted from it. To sum up, R-LDA
achieves the best performance in modeling the re-
lations between objects and objects among all lan-
guage models.
</bodyText>
<subsectionHeader confidence="0.999052">
5.2 Language Models for Visual Recognition
</subsectionHeader>
<bodyText confidence="0.9995056">
To measure the performance of the two visual recog-
nition scenarios, we use the position pi of the correct
action found in the ranked list for each image i.
We report the average ranking over all images
(ARI) and over all objects or actions (ARO, ARA):
</bodyText>
<equation confidence="0.985045">
N i Na
ARI = F N p ;ARO = j=0
(12)
No
</equation>
<bodyText confidence="0.999921666666667">
where N is the number of images, No is the number
of objects and pjo is the average rank of all images
having object j. The average rank over all actions
</bodyText>
<equation confidence="0.8790635">
�!#$
ARA is defined similarly to ARO. The average rank
�!&amp;quot;#�%
�!$#�%
</equation>
<bodyText confidence="0.9416785">
over all image measures the performance over the
image dataset, but infrequent objects/events have lit-
tle impact on this performance. The average rank
over objects or actions gives more weight to rare ex-
�� ����
amples.
</bodyText>
<subsectionHeader confidence="0.660993">
5.2.1 Human Action Recognition
</subsectionHeader>
<bodyText confidence="0.998335125">
We evaluate the performance of human action
recognition in images based on objects and scenes
individually, and then study the integration of them.
The training set contains 1,104 images (for training
the image relations) and the test set has 710 im-
ages. First, we test how the model predicts an ac-
tion knowing the actual object and/or scene appear-
ing in an image (given object/scene gold standard),
i.e., Ogs, Sgs and OgsSgs in the settings. After that,
we test a complete model which is based on the out-
put of our object recognizer and our scene recog-
nizer (Orec, Srec, OrecSrec).
For each setting, we try different action models,
either learnt from the training images (Image), or
from each of the language models (TypeDM, R-
LDA, Window2, Window20, ConceptNet).
</bodyText>
<figure confidence="0.9889203">
Chi square distance
(000
&apos;OW
&amp;000
5000
3000
2000
,000
OW
0
</figure>
<page confidence="0.884163">
776
</page>
<figureCaption confidence="0.980158333333333">
Figure 5: Probability distributions of scene over object extracted from: (left) image dataset; (right) TypeDM model
(as there are many &lt;object - scene&gt; relations, only a few are shown on the Y-axises). The number of relations in the
TypeDM is much bigger than in the image model, which shows a more general model than the image one.
</figureCaption>
<bodyText confidence="0.999719814814815">
Table 5 presents the average ranking over all im-
ages. Results show that the action model learnt di-
rectly from the training images achieves the best per-
formance in all settings, even if we give more weight
to infrequent actions by taking the average ranking
over all actions, as presented in Table 6. One ex-
planation may be that the action dataset has a lim-
ited domain of only 19 objects, while the language
models were learnt from broad knowledge (See Fig-
ure 5). Another possibility is that verbs used for
describing actions in images are more specific than
verbs used in language. For example, in language
one would “use the car”, while in images such ac-
tion would be labelled “drive a car”.
If we look at the performance of the language
models, TypeDM performs best by a significant mar-
gin. This makes sense, as the most powerful term
for predicting an action is obviously P(V IO), and
we saw earlier that TypeDM produces probabili-
ties P(V IO) which are closest to the image model.
For the same reason, the second and third best
language model are the Window2 and Window20
models, although their performance is significantly
lower when using the predictions for objects and/or
scenes. This is somewhat surprising considering
that TypeDM, Window2 and Window20 are all very
close in distance to the image model. Of course,
</bodyText>
<table confidence="0.999702571428571">
Image TypeDM R-LDA Window2 Window20 C.Net
Ogs 0.3 16.1 63.4 16.4 18.3 86.1
Orec 14.9 26.9 66.7 44.7 54.9 115.6
Sgs 35.7 181.7 174.9 168.5 174.8 252.5
Srec 46.8 250.5 348 190.2 189.8 241.2
OgsSgs 0.28 10.2 15.2 13.8 13.6 81.9
OrecSrec 13.6 26.9 66.7 44.7 54.9 115.6
</table>
<tableCaption confidence="0.8022304">
Table 5: Average rank over all images ARI of the human
action recognition using different settings: Ogs, Orec
use only objects (gold standard and object recognizer);
Sgs, Srec use only scenes, OgsSgs and OrecSrec integrate
both objects and scenes together
</tableCaption>
<bodyText confidence="0.999919714285714">
the distance is just an indication. R-LDA performs
poorly because it is much more contextual. Finally,
ConceptNet performs the worst.
Another observation is that using the scene iden-
tity should theoretically help in human action recog-
nition: Using TypeDM, the use of the gold standard
object identity yields an average ranking over all im-
ages of 16.1, while using both the scene and object
identity yields an average ranking of 10.2, which
is significantly better. It means that the use of the
scene can disambiguate some actions (e.g. “ride a
horse” vs. “feed a horse”). However, when using
the recognition system, using the scene does not in-
crease the overall performance. This shows that the
</bodyText>
<page confidence="0.988916">
777
</page>
<bodyText confidence="0.999917222222222">
visual recognition system may not be strong enough
for recognizing these 15 scenes. Another problem
may be the limitation of 15 scenes only: while an-
notating we frequently found that it was hard for nu-
merous images to put them into one of the 15 scenes.
So a bigger scene database may help.
The main problem with most available annotated
human action datasets is that they are very restricted
and domain-specific. For example, in this dataset
with 19 objects and 15 scenes, there are many photos
of a person riding a motorbike on rocky mountains
as a kind of sport. Consequently, the probability
of “riding” given “mountain” learnt from the image
dataset is high according to the image data (78%)
but is uncommon in general. So the image dataset
might be too restricted or biased for general knowl-
edge to work well. In the next section we therefore
use a more general dataset.
</bodyText>
<table confidence="0.999231666666667">
Image TypeDM R-LDA Window2 Window20 C.Net
ARI 13.6 26.9 66.7 44.7 54.9 115.6
ARA 16.4 30.8 64.7 45.3 51.9 131.7
</table>
<tableCaption confidence="0.994948">
Table 6: Average rank over all images vs. actions of the
human action recognition using the O,,,,S,,, setting
</tableCaption>
<subsubsectionHeader confidence="0.624692">
5.2.2 Objects in Context
</subsubsectionHeader>
<bodyText confidence="0.999925714285714">
For every object in every image in the test set of
the SUN database, we guess the identity of an object
given the identity of all other objects in the image.
In total, there are 78,306 object predictions within
10,652 images.
As shown in Figure 7, the R-LDA model outper-
forms all other models for both average rank over
images and over objects. Interestingly, both R-LDA
and TypeDM are better at predicting the correct ob-
jects in images than the model learnt from the image
training set itself. It shows that for many cases, the
relation statistics learnt from language data can help
in visual recognition. These language models are
even better than the information extracted from gen-
eral, relatively unbiased image datasets, where an-
notation is limited. For the limited annotation, this
hypothesis is further supported by looking at the av-
erage rank over objects, which gives more weight
to rarely occurring objects. As seen in Figure 7, all
language models except ConceptNet outperform the
image model. We conclude that language models
</bodyText>
<figureCaption confidence="0.998042">
Figure 7: Average rank over all images and objects using
different language models and ID (image data)
</figureCaption>
<bodyText confidence="0.9998665">
can aid visual models in large-scale visual recogni-
tion problems which use co-occurrence of objects as
their context, especially when the annotation is lim-
ited, as is often the case.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998581034482759">
In this paper, we investigated the problem of ap-
plying knowledge learnt from language corpora to
visual recognition. We compared statistics of var-
ious language models mined on general corpora
with statistics observed in image datasets. It shows
that the generative R-LDA model is good at relat-
ing contextual relations (e.g., object - object, ob-
ject - scene), while the syntactic based distributional
model TypeDM is good at representing direct rela-
tions such as verb - object in images.
�age 1
We have evaluated the performance of the lan-
guage models in two visual scenarios: human action
recognition and object prediction. It suggests that
the language models need some tailoring when ap-
plied to restricted datasets, but for a bigger and more
general dataset, the language models even outper-
form the model learnt from annotated images itself.
This shows that language models built from avail-
able text corpora can be used for visual recognition
instead of expensive annotated image data.
In the future, we want to further investigate the
problem of domain adaptation when applying gen-
eral language models to a new image dataset. This
problem can be integrated into the energy-based
model during the training phase. We plan to extend
work on human action recognition by including the
relative position between the human and object in
the images.
</bodyText>
<figure confidence="0.996599636363636">
Average Rank over images Average Rank over objects
00
350
300
250
200
150
100
50
0
RLDA TypeDM ID Window 20 Window 2 ConceptNet
</figure>
<page confidence="0.983782">
778
</page>
<sectionHeader confidence="0.992098" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999882384615384">
Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional memory: A general framework for corpus-based
semantics. Computational Linguistics.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics, ACL.
ACL.
Vincent Delaitre, Ivan Laptev, and Josef Sivic. 2010.
Recognizing human actions in still images: a study
of bag-of-features and part-based representations. In
Proceedings of the British Machine Vision Conference.
BMVA Press.
M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
and A. Zisserman. 2012. The PASCAL Visual Object
Classes Challenge 2012 (VOC2012) Results.
I. Everts, J. van Gemert, and T. Gevers. 2013. Evaluation
of color stips for human action recognition. In CVPR.
Abhinav Gupta and Larry S. Davis. 2008. Beyond
nouns: Exploiting prepositions and comparative adjec-
tives for learning visual classifiers. In Proceedings of
the 10th European Conference on Computer Vision.
Gregor Heinrich. 2004. Parameter estimation for text
analysis. Technical report.
H. Kuehne, D. Gehrig, T. Schultz, and R. Stiefelhagen.
2012. On-line action recognition from sparse feature
flow. In VISAPP.
C.H. Lampert, H. Nickisch, and S. Harmeling. 2009.
Learning to detect unseen object classes by between-
class attribute transfer. In Conference on Computer
Vision and Pattern Recognition CVPR.
S. Lazebnik, C. Schmid, and J. Ponce. 2006. Beyond
bags of features: Spatial pyramid matching for rec-
ognizing natural scene categories. In Conference on
Computer Vision and Pattern Recognition CVPR, vol-
ume 2, pages 2169–2178.
Dieu Thu Le, Raffaella Bernardi, and Jasper Uijlings.
2013. Exploiting language models to recognize un-
seen actions. In Proceedings of the 3rd ACM con-
ference on International conference on multimedia re-
trieval ICMR. ACM.
Yann Lecun, Sumit Chopra, Raia Hadsell, Fu J. Huang,
G. Bakir, T. Hofman, B. Sch¨olkopf, A. Smola, and
B. Taskar Eds. 2006. A tutorial on energy-based
learning. In Predicting Structured Data.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments,
and Computers.
K. Reddy and M. Shah. 2012. Recognizing 50 human
action categories of web videos. In Machine Vision
and Applications.
Diarmuid ´O. S´eaghdha. 2010. Latent variable mod-
els of selectional preference. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, ACL, pages 435–444. Association
for Computational Linguistics.
Robert Speer and Catherine Havasi. 2013. Conceptnet 5:
A large semantic network for relational knowledge. In
The People’s Web Meets NLP. Springer Berlin Heidel-
berg.
Munirathnam Srikanth, Joshua Varner, Mitchell Bowden,
and Dan Moldovan. 2005. Exploiting ontologies for
automatic image annotation. In Special Interest Group
on Information Retrieval SIGIR. ACM.
C.L. Teo, Yezhou Yang, H. Daume, C. Fermuller, and
Y. Aloimonos. 2012. Towards a watson that sees:
Language-guided action recognition for robots. In
2012 IEEE International Conference on Robotics and
Automation ICRA.
J R R Uijlings, A W M Smeulders, and R J H Scha. 2010.
Real-time Visual Concept Classification. IEEE Trans-
actions on Multimedia, 12.
J R R Uijlings, K.E.A. van de Sande, T. Gevers, and
A.W.M. Smeulders. 2013. Selective search for ob-
ject recognition. International Journal of Computer
Vision.
Yoshitaka Ushiku, Tatsuya Harada, and Yasuo Ku-
niyoshi. 2012. Efficient image annotation for auto-
matic sentence generation. In ACM International Con-
ference on Multimedia ACM MM.
H. Wang, A. Kl¨aser, C. Schmid, and C. Liu. 2013. Dense
trajectories and motion boundary descriptors for ac-
tion recognition. International Journal of Computer
Vision, 103:60–79.
Jianxiong Xiao, J. Hays, K.A. Ehinger, A. Oliva, and
A. Torralba. 2010. Sun database: Large-scale scene
recognition from abbey to zoo. In Conference on
Computer Vision and Pattern Recognition CVPR.
Yezhou Yang, Ching Lik Teo, Hal Daum´e, III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gener-
ation of natural images. In Conference on Empirical
Methods in Natural Language Processing EMNLP.
Bangpeng Yao, Xiaoye Jiang, Aditya Khosla, Andy Lai
Lin, Leonidas J. Guibas, and Li Fei-Fei. 2011. Ac-
tion recognition by learning bases of action attributes
and parts. In International Conference on Computer
Vision ICCV.
Xiaodong Yu, Cornelia Fermuller, Ching Lik Teo,
Yezhou Yang, and Yiannis Aloimonos. 2011. Ac-
tive scene recognition with vision and language. In
Proceedings of the 2011 International Conference on
Computer Vision, International Conference on Com-
puter Vision ICCV.
</reference>
<page confidence="0.998647">
779
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.917801">
<title confidence="0.999929">Exploiting language models for visual recognition</title>
<author confidence="0.999308">Dieu-Thu Le_Jasper Uijlings Raffaella Bernardi</author>
<affiliation confidence="0.999928">DISI, University of Trento DISI, University of Trento DISI, University of Trento</affiliation>
<address confidence="0.999061">Povo, 38123, Italy Povo, 38123, Italy Povo, 38123, Italy</address>
<email confidence="0.986539">dle@disi.unitn.itjrr@disi.unitn.itbernardi@disi.unitn.it</email>
<abstract confidence="0.996758652173913">The problem of learning language models from large text corpora has been widely studied within the computational linguistic community. However, little is known about the performance of these language models when applied to the computer vision domain. In this work, we compare representative models: a window-based model, a topic model, a distributional memory and a commonsense knowledge database, ConceptNet, in two visual recognition scenarios: human action recognition and object prediction. We examine whether the knowledge extracted from texts through these models are compatible to the knowledge represented in images. We determine the usefulness of different language models in aiding the two visual recognition tasks. The study shows that the language models built from general text corpora can be used instead of expensive annotated images and even outperform the image model when testing on a big general dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics. Computational Linguistics.</title>
<date>2010</date>
<contexts>
<context position="3007" citStr="Baroni and Lenci, 2010" startWordPosition="450" endWordPosition="453">nefit from natural language processing as learning from images requires a prohibitively expensive annotation effort. A major goal of natural language processing is to obtain general knowledge from text and in this paper we test which model provides the best knowledge for use in the visual domain. Within the two visual scenarios, we compare three state-of-the-art language models and a knowledge base: (1) A window-based model, which counts co-occurrence frequencies within a fixed window; (2) R-LDA (S´eaghdha, 2010), an extension of LDA that enables generation of joint probabilities; (3) TypeDM (Baroni and Lenci, 2010), a strong Distributional Memory model; (4) ConceptNet (Speer and Havasi, 2013), an automatically generated semantic graph containing concepts with their relations. We test the language models in two ways: (1) We directly compare the statistics of the linguistic models with statistics extracted from the visual domain. 769 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 769–779, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics (2) We compare the linguistic models inside the two computer vision applicatio</context>
<context position="20331" citStr="Baroni and Lenci, 2010" startWordPosition="3342" endWordPosition="3345">at dog whisker cat sheep lamb table desk sheep wool boat float on water bird forest chair store horse zebra desk chair sheep wool cat feline table put thing on table eat at car city bottle store cat household train railroad dog a wolf dog canine boat travel on water cat catch mouse table kitchen chair office horsehair horse sheep wool cat dog plant flower chair sit cow milk chair office chair city Table 1: Examples of relations extracted from ConceptNet 5 ing function provided by the models, for example: P(viloj) = LMIvi,oj (10) Ev„ EV LMIv.,oj 4.3 Distributional Memory Distributional Memory (Baroni and Lenci, 2010) (DM) is a multi-purpose framework for semantic modeling. This model is more complex than the Window models because it exploits different degrees of lexicalization for each relation. Distributional information is extracted as a set of weighted &lt;word-link-word&gt; tuples obtained from a dependency parse of corpora. In the Window model the relation between each word pair is decided by their co-occurences within a sliding window, while in DM this relation is defined by distributional properties of the two words. These distributional properties are based on a syntactic relation or lexico-syntactic pa</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>NamKhanh Tran</author>
</authors>
<title>Distributional semantics in technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, ACL.</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="18514" citStr="Bruni et al., 2012" startWordPosition="3032" endWordPosition="3035">l probabilities using their frequency counts. For example, to compute the conditional probability of an object given a scene P (oi|sj), we extract all triples having the form &lt;object, rel, scene&gt;, where “rel” can be “AtLocation”, “LocatedNear”, etc. f req(&lt; oi, rel, sj &gt;) P(oi |sj) = Eo EO f req&lt; om, rel, s� &gt; (9) , 4.2 Window model One of the most famous and basic statistical model is based on counting co-occurrences within a window of fixed width, which follows the tradition of hyperspace analogue to language (Lund and Burgess, 1996). We took the Window2, 20 models which have been built in (Bruni et al., 2012) using the ukWaC (1.9B tokens) and Wackypedia (820M tokens). As the Window2 model only looks at 2 words on the left and right of the current one, it reflects the relationships between words occurring near each other, while the Window20 searches for a broader view of how words are related to each other. The weights of each pairs of words are calculated using the Local Mutual Information (LMI). To compute the conditional probabilities, we use the LMI scor773 LocatedNear RelatedTo UsedFor AtLocation oil car seatbelt car horse zebra plant garden bottle store liquid horse race bus city car city cha</context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Elia Bruni, Gemma Boleda, Marco Baroni, and NamKhanh Tran. 2012. Distributional semantics in technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, ACL. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Delaitre</author>
<author>Ivan Laptev</author>
<author>Josef Sivic</author>
</authors>
<title>Recognizing human actions in still images: a study of bag-of-features and part-based representations.</title>
<date>2010</date>
<booktitle>In Proceedings of the British Machine Vision Conference.</booktitle>
<publisher>BMVA Press.</publisher>
<contexts>
<context position="6456" citStr="Delaitre et al., 2010" startWordPosition="995" endWordPosition="998">- object - scene), hence our method does not limit the number of actions in images. (Lampert et al., 2009) use attributes of nouns (e.g., an animal: white, eat fish, water, etc.). They can detect animals without having seen training examples by manually defining the attributes of the target animal. In this work, rather than relying on manual definitions, our aim is to find the best language models built automatically from available corpora to extract relations from natural language. Currently, human action recognition is popular and mostly studied in video using the Bag-of-VisualWords method (Delaitre et al., 2010; Everts et al., 2013; Kuehne et al., 2012; Reddy and Shah, 2012; Wang et al., 2013). In this method one extracts small local visual patches of, say, 24 by 24 pixels by 10 frames at every 12th pixel at every 5th frame. For each patch local gradients or local movement (optical flow) histograms are calculated. Then these local visual features are mapped to abstract, predefined “visual words”, previously obtained using k-means clustering on a set of random features. While results are good, there are two main drawbacks with this approach. First of all, human actions are semantic and more naturally</context>
<context position="13275" citStr="Delaitre et al., 2010" startWordPosition="2158" endWordPosition="2161">rgmin L(wF) (7) wF As we have only four parameters to learn in our energy model, we do this by performing an exhaustive search and cross validation. We require wF ∈ {0.0, 0.1, 0.2, ..., 0.9} and set the constraint E FEF wF = 1. We note that the factor graph formulation of our framework would allow us to use more advanced learning algorithms. We plan to look into this once the model becomes more complex by adding, for example, information about the position of the objects and the human. 3.1.3 Dataset Recently, researchers have released many image action datasets such as the 7 everyday actions (Delaitre et al., 2010), the Stanford 40 action dataset (Yao et al., 2011), the PASCAL action classification competition (Everingham et al., 2012), and the 89 action dataset (Le et al., 2013). The 89 action dataset was originally created for the recognition of 20 objects. Afterwards also actions were annotated. Therefore, the actions occurring with these objects are mostly unbiased, unlike in other action datasets. Hence we choose to use the 89 action dataset. In the 89 action dataset, every image has been annotated with human actions, where each action is composed of a verb and an object. We additionally annotated </context>
</contexts>
<marker>Delaitre, Laptev, Sivic, 2010</marker>
<rawString>Vincent Delaitre, Ivan Laptev, and Josef Sivic. 2010. Recognizing human actions in still images: a study of bag-of-features and part-based representations. In Proceedings of the British Machine Vision Conference. BMVA Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Everingham</author>
<author>L Van Gool</author>
<author>C K I Williams</author>
<author>J Winn</author>
<author>A Zisserman</author>
</authors>
<date>2012</date>
<booktitle>The PASCAL Visual Object Classes Challenge</booktitle>
<note>(VOC2012) Results.</note>
<marker>Everingham, Van Gool, Williams, Winn, Zisserman, 2012</marker>
<rawString>M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. 2012. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Everts</author>
<author>J van Gemert</author>
<author>T Gevers</author>
</authors>
<title>Evaluation of color stips for human action recognition.</title>
<date>2013</date>
<booktitle>In CVPR.</booktitle>
<marker>Everts, van Gemert, Gevers, 2013</marker>
<rawString>I. Everts, J. van Gemert, and T. Gevers. 2013. Evaluation of color stips for human action recognition. In CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhinav Gupta</author>
<author>Larry S Davis</author>
</authors>
<title>Beyond nouns: Exploiting prepositions and comparative adjectives for learning visual classifiers.</title>
<date>2008</date>
<booktitle>In Proceedings of the 10th European Conference on Computer Vision.</booktitle>
<contexts>
<context position="4264" citStr="Gupta and Davis, 2008" startWordPosition="641" endWordPosition="644">of their usefulness. To summarize, our main research questions are: (1) Is the knowledge from language compatible with the knowledge from vision? (2) Can the knowledge extracted from language help in computer vision scenarios? 2 Related Work Using high level knowledge to aid image understanding has become a recent interest in the computer vision community. Objects, actions and scenes are detected and localized in images using lowlevel features. This detection and localization process is guided by reasoning and knowledge. Such knowledge is employed to disambiguate locations between objects in (Gupta and Davis, 2008). From the defined relationships between nouns (e.g., above, below, brighter, smaller), the system constrains which region in an image corresponds to which object/noun. Similarly, (Srikanth et al., 2005) exploit ontologies extracted from WordNet to associate words and images and image regions. (Yu et al., 2011) employ relations between scenes and objects introducing an active model to recognize scenes through objects. The reasoning knowledge limits the detector to search for an object within a particular region rather than on the whole image. Language models have also been employed to generate</context>
</contexts>
<marker>Gupta, Davis, 2008</marker>
<rawString>Abhinav Gupta and Larry S. Davis. 2008. Beyond nouns: Exploiting prepositions and comparative adjectives for learning visual classifiers. In Proceedings of the 10th European Conference on Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Heinrich</author>
</authors>
<title>Parameter estimation for text analysis.</title>
<date>2004</date>
<tech>Technical report.</tech>
<contexts>
<context position="22593" citStr="Heinrich, 2004" startWordPosition="3707" endWordPosition="3708"> LMI scores provided by the model (Equation 10). 4.4 R-LDA To model the relationships between verbs, objects and scenes, we adapt the R-LDA model (S´eaghdha, 2010) (ROOTH-LDA), which has been used for the selectional preference task in order to obtain conditional probabilities of two words. Each relation m of &lt; w1, w2 &gt; is generated by picking up a distribution over topics, then both elements of the relation m share the same topic assignment zm, which keep two different w1-topic and w2-topic distributions sharing the same topic (Figure 4). The models are estimated by Gibbs sampling following (Heinrich, 2004). It is also noted that these models are generative, hence they also predict the probabilities of tuples that do not occur in the corpus. Figure 4: Generative graphical model of R-LDA: modeling the relations between two words To model the relations between objects and verbs, we follow the data preparation in (Le et al., 2013), using the British National Corpus (BNC) which has been preprocessed and parsed using TreeTagger and Maltparser. Verbs are heads of sentences while objects are either direct or indirect objects related to those verbs by the parser. For the relations between verbs and scen</context>
</contexts>
<marker>Heinrich, 2004</marker>
<rawString>Gregor Heinrich. 2004. Parameter estimation for text analysis. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kuehne</author>
<author>D Gehrig</author>
<author>T Schultz</author>
<author>R Stiefelhagen</author>
</authors>
<title>On-line action recognition from sparse feature flow.</title>
<date>2012</date>
<booktitle>In VISAPP.</booktitle>
<contexts>
<context position="6498" citStr="Kuehne et al., 2012" startWordPosition="1003" endWordPosition="1006"> limit the number of actions in images. (Lampert et al., 2009) use attributes of nouns (e.g., an animal: white, eat fish, water, etc.). They can detect animals without having seen training examples by manually defining the attributes of the target animal. In this work, rather than relying on manual definitions, our aim is to find the best language models built automatically from available corpora to extract relations from natural language. Currently, human action recognition is popular and mostly studied in video using the Bag-of-VisualWords method (Delaitre et al., 2010; Everts et al., 2013; Kuehne et al., 2012; Reddy and Shah, 2012; Wang et al., 2013). In this method one extracts small local visual patches of, say, 24 by 24 pixels by 10 frames at every 12th pixel at every 5th frame. For each patch local gradients or local movement (optical flow) histograms are calculated. Then these local visual features are mapped to abstract, predefined “visual words”, previously obtained using k-means clustering on a set of random features. While results are good, there are two main drawbacks with this approach. First of all, human actions are semantic and more naturally recognized through their components (huma</context>
</contexts>
<marker>Kuehne, Gehrig, Schultz, Stiefelhagen, 2012</marker>
<rawString>H. Kuehne, D. Gehrig, T. Schultz, and R. Stiefelhagen. 2012. On-line action recognition from sparse feature flow. In VISAPP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Lampert</author>
<author>H Nickisch</author>
<author>S Harmeling</author>
</authors>
<title>Learning to detect unseen object classes by betweenclass attribute transfer.</title>
<date>2009</date>
<booktitle>In Conference on Computer Vision and Pattern Recognition CVPR.</booktitle>
<contexts>
<context position="5329" citStr="Lampert et al., 2009" startWordPosition="808" endWordPosition="811">e limits the detector to search for an object within a particular region rather than on the whole image. Language models have also been employed to generate descriptive sentences for images. (Ushiku et al., 2012) introduce an online learning method for multi-keyphrase estimation to generate a sentence using a grammar model to describe an image. Similarly, from objects and scenes detected in an image, (Yang et al., 2011) estimated a sentence structure to generate a sentence description composed of a noun, verb, scene and preposition. The studies most similar to ours are (Teo et al., 2012) and (Lampert et al., 2009). In (Teo et al., 2012), the Gigaword corpus is used to extract relationships between tools and actions (e.g., knife - cut, cup - drink) by counting their co-occurences. These relationships are used to constrain and select the most plausible actions within a predefined set of actions in cooking videos. Instead of using this knowledge as a guidance during recognition, we compare different language models and build a general framework that is able to detect unseen actions through their components (verb - object - scene), hence our method does not limit the number of actions in images. (Lampert e</context>
</contexts>
<marker>Lampert, Nickisch, Harmeling, 2009</marker>
<rawString>C.H. Lampert, H. Nickisch, and S. Harmeling. 2009. Learning to detect unseen object classes by betweenclass attribute transfer. In Conference on Computer Vision and Pattern Recognition CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lazebnik</author>
<author>C Schmid</author>
<author>J Ponce</author>
</authors>
<title>Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories.</title>
<date>2006</date>
<booktitle>In Conference on Computer Vision and Pattern Recognition CVPR,</booktitle>
<volume>2</volume>
<pages>2169--2178</pages>
<contexts>
<context position="13962" citStr="Lazebnik et al., 2006" startWordPosition="2274" endWordPosition="2277">ction classification competition (Everingham et al., 2012), and the 89 action dataset (Le et al., 2013). The 89 action dataset was originally created for the recognition of 20 objects. Afterwards also actions were annotated. Therefore, the actions occurring with these objects are mostly unbiased, unlike in other action datasets. Hence we choose to use the 89 action dataset. In the 89 action dataset, every image has been annotated with human actions, where each action is composed of a verb and an object. We additionally annotated every image with one of the 15 scenes from the 15 scene dataset (Lazebnik et al., 2006). 3.2 Objects in Context Our other computer vision scenario is about objects in context. Context is useful in visual recognition for two reasons: Firstly, context can significantly reduce the number of possible object categories simplifying the problem. Secondly, when the object appearance is inconclusive for its identity, context can be used for disambiguation. For example, a grey rectangle on a desk may be recognized as a pen, while a grey rectangle on a table may be recognized as a knife. As the recognition systems are not always reliable, the use of context can greatly improve results. For</context>
</contexts>
<marker>Lazebnik, Schmid, Ponce, 2006</marker>
<rawString>S. Lazebnik, C. Schmid, and J. Ponce. 2006. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In Conference on Computer Vision and Pattern Recognition CVPR, volume 2, pages 2169–2178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dieu Thu Le</author>
<author>Raffaella Bernardi</author>
<author>Jasper Uijlings</author>
</authors>
<title>Exploiting language models to recognize unseen actions.</title>
<date>2013</date>
<booktitle>In Proceedings of the 3rd ACM conference on International conference on multimedia retrieval ICMR.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="13443" citStr="Le et al., 2013" startWordPosition="2186" endWordPosition="2189">, 0.2, ..., 0.9} and set the constraint E FEF wF = 1. We note that the factor graph formulation of our framework would allow us to use more advanced learning algorithms. We plan to look into this once the model becomes more complex by adding, for example, information about the position of the objects and the human. 3.1.3 Dataset Recently, researchers have released many image action datasets such as the 7 everyday actions (Delaitre et al., 2010), the Stanford 40 action dataset (Yao et al., 2011), the PASCAL action classification competition (Everingham et al., 2012), and the 89 action dataset (Le et al., 2013). The 89 action dataset was originally created for the recognition of 20 objects. Afterwards also actions were annotated. Therefore, the actions occurring with these objects are mostly unbiased, unlike in other action datasets. Hence we choose to use the 89 action dataset. In the 89 action dataset, every image has been annotated with human actions, where each action is composed of a verb and an object. We additionally annotated every image with one of the 15 scenes from the 15 scene dataset (Lazebnik et al., 2006). 3.2 Objects in Context Our other computer vision scenario is about objects in c</context>
<context position="22920" citStr="Le et al., 2013" startWordPosition="3761" endWordPosition="3764">enerated by picking up a distribution over topics, then both elements of the relation m share the same topic assignment zm, which keep two different w1-topic and w2-topic distributions sharing the same topic (Figure 4). The models are estimated by Gibbs sampling following (Heinrich, 2004). It is also noted that these models are generative, hence they also predict the probabilities of tuples that do not occur in the corpus. Figure 4: Generative graphical model of R-LDA: modeling the relations between two words To model the relations between objects and verbs, we follow the data preparation in (Le et al., 2013), using the British National Corpus (BNC) which has been preprocessed and parsed using TreeTagger and Maltparser. Verbs are heads of sentences while objects are either direct or indirect objects related to those verbs by the parser. For the relations between verbs and scenes, we consider also verbs as heads SZ . . a w2 w tpk (Pk Y R 774 Topic 8: Topic 14: Topic 0: Topic 54: Noun Verb Noun Verb Noun Noun Noun Noun people 0.0208 have 0.157 year 0.0154 win 0.109 attention 0.0172 study 0.01 decision 0.02 case .0176 job 0.0167 work 0.108 Cup 0.0093 have 0.099 model 0.0147 research 0.0123 view 0.024</context>
</contexts>
<marker>Le, Bernardi, Uijlings, 2013</marker>
<rawString>Dieu Thu Le, Raffaella Bernardi, and Jasper Uijlings. 2013. Exploiting language models to recognize unseen actions. In Proceedings of the 3rd ACM conference on International conference on multimedia retrieval ICMR. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yann Lecun</author>
<author>Sumit Chopra</author>
<author>Raia Hadsell</author>
<author>Fu J Huang</author>
<author>G Bakir</author>
<author>T Hofman</author>
<author>B Sch¨olkopf</author>
<author>A Smola</author>
<author>B Taskar Eds</author>
</authors>
<title>A tutorial on energy-based learning.</title>
<date>2006</date>
<booktitle>In Predicting Structured Data.</booktitle>
<marker>Lecun, Chopra, Hadsell, Huang, Bakir, Hofman, Sch¨olkopf, Smola, Eds, 2006</marker>
<rawString>Yann Lecun, Sumit Chopra, Raia Hadsell, Fu J. Huang, G. Bakir, T. Hofman, B. Sch¨olkopf, A. Smola, and B. Taskar Eds. 2006. A tutorial on energy-based learning. In Predicting Structured Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments, and Computers.</title>
<date>1996</date>
<contexts>
<context position="18436" citStr="Lund and Burgess, 1996" startWordPosition="3017" endWordPosition="3020">r, Horse - Related to - Zebra. From these relations, we define the four conditional probabilities using their frequency counts. For example, to compute the conditional probability of an object given a scene P (oi|sj), we extract all triples having the form &lt;object, rel, scene&gt;, where “rel” can be “AtLocation”, “LocatedNear”, etc. f req(&lt; oi, rel, sj &gt;) P(oi |sj) = Eo EO f req&lt; om, rel, s� &gt; (9) , 4.2 Window model One of the most famous and basic statistical model is based on counting co-occurrences within a window of fixed width, which follows the tradition of hyperspace analogue to language (Lund and Burgess, 1996). We took the Window2, 20 models which have been built in (Bruni et al., 2012) using the ukWaC (1.9B tokens) and Wackypedia (820M tokens). As the Window2 model only looks at 2 words on the left and right of the current one, it reflects the relationships between words occurring near each other, while the Window20 searches for a broader view of how words are related to each other. The weights of each pairs of words are calculated using the Local Mutual Information (LMI). To compute the conditional probabilities, we use the LMI scor773 LocatedNear RelatedTo UsedFor AtLocation oil car seatbelt car</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments, and Computers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Reddy</author>
<author>M Shah</author>
</authors>
<title>Recognizing 50 human action categories of web videos.</title>
<date>2012</date>
<booktitle>In Machine Vision and Applications.</booktitle>
<contexts>
<context position="6520" citStr="Reddy and Shah, 2012" startWordPosition="1007" endWordPosition="1010">actions in images. (Lampert et al., 2009) use attributes of nouns (e.g., an animal: white, eat fish, water, etc.). They can detect animals without having seen training examples by manually defining the attributes of the target animal. In this work, rather than relying on manual definitions, our aim is to find the best language models built automatically from available corpora to extract relations from natural language. Currently, human action recognition is popular and mostly studied in video using the Bag-of-VisualWords method (Delaitre et al., 2010; Everts et al., 2013; Kuehne et al., 2012; Reddy and Shah, 2012; Wang et al., 2013). In this method one extracts small local visual patches of, say, 24 by 24 pixels by 10 frames at every 12th pixel at every 5th frame. For each patch local gradients or local movement (optical flow) histograms are calculated. Then these local visual features are mapped to abstract, predefined “visual words”, previously obtained using k-means clustering on a set of random features. While results are good, there are two main drawbacks with this approach. First of all, human actions are semantic and more naturally recognized through their components (human, objects, scene) rat</context>
</contexts>
<marker>Reddy, Shah, 2012</marker>
<rawString>K. Reddy and M. Shah. 2012. Recognizing 50 human action categories of web videos. In Machine Vision and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid ´O S´eaghdha</author>
</authors>
<title>Latent variable models of selectional preference.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL,</booktitle>
<pages>435--444</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>S´eaghdha, 2010</marker>
<rawString>Diarmuid ´O. S´eaghdha. 2010. Latent variable models of selectional preference. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL, pages 435–444. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Speer</author>
<author>Catherine Havasi</author>
</authors>
<title>Conceptnet 5: A large semantic network for relational knowledge. In The People’s Web Meets NLP.</title>
<date>2013</date>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="3086" citStr="Speer and Havasi, 2013" startWordPosition="462" endWordPosition="465">bitively expensive annotation effort. A major goal of natural language processing is to obtain general knowledge from text and in this paper we test which model provides the best knowledge for use in the visual domain. Within the two visual scenarios, we compare three state-of-the-art language models and a knowledge base: (1) A window-based model, which counts co-occurrence frequencies within a fixed window; (2) R-LDA (S´eaghdha, 2010), an extension of LDA that enables generation of joint probabilities; (3) TypeDM (Baroni and Lenci, 2010), a strong Distributional Memory model; (4) ConceptNet (Speer and Havasi, 2013), an automatically generated semantic graph containing concepts with their relations. We test the language models in two ways: (1) We directly compare the statistics of the linguistic models with statistics extracted from the visual domain. 769 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 769–779, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics (2) We compare the linguistic models inside the two computer vision applications, leading to a direct estimation of their usefulness. To summarize, our main </context>
<context position="16745" citStr="Speer and Havasi, 2013" startWordPosition="2736" endWordPosition="2739">ed the predefined training and testing parts of the SUN dataset and obtained around 4,500 images for learning the object relations and 10,600 images for testing the object prediction. We obtain conditional probabilities P(oj|oi) from frequency counts. 4 Language models &amp; distribution extraction To extract probability distributions from texts, we use ConceptNet, the Window2 and 20 model, TypeDM and R-LDA. We will now describe how we estimate the four conditional probabilities P(V |O), P(V |S), P(O|S), P(O|O) needed in the two visual scenarios for each language model. 4.1 ConceptNet ConceptNet (Speer and Havasi, 2013) is a large semantic graph containing concepts and relations between them. It includes everyday basic, cultural and scientific knowledge, which have been automatically extracted from Internet using predefined rules. In this work we use the most current version, ConceptNet 5. As it was mined from free text using rules, the database has uncontrolled vocabulary and contains many false/nonesense statements. To extract relations from ConceptNet5, we first examine all relations in the database and define those that are relevant to our scenarios (Figure 3). For Figure 3: List of relations in ConceptN</context>
</contexts>
<marker>Speer, Havasi, 2013</marker>
<rawString>Robert Speer and Catherine Havasi. 2013. Conceptnet 5: A large semantic network for relational knowledge. In The People’s Web Meets NLP. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Munirathnam Srikanth</author>
<author>Joshua Varner</author>
<author>Mitchell Bowden</author>
<author>Dan Moldovan</author>
</authors>
<title>Exploiting ontologies for automatic image annotation.</title>
<date>2005</date>
<booktitle>In Special Interest Group on Information Retrieval SIGIR.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="4467" citStr="Srikanth et al., 2005" startWordPosition="670" endWordPosition="673">puter vision scenarios? 2 Related Work Using high level knowledge to aid image understanding has become a recent interest in the computer vision community. Objects, actions and scenes are detected and localized in images using lowlevel features. This detection and localization process is guided by reasoning and knowledge. Such knowledge is employed to disambiguate locations between objects in (Gupta and Davis, 2008). From the defined relationships between nouns (e.g., above, below, brighter, smaller), the system constrains which region in an image corresponds to which object/noun. Similarly, (Srikanth et al., 2005) exploit ontologies extracted from WordNet to associate words and images and image regions. (Yu et al., 2011) employ relations between scenes and objects introducing an active model to recognize scenes through objects. The reasoning knowledge limits the detector to search for an object within a particular region rather than on the whole image. Language models have also been employed to generate descriptive sentences for images. (Ushiku et al., 2012) introduce an online learning method for multi-keyphrase estimation to generate a sentence using a grammar model to describe an image. Similarly, f</context>
</contexts>
<marker>Srikanth, Varner, Bowden, Moldovan, 2005</marker>
<rawString>Munirathnam Srikanth, Joshua Varner, Mitchell Bowden, and Dan Moldovan. 2005. Exploiting ontologies for automatic image annotation. In Special Interest Group on Information Retrieval SIGIR. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Teo</author>
<author>Yezhou Yang</author>
<author>H Daume</author>
<author>C Fermuller</author>
<author>Y Aloimonos</author>
</authors>
<title>Towards a watson that sees: Language-guided action recognition for robots.</title>
<date>2012</date>
<booktitle>In 2012 IEEE International Conference on Robotics and Automation ICRA.</booktitle>
<contexts>
<context position="5302" citStr="Teo et al., 2012" startWordPosition="803" endWordPosition="806"> The reasoning knowledge limits the detector to search for an object within a particular region rather than on the whole image. Language models have also been employed to generate descriptive sentences for images. (Ushiku et al., 2012) introduce an online learning method for multi-keyphrase estimation to generate a sentence using a grammar model to describe an image. Similarly, from objects and scenes detected in an image, (Yang et al., 2011) estimated a sentence structure to generate a sentence description composed of a noun, verb, scene and preposition. The studies most similar to ours are (Teo et al., 2012) and (Lampert et al., 2009). In (Teo et al., 2012), the Gigaword corpus is used to extract relationships between tools and actions (e.g., knife - cut, cup - drink) by counting their co-occurences. These relationships are used to constrain and select the most plausible actions within a predefined set of actions in cooking videos. Instead of using this knowledge as a guidance during recognition, we compare different language models and build a general framework that is able to detect unseen actions through their components (verb - object - scene), hence our method does not limit the number of ac</context>
</contexts>
<marker>Teo, Yang, Daume, Fermuller, Aloimonos, 2012</marker>
<rawString>C.L. Teo, Yezhou Yang, H. Daume, C. Fermuller, and Y. Aloimonos. 2012. Towards a watson that sees: Language-guided action recognition for robots. In 2012 IEEE International Conference on Robotics and Automation ICRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R R Uijlings</author>
</authors>
<title>Real-time Visual Concept Classification.</title>
<date>2010</date>
<journal>A W M Smeulders, and R J H Scha.</journal>
<volume>12</volume>
<marker>Uijlings, 2010</marker>
<rawString>J R R Uijlings, A W M Smeulders, and R J H Scha. 2010. Real-time Visual Concept Classification. IEEE Transactions on Multimedia, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R R Uijlings</author>
<author>K E A van de Sande</author>
<author>T Gevers</author>
<author>A W M Smeulders</author>
</authors>
<title>Selective search for object recognition.</title>
<date>2013</date>
<journal>International Journal of Computer Vision.</journal>
<marker>Uijlings, van de Sande, Gevers, Smeulders, 2013</marker>
<rawString>J R R Uijlings, K.E.A. van de Sande, T. Gevers, and A.W.M. Smeulders. 2013. Selective search for object recognition. International Journal of Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshitaka Ushiku</author>
<author>Tatsuya Harada</author>
<author>Yasuo Kuniyoshi</author>
</authors>
<title>Efficient image annotation for automatic sentence generation.</title>
<date>2012</date>
<booktitle>In ACM International Conference on Multimedia ACM MM.</booktitle>
<contexts>
<context position="4920" citStr="Ushiku et al., 2012" startWordPosition="741" endWordPosition="744">ween nouns (e.g., above, below, brighter, smaller), the system constrains which region in an image corresponds to which object/noun. Similarly, (Srikanth et al., 2005) exploit ontologies extracted from WordNet to associate words and images and image regions. (Yu et al., 2011) employ relations between scenes and objects introducing an active model to recognize scenes through objects. The reasoning knowledge limits the detector to search for an object within a particular region rather than on the whole image. Language models have also been employed to generate descriptive sentences for images. (Ushiku et al., 2012) introduce an online learning method for multi-keyphrase estimation to generate a sentence using a grammar model to describe an image. Similarly, from objects and scenes detected in an image, (Yang et al., 2011) estimated a sentence structure to generate a sentence description composed of a noun, verb, scene and preposition. The studies most similar to ours are (Teo et al., 2012) and (Lampert et al., 2009). In (Teo et al., 2012), the Gigaword corpus is used to extract relationships between tools and actions (e.g., knife - cut, cup - drink) by counting their co-occurences. These relationships a</context>
</contexts>
<marker>Ushiku, Harada, Kuniyoshi, 2012</marker>
<rawString>Yoshitaka Ushiku, Tatsuya Harada, and Yasuo Kuniyoshi. 2012. Efficient image annotation for automatic sentence generation. In ACM International Conference on Multimedia ACM MM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wang</author>
<author>A Kl¨aser</author>
<author>C Schmid</author>
<author>C Liu</author>
</authors>
<title>Dense trajectories and motion boundary descriptors for action recognition.</title>
<date>2013</date>
<journal>International Journal of Computer Vision,</journal>
<pages>103--60</pages>
<marker>Wang, Kl¨aser, Schmid, Liu, 2013</marker>
<rawString>H. Wang, A. Kl¨aser, C. Schmid, and C. Liu. 2013. Dense trajectories and motion boundary descriptors for action recognition. International Journal of Computer Vision, 103:60–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianxiong Xiao</author>
<author>J Hays</author>
<author>K A Ehinger</author>
<author>A Oliva</author>
<author>A Torralba</author>
</authors>
<title>Sun database: Large-scale scene recognition from abbey to zoo.</title>
<date>2010</date>
<booktitle>In Conference on Computer Vision and Pattern Recognition CVPR.</booktitle>
<contexts>
<context position="15507" citStr="Xiao et al., 2010" startWordPosition="2549" endWordPosition="2552">edict the object identities. Formally, we can describe this scenario as follows: Given an image I with N objects O = {o1, o2, · · · , oN}, we want to predict the identity of object oi given all other objects O \ oi. In this paper we use a Naive Bayes assumption, leading to: P (O \ oi|oi) × P (oi) P (oi|O \ oi) = P (O \ oi) ≈ P(oi) × rI P(oj|oi). (8) ojEd\oi P(oj|S) × P(S|I) Pi. (6) 772 In this scenario, we need conditional relations P(oj|oi) and priors. We obtain these from language data or from images directly. 3.2.1 Dataset For the objects in context scenario, we use the SUN object dataset (Xiao et al., 2010), which contains more than 16 thousand images, more than 79,000 objects whose locations are annotated using polygons. The dataset has been annotated by various people who could choose their own object categories, leading to duplicate categories such as “building” and “buildings”, “person” and “person walking”. Furthermore, for some images large parts are not annotated leading to an incomplete context. We therefore cleaned the object categories (mapping from around 7,500 objects to over 700 unique object categories) and considered only images whose content was sufficiently annotated. In our exp</context>
</contexts>
<marker>Xiao, Hays, Ehinger, Oliva, Torralba, 2010</marker>
<rawString>Jianxiong Xiao, J. Hays, K.A. Ehinger, A. Oliva, and A. Torralba. 2010. Sun database: Large-scale scene recognition from abbey to zoo. In Conference on Computer Vision and Pattern Recognition CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yezhou Yang</author>
</authors>
<title>Ching Lik Teo, Hal Daum´e, III, and Yiannis Aloimonos.</title>
<date>2011</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing EMNLP.</booktitle>
<marker>Yang, 2011</marker>
<rawString>Yezhou Yang, Ching Lik Teo, Hal Daum´e, III, and Yiannis Aloimonos. 2011. Corpus-guided sentence generation of natural images. In Conference on Empirical Methods in Natural Language Processing EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bangpeng Yao</author>
<author>Xiaoye Jiang</author>
<author>Aditya Khosla</author>
<author>Andy Lai Lin</author>
<author>Leonidas J Guibas</author>
<author>Li Fei-Fei</author>
</authors>
<title>Action recognition by learning bases of action attributes and parts.</title>
<date>2011</date>
<booktitle>In International Conference on Computer Vision ICCV.</booktitle>
<contexts>
<context position="13326" citStr="Yao et al., 2011" startWordPosition="2167" endWordPosition="2170">arn in our energy model, we do this by performing an exhaustive search and cross validation. We require wF ∈ {0.0, 0.1, 0.2, ..., 0.9} and set the constraint E FEF wF = 1. We note that the factor graph formulation of our framework would allow us to use more advanced learning algorithms. We plan to look into this once the model becomes more complex by adding, for example, information about the position of the objects and the human. 3.1.3 Dataset Recently, researchers have released many image action datasets such as the 7 everyday actions (Delaitre et al., 2010), the Stanford 40 action dataset (Yao et al., 2011), the PASCAL action classification competition (Everingham et al., 2012), and the 89 action dataset (Le et al., 2013). The 89 action dataset was originally created for the recognition of 20 objects. Afterwards also actions were annotated. Therefore, the actions occurring with these objects are mostly unbiased, unlike in other action datasets. Hence we choose to use the 89 action dataset. In the 89 action dataset, every image has been annotated with human actions, where each action is composed of a verb and an object. We additionally annotated every image with one of the 15 scenes from the 15 s</context>
</contexts>
<marker>Yao, Jiang, Khosla, Lin, Guibas, Fei-Fei, 2011</marker>
<rawString>Bangpeng Yao, Xiaoye Jiang, Aditya Khosla, Andy Lai Lin, Leonidas J. Guibas, and Li Fei-Fei. 2011. Action recognition by learning bases of action attributes and parts. In International Conference on Computer Vision ICCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong Yu</author>
</authors>
<title>Cornelia Fermuller, Ching Lik Teo, Yezhou Yang, and Yiannis Aloimonos.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 International Conference on Computer Vision, International Conference on Computer Vision ICCV.</booktitle>
<marker>Yu, 2011</marker>
<rawString>Xiaodong Yu, Cornelia Fermuller, Ching Lik Teo, Yezhou Yang, and Yiannis Aloimonos. 2011. Active scene recognition with vision and language. In Proceedings of the 2011 International Conference on Computer Vision, International Conference on Computer Vision ICCV.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
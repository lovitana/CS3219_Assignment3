<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996067">
Adaptor Grammars for Learning Non-Concatenative Morphology
</title>
<author confidence="0.983593">
Jan A. Botha and Phil Blunsom
</author>
<affiliation confidence="0.997733">
Department of Computer Science
University of Oxford
</affiliation>
<address confidence="0.996846">
Oxford, OX1 3QD, UK
</address>
<email confidence="0.99799">
{jan.botha,phil.blunsom}@cs.ox.ac.uk
</email>
<sectionHeader confidence="0.982947" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999375">
This paper contributes an approach for
expressing non-concatenative morphological
phenomena, such as stem derivation in
Semitic languages, in terms of a mildly
context-sensitive grammar formalism. This
offers a convenient level of modelling ab-
straction while remaining computationally
tractable. The nonparametric Bayesian frame-
work of adaptor grammars is extended to this
richer grammar formalism to propose a prob-
abilistic model that can learn word segmenta-
tion and morpheme lexicons, including ones
with discontiguous strings as elements, from
unannotated data. Our experiments on He-
brew and three variants of Arabic data find
that the additional expressiveness to capture
roots and templates as atomic units improves
the quality of concatenative segmentation and
stem identification. We obtain 74% accuracy
in identifying triliteral Hebrew roots, while
performing morphological segmentation with
an F1-score of 78.1.
</bodyText>
<sectionHeader confidence="0.995148" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972957446809">
Unsupervised learning of morphology is the task
of acquiring, from unannotated data, the intra-word
building blocks of a language and the rules by which
they combine to form words. This task is of interest
both as a gateway for studying language acquisition
in humans and as a way of producing morphological
analyses that are of practical use in a variety of nat-
ural language processing tasks, including machine
translation, parsing and information retrieval.
A particularly interesting version of the morphol-
ogy learning problem comes from languages that
use templatic morphology, such as Arabic, He-
brew and Amharic. These Semitic languages de-
rive verb and noun stems by interspersing abstract
root morphemes into templatic structures in a non-
concatenative way. For example, the Arabic root
k·t·b can combine with the template (i-a) to derive
the noun stem kitab (book). Established morpho-
logical analysers typically ignore this process and
simply view the derived stems as elementary units
(Buckwalter, 2002), or their account of it coincides
with a requirement for extensive linguistic knowl-
edge and hand-crafting of rules (Finkel and Stump,
2002; Schneider, 2010; Altantawy et al., 2010). The
former approach is bound to suffer from vocabu-
lary coverage issues, while the latter clearly does
not transfer easily across languages. The practical
appeal of unsupervised learning of templatic mor-
phology is that it can overcome these shortcomings.
Unsupervised learning of concatenative morphol-
ogy has received extensive attention, partly driven
by the MorphoChallenge (Kurimo et al., 2010) in re-
cent years, but that is not the case for root-templatic
morphology (Hammarstr¨om and Borin, 2011).
In this paper we present a model-based method
that learns concatenative and root-templatic mor-
phology in a unified framework. We build on two
disparate strands of work from the literature: Firstly,
we apply simple Range Concatenating Grammars
(SRCGs) (Boullier, 2000) to parse contiguous and
discontiguous morphemes from an input string.
These grammars are mildly-context sensitive (Joshi,
1985), a superset of context-free grammars that
retains polynomial parsing time-complexity. Sec-
ondly, we generalise the nonparametric Bayesian
learning framework of adaptor grammars (Johnson
et al., 2007) to SRCGs.1 This should also be rel-
</bodyText>
<note confidence="0.85692425">
1Our formulation is in terms of SRCGs, which are equiv-
alent in power to linear context-free rewrite systems (Vijay-
Shanker et al., 1987) and multiple context-free grammars (Seki
et al., 1991), all of which are weaker than (non-simple) range
concatenating grammars (Boullier, 2000).
345
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 345–356,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999796066666667">
evant to other applications of probabilistic SRCGs,
e.g. in parsing (Maier, 2010), translation (Kaesham-
mer, 2013) and genetics (Kato et al., 2006).
In addition to unannotated data, our method re-
quires as input a minimal set of high-level grammar
rules that encode basic intuitions of the morphology.
This is where there would be room to become very
language specific. Our aim, however, is not to obtain
a best-published result in a particular language, but
rather to create a method that is applicable across
a variety of morphological processes. The specific
rules used in our empirical evaluation on Arabic and
Hebrew therefore contain hardly any explicit lin-
guistic knowledge about the languages and are ap-
plicable across the family of Semitic languages.
</bodyText>
<sectionHeader confidence="0.618594" genericHeader="method">
2 A powerful grammar for morphology
</sectionHeader>
<bodyText confidence="0.999495358974359">
Concatenative morphology lends itself well to an
analysis in terms of finite-state transducers (FSTs)
(Koskenniemi, 1984). With some additional effort,
FSTs can also encode non-concatenative morphol-
ogy (Kiraz, 2000; Beesley and Karttunen, 2003;
Cohen-Sygal and Wintner, 2006; Gasser, 2009). De-
spite this seeming adequacy of regular languages to
describe morphology, we see two main shortcom-
ings that motivate moving further up the Chom-
sky hierarchy of formal languages: first is the is-
sue of learning. We are not aware of successful at-
tempts at inducing FST-based morphological analy-
sers in an unsupervised way, and believe the chal-
lenge lies in the fact that FSTs do not offer a conve-
nient way of expressing prior linguistic intuitions to
guide the learning process. Secondly, an FST com-
posed of multiple machines might capture morpho-
logical processes well and excel at analysis, but in-
terpretability of its internal operations are limited.
These shortcomings are overcome for concate-
native morphology by context-free adaptor gram-
mars, which allowed diverse segmentation models
to be formulated and investigated within a single
framework (Johnson et al., 2007; Johnson, 2008;
Sirts and Goldwater, 2013). In principle, that cov-
ers a wide range of phenomena (typical example
language in parentheses): affixal inflection (Czech)
and derivation (English), agglutinative derivation
(Turkish, Finnish), compounding (German). Our
agenda here is to extend that approach to include
non-concatenative processes such as root-templatic
derivation (Arabic), infixation (Tagalog) and cir-
cumfixation (Indonesian). In this pursuit, an ab-
straction that permits discontiguous constituents is
a highly useful modelling tool, but requires looking
beyond context-free grammars.
An idealised generative grammar that would cap-
ture all the aforementioned phenomena could look
like this:
</bodyText>
<equation confidence="0.964386">
Word → (Pre* Stem Suf*)+ (1)
e.g. English un+accept+able
Stem  |Pre  |Suf → Morph (2)
Stem → intercal (Root, Template) (3)
</equation>
<bodyText confidence="0.94987536">
e.g. Arabic derivation k·t·b + i·a ⇒ kitab (book)
Stem → infix (Stem, Infix) (4)
e.g. Tagalog sulat (write) ⇒ sumulat (wrote)
Stem → circfix (Stem, Circumfix) (5)
e.g. Indonesian percaya (to trust)
⇒ kepercayaan (belief)
where the symbols (excluding Word and Stem) im-
plicitly expand to the relevant terminal strings. The
bold-faced “functions” combine the potentially dis-
contiguous yields of the argument symbols into sin-
gle contiguous strings, e.g. infix(s·ulat, um) pro-
duces stem sumulat.
Taken by themselves, the first two rules are sim-
ply a CFG that describes word formation as the
concatenation of stems and affixes, a formulation
that matches the underlying grammar of Morfessor
(Creutz and Lagus, 2007), a well-studied unsuper-
vised model.
The key aim of our extension is that we want the
grammar to capture a discontiguous string like k·t·b
as a single constituent in a parse tree. This leads to
well-understood problems in probabilistic grammars
(e.g. what is this rule’s probability?), but also corre-
sponds to the linguistic consideration that k·t·b is a
proper morpheme of the language (Prunet, 2006).
</bodyText>
<sectionHeader confidence="0.896753" genericHeader="method">
3 Simple range concatenating grammars
</sectionHeader>
<bodyText confidence="0.999934">
In this section we define SRCGs formally and
illustrate how they can be used to model non-
concatenative morphology. SRCGs define lan-
guages that are recognisable in polynomial time, yet
can capture discontiguous elements of a string un-
der a single category (Boullier, 2000). An SRCG-
</bodyText>
<page confidence="0.902493">
346
</page>
<bodyText confidence="0.9998984">
rule operates on vectors of ranges in contrast to the
way a CFG-rule operates on single ranges (spans).
In other words, a non-terminal symbol in an SRCG
(CFG) derivation can dominate a subset (substring)
of terminals in an input string.
</bodyText>
<subsectionHeader confidence="0.988778">
3.1 Formalism
</subsectionHeader>
<bodyText confidence="0.934053666666667">
An SRCG G is a tuple (N, T, V, P, S), with
finite sets of non-terminals (N), termi-
nals (T) and variables (V ), with a start sym-
</bodyText>
<equation confidence="0.826603333333333">
bol S E N. A rewrite rule p E P of rank
r = p(p) &gt; 0 h/&apos;as the form A(α1, ... , αψ(A))
B1(01,1,...,01,V,(B1)) ... Br(0r,1,...,0r,ψ(B�)),
</equation>
<bodyText confidence="0.999301148148148">
where each α, 0 E (T U V)*, and (A) is the
number of arguments a non-terminal A has, called
its arity. By definition, the start symbol has arity 1.
Any variable v E V appearing in a given rule
must be used exactly once on each side of the
rule. Terminating rules are written with a as the
right-hand side and thus have rank 0.
A range is a pair of integers (i, j) denoting the
substring wi+1 ... wj of a string w = w1 ... wn.
A non-terminal becomes instantiated when its vari-
ables are bound to ranges through substitution. Vari-
ables within an argument imply concatenation and
therefore have to bind to adjacent ranges.
An instantiated non-terminal A&apos; is said to de-
rive a if the consecutive application of a sequence
of instantiated rules rewrite it as e. A string w is
within the language defined by a particular SRCG
iff the start symbol S, instantiated with the exhaus-
tive range (0, wn), derives e.
An important distinction with regard to CFGs is
that, due to the instantiation mechanism, the order-
ing of non-terminals on the right-hand side of an
SRCG rule is irrelevant, i.e. A(ab) —* B(a)C(b)
and A(ab) —* C(b)B(a) are the same rule.2 Con-
sequently, the isomorphisms of any given SRCG
derivation tree all encode the same string, which is
uniquely defined through the instantiation process.
</bodyText>
<subsectionHeader confidence="0.999833">
3.2 Application to morphological analysis
</subsectionHeader>
<bodyText confidence="0.8106645">
A fragment of the idealised grammar schema from
the previous section (§2) can be rephrased as an
SRCG by writing the rules in the newly introduced
2Certain ordering restrictions over the variables within an
argument need to hold for an SRCG to indeed be a simple RCG
(Boullier, 2000).
</bodyText>
<figure confidence="0.483405">
Word(wakitabi)
</figure>
<figureCaption confidence="0.918207">
Figure 1: Example derivation for wakitabi (and my
</figureCaption>
<bodyText confidence="0.921448">
book) using the SRCG fragment from §3.2. CFGs
cannot capture such crossing branches.
notation, and supplying a definition of the intercal
function as simply another rule of the grammar, with
instantiation for w = kitab shown below:
</bodyText>
<equation confidence="0.9992724">
Word(abc) —* Pre(a) Stem(b) Suf(c)
Stem(abcde) —* Root(a, c, e) Template(b, d),
Stem((0..1), (1..2), (2..3), (3..4), (4..5))
� Root((0..1), (2..3), (4..5))
Template((1..2), (3..4))
</equation>
<bodyText confidence="0.9835424">
Given an appropriate set of grammar rules (as we
present in §5), we can parse an input string to ob-
tain a tree as shown in Figure 1. The overlapping
branches of the tree demonstrate that this grammar
captures something a CFG could not. From the parse
tree one can read off the word’s root morpheme and
the template used.
Although SRCGs specify mildly context-sensitive
grammars, each step in a derivation is context-free –
a node’s expansion does not depend on other parts
of the tree. This property implies that a recogni-
tion/parsing algorithm can have a worst-case time
complexity that is polynomial in the input length n,
O(n(ρ+1)ψ) for arity 0 and rank p, which reduces
to O(n3ψ) for a binarised grammar. To capture the
maximal case of a root with k − 1 characters and
k discontiguous templatic characters forming a stem
would require a grammar that has arity 0 = k. For
Arabic, which has up to quadriliteral roots (k = 5),
the time complexity would be O(n15).3 This is a
daunting proposition for parsing, but we are careful
3The trade-off between arity and rank with respect to pars-
ing complexity has been characterised (Gildea, 2010), and the
appropriate refactoring may bring down the complexity for our
grammars too.
</bodyText>
<equation confidence="0.869066285714286">
Suf(i)
Pre(wa)
Stm(kitab)
Root(k,t,b) Template(i,a)
w a k i t a b
i
347
</equation>
<bodyText confidence="0.9987403125">
to set up our application of SRCGs in such a way
that this is not too big an obstacle:
Firstly, our grammars are defined over the char-
acters that make up a word, and not over words that
make up a sentence. As such, the input length n
would tend to be shorter than when parsing full sen-
tences from a corpus.
Secondly, we do type-based morphological analy-
sis, a view supported by evidence from Goldwater et
al. (2006), so each unique word in a dataset is only
ever parsed once with a given grammar. The set of
word types attested in the data sources of interest
here is fairly limited, typically in the tens of thou-
sands. For these reasons, our parsing and inference
tasks turn out to be tractable despite the high time
complexity.
</bodyText>
<sectionHeader confidence="0.997026" genericHeader="method">
4 Learning
</sectionHeader>
<subsectionHeader confidence="0.999512">
4.1 Probabilistic SRCG
</subsectionHeader>
<bodyText confidence="0.99999088">
The probabilistic extension of SRCGs is similar to
the probabilistic extension of CFGs, and has been
used in other guises (Kato et al., 2006; Maier, 2010).
Each rule r E P has an associated probability 0r
such that ErEPA 0r = 1. A random string in
the language of the grammar can then be obtained
through a generative procedure that begins with the
start symbol S and iteratively expands it until deriv-
ing e: At each step for some current symbol A, a
rewrite rule r is sampled randomly from PA in ac-
cordance with the distribution over rules and used
to expand A. This procedure terminates when no
further expansions are possible. Of course, expan-
sions need to respect the range concatenating and or-
dering constraints imposed by the variables in rules.
The expansions imply a chain of variable bindings
going down the tree, and instantiation happens only
when rewriting into es but then propagates back up
the tree.
The probability P(w, t) of the resulting tree t and
terminal string w is the product Hr 0r over the se-
quence of rewrite rules used. This generative proce-
dure is a conceptual device; in practice, one would
care about parsing some input string under this prob-
abilistic grammar.
</bodyText>
<subsectionHeader confidence="0.908817">
4.2 PYSRCAG
</subsectionHeader>
<bodyText confidence="0.999460902439024">
A central property of the generative procedure un-
derlying probabilistic SRCGs is the fact that each
expansion happens independently, both of the other
expansions in the tree under construction and of any
other trees. To some extent, this flies in the face of
the reality of estimating a grammar from text, where
one would expect certain sub-trees to be used repeat-
edly across different input strings.
Adaptor grammars weaken this independence as-
sumption by allowing whole subtrees to be reused
during expansion. Informally, they act as a cache of
tree fragments whose tendency to be reused during
expansion is governed by the choice of adaptor func-
tion. Following earlier applications of adaptor gram-
mars (Johnson et al., 2007; Huang et al., 2011), we
employ the Pitman-Yor process (Pitman, 1995; Pit-
man and Yor, 1997) as adaptor function.
A Pitman-Yor Simple Range Concatenat-
ing Adaptor Grammar (PYSRCAG) is a tuple
G = (GS, M, a, b, α), where GS is a probabilistic
SRCG as defined before and M C_ N is a set
of adapted non-terminals. The vectors a and b,
indexed by the elements of M, are the discount
and concentration parameters for each adapted non-
terminal, with a E [0, 1], b &gt; 0. α are parameters to
Dirichlet priors on the rule probabilities 0.
PYSRCAG defines a generative process over a set
of trees T. Unadapted non-terminals A&apos; E N \ M
are expanded as before (�4.1). For each adapted
non-terminal A E M, a cache CA is maintained
for storing the terminating tree fragments expanded
from A earlier in the process, and we denote the
fragment corresponding to the i-th expansion of A
as zi. In other words, the sequence of indices zi
is the assignment of a sequence of expansions of
A to particular tree fragments. Given a cache CA
that has n previously generated trees comprising
m unique trees each used n1, ... , nm times (where
n = Ek nk), the tree fragment for the next expan-
sion of A, zn+1, is sampled conditional on the pre-
vious assignments z&lt; according to
</bodyText>
<equation confidence="0.971799">
�n��a
n+b if zn+1 = k E [1, m]
zn+1|z&lt; �ma+b
n+b if zn+1 = m + 1,
</equation>
<bodyText confidence="0.999975166666667">
where a and b are those elements of a and b cor-
responding to A. The first case denotes the situa-
tion where a previously cached tree is reused for this
n + 1-th expansion of A; to be clear, this expands
A with a fully terminating tree fragment, meaning
that none of the nodes descending from A in the
</bodyText>
<page confidence="0.688232">
348
</page>
<bodyText confidence="0.999822684210526">
tree being generated are subject to further expan-
sion. The second case by-passes the cache and ex-
pands A according to the rules PA and rule probabil-
ities BA of the underlying SRCG GS. Other caches
CB(B ∈ M) may come into play during those
expansions of the descendants of A; thus a PYS-
RCAG can define a hierarchical stochastic process.
Both cases eventually result in a terminating tree-
fragment for A, which is then added to the cache,
updating the counts n, nz,z+1 and potentially m.
The adaptation does not affect the string language
of GS, but it maps the distribution over trees to one
that is distributed according to the PYP.
The invariance of SRCGs trees under isomor-
phism would make the probabilistic model deficient,
but we side-step this issue by requiring that grammar
rules are specified in a canonical way that ensures
a one-to-one correspondence between the order of
nodes in a tree and of terminals in the yield.
</bodyText>
<subsectionHeader confidence="0.988632">
4.3 Inference under PYSRCAG
</subsectionHeader>
<bodyText confidence="0.9891689">
The inference procedure under our model is very
similar to that of CFG PY-adaptor grammars, so we
restate the central aspects here but refer the reader
to the original article by Johnson et al. (2007) for
further details. First, one may integrate out the
adaptors to obtain a single distribution over the set
of trees generated from a particular non-terminal.
Thus, the joint probability of a particular sequence z
for the adapted non-terminal A with cached counts
(n1, ... , nm) is
</bodyText>
<equation confidence="0.999274">
1m�=1 (a(k − 1) + b) 1�k�1
�=1 (j − a)
1��1
�=0 (i + b)
</equation>
<bodyText confidence="0.965505666666667">
Taking all the adapted non-terminals into account,
the joint probability of a set of full trees T under the
grammar G is
</bodyText>
<equation confidence="0.97568275">
P(T|a,b,�) _
AEM
rl B(aA + fA) PY (z(T) |a, b),
B(aA)
</equation>
<bodyText confidence="0.96583484">
where fA is a vector of the usage counts of rules
r ∈ PA across T, and B is the Euler beta function.
The posterior distribution over a set of strings
w is obtained by marginalising (7) over all trees
that have w as their yields. This is intractable to
compute directly, so instead we use MCMC tech-
niques to obtain samples from that posterior using a
component-wise Metropolis-Hastings sampler. The
sampler works by visiting each string w in turn and
drawing a new tree for it under a proposal grammar
GQ and randomly accepting that as the new analysis
for w according to the Metropolis-Hastings accept-
reject probability. As proposal grammar, we use the
analogous approximation of our G as Johnson et al.
used for PCFGs, namely by taking a static snapshot
GQ of the adaptor grammar where additional rules
rewrite adapted non-terminals as the terminal strings
of their cached trees. Drawing a sample from the
proposal distribution is then a matter of drawing a
random tree from the parse chart of w under GQ.
Lastly, the adaptor hyperparameters a and b
are modelled by placing flat Beta(1,1) and vague
Gamma(10, 0.1) priors on them, respectively, and
inferring their values using slice sampling (Johnson
and Goldwater, 2009).
</bodyText>
<sectionHeader confidence="0.759486" genericHeader="method">
5 Modelling root-templatic morphology
</sectionHeader>
<bodyText confidence="0.999753666666667">
We start with a CFG-based adaptor grammar4 that
models words as a stem and any number of prefixes
and suffixes:
</bodyText>
<equation confidence="0.987459">
Word → Pre* Stem Suf* (8)
Pre  |Stem  |Suf → Char+ (9)
</equation>
<bodyText confidence="0.9983613">
This fragment can be seen as building on the stem-
and-affix adaptor grammar presented in (Johnson et
al., 2007) for morphological analysis of English, of
which a later version also covers multiple affixes
(Sirts and Goldwater, 2013). In the particular case of
Arabic, multiple affixes are required to handle the at-
.tachment of particles and proclitics onto base words.
To extend this to complex stems consisting of a
root with three radicals we have rules like the fol-
lowing:
</bodyText>
<equation confidence="0.6784074">
Stem(abcdefg) → R3(b, d, e) T4(a, c, e, g)
Stem(abcdef) → R3(a, c, e) T3(b, d, f)
Stem(abcde) → R3(a, c, e) T2(b, d)
Stem(abcd) → R3(a, c, d) T1(b)
Stem(abc) → R3(a, b, c)
</equation>
<bodyText confidence="0.856364428571428">
4Adapted non-terminals are indicated by underlining and
we use the following abbreviations: X → Y+ means one
or more instances of Y and encodes the rules X → Ys and
Ys → Ys Y  |Y. Similarly, X → Y* Z allows zero or more
instances of Y and encodes the rules X → Z and X → Y+ Z.
Further relabelling is added as necessary to avoid cycles among
adapted non-terminals.
</bodyText>
<equation confidence="0.5269665">
PY (z|a, b) _
349
</equation>
<bodyText confidence="0.999944105263158">
The actual rules include certain permutations of
these, e.g. rule (13) has a variant R3(a, b, d)T1(c).
In unvocalised text, the standard written form of
Modern Standard Arabic (MSA), it may happen that
the stem and the root of a word form are one and the
same. So while rule (14) may look trivial, it ensures
that in such cases the radicals are still captured as de-
scendants of the non-terminal category R3, thereby
making their appearance in the cache.
A discontiguous non-terminal An is rewritten
through recursion on its arity down to 1, i.e.
An(v1, ... , v,,,) —* Al(v1, ... , v,,,�1) Char(v,,,) with
base case A1(v) —* Char(v), where Char rewrites
all individual terminals as c, vi are variables and
l = n−1.5 Note that although we provide the model
with two sets of discontiguous non-terminals R and
T, we do not specify their mapping onto the actual
terminal strings; no subdivision of the alphabet into
vowels and consonants is hard-wired.
</bodyText>
<sectionHeader confidence="0.999309" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9999917">
We evaluate our model on standard Arabic, Quranic
Arabic and Hebrew in terms of segmentation quality
and lexicon induction ability. These languages share
various properties, including morphology and lexi-
cal cognates, but are sufficiently different so as to
require manual intervention when transferring rule-
based morphological analysers across languages. A
key question in this evaluation is therefore whether
an appropriate instantiation of our model success-
fully generalises across related languages.
</bodyText>
<subsectionHeader confidence="0.997038">
6.1 Data sets
</subsectionHeader>
<bodyText confidence="0.999942875">
Our models are unsupervised and therefore learn
from raw text, but their evaluation requires anno-
tated data as a gold-standard and these were derived6
as follows:
Arabic (MSA) We created the dataset BW by syn-
thesising 50k morphotactically correct word types
from the morpheme lexicons and consistency rules
supplied with the Buckwalter Arabic Morphological
</bodyText>
<footnote confidence="0.526420833333333">
5Including the arity as part of the non-terminal symbol
names forms part of our convention here to ensure that the
grammar contains no cycles, a situation which would compli-
cate inference under PYSRCAG.
6Our data preprocessing scripts are obtainable from
http://github.com/bothameister/pysrcag-data.
</footnote>
<table confidence="0.9992106">
Types Stems Roots m/w c/w
BW 48428 24197 4717 2.3 6.4
BW&apos; 48428 30891 4707 2.3 10.7
QU&apos; 18808 12021 1270 1.9 9.9
HEB 5231 3164 492 2.1 6.7
</table>
<tableCaption confidence="0.999061">
Table 1: Corpus statistics, including average number
</tableCaption>
<bodyText confidence="0.981724272727272">
of morphemes (m/w) and characters (c/w) per word,
and total surface-realised roots of length 3 or 4.
Analyser (BAMA).7 This allowed control over the
word shapes, which is important to focus the evalu-
ation, while yielding reliable segmentation and root
annotations. BW has no vocalisation; we denote the
&apos;
corresponding vocalised dataset as BW.
Quranic Arabic We extracted the roughly 18k
word types from a morphologically analysed version
of the Quran (Dukes and Habash, 2010). As an ad-
ditional challenge, we left all given diacritics intact
&apos;
for this dataset, QU.
Hebrew We leveraged the Hebrew CHILDES
database as an annotated resource (Albert et al.,
2013) and were able to extract 5k word types that
feature at least one affix to use as dataset HEB. The
corrected versions of words marked as non-standard
child language were used, diacritics were dropped,
and we conflated stressed and unstressed vowels to
overcome inconsistencies in the source data.
</bodyText>
<subsectionHeader confidence="0.998271">
6.2 Models
</subsectionHeader>
<bodyText confidence="0.999766105263158">
We consider two classes of models. The first
is the strictly context-free adaptor grammar for
morphemes as sequences of characters using
rules (8)-(9), which we denote as Coneat and
MConeat, where the latter allows multiple pre-
fixes/suffixes in a word. These serve as baselines for
the second class in which non-concatenative rules
are added. MTpl and Tpl denote the canonical ver-
7We used version 2.0, LDC2004L02, and sampled word
types having a single stem and at most one prefix, suffix or both,
according to the following random procedure: Sample a shape
(stem: 0.1, pre+stem: 0.25 stem+suf: 0.25, pre+stem+suf: 0.4).
Sample uniformly at random (with replacement) a stem from
the BAMA stem lexicon, and affix(es) from the ones consis-
tent with the chosen stem. The BAMA lexicons contain affixes
and their legitimate concatenations, so some of the generated
words would permit a linguistic segmentation into multiple pre-
fixes/suffixes. Nonetheless, we take as gold-standard segmenta-
tion precisely the items used by our procedure.
</bodyText>
<page confidence="0.669356">
350
</page>
<bodyText confidence="0.999958142857143">
sions with stems as shown in the set of rules above,
and we experiment with a variant Tpl3Ch that al-
lows the non-terminal T1 to be rewritten as up to
three Char symbols, since the data indicate there are
cases where multiple characters intervene between
the radicals of a root.
These models exclude rule (10), which we include
only in the variant Tpl+T4. Lastly, TplR4 is the ex-
tension of Tpl+T4 to include a stem-forming rule
that uses R4.
As external baseline model we used Morfessor
(Creutz and Lagus, 2007), which performs decently
in morphological segmentation of a variety of lan-
guages, but only handles concatenation.
</bodyText>
<subsectionHeader confidence="0.998708">
6.3 Method
</subsectionHeader>
<bodyText confidence="0.99999128">
The MCMC samplers converged within a few hun-
dred iterations and we collected 100 posterior sam-
ples after 900 iterations of burn-in. Collected sam-
ples, each of which is a set of parse trees of the input
word types, are used in two ways:
First, by averaging over the samples we can es-
timate the joint probability of a word type w and a
parse tree t under the adaptor grammar, conditional
on the data and the model’s hyperparameters. We
take the most probable parse of each word type and
evaluate the implied segmentation against the gold
standard segmentation. Likewise, we evaluate the
implied lexicon of stems, affixes and roots against
the corresponding reference sets. It should be em-
phasised that using this maximally probable analy-
sis is aimed at simplifying the evaluation set-up; one
could also extract multiple analyses of a word since
the model defines a distribution over them.
The second method abstracts away from individ-
ual word-types and instead averages over the union
of all samples to obtain an estimate of the probabil-
ity of a string s being generated by a certain category
(non-terminal) of the grammar. In this way we can
obtain a lexicon of the morphemes in each category,
ranked by their probability under the model.
</bodyText>
<subsectionHeader confidence="0.993432">
6.4 Inducing Morpheme Lexicons
</subsectionHeader>
<bodyText confidence="0.9955776">
The quality of each induced lexicon is measured
with standard set-based precision and recall with re-
spect to the corresponding gold lexicon. The results
are summarised by balanced F-scores in Table 2.
The main result is that all our models capable of
forming complex stems obtain a marked improve-
ment in F-scores over the baseline concatenative
adaptor grammar, and the margin of improvement
grows along with the expressivity of the complex-
stem models tested. This applies across prefix, stem
and suffix categories and across our datasets, with
&apos;
the exception of QU, which we elaborate on in §6.5.
Stem lexicons of Arabic were learnt with rel-
atively constant precision (-70%), but modelling
complex stems broadened the coverage by about
3000 stems over the concatenative model (against a
reference set of 24k stems). On vocalised Arabic,
the improvements for stems are along both dimen-
sions. In contrast, affix lexicons for both BW and
&apos;
BWare noisy and the models all generate greedily
to obtain near perfect recall but low precision.
On our Hebrew data, which comprises only 5k
words, the gains in lexicon quality from modelling
complex stems tend to be larger than on Arabic. This
is consistent with our intuition that an appropriate,
richer Bayesian prior helps overcome data sparsity.
Extracting a lexicon of roots is rendered challeng-
ing by the unsupervised nature of the model as the
labelling of grammar symbols is ultimately arbitrary.
Our simple approach was to regard a character tuple
parsed under category R3 as a root. This had mixed
success, as demonstrated by the outlier scores in Ta-
ble 2. In the one case where it was obvious that T3
had been been co-opted for the role, we report the
F-score obtained on the union of R3 and T3 strings.
Soft decisions The preceding set-based evaluation
imposes hard decisions about category membership.
But adaptor grammars are probabilistic by definition
and should thus also be evaluated in terms of prob-
abilistic ability. One method is to turn the model
predictions into a binary classifier of strings us-
ing Receiver-Operator-Characteristic (ROC) theory.
We plot the true positive rate versus the false pos-
itive rate for each prediction lexicon LT containing
strings that have probability greater than z under the
model (for a grammar category of interest). A per-
fect classifier would rank all true positives (e.g. stem
strings) above false positives (e.g. non-stem strings),
corresponding to a curve in the upper left corner of
the ROC plot. A random guesser would trace a di-
agonal line. The area under the curves (AUC) is
the probability that the classifier would discriminate
correctly.
</bodyText>
<page confidence="0.572085">
351
</page>
<table confidence="0.999234714285714">
Vocalised Arabic (BW�) Unvocalised Arabic (BW) Pre Hebrew (HEB) R3
Pre Stem Suf R3 Pre Stem Suf R3 Stem Suf
Concat 15.0 20.2 25.4 - 32.8 44.1 40.3 - 18.7 20.9 29.2 -
Tpl 24.7 39.4 35.2 142.4 45.9 54.7 47.9 62.7 35.1 59.6 52.9 34.8
Tpl3Ch 28.4 36.0 36.5 5.2 50.3 55.1 48.5 62.4 38.6 61.5 56.6 7.1
Tpl+T4 29.0 44.8 41.0 3.9 46.2 54.2 47.7 62.3 32.5 59.6 53.0 36.4
TplR4 37.8 60.3 47.0 5.2 53.0 57.7 51.9 62.4 38.0 62.4 55.2 34.7
</table>
<tableCaption confidence="0.578082333333333">
Table 2: Morpheme lexicon induction quality. F1-scores for lexicons induced from the most probable parse
of each different dataset under each models. 142.4 was obtained by taking the union of R3 and T3 items to
match the way the model used them (see §6.4).
</tableCaption>
<table confidence="0.987726">
BW� BW QU� HEB
Morfessor 55.57 40.04 44.34 24.20
Concat 47.36 64.22 19.64 60.05
Tpl 60.42 71.91 22.53 77.26
Tpl3Ch 60.52 72.20 25.72 77.41
Tpl+T4 64.49 71.59 24.81 77.14
TplR4 74.54 73.66 - 78.14
0
</table>
<tableCaption confidence="0.794054">
Table 3: Segmentation quality in SBF1. The QU
results are for the corresponding M* models .
</tableCaption>
<bodyText confidence="0.999712">
Our models with complex stem formation im-
prove over the baseline on the AUC metric too. We
include the ROC plots for Hebrew stem and root in-
duction in Figure 2, along with the roots the model
was most confident about (Table 4).
</bodyText>
<subsectionHeader confidence="0.999169">
6.5 Morphological Analysis per Word Type
</subsectionHeader>
<bodyText confidence="0.999847230769231">
In this section we turn to the analyses our models
assign to each word type. Two aspects of interest are
the segmentation into sequential morphemes and the
identification of the root.
Our intercalating adaptor grammars consistently
obtain large gains in segmentation accuracy over the
baseline concatenative model, across all our datasets
(Table 3). We measure segmentation quality as seg-
ment border F1-score (SBF) (Sirts and Goldwater,
2013), which is the F-score over word-internal seg-
mentation points of the predicted analysis with re-
spect to the gold segmentation.
Of the two MSA datasets, the vocalised version
</bodyText>
<equation confidence="0.512279">
0
</equation>
<bodyText confidence="0.999950314285714">
BWpresents a more difficult segmentation task as
its words are on average longer and feature 31k
unique contiguous morphemes, compared to the 24k
in BW for the same number of words. It should thus
benefit more from additional model expressivity, as
is reflected in the increase of 10 SBF when adding
the TplR4 rule to the other triliteral ones.
The best triliteral root identification accuracy (on
a per-word basis) was found for HEB (74%) and BW
(67%).8,9 Refer to Figure 3 for example analyses.
An interesting aspect of these results is that tem-
platic rules may aid segmentation quality without
necessarily giving perfect root identification. Mod-
elling stem substructure allows any regularities that
give rise to a higher data likelihood to be picked up.
The low performance on the Quran demands fur-
ther explanation. All our adaptor grammars severely
oversegmented this data, although the mistakes were
not uniformly distributed. Most of the performance
loss is on the 79% of words that have 1-2 mor-
phemes. On the remaining words (having 3-5 mor-
phemes), our models recover and approach the Mor-
fessor baseline (MConcat: 32.7 , MTpl3Ch: 38.6).
Preliminary experiments on BW had indicated
that adaptation of (single) affix categories is crucial
for good performance. Our multi-affixing models
used on QU� lacked a further level of adaptation for
composite affixes, which we suspect as a contribut-
ing factor to the lower performance on that dataset.
This remains to be confirmed in future experiments,
but would be consistent with other observations on
the role of hierarchical adaptation in adaptor gram-
mars (Sirts and Goldwater, 2013). The trend that
intercalated rules improve segmentation (compared
to the concatenative grammar) remains consistent
</bodyText>
<tableCaption confidence="0.865811857142857">
8When excluding cases where root equals stem, root identi-
fication on BW is 55%. Those cases are still not trivial, since
words without roots also exist.
9By way of comparison, Rodrigues and ´Cavar (2007)
presented an unsupervised statistics-based root identification
method that obtained precision ranging between 50-75%, the
higher requiring vocalised words.
</tableCaption>
<figure confidence="0.994955236842105">
352
(a) Stems
(b) Triliteral roots
True Positive Rate
0.4
0.9
0.8
0.7
0.6
0.5
0.3
0.1
0.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
False Positive Rate
0.2
1.0
TplR4 (0.84)
Tpl+T4 (0.83)
Concat (0.63)
random (0.5)
True Positive Rate
0.4
0.9
0.8
0.7
0.6
0.5
0.3
0.1
0.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
False Positive Rate
0.2
1.0
TplR4 (0.77)
Tpl+T4 (0.77)
random (0.5)
</figure>
<figureCaption confidence="0.952683">
Figure 2: ROC curves for predicting the stem and root lexicons for the HEB dataset. The area under each
curve (AUC), as computed with the trapezium rule, is given in parentheses.
</figureCaption>
<bodyText confidence="0.994030733333333">
across datasets, despite the lower absolute perfor-
mance on QU&apos;.
The performance of the Morfessor baseline was
quite mixed. Contrary to our expectations, it per-
forms best on the “harder” BW&apos;, worst on the ar-
guably simpler HEB and struggled less than the
&apos;
adaptor grammars on QU.
One factor here is that it learns according to
a grammar with multiple consecutive affixes and
stems, whereas all our experiments (except on QU&apos;)
presupposed single affixes. This biases the evalua-
tion slightly in our favour, but works in Morfessor’s
favour on the QU&apos; data which is annotated with mul-
tiple affixes.
</bodyText>
<sectionHeader confidence="0.999793" genericHeader="conclusions">
7 Related work
</sectionHeader>
<bodyText confidence="0.999916583333333">
The distinctive feature of our morphological model
is that it jointly addresses root identification and
morpheme segmentation, and our results demon-
strate the mutual benefit of this.
In contrast, earlier unsupervised approaches tend
to focus on these tasks in isolation.
In unsupervised Arabic segmentation, the para-
metric Bayesian model of (Lee et al., 2011) achieves
F1-scores in the high eighties by incorporating sen-
tential context and inferred syntactic categories,
both of which our model forgoes, although theirs has
no account of discontiguous root morphemes.
</bodyText>
<figure confidence="0.970337625">
Root Example instances
1. spr ✓ G ˇsaˇpaˇr.ti te.ˇsaˇpˇr ye.ˇsaˇpˇr.u
B sipur.im hixˇstaˇpxaˇr t
2. lbs ✓ G ˇlaˇbaˇS.t li.ˇlˇboSˇ ti.ˇlˇbeˇS.i
B le haxˇlˇbiSˇ ti txˇlabˇˇS.i
3. ptx ✓ G ˇpaˇtaˇx.ti ti.ˇpˇteˇx.i
B li.ˇpˇtoaˇx nix ˇpˇtaˇx.at
5. !al x B ya.!al.u ˇmax ˇ!aˇl.a ˇ!aˇcˇlxan it
</figure>
<tableCaption confidence="0.712363">
Table 4: Top Hebrew roots hypothesised by Tpl+T4.
Numbers indicate position when ranked by model
</tableCaption>
<bodyText confidence="0.99888175">
probability. (G)ood and (B)ad instances from
the corpus are given with morpheme boundaries
marked: true positive (.), false negative ( ) and false
positive (x). Hypothesised root characters are bold-
faced, while accent (ˇ) marks gold root characters.
Previous approaches to Arabic root identifica-
tion that sought to use little supervision typically
constrain the search space of candidate characters
within a word, leveraging pre-existing dictionar-
ies (Darwish, 2002; Boudlal et al., 2009) or rule
constraints (Elghamry, 2005; Rodrigues and ´Cavar,
2007; Daya et al., 2008).
In contrast to these approaches, our model re-
quires no dictionary, and while our grammar rules
effect some constraints on what could be a root, they
are specified in a convenient and flexible manner that
</bodyText>
<figure confidence="0.87466425">
353
Word
Suf
k
</figure>
<page confidence="0.520975">
m
354
</page>
<sectionHeader confidence="0.956903" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99982083018868">
Aviad Albert, Brian MacWhinney, Bracha Nir, and Shuly
Wintner. 2013. The Hebrew CHILDES corpus: tran-
scription and morphological analysis. Language Re-
sources and Evaluation, pages 1–33.
Mohamed Altantawy, Nizar Habash, Owen Rambow, and
Ibrahim Saleh. 2010. Morphological Analysis and
Generation of Arabic Nouns: A Morphemic Func-
tional Approach. In Proceedings of LREC, pages 851–
858.
Kenneth R Beesley and Lauri Karttunen. 2003. Fi-
nite state morphology, volume 18. CSLI publications
Stanford.
Abderrahim Boudlal, Rachid Belahbib, Abdelhak
Lakhouaja, Azzeddine Mazroui, Abdelouafi Meziane,
and Mohamed Bebah. 2009. A Markovian approach
for Arabic Root Extraction. The International Arab
Journal of Information Technology, 8(1):91–98.
Pierre Boullier. 2000. A cubic time extension of context-
free grammars. Grammars, 3(2-3):111–131.
Tim Buckwalter. 2002. Arabic Morphological Ana-
lyzer. Technical report, Linguistic Data Consortium,
Philedelphia.
Alexander Clark. 2001. Learning Morphology with Pair
Hidden Markov Models. In Proceedings of the ACL
Student Workshop, pages 55–60.
Yael Cohen-Sygal and Shuly Wintner. 2006. Finite-
state registered automata for non-concatenative mor-
phology. Computational Linguistics, 32(1):49–82.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1):1–34.
Kareem Darwish. 2002. Building a shallow Arabic
morphological analyzer in one day. In Proceedings
of the ACL Workshop on Computational Approaches
to Semitic Languages, pages 47–54. Association for
Computational Linguistics.
Ezra Daya, Dan Roth, and Shuly Wintner. 2008.
Identifying Semitic Roots: Machine Learning with
Linguistic Constraints. Computational Linguistics,
34(3):429–448.
Markus Dreyer and Jason Eisner. 2011. Discovering
Morphological Paradigms from Plain Text Using a
Dirichlet Process Mixture Model. In Proceedings of
EMNLP, pages 616–627, Edinburgh, Scotland.
Kais Dukes and Nizar Habash. 2010. Morphological An-
notation of Quranic Arabic. In Proceedings of LREC.
Greg Durrett and John DeNero. 2013. Supervised Learn-
ing of Complete Morphological Paradigms. In Pro-
ceedings of NAACL-HLT, pages 1185–1195, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Khaled Elghamry. 2005. A Constraint-based Algorithm
for the Identification of Arabic Roots. In Proceed-
ings of the Midwest Computational Linguistics Collo-
quium. Indiana University. Bloomington, IN.
Raphael Finkel and Gregory Stump. 2002. Generating
Hebrew verb morphology by default inheritance hier-
archies. In Proceedings of the ACL Workshop on Com-
putational Approaches to Semitic Languages. Associ-
ation for Computational Linguistics.
Michelle A. Fullwood and Timothy J. O’Donnell. 2013.
Learning non-concatenative morphology. In Proceed-
ings of the Workshop on Cognitive Modeling and Com-
putational Linguistics, pages 21–27, Sofia, Bulgaria.
Association for Computational Linguistics.
Michael Gasser. 2009. Semitic morphological analysis
and generation using finite state transducers with fea-
ture structures. In Proceedings of EACL, pages 309–
317. Association for Computational Linguistics.
Daniel Gildea. 2010. Optimal Parsing Strategies for Lin-
ear Context-Free Rewriting Systems. In Proceedings
of NAACL, pages 769–776. Association for Computa-
tional Linguistics.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Interpolating Between Types and Tokens
by Estimating Power-Law Generators. In Advances in
Neural Information Processing Systems, Volume 18.
Harald Hammarstr¨om and Lars Borin. 2011. Unsuper-
vised Learning of Morphology. Computational Lin-
guistics, 37(2):309–350.
Yun Huang, Min Zhang, and Chew Lim Tan. 2011.
Nonparametric Bayesian Machine Transliteration with
Synchronous Adaptor Grammars. In Proceedings of
ACL (Short papers), pages 534–539.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric Bayesian inference: Experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of NAACL-HLT, pages 317–325.
Association for Computational Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Adaptor Grammars: A Framework for
Specifying Compositional Nonparametric Bayesian
Models. In Advances in Neural Information Process-
ing Systems, volume 19, page 641. MIT.
Mark Johnson. 2008. Unsupervised word segmentation
for Sesotho using Adaptor Grammars. In Proceedings
ofACL Special Interest Group on Computational Mor-
phology and Phonology (SigMorPhon), pages 20–27.
Association for Computational Linguistics.
Aravind K. Joshi. 1985. Tree adjoining grammars: How
much context-sensitivity is required to provide reason-
able structural descriptions? In D.R. Dowty, L. Kart-
tunen, and A.M. Zwicky, editors, Natural Language
Parsing, chapter 6, pages 206–250. Cambridge Uni-
versity Press.
</reference>
<page confidence="0.615769">
355
</page>
<reference confidence="0.999902461538462">
Miriam Kaeshammer. 2013. Synchronous Linear
Context-Free Rewriting Systems for Machine Trans-
lation. In Proceedings of the Workshop on Syntax, Se-
mantics and Structure in Statistical Translation, pages
68–77, Atlanta, Georgia. Association for Computa-
tional Linguistics.
Yuki Kato, Hiroyuki Seki, and Tadao Kasami. 2006.
Stochastic Multiple Context-Free Grammar for RNA
Pseudoknot Modeling. In Proceedings of the Inter-
national Workshop on Tree Adjoining Grammar and
Related Formalisms, pages 57–64.
George Anton Kiraz. 2000. Multitiered Nonlinear Mor-
phology Using Multitape Finite Automata: A Case
Study on Syriac and Arabic. Computational Linguis-
tics, 26(1):77–105, March.
Kimmo Koskenniemi. 1984. A general computational
model for word-form recognition and production. In
Proceedings of the 10th international conference on
Computational Linguistics, pages 178–181. Associa-
tion for Computational Linguistics.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2010.
Overview and Results of Morpho Challenge 2009. In
Multilingual Information Access Evaluation I. TextRe-
trieval Experiments, volume 6241 of Lecture Notes in
Computer Science, pages 578–597. Springer Berlin /
Heidelberg.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2011. Modeling syntactic context improves morpho-
logical segmentation. In Proceedings of CoNLL.
Wolfgang Maier. 2010. Direct Parsing of Discon-
tinuous Constituents in German. In Proceedings of
the NAACL-HLT Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 58–66. Asso-
ciation for Computational Linguistics.
Jim Pitman and Marc Yor. 1997. The Two-Parameter
Poisson-Dirichlet Distribution Derived from a Stable
Subordinator. The Annals of Probability, 25(2):855–
900.
Jim Pitman. 1995. Exchangeable and partially exchange-
able random partitions. Probability Theory and Re-
lated Fields, 102:145–158.
Jean-Franc¸ois Prunet. 2006. External Evidence and the
Semitic Root. Morphology, 16(1):41–67.
Paul Rodrigues and Damir ´Cavar. 2007. Learning Arabic
Morphology Using Statistical Constraint-Satisfaction
Models. In Elabbas Benmamoun, editor, Perspectives
on Arabic Linguistics: Proceedings of the 19th Ara-
bic Linguistics Symposium, pages 63–75, Urbana, IL,
USA. John Benjamins Publishing Company.
Nathan Schneider. 2010. Computational Cognitive
Morphosemantics: Modeling Morphological Compo-
sitionality in Hebrew Verbs with Embodied Construc-
tion Grammar. In Proceedings of the Annual Meeting
of the Berkeley Linguistics Society, Berkeley, CA.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context-free gram-
mars. Theoretical Computer Science, 88(2):191–229.
Kairit Sirts and Sharon Goldwater. 2013. Minimally-
Supervised Morphological Segmentation using Adap-
tor Grammars. Transactions of the ACL.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions produced
by various grammatical formalisms. In Proceedings of
ACL, pages 104–111.
</reference>
<page confidence="0.943247">
356
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.703102">
<title confidence="0.997288">Adaptor Grammars for Learning Non-Concatenative Morphology</title>
<author confidence="0.999888">A Botha</author>
<affiliation confidence="0.998262">Department of Computer University of</affiliation>
<address confidence="0.719649">Oxford, OX1 3QD,</address>
<abstract confidence="0.999129739130435">This paper contributes an approach for expressing non-concatenative morphological phenomena, such as stem derivation in Semitic languages, in terms of a mildly context-sensitive grammar formalism. This offers a convenient level of modelling abstraction while remaining computationally tractable. The nonparametric Bayesian framework of adaptor grammars is extended to this richer grammar formalism to propose a probabilistic model that can learn word segmentation and morpheme lexicons, including ones with discontiguous strings as elements, from unannotated data. Our experiments on Hebrew and three variants of Arabic data find that the additional expressiveness to capture roots and templates as atomic units improves the quality of concatenative segmentation and stem identification. We obtain 74% accuracy in identifying triliteral Hebrew roots, while performing morphological segmentation with an F1-score of 78.1.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aviad Albert</author>
<author>Brian MacWhinney</author>
<author>Bracha Nir</author>
<author>Shuly Wintner</author>
</authors>
<title>The Hebrew CHILDES corpus: transcription and morphological analysis. Language Resources and Evaluation,</title>
<date>2013</date>
<pages>1--33</pages>
<contexts>
<context position="23495" citStr="Albert et al., 2013" startWordPosition="3887" endWordPosition="3890">per word, and total surface-realised roots of length 3 or 4. Analyser (BAMA).7 This allowed control over the word shapes, which is important to focus the evaluation, while yielding reliable segmentation and root annotations. BW has no vocalisation; we denote the &apos; corresponding vocalised dataset as BW. Quranic Arabic We extracted the roughly 18k word types from a morphologically analysed version of the Quran (Dukes and Habash, 2010). As an additional challenge, we left all given diacritics intact &apos; for this dataset, QU. Hebrew We leveraged the Hebrew CHILDES database as an annotated resource (Albert et al., 2013) and were able to extract 5k word types that feature at least one affix to use as dataset HEB. The corrected versions of words marked as non-standard child language were used, diacritics were dropped, and we conflated stressed and unstressed vowels to overcome inconsistencies in the source data. 6.2 Models We consider two classes of models. The first is the strictly context-free adaptor grammar for morphemes as sequences of characters using rules (8)-(9), which we denote as Coneat and MConeat, where the latter allows multiple prefixes/suffixes in a word. These serve as baselines for the second</context>
</contexts>
<marker>Albert, MacWhinney, Nir, Wintner, 2013</marker>
<rawString>Aviad Albert, Brian MacWhinney, Bracha Nir, and Shuly Wintner. 2013. The Hebrew CHILDES corpus: transcription and morphological analysis. Language Resources and Evaluation, pages 1–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Altantawy</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Ibrahim Saleh</author>
</authors>
<title>Morphological Analysis and Generation of Arabic Nouns: A Morphemic Functional Approach.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>851--858</pages>
<contexts>
<context position="2334" citStr="Altantawy et al., 2010" startWordPosition="337" endWordPosition="340">c morphology, such as Arabic, Hebrew and Amharic. These Semitic languages derive verb and noun stems by interspersing abstract root morphemes into templatic structures in a nonconcatenative way. For example, the Arabic root k·t·b can combine with the template (i-a) to derive the noun stem kitab (book). Established morphological analysers typically ignore this process and simply view the derived stems as elementary units (Buckwalter, 2002), or their account of it coincides with a requirement for extensive linguistic knowledge and hand-crafting of rules (Finkel and Stump, 2002; Schneider, 2010; Altantawy et al., 2010). The former approach is bound to suffer from vocabulary coverage issues, while the latter clearly does not transfer easily across languages. The practical appeal of unsupervised learning of templatic morphology is that it can overcome these shortcomings. Unsupervised learning of concatenative morphology has received extensive attention, partly driven by the MorphoChallenge (Kurimo et al., 2010) in recent years, but that is not the case for root-templatic morphology (Hammarstr¨om and Borin, 2011). In this paper we present a model-based method that learns concatenative and root-templatic morpho</context>
</contexts>
<marker>Altantawy, Habash, Rambow, Saleh, 2010</marker>
<rawString>Mohamed Altantawy, Nizar Habash, Owen Rambow, and Ibrahim Saleh. 2010. Morphological Analysis and Generation of Arabic Nouns: A Morphemic Functional Approach. In Proceedings of LREC, pages 851– 858.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth R Beesley</author>
<author>Lauri Karttunen</author>
</authors>
<title>Finite state morphology, volume 18. CSLI publications Stanford.</title>
<date>2003</date>
<contexts>
<context position="4986" citStr="Beesley and Karttunen, 2003" startWordPosition="731" endWordPosition="734">published result in a particular language, but rather to create a method that is applicable across a variety of morphological processes. The specific rules used in our empirical evaluation on Arabic and Hebrew therefore contain hardly any explicit linguistic knowledge about the languages and are applicable across the family of Semitic languages. 2 A powerful grammar for morphology Concatenative morphology lends itself well to an analysis in terms of finite-state transducers (FSTs) (Koskenniemi, 1984). With some additional effort, FSTs can also encode non-concatenative morphology (Kiraz, 2000; Beesley and Karttunen, 2003; Cohen-Sygal and Wintner, 2006; Gasser, 2009). Despite this seeming adequacy of regular languages to describe morphology, we see two main shortcomings that motivate moving further up the Chomsky hierarchy of formal languages: first is the issue of learning. We are not aware of successful attempts at inducing FST-based morphological analysers in an unsupervised way, and believe the challenge lies in the fact that FSTs do not offer a convenient way of expressing prior linguistic intuitions to guide the learning process. Secondly, an FST composed of multiple machines might capture morphological </context>
</contexts>
<marker>Beesley, Karttunen, 2003</marker>
<rawString>Kenneth R Beesley and Lauri Karttunen. 2003. Finite state morphology, volume 18. CSLI publications Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abderrahim Boudlal</author>
</authors>
<title>Rachid Belahbib, Abdelhak Lakhouaja, Azzeddine Mazroui, Abdelouafi Meziane, and Mohamed Bebah.</title>
<date>2009</date>
<journal>Journal of Information Technology,</journal>
<volume>8</volume>
<issue>1</issue>
<marker>Boudlal, 2009</marker>
<rawString>Abderrahim Boudlal, Rachid Belahbib, Abdelhak Lakhouaja, Azzeddine Mazroui, Abdelouafi Meziane, and Mohamed Bebah. 2009. A Markovian approach for Arabic Root Extraction. The International Arab Journal of Information Technology, 8(1):91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Boullier</author>
</authors>
<title>A cubic time extension of contextfree grammars.</title>
<date>2000</date>
<journal>Grammars,</journal>
<pages>3--2</pages>
<contexts>
<context position="3104" citStr="Boullier, 2000" startWordPosition="454" endWordPosition="455"> appeal of unsupervised learning of templatic morphology is that it can overcome these shortcomings. Unsupervised learning of concatenative morphology has received extensive attention, partly driven by the MorphoChallenge (Kurimo et al., 2010) in recent years, but that is not the case for root-templatic morphology (Hammarstr¨om and Borin, 2011). In this paper we present a model-based method that learns concatenative and root-templatic morphology in a unified framework. We build on two disparate strands of work from the literature: Firstly, we apply simple Range Concatenating Grammars (SRCGs) (Boullier, 2000) to parse contiguous and discontiguous morphemes from an input string. These grammars are mildly-context sensitive (Joshi, 1985), a superset of context-free grammars that retains polynomial parsing time-complexity. Secondly, we generalise the nonparametric Bayesian learning framework of adaptor grammars (Johnson et al., 2007) to SRCGs.1 This should also be rel1Our formulation is in terms of SRCGs, which are equivalent in power to linear context-free rewrite systems (VijayShanker et al., 1987) and multiple context-free grammars (Seki et al., 1991), all of which are weaker than (non-simple) rang</context>
<context position="8149" citStr="Boullier, 2000" startWordPosition="1220" endWordPosition="1221">capture a discontiguous string like k·t·b as a single constituent in a parse tree. This leads to well-understood problems in probabilistic grammars (e.g. what is this rule’s probability?), but also corresponds to the linguistic consideration that k·t·b is a proper morpheme of the language (Prunet, 2006). 3 Simple range concatenating grammars In this section we define SRCGs formally and illustrate how they can be used to model nonconcatenative morphology. SRCGs define languages that are recognisable in polynomial time, yet can capture discontiguous elements of a string under a single category (Boullier, 2000). An SRCG346 rule operates on vectors of ranges in contrast to the way a CFG-rule operates on single ranges (spans). In other words, a non-terminal symbol in an SRCG (CFG) derivation can dominate a subset (substring) of terminals in an input string. 3.1 Formalism An SRCG G is a tuple (N, T, V, P, S), with finite sets of non-terminals (N), terminals (T) and variables (V ), with a start symbol S E N. A rewrite rule p E P of rank r = p(p) &gt; 0 h/&apos;as the form A(α1, ... , αψ(A)) B1(01,1,...,01,V,(B1)) ... Br(0r,1,...,0r,ψ(B�)), where each α, 0 E (T U V)*, and (A) is the number of arguments a non-ter</context>
<context position="10296" citStr="Boullier, 2000" startWordPosition="1607" endWordPosition="1608">of non-terminals on the right-hand side of an SRCG rule is irrelevant, i.e. A(ab) —* B(a)C(b) and A(ab) —* C(b)B(a) are the same rule.2 Consequently, the isomorphisms of any given SRCG derivation tree all encode the same string, which is uniquely defined through the instantiation process. 3.2 Application to morphological analysis A fragment of the idealised grammar schema from the previous section (§2) can be rephrased as an SRCG by writing the rules in the newly introduced 2Certain ordering restrictions over the variables within an argument need to hold for an SRCG to indeed be a simple RCG (Boullier, 2000). Word(wakitabi) Figure 1: Example derivation for wakitabi (and my book) using the SRCG fragment from §3.2. CFGs cannot capture such crossing branches. notation, and supplying a definition of the intercal function as simply another rule of the grammar, with instantiation for w = kitab shown below: Word(abc) —* Pre(a) Stem(b) Suf(c) Stem(abcde) —* Root(a, c, e) Template(b, d), Stem((0..1), (1..2), (2..3), (3..4), (4..5)) � Root((0..1), (2..3), (4..5)) Template((1..2), (3..4)) Given an appropriate set of grammar rules (as we present in §5), we can parse an input string to obtain a tree as shown </context>
</contexts>
<marker>Boullier, 2000</marker>
<rawString>Pierre Boullier. 2000. A cubic time extension of contextfree grammars. Grammars, 3(2-3):111–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Buckwalter</author>
</authors>
<title>Arabic Morphological Analyzer.</title>
<date>2002</date>
<tech>Technical report, Linguistic Data Consortium, Philedelphia.</tech>
<contexts>
<context position="2153" citStr="Buckwalter, 2002" startWordPosition="311" endWordPosition="312"> including machine translation, parsing and information retrieval. A particularly interesting version of the morphology learning problem comes from languages that use templatic morphology, such as Arabic, Hebrew and Amharic. These Semitic languages derive verb and noun stems by interspersing abstract root morphemes into templatic structures in a nonconcatenative way. For example, the Arabic root k·t·b can combine with the template (i-a) to derive the noun stem kitab (book). Established morphological analysers typically ignore this process and simply view the derived stems as elementary units (Buckwalter, 2002), or their account of it coincides with a requirement for extensive linguistic knowledge and hand-crafting of rules (Finkel and Stump, 2002; Schneider, 2010; Altantawy et al., 2010). The former approach is bound to suffer from vocabulary coverage issues, while the latter clearly does not transfer easily across languages. The practical appeal of unsupervised learning of templatic morphology is that it can overcome these shortcomings. Unsupervised learning of concatenative morphology has received extensive attention, partly driven by the MorphoChallenge (Kurimo et al., 2010) in recent years, but</context>
</contexts>
<marker>Buckwalter, 2002</marker>
<rawString>Tim Buckwalter. 2002. Arabic Morphological Analyzer. Technical report, Linguistic Data Consortium, Philedelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Learning Morphology with Pair Hidden Markov Models.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACL Student Workshop,</booktitle>
<pages>55--60</pages>
<marker>Clark, 2001</marker>
<rawString>Alexander Clark. 2001. Learning Morphology with Pair Hidden Markov Models. In Proceedings of the ACL Student Workshop, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yael Cohen-Sygal</author>
<author>Shuly Wintner</author>
</authors>
<title>Finitestate registered automata for non-concatenative morphology.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="5017" citStr="Cohen-Sygal and Wintner, 2006" startWordPosition="735" endWordPosition="738">lar language, but rather to create a method that is applicable across a variety of morphological processes. The specific rules used in our empirical evaluation on Arabic and Hebrew therefore contain hardly any explicit linguistic knowledge about the languages and are applicable across the family of Semitic languages. 2 A powerful grammar for morphology Concatenative morphology lends itself well to an analysis in terms of finite-state transducers (FSTs) (Koskenniemi, 1984). With some additional effort, FSTs can also encode non-concatenative morphology (Kiraz, 2000; Beesley and Karttunen, 2003; Cohen-Sygal and Wintner, 2006; Gasser, 2009). Despite this seeming adequacy of regular languages to describe morphology, we see two main shortcomings that motivate moving further up the Chomsky hierarchy of formal languages: first is the issue of learning. We are not aware of successful attempts at inducing FST-based morphological analysers in an unsupervised way, and believe the challenge lies in the fact that FSTs do not offer a convenient way of expressing prior linguistic intuitions to guide the learning process. Secondly, an FST composed of multiple machines might capture morphological processes well and excel at ana</context>
</contexts>
<marker>Cohen-Sygal, Wintner, 2006</marker>
<rawString>Yael Cohen-Sygal and Shuly Wintner. 2006. Finitestate registered automata for non-concatenative morphology. Computational Linguistics, 32(1):49–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised models for morpheme segmentation and morphology learning.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="7437" citStr="Creutz and Lagus, 2007" startWordPosition="1104" endWordPosition="1107">Tagalog sulat (write) ⇒ sumulat (wrote) Stem → circfix (Stem, Circumfix) (5) e.g. Indonesian percaya (to trust) ⇒ kepercayaan (belief) where the symbols (excluding Word and Stem) implicitly expand to the relevant terminal strings. The bold-faced “functions” combine the potentially discontiguous yields of the argument symbols into single contiguous strings, e.g. infix(s·ulat, um) produces stem sumulat. Taken by themselves, the first two rules are simply a CFG that describes word formation as the concatenation of stems and affixes, a formulation that matches the underlying grammar of Morfessor (Creutz and Lagus, 2007), a well-studied unsupervised model. The key aim of our extension is that we want the grammar to capture a discontiguous string like k·t·b as a single constituent in a parse tree. This leads to well-understood problems in probabilistic grammars (e.g. what is this rule’s probability?), but also corresponds to the linguistic consideration that k·t·b is a proper morpheme of the language (Prunet, 2006). 3 Simple range concatenating grammars In this section we define SRCGs formally and illustrate how they can be used to model nonconcatenative morphology. SRCGs define languages that are recognisable</context>
<context position="25348" citStr="Creutz and Lagus, 2007" startWordPosition="4189" endWordPosition="4192">ixes/suffixes. Nonetheless, we take as gold-standard segmentation precisely the items used by our procedure. 350 sions with stems as shown in the set of rules above, and we experiment with a variant Tpl3Ch that allows the non-terminal T1 to be rewritten as up to three Char symbols, since the data indicate there are cases where multiple characters intervene between the radicals of a root. These models exclude rule (10), which we include only in the variant Tpl+T4. Lastly, TplR4 is the extension of Tpl+T4 to include a stem-forming rule that uses R4. As external baseline model we used Morfessor (Creutz and Lagus, 2007), which performs decently in morphological segmentation of a variety of languages, but only handles concatenation. 6.3 Method The MCMC samplers converged within a few hundred iterations and we collected 100 posterior samples after 900 iterations of burn-in. Collected samples, each of which is a set of parse trees of the input word types, are used in two ways: First, by averaging over the samples we can estimate the joint probability of a word type w and a parse tree t under the adaptor grammar, conditional on the data and the model’s hyperparameters. We take the most probable parse of each wor</context>
</contexts>
<marker>Creutz, Lagus, 2007</marker>
<rawString>Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. ACM Transactions on Speech and Language Processing, 4(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kareem Darwish</author>
</authors>
<title>Building a shallow Arabic morphological analyzer in one day.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>47--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Darwish, 2002</marker>
<rawString>Kareem Darwish. 2002. Building a shallow Arabic morphological analyzer in one day. In Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 47–54. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra Daya</author>
<author>Dan Roth</author>
<author>Shuly Wintner</author>
</authors>
<title>Identifying Semitic Roots: Machine Learning with Linguistic Constraints.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>3</issue>
<marker>Daya, Roth, Wintner, 2008</marker>
<rawString>Ezra Daya, Dan Roth, and Shuly Wintner. 2008. Identifying Semitic Roots: Machine Learning with Linguistic Constraints. Computational Linguistics, 34(3):429–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Eisner</author>
</authors>
<title>Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>616--627</pages>
<location>Edinburgh, Scotland.</location>
<marker>Dreyer, Eisner, 2011</marker>
<rawString>Markus Dreyer and Jason Eisner. 2011. Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model. In Proceedings of EMNLP, pages 616–627, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kais Dukes</author>
<author>Nizar Habash</author>
</authors>
<title>Morphological Annotation of Quranic Arabic.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="23311" citStr="Dukes and Habash, 2010" startWordPosition="3856" endWordPosition="3859"> 2.3 6.4 BW&apos; 48428 30891 4707 2.3 10.7 QU&apos; 18808 12021 1270 1.9 9.9 HEB 5231 3164 492 2.1 6.7 Table 1: Corpus statistics, including average number of morphemes (m/w) and characters (c/w) per word, and total surface-realised roots of length 3 or 4. Analyser (BAMA).7 This allowed control over the word shapes, which is important to focus the evaluation, while yielding reliable segmentation and root annotations. BW has no vocalisation; we denote the &apos; corresponding vocalised dataset as BW. Quranic Arabic We extracted the roughly 18k word types from a morphologically analysed version of the Quran (Dukes and Habash, 2010). As an additional challenge, we left all given diacritics intact &apos; for this dataset, QU. Hebrew We leveraged the Hebrew CHILDES database as an annotated resource (Albert et al., 2013) and were able to extract 5k word types that feature at least one affix to use as dataset HEB. The corrected versions of words marked as non-standard child language were used, diacritics were dropped, and we conflated stressed and unstressed vowels to overcome inconsistencies in the source data. 6.2 Models We consider two classes of models. The first is the strictly context-free adaptor grammar for morphemes as s</context>
</contexts>
<marker>Dukes, Habash, 2010</marker>
<rawString>Kais Dukes and Nizar Habash. 2010. Morphological Annotation of Quranic Arabic. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>John DeNero</author>
</authors>
<title>Supervised Learning of Complete Morphological Paradigms.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>1185--1195</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<marker>Durrett, DeNero, 2013</marker>
<rawString>Greg Durrett and John DeNero. 2013. Supervised Learning of Complete Morphological Paradigms. In Proceedings of NAACL-HLT, pages 1185–1195, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khaled Elghamry</author>
</authors>
<title>A Constraint-based Algorithm for the Identification of Arabic Roots.</title>
<date>2005</date>
<booktitle>In Proceedings of the Midwest Computational Linguistics</booktitle>
<institution>Colloquium. Indiana University.</institution>
<location>Bloomington, IN.</location>
<marker>Elghamry, 2005</marker>
<rawString>Khaled Elghamry. 2005. A Constraint-based Algorithm for the Identification of Arabic Roots. In Proceedings of the Midwest Computational Linguistics Colloquium. Indiana University. Bloomington, IN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Finkel</author>
<author>Gregory Stump</author>
</authors>
<title>Generating Hebrew verb morphology by default inheritance hierarchies.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2292" citStr="Finkel and Stump, 2002" startWordPosition="331" endWordPosition="334">em comes from languages that use templatic morphology, such as Arabic, Hebrew and Amharic. These Semitic languages derive verb and noun stems by interspersing abstract root morphemes into templatic structures in a nonconcatenative way. For example, the Arabic root k·t·b can combine with the template (i-a) to derive the noun stem kitab (book). Established morphological analysers typically ignore this process and simply view the derived stems as elementary units (Buckwalter, 2002), or their account of it coincides with a requirement for extensive linguistic knowledge and hand-crafting of rules (Finkel and Stump, 2002; Schneider, 2010; Altantawy et al., 2010). The former approach is bound to suffer from vocabulary coverage issues, while the latter clearly does not transfer easily across languages. The practical appeal of unsupervised learning of templatic morphology is that it can overcome these shortcomings. Unsupervised learning of concatenative morphology has received extensive attention, partly driven by the MorphoChallenge (Kurimo et al., 2010) in recent years, but that is not the case for root-templatic morphology (Hammarstr¨om and Borin, 2011). In this paper we present a model-based method that lear</context>
</contexts>
<marker>Finkel, Stump, 2002</marker>
<rawString>Raphael Finkel and Gregory Stump. 2002. Generating Hebrew verb morphology by default inheritance hierarchies. In Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelle A Fullwood</author>
<author>Timothy J O’Donnell</author>
</authors>
<title>Learning non-concatenative morphology.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,</booktitle>
<pages>21--27</pages>
<institution>Sofia, Bulgaria. Association for Computational Linguistics.</institution>
<marker>Fullwood, O’Donnell, 2013</marker>
<rawString>Michelle A. Fullwood and Timothy J. O’Donnell. 2013. Learning non-concatenative morphology. In Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 21–27, Sofia, Bulgaria. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gasser</author>
</authors>
<title>Semitic morphological analysis and generation using finite state transducers with feature structures.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>309--317</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5032" citStr="Gasser, 2009" startWordPosition="739" endWordPosition="740">ate a method that is applicable across a variety of morphological processes. The specific rules used in our empirical evaluation on Arabic and Hebrew therefore contain hardly any explicit linguistic knowledge about the languages and are applicable across the family of Semitic languages. 2 A powerful grammar for morphology Concatenative morphology lends itself well to an analysis in terms of finite-state transducers (FSTs) (Koskenniemi, 1984). With some additional effort, FSTs can also encode non-concatenative morphology (Kiraz, 2000; Beesley and Karttunen, 2003; Cohen-Sygal and Wintner, 2006; Gasser, 2009). Despite this seeming adequacy of regular languages to describe morphology, we see two main shortcomings that motivate moving further up the Chomsky hierarchy of formal languages: first is the issue of learning. We are not aware of successful attempts at inducing FST-based morphological analysers in an unsupervised way, and believe the challenge lies in the fact that FSTs do not offer a convenient way of expressing prior linguistic intuitions to guide the learning process. Secondly, an FST composed of multiple machines might capture morphological processes well and excel at analysis, but inte</context>
</contexts>
<marker>Gasser, 2009</marker>
<rawString>Michael Gasser. 2009. Semitic morphological analysis and generation using finite state transducers with feature structures. In Proceedings of EACL, pages 309– 317. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Optimal Parsing Strategies for Linear Context-Free Rewriting Systems.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>769--776</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11915" citStr="Gildea, 2010" startWordPosition="1875" endWordPosition="1876">/parsing algorithm can have a worst-case time complexity that is polynomial in the input length n, O(n(ρ+1)ψ) for arity 0 and rank p, which reduces to O(n3ψ) for a binarised grammar. To capture the maximal case of a root with k − 1 characters and k discontiguous templatic characters forming a stem would require a grammar that has arity 0 = k. For Arabic, which has up to quadriliteral roots (k = 5), the time complexity would be O(n15).3 This is a daunting proposition for parsing, but we are careful 3The trade-off between arity and rank with respect to parsing complexity has been characterised (Gildea, 2010), and the appropriate refactoring may bring down the complexity for our grammars too. Suf(i) Pre(wa) Stm(kitab) Root(k,t,b) Template(i,a) w a k i t a b i 347 to set up our application of SRCGs in such a way that this is not too big an obstacle: Firstly, our grammars are defined over the characters that make up a word, and not over words that make up a sentence. As such, the input length n would tend to be shorter than when parsing full sentences from a corpus. Secondly, we do type-based morphological analysis, a view supported by evidence from Goldwater et al. (2006), so each unique word in a </context>
</contexts>
<marker>Gildea, 2010</marker>
<rawString>Daniel Gildea. 2010. Optimal Parsing Strategies for Linear Context-Free Rewriting Systems. In Proceedings of NAACL, pages 769–776. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating Between Types and Tokens by Estimating Power-Law Generators.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems, Volume</booktitle>
<volume>18</volume>
<contexts>
<context position="12488" citStr="Goldwater et al. (2006)" startWordPosition="1979" endWordPosition="1982">g complexity has been characterised (Gildea, 2010), and the appropriate refactoring may bring down the complexity for our grammars too. Suf(i) Pre(wa) Stm(kitab) Root(k,t,b) Template(i,a) w a k i t a b i 347 to set up our application of SRCGs in such a way that this is not too big an obstacle: Firstly, our grammars are defined over the characters that make up a word, and not over words that make up a sentence. As such, the input length n would tend to be shorter than when parsing full sentences from a corpus. Secondly, we do type-based morphological analysis, a view supported by evidence from Goldwater et al. (2006), so each unique word in a dataset is only ever parsed once with a given grammar. The set of word types attested in the data sources of interest here is fairly limited, typically in the tens of thousands. For these reasons, our parsing and inference tasks turn out to be tractable despite the high time complexity. 4 Learning 4.1 Probabilistic SRCG The probabilistic extension of SRCGs is similar to the probabilistic extension of CFGs, and has been used in other guises (Kato et al., 2006; Maier, 2010). Each rule r E P has an associated probability 0r such that ErEPA 0r = 1. A random string in the</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Interpolating Between Types and Tokens by Estimating Power-Law Generators. In Advances in Neural Information Processing Systems, Volume 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Hammarstr¨om</author>
<author>Lars Borin</author>
</authors>
<date>2011</date>
<journal>Unsupervised Learning of Morphology. Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<marker>Hammarstr¨om, Borin, 2011</marker>
<rawString>Harald Hammarstr¨om and Lars Borin. 2011. Unsupervised Learning of Morphology. Computational Linguistics, 37(2):309–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun Huang</author>
<author>Min Zhang</author>
<author>Chew Lim Tan</author>
</authors>
<title>Nonparametric Bayesian Machine Transliteration with Synchronous Adaptor Grammars.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL (Short papers),</booktitle>
<pages>534--539</pages>
<contexts>
<context position="14787" citStr="Huang et al., 2011" startWordPosition="2374" endWordPosition="2377">tly, both of the other expansions in the tree under construction and of any other trees. To some extent, this flies in the face of the reality of estimating a grammar from text, where one would expect certain sub-trees to be used repeatedly across different input strings. Adaptor grammars weaken this independence assumption by allowing whole subtrees to be reused during expansion. Informally, they act as a cache of tree fragments whose tendency to be reused during expansion is governed by the choice of adaptor function. Following earlier applications of adaptor grammars (Johnson et al., 2007; Huang et al., 2011), we employ the Pitman-Yor process (Pitman, 1995; Pitman and Yor, 1997) as adaptor function. A Pitman-Yor Simple Range Concatenating Adaptor Grammar (PYSRCAG) is a tuple G = (GS, M, a, b, α), where GS is a probabilistic SRCG as defined before and M C_ N is a set of adapted non-terminals. The vectors a and b, indexed by the elements of M, are the discount and concentration parameters for each adapted nonterminal, with a E [0, 1], b &gt; 0. α are parameters to Dirichlet priors on the rule probabilities 0. PYSRCAG defines a generative process over a set of trees T. Unadapted non-terminals A&apos; E N \ M</context>
</contexts>
<marker>Huang, Zhang, Tan, 2011</marker>
<rawString>Yun Huang, Min Zhang, and Chew Lim Tan. 2011. Nonparametric Bayesian Machine Transliteration with Synchronous Adaptor Grammars. In Proceedings of ACL (Short papers), pages 534–539.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparameteric Bayesian inference: Experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>317--325</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19274" citStr="Johnson and Goldwater, 2009" startWordPosition="3185" endWordPosition="3188"> acceptreject probability. As proposal grammar, we use the analogous approximation of our G as Johnson et al. used for PCFGs, namely by taking a static snapshot GQ of the adaptor grammar where additional rules rewrite adapted non-terminals as the terminal strings of their cached trees. Drawing a sample from the proposal distribution is then a matter of drawing a random tree from the parse chart of w under GQ. Lastly, the adaptor hyperparameters a and b are modelled by placing flat Beta(1,1) and vague Gamma(10, 0.1) priors on them, respectively, and inferring their values using slice sampling (Johnson and Goldwater, 2009). 5 Modelling root-templatic morphology We start with a CFG-based adaptor grammar4 that models words as a stem and any number of prefixes and suffixes: Word → Pre* Stem Suf* (8) Pre |Stem |Suf → Char+ (9) This fragment can be seen as building on the stemand-affix adaptor grammar presented in (Johnson et al., 2007) for morphological analysis of English, of which a later version also covers multiple affixes (Sirts and Goldwater, 2013). In the particular case of Arabic, multiple affixes are required to handle the at.tachment of particles and proclitics onto base words. To extend this to complex s</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric Bayesian inference: Experiments on unsupervised word segmentation with adaptor grammars. In Proceedings of NAACL-HLT, pages 317–325. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Adaptor Grammars: A Framework for Specifying Compositional Nonparametric Bayesian Models.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>19</volume>
<pages>641</pages>
<publisher>MIT.</publisher>
<contexts>
<context position="3431" citStr="Johnson et al., 2007" startWordPosition="495" endWordPosition="498">arstr¨om and Borin, 2011). In this paper we present a model-based method that learns concatenative and root-templatic morphology in a unified framework. We build on two disparate strands of work from the literature: Firstly, we apply simple Range Concatenating Grammars (SRCGs) (Boullier, 2000) to parse contiguous and discontiguous morphemes from an input string. These grammars are mildly-context sensitive (Joshi, 1985), a superset of context-free grammars that retains polynomial parsing time-complexity. Secondly, we generalise the nonparametric Bayesian learning framework of adaptor grammars (Johnson et al., 2007) to SRCGs.1 This should also be rel1Our formulation is in terms of SRCGs, which are equivalent in power to linear context-free rewrite systems (VijayShanker et al., 1987) and multiple context-free grammars (Seki et al., 1991), all of which are weaker than (non-simple) range concatenating grammars (Boullier, 2000). 345 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 345–356, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics evant to other applications of probabilistic SRCGs, e.g. in parsing (Maier, 2010),</context>
<context position="5903" citStr="Johnson et al., 2007" startWordPosition="877" endWordPosition="880">s at inducing FST-based morphological analysers in an unsupervised way, and believe the challenge lies in the fact that FSTs do not offer a convenient way of expressing prior linguistic intuitions to guide the learning process. Secondly, an FST composed of multiple machines might capture morphological processes well and excel at analysis, but interpretability of its internal operations are limited. These shortcomings are overcome for concatenative morphology by context-free adaptor grammars, which allowed diverse segmentation models to be formulated and investigated within a single framework (Johnson et al., 2007; Johnson, 2008; Sirts and Goldwater, 2013). In principle, that covers a wide range of phenomena (typical example language in parentheses): affixal inflection (Czech) and derivation (English), agglutinative derivation (Turkish, Finnish), compounding (German). Our agenda here is to extend that approach to include non-concatenative processes such as root-templatic derivation (Arabic), infixation (Tagalog) and circumfixation (Indonesian). In this pursuit, an abstraction that permits discontiguous constituents is a highly useful modelling tool, but requires looking beyond context-free grammars. An</context>
<context position="14766" citStr="Johnson et al., 2007" startWordPosition="2370" endWordPosition="2373">ion happens independently, both of the other expansions in the tree under construction and of any other trees. To some extent, this flies in the face of the reality of estimating a grammar from text, where one would expect certain sub-trees to be used repeatedly across different input strings. Adaptor grammars weaken this independence assumption by allowing whole subtrees to be reused during expansion. Informally, they act as a cache of tree fragments whose tendency to be reused during expansion is governed by the choice of adaptor function. Following earlier applications of adaptor grammars (Johnson et al., 2007; Huang et al., 2011), we employ the Pitman-Yor process (Pitman, 1995; Pitman and Yor, 1997) as adaptor function. A Pitman-Yor Simple Range Concatenating Adaptor Grammar (PYSRCAG) is a tuple G = (GS, M, a, b, α), where GS is a probabilistic SRCG as defined before and M C_ N is a set of adapted non-terminals. The vectors a and b, indexed by the elements of M, are the discount and concentration parameters for each adapted nonterminal, with a E [0, 1], b &gt; 0. α are parameters to Dirichlet priors on the rule probabilities 0. PYSRCAG defines a generative process over a set of trees T. Unadapted non</context>
<context position="17538" citStr="Johnson et al. (2007)" startWordPosition="2880" endWordPosition="2883"> of GS, but it maps the distribution over trees to one that is distributed according to the PYP. The invariance of SRCGs trees under isomorphism would make the probabilistic model deficient, but we side-step this issue by requiring that grammar rules are specified in a canonical way that ensures a one-to-one correspondence between the order of nodes in a tree and of terminals in the yield. 4.3 Inference under PYSRCAG The inference procedure under our model is very similar to that of CFG PY-adaptor grammars, so we restate the central aspects here but refer the reader to the original article by Johnson et al. (2007) for further details. First, one may integrate out the adaptors to obtain a single distribution over the set of trees generated from a particular non-terminal. Thus, the joint probability of a particular sequence z for the adapted non-terminal A with cached counts (n1, ... , nm) is 1m�=1 (a(k − 1) + b) 1�k�1 �=1 (j − a) 1��1 �=0 (i + b) Taking all the adapted non-terminals into account, the joint probability of a set of full trees T under the grammar G is P(T|a,b,�) _ AEM rl B(aA + fA) PY (z(T) |a, b), B(aA) where fA is a vector of the usage counts of rules r ∈ PA across T, and B is the Euler </context>
<context position="19589" citStr="Johnson et al., 2007" startWordPosition="3240" endWordPosition="3243">distribution is then a matter of drawing a random tree from the parse chart of w under GQ. Lastly, the adaptor hyperparameters a and b are modelled by placing flat Beta(1,1) and vague Gamma(10, 0.1) priors on them, respectively, and inferring their values using slice sampling (Johnson and Goldwater, 2009). 5 Modelling root-templatic morphology We start with a CFG-based adaptor grammar4 that models words as a stem and any number of prefixes and suffixes: Word → Pre* Stem Suf* (8) Pre |Stem |Suf → Char+ (9) This fragment can be seen as building on the stemand-affix adaptor grammar presented in (Johnson et al., 2007) for morphological analysis of English, of which a later version also covers multiple affixes (Sirts and Goldwater, 2013). In the particular case of Arabic, multiple affixes are required to handle the at.tachment of particles and proclitics onto base words. To extend this to complex stems consisting of a root with three radicals we have rules like the following: Stem(abcdefg) → R3(b, d, e) T4(a, c, e, g) Stem(abcdef) → R3(a, c, e) T3(b, d, f) Stem(abcde) → R3(a, c, e) T2(b, d) Stem(abcd) → R3(a, c, d) T1(b) Stem(abc) → R3(a, b, c) 4Adapted non-terminals are indicated by underlining and we use </context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007. Adaptor Grammars: A Framework for Specifying Compositional Nonparametric Bayesian Models. In Advances in Neural Information Processing Systems, volume 19, page 641. MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Unsupervised word segmentation for Sesotho using Adaptor Grammars.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL Special Interest Group on Computational Morphology and Phonology (SigMorPhon),</booktitle>
<pages>20--27</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5918" citStr="Johnson, 2008" startWordPosition="881" endWordPosition="882">d morphological analysers in an unsupervised way, and believe the challenge lies in the fact that FSTs do not offer a convenient way of expressing prior linguistic intuitions to guide the learning process. Secondly, an FST composed of multiple machines might capture morphological processes well and excel at analysis, but interpretability of its internal operations are limited. These shortcomings are overcome for concatenative morphology by context-free adaptor grammars, which allowed diverse segmentation models to be formulated and investigated within a single framework (Johnson et al., 2007; Johnson, 2008; Sirts and Goldwater, 2013). In principle, that covers a wide range of phenomena (typical example language in parentheses): affixal inflection (Czech) and derivation (English), agglutinative derivation (Turkish, Finnish), compounding (German). Our agenda here is to extend that approach to include non-concatenative processes such as root-templatic derivation (Arabic), infixation (Tagalog) and circumfixation (Indonesian). In this pursuit, an abstraction that permits discontiguous constituents is a highly useful modelling tool, but requires looking beyond context-free grammars. An idealised gene</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>Mark Johnson. 2008. Unsupervised word segmentation for Sesotho using Adaptor Grammars. In Proceedings ofACL Special Interest Group on Computational Morphology and Phonology (SigMorPhon), pages 20–27. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>Tree adjoining grammars: How much context-sensitivity is required to provide reasonable structural descriptions?</title>
<date>1985</date>
<booktitle>Natural Language Parsing, chapter 6,</booktitle>
<pages>206--250</pages>
<editor>In D.R. Dowty, L. Karttunen, and A.M. Zwicky, editors,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3232" citStr="Joshi, 1985" startWordPosition="471" endWordPosition="472">tenative morphology has received extensive attention, partly driven by the MorphoChallenge (Kurimo et al., 2010) in recent years, but that is not the case for root-templatic morphology (Hammarstr¨om and Borin, 2011). In this paper we present a model-based method that learns concatenative and root-templatic morphology in a unified framework. We build on two disparate strands of work from the literature: Firstly, we apply simple Range Concatenating Grammars (SRCGs) (Boullier, 2000) to parse contiguous and discontiguous morphemes from an input string. These grammars are mildly-context sensitive (Joshi, 1985), a superset of context-free grammars that retains polynomial parsing time-complexity. Secondly, we generalise the nonparametric Bayesian learning framework of adaptor grammars (Johnson et al., 2007) to SRCGs.1 This should also be rel1Our formulation is in terms of SRCGs, which are equivalent in power to linear context-free rewrite systems (VijayShanker et al., 1987) and multiple context-free grammars (Seki et al., 1991), all of which are weaker than (non-simple) range concatenating grammars (Boullier, 2000). 345 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Proce</context>
</contexts>
<marker>Joshi, 1985</marker>
<rawString>Aravind K. Joshi. 1985. Tree adjoining grammars: How much context-sensitivity is required to provide reasonable structural descriptions? In D.R. Dowty, L. Karttunen, and A.M. Zwicky, editors, Natural Language Parsing, chapter 6, pages 206–250. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miriam Kaeshammer</author>
</authors>
<title>Synchronous Linear Context-Free Rewriting Systems for Machine Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<pages>68--77</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta,</location>
<contexts>
<context position="4062" citStr="Kaeshammer, 2013" startWordPosition="588" endWordPosition="590">is should also be rel1Our formulation is in terms of SRCGs, which are equivalent in power to linear context-free rewrite systems (VijayShanker et al., 1987) and multiple context-free grammars (Seki et al., 1991), all of which are weaker than (non-simple) range concatenating grammars (Boullier, 2000). 345 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 345–356, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics evant to other applications of probabilistic SRCGs, e.g. in parsing (Maier, 2010), translation (Kaeshammer, 2013) and genetics (Kato et al., 2006). In addition to unannotated data, our method requires as input a minimal set of high-level grammar rules that encode basic intuitions of the morphology. This is where there would be room to become very language specific. Our aim, however, is not to obtain a best-published result in a particular language, but rather to create a method that is applicable across a variety of morphological processes. The specific rules used in our empirical evaluation on Arabic and Hebrew therefore contain hardly any explicit linguistic knowledge about the languages and are applic</context>
</contexts>
<marker>Kaeshammer, 2013</marker>
<rawString>Miriam Kaeshammer. 2013. Synchronous Linear Context-Free Rewriting Systems for Machine Translation. In Proceedings of the Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 68–77, Atlanta, Georgia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuki Kato</author>
<author>Hiroyuki Seki</author>
<author>Tadao Kasami</author>
</authors>
<title>Stochastic Multiple Context-Free Grammar for RNA Pseudoknot Modeling.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Workshop on Tree Adjoining Grammar and Related Formalisms,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="4095" citStr="Kato et al., 2006" startWordPosition="593" endWordPosition="596">ation is in terms of SRCGs, which are equivalent in power to linear context-free rewrite systems (VijayShanker et al., 1987) and multiple context-free grammars (Seki et al., 1991), all of which are weaker than (non-simple) range concatenating grammars (Boullier, 2000). 345 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 345–356, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics evant to other applications of probabilistic SRCGs, e.g. in parsing (Maier, 2010), translation (Kaeshammer, 2013) and genetics (Kato et al., 2006). In addition to unannotated data, our method requires as input a minimal set of high-level grammar rules that encode basic intuitions of the morphology. This is where there would be room to become very language specific. Our aim, however, is not to obtain a best-published result in a particular language, but rather to create a method that is applicable across a variety of morphological processes. The specific rules used in our empirical evaluation on Arabic and Hebrew therefore contain hardly any explicit linguistic knowledge about the languages and are applicable across the family of Semitic</context>
<context position="12977" citStr="Kato et al., 2006" startWordPosition="2065" endWordPosition="2068">nces from a corpus. Secondly, we do type-based morphological analysis, a view supported by evidence from Goldwater et al. (2006), so each unique word in a dataset is only ever parsed once with a given grammar. The set of word types attested in the data sources of interest here is fairly limited, typically in the tens of thousands. For these reasons, our parsing and inference tasks turn out to be tractable despite the high time complexity. 4 Learning 4.1 Probabilistic SRCG The probabilistic extension of SRCGs is similar to the probabilistic extension of CFGs, and has been used in other guises (Kato et al., 2006; Maier, 2010). Each rule r E P has an associated probability 0r such that ErEPA 0r = 1. A random string in the language of the grammar can then be obtained through a generative procedure that begins with the start symbol S and iteratively expands it until deriving e: At each step for some current symbol A, a rewrite rule r is sampled randomly from PA in accordance with the distribution over rules and used to expand A. This procedure terminates when no further expansions are possible. Of course, expansions need to respect the range concatenating and ordering constraints imposed by the variable</context>
</contexts>
<marker>Kato, Seki, Kasami, 2006</marker>
<rawString>Yuki Kato, Hiroyuki Seki, and Tadao Kasami. 2006. Stochastic Multiple Context-Free Grammar for RNA Pseudoknot Modeling. In Proceedings of the International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Anton Kiraz</author>
</authors>
<title>Multitiered Nonlinear Morphology Using Multitape Finite Automata: A Case</title>
<date>2000</date>
<booktitle>Study on Syriac and Arabic. Computational Linguistics,</booktitle>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="4957" citStr="Kiraz, 2000" startWordPosition="729" endWordPosition="730">btain a best-published result in a particular language, but rather to create a method that is applicable across a variety of morphological processes. The specific rules used in our empirical evaluation on Arabic and Hebrew therefore contain hardly any explicit linguistic knowledge about the languages and are applicable across the family of Semitic languages. 2 A powerful grammar for morphology Concatenative morphology lends itself well to an analysis in terms of finite-state transducers (FSTs) (Koskenniemi, 1984). With some additional effort, FSTs can also encode non-concatenative morphology (Kiraz, 2000; Beesley and Karttunen, 2003; Cohen-Sygal and Wintner, 2006; Gasser, 2009). Despite this seeming adequacy of regular languages to describe morphology, we see two main shortcomings that motivate moving further up the Chomsky hierarchy of formal languages: first is the issue of learning. We are not aware of successful attempts at inducing FST-based morphological analysers in an unsupervised way, and believe the challenge lies in the fact that FSTs do not offer a convenient way of expressing prior linguistic intuitions to guide the learning process. Secondly, an FST composed of multiple machines</context>
</contexts>
<marker>Kiraz, 2000</marker>
<rawString>George Anton Kiraz. 2000. Multitiered Nonlinear Morphology Using Multitape Finite Automata: A Case Study on Syriac and Arabic. Computational Linguistics, 26(1):77–105, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>A general computational model for word-form recognition and production.</title>
<date>1984</date>
<booktitle>In Proceedings of the 10th international conference on Computational Linguistics,</booktitle>
<pages>178--181</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4864" citStr="Koskenniemi, 1984" startWordPosition="716" endWordPosition="717">y. This is where there would be room to become very language specific. Our aim, however, is not to obtain a best-published result in a particular language, but rather to create a method that is applicable across a variety of morphological processes. The specific rules used in our empirical evaluation on Arabic and Hebrew therefore contain hardly any explicit linguistic knowledge about the languages and are applicable across the family of Semitic languages. 2 A powerful grammar for morphology Concatenative morphology lends itself well to an analysis in terms of finite-state transducers (FSTs) (Koskenniemi, 1984). With some additional effort, FSTs can also encode non-concatenative morphology (Kiraz, 2000; Beesley and Karttunen, 2003; Cohen-Sygal and Wintner, 2006; Gasser, 2009). Despite this seeming adequacy of regular languages to describe morphology, we see two main shortcomings that motivate moving further up the Chomsky hierarchy of formal languages: first is the issue of learning. We are not aware of successful attempts at inducing FST-based morphological analysers in an unsupervised way, and believe the challenge lies in the fact that FSTs do not offer a convenient way of expressing prior lingui</context>
</contexts>
<marker>Koskenniemi, 1984</marker>
<rawString>Kimmo Koskenniemi. 1984. A general computational model for word-form recognition and production. In Proceedings of the 10th international conference on Computational Linguistics, pages 178–181. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
<author>Sami Virpioja</author>
<author>Ville T Turunen</author>
<author>Graeme W Blackwood</author>
<author>William Byrne</author>
</authors>
<title>Overview and Results of Morpho Challenge</title>
<date>2010</date>
<booktitle>In Multilingual Information Access Evaluation I. TextRetrieval Experiments,</booktitle>
<volume>6241</volume>
<pages>578--597</pages>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="2732" citStr="Kurimo et al., 2010" startWordPosition="395" endWordPosition="398">ms as elementary units (Buckwalter, 2002), or their account of it coincides with a requirement for extensive linguistic knowledge and hand-crafting of rules (Finkel and Stump, 2002; Schneider, 2010; Altantawy et al., 2010). The former approach is bound to suffer from vocabulary coverage issues, while the latter clearly does not transfer easily across languages. The practical appeal of unsupervised learning of templatic morphology is that it can overcome these shortcomings. Unsupervised learning of concatenative morphology has received extensive attention, partly driven by the MorphoChallenge (Kurimo et al., 2010) in recent years, but that is not the case for root-templatic morphology (Hammarstr¨om and Borin, 2011). In this paper we present a model-based method that learns concatenative and root-templatic morphology in a unified framework. We build on two disparate strands of work from the literature: Firstly, we apply simple Range Concatenating Grammars (SRCGs) (Boullier, 2000) to parse contiguous and discontiguous morphemes from an input string. These grammars are mildly-context sensitive (Joshi, 1985), a superset of context-free grammars that retains polynomial parsing time-complexity. Secondly, we </context>
</contexts>
<marker>Kurimo, Virpioja, Turunen, Blackwood, Byrne, 2010</marker>
<rawString>Mikko Kurimo, Sami Virpioja, Ville T. Turunen, Graeme W. Blackwood, and William Byrne. 2010. Overview and Results of Morpho Challenge 2009. In Multilingual Information Access Evaluation I. TextRetrieval Experiments, volume 6241 of Lecture Notes in Computer Science, pages 578–597. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Modeling syntactic context improves morphological segmentation.</title>
<date>2011</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="34894" citStr="Lee et al., 2011" startWordPosition="5776" endWordPosition="5779">onsecutive affixes and stems, whereas all our experiments (except on QU&apos;) presupposed single affixes. This biases the evaluation slightly in our favour, but works in Morfessor’s favour on the QU&apos; data which is annotated with multiple affixes. 7 Related work The distinctive feature of our morphological model is that it jointly addresses root identification and morpheme segmentation, and our results demonstrate the mutual benefit of this. In contrast, earlier unsupervised approaches tend to focus on these tasks in isolation. In unsupervised Arabic segmentation, the parametric Bayesian model of (Lee et al., 2011) achieves F1-scores in the high eighties by incorporating sentential context and inferred syntactic categories, both of which our model forgoes, although theirs has no account of discontiguous root morphemes. Root Example instances 1. spr ✓ G ˇsaˇpaˇr.ti te.ˇsaˇpˇr ye.ˇsaˇpˇr.u B sipur.im hixˇstaˇpxaˇr t 2. lbs ✓ G ˇlaˇbaˇS.t li.ˇlˇboSˇ ti.ˇlˇbeˇS.i B le haxˇlˇbiSˇ ti txˇlabˇˇS.i 3. ptx ✓ G ˇpaˇtaˇx.ti ti.ˇpˇteˇx.i B li.ˇpˇtoaˇx nix ˇpˇtaˇx.at 5. !al x B ya.!al.u ˇmax ˇ!aˇl.a ˇ!aˇcˇlxan it Table 4: Top Hebrew roots hypothesised by Tpl+T4. Numbers indicate position when ranked by model probabil</context>
</contexts>
<marker>Lee, Haghighi, Barzilay, 2011</marker>
<rawString>Yoong Keok Lee, Aria Haghighi, and Regina Barzilay. 2011. Modeling syntactic context improves morphological segmentation. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
</authors>
<title>Direct Parsing of Discontinuous Constituents in German.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL-HLT Workshop on Statistical Parsing of Morphologically-Rich Languages,</booktitle>
<pages>58--66</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4030" citStr="Maier, 2010" startWordPosition="585" endWordPosition="586">et al., 2007) to SRCGs.1 This should also be rel1Our formulation is in terms of SRCGs, which are equivalent in power to linear context-free rewrite systems (VijayShanker et al., 1987) and multiple context-free grammars (Seki et al., 1991), all of which are weaker than (non-simple) range concatenating grammars (Boullier, 2000). 345 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 345–356, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics evant to other applications of probabilistic SRCGs, e.g. in parsing (Maier, 2010), translation (Kaeshammer, 2013) and genetics (Kato et al., 2006). In addition to unannotated data, our method requires as input a minimal set of high-level grammar rules that encode basic intuitions of the morphology. This is where there would be room to become very language specific. Our aim, however, is not to obtain a best-published result in a particular language, but rather to create a method that is applicable across a variety of morphological processes. The specific rules used in our empirical evaluation on Arabic and Hebrew therefore contain hardly any explicit linguistic knowledge ab</context>
<context position="12991" citStr="Maier, 2010" startWordPosition="2069" endWordPosition="2070"> Secondly, we do type-based morphological analysis, a view supported by evidence from Goldwater et al. (2006), so each unique word in a dataset is only ever parsed once with a given grammar. The set of word types attested in the data sources of interest here is fairly limited, typically in the tens of thousands. For these reasons, our parsing and inference tasks turn out to be tractable despite the high time complexity. 4 Learning 4.1 Probabilistic SRCG The probabilistic extension of SRCGs is similar to the probabilistic extension of CFGs, and has been used in other guises (Kato et al., 2006; Maier, 2010). Each rule r E P has an associated probability 0r such that ErEPA 0r = 1. A random string in the language of the grammar can then be obtained through a generative procedure that begins with the start symbol S and iteratively expands it until deriving e: At each step for some current symbol A, a rewrite rule r is sampled randomly from PA in accordance with the distribution over rules and used to expand A. This procedure terminates when no further expansions are possible. Of course, expansions need to respect the range concatenating and ordering constraints imposed by the variables in rules. Th</context>
</contexts>
<marker>Maier, 2010</marker>
<rawString>Wolfgang Maier. 2010. Direct Parsing of Discontinuous Constituents in German. In Proceedings of the NAACL-HLT Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 58–66. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Pitman</author>
<author>Marc Yor</author>
</authors>
<title>The Two-Parameter Poisson-Dirichlet Distribution Derived from a Stable Subordinator. The Annals of Probability,</title>
<date>1997</date>
<volume>25</volume>
<issue>2</issue>
<pages>900</pages>
<contexts>
<context position="14858" citStr="Pitman and Yor, 1997" startWordPosition="2385" endWordPosition="2389">of any other trees. To some extent, this flies in the face of the reality of estimating a grammar from text, where one would expect certain sub-trees to be used repeatedly across different input strings. Adaptor grammars weaken this independence assumption by allowing whole subtrees to be reused during expansion. Informally, they act as a cache of tree fragments whose tendency to be reused during expansion is governed by the choice of adaptor function. Following earlier applications of adaptor grammars (Johnson et al., 2007; Huang et al., 2011), we employ the Pitman-Yor process (Pitman, 1995; Pitman and Yor, 1997) as adaptor function. A Pitman-Yor Simple Range Concatenating Adaptor Grammar (PYSRCAG) is a tuple G = (GS, M, a, b, α), where GS is a probabilistic SRCG as defined before and M C_ N is a set of adapted non-terminals. The vectors a and b, indexed by the elements of M, are the discount and concentration parameters for each adapted nonterminal, with a E [0, 1], b &gt; 0. α are parameters to Dirichlet priors on the rule probabilities 0. PYSRCAG defines a generative process over a set of trees T. Unadapted non-terminals A&apos; E N \ M are expanded as before (�4.1). For each adapted non-terminal A E M, a </context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>Jim Pitman and Marc Yor. 1997. The Two-Parameter Poisson-Dirichlet Distribution Derived from a Stable Subordinator. The Annals of Probability, 25(2):855– 900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Pitman</author>
</authors>
<title>Exchangeable and partially exchangeable random partitions. Probability Theory and Related Fields,</title>
<date>1995</date>
<pages>102--145</pages>
<contexts>
<context position="14835" citStr="Pitman, 1995" startWordPosition="2383" endWordPosition="2384">struction and of any other trees. To some extent, this flies in the face of the reality of estimating a grammar from text, where one would expect certain sub-trees to be used repeatedly across different input strings. Adaptor grammars weaken this independence assumption by allowing whole subtrees to be reused during expansion. Informally, they act as a cache of tree fragments whose tendency to be reused during expansion is governed by the choice of adaptor function. Following earlier applications of adaptor grammars (Johnson et al., 2007; Huang et al., 2011), we employ the Pitman-Yor process (Pitman, 1995; Pitman and Yor, 1997) as adaptor function. A Pitman-Yor Simple Range Concatenating Adaptor Grammar (PYSRCAG) is a tuple G = (GS, M, a, b, α), where GS is a probabilistic SRCG as defined before and M C_ N is a set of adapted non-terminals. The vectors a and b, indexed by the elements of M, are the discount and concentration parameters for each adapted nonterminal, with a E [0, 1], b &gt; 0. α are parameters to Dirichlet priors on the rule probabilities 0. PYSRCAG defines a generative process over a set of trees T. Unadapted non-terminals A&apos; E N \ M are expanded as before (�4.1). For each adapted</context>
</contexts>
<marker>Pitman, 1995</marker>
<rawString>Jim Pitman. 1995. Exchangeable and partially exchangeable random partitions. Probability Theory and Related Fields, 102:145–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Franc¸ois Prunet</author>
</authors>
<title>External Evidence and the Semitic Root.</title>
<date>2006</date>
<journal>Morphology,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="7838" citStr="Prunet, 2006" startWordPosition="1171" endWordPosition="1172">by themselves, the first two rules are simply a CFG that describes word formation as the concatenation of stems and affixes, a formulation that matches the underlying grammar of Morfessor (Creutz and Lagus, 2007), a well-studied unsupervised model. The key aim of our extension is that we want the grammar to capture a discontiguous string like k·t·b as a single constituent in a parse tree. This leads to well-understood problems in probabilistic grammars (e.g. what is this rule’s probability?), but also corresponds to the linguistic consideration that k·t·b is a proper morpheme of the language (Prunet, 2006). 3 Simple range concatenating grammars In this section we define SRCGs formally and illustrate how they can be used to model nonconcatenative morphology. SRCGs define languages that are recognisable in polynomial time, yet can capture discontiguous elements of a string under a single category (Boullier, 2000). An SRCG346 rule operates on vectors of ranges in contrast to the way a CFG-rule operates on single ranges (spans). In other words, a non-terminal symbol in an SRCG (CFG) derivation can dominate a subset (substring) of terminals in an input string. 3.1 Formalism An SRCG G is a tuple (N, </context>
</contexts>
<marker>Prunet, 2006</marker>
<rawString>Jean-Franc¸ois Prunet. 2006. External Evidence and the Semitic Root. Morphology, 16(1):41–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Rodrigues</author>
<author>Damir ´Cavar</author>
</authors>
<title>Learning Arabic Morphology Using Statistical Constraint-Satisfaction Models.</title>
<date>2007</date>
<booktitle>In Elabbas Benmamoun, editor, Perspectives on Arabic Linguistics: Proceedings of the 19th Arabic Linguistics Symposium,</booktitle>
<pages>63--75</pages>
<publisher>John Benjamins Publishing Company.</publisher>
<location>Urbana, IL, USA.</location>
<marker>Rodrigues, ´Cavar, 2007</marker>
<rawString>Paul Rodrigues and Damir ´Cavar. 2007. Learning Arabic Morphology Using Statistical Constraint-Satisfaction Models. In Elabbas Benmamoun, editor, Perspectives on Arabic Linguistics: Proceedings of the 19th Arabic Linguistics Symposium, pages 63–75, Urbana, IL, USA. John Benjamins Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Schneider</author>
</authors>
<title>Computational Cognitive Morphosemantics: Modeling Morphological Compositionality in Hebrew Verbs with Embodied Construction Grammar.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the Berkeley Linguistics Society,</booktitle>
<location>Berkeley, CA.</location>
<contexts>
<context position="2309" citStr="Schneider, 2010" startWordPosition="335" endWordPosition="336">that use templatic morphology, such as Arabic, Hebrew and Amharic. These Semitic languages derive verb and noun stems by interspersing abstract root morphemes into templatic structures in a nonconcatenative way. For example, the Arabic root k·t·b can combine with the template (i-a) to derive the noun stem kitab (book). Established morphological analysers typically ignore this process and simply view the derived stems as elementary units (Buckwalter, 2002), or their account of it coincides with a requirement for extensive linguistic knowledge and hand-crafting of rules (Finkel and Stump, 2002; Schneider, 2010; Altantawy et al., 2010). The former approach is bound to suffer from vocabulary coverage issues, while the latter clearly does not transfer easily across languages. The practical appeal of unsupervised learning of templatic morphology is that it can overcome these shortcomings. Unsupervised learning of concatenative morphology has received extensive attention, partly driven by the MorphoChallenge (Kurimo et al., 2010) in recent years, but that is not the case for root-templatic morphology (Hammarstr¨om and Borin, 2011). In this paper we present a model-based method that learns concatenative </context>
</contexts>
<marker>Schneider, 2010</marker>
<rawString>Nathan Schneider. 2010. Computational Cognitive Morphosemantics: Modeling Morphological Compositionality in Hebrew Verbs with Embodied Construction Grammar. In Proceedings of the Annual Meeting of the Berkeley Linguistics Society, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Seki</author>
<author>Takashi Matsumura</author>
<author>Mamoru Fujii</author>
<author>Tadao Kasami</author>
</authors>
<title>On multiple context-free grammars.</title>
<date>1991</date>
<journal>Theoretical Computer Science,</journal>
<volume>88</volume>
<issue>2</issue>
<contexts>
<context position="3656" citStr="Seki et al., 1991" startWordPosition="533" endWordPosition="536">apply simple Range Concatenating Grammars (SRCGs) (Boullier, 2000) to parse contiguous and discontiguous morphemes from an input string. These grammars are mildly-context sensitive (Joshi, 1985), a superset of context-free grammars that retains polynomial parsing time-complexity. Secondly, we generalise the nonparametric Bayesian learning framework of adaptor grammars (Johnson et al., 2007) to SRCGs.1 This should also be rel1Our formulation is in terms of SRCGs, which are equivalent in power to linear context-free rewrite systems (VijayShanker et al., 1987) and multiple context-free grammars (Seki et al., 1991), all of which are weaker than (non-simple) range concatenating grammars (Boullier, 2000). 345 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 345–356, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics evant to other applications of probabilistic SRCGs, e.g. in parsing (Maier, 2010), translation (Kaeshammer, 2013) and genetics (Kato et al., 2006). In addition to unannotated data, our method requires as input a minimal set of high-level grammar rules that encode basic intuitions of the morphology. This is</context>
</contexts>
<marker>Seki, Matsumura, Fujii, Kasami, 1991</marker>
<rawString>Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and Tadao Kasami. 1991. On multiple context-free grammars. Theoretical Computer Science, 88(2):191–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kairit Sirts</author>
<author>Sharon Goldwater</author>
</authors>
<title>MinimallySupervised Morphological Segmentation using Adaptor Grammars.</title>
<date>2013</date>
<journal>Transactions of the ACL.</journal>
<contexts>
<context position="5946" citStr="Sirts and Goldwater, 2013" startWordPosition="883" endWordPosition="886"> analysers in an unsupervised way, and believe the challenge lies in the fact that FSTs do not offer a convenient way of expressing prior linguistic intuitions to guide the learning process. Secondly, an FST composed of multiple machines might capture morphological processes well and excel at analysis, but interpretability of its internal operations are limited. These shortcomings are overcome for concatenative morphology by context-free adaptor grammars, which allowed diverse segmentation models to be formulated and investigated within a single framework (Johnson et al., 2007; Johnson, 2008; Sirts and Goldwater, 2013). In principle, that covers a wide range of phenomena (typical example language in parentheses): affixal inflection (Czech) and derivation (English), agglutinative derivation (Turkish, Finnish), compounding (German). Our agenda here is to extend that approach to include non-concatenative processes such as root-templatic derivation (Arabic), infixation (Tagalog) and circumfixation (Indonesian). In this pursuit, an abstraction that permits discontiguous constituents is a highly useful modelling tool, but requires looking beyond context-free grammars. An idealised generative grammar that would ca</context>
<context position="19710" citStr="Sirts and Goldwater, 2013" startWordPosition="3258" endWordPosition="3261">parameters a and b are modelled by placing flat Beta(1,1) and vague Gamma(10, 0.1) priors on them, respectively, and inferring their values using slice sampling (Johnson and Goldwater, 2009). 5 Modelling root-templatic morphology We start with a CFG-based adaptor grammar4 that models words as a stem and any number of prefixes and suffixes: Word → Pre* Stem Suf* (8) Pre |Stem |Suf → Char+ (9) This fragment can be seen as building on the stemand-affix adaptor grammar presented in (Johnson et al., 2007) for morphological analysis of English, of which a later version also covers multiple affixes (Sirts and Goldwater, 2013). In the particular case of Arabic, multiple affixes are required to handle the at.tachment of particles and proclitics onto base words. To extend this to complex stems consisting of a root with three radicals we have rules like the following: Stem(abcdefg) → R3(b, d, e) T4(a, c, e, g) Stem(abcdef) → R3(a, c, e) T3(b, d, f) Stem(abcde) → R3(a, c, e) T2(b, d) Stem(abcd) → R3(a, c, d) T1(b) Stem(abc) → R3(a, b, c) 4Adapted non-terminals are indicated by underlining and we use the following abbreviations: X → Y+ means one or more instances of Y and encodes the rules X → Ys and Ys → Ys Y |Y. Simil</context>
<context position="31078" citStr="Sirts and Goldwater, 2013" startWordPosition="5154" endWordPosition="5157">o. We include the ROC plots for Hebrew stem and root induction in Figure 2, along with the roots the model was most confident about (Table 4). 6.5 Morphological Analysis per Word Type In this section we turn to the analyses our models assign to each word type. Two aspects of interest are the segmentation into sequential morphemes and the identification of the root. Our intercalating adaptor grammars consistently obtain large gains in segmentation accuracy over the baseline concatenative model, across all our datasets (Table 3). We measure segmentation quality as segment border F1-score (SBF) (Sirts and Goldwater, 2013), which is the F-score over word-internal segmentation points of the predicted analysis with respect to the gold segmentation. Of the two MSA datasets, the vocalised version 0 BWpresents a more difficult segmentation task as its words are on average longer and feature 31k unique contiguous morphemes, compared to the 24k in BW for the same number of words. It should thus benefit more from additional model expressivity, as is reflected in the increase of 10 SBF when adding the TplR4 rule to the other triliteral ones. The best triliteral root identification accuracy (on a per-word basis) was foun</context>
<context position="32888" citStr="Sirts and Goldwater, 2013" startWordPosition="5446" endWordPosition="5449"> On the remaining words (having 3-5 morphemes), our models recover and approach the Morfessor baseline (MConcat: 32.7 , MTpl3Ch: 38.6). Preliminary experiments on BW had indicated that adaptation of (single) affix categories is crucial for good performance. Our multi-affixing models used on QU� lacked a further level of adaptation for composite affixes, which we suspect as a contributing factor to the lower performance on that dataset. This remains to be confirmed in future experiments, but would be consistent with other observations on the role of hierarchical adaptation in adaptor grammars (Sirts and Goldwater, 2013). The trend that intercalated rules improve segmentation (compared to the concatenative grammar) remains consistent 8When excluding cases where root equals stem, root identification on BW is 55%. Those cases are still not trivial, since words without roots also exist. 9By way of comparison, Rodrigues and ´Cavar (2007) presented an unsupervised statistics-based root identification method that obtained precision ranging between 50-75%, the higher requiring vocalised words. 352 (a) Stems (b) Triliteral roots True Positive Rate 0.4 0.9 0.8 0.7 0.6 0.5 0.3 0.1 0.0 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.</context>
</contexts>
<marker>Sirts, Goldwater, 2013</marker>
<rawString>Kairit Sirts and Sharon Goldwater. 2013. MinimallySupervised Morphological Segmentation using Adaptor Grammars. Transactions of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
<author>Aravind K Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms.</title>
<date>1987</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>104--111</pages>
<marker>Vijay-Shanker, Weir, Joshi, 1987</marker>
<rawString>K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi. 1987. Characterizing structural descriptions produced by various grammatical formalisms. In Proceedings of ACL, pages 104–111.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
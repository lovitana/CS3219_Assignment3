<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000028">
<title confidence="0.980269">
Learning Biological Processes with Global Constraints
</title>
<author confidence="0.635788">
Aju Thalappillil Scaria; Jonathan Berant, Mengqiu Wang and Christopher D. Manning
</author>
<affiliation confidence="0.584912">
Stanford University, Stanford
</affiliation>
<author confidence="0.793792">
Justin Lewis and Brittany Harding
</author>
<affiliation confidence="0.974497">
University of Washington, Seattle
</affiliation>
<sectionHeader confidence="0.978718" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999751192307692">
Biological processes are complex phenom-
ena involving a series of events that are re-
lated to one another through various relation-
ships. Systems that can understand and rea-
son over biological processes would dramat-
ically improve the performance of semantic
applications involving inference such as ques-
tion answering (QA) – specifically “How?”
and “Why?” questions. In this paper, we
present the task of process extraction, in
which events within a process and the rela-
tions between the events are automatically ex-
tracted from text. We represent processes by
graphs whose edges describe a set of temporal,
causal and co-reference event-event relations,
and characterize the structural properties of
these graphs (e.g., the graphs are connected).
Then, we present a method for extracting rela-
tions between the events, which exploits these
structural properties by performing joint in-
ference over the set of extracted relations.
On a novel dataset containing 148 descrip-
tions of biological processes (released with
this paper), we show significant improvement
comparing to baselines that disregard process
structure.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.956221461538462">
A process is defined as a series of inter-related
events that involve multiple entities and lead to an
end result. Product manufacturing, economical de-
velopments, and various phenomena in life and so-
cial sciences can all be viewed as types of processes.
Processes are complicated objects; consider for ex-
ample the biological process of ATP synthesis de-
scribed in Figure 1. This process involves 12 en-
tities and 8 events. Additionally, it describes rela-
tions between events and entities, and the relation-
ship between events (e.g., the second occurrence of
the event ‘enter’, causes the event ‘changing’).
*Both authors equally contributed to the paper
</bodyText>
<subsectionHeader confidence="0.563507">
Peter Clark
</subsectionHeader>
<bodyText confidence="0.986490463414634">
Allen Institute for Artificial Intelligence, Seattle
Automatically extracting the structure of pro-
cesses from text is crucial for applications that re-
quire reasoning, such as non-factoid QA. For in-
stance, answering a question on ATP synthesis, such
as “How do H+ ions contribute to the production
of ATP?” requires a structure that links H+ ions
(Figure 1, sentence 1) to ATP (Figure 1, sentence
4) through a sequence of intermediate events. Such
“How?” questions are common on FAQ websites
(Surdeanu et al., 2011), which further supports the
importance of process extraction.
Process extraction is related to two recent lines
of work in Information Extraction – event extrac-
tion and timeline construction. Traditional event ex-
traction focuses on identifying a closed set of events
within a single sentence. For example, the BioNLP
2009 and 2011 shared tasks (Kim et al., 2009; Kim
et al., 2011) consider nine events types related to
proteins. In practice, events are currently almost al-
ways extracted from a single sentence. Process ex-
traction, on the other hand, is centered around dis-
covering relations between events that span multiple
sentences. The set of possible event types in process
extraction is also much larger.
Timeline construction involves identifying tem-
poral relations between events (Do et al., 2012; Mc-
Closky and Manning, 2012; D’Souza and Ng, 2013),
and is thus related to process extraction as both fo-
cus on event-event relations spanning multiple sen-
tences. However, events in processes are tightly cou-
pled in ways that go beyond simple temporal order-
ing, and these dependencies are central for the pro-
cess extraction task. Hence, capturing process struc-
ture requires modeling a larger set of relations that
includes temporal, causal and co-reference relations.
In this paper, we formally define the task of
process extraction and present automatic extraction
methods. Our approach handles an open set of event
types and works over multiple sentences, extract-
ing a rich set of event-event relations. Furthermore,
</bodyText>
<page confidence="0.902239">
1710
</page>
<note confidence="0.879231">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1710–1720,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999779">
Figure 1: Partial annotation of the ATP synthesis process. Most of the semantic roles have been removed for simplicity.
</figureCaption>
<figure confidence="0.99928696">
prev
Entity
Entity
H+ ions flowing down their gradient enter
a half channel in a stator, which is anchored in the membrane.
spin
as well.
Spinning of the rotor causes an internal rod to
Event
Entity
same causes
Event
Entity
same
Event
cotemp
Entity
Event
Entity
enter binding sites within a rotor, changing the shape of each subunit so that the rotor
H+ ions
the membrane.
spins within
prev
Entity
Entity
Event
same
Entity
Entity
Event
Entity
Event
Entity
causes causes
ATP from
ADP and P_i.
same
Entity
Event
Entity
Event
causes causes
Turning of the rod activates catalytic sites in the knob that can
produce
raw-material
Event
result
Entity
Entity
</figure>
<bodyText confidence="0.99845725">
we characterize a set of global properties of process
structure that can be utilized during process extrac-
tion. For example, all events in a process are some-
how connected to one another. Also, processes usu-
ally exhibit a “chain-like” structure reflecting pro-
cess progression over time. We show that incor-
porating such global properties into our model and
performing joint inference over the extracted rela-
tions significantly improves the quality of process
structures predicted. We conduct experiments on a
novel dataset of process descriptions from the text-
book “Biology” (Campbell and Reece, 2005) that
were annotated by trained biologists. Our method
does not require any domain-specific knowledge and
can be easily adapted to non-biology domains.
The main contributions of this paper are:
</bodyText>
<listItem confidence="0.94530425">
1. We define process extraction and characterize
processes’ structural properties.
2. We model global structural properties in pro-
cesses and demonstrate significant improve-
ment in extraction accuracy.
3. We publicly release a novel data set of 148
fully annotated biological process descrip-
tions along with the source code for our sys-
</listItem>
<equation confidence="0.395296">
01:801/indexhtml#/exampes/emnlp213/p66
</equation>
<bodyText confidence="0.991148666666667">
tem. The dataset and code can be down-
loaded from http://nlp.stanford.edu/
software/bioprocess/.
</bodyText>
<sectionHeader confidence="0.842621" genericHeader="method">
2 Process Definition and Dataset
</sectionHeader>
<bodyText confidence="0.999981838709677">
We define a process description as a paragraph or
sequence of tokens x = {x1,... x|x|} that describes
a series of events related by temporal and/or causal
relations. For example, in ATP synthesis (Figure 1),
the event of rotor spinning causes the event where
an internal rod spins.
We model the events within a process and their
relations by a directed graph P = (V, E), where
the nodes V = {1, ... , |V |} represent event men-
tions and labeled edges E correspond to event-event
relations. An event mention v E V is defined by a
trigger tv, which is a span of words xi, xi+1, ... , xj;
and by a set of argument mentions Av, where each
argument mention av E Av is also a span of words
labeled by a semantic role l taken from a set L. For
example, in the last event mention of ATP synthesis,
tv = produce, and one of the argument mentions is
av = (ATP, RESULT). A labeled edge (u, v, r) in the
graph describes a relation r E R between the event
mentions u and v. The task of process extraction is
to extract the graph P from the text x.1
A natural way to break down process extraction
into sub-parts is to first perform semantic role label-
ing (SRL), that is, identify triggers and predict ar-
gument mentions with their semantic role, and then
extract event-event relations between pairs of event
mentions. In this paper, we focus on the second
step, where given a set of event triggers T , we find
all event-event relations, where a trigger represents
the entire event. For completeness, we now describe
the semantic roles L used in our dataset, and then
</bodyText>
<footnote confidence="0.8351475">
1Argument mentions are also related by coreference rela-
tions, but we neglect that since it is not central in this paper.
</footnote>
<page confidence="0.993237">
1711
</page>
<bodyText confidence="0.999570083333333">
present the set of event-event relations R.
The set L contains standard semantic roles such as
AGENT, THEME, ORIGIN, DESTINATION and LO-
CATION. Two additional semantic roles were em-
ployed that are relevant for biological text: RESULT
corresponds to an entity that is the result of an event,
and RAW-MATERIAL describes an entity that is used
or consumed during an event. For example, the last
event ‘produce’ in Figure 1, has ‘ATP’ as the RE-
SULT, and ‘ADP’ as the RAW-MATERIAL.
The event-event relation set R contains the fol-
lowing (assuming a labeled edge (u, v, r)):
</bodyText>
<listItem confidence="0.5314755">
1. PREV denotes that u is an event immediately
before v. Thus, the edges (u, v, PREV) and
</listItem>
<bodyText confidence="0.956568">
(v, w, PREV), preclude the edge (u, w, PREV).
For example, in “When a photon strikes
... energy is passed ... until it reaches ... ”,
there is no edge (strikes, reaches, PREV) due
to the intervening event ‘passed’.
</bodyText>
<listItem confidence="0.65319725">
2. COTEMP denotes that events u and v overlap in
time (e.g., the first two event mentions flowing
and enter in Figure 1).
3. SUPER denotes that event u includes event
</listItem>
<bodyText confidence="0.785062">
v. For instance, in “During DNA replica-
tion, DNA polymerases proofread each nu-
cleotide... ” there is an edge (DNA replication,
proofread, SUPER).
4. CAUSES denotes that event u causes event v
(e.g., the relation between changing and spins
in sentence 2 of Figure 1).
5. ENABLES denotes that event u creates precon-
ditions that allow event v to take place. For
example, the description “... cause cancer cells
to lose attachments to neighboring cells... , al-
lowing them to spread into nearby tissues” has
the edge (lose, spread, ENABLES). An in-
tuitive way to think about the difference be-
tween Causes and Enables is the following: if
u causes v this means that if u happens, then
v happens. If u enables v, then if u does not
happen, then v does not happen.
6. SAME denotes that u and v both refer to the
same event (spins and Spinning in Figure 1).
Early work on temporal logic (Allen, 1983) con-
tained more temporal relations than are used in our
</bodyText>
<table confidence="0.994077">
Avg Min Max
# of sentences 3.80 1 15
# of tokens 89.98 19 319
# of events 6.20 2 15
# of non-NONE relations 5.64 1 24
</table>
<tableCaption confidence="0.9897045">
Table 1: Process statistics over 148 process descriptions.
NONE is used to indicate no relation.
</tableCaption>
<bodyText confidence="0.999966358974359">
relation set R. We chose a relation set R that cap-
tures the essential aspects of temporal relations be-
tween events in a process, while keeping the annota-
tion as simple as possible. For instance, we include
the SUPER relation that appears in temporal anno-
tations such as the Timebank corpus (Pustejovsky
et al., 2003) and Allen’s work, but in practice was
not considered by many temporal ordering systems
(Chambers and Jurafsky, 2008; Yoshikawa et al.,
2009; Do et al., 2012). Importantly, our relation set
also includes the relations CAUSES and ENABLES,
which are fundamental to modeling processes and
go beyond simple temporal ordering.
We also added event coreference (SAME) to R.
Do et al. (2012) used event coreference information
in a temporal ordering task to modify probabilities
provided by pairwise classifiers prior to joint infer-
ence. In this paper, we simply treat SAME as an-
other event-event relation, which allows us to easily
perform joint inference and employ structural con-
straints that combine both coreference and temporal
relations simultaneously. For example, if u and v are
the same event, then there can exist no w, such that
u is before w, but v is after w (see Section 3.3)
We annotated 148 process descriptions based on
the aforementioned definitions. Further details on
annotation and data set statistics are provided in Sec-
tion 4 and Table 1.
Structural properties of processes Coherent pro-
cesses exhibit many structural properties. For ex-
ample, two argument mentions related to the same
event cannot overlap – a constraint that has been
used in the past in SRL (Toutanova et al., 2008). In
this paper we focus on three main structural prop-
erties of the graph P. First, in a coherent pro-
cess, all events mentioned are related to one another,
and hence the graph P must be connected. Sec-
ond, processes tend to have a “chain-like” structure
where one event follows another, and thus we expect
</bodyText>
<page confidence="0.969467">
1712
</page>
<table confidence="0.998693666666667">
Deg. Gold Local Global
0 0 29 0
1 219 274 224
2 369 337 408
3 46 14 17
&gt;4 22 2 7
</table>
<tableCaption confidence="0.977290333333333">
Table 2: Node degree distribution for event mentions on
the training set. Predictions for the Local and Global
models were obtained using 10-fold cross validation.
</tableCaption>
<bodyText confidence="0.999432555555555">
nodes’ degree to generally be &lt; 2. Indeed, 90% of
event mentions have degree &lt; 2, as demonstrated
by the Gold column of Table 2. Last, if we consider
relations between all possible triples of events in a
process, clearly some configurations are impossible,
while others are common (illustrated in Figure 2).
In Section 3.3, we show that modeling these proper-
ties using a joint inference framework improves the
quality of process extraction significantly.
</bodyText>
<sectionHeader confidence="0.987924" genericHeader="method">
3 Joint Model for Process Extraction
</sectionHeader>
<bodyText confidence="0.9999775">
Given a paragraph x and a trigger set T, we wish
to extract all event-event relations E. Similar to Do
et al. (2012), our model consists of a local pairwise
classifier and global constraints. We first introduce
a classifier that is based on features from previous
work. Next, we describe novel features specific for
process extraction. Last, we incorporate global con-
straints into our model using an ILP formulation.
</bodyText>
<subsectionHeader confidence="0.999781">
3.1 Local pairwise classifier
</subsectionHeader>
<bodyText confidence="0.995927125">
The local pairwise classifier predicts relations be-
tween all event mention pairs. In order to model
the direction of relations, we expand the set R to
include the reverse of four directed relations: PREV-
NEXT, SUPER- SUB, CAUSES-CAUSED, ENABLES-
ENABLED. After adding NONE to indicate no rela-
tion, and including the undirected relations COTEMP
and SAME, R contains 11 relations. The classifier is
hence a function f : T x T R. As an example,
f(ti, tj) = PREV iff f(tj, ti) = NEXT. Let n be the
number of triggers in a process, and ti be the i-th
trigger in its description. Since f(ti, tj) completely
determines f(tj, ti), it suffices to consider only pairs
with i &lt; j. Note that the process graph P is undi-
rected under the new definition of R.
Table 3 describes features from previous
</bodyText>
<table confidence="0.997499230769231">
Feature Description
POS Pair of POS tags
Lemma Pair of lemmas
Prep* Preposition lexeme, if in a prepositional phrase
Sent. count Quantized number of sentences between triggers
Word count Quantized number of words between triggers
LCA Least common ancestor on constituency tree, if exists
Dominates* Whether one trigger dominates other
Share Whether triggers share a child on dependency tree
Adjacency Whether two triggers are adjacent
Words btw. For adjacent triggers, content words between triggers
Temp. btw. For adjacent triggers, temporal connectives (from a
small list) between triggers
</table>
<tableCaption confidence="0.990642333333333">
Table 3: Features extracted for a trigger pair (ti, tj). As-
teriks (*) indicate features that are duplicated, once for
each trigger.
</tableCaption>
<bodyText confidence="0.999488545454545">
work (Chambers and Jurafsky, 2008; Do et al.,
2012) extracted for a trigger pair (ti, tj). Some
features were omitted since they did not yield
improvement in performance on a development set
(e.g., lemmas and part-of-speech tags of context
words surrounding ti and tj), or they require gold
annotations provided in TimeBank, which we do
not have (e.g., tense and aspect of triggers). To
reduce sparseness, we convert nominalizations into
their verbal forms when computing word lemmas,
using WordNet’s (Fellbaum, 1998) derivation links.
</bodyText>
<subsectionHeader confidence="0.998827">
3.2 Classifier extensions
</subsectionHeader>
<bodyText confidence="0.999890833333333">
A central source of information to extract event-
event relations from text are connectives such as af-
ter, during, etc. However, there is variability in the
occurrence of these connectives as demonstrated by
the following two sentences (connectives in bold-
face, triggers in italics):
</bodyText>
<listItem confidence="0.99210625">
1. Because alleles are exchanged during gene flow, ge-
netic differences are reduced.
2. During gene flow, alleles are exchanged, and genetic
differences are hence reduced.
</listItem>
<bodyText confidence="0.999899888888889">
Even though both sentences express the same re-
lation (exchanged, reduced, CAUSES), the connec-
tives used and their linear position with respect to the
triggers are different. Also, in sentence 1, gene flow
intervenes between exchanged and reduced. Since
our dataset is small, we wish to identify the trig-
gers related to each connective, and share features
between such sentences. We do this using the syn-
tactic structure and by clustering the connectives.
</bodyText>
<page confidence="0.918551">
1713
</page>
<figure confidence="0.999544454545454">
tj
tj
tj
tj
tj
SAME SAME
CAUSES COTEMP
COTEMP
COTEMP
PREV PREV
PREV PREV
SAME
ti tk
CAUSES
ti tk
COTEMP / SAME
ti tk
SAME
ti tk
PREV
ti tk
(a) SAME transitivity (b) CAUSE-COTEMP (c) COTEMP transitivity (d) SAME contradiction (e) PREV contradiction
</figure>
<figureCaption confidence="0.999745">
Figure 2: Relation triangles (a)-(c) are common in the gold standard while (d)-(e) are impossible.
</figureCaption>
<bodyText confidence="0.986959446428571">
Sentence 1 presents a typical case where by walk-
ing up the dependency tree from the marker because,
we can find the triggers related by this marker:
because mark
���� exchangedadvcl
���� reduced. When-
ever a trigger is the head of an adverbial clause and
marked by a mark dependency label, we walk on the
dependency tree and look for a trigger in the main
clause that is closest to the root (or the root itself
in this example). By utilizing the syntactic struc-
ture, we can correctly spot that the trigger gene flow
is not related to the trigger exchanged through the
connective because, even though they are linearly
closer. In order to reduce sparseness of connectives,
we created a hand-made clustering of 30 connectives
that maps words into clusters2 (e.g., because, since
and hence to a “causality” cluster). After locating
the relevant pair of triggers, we use these clusters
to fire the same feature for connectives belonging to
the same cluster. We perform a similar procedure
whenever a trigger is part of a prepositional phrase
(imagine sentence 1 starting with “due to allele ex-
change during gene flow ... ”) by walking up the
constituency tree, but details are omitted for brevity.
In sentence 2, the connective hence is an adverbial
modifier of the trigger reduced. We look up the clus-
ter for the connective hence and fire the same feature
for the adjacent triggers exchanged and reduced.
We further extend our features to handle the rich
relation set necessary for process extraction. The
first event of a process is often expressed as a nom-
inalization and includes subsequent events (SUPER
relation), e.g., “The Calvin cycle begins by incor-
porating...”. To capture this, we add a feature that
fires when the first event of the process description
is a noun. We also add two features targeted at the
2The full set of connectives and their clustering are provided
as part of our publicly released package.
SAME relation: one indicating if the lemmas of ti
and tj are the same, and another specifying the de-
terminer of tj, if it exists. Certain determiners in-
dicate that an event trigger has already been men-
tioned, e.g., the determiner this hints a SAME rela-
tion in “The next steps decompose citrate back to
oxaloacetate. This regeneration makes ... ”. Last,
we add as a feature the dependency path between ti
and tj, if it exists, e.g., in “meiosis produces cells
that divide ... ”, the feature dobj ) rcmod ) is fired for
the trigger pair produces and divide. In Section 4.1
we empirically show that our extensions to the local
classifier substantially improve performance.
For our pairwise classifier, we train a maximum
entropy classifier that computes a probability pijr
for every trigger pair (ti, tj) and relation r. Hence,
f(ti, tj) = aYg maxr pijr.
</bodyText>
<subsectionHeader confidence="0.991628">
3.3 Global Constraints
</subsectionHeader>
<bodyText confidence="0.999939357142857">
Naturally, pairwise classifiers are local models that
can violate global properties in the process structure.
Figure 3 (left) presents an example for predictions
made by the pairwise classifier, which result in two
triggers (deleted and dupcliated) that are isolated
from the rest of the triggers. In this section, we dis-
cuss how we incorporate constraints into our model
to generate coherent global process structures.
Let Bijr be the score for a relation r between the
trigger pair (ti, tj) (e.g., Bijr = log pijr), and yijr be
the corresponding indicator variable. Our goal is to
find an assignment for the indicators y = {yijr  |1 &lt;
i &lt; j &lt; n, r E R}. With no global constraints this
can be formulated as the following ILP:
</bodyText>
<page confidence="0.643154">
1714
</page>
<equation confidence="0.99095575">
arg max X Bijryijr (1)
Y ijr
Xs.t.∀i,j yijr = 1
r
</equation>
<bodyText confidence="0.999735863636364">
where the constraint ensures exactly one relation be-
tween each event pair. We now describe constraints
that result in a coherent global process structure.
Connectivity Our ILP formulation for enforcing
connectivity is a minor variation of the one sug-
gested by Martins et al. (2009) for dependency pars-
ing. In our setup, we want P to be a connected undi-
rected graph, and not a directed tree. However, an
undirected graph P is connected iff there exists a
directed tree that is a subgraph of P when edge di-
rections are ignored. Thus the resulting formulation
is almost identical and is based on flow constraints
which ensure that there is a path from a designated
root in the graph to all other nodes.
Let R be the set R \ NONE. An edge (ti, tj) is
in E iff there is some non-NONE relation between
ti and tj, i.e. iff yij := PrE R yijr is equal to 1.
For each variable yij we define two auxiliary binary
variables zij and zji that correspond to edges of the
directed tree that is a subgraph of P. We ensure that
the edges in the tree exist also in P by tying each
auxiliary variable to its corresponding ILP variable:
</bodyText>
<equation confidence="0.610263">
∀i&lt;j zij ≤ yij, zji ≤ yij (2)
</equation>
<bodyText confidence="0.9999435">
Next, we add constraints that ensure that the graph
structure induced by the auxiliary variables is a tree
rooted in an arbitrary node 1 (The choice of root
does not affect connectivity). We add for every i =6 j
a flow variable Oij which specifies the amount of
flow on the directed edge zij.
</bodyText>
<equation confidence="0.998403571428572">
X Xzi1 = 0, ∀j=,41 zij = 1 (3)
i i
X O1i = n − 1 (4)
i
∀j�=1 X XOij − Ojk = 1 (5)
i k
∀i�j Oij ≤ n · zij (6)
</equation>
<bodyText confidence="0.9994914375">
Equation 3 says that all nodes in the graph have
exactly one parent, except for the root that has no
parents. Equation 4 ensures that the outgoing flow
from the root is n−1, and Equation 5 states that each
of the other n − 1 nodes consume exactly one unit
of flow. Last, Equation 6 ties the auxiliary variables
to the flow variables, making sure that flow occurs
only on edges. The combination of these constraints
guarantees that the graph induced by the variables
zij is a directed tree and consequently the graph in-
duced by the objective variables y is connected.
Chain structure A chain is a connected graph
where the degree of all nodes is ≤ 2. Table 2
presents nodes’ degree and demonstrates that indeed
process graphs are close to being chains. The fol-
lowing constraint bounds nodes’ degree by 2:
</bodyText>
<equation confidence="0.635197">
yjk ≤ 2) (7)
</equation>
<bodyText confidence="0.999776384615385">
Since graph structures are not always chains, we
add this as a soft constraint, that is, we penalize the
objective for each node with degree &gt; 2. The chain
structure is one of the several soft constraints we
enforce. Thus, our modified objective function is
Pijr Bijryijr + PkE)c αkCk, where K is the set of
soft constraints, αk is the penalty (or reward for de-
sirable structures), and Ck indicates whether a con-
straint is violated (or satisfied). Note that under this
formulation our model is simply a constrained con-
ditional model (wei Chang et al., 2012). The param-
eters αk are tuned on a development set (see Sec-
tion 4).
Relation triads A relation triad (or a re-
lation triangle) for any three triggers ti, tj
and tk in a process is a 3-tuple of relations
(f(ti, tj), f(tj, tk), f(ti, tk)). Clearly, some triads
are impossible while others are quite common. To
find triads that could improve process extraction, the
frequency of all possible triads in both the training
set and the output of the pairwise classifier were
found, and we focused on those for which the clas-
sifier and the gold standard disagree. We are inter-
ested in triads that never occur in training data but
are predicted by the classifier, and vice versa. Fig-
ure 2 illustrates some of the triads found and Equa-
</bodyText>
<equation confidence="0.932315333333333">
X X
∀j( yij +
i&lt;j j&lt;k
</equation>
<page confidence="0.949877">
1715
</page>
<bodyText confidence="0.999747166666667">
tions 8-12 provide the corresponding ILP formula-
tions. Equations 8-10 were formulated as soft con-
straints (expanding the set )Q and were incorporated
by defining a reward αk for each triad type.3 On
the other hand, Equations 11-12 were formulated as
hard constraints to prevent certain structures.
</bodyText>
<listItem confidence="0.985820448275862">
1. SAME transitivity (Figure 2a, Eqn. 8): Co-
reference transitivity has been used in past
work (Finkel and Manning, 2008) and we in-
corporate it by a constraint that encourages tri-
ads that respect transitivity.
2. CAUSE-COTEMP (Figure 2b, Eqn. 9): If ti
causes both tj and tk, then often tj and tk are
co-temporal. E.g, in “genetic drift has led to
a loss of genetic variation and an increase in
the frequency of ...”, a single event causes two
subsequent events that occur simultaneously.
3. COTEMP transitivity (Figure 2c, Eqn. 10): If
ti is co-temporal with tj and tj is co-temporal
with tk, then usually ti and tk are either co-
temporal or denote the same event.
4. SAME contradiction (Figure 2d, Eqn. 11): If
ti is the same event as tk, then their tempo-
ral ordering with respect to a third trigger tj
may result in a contradiction, e.g., if tj is af-
ter ti, but before tk. We define 5 temporal
categories that generate (2) possible contradic-
tions, but for brevity present just one represen-
tative hard constraint. This constraint depends
on prediction of temporal and co-reference re-
lations jointly.
5. PREV contradiction (Figure 2e, Eqn. 12): As
mentioned (Section 3.3), if ti is immediately
before tj, and tj is immediately before tk, then
ti cannot be immediately before tk.
</listItem>
<equation confidence="0.998064">
yijSAME + yjkSAME + yikSAME 3
yijCAUSES + yikCAUSES + yjkCOTEMP 3
yijCOTEMP + yjkCOTEMP + yikCOTEMP+
yikSAME 3
yijPREV + yjkPREV + yikSAME 2
yijPREV + yjkPREV − yikNONE � 1
</equation>
<bodyText confidence="0.98560475">
3We experimented with a reward for certain triads or a
penalty for others and empirically found that using rewards re-
sults in better performance on the development set.
We used the Gurobi optimization package4 to
find an exact solution for our ILP, which contains
O(n2|R|) variables and O(n3) constraints. We also
developed an equivalent formulation amenable to
dual decomposition (Sontag et al., 2011), which is a
faster approximation method. But practically, solv-
ing the ILP exactly with Gurobi was quite fast (av-
erage/median time per process: 0.294 sec/0.152 sec
on a standard laptop).
</bodyText>
<sectionHeader confidence="0.998014" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999926885714286">
We extracted 148 process descriptions by going
through chapters from the textbook ”Biology” and
marking any contiguous sequence of sentences that
describes a process, i.e., a series of events that lead
towards some objective. Then, each process descrip-
tion was annotated by a biologist. The annotator was
first presented with annotation guidelines and anno-
tated 20 descriptions. The annotations were then
discussed with the authors, after which all process
descriptions were annotated. After training a sec-
ond biologist, we measured inter-annotator agree-
ment n = 0.69, on 30 random process descriptions.
Process descriptions were parsed with Stanford
constituency and dependency parsers (Klein and
Manning, 2003; de Marneffe et al., 2006), and 35
process descriptions were set aside as a test set
(number of training set trigger pairs: 1932, number
of test set trigger pairs: 906). We performed 10-
fold cross validation over the training set for feature
selection and tuning of constraint parameters. For
each constraint type (connectivity, chain-structure,
and five triad constraints) we introduced a param-
eter and tuned the seven parameters by coordinate-
wise ascent, where for hard constraints a binary pa-
rameter controls whether the constraint is used, and
for soft constraints we attempted 10 different re-
ward/penalty values. For our global model we de-
fined Bijr = logpijr, where pijr is the probability at
edge (ti, tj) for label r, given by the pairwise clas-
sifier.
We test the following systems: (a) All-Prev: Since
the most common process structure was chain-like,
we simply predict PREV for every two adjacent trig-
gers in text. (b) Localbase: A pairwise classifier with
features from previous work (Section 3.1) (c) Local:
</bodyText>
<footnote confidence="0.994432">
4www.gurobi.com
</footnote>
<page confidence="0.935868">
1716
</page>
<table confidence="0.999966428571429">
P Temporal P Full F1
R F1 R
All-Prev 58.4 54.8 56.6 34.1 32.0 33.0
Localbase 61.5 51.8 56.2 52.1 43.9 47.6
Local 63.2 55.7† 59.2 54.7 48.3† 51.3
Chain 64.5 60.5†‡ 62.4† 56.1 52.6†‡ 54.3†
Global 63.9 61.4†‡ 62.6†‡ 56.2 54.0†‡ 55.0†‡
</table>
<tableCaption confidence="0.87812225">
Table 4: Test set results on all experiments. Best number
in each column is bolded. † and $ denote statistical signif-
icance (p &lt; 0.01) against Localbase and Local baselines,
respectively.
</tableCaption>
<bodyText confidence="0.991290086956522">
A pairwise classifier with all features (Section 3.2)
(d) Chain: For every two adjacent triggers, choose
the non-NONE relation with highest probability ac-
cording to Local. This baseline heuristically com-
bines our structural assumptions with the pairwise
classifier. We deterministically choose a connected
chain structure, and then use the classifier to label
the edges. (e) Global: Our full model that uses ILP
inference.
To evaluate system performance we compare the
set of predictions on all trigger pairs to the gold stan-
dard annotations and compute micro-averaged pre-
cision, recall and F1. We perform two types of eval-
uations: (a) Full: evaluation on our full set of 11
relations (b) Temporal: Evaluation on temporal re-
lations only, by collapsing PREY, CAUSES, and EN-
ABLES to a single category and similarly for NEXT,
CAUSED, and ENABLED (inter-annotator agreement
n = 0.75). We computed statistical significance
of our results with the paired bootstrap resampling
method of 2000 iterations (Efron and Tibshirani,
1993), where the units resampled are trigger-trigger-
relation triples.
</bodyText>
<subsectionHeader confidence="0.7849">
4.1 Results
</subsectionHeader>
<bodyText confidence="0.99993113559322">
Table 4 presents performance of all systems. We see
that using global constraints improves performance
almost invariably on all measures in both full and
temporal evaluations. Particularly, in the full eval-
uation Global improves recall by 12% and overall
F1 improves significantly by 3.7 points against Lo-
cal (p &lt; 0.01). Recall improvement suggests that
modeling connectivity allowed Global to add cor-
rect relations in cases where some events were not
connected to one another.
The Local classifier substantially outperforms
Localbase. This indicates that our novel features
(Section 3.2) are important for discriminating be-
tween process relations. Specifically, in the full eval-
uation Local improves precision more than in the
temporal evaluation, suggesting that designing syn-
tactic and semantic features for connectives is useful
for distinguishing PREY, CAUSES, and ENABLES
when the amount of training data is small.
The Chain baseline performs only slightly worse
than our global model. This demonstrates the strong
tendency of processes to proceed linearly from one
event to the other, which is a known property of dis-
course structure (Schegloff and Sacks, 1973). How-
ever, since the structure is deterministically fixed,
Chain is highly inflexible and does not allow any
extensions or incorporation of other structural con-
straints or domain knowledge. Thus, it can be used
as a simple and efficient approximation but is not a
good candidate for a real system. Further support
for the linear nature of process structure is provided
by the All-Prev baseline, which performs poorly in
the full evaluation, but in temporal evaluation works
reasonably well.
Table 2 presents the degree distribution of Local
and Global on the development set comparing to the
gold standard. The degree distribution of Global is
more similar to the gold standard than Local. In par-
ticular, the connectivity constraint ensures that there
are no isolated nodes and shifts mass from nodes
with degree 0 and 1 to nodes with degree 2.
Table 5 presents the order in which constraints
were introduced into the global model using coor-
dinate ascent on the development set. Connectivity
is the first constraint to be introduced, and improves
performance considerably. The chain constraint, on
the other hand, is included third and the improve-
ment in F1 score is relatively smaller. This can be
explained by the distribution of degrees in Table 2
which shows that the predictions of Local does not
have many nodes with degree &gt; 2. As for triad con-
straints, we see that four constraints are important
and are included in the model, but one is discarded.
Last, we examined the results of Global when
macro-averaging over processes, i.e., assigning each
process the same weight by computing recall, pre-
cision and F1 for each process and averaging those
scores. We found that results are quite similar
(with a slight improvement): in the full evalua-
</bodyText>
<page confidence="0.979928">
1717
</page>
<table confidence="0.9826535">
Order Parameter name Value (α) Fl score
– Local model – 49.9
1 Connectivity constraint cc 51.2
2 SAME transitivity 0.5 52.9
3 Chain constraint -0.5 53.3
4 CAUSE-COTEMP 1.0 53.7
6 PREV contradiction cc 53.8
7 SAME contradiction cc 53.9
</table>
<tableCaption confidence="0.838209833333333">
Table 5: Order by which constraint parameters were set
using coordinate ascent on the development set. For each
parameter, the value chosen and Fl score after including
the constraint are provided. Negative values correspond
to penalties, positive values to rewards, and a value of oo
indicates a hard constraint.
</tableCaption>
<bodyText confidence="0.999204">
tion Global obtains R/P/F1 of 56.4/55.0/55.7, and
in the temporal evaluation Global obtains R/P/F1 of
63.8/62.3/63.1.
</bodyText>
<subsectionHeader confidence="0.997077">
4.2 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.999772261904762">
Figure 3 shows two examples where global con-
straints corrected the predictions of Local. In Fig-
ure 3, left, Local failed to predict the causal rela-
tions skipped-deleted and used-duplicated, possibly
because they are not in the same sentence and are not
adjacent to one another. By enforcing the connectiv-
ity constraint, Global correctly adds the correct re-
lations and connects deleted and duplicated to the
other triggers in the process.
In Figure 3, right, Local predicts a structure that
results in a “SAME contradiction” structure. The
triggers bind and binds cannot denote the same event
if a third trigger secrete is temporally between them.
However, Local predicts they are the same event, as
they share a lemma. Global prohibits this structure
and correctly predicts the relation as NONE.
To better understand the performance of Local,
we analyzed the confusion matrix generated based
on its predictions. Although this is a challenging
11-class classification task, most of the mass is con-
centrated on the matrix diagonal, as desired. Error
analysis reveals that 17.5% of all errors are con-
fusions between NONE and PREV, 11.1% between
PREV and CAUSES, and 8.6% between PREV and
COTEMP. This demonstrates that distinguishing the
classes PREV, CAUSES and COTEMP is challenging
for Local. Our current global constraints do not ad-
dress this type of error, and thus an important direc-
tion for future work is to improve the local model.
The global model depends on the predictions of
the local classifier, and so enforcing global con-
straints does not guarantee improvement in perfor-
mance. For instance, if Local produces a graph that
is disconnected (e.g., deleted in Figure 3, left), then
Global will add an edge. However, the label of the
edge is determined by scores computed based on
the local classifier, and if this prediction is wrong,
we will now be penalized for both the false nega-
tive of the correct class (just as before), and also for
the false positive of the predicted class. Despite that
we see that Global improves overall performance by
3.7 F1 points on the test set.
</bodyText>
<sectionHeader confidence="0.999968" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99995065625">
A related line of work is biomedical event extrac-
tion in recent BioNLP shared tasks (Kim et al.,
2009; Kim et al., 2011). Earlier work employed a
pipeline architecture where first events are found,
and then their arguments are identified (Miwa et al.,
2010; Bj¨orne et al., 2011). Subsequent methods pre-
dicted events and arguments jointly using Markov
logic (Poon and Vanderwende, 2010) and depen-
dency parsing algorithms (McClosky et al., 2011).
Riedel and McCallum (2011) further improved per-
formance by capturing correlations between events
and enforcing consistency across arguments.
Temporal event-event relations have been ex-
tensively studied (Chambers and Jurafsky, 2008;
Yoshikawa et al., 2009; Denis and Muller, 2011;
Do et al., 2012; McClosky and Manning, 2012;
D’Souza and Ng, 2013), and we leverage such
techniques in our work (Section 3.1). However,
we extend beyond temporal relations alone, and
strongly rely on dependencies between process
events. Chambers and Jurafsky (2011) learned event
templates (or frames), where events that are related
to one another and their semantic roles are extracted.
Recently, Cheung et al. (2013) proposed an unsuper-
vised generative model for inducing such templates.
A major difference in our work is that we do not
learn typical event relations from a large and redun-
dant corpus, but are given a paragraph and have a
“one-shot” chance to extract the process structure.
We showed in this paper that global structural
properties lead to significant improvements in ex-
traction accuracy, and ILP is an effective framework
</bodyText>
<page confidence="0.933305">
1718
</page>
<figure confidence="0.999702318181818">
shifts
CAUSES
CAUSES
CAUSES
CAUSES
skipped
used
CAUSES
CAUSES
CAUSES
CAUSES
duplicated
deleted
bind
COTEMP
PREV
secrete
SAME
NONE
ENABLES
PREV
binds
</figure>
<figureCaption confidence="0.872222">
Figure 3: Process graph fragments. Black edges (dotted) are predictions of Local, green (solid) are predictions of
Global, and gold (dashed) are gold standard edges. To reduce clutter, we present the predictions of Global only when
it disagrees with Local. In all other cases, the predictions of Global and Local are identical. Original text, Left: “... the
template shifts ..., and a part of the template strand is either skipped by the replication machinery or used twice as a
template. As a result, a segment of DNA is deleted or duplicated.” Right: “Cells of mating type A secrete a signaling
molecule, which can bind to specific receptor proteins on nearby cells. At the same time, cells secrete factor, which
binds to receptors on A cells.”
</figureCaption>
<bodyText confidence="0.9999345">
for modeling global constraints. Similar observa-
tions and techniques have been proposed in other
information extraction tasks. Reichart and Barzi-
lay (2012) tied information from multiple sequence
models that describe the same event by using global
higher-order potentials. Berant et al. (2011) pro-
posed a global inference algorithm to identify entail-
ment relations. There is an abundance of examples
of enforcing global constraints in other NLP tasks,
such as in coreference resolution (Finkel and Man-
ning, 2008), parsing (Rush et al., 2012) and named
entity recognition (Wang et al., 2013).
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999865459459459">
Developing systems that understand process de-
scriptions is an important step towards building ap-
plications that require deeper reasoning, such as bi-
ological process models from text, intelligent tutor-
ing systems, and non-factoid QA systems. In this
paper we have presented the task of process extrac-
tion, and developed methods for extracting relations
between process events. Processes contain events
that are tightly coupled through strong dependen-
cies. We have shown that exploiting these structural
dependencies and performing joint inference over all
event mentions can significantly improve accuracy
over several baselines. We have also released a new
dataset containing 148 fully annotated descriptions
of biological processes. Though the models we built
were trained on biological processes, they do not en-
code domain specific information, and hence should
be extensible to other domains.
In this paper we assumed that event triggers are
given as input. In future work, we want to perform
trigger identification jointly with extraction of event-
event relations. As explained in Section 4.2, the
performance of our system is confined by the per-
formance of the local classifier, which is trained on
relatively small amounts of data. Since data annota-
tion is expensive, it is important to improve the lo-
cal classifier without increasing the annotation bur-
den. For example, one can use unsupervised meth-
ods that learn narrative chains (Chambers and Ju-
rafsky, 2011) to provide some prior on the typical
order of events. Alternatively, we can search on the
web for redundant descriptions of the same process
and use this redundancy to improve classification.
Last, we would like to integrate our method into QA
systems and allow non-factoid questions that require
deeper reasoning to be answered by matching the
questions against the learned process structures.
</bodyText>
<sectionHeader confidence="0.998361" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999098">
The authors would like to thank Roi Reichart for
fruitful discussion and the anonymous reviewers for
their constructive feedback. This work was partially
funded by Vulcan Inc. The second author was spon-
sored by a Rothschild fellowship.
</bodyText>
<sectionHeader confidence="0.999478" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997986">
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Commun. ACM, 26(11):832–843.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Learning entailment relations by global graph
structure optimization. Journal of Computational Lin-
guistics, 38(1).
</reference>
<page confidence="0.883263">
1719
</page>
<reference confidence="0.999917895238095">
Jari Bj¨orne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2011. Extract-
ing contextualized complex biological events with rich
graph-based feature sets. Computational Intelligence,
27(4):541–557.
Neil Campbell and Jane Reece. 2005. Biology. Ben-
jamin Cummings.
Nathanael Chambers and Daniel Jurafsky. 2008. Jointly
combining implicit constraints improves temporal or-
dering. In Proceedings of EMNLP.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
ACL, pages 976–986.
Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
Proceedings of NAACL-HLT.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC.
Pascal Denis and Philippe Muller. 2011. Predicting
globally-coherent temporal structures from texts via
endpoint inference and graph decomposition. In Pro-
ceedings of IJCAI.
Quang Do, Wei Lu, and Dan Roth. 2012. Joint infer-
ence for event timeline construction. In Proceedings
of EMNLP-CoNLL.
Jennifer D’Souza and Vincent Ng. 2013. Classifying
temporal relations with rich linguistic knowledge. In
Proceedings of NAACL-HLT.
Bradley Efron and Robert Tibshirani. 1993. An introduc-
tion to the bootstrap, volume 57. CRC press.
Christiane Fellbaum, editor. 1998. WordNet: An elec-
tronic lexical database. MIT Press.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of ACL.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Junichi Tsujii. 2009. Overview of
BioNLP 09 shared task on event extraction. In Pro-
ceedings of BioNLP.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Junichi Tsujii. 2011. Overview of BioNLP
shared task 2011. In Proceedings of BioNLP.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL.
Andr´e L. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
ACL/IJCNLP.
David McClosky and Christopher D. Manning. 2012.
Learning constraints for consistent timeline extraction.
In Proceedings of EMNLP-CoNLL, pages 873–882.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011. Event extraction as dependency pars-
ing. In Proceedings ofACL, pages 1626–1635.
Makoto Miwa, Rune Sætre, Jin-Dong Kim, and Jun’ichi
Tsujii. 2010. Event extraction with complex event
classification using rich features. J. Bioinformatics
and Computational Biology, 8(1).
Hoifung Poon and Lucy Vanderwende. 2010. Joint in-
ference for knowledge extraction from biomedical lit-
erature. In Proceedings of HLT-NAACL.
James Pustejovsky, Jos´e M. Casta˜no, Robert Ingria,
Roser Sauri, Robert J. Gaizauskas, Andrea Setzer,
Graham Katz, and Dragomir R. Radev. 2003.
TimeML: Robust specification of event and temporal
expressions in text. In New Directions in Question An-
swering.
Roi Reichart and Regina Barzilay. 2012. Multi-event ex-
traction guided by global constraints. In Proceedings
of HLT-NAACL.
Sebastian Riedel and Andrew McCallum. 2011. Fast and
robust joint models for biomedical event extraction. In
Proceedings of EMNLP.
Alexander M. Rush, Roi Reichert, Michael Collins, and
Amir Globerson. 2012. Improved parsing and POS
tagging using inter-sentence consistency constraints.
In Proceedings of EMNLP.
Emanuel A Schegloff and Harvey Sacks. 1973. Opening
up closings. Semiotica, 8(4):289–327.
David Sontag, Amir Globerson, and Tommi Jaakkola.
2011. Introduction to dual decomposition for in-
ference. In Suvrit Sra, Sebastian Nowozin, and
Stephen J. Wright, editors, Optimization for Machine
Learning. MIT Press.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computa-
tional Linguistics, 37(2).
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2):161–
191.
Mengqiu Wang, Wanxiang Che, and Christopher D. Man-
ning. 2013. Effective bilingual constraints for semi-
supervised learning of named entity recognizers. In
Proceedings of AAAI.
Ming wei Chang, Lev Ratinov, and Dan Roth. 2012.
Structured learning with constrained conditional mod-
els. Machine Learning, 88(3):399–431, 6.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identifying
temporal relations with Markov logic. In Proceedings
of ACL/IJCNLP.
</reference>
<page confidence="0.989679">
1720
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.497931">
<title confidence="0.999933">Learning Biological Processes with Global Constraints</title>
<author confidence="0.982333">Thalappillil Wang</author>
<author confidence="0.982333">D Christopher</author>
<affiliation confidence="0.995147">Stanford University, Stanford</affiliation>
<author confidence="0.552775">Justin Lewis</author>
<author confidence="0.552775">Brittany Harding</author>
<affiliation confidence="0.969838">University of Washington, Seattle</affiliation>
<abstract confidence="0.997511666666667">Biological processes are complex phenomena involving a series of events that are related to one another through various relationships. Systems that can understand and reason over biological processes would dramatically improve the performance of semantic applications involving inference such as quesanswering (QA) – specifically In this paper, we the task of in which events within a process and the relations between the events are automatically extracted from text. We represent processes by graphs whose edges describe a set of temporal, causal and co-reference event-event relations, and characterize the structural properties of graphs (e.g., the graphs are Then, we present a method for extracting relations between the events, which exploits these structural properties by performing joint inference over the set of extracted relations. On a novel dataset containing 148 descriptions of biological processes (released with this paper), we show significant improvement comparing to baselines that disregard process structure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James F Allen</author>
</authors>
<title>Maintaining knowledge about temporal intervals.</title>
<date>1983</date>
<journal>Commun. ACM,</journal>
<volume>26</volume>
<issue>11</issue>
<contexts>
<context position="9948" citStr="Allen, 1983" startWordPosition="1632" endWordPosition="1633">denotes that event u creates preconditions that allow event v to take place. For example, the description “... cause cancer cells to lose attachments to neighboring cells... , allowing them to spread into nearby tissues” has the edge (lose, spread, ENABLES). An intuitive way to think about the difference between Causes and Enables is the following: if u causes v this means that if u happens, then v happens. If u enables v, then if u does not happen, then v does not happen. 6. SAME denotes that u and v both refer to the same event (spins and Spinning in Figure 1). Early work on temporal logic (Allen, 1983) contained more temporal relations than are used in our Avg Min Max # of sentences 3.80 1 15 # of tokens 89.98 19 319 # of events 6.20 2 15 # of non-NONE relations 5.64 1 24 Table 1: Process statistics over 148 process descriptions. NONE is used to indicate no relation. relation set R. We chose a relation set R that captures the essential aspects of temporal relations between events in a process, while keeping the annotation as simple as possible. For instance, we include the SUPER relation that appears in temporal annotations such as the Timebank corpus (Pustejovsky et al., 2003) and Allen’s </context>
</contexts>
<marker>Allen, 1983</marker>
<rawString>James F. Allen. 1983. Maintaining knowledge about temporal intervals. Commun. ACM, 26(11):832–843.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Learning entailment relations by global graph structure optimization.</title>
<date>2011</date>
<journal>Journal of Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="38021" citStr="Berant et al. (2011)" startWordPosition="6334" endWordPosition="6337">er skipped by the replication machinery or used twice as a template. As a result, a segment of DNA is deleted or duplicated.” Right: “Cells of mating type A secrete a signaling molecule, which can bind to specific receptor proteins on nearby cells. At the same time, cells secrete factor, which binds to receptors on A cells.” for modeling global constraints. Similar observations and techniques have been proposed in other information extraction tasks. Reichart and Barzilay (2012) tied information from multiple sequence models that describe the same event by using global higher-order potentials. Berant et al. (2011) proposed a global inference algorithm to identify entailment relations. There is an abundance of examples of enforcing global constraints in other NLP tasks, such as in coreference resolution (Finkel and Manning, 2008), parsing (Rush et al., 2012) and named entity recognition (Wang et al., 2013). 6 Conclusion Developing systems that understand process descriptions is an important step towards building applications that require deeper reasoning, such as biological process models from text, intelligent tutoring systems, and non-factoid QA systems. In this paper we have presented the task of pro</context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2011</marker>
<rawString>Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2011. Learning entailment relations by global graph structure optimization. Journal of Computational Linguistics, 38(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jari Bj¨orne</author>
<author>Juho Heimonen</author>
<author>Filip Ginter</author>
<author>Antti Airola</author>
<author>Tapio Pahikkala</author>
<author>Tapio Salakoski</author>
</authors>
<title>Extracting contextualized complex biological events with rich graph-based feature sets.</title>
<date>2011</date>
<journal>Computational Intelligence,</journal>
<volume>27</volume>
<issue>4</issue>
<marker>Bj¨orne, Heimonen, Ginter, Airola, Pahikkala, Salakoski, 2011</marker>
<rawString>Jari Bj¨orne, Juho Heimonen, Filip Ginter, Antti Airola, Tapio Pahikkala, and Tapio Salakoski. 2011. Extracting contextualized complex biological events with rich graph-based feature sets. Computational Intelligence, 27(4):541–557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neil Campbell</author>
<author>Jane Reece</author>
</authors>
<date>2005</date>
<location>Biology. Benjamin Cummings.</location>
<contexts>
<context position="5646" citStr="Campbell and Reece, 2005" startWordPosition="873" endWordPosition="876">vent result Entity Entity we characterize a set of global properties of process structure that can be utilized during process extraction. For example, all events in a process are somehow connected to one another. Also, processes usually exhibit a “chain-like” structure reflecting process progression over time. We show that incorporating such global properties into our model and performing joint inference over the extracted relations significantly improves the quality of process structures predicted. We conduct experiments on a novel dataset of process descriptions from the textbook “Biology” (Campbell and Reece, 2005) that were annotated by trained biologists. Our method does not require any domain-specific knowledge and can be easily adapted to non-biology domains. The main contributions of this paper are: 1. We define process extraction and characterize processes’ structural properties. 2. We model global structural properties in processes and demonstrate significant improvement in extraction accuracy. 3. We publicly release a novel data set of 148 fully annotated biological process descriptions along with the source code for our sys01:801/indexhtml#/exampes/emnlp213/p66 tem. The dataset and code can be </context>
</contexts>
<marker>Campbell, Reece, 2005</marker>
<rawString>Neil Campbell and Jane Reece. 2005. Biology. Benjamin Cummings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Jointly combining implicit constraints improves temporal ordering.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="10651" citStr="Chambers and Jurafsky, 2008" startWordPosition="1756" endWordPosition="1759">tences 3.80 1 15 # of tokens 89.98 19 319 # of events 6.20 2 15 # of non-NONE relations 5.64 1 24 Table 1: Process statistics over 148 process descriptions. NONE is used to indicate no relation. relation set R. We chose a relation set R that captures the essential aspects of temporal relations between events in a process, while keeping the annotation as simple as possible. For instance, we include the SUPER relation that appears in temporal annotations such as the Timebank corpus (Pustejovsky et al., 2003) and Allen’s work, but in practice was not considered by many temporal ordering systems (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012). Importantly, our relation set also includes the relations CAUSES and ENABLES, which are fundamental to modeling processes and go beyond simple temporal ordering. We also added event coreference (SAME) to R. Do et al. (2012) used event coreference information in a temporal ordering task to modify probabilities provided by pairwise classifiers prior to joint inference. In this paper, we simply treat SAME as another event-event relation, which allows us to easily perform joint inference and employ structural constraints that combine both coreference and</context>
<context position="14867" citStr="Chambers and Jurafsky, 2008" startWordPosition="2463" endWordPosition="2466">uantized number of sentences between triggers Word count Quantized number of words between triggers LCA Least common ancestor on constituency tree, if exists Dominates* Whether one trigger dominates other Share Whether triggers share a child on dependency tree Adjacency Whether two triggers are adjacent Words btw. For adjacent triggers, content words between triggers Temp. btw. For adjacent triggers, temporal connectives (from a small list) between triggers Table 3: Features extracted for a trigger pair (ti, tj). Asteriks (*) indicate features that are duplicated, once for each trigger. work (Chambers and Jurafsky, 2008; Do et al., 2012) extracted for a trigger pair (ti, tj). Some features were omitted since they did not yield improvement in performance on a development set (e.g., lemmas and part-of-speech tags of context words surrounding ti and tj), or they require gold annotations provided in TimeBank, which we do not have (e.g., tense and aspect of triggers). To reduce sparseness, we convert nominalizations into their verbal forms when computing word lemmas, using WordNet’s (Fellbaum, 1998) derivation links. 3.2 Classifier extensions A central source of information to extract eventevent relations from te</context>
<context position="35935" citStr="Chambers and Jurafsky, 2008" startWordPosition="6001" endWordPosition="6004">tion in recent BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011). Earlier work employed a pipeline architecture where first events are found, and then their arguments are identified (Miwa et al., 2010; Bj¨orne et al., 2011). Subsequent methods predicted events and arguments jointly using Markov logic (Poon and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky (2011) learned event templates (or frames), where events that are related to one another and their semantic roles are extracted. Recently, Cheung et al. (2013) proposed an unsupervised generative model for inducing such templates. A major difference in our work is that we do not learn typic</context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Daniel Jurafsky. 2008. Jointly combining implicit constraints improves temporal ordering. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Templatebased information extraction without the templates. In</title>
<date>2011</date>
<booktitle>ACL,</booktitle>
<pages>976--986</pages>
<contexts>
<context position="36250" citStr="Chambers and Jurafsky (2011)" startWordPosition="6050" endWordPosition="6053"> and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky (2011) learned event templates (or frames), where events that are related to one another and their semantic roles are extracted. Recently, Cheung et al. (2013) proposed an unsupervised generative model for inducing such templates. A major difference in our work is that we do not learn typical event relations from a large and redundant corpus, but are given a paragraph and have a “one-shot” chance to extract the process structure. We showed in this paper that global structural properties lead to significant improvements in extraction accuracy, and ILP is an effective framework 1718 shifts CAUSES CAUS</context>
<context position="39799" citStr="Chambers and Jurafsky, 2011" startWordPosition="6609" endWordPosition="6613">rmation, and hence should be extensible to other domains. In this paper we assumed that event triggers are given as input. In future work, we want to perform trigger identification jointly with extraction of eventevent relations. As explained in Section 4.2, the performance of our system is confined by the performance of the local classifier, which is trained on relatively small amounts of data. Since data annotation is expensive, it is important to improve the local classifier without increasing the annotation burden. For example, one can use unsupervised methods that learn narrative chains (Chambers and Jurafsky, 2011) to provide some prior on the typical order of events. Alternatively, we can search on the web for redundant descriptions of the same process and use this redundancy to improve classification. Last, we would like to integrate our method into QA systems and allow non-factoid questions that require deeper reasoning to be answered by matching the questions against the learned process structures. Acknowledgments The authors would like to thank Roi Reichart for fruitful discussion and the anonymous reviewers for their constructive feedback. This work was partially funded by Vulcan Inc. The second a</context>
</contexts>
<marker>Chambers, Jurafsky, 2011</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2011. Templatebased information extraction without the templates. In ACL, pages 976–986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackie Chi Kit Cheung</author>
<author>Hoifung Poon</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Probabilistic frame induction.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="36403" citStr="Cheung et al. (2013)" startWordPosition="6074" endWordPosition="6077">ons between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky (2011) learned event templates (or frames), where events that are related to one another and their semantic roles are extracted. Recently, Cheung et al. (2013) proposed an unsupervised generative model for inducing such templates. A major difference in our work is that we do not learn typical event relations from a large and redundant corpus, but are given a paragraph and have a “one-shot” chance to extract the process structure. We showed in this paper that global structural properties lead to significant improvements in extraction accuracy, and ILP is an effective framework 1718 shifts CAUSES CAUSES CAUSES CAUSES skipped used CAUSES CAUSES CAUSES CAUSES duplicated deleted bind COTEMP PREV secrete SAME NONE ENABLES PREV binds Figure 3: Process grap</context>
</contexts>
<marker>Cheung, Poon, Vanderwende, 2013</marker>
<rawString>Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Vanderwende. 2013. Probabilistic frame induction. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Philippe Muller</author>
</authors>
<title>Predicting globally-coherent temporal structures from texts via endpoint inference and graph decomposition.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="35983" citStr="Denis and Muller, 2011" startWordPosition="6009" endWordPosition="6012"> Kim et al., 2011). Earlier work employed a pipeline architecture where first events are found, and then their arguments are identified (Miwa et al., 2010; Bj¨orne et al., 2011). Subsequent methods predicted events and arguments jointly using Markov logic (Poon and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky (2011) learned event templates (or frames), where events that are related to one another and their semantic roles are extracted. Recently, Cheung et al. (2013) proposed an unsupervised generative model for inducing such templates. A major difference in our work is that we do not learn typical event relations from a large and redundant co</context>
</contexts>
<marker>Denis, Muller, 2011</marker>
<rawString>Pascal Denis and Philippe Muller. 2011. Predicting globally-coherent temporal structures from texts via endpoint inference and graph decomposition. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quang Do</author>
<author>Wei Lu</author>
<author>Dan Roth</author>
</authors>
<title>Joint inference for event timeline construction.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="3353" citStr="Do et al., 2012" startWordPosition="511" endWordPosition="514">n. Traditional event extraction focuses on identifying a closed set of events within a single sentence. For example, the BioNLP 2009 and 2011 shared tasks (Kim et al., 2009; Kim et al., 2011) consider nine events types related to proteins. In practice, events are currently almost always extracted from a single sentence. Process extraction, on the other hand, is centered around discovering relations between events that span multiple sentences. The set of possible event types in process extraction is also much larger. Timeline construction involves identifying temporal relations between events (Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and is thus related to process extraction as both focus on event-event relations spanning multiple sentences. However, events in processes are tightly coupled in ways that go beyond simple temporal ordering, and these dependencies are central for the process extraction task. Hence, capturing process structure requires modeling a larger set of relations that includes temporal, causal and co-reference relations. In this paper, we formally define the task of process extraction and present automatic extraction methods. Our approach handles an op</context>
<context position="10693" citStr="Do et al., 2012" startWordPosition="1764" endWordPosition="1767"> 6.20 2 15 # of non-NONE relations 5.64 1 24 Table 1: Process statistics over 148 process descriptions. NONE is used to indicate no relation. relation set R. We chose a relation set R that captures the essential aspects of temporal relations between events in a process, while keeping the annotation as simple as possible. For instance, we include the SUPER relation that appears in temporal annotations such as the Timebank corpus (Pustejovsky et al., 2003) and Allen’s work, but in practice was not considered by many temporal ordering systems (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012). Importantly, our relation set also includes the relations CAUSES and ENABLES, which are fundamental to modeling processes and go beyond simple temporal ordering. We also added event coreference (SAME) to R. Do et al. (2012) used event coreference information in a temporal ordering task to modify probabilities provided by pairwise classifiers prior to joint inference. In this paper, we simply treat SAME as another event-event relation, which allows us to easily perform joint inference and employ structural constraints that combine both coreference and temporal relations simultaneously. For ex</context>
<context position="12996" citStr="Do et al. (2012)" startWordPosition="2158" endWordPosition="2161">dation. nodes’ degree to generally be &lt; 2. Indeed, 90% of event mentions have degree &lt; 2, as demonstrated by the Gold column of Table 2. Last, if we consider relations between all possible triples of events in a process, clearly some configurations are impossible, while others are common (illustrated in Figure 2). In Section 3.3, we show that modeling these properties using a joint inference framework improves the quality of process extraction significantly. 3 Joint Model for Process Extraction Given a paragraph x and a trigger set T, we wish to extract all event-event relations E. Similar to Do et al. (2012), our model consists of a local pairwise classifier and global constraints. We first introduce a classifier that is based on features from previous work. Next, we describe novel features specific for process extraction. Last, we incorporate global constraints into our model using an ILP formulation. 3.1 Local pairwise classifier The local pairwise classifier predicts relations between all event mention pairs. In order to model the direction of relations, we expand the set R to include the reverse of four directed relations: PREVNEXT, SUPER- SUB, CAUSES-CAUSED, ENABLESENABLED. After adding NONE</context>
<context position="14885" citStr="Do et al., 2012" startWordPosition="2467" endWordPosition="2470">between triggers Word count Quantized number of words between triggers LCA Least common ancestor on constituency tree, if exists Dominates* Whether one trigger dominates other Share Whether triggers share a child on dependency tree Adjacency Whether two triggers are adjacent Words btw. For adjacent triggers, content words between triggers Temp. btw. For adjacent triggers, temporal connectives (from a small list) between triggers Table 3: Features extracted for a trigger pair (ti, tj). Asteriks (*) indicate features that are duplicated, once for each trigger. work (Chambers and Jurafsky, 2008; Do et al., 2012) extracted for a trigger pair (ti, tj). Some features were omitted since they did not yield improvement in performance on a development set (e.g., lemmas and part-of-speech tags of context words surrounding ti and tj), or they require gold annotations provided in TimeBank, which we do not have (e.g., tense and aspect of triggers). To reduce sparseness, we convert nominalizations into their verbal forms when computing word lemmas, using WordNet’s (Fellbaum, 1998) derivation links. 3.2 Classifier extensions A central source of information to extract eventevent relations from text are connectives</context>
<context position="36000" citStr="Do et al., 2012" startWordPosition="6013" endWordPosition="6016">ier work employed a pipeline architecture where first events are found, and then their arguments are identified (Miwa et al., 2010; Bj¨orne et al., 2011). Subsequent methods predicted events and arguments jointly using Markov logic (Poon and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky (2011) learned event templates (or frames), where events that are related to one another and their semantic roles are extracted. Recently, Cheung et al. (2013) proposed an unsupervised generative model for inducing such templates. A major difference in our work is that we do not learn typical event relations from a large and redundant corpus, but are giv</context>
</contexts>
<marker>Do, Lu, Roth, 2012</marker>
<rawString>Quang Do, Wei Lu, and Dan Roth. 2012. Joint inference for event timeline construction. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer D’Souza</author>
<author>Vincent Ng</author>
</authors>
<title>Classifying temporal relations with rich linguistic knowledge.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<marker>D’Souza, Ng, 2013</marker>
<rawString>Jennifer D’Souza and Vincent Ng. 2013. Classifying temporal relations with rich linguistic knowledge. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Robert Tibshirani</author>
</authors>
<title>An introduction to the bootstrap, volume 57.</title>
<date>1993</date>
<publisher>CRC press.</publisher>
<contexts>
<context position="29466" citStr="Efron and Tibshirani, 1993" startWordPosition="4961" endWordPosition="4964"> that uses ILP inference. To evaluate system performance we compare the set of predictions on all trigger pairs to the gold standard annotations and compute micro-averaged precision, recall and F1. We perform two types of evaluations: (a) Full: evaluation on our full set of 11 relations (b) Temporal: Evaluation on temporal relations only, by collapsing PREY, CAUSES, and ENABLES to a single category and similarly for NEXT, CAUSED, and ENABLED (inter-annotator agreement n = 0.75). We computed statistical significance of our results with the paired bootstrap resampling method of 2000 iterations (Efron and Tibshirani, 1993), where the units resampled are trigger-triggerrelation triples. 4.1 Results Table 4 presents performance of all systems. We see that using global constraints improves performance almost invariably on all measures in both full and temporal evaluations. Particularly, in the full evaluation Global improves recall by 12% and overall F1 improves significantly by 3.7 points against Local (p &lt; 0.01). Recall improvement suggests that modeling connectivity allowed Global to add correct relations in cases where some events were not connected to one another. The Local classifier substantially outperform</context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>Bradley Efron and Robert Tibshirani. 1993. An introduction to the bootstrap, volume 57. CRC press.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Enforcing transitivity in coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="24322" citStr="Finkel and Manning, 2008" startWordPosition="4124" endWordPosition="4127"> disagree. We are interested in triads that never occur in training data but are predicted by the classifier, and vice versa. Figure 2 illustrates some of the triads found and EquaX X ∀j( yij + i&lt;j j&lt;k 1715 tions 8-12 provide the corresponding ILP formulations. Equations 8-10 were formulated as soft constraints (expanding the set )Q and were incorporated by defining a reward αk for each triad type.3 On the other hand, Equations 11-12 were formulated as hard constraints to prevent certain structures. 1. SAME transitivity (Figure 2a, Eqn. 8): Coreference transitivity has been used in past work (Finkel and Manning, 2008) and we incorporate it by a constraint that encourages triads that respect transitivity. 2. CAUSE-COTEMP (Figure 2b, Eqn. 9): If ti causes both tj and tk, then often tj and tk are co-temporal. E.g, in “genetic drift has led to a loss of genetic variation and an increase in the frequency of ...”, a single event causes two subsequent events that occur simultaneously. 3. COTEMP transitivity (Figure 2c, Eqn. 10): If ti is co-temporal with tj and tj is co-temporal with tk, then usually ti and tk are either cotemporal or denote the same event. 4. SAME contradiction (Figure 2d, Eqn. 11): If ti is the</context>
<context position="38240" citStr="Finkel and Manning, 2008" startWordPosition="6368" endWordPosition="6372">receptor proteins on nearby cells. At the same time, cells secrete factor, which binds to receptors on A cells.” for modeling global constraints. Similar observations and techniques have been proposed in other information extraction tasks. Reichart and Barzilay (2012) tied information from multiple sequence models that describe the same event by using global higher-order potentials. Berant et al. (2011) proposed a global inference algorithm to identify entailment relations. There is an abundance of examples of enforcing global constraints in other NLP tasks, such as in coreference resolution (Finkel and Manning, 2008), parsing (Rush et al., 2012) and named entity recognition (Wang et al., 2013). 6 Conclusion Developing systems that understand process descriptions is an important step towards building applications that require deeper reasoning, such as biological process models from text, intelligent tutoring systems, and non-factoid QA systems. In this paper we have presented the task of process extraction, and developed methods for extracting relations between process events. Processes contain events that are tightly coupled through strong dependencies. We have shown that exploiting these structural depen</context>
</contexts>
<marker>Finkel, Manning, 2008</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2008. Enforcing transitivity in coreference resolution. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Sampo Pyysalo</author>
<author>Yoshinobu Kano</author>
<author>Junichi Tsujii</author>
</authors>
<title>shared task on event extraction.</title>
<date>2009</date>
<journal>Overview of BioNLP</journal>
<booktitle>In Proceedings of BioNLP.</booktitle>
<volume>09</volume>
<contexts>
<context position="2910" citStr="Kim et al., 2009" startWordPosition="442" endWordPosition="445">H+ ions contribute to the production of ATP?” requires a structure that links H+ ions (Figure 1, sentence 1) to ATP (Figure 1, sentence 4) through a sequence of intermediate events. Such “How?” questions are common on FAQ websites (Surdeanu et al., 2011), which further supports the importance of process extraction. Process extraction is related to two recent lines of work in Information Extraction – event extraction and timeline construction. Traditional event extraction focuses on identifying a closed set of events within a single sentence. For example, the BioNLP 2009 and 2011 shared tasks (Kim et al., 2009; Kim et al., 2011) consider nine events types related to proteins. In practice, events are currently almost always extracted from a single sentence. Process extraction, on the other hand, is centered around discovering relations between events that span multiple sentences. The set of possible event types in process extraction is also much larger. Timeline construction involves identifying temporal relations between events (Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and is thus related to process extraction as both focus on event-event relations spanning multiple sente</context>
<context position="35360" citStr="Kim et al., 2009" startWordPosition="5918" endWordPosition="5921">e. For instance, if Local produces a graph that is disconnected (e.g., deleted in Figure 3, left), then Global will add an edge. However, the label of the edge is determined by scores computed based on the local classifier, and if this prediction is wrong, we will now be penalized for both the false negative of the correct class (just as before), and also for the false positive of the predicted class. Despite that we see that Global improves overall performance by 3.7 F1 points on the test set. 5 Related Work A related line of work is biomedical event extraction in recent BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011). Earlier work employed a pipeline architecture where first events are found, and then their arguments are identified (Miwa et al., 2010; Bj¨orne et al., 2011). Subsequent methods predicted events and arguments jointly using Markov logic (Poon and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009;</context>
</contexts>
<marker>Kim, Ohta, Pyysalo, Kano, Tsujii, 2009</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Junichi Tsujii. 2009. Overview of BioNLP 09 shared task on event extraction. In Proceedings of BioNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Sampo Pyysalo</author>
<author>Tomoko Ohta</author>
<author>Robert Bossy</author>
<author>Junichi Tsujii</author>
</authors>
<title>Overview of BioNLP shared task</title>
<date>2011</date>
<booktitle>In Proceedings of BioNLP.</booktitle>
<contexts>
<context position="2929" citStr="Kim et al., 2011" startWordPosition="446" endWordPosition="449"> to the production of ATP?” requires a structure that links H+ ions (Figure 1, sentence 1) to ATP (Figure 1, sentence 4) through a sequence of intermediate events. Such “How?” questions are common on FAQ websites (Surdeanu et al., 2011), which further supports the importance of process extraction. Process extraction is related to two recent lines of work in Information Extraction – event extraction and timeline construction. Traditional event extraction focuses on identifying a closed set of events within a single sentence. For example, the BioNLP 2009 and 2011 shared tasks (Kim et al., 2009; Kim et al., 2011) consider nine events types related to proteins. In practice, events are currently almost always extracted from a single sentence. Process extraction, on the other hand, is centered around discovering relations between events that span multiple sentences. The set of possible event types in process extraction is also much larger. Timeline construction involves identifying temporal relations between events (Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and is thus related to process extraction as both focus on event-event relations spanning multiple sentences. However, even</context>
<context position="35379" citStr="Kim et al., 2011" startWordPosition="5922" endWordPosition="5925">f Local produces a graph that is disconnected (e.g., deleted in Figure 3, left), then Global will add an edge. However, the label of the edge is determined by scores computed based on the local classifier, and if this prediction is wrong, we will now be penalized for both the false negative of the correct class (just as before), and also for the false positive of the predicted class. Despite that we see that Global improves overall performance by 3.7 F1 points on the test set. 5 Related Work A related line of work is biomedical event extraction in recent BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011). Earlier work employed a pipeline architecture where first events are found, and then their arguments are identified (Miwa et al., 2010; Bj¨orne et al., 2011). Subsequent methods predicted events and arguments jointly using Markov logic (Poon and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, </context>
</contexts>
<marker>Kim, Pyysalo, Ohta, Bossy, Tsujii, 2011</marker>
<rawString>Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert Bossy, and Junichi Tsujii. 2011. Overview of BioNLP shared task 2011. In Proceedings of BioNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="26979" citStr="Klein and Manning, 2003" startWordPosition="4560" endWordPosition="4563">gy” and marking any contiguous sequence of sentences that describes a process, i.e., a series of events that lead towards some objective. Then, each process description was annotated by a biologist. The annotator was first presented with annotation guidelines and annotated 20 descriptions. The annotations were then discussed with the authors, after which all process descriptions were annotated. After training a second biologist, we measured inter-annotator agreement n = 0.69, on 30 random process descriptions. Process descriptions were parsed with Stanford constituency and dependency parsers (Klein and Manning, 2003; de Marneffe et al., 2006), and 35 process descriptions were set aside as a test set (number of training set trigger pairs: 1932, number of test set trigger pairs: 906). We performed 10- fold cross validation over the training set for feature selection and tuning of constraint parameters. For each constraint type (connectivity, chain-structure, and five triad constraints) we introduced a parameter and tuned the seven parameters by coordinatewise ascent, where for hard constraints a binary parameter controls whether the constraint is used, and for soft constraints we attempted 10 different rew</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e L Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL/IJCNLP.</booktitle>
<contexts>
<context position="20512" citStr="Martins et al. (2009)" startWordPosition="3411" endWordPosition="3414">for a relation r between the trigger pair (ti, tj) (e.g., Bijr = log pijr), and yijr be the corresponding indicator variable. Our goal is to find an assignment for the indicators y = {yijr |1 &lt; i &lt; j &lt; n, r E R}. With no global constraints this can be formulated as the following ILP: 1714 arg max X Bijryijr (1) Y ijr Xs.t.∀i,j yijr = 1 r where the constraint ensures exactly one relation between each event pair. We now describe constraints that result in a coherent global process structure. Connectivity Our ILP formulation for enforcing connectivity is a minor variation of the one suggested by Martins et al. (2009) for dependency parsing. In our setup, we want P to be a connected undirected graph, and not a directed tree. However, an undirected graph P is connected iff there exists a directed tree that is a subgraph of P when edge directions are ignored. Thus the resulting formulation is almost identical and is based on flow constraints which ensure that there is a path from a designated root in the graph to all other nodes. Let R be the set R \ NONE. An edge (ti, tj) is in E iff there is some non-NONE relation between ti and tj, i.e. iff yij := PrE R yijr is equal to 1. For each variable yij we define </context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>Andr´e L. Martins, Noah A. Smith, and Eric P. Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proceedings of ACL/IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning constraints for consistent timeline extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>873--882</pages>
<contexts>
<context position="3381" citStr="McClosky and Manning, 2012" startWordPosition="515" endWordPosition="519">ent extraction focuses on identifying a closed set of events within a single sentence. For example, the BioNLP 2009 and 2011 shared tasks (Kim et al., 2009; Kim et al., 2011) consider nine events types related to proteins. In practice, events are currently almost always extracted from a single sentence. Process extraction, on the other hand, is centered around discovering relations between events that span multiple sentences. The set of possible event types in process extraction is also much larger. Timeline construction involves identifying temporal relations between events (Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and is thus related to process extraction as both focus on event-event relations spanning multiple sentences. However, events in processes are tightly coupled in ways that go beyond simple temporal ordering, and these dependencies are central for the process extraction task. Hence, capturing process structure requires modeling a larger set of relations that includes temporal, causal and co-reference relations. In this paper, we formally define the task of process extraction and present automatic extraction methods. Our approach handles an open set of event types and wo</context>
<context position="36028" citStr="McClosky and Manning, 2012" startWordPosition="6017" endWordPosition="6020"> a pipeline architecture where first events are found, and then their arguments are identified (Miwa et al., 2010; Bj¨orne et al., 2011). Subsequent methods predicted events and arguments jointly using Markov logic (Poon and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky (2011) learned event templates (or frames), where events that are related to one another and their semantic roles are extracted. Recently, Cheung et al. (2013) proposed an unsupervised generative model for inducing such templates. A major difference in our work is that we do not learn typical event relations from a large and redundant corpus, but are given a paragraph and have a “o</context>
</contexts>
<marker>McClosky, Manning, 2012</marker>
<rawString>David McClosky and Christopher D. Manning. 2012. Learning constraints for consistent timeline extraction. In Proceedings of EMNLP-CoNLL, pages 873–882.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Mihai Surdeanu</author>
<author>Christopher D Manning</author>
</authors>
<title>Event extraction as dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>1626--1635</pages>
<contexts>
<context position="35703" citStr="McClosky et al., 2011" startWordPosition="5971" endWordPosition="5974">as before), and also for the false positive of the predicted class. Despite that we see that Global improves overall performance by 3.7 F1 points on the test set. 5 Related Work A related line of work is biomedical event extraction in recent BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011). Earlier work employed a pipeline architecture where first events are found, and then their arguments are identified (Miwa et al., 2010; Bj¨orne et al., 2011). Subsequent methods predicted events and arguments jointly using Markov logic (Poon and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky (2011) learned event templates (or frames), where events th</context>
</contexts>
<marker>McClosky, Surdeanu, Manning, 2011</marker>
<rawString>David McClosky, Mihai Surdeanu, and Christopher D. Manning. 2011. Event extraction as dependency parsing. In Proceedings ofACL, pages 1626–1635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Rune Sætre</author>
<author>Jin-Dong Kim</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Event extraction with complex event classification using rich features.</title>
<date>2010</date>
<journal>J. Bioinformatics and Computational Biology,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="35515" citStr="Miwa et al., 2010" startWordPosition="5943" endWordPosition="5946"> edge is determined by scores computed based on the local classifier, and if this prediction is wrong, we will now be penalized for both the false negative of the correct class (just as before), and also for the false positive of the predicted class. Despite that we see that Global improves overall performance by 3.7 F1 points on the test set. 5 Related Work A related line of work is biomedical event extraction in recent BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011). Earlier work employed a pipeline architecture where first events are found, and then their arguments are identified (Miwa et al., 2010; Bj¨orne et al., 2011). Subsequent methods predicted events and arguments jointly using Markov logic (Poon and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). How</context>
</contexts>
<marker>Miwa, Sætre, Kim, Tsujii, 2010</marker>
<rawString>Makoto Miwa, Rune Sætre, Jin-Dong Kim, and Jun’ichi Tsujii. 2010. Event extraction with complex event classification using rich features. J. Bioinformatics and Computational Biology, 8(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Joint inference for knowledge extraction from biomedical literature.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="35645" citStr="Poon and Vanderwende, 2010" startWordPosition="5962" endWordPosition="5965">nalized for both the false negative of the correct class (just as before), and also for the false positive of the predicted class. Despite that we see that Global improves overall performance by 3.7 F1 points on the test set. 5 Related Work A related line of work is biomedical event extraction in recent BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011). Earlier work employed a pipeline architecture where first events are found, and then their arguments are identified (Miwa et al., 2010; Bj¨orne et al., 2011). Subsequent methods predicted events and arguments jointly using Markov logic (Poon and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky (</context>
</contexts>
<marker>Poon, Vanderwende, 2010</marker>
<rawString>Hoifung Poon and Lucy Vanderwende. 2010. Joint inference for knowledge extraction from biomedical literature. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Jos´e M Casta˜no</author>
<author>Robert Ingria</author>
<author>Roser Sauri</author>
<author>Robert J Gaizauskas</author>
<author>Andrea Setzer</author>
<author>Graham Katz</author>
<author>Dragomir R Radev</author>
</authors>
<title>TimeML: Robust specification of event and temporal expressions in text.</title>
<date>2003</date>
<booktitle>In New Directions in Question Answering.</booktitle>
<marker>Pustejovsky, Casta˜no, Ingria, Sauri, Gaizauskas, Setzer, Katz, Radev, 2003</marker>
<rawString>James Pustejovsky, Jos´e M. Casta˜no, Robert Ingria, Roser Sauri, Robert J. Gaizauskas, Andrea Setzer, Graham Katz, and Dragomir R. Radev. 2003. TimeML: Robust specification of event and temporal expressions in text. In New Directions in Question Answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Regina Barzilay</author>
</authors>
<title>Multi-event extraction guided by global constraints.</title>
<date>2012</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="37883" citStr="Reichart and Barzilay (2012)" startWordPosition="6313" endWordPosition="6317">s, the predictions of Global and Local are identical. Original text, Left: “... the template shifts ..., and a part of the template strand is either skipped by the replication machinery or used twice as a template. As a result, a segment of DNA is deleted or duplicated.” Right: “Cells of mating type A secrete a signaling molecule, which can bind to specific receptor proteins on nearby cells. At the same time, cells secrete factor, which binds to receptors on A cells.” for modeling global constraints. Similar observations and techniques have been proposed in other information extraction tasks. Reichart and Barzilay (2012) tied information from multiple sequence models that describe the same event by using global higher-order potentials. Berant et al. (2011) proposed a global inference algorithm to identify entailment relations. There is an abundance of examples of enforcing global constraints in other NLP tasks, such as in coreference resolution (Finkel and Manning, 2008), parsing (Rush et al., 2012) and named entity recognition (Wang et al., 2013). 6 Conclusion Developing systems that understand process descriptions is an important step towards building applications that require deeper reasoning, such as biol</context>
</contexts>
<marker>Reichart, Barzilay, 2012</marker>
<rawString>Roi Reichart and Regina Barzilay. 2012. Multi-event extraction guided by global constraints. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Fast and robust joint models for biomedical event extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="35731" citStr="Riedel and McCallum (2011)" startWordPosition="5975" endWordPosition="5978"> the false positive of the predicted class. Despite that we see that Global improves overall performance by 3.7 F1 points on the test set. 5 Related Work A related line of work is biomedical event extraction in recent BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011). Earlier work employed a pipeline architecture where first events are found, and then their arguments are identified (Miwa et al., 2010; Bj¨orne et al., 2011). Subsequent methods predicted events and arguments jointly using Markov logic (Poon and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky (2011) learned event templates (or frames), where events that are related to one anothe</context>
</contexts>
<marker>Riedel, McCallum, 2011</marker>
<rawString>Sebastian Riedel and Andrew McCallum. 2011. Fast and robust joint models for biomedical event extraction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Roi Reichert</author>
<author>Michael Collins</author>
<author>Amir Globerson</author>
</authors>
<title>Improved parsing and POS tagging using inter-sentence consistency constraints.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="38269" citStr="Rush et al., 2012" startWordPosition="6374" endWordPosition="6377">t the same time, cells secrete factor, which binds to receptors on A cells.” for modeling global constraints. Similar observations and techniques have been proposed in other information extraction tasks. Reichart and Barzilay (2012) tied information from multiple sequence models that describe the same event by using global higher-order potentials. Berant et al. (2011) proposed a global inference algorithm to identify entailment relations. There is an abundance of examples of enforcing global constraints in other NLP tasks, such as in coreference resolution (Finkel and Manning, 2008), parsing (Rush et al., 2012) and named entity recognition (Wang et al., 2013). 6 Conclusion Developing systems that understand process descriptions is an important step towards building applications that require deeper reasoning, such as biological process models from text, intelligent tutoring systems, and non-factoid QA systems. In this paper we have presented the task of process extraction, and developed methods for extracting relations between process events. Processes contain events that are tightly coupled through strong dependencies. We have shown that exploiting these structural dependencies and performing joint </context>
</contexts>
<marker>Rush, Reichert, Collins, Globerson, 2012</marker>
<rawString>Alexander M. Rush, Roi Reichert, Michael Collins, and Amir Globerson. 2012. Improved parsing and POS tagging using inter-sentence consistency constraints. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuel A Schegloff</author>
<author>Harvey Sacks</author>
</authors>
<title>Opening up closings.</title>
<date>1973</date>
<journal>Semiotica,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="30711" citStr="Schegloff and Sacks, 1973" startWordPosition="5149" endWordPosition="5152">ndicates that our novel features (Section 3.2) are important for discriminating between process relations. Specifically, in the full evaluation Local improves precision more than in the temporal evaluation, suggesting that designing syntactic and semantic features for connectives is useful for distinguishing PREY, CAUSES, and ENABLES when the amount of training data is small. The Chain baseline performs only slightly worse than our global model. This demonstrates the strong tendency of processes to proceed linearly from one event to the other, which is a known property of discourse structure (Schegloff and Sacks, 1973). However, since the structure is deterministically fixed, Chain is highly inflexible and does not allow any extensions or incorporation of other structural constraints or domain knowledge. Thus, it can be used as a simple and efficient approximation but is not a good candidate for a real system. Further support for the linear nature of process structure is provided by the All-Prev baseline, which performs poorly in the full evaluation, but in temporal evaluation works reasonably well. Table 2 presents the degree distribution of Local and Global on the development set comparing to the gold sta</context>
</contexts>
<marker>Schegloff, Sacks, 1973</marker>
<rawString>Emanuel A Schegloff and Harvey Sacks. 1973. Opening up closings. Semiotica, 8(4):289–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Sontag</author>
<author>Amir Globerson</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Introduction to dual decomposition for inference.</title>
<date>2011</date>
<booktitle>Optimization for Machine Learning.</booktitle>
<editor>In Suvrit Sra, Sebastian Nowozin, and Stephen J. Wright, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="26055" citStr="Sontag et al., 2011" startWordPosition="4422" endWordPosition="4425">tk, then ti cannot be immediately before tk. yijSAME + yjkSAME + yikSAME 3 yijCAUSES + yikCAUSES + yjkCOTEMP 3 yijCOTEMP + yjkCOTEMP + yikCOTEMP+ yikSAME 3 yijPREV + yjkPREV + yikSAME 2 yijPREV + yjkPREV − yikNONE � 1 3We experimented with a reward for certain triads or a penalty for others and empirically found that using rewards results in better performance on the development set. We used the Gurobi optimization package4 to find an exact solution for our ILP, which contains O(n2|R|) variables and O(n3) constraints. We also developed an equivalent formulation amenable to dual decomposition (Sontag et al., 2011), which is a faster approximation method. But practically, solving the ILP exactly with Gurobi was quite fast (average/median time per process: 0.294 sec/0.152 sec on a standard laptop). 4 Experimental Evaluation We extracted 148 process descriptions by going through chapters from the textbook ”Biology” and marking any contiguous sequence of sentences that describes a process, i.e., a series of events that lead towards some objective. Then, each process description was annotated by a biologist. The annotator was first presented with annotation guidelines and annotated 20 descriptions. The anno</context>
</contexts>
<marker>Sontag, Globerson, Jaakkola, 2011</marker>
<rawString>David Sontag, Amir Globerson, and Tommi Jaakkola. 2011. Introduction to dual decomposition for inference. In Suvrit Sra, Sebastian Nowozin, and Stephen J. Wright, editors, Optimization for Machine Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Massimiliano Ciaramita</author>
<author>Hugo Zaragoza</author>
</authors>
<title>Learning to rank answers to nonfactoid questions from web collections.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="2548" citStr="Surdeanu et al., 2011" startWordPosition="385" endWordPosition="388">the event ‘enter’, causes the event ‘changing’). *Both authors equally contributed to the paper Peter Clark Allen Institute for Artificial Intelligence, Seattle Automatically extracting the structure of processes from text is crucial for applications that require reasoning, such as non-factoid QA. For instance, answering a question on ATP synthesis, such as “How do H+ ions contribute to the production of ATP?” requires a structure that links H+ ions (Figure 1, sentence 1) to ATP (Figure 1, sentence 4) through a sequence of intermediate events. Such “How?” questions are common on FAQ websites (Surdeanu et al., 2011), which further supports the importance of process extraction. Process extraction is related to two recent lines of work in Information Extraction – event extraction and timeline construction. Traditional event extraction focuses on identifying a closed set of events within a single sentence. For example, the BioNLP 2009 and 2011 shared tasks (Kim et al., 2009; Kim et al., 2011) consider nine events types related to proteins. In practice, events are currently almost always extracted from a single sentence. Process extraction, on the other hand, is centered around discovering relations between </context>
</contexts>
<marker>Surdeanu, Ciaramita, Zaragoza, 2011</marker>
<rawString>Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2011. Learning to rank answers to nonfactoid questions from web collections. Computational Linguistics, 37(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>A global joint model for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<pages>191</pages>
<contexts>
<context position="11833" citStr="Toutanova et al., 2008" startWordPosition="1950" endWordPosition="1953">traints that combine both coreference and temporal relations simultaneously. For example, if u and v are the same event, then there can exist no w, such that u is before w, but v is after w (see Section 3.3) We annotated 148 process descriptions based on the aforementioned definitions. Further details on annotation and data set statistics are provided in Section 4 and Table 1. Structural properties of processes Coherent processes exhibit many structural properties. For example, two argument mentions related to the same event cannot overlap – a constraint that has been used in the past in SRL (Toutanova et al., 2008). In this paper we focus on three main structural properties of the graph P. First, in a coherent process, all events mentioned are related to one another, and hence the graph P must be connected. Second, processes tend to have a “chain-like” structure where one event follows another, and thus we expect 1712 Deg. Gold Local Global 0 0 29 0 1 219 274 224 2 369 337 408 3 46 14 17 &gt;4 22 2 7 Table 2: Node degree distribution for event mentions on the training set. Predictions for the Local and Global models were obtained using 10-fold cross validation. nodes’ degree to generally be &lt; 2. Indeed, 90</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2008</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2008. A global joint model for semantic role labeling. Computational Linguistics, 34(2):161– 191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Wanxiang Che</author>
<author>Christopher D Manning</author>
</authors>
<title>Effective bilingual constraints for semisupervised learning of named entity recognizers.</title>
<date>2013</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="38318" citStr="Wang et al., 2013" startWordPosition="6382" endWordPosition="6385">s to receptors on A cells.” for modeling global constraints. Similar observations and techniques have been proposed in other information extraction tasks. Reichart and Barzilay (2012) tied information from multiple sequence models that describe the same event by using global higher-order potentials. Berant et al. (2011) proposed a global inference algorithm to identify entailment relations. There is an abundance of examples of enforcing global constraints in other NLP tasks, such as in coreference resolution (Finkel and Manning, 2008), parsing (Rush et al., 2012) and named entity recognition (Wang et al., 2013). 6 Conclusion Developing systems that understand process descriptions is an important step towards building applications that require deeper reasoning, such as biological process models from text, intelligent tutoring systems, and non-factoid QA systems. In this paper we have presented the task of process extraction, and developed methods for extracting relations between process events. Processes contain events that are tightly coupled through strong dependencies. We have shown that exploiting these structural dependencies and performing joint inference over all event mentions can significant</context>
</contexts>
<marker>Wang, Che, Manning, 2013</marker>
<rawString>Mengqiu Wang, Wanxiang Che, and Christopher D. Manning. 2013. Effective bilingual constraints for semisupervised learning of named entity recognizers. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming wei Chang</author>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Structured learning with constrained conditional models.</title>
<date>2012</date>
<booktitle>Machine Learning,</booktitle>
<volume>88</volume>
<issue>3</issue>
<pages>6</pages>
<contexts>
<context position="23153" citStr="Chang et al., 2012" startWordPosition="3918" endWordPosition="3921"> following constraint bounds nodes’ degree by 2: yjk ≤ 2) (7) Since graph structures are not always chains, we add this as a soft constraint, that is, we penalize the objective for each node with degree &gt; 2. The chain structure is one of the several soft constraints we enforce. Thus, our modified objective function is Pijr Bijryijr + PkE)c αkCk, where K is the set of soft constraints, αk is the penalty (or reward for desirable structures), and Ck indicates whether a constraint is violated (or satisfied). Note that under this formulation our model is simply a constrained conditional model (wei Chang et al., 2012). The parameters αk are tuned on a development set (see Section 4). Relation triads A relation triad (or a relation triangle) for any three triggers ti, tj and tk in a process is a 3-tuple of relations (f(ti, tj), f(tj, tk), f(ti, tk)). Clearly, some triads are impossible while others are quite common. To find triads that could improve process extraction, the frequency of all possible triads in both the training set and the output of the pairwise classifier were found, and we focused on those for which the classifier and the gold standard disagree. We are interested in triads that never occur </context>
</contexts>
<marker>Chang, Ratinov, Roth, 2012</marker>
<rawString>Ming wei Chang, Lev Ratinov, and Dan Roth. 2012. Structured learning with constrained conditional models. Machine Learning, 88(3):399–431, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsumasa Yoshikawa</author>
<author>Sebastian Riedel</author>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Jointly identifying temporal relations with Markov logic.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL/IJCNLP.</booktitle>
<contexts>
<context position="10675" citStr="Yoshikawa et al., 2009" startWordPosition="1760" endWordPosition="1763">89.98 19 319 # of events 6.20 2 15 # of non-NONE relations 5.64 1 24 Table 1: Process statistics over 148 process descriptions. NONE is used to indicate no relation. relation set R. We chose a relation set R that captures the essential aspects of temporal relations between events in a process, while keeping the annotation as simple as possible. For instance, we include the SUPER relation that appears in temporal annotations such as the Timebank corpus (Pustejovsky et al., 2003) and Allen’s work, but in practice was not considered by many temporal ordering systems (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012). Importantly, our relation set also includes the relations CAUSES and ENABLES, which are fundamental to modeling processes and go beyond simple temporal ordering. We also added event coreference (SAME) to R. Do et al. (2012) used event coreference information in a temporal ordering task to modify probabilities provided by pairwise classifiers prior to joint inference. In this paper, we simply treat SAME as another event-event relation, which allows us to easily perform joint inference and employ structural constraints that combine both coreference and temporal relations simu</context>
<context position="35959" citStr="Yoshikawa et al., 2009" startWordPosition="6005" endWordPosition="6008">tasks (Kim et al., 2009; Kim et al., 2011). Earlier work employed a pipeline architecture where first events are found, and then their arguments are identified (Miwa et al., 2010; Bj¨orne et al., 2011). Subsequent methods predicted events and arguments jointly using Markov logic (Poon and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky (2011) learned event templates (or frames), where events that are related to one another and their semantic roles are extracted. Recently, Cheung et al. (2013) proposed an unsupervised generative model for inducing such templates. A major difference in our work is that we do not learn typical event relations from </context>
</contexts>
<marker>Yoshikawa, Riedel, Asahara, Matsumoto, 2009</marker>
<rawString>Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asahara, and Yuji Matsumoto. 2009. Jointly identifying temporal relations with Markov logic. In Proceedings of ACL/IJCNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
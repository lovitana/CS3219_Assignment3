<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000094">
<title confidence="0.997532">
Joint Coreference Resolution and Named-Entity Linking
with Multi-pass Sieves
</title>
<author confidence="0.994203">
Hannaneh Hajishirzi Leila Zilles Daniel S. Weld Luke Zettlemoyer
</author>
<affiliation confidence="0.9978495">
Department of Computer Science and Electrical Engineering
University of Washington
</affiliation>
<email confidence="0.997894">
{hannaneh,lzilles,lsz,weld}@cs.washington.edu
</email>
<note confidence="0.720513">
[Michael Eisner], and [Donald Tsang]2 announced the
grand opening of [[Hong Kong]� Disneyland]4 yester-
</note>
<figureCaption confidence="0.71880525">
day. [Eisner], thanked [the President]2 and welcomed
[fans]5 to [the park]4.
Figure 1: A text passage illustrating interactions between
coreference resolution and NEL.
</figureCaption>
<bodyText confidence="0.999600666666667">
ual. The biggest challenge in coreference resolu-
tion — accounting for 42% of errors in the state-
of-the-art Stanford system — is the inability to rea-
son effectively about background semantic knowl-
edge (Lee et al., 2013). For example, consider the
sentence in Figure 1. “President” refers to “Donald
Tsang” and “the park” refers to “Hong Kong Dis-
neyland,” but automated algorithms typically lack
the background knowledge to draw such inferences.
Incorporating knowledge is challenging, and many
efforts to do so have actually hurt performance,
e.g. (Lee et al., 2011; Durrett and Klein, 2013).
Named-entity linking (NEL) is the task of match-
ing textual mentions to corresponding entities in a
knowledge base, such as Wikipedia or Freebase.
Such links provide rich sources of semantic knowl-
edge about entity attributes — Freebase includes
president as Tsang’s title and Disneyland as hav-
ing the attribute park. But NEL is itself a chal-
lenging problem, and finding the correct link re-
quires disambiguating based on the mention string
and often non-local contextual features. For exam-
ple, “Michael Eisner” is relatively unambiguous but
the isolated mention “Eisner” is more challenging.
However, these mentions could be clustered with
a coreference model, allowing for improved NEL
through link propagation from the easier mentions.
</bodyText>
<sectionHeader confidence="0.667923" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99969324">
Many errors in coreference resolution come
from semantic mismatches due to inadequate
world knowledge. Errors in named-entity
linking (NEL), on the other hand, are of-
ten caused by superficial modeling of entity
context. This paper demonstrates that these
two tasks are complementary. We introduce
NECO, a new model for named entity linking
and coreference resolution, which solves both
problems jointly, reducing the errors made on
each. NECO extends the Stanford determinis-
tic coreference system by automatically link-
ing mentions to Wikipedia and introducing
new NEL-informed mention-merging sieves.
Linking improves mention-detection and en-
ables new semantic attributes to be incorpo-
rated from Freebase, while coreference pro-
vides better context modeling by propagat-
ing named-entity links within mention clus-
ters. Experiments show consistent improve-
ments across a number of datasets and ex-
perimental conditions, including over 11% re-
duction in MUC coreference error and nearly
21% reduction in F1 NEL error on ACE 2004
newswire data.
</bodyText>
<sectionHeader confidence="0.985999" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997837857142857">
Coreference resolution and named-entity linking are
closely related problems, but have been largely stud-
ied in isolation. This paper demonstrates that they
are complementary by introducing a simple joint
model that improves performance on both tasks.
Coreference resolution is the task of determining
when two textual mentions name the same individ-
</bodyText>
<page confidence="0.976673">
289
</page>
<note confidence="0.7328625">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 289–299,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999921173913044">
We present NECO, a new algorithm for jointly solv-
ing named entity linking and coreference resolu-
tion. Our work is related to that of Ratinov and
Roth (2012), which also uses knowledge derived
from an NEL system to improve coreference. How-
ever, NECO is the first joint model we know of, is
purely deterministic with no learning phase, does
automatic mention detection, and improves perfor-
mance on both tasks.
NECO extends the Stanford’s sieve-based model,
in which a high recall mention detection phase is
followed by a sequence of cluster merging opera-
tions ordered by decreasing precision (Raghunathan
et al., 2010; Lee et al., 2013). At each step, it
merges two clusters only if all available information
about their respective entities is consistent. We use
NEL to increase recall during the mention detection
phase and introduce two new cluster-merging sieves,
which compare the Freebase attributes of entities.
NECO also improves NEL by initially favoring high
precision linking results and then propagating links
and attributes as clusters are formed.
In summary we make the following contributions:
</bodyText>
<listItem confidence="0.9920395">
• We introduce NECO, a novel, joint approach
to solving coreference and NEL, demonstrating
that these tasks are complementary by achiev-
ing joint error reduction.
• We present experiments showing improved per-
formance at coreference resolution, given both
gold and automatic mention detection: e.g.,
6.2 point improvement in MUC recall on ACE
2004 newswire text and 3.1 point improvement
in MUC precision the CoNLL 2011 test set.
• NECO also leads to better performance at
named-entity linking, given both gold and au-
tomatic linking, improving F1 from 61.7% to
69.2% on a newly labeled test set.1
</listItem>
<sectionHeader confidence="0.983991" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999077">
We make use of existing models for coreference res-
olution and named entity linking.
</bodyText>
<footnote confidence="0.992364">
1Our corpus and the source code for NECO can be down-
loaded from https://www.cs.washington.edu/
research-projects/nlp/neco.
</footnote>
<subsectionHeader confidence="0.994765">
2.1 Coreference Resolution
</subsectionHeader>
<bodyText confidence="0.99994825">
Coreference resolution is the the task of identifying
all text spans (called mentions) that refer to the same
entity, forming mention clusters.
Stanford’s Sieve Model is a state-of-the-art coref-
erence resolver comprising a pipeline of “sieves”
that merge coreferent mentions according to deter-
ministic rules. Mentions are automatically predicted
by selecting all noun phrases (NP), pronouns, and
named entities. Each sieve either merges a cluster
to its single best antecedent from a list of previous
clusters, or declines to merge.
Higher precision sieves are applied earlier in the
pipeline according to the following order, looking at
different aspects of the text, including: (1) speaker
identification, (2-3) exact and relaxed string matches
between mentions, (4) precise constructs, including
appositives, acronyms and demonyms, (5-9) differ-
ent notions of strict and relaxed head matches be-
tween mentions, and finally (10) a number of syn-
tactic and distance cues for pronoun resolution.
</bodyText>
<subsectionHeader confidence="0.999039">
2.2 Named Entity Linking
</subsectionHeader>
<bodyText confidence="0.999347304347826">
Named-entity linking (NEL) is the task of identi-
fying mentions in a text and linking them to the
entity they name in a knowledge base, usually
Wikipedia. NECO uses two existing NEL sys-
tems: GLOW (Ratinov et al., 2011) and Wikipedi-
aMiner (Milne and Witten, 2008).
WikipediaMiner links mentions based on a notion
of semantic similarity to Wikipedia pages, consider-
ing all substrings up to a fixed length. Since there
are often many possible links, it disambiguates by
choosing the entity whose Wikipedia page is most
semantically related to the nearby context of the
mention. The semantic scoring function includes n-
gram statistics and also counts shared links to other
unambiguous mentions in the text.
GLOW finds mentions by selecting all the NPs
and named entities in the text. Linking is framed
as an integer linear programming optimization prob-
lem that takes into account using similar local con-
straints but also includes global constraints such as
entity link co-occurrence.
Both systems return confidence values. To main-
tain high precision, NECO uses an ensemble of
</bodyText>
<page confidence="0.983654">
290
</page>
<listItem confidence="0.9989185">
• Let Exemplar(c) be a representative mention of the cluster c, computed as defined below
• Let cj be an antecedent cluster of ci if cj has a mention which is before the first mention of ci
• Let l(m) be a Wikipedia page linked to mention m or 0 if there is no link
• Let l(c) be a Wikipedia page linked to mention Exemplar(c) or 0 if there is no link
</listItem>
<figure confidence="0.913649333333333">
1. Initialize Linked Mentions:
(a) Let MNEL = {mi  |i = 1... p} be the NEL output mentions, mi, each with a link l(mi)
(b) Let MCR = {mi  |i = 1... q} be the mentions mi from coreference mention detection
(c) Let M F MCR U MNEL (Sec. 3.1)
(d) Update entity links for all m E M and prune M (Sec. 3.2)
(e) Extract attributes from Wikipedia and Freebase for all m E M (Sec. 3.3)
(f) Let C F M be singleton mention clusters where Exemplar(ci) = mi, l(ci) = l(mi)
2. Merge Clusters: For every sieve S (including NEL sieves, Sec. 3.6) and cluster ci E C
(a) For every cluster cj, j = [i − 1...1] (traverse the preceding clusters in reverse order)
i. NEL constraints: Prevent merge if l(ci) =� l(cj) (Sec. 3.4)
ii. If all rules of sieve S are satisfied for clusters ci and cj
A. ck F Mer J
ge(ci, cj ), including entity link and attribute updates (Sec. 3.5)
B. C F C U {ck} \ Ci, cj}
3. Output: Coreference clusters C and linked Wikipedia pages l(ci)flci E C
</figure>
<figureCaption confidence="0.992568">
Figure 2: NECO: A joint algorithm for named-entity linking and coreference resolution.
</figureCaption>
<bodyText confidence="0.8082895">
GLOW and WikipediaMiner, selecting only high
confidence links.
</bodyText>
<sectionHeader confidence="0.849974" genericHeader="method">
3 Joint Coreference and Linking
</sectionHeader>
<bodyText confidence="0.999701">
We introduce a joint model for coreference resolu-
tion and NEL. Building on the Stanford sieve ar-
chitecture, our algorithm incrementally constructs
clusters of mentions using deterministic coreference
rules under NEL constraints.
Figure 2 presents the complete algorithm. The in-
put to NECO is a document and the output is a set C
of coreference clusters, with links l(c) to Wikipedia
pages for a subset of the clusters c E C. Step 1
detects mentions, merging the outputs of the base
systems (Sec. 3.1). Step 2 repeatedly merges coref-
erence clusters, while ensuring that NEL constraints
(Sec. 3.4) are satisfied. It uses the original Stan-
ford sieves and also two new NEL-informed sieves
(Sec. 3.6). NEL links are propagated to new clusters
as they are formed (Sec. 3.5).
</bodyText>
<subsectionHeader confidence="0.99951">
3.1 Mention Detection
</subsectionHeader>
<bodyText confidence="0.999945692307692">
In Steps 1(a-c) in Fig. 2, NECO combines mentions
from the base coreference and NEL systems.
Let MCR be the set of mentions returned by us-
ing Stanford’s rule-based mention detection algo-
rithm (Lee et al., 2013). Let MNEL be the set of
mentions output by the two NEL systems. NECO
creates an initial set of mentions, M, by taking the
union of all the mentions in MNEL and MCR. In
practice, taking the union increases diversity in the
mention pool. For example, it is often the case that
MNEL will include sub-phrases such as “Suharto”
when they are part of a larger mention “ex-dictator
Suharto” that is detected in MCR.
</bodyText>
<subsectionHeader confidence="0.999958">
3.2 Mention Entity Links and Pruning
</subsectionHeader>
<bodyText confidence="0.995817476190476">
Step 1(d) in Fig. 2 assigns Wikipedia links to a sub-
set of the detected mentions.
For mentions m output by the base NEL sys-
tems, we assign an exact link l(m) if the entire
mention span is linked. Mentions m&apos; that differ
from an exact linked mention m by only a pre- or
post-fix stop word are similarly assigned exact links
l(m&apos;) = l(m). For example, the mention “the pres-
ident” will be assigned the same link as “president”
but “The governor of Alaska Sarah Palin” would not
be assigned an exact link to Sarah Palin.
For mentions m&apos; that do not receive an exact link,
we assign a head link h(m&apos;) if the head word2 m has
been linked, by setting h(m&apos;) = l(m). For instance,
the head link for the mention “President Clinton”
(with “Clinton” as head word) will be the Wikipedia
title of Bill Clinton. We use head links for the
Relaxed NEL sieve (Sec. 3.6).
Next, we define L(m) to be the set con-
2A head word is assigned to every mention with the Stanford
parser head finding rules (Klein and Manning, 2003).
</bodyText>
<page confidence="0.935753">
291
</page>
<table confidence="0.564345">
country president city area
company state region location
place agency power unit
body market park province
manager organization owner trial
site prosecutor attorney county
senator stadium network building
attraction government department person
origin plant airport kingdom
capital operation author period
nominee candidate film venue
</table>
<figureCaption confidence="0.999690666666667">
Figure 3: The most commonly used fine-grained at-
tributes from Freebase and Wikipedia (out of over 500
total attributes).
</figureCaption>
<bodyText confidence="0.9993458125">
taining l(m) and l(m&apos;) for all sub-phrases m&apos;
of m. We add the sub-phrase links only
if their confidence is higher than the confi-
dence for l(m). For instance, assuming ap-
propriate confidence values, L(m) would in-
clude the pages for {List of governors of
Alaska, Alaska, Sarah Palin} given the
mention “The governor of Alaska Sarah Palin.” We
will use L(m) for NEL constraints and filtering
(Sec. 3.4).
After updating the entity links for all mentions,
NECO prunes spurious mentions that begin or
end with a stop word where the remaining sub-
expression of the mention exists in M. It also re-
moves time expressions and numbers from M if they
are not included in MNEL.
</bodyText>
<subsectionHeader confidence="0.999431">
3.3 Mention Attributes
</subsectionHeader>
<bodyText confidence="0.99997275">
Step 1(e) in Fig. 2 also assigns attributes for a
mention m linked to Wikipedia page l(m), at both
coarse and fine-grained levels, based on information
from the Freebase entry corresponding to exact link
l(m) or head link h(m).
The coarse attributes include gender, type, and
NER classes such as PERSON, LOCATION, and OR-
GANIZATION. These attributes are part of the orig-
inal Stanford coreference system and are used to
avoid merging conflicting clusters. We use the Free-
base values for these attributes when available. For
instance, if the linked entity contains the Freebase
type location or organization, we include the coarse
type to LOCATION or ORGANIZATION respectively.
In order to account for both links to specific peo-
ple (Barack Obama) and generic links to positions
held by people (President), we include the type PER-
SON if the linked entity has any of the Freebase types
person, job title, or government office or title. If no
coarse Freebase types are available for an attribute,
we default to predicted NER classes.
We add fine-grained attributes from Freebase and
Wikipedia by importing additional type information.
We use all of the Freebase notable types, a set of
hundreds of commonly used Freebase types, rang-
ing from us president to tropical cyclone and syn-
thpop album. We also include all of the Wikipedia
categories, on average six per entity. For example,
the mention “Indonesia” is assigned fine-grained at-
tributes such as book subject, military power, and
olympic participating country. Since many of these
fine-grained attributes are extremely specific, we use
the last word of each attribute to define an addi-
tional fine-grained attribute (see Fig. 3). These fine-
grained attributes are used in the Relaxed NEL sieve
(Sec. 3.6).
</bodyText>
<subsectionHeader confidence="0.929389">
3.4 NEL Constraints
</subsectionHeader>
<bodyText confidence="0.999441866666667">
While applying sieves to merge clusters in Figure 2
Step 2(a), NECO uses NEL constraints to eliminate
some otherwise acceptable merges.
We avoid merging inconsistent clusters that link
to different entities. Clusters ci and cj are incon-
sistent if both are linked (i.e., both clusters have
non-null entity assignments) and l(ci) 7� l(cj) or
h(ci) 7� h(cj). Also, in order to consider an an-
tecedent cluster c as a merge candidate, we require a
pair of entities in the set of linked entities L(c) to be
related to one another in Freebase. Two entities are
related in Freebase if they both appear in a relation;
for example, Bill Clinton and Arkansas are
related because Bill Clinton has a “governor-of” re-
lation with Arkansas.
</bodyText>
<subsectionHeader confidence="0.999516">
3.5 Merging Clusters and Update Entity Links
</subsectionHeader>
<bodyText confidence="0.999196375">
When two clusters ci and cj are merged to form a
new cluster ck, the entity link information L(ck),
l(ck), and h(ck) must be updated (Step 2 of Fig. 2).
We set L(ck) to the union of the linked entities found
in l(ci) and l(cj) and merge coarse attributes at this
point.
In order to set the exact and head entity links
l(ck) and h(ck), we use the exemplar mention
</bodyText>
<page confidence="0.981562">
292
</page>
<bodyText confidence="0.997882909090909">
Exemplar(ck) that denotes the most representative
mention of the cluster. Exemplar(c) is selected
according to a set of rules in the Stanford system,
based on textual position and mention type (proper
noun vs. common). We augment this function by
considering information from exact and head en-
tity links as well. Mentions appearing earlier in
text, proper mentions, and mentions that have ex-
act or head named-entity links are preferred to those
which do not. Given exemplars, we set l(ck) =
l(Exemplar(ck)) and h(ck) = h(Exemplar(ck)).
</bodyText>
<subsectionHeader confidence="0.945132">
3.6 NEL Sieves
</subsectionHeader>
<bodyText confidence="0.996901741935484">
Finally, we introduce two new sieves that use NEL
information at the beginning and end of the Stan-
ford sieves pipeline in the merging stage (Step 2 of
Fig. 2).
Exact NEL sieve The Exact NEL sieve merges
two clusters ci and cj if both are linked and their
links match, l(ci) = l(cj). For example, all men-
tions that have been linked to Barack Obama will
become members of the same coreference cluster.
Because the Exact NEL sieve has high precision, we
place it at the very beginning of the pipeline.
Relaxed NEL sieve The Relaxed NEL sieve uses
fine-grained attributes of the linked mentions to
merge proper nouns with common nouns when they
share attributes. For example, this sieve is able to
merge the proper mention “Disneyland” with the
“the mysterious park”, because park is one of the
fine-grained attributes assigned to Disneyland.
More formally, let mi = Exemplar(ci) and
mj = Exemplar(cj). For every common noun
mention mi, we merge ci with an antecedent clus-
ter cj if (1) mj is a linked proper noun, (2) if mi or
the title of its linked Wikipedia page is in the list of
fine-grained attributes of mj, or (3) if h(mj) is re-
lated to the head link h(mi) according to Freebase
as defined above.
Because this sieve has low precision, we only
allow merges between mentions that have a maxi-
mum distance of three sentences between one an-
other. We add the Relaxed NEL sieve near the end
of the pipeline, just before pronoun resolution.
</bodyText>
<sectionHeader confidence="0.998686" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999839133333334">
Core Components and Baselines The Stanford
sieve-based coreference system (Lee et al., 2013),
the GLOW NEL system (Ratinov et al., 2011), and
WikipediaMiner (Milne and Witten, 2008) provide
core functionality for our joint model, and are also
the state-of-the-art baselines against which we mea-
sure performance.
Parameter Settings Based on performance on the
development set, we set the GLOW’s confidence pa-
rameter to 1.0 and WikipediaMiner’s to 0.4 to assure
high-precision NEL. We also optimized for the set of
fine-grained attributes to import from Wikipedia and
Freebase, and the best way to incorporate the NEL
constraints into the sieve architecture.
Datasets We report results on the following
three datasets: ACE2004-NWIRE, CONLL2011,
and ACE2004-NWIRE-NEL. ACE2004-NWIRE, the
newswire subset of the ACE 2004 corpus (NIST,
2004), includes 128 documents. The CONLL2011
coreference dataset includes text from five different
domains: broadcast conversation (BC), broadcast
news (BN), magazine (MZ), newswire (NW), and
web data (WB) (Pradhan et al., 2011). The broadcast
conversation and broadcast news domains consist of
transcripts, whereas magazine and newswire contain
more standard written text. The development data
includes 303 documents and the test data includes
322 documents.
We created ACE2004-NWIRE-NEL by taking a
subset of ACE2004-NWIRE and annotating with
gold-standard entity links. We segment and link all
the expressions in text that refer to Wikipedia pages,
allowing for nested linking. For instance, both the
phrase “Hong Kong Disneyland,” and the sub-phrase
“Hong Kong” are linked. This dataset includes 12
documents and 350 linked entities.
Metrics We evaluate our system using MUC (Vi-
lain et al., 1995), B3 (Bagga and Baldwin, 1998),
and pairwise scores. MUC is a link-based met-
ric which measures how many clusters need to be
merged to cover the gold clusters and favors larger
clusters; B3 computes the proportion of intersec-
tion between predicted and gold clusters for every
mention and favors singletons (Recasens and Hovy,
2010). We computed the scores using the Stanford
</bodyText>
<page confidence="0.989197">
293
</page>
<table confidence="0.99987875">
Method P MUC F1 P B3 F1 P Pairwise
R R R F1
Stanford Sieves 39.9 46.2 42.8 67.9 71.8 69.8 44.2 29.7 35.6
NECO 46.8 52.5 49.5 70.4 72.6 71.5 51.5 34.6 41.4
No NEL Mentions 46.1 48.3 47.2 71.4 70.0 70.9 49.7 30.9 38.1
No Mention Pruning 43.6 45.6 44.6 70.5 69.9 70.2 46.2 29.4 35.9
No Attributes 45.9 47.4 46.6 71.8 69.7 70.7 48.6 27.0 34.7
No Constraints 42.3 49.3 45.5 68.3 72.3 70.2 44.2 28.6 34.7
</table>
<tableCaption confidence="0.999696">
Table 1: Coreference results on ACE2004-NWIRE with predicted mentions and automatic linking.
</tableCaption>
<bodyText confidence="0.703325">
coreference software for ACE2004 and using the
CoNLL scorer for the CoNLL 2011 dataset.
</bodyText>
<sectionHeader confidence="0.998571" genericHeader="method">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999908">
We first look at NECO’s performance at coreference
resolution and then evaluate its ability at NEL.
</bodyText>
<subsectionHeader confidence="0.875418">
5.1 Coref. Results with Predicted Mentions
</subsectionHeader>
<bodyText confidence="0.994171307692308">
Overall System Performance on ACE Data Ta-
ble 1 shows NECO’s performance at coreference
resolution on ACE-2004 compared to the Stanford
sieve implementation (Lee et al., 2013). The table
shows that NECO has both significantly improved
precision and recall compared to the Stanford base-
line, across all metrics. We generally observe larger
gains in MUC due to better mention detection and
the Relaxed NEL Sieve.
Contribution of System Components Table 1
also details the performance of four variants of our
system that ablate various components and features.
Specifically, we consider the following cases:
</bodyText>
<listItem confidence="0.993974028571428">
• No NEL Mentions: We discard additional
mentions, MNEL, provided by NEL (Sec. 3.1).
This increases B3 precision at the expense of
recall. Inspection shows that some of the errors
introduced by MNEL are actually due to cor-
rectly linked entities that were not annotated as
mentions in the dataset, but also some improp-
erly linked mentions.
• No Mention Pruning: We disable the initial
step of updating mention boundaries and re-
moving spurious mentions (Sec. 3.2). As ex-
pected, removing this step drops precision and
recall significantly, even compared to the No
NEL Mentions variant.
• No Attributes: Ablating coarse and fine-
grained attributes (Sec. 3.3) drops F1 and re-
call measures across all metrics. To under-
stand this effect, note that NECO uses at-
tributes in two different settings. Updating
coarse attributes tends to increase precision be-
cause it prevents dangerous merges, such as
merging “Staples” with the mention “it” in
a situation when “Staples” refers to the per-
son entity Todd Staples. Fine-grained at-
tributes also help with recall, when merging
a specific name of an entity with a mention
that uses a more general term; for instance,
“Hong Kong Disneyland” can be merged with
“the mysterious park” because “park” is a fine-
grained attribute for Disneyland. However,
when fine-grained attributes are used, precision
sometimes drops (e.g., when “president” might
merge with “Bush” when it should really merge
with “Clinton”).
• No NEL Constraints: Removing these con-
</listItem>
<bodyText confidence="0.888514181818182">
straints (Sec. 3.4) drops precision dramatically
leading to drop in F1. In the case of incor-
rect linking, however, NEL constraints can af-
fect recall. For instance, NEL constraints might
prevent merging “Staples” with “Todd Staples”
if the former were linked to the company and
the latter to the politician.
Overall System Performance on CoNLL Data
We also compare our full system (with added NEL
sieves, constraints, and mention pruning3) with the
Stanford sieve coreference system on CoNLL data
</bodyText>
<footnote confidence="0.914922">
3Due to CoNLL annotation guidelines, a named entity is
added to the mention list if it is not inside a larger mention with
an exact named entity link.
</footnote>
<page confidence="0.991886">
294
</page>
<table confidence="0.9997516">
Category: Method P MUC F1 P B3 F1
R R
BC: NECO 62.1 64.7 63.4 69.8 57.8 63.2
BC: Stanford Sieves 60.9 65.0 62.9 69.2 58.0 63.1
BN: NECO 69.3 59.4 64.0 78.8 60.8 68.6
BN: Stanford Sieves 68.0 58.9 63.1 79.0 60.2 68.3
MZ: NECO 67.6 62.9 65.2 78.4 61.1 68.7
MZ: Stanford Sieves 66.0 63.4 64.9 77.9 61.5 68.7
NW: NECO 62.0 54.5 58.0 74.9 57.4 65.0
NW: Stanford Sieves 60.0 54.2 56.9 75.3 57.0 64.9
</table>
<tableCaption confidence="0.979487">
Table 3: Coreference results on the individual categories of CoNLL 2011 development data. (BC=broadcast conver-
sation, BN=broadcast news, MZ=magazine, NW=newswire)
</tableCaption>
<table confidence="0.998018416666667">
MUC B3
Method P R F1 P R F1
Development Data
NECO 64.1+ 59.4 61.7+ 74.7 58.7 65.7
Stanford 62.7 59.0 60.8 74.8 58.3 65.6
NECO* 56.4+ 50.0 53.0+ 72.6 51.6 60.3
Stanford* 53.5 50.0 51.6 71.8 51.3 59.9
Test Data
NECO 61.2+ 58.4 59.8+ 72.2 56.4 63.3
Stanford 59.2 58.8 59.0 71.3 56.1 62.8
NECO* 55.1+ 51.7 53.3+ 70.0 50.8 58.8
Stanford* 52.0 52.3+ 52.1 68.9 50.8 58.5
</table>
<tableCaption confidence="0.805809">
Table 2: Coreference results on CoNLL 2011 develop-
ment and test data, using predicted mentions. Rows de-
noted with * indicate runs using the fully automated Stan-
</tableCaption>
<bodyText confidence="0.986972432432432">
ford CoreNLP pipeline rather than the predicted annota-
tions provided with the CoNLL data. Given the relatively
close results, we ran the Mann-Whitney U test for this
table; values with the + superscript are significant with
p &lt; 0.05.
(Table 2). We ran NECO and the baseline in two set-
tings: in the first, we use the standard predicted an-
notations (for POS, parses, NER, and speaker tags)
provided with the CoNLL data, and in the second,
we use the automated Stanford CoreNLP pipeline
to predict this information. On both the develop-
ment and test sets, we gain about 1 point in MUC
F1 as well as a smaller improvement in B3. Closer
inspection indicates that our system increases pre-
cision primarily due to mention pruning and NEL
constraints. Due to the differences in mention anno-
tation guidelines between ACE and CoNLL, perfor-
mance on ACE benefits more from improved men-
tion detection from NEL. Moreover, the ACE cor-
pus is all newswire text, which contains more enti-
ties that can benefit from linking. CoNLL, on the
other hand, contains a wider variety of texts, some
of which do not mention many named entities in
Wikipedia.
To examine the performance of our system on the
different domains covered by the CoNLL data, we
also test our system on each domain separately (Ta-
ble 3). We found NEL provided the biggest im-
provement for the news domains, broadcast news
(BN) and newswire (NW). These domains espe-
cially benefit from the improved mention detection
and pruning provided by NEL, and strong linking
benefitted both precision and recall in these do-
mains. We found that the magazine (MZ) section
of the corpus benefited the least from NEL, as there
were relatively few entities that our NEL systems
were able to connect to Wikipedia.
</bodyText>
<subsectionHeader confidence="0.998624">
5.2 Coreference Results with Gold Linking
</subsectionHeader>
<bodyText confidence="0.999599214285714">
Some of the errors introduced in our system are due
to incorrect or incomplete links discovered by the
automatic linking system. To assess the effect of
NEL performance on NECO, we tested on a por-
tion of ACE2004-NWIRE dataset for which we hand-
labeled correct links for the gold and predicted men-
tions. “NECO + Gold NEL” denotes a version of our
system which uses gold links instead of those pre-
dicted by NEL. As shown in Table 4, gold linking
significantly improves the performance of our sys-
tem across all measures. This suggests that further
work to improve automatic NEL may have substan-
tial reward.
Gold linking improves precision for two main rea-
</bodyText>
<page confidence="0.994759">
295
</page>
<table confidence="0.9990793">
Method MUC B3 Pairwise
P R F1 P R F1 P R F1
Gold Mentions
NECO + Gold NEL 85.8 75.5 80.3 91.4 81.2 86.0 89.1 68.0 77.1
NECO 84.6 74.0 78.9 90.5 80.4 85.2 83.9 66.0 73.9
Stanford Sieves 84.5 72.2 77.8 89.9 77.7 83.4 89.9 57.3 68.1
Predicted Mentions
NECO + Gold NEL 56.4 58.8 57.5 78.2 78.3 78.3 68.0 54.3 60.4
NECO 51.3 53.5 52.4 76.5 76.4 76.5 61.2 45.6 52.2
Stanford Sieves 43.9 46.4 45.1 74.4 74.2 74.3 51.3 36.1 42.4
</table>
<tableCaption confidence="0.973648">
Table 4: Coreference results on ACE2004-NWIRE-NEL with gold and predicted mentions and gold or automatic linking.
</tableCaption>
<table confidence="0.99969">
Method P MUC F1 P B3 F1 P Pairwise
R R R F1
NECO 85.0 76.6 80.6 87.6 76.4 81.6 79.3 56.1 65.8
Stanford Sieves 84.6 75.1 79.6 87.3 74.1 80.2 79.4 50.1 61.4
Haghighi and Klein (2009) 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7
Poon and Domingos (2008) 71.3 70.5 70.9 - - - 62.6 38.9 48.0
Finkel and Manning (2008) 78.7 58.5 67.1 86.8 65.2 74.5 76.1 44.2 55.9
</table>
<tableCaption confidence="0.999242">
Table 5: Coreference results on ACE2004-NWIRE with gold mentions and automatic linking.
</tableCaption>
<bodyText confidence="0.9998974">
sons. First, it reduces the coreference errors caused
by incorrect NEL links. For instance, gold link-
ing replaces the erroneous link generated by our
NEL systems for “Nasser al-Kidwa” to the correct
Wikipedia entity. As another example, two men-
tions of “Rutgers” will not be merged if one links
to the university and the other links to their football
team. Second, gold linking leads to better mention
detection and better linked mentions. For instance,
under gold linking, the whole mention, “The gover-
nor of Alaska, Sarah Palin,” is linked to the politi-
cian, while automatic linking systems only link the
substring containing her name, “Sarah Palin.” Still,
gold NEL cannot compensate for all coreference er-
rors in cases of generic or unlinked entities.
</bodyText>
<subsectionHeader confidence="0.998632">
5.3 Coreference Results with Gold Mentions
</subsectionHeader>
<bodyText confidence="0.999973545454545">
Many of the previous papers evaluate coreference
resolution assuming gold mentions so we also run
under that condition (Table 5) using ACE2004-
NWIRE data. As the table shows, with gold mentions
our system outperforms Haghighi and Klein (2009),
Poon and Domingos (2008), Finkel and Man-
ning (2008) and the Stanford sieve algorithm across
all metrics. Our method shows a relatively smaller
gain in precision, because this condition adds no
benefit to our technique of using NEL information
for pruning mentions.
</bodyText>
<subsectionHeader confidence="0.996869">
5.4 Improving Named Entity Linking
</subsectionHeader>
<bodyText confidence="0.965352">
While our previous experiments show that named-
entity linking can improve coreference resolution,
we now address the question of whether coreference
techniques can help NEL. We compare NECO with
a baseline ensemble4 composed of GLOW (Ratinov
et al., 2011) and WikipediaMiner (Milne and Witten,
2008) on our ACE2004-NWIRE-NEL dataset (Table
6). Our system gains about 8% in absolute recall
and 5% in absolute precision. For instance, our sys-
tem correctly adds links from “Bullock” to the en-
tity Sandra Bullock because coreference reso-
lution merges two mentions. In another example, it
correctly links “company” to Nokia. Overall, there
is a 21% relative reduction in F1 error.
4We take the union of all the links returned by GLOW and
WikipediaMiner, but if they link a mention to two different en-
tities, we use only the output of WikipediaMiner.
</bodyText>
<page confidence="0.99393">
296
</page>
<table confidence="0.998174666666667">
Method F1 Precision Recall
NECO 70.6 72.0 69.2
Baseline NEL 64.4 67.4 61.7
</table>
<tableCaption confidence="0.944584">
Table 6: NEL performance of our system and the ensem-
ble baseline linker on ACE2004-NWIRE-NEL.
</tableCaption>
<subsectionHeader confidence="0.943365">
5.5 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999980260869565">
We analyzed 90 precision and recall errors and
present our findings in Table 7. Spurious mentions
accounted for the majority of non-semantic errors.
Despite the improvements that come from NEL, a
large portion of coreference errors can still be at-
tributed to incomplete semantic information, includ-
ing precision errors caused by incorrect linking. For
instance, the mention “Disney” sometimes refers to
the company, and other times refers to the amuse-
ment park; however, the NEL systems we used had
difficulty disambiguating these cases, and NECO of-
ten incorrectly merges such mentions. Overly gen-
eral fine-grained attributes caused precision errors in
cases where many proper noun mentions were po-
tential antecedents for a common noun. Although
attributes such as country are useful for resolving a
generic “country” mention, this information is insuf-
ficient when two distinct mentions such as “China”
and “Russia” both have the country attribute.
However, many recall errors are also caused by
the lack of fine-grained attributes. Finding the ideal
set of fine-grained attributes remains an open prob-
lem.
</bodyText>
<sectionHeader confidence="0.9999" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.99997450877193">
Coreference resolution has a fifty year history which
defies brief summarization; see Ng (2010) for a
recent survey. Section 2.1 described the Stanford
multi-pass sieve algorithm, which is the foundation
for NECO.
Earlier coreference resolution systems used shal-
low semantics and pioneered knowledge extraction
from online encyclopedias (Ponzetto and Strube,
2006; Daum´e III and Marcu, 2005; Ng, 2007). Some
recent work shows improvement in coreference res-
olution by incorporating semantic information from
Web-scale structured knowledge bases. Haghighi
and Klein (2009) use a rule-based system to extract
fine-grained attributes for mentions by analyzing
precise constructs (e.g., appositives) in Wikipedia
articles. Subsequently, Haghighi and Klein (2010)
used a generative approach to learn entity types from
an initial list of unambiguous mention types. Bansal
and Klein (2012) use statistical analysis of Web n-
gram features including lexical relations.
Rahman and Ng (2011) use YAGO to extract type
relations for all mentions. These methods incor-
porate knowledge about all possible meanings of a
mention. If a mention has multiple meanings, ex-
traneous information might be associated with it.
Zheng et al. (2013) use a ranked list of candidate en-
tities for each mention and maintain the ranked list
when mentions are merged. Unlike previous work,
our method relies on NEL systems to disambiguate
possible meanings of a mention and capture high-
precision semantic knowledge from Wikipedia cate-
gories and Freebase notable types.
Ratinov and Roth (2012) investigated using NEL
to improve coreference resolution, but did not con-
sider a joint approach. They extracted attributes
from Wikipedia categories and used them as fea-
tures in a learned mention-pair model, but did not
do mention detection. Unfortunately, it is difficult
to compare directly to the results of both systems,
since they reported results on portions of ACE and
CoNLL datasets using gold mentions. However,
our approach provides independent evidence for the
benefit of NEL, and joint modeling in particular,
since it outperforms the state-of-the-art Stanford
sieve system (winner of the CoNLL 2011 shared
task (Pradhan et al., 2011)) and other recent com-
parable approaches on benchmark datasets.
Our work also builds on a long trajectory of
work in named entity resolution stemming from
SemTag (Dill et al., 2003). Section 2.2 discussed
GLOW and WikipediaMiner (Ratinov et al., 2011;
Milne and Witten, 2008). Kulkarni et al. (2009)
present an elegant collective disambiguation model,
but do not exploit the syntactic nuances gleaned by
within-document coreference resolution. Hachey et
al. (2013) provide an insightful summary and evalu-
ation of different approaches to NEL.
</bodyText>
<sectionHeader confidence="0.999411" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9859975">
Observing that existing coreference resolution and
named-entity linking have complementary strengths
</bodyText>
<page confidence="0.985491">
297
</page>
<bodyText confidence="0.839014928571428">
Error Type Percentage Example
Extra mentions 31.1 The other thing Paula really important is that they talk a lot about the
fact ...
Pronoun 27.7 However , [all 3 women gymnasts , taking part in the internationals for
the first time], performed well , because they had strong events and their
movements had difficulty.
Contextual 16.6 [The Chinese side] hopes that each party concerned continues to make
semantic constructive efforts to ...Considering the requirements of the Korean side
, ... the Chinese government decided to ...
NEL semantic 13.3 The most important thing about Disney is that it is a global brand.... The
subway to Disney has already been constructed.
Attributes 11.1 The Hong Kong government turned over to Disney Corporation [200
hectares of land ...].... this area has become a prohibited zone in Hong
Kong.
</bodyText>
<tableCaption confidence="0.905270666666667">
Table 7: Examples of different error categories and the relative frequency of each. For every example, the mention to
be resolved is underlined, and the correct antecedent is italicized. For precision errors, the wrongly merged mention
is bolded. For recall errors, the missed mention is surrounded by [brackets].
</tableCaption>
<bodyText confidence="0.9999416">
and weaknesses, we present a joint approach. We
introduce NECO, a novel algorithm which solves
the problems jointly, demonstrating improved per-
formance on both tasks.
We envision several ways to improve the joint
model. While the current implementation of NECO
only introduces NEL once, we could also integrate
predictions with different levels of confidence into
different sieves. It would be interesting to more
tightly integrate the NEL system so it operates on
clusters rather than individual mentions — after
each sieve merges an unlinked cluster, the algorithm
would retry NEL with the new context information.
NECO uses a relatively modest number of Freebase
attributes. While using more semantic knowledge
holds the promise of increased recall, the challenge
is maintaining precision. Finally, we would also like
to explore the extent to which a joint probabilistic
model (e.g., (Durrett and Klein, 2013)) might be
used to learn how to best make this tradeoff.
</bodyText>
<sectionHeader confidence="0.99839" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999918615384615">
The research was supported in part by grants
from DARPA under the DEFT program through
the AFRL (FA8750-13-2-0019) and the CSSG
(N11AP20020), the ONR (N00014-12-1-0211), and
the NSF (IIS-1115966). Support was also provided
by a gift from Google, an NSF Graduate Research
Fellowship, and the WRF / TJ Cable Professor-
ship. The authors thank Greg Durrett, Heeyoung
Lee, Mitchell Koch, Xiao Ling, Mark Yatskar, Ken-
ton Lee, Eunsol Choi, Gabriel Schubiner, Nicholas
FitzGerald, Tom Kwiatkowski, and the anonymous
reviewers for helpful comments and feedback on the
work.
</bodyText>
<sectionHeader confidence="0.998289" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.985435">
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In International Confer-
ence on Language Resources and Evaluation Work-
shop on Linguistics Coreference.
Mohit Bansal and Dan Klein. 2012. Coreference se-
mantics from web features. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics.
Hal Daum´e III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing.
Stephen Dill, Nadav Eiron, David Gibson, Daniel Gruhl,
R. Guha, Anant Jhingran, Tapas Kanungo, Sridhar Ra-
jagopalan, Andrew Tomkins, John A. Tomlin, and Ja-
son Y. Zien. 2003. SemTag and Seeker: bootstrapping
the semantic web via automated semantic annotation.
In Proceedings of the 12th International Conference
on World Wide Web.
</reference>
<page confidence="0.96816">
298
</page>
<reference confidence="0.999508078431372">
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics on Human Lan-
guage Technologies.
Ben Hachey, Will Radford, Joel Nothman, Matthew Hon-
nibal, and James R. Curran. 2013. Evaluating entity
linking with Wikipedia. Artificial Intelligence Jour-
nal, 194.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
Aria Haghighi and Dan Klein. 2010. Coreference res-
olution in a modular, entity-centered model. In Hu-
man Language Technologies: Annual Conference of
the North American Chapter of the Association for
Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation of
Wikipedia entities in Web text. In Proceedings of the
2009 Conference on Knowledge Discovery and Data
Mining.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford’s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings of
the Conference on Computational Natural Language
Learning.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-
centric, precision-ranked rules. Computational Lin-
guistics, 39(4).
Dan Milne and Ian H. Witten. 2008. Learning to link
with Wikipedia. In Proceedings of the ACM Confer-
ence on Information and Knowledge Management.
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics.
NIST. 2004. The ACE 2004 evaluation planXPToolkit
architecture.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, Wordnet and
Wikipedia for coreference resolution. In Proceedings
of the North American Association for Natural Lan-
guage Processing on Human Language Technologies.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov logic. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 shared task: modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of the
49th Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies.
Lev Ratinov and Dan Roth. 2012. Learning-based multi-
sieve co-reference resolution with knowledge. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to Wikipedia. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics.
Marta Recasens and Eduard Hovy. 2010. Coreference
resolution across corpora: languages, coding schemes,
and preprocessing information. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of the 6th conference on Message Understanding.
Jiaping Zheng, Luke Vilnis, Sameer Singh, Jinho D.
Choi, and Andrew McCallum. 2013. Dynamic
knowledge-base alignment for coreference resolution.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning.
</reference>
<page confidence="0.998628">
299
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.823992">
<title confidence="0.9983995">Joint Coreference Resolution and Named-Entity with Multi-pass Sieves</title>
<author confidence="0.999764">Hannaneh Hajishirzi Leila Zilles Daniel S Weld Luke Zettlemoyer</author>
<affiliation confidence="0.999402">Department of Computer Science and Electrical University of Washington</affiliation>
<abstract confidence="0.99675079661017">Donald the opening of [[Hong yester- [the welcomed [the Figure 1: A text passage illustrating interactions between coreference resolution and NEL. ual. The biggest challenge in coreference resolu- — accounting for errors in the stateof-the-art Stanford system — is the inability to reason effectively about background semantic knowledge (Lee et al., 2013). For example, consider the sentence in Figure 1. “President” refers to “Donald Tsang” and “the park” refers to “Hong Kong Disneyland,” but automated algorithms typically lack the background knowledge to draw such inferences. Incorporating knowledge is challenging, and many efforts to do so have actually hurt performance, e.g. (Lee et al., 2011; Durrett and Klein, 2013). Named-entity linking (NEL) is the task of matching textual mentions to corresponding entities in a knowledge base, such as Wikipedia or Freebase. Such links provide rich sources of semantic knowledge about entity attributes — Freebase includes Tsang’s title and havthe attribute But NEL is itself a challenging problem, and finding the correct link requires disambiguating based on the mention string and often non-local contextual features. For example, “Michael Eisner” is relatively unambiguous but the isolated mention “Eisner” is more challenging. However, these mentions could be clustered with a coreference model, allowing for improved NEL through link propagation from the easier mentions. Abstract Many errors in coreference resolution come from semantic mismatches due to inadequate world knowledge. Errors in named-entity linking (NEL), on the other hand, are often caused by superficial modeling of entity context. This paper demonstrates that these two tasks are complementary. We introduce a new model for named entity linking and coreference resolution, which solves both problems jointly, reducing the errors made on the Stanford deterministic coreference system by automatically linking mentions to Wikipedia and introducing new NEL-informed mention-merging sieves. Linking improves mention-detection and enables new semantic attributes to be incorporated from Freebase, while coreference provides better context modeling by propagating named-entity links within mention clusters. Experiments show consistent improvements across a number of datasets and experimental conditions, including over 11% reduction in MUC coreference error and nearly 21% reduction in F1 NEL error on ACE 2004 newswire data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference.</booktitle>
<contexts>
<context position="19284" citStr="Bagga and Baldwin, 1998" startWordPosition="3141" endWordPosition="3144">, whereas magazine and newswire contain more standard written text. The development data includes 303 documents and the test data includes 322 documents. We created ACE2004-NWIRE-NEL by taking a subset of ACE2004-NWIRE and annotating with gold-standard entity links. We segment and link all the expressions in text that refer to Wikipedia pages, allowing for nested linking. For instance, both the phrase “Hong Kong Disneyland,” and the sub-phrase “Hong Kong” are linked. This dataset includes 12 documents and 350 linked entities. Metrics We evaluate our system using MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and pairwise scores. MUC is a link-based metric which measures how many clusters need to be merged to cover the gold clusters and favors larger clusters; B3 computes the proportion of intersection between predicted and gold clusters for every mention and favors singletons (Recasens and Hovy, 2010). We computed the scores using the Stanford 293 Method P MUC F1 P B3 F1 P Pairwise R R R F1 Stanford Sieves 39.9 46.2 42.8 67.9 71.8 69.8 44.2 29.7 35.6 NECO 46.8 52.5 49.5 70.4 72.6 71.5 51.5 34.6 41.4 No NEL Mentions 46.1 48.3 47.2 71.4 70.0 70.9 49.7 30.9 38.1 No Mention Pruning 43.6 45.6 44.6 70</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Dan Klein</author>
</authors>
<title>Coreference semantics from web features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="31993" citStr="Bansal and Klein (2012)" startWordPosition="5252" endWordPosition="5255">hallow semantics and pioneered knowledge extraction from online encyclopedias (Ponzetto and Strube, 2006; Daum´e III and Marcu, 2005; Ng, 2007). Some recent work shows improvement in coreference resolution by incorporating semantic information from Web-scale structured knowledge bases. Haghighi and Klein (2009) use a rule-based system to extract fine-grained attributes for mentions by analyzing precise constructs (e.g., appositives) in Wikipedia articles. Subsequently, Haghighi and Klein (2010) used a generative approach to learn entity types from an initial list of unambiguous mention types. Bansal and Klein (2012) use statistical analysis of Web ngram features including lexical relations. Rahman and Ng (2011) use YAGO to extract type relations for all mentions. These methods incorporate knowledge about all possible meanings of a mention. If a mention has multiple meanings, extraneous information might be associated with it. Zheng et al. (2013) use a ranked list of candidate entities for each mention and maintain the ranked list when mentions are merged. Unlike previous work, our method relies on NEL systems to disambiguate possible meanings of a mention and capture highprecision semantic knowledge from</context>
</contexts>
<marker>Bansal, Klein, 2012</marker>
<rawString>Mohit Bansal and Dan Klein. 2012. Coreference semantics from web features. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>A large-scale exploration of effective global features for a joint entity detection and tracking model.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing.</booktitle>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2005. A large-scale exploration of effective global features for a joint entity detection and tracking model. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Dill</author>
<author>Nadav Eiron</author>
<author>David Gibson</author>
<author>Daniel Gruhl</author>
<author>R Guha</author>
</authors>
<title>Anant Jhingran, Tapas Kanungo, Sridhar Rajagopalan, Andrew Tomkins,</title>
<date>2003</date>
<booktitle>In Proceedings of the 12th International Conference on World Wide Web.</booktitle>
<location>John</location>
<contexts>
<context position="33494" citStr="Dill et al., 2003" startWordPosition="5490" endWordPosition="5493">ut did not do mention detection. Unfortunately, it is difficult to compare directly to the results of both systems, since they reported results on portions of ACE and CoNLL datasets using gold mentions. However, our approach provides independent evidence for the benefit of NEL, and joint modeling in particular, since it outperforms the state-of-the-art Stanford sieve system (winner of the CoNLL 2011 shared task (Pradhan et al., 2011)) and other recent comparable approaches on benchmark datasets. Our work also builds on a long trajectory of work in named entity resolution stemming from SemTag (Dill et al., 2003). Section 2.2 discussed GLOW and WikipediaMiner (Ratinov et al., 2011; Milne and Witten, 2008). Kulkarni et al. (2009) present an elegant collective disambiguation model, but do not exploit the syntactic nuances gleaned by within-document coreference resolution. Hachey et al. (2013) provide an insightful summary and evaluation of different approaches to NEL. 7 Conclusions Observing that existing coreference resolution and named-entity linking have complementary strengths 297 Error Type Percentage Example Extra mentions 31.1 The other thing Paula really important is that they talk a lot about t</context>
</contexts>
<marker>Dill, Eiron, Gibson, Gruhl, Guha, 2003</marker>
<rawString>Stephen Dill, Nadav Eiron, David Gibson, Daniel Gruhl, R. Guha, Anant Jhingran, Tapas Kanungo, Sridhar Rajagopalan, Andrew Tomkins, John A. Tomlin, and Jason Y. Zien. 2003. SemTag and Seeker: bootstrapping the semantic web via automated semantic annotation. In Proceedings of the 12th International Conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>Easy victories and uphill battles in coreference resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1131" citStr="Durrett and Klein, 2013" startWordPosition="156" endWordPosition="159">n coreference resolution and NEL. ual. The biggest challenge in coreference resolution — accounting for 42% of errors in the stateof-the-art Stanford system — is the inability to reason effectively about background semantic knowledge (Lee et al., 2013). For example, consider the sentence in Figure 1. “President” refers to “Donald Tsang” and “the park” refers to “Hong Kong Disneyland,” but automated algorithms typically lack the background knowledge to draw such inferences. Incorporating knowledge is challenging, and many efforts to do so have actually hurt performance, e.g. (Lee et al., 2011; Durrett and Klein, 2013). Named-entity linking (NEL) is the task of matching textual mentions to corresponding entities in a knowledge base, such as Wikipedia or Freebase. Such links provide rich sources of semantic knowledge about entity attributes — Freebase includes president as Tsang’s title and Disneyland as having the attribute park. But NEL is itself a challenging problem, and finding the correct link requires disambiguating based on the mention string and often non-local contextual features. For example, “Michael Eisner” is relatively unambiguous but the isolated mention “Eisner” is more challenging. However,</context>
<context position="36030" citStr="Durrett and Klein, 2013" startWordPosition="5880" endWordPosition="5883">e could also integrate predictions with different levels of confidence into different sieves. It would be interesting to more tightly integrate the NEL system so it operates on clusters rather than individual mentions — after each sieve merges an unlinked cluster, the algorithm would retry NEL with the new context information. NECO uses a relatively modest number of Freebase attributes. While using more semantic knowledge holds the promise of increased recall, the challenge is maintaining precision. Finally, we would also like to explore the extent to which a joint probabilistic model (e.g., (Durrett and Klein, 2013)) might be used to learn how to best make this tradeoff. 8 Acknowledgements The research was supported in part by grants from DARPA under the DEFT program through the AFRL (FA8750-13-2-0019) and the CSSG (N11AP20020), the ONR (N00014-12-1-0211), and the NSF (IIS-1115966). Support was also provided by a gift from Google, an NSF Graduate Research Fellowship, and the WRF / TJ Cable Professorship. The authors thank Greg Durrett, Heeyoung Lee, Mitchell Koch, Xiao Ling, Mark Yatskar, Kenton Lee, Eunsol Choi, Gabriel Schubiner, Nicholas FitzGerald, Tom Kwiatkowski, and the anonymous reviewers for hel</context>
</contexts>
<marker>Durrett, Klein, 2013</marker>
<rawString>Greg Durrett and Dan Klein. 2013. Easy victories and uphill battles in coreference resolution. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Enforcing transitivity in coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies.</booktitle>
<contexts>
<context position="27475" citStr="Finkel and Manning (2008)" startWordPosition="4550" endWordPosition="4553">.3 68.1 Predicted Mentions NECO + Gold NEL 56.4 58.8 57.5 78.2 78.3 78.3 68.0 54.3 60.4 NECO 51.3 53.5 52.4 76.5 76.4 76.5 61.2 45.6 52.2 Stanford Sieves 43.9 46.4 45.1 74.4 74.2 74.3 51.3 36.1 42.4 Table 4: Coreference results on ACE2004-NWIRE-NEL with gold and predicted mentions and gold or automatic linking. Method P MUC F1 P B3 F1 P Pairwise R R R F1 NECO 85.0 76.6 80.6 87.6 76.4 81.6 79.3 56.1 65.8 Stanford Sieves 84.6 75.1 79.6 87.3 74.1 80.2 79.4 50.1 61.4 Haghighi and Klein (2009) 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7 Poon and Domingos (2008) 71.3 70.5 70.9 - - - 62.6 38.9 48.0 Finkel and Manning (2008) 78.7 58.5 67.1 86.8 65.2 74.5 76.1 44.2 55.9 Table 5: Coreference results on ACE2004-NWIRE with gold mentions and automatic linking. sons. First, it reduces the coreference errors caused by incorrect NEL links. For instance, gold linking replaces the erroneous link generated by our NEL systems for “Nasser al-Kidwa” to the correct Wikipedia entity. As another example, two mentions of “Rutgers” will not be merged if one links to the university and the other links to their football team. Second, gold linking leads to better mention detection and better linked mentions. For instance, under gold l</context>
<context position="28704" citStr="Finkel and Manning (2008)" startWordPosition="4746" endWordPosition="4750">ng, the whole mention, “The governor of Alaska, Sarah Palin,” is linked to the politician, while automatic linking systems only link the substring containing her name, “Sarah Palin.” Still, gold NEL cannot compensate for all coreference errors in cases of generic or unlinked entities. 5.3 Coreference Results with Gold Mentions Many of the previous papers evaluate coreference resolution assuming gold mentions so we also run under that condition (Table 5) using ACE2004- NWIRE data. As the table shows, with gold mentions our system outperforms Haghighi and Klein (2009), Poon and Domingos (2008), Finkel and Manning (2008) and the Stanford sieve algorithm across all metrics. Our method shows a relatively smaller gain in precision, because this condition adds no benefit to our technique of using NEL information for pruning mentions. 5.4 Improving Named Entity Linking While our previous experiments show that namedentity linking can improve coreference resolution, we now address the question of whether coreference techniques can help NEL. We compare NECO with a baseline ensemble4 composed of GLOW (Ratinov et al., 2011) and WikipediaMiner (Milne and Witten, 2008) on our ACE2004-NWIRE-NEL dataset (Table 6). Our syst</context>
</contexts>
<marker>Finkel, Manning, 2008</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2008. Enforcing transitivity in coreference resolution. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hachey</author>
<author>Will Radford</author>
<author>Joel Nothman</author>
<author>Matthew Honnibal</author>
<author>James R Curran</author>
</authors>
<title>Evaluating entity linking with Wikipedia.</title>
<date>2013</date>
<journal>Artificial Intelligence Journal,</journal>
<volume>194</volume>
<contexts>
<context position="33777" citStr="Hachey et al. (2013)" startWordPosition="5530" endWordPosition="5533">d joint modeling in particular, since it outperforms the state-of-the-art Stanford sieve system (winner of the CoNLL 2011 shared task (Pradhan et al., 2011)) and other recent comparable approaches on benchmark datasets. Our work also builds on a long trajectory of work in named entity resolution stemming from SemTag (Dill et al., 2003). Section 2.2 discussed GLOW and WikipediaMiner (Ratinov et al., 2011; Milne and Witten, 2008). Kulkarni et al. (2009) present an elegant collective disambiguation model, but do not exploit the syntactic nuances gleaned by within-document coreference resolution. Hachey et al. (2013) provide an insightful summary and evaluation of different approaches to NEL. 7 Conclusions Observing that existing coreference resolution and named-entity linking have complementary strengths 297 Error Type Percentage Example Extra mentions 31.1 The other thing Paula really important is that they talk a lot about the fact ... Pronoun 27.7 However , [all 3 women gymnasts , taking part in the internationals for the first time], performed well , because they had strong events and their movements had difficulty. Contextual 16.6 [The Chinese side] hopes that each party concerned continues to make </context>
</contexts>
<marker>Hachey, Radford, Nothman, Honnibal, Curran, 2013</marker>
<rawString>Ben Hachey, Will Radford, Joel Nothman, Matthew Honnibal, and James R. Curran. 2013. Evaluating entity linking with Wikipedia. Artificial Intelligence Journal, 194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Simple coreference resolution with rich syntactic and semantic features.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="27343" citStr="Haghighi and Klein (2009)" startWordPosition="4524" endWordPosition="4527">1.4 81.2 86.0 89.1 68.0 77.1 NECO 84.6 74.0 78.9 90.5 80.4 85.2 83.9 66.0 73.9 Stanford Sieves 84.5 72.2 77.8 89.9 77.7 83.4 89.9 57.3 68.1 Predicted Mentions NECO + Gold NEL 56.4 58.8 57.5 78.2 78.3 78.3 68.0 54.3 60.4 NECO 51.3 53.5 52.4 76.5 76.4 76.5 61.2 45.6 52.2 Stanford Sieves 43.9 46.4 45.1 74.4 74.2 74.3 51.3 36.1 42.4 Table 4: Coreference results on ACE2004-NWIRE-NEL with gold and predicted mentions and gold or automatic linking. Method P MUC F1 P B3 F1 P Pairwise R R R F1 NECO 85.0 76.6 80.6 87.6 76.4 81.6 79.3 56.1 65.8 Stanford Sieves 84.6 75.1 79.6 87.3 74.1 80.2 79.4 50.1 61.4 Haghighi and Klein (2009) 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7 Poon and Domingos (2008) 71.3 70.5 70.9 - - - 62.6 38.9 48.0 Finkel and Manning (2008) 78.7 58.5 67.1 86.8 65.2 74.5 76.1 44.2 55.9 Table 5: Coreference results on ACE2004-NWIRE with gold mentions and automatic linking. sons. First, it reduces the coreference errors caused by incorrect NEL links. For instance, gold linking replaces the erroneous link generated by our NEL systems for “Nasser al-Kidwa” to the correct Wikipedia entity. As another example, two mentions of “Rutgers” will not be merged if one links to the university and the other links t</context>
<context position="28651" citStr="Haghighi and Klein (2009)" startWordPosition="4738" endWordPosition="4741">etter linked mentions. For instance, under gold linking, the whole mention, “The governor of Alaska, Sarah Palin,” is linked to the politician, while automatic linking systems only link the substring containing her name, “Sarah Palin.” Still, gold NEL cannot compensate for all coreference errors in cases of generic or unlinked entities. 5.3 Coreference Results with Gold Mentions Many of the previous papers evaluate coreference resolution assuming gold mentions so we also run under that condition (Table 5) using ACE2004- NWIRE data. As the table shows, with gold mentions our system outperforms Haghighi and Klein (2009), Poon and Domingos (2008), Finkel and Manning (2008) and the Stanford sieve algorithm across all metrics. Our method shows a relatively smaller gain in precision, because this condition adds no benefit to our technique of using NEL information for pruning mentions. 5.4 Improving Named Entity Linking While our previous experiments show that namedentity linking can improve coreference resolution, we now address the question of whether coreference techniques can help NEL. We compare NECO with a baseline ensemble4 composed of GLOW (Ratinov et al., 2011) and WikipediaMiner (Milne and Witten, 2008)</context>
<context position="31682" citStr="Haghighi and Klein (2009)" startWordPosition="5208" endWordPosition="5211">ed attributes remains an open problem. 6 Related Work Coreference resolution has a fifty year history which defies brief summarization; see Ng (2010) for a recent survey. Section 2.1 described the Stanford multi-pass sieve algorithm, which is the foundation for NECO. Earlier coreference resolution systems used shallow semantics and pioneered knowledge extraction from online encyclopedias (Ponzetto and Strube, 2006; Daum´e III and Marcu, 2005; Ng, 2007). Some recent work shows improvement in coreference resolution by incorporating semantic information from Web-scale structured knowledge bases. Haghighi and Klein (2009) use a rule-based system to extract fine-grained attributes for mentions by analyzing precise constructs (e.g., appositives) in Wikipedia articles. Subsequently, Haghighi and Klein (2010) used a generative approach to learn entity types from an initial list of unambiguous mention types. Bansal and Klein (2012) use statistical analysis of Web ngram features including lexical relations. Rahman and Ng (2011) use YAGO to extract type relations for all mentions. These methods incorporate knowledge about all possible meanings of a mention. If a mention has multiple meanings, extraneous information m</context>
</contexts>
<marker>Haghighi, Klein, 2009</marker>
<rawString>Aria Haghighi and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coreference resolution in a modular, entity-centered model.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="31869" citStr="Haghighi and Klein (2010)" startWordPosition="5232" endWordPosition="5235">ribed the Stanford multi-pass sieve algorithm, which is the foundation for NECO. Earlier coreference resolution systems used shallow semantics and pioneered knowledge extraction from online encyclopedias (Ponzetto and Strube, 2006; Daum´e III and Marcu, 2005; Ng, 2007). Some recent work shows improvement in coreference resolution by incorporating semantic information from Web-scale structured knowledge bases. Haghighi and Klein (2009) use a rule-based system to extract fine-grained attributes for mentions by analyzing precise constructs (e.g., appositives) in Wikipedia articles. Subsequently, Haghighi and Klein (2010) used a generative approach to learn entity types from an initial list of unambiguous mention types. Bansal and Klein (2012) use statistical analysis of Web ngram features including lexical relations. Rahman and Ng (2011) use YAGO to extract type relations for all mentions. These methods incorporate knowledge about all possible meanings of a mention. If a mention has multiple meanings, extraneous information might be associated with it. Zheng et al. (2013) use a ranked list of candidate entities for each mention and maintain the ranked list when mentions are merged. Unlike previous work, our m</context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Aria Haghighi and Dan Klein. 2010. Coreference resolution in a modular, entity-centered model. In Human Language Technologies: Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11454" citStr="Klein and Manning, 2003" startWordPosition="1859" endWordPosition="1862">will be assigned the same link as “president” but “The governor of Alaska Sarah Palin” would not be assigned an exact link to Sarah Palin. For mentions m&apos; that do not receive an exact link, we assign a head link h(m&apos;) if the head word2 m has been linked, by setting h(m&apos;) = l(m). For instance, the head link for the mention “President Clinton” (with “Clinton” as head word) will be the Wikipedia title of Bill Clinton. We use head links for the Relaxed NEL sieve (Sec. 3.6). Next, we define L(m) to be the set con2A head word is assigned to every mention with the Stanford parser head finding rules (Klein and Manning, 2003). 291 country president city area company state region location place agency power unit body market park province manager organization owner trial site prosecutor attorney county senator stadium network building attraction government department person origin plant airport kingdom capital operation author period nominee candidate film venue Figure 3: The most commonly used fine-grained attributes from Freebase and Wikipedia (out of over 500 total attributes). taining l(m) and l(m&apos;) for all sub-phrases m&apos; of m. We add the sub-phrase links only if their confidence is higher than the confidence fo</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sayali Kulkarni</author>
<author>Amit Singh</author>
<author>Ganesh Ramakrishnan</author>
<author>Soumen Chakrabarti</author>
</authors>
<title>Collective annotation of Wikipedia entities in Web text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="33612" citStr="Kulkarni et al. (2009)" startWordPosition="5508" endWordPosition="5511">, since they reported results on portions of ACE and CoNLL datasets using gold mentions. However, our approach provides independent evidence for the benefit of NEL, and joint modeling in particular, since it outperforms the state-of-the-art Stanford sieve system (winner of the CoNLL 2011 shared task (Pradhan et al., 2011)) and other recent comparable approaches on benchmark datasets. Our work also builds on a long trajectory of work in named entity resolution stemming from SemTag (Dill et al., 2003). Section 2.2 discussed GLOW and WikipediaMiner (Ratinov et al., 2011; Milne and Witten, 2008). Kulkarni et al. (2009) present an elegant collective disambiguation model, but do not exploit the syntactic nuances gleaned by within-document coreference resolution. Hachey et al. (2013) provide an insightful summary and evaluation of different approaches to NEL. 7 Conclusions Observing that existing coreference resolution and named-entity linking have complementary strengths 297 Error Type Percentage Example Extra mentions 31.1 The other thing Paula really important is that they talk a lot about the fact ... Pronoun 27.7 However , [all 3 women gymnasts , taking part in the internationals for the first time], perf</context>
</contexts>
<marker>Kulkarni, Singh, Ramakrishnan, Chakrabarti, 2009</marker>
<rawString>Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and Soumen Chakrabarti. 2009. Collective annotation of Wikipedia entities in Web text. In Proceedings of the 2009 Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="1105" citStr="Lee et al., 2011" startWordPosition="152" endWordPosition="155">nteractions between coreference resolution and NEL. ual. The biggest challenge in coreference resolution — accounting for 42% of errors in the stateof-the-art Stanford system — is the inability to reason effectively about background semantic knowledge (Lee et al., 2013). For example, consider the sentence in Figure 1. “President” refers to “Donald Tsang” and “the park” refers to “Hong Kong Disneyland,” but automated algorithms typically lack the background knowledge to draw such inferences. Incorporating knowledge is challenging, and many efforts to do so have actually hurt performance, e.g. (Lee et al., 2011; Durrett and Klein, 2013). Named-entity linking (NEL) is the task of matching textual mentions to corresponding entities in a knowledge base, such as Wikipedia or Freebase. Such links provide rich sources of semantic knowledge about entity attributes — Freebase includes president as Tsang’s title and Disneyland as having the attribute park. But NEL is itself a challenging problem, and finding the correct link requires disambiguating based on the mention string and often non-local contextual features. For example, “Michael Eisner” is relatively unambiguous but the isolated mention “Eisner” is </context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task. In Proceedings of the Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic coreference resolution based on entitycentric, precision-ranked rules.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="759" citStr="Lee et al., 2013" startWordPosition="99" endWordPosition="102">Department of Computer Science and Electrical Engineering University of Washington {hannaneh,lzilles,lsz,weld}@cs.washington.edu [Michael Eisner], and [Donald Tsang]2 announced the grand opening of [[Hong Kong]� Disneyland]4 yesterday. [Eisner], thanked [the President]2 and welcomed [fans]5 to [the park]4. Figure 1: A text passage illustrating interactions between coreference resolution and NEL. ual. The biggest challenge in coreference resolution — accounting for 42% of errors in the stateof-the-art Stanford system — is the inability to reason effectively about background semantic knowledge (Lee et al., 2013). For example, consider the sentence in Figure 1. “President” refers to “Donald Tsang” and “the park” refers to “Hong Kong Disneyland,” but automated algorithms typically lack the background knowledge to draw such inferences. Incorporating knowledge is challenging, and many efforts to do so have actually hurt performance, e.g. (Lee et al., 2011; Durrett and Klein, 2013). Named-entity linking (NEL) is the task of matching textual mentions to corresponding entities in a knowledge base, such as Wikipedia or Freebase. Such links provide rich sources of semantic knowledge about entity attributes — </context>
<context position="4113" citStr="Lee et al., 2013" startWordPosition="608" endWordPosition="611">a new algorithm for jointly solving named entity linking and coreference resolution. Our work is related to that of Ratinov and Roth (2012), which also uses knowledge derived from an NEL system to improve coreference. However, NECO is the first joint model we know of, is purely deterministic with no learning phase, does automatic mention detection, and improves performance on both tasks. NECO extends the Stanford’s sieve-based model, in which a high recall mention detection phase is followed by a sequence of cluster merging operations ordered by decreasing precision (Raghunathan et al., 2010; Lee et al., 2013). At each step, it merges two clusters only if all available information about their respective entities is consistent. We use NEL to increase recall during the mention detection phase and introduce two new cluster-merging sieves, which compare the Freebase attributes of entities. NECO also improves NEL by initially favoring high precision linking results and then propagating links and attributes as clusters are formed. In summary we make the following contributions: • We introduce NECO, a novel, joint approach to solving coreference and NEL, demonstrating that these tasks are complementary by</context>
<context position="10005" citStr="Lee et al., 2013" startWordPosition="1591" endWordPosition="1594">ages for a subset of the clusters c E C. Step 1 detects mentions, merging the outputs of the base systems (Sec. 3.1). Step 2 repeatedly merges coreference clusters, while ensuring that NEL constraints (Sec. 3.4) are satisfied. It uses the original Stanford sieves and also two new NEL-informed sieves (Sec. 3.6). NEL links are propagated to new clusters as they are formed (Sec. 3.5). 3.1 Mention Detection In Steps 1(a-c) in Fig. 2, NECO combines mentions from the base coreference and NEL systems. Let MCR be the set of mentions returned by using Stanford’s rule-based mention detection algorithm (Lee et al., 2013). Let MNEL be the set of mentions output by the two NEL systems. NECO creates an initial set of mentions, M, by taking the union of all the mentions in MNEL and MCR. In practice, taking the union increases diversity in the mention pool. For example, it is often the case that MNEL will include sub-phrases such as “Suharto” when they are part of a larger mention “ex-dictator Suharto” that is detected in MCR. 3.2 Mention Entity Links and Pruning Step 1(d) in Fig. 2 assigns Wikipedia links to a subset of the detected mentions. For mentions m output by the base NEL systems, we assign an exact link </context>
<context position="17615" citStr="Lee et al., 2013" startWordPosition="2893" endWordPosition="2896">we merge ci with an antecedent cluster cj if (1) mj is a linked proper noun, (2) if mi or the title of its linked Wikipedia page is in the list of fine-grained attributes of mj, or (3) if h(mj) is related to the head link h(mi) according to Freebase as defined above. Because this sieve has low precision, we only allow merges between mentions that have a maximum distance of three sentences between one another. We add the Relaxed NEL sieve near the end of the pipeline, just before pronoun resolution. 4 Experimental Setup Core Components and Baselines The Stanford sieve-based coreference system (Lee et al., 2013), the GLOW NEL system (Ratinov et al., 2011), and WikipediaMiner (Milne and Witten, 2008) provide core functionality for our joint model, and are also the state-of-the-art baselines against which we measure performance. Parameter Settings Based on performance on the development set, we set the GLOW’s confidence parameter to 1.0 and WikipediaMiner’s to 0.4 to assure high-precision NEL. We also optimized for the set of fine-grained attributes to import from Wikipedia and Freebase, and the best way to incorporate the NEL constraints into the sieve architecture. Datasets We report results on the f</context>
<context position="20552" citStr="Lee et al., 2013" startWordPosition="3356" endWordPosition="3359"> 46.6 71.8 69.7 70.7 48.6 27.0 34.7 No Constraints 42.3 49.3 45.5 68.3 72.3 70.2 44.2 28.6 34.7 Table 1: Coreference results on ACE2004-NWIRE with predicted mentions and automatic linking. coreference software for ACE2004 and using the CoNLL scorer for the CoNLL 2011 dataset. 5 Experimental Results We first look at NECO’s performance at coreference resolution and then evaluate its ability at NEL. 5.1 Coref. Results with Predicted Mentions Overall System Performance on ACE Data Table 1 shows NECO’s performance at coreference resolution on ACE-2004 compared to the Stanford sieve implementation (Lee et al., 2013). The table shows that NECO has both significantly improved precision and recall compared to the Stanford baseline, across all metrics. We generally observe larger gains in MUC due to better mention detection and the Relaxed NEL Sieve. Contribution of System Components Table 1 also details the performance of four variants of our system that ablate various components and features. Specifically, we consider the following cases: • No NEL Mentions: We discard additional mentions, MNEL, provided by NEL (Sec. 3.1). This increases B3 precision at the expense of recall. Inspection shows that some of t</context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic coreference resolution based on entitycentric, precision-ranked rules. Computational Linguistics, 39(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Milne</author>
<author>Ian H Witten</author>
</authors>
<title>Learning to link with Wikipedia.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACM Conference on Information and Knowledge Management.</booktitle>
<contexts>
<context position="6706" citStr="Milne and Witten, 2008" startWordPosition="1006" endWordPosition="1009">of the text, including: (1) speaker identification, (2-3) exact and relaxed string matches between mentions, (4) precise constructs, including appositives, acronyms and demonyms, (5-9) different notions of strict and relaxed head matches between mentions, and finally (10) a number of syntactic and distance cues for pronoun resolution. 2.2 Named Entity Linking Named-entity linking (NEL) is the task of identifying mentions in a text and linking them to the entity they name in a knowledge base, usually Wikipedia. NECO uses two existing NEL systems: GLOW (Ratinov et al., 2011) and WikipediaMiner (Milne and Witten, 2008). WikipediaMiner links mentions based on a notion of semantic similarity to Wikipedia pages, considering all substrings up to a fixed length. Since there are often many possible links, it disambiguates by choosing the entity whose Wikipedia page is most semantically related to the nearby context of the mention. The semantic scoring function includes ngram statistics and also counts shared links to other unambiguous mentions in the text. GLOW finds mentions by selecting all the NPs and named entities in the text. Linking is framed as an integer linear programming optimization problem that takes</context>
<context position="17704" citStr="Milne and Witten, 2008" startWordPosition="2907" endWordPosition="2910"> mi or the title of its linked Wikipedia page is in the list of fine-grained attributes of mj, or (3) if h(mj) is related to the head link h(mi) according to Freebase as defined above. Because this sieve has low precision, we only allow merges between mentions that have a maximum distance of three sentences between one another. We add the Relaxed NEL sieve near the end of the pipeline, just before pronoun resolution. 4 Experimental Setup Core Components and Baselines The Stanford sieve-based coreference system (Lee et al., 2013), the GLOW NEL system (Ratinov et al., 2011), and WikipediaMiner (Milne and Witten, 2008) provide core functionality for our joint model, and are also the state-of-the-art baselines against which we measure performance. Parameter Settings Based on performance on the development set, we set the GLOW’s confidence parameter to 1.0 and WikipediaMiner’s to 0.4 to assure high-precision NEL. We also optimized for the set of fine-grained attributes to import from Wikipedia and Freebase, and the best way to incorporate the NEL constraints into the sieve architecture. Datasets We report results on the following three datasets: ACE2004-NWIRE, CONLL2011, and ACE2004-NWIRE-NEL. ACE2004-NWIRE, </context>
<context position="29251" citStr="Milne and Witten, 2008" startWordPosition="4830" endWordPosition="4833">ghighi and Klein (2009), Poon and Domingos (2008), Finkel and Manning (2008) and the Stanford sieve algorithm across all metrics. Our method shows a relatively smaller gain in precision, because this condition adds no benefit to our technique of using NEL information for pruning mentions. 5.4 Improving Named Entity Linking While our previous experiments show that namedentity linking can improve coreference resolution, we now address the question of whether coreference techniques can help NEL. We compare NECO with a baseline ensemble4 composed of GLOW (Ratinov et al., 2011) and WikipediaMiner (Milne and Witten, 2008) on our ACE2004-NWIRE-NEL dataset (Table 6). Our system gains about 8% in absolute recall and 5% in absolute precision. For instance, our system correctly adds links from “Bullock” to the entity Sandra Bullock because coreference resolution merges two mentions. In another example, it correctly links “company” to Nokia. Overall, there is a 21% relative reduction in F1 error. 4We take the union of all the links returned by GLOW and WikipediaMiner, but if they link a mention to two different entities, we use only the output of WikipediaMiner. 296 Method F1 Precision Recall NECO 70.6 72.0 69.2 Bas</context>
<context position="33588" citStr="Milne and Witten, 2008" startWordPosition="5504" endWordPosition="5507">e results of both systems, since they reported results on portions of ACE and CoNLL datasets using gold mentions. However, our approach provides independent evidence for the benefit of NEL, and joint modeling in particular, since it outperforms the state-of-the-art Stanford sieve system (winner of the CoNLL 2011 shared task (Pradhan et al., 2011)) and other recent comparable approaches on benchmark datasets. Our work also builds on a long trajectory of work in named entity resolution stemming from SemTag (Dill et al., 2003). Section 2.2 discussed GLOW and WikipediaMiner (Ratinov et al., 2011; Milne and Witten, 2008). Kulkarni et al. (2009) present an elegant collective disambiguation model, but do not exploit the syntactic nuances gleaned by within-document coreference resolution. Hachey et al. (2013) provide an insightful summary and evaluation of different approaches to NEL. 7 Conclusions Observing that existing coreference resolution and named-entity linking have complementary strengths 297 Error Type Percentage Example Extra mentions 31.1 The other thing Paula really important is that they talk a lot about the fact ... Pronoun 27.7 However , [all 3 women gymnasts , taking part in the internationals f</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>Dan Milne and Ian H. Witten. 2008. Learning to link with Wikipedia. In Proceedings of the ACM Conference on Information and Knowledge Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Shallow semantics for coreference resolution.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="31513" citStr="Ng, 2007" startWordPosition="5188" endWordPosition="5189"> both have the country attribute. However, many recall errors are also caused by the lack of fine-grained attributes. Finding the ideal set of fine-grained attributes remains an open problem. 6 Related Work Coreference resolution has a fifty year history which defies brief summarization; see Ng (2010) for a recent survey. Section 2.1 described the Stanford multi-pass sieve algorithm, which is the foundation for NECO. Earlier coreference resolution systems used shallow semantics and pioneered knowledge extraction from online encyclopedias (Ponzetto and Strube, 2006; Daum´e III and Marcu, 2005; Ng, 2007). Some recent work shows improvement in coreference resolution by incorporating semantic information from Web-scale structured knowledge bases. Haghighi and Klein (2009) use a rule-based system to extract fine-grained attributes for mentions by analyzing precise constructs (e.g., appositives) in Wikipedia articles. Subsequently, Haghighi and Klein (2010) used a generative approach to learn entity types from an initial list of unambiguous mention types. Bansal and Klein (2012) use statistical analysis of Web ngram features including lexical relations. Rahman and Ng (2011) use YAGO to extract ty</context>
</contexts>
<marker>Ng, 2007</marker>
<rawString>Vincent Ng. 2007. Shallow semantics for coreference resolution. In Proceedings of the 20th International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Supervised noun phrase coreference research: The first fifteen years.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="31206" citStr="Ng (2010)" startWordPosition="5144" endWordPosition="5145">rained attributes caused precision errors in cases where many proper noun mentions were potential antecedents for a common noun. Although attributes such as country are useful for resolving a generic “country” mention, this information is insufficient when two distinct mentions such as “China” and “Russia” both have the country attribute. However, many recall errors are also caused by the lack of fine-grained attributes. Finding the ideal set of fine-grained attributes remains an open problem. 6 Related Work Coreference resolution has a fifty year history which defies brief summarization; see Ng (2010) for a recent survey. Section 2.1 described the Stanford multi-pass sieve algorithm, which is the foundation for NECO. Earlier coreference resolution systems used shallow semantics and pioneered knowledge extraction from online encyclopedias (Ponzetto and Strube, 2006; Daum´e III and Marcu, 2005; Ng, 2007). Some recent work shows improvement in coreference resolution by incorporating semantic information from Web-scale structured knowledge bases. Haghighi and Klein (2009) use a rule-based system to extract fine-grained attributes for mentions by analyzing precise constructs (e.g., appositives)</context>
</contexts>
<marker>Ng, 2010</marker>
<rawString>Vincent Ng. 2010. Supervised noun phrase coreference research: The first fifteen years. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>The ACE</title>
<date>2004</date>
<contexts>
<context position="18359" citStr="NIST, 2004" startWordPosition="3006" endWordPosition="3007">del, and are also the state-of-the-art baselines against which we measure performance. Parameter Settings Based on performance on the development set, we set the GLOW’s confidence parameter to 1.0 and WikipediaMiner’s to 0.4 to assure high-precision NEL. We also optimized for the set of fine-grained attributes to import from Wikipedia and Freebase, and the best way to incorporate the NEL constraints into the sieve architecture. Datasets We report results on the following three datasets: ACE2004-NWIRE, CONLL2011, and ACE2004-NWIRE-NEL. ACE2004-NWIRE, the newswire subset of the ACE 2004 corpus (NIST, 2004), includes 128 documents. The CONLL2011 coreference dataset includes text from five different domains: broadcast conversation (BC), broadcast news (BN), magazine (MZ), newswire (NW), and web data (WB) (Pradhan et al., 2011). The broadcast conversation and broadcast news domains consist of transcripts, whereas magazine and newswire contain more standard written text. The development data includes 303 documents and the test data includes 322 documents. We created ACE2004-NWIRE-NEL by taking a subset of ACE2004-NWIRE and annotating with gold-standard entity links. We segment and link all the expr</context>
</contexts>
<marker>NIST, 2004</marker>
<rawString>NIST. 2004. The ACE 2004 evaluation planXPToolkit architecture.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Exploiting semantic role labeling, Wordnet and Wikipedia for coreference resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the North American Association for Natural Language Processing on Human Language Technologies.</booktitle>
<contexts>
<context position="31474" citStr="Ponzetto and Strube, 2006" startWordPosition="5179" endWordPosition="5182">when two distinct mentions such as “China” and “Russia” both have the country attribute. However, many recall errors are also caused by the lack of fine-grained attributes. Finding the ideal set of fine-grained attributes remains an open problem. 6 Related Work Coreference resolution has a fifty year history which defies brief summarization; see Ng (2010) for a recent survey. Section 2.1 described the Stanford multi-pass sieve algorithm, which is the foundation for NECO. Earlier coreference resolution systems used shallow semantics and pioneered knowledge extraction from online encyclopedias (Ponzetto and Strube, 2006; Daum´e III and Marcu, 2005; Ng, 2007). Some recent work shows improvement in coreference resolution by incorporating semantic information from Web-scale structured knowledge bases. Haghighi and Klein (2009) use a rule-based system to extract fine-grained attributes for mentions by analyzing precise constructs (e.g., appositives) in Wikipedia articles. Subsequently, Haghighi and Klein (2010) used a generative approach to learn entity types from an initial list of unambiguous mention types. Bansal and Klein (2012) use statistical analysis of Web ngram features including lexical relations. Rahm</context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2006. Exploiting semantic role labeling, Wordnet and Wikipedia for coreference resolution. In Proceedings of the North American Association for Natural Language Processing on Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint unsupervised coreference resolution with Markov logic.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="27413" citStr="Poon and Domingos (2008)" startWordPosition="4537" endWordPosition="4540">.0 73.9 Stanford Sieves 84.5 72.2 77.8 89.9 77.7 83.4 89.9 57.3 68.1 Predicted Mentions NECO + Gold NEL 56.4 58.8 57.5 78.2 78.3 78.3 68.0 54.3 60.4 NECO 51.3 53.5 52.4 76.5 76.4 76.5 61.2 45.6 52.2 Stanford Sieves 43.9 46.4 45.1 74.4 74.2 74.3 51.3 36.1 42.4 Table 4: Coreference results on ACE2004-NWIRE-NEL with gold and predicted mentions and gold or automatic linking. Method P MUC F1 P B3 F1 P Pairwise R R R F1 NECO 85.0 76.6 80.6 87.6 76.4 81.6 79.3 56.1 65.8 Stanford Sieves 84.6 75.1 79.6 87.3 74.1 80.2 79.4 50.1 61.4 Haghighi and Klein (2009) 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7 Poon and Domingos (2008) 71.3 70.5 70.9 - - - 62.6 38.9 48.0 Finkel and Manning (2008) 78.7 58.5 67.1 86.8 65.2 74.5 76.1 44.2 55.9 Table 5: Coreference results on ACE2004-NWIRE with gold mentions and automatic linking. sons. First, it reduces the coreference errors caused by incorrect NEL links. For instance, gold linking replaces the erroneous link generated by our NEL systems for “Nasser al-Kidwa” to the correct Wikipedia entity. As another example, two mentions of “Rutgers” will not be merged if one links to the university and the other links to their football team. Second, gold linking leads to better mention de</context>
<context position="28677" citStr="Poon and Domingos (2008)" startWordPosition="4742" endWordPosition="4745">instance, under gold linking, the whole mention, “The governor of Alaska, Sarah Palin,” is linked to the politician, while automatic linking systems only link the substring containing her name, “Sarah Palin.” Still, gold NEL cannot compensate for all coreference errors in cases of generic or unlinked entities. 5.3 Coreference Results with Gold Mentions Many of the previous papers evaluate coreference resolution assuming gold mentions so we also run under that condition (Table 5) using ACE2004- NWIRE data. As the table shows, with gold mentions our system outperforms Haghighi and Klein (2009), Poon and Domingos (2008), Finkel and Manning (2008) and the Stanford sieve algorithm across all metrics. Our method shows a relatively smaller gain in precision, because this condition adds no benefit to our technique of using NEL information for pruning mentions. 5.4 Improving Named Entity Linking While our previous experiments show that namedentity linking can improve coreference resolution, we now address the question of whether coreference techniques can help NEL. We compare NECO with a baseline ensemble4 composed of GLOW (Ratinov et al., 2011) and WikipediaMiner (Milne and Witten, 2008) on our ACE2004-NWIRE-NEL </context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2008. Joint unsupervised coreference resolution with Markov logic. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Ralph Weischedel</author>
<author>Nianwen Xue</author>
</authors>
<title>CoNLL-2011 shared task: modeling unrestricted coreference in OntoNotes.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task.</booktitle>
<contexts>
<context position="18582" citStr="Pradhan et al., 2011" startWordPosition="3035" endWordPosition="3038">er’s to 0.4 to assure high-precision NEL. We also optimized for the set of fine-grained attributes to import from Wikipedia and Freebase, and the best way to incorporate the NEL constraints into the sieve architecture. Datasets We report results on the following three datasets: ACE2004-NWIRE, CONLL2011, and ACE2004-NWIRE-NEL. ACE2004-NWIRE, the newswire subset of the ACE 2004 corpus (NIST, 2004), includes 128 documents. The CONLL2011 coreference dataset includes text from five different domains: broadcast conversation (BC), broadcast news (BN), magazine (MZ), newswire (NW), and web data (WB) (Pradhan et al., 2011). The broadcast conversation and broadcast news domains consist of transcripts, whereas magazine and newswire contain more standard written text. The development data includes 303 documents and the test data includes 322 documents. We created ACE2004-NWIRE-NEL by taking a subset of ACE2004-NWIRE and annotating with gold-standard entity links. We segment and link all the expressions in text that refer to Wikipedia pages, allowing for nested linking. For instance, both the phrase “Hong Kong Disneyland,” and the sub-phrase “Hong Kong” are linked. This dataset includes 12 documents and 350 linked </context>
<context position="33313" citStr="Pradhan et al., 2011" startWordPosition="5460" endWordPosition="5463"> improve coreference resolution, but did not consider a joint approach. They extracted attributes from Wikipedia categories and used them as features in a learned mention-pair model, but did not do mention detection. Unfortunately, it is difficult to compare directly to the results of both systems, since they reported results on portions of ACE and CoNLL datasets using gold mentions. However, our approach provides independent evidence for the benefit of NEL, and joint modeling in particular, since it outperforms the state-of-the-art Stanford sieve system (winner of the CoNLL 2011 shared task (Pradhan et al., 2011)) and other recent comparable approaches on benchmark datasets. Our work also builds on a long trajectory of work in named entity resolution stemming from SemTag (Dill et al., 2003). Section 2.2 discussed GLOW and WikipediaMiner (Ratinov et al., 2011; Milne and Witten, 2008). Kulkarni et al. (2009) present an elegant collective disambiguation model, but do not exploit the syntactic nuances gleaned by within-document coreference resolution. Hachey et al. (2013) provide an insightful summary and evaluation of different approaches to NEL. 7 Conclusions Observing that existing coreference resoluti</context>
</contexts>
<marker>Pradhan, Ramshaw, Marcus, Palmer, Weischedel, Xue, 2011</marker>
<rawString>Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha Palmer, Ralph Weischedel, and Nianwen Xue. 2011. CoNLL-2011 shared task: modeling unrestricted coreference in OntoNotes. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Raghunathan</author>
<author>Heeyoung Lee</author>
<author>Sudarshan Rangarajan</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A multipass sieve for coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="4094" citStr="Raghunathan et al., 2010" startWordPosition="604" endWordPosition="607">guistics We present NECO, a new algorithm for jointly solving named entity linking and coreference resolution. Our work is related to that of Ratinov and Roth (2012), which also uses knowledge derived from an NEL system to improve coreference. However, NECO is the first joint model we know of, is purely deterministic with no learning phase, does automatic mention detection, and improves performance on both tasks. NECO extends the Stanford’s sieve-based model, in which a high recall mention detection phase is followed by a sequence of cluster merging operations ordered by decreasing precision (Raghunathan et al., 2010; Lee et al., 2013). At each step, it merges two clusters only if all available information about their respective entities is consistent. We use NEL to increase recall during the mention detection phase and introduce two new cluster-merging sieves, which compare the Freebase attributes of entities. NECO also improves NEL by initially favoring high precision linking results and then propagating links and attributes as clusters are formed. In summary we make the following contributions: • We introduce NECO, a novel, joint approach to solving coreference and NEL, demonstrating that these tasks a</context>
</contexts>
<marker>Raghunathan, Lee, Rangarajan, Chambers, Surdeanu, Jurafsky, Manning, 2010</marker>
<rawString>Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky, and Christopher Manning. 2010. A multipass sieve for coreference resolution. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Coreference resolution with world knowledge.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<contexts>
<context position="32090" citStr="Rahman and Ng (2011)" startWordPosition="5267" endWordPosition="5270">2006; Daum´e III and Marcu, 2005; Ng, 2007). Some recent work shows improvement in coreference resolution by incorporating semantic information from Web-scale structured knowledge bases. Haghighi and Klein (2009) use a rule-based system to extract fine-grained attributes for mentions by analyzing precise constructs (e.g., appositives) in Wikipedia articles. Subsequently, Haghighi and Klein (2010) used a generative approach to learn entity types from an initial list of unambiguous mention types. Bansal and Klein (2012) use statistical analysis of Web ngram features including lexical relations. Rahman and Ng (2011) use YAGO to extract type relations for all mentions. These methods incorporate knowledge about all possible meanings of a mention. If a mention has multiple meanings, extraneous information might be associated with it. Zheng et al. (2013) use a ranked list of candidate entities for each mention and maintain the ranked list when mentions are merged. Unlike previous work, our method relies on NEL systems to disambiguate possible meanings of a mention and capture highprecision semantic knowledge from Wikipedia categories and Freebase notable types. Ratinov and Roth (2012) investigated using NEL </context>
</contexts>
<marker>Rahman, Ng, 2011</marker>
<rawString>Altaf Rahman and Vincent Ng. 2011. Coreference resolution with world knowledge. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Learning-based multisieve co-reference resolution with knowledge.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="3635" citStr="Ratinov and Roth (2012)" startWordPosition="531" endWordPosition="534">ave been largely studied in isolation. This paper demonstrates that they are complementary by introducing a simple joint model that improves performance on both tasks. Coreference resolution is the task of determining when two textual mentions name the same individ289 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 289–299, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics We present NECO, a new algorithm for jointly solving named entity linking and coreference resolution. Our work is related to that of Ratinov and Roth (2012), which also uses knowledge derived from an NEL system to improve coreference. However, NECO is the first joint model we know of, is purely deterministic with no learning phase, does automatic mention detection, and improves performance on both tasks. NECO extends the Stanford’s sieve-based model, in which a high recall mention detection phase is followed by a sequence of cluster merging operations ordered by decreasing precision (Raghunathan et al., 2010; Lee et al., 2013). At each step, it merges two clusters only if all available information about their respective entities is consistent. We</context>
<context position="32666" citStr="Ratinov and Roth (2012)" startWordPosition="5360" endWordPosition="5363">cluding lexical relations. Rahman and Ng (2011) use YAGO to extract type relations for all mentions. These methods incorporate knowledge about all possible meanings of a mention. If a mention has multiple meanings, extraneous information might be associated with it. Zheng et al. (2013) use a ranked list of candidate entities for each mention and maintain the ranked list when mentions are merged. Unlike previous work, our method relies on NEL systems to disambiguate possible meanings of a mention and capture highprecision semantic knowledge from Wikipedia categories and Freebase notable types. Ratinov and Roth (2012) investigated using NEL to improve coreference resolution, but did not consider a joint approach. They extracted attributes from Wikipedia categories and used them as features in a learned mention-pair model, but did not do mention detection. Unfortunately, it is difficult to compare directly to the results of both systems, since they reported results on portions of ACE and CoNLL datasets using gold mentions. However, our approach provides independent evidence for the benefit of NEL, and joint modeling in particular, since it outperforms the state-of-the-art Stanford sieve system (winner of th</context>
</contexts>
<marker>Ratinov, Roth, 2012</marker>
<rawString>Lev Ratinov and Dan Roth. 2012. Learning-based multisieve co-reference resolution with knowledge. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
<author>Doug Downey</author>
<author>Mike Anderson</author>
</authors>
<title>Local and global algorithms for disambiguation to Wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6662" citStr="Ratinov et al., 2011" startWordPosition="999" endWordPosition="1002">owing order, looking at different aspects of the text, including: (1) speaker identification, (2-3) exact and relaxed string matches between mentions, (4) precise constructs, including appositives, acronyms and demonyms, (5-9) different notions of strict and relaxed head matches between mentions, and finally (10) a number of syntactic and distance cues for pronoun resolution. 2.2 Named Entity Linking Named-entity linking (NEL) is the task of identifying mentions in a text and linking them to the entity they name in a knowledge base, usually Wikipedia. NECO uses two existing NEL systems: GLOW (Ratinov et al., 2011) and WikipediaMiner (Milne and Witten, 2008). WikipediaMiner links mentions based on a notion of semantic similarity to Wikipedia pages, considering all substrings up to a fixed length. Since there are often many possible links, it disambiguates by choosing the entity whose Wikipedia page is most semantically related to the nearby context of the mention. The semantic scoring function includes ngram statistics and also counts shared links to other unambiguous mentions in the text. GLOW finds mentions by selecting all the NPs and named entities in the text. Linking is framed as an integer linear</context>
<context position="17659" citStr="Ratinov et al., 2011" startWordPosition="2901" endWordPosition="2904">j if (1) mj is a linked proper noun, (2) if mi or the title of its linked Wikipedia page is in the list of fine-grained attributes of mj, or (3) if h(mj) is related to the head link h(mi) according to Freebase as defined above. Because this sieve has low precision, we only allow merges between mentions that have a maximum distance of three sentences between one another. We add the Relaxed NEL sieve near the end of the pipeline, just before pronoun resolution. 4 Experimental Setup Core Components and Baselines The Stanford sieve-based coreference system (Lee et al., 2013), the GLOW NEL system (Ratinov et al., 2011), and WikipediaMiner (Milne and Witten, 2008) provide core functionality for our joint model, and are also the state-of-the-art baselines against which we measure performance. Parameter Settings Based on performance on the development set, we set the GLOW’s confidence parameter to 1.0 and WikipediaMiner’s to 0.4 to assure high-precision NEL. We also optimized for the set of fine-grained attributes to import from Wikipedia and Freebase, and the best way to incorporate the NEL constraints into the sieve architecture. Datasets We report results on the following three datasets: ACE2004-NWIRE, CONL</context>
<context position="29207" citStr="Ratinov et al., 2011" startWordPosition="4824" endWordPosition="4827">th gold mentions our system outperforms Haghighi and Klein (2009), Poon and Domingos (2008), Finkel and Manning (2008) and the Stanford sieve algorithm across all metrics. Our method shows a relatively smaller gain in precision, because this condition adds no benefit to our technique of using NEL information for pruning mentions. 5.4 Improving Named Entity Linking While our previous experiments show that namedentity linking can improve coreference resolution, we now address the question of whether coreference techniques can help NEL. We compare NECO with a baseline ensemble4 composed of GLOW (Ratinov et al., 2011) and WikipediaMiner (Milne and Witten, 2008) on our ACE2004-NWIRE-NEL dataset (Table 6). Our system gains about 8% in absolute recall and 5% in absolute precision. For instance, our system correctly adds links from “Bullock” to the entity Sandra Bullock because coreference resolution merges two mentions. In another example, it correctly links “company” to Nokia. Overall, there is a 21% relative reduction in F1 error. 4We take the union of all the links returned by GLOW and WikipediaMiner, but if they link a mention to two different entities, we use only the output of WikipediaMiner. 296 Method</context>
<context position="33563" citStr="Ratinov et al., 2011" startWordPosition="5500" endWordPosition="5503">compare directly to the results of both systems, since they reported results on portions of ACE and CoNLL datasets using gold mentions. However, our approach provides independent evidence for the benefit of NEL, and joint modeling in particular, since it outperforms the state-of-the-art Stanford sieve system (winner of the CoNLL 2011 shared task (Pradhan et al., 2011)) and other recent comparable approaches on benchmark datasets. Our work also builds on a long trajectory of work in named entity resolution stemming from SemTag (Dill et al., 2003). Section 2.2 discussed GLOW and WikipediaMiner (Ratinov et al., 2011; Milne and Witten, 2008). Kulkarni et al. (2009) present an elegant collective disambiguation model, but do not exploit the syntactic nuances gleaned by within-document coreference resolution. Hachey et al. (2013) provide an insightful summary and evaluation of different approaches to NEL. 7 Conclusions Observing that existing coreference resolution and named-entity linking have complementary strengths 297 Error Type Percentage Example Extra mentions 31.1 The other thing Paula really important is that they talk a lot about the fact ... Pronoun 27.7 However , [all 3 women gymnasts , taking par</context>
</contexts>
<marker>Ratinov, Roth, Downey, Anderson, 2011</marker>
<rawString>Lev Ratinov, Dan Roth, Doug Downey, and Mike Anderson. 2011. Local and global algorithms for disambiguation to Wikipedia. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
</authors>
<title>Coreference resolution across corpora: languages, coding schemes, and preprocessing information.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="19584" citStr="Recasens and Hovy, 2010" startWordPosition="3190" endWordPosition="3193">ssions in text that refer to Wikipedia pages, allowing for nested linking. For instance, both the phrase “Hong Kong Disneyland,” and the sub-phrase “Hong Kong” are linked. This dataset includes 12 documents and 350 linked entities. Metrics We evaluate our system using MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and pairwise scores. MUC is a link-based metric which measures how many clusters need to be merged to cover the gold clusters and favors larger clusters; B3 computes the proportion of intersection between predicted and gold clusters for every mention and favors singletons (Recasens and Hovy, 2010). We computed the scores using the Stanford 293 Method P MUC F1 P B3 F1 P Pairwise R R R F1 Stanford Sieves 39.9 46.2 42.8 67.9 71.8 69.8 44.2 29.7 35.6 NECO 46.8 52.5 49.5 70.4 72.6 71.5 51.5 34.6 41.4 No NEL Mentions 46.1 48.3 47.2 71.4 70.0 70.9 49.7 30.9 38.1 No Mention Pruning 43.6 45.6 44.6 70.5 69.9 70.2 46.2 29.4 35.9 No Attributes 45.9 47.4 46.6 71.8 69.7 70.7 48.6 27.0 34.7 No Constraints 42.3 49.3 45.5 68.3 72.3 70.2 44.2 28.6 34.7 Table 1: Coreference results on ACE2004-NWIRE with predicted mentions and automatic linking. coreference software for ACE2004 and using the CoNLL scorer </context>
</contexts>
<marker>Recasens, Hovy, 2010</marker>
<rawString>Marta Recasens and Eduard Hovy. 2010. Coreference resolution across corpora: languages, coding schemes, and preprocessing information. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th conference on Message Understanding.</booktitle>
<contexts>
<context position="19254" citStr="Vilain et al., 1995" startWordPosition="3135" endWordPosition="3139">ins consist of transcripts, whereas magazine and newswire contain more standard written text. The development data includes 303 documents and the test data includes 322 documents. We created ACE2004-NWIRE-NEL by taking a subset of ACE2004-NWIRE and annotating with gold-standard entity links. We segment and link all the expressions in text that refer to Wikipedia pages, allowing for nested linking. For instance, both the phrase “Hong Kong Disneyland,” and the sub-phrase “Hong Kong” are linked. This dataset includes 12 documents and 350 linked entities. Metrics We evaluate our system using MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and pairwise scores. MUC is a link-based metric which measures how many clusters need to be merged to cover the gold clusters and favors larger clusters; B3 computes the proportion of intersection between predicted and gold clusters for every mention and favors singletons (Recasens and Hovy, 2010). We computed the scores using the Stanford 293 Method P MUC F1 P B3 F1 P Pairwise R R R F1 Stanford Sieves 39.9 46.2 42.8 67.9 71.8 69.8 44.2 29.7 35.6 NECO 46.8 52.5 49.5 70.4 72.6 71.5 51.5 34.6 41.4 No NEL Mentions 46.1 48.3 47.2 71.4 70.0 70.9 49.7 30.9 38.1 No Men</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings of the 6th conference on Message Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiaping Zheng</author>
<author>Luke Vilnis</author>
<author>Sameer Singh</author>
<author>Jinho D Choi</author>
<author>Andrew McCallum</author>
</authors>
<title>Dynamic knowledge-base alignment for coreference resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="32329" citStr="Zheng et al. (2013)" startWordPosition="5306" endWordPosition="5309">extract fine-grained attributes for mentions by analyzing precise constructs (e.g., appositives) in Wikipedia articles. Subsequently, Haghighi and Klein (2010) used a generative approach to learn entity types from an initial list of unambiguous mention types. Bansal and Klein (2012) use statistical analysis of Web ngram features including lexical relations. Rahman and Ng (2011) use YAGO to extract type relations for all mentions. These methods incorporate knowledge about all possible meanings of a mention. If a mention has multiple meanings, extraneous information might be associated with it. Zheng et al. (2013) use a ranked list of candidate entities for each mention and maintain the ranked list when mentions are merged. Unlike previous work, our method relies on NEL systems to disambiguate possible meanings of a mention and capture highprecision semantic knowledge from Wikipedia categories and Freebase notable types. Ratinov and Roth (2012) investigated using NEL to improve coreference resolution, but did not consider a joint approach. They extracted attributes from Wikipedia categories and used them as features in a learned mention-pair model, but did not do mention detection. Unfortunately, it is</context>
</contexts>
<marker>Zheng, Vilnis, Singh, Choi, McCallum, 2013</marker>
<rawString>Jiaping Zheng, Luke Vilnis, Sameer Singh, Jinho D. Choi, and Andrew McCallum. 2013. Dynamic knowledge-base alignment for coreference resolution. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
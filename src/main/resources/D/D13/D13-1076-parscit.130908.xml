<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000177">
<title confidence="0.998892">
Using Paraphrases and Lexical Semantics to Improve the Accuracy and the
Robustness of Supervised Models in Situated Dialogue Systems
</title>
<author confidence="0.985695">
Claire Gardent Lina M. Rojas Barahona
</author>
<affiliation confidence="0.965022">
CNRS/LORIA, Nancy Université de Lorraine/LORIA, Nancy
</affiliation>
<email confidence="0.997357">
claire.gardent@loria.fr lina.rojas@loria.fr
</email>
<sectionHeader confidence="0.995604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999738">
This paper explores to what extent lemmati-
sation, lexical resources, distributional seman-
tics and paraphrases can increase the accuracy
of supervised models for dialogue manage-
ment. The results suggest that each of these
factors can help improve performance but that
the impact will vary depending on their com-
bination and on the evaluation mode.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999638580645161">
One strand of work in dialog research targets the
rapid prototyping of virtual humans capable of con-
ducting a conversation with humans in the context
of a virtual world. In particular, question answering
(QA) characters can respond to a restricted set of
topics after training on a set of dialogs whose utter-
ances are annotated with dialogue acts (Leuski and
Traum, 2008).
As argued in (Sagae et al., 2009), the size of the
training corpus is a major factor in allowing QA
characters that are both robust and accurate. In ad-
dition, the training corpus should arguably be of
good quality in that (i) it should contain the various
ways of expressing the same content (paraphrases)
and (ii) the data should not be skewed. In sum, the
ideal training data should be large (more data is
better data) ; balanced (similar amount of data for
each class targeted by the classifier) and varied (it
should encompass the largest possible number of
paraphrases and synonyms for the utterances of each
class).
In this paper, we explore different ways of im-
proving and complementing the training data of a
supervised QA character. We expand the size and
the quality (less skewed data) of the training corpus
using paraphrase generation techniques. We com-
pare the performance obtained on lemmatised vs.
non lemmatised data. And we investigate how vari-
ous resources (synonym dictionaries, WordNet, dis-
tributional neighbours) can be used to handle unseen
words at run time.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.9999121">
Previous work on improving robustness of super-
vised dialog systems includes detecting and han-
dling out of domain utterances for generating feed-
back (Lane et al., 2004) ; using domain-restricted
lexical semantics (Hardy et al., 2004) ; and work on
manual data expansion (DeVault et al., 2011). Our
work follows up on this research but provides a sys-
tematic investigation of how data expansion, lemma-
tisation and synonym handling impacts the perfor-
mance of a supervised QA engine.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9997515">
We run our experiments on a dialog engine de-
veloped for a serious game called Mission Plastech-
nologie. In this game, the player must interact with
different virtual humans through a sequence of 12
subdialogs, each of them occurring in a different part
of the virtual world.
Training Data. The training corpus consists of
around 1250 Human-Human dialogues which were
manually annotated with dialog moves. As the fol-
lowing dialog excerpt illustrates, the dialogs are con-
ducted in French and each dialog turn is manu-
ally annotated using a set of 28 dialog acts. For
</bodyText>
<page confidence="0.962511">
808
</page>
<bodyText confidence="0.8168626">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 808–813,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
a more detailed presentation of the training corpus
and of the annotation scheme, the reader is referred
to (Rojas-Barahona et al., 2012a)
</bodyText>
<note confidence="0.413928">
dialog: 01_dialogDirecteur-Tue Jun 14 11 :04 :23 2011
</note>
<equation confidence="0.966277285714286">
&gt;M.Jasper : Bonjour, je suis M.Jasper le directeur.  ||greet
(Hello, I am the director, Mr. Jasper.)
&gt;M.Jasper : Qu’est-ce que je peux faire pour vous ?  ||ask(task(X))
(What can I do for you ?)
&gt;Lucas : je dois sauver mon oncle  ||first_step
(I must rescue my uncle)
&gt;M.Jasper : Pour faire votre manette, il vous faut
</equation>
<bodyText confidence="0.67786725">
des plans. Allez voir dans le bureau d’études,
ils devraient y être.  ||inform(do(first_step))
(To build the joystick you will need the plans.
You will find them in the Designing Office.)
</bodyText>
<equation confidence="0.844771">
&gt;M.Jasper : Bonne Chance!  ||quit
(Good Luck!)
</equation>
<bodyText confidence="0.999323384615385">
Dialog Systems For our experiments, we use a hy-
brid dialog system similar to that described in (Ro-
jas Barahona et al., 2012b; Rojas Barahona and
Gardent, 2012). This system combines a classifier
for interpreting the players utterances with an infor-
mation state dialog manager which selects an appro-
priate system response based on the dialog move as-
signed by the classifier to the user turn. The clas-
sifier is a logistic regression classifier 1 which was
trained for each subdialog in the game. The features
used for training are the set of content words which
are associated with a given dialog move and which
remain after TF*IDF 2 filtering. Note that in this ex-
periment, we do not use contextual features such as
the dialog acts labeling the previous turns. There are
two reasons for this. First, we want to focus on the
impact of synonym handling, paraphrasing and lem-
matisation on dialog management. Removing con-
textual features allows us to focus on how content
features (content words) can be improved by these
mechanisms. Second, when evaluating on the H-C
corpus (see below), contextual features are often in-
correct (because the system might incorrectly inter-
pret and thus label a user turn). Excluding contextual
features from training allows for a fair comparison
between the H-H and the H-C evaluation.
</bodyText>
<listItem confidence="0.6370332">
Test Data and Evaluation Metrics We use accu-
1. We used MALLET (McCallum, 2002) for the LR classi-
fier with L1 Regularisation.
2. TF*IDF = Term Frequency*Inverse Document Fre-
quency
</listItem>
<bodyText confidence="0.999656181818182">
racy (the number of correct classifications divided
by the number of instances in the testset) to mea-
sure performance and we carry out two types of
evaluation. On the one hand, we use 10-fold cross-
validation on the EmoSpeech corpus (H-H data). On
the other hand, we report accuracy on a corpus of
550 Human-Computer (H-C) dialogues obtained by
having 22 subjects play the game against the QA
character trained on the H-H corpus. As we shall see
below, performance decreases in this second evalua-
tion suggesting that subjects produce different turns
when playing with a computer than with a human
thereby inducing a weak out-of-domain effect and
negatively impacting classification. Evaluation on
the H-H corpus therefore gives a measure of how
well the techniques explored help improving the di-
alog engine when used in a real life setting.
Correspondingly, we use two different tests for
measuring statistical significance. In the H-H eval-
uation, significance is computed using the Wilcoxon
signed rank test because data are dependent and are
not assumed to be normally distributed. When build-
ing the testset we took care of not including para-
phrases of utterances in the training partition (for
each paraphrase generated automatically we keep
track of the original utterance), however utterances
in both datasets might be generated by the same sub-
ject, since a subject completed 12 distinct dialogues
during the game. Conversely, in the H-C evaluation,
training (H-H data) and test (H-C data) sets were
collected under different conditions with different
subjects therefore significance was computed using
the McNemar sign-test (Dietterich, 1998).
</bodyText>
<sectionHeader confidence="0.9043505" genericHeader="method">
4 Paraphrases, Synonyms and
Lemmatisation
</sectionHeader>
<bodyText confidence="0.999905888888889">
We explore three main ways of modifying the
content features used for classification: lemmatising
the training and the test data; augmenting the train-
ing data with automatically acquired paraphrases;
and substituting unknown words with synonyms at
run time.
Lemmatisation We use the French version of
Treetagger 3 to lemmatise both the training and the
test data. Lemmas without any filtering were used
</bodyText>
<page confidence="0.680750666666667">
3. http://www.ims.uni-stuttgart.de/projekte/
corplex/TreeTagger/
809
</page>
<bodyText confidence="0.9951461">
to train classifiers. We then compare performance
with and without lemmatisation. As we shall see,
the lemma and the POS tag provided by TreeTag-
ger are also used to lookup synonym dictionaries and
EuroWordNet when using synonym handling at run
time.
Paraphrases : (DeVault et al., 2011) showed that
enriching the training corpus with manually added
paraphrases increases accuracy. Here we exploit au-
tomatically acquired paraphrases and use these not
only to increase the size of the training corpus but
also to better balance it 4. We proceed as follows.
First, we generated paraphrases using a pivot ma-
chine translation approach where each user utter-
ance in the training corpus (around 3610 utterances)
was translated into some target language and back
into French. Using six different languages (English,
Spanish, Italian, German, Chinese and Arabian),
we generated around 38000 paraphrases. We used
Google Translate API for translating.
</bodyText>
<table confidence="0.9997426">
Category Train Instances Balanced Instances
greet 24 86
help 20 82
yes 92 123
no 55 117
ack 73 135
other 27 89
quit 38 100
find_plans 115 146
job 26 88
staff 15 77
studies 20 82
security_policies 24 86
µ 44.08 100.92
or ±32.68 ±23.32
</table>
<tableCaption confidence="0.5633194">
TABLE 1: Skewed and Balanced Data on a sample sub-
dialog. The category with lowest number of paraphrases
is greet, with 62 paraphrases, hence lp = 62. All cat-
egories were increased by 62 except find_plans and
yes that were increased by half: 31.
</tableCaption>
<bodyText confidence="0.986426666666667">
Second, we eliminate from these paraphrases,
words that are likely to be incorrect lexical transla-
tions by removing words with low normalized term
</bodyText>
<footnote confidence="0.8162385">
4. The Emospeech data is highly skewed with some classes
being populated with many utterances and others with few.
</footnote>
<figureCaption confidence="0.9836035">
FIGURE 1: Algorithm for augmenting the training data
with paraphrases.
</figureCaption>
<bodyText confidence="0.999962517241379">
frequency (&lt; 0.001) across translations i.e., lexical
translations given by few translations and/or transla-
tion systems. We then preprocessed the paraphrases
in the same way the utterances of the initial train-
ing corpus were preprocessed i.e., utterances were
unaccented, converted to lower-case and stop words
were removed, the remaining words were filtered
with TF*IDF. After preprocessing, duplicates were
removed.
Third, we added the paraphrases to the training
data seeking to improve the balance between dialog
moves per dialog, as shown in Figure 1. To this end,
we look for the category c with the lowest number
of paraphrases lp (line 5). We then compute the de-
viation di for each dialog move ci from the mean
p in the original training set (line 9). If the devia-
tion di is lower than the standard deviation then we
add lp number of paraphrases instances (line 11).
Conversely, if di is higher than the standard devia-
tion, we reduce the number of instances to be added
by half 2 (line 13). Table 1 shows the original and
the extended training data for the third sub-dialog
in the Emospeech game. In this dialogue the player
is supposed to ask information about the joystick
plans (find_plans, which is the mandatory goal).
The categories cover mandatory and optional goals
and general dialogue acts, such as greetings, asking
for help, confirm and disconfirm, acknowledgment
and out of topic questions (i.e. other).
</bodyText>
<footnote confidence="0.727013">
Substituting Synonyms for Unknown Words A
word is unknown, if it is a well-formed French
</footnote>
<figure confidence="0.8537667">
Algorithm extendingDataWithParaphrases(trainingset ts)
Let c be the set of categories in ts.
µ be the mean of train instances per category
σ be the standard deviation of train instances per category
Let Npc be the number of paraphrases per category
Let lp min Npcj
Repeat
set i - 0
Ninstci be the number of instances per category ci
di Ninstci - µ
if di &lt; σ then
Ninstci - lp
else
lp
Ninstci � 2
end if
set i_i+1
if i&gt;11c11 then
terminate
end
</figure>
<page confidence="0.990364">
810
</page>
<bodyText confidence="0.999700741935484">
word 5 and if it does not appear in the training cor-
pus. Conversely, a word is known if it is not un-
known.
When an unknown word w is detected in a player
utterance at runtime, we search for a word w&apos; which
occurs in the training data and is either a synonym of
w or a distributional neighbour. After disambigua-
tion, we substitute the unknown word for the syn-
onym.
To identify synonyms, we make use of two lexical
resources namely, the French version of EuroWord-
Net (EWN) (Vossen, 1998), which includes 92833
synonyms, hyperonyms and hyponyms pairs, and a
synonym lexicon for French (DIC) 6 which contains
38505 lemmas and 254149 synonym pairs. While
words are categorised into Noun, Verbs and Adjec-
tives in EWN, DIC contains no POS tag information.
To identify distributional neighbours, we con-
structed semantic word spaces for each subdialog
in the EmoSpeech corpus 7 using random indexing
(RI) 8 on the training corpus expanded with para-
phrases. Using the cosine measure as similarity met-
rics, we then retrieve for any unknown word w, the
word w&apos; which is most similar to w and which ap-
pear in the training corpus.
For lexical disambiguation, two methods are com-
pared. We use the POS tag provided by TreeTagger.
In this case, disambiguation is syntactic only. Or we
pick the synonym with highest probability based on
a trigram language model trained on the H-H cor-
pus 9.
</bodyText>
<sectionHeader confidence="0.999011" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.93875075">
Table 2 summarises the results obtained in four
main configurations : (i) with and without para-
phrases; (ii) with and without synonym handling;
(iii) with and without lemmatisation; and (iv) when
</bodyText>
<listItem confidence="0.869429923076923">
5. A word is determined to be a well-formed French word if
it occurs in the LEFFF dictionary, a large-scale morphological
and syntactic lexicon for French (Sagot, 2010)
6. DICOSYN (http ://elsap1.unicaen.fr/dicosyn.html).
7. We also used distributional semantics from the Gigaword
corpus but the results were poor probably because of the very
different text genre and domains between the the Gigaword and
the MP game.
8. Topics are Dialog acts while documents are utterances;
we used the S-Space Package http://code.google.com/p/
airhead-research/wiki/RandomIndexing
9. We used SRILM (http://www.speech.sri.com/
projects/srilm)
</listItem>
<bodyText confidence="0.991306574468085">
combining lemmatisation with synonym handling.
We also compare the results obtained when evalu-
ating using 10-fold cross validation on the training
data (H-H dialogs) vs. evaluating the performance
of the system on H-C interactions.
Overall Impact The largest performance gain is
obtained by a combination of the three techniques
explored in this paper namely, data expansion, syn-
onym handling and lemmatisation (+8.9 points for
the cross-validation experiment and +2.3 for the H-
C evaluation).
Impact of Lexical Substitution at Run Time Be-
cause of space restrictions, we do not report here
the results obtained using lexical resources without
lemmatisation. However, we found that lexical re-
sources are only useful when combined with lemma-
tisation. This is unsurprising since synonym dictio-
naries and EuroWordNet only contain lemmas. In-
deed when distributional neighbours are used, lem-
matisation has little impact (e.g., 65.11% using dis-
tributional neighbours without lemmatisation on the
H-H corpus without paraphrases vs. 66.41% when
using lemmatisation).
Another important issue when searching for a
word synonym concerns lexical disambiguation: the
synonym used to replace an unknown word should
capture the meaning of that word in its given con-
text. We tried using a language model trained on the
training corpus to choose between synonym candi-
dates (i.e., selecting the synonym yielding the high-
est sentence probability when substituting that syn-
onym for the unknown word) but did not obtain a
significant improvement. In contrast, it is noticeable
that synonym handling has a higher impact when us-
ing EuroWordNet as a lexical resource. Since Eu-
roWordNet contain categorial information while the
synonym dictionaries we used do not, this suggests
that the categorial disambiguation provided by Tree-
Tagger helps identifying an appropriate synonym in
EuroWordNet.
Finally, it is clear that the lexical resources used
for this experiment are limited in coverage and qual-
ity. We observed in particular that some words which
are very frequent in the training data (and thus which
could be used to replace unknown words) do not oc-
cur in the synonym dictionaries. For instance when
using paraphrases and dictionaries (fourth row and
</bodyText>
<page confidence="0.994247">
811
</page>
<table confidence="0.998825">
H Lemmatisation
H-H Orig. Lemmas +EWN +DIC +RI
Orig. 65.70% f 5.62 66.04% f 6.49 68.17% f 6.98 67.92% f 4.51 66.83% f 5.92
Parap. 70.89% f 6.45 74.31% f 4.78* 74.60% f 5.99* 73.07% f 7.71* 72.63% f 5.82*
H-C Orig. Lemmas +EWN +DIC +RI
Orig. 59.71% f 16.42 59.88% f 7.19 61.14% f 16.65 61.41% f 16.59 60.75% f 17.39
Parap. 59.82% f 15.53 59.48% f 14.02 61.70% f 14.09* 62.01% f 14.37* 61.16% f 14.41*
</table>
<tableCaption confidence="0.971167">
TABLE 2: Accuracy on the H-H and on the H-C corpus. The star denotes statistical significance with the Wilcoxon test
(p &lt; 0.005) used for the HH corpus and the McNemar test (p &lt; 0.005) for the HC corpus.
</tableCaption>
<bodyText confidence="0.999896933333334">
fourth column in Table 2) 50% of the unknown
words were solved, 17% were illformed and 33% re-
mained unsolved. To compensate this deficiency, we
tried combining the three lexical resources in vari-
ous ways (taking the union or combining them in a
pipeline using the first resource that would yield a
synonym). However the results did not improve and
even in some cases worsened due probably to the in-
sufficient lexical disambiguation. Interestingly, the
results show that paraphrases always improves syn-
onym handling presumably because it increases the
size of the known vocabulary thereby increasing the
possibility of finding a known synonym.
In sum, synonym handling helps most when (i)
words are lemmatised and (ii) unknown words can
be at least partially (i.e., using POS tag information)
disambiguated. Moreover since data expansion in-
creases the set of known words available as potential
synonyms for unknown words, combining synonym
handling with data expansion further improves ac-
curacy.
Impact of Lemmatisation When evaluating using
cross validation on the training corpus, lemmatisa-
tion increases accuracy by up to 3.42 points indi-
cating that unseen word forms negatively impact ac-
curacy. Noticeably however, lemmatisation has no
significant impact when evaluating on the H-C cor-
pus. This in turn suggests that the lower accuracy
obtained on the H-C corpus results not from unseen
word forms but from unseen lemmas.
Impact of Paraphrases On the H-H corpus, data
expansion has no significant impact when used
alone. However it yields an increase of up to 8.27
points and in fact, has a statistically significant im-
pact, for all configurations involving lemmatisation.
Thus, data expansion is best used in combination
with lemmatisation and their combination permits
creating better, more balanced and more general
training data. On the H-C corpus however, the im-
pact is negative or insignificant suggesting that the
decrease in performance on the H-C corpus is due to
content words that are new with respect to the train-
ing data i.e., content words for which neither a syn-
onym nor a lemma can be found in the expanded
training data.
</bodyText>
<sectionHeader confidence="0.908525" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999952722222222">
While classifiers are routinely trained on dialog
data to model the dialog management process, the
impact of such basic factors as lemmatisation, au-
tomatic data expansion and synonym handling has
remained largely unexplored. The empirical eval-
uation described here suggests that each of these
factors can help improve performance but that the
impact will vary depending on their combination
and on the evaluation mode. Combining all three
techniques yields the best results. We conjecture
that there are two main reasons for this. First, syn-
onym handling is best used in combination with
POS tagging and lemmatisation because these sup-
ports partial lexical semantic disambiguation. Sec-
ond, data expansion permits expanding the set of
known words thereby increasing the possibility of
finding a known synonym to replace an unknown
word with.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9973825">
This work was partially supported by the EU
funded Eurostar EmoSpeech project. We thank
Google for giving us access to the University Re-
search Program of Google Translate.
</bodyText>
<page confidence="0.996197">
812
</page>
<sectionHeader confidence="0.995861" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999847142857143">
David DeVault, Anton Leuski, and Kenji Sagae. 2011.
Toward learning and evaluation of dialogue policies
with text examples. In 12th SIGdial Workshop on Dis-
course and Dialogue, Portland, OR, June.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learning
algorithms. Neural Computation, 10 :1895–1923.
Hilda Hardy, Tomek Strzalkowski, Min Wu, Cristian
Ursu, Nick Webb, Alan W. Biermann, R. Bryce In-
ouye, and Ashley McKenzie. 2004. Data-driven
strategies for an automated dialogue system. In ACL,
pages 71–78.
Ian Richard Lane, Tatsuya Kawahara, and Shinichi Ueno.
2004. Example-based training of dialogue planning
incorporating user and situation models. In INTER-
SPEECH.
Anton Leuski and David Traum. 2008. A statistical ap-
proach for text processing in virtual humans. In Pro-
ceedings of the 26th Army Science Conference.
Andrew Kachites McCallum. 2002. Mallet : A ma-
chine learning for language toolkit. http ://mal-
let.cs.umass.edu.
Lina Maria Rojas Barahona and Claire Gardent. 2012.
What should I do now? Supporting conversations in
a serious game. In SeineDial 2012 - 16th Workshop
on the Semantics and Pragmatics of Dialogue, Paris,
France. Jonathan Ginzburg (chair), Anne Abeillé, Mar-
got Colinet, Gregoire Winterstein.
Lina M. Rojas-Barahona, Alejandra Lorenzo, and Claire
Gardent. 2012a. Building and exploiting a corpus
of dialog interactions between french speaking virtual
and human agents. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Evalu-
ation.
Lina M. Rojas Barahona, Alejandra Lorenzo, and Claire
Gardent. 2012b. An end-to-end evaluation of two
situated dialog systems. In Proceedings of the 13th
Annual Meeting of the Special Interest Group on Dis-
course and Dialogue, pages 10–19, Seoul, South Ko-
rea, July. Association for Computational Linguistics.
K. Sagae, G. Christian, D. DeVault, , and D.R. Traum.
2009. Towards natural language understanding of par-
tial speech recognition results in dialogue systems. In
Proceedings of Human Language Technologies : The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL), Companion Volume : Short Papers, pages
53–56.
Benoît Sagot. 2010. The Lefff, a freely available and
large-coverage morphological and syntactic lexicon
for French. In 7th international conference on Lan-
guage Resources and Evaluation (LREC 2010), Val-
letta, Malta.
Piek Vossen, editor. 1998. EuroWordNet : a multilin-
gual database with lexical semantic networks. Kluwer
Academic Publishers, Norwell, MA, USA.
</reference>
<page confidence="0.999185">
813
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.683291">
<title confidence="0.999212">Using Paraphrases and Lexical Semantics to Improve the Accuracy and the Robustness of Supervised Models in Situated Dialogue Systems</title>
<author confidence="0.987774">Claire Gardent Lina M Rojas Barahona</author>
<address confidence="0.737527">CNRS/LORIA, Nancy Université de Lorraine/LORIA, Nancy</address>
<email confidence="0.951142">claire.gardent@loria.frlina.rojas@loria.fr</email>
<abstract confidence="0.997655444444444">This paper explores to what extent lemmatisation, lexical resources, distributional semantics and paraphrases can increase the accuracy of supervised models for dialogue management. The results suggest that each of these factors can help improve performance but that the impact will vary depending on their combination and on the evaluation mode.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>Anton Leuski</author>
<author>Kenji Sagae</author>
</authors>
<title>Toward learning and evaluation of dialogue policies with text examples.</title>
<date>2011</date>
<booktitle>In 12th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<location>Portland, OR,</location>
<contexts>
<context position="2401" citStr="DeVault et al., 2011" startWordPosition="379" endWordPosition="382">e quality (less skewed data) of the training corpus using paraphrase generation techniques. We compare the performance obtained on lemmatised vs. non lemmatised data. And we investigate how various resources (synonym dictionaries, WordNet, distributional neighbours) can be used to handle unseen words at run time. 2 Related work Previous work on improving robustness of supervised dialog systems includes detecting and handling out of domain utterances for generating feedback (Lane et al., 2004) ; using domain-restricted lexical semantics (Hardy et al., 2004) ; and work on manual data expansion (DeVault et al., 2011). Our work follows up on this research but provides a systematic investigation of how data expansion, lemmatisation and synonym handling impacts the performance of a supervised QA engine. 3 Experimental Setup We run our experiments on a dialog engine developed for a serious game called Mission Plastechnologie. In this game, the player must interact with different virtual humans through a sequence of 12 subdialogs, each of them occurring in a different part of the virtual world. Training Data. The training corpus consists of around 1250 Human-Human dialogues which were manually annotated with d</context>
<context position="8059" citStr="DeVault et al., 2011" startWordPosition="1285" endWordPosition="1288">ing the training data with automatically acquired paraphrases; and substituting unknown words with synonyms at run time. Lemmatisation We use the French version of Treetagger 3 to lemmatise both the training and the test data. Lemmas without any filtering were used 3. http://www.ims.uni-stuttgart.de/projekte/ corplex/TreeTagger/ 809 to train classifiers. We then compare performance with and without lemmatisation. As we shall see, the lemma and the POS tag provided by TreeTagger are also used to lookup synonym dictionaries and EuroWordNet when using synonym handling at run time. Paraphrases : (DeVault et al., 2011) showed that enriching the training corpus with manually added paraphrases increases accuracy. Here we exploit automatically acquired paraphrases and use these not only to increase the size of the training corpus but also to better balance it 4. We proceed as follows. First, we generated paraphrases using a pivot machine translation approach where each user utterance in the training corpus (around 3610 utterances) was translated into some target language and back into French. Using six different languages (English, Spanish, Italian, German, Chinese and Arabian), we generated around 38000 parap</context>
</contexts>
<marker>DeVault, Leuski, Sagae, 2011</marker>
<rawString>David DeVault, Anton Leuski, and Kenji Sagae. 2011. Toward learning and evaluation of dialogue policies with text examples. In 12th SIGdial Workshop on Discourse and Dialogue, Portland, OR, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>Approximate statistical tests for comparing supervised classification learning algorithms.</title>
<date>1998</date>
<journal>Neural Computation,</journal>
<volume>10</volume>
<pages>1895--1923</pages>
<contexts>
<context position="7257" citStr="Dietterich, 1998" startWordPosition="1169" endWordPosition="1170">dent and are not assumed to be normally distributed. When building the testset we took care of not including paraphrases of utterances in the training partition (for each paraphrase generated automatically we keep track of the original utterance), however utterances in both datasets might be generated by the same subject, since a subject completed 12 distinct dialogues during the game. Conversely, in the H-C evaluation, training (H-H data) and test (H-C data) sets were collected under different conditions with different subjects therefore significance was computed using the McNemar sign-test (Dietterich, 1998). 4 Paraphrases, Synonyms and Lemmatisation We explore three main ways of modifying the content features used for classification: lemmatising the training and the test data; augmenting the training data with automatically acquired paraphrases; and substituting unknown words with synonyms at run time. Lemmatisation We use the French version of Treetagger 3 to lemmatise both the training and the test data. Lemmas without any filtering were used 3. http://www.ims.uni-stuttgart.de/projekte/ corplex/TreeTagger/ 809 to train classifiers. We then compare performance with and without lemmatisation. As</context>
</contexts>
<marker>Dietterich, 1998</marker>
<rawString>Thomas G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10 :1895–1923.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hilda Hardy</author>
<author>Tomek Strzalkowski</author>
<author>Min Wu</author>
<author>Cristian Ursu</author>
<author>Nick Webb</author>
<author>Alan W Biermann</author>
<author>R Bryce Inouye</author>
<author>Ashley McKenzie</author>
</authors>
<title>Data-driven strategies for an automated dialogue system.</title>
<date>2004</date>
<booktitle>In ACL,</booktitle>
<pages>71--78</pages>
<contexts>
<context position="2342" citStr="Hardy et al., 2004" startWordPosition="368" endWordPosition="371">a of a supervised QA character. We expand the size and the quality (less skewed data) of the training corpus using paraphrase generation techniques. We compare the performance obtained on lemmatised vs. non lemmatised data. And we investigate how various resources (synonym dictionaries, WordNet, distributional neighbours) can be used to handle unseen words at run time. 2 Related work Previous work on improving robustness of supervised dialog systems includes detecting and handling out of domain utterances for generating feedback (Lane et al., 2004) ; using domain-restricted lexical semantics (Hardy et al., 2004) ; and work on manual data expansion (DeVault et al., 2011). Our work follows up on this research but provides a systematic investigation of how data expansion, lemmatisation and synonym handling impacts the performance of a supervised QA engine. 3 Experimental Setup We run our experiments on a dialog engine developed for a serious game called Mission Plastechnologie. In this game, the player must interact with different virtual humans through a sequence of 12 subdialogs, each of them occurring in a different part of the virtual world. Training Data. The training corpus consists of around 1250</context>
</contexts>
<marker>Hardy, Strzalkowski, Wu, Ursu, Webb, Biermann, Inouye, McKenzie, 2004</marker>
<rawString>Hilda Hardy, Tomek Strzalkowski, Min Wu, Cristian Ursu, Nick Webb, Alan W. Biermann, R. Bryce Inouye, and Ashley McKenzie. 2004. Data-driven strategies for an automated dialogue system. In ACL, pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Richard Lane</author>
<author>Tatsuya Kawahara</author>
<author>Shinichi Ueno</author>
</authors>
<title>Example-based training of dialogue planning incorporating user and situation models.</title>
<date>2004</date>
<booktitle>In INTERSPEECH.</booktitle>
<contexts>
<context position="2277" citStr="Lane et al., 2004" startWordPosition="359" endWordPosition="362">e different ways of improving and complementing the training data of a supervised QA character. We expand the size and the quality (less skewed data) of the training corpus using paraphrase generation techniques. We compare the performance obtained on lemmatised vs. non lemmatised data. And we investigate how various resources (synonym dictionaries, WordNet, distributional neighbours) can be used to handle unseen words at run time. 2 Related work Previous work on improving robustness of supervised dialog systems includes detecting and handling out of domain utterances for generating feedback (Lane et al., 2004) ; using domain-restricted lexical semantics (Hardy et al., 2004) ; and work on manual data expansion (DeVault et al., 2011). Our work follows up on this research but provides a systematic investigation of how data expansion, lemmatisation and synonym handling impacts the performance of a supervised QA engine. 3 Experimental Setup We run our experiments on a dialog engine developed for a serious game called Mission Plastechnologie. In this game, the player must interact with different virtual humans through a sequence of 12 subdialogs, each of them occurring in a different part of the virtual </context>
</contexts>
<marker>Lane, Kawahara, Ueno, 2004</marker>
<rawString>Ian Richard Lane, Tatsuya Kawahara, and Shinichi Ueno. 2004. Example-based training of dialogue planning incorporating user and situation models. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anton Leuski</author>
<author>David Traum</author>
</authors>
<title>A statistical approach for text processing in virtual humans.</title>
<date>2008</date>
<booktitle>In Proceedings of the 26th Army Science Conference.</booktitle>
<contexts>
<context position="1011" citStr="Leuski and Traum, 2008" startWordPosition="148" endWordPosition="151">can increase the accuracy of supervised models for dialogue management. The results suggest that each of these factors can help improve performance but that the impact will vary depending on their combination and on the evaluation mode. 1 Introduction One strand of work in dialog research targets the rapid prototyping of virtual humans capable of conducting a conversation with humans in the context of a virtual world. In particular, question answering (QA) characters can respond to a restricted set of topics after training on a set of dialogs whose utterances are annotated with dialogue acts (Leuski and Traum, 2008). As argued in (Sagae et al., 2009), the size of the training corpus is a major factor in allowing QA characters that are both robust and accurate. In addition, the training corpus should arguably be of good quality in that (i) it should contain the various ways of expressing the same content (paraphrases) and (ii) the data should not be skewed. In sum, the ideal training data should be large (more data is better data) ; balanced (similar amount of data for each class targeted by the classifier) and varied (it should encompass the largest possible number of paraphrases and synonyms for the utt</context>
</contexts>
<marker>Leuski, Traum, 2008</marker>
<rawString>Anton Leuski and David Traum. 2008. A statistical approach for text processing in virtual humans. In Proceedings of the 26th Army Science Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet : A machine learning for language toolkit. http ://mallet.cs.umass.edu.</title>
<date>2002</date>
<contexts>
<context position="5509" citStr="McCallum, 2002" startWordPosition="893" endWordPosition="894">sons for this. First, we want to focus on the impact of synonym handling, paraphrasing and lemmatisation on dialog management. Removing contextual features allows us to focus on how content features (content words) can be improved by these mechanisms. Second, when evaluating on the H-C corpus (see below), contextual features are often incorrect (because the system might incorrectly interpret and thus label a user turn). Excluding contextual features from training allows for a fair comparison between the H-H and the H-C evaluation. Test Data and Evaluation Metrics We use accu1. We used MALLET (McCallum, 2002) for the LR classifier with L1 Regularisation. 2. TF*IDF = Term Frequency*Inverse Document Frequency racy (the number of correct classifications divided by the number of instances in the testset) to measure performance and we carry out two types of evaluation. On the one hand, we use 10-fold crossvalidation on the EmoSpeech corpus (H-H data). On the other hand, we report accuracy on a corpus of 550 Human-Computer (H-C) dialogues obtained by having 22 subjects play the game against the QA character trained on the H-H corpus. As we shall see below, performance decreases in this second evaluation</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet : A machine learning for language toolkit. http ://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lina Maria Rojas Barahona</author>
<author>Claire Gardent</author>
</authors>
<title>What should I do now? Supporting conversations in a serious game.</title>
<date>2012</date>
<journal>Jonathan Ginzburg (chair), Anne Abeillé, Margot Colinet, Gregoire Winterstein.</journal>
<booktitle>In SeineDial 2012 - 16th Workshop on the Semantics and Pragmatics of Dialogue,</booktitle>
<location>Paris, France.</location>
<contexts>
<context position="4276" citStr="Barahona and Gardent, 2012" startWordPosition="685" endWordPosition="688">ctor, Mr. Jasper.) &gt;M.Jasper : Qu’est-ce que je peux faire pour vous ? ||ask(task(X)) (What can I do for you ?) &gt;Lucas : je dois sauver mon oncle ||first_step (I must rescue my uncle) &gt;M.Jasper : Pour faire votre manette, il vous faut des plans. Allez voir dans le bureau d’études, ils devraient y être. ||inform(do(first_step)) (To build the joystick you will need the plans. You will find them in the Designing Office.) &gt;M.Jasper : Bonne Chance! ||quit (Good Luck!) Dialog Systems For our experiments, we use a hybrid dialog system similar to that described in (Rojas Barahona et al., 2012b; Rojas Barahona and Gardent, 2012). This system combines a classifier for interpreting the players utterances with an information state dialog manager which selects an appropriate system response based on the dialog move assigned by the classifier to the user turn. The classifier is a logistic regression classifier 1 which was trained for each subdialog in the game. The features used for training are the set of content words which are associated with a given dialog move and which remain after TF*IDF 2 filtering. Note that in this experiment, we do not use contextual features such as the dialog acts labeling the previous turns.</context>
</contexts>
<marker>Barahona, Gardent, 2012</marker>
<rawString>Lina Maria Rojas Barahona and Claire Gardent. 2012. What should I do now? Supporting conversations in a serious game. In SeineDial 2012 - 16th Workshop on the Semantics and Pragmatics of Dialogue, Paris, France. Jonathan Ginzburg (chair), Anne Abeillé, Margot Colinet, Gregoire Winterstein.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lina M Rojas-Barahona</author>
<author>Alejandra Lorenzo</author>
<author>Claire Gardent</author>
</authors>
<title>Building and exploiting a corpus of dialog interactions between french speaking virtual and human agents.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="3511" citStr="Rojas-Barahona et al., 2012" startWordPosition="556" endWordPosition="559">Training Data. The training corpus consists of around 1250 Human-Human dialogues which were manually annotated with dialog moves. As the following dialog excerpt illustrates, the dialogs are conducted in French and each dialog turn is manually annotated using a set of 28 dialog acts. For 808 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 808–813, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics a more detailed presentation of the training corpus and of the annotation scheme, the reader is referred to (Rojas-Barahona et al., 2012a) dialog: 01_dialogDirecteur-Tue Jun 14 11 :04 :23 2011 &gt;M.Jasper : Bonjour, je suis M.Jasper le directeur. ||greet (Hello, I am the director, Mr. Jasper.) &gt;M.Jasper : Qu’est-ce que je peux faire pour vous ? ||ask(task(X)) (What can I do for you ?) &gt;Lucas : je dois sauver mon oncle ||first_step (I must rescue my uncle) &gt;M.Jasper : Pour faire votre manette, il vous faut des plans. Allez voir dans le bureau d’études, ils devraient y être. ||inform(do(first_step)) (To build the joystick you will need the plans. You will find them in the Designing Office.) &gt;M.Jasper : Bonne Chance! ||quit (Good L</context>
</contexts>
<marker>Rojas-Barahona, Lorenzo, Gardent, 2012</marker>
<rawString>Lina M. Rojas-Barahona, Alejandra Lorenzo, and Claire Gardent. 2012a. Building and exploiting a corpus of dialog interactions between french speaking virtual and human agents. In Proceedings of the 8th International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lina M Rojas Barahona</author>
<author>Alejandra Lorenzo</author>
<author>Claire Gardent</author>
</authors>
<title>An end-to-end evaluation of two situated dialog systems.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>10--19</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seoul, South</location>
<contexts>
<context position="3511" citStr="Barahona et al., 2012" startWordPosition="556" endWordPosition="559">ng Data. The training corpus consists of around 1250 Human-Human dialogues which were manually annotated with dialog moves. As the following dialog excerpt illustrates, the dialogs are conducted in French and each dialog turn is manually annotated using a set of 28 dialog acts. For 808 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 808–813, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics a more detailed presentation of the training corpus and of the annotation scheme, the reader is referred to (Rojas-Barahona et al., 2012a) dialog: 01_dialogDirecteur-Tue Jun 14 11 :04 :23 2011 &gt;M.Jasper : Bonjour, je suis M.Jasper le directeur. ||greet (Hello, I am the director, Mr. Jasper.) &gt;M.Jasper : Qu’est-ce que je peux faire pour vous ? ||ask(task(X)) (What can I do for you ?) &gt;Lucas : je dois sauver mon oncle ||first_step (I must rescue my uncle) &gt;M.Jasper : Pour faire votre manette, il vous faut des plans. Allez voir dans le bureau d’études, ils devraient y être. ||inform(do(first_step)) (To build the joystick you will need the plans. You will find them in the Designing Office.) &gt;M.Jasper : Bonne Chance! ||quit (Good L</context>
</contexts>
<marker>Barahona, Lorenzo, Gardent, 2012</marker>
<rawString>Lina M. Rojas Barahona, Alejandra Lorenzo, and Claire Gardent. 2012b. An end-to-end evaluation of two situated dialog systems. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 10–19, Seoul, South Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>G Christian</author>
<author>D DeVault</author>
</authors>
<title>Towards natural language understanding of partial speech recognition results in dialogue systems.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies : The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), Companion Volume : Short Papers,</booktitle>
<pages>53--56</pages>
<contexts>
<context position="1046" citStr="Sagae et al., 2009" startWordPosition="155" endWordPosition="158"> models for dialogue management. The results suggest that each of these factors can help improve performance but that the impact will vary depending on their combination and on the evaluation mode. 1 Introduction One strand of work in dialog research targets the rapid prototyping of virtual humans capable of conducting a conversation with humans in the context of a virtual world. In particular, question answering (QA) characters can respond to a restricted set of topics after training on a set of dialogs whose utterances are annotated with dialogue acts (Leuski and Traum, 2008). As argued in (Sagae et al., 2009), the size of the training corpus is a major factor in allowing QA characters that are both robust and accurate. In addition, the training corpus should arguably be of good quality in that (i) it should contain the various ways of expressing the same content (paraphrases) and (ii) the data should not be skewed. In sum, the ideal training data should be large (more data is better data) ; balanced (similar amount of data for each class targeted by the classifier) and varied (it should encompass the largest possible number of paraphrases and synonyms for the utterances of each class). In this pap</context>
</contexts>
<marker>Sagae, Christian, DeVault, 2009</marker>
<rawString>K. Sagae, G. Christian, D. DeVault, , and D.R. Traum. 2009. Towards natural language understanding of partial speech recognition results in dialogue systems. In Proceedings of Human Language Technologies : The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), Companion Volume : Short Papers, pages 53–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoît Sagot</author>
</authors>
<title>The Lefff, a freely available and large-coverage morphological and syntactic lexicon for French.</title>
<date>2010</date>
<booktitle>In 7th international conference on Language Resources and Evaluation (LREC 2010),</booktitle>
<location>Valletta,</location>
<contexts>
<context position="13248" citStr="Sagot, 2010" startWordPosition="2169" endWordPosition="2170">ethods are compared. We use the POS tag provided by TreeTagger. In this case, disambiguation is syntactic only. Or we pick the synonym with highest probability based on a trigram language model trained on the H-H corpus 9. 5 Results and Discussion Table 2 summarises the results obtained in four main configurations : (i) with and without paraphrases; (ii) with and without synonym handling; (iii) with and without lemmatisation; and (iv) when 5. A word is determined to be a well-formed French word if it occurs in the LEFFF dictionary, a large-scale morphological and syntactic lexicon for French (Sagot, 2010) 6. DICOSYN (http ://elsap1.unicaen.fr/dicosyn.html). 7. We also used distributional semantics from the Gigaword corpus but the results were poor probably because of the very different text genre and domains between the the Gigaword and the MP game. 8. Topics are Dialog acts while documents are utterances; we used the S-Space Package http://code.google.com/p/ airhead-research/wiki/RandomIndexing 9. We used SRILM (http://www.speech.sri.com/ projects/srilm) combining lemmatisation with synonym handling. We also compare the results obtained when evaluating using 10-fold cross validation on the tr</context>
</contexts>
<marker>Sagot, 2010</marker>
<rawString>Benoît Sagot. 2010. The Lefff, a freely available and large-coverage morphological and syntactic lexicon for French. In 7th international conference on Language Resources and Evaluation (LREC 2010), Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<title>EuroWordNet : a multilingual database with lexical semantic networks.</title>
<date>1998</date>
<editor>Piek Vossen, editor.</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Norwell, MA, USA.</location>
<marker>1998</marker>
<rawString>Piek Vossen, editor. 1998. EuroWordNet : a multilingual database with lexical semantic networks. Kluwer Academic Publishers, Norwell, MA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
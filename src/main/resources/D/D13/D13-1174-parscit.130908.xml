<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.832814">
Translating into Morphologically Rich Languages with Synthetic Phrases
</title>
<author confidence="0.993989">
Victor Chahuneau Eva Schlinger Noah A. Smith Chris Dyer
</author>
<affiliation confidence="0.877537666666667">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.999153">
{vchahune,eschling,nasmith,cdyer}@cs.cmu.edu
</email>
<sectionHeader confidence="0.996665" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996539888888889">
Translation into morphologically rich lan-
guages is an important but recalcitrant prob-
lem in MT. We present a simple and effec-
tive approach that deals with the problem in
two phases. First, a discriminative model is
learned to predict inflections of target words
from rich source-side annotations. Then, this
model is used to create additional sentence-
specific word- and phrase-level translations
that are added to a standard translation model
as “synthetic” phrases. Our approach re-
lies on morphological analysis of the target
language, but we show that an unsupervised
Bayesian model of morphology can success-
fully be used in place of a supervised analyzer.
We report significant improvements in transla-
tion quality when translating from English to
Russian, Hebrew and Swahili.
</bodyText>
<sectionHeader confidence="0.998906" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999788595744681">
Machine translation into morphologically rich lan-
guages is challenging, due to lexical sparsity and the
large variety of grammatical features expressed with
morphology. In this paper, we introduce a method
that uses target language morphological grammars
(either hand-crafted or learned unsupervisedly) to
address this challenge and demonstrate its effective-
ness at improving translation from English into sev-
eral morphologically rich target languages.
Our approach decomposes the process of produc-
ing a translation for a word (or phrase) into two
steps. First, a meaning-bearing stem is chosen and
then an appropriate inflection is selected using a
feature-rich discriminative model that conditions on
the source context of the word being translated.
Rather than attempting to directly produce full-
sentence translations using such an elementary pro-
cess, we use our model to generate translations of
individual words and short phrases that augment—
on a sentence-by-sentence basis—the inventory of
translation rules obtained using standard translation
rule extraction techniques (Chiang, 2007). We call
these synthetic phrases.
The major advantages of our approach are: (i)
synthesized forms are targeted to a specific transla-
tion context; (ii) multiple, alternative phrases may
be generated with the final choice among rules left
to the global translation model; (iii) virtually no
language-specific engineering is necessary; (iv) any
phrase- or syntax-based decoder can be used with-
out modification; and (v) we can generate forms that
were not attested in the bilingual training data.
The paper is structured as follows. We first
present our “translate-and-inflect” model for pre-
dicting lexical translations into morphologically rich
languages given a source word and its context (§2).
Our approach requires a morphological grammar to
relate surface forms to underlying (stem, inflection)
pairs; we discuss how either a standard morpholog-
ical analyzer or a simple Bayesian unsupervised an-
alyzer can be used (§3). After describing an ef-
ficient parameter estimation procedure for the in-
flection model (§4), we employ the translate-and-
inflect model in an MT system. We describe
how we use our model to synthesize translation
options (§5) and then evaluate translation quality
on English–Russian, English–Hebrew, and English–
</bodyText>
<page confidence="0.942508">
1677
</page>
<note confidence="0.732494">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1677–1687,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.950597">
Swahili translation tasks, finding significant im-
provements in all language pairs (§6). We finally
review related work (§7) and conclude (§8).
use a standard phrase-based model to generate se-
quences of stems and only the inflection model op-
erates word-by-word. We turn next to the inflection
model.
</bodyText>
<sectionHeader confidence="0.980296" genericHeader="introduction">
2 TYanslate-and-Inflect Model
</sectionHeader>
<bodyText confidence="0.99961178125">
The task of the translate-and-inflect model is illus-
trated in Fig. 1 for an English–Russian sentence pair.
The input will be a sentence e in the source language
(in this paper, always English) and any available lin-
guistic analysis of e. The output f will be composed
of (i) a sequence of stems, each denoted σ and (ii)
one morphological inflection pattern for each stem,
denoted µ. When the information is available, a
stem σ is composed of a lemma and an inflectional
class. Throughout, we use Ωσ to denote the set
of possible morphological inflection patterns for a
given stem σ. Ωσ might be defined by a grammar;
our models restrict Ωσ to be the set of inflections
observed anywhere in our monolingual or bilingual
training data as a realization of σ.1
We assume the availability of a deterministic
function that maps a stem σ and morphological in-
flection µ to a target language surface form f. In
some cases, such as our unsupervised approach in
§3.2, this will be a concatenation operation, though
finite-state transducers are traditionally used to de-
fine such relations (§3.1). We abstractly denote this
operation by ?: f = σ ? µ.
Our approach consists in defining a probabilistic
model over target words f. The model assumes in-
dependence between each target word f conditioned
on the source sentence e and its aligned position i in
this sentence.2 This assumption is further relaxed
in §5 when the model is integrated in the translation
system.
We decompose the probability of generating each
target word f in the following way:
</bodyText>
<equation confidence="0.969207">
1:
p(f  |e, i) =
σ?µ=f
</equation>
<bodyText confidence="0.802172">
Here, each stem is generated independently from a
single aligned source word ei, but in practice we
</bodyText>
<footnote confidence="0.987328">
1This prevents the model from generating words that would
be difficult for the language model to reliably score.
2This is the same assumption that Brown et al. (1993) make
in, for example, IBM Model 1.
</footnote>
<subsectionHeader confidence="0.996276">
2.1 Modeling Inflection
</subsectionHeader>
<bodyText confidence="0.99935825">
In morphologically rich languages, each stem may
be combined with one or more inflectional mor-
phemes to express many different grammatical fea-
tures (e.g., case, definiteness, mood, tense, etc.).
Since the inflectional morphology of a word gen-
erally expresses multiple grammatical features, we
would like a model that naturally incorporates rich,
possibly overlapping features in its representation of
both the input (i.e., conditioning context) and out-
put (i.e., the inflection pattern). We therefore use
the following parametric form to model inflectional
probabilities:
</bodyText>
<equation confidence="0.996377285714286">
[u(µ, e, i) = exp ϕ(e, i)TWψ(µ)+
�
ψ(µ)TVψ(µ) ,
u(µ, e, i)
p(µ  |σ, e, i) =
&apos;(1)
µ,EQ, u(µl, e, i).
</equation>
<bodyText confidence="0.996498857142857">
Here, ϕ is an m-dimensional source context fea-
ture vector function, ψ is an n-dimensional mor-
phology feature vector function, W E Rmxn and
V E Rnxn are parameter matrices. As with the
more familiar log-linear parametrization that is writ-
ten with a single feature vector, single weight vec-
tor and single bias vector, this model is linear in its
parameters (it can be understood as working with
a feature space that is the outer product of the two
feature spaces). However, using two feature vectors
allows to define overlapping features of both the in-
put and the output, which is important for modeling
morphology in which output variables are naturally
expressed as bundles of features. The second term
in the sum in u enables correlations among output
features to be modeled independently of input, and
as such can be understood as a generalization of the
bias terms in multi-class logistic regression (on the
diagonal Vii) and interaction terms between output
variables in a conditional random field (off the diag-
onal Vij).
</bodyText>
<listItem confidence="0.640807">
p(σ  |ei) X p(µ  |σ, e, i)
• ./ • • ./ •
gen. stem gen. inflection
</listItem>
<page confidence="0.920802">
1678
</page>
<bodyText confidence="0.9068158">
a:nwTaTbca_v + µ:mis-sfm-e
она n������ персчь пути на ее влосипед
она пыталась пересечь пути на ее велосипе
she had attempted to cross the road on her bike
she had attempted to cross the road on her bi
</bodyText>
<equation confidence="0.98565975">
C50 C473 C28 C8 C275 C37 C43 C82 C94 C331
C50 C4 C 8 C275 C37 C43 C82 C94 C
PR VBD VBN TO VB DT NN IN PR$ NN
PRP V V O T N IN PRP$
</equation>
<figureCaption confidence="0.89499075">
Figure 1: The inflection model predicts a form for the target verb lemma u =nbiTaTbca (pytat’sya) based on its
source attempted and the linear and syntactic source context. The correct inflection string for the observed Russian
form in this particular training instance is µ = mis-sfm-e (equivalent to the more traditional morphological string:
+MAIN+IND+PAST+SING+FEM+MEDIAL+PERF).
</figureCaption>
<bodyText confidence="0.714065166666667">
I source aligned word ei I I token }
parent word e,, with its dependency gyri → i part-of-speech tag
all children ej  |7rj = i with their dependency i → j word cluster
source words ei−1 and ei+1
– are ei, e,, at the root of the dependency tree?
– number of children, siblings of ei
</bodyText>
<figureCaption confidence="0.965553">
Figure 2: Source features W(a, i) extracted from a and its linguistic analysis. gyri denotes the parent of the token in
position i in the dependency tree and gyri → i the typed dependency link.
</figureCaption>
<figure confidence="0.925136666666667">
-1 +1
nsubj root xcompxc
aux
</figure>
<subsectionHeader confidence="0.830457">
2.2 Source Context Features: W(a, i)
</subsectionHeader>
<bodyText confidence="0.988867466666667">
In order to select the best inflection of a target-
language word, given the source word it translates
and the context of that source word, we seek to ex-
ploit as many features of the context as are avail-
able. Consider the example shown in Fig. 1, where
most of the inflection features of the Russian word
(past tense, singular number, and feminine gender)
can be inferred from the context of the English word
it is aligned to. Indeed, many grammatical functions
expressed morphologically in Russian are expressed
syntactically in English. Fortunately, high-quality
parsers and other linguistic analyzers are available
for English.
On the source side, we apply the following pro-
cessing steps:
</bodyText>
<listItem confidence="0.953042111111111">
• Part-of-speech tagging with a CRF tagger
trained on sections 02–21 of the Penn Tree-
bank.
• Dependency parsing with TurboParser (Mar-
tins et al., 2010), a non-projective dependency
parser trained on the Penn Treebank to produce
basic Stanford dependencies.
• Assignment of tokens to one of 600 Brown
clusters, trained on 8G words of English text.3
</listItem>
<bodyText confidence="0.998721">
We then extract binary features from a using this
information, by considering the aligned source word
ei, its preceding and following words, and its syn-
tactic neighbors. These are detailed in Figure 2.
</bodyText>
<sectionHeader confidence="0.680169" genericHeader="method">
3 Morphological Grammars and Features
</sectionHeader>
<bodyText confidence="0.99996075">
We now describe how to obtain morphological anal-
yses and convert them into feature vectors (ii) for
our target languages, Russian, Hebrew, and Swahili,
using supervised and unsupervised methods.
</bodyText>
<subsectionHeader confidence="0.998723">
3.1 Supervised Morphology
</subsectionHeader>
<bodyText confidence="0.8058218">
The state-of-the-art in morphological analysis uses
unweighted morphological transduction rules (usu-
3The entire monolingual data available for the translation
task of the 8th ACL Workshop on Statistical Machine Transla-
tion was used.
</bodyText>
<page confidence="0.981363">
1679
</page>
<bodyText confidence="0.999981392857143">
ally in the form of an FST) to produce candidate
analyses for each word in a sentence and then sta-
tistical models to disambiguate among the analy-
ses in context (Hakkani-T¨ur et al., 2000; Hajiˇc et
al., 2001; Smith et al., 2005; Habash and Rambow,
2005, inter alia). While this technique is capable
of producing high quality linguistic analyses, it is
expensive to develop, requiring hand-crafted rule-
based analyzers and annotated corpora to train the
disambiguation models. As a result, such analyzers
are only available for a small number of languages,
and, as a practical matter, each analyzer (which re-
sulted from different development efforts) operates
differently from the others.
We therefore focus on using supervised analysis
for a single target language, Russian. We use the
analysis tool of Sharoff et al. (2008) which produces
for each word in context a lemma and a fixed-length
morphological tag encoding the grammatical fea-
tures. We process the target side of the parallel data
with this tool to obtain the information necessary
to extract (lemma, inflection) pairs, from which we
compute σ and morphological feature vectors ii(µ).
Supervised morphology features: O(µ). Since
a positional tag set is used, it is straightforward to
convert each fixed-length tag µ into a feature vector
by defining a binary feature for each key-value pair
(e.g., Tense=past) composing the tag.
</bodyText>
<subsectionHeader confidence="0.999235">
3.2 Unsupervised Morphology
</subsectionHeader>
<bodyText confidence="0.997704681818182">
Since many languages into which we might want to
translate do not have supervised morphological an-
alyzers, we now turn to the question of how to gen-
erate morphological analyses and features using an
unsupervised analyzer. We hypothesize that perfect
decomposition into rich linguistic structures may not
be required for accurate generation of new inflected
forms. We will test this hypothesis by experimenting
with a simple, unsupervised model of morphology
that segments words into sequences of morphemes,
assuming a (naive) concatenative generation process
and a single analysis per type.
Unsupervised morphological segmentation. We
assume that each word can be decomposed into any
number of prefixes, a stem, and any number of suf-
fixes. Formally, we let M represent the set of all
possible morphemes and define a regular grammar
M∗MM∗ (i.e., zero or more prefixes, a stem, and
zero or more suffixes). To infer the decomposition
structure for the words in the target language, we as-
sume that the vocabulary was generated by the fol-
lowing process:
</bodyText>
<listItem confidence="0.996618941176471">
1. Sample morpheme distributions from symmet-
ric Dirichlet distributions: θp — Dir|M|(αp)
for prefixes, θσ — Dir|M|(ασ) for stems, and
θs — Dir|M|(αs) for suffixes.
2. Sample length distribution parameters
λp — Beta(βp, γp) for prefix sequences
and λs — Beta(βs, γs) for suffix sequences.
3. Sample a vocabulary by creating each word
type w using the following steps:
(a) Sample affix sequence lengths:
lp — Geometric(λp);
ls — Geometric(λs).
(b) Sample lp prefixes pi, ... , pl, indepen-
dently from θp; ls suffixes s1,... , sl3 in-
dependently from θs; and a stem σ — θσ.
(c) Concatenate prefixes, the stem, and suf-
fixes: w = p1+· · ·+pl,+σ+s1+· · ·+sl3.
</listItem>
<bodyText confidence="0.9314025">
We use blocked Gibbs sampling to sample seg-
mentations for each word in the training vocabulary.
Because of our particular choice of priors, it possible
to approximately decompose the posterior over the
arcs of a compact finite-state machine. Sampling a
segmentation or obtaining the most likely segmenta-
tion a posteriori then reduces to familiar FST opera-
tions. This model is reminiscent of work on learning
morphology using adaptor grammars (Johnson et al.,
2006; Johnson, 2008).
The inferred morphological grammar is very sen-
sitive to the Dirichlet hyperparameters (αp, αs, ασ)
and these are, in turn, sensitive to the number of
types in the vocabulary. Using αp, αs « ασ « 1
tended to recover useful segmentations, but we have
not yet been able to find reliable generic priors for
these values. Therefore, we selected them empiri-
cally to obtain a stem vocabulary size on the parallel
data that is one-to-one with English.4 Future work
4Our default starting point was to use α, = α3 =
10−s, αo = 10−4 and then to adjust all parameters by factors
of 10.
</bodyText>
<page confidence="0.994859">
1680
</page>
<tableCaption confidence="0.999874">
Table 1: Corpus statistics.
</tableCaption>
<table confidence="0.9983486">
Sentences EN-tokens Parallel TRG-types Parallel+Monolingual
TRG-tokens EN-types Sentences TRG-tokens TRG-types
Russian 150k 3.5M 3.3M 131k 254k 20M 360M 1,971k
Hebrew 134k 2.7M 2.0M 48k 120k 806k 15M 316k
Swahili 15k 0.3M 0.3M 23k 35k 596k 13M 334k
</table>
<bodyText confidence="0.9989937">
will involve a more direct method for specifying or
inferring these values.
Unsupervised morphology features: O(A). For
the unsupervised analyzer, we do not have a map-
ping from morphemes to structured morphological
attributes; however, we can create features from the
affix sequences obtained after morphological seg-
mentation. We produce binary features correspond-
ing to the content of each potential affixation posi-
tion relative to the stem:
</bodyText>
<equation confidence="0.4688845">
prefix suffix
...-3 -2 -1 STEM +1 +2 +3...
</equation>
<bodyText confidence="0.999893">
For example, the unsupervised analysis A =
wa+ki+wa+STEM of the Swahili word wakiwapiga
will produce the following features:
</bodyText>
<equation confidence="0.968870333333333">
/prefix [−3] [wa] (A) = 1,
4&apos;prefix[−2] [ki] (A) = 1,
�prefix[−1][wa](A) = 1-
</equation>
<sectionHeader confidence="0.998762" genericHeader="method">
4 Inflection Model Parameter Estimation
</sectionHeader>
<bodyText confidence="0.999989787878788">
To set the parameters W and V of the inflection pre-
diction model (Eq. 1), we use stochastic gradient de-
scent to maximize the conditional log-likelihood of
a training set consisting of pairs of source (English)
sentence contextual features (W) and target word in-
flectional features (,0). The training instances are
extracted from the word-aligned parallel corpus with
the English side preprocessed as discussed in §2.2
and the target side disambiguated as discussed in §3.
When morphological category information is avail-
able, we train an independent model for each open-
class category (in Russian, nouns, verbs, adjectives,
numerals, adverbs); otherwise a single model is used
for all words (excluding words less than four char-
acters long, which are ignored).
Statistics of the parallel corpora used to train the
inflection model are summarized in Table 1. It is
important to note here that our richly parameterized
model is trained on the full parallel training cor-
pus, not just on a handful of development sentences
(which are typically used to tune MT system param-
eters). Despite this scale, training is simple: the in-
flection model is trained to discriminate among dif-
ferent inflectional paradigms, not over all possible
target language sentences (Blunsom et al., 2008) or
learning from all observable rules (Subotin, 2011).
This makes the training problem relatively tractable:
all experiments in this paper were trained on a sin-
gle processor using a Cython implementation of the
SGD optimizer. For our largest model, trained on
3.3M Russian words, n = 231K x m = 336 fea-
tures were produced, and 10 SGD iterations were
performed in less than 16 hours.
</bodyText>
<subsectionHeader confidence="0.885925">
4.1 Intrinsic Evaluation
</subsectionHeader>
<bodyText confidence="0.9999575">
Before considering the broader problem of integrat-
ing the inflection model in a machine translation
system, we perform an artificial evaluation to ver-
ify that the model learns sensible source sentence-
target inflection patterns. To do so, we create an
inflection test set as follows. We preprocess the
source (English) sentences exactly as during train-
ing (§2.2), and using the target language morpholog-
ical analyzer, we convert each aligned target word to
(stem, inflection) pairs. We perform word alignment
on the held-out MT development data for each lan-
guage pair (cf. Table 1), exactly as if it were going to
produce training instances, but instead we use them
for testing.
Although the resulting dataset is noisy (e.g., due
to alignment errors), this becomes our intrinsic eval-
uation test set. Using this data, we measure inflec-
tion quality using two measurements:5
</bodyText>
<footnote confidence="0.774973">
5Note that we are not evaluating the stem translation model,
</footnote>
<page confidence="0.790266">
1681
</page>
<table confidence="0.999205444444444">
acc. ppl. JQ&apos;J
Unsup. Supervised N 64.1% 3.46 9.16
V 63.7% 3.41 20.12
Russian A 51.5% 6.24 19.56
M 73.0% 2.81 9.14
average 63.1% 3.98 14.49
Russian all 71.2% 2.15 4.73
Hebrew all 85.5% 1.49 2.55
Swahili all 78.2% 2.09 11.46
</table>
<tableCaption confidence="0.98133">
Table 2: Intrinsic evaluation of inflection model (N:
nouns, V: verbs, A: adjectives, M: numerals).
</tableCaption>
<listItem confidence="0.9958425">
• the accuracy of predicting the inflection given
the source, source context and target stem, and
• the inflection model perplexity on the same set
of test instances.
</listItem>
<bodyText confidence="0.999981857142857">
Additionally, we report the average number of pos-
sible inflections for each stem, an upper bound to the
perplexity that indicates the inherent difficulty of the
task. The results of this evaluation are presented in
Table 2 for the three language pairs considered. We
remark on two patterns in these results. First, per-
plexity is substantially lower than the perplexity of a
uniform model, indicating our model is overall quite
effective at predicting inflections using source con-
text only. Second, in the supervised Russian results,
we see that predicting the inflections of adjectives
is relatively more difficult than for other parts-of-
speech. Since adjectives agree with the nouns they
modify in gender and case, and gender is an idiosyn-
cratic feature of Russian nouns (and therefore not
directly predictable from the English source), this
difficulty is unsurprising.
We can also inspect the weights learned by the
model to assess the effectiveness of the features
in relating source-context structure with target-side
morphology. Such an analysis is presented in Fig. 3.
</bodyText>
<subsectionHeader confidence="0.97404">
4.2 Feature Ablation
</subsectionHeader>
<bodyText confidence="0.971565333333333">
Our inflection model makes use of numerous fea-
ture types. Table 3 explores the effect of removing
different kinds of (source) features from the model,
evaluated on predicting Russian inflections using
supervised morphological grammars.6 Rows 2–3
just the inflection prediction model.
</bodyText>
<footnote confidence="0.7479075">
6The models used in the feature ablation experiment were
trained on fewer examples, resulting in overall lower accuracies
</footnote>
<bodyText confidence="0.999901">
show the effect of removing either linear or depen-
dency context. We see that both are necessary for
good performance; however removing dependency
context substantially degrades performance of the
model (we interpret this result as evidence that Rus-
sian morphological inflection captures grammatical
relationships that would be expressed structurally in
English). The bottom four rows explore the effect
of source language word representation. The results
indicate that lexical features are important for accu-
rate prediction of inflection, and that POS tags and
Brown clusters are likewise important, but they seem
to capture similar information (removing one has lit-
tle impact, but removing both substantially degrades
performance).
</bodyText>
<tableCaption confidence="0.975771">
Table 3: Feature ablation experiments using supervised
Russian classification experiments.
</tableCaption>
<table confidence="0.99915075">
Features (W(e, i)) acc.
all 54.7%
−linear context 52.7%
−dependency context 44.4%
−POS tags 54.5%
−Brown clusters 54.5%
−POS tags, −Brown cl. 50.9%
−lexical items 51.2%
</table>
<sectionHeader confidence="0.980796" genericHeader="method">
5 Synthetic Phrases
</sectionHeader>
<bodyText confidence="0.995621352941177">
We turn now to translation; recall that our translate-
and-inflect model is used to augment the set of rules
available to a conventional statistical machine trans-
lation decoder. We refer to the phrases it produces
as synthetic phrases.
Our baseline system is a standard hierarchical
phrase-based translation model (Chiang, 2007). Fol-
lowing Lopez (2007), the training data is compiled
into an efficient binary representation which allows
extraction of sentence-specific grammars just before
decoding. In our case, this also allows the creation
of synthetic inflected phrases that are produced con-
ditioning on the sentence to translate.
To generate these synthetic phrases with new in-
flections possibly unseen in the parallel training
than seen in Table 2, but the pattern of results is the relevant
datapoint here.
</bodyText>
<page confidence="0.989683">
1682
</page>
<figure confidence="0.996715923076923">
Russian supervised
Verb: 1st Person
child(nsubj)=I child(nsubj)=we
Verb: Future tense
child(aux)=MD child(aux)=will
Noun: Animate
source=animals/victims/...
Noun: Feminine gender
source=obama/economy/...
Noun: Dative case
parent(iobj)
Adjective: Genitive case
grandparent(poss)
Hebrew
Suffix n+ (masculine plural)
parent=NNS after=NNS
Prefix K (first person sing. + future)
child(nsubj)=I child(aux)=&apos;ll
Prefix :) (preposition like/as)
child(prep)=IN parent=as
Suffix + (possesive mark)
before=my child(poss)=my
Suffix , t (feminine mark)
child(nsubj)=she before=she
Prefix tun (when)
before=when before=WRB
Swahili
Prefix li (past)
source=VBD source=VBN
Prefix nita (1st person sing. + future)
child(aux) child(nsubj)=I
Prefix ana (3rd person sing. + present)
source=VBZ
Prefix wa (3rd person plural)
before=they child(nsubj)=NNS
Suffix to (1st person plural)
child(nsubj)=she before=she
Prefix ha (negative tense)
source=no after=not
</figure>
<figureCaption confidence="0.9964465">
Figure 3: Examples of highly weighted features learned by the inflection model. We selected a few frequent morpho-
logical features and show their top corresponding source context features.
</figureCaption>
<bodyText confidence="0.996885538461538">
data, we first construct an additional phrase-based
translation model on the parallel corpus prepro-
cessed to replace inflected surface words with their
stems. We then extract a set of non-gappy phrases
for each sentence (e.g., X —* &lt;attempted,
IIbiTaTbcR V&gt;). The target side of each such phrase
is re-inflected, conditioned on the source sentence,
using the inflection model from §2. Each stem is
given its most likely inflection.7
The original features extracted for the stemmed
phrase are conserved, and the following features
are added to help the decoder select good synthetic
phrases:
</bodyText>
<listItem confidence="0.928893714285714">
• a binary feature indicating that the phrase is
synthetic,
• the log-probability of the inflected forms ac-
cording to our model,
• the count of words that have been inflected,
with a separate feature for each morphological
category in the supervised case.
</listItem>
<bodyText confidence="0.956442421052632">
Finally, these synthetic phrases are combined with
the original translation rules obtained for the base-
line system to produce an extended sentence-specific
grammar which is used as input to the decoder. If a
7Several reviewers asked about what happens when k-best
inflections are added. The results for k E {2, 4, 81 range from
no effect to an improvement over k = 1 of about 0.2 BLEU
(absolute). We hypothesize that larger values of k could have a
greater impact, perhaps in a more “global” model of the target
string; however, exploration of this question is beyond the scope
of this paper.
phrase already existing in the standard phrase table
happens to be recreated, both phrases are kept and
will compete with each other with different features
in the decoder.
For example, for the large EN—*RU system, 6%
of all the rules used for translation are synthetic
phrases, with 65% of these phrases being entirely
new rules.
</bodyText>
<sectionHeader confidence="0.988104" genericHeader="method">
6 Translation Experiments
</sectionHeader>
<bodyText confidence="0.944698166666667">
We evaluate our approach in the standard discrim-
inative MT framework. We use cdec (Dyer et al.,
2010) as our decoder and perform MIRA training
to learn feature weights of the sentence translation
model (Chiang, 2012). We compare the following
configurations:
</bodyText>
<listItem confidence="0.99988225">
• A baseline system, using a 4-gram language
model trained on the entire monolingual and
bilingual data available.
• An enriched system with a class-based n-gram
</listItem>
<bodyText confidence="0.809475222222222">
language model8 trained on the monolingual
data mapped to 600 Brown clusters. Class-
based language modeling is a strong baseline
for scenarios with high out-of-vocabulary rates
but in which large amounts of monolingual
target-language data are available.
• The enriched system further augmented with
our inflected synthetic phrases. We expect the
class-based language model to be especially
</bodyText>
<footnote confidence="0.976954">
8For Swahili and Hebrew, n = 6; for Russian, n = 7.
</footnote>
<page confidence="0.983333">
1683
</page>
<bodyText confidence="0.89576275">
helpful here and capture some basic agreement
patterns that can be learned more easily on
dense clusters than from plain word sequences.
Detailed corpus statistics are given in Table 1:
</bodyText>
<listItem confidence="0.903236909090909">
• The Russian data consist of the News Com-
mentary parallel corpus and additional mono-
lingual data crawled from news websites.9
• The Hebrew parallel corpus is composed of
transcribed TED talks (Cettolo et al., 2012).
Additional monolingual news data is also used.
• The Swahili parallel corpus was obtained by
crawling the Global Voices project website10
for parallel articles. Additional monolingual
data was taken from the Helsinki Corpus of
Swahili.11
</listItem>
<bodyText confidence="0.9999145">
We evaluate translation quality by translating and
measuring the BLEU score of a 2000–3000 sentence-
long evaluation corpus, averaging the results over 3
MIRA runs to control for optimizer instability (Clark
et al., 2011). Table 4 reports the results. For all lan-
guages, using class language models improves over
the baseline. When synthetic phrases are added, sig-
nificant additional improvements are obtained. For
the English–Russian language pair, where both su-
pervised and unsupervised analyses can be obtained,
we notice that expert-crafted morphological analyz-
ers are more efficient at improving translation qual-
ity. Globally, the amount of improvement observed
varies depending on the language; this is most likely
indicative of the quality of unsupervised morpholog-
ical segmentations produced and the kinds of gram-
matical relations expressed morphologically.
Finally, to confirm the effectiveness of our ap-
proach as corpus size increases, we use our tech-
nique on top of a state-of-the art English–Russian
system trained on data from the 8th ACL Work-
shop on Machine Translation (30M words of bilin-
gual text and 410M words of monolingual text). The
setup is identical except for the addition of sparse
</bodyText>
<footnote confidence="0.8886042">
9http://www.statmt.org/wmt13/
translation-task.html
10http://sw.globalvoicesonline.org
11http://www.aakkl.helsinki.fi/cameel/
corpus/intro.htm
</footnote>
<tableCaption confidence="0.950438">
Table 4: Translation quality (measured by BLEU) aver-
aged over 3 MIRA runs.
</tableCaption>
<table confidence="0.996997333333333">
EN→RU EN→HE EN→SW
Baseline 14.7±0.1 15.8±0.3 18.3±0.1
+Class LM 15.7±0.1 16.8±0.4 18.7±0.2
+Synthetic 16.2±0.1 17.6±0.1 19.0±0.1
unsupervised
supervised 16.7±0.1 — —
</table>
<bodyText confidence="0.9995095">
rule shape indicator features and bigram cluster fea-
tures. In these large scale conditions, the BLEU score
improves from 18.8 to 19.6 with the addition of word
clusters and reaches 20.0 with synthetic phrases.
Details regarding this system are reported in Ammar
et al. (2013).
</bodyText>
<sectionHeader confidence="0.999784" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.99975675862069">
Translation into morphologically rich languages is
a widely studied problem and there is a tremen-
dous amount of related work. Our technique of syn-
thesizing translation options to improve generation
of inflected forms is closely related to the factored
translation approach proposed by Koehn and Hoang
(2007); however, an important difference to that
work is that we use a discriminative model that con-
ditions on source context to make “local” decisions
about what inflections may be used before combin-
ing the phrases into a complete sentence translation.
Combination pre-/post-processing solutions are
also frequently proposed. In these, the tar-
get language is generally transformed from multi-
morphemic surface words into smaller units more
amenable to direct translation, and then a post-
processing step is applied independent of the trans-
lation model. For example, Oflazer and El-Kahlout
(2007) experiment with partial morpheme groupings
to produce novel inflected forms when translating
into Turkish; Al-Haj and Lavie (2010) compare dif-
ferent processing schemes for Arabic. A related but
different approach is to enrich the source language
items with grammatical features (e.g., a source sen-
tence like John saw Mary is preprocessed into, e.g.,
John+subj saw+msubj+fobj Mary+obj) so as
to make the source and target lexicons have simi-
lar morphological contrasts (Avramidis and Koehn,
2008; Yeniterzi and Oflazer, 2010; Chang et al.,
</bodyText>
<page confidence="0.98762">
1684
</page>
<bodyText confidence="0.999945596774193">
2009). In general, this work suffers from the prob-
lem that it is extremely difficult to know a priori
what the right preprocessing is for a given language
pair, data size, and domain.
Several post-processing approaches have relied
on supervised classifiers to predict the optimal com-
plete inflection for an incomplete or lemmatized
translation. Minkov et al. (2007) present a method
for predicting the inflection of Russian and Arabic
sentences aligned to English sentences. They train a
sequence model to predict target morphological fea-
tures from the lemmas and the syntactic structures
of both aligned sentences and demonstrate its ability
to recover accurately inflections on reference trans-
lations. Toutanova et al. (2008) apply this method
to generate inflections after translation in two differ-
ent ways: by rescoring inflected n-best outputs or by
translating lemmas and re-inflecting them a posteri-
ori. El Kholy and Habash (2012) follow a similar
method and compare different approaches for gen-
erating rich morphology in Arabic after a transla-
tion step. Fraser et al. (2012) observe improvements
for translation into German with a similar method.
As in that work, we model morphological features
rather than directly inflected forms. However, that
work may be criticized for providing no mechanism
to translate surface forms directly, even when evi-
dence for a direct translation is available in the par-
allel data.
Unsupervised morphology has begun to play a
role in translation between morphologically com-
plex languages. Stallard et al. (2012) show that an
unsupervised approach to Arabic segmentation per-
forms as well as a supervised segmenter for source-
side preprocessing (in terms of English translation
quality). For translation into morphological rich lan-
guages, Clifton and Sarkar (2011) use an unsuper-
vised morphological analyzer to produce morpho-
logical affixes in Finnish, injecting some linguistic
knowledge in the generation process.
Several authors have proposed using conditional
models to predict the probability of phrase transla-
tion in context (Gimpel and Smith, 2008; Chan et
al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010).
Of particular note is the work of Subotin (2011),
who use a conditional model to predict morpholog-
ical features conditioned on rich linguistic features;
however, this latter work also conditions on target
context, which substantially complicates decoding.
Finally, synthetic phrases have been used for
different purposes than generating morphology.
Callison-Burch et al. (2006) expanded the cov-
erage of a phrase table by adding synthesized
phrases by paraphrasing source language phrases,
Chen et al. (2011) produced “fabricated” phrases
by paraphrasing both source and target phrases, and
Habash (2009) created new rules to handle out-of-
vocabulary words. In related work, Tsvetkov et al.
(2013) used synthetic phrases to improve generation
of (in)definite articles when translating into English
from Russian and Czech, two languages which do
not lexically mark definiteness.
</bodyText>
<sectionHeader confidence="0.998456" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.998311428571429">
We have presented an efficient technique that ex-
ploits morphologically analyzed corpora to produce
new inflections possibly unseen in the bilingual
training data. Our method decomposes into two
simple independent steps involving well-understood
discriminative models.
By relying on source-side context to generate ad-
ditional local translation options and by leaving the
choice of the full sentence translation to the decoder,
we sidestep the difficulty of computing features on
target translations hypotheses. However, many mor-
phological processes (most notably, agreement) are
most best modeled using target language context. To
capture target context effects, we depend on strong
target language models. Therefore, an important
extension of our work is to explore the interaction
of our approach with more sophisticated language
models that more directly model morphology, e.g.,
the models of Bilmes and Kirchhoff (2003), or, alter-
natively, ways to incorporate target language context
in the inflection model.
We also achieve language independence by
exploiting unsupervised morphological segmen-
tations in the absence of linguistically informed
morphological analyses.
Code for replicating the experiments is available from
https://github.com/eschling/morphogen;
further details are available in (Schlinger et al., 2013).
</bodyText>
<page confidence="0.988865">
1685
</page>
<sectionHeader confidence="0.998097" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998703571428572">
This work was supported by the U. S. Army Research
Laboratory and the U. S. Army Research Office under
contract/grant number W911NF-10-1-0533. We would
like to thank Kim Spasaro for curating the Swahili devel-
opment and test sets, Yulia Tsvetkov for assistance with
Russian, and the anonymous reviewers for their helpful
comments.
</bodyText>
<sectionHeader confidence="0.997825" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999819642105263">
Hassan Al-Haj and Alon Lavie. 2010. The im-
pact of Arabic morphological segmentation on broad-
coverage English-to-Arabic statistical machine trans-
lation. In Proc. of AMTA.
Waleed Ammar, Victor Chahuneau, Michael Denkowski,
Greg Hanneman, Wang Ling, Austin Matthews, Ken-
ton Murray, Nicola Segall, Yulia Tsvetkov, Alon
Lavie, and Chris Dyer. 2013. The CMU machine
translation systems at WMT 2013: Syntax, synthetic
translation options, and pseudo-references. In Proc. of
WMT.
Eleftherios Avramidis and Philipp Koehn. 2008. Enrich-
ing morphologically poor languages for statistical ma-
chine translation. In Proc. of ACL.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff. In
Proc. of NAACL.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. of ACL.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter es-
timation. Computational Linguistics, 19(2):263–311.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Improved statistical machine transla-
tion using paraphrases. In Proc. of NAACL.
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proc. of EMNLP.
Mauro Cettolo, Christian Girardi, and Marcello Federico.
2012. WIT3: Web inventory of transcribed and trans-
lated talks. In Proc. of EAMT.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. of ACL.
Pi-Chuan Chang, Dan Jurafsky, and Christopher D. Man-
ning. 2009. Disambiguating “DE” for Chinese–
English machine translation. In Proc. of WMT.
Boxing Chen, Roland Kuhn, and George Foster. 2011.
Semantic smoothing and fabrication of phrase pairs for
SMT. In Proc. of IWSLT.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
David Chiang. 2012. Hope and fear for discrimina-
tive training of statistical translation models. JMLR,
13:1159–1187.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In Proc. of ACL.
Ann Clifton and Anoop Sarkar. 2011. Combin-
ing morpheme-based machine translation with post-
processing morpheme prediction. In Proc. of ACL.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proc. of
ACL.
Ahmed El Kholy and Nizar Habash. 2012. Translate,
predict or generate: Modeling rich morphology in sta-
tistical machine translation. In Proc. of EAMT.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling inflection and word-
formation in SMT. In Proc. of EACL.
Kevin Gimpel and Noah A. Smith. 2008. Rich source-
side context for statistical machine translation. In
Proc. of WMT.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proc. of ACL.
Nizar Habash. 2009. REMOOV: A tool for online han-
dling of out-of-vocabulary words in machine transla-
tion. In Proceedings of the 2nd International Confer-
ence on Arabic Language Resources and Tools.
Jan Hajiˇc, Pavel Krbec, Pavel Kvˇetoˇn, Karel Oliva, and
Vladimir Petkeviˇc. 2001. Serial combination of rules
and statistics: A case study in Czech tagging. In Proc.
of ACL.
Dilek Z. Hakkani-T¨ur, Kemal Oflazer, and G¨okhan T¨ur.
2000. Statistical morphological disambiguation for
agglutinative languages. In Proc. of COLING.
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki, and
Chris Quirk. 2010. A discriminative lexicon model
for complex morphology. In Proc. of AMTA.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2006. Adaptor grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
NIPS, pages 641–648.
Mark Johnson. 2008. Unsupervised word segmentation
for Sesotho using adaptor grammars. In Proc. SIG-
MORPHON.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proc. of EMNLP.
</reference>
<page confidence="0.810656">
1686
</page>
<reference confidence="0.999894125">
Adam Lopez. 2007. Hierarchical phrase-based transla-
tion with suffix arrays. In Proc. of EMNLP.
Andr´e F.T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M.Q. Aguiar, and M´ario A.T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proc. of EMNLP.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proc. of ACL.
Kemal Oflazer and ˙Ilknur Durgar El-Kahlout. 2007. Ex-
ploring different representational units in English-to-
Turkish statistical machine translation. In Proc. of
WMT.
Eva Schlinger, Victor Chahuneau, and Chris Dyer. 2013.
morphogen: Translation into morphologically rich lan-
guages with synthetic phrases. Prague Bulletin of
Mathematical Linguistics, (100).
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing and
evaluating a Russian tagset. In Proc. of LREC.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proc. of EMNLP.
David Stallard, Jacob Devlin, Michael Kayser,
Yoong Keok Lee, and Regina Barzilay. 2012.
Unsupervised morphology rivals supervised morphol-
ogy for Arabic MT. In Proc. of ACL.
Michael Subotin. 2011. An exponential translation
model for target language morphology. In Proc. ACL.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proc. of ACL.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna Bha-
tia. 2013. Generating English determiners in phrase-
based translation with synthetic translation options. In
Proc. of WMT.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based statis-
tical machine translation from English to Turkish. In
Proc. of ACL.
</reference>
<page confidence="0.993451">
1687
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.325810">
<title confidence="0.99994">Translating into Morphologically Rich Languages with Synthetic Phrases</title>
<author confidence="0.977794">Victor Chahuneau Eva Schlinger Noah A Smith Chris</author>
<affiliation confidence="0.8400755">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.992912">Pittsburgh, PA 15213,</address>
<abstract confidence="0.971418894736842">Translation into morphologically rich languages is an important but recalcitrant problem in MT. We present a simple and effective approach that deals with the problem in two phases. First, a discriminative model is learned to predict inflections of target words from rich source-side annotations. Then, this model is used to create additional sentencespecific wordand phrase-level translations that are added to a standard translation model as “synthetic” phrases. Our approach relies on morphological analysis of the target language, but we show that an unsupervised Bayesian model of morphology can successfully be used in place of a supervised analyzer. We report significant improvements in translation quality when translating from English to Russian, Hebrew and Swahili.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hassan Al-Haj</author>
<author>Alon Lavie</author>
</authors>
<title>The impact of Arabic morphological segmentation on broadcoverage English-to-Arabic statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proc. of AMTA.</booktitle>
<contexts>
<context position="29526" citStr="Al-Haj and Lavie (2010)" startWordPosition="4636" endWordPosition="4639">tions on source context to make “local” decisions about what inflections may be used before combining the phrases into a complete sentence translation. Combination pre-/post-processing solutions are also frequently proposed. In these, the target language is generally transformed from multimorphemic surface words into smaller units more amenable to direct translation, and then a postprocessing step is applied independent of the translation model. For example, Oflazer and El-Kahlout (2007) experiment with partial morpheme groupings to produce novel inflected forms when translating into Turkish; Al-Haj and Lavie (2010) compare different processing schemes for Arabic. A related but different approach is to enrich the source language items with grammatical features (e.g., a source sentence like John saw Mary is preprocessed into, e.g., John+subj saw+msubj+fobj Mary+obj) so as to make the source and target lexicons have similar morphological contrasts (Avramidis and Koehn, 2008; Yeniterzi and Oflazer, 2010; Chang et al., 1684 2009). In general, this work suffers from the problem that it is extremely difficult to know a priori what the right preprocessing is for a given language pair, data size, and domain. Sev</context>
</contexts>
<marker>Al-Haj, Lavie, 2010</marker>
<rawString>Hassan Al-Haj and Alon Lavie. 2010. The impact of Arabic morphological segmentation on broadcoverage English-to-Arabic statistical machine translation. In Proc. of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Waleed Ammar</author>
<author>Victor Chahuneau</author>
<author>Michael Denkowski</author>
<author>Greg Hanneman</author>
<author>Wang Ling</author>
<author>Austin Matthews</author>
<author>Kenton Murray</author>
<author>Nicola Segall</author>
<author>Yulia Tsvetkov</author>
<author>Alon Lavie</author>
<author>Chris Dyer</author>
</authors>
<title>The CMU machine translation systems at WMT 2013: Syntax, synthetic translation options, and pseudo-references.</title>
<date>2013</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="28483" citStr="Ammar et al. (2013)" startWordPosition="4479" endWordPosition="4482">n-task.html 10http://sw.globalvoicesonline.org 11http://www.aakkl.helsinki.fi/cameel/ corpus/intro.htm Table 4: Translation quality (measured by BLEU) averaged over 3 MIRA runs. EN→RU EN→HE EN→SW Baseline 14.7±0.1 15.8±0.3 18.3±0.1 +Class LM 15.7±0.1 16.8±0.4 18.7±0.2 +Synthetic 16.2±0.1 17.6±0.1 19.0±0.1 unsupervised supervised 16.7±0.1 — — rule shape indicator features and bigram cluster features. In these large scale conditions, the BLEU score improves from 18.8 to 19.6 with the addition of word clusters and reaches 20.0 with synthetic phrases. Details regarding this system are reported in Ammar et al. (2013). 7 Related Work Translation into morphologically rich languages is a widely studied problem and there is a tremendous amount of related work. Our technique of synthesizing translation options to improve generation of inflected forms is closely related to the factored translation approach proposed by Koehn and Hoang (2007); however, an important difference to that work is that we use a discriminative model that conditions on source context to make “local” decisions about what inflections may be used before combining the phrases into a complete sentence translation. Combination pre-/post-proces</context>
</contexts>
<marker>Ammar, Chahuneau, Denkowski, Hanneman, Ling, Matthews, Murray, Segall, Tsvetkov, Lavie, Dyer, 2013</marker>
<rawString>Waleed Ammar, Victor Chahuneau, Michael Denkowski, Greg Hanneman, Wang Ling, Austin Matthews, Kenton Murray, Nicola Segall, Yulia Tsvetkov, Alon Lavie, and Chris Dyer. 2013. The CMU machine translation systems at WMT 2013: Syntax, synthetic translation options, and pseudo-references. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleftherios Avramidis</author>
<author>Philipp Koehn</author>
</authors>
<title>Enriching morphologically poor languages for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="29889" citStr="Avramidis and Koehn, 2008" startWordPosition="4692" endWordPosition="4695">slation, and then a postprocessing step is applied independent of the translation model. For example, Oflazer and El-Kahlout (2007) experiment with partial morpheme groupings to produce novel inflected forms when translating into Turkish; Al-Haj and Lavie (2010) compare different processing schemes for Arabic. A related but different approach is to enrich the source language items with grammatical features (e.g., a source sentence like John saw Mary is preprocessed into, e.g., John+subj saw+msubj+fobj Mary+obj) so as to make the source and target lexicons have similar morphological contrasts (Avramidis and Koehn, 2008; Yeniterzi and Oflazer, 2010; Chang et al., 1684 2009). In general, this work suffers from the problem that it is extremely difficult to know a priori what the right preprocessing is for a given language pair, data size, and domain. Several post-processing approaches have relied on supervised classifiers to predict the optimal complete inflection for an incomplete or lemmatized translation. Minkov et al. (2007) present a method for predicting the inflection of Russian and Arabic sentences aligned to English sentences. They train a sequence model to predict target morphological features from t</context>
</contexts>
<marker>Avramidis, Koehn, 2008</marker>
<rawString>Eleftherios Avramidis and Philipp Koehn. 2008. Enriching morphologically poor languages for statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff A Bilmes</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Factored language models and generalized parallel backoff.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="33905" citStr="Bilmes and Kirchhoff (2003)" startWordPosition="5293" endWordPosition="5296">ontext to generate additional local translation options and by leaving the choice of the full sentence translation to the decoder, we sidestep the difficulty of computing features on target translations hypotheses. However, many morphological processes (most notably, agreement) are most best modeled using target language context. To capture target context effects, we depend on strong target language models. Therefore, an important extension of our work is to explore the interaction of our approach with more sophisticated language models that more directly model morphology, e.g., the models of Bilmes and Kirchhoff (2003), or, alternatively, ways to incorporate target language context in the inflection model. We also achieve language independence by exploiting unsupervised morphological segmentations in the absence of linguistically informed morphological analyses. Code for replicating the experiments is available from https://github.com/eschling/morphogen; further details are available in (Schlinger et al., 2013). 1685 Acknowledgments This work was supported by the U. S. Army Research Laboratory and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533. We would like to thank Kim Spasaro</context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored language models and generalized parallel backoff. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="17020" citStr="Blunsom et al., 2008" startWordPosition="2732" endWordPosition="2735">e a single model is used for all words (excluding words less than four characters long, which are ignored). Statistics of the parallel corpora used to train the inflection model are summarized in Table 1. It is important to note here that our richly parameterized model is trained on the full parallel training corpus, not just on a handful of development sentences (which are typically used to tune MT system parameters). Despite this scale, training is simple: the inflection model is trained to discriminate among different inflectional paradigms, not over all possible target language sentences (Blunsom et al., 2008) or learning from all observable rules (Subotin, 2011). This makes the training problem relatively tractable: all experiments in this paper were trained on a single processor using a Cython implementation of the SGD optimizer. For our largest model, trained on 3.3M Russian words, n = 231K x m = 336 features were produced, and 10 SGD iterations were performed in less than 16 hours. 4.1 Intrinsic Evaluation Before considering the broader problem of integrating the inflection model in a machine translation system, we perform an artificial evaluation to verify that the model learns sensible source</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="5742" citStr="Brown et al. (1993)" startWordPosition="881" endWordPosition="884">words f. The model assumes independence between each target word f conditioned on the source sentence e and its aligned position i in this sentence.2 This assumption is further relaxed in §5 when the model is integrated in the translation system. We decompose the probability of generating each target word f in the following way: 1: p(f |e, i) = σ?µ=f Here, each stem is generated independently from a single aligned source word ei, but in practice we 1This prevents the model from generating words that would be difficult for the language model to reliably score. 2This is the same assumption that Brown et al. (1993) make in, for example, IBM Model 1. 2.1 Modeling Inflection In morphologically rich languages, each stem may be combined with one or more inflectional morphemes to express many different grammatical features (e.g., case, definiteness, mood, tense, etc.). Since the inflectional morphology of a word generally expresses multiple grammatical features, we would like a model that naturally incorporates rich, possibly overlapping features in its representation of both the input (i.e., conditioning context) and output (i.e., the inflection pattern). We therefore use the following parametric form to mo</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="32471" citStr="Callison-Burch et al. (2006)" startWordPosition="5085" endWordPosition="5088"> linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphrasing both source and target phrases, and Habash (2009) created new rules to handle out-ofvocabulary words. In related work, Tsvetkov et al. (2013) used synthetic phrases to improve generation of (in)definite articles when translating into English from Russian and Czech, two languages which do not lexically mark definiteness. 8 Conclusion We have presented an efficient technique that exploits morphologically analyzed corpora </context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Improved statistical machine translation using paraphrases. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="32071" citStr="Carpuat and Wu, 2007" startWordPosition="5028" endWordPosition="5031">mplex languages. Stallard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing (in terms of English translation quality). For translation into morphological rich languages, Clifton and Sarkar (2011) use an unsupervised morphological analyzer to produce morphological affixes in Finnish, injecting some linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphrasing both source and target </context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Christian Girardi</author>
<author>Marcello Federico</author>
</authors>
<title>WIT3: Web inventory of transcribed and translated talks.</title>
<date>2012</date>
<booktitle>In Proc. of EAMT.</booktitle>
<contexts>
<context position="26379" citStr="Cettolo et al., 2012" startWordPosition="4174" endWordPosition="4177">nguage data are available. • The enriched system further augmented with our inflected synthetic phrases. We expect the class-based language model to be especially 8For Swahili and Hebrew, n = 6; for Russian, n = 7. 1683 helpful here and capture some basic agreement patterns that can be learned more easily on dense clusters than from plain word sequences. Detailed corpus statistics are given in Table 1: • The Russian data consist of the News Commentary parallel corpus and additional monolingual data crawled from news websites.9 • The Hebrew parallel corpus is composed of transcribed TED talks (Cettolo et al., 2012). Additional monolingual news data is also used. • The Swahili parallel corpus was obtained by crawling the Global Voices project website10 for parallel articles. Additional monolingual data was taken from the Helsinki Corpus of Swahili.11 We evaluate translation quality by translating and measuring the BLEU score of a 2000–3000 sentencelong evaluation corpus, averaging the results over 3 MIRA runs to control for optimizer instability (Clark et al., 2011). Table 4 reports the results. For all languages, using class language models improves over the baseline. When synthetic phrases are added, s</context>
</contexts>
<marker>Cettolo, Girardi, Federico, 2012</marker>
<rawString>Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. WIT3: Web inventory of transcribed and translated talks. In Proc. of EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>David Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="32049" citStr="Chan et al., 2007" startWordPosition="5024" endWordPosition="5027"> morphologically complex languages. Stallard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing (in terms of English translation quality). For translation into morphological rich languages, Clifton and Sarkar (2011) use an unsupervised morphological analyzer to produce morphological affixes in Finnish, injecting some linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphrasing b</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Dan Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Disambiguating “DE” for Chinese– English machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of WMT.</booktitle>
<marker>Chang, Jurafsky, Manning, 2009</marker>
<rawString>Pi-Chuan Chang, Dan Jurafsky, and Christopher D. Manning. 2009. Disambiguating “DE” for Chinese– English machine translation. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Roland Kuhn</author>
<author>George Foster</author>
</authors>
<title>Semantic smoothing and fabrication of phrase pairs for SMT.</title>
<date>2011</date>
<booktitle>In Proc. of IWSLT.</booktitle>
<contexts>
<context position="32601" citStr="Chen et al. (2011)" startWordPosition="5106" endWordPosition="5109"> translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphrasing both source and target phrases, and Habash (2009) created new rules to handle out-ofvocabulary words. In related work, Tsvetkov et al. (2013) used synthetic phrases to improve generation of (in)definite articles when translating into English from Russian and Czech, two languages which do not lexically mark definiteness. 8 Conclusion We have presented an efficient technique that exploits morphologically analyzed corpora to produce new inflections possibly unseen in the bilingual training data. Our method decomposes into two simple independent steps</context>
</contexts>
<marker>Chen, Kuhn, Foster, 2011</marker>
<rawString>Boxing Chen, Roland Kuhn, and George Foster. 2011. Semantic smoothing and fabrication of phrase pairs for SMT. In Proc. of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="2153" citStr="Chiang, 2007" startWordPosition="306" endWordPosition="307"> process of producing a translation for a word (or phrase) into two steps. First, a meaning-bearing stem is chosen and then an appropriate inflection is selected using a feature-rich discriminative model that conditions on the source context of the word being translated. Rather than attempting to directly produce fullsentence translations using such an elementary process, we use our model to generate translations of individual words and short phrases that augment— on a sentence-by-sentence basis—the inventory of translation rules obtained using standard translation rule extraction techniques (Chiang, 2007). We call these synthetic phrases. The major advantages of our approach are: (i) synthesized forms are targeted to a specific translation context; (ii) multiple, alternative phrases may be generated with the final choice among rules left to the global translation model; (iii) virtually no language-specific engineering is necessary; (iv) any phrase- or syntax-based decoder can be used without modification; and (v) we can generate forms that were not attested in the bilingual training data. The paper is structured as follows. We first present our “translate-and-inflect” model for predicting lexi</context>
<context position="21698" citStr="Chiang, 2007" startWordPosition="3464" endWordPosition="3465">ance). Table 3: Feature ablation experiments using supervised Russian classification experiments. Features (W(e, i)) acc. all 54.7% −linear context 52.7% −dependency context 44.4% −POS tags 54.5% −Brown clusters 54.5% −POS tags, −Brown cl. 50.9% −lexical items 51.2% 5 Synthetic Phrases We turn now to translation; recall that our translateand-inflect model is used to augment the set of rules available to a conventional statistical machine translation decoder. We refer to the phrases it produces as synthetic phrases. Our baseline system is a standard hierarchical phrase-based translation model (Chiang, 2007). Following Lopez (2007), the training data is compiled into an efficient binary representation which allows extraction of sentence-specific grammars just before decoding. In our case, this also allows the creation of synthetic inflected phrases that are produced conditioning on the sentence to translate. To generate these synthetic phrases with new inflections possibly unseen in the parallel training than seen in Table 2, but the pattern of results is the relevant datapoint here. 1682 Russian supervised Verb: 1st Person child(nsubj)=I child(nsubj)=we Verb: Future tense child(aux)=MD child(aux</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hope and fear for discriminative training of statistical translation models.</title>
<date>2012</date>
<journal>JMLR,</journal>
<pages>13--1159</pages>
<contexts>
<context position="25326" citStr="Chiang, 2012" startWordPosition="4009" endWordPosition="4010">tion is beyond the scope of this paper. phrase already existing in the standard phrase table happens to be recreated, both phrases are kept and will compete with each other with different features in the decoder. For example, for the large EN—*RU system, 6% of all the rules used for translation are synthetic phrases, with 65% of these phrases being entirely new rules. 6 Translation Experiments We evaluate our approach in the standard discriminative MT framework. We use cdec (Dyer et al., 2010) as our decoder and perform MIRA training to learn feature weights of the sentence translation model (Chiang, 2012). We compare the following configurations: • A baseline system, using a 4-gram language model trained on the entire monolingual and bilingual data available. • An enriched system with a class-based n-gram language model8 trained on the monolingual data mapped to 600 Brown clusters. Classbased language modeling is a strong baseline for scenarios with high out-of-vocabulary rates but in which large amounts of monolingual target-language data are available. • The enriched system further augmented with our inflected synthetic phrases. We expect the class-based language model to be especially 8For </context>
</contexts>
<marker>Chiang, 2012</marker>
<rawString>David Chiang. 2012. Hope and fear for discriminative training of statistical translation models. JMLR, 13:1159–1187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="26838" citStr="Clark et al., 2011" startWordPosition="4243" endWordPosition="4246">lel corpus and additional monolingual data crawled from news websites.9 • The Hebrew parallel corpus is composed of transcribed TED talks (Cettolo et al., 2012). Additional monolingual news data is also used. • The Swahili parallel corpus was obtained by crawling the Global Voices project website10 for parallel articles. Additional monolingual data was taken from the Helsinki Corpus of Swahili.11 We evaluate translation quality by translating and measuring the BLEU score of a 2000–3000 sentencelong evaluation corpus, averaging the results over 3 MIRA runs to control for optimizer instability (Clark et al., 2011). Table 4 reports the results. For all languages, using class language models improves over the baseline. When synthetic phrases are added, significant additional improvements are obtained. For the English–Russian language pair, where both supervised and unsupervised analyses can be obtained, we notice that expert-crafted morphological analyzers are more efficient at improving translation quality. Globally, the amount of improvement observed varies depending on the language; this is most likely indicative of the quality of unsupervised morphological segmentations produced and the kinds of gram</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Clifton</author>
<author>Anoop Sarkar</author>
</authors>
<title>Combining morpheme-based machine translation with postprocessing morpheme prediction.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="31740" citStr="Clifton and Sarkar (2011)" startWordPosition="4978" endWordPosition="4981">odel morphological features rather than directly inflected forms. However, that work may be criticized for providing no mechanism to translate surface forms directly, even when evidence for a direct translation is available in the parallel data. Unsupervised morphology has begun to play a role in translation between morphologically complex languages. Stallard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing (in terms of English translation quality). For translation into morphological rich languages, Clifton and Sarkar (2011) use an unsupervised morphological analyzer to produce morphological affixes in Finnish, injecting some linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target context, which substantially complicates </context>
</contexts>
<marker>Clifton, Sarkar, 2011</marker>
<rawString>Ann Clifton and Anoop Sarkar. 2011. Combining morpheme-based machine translation with postprocessing morpheme prediction. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Johnathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="25211" citStr="Dyer et al., 2010" startWordPosition="3989" endWordPosition="3992"> k could have a greater impact, perhaps in a more “global” model of the target string; however, exploration of this question is beyond the scope of this paper. phrase already existing in the standard phrase table happens to be recreated, both phrases are kept and will compete with each other with different features in the decoder. For example, for the large EN—*RU system, 6% of all the rules used for translation are synthetic phrases, with 65% of these phrases being entirely new rules. 6 Translation Experiments We evaluate our approach in the standard discriminative MT framework. We use cdec (Dyer et al., 2010) as our decoder and perform MIRA training to learn feature weights of the sentence translation model (Chiang, 2012). We compare the following configurations: • A baseline system, using a 4-gram language model trained on the entire monolingual and bilingual data available. • An enriched system with a class-based n-gram language model8 trained on the monolingual data mapped to 600 Brown clusters. Classbased language modeling is a strong baseline for scenarios with high out-of-vocabulary rates but in which large amounts of monolingual target-language data are available. • The enriched system furt</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed El Kholy</author>
<author>Nizar Habash</author>
</authors>
<title>Translate, predict or generate: Modeling rich morphology in statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proc. of EAMT.</booktitle>
<marker>El Kholy, Habash, 2012</marker>
<rawString>Ahmed El Kholy and Nizar Habash. 2012. Translate, predict or generate: Modeling rich morphology in statistical machine translation. In Proc. of EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Marion Weller</author>
<author>Aoife Cahill</author>
<author>Fabienne Cap</author>
</authors>
<title>Modeling inflection and wordformation in SMT.</title>
<date>2012</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="31021" citStr="Fraser et al. (2012)" startWordPosition="4869" endWordPosition="4872">h sentences. They train a sequence model to predict target morphological features from the lemmas and the syntactic structures of both aligned sentences and demonstrate its ability to recover accurately inflections on reference translations. Toutanova et al. (2008) apply this method to generate inflections after translation in two different ways: by rescoring inflected n-best outputs or by translating lemmas and re-inflecting them a posteriori. El Kholy and Habash (2012) follow a similar method and compare different approaches for generating rich morphology in Arabic after a translation step. Fraser et al. (2012) observe improvements for translation into German with a similar method. As in that work, we model morphological features rather than directly inflected forms. However, that work may be criticized for providing no mechanism to translate surface forms directly, even when evidence for a direct translation is available in the parallel data. Unsupervised morphology has begun to play a role in translation between morphologically complex languages. Stallard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing </context>
</contexts>
<marker>Fraser, Weller, Cahill, Cap, 2012</marker>
<rawString>Alexander Fraser, Marion Weller, Aoife Cahill, and Fabienne Cap. 2012. Modeling inflection and wordformation in SMT. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Rich sourceside context for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="32030" citStr="Gimpel and Smith, 2008" startWordPosition="5020" endWordPosition="5023">e in translation between morphologically complex languages. Stallard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing (in terms of English translation quality). For translation into morphological rich languages, Clifton and Sarkar (2011) use an unsupervised morphological analyzer to produce morphological affixes in Finnish, injecting some linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrase</context>
</contexts>
<marker>Gimpel, Smith, 2008</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2008. Rich sourceside context for statistical machine translation. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="10815" citStr="Habash and Rambow, 2005" startWordPosition="1738" endWordPosition="1741">rs (ii) for our target languages, Russian, Hebrew, and Swahili, using supervised and unsupervised methods. 3.1 Supervised Morphology The state-of-the-art in morphological analysis uses unweighted morphological transduction rules (usu3The entire monolingual data available for the translation task of the 8th ACL Workshop on Statistical Machine Translation was used. 1679 ally in the form of an FST) to produce candidate analyses for each word in a sentence and then statistical models to disambiguate among the analyses in context (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001; Smith et al., 2005; Habash and Rambow, 2005, inter alia). While this technique is capable of producing high quality linguistic analyses, it is expensive to develop, requiring hand-crafted rulebased analyzers and annotated corpora to train the disambiguation models. As a result, such analyzers are only available for a small number of languages, and, as a practical matter, each analyzer (which resulted from different development efforts) operates differently from the others. We therefore focus on using supervised analysis for a single target language, Russian. We use the analysis tool of Sharoff et al. (2008) which produces for each word</context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>Nizar Habash and Owen Rambow. 2005. Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>REMOOV: A tool for online handling of out-of-vocabulary words in machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2nd International Conference on Arabic Language Resources and Tools.</booktitle>
<contexts>
<context position="32697" citStr="Habash (2009)" startWordPosition="5121" endWordPosition="5122">., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphrasing both source and target phrases, and Habash (2009) created new rules to handle out-ofvocabulary words. In related work, Tsvetkov et al. (2013) used synthetic phrases to improve generation of (in)definite articles when translating into English from Russian and Czech, two languages which do not lexically mark definiteness. 8 Conclusion We have presented an efficient technique that exploits morphologically analyzed corpora to produce new inflections possibly unseen in the bilingual training data. Our method decomposes into two simple independent steps involving well-understood discriminative models. By relying on source-side context to generate </context>
</contexts>
<marker>Habash, 2009</marker>
<rawString>Nizar Habash. 2009. REMOOV: A tool for online handling of out-of-vocabulary words in machine translation. In Proceedings of the 2nd International Conference on Arabic Language Resources and Tools.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Pavel Krbec</author>
<author>Pavel Kvˇetoˇn</author>
<author>Karel Oliva</author>
<author>Vladimir Petkeviˇc</author>
</authors>
<title>Serial combination of rules and statistics: A case study in Czech tagging.</title>
<date>2001</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Hajiˇc, Krbec, Kvˇetoˇn, Oliva, Petkeviˇc, 2001</marker>
<rawString>Jan Hajiˇc, Pavel Krbec, Pavel Kvˇetoˇn, Karel Oliva, and Vladimir Petkeviˇc. 2001. Serial combination of rules and statistics: A case study in Czech tagging. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dilek Z Hakkani-T¨ur</author>
<author>Kemal Oflazer</author>
<author>G¨okhan T¨ur</author>
</authors>
<title>Statistical morphological disambiguation for agglutinative languages. In</title>
<date>2000</date>
<booktitle>Proc. of COLING.</booktitle>
<marker>Hakkani-T¨ur, Oflazer, T¨ur, 2000</marker>
<rawString>Dilek Z. Hakkani-T¨ur, Kemal Oflazer, and G¨okhan T¨ur. 2000. Statistical morphological disambiguation for agglutinative languages. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwoo Jeong</author>
<author>Kristina Toutanova</author>
<author>Hisami Suzuki</author>
<author>Chris Quirk</author>
</authors>
<title>A discriminative lexicon model for complex morphology.</title>
<date>2010</date>
<booktitle>In Proc. of AMTA.</booktitle>
<contexts>
<context position="32092" citStr="Jeong et al., 2010" startWordPosition="5032" endWordPosition="5035">ard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing (in terms of English translation quality). For translation into morphological rich languages, Clifton and Sarkar (2011) use an unsupervised morphological analyzer to produce morphological affixes in Finnish, injecting some linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphrasing both source and target phrases, and Habash (</context>
</contexts>
<marker>Jeong, Toutanova, Suzuki, Quirk, 2010</marker>
<rawString>Minwoo Jeong, Kristina Toutanova, Hisami Suzuki, and Chris Quirk. 2010. A discriminative lexicon model for complex morphology. In Proc. of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models.</title>
<date>2006</date>
<journal>NIPS,</journal>
<pages>641--648</pages>
<contexts>
<context position="14145" citStr="Johnson et al., 2006" startWordPosition="2269" endWordPosition="2272">om θp; ls suffixes s1,... , sl3 independently from θs; and a stem σ — θσ. (c) Concatenate prefixes, the stem, and suffixes: w = p1+· · ·+pl,+σ+s1+· · ·+sl3. We use blocked Gibbs sampling to sample segmentations for each word in the training vocabulary. Because of our particular choice of priors, it possible to approximately decompose the posterior over the arcs of a compact finite-state machine. Sampling a segmentation or obtaining the most likely segmentation a posteriori then reduces to familiar FST operations. This model is reminiscent of work on learning morphology using adaptor grammars (Johnson et al., 2006; Johnson, 2008). The inferred morphological grammar is very sensitive to the Dirichlet hyperparameters (αp, αs, ασ) and these are, in turn, sensitive to the number of types in the vocabulary. Using αp, αs « ασ « 1 tended to recover useful segmentations, but we have not yet been able to find reliable generic priors for these values. Therefore, we selected them empirically to obtain a stem vocabulary size on the parallel data that is one-to-one with English.4 Future work 4Our default starting point was to use α, = α3 = 10−s, αo = 10−4 and then to adjust all parameters by factors of 10. 1680 Tab</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2006</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2006. Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models. NIPS, pages 641–648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Unsupervised word segmentation for Sesotho using adaptor grammars.</title>
<date>2008</date>
<booktitle>In Proc. SIGMORPHON.</booktitle>
<contexts>
<context position="14161" citStr="Johnson, 2008" startWordPosition="2273" endWordPosition="2274">... , sl3 independently from θs; and a stem σ — θσ. (c) Concatenate prefixes, the stem, and suffixes: w = p1+· · ·+pl,+σ+s1+· · ·+sl3. We use blocked Gibbs sampling to sample segmentations for each word in the training vocabulary. Because of our particular choice of priors, it possible to approximately decompose the posterior over the arcs of a compact finite-state machine. Sampling a segmentation or obtaining the most likely segmentation a posteriori then reduces to familiar FST operations. This model is reminiscent of work on learning morphology using adaptor grammars (Johnson et al., 2006; Johnson, 2008). The inferred morphological grammar is very sensitive to the Dirichlet hyperparameters (αp, αs, ασ) and these are, in turn, sensitive to the number of types in the vocabulary. Using αp, αs « ασ « 1 tended to recover useful segmentations, but we have not yet been able to find reliable generic priors for these values. Therefore, we selected them empirically to obtain a stem vocabulary size on the parallel data that is one-to-one with English.4 Future work 4Our default starting point was to use α, = α3 = 10−s, αo = 10−4 and then to adjust all parameters by factors of 10. 1680 Table 1: Corpus sta</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>Mark Johnson. 2008. Unsupervised word segmentation for Sesotho using adaptor grammars. In Proc. SIGMORPHON.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="28807" citStr="Koehn and Hoang (2007)" startWordPosition="4529" endWordPosition="4532">upervised 16.7±0.1 — — rule shape indicator features and bigram cluster features. In these large scale conditions, the BLEU score improves from 18.8 to 19.6 with the addition of word clusters and reaches 20.0 with synthetic phrases. Details regarding this system are reported in Ammar et al. (2013). 7 Related Work Translation into morphologically rich languages is a widely studied problem and there is a tremendous amount of related work. Our technique of synthesizing translation options to improve generation of inflected forms is closely related to the factored translation approach proposed by Koehn and Hoang (2007); however, an important difference to that work is that we use a discriminative model that conditions on source context to make “local” decisions about what inflections may be used before combining the phrases into a complete sentence translation. Combination pre-/post-processing solutions are also frequently proposed. In these, the target language is generally transformed from multimorphemic surface words into smaller units more amenable to direct translation, and then a postprocessing step is applied independent of the translation model. For example, Oflazer and El-Kahlout (2007) experiment </context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Hierarchical phrase-based translation with suffix arrays.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="21722" citStr="Lopez (2007)" startWordPosition="3468" endWordPosition="3469">blation experiments using supervised Russian classification experiments. Features (W(e, i)) acc. all 54.7% −linear context 52.7% −dependency context 44.4% −POS tags 54.5% −Brown clusters 54.5% −POS tags, −Brown cl. 50.9% −lexical items 51.2% 5 Synthetic Phrases We turn now to translation; recall that our translateand-inflect model is used to augment the set of rules available to a conventional statistical machine translation decoder. We refer to the phrases it produces as synthetic phrases. Our baseline system is a standard hierarchical phrase-based translation model (Chiang, 2007). Following Lopez (2007), the training data is compiled into an efficient binary representation which allows extraction of sentence-specific grammars just before decoding. In our case, this also allows the creation of synthetic inflected phrases that are produced conditioning on the sentence to translate. To generate these synthetic phrases with new inflections possibly unseen in the parallel training than seen in Table 2, but the pattern of results is the relevant datapoint here. 1682 Russian supervised Verb: 1st Person child(nsubj)=I child(nsubj)=we Verb: Future tense child(aux)=MD child(aux)=will Noun: Animate sou</context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>Adam Lopez. 2007. Hierarchical phrase-based translation with suffix arrays. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
<author>Pedro M Q Aguiar</author>
<author>M´ario A T Figueiredo</author>
</authors>
<title>Turbo parsers: Dependency parsing by approximate variational inference.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="9667" citStr="Martins et al., 2010" startWordPosition="1557" endWordPosition="1561">example shown in Fig. 1, where most of the inflection features of the Russian word (past tense, singular number, and feminine gender) can be inferred from the context of the English word it is aligned to. Indeed, many grammatical functions expressed morphologically in Russian are expressed syntactically in English. Fortunately, high-quality parsers and other linguistic analyzers are available for English. On the source side, we apply the following processing steps: • Part-of-speech tagging with a CRF tagger trained on sections 02–21 of the Penn Treebank. • Dependency parsing with TurboParser (Martins et al., 2010), a non-projective dependency parser trained on the Penn Treebank to produce basic Stanford dependencies. • Assignment of tokens to one of 600 Brown clusters, trained on 8G words of English text.3 We then extract binary features from a using this information, by considering the aligned source word ei, its preceding and following words, and its syntactic neighbors. These are detailed in Figure 2. 3 Morphological Grammars and Features We now describe how to obtain morphological analyses and convert them into feature vectors (ii) for our target languages, Russian, Hebrew, and Swahili, using super</context>
</contexts>
<marker>Martins, Smith, Xing, Aguiar, Figueiredo, 2010</marker>
<rawString>Andr´e F.T. Martins, Noah A. Smith, Eric P. Xing, Pedro M.Q. Aguiar, and M´ario A.T. Figueiredo. 2010. Turbo parsers: Dependency parsing by approximate variational inference. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>Kristina Toutanova</author>
<author>Hisami Suzuki</author>
</authors>
<title>Generating complex morphology for machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="30304" citStr="Minkov et al. (2007)" startWordPosition="4758" endWordPosition="4761">ource sentence like John saw Mary is preprocessed into, e.g., John+subj saw+msubj+fobj Mary+obj) so as to make the source and target lexicons have similar morphological contrasts (Avramidis and Koehn, 2008; Yeniterzi and Oflazer, 2010; Chang et al., 1684 2009). In general, this work suffers from the problem that it is extremely difficult to know a priori what the right preprocessing is for a given language pair, data size, and domain. Several post-processing approaches have relied on supervised classifiers to predict the optimal complete inflection for an incomplete or lemmatized translation. Minkov et al. (2007) present a method for predicting the inflection of Russian and Arabic sentences aligned to English sentences. They train a sequence model to predict target morphological features from the lemmas and the syntactic structures of both aligned sentences and demonstrate its ability to recover accurately inflections on reference translations. Toutanova et al. (2008) apply this method to generate inflections after translation in two different ways: by rescoring inflected n-best outputs or by translating lemmas and re-inflecting them a posteriori. El Kholy and Habash (2012) follow a similar method and</context>
</contexts>
<marker>Minkov, Toutanova, Suzuki, 2007</marker>
<rawString>Einat Minkov, Kristina Toutanova, and Hisami Suzuki. 2007. Generating complex morphology for machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
<author>˙Ilknur Durgar El-Kahlout</author>
</authors>
<title>Exploring different representational units in English-toTurkish statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="29395" citStr="Oflazer and El-Kahlout (2007)" startWordPosition="4618" endWordPosition="4621">proach proposed by Koehn and Hoang (2007); however, an important difference to that work is that we use a discriminative model that conditions on source context to make “local” decisions about what inflections may be used before combining the phrases into a complete sentence translation. Combination pre-/post-processing solutions are also frequently proposed. In these, the target language is generally transformed from multimorphemic surface words into smaller units more amenable to direct translation, and then a postprocessing step is applied independent of the translation model. For example, Oflazer and El-Kahlout (2007) experiment with partial morpheme groupings to produce novel inflected forms when translating into Turkish; Al-Haj and Lavie (2010) compare different processing schemes for Arabic. A related but different approach is to enrich the source language items with grammatical features (e.g., a source sentence like John saw Mary is preprocessed into, e.g., John+subj saw+msubj+fobj Mary+obj) so as to make the source and target lexicons have similar morphological contrasts (Avramidis and Koehn, 2008; Yeniterzi and Oflazer, 2010; Chang et al., 1684 2009). In general, this work suffers from the problem th</context>
</contexts>
<marker>Oflazer, El-Kahlout, 2007</marker>
<rawString>Kemal Oflazer and ˙Ilknur Durgar El-Kahlout. 2007. Exploring different representational units in English-toTurkish statistical machine translation. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Schlinger</author>
<author>Victor Chahuneau</author>
<author>Chris Dyer</author>
</authors>
<title>morphogen: Translation into morphologically rich languages with synthetic phrases.</title>
<date>2013</date>
<journal>Prague Bulletin of Mathematical Linguistics,</journal>
<volume>100</volume>
<marker>Schlinger, Chahuneau, Dyer, 2013</marker>
<rawString>Eva Schlinger, Victor Chahuneau, and Chris Dyer. 2013. morphogen: Translation into morphologically rich languages with synthetic phrases. Prague Bulletin of Mathematical Linguistics, (100).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serge Sharoff</author>
</authors>
<title>Mikhail Kopotev, Tomaz Erjavec,</title>
<date>2008</date>
<booktitle>In Proc. of LREC.</booktitle>
<location>Anna</location>
<marker>Sharoff, 2008</marker>
<rawString>Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna Feldman, and Dagmar Divjak. 2008. Designing and evaluating a Russian tagset. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>David A Smith</author>
<author>Roy W Tromble</author>
</authors>
<title>Context-based morphological disambiguation with random fields.</title>
<date>2005</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="10790" citStr="Smith et al., 2005" startWordPosition="1734" endWordPosition="1737">m into feature vectors (ii) for our target languages, Russian, Hebrew, and Swahili, using supervised and unsupervised methods. 3.1 Supervised Morphology The state-of-the-art in morphological analysis uses unweighted morphological transduction rules (usu3The entire monolingual data available for the translation task of the 8th ACL Workshop on Statistical Machine Translation was used. 1679 ally in the form of an FST) to produce candidate analyses for each word in a sentence and then statistical models to disambiguate among the analyses in context (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001; Smith et al., 2005; Habash and Rambow, 2005, inter alia). While this technique is capable of producing high quality linguistic analyses, it is expensive to develop, requiring hand-crafted rulebased analyzers and annotated corpora to train the disambiguation models. As a result, such analyzers are only available for a small number of languages, and, as a practical matter, each analyzer (which resulted from different development efforts) operates differently from the others. We therefore focus on using supervised analysis for a single target language, Russian. We use the analysis tool of Sharoff et al. (2008) whi</context>
</contexts>
<marker>Smith, Smith, Tromble, 2005</marker>
<rawString>Noah A. Smith, David A. Smith, and Roy W. Tromble. 2005. Context-based morphological disambiguation with random fields. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Stallard</author>
<author>Jacob Devlin</author>
<author>Michael Kayser</author>
<author>Yoong Keok Lee</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised morphology rivals supervised morphology for Arabic MT.</title>
<date>2012</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="31490" citStr="Stallard et al. (2012)" startWordPosition="4941" endWordPosition="4944">h (2012) follow a similar method and compare different approaches for generating rich morphology in Arabic after a translation step. Fraser et al. (2012) observe improvements for translation into German with a similar method. As in that work, we model morphological features rather than directly inflected forms. However, that work may be criticized for providing no mechanism to translate surface forms directly, even when evidence for a direct translation is available in the parallel data. Unsupervised morphology has begun to play a role in translation between morphologically complex languages. Stallard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing (in terms of English translation quality). For translation into morphological rich languages, Clifton and Sarkar (2011) use an unsupervised morphological analyzer to produce morphological affixes in Finnish, injecting some linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 201</context>
</contexts>
<marker>Stallard, Devlin, Kayser, Lee, Barzilay, 2012</marker>
<rawString>David Stallard, Jacob Devlin, Michael Kayser, Yoong Keok Lee, and Regina Barzilay. 2012. Unsupervised morphology rivals supervised morphology for Arabic MT. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Subotin</author>
</authors>
<title>An exponential translation model for target language morphology.</title>
<date>2011</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="17074" citStr="Subotin, 2011" startWordPosition="2742" endWordPosition="2743">than four characters long, which are ignored). Statistics of the parallel corpora used to train the inflection model are summarized in Table 1. It is important to note here that our richly parameterized model is trained on the full parallel training corpus, not just on a handful of development sentences (which are typically used to tune MT system parameters). Despite this scale, training is simple: the inflection model is trained to discriminate among different inflectional paradigms, not over all possible target language sentences (Blunsom et al., 2008) or learning from all observable rules (Subotin, 2011). This makes the training problem relatively tractable: all experiments in this paper were trained on a single processor using a Cython implementation of the SGD optimizer. For our largest model, trained on 3.3M Russian words, n = 231K x m = 336 features were produced, and 10 SGD iterations were performed in less than 16 hours. 4.1 Intrinsic Evaluation Before considering the broader problem of integrating the inflection model in a machine translation system, we perform an artificial evaluation to verify that the model learns sensible source sentencetarget inflection patterns. To do so, we crea</context>
<context position="32142" citStr="Subotin (2011)" startWordPosition="5043" endWordPosition="5044"> Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing (in terms of English translation quality). For translation into morphological rich languages, Clifton and Sarkar (2011) use an unsupervised morphological analyzer to produce morphological affixes in Finnish, injecting some linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphrasing both source and target phrases, and Habash (2009) created new rules to handle out-ofvocabulary</context>
</contexts>
<marker>Subotin, 2011</marker>
<rawString>Michael Subotin. 2011. An exponential translation model for target language morphology. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Hisami Suzuki</author>
<author>Achim Ruopp</author>
</authors>
<title>Applying morphology generation models to machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="30666" citStr="Toutanova et al. (2008)" startWordPosition="4812" endWordPosition="4815">at the right preprocessing is for a given language pair, data size, and domain. Several post-processing approaches have relied on supervised classifiers to predict the optimal complete inflection for an incomplete or lemmatized translation. Minkov et al. (2007) present a method for predicting the inflection of Russian and Arabic sentences aligned to English sentences. They train a sequence model to predict target morphological features from the lemmas and the syntactic structures of both aligned sentences and demonstrate its ability to recover accurately inflections on reference translations. Toutanova et al. (2008) apply this method to generate inflections after translation in two different ways: by rescoring inflected n-best outputs or by translating lemmas and re-inflecting them a posteriori. El Kholy and Habash (2012) follow a similar method and compare different approaches for generating rich morphology in Arabic after a translation step. Fraser et al. (2012) observe improvements for translation into German with a similar method. As in that work, we model morphological features rather than directly inflected forms. However, that work may be criticized for providing no mechanism to translate surface </context>
</contexts>
<marker>Toutanova, Suzuki, Ruopp, 2008</marker>
<rawString>Kristina Toutanova, Hisami Suzuki, and Achim Ruopp. 2008. Applying morphology generation models to machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Chris Dyer</author>
<author>Lori Levin</author>
<author>Archna Bhatia</author>
</authors>
<title>Generating English determiners in phrasebased translation with synthetic translation options.</title>
<date>2013</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="32789" citStr="Tsvetkov et al. (2013)" startWordPosition="5134" endWordPosition="5137">odel to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphrasing both source and target phrases, and Habash (2009) created new rules to handle out-ofvocabulary words. In related work, Tsvetkov et al. (2013) used synthetic phrases to improve generation of (in)definite articles when translating into English from Russian and Czech, two languages which do not lexically mark definiteness. 8 Conclusion We have presented an efficient technique that exploits morphologically analyzed corpora to produce new inflections possibly unseen in the bilingual training data. Our method decomposes into two simple independent steps involving well-understood discriminative models. By relying on source-side context to generate additional local translation options and by leaving the choice of the full sentence translat</context>
</contexts>
<marker>Tsvetkov, Dyer, Levin, Bhatia, 2013</marker>
<rawString>Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna Bhatia. 2013. Generating English determiners in phrasebased translation with synthetic translation options. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reyyan Yeniterzi</author>
<author>Kemal Oflazer</author>
</authors>
<title>Syntax-tomorphology mapping in factored phrase-based statistical machine translation from English to Turkish.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="29918" citStr="Yeniterzi and Oflazer, 2010" startWordPosition="4696" endWordPosition="4699">cessing step is applied independent of the translation model. For example, Oflazer and El-Kahlout (2007) experiment with partial morpheme groupings to produce novel inflected forms when translating into Turkish; Al-Haj and Lavie (2010) compare different processing schemes for Arabic. A related but different approach is to enrich the source language items with grammatical features (e.g., a source sentence like John saw Mary is preprocessed into, e.g., John+subj saw+msubj+fobj Mary+obj) so as to make the source and target lexicons have similar morphological contrasts (Avramidis and Koehn, 2008; Yeniterzi and Oflazer, 2010; Chang et al., 1684 2009). In general, this work suffers from the problem that it is extremely difficult to know a priori what the right preprocessing is for a given language pair, data size, and domain. Several post-processing approaches have relied on supervised classifiers to predict the optimal complete inflection for an incomplete or lemmatized translation. Minkov et al. (2007) present a method for predicting the inflection of Russian and Arabic sentences aligned to English sentences. They train a sequence model to predict target morphological features from the lemmas and the syntactic s</context>
</contexts>
<marker>Yeniterzi, Oflazer, 2010</marker>
<rawString>Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-tomorphology mapping in factored phrase-based statistical machine translation from English to Turkish. In Proc. of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004034">
<title confidence="0.995212">
Predicting the resolution of referring expressions from user behavior
</title>
<author confidence="0.999446">
Nikos Engonopoulos&apos; Martin Villalba&apos; Ivan Titov2 Alexander Koller&apos;
</author>
<affiliation confidence="0.7265185">
&apos;University of Potsdam, Germany 2University of Amsterdam, Netherlands
{nikolaos.engonopoulos, martin.villalba}@uni-potsdam.de
</affiliation>
<email confidence="0.987074">
titov@uva.nl, koller@ling.uni-potsdam.de
</email>
<sectionHeader confidence="0.993688" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999879666666667">
We present a statistical model for predicting
how the user of an interactive, situated NLP
system resolved a referring expression. The
model makes an initial prediction based on the
meaning of the utterance, and revises it con-
tinuously based on the user’s behavior. The
combined model outperforms its components
in predicting reference resolution and when to
give feedback.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958322033899">
Speakers and listeners in natural communication are
engaged in a highly interactive process. In order to
achieve some communicative goal, the speaker will
perform an utterance which they believe has a high
chance of achieving that goal. They will then moni-
tor the listener’s behavior to see whether this goal is
actually being achieved. This process is a core part
of what is commonly called grounding in the dia-
logue literature (see e.g. (Clark, 1996; Traum, 1994;
Paek and Horvitz, 1999; Hirst et al., 1994)). Inter-
active computer systems that are to carry out an ef-
fective and efficient conversation with a user must
model this grounding process, and should ideally re-
spond to the user’s observed behavior in real time.
For instance, if the user of a pedestrian navigation
system takes a wrong turn, the system should inter-
pret this as evidence of misunderstanding and bring
the user back on track.
We focus here on the problem of predicting how
the user has resolved a referring expression (RE) that
was generated by the system, i.e. a noun phrase that
is intended to identify some object uniquely to the
listener. A number of authors have recently offered
statistical models for parts of this problem. Golland
et al. (2010) and Garoufi and Koller (2011) have
presented log-linear models for predicting how the
listener will resolve a given RE in a given scene;
however, these models do not update the probabil-
ity model based on observing the user’s reactions.
Nakano et al. (2007), Buschmeier and Kopp (2012),
and Koller et al. (2012) all predict what the listener
understood based on their behavior, but do not con-
sider the RE itself in the model. The models of
Frank and Goodman (2012) and Vogel et al. (2013)
aim at explaining the effect of implicatures on the
listener’s RE resolution process in terms of hypothe-
sized interactions, but do not actually support a real-
time interaction between a system and a user.
In this paper, we show how to predict how the
listener has resolved an RE by combining a statis-
tical model of RE resolution based on the RE itself
with a statistical model of RE resolution based on
the listener’s behavior. To our knowledge, this is
the first approach to combine two such models ex-
plicitly. We consider the RE grounding problem in
the context of interactive, situated natural language
generation (NLG) for the GIVE Challenge (Koller et
al., 2010a), where NLG systems must generate real-
time instructions in virtual 3D environments. Our
evaluation is based on interaction corpora from the
GIVE-2 and GIVE-2.5 Challenges, which contain
the systems’ utterances along with the behavior of
human hearers in response to these utterances. We
find that the combined model predicts RE resolu-
tion more accurately than each of the two compo-
nent models alone. We see this as a first step towards
implementing an actual interactive system that per-
forms human-like grounding based on our RE reso-
lution model.
</bodyText>
<page confidence="0.933039">
1354
</page>
<note confidence="0.4580215">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1354–1359,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.997619">
Figure 1: An example scene in the GIVE environment.
</figureCaption>
<sectionHeader confidence="0.828604" genericHeader="method">
2 Problem definition
</sectionHeader>
<bodyText confidence="0.999883369863014">
In the GIVE Challenge, an interactive NLG system
faces the task of guiding a human instruction fol-
lower (IF) through a treasure-hunt game in a vir-
tual 3D environment (see Fig. 1). To complete the
task, the IF must press a number of buttons in the
correct order; these buttons are the colored boxes in
Fig. 1, and are scattered all over the virtual environ-
ment. The IF can move around freely in the virtual
environment, but has no prior knowledge about the
world. The NLG system’s task is to guide the IF to-
wards the successful completion of the treasure-hunt
task. To this end, it is continuously being informed
about the IF’s movements and visual field, and can
generate written utterances at any time. As a com-
parative evaluation effort, the GIVE Challenges con-
nected NLG systems to thousands of users over the
Internet (see e.g. Koller et al. (2010a) for details).
Many system utterances are manipulation instruc-
tions, such as “press the blue button”, containing an
RE in the form of a definite NP. We call a given part
of an interaction between the system and the IF an
episode of that interaction if it starts with a manip-
ulation instruction, ends with the IF performing an
action (i.e., pressing a button), and contains only
IF movements and no further utterances in between.
Not all manipulation instructions initiate an episode,
because the system may decide to perform further
utterances (not containing REs) before the IF per-
forms their action. An NLG system will choose the
RE for an instruction at runtime out of potentially
many semantically valid alternatives (“the blue but-
ton”, “the button next to the chair”, “the button to
the right of the red button”, etc.). Ideally, it will pre-
dict which of these REs has the highest chance to be
understood by the IF, given the current scene, and
utter an instruction that uses this RE.
After uttering the manipulation instruction, the
system needs to ascertain whether the IF understood
the RE correctly, i.e. it must engage in grounding.
A naive grounding mechanism might wait until the
IF actually presses a button and check whether it was
the right one. This is what many NLG systems in the
GIVE Challenges actually did. However, this can
make the communication ineffective (IF performs
many useless actions) and risky (IF may press the
wrong button and lose). Thus, it is important that the
system updates its prediction of how the IF resolved
the RE continuously by observing the IF’s behavior,
before the actual button press. For instance, if the
IF walks towards the target, this might reinforce the
system’s belief in a correct understanding; turning
away or exiting the room could be strong evidence
of the opposite. The system can then exploit the up-
dated prediction to give the IF feedback (“no, the
blue button”) to prevent costly mistakes.
We address these challenges by estimating the
probability distribution over the possible objects to
which the IF may resolve the RE. We then update
this distribution in real time by observing the IF’s
movements. More specifically, assume that a sys-
tem tries to refer to some object a* among some set
A of available objects. Given an RE r generated for
a* at time to, the state of the world s at to, and the
observed behavior u(t) of the user at t &gt; to, we
estimate the probability p(aJr, s, u(t)) that the user
resolved r to an object a E A. When generating the
instruction, an optimal NLG system will use the RE
r that maximizes p(a*Jr, s, a(to)). It can then track
p(aJr, s, u(t)) for time points t &gt; to throughout the
episode, and generate feedback when p(a&apos;Jr, s, Q(t))
exceeds p(a*Jr, s, a(t)) for some a&apos; =� a*; that is,
when the updated probability distribution predicts
that the IF resolved r to an incorrect button.
</bodyText>
<sectionHeader confidence="0.990354" genericHeader="method">
3 A model of RE resolution
</sectionHeader>
<bodyText confidence="0.98095825">
In order to model the distribution over possible ob-
jects, we assume the following generative story:
when receiving an instruction containing an RE r at
a given world state s, the IF resolves it to an object
a; depending on the object a, the IF then moves to-
wards it, exhibiting behavior Q. These assumptions
correspond to the following factorization:
p(a, aJr, s) = p(aJa)p(aJr, s)
</bodyText>
<page confidence="0.927433">
1355
</page>
<bodyText confidence="0.999639333333333">
The posterior probability distribution over objects a
can be obtained by applying the Bayes rule and us-
ing the above assumptions:
</bodyText>
<equation confidence="0.664579">
p(a|r, s, Q) a p(a|r, s)p(a|Q)/p(a)
</equation>
<bodyText confidence="0.999987157894737">
For simplicity, we assume a uniform p(a) over all
objects in a world. We can thus represent p(a|r, s, Q)
as the normalized product of a semantic model
psem(a|r, s) and an observational model pobs(a|Q).
We use log-linear models for both, and train them
separately. The feature functions we use only con-
sider general properties of objects (such as color and
distance), and not the identity of the objects them-
selves. This means that we can train a model on one
virtual environment (containing a certain set of ob-
jects), and then apply the model to another virtual
environment, containing a different set of objects.
Semantic model The semantic model estimates
for each object a in the environment the initial prob-
ability psem(a|r, s) that the IF will understand a
given RE r uttered in a scene s as referring to a. It
represents the meaning of r, contextualized to s, and
is only ever evaluated at the time t0 of the utterance.
The features used by this model are:
</bodyText>
<listItem confidence="0.990715956521739">
• Semantic features aim to encode whether r
is a good description of a. IsColorModifying
evaluates to 1 if a’s color appears as an adjec-
tive modifying the head noun of r, e.g. “the
blue button”. IsRelPosModifying evaluates to
1 if a’s relative position to the IF is mentioned
as an adjective in r, e.g. “the left button”.
• Confusion features capture the hypothesis that
the IF may be confused by the description of a
landmark when resolving the RE; e.g. an RE
like “the button next to the red button” might
confuse the IF into pressing a red button, rather
than the one meant by the system. These are
the same features as in the Semantic case, but
looking for modifier keywords in the entire RE,
including the head.
• Salience features account for the fact that an
IF is more likely to resolve r to a if a was visu-
ally salient in s. IsVisible evaluates to 1 if a is
visible to the IF in s. IsInRoom evaluates to 1
if the IF and a are in the same room. IsTarget-
InFront evaluates to 1 if the angular distance
towards a, i.e. the absolute angle between the
</listItem>
<bodyText confidence="0.998029857142857">
camera direction and the straight line from the
IF to a, is less than 4 . VisualSalience approx-
imates the visual salience of Kelleher and van
Genabith (2004), a weighted count of the num-
ber of pixels on which a is rendered (pixels near
the center of the screen have higher weights).
Observational model The observational model
estimates for each object a the probability pobs(a|Q)
that the IF will interact with a, given the IF’s recent
behavior Q(t) _ (Q1, ... , Qn), where Qi is the state
of the world at time t − (i − 1) · 500ms, and n &gt; 1
is the length of the observed behavior. pobs is con-
stantly re-evaluated for times t &gt; t0 as the IF moves
around. pobs uses the following features:
</bodyText>
<listItem confidence="0.99941996969697">
• Linear distance features assume that the clos-
est button is also the one the IF understood. In-
Room returns the number of frames Qi in Q in
which the IF and a are in the same room. But-
tonDistance returns the distance between the IF
and a at Q1 divided by a constant such that the
result never exceeds 1. If a is neither in the
same room nor visible, the feature returns 1.
• Angular distance features analyze the direc-
tion in which the IF looks. TargetInFront re-
turns the angular distance towards a at Q1. An-
gleToTarget returns TargetInFront divided by
7r, or 1 if a is neither in the same room nor
visible. LinearRegAngleTo applies linear re-
gression to a list of observed angular distances
towards a over all frames Qi, and returns the
slope of the regression as a measure of varia-
tion. Negative values indicate that the IF turned
towards a, while positive values mean the op-
posite. If a is neither visible nor in the same
room as the IF at Qi, the angle is set to 7r.
• Combined distance feature: a weighted sum
of linear and angular distance towards a, called
overall distance in Koller et al. (2012).
• Salience features capture visual salience and
its change over time. Defining VSi as the result
of applying the psem feature VisualSalience to
Qi and a, LastVisualSalience returns VSn. Lin-
earRegVisualSalience applies linear regression
to all values VSi and returns the slope as a mea-
sure of change in salience. VisualSalienceSum
returns (Eni=1VSi) * VS1. This emphasizes the
contribution of VS1, which we assume is the
</listItem>
<page confidence="0.934643">
1356
</page>
<figure confidence="0.91021575">
0.0 0.2 0.4 0.6
Accuracy
0.0 0.2 0.4 0.6
Accuracy
</figure>
<bodyText confidence="0.713427">
most reliable predictor of the IF’s intentions.
</bodyText>
<listItem confidence="0.9836666">
• Binary features aim to detect concrete behav-
ior patterns: LastIsVisible applies the psem fea-
ture IsVisible to al, and IsClose evaluates to 1
if the IF is close enough and correctly oriented
to manipulate a in the GIVE environment at al.
</listItem>
<sectionHeader confidence="0.99583" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999747282051282">
Data We evaluated our model using data from the
GIVE-2 (Koller et al., 2010b) and the GIVE-2.5
Challenges (Striegnitz et al., 2011), obtained from
GIVE Organizers (2012). These datasets constitute
interaction corpora, in which the IF’s activities in
the virtual environment were recorded along with
the utterances automatically generated by the par-
ticipating NLG systems. The data consists of 1833
games for GIVE-2 and 687 games for GIVE-2.5.
To extract training data for our model from the
GIVE-2.5 data, we first identified moments in the
recorded data where the IF pressed a button. From
these, we discarded all instances from the tutorial
phase of the GIVE game and those that happened
within 200 ms after the previous utterance, as these
clearly didn’t happen in response to it. This yielded
6478 training instances for pobs, each consisting of
a at 1 second before the action, and the button a
which the IF pressed. We chose n = 4 for rep-
resenting a, except to ensure that the features only
considered IF behavior that happened in response to
an utterance. We achieved this by reducing n for the
first few frames after each utterance, such that the
time of Un was always after the time of the utter-
ance. Finally, we selected those instances which are
episodes in the sense of Section 2, i.e. those in which
the last utterance before the action contained an RE
r. This gave us 3414 training instances for psem,
each consisting of a, r, the time to of the utterance,
and the world state s at time to.
We obtained test instances from the GIVE-2 data
in the same way. This yielded 5028 instances, each
representing an episode. We chose GIVE-2 for test-
ing because the mean episode length is higher (3.3s,
vs. 2.0s in GIVE-2.5), thus making the evaluation
more challenging. Feature selection was done using
the training data and a similar dataset from Koller et
al. (2012). Note that the test data and training data
are based on distinct sets of three virtual environ-
</bodyText>
<figure confidence="0.39609">
−3 −2 −1 0
Time before action (sec)
</figure>
<figureCaption confidence="0.9143825">
Figure 2: Prediction accuracy for (a) all episodes, (b) un-
successful episodes as a function of time.
</figureCaption>
<bodyText confidence="0.99956808">
ments each, and were obtained with different NLG
systems and users. This demonstrates the ability of
our model to generalize to unseen environments.
An example video showing our models’ predic-
tions on some training episodes can be found at
http://tinyurl.com/re-demo-v.
Prediction accuracy We first evaluated the abil-
ity of our model to predict the button to which
the IF resolved each RE. For each test instance
(r, s, a, a), we compare the object returned by
argmaxa p(a|r, s, a(t)) to the one manipulated by
the IF. We call the proportion of correctly classified
instances the prediction accuracy.
Fig. 2a compares our model’s prediction accuracy
to that of several baselines. We plot prediction ac-
curacy as a function of the time at which the model
is queried for a prediction, by evaluating at 3s, 2s,
1s, and 0s before the button press. The graph is
based on the 2094 test instances with an episode
length of at least three seconds, to ensure that re-
sults for different prediction times are comparable.
As expected, prediction accuracy increases as we ap-
proach the time of the action. Furthermore, the com-
bined model outperforms both psem and pobs reli-
ably. This indicates that the component models pro-
</bodyText>
<figure confidence="0.9986939375">
● combined
semantic
observational
KGSC
random visible
●
●
●
●
● ●
● ●
● combined
semantic
observational
KGSC
random visible
</figure>
<page confidence="0.991168">
1357
</page>
<bodyText confidence="0.999986042553192">
vide complementary useful information. Our model
also outperforms two more baselines: KGSC pre-
dicts that the IF will press the button with the min-
imal overall distance, which is the distance metric
used by the “movement-based system” of Koller et
al. (2012); random visible selects a random button
from the ones that are currently visible to the IF.
The fact that this last baseline does not approach 1
at action time suggests that multiple buttons tend to
be visible when the IF presses one, confirming that
the prediction task is not trivial.
Correctly predicting the button that the IF will
press is especially useful, and challenging, in those
cases where the IF pressed a different button than
the one the NLG system intended. Fig. 2b shows
a closer look at the 125 unsuccessful episodes of at
least three seconds in the test data. These tend to
be hard instances, and thus as expected, prediction
accuracy drops for all systems. However, by inte-
grating semantic and observational information, the
combined model compensates better for this than all
other systems, with an accuracy of 37.6% against
31.2% for each individual component.
Feedback appropriateness Second, we evaluated
the ability of our model to predict whether the user
misunderstood the RE and requires feedback. For all
the above models, we assumed a simple feedback
mechanism which predicts that the user misunder-
stood the RE if p(a&apos;) − p(a*) &gt; 0 for some object
a&apos; =� a*, where 0 is a confidence threshold; we used
0 = 0.1 here. We can thus test on recorded data in
which no actual feedback can be given anymore.
We evaluated the models on the 848 test episodes
of at least 3s in which the NLG systems logged the
button they tried to refer to. The results are shown
in Fig. 3 in terms of F1 measure. Here precision is
the proportion of instances in which the IF pressed
the wrong button (i.e., where feedback should have
been given) among the instances where the model
actually suggested feedback. Recall is the propor-
tion of instances in which the model suggested feed-
back among the instances where the IF pressed the
wrong button. Again, the combined model outper-
forms its components and the baselines, primarily
due to increased recall. The difference is particu-
larly pronounced early on, which would be useful in
giving timely feedback in an actual real-time system.
</bodyText>
<figure confidence="0.694191">
−3 −2 −1 0
Time before action (s)
</figure>
<figureCaption confidence="0.997315">
Figure 3: Feedback F1-measure as a function of time.
</figureCaption>
<sectionHeader confidence="0.935846" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.99981606060606">
We presented a statistical model for predicting how
a user will resolve the REs generated by an interac-
tive, situated NLG system. The model continuously
updates an initial estimate based on the meaning of
the RE with a model of the user’s behavior. It out-
performs its components and two baselines on pre-
diction and feedback accuracy.
Our model captures a real-time grounding process
on the part of the interactive system. We thus believe
that it provides a solid foundation for detecting mis-
understandings and generating suitable feedback in
an end-to-end dialogue system. We have presented
our model in terms of a situated dialogue setting,
where clues about what the hearer understood can be
observed directly. However, we believe that the fun-
damental mechanism should apply to other domains
as well. This would amount to finding observable
linguistic and non-linguistic clues of hearer under-
standing that can be used as features of pobs.
The immediate next step for future research is
to extend our model to an implemented end-to-end
situated NLG system for the GIVE Challenge, and
evaluate whether this actually improves task perfor-
mance. This requires, in particular, to compute the
RE that is optimal with respect to ps,,,,. We will fur-
thermore improve pobs by switching to a more tem-
porally dynamic probability model.
Acknowledgments. We thank Konstantina
Garoufi and the anonymous reviewers for their
insightful comments and suggestions. The first two
authors were supported by the SFB 632 “Informa-
tion Structure”; Titov’s work was supported by the
Cluster of Excellence at Saarland University.
</bodyText>
<figure confidence="0.996797636363636">
● combined
semantic
observational
KGSC
random visible
●
●
●
●
0.0 0.2 0.4 0.6
Feedback F1
</figure>
<page confidence="0.972661">
1358
</page>
<sectionHeader confidence="0.985251" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996953361111112">
Hendrik Buschmeier and Stefan Kopp. 2012. Adapting
language production to listener feedback behaviour.
In Proceedings of the Interdisciplinary Workshop on
Feedback Behaviors in Dialog.
Herbert C. Clark. 1996. Using Language. Cambridge
University Press.
Michael C. Frank and Noah D. Goodman. 2012. Predict-
ing pragmatic reasoning in language games. Science,
336(6084):998.
Konstantina Garoufi and Alexander Koller. 2011. Com-
bining symbolic and corpus-based approaches for the
generation of successful referring expressions. In Pro-
ceedings of the 13th European Workshop on Natural
Language Generation (ENLG).
GIVE Organizers. 2012. Give challenge web-
site: Corpora. http://give-challenge.org/
research/page.php?id=corpora.
Dave Golland, Percy Liang, and Dan Klein. 2010. A
game-theoretic approach to generating spatial descrip-
tions. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Graeme Hirst, Susan McRoy, Peter Heeman, Philip Ed-
monds, and Diane Horton. 1994. Repairing conver-
sational misunderstandings and non-understandings.
Speech Communications, 15:213–229.
J. D. Kelleher and J. van Genabith. 2004. Visual salience
and reference resolution in simulated 3-D environ-
ments. Artificial Intelligence Review, 21(3).
Alexander Koller, Kristina Striegnitz, Donna Byron, Jus-
tine Cassell, Robert Dale, Johanna Moore, and Jon
Oberlander. 2010a. The First Challenge on Generat-
ing Instructions in Virtual Environments. In E. Krah-
mer and M. Theune, editors, Empirical Methods in
Natural Language Generation, number 5790 in LNCS,
pages 337–361. Springer.
Alexander Koller, Kristina Striegnitz, Andrew Gargett,
Donna Byron, Justine Cassell, Robert Dale, Johanna
Moore, and Jon Oberlander. 2010b. Report on the
Second NLG Challenge on Generating Instructions in
Virtual Environments (GIVE-2). In Proceedings of the
6th International Natural Language Generation Con-
ference (INLG).
Alexander Koller, Konstantina Garoufi, Maria Staudte,
and Matthew Crocker. 2012. Enhancing referential
success by tracking hearer gaze. In Proceedings of the
13th Annual SIGdial Meeting on Discourse and Dia-
logue (SIGDIAL), Seoul.
Yukiko Nakano, Kazuyoshi Murata, Mika Enomoto,
Yoshiko Arimoto, Yasuhiro Asa, and Hirohiko
Sagawa. 2007. Predicting evidence of understanding
by monitoring user’s task manipulation in multimodal
conversations. In Proceedings of the ACL 2007 Demo
and Poster Sessions.
Tim Paek and Eric Horvitz. 1999. Uncertainty, utility,
and misunderstanding: A decision-theoretic perspec-
tive on grounding in conversational systems. In AAAI
Fall Symposium on Psychological Models of Commu-
nication in Collaborative Systems.
Kristina Striegnitz, Alexandre Denis, Andrew Gargett,
Konstantina Garoufi, Alexander Koller, and Mariet
Theune. 2011. Report on the Second Second Chal-
lenge on Generating Instructions in Virtual Environ-
ments (GIVE-2.5). In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation
(ENLG).
David Traum. 1994. A computational theory of ground-
ing in natural language conversation. Ph.D. thesis,
University of Rochester.
Adam Vogel, Christopher Potts, and Dan Jurafsky.
2013. Implicatures and nested beliefs in approximate
Decentralized-POMDPs. In Proceedings of ACL.
</reference>
<page confidence="0.995964">
1359
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.867257">
<title confidence="0.999696">Predicting the resolution of referring expressions from user behavior</title>
<author confidence="0.996639">Alexander</author>
<affiliation confidence="0.980289">of Potsdam, Germany of Amsterdam,</affiliation>
<email confidence="0.972098">titov@uva.nl,koller@ling.uni-potsdam.de</email>
<abstract confidence="0.9910543">We present a statistical model for predicting how the user of an interactive, situated NLP system resolved a referring expression. The model makes an initial prediction based on the meaning of the utterance, and revises it continuously based on the user’s behavior. The combined model outperforms its components in predicting reference resolution and when to give feedback.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hendrik Buschmeier</author>
<author>Stefan Kopp</author>
</authors>
<title>Adapting language production to listener feedback behaviour.</title>
<date>2012</date>
<booktitle>In Proceedings of the Interdisciplinary Workshop on Feedback Behaviors in Dialog.</booktitle>
<contexts>
<context position="2215" citStr="Buschmeier and Kopp (2012)" startWordPosition="341" endWordPosition="344">back on track. We focus here on the problem of predicting how the user has resolved a referring expression (RE) that was generated by the system, i.e. a noun phrase that is intended to identify some object uniquely to the listener. A number of authors have recently offered statistical models for parts of this problem. Golland et al. (2010) and Garoufi and Koller (2011) have presented log-linear models for predicting how the listener will resolve a given RE in a given scene; however, these models do not update the probability model based on observing the user’s reactions. Nakano et al. (2007), Buschmeier and Kopp (2012), and Koller et al. (2012) all predict what the listener understood based on their behavior, but do not consider the RE itself in the model. The models of Frank and Goodman (2012) and Vogel et al. (2013) aim at explaining the effect of implicatures on the listener’s RE resolution process in terms of hypothesized interactions, but do not actually support a realtime interaction between a system and a user. In this paper, we show how to predict how the listener has resolved an RE by combining a statistical model of RE resolution based on the RE itself with a statistical model of RE resolution bas</context>
</contexts>
<marker>Buschmeier, Kopp, 2012</marker>
<rawString>Hendrik Buschmeier and Stefan Kopp. 2012. Adapting language production to listener feedback behaviour. In Proceedings of the Interdisciplinary Workshop on Feedback Behaviors in Dialog.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert C Clark</author>
</authors>
<title>Using Language.</title>
<date>1996</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1153" citStr="Clark, 1996" startWordPosition="163" endWordPosition="164">uously based on the user’s behavior. The combined model outperforms its components in predicting reference resolution and when to give feedback. 1 Introduction Speakers and listeners in natural communication are engaged in a highly interactive process. In order to achieve some communicative goal, the speaker will perform an utterance which they believe has a high chance of achieving that goal. They will then monitor the listener’s behavior to see whether this goal is actually being achieved. This process is a core part of what is commonly called grounding in the dialogue literature (see e.g. (Clark, 1996; Traum, 1994; Paek and Horvitz, 1999; Hirst et al., 1994)). Interactive computer systems that are to carry out an effective and efficient conversation with a user must model this grounding process, and should ideally respond to the user’s observed behavior in real time. For instance, if the user of a pedestrian navigation system takes a wrong turn, the system should interpret this as evidence of misunderstanding and bring the user back on track. We focus here on the problem of predicting how the user has resolved a referring expression (RE) that was generated by the system, i.e. a noun phrase</context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>Herbert C. Clark. 1996. Using Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C Frank</author>
<author>Noah D Goodman</author>
</authors>
<title>Predicting pragmatic reasoning in language games.</title>
<date>2012</date>
<journal>Science,</journal>
<volume>336</volume>
<issue>6084</issue>
<contexts>
<context position="2394" citStr="Frank and Goodman (2012)" startWordPosition="374" endWordPosition="377"> to identify some object uniquely to the listener. A number of authors have recently offered statistical models for parts of this problem. Golland et al. (2010) and Garoufi and Koller (2011) have presented log-linear models for predicting how the listener will resolve a given RE in a given scene; however, these models do not update the probability model based on observing the user’s reactions. Nakano et al. (2007), Buschmeier and Kopp (2012), and Koller et al. (2012) all predict what the listener understood based on their behavior, but do not consider the RE itself in the model. The models of Frank and Goodman (2012) and Vogel et al. (2013) aim at explaining the effect of implicatures on the listener’s RE resolution process in terms of hypothesized interactions, but do not actually support a realtime interaction between a system and a user. In this paper, we show how to predict how the listener has resolved an RE by combining a statistical model of RE resolution based on the RE itself with a statistical model of RE resolution based on the listener’s behavior. To our knowledge, this is the first approach to combine two such models explicitly. We consider the RE grounding problem in the context of interacti</context>
</contexts>
<marker>Frank, Goodman, 2012</marker>
<rawString>Michael C. Frank and Noah D. Goodman. 2012. Predicting pragmatic reasoning in language games. Science, 336(6084):998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Konstantina Garoufi</author>
<author>Alexander Koller</author>
</authors>
<title>Combining symbolic and corpus-based approaches for the generation of successful referring expressions.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG).</booktitle>
<contexts>
<context position="1960" citStr="Garoufi and Koller (2011)" startWordPosition="299" endWordPosition="302">ounding process, and should ideally respond to the user’s observed behavior in real time. For instance, if the user of a pedestrian navigation system takes a wrong turn, the system should interpret this as evidence of misunderstanding and bring the user back on track. We focus here on the problem of predicting how the user has resolved a referring expression (RE) that was generated by the system, i.e. a noun phrase that is intended to identify some object uniquely to the listener. A number of authors have recently offered statistical models for parts of this problem. Golland et al. (2010) and Garoufi and Koller (2011) have presented log-linear models for predicting how the listener will resolve a given RE in a given scene; however, these models do not update the probability model based on observing the user’s reactions. Nakano et al. (2007), Buschmeier and Kopp (2012), and Koller et al. (2012) all predict what the listener understood based on their behavior, but do not consider the RE itself in the model. The models of Frank and Goodman (2012) and Vogel et al. (2013) aim at explaining the effect of implicatures on the listener’s RE resolution process in terms of hypothesized interactions, but do not actual</context>
</contexts>
<marker>Garoufi, Koller, 2011</marker>
<rawString>Konstantina Garoufi and Alexander Koller. 2011. Combining symbolic and corpus-based approaches for the generation of successful referring expressions. In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>GIVE Organizers</author>
</authors>
<title>Give challenge website: Corpora.</title>
<date>2012</date>
<note>http://give-challenge.org/ research/page.php?id=corpora.</note>
<contexts>
<context position="12982" citStr="Organizers (2012)" startWordPosition="2235" endWordPosition="2236">um returns (Eni=1VSi) * VS1. This emphasizes the contribution of VS1, which we assume is the 1356 0.0 0.2 0.4 0.6 Accuracy 0.0 0.2 0.4 0.6 Accuracy most reliable predictor of the IF’s intentions. • Binary features aim to detect concrete behavior patterns: LastIsVisible applies the psem feature IsVisible to al, and IsClose evaluates to 1 if the IF is close enough and correctly oriented to manipulate a in the GIVE environment at al. 4 Evaluation Data We evaluated our model using data from the GIVE-2 (Koller et al., 2010b) and the GIVE-2.5 Challenges (Striegnitz et al., 2011), obtained from GIVE Organizers (2012). These datasets constitute interaction corpora, in which the IF’s activities in the virtual environment were recorded along with the utterances automatically generated by the participating NLG systems. The data consists of 1833 games for GIVE-2 and 687 games for GIVE-2.5. To extract training data for our model from the GIVE-2.5 data, we first identified moments in the recorded data where the IF pressed a button. From these, we discarded all instances from the tutorial phase of the GIVE game and those that happened within 200 ms after the previous utterance, as these clearly didn’t happen in r</context>
</contexts>
<marker>Organizers, 2012</marker>
<rawString>GIVE Organizers. 2012. Give challenge website: Corpora. http://give-challenge.org/ research/page.php?id=corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dave Golland</author>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>A game-theoretic approach to generating spatial descriptions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1930" citStr="Golland et al. (2010)" startWordPosition="294" endWordPosition="297"> a user must model this grounding process, and should ideally respond to the user’s observed behavior in real time. For instance, if the user of a pedestrian navigation system takes a wrong turn, the system should interpret this as evidence of misunderstanding and bring the user back on track. We focus here on the problem of predicting how the user has resolved a referring expression (RE) that was generated by the system, i.e. a noun phrase that is intended to identify some object uniquely to the listener. A number of authors have recently offered statistical models for parts of this problem. Golland et al. (2010) and Garoufi and Koller (2011) have presented log-linear models for predicting how the listener will resolve a given RE in a given scene; however, these models do not update the probability model based on observing the user’s reactions. Nakano et al. (2007), Buschmeier and Kopp (2012), and Koller et al. (2012) all predict what the listener understood based on their behavior, but do not consider the RE itself in the model. The models of Frank and Goodman (2012) and Vogel et al. (2013) aim at explaining the effect of implicatures on the listener’s RE resolution process in terms of hypothesized i</context>
</contexts>
<marker>Golland, Liang, Klein, 2010</marker>
<rawString>Dave Golland, Percy Liang, and Dan Klein. 2010. A game-theoretic approach to generating spatial descriptions. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>Susan McRoy</author>
<author>Peter Heeman</author>
<author>Philip Edmonds</author>
<author>Diane Horton</author>
</authors>
<title>Repairing conversational misunderstandings and non-understandings.</title>
<date>1994</date>
<journal>Speech Communications,</journal>
<pages>15--213</pages>
<contexts>
<context position="1211" citStr="Hirst et al., 1994" startWordPosition="171" endWordPosition="174">model outperforms its components in predicting reference resolution and when to give feedback. 1 Introduction Speakers and listeners in natural communication are engaged in a highly interactive process. In order to achieve some communicative goal, the speaker will perform an utterance which they believe has a high chance of achieving that goal. They will then monitor the listener’s behavior to see whether this goal is actually being achieved. This process is a core part of what is commonly called grounding in the dialogue literature (see e.g. (Clark, 1996; Traum, 1994; Paek and Horvitz, 1999; Hirst et al., 1994)). Interactive computer systems that are to carry out an effective and efficient conversation with a user must model this grounding process, and should ideally respond to the user’s observed behavior in real time. For instance, if the user of a pedestrian navigation system takes a wrong turn, the system should interpret this as evidence of misunderstanding and bring the user back on track. We focus here on the problem of predicting how the user has resolved a referring expression (RE) that was generated by the system, i.e. a noun phrase that is intended to identify some object uniquely to the </context>
</contexts>
<marker>Hirst, McRoy, Heeman, Edmonds, Horton, 1994</marker>
<rawString>Graeme Hirst, Susan McRoy, Peter Heeman, Philip Edmonds, and Diane Horton. 1994. Repairing conversational misunderstandings and non-understandings. Speech Communications, 15:213–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Kelleher</author>
<author>J van Genabith</author>
</authors>
<title>Visual salience and reference resolution in simulated 3-D environments.</title>
<date>2004</date>
<journal>Artificial Intelligence Review,</journal>
<volume>21</volume>
<issue>3</issue>
<marker>Kelleher, van Genabith, 2004</marker>
<rawString>J. D. Kelleher and J. van Genabith. 2004. Visual salience and reference resolution in simulated 3-D environments. Artificial Intelligence Review, 21(3).</rawString>
</citation>
<citation valid="false">
<booktitle>Oberlander. 2010a. The First Challenge on Generating Instructions in Virtual Environments. In</booktitle>
<pages>337--361</pages>
<editor>Alexander Koller, Kristina Striegnitz, Donna Byron, Justine Cassell, Robert Dale, Johanna Moore, and Jon</editor>
<publisher>Springer.</publisher>
<marker></marker>
<rawString>Alexander Koller, Kristina Striegnitz, Donna Byron, Justine Cassell, Robert Dale, Johanna Moore, and Jon Oberlander. 2010a. The First Challenge on Generating Instructions in Virtual Environments. In E. Krahmer and M. Theune, editors, Empirical Methods in Natural Language Generation, number 5790 in LNCS, pages 337–361. Springer.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alexander Koller</author>
<author>Kristina Striegnitz</author>
<author>Andrew Gargett</author>
<author>Donna Byron</author>
<author>Justine Cassell</author>
<author>Robert Dale</author>
<author>Johanna Moore</author>
<author>Jon Oberlander</author>
</authors>
<booktitle>2010b. Report on the Second NLG Challenge on Generating Instructions in Virtual Environments (GIVE-2). In Proceedings of the 6th International Natural Language Generation Conference (INLG).</booktitle>
<marker>Koller, Striegnitz, Gargett, Byron, Cassell, Dale, Moore, Oberlander, </marker>
<rawString>Alexander Koller, Kristina Striegnitz, Andrew Gargett, Donna Byron, Justine Cassell, Robert Dale, Johanna Moore, and Jon Oberlander. 2010b. Report on the Second NLG Challenge on Generating Instructions in Virtual Environments (GIVE-2). In Proceedings of the 6th International Natural Language Generation Conference (INLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Konstantina Garoufi</author>
<author>Maria Staudte</author>
<author>Matthew Crocker</author>
</authors>
<title>Enhancing referential success by tracking hearer gaze.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL), Seoul.</booktitle>
<contexts>
<context position="2241" citStr="Koller et al. (2012)" startWordPosition="346" endWordPosition="349">the problem of predicting how the user has resolved a referring expression (RE) that was generated by the system, i.e. a noun phrase that is intended to identify some object uniquely to the listener. A number of authors have recently offered statistical models for parts of this problem. Golland et al. (2010) and Garoufi and Koller (2011) have presented log-linear models for predicting how the listener will resolve a given RE in a given scene; however, these models do not update the probability model based on observing the user’s reactions. Nakano et al. (2007), Buschmeier and Kopp (2012), and Koller et al. (2012) all predict what the listener understood based on their behavior, but do not consider the RE itself in the model. The models of Frank and Goodman (2012) and Vogel et al. (2013) aim at explaining the effect of implicatures on the listener’s RE resolution process in terms of hypothesized interactions, but do not actually support a realtime interaction between a system and a user. In this paper, we show how to predict how the listener has resolved an RE by combining a statistical model of RE resolution based on the RE itself with a statistical model of RE resolution based on the listener’s behav</context>
<context position="12036" citStr="Koller et al. (2012)" startWordPosition="2078" endWordPosition="2081">tance towards a at Q1. AngleToTarget returns TargetInFront divided by 7r, or 1 if a is neither in the same room nor visible. LinearRegAngleTo applies linear regression to a list of observed angular distances towards a over all frames Qi, and returns the slope of the regression as a measure of variation. Negative values indicate that the IF turned towards a, while positive values mean the opposite. If a is neither visible nor in the same room as the IF at Qi, the angle is set to 7r. • Combined distance feature: a weighted sum of linear and angular distance towards a, called overall distance in Koller et al. (2012). • Salience features capture visual salience and its change over time. Defining VSi as the result of applying the psem feature VisualSalience to Qi and a, LastVisualSalience returns VSn. LinearRegVisualSalience applies linear regression to all values VSi and returns the slope as a measure of change in salience. VisualSalienceSum returns (Eni=1VSi) * VS1. This emphasizes the contribution of VS1, which we assume is the 1356 0.0 0.2 0.4 0.6 Accuracy 0.0 0.2 0.4 0.6 Accuracy most reliable predictor of the IF’s intentions. • Binary features aim to detect concrete behavior patterns: LastIsVisible a</context>
<context position="14685" citStr="Koller et al. (2012)" startWordPosition="2529" endWordPosition="2532"> are episodes in the sense of Section 2, i.e. those in which the last utterance before the action contained an RE r. This gave us 3414 training instances for psem, each consisting of a, r, the time to of the utterance, and the world state s at time to. We obtained test instances from the GIVE-2 data in the same way. This yielded 5028 instances, each representing an episode. We chose GIVE-2 for testing because the mean episode length is higher (3.3s, vs. 2.0s in GIVE-2.5), thus making the evaluation more challenging. Feature selection was done using the training data and a similar dataset from Koller et al. (2012). Note that the test data and training data are based on distinct sets of three virtual environ−3 −2 −1 0 Time before action (sec) Figure 2: Prediction accuracy for (a) all episodes, (b) unsuccessful episodes as a function of time. ments each, and were obtained with different NLG systems and users. This demonstrates the ability of our model to generalize to unseen environments. An example video showing our models’ predictions on some training episodes can be found at http://tinyurl.com/re-demo-v. Prediction accuracy We first evaluated the ability of our model to predict the button to which the</context>
<context position="16511" citStr="Koller et al. (2012)" startWordPosition="2840" endWordPosition="2843">t prediction times are comparable. As expected, prediction accuracy increases as we approach the time of the action. Furthermore, the combined model outperforms both psem and pobs reliably. This indicates that the component models pro● combined semantic observational KGSC random visible ● ● ● ● ● ● ● ● ● combined semantic observational KGSC random visible 1357 vide complementary useful information. Our model also outperforms two more baselines: KGSC predicts that the IF will press the button with the minimal overall distance, which is the distance metric used by the “movement-based system” of Koller et al. (2012); random visible selects a random button from the ones that are currently visible to the IF. The fact that this last baseline does not approach 1 at action time suggests that multiple buttons tend to be visible when the IF presses one, confirming that the prediction task is not trivial. Correctly predicting the button that the IF will press is especially useful, and challenging, in those cases where the IF pressed a different button than the one the NLG system intended. Fig. 2b shows a closer look at the 125 unsuccessful episodes of at least three seconds in the test data. These tend to be har</context>
</contexts>
<marker>Koller, Garoufi, Staudte, Crocker, 2012</marker>
<rawString>Alexander Koller, Konstantina Garoufi, Maria Staudte, and Matthew Crocker. 2012. Enhancing referential success by tracking hearer gaze. In Proceedings of the 13th Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL), Seoul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yukiko Nakano</author>
<author>Kazuyoshi Murata</author>
<author>Mika Enomoto</author>
<author>Yoshiko Arimoto</author>
<author>Yasuhiro Asa</author>
<author>Hirohiko Sagawa</author>
</authors>
<title>Predicting evidence of understanding by monitoring user’s task manipulation in multimodal conversations.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL</booktitle>
<contexts>
<context position="2187" citStr="Nakano et al. (2007)" startWordPosition="337" endWordPosition="340">ng and bring the user back on track. We focus here on the problem of predicting how the user has resolved a referring expression (RE) that was generated by the system, i.e. a noun phrase that is intended to identify some object uniquely to the listener. A number of authors have recently offered statistical models for parts of this problem. Golland et al. (2010) and Garoufi and Koller (2011) have presented log-linear models for predicting how the listener will resolve a given RE in a given scene; however, these models do not update the probability model based on observing the user’s reactions. Nakano et al. (2007), Buschmeier and Kopp (2012), and Koller et al. (2012) all predict what the listener understood based on their behavior, but do not consider the RE itself in the model. The models of Frank and Goodman (2012) and Vogel et al. (2013) aim at explaining the effect of implicatures on the listener’s RE resolution process in terms of hypothesized interactions, but do not actually support a realtime interaction between a system and a user. In this paper, we show how to predict how the listener has resolved an RE by combining a statistical model of RE resolution based on the RE itself with a statistica</context>
</contexts>
<marker>Nakano, Murata, Enomoto, Arimoto, Asa, Sagawa, 2007</marker>
<rawString>Yukiko Nakano, Kazuyoshi Murata, Mika Enomoto, Yoshiko Arimoto, Yasuhiro Asa, and Hirohiko Sagawa. 2007. Predicting evidence of understanding by monitoring user’s task manipulation in multimodal conversations. In Proceedings of the ACL 2007 Demo and Poster Sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Paek</author>
<author>Eric Horvitz</author>
</authors>
<title>Uncertainty, utility, and misunderstanding: A decision-theoretic perspective on grounding in conversational systems.</title>
<date>1999</date>
<booktitle>In AAAI Fall Symposium on Psychological Models of Communication in Collaborative Systems.</booktitle>
<contexts>
<context position="1190" citStr="Paek and Horvitz, 1999" startWordPosition="167" endWordPosition="170"> behavior. The combined model outperforms its components in predicting reference resolution and when to give feedback. 1 Introduction Speakers and listeners in natural communication are engaged in a highly interactive process. In order to achieve some communicative goal, the speaker will perform an utterance which they believe has a high chance of achieving that goal. They will then monitor the listener’s behavior to see whether this goal is actually being achieved. This process is a core part of what is commonly called grounding in the dialogue literature (see e.g. (Clark, 1996; Traum, 1994; Paek and Horvitz, 1999; Hirst et al., 1994)). Interactive computer systems that are to carry out an effective and efficient conversation with a user must model this grounding process, and should ideally respond to the user’s observed behavior in real time. For instance, if the user of a pedestrian navigation system takes a wrong turn, the system should interpret this as evidence of misunderstanding and bring the user back on track. We focus here on the problem of predicting how the user has resolved a referring expression (RE) that was generated by the system, i.e. a noun phrase that is intended to identify some ob</context>
</contexts>
<marker>Paek, Horvitz, 1999</marker>
<rawString>Tim Paek and Eric Horvitz. 1999. Uncertainty, utility, and misunderstanding: A decision-theoretic perspective on grounding in conversational systems. In AAAI Fall Symposium on Psychological Models of Communication in Collaborative Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Striegnitz</author>
<author>Alexandre Denis</author>
<author>Andrew Gargett</author>
<author>Konstantina Garoufi</author>
<author>Alexander Koller</author>
<author>Mariet Theune</author>
</authors>
<date>2011</date>
<booktitle>Report on the Second Second Challenge on Generating Instructions in Virtual Environments (GIVE-2.5). In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG).</booktitle>
<contexts>
<context position="12944" citStr="Striegnitz et al., 2011" startWordPosition="2228" endWordPosition="2231">easure of change in salience. VisualSalienceSum returns (Eni=1VSi) * VS1. This emphasizes the contribution of VS1, which we assume is the 1356 0.0 0.2 0.4 0.6 Accuracy 0.0 0.2 0.4 0.6 Accuracy most reliable predictor of the IF’s intentions. • Binary features aim to detect concrete behavior patterns: LastIsVisible applies the psem feature IsVisible to al, and IsClose evaluates to 1 if the IF is close enough and correctly oriented to manipulate a in the GIVE environment at al. 4 Evaluation Data We evaluated our model using data from the GIVE-2 (Koller et al., 2010b) and the GIVE-2.5 Challenges (Striegnitz et al., 2011), obtained from GIVE Organizers (2012). These datasets constitute interaction corpora, in which the IF’s activities in the virtual environment were recorded along with the utterances automatically generated by the participating NLG systems. The data consists of 1833 games for GIVE-2 and 687 games for GIVE-2.5. To extract training data for our model from the GIVE-2.5 data, we first identified moments in the recorded data where the IF pressed a button. From these, we discarded all instances from the tutorial phase of the GIVE game and those that happened within 200 ms after the previous utteranc</context>
</contexts>
<marker>Striegnitz, Denis, Gargett, Garoufi, Koller, Theune, 2011</marker>
<rawString>Kristina Striegnitz, Alexandre Denis, Andrew Gargett, Konstantina Garoufi, Alexander Koller, and Mariet Theune. 2011. Report on the Second Second Challenge on Generating Instructions in Virtual Environments (GIVE-2.5). In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Traum</author>
</authors>
<title>A computational theory of grounding in natural language conversation.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Rochester.</institution>
<contexts>
<context position="1166" citStr="Traum, 1994" startWordPosition="165" endWordPosition="166">on the user’s behavior. The combined model outperforms its components in predicting reference resolution and when to give feedback. 1 Introduction Speakers and listeners in natural communication are engaged in a highly interactive process. In order to achieve some communicative goal, the speaker will perform an utterance which they believe has a high chance of achieving that goal. They will then monitor the listener’s behavior to see whether this goal is actually being achieved. This process is a core part of what is commonly called grounding in the dialogue literature (see e.g. (Clark, 1996; Traum, 1994; Paek and Horvitz, 1999; Hirst et al., 1994)). Interactive computer systems that are to carry out an effective and efficient conversation with a user must model this grounding process, and should ideally respond to the user’s observed behavior in real time. For instance, if the user of a pedestrian navigation system takes a wrong turn, the system should interpret this as evidence of misunderstanding and bring the user back on track. We focus here on the problem of predicting how the user has resolved a referring expression (RE) that was generated by the system, i.e. a noun phrase that is inte</context>
</contexts>
<marker>Traum, 1994</marker>
<rawString>David Traum. 1994. A computational theory of grounding in natural language conversation. Ph.D. thesis, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Vogel</author>
<author>Christopher Potts</author>
<author>Dan Jurafsky</author>
</authors>
<title>Implicatures and nested beliefs in approximate Decentralized-POMDPs.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2418" citStr="Vogel et al. (2013)" startWordPosition="379" endWordPosition="382">uely to the listener. A number of authors have recently offered statistical models for parts of this problem. Golland et al. (2010) and Garoufi and Koller (2011) have presented log-linear models for predicting how the listener will resolve a given RE in a given scene; however, these models do not update the probability model based on observing the user’s reactions. Nakano et al. (2007), Buschmeier and Kopp (2012), and Koller et al. (2012) all predict what the listener understood based on their behavior, but do not consider the RE itself in the model. The models of Frank and Goodman (2012) and Vogel et al. (2013) aim at explaining the effect of implicatures on the listener’s RE resolution process in terms of hypothesized interactions, but do not actually support a realtime interaction between a system and a user. In this paper, we show how to predict how the listener has resolved an RE by combining a statistical model of RE resolution based on the RE itself with a statistical model of RE resolution based on the listener’s behavior. To our knowledge, this is the first approach to combine two such models explicitly. We consider the RE grounding problem in the context of interactive, situated natural lan</context>
</contexts>
<marker>Vogel, Potts, Jurafsky, 2013</marker>
<rawString>Adam Vogel, Christopher Potts, and Dan Jurafsky. 2013. Implicatures and nested beliefs in approximate Decentralized-POMDPs. In Proceedings of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.8073285">
Harvesting Parallel News Streams to Generate Paraphrases of Event
Relations
</title>
<author confidence="0.832636">
Congle Zhang, Daniel S. Weld
</author>
<affiliation confidence="0.971421">
Computer Science &amp; Engineering
University of Washington
</affiliation>
<address confidence="0.75652">
Seattle, WA 98195, USA
</address>
<email confidence="0.998812">
{clzhang,weld}@cs.washington.edu
</email>
<sectionHeader confidence="0.984898" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998785772727273">
The distributional hypothesis, which states
that words that occur in similar contexts tend
to have similar meanings, has inspired sev-
eral Web mining algorithms for paraphras-
ing semantically equivalent phrases. Unfortu-
nately, these methods have several drawbacks,
such as confusing synonyms with antonyms
and causes with effects. This paper intro-
duces three Temporal Correspondence Heuris-
tics, that characterize regularities in parallel
news streams, and shows how they may be
used to generate high precision paraphrases
for event relations. We encode the heuristics
in a probabilistic graphical model to create
the NEWSSPIKE algorithm for mining news
streams. We present experiments demon-
strating that NEWSSPIKE significantly outper-
forms several competitive baselines. In order
to spur further research, we provide a large
annotated corpus of timestamped news arti-
cles as well as the paraphrases produced by
NEWSSPIKE.
</bodyText>
<sectionHeader confidence="0.992349" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975543478261">
Paraphrasing, the task of finding sets of semantically
equivalent surface forms, is crucial to many natu-
ral language processing applications, including re-
lation extraction (Bhagat and Ravichandran, 2008),
question answering (Fader et al., 2013), summa-
rization (Barzilay et al., 1999) and machine transla-
tion (Callison-Burch et al., 2006). While the benefits
of paraphrasing have been demonstrated, creating a
large-scale corpus of high precision paraphrases re-
mains a challenge — especially for event relations.
Many researchers have considered generating
paraphrases by mining the Web guided by the dis-
tributional hypothesis, which states that words oc-
curring in similar contexts tend to have similar
meanings (Harris, 1954). For example, DIRT (Lin
and Pantel, 2001) and Resolver (Yates and Etzioni,
2009) identify synonymous relation phrases by the
distributions of their arguments. However, the dis-
tributional hypothesis has several drawbacks. First,
it can confuse antonyms with synonyms because
antonymous phrases appear in similar contexts as of-
ten as synonymous phrases. For the same reasons, it
also often confuses causes with effects. For exam-
ple, DIRT reports that the closest phrase to fall is
rise, and the closest phrase to shoot is kill.1 Sec-
ond, the distributional hypothesis relies on statis-
tics over large corpora to produce accurate similarity
statistics. It remains unclear how to accurately para-
phrase less frequent relations with the distributional
hypothesis.
Another common approach employs the use of
parallel corpora. News articles are an interesting
target, because there often exist articles from dif-
ferent sources describing the same daily events.
This peculiar property allows the use of the tem-
poral assumption, which assumes that phrases in
articles published at the same time tend to have
similar meanings. For example, the approaches by
Dolan et al. (2004) and Barzilay et al. (2003) iden-
tify pairs of sentential paraphrases in similar arti-
cles that have appeared in the same period of time.
While these approaches use temporal information
as a coarse filter in the data generation stage, they
still largely rely on text metrics in the prediction
stage. This not only reduces precision, but also lim-
its the discovery of paraphrases with dissimilar sur-
</bodyText>
<footnote confidence="0.6399235">
1http://demo.patrickpantel.com/demos/
lexsem/paraphrase.htm
</footnote>
<note confidence="0.770487333333333">
1776
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1776–1786,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9966325">
face strings.
The goal of our research is to develop a technique
to generate paraphrases for large numbers of event
relation with high precision, using only minimal hu-
man effort. The key to our approach is a joint cluster
model using the temporal attributes of news streams,
which allows us to identify semantic equivalence
of event relation phrases with greater precision. In
summary, this paper makes the following contribu-
tions:
</bodyText>
<listItem confidence="0.878886352941176">
• We formulate a set of three temporal corre-
spondence heuristics that characterize regulari-
ties over parallel news streams.
• We develop a novel program, NEWSSPIKE,
based on a probabilistic graphical model that
jointly encodes these heuristics. We present in-
ference and learning algorithms for our model.
• We present a series of detailed experiments
demonstrating that NEWSSPIKE outperforms
several competitive baselines, and show through
ablation tests how each of the temporal heuris-
tics affects performance.
• To spur further research on this topic, we pro-
vide both our generated paraphrase clusters and
a corpus of 0.5M time-stamped news articles2,
collected over a period of about 50 days from
hundreds of news sources.
</listItem>
<sectionHeader confidence="0.859014" genericHeader="method">
2 System Overview
</sectionHeader>
<bodyText confidence="0.9914851875">
The main goal of this work is to generate high preci-
sion paraphrases for relation phrases. News streams
are a promising resource, since articles from dif-
ferent sources tend to use semantically equivalent
phrases to describe the same daily events. For ex-
ample, when a recent scandal hit, headlines read:
“Armstrong steps down from Livestrong”; “Arm-
strong resigns from Livestrong” and “Armstrong
cuts ties with Livestrong”. From these we can con-
clude that the following relation phrases are seman-
tically similar: {step down from, resign from, cut ties
with}.
To realize this intuition, our first challenge is
to represent an event. In practice, a question like
“What happened to Armstrong and Livestrong on
Oct 17?” could often lead to a unique answer. It im-
</bodyText>
<note confidence="0.5116745">
2https://www.cs.washington.edu/node/
9473/
</note>
<figureCaption confidence="0.981722">
Figure 1: NEWSSPIKE first applies open informa-
</figureCaption>
<bodyText confidence="0.984619351351351">
tion extraction to articles in the news streams, obtain-
ing shallow extractions with time-stamps. Next, an
extracted event candidate (EEC) is obtained after group-
ing daily extractions by argument pairs. Temporal fea-
tures and constraints are developed based on our tempo-
ral correspondence heuristics and encoded into a joint in-
ference model. The model finally creates the paraphrase
clusters by predicting the relation phrases that describe
the EEC.
plies that using an argument pair and a time-stamp
could be an effective way to identify an event (e.g.
(Armstrong, Livestrong, Oct 17) for the previous
question). Based on this observation, this paper in-
troduces a novel mechanism to paraphrase relations
as summarized in Figure 1.
NEWSSPIKE first applies the ReVerb open infor-
mation extraction (IE) system (Fader et al., 2011)
on the news streams to obtain a set of (a1, r, a2, t)
tuples, where the az are the arguments, r is a re-
lation phrase, and t is the time-stamp of the cor-
responding news article. When (a1, a2, t) suggests
a real word event, the relation r of (a1, r, a2, t) is
likely to describe that event (e.g. (Armstrong, resign
from, Livestrong, Oct 17). We call every (a1, a2, t)
an extracted event candidate (EEC), and every rela-
tion describing the event an event-mention.
For each EEC (a1, a2, t), suppose there are m ex-
traction tuples (a1, r1, a2, t) ... (a1, rm, a2, t) shar-
ing the values of a1, a2, and t. We refer to this
set of extraction tuples as the EEC-set, and denote
it (a1, a2, t, {r1 ... rm}). All the event-mentions in
the EEC-set may be semantically equivalent and are
hence candidates for a good paraphrase cluster.
Thus, the paraphrasing problem becomes a pre-
diction problem: for each relation rz in the EEC-set,
does it or does it not describe the hypothesized
event? We solve this problem in two steps. The
</bodyText>
<figure confidence="0.957392884615385">
Given news streams
Joint inference
model
Temporal features
&amp; constraints
OpenIE
Temporal Heuristics
Shallow timestamped
extractions
(a1,r,a2,t)
Relation phrases
Describing the EEC
(a1,a2,t)
(a1a2t)
12 r3
r1 3 4
r1 r2 r3
r r r
Group
Create
clusters
Extracted Event
candidates (EEC) &amp;
relation phrases
(a1,a2,t)
(a1a2,t)
</figure>
<equation confidence="0.9649547">
r1
r1 r2 r3r3
r r2 r
r4
r4 r5
r1 r2
fry, r3, r4}
Paraphrase
clusters
1777
</equation>
<bodyText confidence="0.9998205">
next section proposes a set of temporal correspon-
dence heuristics that partially characterize semanti-
cally equivalent EEC-sets. Then, in Section 4, we
present a joint inference model designed to use these
heuristics to solve the prediction problem and to
generate paraphrase clusters.
</bodyText>
<sectionHeader confidence="0.982112" genericHeader="method">
3 Temporal Correspondence Heuristics
</sectionHeader>
<bodyText confidence="0.998534517241379">
In this section, we propose a set of temporal heuris-
tics that are useful to generate paraphrases at high
precision. Our heuristics start from the basic obser-
vation mentioned previously — events can often be
uniquely determined by their arguments and time.
Additionally, we find that it is not just the publica-
tion time of the news story that matters, the verb
tenses of the sentences are also important. For ex-
ample, the two sentences “Armstrong was the chair-
man of Livestrong” and “Armstrong steps down
from Livestrong” have past and present tense re-
spectively, which suggests that the relation phrases
are less likely to describe the same event and are
thus not semantically equivalent. To capture these
intuitions, we propose the Temporal Functionality
Heuristic:
Temporal Functionality Heuristic. News articles
published at the same time that mention the same
entities and use the same tense tend to describe the
same events.
Unfortunately, we find that not all the event can-
didates, (a1, a2, t), are equally good for paraphras-
ing. For example, today’s news might include
both “Barack Obama heads to the White House”
and “Barack Obama greets reporters at the White
House”. Although the two sentences are highly
similar, sharing a1 = “Barack Obama” and a2 =
“White House,” and were published at the same
time, they describe different events.
From a probabilistic point of view, we can treat
each sentence as being generated by a particular hid-
den event which involves several actors. Clearly,
some of these actors, like Obama, participate in
many more events than others, and in such cases
we observe sentences generated from a mixture of
events. Since two event mentions from such a mix-
ture are much less likely to denote the same event
or relation, we wish to distinguish them from the
better (semantically homogeneous) EECs like the
(Armstrong, Livestrong) example. The question be-
comes “How one can distinguish good entity pairs
from bad?”
Our method rests on the simple observation that
an entity which participates in many different events
on one day is likely to have participated in events
in recent days. Therefore we can judge whether an
entity pair is good for paraphrasing by looking at
the history of the frequencies that the entity pair is
mentioned in the news streams, which is the time
series of that entity pair. The time series of the entity
pair (Barack Obama, the White House) tends to be
high over time, while the time series of the entity
pair (Armstrong, Livestrong) is flat for a long time
and suddenly spikes upwards on a single day. This
observation leads to:
Temporal Burstiness Heuristic. If an entity or an
entity pair appears significantly more frequently in
one day’s news than in recent history, the corre-
sponding event candidates are likely to be good to
generate paraphrase.
The temporal burstiness heuristic implies that a
good EEC (a1, a2, t) tends to have a spike in the time
series of its entities az, or argument pair (a1, a2), on
day t.
However, even if we have selected a good EEC
for paraphrasing, it is likely that it contains a few
relation phrases that are related to (but not synony-
mous with) the other relations included in the EEC.
For example, it’s likely that the news story report-
ing “Armstrong steps down from Livestrong.” might
also mention “Armstrong is the founder of Live-
strong.” and so both “steps down from” and “is the
founder of” relation phrases would be part of the
same EEC-set. Inspired by the idea of one sense per
discourse from (Gale et al., 1992), we propose:
One Event-Mention Per Discourse Heuristic. A
news article tends not to state the same fact more
than once.
The one event-mention per discourse heuristic is
proposed in order to gain precision at the expense
of recall — the heuristic directs an algorithm to
choose, from a news story, the single “best” relation
phrase connecting a pair of two entities. Of course,
this doesn’t answer the question of deciding which
phrase is “best.” In Section 4.3, we describe how
to learn a probabilistic graphical model which does
exactly this.
</bodyText>
<page confidence="0.512744">
1778
</page>
<sectionHeader confidence="0.983446" genericHeader="method">
4 Exploiting the Temporal Heuristics
</sectionHeader>
<bodyText confidence="0.999955666666667">
In this section we propose several models to capture
the temporal correspondence heuristics, and discuss
their pros and cons.
</bodyText>
<subsectionHeader confidence="0.964832">
4.1 Baseline Model
</subsectionHeader>
<bodyText confidence="0.995164533333333">
An easy way to use an EEC-set is to simply predict
that all rz in the EEC-set are event-mentions, and
hence are semantically equivalent. That is, given
EEC-set (a1, a2, t, {r1 ... r,,,}), the output cluster is
{r1 ... r&amp;quot;&apos;}.
This baseline model captures the most of the tem-
poral functionality heuristic, except for the tense re-
quirement. Our empirical study shows that it per-
forms surprisingly well. This demonstrates that the
quality of our input for the learning model is good:
the EEC-sets are promising resources for paraphras-
ing.
Unfortunately, the baseline model cannot deal
with the other heuristics, a problem we will remedy
in the following sections.
</bodyText>
<subsectionHeader confidence="0.966082">
4.2 Pairwise Model
</subsectionHeader>
<bodyText confidence="0.991982413793103">
The temporal functionality heuristic suggests we ex-
ploit the tenses of the relations in an EEC-set; while
the temporal burstiness heuristic suggests we ex-
ploit the time series of its arguments. A pairwise
model can be designed to capture them: we compare
pairs of relations in the EEC-set, and predict whether
each pair is synonymous or non-synonymous. Para-
phrase clusters are then generated according to some
heuristic rules (e.g. assuming transitivity among
synonyms). The tenses of the relations and time se-
ries of the arguments are encoded as features, which
we call tense features and spike features respec-
tively. An example tense feature is whether one re-
lation is past tense while the other relation is present
tense; an example spike feature is the covariance of
the time series.
The pairwise model can be considered similar to
paraphrasing techniques which examine two sen-
tences and determine whether they are semantically
equivalent (Dolan and Brockett, 2005; Socher et al.,
2011). Unfortunately, these techniques often based
purely on text metrics and does not consider any
temporal attributes. In section 5, we evaluate the
effect of applying these techniques.
Figure 2: an example model for EEC (Armstrong, Live-
strong, Oct 17). Y and Z are binary random variables.
4)Y, 4)Z and 6oint are factors. be founder of and step
down come from article 1 while give speech at, be chair-
man of and resign from come from article 2.
</bodyText>
<subsectionHeader confidence="0.99922">
4.3 Joint Cluster Model
</subsectionHeader>
<bodyText confidence="0.999646807692308">
The pairwise model has several drawbacks: 1) it
lacks the ability to handle constraints, such as the
mutual exclusion constraint implied by the one-
mention per discourse heuristic; 2) ad-hoc rules,
rather than formal optimizations, are required to
generate clusters containing more than two relations.
A common approach to overcome the drawbacks
of the pairwise model and to combine heuristics to-
gether is to introduce a joint cluster model, in which
heuristics are encoded as features and constraints.
Data, instead of ad-hoc rules, determines the rel-
evance of different insights, which can be learned
as parameters. The advantage of the joint model
is analogous to that of cluster-based approaches for
coreference resolution (CR). In particular, a joint
model can better capture constraints on multiple
variables and can yield higher quality results than
pairwise CR models (Rahman and Ng, 2009).
We propose an undirected graphical model,
NEWSSPIKE, which jointly clusters relations. Con-
straints are captured by factors connecting multiple
random variables. We introduce random variables,
the factors, the objective function, the inference al-
gorithm, and the learning algorithm in the following
sections. Figure 2 shows an example model for EEC
(Armstrong, Livestrong, Oct 17).
</bodyText>
<subsectionHeader confidence="0.547135">
4.3.1 Random Variables
</subsectionHeader>
<bodyText confidence="0.993971666666667">
For the EEC-set (a1, a2, t, {r1,... r,,,}), we intro-
duce one event variable and m relation variables, all
boolean valued. The event variable Z(a1,a2,t) indi-
</bodyText>
<figure confidence="0.965624894736842">
y be founder of
y give speech at
0
y step down
y resign from
Φ2Y
Article1
Article2
𝑍 (Armstrong,Livestrong,Oct.17)
1
ΦZ
Φjoint
Φ1Y
0
0
y be chairman of
1
1
1779
</figure>
<bodyText confidence="0.998911416666667">
cates whether (a1, a2, t) is a good event for para-
phrasing. It is designed in accordance with the
temporal burstiness heuristic: for the EEC (Barack
Obama, the White House, Oct 17), Z should be as-
signed the value 0.
The relation variable Y r indicates whether rela-
tion r describes the EEC (a1, a2, t) or not (i.e. r is an
event-mention or not). The set of all event-mentions
with Y r = 1 define a paraphrase cluster, contain-
ing relation phrases. For example, the assignments
Ystep down = Y resign from = 1 produce a paraphrase
cluster {step down, resign from}.
</bodyText>
<subsubsectionHeader confidence="0.919437">
4.3.2 Factors and the Joint Distribution
</subsubsectionHeader>
<bodyText confidence="0.991581606060606">
In this section, we introduce a conditional proba-
bility model defining a joint distribution over all of
the event and relation variables. The joint distribu-
tion is a function over factors. Our model contains
event factors, relation factors and joint factors.
The event factor Φ� is a log-linear function with
spike features, used to distinguish good events. A re-
lation factor ΦY is also a log-linear function. It can
be defined for individual relation variables (e.g. ΦY 1
in Figure 2) with features such as whether a relation
phrase comes from a clausal complement3. A rela-
tion factor can also be defined for a pair of relation
variables (e.g. ΦY2 in Figure 2) with features captur-
ing the pairwise evidence for paraphrasing, such as
if two relation phrases have the same tense.
The joint factors Φjoint are defined to apply con-
straints implied by the temporal heuristics. They
play two roles in our model: 1) to satisfy the tempo-
ral burstiness heuristic, when the value of the event
variable is false, the EEC is not appropriate for para-
phrasing, and so all relation variables should also be
false; and 2) to satisfy the one-mention per discourse
heuristic, at most one relation variable from a single
article could be true.
We define the joint distribution over these vari-
ables and factors as follows. Let Y = (Yrl ... Yr—)
be the vector of relation variables; let x be the fea-
tures. The joint distribution is:
3Relation phrases in clausal complement are less useful for
paraphrasing because they often do not describe a fact. For ex-
ample, in the sentence He heard Romney had won the election,
the extraction (Romney, had won, the election) is not a fact at
all.
</bodyText>
<equation confidence="0.95483775">
p(Z = z, Y = y|x; Θ) def= 1Φ�(z, x)
Z�
rlx rlΦjoint(z, yd, x) ΦY (yi,yj,x)
d i,j
</equation>
<bodyText confidence="0.99968775">
where yd indicates the subset of relation variables
from a particular article d, and the parameter vector
Θ is the weight vector of the features in Φ� and ΦY ,
which are log-linear functions; i.e.,
</bodyText>
<equation confidence="0.94897325">
�
��
ΦY (yi, yj, x) def = exp θjφj(yi, yj, x)
j
</equation>
<bodyText confidence="0.998833">
where φj is the jth feature function.
The joint factors Φjoint are used to apply the tem-
poral burstiness heuristic and the one event-mention
per discourse heuristic. Φjoint is zero when the EEC
is not good for paraphrasing, but some yr = 1; or
when there is more than one r in a single article such
that yr = 1. Formally, it is calculated as:
</bodyText>
<listItem confidence="0.517596">
{ 0 if z = 0 ∧ ]yr = 1
0 if Ey&apos;Eyd yr &gt; 1
1 otherwise
</listItem>
<subsectionHeader confidence="0.78573">
4.3.3 Maximum a Posteriori Inference
</subsectionHeader>
<bodyText confidence="0.99911">
The goal of inference is to find the predictions z, y
which yield the greatest probability, i.e.,
</bodyText>
<equation confidence="0.988907">
p(Z = z,Y = y|x;Θ)
</equation>
<bodyText confidence="0.999432941176471">
This can be viewed as a MAP inference problem.
In general, inference in a graphical model is chal-
lenging. Fortunately, the joint factors in our model
are linear, and the event and relation factors are log-
linear; we can cast MAP inference as an integer lin-
ear programming (ILP) problem, and then compute
an approximation in polynomial time by means of
linear programming using randomized rounding, as
proposed in (Yannakakis, 1992).
We build one ILP problem for every EEC. The
variables of the ILP are Z and Y, which only take
values of 0 or 1. The objective function is the sum
of logs of the event and relation factors Φ� and
ΦY . The temporal burstiness heuristic of Φjoint is
encoded as a linear inequality constraint z &gt; yi; the
one-mention per discourse heuristic of Φjoint is en-
coded as the constraint EyzEyd yi &lt; 1.
</bodyText>
<equation confidence="0.930251">
Φjoint(z, yd, x) def=
z*, y* = arg max
z,y
1780
</equation>
<subsectionHeader confidence="0.725329">
4.3.4 Learning
</subsectionHeader>
<bodyText confidence="0.964978">
Our training data consists a set of N = 500 la-
beled EEC-sets each in the form of {(Ri, Rgold
i ) |N i=1
}. Each R is the set of all relations in the EEC-set
while Rgold is a manually selected subset of R con-
taining relations describing the EEC. Rgold could be
empty if the EEC was deemed poor for paraphras-
ing. For our model, the gold assignment yrgold = 1
if r E Rgold; the gold assignment zgold = 1 if Rgold
is not empty.
Given {(Ri, Rgold
i ) |Ni=1}, learning over similar
models is commonly done via maximum likelihood
estimation as follows:
</bodyText>
<equation confidence="0.5597275">
gold gold
p(Zi = zi , Yi = yi  |xi, Θ)
</equation>
<bodyText confidence="0.886854">
For features in relation factors, the partial deriva-
tive for the ith model is:
gold
Φj (yi , xi) − Ep(zi,yi|,xi,Θ)Φj(yi, xi)
where Φj(yi, xi) = Oj(X, Y, x), the sum of val-
ues for the jth feature in the ith model; and values
of X, Y come from the assignment yi. For features
in event factors, the partial derivative is derived sim-
ilarly as
</bodyText>
<equation confidence="0.941193666666667">
gold �j
Oj(zi , xi) − Ep(zi,yi|,xi,Θ) Oj (zi,
xi)
</equation>
<bodyText confidence="0.999906083333333">
It is unclear how to efficiently compute the expec-
tations in the above formula, a brute force approach
requires enumerating all assignments of yi, which
is exponentially large with the number of relations.
Instead, we opt to use a more tractable perceptron
learning approach (Collins, 2002; Hoffmann et al.,
2011). Instead of computing the expectations, we
simply compute Oj(z∗i , xi) and Φj(y∗i , xi), where
z∗i , y∗i is the assignment with the highest probabil-
ity, generated by the MAP inference algorithm us-
ing the current weight vector. The weight updates
are the following:
</bodyText>
<equation confidence="0.93907325">
Φj(ygold
i , xi) − Φj(y∗i ,xi) (1)
Oj(zgold
i , xi) − Oj(z∗i , xi) (2)
</equation>
<bodyText confidence="0.999790666666667">
The updates can be intuitively explained as penal-
ties on errors. In sum, our learning algorithm con-
sists of iterating the following two steps: (1) in-
fer the most probable assignment given the current
weights; (2) update the weights by comparing in-
ferred assignments and the truth assignment.
</bodyText>
<sectionHeader confidence="0.975955" genericHeader="method">
5 Empirical Study
</sectionHeader>
<bodyText confidence="0.999903555555556">
We first introduce the experimental setup for our em-
pirical study, and then we attempt to answer two
questions in sections 5.2 and 5.3 respectively: First,
does the NEWSSPIKE algorithm effectively exploit
the proposed heuristics and outperform other ap-
proaches which also use news streams? Secondly,
do the proposed temporal heuristics paraphrase re-
lations with greater precision than the distributional
hypothesis?
</bodyText>
<subsectionHeader confidence="0.984217">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.99989">
Since we were unable to find any elaborate time-
stamped, parallel, news corpus, we collected data
using the following procedure:
</bodyText>
<listItem confidence="0.9990718">
• Collect RSS news seeds, which contain the title,
time-stamp, and abstract of the news items.
• Use these titles to query the Bing news search
engine API and collect additional time-stamped
news articles.
• Strip HTML tags from the news articles using
Boilerpipe (Kohlsch¨utter et al., 2010); keep only
the title and first paragraph of each article.
• Extract shallow relation tuples using the OpenIE
system (Fader et al., 2011).
</listItem>
<bodyText confidence="0.999898090909091">
We performed these steps every day from Jan-
uary 1 to February 22, 2013. In total, we collected
546,713 news articles, for which 2.6 million extrac-
tions had 529 thousand unique relations.
We used several types of features for paraphras-
ing: 1) spike features obtained from time series; 2)
tense features, such as whether two relation phrases
are both in the present tense; 3) cause-effect fea-
tures, such as whether two relation phrases often ap-
pear successively in the news articles; 4) text fea-
tures, such as whether sentences are similar; S) syn-
tactic features, such as whether a relation phrase
appears in a clausal complement; and 6) semantic
features, such as whether a relation phrase contains
negative words.
Text and semantic features are encoded using the
relation factors of section 4.3.2. For example, in Fig-
ure 2, the factor ΦY 2 includes the textual similarity
between the sentences containing the phrases “step
down” and “be chairman of” respectively; it also
includes the feature that the tense of “step down”
(present) is different from the tense of “be chairman
</bodyText>
<equation confidence="0.944726">
�
L(Θ) = log
i
</equation>
<table confidence="0.579647142857143">
1781
output {go into, go to, speak, return,
head to}
gold {go into, go to, approach, head to}
golddiv {go *, approach, head to}
P/R precision = 3/5 recall = 3/4
P/Rdiv precisiondiv = 2/4 recalldiv = 2/3
</table>
<figureCaption confidence="0.986026333333333">
Figure 3: an example pair of the output cluster and the
gold cluster, and the corresponding precision recall num-
bers.
</figureCaption>
<bodyText confidence="0.866125">
of” (past).
</bodyText>
<subsectionHeader confidence="0.9987875">
5.2 Comparison with Methods using Parallel
News Corpora
</subsectionHeader>
<bodyText confidence="0.999800515151515">
We evaluated NEWSSPIKE against other methods
that also use time-stamped news. These include the
models mentioned in section 3 and state-of-the-art
paraphrasing techniques.
Human annotators created gold paraphrase clus-
ters for 500 EEC-sets; note that some EEC-sets
yield no gold cluster, since at least two synonymous
phrases. Two annotators were shown a set of candi-
date relation phrases in context and asked to select a
subset of these that described a shared event (if one
existed). There was 98% phrase-level agreement.
Precision and recall were computed by comparing
an algorithm’s output clusters to the gold cluster of
each EEC. We consider paraphrases with minor lex-
ical diversity, e.g. (go to, go into), to be of lesser in-
terest. Since counting these trivial paraphrases tends
to exaggerate the performance of a system, we also
report precision and recall on diverse clusters i.e.,
those whose relation phrases all have different head
verbs. Figure 3 illustrates these metrics with an ex-
ample; note under our diverse metrics, all phrases
matching go * count as one when computing both
precision and recall. We conduct 5-fold cross val-
idation on our labeled dataset to get precision and
recall numbers when the system requires training.
We compare NEWSSPIKE with the models in Sec-
tion 4, and also with the state-of-the-art paraphrase
extraction method:
Baseline: the model discussed in Section 4.1.
This system does not need any training, and gener-
ates outputs with perfect recall.
Pairwise: the pairwise model discussed in Sec-
tion 4.2 and using the same set of features as used
</bodyText>
<table confidence="0.999798166666667">
System P/R P/R diverse
prec rec prec rec
Baseline 0.67 1.00 0.53 1.00
Pairwise 0.90 0.60 0.81 0.37
Socher 0.81 0.35 0.68 0.29
NEWSSPIKE 0.92 0.55 0.87 0.31
</table>
<tableCaption confidence="0.9880485">
Table 1: Comparison with methods using parallel news
corpora
</tableCaption>
<bodyText confidence="0.979172846153846">
by NEWSSPIKE. To generate output clusters, transi-
tivity is assumed inside the EEC-set. For example,
when the pairwise model predicts that (r1, r2) and
(r1, r3) are both paraphrases, the resulting cluster is
{r1, r2, r3}.
Socher: Socher et al. (2011) achieved the best re-
sults on the Dolan et al. (2004) dataset, and released
their code and models. We used their off-the-shelf
predictor to replace the classifier in our Pairwise
model. Given sentential paraphrases, aligning rela-
tion phrases is natural, because OpenIE has already
identified the relation phrases.
Table 1 shows precision and recall numbers. It
is interesting that the basic model already obtains
0.67 precision overall and 0.53 in the diverse con-
dition. This demonstrates that the EEC-sets gen-
erated from the news streams are a promising re-
source for paraphrasing. Socher’s method performs
better, but not as well as Pairwise or NEWSSPIKE,
especially in the diverse cases. This is probably
due to the fact that Socher’s method is based purely
on text metrics and does not consider any tempo-
ral attributes. Taking into account the features used
by NEWSSPIKE, Pairwise significantly improves the
precision, which demonstrates the power of our tem-
poral correspondence heuristics. Our joint cluster
model, NEWSSPIKE, which considers both temporal
features and constraints, gets the best performance
in both conditions.
We conducted ablation testing to evaluate how
spike features and tense features, which are par-
ticularly relevant to the temporal aspects of news
streams, can improve performance. Figure 4 com-
pares the precision/recall curves for three systems
in the diverse condition: (1) NEWSSPIKE; (2)
w/oSpike: turning off all spike features; and (3)
w/oTense: turning off all features about tense.
(4) w/oDiscourse: turning off one event-mention
per discourse heuristic. There are some dips in
</bodyText>
<figure confidence="0.815702">
1782
0.1 0.2 0.3 0.4
Recall
</figure>
<figureCaption confidence="0.986655">
Figure 4: Precision recall curves on hard, diverse cases
for NewsSpike, w/oSpike, w/oTense and w/oDiscourse.
</figureCaption>
<bodyText confidence="0.999926333333333">
the curves because they are drawn after sorting
the predictions by the value of the corresponding
ILP objective functions, which do not perfectly re-
flect prediction accuracy. However, it is clear that
NEWSSPIKE produces greater precision over all
ranges of recall.
</bodyText>
<subsectionHeader confidence="0.998971">
5.3 Comparison with Methods using the
Distributional Hypothesis
</subsectionHeader>
<bodyText confidence="0.999755363636364">
We evaluated our model against methods based on
the distributional hypothesis. We ran NEWSSPIKE
over all EEC-sets except for the development set and
compared to the following systems:
Resolver: Resolver (Yates and Etzioni, 2009)
uses a set of extraction tuples in the form of
(a1, r, a2) as the input and creates a set of relation
clusters as the output paraphrases. Resolver also
produces argument clusters, but this paper only eval-
uates relation clustering. We evaluated Resolver’s
performance with an input of the 2.6 million extrac-
tions described in section 5.1, using Resolver’s de-
fault parameters.
ResolverNYT: Since Resolver is supposed to
perform better when given more accurate statis-
tics from a larger corpus, we tried giving it more
data. Specifically, we ran ReVerb on 1.8 million NY
Times articles published between 1987 and 2007 ob-
tain 60 million extractions (Sandhaus, 2008). We ran
Resolver on the union of this and our standard test
set, but report performance only on clusters whose
relations were seen in our news stream.
</bodyText>
<table confidence="0.999258875">
System all diverse
prec #rels prec #rels
Resolver 0.78 129 0.65 57
ResolverNyt 0.64 1461 0.52 841
ResolverNytTop 0.83 207 0.72 79
Cosine 0.65 17 0.33 9
CosineNyt 0.56 73 0.46 59
NEWSSPIKE 0.93 24843 0.87 5574
</table>
<tableCaption confidence="0.993498">
Table 2: Comparison with methods using the distribu-
tional hypothesis
</tableCaption>
<note confidence="0.503211">
ResolverNytTop: Resolver is designed to
</note>
<bodyText confidence="0.999804216216216">
achieve good performance on its top results. We thus
ranked the ResolverNYT outputs by their scores and
report the precision of the top 100 clusters.
Cosine: Cosine similarity is a basic metric for
the distributional hypothesis. This system employs
the same setup as Resolver in order to generate
paraphrase clusters, except that Resolver’s similar-
ity metric is replaced with the cosine. Each relation
is represented by a vector of argument pairs. The
similarity threshold to merge two clusters was 0.5.
CosineNYT: As for ResolverNYT, we ran Cosi-
neNYT with an extra 60 million extractions and re-
ported the performance on relations seen in our news
stream.
We measured the precision of each system by
manually labeling all output if 100 or fewer clus-
ters were generated (e.g. ResolverNytTop), other-
wise 100 randomly chosen clusters were sampled.
Annotators first determined the meaning of every
output cluster and then created a gold cluster by
choosing the correct relations. The gold cluster
could be empty if the output cluster was nonsensi-
cal. Unlike many papers that simply report recall on
the most frequent relations, we evaluated the total
number of returned relations in the output clusters.
As in Section 5.2, we also report numbers for the
case of lexically diverse relation phrases.
As can be seen in Table 2, NEWSSPIKE outper-
formed methods based on the distributional hypoth-
esis. The performance of the Cosine and Cosi-
neNyt was very low, suggesting that simple simi-
larity metrics are insufficient for handling the para-
phrasing problem, even when large-scale input is in-
volved. Resolver and ResolverNyt employ an ad-
vanced similarity measurement and achieve better
results. However, it is surprising that Resolver re-
sults in a greater precision than ResolverNyt. It
</bodyText>
<figure confidence="0.979572818181818">
NewsSpike
w/oSpike
w/oTense
1.0
0.9
w/oDiscourse
0.8
0.7
0.6
Precision
1783
</figure>
<bodyText confidence="0.998450928571428">
is possible that argument pairs from news streams
spanning 20 years sometimes provide incorrect ev-
idence for paraphrasing. For example, there were
extractions like (the Rangers, be third in, the NHL)
and (the Rangers, be fourth in, the NHL) from news
in 2007 and 2003 respectively. Using these phrases,
ResolverNyt produced the incorrect cluster Tbe third
in, be fourth in}. NEWSSPIKE achieves greater pre-
cision than even the best results from ResolverNyt-
Top, because NEWSSPIKE successfully captures the
temporal heuristics, and does not confuse synonyms
with antonyms, or causes with effects. NEWSSPIKE
also returned on order of magnitude more relations
than other methods.
</bodyText>
<subsectionHeader confidence="0.955551">
5.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999980885714286">
Unlike some domain-specific clustering methods,
we tested on all relation phrases extracted by Ope-
nIE on the collected news streams. There are no
restrictions on the types of relations. Output para-
phrases cover a broad range, including politics,
sports, entertainment, health, science, etc. There
are 10 thousand nonempty clusters over 17 thousand
distinct phrases with average size 2.4. Unlike meth-
ods based on distributional similarity, NewsSpike
correctly clusters infrequently appearing phrases.
Since we focus on high precision, it is not sur-
prising that most clusters are of size 2 and 3. These
high precision clusters can contribute a lot to gen-
erate larger paraphrase clusters. For example, one
can invent the technique to merge smaller clusters
together. The work presented here provides a foun-
dation for future work to more closely examine these
challenges.
While this paper gives promising results, there
are still behaviors found in news streams that prove
challenging. Many errors are due to the discourse
context: the two sentences are synonymous in the
given EEC-set, but the relation phrases are not
paraphrases in general. For example, consider the
following two sentences: “DA14 narrowly misses
Earth” and “DA14 flies so close to Earth”. Statis-
tics information from large corpus would be helpful
to handle such challenges. Note in this paper, in or-
der to fairly compare with the distributional hypoth-
esis, we purposely forced NEWSSPIKE not to rely
on any distributional similarity. But NEWSSPIKE’s
graphical model has the flexibility to incorporate any
similarity metrics as features. Such a hybrid model
has great potential to increase both precision and re-
call, which is one goal for future work.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999958090909091">
The vast majority of paraphrasing work falls into
two categories: approaches based on the distribu-
tional hypothesis or those exploiting on correspon-
dences between parallel corpora (Androutsopoulos
and Malakasiotis, 2010; Madnani and Dorr, 2010).
Using Distribution Similarity: Lin and Pan-
tel’s (2001) DIRT employ mutual information statis-
tics to compute the similarity between relations rep-
resented in dependency paths. Resolver (Yates and
Etzioni, 2009) introduces a new similarity metric
called the Extracted Shared Property (ESP) and uses
a probabilistic model to merge ESP with surface
string similarity.
Identifying the semantic equivalence of relation
phrases is also called relation discovery or unsu-
pervised semantic parsing. Often techniques don’t
compute the similarity explicitly but rely implic-
itly on the distributional hypothesis. Poon and
Domingos’ (2009) USP clusters relations repre-
sented with fragments of dependency trees by re-
peatedly merging relations having similar context.
Yao et al. (2011; 2012) introduces generative mod-
els for relation discovery using LDA-style algorithm
over a relation-feature matrix. Chen et al. (2011) fo-
cuses on domain-dependent relation discovery, ex-
tending a generative model with meta-constraints
from lexical, syntactic and discourse regularities.
Our work solves a major problem with these ap-
proaches, avoiding errors such as confusing syn-
onyms with antonyms and causes with effects. Fur-
thermore, NEWSSPIKE doesn’t require massive sta-
tistical evidence as do most approaches based on the
distributional hypothesis.
Using Parallel Corpora: Comparable and par-
allel corpora, including news streams and multiple
translations of the same story, have been used to
generate paraphrases, both sentential (Barzilay and
Lee, 2003; Dolan et al., 2004; Shinyama and Sekine,
2003) and phrasal (Barzilay and McKeown, 2001;
Shen et al., 2006; Pang et al., 2003). Typical meth-
ods first gather relevant articles and then pair sen-
tences that are potential paraphrases. Given a train-
ing set of paraphrases, models are learned and ap-
plied to unlabeled pairs (Dolan and Brockett, 2005;
</bodyText>
<page confidence="0.469871">
1784
</page>
<bodyText confidence="0.999508173913043">
Socher et al., 2011). Phrasal paraphrases are often
obtained by running an alignment algorithm over the
paraphrased sentence pairs.
While prior work uses the temporal aspects of
news streams as a coarse filter, it largely relies on
text metrics, such as context similarity and edit dis-
tance, to make predictions and alignments. These
metrics are usually insufficient to produce high pre-
cision results; moreover they tend to produce para-
phrases that are simple lexical variants (e.g. {go to,
go into}.). In contrast, NEWSSPIKE generates para-
phrase clusters with both high precision and high di-
versity.
Others: Textual entailment (Dagan et al., 2009),
which finds a phrase implying another phrase,
is closely related to the paraphrasing task. Be-
rant et al. (2011) notes the flaws in distributional
similarity and proposes local entailment classi-
fiers, which are able to combine many features.
Lin et al. (2012) also uses temporal information to
detect the semantics of entities. In a manner similar
to our approach, Recasens et al. (2013) mines paral-
lel news stories to find opaque coreferent mentions.
</bodyText>
<sectionHeader confidence="0.996433" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999981285714286">
Paraphrasing event relations is crucial to many natu-
ral language processing applications, including re-
lation extraction, question answering, summariza-
tion, and machine translation. Unfortunately, previ-
ous approaches based on distribution similarity and
parallel corpora, often produce low precision clus-
ters. This paper introduces three Temporal Corre-
spondence Heuristics that characterize semantically
equivalent phrases in news streams. We present a
novel algorithm, NEWSSPIKE, based on a proba-
bilistic graphical model encoding these heuristics,
which harvests high-quality paraphrases of event re-
lations.
Experiments show NEWSSPIKE’s improvement
relative to several other methods, especially at pro-
ducing lexically diverse clusters. Ablation tests
confirm that our temporal features are crucial to
NEWSSPIKE’s precision. In order to spur future
research, we are releasing an annotated corpus of
time-stamped news articles and our harvested rela-
tion clusters.
</bodyText>
<sectionHeader confidence="0.995469" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999961133333333">
We thank Oren Etzioni, Anthony Fader, Raphael
Hoffmann, Ben Taskar, Luke Zettlemoyer, and the
anonymous reviewers for providing valuable ad-
vice. We also thank Shengliang Xu for annotat-
ing the datasets. We gratefully acknowledge the
support of the Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181, ONR grant N00014-
12-1-0211, a gift from Google, and the WRF / TJ
Cable Professorship. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the author(s) and do not neces-
sarily reflect the view of DARPA, AFRL, or the US
government.
</bodyText>
<sectionHeader confidence="0.994297" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999101984615385">
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entail-
ment methods. In Journal ofArtificial Intelligence Re-
search, pages 135–187.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In HLT-NAACL, pages 16–23.
Association for Computational Linguistics.
Regina Barzilay and Kathleen R McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In ACL,
pages 50–57. Association for Computational Linguis-
tics.
Regina Barzilay, Kathleen R McKeown, and Michael El-
hadad. 1999. Information fusion in the context of
multi-document summarization. In ACL, pages 550–
557. Association for Computational Linguistics.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
ACL-HLT, pages 610–619. Association for Computa-
tional Linguistics.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In ACL, volume 8, pages 674–682. Associa-
tion for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In NAACL, pages 17–24. Associa-
tion for Computational Linguistics.
Harr Chen, Edward Benson, Tahira Naseem, and Regina
Barzilay. 2011. In-domain relation discovery with
meta-constraints via posterior regularization. In ACL-
HLT, pages 530–540. Association for Computational
Linguistics.
1785
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In ACL, pages 1–8. Asso-
ciation for Computational Linguistics.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Natural Language Engineer-
ing, 15(04):i–xvii.
William B Dolan and Chris Brockett. 2005. Automat-
ically constructing a corpus of sentential paraphrases.
In Proceedings of IWP.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Computational Linguistics, page 350. Association for
Computational Linguistics.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In EMNLP. Association for Computational
Linguistics, July 27-31.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In ACL. Association for Computational
Linguistics.
William A Gale, Kenneth W Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural Lan-
guage, pages 233–237. Association for Computational
Linguistics.
Zellig S Harris. 1954. Distributional structure. Word.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In ACL-HLT, pages 541–550.
Christian Kohlsch¨utter, Peter Fankhauser, and Wolfgang
Nejdl. 2010. Boilerplate detection using shallow text
features. In WSDM, pages 441–450. ACM.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question-answering. Natural Language
Engineering, 7(4):343–360.
Thomas Lin, Oren Etzioni, et al. 2012. No noun phrase
left behind: detecting and typing unlinkable entities.
In EMNLP, pages 893–903. Association for Computa-
tional Linguistics.
Nitin Madnani and Bonnie J Dorr. 2010. Gener-
ating phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341–387.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
NAACL, pages 102–109. Association for Computa-
tional Linguistics.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP, pages 1–10. As-
sociation for Computational Linguistics.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In EMNLP, pages 968–
977. Association for Computational Linguistics.
Marta Recasens, Matthew Can, and Dan Jurafsky. 2013.
Same referent, different words: Unsupervised min-
ing of opaque coreferent mentions. In Proceedings of
NAACL-HLT, pages 897–906.
Evan Sandhaus. 2008. The New York Times annotated
corpus. Linguistic Data Consortium.
Siwei Shen, Dragomir R Radev, Agam Patel, and G¨unes¸
Erkan. 2006. Adding syntax to dynamic program-
ming for aligning comparable texts for the generation
of paraphrases. In Proceedings of the COLING/ACL
on Main conference poster sessions, pages 747–754.
Association for Computational Linguistics.
Yusuke Shinyama and Satoshi Sekine. 2003. Para-
phrase acquisition for information extraction. In Pro-
ceedings of the second international workshop on
Paraphrasing-Volume 16, pages 65–71. Association
for Computational Linguistics.
Richard Socher, Eric H Huang, Jeffrey Pennington, An-
drew Y Ng, and Christopher D Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. NIPS, 24:801–809.
Mihalis Yannakakis. 1992. On the approximation of
maximum satisfiability. In Proceedings of the third an-
nual ACM-SIAM symposium on Discrete algorithms,
SODA ’92, pages 1–9.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
generative models. In EMNLP, pages 1456–1466. As-
sociation for Computational Linguistics.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012. Unsupervised relation discovery with sense dis-
ambiguation. In ACL, pages 712–720. Association for
Computational Linguistics.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal ofArtificial Intelligence Research,
34(1):255.
</reference>
<page confidence="0.762974">
1786
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.575610">
<title confidence="0.997084">Harvesting Parallel News Streams to Generate Paraphrases of Event Relations</title>
<author confidence="0.963775">Congle Zhang</author>
<author confidence="0.963775">S Daniel</author>
<affiliation confidence="0.999802">Computer Science &amp; University of</affiliation>
<address confidence="0.999544">Seattle, WA 98195,</address>
<abstract confidence="0.998193904761905">The distributional hypothesis, which states that words that occur in similar contexts tend to have similar meanings, has inspired several Web mining algorithms for paraphrasing semantically equivalent phrases. Unfortunately, these methods have several drawbacks, such as confusing synonyms with antonyms and causes with effects. This paper introduces three Temporal Correspondence Heuristics, that characterize regularities in parallel news streams, and shows how they may be used to generate high precision paraphrases for event relations. We encode the heuristics in a probabilistic graphical model to create for mining news streams. We present experiments demonthat outperforms several competitive baselines. In order to spur further research, we provide a large annotated corpus of timestamped news arti-</abstract>
<intro confidence="0.601972">cles as well as the paraphrases produced by</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ion Androutsopoulos</author>
<author>Prodromos Malakasiotis</author>
</authors>
<title>A survey of paraphrasing and textual entailment methods.</title>
<date>2010</date>
<booktitle>In Journal ofArtificial Intelligence Research,</booktitle>
<pages>135--187</pages>
<contexts>
<context position="34715" citStr="Androutsopoulos and Malakasiotis, 2010" startWordPosition="5688" endWordPosition="5691"> to handle such challenges. Note in this paper, in order to fairly compare with the distributional hypothesis, we purposely forced NEWSSPIKE not to rely on any distributional similarity. But NEWSSPIKE’s graphical model has the flexibility to incorporate any similarity metrics as features. Such a hybrid model has great potential to increase both precision and recall, which is one goal for future work. 6 Related Work The vast majority of paraphrasing work falls into two categories: approaches based on the distributional hypothesis or those exploiting on correspondences between parallel corpora (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010). Using Distribution Similarity: Lin and Pantel’s (2001) DIRT employ mutual information statistics to compute the similarity between relations represented in dependency paths. Resolver (Yates and Etzioni, 2009) introduces a new similarity metric called the Extracted Shared Property (ESP) and uses a probabilistic model to merge ESP with surface string similarity. Identifying the semantic equivalence of relation phrases is also called relation discovery or unsupervised semantic parsing. Often techniques don’t compute the similarity explicitly but rely implicitly on the d</context>
</contexts>
<marker>Androutsopoulos, Malakasiotis, 2010</marker>
<rawString>Ion Androutsopoulos and Prodromos Malakasiotis. 2010. A survey of paraphrasing and textual entailment methods. In Journal ofArtificial Intelligence Research, pages 135–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiplesequence alignment.</title>
<date>2003</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>16--23</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="36268" citStr="Barzilay and Lee, 2003" startWordPosition="5912" endWordPosition="5915">focuses on domain-dependent relation discovery, extending a generative model with meta-constraints from lexical, syntactic and discourse regularities. Our work solves a major problem with these approaches, avoiding errors such as confusing synonyms with antonyms and causes with effects. Furthermore, NEWSSPIKE doesn’t require massive statistical evidence as do most approaches based on the distributional hypothesis. Using Parallel Corpora: Comparable and parallel corpora, including news streams and multiple translations of the same story, have been used to generate paraphrases, both sentential (Barzilay and Lee, 2003; Dolan et al., 2004; Shinyama and Sekine, 2003) and phrasal (Barzilay and McKeown, 2001; Shen et al., 2006; Pang et al., 2003). Typical methods first gather relevant articles and then pair sentences that are potential paraphrases. Given a training set of paraphrases, models are learned and applied to unlabeled pairs (Dolan and Brockett, 2005; 1784 Socher et al., 2011). Phrasal paraphrases are often obtained by running an alignment algorithm over the paraphrased sentence pairs. While prior work uses the temporal aspects of news streams as a coarse filter, it largely relies on text metrics, suc</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: An unsupervised approach using multiplesequence alignment. In HLT-NAACL, pages 16–23. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus. In</title>
<date>2001</date>
<booktitle>ACL,</booktitle>
<pages>50--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="36356" citStr="Barzilay and McKeown, 2001" startWordPosition="5926" endWordPosition="5929">ta-constraints from lexical, syntactic and discourse regularities. Our work solves a major problem with these approaches, avoiding errors such as confusing synonyms with antonyms and causes with effects. Furthermore, NEWSSPIKE doesn’t require massive statistical evidence as do most approaches based on the distributional hypothesis. Using Parallel Corpora: Comparable and parallel corpora, including news streams and multiple translations of the same story, have been used to generate paraphrases, both sentential (Barzilay and Lee, 2003; Dolan et al., 2004; Shinyama and Sekine, 2003) and phrasal (Barzilay and McKeown, 2001; Shen et al., 2006; Pang et al., 2003). Typical methods first gather relevant articles and then pair sentences that are potential paraphrases. Given a training set of paraphrases, models are learned and applied to unlabeled pairs (Dolan and Brockett, 2005; 1784 Socher et al., 2011). Phrasal paraphrases are often obtained by running an alignment algorithm over the paraphrased sentence pairs. While prior work uses the temporal aspects of news streams as a coarse filter, it largely relies on text metrics, such as context similarity and edit distance, to make predictions and alignments. These met</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen R McKeown. 2001. Extracting paraphrases from a parallel corpus. In ACL, pages 50–57. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
<author>Michael Elhadad</author>
</authors>
<title>Information fusion in the context of multi-document summarization.</title>
<date>1999</date>
<booktitle>In ACL,</booktitle>
<pages>550--557</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1443" citStr="Barzilay et al., 1999" startWordPosition="200" endWordPosition="203">model to create the NEWSSPIKE algorithm for mining news streams. We present experiments demonstrating that NEWSSPIKE significantly outperforms several competitive baselines. In order to spur further research, we provide a large annotated corpus of timestamped news articles as well as the paraphrases produced by NEWSSPIKE. 1 Introduction Paraphrasing, the task of finding sets of semantically equivalent surface forms, is crucial to many natural language processing applications, including relation extraction (Bhagat and Ravichandran, 2008), question answering (Fader et al., 2013), summarization (Barzilay et al., 1999) and machine translation (Callison-Burch et al., 2006). While the benefits of paraphrasing have been demonstrated, creating a large-scale corpus of high precision paraphrases remains a challenge — especially for event relations. Many researchers have considered generating paraphrases by mining the Web guided by the distributional hypothesis, which states that words occurring in similar contexts tend to have similar meanings (Harris, 1954). For example, DIRT (Lin and Pantel, 2001) and Resolver (Yates and Etzioni, 2009) identify synonymous relation phrases by the distributions of their arguments</context>
</contexts>
<marker>Barzilay, McKeown, Elhadad, 1999</marker>
<rawString>Regina Barzilay, Kathleen R McKeown, and Michael Elhadad. 1999. Information fusion in the context of multi-document summarization. In ACL, pages 550– 557. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Global learning of typed entailment rules.</title>
<date>2011</date>
<booktitle>In ACL-HLT,</booktitle>
<pages>610--619</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="37380" citStr="Berant et al. (2011)" startWordPosition="6091" endWordPosition="6095">or work uses the temporal aspects of news streams as a coarse filter, it largely relies on text metrics, such as context similarity and edit distance, to make predictions and alignments. These metrics are usually insufficient to produce high precision results; moreover they tend to produce paraphrases that are simple lexical variants (e.g. {go to, go into}.). In contrast, NEWSSPIKE generates paraphrase clusters with both high precision and high diversity. Others: Textual entailment (Dagan et al., 2009), which finds a phrase implying another phrase, is closely related to the paraphrasing task. Berant et al. (2011) notes the flaws in distributional similarity and proposes local entailment classifiers, which are able to combine many features. Lin et al. (2012) also uses temporal information to detect the semantics of entities. In a manner similar to our approach, Recasens et al. (2013) mines parallel news stories to find opaque coreferent mentions. 7 Conclusion Paraphrasing event relations is crucial to many natural language processing applications, including relation extraction, question answering, summarization, and machine translation. Unfortunately, previous approaches based on distribution similarit</context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2011</marker>
<rawString>Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2011. Global learning of typed entailment rules. In ACL-HLT, pages 610–619. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Large scale acquisition of paraphrases for learning surface patterns.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<volume>8</volume>
<pages>674--682</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1363" citStr="Bhagat and Ravichandran, 2008" startWordPosition="188" endWordPosition="191"> paraphrases for event relations. We encode the heuristics in a probabilistic graphical model to create the NEWSSPIKE algorithm for mining news streams. We present experiments demonstrating that NEWSSPIKE significantly outperforms several competitive baselines. In order to spur further research, we provide a large annotated corpus of timestamped news articles as well as the paraphrases produced by NEWSSPIKE. 1 Introduction Paraphrasing, the task of finding sets of semantically equivalent surface forms, is crucial to many natural language processing applications, including relation extraction (Bhagat and Ravichandran, 2008), question answering (Fader et al., 2013), summarization (Barzilay et al., 1999) and machine translation (Callison-Burch et al., 2006). While the benefits of paraphrasing have been demonstrated, creating a large-scale corpus of high precision paraphrases remains a challenge — especially for event relations. Many researchers have considered generating paraphrases by mining the Web guided by the distributional hypothesis, which states that words occurring in similar contexts tend to have similar meanings (Harris, 1954). For example, DIRT (Lin and Pantel, 2001) and Resolver (Yates and Etzioni, 20</context>
</contexts>
<marker>Bhagat, Ravichandran, 2008</marker>
<rawString>Rahul Bhagat and Deepak Ravichandran. 2008. Large scale acquisition of paraphrases for learning surface patterns. In ACL, volume 8, pages 674–682. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In NAACL,</booktitle>
<pages>17--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1497" citStr="Callison-Burch et al., 2006" startWordPosition="208" endWordPosition="211">ng news streams. We present experiments demonstrating that NEWSSPIKE significantly outperforms several competitive baselines. In order to spur further research, we provide a large annotated corpus of timestamped news articles as well as the paraphrases produced by NEWSSPIKE. 1 Introduction Paraphrasing, the task of finding sets of semantically equivalent surface forms, is crucial to many natural language processing applications, including relation extraction (Bhagat and Ravichandran, 2008), question answering (Fader et al., 2013), summarization (Barzilay et al., 1999) and machine translation (Callison-Burch et al., 2006). While the benefits of paraphrasing have been demonstrated, creating a large-scale corpus of high precision paraphrases remains a challenge — especially for event relations. Many researchers have considered generating paraphrases by mining the Web guided by the distributional hypothesis, which states that words occurring in similar contexts tend to have similar meanings (Harris, 1954). For example, DIRT (Lin and Pantel, 2001) and Resolver (Yates and Etzioni, 2009) identify synonymous relation phrases by the distributions of their arguments. However, the distributional hypothesis has several d</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved statistical machine translation using paraphrases. In NAACL, pages 17–24. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harr Chen</author>
<author>Edward Benson</author>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
</authors>
<title>In-domain relation discovery with meta-constraints via posterior regularization.</title>
<date>2011</date>
<booktitle>In ACLHLT,</booktitle>
<pages>530--540</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="35645" citStr="Chen et al. (2011)" startWordPosition="5822" endWordPosition="5825">) and uses a probabilistic model to merge ESP with surface string similarity. Identifying the semantic equivalence of relation phrases is also called relation discovery or unsupervised semantic parsing. Often techniques don’t compute the similarity explicitly but rely implicitly on the distributional hypothesis. Poon and Domingos’ (2009) USP clusters relations represented with fragments of dependency trees by repeatedly merging relations having similar context. Yao et al. (2011; 2012) introduces generative models for relation discovery using LDA-style algorithm over a relation-feature matrix. Chen et al. (2011) focuses on domain-dependent relation discovery, extending a generative model with meta-constraints from lexical, syntactic and discourse regularities. Our work solves a major problem with these approaches, avoiding errors such as confusing synonyms with antonyms and causes with effects. Furthermore, NEWSSPIKE doesn’t require massive statistical evidence as do most approaches based on the distributional hypothesis. Using Parallel Corpora: Comparable and parallel corpora, including news streams and multiple translations of the same story, have been used to generate paraphrases, both sentential </context>
</contexts>
<marker>Chen, Benson, Naseem, Barzilay, 2011</marker>
<rawString>Harr Chen, Edward Benson, Tahira Naseem, and Regina Barzilay. 2011. In-domain relation discovery with meta-constraints via posterior regularization. In ACLHLT, pages 530–540. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21581" citStr="Collins, 2002" startWordPosition="3589" endWordPosition="3590">ith model is: gold Φj (yi , xi) − Ep(zi,yi|,xi,Θ)Φj(yi, xi) where Φj(yi, xi) = Oj(X, Y, x), the sum of values for the jth feature in the ith model; and values of X, Y come from the assignment yi. For features in event factors, the partial derivative is derived similarly as gold �j Oj(zi , xi) − Ep(zi,yi|,xi,Θ) Oj (zi, xi) It is unclear how to efficiently compute the expectations in the above formula, a brute force approach requires enumerating all assignments of yi, which is exponentially large with the number of relations. Instead, we opt to use a more tractable perceptron learning approach (Collins, 2002; Hoffmann et al., 2011). Instead of computing the expectations, we simply compute Oj(z∗i , xi) and Φj(y∗i , xi), where z∗i , y∗i is the assignment with the highest probability, generated by the MAP inference algorithm using the current weight vector. The weight updates are the following: Φj(ygold i , xi) − Φj(y∗i ,xi) (1) Oj(zgold i , xi) − Oj(z∗i , xi) (2) The updates can be intuitively explained as penalties on errors. In sum, our learning algorithm consists of iterating the following two steps: (1) infer the most probable assignment given the current weights; (2) update the weights by comp</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In ACL, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Bernardo Magnini</author>
<author>Dan Roth</author>
</authors>
<title>Recognizing textual entailment: Rational, evaluation and approaches.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>04</issue>
<contexts>
<context position="37267" citStr="Dagan et al., 2009" startWordPosition="6073" endWordPosition="6076"> paraphrases are often obtained by running an alignment algorithm over the paraphrased sentence pairs. While prior work uses the temporal aspects of news streams as a coarse filter, it largely relies on text metrics, such as context similarity and edit distance, to make predictions and alignments. These metrics are usually insufficient to produce high precision results; moreover they tend to produce paraphrases that are simple lexical variants (e.g. {go to, go into}.). In contrast, NEWSSPIKE generates paraphrase clusters with both high precision and high diversity. Others: Textual entailment (Dagan et al., 2009), which finds a phrase implying another phrase, is closely related to the paraphrasing task. Berant et al. (2011) notes the flaws in distributional similarity and proposes local entailment classifiers, which are able to combine many features. Lin et al. (2012) also uses temporal information to detect the semantics of entities. In a manner similar to our approach, Recasens et al. (2013) mines parallel news stories to find opaque coreferent mentions. 7 Conclusion Paraphrasing event relations is crucial to many natural language processing applications, including relation extraction, question answ</context>
</contexts>
<marker>Dagan, Dolan, Magnini, Roth, 2009</marker>
<rawString>Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entailment: Rational, evaluation and approaches. Natural Language Engineering, 15(04):i–xvii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Dolan</author>
<author>Chris Brockett</author>
</authors>
<title>Automatically constructing a corpus of sentential paraphrases.</title>
<date>2005</date>
<booktitle>In Proceedings of IWP.</booktitle>
<contexts>
<context position="14181" citStr="Dolan and Brockett, 2005" startWordPosition="2274" endWordPosition="2277">onymous. Paraphrase clusters are then generated according to some heuristic rules (e.g. assuming transitivity among synonyms). The tenses of the relations and time series of the arguments are encoded as features, which we call tense features and spike features respectively. An example tense feature is whether one relation is past tense while the other relation is present tense; an example spike feature is the covariance of the time series. The pairwise model can be considered similar to paraphrasing techniques which examine two sentences and determine whether they are semantically equivalent (Dolan and Brockett, 2005; Socher et al., 2011). Unfortunately, these techniques often based purely on text metrics and does not consider any temporal attributes. In section 5, we evaluate the effect of applying these techniques. Figure 2: an example model for EEC (Armstrong, Livestrong, Oct 17). Y and Z are binary random variables. 4)Y, 4)Z and 6oint are factors. be founder of and step down come from article 1 while give speech at, be chairman of and resign from come from article 2. 4.3 Joint Cluster Model The pairwise model has several drawbacks: 1) it lacks the ability to handle constraints, such as the mutual excl</context>
<context position="36612" citStr="Dolan and Brockett, 2005" startWordPosition="5970" endWordPosition="5973">ical evidence as do most approaches based on the distributional hypothesis. Using Parallel Corpora: Comparable and parallel corpora, including news streams and multiple translations of the same story, have been used to generate paraphrases, both sentential (Barzilay and Lee, 2003; Dolan et al., 2004; Shinyama and Sekine, 2003) and phrasal (Barzilay and McKeown, 2001; Shen et al., 2006; Pang et al., 2003). Typical methods first gather relevant articles and then pair sentences that are potential paraphrases. Given a training set of paraphrases, models are learned and applied to unlabeled pairs (Dolan and Brockett, 2005; 1784 Socher et al., 2011). Phrasal paraphrases are often obtained by running an alignment algorithm over the paraphrased sentence pairs. While prior work uses the temporal aspects of news streams as a coarse filter, it largely relies on text metrics, such as context similarity and edit distance, to make predictions and alignments. These metrics are usually insufficient to produce high precision results; moreover they tend to produce paraphrases that are simple lexical variants (e.g. {go to, go into}.). In contrast, NEWSSPIKE generates paraphrase clusters with both high precision and high div</context>
</contexts>
<marker>Dolan, Brockett, 2005</marker>
<rawString>William B Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of IWP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Computational Linguistics,</booktitle>
<pages>350</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3049" citStr="Dolan et al. (2004)" startWordPosition="445" endWordPosition="448">ibutional hypothesis relies on statistics over large corpora to produce accurate similarity statistics. It remains unclear how to accurately paraphrase less frequent relations with the distributional hypothesis. Another common approach employs the use of parallel corpora. News articles are an interesting target, because there often exist articles from different sources describing the same daily events. This peculiar property allows the use of the temporal assumption, which assumes that phrases in articles published at the same time tend to have similar meanings. For example, the approaches by Dolan et al. (2004) and Barzilay et al. (2003) identify pairs of sentential paraphrases in similar articles that have appeared in the same period of time. While these approaches use temporal information as a coarse filter in the data generation stage, they still largely rely on text metrics in the prediction stage. This not only reduces precision, but also limits the discovery of paraphrases with dissimilar sur1http://demo.patrickpantel.com/demos/ lexsem/paraphrase.htm 1776 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1776–1786, Seattle, Washington, USA, 18-21 Oct</context>
<context position="26836" citStr="Dolan et al. (2004)" startWordPosition="4456" endWordPosition="4459">l. Pairwise: the pairwise model discussed in Section 4.2 and using the same set of features as used System P/R P/R diverse prec rec prec rec Baseline 0.67 1.00 0.53 1.00 Pairwise 0.90 0.60 0.81 0.37 Socher 0.81 0.35 0.68 0.29 NEWSSPIKE 0.92 0.55 0.87 0.31 Table 1: Comparison with methods using parallel news corpora by NEWSSPIKE. To generate output clusters, transitivity is assumed inside the EEC-set. For example, when the pairwise model predicts that (r1, r2) and (r1, r3) are both paraphrases, the resulting cluster is {r1, r2, r3}. Socher: Socher et al. (2011) achieved the best results on the Dolan et al. (2004) dataset, and released their code and models. We used their off-the-shelf predictor to replace the classifier in our Pairwise model. Given sentential paraphrases, aligning relation phrases is natural, because OpenIE has already identified the relation phrases. Table 1 shows precision and recall numbers. It is interesting that the basic model already obtains 0.67 precision overall and 0.53 in the diverse condition. This demonstrates that the EEC-sets generated from the news streams are a promising resource for paraphrasing. Socher’s method performs better, but not as well as Pairwise or NEWSSPI</context>
<context position="36288" citStr="Dolan et al., 2004" startWordPosition="5916" endWordPosition="5919">ent relation discovery, extending a generative model with meta-constraints from lexical, syntactic and discourse regularities. Our work solves a major problem with these approaches, avoiding errors such as confusing synonyms with antonyms and causes with effects. Furthermore, NEWSSPIKE doesn’t require massive statistical evidence as do most approaches based on the distributional hypothesis. Using Parallel Corpora: Comparable and parallel corpora, including news streams and multiple translations of the same story, have been used to generate paraphrases, both sentential (Barzilay and Lee, 2003; Dolan et al., 2004; Shinyama and Sekine, 2003) and phrasal (Barzilay and McKeown, 2001; Shen et al., 2006; Pang et al., 2003). Typical methods first gather relevant articles and then pair sentences that are potential paraphrases. Given a training set of paraphrases, models are learned and applied to unlabeled pairs (Dolan and Brockett, 2005; 1784 Socher et al., 2011). Phrasal paraphrases are often obtained by running an alignment algorithm over the paraphrased sentence pairs. While prior work uses the temporal aspects of news streams as a coarse filter, it largely relies on text metrics, such as context similar</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Computational Linguistics, page 350. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In EMNLP. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="6555" citStr="Fader et al., 2011" startWordPosition="987" endWordPosition="990">oral features and constraints are developed based on our temporal correspondence heuristics and encoded into a joint inference model. The model finally creates the paraphrase clusters by predicting the relation phrases that describe the EEC. plies that using an argument pair and a time-stamp could be an effective way to identify an event (e.g. (Armstrong, Livestrong, Oct 17) for the previous question). Based on this observation, this paper introduces a novel mechanism to paraphrase relations as summarized in Figure 1. NEWSSPIKE first applies the ReVerb open information extraction (IE) system (Fader et al., 2011) on the news streams to obtain a set of (a1, r, a2, t) tuples, where the az are the arguments, r is a relation phrase, and t is the time-stamp of the corresponding news article. When (a1, a2, t) suggests a real word event, the relation r of (a1, r, a2, t) is likely to describe that event (e.g. (Armstrong, resign from, Livestrong, Oct 17). We call every (a1, a2, t) an extracted event candidate (EEC), and every relation describing the event an event-mention. For each EEC (a1, a2, t), suppose there are m extraction tuples (a1, r1, a2, t) ... (a1, rm, a2, t) sharing the values of a1, a2, and t. We</context>
<context position="23248" citStr="Fader et al., 2011" startWordPosition="3859" endWordPosition="3862">on than the distributional hypothesis? 5.1 Experimental Setup Since we were unable to find any elaborate timestamped, parallel, news corpus, we collected data using the following procedure: • Collect RSS news seeds, which contain the title, time-stamp, and abstract of the news items. • Use these titles to query the Bing news search engine API and collect additional time-stamped news articles. • Strip HTML tags from the news articles using Boilerpipe (Kohlsch¨utter et al., 2010); keep only the title and first paragraph of each article. • Extract shallow relation tuples using the OpenIE system (Fader et al., 2011). We performed these steps every day from January 1 to February 22, 2013. In total, we collected 546,713 news articles, for which 2.6 million extractions had 529 thousand unique relations. We used several types of features for paraphrasing: 1) spike features obtained from time series; 2) tense features, such as whether two relation phrases are both in the present tense; 3) cause-effect features, such as whether two relation phrases often appear successively in the news articles; 4) text features, such as whether sentences are similar; S) syntactic features, such as whether a relation phrase ap</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In EMNLP. Association for Computational Linguistics, July 27-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Luke Zettlemoyer</author>
<author>Oren Etzioni</author>
</authors>
<title>Paraphrase-driven learning for open question answering.</title>
<date>2013</date>
<booktitle>In ACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1404" citStr="Fader et al., 2013" startWordPosition="194" endWordPosition="197">istics in a probabilistic graphical model to create the NEWSSPIKE algorithm for mining news streams. We present experiments demonstrating that NEWSSPIKE significantly outperforms several competitive baselines. In order to spur further research, we provide a large annotated corpus of timestamped news articles as well as the paraphrases produced by NEWSSPIKE. 1 Introduction Paraphrasing, the task of finding sets of semantically equivalent surface forms, is crucial to many natural language processing applications, including relation extraction (Bhagat and Ravichandran, 2008), question answering (Fader et al., 2013), summarization (Barzilay et al., 1999) and machine translation (Callison-Burch et al., 2006). While the benefits of paraphrasing have been demonstrated, creating a large-scale corpus of high precision paraphrases remains a challenge — especially for event relations. Many researchers have considered generating paraphrases by mining the Web guided by the distributional hypothesis, which states that words occurring in similar contexts tend to have similar meanings (Harris, 1954). For example, DIRT (Lin and Pantel, 2001) and Resolver (Yates and Etzioni, 2009) identify synonymous relation phrases </context>
</contexts>
<marker>Fader, Zettlemoyer, Etzioni, 2013</marker>
<rawString>Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2013. Paraphrase-driven learning for open question answering. In ACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>One sense per discourse.</title>
<date>1992</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language,</booktitle>
<pages>233--237</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11803" citStr="Gale et al., 1992" startWordPosition="1888" endWordPosition="1891">pike in the time series of its entities az, or argument pair (a1, a2), on day t. However, even if we have selected a good EEC for paraphrasing, it is likely that it contains a few relation phrases that are related to (but not synonymous with) the other relations included in the EEC. For example, it’s likely that the news story reporting “Armstrong steps down from Livestrong.” might also mention “Armstrong is the founder of Livestrong.” and so both “steps down from” and “is the founder of” relation phrases would be part of the same EEC-set. Inspired by the idea of one sense per discourse from (Gale et al., 1992), we propose: One Event-Mention Per Discourse Heuristic. A news article tends not to state the same fact more than once. The one event-mention per discourse heuristic is proposed in order to gain precision at the expense of recall — the heuristic directs an algorithm to choose, from a news story, the single “best” relation phrase connecting a pair of two entities. Of course, this doesn’t answer the question of deciding which phrase is “best.” In Section 4.3, we describe how to learn a probabilistic graphical model which does exactly this. 1778 4 Exploiting the Temporal Heuristics In this secti</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William A Gale, Kenneth W Church, and David Yarowsky. 1992. One sense per discourse. In Proceedings of the workshop on Speech and Natural Language, pages 233–237. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<date>1954</date>
<note>Distributional structure. Word.</note>
<contexts>
<context position="1885" citStr="Harris, 1954" startWordPosition="267" endWordPosition="268">uage processing applications, including relation extraction (Bhagat and Ravichandran, 2008), question answering (Fader et al., 2013), summarization (Barzilay et al., 1999) and machine translation (Callison-Burch et al., 2006). While the benefits of paraphrasing have been demonstrated, creating a large-scale corpus of high precision paraphrases remains a challenge — especially for event relations. Many researchers have considered generating paraphrases by mining the Web guided by the distributional hypothesis, which states that words occurring in similar contexts tend to have similar meanings (Harris, 1954). For example, DIRT (Lin and Pantel, 2001) and Resolver (Yates and Etzioni, 2009) identify synonymous relation phrases by the distributions of their arguments. However, the distributional hypothesis has several drawbacks. First, it can confuse antonyms with synonyms because antonymous phrases appear in similar contexts as often as synonymous phrases. For the same reasons, it also often confuses causes with effects. For example, DIRT reports that the closest phrase to fall is rise, and the closest phrase to shoot is kill.1 Second, the distributional hypothesis relies on statistics over large co</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig S Harris. 1954. Distributional structure. Word.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledgebased weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In ACL-HLT,</booktitle>
<pages>541--550</pages>
<contexts>
<context position="21605" citStr="Hoffmann et al., 2011" startWordPosition="3591" endWordPosition="3594">old Φj (yi , xi) − Ep(zi,yi|,xi,Θ)Φj(yi, xi) where Φj(yi, xi) = Oj(X, Y, x), the sum of values for the jth feature in the ith model; and values of X, Y come from the assignment yi. For features in event factors, the partial derivative is derived similarly as gold �j Oj(zi , xi) − Ep(zi,yi|,xi,Θ) Oj (zi, xi) It is unclear how to efficiently compute the expectations in the above formula, a brute force approach requires enumerating all assignments of yi, which is exponentially large with the number of relations. Instead, we opt to use a more tractable perceptron learning approach (Collins, 2002; Hoffmann et al., 2011). Instead of computing the expectations, we simply compute Oj(z∗i , xi) and Φj(y∗i , xi), where z∗i , y∗i is the assignment with the highest probability, generated by the MAP inference algorithm using the current weight vector. The weight updates are the following: Φj(ygold i , xi) − Φj(y∗i ,xi) (1) Oj(zgold i , xi) − Oj(z∗i , xi) (2) The updates can be intuitively explained as penalties on errors. In sum, our learning algorithm consists of iterating the following two steps: (1) infer the most probable assignment given the current weights; (2) update the weights by comparing inferred assignmen</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011. Knowledgebased weak supervision for information extraction of overlapping relations. In ACL-HLT, pages 541–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Kohlsch¨utter</author>
<author>Peter Fankhauser</author>
<author>Wolfgang Nejdl</author>
</authors>
<title>Boilerplate detection using shallow text features.</title>
<date>2010</date>
<booktitle>In WSDM,</booktitle>
<pages>441--450</pages>
<publisher>ACM.</publisher>
<marker>Kohlsch¨utter, Fankhauser, Nejdl, 2010</marker>
<rawString>Christian Kohlsch¨utter, Peter Fankhauser, and Wolfgang Nejdl. 2010. Boilerplate detection using shallow text features. In WSDM, pages 441–450. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question-answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="1927" citStr="Lin and Pantel, 2001" startWordPosition="272" endWordPosition="275">uding relation extraction (Bhagat and Ravichandran, 2008), question answering (Fader et al., 2013), summarization (Barzilay et al., 1999) and machine translation (Callison-Burch et al., 2006). While the benefits of paraphrasing have been demonstrated, creating a large-scale corpus of high precision paraphrases remains a challenge — especially for event relations. Many researchers have considered generating paraphrases by mining the Web guided by the distributional hypothesis, which states that words occurring in similar contexts tend to have similar meanings (Harris, 1954). For example, DIRT (Lin and Pantel, 2001) and Resolver (Yates and Etzioni, 2009) identify synonymous relation phrases by the distributions of their arguments. However, the distributional hypothesis has several drawbacks. First, it can confuse antonyms with synonyms because antonymous phrases appear in similar contexts as often as synonymous phrases. For the same reasons, it also often confuses causes with effects. For example, DIRT reports that the closest phrase to fall is rise, and the closest phrase to shoot is kill.1 Second, the distributional hypothesis relies on statistics over large corpora to produce accurate similarity stati</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question-answering. Natural Language Engineering, 7(4):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lin</author>
<author>Oren Etzioni</author>
</authors>
<title>No noun phrase left behind: detecting and typing unlinkable entities.</title>
<date>2012</date>
<booktitle>In EMNLP,</booktitle>
<pages>893--903</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Lin, Etzioni, 2012</marker>
<rawString>Thomas Lin, Oren Etzioni, et al. 2012. No noun phrase left behind: detecting and typing unlinkable entities. In EMNLP, pages 893–903. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: A survey of data-driven methods.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="34740" citStr="Madnani and Dorr, 2010" startWordPosition="5692" endWordPosition="5695"> paper, in order to fairly compare with the distributional hypothesis, we purposely forced NEWSSPIKE not to rely on any distributional similarity. But NEWSSPIKE’s graphical model has the flexibility to incorporate any similarity metrics as features. Such a hybrid model has great potential to increase both precision and recall, which is one goal for future work. 6 Related Work The vast majority of paraphrasing work falls into two categories: approaches based on the distributional hypothesis or those exploiting on correspondences between parallel corpora (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010). Using Distribution Similarity: Lin and Pantel’s (2001) DIRT employ mutual information statistics to compute the similarity between relations represented in dependency paths. Resolver (Yates and Etzioni, 2009) introduces a new similarity metric called the Extracted Shared Property (ESP) and uses a probabilistic model to merge ESP with surface string similarity. Identifying the semantic equivalence of relation phrases is also called relation discovery or unsupervised semantic parsing. Often techniques don’t compute the similarity explicitly but rely implicitly on the distributional hypothesis.</context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>Nitin Madnani and Bonnie J Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of data-driven methods. Computational Linguistics, 36(3):341–387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences.</title>
<date>2003</date>
<booktitle>In NAACL,</booktitle>
<pages>102--109</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="36395" citStr="Pang et al., 2003" startWordPosition="5934" endWordPosition="5937">ourse regularities. Our work solves a major problem with these approaches, avoiding errors such as confusing synonyms with antonyms and causes with effects. Furthermore, NEWSSPIKE doesn’t require massive statistical evidence as do most approaches based on the distributional hypothesis. Using Parallel Corpora: Comparable and parallel corpora, including news streams and multiple translations of the same story, have been used to generate paraphrases, both sentential (Barzilay and Lee, 2003; Dolan et al., 2004; Shinyama and Sekine, 2003) and phrasal (Barzilay and McKeown, 2001; Shen et al., 2006; Pang et al., 2003). Typical methods first gather relevant articles and then pair sentences that are potential paraphrases. Given a training set of paraphrases, models are learned and applied to unlabeled pairs (Dolan and Brockett, 2005; 1784 Socher et al., 2011). Phrasal paraphrases are often obtained by running an alignment algorithm over the paraphrased sentence pairs. While prior work uses the temporal aspects of news streams as a coarse filter, it largely relies on text metrics, such as context similarity and edit distance, to make predictions and alignments. These metrics are usually insufficient to produc</context>
</contexts>
<marker>Pang, Knight, Marcu, 2003</marker>
<rawString>Bo Pang, Kevin Knight, and Daniel Marcu. 2003. Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences. In NAACL, pages 102–109. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>1--10</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In EMNLP, pages 1–10. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Supervised models for coreference resolution. In</title>
<date>2009</date>
<booktitle>EMNLP,</booktitle>
<pages>968--977</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15565" citStr="Rahman and Ng, 2009" startWordPosition="2498" endWordPosition="2501">more than two relations. A common approach to overcome the drawbacks of the pairwise model and to combine heuristics together is to introduce a joint cluster model, in which heuristics are encoded as features and constraints. Data, instead of ad-hoc rules, determines the relevance of different insights, which can be learned as parameters. The advantage of the joint model is analogous to that of cluster-based approaches for coreference resolution (CR). In particular, a joint model can better capture constraints on multiple variables and can yield higher quality results than pairwise CR models (Rahman and Ng, 2009). We propose an undirected graphical model, NEWSSPIKE, which jointly clusters relations. Constraints are captured by factors connecting multiple random variables. We introduce random variables, the factors, the objective function, the inference algorithm, and the learning algorithm in the following sections. Figure 2 shows an example model for EEC (Armstrong, Livestrong, Oct 17). 4.3.1 Random Variables For the EEC-set (a1, a2, t, {r1,... r,,,}), we introduce one event variable and m relation variables, all boolean valued. The event variable Z(a1,a2,t) indiy be founder of y give speech at 0 y s</context>
</contexts>
<marker>Rahman, Ng, 2009</marker>
<rawString>Altaf Rahman and Vincent Ng. 2009. Supervised models for coreference resolution. In EMNLP, pages 968– 977. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Matthew Can</author>
<author>Dan Jurafsky</author>
</authors>
<title>Same referent, different words: Unsupervised mining of opaque coreferent mentions.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>897--906</pages>
<contexts>
<context position="37655" citStr="Recasens et al. (2013)" startWordPosition="6136" endWordPosition="6139">y tend to produce paraphrases that are simple lexical variants (e.g. {go to, go into}.). In contrast, NEWSSPIKE generates paraphrase clusters with both high precision and high diversity. Others: Textual entailment (Dagan et al., 2009), which finds a phrase implying another phrase, is closely related to the paraphrasing task. Berant et al. (2011) notes the flaws in distributional similarity and proposes local entailment classifiers, which are able to combine many features. Lin et al. (2012) also uses temporal information to detect the semantics of entities. In a manner similar to our approach, Recasens et al. (2013) mines parallel news stories to find opaque coreferent mentions. 7 Conclusion Paraphrasing event relations is crucial to many natural language processing applications, including relation extraction, question answering, summarization, and machine translation. Unfortunately, previous approaches based on distribution similarity and parallel corpora, often produce low precision clusters. This paper introduces three Temporal Correspondence Heuristics that characterize semantically equivalent phrases in news streams. We present a novel algorithm, NEWSSPIKE, based on a probabilistic graphical model e</context>
</contexts>
<marker>Recasens, Can, Jurafsky, 2013</marker>
<rawString>Marta Recasens, Matthew Can, and Dan Jurafsky. 2013. Same referent, different words: Unsupervised mining of opaque coreferent mentions. In Proceedings of NAACL-HLT, pages 897–906.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<title>The New York Times annotated corpus. Linguistic Data Consortium.</title>
<date>2008</date>
<contexts>
<context position="29752" citStr="Sandhaus, 2008" startWordPosition="4907" endWordPosition="4908">f (a1, r, a2) as the input and creates a set of relation clusters as the output paraphrases. Resolver also produces argument clusters, but this paper only evaluates relation clustering. We evaluated Resolver’s performance with an input of the 2.6 million extractions described in section 5.1, using Resolver’s default parameters. ResolverNYT: Since Resolver is supposed to perform better when given more accurate statistics from a larger corpus, we tried giving it more data. Specifically, we ran ReVerb on 1.8 million NY Times articles published between 1987 and 2007 obtain 60 million extractions (Sandhaus, 2008). We ran Resolver on the union of this and our standard test set, but report performance only on clusters whose relations were seen in our news stream. System all diverse prec #rels prec #rels Resolver 0.78 129 0.65 57 ResolverNyt 0.64 1461 0.52 841 ResolverNytTop 0.83 207 0.72 79 Cosine 0.65 17 0.33 9 CosineNyt 0.56 73 0.46 59 NEWSSPIKE 0.93 24843 0.87 5574 Table 2: Comparison with methods using the distributional hypothesis ResolverNytTop: Resolver is designed to achieve good performance on its top results. We thus ranked the ResolverNYT outputs by their scores and report the precision of th</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus. 2008. The New York Times annotated corpus. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siwei Shen</author>
<author>Dragomir R Radev</author>
<author>Agam Patel</author>
<author>G¨unes¸ Erkan</author>
</authors>
<title>Adding syntax to dynamic programming for aligning comparable texts for the generation of paraphrases.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>747--754</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="36375" citStr="Shen et al., 2006" startWordPosition="5930" endWordPosition="5933"> syntactic and discourse regularities. Our work solves a major problem with these approaches, avoiding errors such as confusing synonyms with antonyms and causes with effects. Furthermore, NEWSSPIKE doesn’t require massive statistical evidence as do most approaches based on the distributional hypothesis. Using Parallel Corpora: Comparable and parallel corpora, including news streams and multiple translations of the same story, have been used to generate paraphrases, both sentential (Barzilay and Lee, 2003; Dolan et al., 2004; Shinyama and Sekine, 2003) and phrasal (Barzilay and McKeown, 2001; Shen et al., 2006; Pang et al., 2003). Typical methods first gather relevant articles and then pair sentences that are potential paraphrases. Given a training set of paraphrases, models are learned and applied to unlabeled pairs (Dolan and Brockett, 2005; 1784 Socher et al., 2011). Phrasal paraphrases are often obtained by running an alignment algorithm over the paraphrased sentence pairs. While prior work uses the temporal aspects of news streams as a coarse filter, it largely relies on text metrics, such as context similarity and edit distance, to make predictions and alignments. These metrics are usually in</context>
</contexts>
<marker>Shen, Radev, Patel, Erkan, 2006</marker>
<rawString>Siwei Shen, Dragomir R Radev, Agam Patel, and G¨unes¸ Erkan. 2006. Adding syntax to dynamic programming for aligning comparable texts for the generation of paraphrases. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 747–754. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Paraphrase acquisition for information extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the second international workshop on Paraphrasing-Volume 16,</booktitle>
<pages>65--71</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="36316" citStr="Shinyama and Sekine, 2003" startWordPosition="5920" endWordPosition="5923">ry, extending a generative model with meta-constraints from lexical, syntactic and discourse regularities. Our work solves a major problem with these approaches, avoiding errors such as confusing synonyms with antonyms and causes with effects. Furthermore, NEWSSPIKE doesn’t require massive statistical evidence as do most approaches based on the distributional hypothesis. Using Parallel Corpora: Comparable and parallel corpora, including news streams and multiple translations of the same story, have been used to generate paraphrases, both sentential (Barzilay and Lee, 2003; Dolan et al., 2004; Shinyama and Sekine, 2003) and phrasal (Barzilay and McKeown, 2001; Shen et al., 2006; Pang et al., 2003). Typical methods first gather relevant articles and then pair sentences that are potential paraphrases. Given a training set of paraphrases, models are learned and applied to unlabeled pairs (Dolan and Brockett, 2005; 1784 Socher et al., 2011). Phrasal paraphrases are often obtained by running an alignment algorithm over the paraphrased sentence pairs. While prior work uses the temporal aspects of news streams as a coarse filter, it largely relies on text metrics, such as context similarity and edit distance, to ma</context>
</contexts>
<marker>Shinyama, Sekine, 2003</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2003. Paraphrase acquisition for information extraction. In Proceedings of the second international workshop on Paraphrasing-Volume 16, pages 65–71. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<journal>NIPS,</journal>
<pages>24--801</pages>
<contexts>
<context position="14203" citStr="Socher et al., 2011" startWordPosition="2278" endWordPosition="2281">rs are then generated according to some heuristic rules (e.g. assuming transitivity among synonyms). The tenses of the relations and time series of the arguments are encoded as features, which we call tense features and spike features respectively. An example tense feature is whether one relation is past tense while the other relation is present tense; an example spike feature is the covariance of the time series. The pairwise model can be considered similar to paraphrasing techniques which examine two sentences and determine whether they are semantically equivalent (Dolan and Brockett, 2005; Socher et al., 2011). Unfortunately, these techniques often based purely on text metrics and does not consider any temporal attributes. In section 5, we evaluate the effect of applying these techniques. Figure 2: an example model for EEC (Armstrong, Livestrong, Oct 17). Y and Z are binary random variables. 4)Y, 4)Z and 6oint are factors. be founder of and step down come from article 1 while give speech at, be chairman of and resign from come from article 2. 4.3 Joint Cluster Model The pairwise model has several drawbacks: 1) it lacks the ability to handle constraints, such as the mutual exclusion constraint impli</context>
<context position="26783" citStr="Socher et al. (2011)" startWordPosition="4445" endWordPosition="4448">any training, and generates outputs with perfect recall. Pairwise: the pairwise model discussed in Section 4.2 and using the same set of features as used System P/R P/R diverse prec rec prec rec Baseline 0.67 1.00 0.53 1.00 Pairwise 0.90 0.60 0.81 0.37 Socher 0.81 0.35 0.68 0.29 NEWSSPIKE 0.92 0.55 0.87 0.31 Table 1: Comparison with methods using parallel news corpora by NEWSSPIKE. To generate output clusters, transitivity is assumed inside the EEC-set. For example, when the pairwise model predicts that (r1, r2) and (r1, r3) are both paraphrases, the resulting cluster is {r1, r2, r3}. Socher: Socher et al. (2011) achieved the best results on the Dolan et al. (2004) dataset, and released their code and models. We used their off-the-shelf predictor to replace the classifier in our Pairwise model. Given sentential paraphrases, aligning relation phrases is natural, because OpenIE has already identified the relation phrases. Table 1 shows precision and recall numbers. It is interesting that the basic model already obtains 0.67 precision overall and 0.53 in the diverse condition. This demonstrates that the EEC-sets generated from the news streams are a promising resource for paraphrasing. Socher’s method pe</context>
<context position="36639" citStr="Socher et al., 2011" startWordPosition="5975" endWordPosition="5978">ches based on the distributional hypothesis. Using Parallel Corpora: Comparable and parallel corpora, including news streams and multiple translations of the same story, have been used to generate paraphrases, both sentential (Barzilay and Lee, 2003; Dolan et al., 2004; Shinyama and Sekine, 2003) and phrasal (Barzilay and McKeown, 2001; Shen et al., 2006; Pang et al., 2003). Typical methods first gather relevant articles and then pair sentences that are potential paraphrases. Given a training set of paraphrases, models are learned and applied to unlabeled pairs (Dolan and Brockett, 2005; 1784 Socher et al., 2011). Phrasal paraphrases are often obtained by running an alignment algorithm over the paraphrased sentence pairs. While prior work uses the temporal aspects of news streams as a coarse filter, it largely relies on text metrics, such as context similarity and edit distance, to make predictions and alignments. These metrics are usually insufficient to produce high precision results; moreover they tend to produce paraphrases that are simple lexical variants (e.g. {go to, go into}.). In contrast, NEWSSPIKE generates paraphrase clusters with both high precision and high diversity. Others: Textual ent</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H Huang, Jeffrey Pennington, Andrew Y Ng, and Christopher D Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. NIPS, 24:801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihalis Yannakakis</author>
</authors>
<title>On the approximation of maximum satisfiability.</title>
<date>1992</date>
<booktitle>In Proceedings of the third annual ACM-SIAM symposium on Discrete algorithms, SODA ’92,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="19863" citStr="Yannakakis, 1992" startWordPosition="3261" endWordPosition="3262">1 0 if Ey&apos;Eyd yr &gt; 1 1 otherwise 4.3.3 Maximum a Posteriori Inference The goal of inference is to find the predictions z, y which yield the greatest probability, i.e., p(Z = z,Y = y|x;Θ) This can be viewed as a MAP inference problem. In general, inference in a graphical model is challenging. Fortunately, the joint factors in our model are linear, and the event and relation factors are loglinear; we can cast MAP inference as an integer linear programming (ILP) problem, and then compute an approximation in polynomial time by means of linear programming using randomized rounding, as proposed in (Yannakakis, 1992). We build one ILP problem for every EEC. The variables of the ILP are Z and Y, which only take values of 0 or 1. The objective function is the sum of logs of the event and relation factors Φ� and ΦY . The temporal burstiness heuristic of Φjoint is encoded as a linear inequality constraint z &gt; yi; the one-mention per discourse heuristic of Φjoint is encoded as the constraint EyzEyd yi &lt; 1. Φjoint(z, yd, x) def= z*, y* = arg max z,y 1780 4.3.4 Learning Our training data consists a set of N = 500 labeled EEC-sets each in the form of {(Ri, Rgold i ) |N i=1 }. Each R is the set of all relations in</context>
</contexts>
<marker>Yannakakis, 1992</marker>
<rawString>Mihalis Yannakakis. 1992. On the approximation of maximum satisfiability. In Proceedings of the third annual ACM-SIAM symposium on Discrete algorithms, SODA ’92, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Aria Haghighi</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Structured relation discovery using generative models.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>1456--1466</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="35509" citStr="Yao et al. (2011" startWordPosition="5803" endWordPosition="5806">d in dependency paths. Resolver (Yates and Etzioni, 2009) introduces a new similarity metric called the Extracted Shared Property (ESP) and uses a probabilistic model to merge ESP with surface string similarity. Identifying the semantic equivalence of relation phrases is also called relation discovery or unsupervised semantic parsing. Often techniques don’t compute the similarity explicitly but rely implicitly on the distributional hypothesis. Poon and Domingos’ (2009) USP clusters relations represented with fragments of dependency trees by repeatedly merging relations having similar context. Yao et al. (2011; 2012) introduces generative models for relation discovery using LDA-style algorithm over a relation-feature matrix. Chen et al. (2011) focuses on domain-dependent relation discovery, extending a generative model with meta-constraints from lexical, syntactic and discourse regularities. Our work solves a major problem with these approaches, avoiding errors such as confusing synonyms with antonyms and causes with effects. Furthermore, NEWSSPIKE doesn’t require massive statistical evidence as do most approaches based on the distributional hypothesis. Using Parallel Corpora: Comparable and parall</context>
</contexts>
<marker>Yao, Haghighi, Riedel, McCallum, 2011</marker>
<rawString>Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew McCallum. 2011. Structured relation discovery using generative models. In EMNLP, pages 1456–1466. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Unsupervised relation discovery with sense disambiguation.</title>
<date>2012</date>
<booktitle>In ACL,</booktitle>
<pages>712--720</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Yao, Riedel, McCallum, 2012</marker>
<rawString>Limin Yao, Sebastian Riedel, and Andrew McCallum. 2012. Unsupervised relation discovery with sense disambiguation. In ACL, pages 712–720. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Oren Etzioni</author>
</authors>
<title>Unsupervised methods for determining object and relation synonyms on the web.</title>
<date>2009</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="1966" citStr="Yates and Etzioni, 2009" startWordPosition="278" endWordPosition="281">d Ravichandran, 2008), question answering (Fader et al., 2013), summarization (Barzilay et al., 1999) and machine translation (Callison-Burch et al., 2006). While the benefits of paraphrasing have been demonstrated, creating a large-scale corpus of high precision paraphrases remains a challenge — especially for event relations. Many researchers have considered generating paraphrases by mining the Web guided by the distributional hypothesis, which states that words occurring in similar contexts tend to have similar meanings (Harris, 1954). For example, DIRT (Lin and Pantel, 2001) and Resolver (Yates and Etzioni, 2009) identify synonymous relation phrases by the distributions of their arguments. However, the distributional hypothesis has several drawbacks. First, it can confuse antonyms with synonyms because antonymous phrases appear in similar contexts as often as synonymous phrases. For the same reasons, it also often confuses causes with effects. For example, DIRT reports that the closest phrase to fall is rise, and the closest phrase to shoot is kill.1 Second, the distributional hypothesis relies on statistics over large corpora to produce accurate similarity statistics. It remains unclear how to accura</context>
<context position="29091" citStr="Yates and Etzioni, 2009" startWordPosition="4797" endWordPosition="4800"> diverse cases for NewsSpike, w/oSpike, w/oTense and w/oDiscourse. the curves because they are drawn after sorting the predictions by the value of the corresponding ILP objective functions, which do not perfectly reflect prediction accuracy. However, it is clear that NEWSSPIKE produces greater precision over all ranges of recall. 5.3 Comparison with Methods using the Distributional Hypothesis We evaluated our model against methods based on the distributional hypothesis. We ran NEWSSPIKE over all EEC-sets except for the development set and compared to the following systems: Resolver: Resolver (Yates and Etzioni, 2009) uses a set of extraction tuples in the form of (a1, r, a2) as the input and creates a set of relation clusters as the output paraphrases. Resolver also produces argument clusters, but this paper only evaluates relation clustering. We evaluated Resolver’s performance with an input of the 2.6 million extractions described in section 5.1, using Resolver’s default parameters. ResolverNYT: Since Resolver is supposed to perform better when given more accurate statistics from a larger corpus, we tried giving it more data. Specifically, we ran ReVerb on 1.8 million NY Times articles published between</context>
<context position="34950" citStr="Yates and Etzioni, 2009" startWordPosition="5722" endWordPosition="5725">rate any similarity metrics as features. Such a hybrid model has great potential to increase both precision and recall, which is one goal for future work. 6 Related Work The vast majority of paraphrasing work falls into two categories: approaches based on the distributional hypothesis or those exploiting on correspondences between parallel corpora (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010). Using Distribution Similarity: Lin and Pantel’s (2001) DIRT employ mutual information statistics to compute the similarity between relations represented in dependency paths. Resolver (Yates and Etzioni, 2009) introduces a new similarity metric called the Extracted Shared Property (ESP) and uses a probabilistic model to merge ESP with surface string similarity. Identifying the semantic equivalence of relation phrases is also called relation discovery or unsupervised semantic parsing. Often techniques don’t compute the similarity explicitly but rely implicitly on the distributional hypothesis. Poon and Domingos’ (2009) USP clusters relations represented with fragments of dependency trees by repeatedly merging relations having similar context. Yao et al. (2011; 2012) introduces generative models for </context>
</contexts>
<marker>Yates, Etzioni, 2009</marker>
<rawString>Alexander Yates and Oren Etzioni. 2009. Unsupervised methods for determining object and relation synonyms on the web. Journal ofArtificial Intelligence Research, 34(1):255.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
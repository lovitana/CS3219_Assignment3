<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003507">
<title confidence="0.978533">
Shift-Reduce Word Reordering for Machine Translation
</title>
<author confidence="0.778384">
Katsuhiko Hayashi†, Katsuhito Sudoh, Hajime Tsukada, Jun Suzuki, Masaaki Nagata
</author>
<affiliation confidence="0.639405">
NTT Communication Science Laboratories, NTT Corporation
</affiliation>
<address confidence="0.827523">
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
</address>
<email confidence="0.998173">
†hayashi.katsuhiko@lab.ntt.co.jp
</email>
<sectionHeader confidence="0.994778" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999913272727273">
This paper presents a novel word reordering
model that employs a shift-reduce parser for
inversion transduction grammars. Our model
uses rich syntax parsing features for word re-
ordering and runs in linear time. We apply it to
postordering of phrase-based machine trans-
lation (PBMT) for Japanese-to-English patent
tasks. Our experimental results show that our
method achieves a significant improvement
of +3.1 BLEU scores against 30.15 BLEU
scores of the baseline PBMT system.
</bodyText>
<sectionHeader confidence="0.998427" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958047619047">
Even though phrase-based machine translation
(PBMT) (Koehn et al., 2007) and tree-based MT
(Graehl and Knight, 2004; Chiang, 2005; Galley
et al., 2006) systems have achieved great success,
many problems remain for distinct language pairs,
including long-distant word reordering.
To improve such word reordering, one promis-
ing way is to separate it from the translation pro-
cess as preordering (Collins et al., 2005; DeNero
and Uszkoreit, 2011) or postordering (Sudoh et al.,
2011; Goto et al., 2012). Many studies utilize a rule-
based or a probabilistic model to perform a reorder-
ing decision at each node of a syntactic parse tree.
This paper presents a parser-based word reorder-
ing model that employs a shift-reduce parser for in-
version transduction grammars (ITG) (Wu, 1997).
To the best of our knowledge, this is the first study
on a shift-reduce parser for word reordering.
The parser-based reordering approach uses rich
syntax parsing features for reordering decisions.
Our propoesd method can also easily define such
</bodyText>
<figureCaption confidence="0.999706">
Figure 1: A description of the postordering MT system.
</figureCaption>
<bodyText confidence="0.999828266666667">
non-local features as the N-gram words of reordered
strings. Even when using these non-local features,
the complexity of the shift-reduce parser does not
increase at all due to give up achieving an optimal
solution. Therefore, it works much more efficient.
In our experiments, we apply our proposed
method to postordering for J-to-E patent tasks be-
cause their training data for reordering have little
noise and they are ideal for evaluating reordering
methods. Although our used J-to-E setups need
a language-dependent scheme and we describe our
proposed method as a J-to-E postordering method,
the key algorithm is language-independent and it can
be applicable to preordering as well as postordering
if the training data for reordering are available.
</bodyText>
<sectionHeader confidence="0.44333" genericHeader="method">
2 Postordering by Parsing
</sectionHeader>
<bodyText confidence="0.999955818181818">
As shown in Fig.1, postordering (Sudoh et al., 2011)
has two steps; the first is a translation step that trans-
lates an input sentence into source-ordered transla-
tions. The second is a reordering step in which the
translations are reordered in the target language or-
der. The key to postordering is the second step.
Goto et al. (2012) modeled the second step by
parsing and created training data for a postordering
parser using a language-dependent rule called head-
finalization. The rule moves syntactic heads of a
lexicalized parse tree of an English sentence to the
</bodyText>
<figure confidence="0.997339125">
Source Sen-
tence (J)
Source-
ordered Target
Sentence (HFE)
reordering
Target Sen-
tence (E)
</figure>
<page confidence="0.932144">
1382
</page>
<bodyText confidence="0.4786395">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1382–1386,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</bodyText>
<equation confidence="0.9703935">
S(saw)
VP(saw)
PP(
with
</equation>
<page confidence="0.877945">
1383
</page>
<bodyText confidence="0.9996685">
top element s0, j is an index of the next input word
of the buffer, and π is a set of predictor states1.
Each stack element has at least the following com-
ponents of its partial derivation tree:
</bodyText>
<equation confidence="0.880075">
s = {H, h, wleft, wright, a}
</equation>
<bodyText confidence="0.999934818181818">
where H is a root nonterminal or a part-of-speech tag
of the subtree, h is a head index of H, a is a variable
to which “the” “a/an” “no articles” or null are as-
signed, and wleft, wright are the leftmost and right-
most words of phrase H. When referring to compo-
nent ∗, we use a s.∗ notation.
Our proposed system has 4 actions shift-X, insert-
x, reduce-MR-X and reduce-SR-X.
The shift-X action pushes the next input word
onto the stack and assigns a part-of-speech tag X to
the word. The deduction step is as follows:
</bodyText>
<equation confidence="0.9974058">
X → wj ∈ P
P
z } |{
ℓ : ⟨i, j, S|s′0⟩ : π
ℓ + 1 : ⟨j, j + 1, S|s′ 0|s0)⟩ : {p}
</equation>
<bodyText confidence="0.935784">
where s0 is {X, j, wj, wj, null}.
The insert-x action determines whether to gener-
ate “the” “a/an” or “no articles” (= x):
</bodyText>
<equation confidence="0.928949666666667">
s′0.X ∈ I ∧ (s′0.a ≠ “the” ∧ s′0.a ≠ “a/an”)
ℓ : ⟨i, j, S|s′0)⟩ : π
ℓ + 1 : ⟨i,j,S|s0⟩ : π
</equation>
<bodyText confidence="0.9990715">
where s′0 is {X, h, wleft, wright, a} and s0 is
{X, h, wleft, wright, x} (i ≤ h, left, right &lt; j).
The side condition prevents the parser from inserting
articles into phrase X more than twice. During pars-
ing, articles are not explicitly inserted into the input
string: they are inserted into it when backtracking to
generate a reordered string after parsing.
The reduce-MR-X action has a deduction rule:
</bodyText>
<equation confidence="0.94958837037037">
X → Y Z ∈ P ∧ q ∈ π
q
z } |{
: ⟨k, i,S|s′2|s′1⟩ : π′ ℓ : ⟨i, j,S|s′1|s′0⟩ : π
ℓ + 1 : ⟨k,j,S|s′2|s0⟩ : π′
1Since our notion of predictor states is identical to that in
(Huang and Sagae, 2010), we omit the details here.
s0.wh ◦ s0.th s0.H s0.H ◦ s0.th s0.wh ◦ s0.H
s1.wh ◦ s1.th s1.H s1.H ◦ s1.th s1.wh ◦ s1.H
s2.th ◦ s2.H s2.wh ◦ s2.H q0.w q1.w q2.w
s0.tl ◦ s0.L s0.wl ◦ s0.L s1.tl ◦ s1.L s1.wl ◦ s1.L
s0.wh ◦ s0.H ◦ s1.wh ◦ s1.H s0.H ◦ s1.wh s0.wh ◦ s1.H
s0.H ◦ s1.H s0.wh ◦ s0.H ◦ q0.w s0.H ◦ q0.w
s1.wh ◦ s1.H ◦ q0.w s1.H ◦ q0.w s1.th ◦ q0.w ◦ q1.w
s0.wh ◦ s0.H ◦ s1.H ◦ q0.w s0.H ◦ s1.wh ◦ s1.H ◦ q0.w
s0.H ◦ s1.H ◦ q0.w s0.th ◦ s1.th ◦ q0.w
s0.wh ◦ s1.H ◦ q0.w ◦ q1.w s0.H ◦ q0.w ◦ q1.w
s0.th ◦ q0.w ◦ q1.w s0.wh ◦ s0.H ◦ s1.H ◦ s2.H
s0.H ◦ s1.wh ◦ s1.H ◦ s2.H s0.H ◦ s1.H ◦ s2.wh ◦ s2.H
s0.H ◦ s1.H ◦ s2.H s0.th ◦ s1.th ◦ s2.th
s0.H ◦ s0.R ◦ s0.L s1.H ◦ s1.R ◦ s1.L s0.H ◦ s0.R ◦ q0.w
s0.H ◦ s0.L ◦ s1.H s0.H ◦ s0.L ◦ s1.wh s0.H ◦ s1.H ◦ s1.L
s0.wh ◦ s1.H ◦ s1.R
s0.wleft ◦ s1.wright s0.tleft ◦ s1.tright
s0.wright ◦ s1.wleft s0.tright ◦ s1.tleft
s0.a ◦ s0.wleft s0.a ◦ s0.tleft s0.a ◦ s0.wleft ◦ s1.wright
s0.a ◦ s0.tleft ◦ s1.tright s0.a ◦ s0.wh s0.a ◦ s0.th
</equation>
<tableCaption confidence="0.818053">
Table 1: Feature templates: s.L and s.R denote the left
and right subnodes of s. l and r are head indices of L and
R. q denotes a buffer element. t is a part-of-speech tag.
</tableCaption>
<bodyText confidence="0.994983666666667">
where s′0 is {Z, h0, wleft0, wright0, a0} and s′1 is
{Y, h1, wleft1, wright1, a1}. The action generates s0
by combining s′0 and s′1 with binary rule X→Y Z:
</bodyText>
<equation confidence="0.982412">
s0 = {X, h0, wleft1, wright0, a1}.
</equation>
<bodyText confidence="0.9988897">
New nonterminal X is lexicalized with head word
wh0 of right nonterminal Z. This action expands Y
and Z in a straight order. The leftmost word of
phrase X is set to leftmost word wleft1 of Y, and the
rightmost word of phrase X is set to rightmost word
wright0 of Z. Variable a is set to a1 of Y.
The difference between reduce-MR-X and
reduce-SR-X actions is new stack element s0. The
reduce-SR-X action generates s0 by combining s′0
and s′1 with binary rule X# →Y Z:
</bodyText>
<equation confidence="0.951187">
s0 = {X#, h0, wleft0,wright1,a0}.
</equation>
<bodyText confidence="0.999918888888889">
This action expands Y and Z in a reverse order, and
the leftmost word of X# is set to wleft0 of Z, and the
rightmost word of X# is set to wright1 of Y. Variable
a is set to a0 of Z.
We use a linear model that is discriminatively
trained with the averaged perceptron (Collins and
Roark, 2004). Table 1 shows the feature templates
used in our experiments and we call the features in
the bottom two rows “non-local” features.
</bodyText>
<page confidence="0.926766">
1384
</page>
<table confidence="0.98210025">
train dev test9 test10
# of sent. 3,191,228 2,000 2,000 2,300
ave. leng. (J) 36.4 36.6 37.0 43.1
ave. leng. (E) 33.3 33.3 33.7 39.6
</table>
<tableCaption confidence="0.991757">
Table 2: NTCIR-9 and 10 data statistics.
</tableCaption>
<sectionHeader confidence="0.999065" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998975">
4.1 Experimental Setups
</subsectionHeader>
<bodyText confidence="0.9999862">
We conducted experiments for NTCIR-9 and 10
patent data using a Japanese-English language pair.
Mecab2 was used for the Japanese morphological
analysis. The data are summarized in Table 2.
We used Enju (Miyao and Tsujii, 2008) for pars-
ing the English training data and converted parse
trees into HFE trees by a head-finalization scheme.
We extracted grammar rules from all the HFE trees
and randomly selected 500,000 HFE trees to train
the shift-reduce parser.
We used Moses (Koehn et al., 2007) with lexical-
ized reordering and a 6-gram language model (LM)
trained using SRILM (Stolcke et al., 2011) to trans-
late the Japanese sentences into HFE sentences.
To recover the English sentences, our shift-reduce
parser reordered only the 1-best HFE sentence. Our
strategy is much simpler than Goto et al. (2012)’s
because they used a linear inteporation of MT cost,
parser cost and N-gram LM cost to generate the best
English sentence from the n-best HFE sentences.
</bodyText>
<subsectionHeader confidence="0.997604">
4.2 Main Results
</subsectionHeader>
<bodyText confidence="0.9999985">
The main results in Table 3 indicate our method was
significantly better and faster than the conventional
PBMT system. Our method also ourperformed Goto
et al. (2012)’s reported systems as well as a tree-
based (moses-chart) system3. Our proposed model
with “non-local” features (w/ nf.) achieved gains
against that without the features (w/o nf.). Further
feature engineering may improve the accuracy more.
</bodyText>
<subsectionHeader confidence="0.999275">
4.3 Analysis
</subsectionHeader>
<bodyText confidence="0.999796666666667">
We show N-gram precisions of PBMT (dist=6,
dist=20) and proposed systems in Table 5. The re-
sults clearly show that improvements of 1-gram pre-
</bodyText>
<footnote confidence="0.995497666666667">
2https://code.google.com/p/mecab/
3All the data and the MT toolkits used in our experiments
are the same as theirs.
</footnote>
<table confidence="0.995973166666667">
test9 test10
BLEU RIBES BLEU RIBES
HFE w/ art. 28.86 73.45 29.9 73.52
proposed 32.93 76.68 33.25 76.74
w/o art. 19.86 75.62 20.17 75.63
N-gram 32.15 76.52 32.28 76.46
</table>
<tableCaption confidence="0.995822">
Table 4: The effects of article generation: “w/o art.” de-
notes evaluation scores for translations of the best system
(“proposed”) in Table 3 from which articles are removed.
“HFE w/ art.” system used HFE data with articles and
generated them by MT system and the shift-reduce parser
performed only reordering. “N-gram” system inserted
articles into the translations of “w/o art.” by Goto et al.
(2012)’s article generation method.
</tableCaption>
<table confidence="0.922846">
(1–4)-gram precision
moses (dist=6) 67.1 / 36.9 / 20.7 / 11.5
moses (dist=20) 67.7 / 38.9 / 23.0 / 13.7
proposed 68.9 / 40.6 / 25.7 / 16.7
</table>
<tableCaption confidence="0.78619">
Table 5: N-gram precisions of moses (dist=6, dist=20)
and proposed systems for test9 data.
</tableCaption>
<bodyText confidence="0.999883461538462">
cisions are the main factors that contribute to bet-
ter performance of our proposed system than PBMT
systems. It seems that the gains of 1-gram presicions
come from postediting (article generation).
In table 4, we show the effectiveness of our joint
reordering and postediting approach (“proposed”).
The “w/o art.” results clearly show that generating
articles has great effects on MT evaluations espe-
cially for BLEU metric. Comparing “proposed” and
“HFE w/ art.” systems, these results show that poste-
diting is much more effective than generating arti-
cles by MT. Our joint approach also outperformed
“N-gram” postediting system.
</bodyText>
<sectionHeader confidence="0.992259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999944111111111">
We proposed a shift-reduce word ordering model
and applied it to J-to-E postordering. Our experi-
mental results indicate our method can significantly
improve the performance of a PBMT system.
Future work will investigate our method’s use-
fulness on various language datasets. We plan to
study more general methods that use word align-
ments to embed swap information in trees (Galley
et al., 2006).
</bodyText>
<page confidence="0.960801">
1385
</page>
<table confidence="0.99959375">
BLEU test9 time (sec.) BLEU test10 time (sec.)
RIBES RIBES
PBMT (dist=6) 27.1 67.76 2.66 27.92 68.13 3.18
PBMT (dist=12) 29.55 69.84 4.15 30.03 69.88 4.93
PBMT (dist=20) 29.98 69.87 6.22 30.15 69.43 7.19
Tree-based MT** (Goto et al., 2012) 29.53 69.22 – – –
PBMT (dist=20)** (Goto et al., 2012) 30.13 68.86 – – –
Goto et al. (2012)** 31.75 72.57 – – – –
PBMT (dist=0) + proposed w/o nf. (beam=12) 32.59 76.35 1.46 + 0.01 32.83 76.44 1.7 + 0.01
PBMT (dist=0) + proposed w/o nf. (beam=48) 32.61 76.58 1.46 + 0.06 32.86 76.6 1.7 + 0.06
PBMT (dist=0) + proposed w/ nf. (beam=12) 32.91 76.38 1.46 + 0.01 33.15 76.53 1.7 + 0.02
PBMT (dist=0) + proposed w/ nf. (beam=48) 32.93 76.68 1.46 + 0.07 33.25 76.74 1.7 + 0.07
</table>
<tableCaption confidence="0.999796">
Table 3: System comparison: time represents the average second per sentence. ** denotes “not our experiments”.
</tableCaption>
<sectionHeader confidence="0.982581" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999655371428571">
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263–270.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, page 111.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 531–540.
John DeNero and Jakob Uszkoreit. 2011. Inducing sen-
tence structure from parallel corpora for reordering. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 193–203.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, pages
961–968.
Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012.
Post-ordering by parsing for japanese-english statisti-
cal machine translation. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics, pages 311–316.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proc. HLT-NAACL, pages 105–112.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1077–1086.
Hideki Isozaki, Jun Suzuki, Hajime Tsukada, Masaaki
Nagata, Sho Hoshino, and Yusuke Miyao. 2012.
HPSG-based preprocessing for English-to-Japanese
translation. ACM Transactions on Asian Language In-
formation Processing (TALIP), 11(3).
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In Proceedings of the
National Conference on Artificial Intelligence, pages
779–779.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al. 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proceedings of the 45th
Annual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 177–180.
Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Computational
Linguistics, 34(1):35–80.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human language tech-
nologies 2007: the conference of the North American
chapter of the Association for Computational Linguis-
tics, pages 404–411.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. Srilm at sixteen: Update and outlook.
In Proceedings of IEEE Automatic Speech Recognition
and Understanding Workshop.
Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Post-ordering in
statistical machine translation. In Proc. MT Summit.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
</reference>
<page confidence="0.99323">
1386
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.918350">
<title confidence="0.999834">Shift-Reduce Word Reordering for Machine Translation</title>
<author confidence="0.988927">Katsuhito Sudoh</author>
<author confidence="0.988927">Hajime Tsukada</author>
<author confidence="0.988927">Jun Suzuki</author>
<author confidence="0.988927">Masaaki</author>
<affiliation confidence="0.982952">NTT Communication Science Laboratories, NTT</affiliation>
<address confidence="0.949392">2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237</address>
<abstract confidence="0.998829333333333">This paper presents a novel word reordering model that employs a shift-reduce parser for inversion transduction grammars. Our model uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement scores against scores of the baseline PBMT system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="911" citStr="Chiang, 2005" startWordPosition="122" endWordPosition="123">er presents a novel word reordering model that employs a shift-reduce parser for inversion transduction grammars. Our model uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 1 Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for in</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>111</pages>
<contexts>
<context position="7282" citStr="Collins and Roark, 2004" startWordPosition="1315" endWordPosition="1318">tmost word wleft1 of Y, and the rightmost word of phrase X is set to rightmost word wright0 of Z. Variable a is set to a1 of Y. The difference between reduce-MR-X and reduce-SR-X actions is new stack element s0. The reduce-SR-X action generates s0 by combining s′0 and s′1 with binary rule X# →Y Z: s0 = {X#, h0, wleft0,wright1,a0}. This action expands Y and Z in a reverse order, and the leftmost word of X# is set to wleft0 of Z, and the rightmost word of X# is set to wright1 of Y. Variable a is set to a0 of Z. We use a linear model that is discriminatively trained with the averaged perceptron (Collins and Roark, 2004). Table 1 shows the feature templates used in our experiments and we call the features in the bottom two rows “non-local” features. 1384 train dev test9 test10 # of sent. 3,191,228 2,000 2,000 2,300 ave. leng. (J) 36.4 36.6 37.0 43.1 ave. leng. (E) 33.3 33.3 33.7 39.6 Table 2: NTCIR-9 and 10 data statistics. 4 Experiments 4.1 Experimental Setups We conducted experiments for NTCIR-9 and 10 patent data using a Japanese-English language pair. Mecab2 was used for the Japanese morphological analysis. The data are summarized in Table 2. We used Enju (Miyao and Tsujii, 2008) for parsing the English t</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>531--540</pages>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 531–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Inducing sentence structure from parallel corpora for reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>193--203</pages>
<contexts>
<context position="1224" citStr="DeNero and Uszkoreit, 2011" startWordPosition="168" endWordPosition="171">patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 1 Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction grammars (ITG) (Wu, 1997). To the best of our knowledge, this is the first study on a shift-reduce parser for word reordering. The parser-based reordering approach uses rich syntax parsing features for reordering decisions. Our propoesd method can also easily define such Figure 1: A descript</context>
</contexts>
<marker>DeNero, Uszkoreit, 2011</marker>
<rawString>John DeNero and Jakob Uszkoreit. 2011. Inducing sentence structure from parallel corpora for reordering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 193–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="933" citStr="Galley et al., 2006" startWordPosition="124" endWordPosition="127">novel word reordering model that employs a shift-reduce parser for inversion transduction grammars. Our model uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 1 Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction g</context>
<context position="11146" citStr="Galley et al., 2006" startWordPosition="1941" endWordPosition="1944"> metric. Comparing “proposed” and “HFE w/ art.” systems, these results show that postediting is much more effective than generating articles by MT. Our joint approach also outperformed “N-gram” postediting system. 5 Conclusion We proposed a shift-reduce word ordering model and applied it to J-to-E postordering. Our experimental results indicate our method can significantly improve the performance of a PBMT system. Future work will investigate our method’s usefulness on various language datasets. We plan to study more general methods that use word alignments to embed swap information in trees (Galley et al., 2006). 1385 BLEU test9 time (sec.) BLEU test10 time (sec.) RIBES RIBES PBMT (dist=6) 27.1 67.76 2.66 27.92 68.13 3.18 PBMT (dist=12) 29.55 69.84 4.15 30.03 69.88 4.93 PBMT (dist=20) 29.98 69.87 6.22 30.15 69.43 7.19 Tree-based MT** (Goto et al., 2012) 29.53 69.22 – – – PBMT (dist=20)** (Goto et al., 2012) 30.13 68.86 – – – Goto et al. (2012)** 31.75 72.57 – – – – PBMT (dist=0) + proposed w/o nf. (beam=12) 32.59 76.35 1.46 + 0.01 32.83 76.44 1.7 + 0.01 PBMT (dist=0) + proposed w/o nf. (beam=48) 32.61 76.58 1.46 + 0.06 32.86 76.6 1.7 + 0.06 PBMT (dist=0) + proposed w/ nf. (beam=12) 32.91 76.38 1.46 +</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Masao Utiyama</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Post-ordering by parsing for japanese-english statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--316</pages>
<contexts>
<context position="1280" citStr="Goto et al., 2012" startWordPosition="178" endWordPosition="181">ves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 1 Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction grammars (ITG) (Wu, 1997). To the best of our knowledge, this is the first study on a shift-reduce parser for word reordering. The parser-based reordering approach uses rich syntax parsing features for reordering decisions. Our propoesd method can also easily define such Figure 1: A description of the postordering MT system. non-local features as</context>
<context position="2969" citStr="Goto et al. (2012)" startWordPosition="449" endWordPosition="452"> need a language-dependent scheme and we describe our proposed method as a J-to-E postordering method, the key algorithm is language-independent and it can be applicable to preordering as well as postordering if the training data for reordering are available. 2 Postordering by Parsing As shown in Fig.1, postordering (Sudoh et al., 2011) has two steps; the first is a translation step that translates an input sentence into source-ordered translations. The second is a reordering step in which the translations are reordered in the target language order. The key to postordering is the second step. Goto et al. (2012) modeled the second step by parsing and created training data for a postordering parser using a language-dependent rule called headfinalization. The rule moves syntactic heads of a lexicalized parse tree of an English sentence to the Source Sentence (J) Sourceordered Target Sentence (HFE) reordering Target Sentence (E) 1382 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1382–1386, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics S(saw) VP(saw) PP( with 1383 top element s0, j is an index of the next inp</context>
<context position="8436" citStr="Goto et al. (2012)" startWordPosition="1506" endWordPosition="1509"> We used Enju (Miyao and Tsujii, 2008) for parsing the English training data and converted parse trees into HFE trees by a head-finalization scheme. We extracted grammar rules from all the HFE trees and randomly selected 500,000 HFE trees to train the shift-reduce parser. We used Moses (Koehn et al., 2007) with lexicalized reordering and a 6-gram language model (LM) trained using SRILM (Stolcke et al., 2011) to translate the Japanese sentences into HFE sentences. To recover the English sentences, our shift-reduce parser reordered only the 1-best HFE sentence. Our strategy is much simpler than Goto et al. (2012)’s because they used a linear inteporation of MT cost, parser cost and N-gram LM cost to generate the best English sentence from the n-best HFE sentences. 4.2 Main Results The main results in Table 3 indicate our method was significantly better and faster than the conventional PBMT system. Our method also ourperformed Goto et al. (2012)’s reported systems as well as a treebased (moses-chart) system3. Our proposed model with “non-local” features (w/ nf.) achieved gains against that without the features (w/o nf.). Further feature engineering may improve the accuracy more. 4.3 Analysis We show N-</context>
<context position="9851" citStr="Goto et al. (2012)" startWordPosition="1734" endWordPosition="1737">s used in our experiments are the same as theirs. test9 test10 BLEU RIBES BLEU RIBES HFE w/ art. 28.86 73.45 29.9 73.52 proposed 32.93 76.68 33.25 76.74 w/o art. 19.86 75.62 20.17 75.63 N-gram 32.15 76.52 32.28 76.46 Table 4: The effects of article generation: “w/o art.” denotes evaluation scores for translations of the best system (“proposed”) in Table 3 from which articles are removed. “HFE w/ art.” system used HFE data with articles and generated them by MT system and the shift-reduce parser performed only reordering. “N-gram” system inserted articles into the translations of “w/o art.” by Goto et al. (2012)’s article generation method. (1–4)-gram precision moses (dist=6) 67.1 / 36.9 / 20.7 / 11.5 moses (dist=20) 67.7 / 38.9 / 23.0 / 13.7 proposed 68.9 / 40.6 / 25.7 / 16.7 Table 5: N-gram precisions of moses (dist=6, dist=20) and proposed systems for test9 data. cisions are the main factors that contribute to better performance of our proposed system than PBMT systems. It seems that the gains of 1-gram presicions come from postediting (article generation). In table 4, we show the effectiveness of our joint reordering and postediting approach (“proposed”). The “w/o art.” results clearly show that </context>
</contexts>
<marker>Goto, Utiyama, Sumita, 2012</marker>
<rawString>Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012. Post-ordering by parsing for japanese-english statistical machine translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 311–316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
</authors>
<title>Training tree transducers.</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>105--112</pages>
<contexts>
<context position="897" citStr="Graehl and Knight, 2004" startWordPosition="118" endWordPosition="121">t.co.jp Abstract This paper presents a novel word reordering model that employs a shift-reduce parser for inversion transduction grammars. Our model uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 1 Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce</context>
</contexts>
<marker>Graehl, Knight, 2004</marker>
<rawString>Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In Proc. HLT-NAACL, pages 105–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1077--1086</pages>
<contexts>
<context position="5150" citStr="Huang and Sagae, 2010" startWordPosition="867" endWordPosition="870">: π ℓ + 1 : ⟨i,j,S|s0⟩ : π where s′0 is {X, h, wleft, wright, a} and s0 is {X, h, wleft, wright, x} (i ≤ h, left, right &lt; j). The side condition prevents the parser from inserting articles into phrase X more than twice. During parsing, articles are not explicitly inserted into the input string: they are inserted into it when backtracking to generate a reordered string after parsing. The reduce-MR-X action has a deduction rule: X → Y Z ∈ P ∧ q ∈ π q z } |{ : ⟨k, i,S|s′2|s′1⟩ : π′ ℓ : ⟨i, j,S|s′1|s′0⟩ : π ℓ + 1 : ⟨k,j,S|s′2|s0⟩ : π′ 1Since our notion of predictor states is identical to that in (Huang and Sagae, 2010), we omit the details here. s0.wh ◦ s0.th s0.H s0.H ◦ s0.th s0.wh ◦ s0.H s1.wh ◦ s1.th s1.H s1.H ◦ s1.th s1.wh ◦ s1.H s2.th ◦ s2.H s2.wh ◦ s2.H q0.w q1.w q2.w s0.tl ◦ s0.L s0.wl ◦ s0.L s1.tl ◦ s1.L s1.wl ◦ s1.L s0.wh ◦ s0.H ◦ s1.wh ◦ s1.H s0.H ◦ s1.wh s0.wh ◦ s1.H s0.H ◦ s1.H s0.wh ◦ s0.H ◦ q0.w s0.H ◦ q0.w s1.wh ◦ s1.H ◦ q0.w s1.H ◦ q0.w s1.th ◦ q0.w ◦ q1.w s0.wh ◦ s0.H ◦ s1.H ◦ q0.w s0.H ◦ s1.wh ◦ s1.H ◦ q0.w s0.H ◦ s1.H ◦ q0.w s0.th ◦ s1.th ◦ q0.w s0.wh ◦ s1.H ◦ q0.w ◦ q1.w s0.H ◦ q0.w ◦ q1.w s0.th ◦ q0.w ◦ q1.w s0.wh ◦ s0.H ◦ s1.H ◦ s2.H s0.H ◦ s1.wh ◦ s1.H ◦ s2.H s0.H ◦ s1.H ◦ s2.wh ◦ s2.</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1077–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Masaaki Nagata</author>
<author>Sho Hoshino</author>
<author>Yusuke Miyao</author>
</authors>
<title>HPSG-based preprocessing for English-to-Japanese translation.</title>
<date>2012</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>11</volume>
<issue>3</issue>
<marker>Isozaki, Suzuki, Tsukada, Nagata, Hoshino, Miyao, 2012</marker>
<rawString>Hideki Isozaki, Jun Suzuki, Hajime Tsukada, Masaaki Nagata, Sho Hoshino, and Yusuke Miyao. 2012. HPSG-based preprocessing for English-to-Japanese translation. ACM Transactions on Asian Language Information Processing (TALIP), 11(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Ishwar Chander</author>
</authors>
<title>Automated postediting of documents.</title>
<date>1994</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<pages>779--779</pages>
<marker>Knight, Chander, 1994</marker>
<rawString>Kevin Knight and Ishwar Chander. 1994. Automated postediting of documents. In Proceedings of the National Conference on Artificial Intelligence, pages 779–779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="854" citStr="Koehn et al., 2007" startWordPosition="111" endWordPosition="114">19-0237 Japan †hayashi.katsuhiko@lab.ntt.co.jp Abstract This paper presents a novel word reordering model that employs a shift-reduce parser for inversion transduction grammars. Our model uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 1 Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word r</context>
<context position="8125" citStr="Koehn et al., 2007" startWordPosition="1456" endWordPosition="1459">37.0 43.1 ave. leng. (E) 33.3 33.3 33.7 39.6 Table 2: NTCIR-9 and 10 data statistics. 4 Experiments 4.1 Experimental Setups We conducted experiments for NTCIR-9 and 10 patent data using a Japanese-English language pair. Mecab2 was used for the Japanese morphological analysis. The data are summarized in Table 2. We used Enju (Miyao and Tsujii, 2008) for parsing the English training data and converted parse trees into HFE trees by a head-finalization scheme. We extracted grammar rules from all the HFE trees and randomly selected 500,000 HFE trees to train the shift-reduce parser. We used Moses (Koehn et al., 2007) with lexicalized reordering and a 6-gram language model (LM) trained using SRILM (Stolcke et al., 2011) to translate the Japanese sentences into HFE sentences. To recover the English sentences, our shift-reduce parser reordered only the 1-best HFE sentence. Our strategy is much simpler than Goto et al. (2012)’s because they used a linear inteporation of MT cost, parser cost and N-gram LM cost to generate the best English sentence from the n-best HFE sentences. 4.2 Main Results The main results in Table 3 indicate our method was significantly better and faster than the conventional PBMT system</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Feature forest models for probabilistic hpsg parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="7856" citStr="Miyao and Tsujii, 2008" startWordPosition="1411" endWordPosition="1414">the averaged perceptron (Collins and Roark, 2004). Table 1 shows the feature templates used in our experiments and we call the features in the bottom two rows “non-local” features. 1384 train dev test9 test10 # of sent. 3,191,228 2,000 2,000 2,300 ave. leng. (J) 36.4 36.6 37.0 43.1 ave. leng. (E) 33.3 33.3 33.7 39.6 Table 2: NTCIR-9 and 10 data statistics. 4 Experiments 4.1 Experimental Setups We conducted experiments for NTCIR-9 and 10 patent data using a Japanese-English language pair. Mecab2 was used for the Japanese morphological analysis. The data are summarized in Table 2. We used Enju (Miyao and Tsujii, 2008) for parsing the English training data and converted parse trees into HFE trees by a head-finalization scheme. We extracted grammar rules from all the HFE trees and randomly selected 500,000 HFE trees to train the shift-reduce parser. We used Moses (Koehn et al., 2007) with lexicalized reordering and a 6-gram language model (LM) trained using SRILM (Stolcke et al., 2011) to translate the Japanese sentences into HFE sentences. To recover the English sentences, our shift-reduce parser reordered only the 1-best HFE sentence. Our strategy is much simpler than Goto et al. (2012)’s because they used</context>
</contexts>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature forest models for probabilistic hpsg parsing. Computational Linguistics, 34(1):35–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Human language technologies 2007: the conference of the North American chapter of the Association for Computational Linguistics,</booktitle>
<pages>404--411</pages>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Human language technologies 2007: the conference of the North American chapter of the Association for Computational Linguistics, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Jing Zheng</author>
<author>Wen Wang</author>
<author>Victor Abrash</author>
</authors>
<title>Srilm at sixteen: Update and outlook.</title>
<date>2011</date>
<booktitle>In Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop.</booktitle>
<contexts>
<context position="8229" citStr="Stolcke et al., 2011" startWordPosition="1473" endWordPosition="1476">1 Experimental Setups We conducted experiments for NTCIR-9 and 10 patent data using a Japanese-English language pair. Mecab2 was used for the Japanese morphological analysis. The data are summarized in Table 2. We used Enju (Miyao and Tsujii, 2008) for parsing the English training data and converted parse trees into HFE trees by a head-finalization scheme. We extracted grammar rules from all the HFE trees and randomly selected 500,000 HFE trees to train the shift-reduce parser. We used Moses (Koehn et al., 2007) with lexicalized reordering and a 6-gram language model (LM) trained using SRILM (Stolcke et al., 2011) to translate the Japanese sentences into HFE sentences. To recover the English sentences, our shift-reduce parser reordered only the 1-best HFE sentence. Our strategy is much simpler than Goto et al. (2012)’s because they used a linear inteporation of MT cost, parser cost and N-gram LM cost to generate the best English sentence from the n-best HFE sentences. 4.2 Main Results The main results in Table 3 indicate our method was significantly better and faster than the conventional PBMT system. Our method also ourperformed Goto et al. (2012)’s reported systems as well as a treebased (moses-chart</context>
</contexts>
<marker>Stolcke, Zheng, Wang, Abrash, 2011</marker>
<rawString>Andreas Stolcke, Jing Zheng, Wen Wang, and Victor Abrash. 2011. Srilm at sixteen: Update and outlook. In Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsuhito Sudoh</author>
<author>Xianchao Wu</author>
<author>Kevin Duh</author>
<author>Hajime Tsukada</author>
<author>Masaaki Nagata</author>
</authors>
<title>Post-ordering in statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proc. MT Summit.</booktitle>
<contexts>
<context position="1260" citStr="Sudoh et al., 2011" startWordPosition="174" endWordPosition="177">hat our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 1 Introduction Even though phrase-based machine translation (PBMT) (Koehn et al., 2007) and tree-based MT (Graehl and Knight, 2004; Chiang, 2005; Galley et al., 2006) systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction grammars (ITG) (Wu, 1997). To the best of our knowledge, this is the first study on a shift-reduce parser for word reordering. The parser-based reordering approach uses rich syntax parsing features for reordering decisions. Our propoesd method can also easily define such Figure 1: A description of the postordering MT system. n</context>
<context position="2689" citStr="Sudoh et al., 2011" startWordPosition="400" endWordPosition="403">ion. Therefore, it works much more efficient. In our experiments, we apply our proposed method to postordering for J-to-E patent tasks because their training data for reordering have little noise and they are ideal for evaluating reordering methods. Although our used J-to-E setups need a language-dependent scheme and we describe our proposed method as a J-to-E postordering method, the key algorithm is language-independent and it can be applicable to preordering as well as postordering if the training data for reordering are available. 2 Postordering by Parsing As shown in Fig.1, postordering (Sudoh et al., 2011) has two steps; the first is a translation step that translates an input sentence into source-ordered translations. The second is a reordering step in which the translations are reordered in the target language order. The key to postordering is the second step. Goto et al. (2012) modeled the second step by parsing and created training data for a postordering parser using a language-dependent rule called headfinalization. The rule moves syntactic heads of a lexicalized parse tree of an English sentence to the Source Sentence (J) Sourceordered Target Sentence (HFE) reordering Target Sentence (E)</context>
</contexts>
<marker>Sudoh, Wu, Duh, Tsukada, Nagata, 2011</marker>
<rawString>Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime Tsukada, and Masaaki Nagata. 2011. Post-ordering in statistical machine translation. In Proc. MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1557" citStr="Wu, 1997" startWordPosition="226" endWordPosition="227">chieved great success, many problems remain for distinct language pairs, including long-distant word reordering. To improve such word reordering, one promising way is to separate it from the translation process as preordering (Collins et al., 2005; DeNero and Uszkoreit, 2011) or postordering (Sudoh et al., 2011; Goto et al., 2012). Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree. This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction grammars (ITG) (Wu, 1997). To the best of our knowledge, this is the first study on a shift-reduce parser for word reordering. The parser-based reordering approach uses rich syntax parsing features for reordering decisions. Our propoesd method can also easily define such Figure 1: A description of the postordering MT system. non-local features as the N-gram words of reordered strings. Even when using these non-local features, the complexity of the shift-reduce parser does not increase at all due to give up achieving an optimal solution. Therefore, it works much more efficient. In our experiments, we apply our proposed</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.981271">
Effectiveness and Efficiency of Open Relation Extraction
</title>
<author confidence="0.991035">
Filipe Mesquita Jordan Schmidek Denilson Barbosa
</author>
<affiliation confidence="0.99707">
Department of Computing Science, University of Alberta, Canada
</affiliation>
<email confidence="0.998211">
{mesquita,schmidek,denilson}@ualberta.ca
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999273058823529">
A large number of Open Relation Extrac-
tion approaches have been proposed recently,
covering a wide range of NLP machinery,
from “shallow” (e.g., part-of-speech tagging)
to “deep” (e.g., semantic role labeling–SRL).
A natural question then is what is the trade-
off between NLP depth (and associated com-
putational cost) versus effectiveness. This pa-
per presents a fair and objective experimental
comparison of 8 state-of-the-art approaches
over 5 different datasets, and sheds some light
on the issue. The paper also describes a novel
method, EXEMPLAR, which adapts ideas from
SRL to less costly NLP machinery, resulting
in substantial gains both in efficiency and ef-
fectiveness, over binary and n-ary relation ex-
traction tasks.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999744346938775">
Open Relation Extraction (ORE) (Banko and Et-
zioni, 2008) has become prevalent over traditional
relation extraction methods, especially on the Web,
because of the intrinsic difficulty in training indi-
vidual extractors for every single relation. Broadly
speaking, existing ORE approaches can be grouped
according to the level of sophistication of the NLP
techniques they rely upon: (1) shallow parsing, (2)
dependency parsing and (3) semantic role labelling
(SRL). Shallow methods annotate the sentences with
part-of-speech (POS) tags and the ORE approaches
in this category, such as ReVerb (Fader et al., 2011)
and SONEX (Merhav et al., 2012), identify rela-
tions by matching patterns over such tags. Depen-
dency parsing gives unambiguous relations among
each word in the sentence, and the ORE approaches
in this category such as PATTY (Nakashole et al.,
2012), OLLIE (Mausam et al., 2012), and TreeK-
ernel (Xu et al., 2013) identify whole subtrees con-
necting the relation predicate and its arguments. Fi-
nally, semantic annotators, such as Lund (Johans-
son and Nugues, 2008) and SwiRL (Surdeanu et al.,
2003), add roles to each node in a parse tree, en-
abling ORE approaches that identify the precise con-
nection between each argument and the predicate in
a relation, independently.
The first contribution of the paper is an objec-
tive and fair experimental comparison of the state-
of-the-art in ORE, on 5 datasets with varying de-
gree of “difficulty”. Of these, 4 datasets were an-
notated manually, covering both well-formed sen-
tences, from the New York Times (NYT) and the
Penn Treebank, as well as mixed-quality sentences
from a popular Web corpus. A much larger corpus
with 12,000 sentences from NYT, automatically an-
notated is also used. Another experiment focuses
on n-ary relation extractions separately. The results
show, as expected, that the three broad classes above
are separated by orders of magnitude when it comes
to throughput. Shallow methods handle ten times
more sentences than dependency parsing methods,
which in turn handle ten times more sentences than
semantic parsing methods. Nevertheless, the cost-
benefit trade-off is not as simple; and the higher
computation cost of dependency or semantic parsing
does not always pays off with higher effectiveness.
The second contribution of the paper is a new
ORE method, called EXEMPLAR, which applies a
key idea in semantic approaches (namely, to iden-
</bodyText>
<page confidence="0.97184">
447
</page>
<note confidence="0.734346">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 447–457,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998614375">
tify the precise connection between the argument
and the predicate words in a relation) over a depen-
dency parse tree (i.e., without applying SRL). The
goal is to achieve the higher accuracy of the seman-
tic approaches at the lower computational cost of the
dependency parsing approaches. EXEMPLAR is a
rule-based system derived from a careful study of all
dependency types identified by the Stanford parser.
(Note, however, that other parsers can be used, as
shown later on.) EXEMPLAR works for both binary
and n-ary relations, and is evaluated separately in
each case. For binary relations, EXEMPLAR outper-
forms all previous methods in terms of accuracy, los-
ing to the shallow methods only in terms of through-
put. As for n-ary relations, EXEMPLAR outperforms
the methods that support this kind of extraction.
</bodyText>
<sectionHeader confidence="0.999943" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.921300607142857">
Others have pointed out the importance of under-
standing the trade-off between “shallow” versus
“deep” NLP in ORE. One side of the argument fa-
vors shallow methods, claiming deep NLP costs or-
ders of magnitude more and provide much less dra-
matic gains in terms of effectiveness (Christensen et
al., 2011). The counterpoint, illustrated with a re-
cent analysis on a industrial-scale Web crawl (Dalvi
et al., 2012), is that the diversity with which infor-
mation is encoded in text is too high. Framing the
debate as “shallow” versus “deep” is perhaps con-
venient, but nevertheless an oversimplification. This
paper sheds more light into the debate by compar-
ing the state-of-the-art from three broad classes of
approaches.
Shallow ORE. TextRunner (Banko and Etzioni,
2008) and its successor ReVerb (Fader et al., 2011)
are based on the idea that most relations are ex-
pressed using few syntactic patterns. ReVerb, for ex-
ample, detects only three types of relations (“verb”,
“verb+preposition” and “verb+noun+preposition”).
Following a similar approach, SONEX (Merhav et
al., 2012) extends ReVerb by detecting patterns with
appositions and possessives.
ORE via dependency parsing. PATTY (Nakas-
hole et al., 2012) extracts textual patterns from sen-
tences based on paths in the dependency graph. For
all pairs of named entities, PATTY finds the shortest
</bodyText>
<note confidence="0.69875925">
Dataset Source # Sentences # Relations
WEB-500 Search Snippets 500 461
NYT-500 New York Times 500 150
PENN-100 Penn Treebank 100 51
</note>
<tableCaption confidence="0.997795">
Table 1: Binary relation datasets.
</tableCaption>
<bodyText confidence="0.999527472222222">
path in the dependency graph that connects the two
named entities. They limit the search to only paths
that start with one of these dependencies: nsubj, rc-
mod and partmod.
OLLIE (Mausam et al., 2012) also extracts rela-
tions between two entities. It applies pattern tem-
plates over the dependency subtree containing pairs
of entities. Pattern templates are learned automat-
ically from a large training set that is bootstrapped
from high confidence extractions from ReVerb. OL-
LIE merges binary relations that differ only in the
preposition and second argument to produce n-ary
extractions, as in: (A, “met with”, B) and (A, “met
in”, C) leading to (A, “met”, [with B, in C]).
The TreeKernel (Xu et al., 2013) method uses a
dependency tree kernel to classify whether candi-
date tree paths are indeed instances of relations. The
shortest path between the two entities along with the
shortest path between relational words and an entity
are used as input to the tree kernel. An expanded set
of syntactic patterns based on those from ReVerb are
used to generate relation candidates.
ORE via semantic parsing. Recently, a method
based on SRL, called SRL-IE, has shown that the ef-
fectiveness of ORE methods can be improved with
semantic features (Christensen et al., 2011). We im-
plemented our version of SRL-IE by relying on the
output of two SRL systems: Lund (Johansson and
Nugues, 2008) and SwiRL (Surdeanu et al., 2003).
SwiRL is trained on PropBank and expands upon the
syntactic features used in previous work. One of its
major limitations is that it is only able to label ar-
guments with verb predicates. Lund, on the other
hand, is based on dependency parsing and is trained
on both PropBank and NomBank, making it able to
extract relations with both verb and noun predicates.
</bodyText>
<sectionHeader confidence="0.992878" genericHeader="method">
3 Experimental Study
</sectionHeader>
<bodyText confidence="0.9994675">
This section compares the effectiveness and effi-
ciency of the following ORE methods: ReVerb,
</bodyText>
<page confidence="0.997618">
448
</page>
<table confidence="0.999847090909091">
Method NYT-500 WEB-500 PENN-100
Time P R F Time P R F Time P R F
ReVerb 0.02 0.70 0.11 0.18 0.01 0.92 0.29 0.44 0.02 0.78 0.14 0.23
SONEX 0.04 0.77 0.22 0.34 0.02 0.98 0.30 0.45 0.04 0.92 0.43 0.59
OLLIE 0.05 0.62 0.27 0.38 0.04 0.81 0.29 0.43 0.14 0.81 0.43 0.56
EXEMPLAR[M] 0.08 0.70 0.39 0.50 0.06 0.95 0.44 0.61 0.16 0.83 0.49 0.62
EXEMPLAR[S] 1.03 0.73 0.39 0.51 0.47 0.96 0.46 0.62 0.62 0.79 0.51 0.62
PATTY 1.18 0.49 0.23 0.32 0.48 0.71 0.48 0.57 0.66 0.46 0.24 0.31
SwiRL 2.96 0.63 0.10 0.17 1.73 0.97 0.34 0.50 2.17 0.89 0.16 0.27
Lund 11.40 0.78 0.24 0.37 2.69 0.91 0.37 0.52 5.21 0.86 0.35 0.50
TreeKernel – – – – – – – – 0.85 0.85 0.33 0.48
</table>
<tableCaption confidence="0.9632265">
Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per
sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
</tableCaption>
<bodyText confidence="0.977240166666667">
SONEX, OLLIE, PATTY, TreeKernel, SwiRL,
Lund and our two variants of our method, EX-
EMPLAR, explained in detail in Appendix A: (1)
EXEMPLAR[S] uses the Stanford parser (Klein and
Manning, 2003) and (2) EXEMPLAR[M] uses the
Malt parser (Nivre and Nilsson, 2004).
</bodyText>
<subsectionHeader confidence="0.999347">
3.1 Binary Relations – Setup
</subsectionHeader>
<bodyText confidence="0.963607406779661">
We start by evaluating the extraction of binary re-
lations. Table 1 shows our experimental datasets.
WEB-500 is a commonly used dataset, developed
for the TextRunner experiments (Banko and Etzioni,
2008). These sentences are often incomplete and
grammatically unsound, representing the challenges
of dealing with web text. NYT-500 represents the
other end of the spectrum with formal, well written
new stories from the New York Times Corpus (Sand-
haus, 2008). PENN-100 contains sentences from the
Penn Treebank recently used in an evaluation of the
TreeKernel method (Xu et al., 2013). We manu-
ally annotated the relations for WEB-500 and NYT-
500 and use the PENN-100 annotations provided by
TreeKernel’s authors (Xu et al., 2013).
We annotate each sentence manually as follows.
We identify exactly two entities and a trigger (a sin-
gle token indicating a relation–see Section A.3) for
the relation between them, if one exists. In addi-
tion, we specify a window of tokens allowed to be
in a relation, including modifiers of the trigger and
prepositions connecting triggers to their arguments.
For each sentence annotated with two entities, a sys-
tem must extract a string representing the relation
between them. Our evaluation method deems an ex-
traction as correct if it contains the trigger and al-
lowed tokens only.
In our annotated sentences, entities are enclosed
in triple square brackets, triggers and enclosed in
triple curly brackets and the window of allowed to-
kens is defined by arrows (“---&gt;” and “&lt;---”),
as in this example:
I’ve got a media call about
[[[ORG Google]]] ---&gt;’s
{{{acquisition}}} of&lt;--- [[[ORG
YouTube]]] ---&gt;today&lt;---.
where “Google” and “YouTube” are entities of the
type organization, “acquisition” is the trigger and the
allowed tokens are “acquisition”, “’s” and “of”. We
include time and location modifiers (e.g., “today”,
“here”) in the list of allowed tokens since OLLIE ex-
tracts them as part of the relation. OLLIE’s extrac-
tions may also include auxiliary verbs and preposi-
tions that are not present in the original sentence. To
be fair with OLLIE, we remove auxiliary verbs and
prepositions from OLLIE extractions.
Our benchmarks are available upon request.
Ensuring entities are recognized properly.
Since every method uses a different tool to recog-
nize entities, we try to ensure every method is able
to recognize the entities marked by our annotators.
We replace the original entities by a single word,
preventing any system from recognizing only part
of an entity. Entities are replaced by “Europe” and
“Asia”, since we empirically found that, for 99.7%
of the sentences in our experiment, all methods
were able to recognize “Europe” and “Asia” as
entities (or nouns, for systems that do not use a NER
tool). In addition, we did not find any occurrence of
</bodyText>
<page confidence="0.998831">
449
</page>
<bodyText confidence="0.999665192307692">
“Europe” and “Asia” in the original sentences that
could conflict with our entity placeholders.
For methods that extract relations between noun
phrases (ReVerb, OLLIE, SwiRL and Lund), there
is the additional task of identifying whether a
noun phrase containing additional words surround-
ing “Europe” and “Asia” is still a reference to the an-
notated entity. For example, “the beautiful Europe”
refers to the entity, while “Europe’s enemy” does
not. In our evaluation, we ignore noun phrases that
do not reference the annotated entity. For SwiRL
and Lund, we ignore any noun phrase that do not
present “Europe” or “Asia” as its head word. For
ReVerb and OLLIE, we ignore noun phrases that do
not contain these words in the end of the phrase.
Metrics. Our evaluation focuses on sentence-level
extractions. Therefore, we only apply the steps of
each method that perform this task. Additional steps
to merge relations and remove infrequent relations
are not applied. In addition, we assume there is
only one relation between a pair of entities in a sen-
tence. The number of entity pairs with more than
one relation was insignificant in our datasets (less
than 0.5%).
The metrics used in this analysis are precision (P),
recall (R) and f-measure (F), defined as usual:
</bodyText>
<equation confidence="0.993847">
_ # correct _
P R
# extractions,
</equation>
<bodyText confidence="0.999773125">
where “# correct” is the number of extractions
deemed as correct.
We also measure the total computing time of each
method, excluding initialization or loading any li-
braries or models in memory. To ensure a fair com-
parison, we make sure each method runs in a single-
threaded mode, thus utilizing a single computing
core at all times.
</bodyText>
<subsectionHeader confidence="0.999704">
3.2 Binary Relations – Results
</subsectionHeader>
<bodyText confidence="0.99993775">
Table 2 presents the results for our experiment with
binary relations. WEB-500 turned out to contain the
easiest sentences as evidenced by the precision of
all methods in this dataset. This is because WEB-
500 sentences were collected by querying a search
engine with known relation instances. The other
two datasets, on the other hand, contain randomly
chosen sentences. Although WEB-500 is a popular
</bodyText>
<figure confidence="0.476007">
0.01 0.1 1 10
seconds per sentence
</figure>
<figureCaption confidence="0.9964555">
Figure 1: Average f-measure vs average time for
NYT-500, WEB-500 and PEN-100.
</figureCaption>
<figure confidence="0.9296245">
0 0.1 0.2 0.3 0.4 0.5 0.6
recall
</figure>
<figureCaption confidence="0.987717">
Figure 2: Average precision vs average recall for
NYT-500, WEB-500 and PEN-100.
</figureCaption>
<bodyText confidence="0.987674266666667">
dataset, it perhaps does not represent the challenges
found in web text.
We were unable to run TreeKernel for NYT-500
and WEB-500 for lack of training data. We ran
TreeKernel, as trained by its authors, on the same
test set used in their paper (Xu et al., 2013).
Comparing methods based on effectiveness (f-
measure) or efficiency (computational cost) alone
can be misleading. To do so, we compare methods
in terms of dominance. We say method A dominates
method B if A is: (1) more effective and as efficient
as B; (2) more efficient and as effective as B; or (3)
both more effective and more efficient than B. The
methods that are not dominated by any other form
the state-of-the-art.
</bodyText>
<figureCaption confidence="0.842574">
Figure 1 plots the effectiveness and efficiency of
</figureCaption>
<figure confidence="0.999768161290323">
SONEX
EXEMPLAR[M]
REVERB
OLLIE LUND
EXEMPLAR[S]
PATTY
SWIRL
precision
0.9
0.8
0.7
0.6
0.5
1
REVERB EXEMPLAR[M]
SWIRL
LUND
SONEX
PATTY
OLLIE
EXEMPLAR[S]
# correct _ 2PR
F
# relations,
P + R
f-measure 0.7
0.6
0.5
0.4
0.3
0.2
</figure>
<page confidence="0.988666">
450
</page>
<bodyText confidence="0.999863454545454">
all methods, averaged over all datasets (TreeKernel
was not included due to missing results). For effi-
ciency, there is a clear separation of approximately
one order of magnitude among methods based on
shallow parsing (ReVerb and SONEX), dependency
parsing (OLLIE, EXEMPLAR[M], EXEMPLAR[S],
and PATTY) and semantic parsing (SwiRL and
Lund). The lines in the plot identify the state-of-
the-art before (dashed) and after (solid) EXEMPLAR.
(Note: Although not clear in the figure, OLLIE and
Lund are dominated by SONEX as they tie in terms
of effectiveness.)
In terms of efficiency, EXEMPLAR[M] and EX-
EMPLAR[S] closely match OLLIE and PATTY, re-
spectively, since they use the same dependency
parsers. EXEMPLAR outperforms both systems by
covering a larger number of relational patterns. This
is possible because EXEMPLAR looks at each argu-
ment separately, as opposed to the whole subtree
connecting two arguments. As it turns out, this de-
sign choice greatly simplifies the task of designing
good patterns.
The poor performance of PATTY is due to its
rather permissive sentence-level extraction of rela-
tions, which looks at the shortest path between argu-
ments. PATTY relies on redundancy of extractions
to normalize the produced relations in order to re-
cover from mistakes done in the sentence-level.
Figure 2 illustrates the dominance relation dif-
ferently, using precision versus recall. Again, the
dashed line shows the previous state-of-the-art, and
the solid line shows the current situation. SONEX
dominates PATTY and Lund, since they tied in
recall. OLLIE, however, achieved greater recall
than SONEX. Somewhat surprisingly, EXEMPLAR
presents 44% more recall than the more sophisti-
cated Lund, at a close level of precision. This can be
explained by Lund’s dependency on training data,
which contains only a subset of all possible pred-
icates and roles. The importance of relations trig-
gered by nouns is illustrated by the higher recall
of SONEX and Lund when compared, respectively,
to ReVerb and SwiRL, similar methods that handle
verb triggers only.
</bodyText>
<subsectionHeader confidence="0.999151">
3.3 Binary Relations – Discussion
</subsectionHeader>
<bodyText confidence="0.993639042553191">
Differences in annotation. It is worth noting
some differences between our annotations (WEB-
500 and NYT-500) and the annotations from PEN-
100. The first difference concerns the definition
of an entity. Consider the following sentence from
PEN-100:
“... says Leslie Quick Jr., chairman of the
Quick &amp; Reilly discount brokerage firm.”
Unlike our annotation style, the original annotation
defines that “Leslie Quick Jr.” is the chairman of
“the Quick &amp; Reilly discount brokerage firm”, as op-
posed to “Quick &amp; Reilly”. While we consider the
words surrounding “Quick &amp; Reilly” as apposition,
the original consider them as part of the entity.
Another difference concerns the definition of the
RE task. We assume that RE methods are respon-
sible for resolving co-references when necessary to
identify a relation. For example, consider the sen-
tence:
“It also marks P&amp;G’s growing concern that its
Japanese rivals, such as Kao Corp., may bring
their superconcentrates to the U.S.”
According to our annotation style, there is a rela-
tion “rivals” between “P&amp;G” and “Kao Corp.” in
this sentence. On the other hand, the original an-
notations for PENN-100 consider only the relation
between “Kap Corp.” and the pronoun “it”, leav-
ing the task of resolving the coreference between
“P&amp;G” and “it” as a posterior step.
These differences in annotation illustrate the chal-
lenges of producing a benchmark for open relation
extraction.
Differences in evaluation methodology. A
sentence-level evaluation like ours focuses on each
sentence, separately. On the other hand, the evalua-
tions of SONEX, ReVerb, PATTY, TreeKernel and
OLLIE are performed at the corpus level. Corpus-
level evaluations consider an extracted relation as
correct regardless of whether a method was able
to identify one or all sentences that describe this
relation.
Creating a ground truth for corpus-level evalu-
ations is extremely hard, since one has to iden-
tify and curate (e.g., merge near-duplicate relations
and co-referential entities) all relations described
in a corpus. As a consequence, most corpus-level
evaluations perform only a manual inspection of a
</bodyText>
<page confidence="0.988834">
451
</page>
<table confidence="0.999691571428571">
Method NYT n-ary
Time P R F
EXEMPLAR[M] 0.11 0.94 0.40 0.56
OLLIE 0.12 0.87 0.14 0.25
EXEMPLAR[S] 0.88 0.92 0.39 0.55
SwiRL 2.90 0.94 0.30 0.45
Lund 9.20 0.95 0.36 0.53
</table>
<tableCaption confidence="0.996838">
Table 3: Results for n-ary relations.
</tableCaption>
<bodyText confidence="0.9990975">
method’s extractions. This manual inspection mea-
sures a method’s precision, but is unable to measure
recall.
Other differences in methodology are as follows.
PATTY’s evaluation concerns relation patterns (e.g.,
“wrote hits for”) and their type signatures (e.g.
Musician–Musician), as opposed to the relation it-
self, which includes its two arguments. The eval-
uations of ReVerb and OLLIE consider any noun
phrase as a potential argument, while the evaluations
of TreeKernel and SONEX consider named entities
only.
Due to the lack of a ground truth and differences
in evaluation methodology, results from different pa-
pers are usually not comparable. This work tries to
alleviate this problem by providing reusable annota-
tions that are flexible and can be used to evaluate a
wide range of methods.
</bodyText>
<subsectionHeader confidence="0.918312">
3.4 n-ary Relations
</subsectionHeader>
<bodyText confidence="0.997511533333333">
The goal of this experiment is to evaluate the accu-
racy and performance of our method when extract-
ing n-ary relations (n &gt; 2). For this experiment, we
manually tagged 222 sentences with n-ary relations
from the New York Times. Every sentence is anno-
tated with a single relation trigger and its arguments.
This experiment measures precision and recall
over the extracted arguments. For each sentence, a
system can extract a number of relations of the form
r(a1, a2, ... , an), where r is the relation name and
ai is an argument. We only use the extracted rela-
tion whose name contains the annotated trigger, if
it exists. An argument of such relation is deemed a
correct extraction if it is annotated in the sentence;
otherwise, it is deemed incorrect.
</bodyText>
<table confidence="0.952079071428572">
Precision and recall are now defined as follows:
P _ # correct R _ # correct
# extracted args, .
# annotated args
Method NYT 12K
Time P R F
ReVerb 0.01 0.84 0.11 0.19
OLLIE 0.02 0.85 0.22 0.35
SONEX 0.03 0.87 0.20 0.32
EXEMPLAR[M] 0.05 0.87 0.26 0.40
EXEMPLAR[S] 1.20 0.86 0.29 0.43
PATTY 1.29 0.86 0.18 0.30
SwiRL 3.58 0.87 0.16 0.27
Lund 11.28 0.86 0.21 0.33
</table>
<tableCaption confidence="0.9419815">
Table 4: Results for binary relations automatically
annotated using Freebase and WordNet.
</tableCaption>
<bodyText confidence="0.999941">
where “# correct” is the number of arguments
deemed as correct. There are 765 annotated argu-
ments in total. Table 3 reports the results for our
experiment with n-ary relations. EXEMPLAR[M]
shows a 6% increase in f-measure over Lund, the
second best system, while being almost two orders
of magnitude faster.
</bodyText>
<subsectionHeader confidence="0.991685">
3.5 Automatically Annotated Sentences
</subsectionHeader>
<bodyText confidence="0.999984666666667">
The creation of datasets for open RE is an extremely
time-consuming task. In this section we investigate
whether external data sources such as Freebase1 and
WordNet2 can be used to automatically annotate a
dataset, leading to a useful benchmark.
Our automatic annotator identifies pairs of enti-
ties and a trigger of the relation between them. It
does so by first trying to link all entities to Wikipedia
(and consequently to Freebase, since Freebase is
linked to Wikipedia) by using the method proposed
by (Cucerzan, 2007). Given two entities appearing
within 10 tokens of each other in a sentence, our an-
notator checks whether there is a relation connect-
ing them in Freebase. If such a relation exists, the
annotator looks for a trigger in the sentence. A trig-
ger must be a synonym for the Freebase relation (ac-
cording to WordNet) and its distance to the nearest
entity cannot be more than 5 tokens.
We applied this method for the New York Times
and were able to annotate over 60,000 sentences
with over 13,000 distinct entity pairs. For our ex-
periments, we randomly selected one sentence for
each entity pair and separated a thousand for devel-
opment and over 12,000 for test.
</bodyText>
<footnote confidence="0.9999825">
1http://www.freebase.com
2http://wordnet.princeton.edu/
</footnote>
<page confidence="0.995284">
452
</page>
<bodyText confidence="0.998897625">
Comparing with human annotators. Although
we expect our automatic annotator to be less accu-
rate than a human annotator, we are interested to
measure the difference in accuracy between them.
To do so, two authors of this paper looked at our de-
velopment set and marked each sentence as correct
or incorrect. The agreement (that is, the percent-
age of matching answers) between the humans was
82%. On other hand, the agreement between our
automatic annotator and each human was 71% and
72%. This shows that our annotator’s accuracy is not
too far below human’s level of accuracy.
Table 4 shows the results for the test sentences.
Both EXEMPLAR[S] and EXEMPLAR[M] outper-
formed all systems in recall, while keeping the same
level of precision.
</bodyText>
<sectionHeader confidence="0.999446" genericHeader="method">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999977857142857">
This work presents a fair and objective evaluation of
several ORE methods, shedding some light on the
trade-offs between f-measure and computational as
well as precision and recall. Our evaluation is able
to assess the effectiveness of different methods by
specifying a trigger and a window of allowed tokens
for each relation.
EXEMPLAR’s promising results indicate that rule-
based methods may still be very competitive, espe-
cially if rules are applied to each argument sepa-
rately. Looking at the recall levels of different meth-
ods, we conjecture that EXEMPLAR outperforms
machine learning methods like Ollie and TreeKer-
nel, because its rules apply in cases not trained by
these methods. From a pragmatic point of view, EX-
EMPLAR is also preferable because it doesn’t require
training data.
An interesting research question is whether ma-
chine learning can be used to learn more rules for
EXEMPLAR in order to improve recall without loss
in precision. Rules could be learned from both de-
pendency parsing and shallow parsing, or just shal-
low parsing if computing time is extremely limited.
The next step for our experimental study is to
evaluate corpus-level extractions, where an auto-
matic annotator is essential due to the massive num-
ber of annotations required for even one relation, let
alone thousands.
</bodyText>
<sectionHeader confidence="0.990879" genericHeader="method">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998224">
We thank the anonymous reviewers for useful sug-
gestions to improve the paper, and the Natural Sci-
ences and Engineering Council of Canada, through
the NSERC Business Intelligence Network, for fi-
nancial support.
</bodyText>
<sectionHeader confidence="0.881634" genericHeader="method">
A EXEMPLAR
</sectionHeader>
<bodyText confidence="0.999902289473684">
ORE methods must recognize relations from the text
alone. To do this, each method tries to model how
relations are expressed in English. Banko and Et-
zioni claim that more than 90% of binary relations
are expressed through a few syntactic patterns, such
as “verb” and “noun+preposition” (Banko and Et-
zioni, 2008). It is unclear, however, whether n-ary
relations follow.
This section presents a study focusing on how n-
ary relations (n &gt; 2) are expressed in English, based
on 100 distinct relations manually annotated from a
random sample of 514 sentences in the New York
Times Corpus (Sandhaus, 2008).
Table 5 shows six syntactic patterns that cover
86% of our n-ary relations. These patterns are
slightly different from those used for binary rela-
tions. In binary relations, the pattern implicitly de-
fines the roles of the two arguments. For instance,
a relation “met with” indicates that the first argu-
ment is the subject and the second one is the object
of “with”. In order to represent n-ary relations, our
patterns do not contain prepositions, possessives or
any other word connecting the relation to the argu-
ment. For instance, the sentence “Obama met with
Putin in Russia” contains the relation “meet” along
three arguments: “Obama” (subject), “Putin” (object
of preposition with) and “Russia” (object of prepo-
sition in).
Relation types. A single relation can be repre-
sented in different ways using the patterns shown
in Table 5. For instance, the relation “donate” can
be expressed as an active verb (“donates”), passive
voice (“was donated by”) and normalized verb (“do-
nation”). In addition, an apposition+noun relation
can be expressed as an copula+noun relation by re-
placing apposition for the copula verb “be”. By
merging these patterns, we have that most relations
fall into one of the following types: verb, verb+noun
</bodyText>
<page confidence="0.998602">
453
</page>
<table confidence="0.999542428571429">
Pattern Frequency Example
Verb 30% Hingis beat Steffi Graf in the Italian Open two weeks ago.
Apposition+noun 19% Jaden and Willow, the famous children of Will Smith,...
Passive verb 14% Jumbo the Elephant was exported from Sudan to Paris.
Verb+noun 14% D-League will move its offices from Greenville to New York.
Copula+noun 5% Kimball was a Fulbright scholar at the University of Heidelberg.
Nominalized verb 4% Thousands died in Saddam Hussein’s attack on Halabja in 1988.
</table>
<tableCaption confidence="0.9907865">
Table 5: Patterns representing 86% of the relations with three or more arguments. Frequencies collected
from 100 relations from the New York Times Corpus. Relation triggers are highlighted in bold.
</tableCaption>
<table confidence="0.9994895">
Relation Type Freq. Example Variations
Verb 48% beat Pass. verb, nom. verb
Copula+noun 24% is son Apposition+noun
Verb+noun 14% sign deal –
</table>
<tableCaption confidence="0.99932">
Table 6: Relation types recognized by EXEMPLAR.
</tableCaption>
<bodyText confidence="0.98344546875">
and copula+noun.
Table 6 present the relation types found through
our analysis. We developed EXEMPLAR to specifi-
cally recognize these relation types, including their
variations.
Argument roles. An argument role defines how
an argument participates in a relation. In ORE, the
roles for each relation are not provided and must also
be recognized from the text.
We use the following roles: subject,
direct object and prep object. An argu-
ment has a role prep object when its connected
to the relation by a preposition. The roles of
prepositional objects consist of their preposition and
the suffix “ object”, indicating that each preposition
corresponds to a different role. In the sentence
“Obama is the president of the U.S.”, “U.S.” is
an object of the preposition “of” and has the role
of object.
Multiple entities can play the same role in a
relation instance. For instance, in the sentence
“Obama and Putin discuss the Syria conflict”, both
“Obama” and “Putin” have the subject role. Fur-
thermore, some relations accept less roles than oth-
ers. Verb relations accept all three roles, while cop-
ula+noun and verb+noun relations accept subject
and prep object only.
Our roles are different from those used in SRL.
SRL roles carry semantic information across differ-
ent relations. This information is unavailable for
ORE systems, and for this reason, we rely on syn-
tactic roles. An open problem is to determine which
</bodyText>
<listItem confidence="0.6636905">
subject: “NFL”
relation: “approve new stadium”
of object: “Falcons”
in object: “Atlanta”
</listItem>
<figureCaption confidence="0.952142666666667">
Figure 3: A relation instance extracted by EXEM-
PLAR for the sentence “NFL approves Falcons’ new
stadium in Atlanta”.
</figureCaption>
<figure confidence="0.9833905">
poss
nsubj amod
NFL approves Falcons&apos; new stadium in Atlanta.
dobj prep_in
</figure>
<figureCaption confidence="0.972762333333333">
Figure 4: A input sentence after pre-processing. En-
tities are in bold, triggers are underline and arrows
represent dependencies.
</figureCaption>
<bodyText confidence="0.99492625">
syntactic roles correspond to the same semantic role
across different relations (Chambers and Jurafsky,
2011). However, this problem is out of the scope
of this work.
</bodyText>
<sectionHeader confidence="0.349874" genericHeader="method">
A.1 The method
</sectionHeader>
<figureCaption confidence="0.776583666666667">
EXEMPLAR takes a stream of textual documents and
extracts instances of n-ary relations as illustrated in
Figure 3.
</figureCaption>
<subsectionHeader confidence="0.886595">
A.2 Preprocessing
</subsectionHeader>
<bodyText confidence="0.9996505">
Given a document, EXEMPLAR extracts its syntactic
structure by applying a pipeline of NLP tools pro-
vided by the Stanford Parser (Klein and Manning,
2003). Our method converts the original text into
sentences, each containing a list of tokens. Each to-
ken is tagged with part of speech, lemma and de-
pendencies. EXEMPLAR also works with other de-
pendency parsers based on Stanford’s dependencies,
</bodyText>
<page confidence="0.998293">
454
</page>
<bodyText confidence="0.861995">
such as the Malt parser (Nivre and Nilsson, 2004).
Figure 4 illustrates our running example where
each word is a token and arrows represent dependen-
cies among tokens. In this example, “stadium” de-
pends on “approves” and the arrow connecting them
can be read as “the direct object of approves is sta-
dium”.
Extracting Named Entities. EXEMPLAR em-
ploys the Stanford NER (Finkel et al., 2005) to rec-
ognize named entities. We consider these types of
entities: people, organization, location, miscella-
neous and date. Figure 4 shows entities highlighted
in bold.
</bodyText>
<subsectionHeader confidence="0.914703">
A.3 Detecting triggers
</subsectionHeader>
<bodyText confidence="0.999967214285714">
After recognizing entities, EXEMPLAR detects re-
lation triggers. A trigger is a single token that in-
dicates the presence of a relation. A relation may
present one or two triggers. For instance, the rela-
tion in our running example has two triggers. EX-
EMPLAR also uses triggers to determine the relation
name, as discussed later.
A trigger can be any noun or verb that was not
tagged as being part of an entity mention. Other re-
quirements are enforced for each canonical pattern.
Verb relations. A verb relation is triggered by a
verb that does not include a noun as its direct object.
The name of the relation is the trigger’s lemma.
A noun must be a nominalized verb to be a trigger
for verb relations. To identify nominalized verbs,
EXEMPLAR checks if a noun is filed under the type
“event” in Wordnet’s Morphosemantic Database3.
Doing so may generate false positives; however,
EXEMPLAR has a filtering step to eliminate these
false positives, as discussed later.
The name of a relation triggered by a nominalized
verb is the trigger’s original verb (before nominal-
ization). For instance, “donation” triggers the rela-
tion “donate”.
Copula+noun relations. Only noun triggers are
accepted for copula+noun relations. The copula
used in the relation name can be a verb with cop-
ula dependency to the trigger, or the verb “be” for
</bodyText>
<footnote confidence="0.8403155">
3http://wordnetcode.princeton.edu/
standoff-files/morphosemantic-links.xls
</footnote>
<bodyText confidence="0.998889666666667">
appositions. The relation name is the concatena-
tion of the copula’s lemma and the trigger’s lemma
along its modifiers. For instance, the relation in the
sentence “Jaden and Willow, the famous children of
Will Smith” is “be famous child”.
Verb+noun relations. EXEMPLAR recognizes
two triggers for each verb+noun relation: a verb
and a noun acting as its direct object. The relation
name is defined by concatenating the verb’s lemma
with the the noun and its modifiers. In our running
example, “approves” and “stadium” trigger the
relation “approve new stadium”.
</bodyText>
<subsectionHeader confidence="0.859902">
A.4 Detecting candidate arguments
</subsectionHeader>
<bodyText confidence="0.999899947368421">
After relation triggers are identified, EXEMPLAR
proceeds to detect their candidate arguments. For
this, we look at the dependency between each entity
and a trigger separately. EXEMPLAR relies on two
observations: (1) an argument is often adjacent to
a trigger in the dependency graph, and (2) the type
of the dependency can accurately predict whether an
entity is an argument for the relation or not.
Table 7 enumerates 12 types of dependencies
(from a total of 53) that often connect arguments and
triggers. EXEMPLAR identifies as a candidate argu-
ment every entity that is connected to trigger, as long
as their dependency type is listed in Table 7.
Our observations can be seen in our running ex-
ample. The entities “NFL” and “Atlanta” depends
on the trigger “approves” and “Falcons” depend on
the trigger “stadium”. Since their dependency types
are listed in Table 7, these entities are marked as can-
didate arguments.
</bodyText>
<subsectionHeader confidence="0.575945">
A.5 Role Detection
</subsectionHeader>
<bodyText confidence="0.9977475">
EXEMPLAR determines the role of an argument
based on the trigger type (noun or verb), the type of
dependency between the trigger and argument and
the direction of the dependency. To take into ac-
count the dependency direction, we prefix each de-
pendency type with “&gt;” when an entity depends on
the trigger and “&lt;” when the trigger depends on the
entity.
Table 8 shows EXEMPLAR’s rules that assign
roles to arguments for each relation type. Rules are
triples (trigger, dependency, role) whose meaning
is as follows:
</bodyText>
<page confidence="0.9995775">
455
456
</page>
<tableCaption confidence="0.749613666666667">
Table 7: Dependencies connecting arguments and
triggers. Arguments are in bold and triggers are un-
derlined.
</tableCaption>
<bodyText confidence="0.999524451612903">
If trigger type = trigger and dependency
type = dependency then assign role.
For example, the first rule in Table 8a specifies
that an argument must be assigned the role of a sub-
ject if this argument depends on a verb trigger and
the dependency type is &gt;nsubj.
Rules are ordered by descending priority in Ta-
ble 8. In case several rules can be applied for an ar-
gument, we apply only the rule with higher priority.
If none of the rules applies to an argument, EXEM-
PLAR removes this argument from the relation.
Exceptions. There are three exceptions for the
rules above. The first exception concerns argu-
ments of verb relations whose dependency type is
&lt;partmod or &lt;rcmod. EXEMPLAR chooses the role
of direct object (as oppose to subject) for these ar-
guments when the verb trigger is in passive form.
For instance, in the sentence “Barbie, (which was)
invented by Handler”, “Barbie” has the role di-
rect object because “invented” is in passive form.
The second exception is for nominalized verbs
followed by the preposition “by”, such as in “Geor-
gian invasion by Russia”. Arguments of this type
of trigger with dependency types &gt;nn, &gt;amod or
&gt;poss are assigned the role direct object.
Finallly, there is an exception for copula+noun
relations expressed with close appositions of the
form: “determiner entity noun”. An example is “the
Greenwood Heights section of Brooklyn”. Here,
EXEMPLAR assigns the subject role to the entity be-
tween the determiner and the noun.
</bodyText>
<figure confidence="0.7398578">
(a) Rules for verb relations.
(b) Rules for copula+noun relations.
(c) Rules for verb+noun relations.
Table 8: Rules for assigning roles to arguments.
A.6 Filtering Relations
</figure>
<bodyText confidence="0.999012454545455">
The final step in EXEMPLAR is to remove incom-
plete relations. EXEMPLAR removes relations with
less than two arguments and relations that do not
present subject and direct object.
For EXEMPLAR, Lund and SwiRL, which extract
n-ary relations, our evaluation needs to convert n-ary
relations into binary ones. This is done by selecting
all pairs of arguments from a n-ary relation and cre-
ating a new (binary) relation for each of them. Bi-
nary relations containing two prepositional objects
(or equivalent for SRL systems) are removed.
</bodyText>
<figure confidence="0.976373467153285">
Dependency
Example
nsubj (Subject)
dobj (Direct Object)
nsubjpass (Pass. Subj.)
agent (Pass. Voice Obj.)
iobj (Indirect Object)
poss (Possessive)
appos (Apposition)
amod (Adj. Modifier)
nn (Noun Comp. Mod.)
prep &amp;quot; (Prep. Modifier)
partmod (Participal Mod.)
rcmod (Rel. Clause Mod.)
Romeo loves Juliet
The Prince exiles Romeo
Romeo was seen in Verona
Juliet is loved by Romeo
Romeo gave Juliet a kiss
Romeo’s father Montague
Capulet, Juliet’s father,
The Italian city of Verona
Romeo’s cousin Benvolio
Romeo lived in Verona
Romeo, born in Italy
Juliet, who loved Romeo
Trigger Type
Verb
Verb
Verb
Verb
Verb
Verb
Verb
Verb
Noun
Noun
Noun
Noun
Noun
Noun
Dependency Type
&gt;nsubj
&gt;agent
&lt;partmod
&lt;rcmod
&gt;dobj
&gt;subjpass
&gt;iobj
&gt;prep &amp;quot;
&gt;prep by
&gt;amod
&gt;nn
&gt;poss
&gt;prep of
&gt;prep &amp;quot;
Role
subject
subject
subject
subject
direct object
direct object
to object
prep object
subject
subject
subject
subject
direct object
prep object
Trigger Type
Noun
Noun
Noun
Noun
Noun
Noun
Noun
Noun
Noun
Noun
Dependency Type
&gt;nsubj
&gt;appos
&lt;appos
&lt;partmod
&lt;rcmod
&gt;prep of
&gt;amod
&gt;nn
&gt;poss
&gt;prep &amp;quot;
Role
subject
subject
subject
subject
subject
of object
of object
of object
of object
prep object
Trigger Type
Verb
Verb
Verb
Verb
Verb
Verb
Noun
Noun
Noun
Noun
Dependency Type
&gt;nsubj
&gt;agent
&lt;partmod
&lt;rcmod
&gt;iobj
&gt;prep &amp;quot;
&gt;amod
&gt;nn
&gt;poss
&gt;prep &amp;quot;
Role
subject
subject
subject
subject
to object
prep object
of object
of object
of object
prep object
</figure>
<sectionHeader confidence="0.943796" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999920433333333">
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics, pages 28–36, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ’11, pages 976–
986, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2011. An analysis of open informa-
tion extraction based on semantic role labeling. In
Proceedings of the Sixth International Conference on
Knowledge Capture, pages 113–120. Association for
Computational Linguistics.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In Proceed-
ings of Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, EMNLP-CoNLL’12, pages 708–
716.
Nilesh Dalvi, Ashwin Machanavajjhala, and Bo Pang.
2012. An analysis of structured data on the web. Proc.
VLDB Endow., 5(7):680–691.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying Relations for Open Information
Extraction. In Proceedings of Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP’12.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL’05,
pages 363–370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of
propbank. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP’08, pages 69–78, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, ACL’03, pages 423–430, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Mausam, Michael Schmitz, Robert Bart, Stephen Soder-
land, and Oren Etzioni. 2012. Open language learning
for information extraction. In Proceedings of Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, EMNLP-CoNLL’12.
Yuval Merhav, Filipe de S´a Mesquita, Denilson Barbosa,
Wai Gen Yee, and Ophir Frieder. 2012. Extracting
information networks from the blogosphere. TWEB,
6(3):11.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: a taxonomy of relational
patterns with semantic types. In Proceedings of the
2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning, EMNLP-CoNLL’12, pages
1135–1145, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Joakim Nivre and Jens Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of Computational
Natural Language Learning, CoNLL’04.
Evan Sandhaus. 2008. The new york times anno-
tated corpus. http://www.ldc.upenn.edu/
Catalog/catalogEntry.jsp?catalogId=
LDC2008T19.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 8–15. Association
for Computational Linguistics.
Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel,
and Denilson Barbosa. 2013. Open information ex-
traction with tree kernels. In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 868–877, Atlanta, Georgia,
June. Association for Computational Linguistics.
</reference>
<page confidence="0.998362">
457
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.957430">
<title confidence="0.999682">Effectiveness and Efficiency of Open Relation Extraction</title>
<author confidence="0.99604">Filipe Mesquita Jordan Schmidek Denilson Barbosa</author>
<affiliation confidence="0.989883">Department of Computing Science, University of Alberta, Canada</affiliation>
<abstract confidence="0.998355166666667">A large number of Open Relation Extraction approaches have been proposed recently, covering a wide range of NLP machinery, from “shallow” (e.g., part-of-speech tagging) to “deep” (e.g., semantic role labeling–SRL). A natural question then is what is the tradeoff between NLP depth (and associated computational cost) versus effectiveness. This paper presents a fair and objective experimental comparison of 8 state-of-the-art approaches over 5 different datasets, and sheds some light on the issue. The paper also describes a novel which adapts ideas from SRL to less costly NLP machinery, resulting in substantial gains both in efficiency and efover binary and relation extraction tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Oren Etzioni</author>
</authors>
<title>The tradeoffs between open and traditional relation extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>28--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1016" citStr="Banko and Etzioni, 2008" startWordPosition="141" endWordPosition="145">deep” (e.g., semantic role labeling–SRL). A natural question then is what is the tradeoff between NLP depth (and associated computational cost) versus effectiveness. This paper presents a fair and objective experimental comparison of 8 state-of-the-art approaches over 5 different datasets, and sheds some light on the issue. The paper also describes a novel method, EXEMPLAR, which adapts ideas from SRL to less costly NLP machinery, resulting in substantial gains both in efficiency and effectiveness, over binary and n-ary relation extraction tasks. 1 Introduction Open Relation Extraction (ORE) (Banko and Etzioni, 2008) has become prevalent over traditional relation extraction methods, especially on the Web, because of the intrinsic difficulty in training individual extractors for every single relation. Broadly speaking, existing ORE approaches can be grouped according to the level of sophistication of the NLP techniques they rely upon: (1) shallow parsing, (2) dependency parsing and (3) semantic role labelling (SRL). Shallow methods annotate the sentences with part-of-speech (POS) tags and the ORE approaches in this category, such as ReVerb (Fader et al., 2011) and SONEX (Merhav et al., 2012), identify rela</context>
<context position="5155" citStr="Banko and Etzioni, 2008" startWordPosition="801" endWordPosition="804">favors shallow methods, claiming deep NLP costs orders of magnitude more and provide much less dramatic gains in terms of effectiveness (Christensen et al., 2011). The counterpoint, illustrated with a recent analysis on a industrial-scale Web crawl (Dalvi et al., 2012), is that the diversity with which information is encoded in text is too high. Framing the debate as “shallow” versus “deep” is perhaps convenient, but nevertheless an oversimplification. This paper sheds more light into the debate by comparing the state-of-the-art from three broad classes of approaches. Shallow ORE. TextRunner (Banko and Etzioni, 2008) and its successor ReVerb (Fader et al., 2011) are based on the idea that most relations are expressed using few syntactic patterns. ReVerb, for example, detects only three types of relations (“verb”, “verb+preposition” and “verb+noun+preposition”). Following a similar approach, SONEX (Merhav et al., 2012) extends ReVerb by detecting patterns with appositions and possessives. ORE via dependency parsing. PATTY (Nakashole et al., 2012) extracts textual patterns from sentences based on paths in the dependency graph. For all pairs of named entities, PATTY finds the shortest Dataset Source # Senten</context>
<context position="9139" citStr="Banko and Etzioni, 2008" startWordPosition="1478" endWordPosition="1481">e ordered by computing time per sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity. SONEX, OLLIE, PATTY, TreeKernel, SwiRL, Lund and our two variants of our method, EXEMPLAR, explained in detail in Appendix A: (1) EXEMPLAR[S] uses the Stanford parser (Klein and Manning, 2003) and (2) EXEMPLAR[M] uses the Malt parser (Nivre and Nilsson, 2004). 3.1 Binary Relations – Setup We start by evaluating the extraction of binary relations. Table 1 shows our experimental datasets. WEB-500 is a commonly used dataset, developed for the TextRunner experiments (Banko and Etzioni, 2008). These sentences are often incomplete and grammatically unsound, representing the challenges of dealing with web text. NYT-500 represents the other end of the spectrum with formal, well written new stories from the New York Times Corpus (Sandhaus, 2008). PENN-100 contains sentences from the Penn Treebank recently used in an evaluation of the TreeKernel method (Xu et al., 2013). We manually annotated the relations for WEB-500 and NYT500 and use the PENN-100 annotations provided by TreeKernel’s authors (Xu et al., 2013). We annotate each sentence manually as follows. We identify exactly two ent</context>
<context position="25582" citStr="Banko and Etzioni, 2008" startWordPosition="4170" endWordPosition="4174">number of annotations required for even one relation, let alone thousands. Acknowledgements We thank the anonymous reviewers for useful suggestions to improve the paper, and the Natural Sciences and Engineering Council of Canada, through the NSERC Business Intelligence Network, for financial support. A EXEMPLAR ORE methods must recognize relations from the text alone. To do this, each method tries to model how relations are expressed in English. Banko and Etzioni claim that more than 90% of binary relations are expressed through a few syntactic patterns, such as “verb” and “noun+preposition” (Banko and Etzioni, 2008). It is unclear, however, whether n-ary relations follow. This section presents a study focusing on how nary relations (n &gt; 2) are expressed in English, based on 100 distinct relations manually annotated from a random sample of 514 sentences in the New York Times Corpus (Sandhaus, 2008). Table 5 shows six syntactic patterns that cover 86% of our n-ary relations. These patterns are slightly different from those used for binary relations. In binary relations, the pattern implicitly defines the roles of the two arguments. For instance, a relation “met with” indicates that the first argument is th</context>
</contexts>
<marker>Banko, Etzioni, 2008</marker>
<rawString>Michele Banko and Oren Etzioni. 2008. The tradeoffs between open and traditional relation extraction. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 28–36, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Templatebased information extraction without the templates.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>976--986</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="29897" citStr="Chambers and Jurafsky, 2011" startWordPosition="4863" endWordPosition="4866">able for ORE systems, and for this reason, we rely on syntactic roles. An open problem is to determine which subject: “NFL” relation: “approve new stadium” of object: “Falcons” in object: “Atlanta” Figure 3: A relation instance extracted by EXEMPLAR for the sentence “NFL approves Falcons’ new stadium in Atlanta”. poss nsubj amod NFL approves Falcons&apos; new stadium in Atlanta. dobj prep_in Figure 4: A input sentence after pre-processing. Entities are in bold, triggers are underline and arrows represent dependencies. syntactic roles correspond to the same semantic role across different relations (Chambers and Jurafsky, 2011). However, this problem is out of the scope of this work. A.1 The method EXEMPLAR takes a stream of textual documents and extracts instances of n-ary relations as illustrated in Figure 3. A.2 Preprocessing Given a document, EXEMPLAR extracts its syntactic structure by applying a pipeline of NLP tools provided by the Stanford Parser (Klein and Manning, 2003). Our method converts the original text into sentences, each containing a list of tokens. Each token is tagged with part of speech, lemma and dependencies. EXEMPLAR also works with other dependency parsers based on Stanford’s dependencies, 4</context>
</contexts>
<marker>Chambers, Jurafsky, 2011</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2011. Templatebased information extraction without the templates. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 976– 986, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janara Christensen</author>
<author>Stephen Soderland Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>An analysis of open information extraction based on semantic role labeling.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth International Conference on Knowledge Capture,</booktitle>
<pages>113--120</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4693" citStr="Christensen et al., 2011" startWordPosition="728" endWordPosition="731">inary and n-ary relations, and is evaluated separately in each case. For binary relations, EXEMPLAR outperforms all previous methods in terms of accuracy, losing to the shallow methods only in terms of throughput. As for n-ary relations, EXEMPLAR outperforms the methods that support this kind of extraction. 2 Related Work Others have pointed out the importance of understanding the trade-off between “shallow” versus “deep” NLP in ORE. One side of the argument favors shallow methods, claiming deep NLP costs orders of magnitude more and provide much less dramatic gains in terms of effectiveness (Christensen et al., 2011). The counterpoint, illustrated with a recent analysis on a industrial-scale Web crawl (Dalvi et al., 2012), is that the diversity with which information is encoded in text is too high. Framing the debate as “shallow” versus “deep” is perhaps convenient, but nevertheless an oversimplification. This paper sheds more light into the debate by comparing the state-of-the-art from three broad classes of approaches. Shallow ORE. TextRunner (Banko and Etzioni, 2008) and its successor ReVerb (Fader et al., 2011) are based on the idea that most relations are expressed using few syntactic patterns. ReVer</context>
<context position="7160" citStr="Christensen et al., 2011" startWordPosition="1126" endWordPosition="1129">ding to (A, “met”, [with B, in C]). The TreeKernel (Xu et al., 2013) method uses a dependency tree kernel to classify whether candidate tree paths are indeed instances of relations. The shortest path between the two entities along with the shortest path between relational words and an entity are used as input to the tree kernel. An expanded set of syntactic patterns based on those from ReVerb are used to generate relation candidates. ORE via semantic parsing. Recently, a method based on SRL, called SRL-IE, has shown that the effectiveness of ORE methods can be improved with semantic features (Christensen et al., 2011). We implemented our version of SRL-IE by relying on the output of two SRL systems: Lund (Johansson and Nugues, 2008) and SwiRL (Surdeanu et al., 2003). SwiRL is trained on PropBank and expands upon the syntactic features used in previous work. One of its major limitations is that it is only able to label arguments with verb predicates. Lund, on the other hand, is based on dependency parsing and is trained on both PropBank and NomBank, making it able to extract relations with both verb and noun predicates. 3 Experimental Study This section compares the effectiveness and efficiency of the follo</context>
</contexts>
<marker>Christensen, Mausam, Etzioni, 2011</marker>
<rawString>Janara Christensen, Mausam, Stephen Soderland, and Oren Etzioni. 2011. An analysis of open information extraction based on semantic role labeling. In Proceedings of the Sixth International Conference on Knowledge Capture, pages 113–120. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL’12,</booktitle>
<pages>708--716</pages>
<contexts>
<context position="22262" citStr="Cucerzan, 2007" startWordPosition="3628" endWordPosition="3629">em, while being almost two orders of magnitude faster. 3.5 Automatically Annotated Sentences The creation of datasets for open RE is an extremely time-consuming task. In this section we investigate whether external data sources such as Freebase1 and WordNet2 can be used to automatically annotate a dataset, leading to a useful benchmark. Our automatic annotator identifies pairs of entities and a trigger of the relation between them. It does so by first trying to link all entities to Wikipedia (and consequently to Freebase, since Freebase is linked to Wikipedia) by using the method proposed by (Cucerzan, 2007). Given two entities appearing within 10 tokens of each other in a sentence, our annotator checks whether there is a relation connecting them in Freebase. If such a relation exists, the annotator looks for a trigger in the sentence. A trigger must be a synonym for the Freebase relation (according to WordNet) and its distance to the nearest entity cannot be more than 5 tokens. We applied this method for the New York Times and were able to annotate over 60,000 sentences with over 13,000 distinct entity pairs. For our experiments, we randomly selected one sentence for each entity pair and separat</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>Silviu Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In Proceedings of Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL’12, pages 708– 716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nilesh Dalvi</author>
<author>Ashwin Machanavajjhala</author>
<author>Bo Pang</author>
</authors>
<title>An analysis of structured data on the web.</title>
<date>2012</date>
<booktitle>Proc. VLDB Endow.,</booktitle>
<volume>5</volume>
<issue>7</issue>
<contexts>
<context position="4800" citStr="Dalvi et al., 2012" startWordPosition="745" endWordPosition="748">ll previous methods in terms of accuracy, losing to the shallow methods only in terms of throughput. As for n-ary relations, EXEMPLAR outperforms the methods that support this kind of extraction. 2 Related Work Others have pointed out the importance of understanding the trade-off between “shallow” versus “deep” NLP in ORE. One side of the argument favors shallow methods, claiming deep NLP costs orders of magnitude more and provide much less dramatic gains in terms of effectiveness (Christensen et al., 2011). The counterpoint, illustrated with a recent analysis on a industrial-scale Web crawl (Dalvi et al., 2012), is that the diversity with which information is encoded in text is too high. Framing the debate as “shallow” versus “deep” is perhaps convenient, but nevertheless an oversimplification. This paper sheds more light into the debate by comparing the state-of-the-art from three broad classes of approaches. Shallow ORE. TextRunner (Banko and Etzioni, 2008) and its successor ReVerb (Fader et al., 2011) are based on the idea that most relations are expressed using few syntactic patterns. ReVerb, for example, detects only three types of relations (“verb”, “verb+preposition” and “verb+noun+prepositio</context>
</contexts>
<marker>Dalvi, Machanavajjhala, Pang, 2012</marker>
<rawString>Nilesh Dalvi, Ashwin Machanavajjhala, and Bo Pang. 2012. An analysis of structured data on the web. Proc. VLDB Endow., 5(7):680–691.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying Relations for Open Information Extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing, EMNLP’12.</booktitle>
<contexts>
<context position="1569" citStr="Fader et al., 2011" startWordPosition="224" endWordPosition="227">roduction Open Relation Extraction (ORE) (Banko and Etzioni, 2008) has become prevalent over traditional relation extraction methods, especially on the Web, because of the intrinsic difficulty in training individual extractors for every single relation. Broadly speaking, existing ORE approaches can be grouped according to the level of sophistication of the NLP techniques they rely upon: (1) shallow parsing, (2) dependency parsing and (3) semantic role labelling (SRL). Shallow methods annotate the sentences with part-of-speech (POS) tags and the ORE approaches in this category, such as ReVerb (Fader et al., 2011) and SONEX (Merhav et al., 2012), identify relations by matching patterns over such tags. Dependency parsing gives unambiguous relations among each word in the sentence, and the ORE approaches in this category such as PATTY (Nakashole et al., 2012), OLLIE (Mausam et al., 2012), and TreeKernel (Xu et al., 2013) identify whole subtrees connecting the relation predicate and its arguments. Finally, semantic annotators, such as Lund (Johansson and Nugues, 2008) and SwiRL (Surdeanu et al., 2003), add roles to each node in a parse tree, enabling ORE approaches that identify the precise connection bet</context>
<context position="5201" citStr="Fader et al., 2011" startWordPosition="809" endWordPosition="812">ers of magnitude more and provide much less dramatic gains in terms of effectiveness (Christensen et al., 2011). The counterpoint, illustrated with a recent analysis on a industrial-scale Web crawl (Dalvi et al., 2012), is that the diversity with which information is encoded in text is too high. Framing the debate as “shallow” versus “deep” is perhaps convenient, but nevertheless an oversimplification. This paper sheds more light into the debate by comparing the state-of-the-art from three broad classes of approaches. Shallow ORE. TextRunner (Banko and Etzioni, 2008) and its successor ReVerb (Fader et al., 2011) are based on the idea that most relations are expressed using few syntactic patterns. ReVerb, for example, detects only three types of relations (“verb”, “verb+preposition” and “verb+noun+preposition”). Following a similar approach, SONEX (Merhav et al., 2012) extends ReVerb by detecting patterns with appositions and possessives. ORE via dependency parsing. PATTY (Nakashole et al., 2012) extracts textual patterns from sentences based on paths in the dependency graph. For all pairs of named entities, PATTY finds the shortest Dataset Source # Sentences # Relations WEB-500 Search Snippets 500 46</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying Relations for Open Information Extraction. In Proceedings of Conference on Empirical Methods in Natural Language Processing, EMNLP’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL’05,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="30887" citStr="Finkel et al., 2005" startWordPosition="5028" endWordPosition="5031">d converts the original text into sentences, each containing a list of tokens. Each token is tagged with part of speech, lemma and dependencies. EXEMPLAR also works with other dependency parsers based on Stanford’s dependencies, 454 such as the Malt parser (Nivre and Nilsson, 2004). Figure 4 illustrates our running example where each word is a token and arrows represent dependencies among tokens. In this example, “stadium” depends on “approves” and the arrow connecting them can be read as “the direct object of approves is stadium”. Extracting Named Entities. EXEMPLAR employs the Stanford NER (Finkel et al., 2005) to recognize named entities. We consider these types of entities: people, organization, location, miscellaneous and date. Figure 4 shows entities highlighted in bold. A.3 Detecting triggers After recognizing entities, EXEMPLAR detects relation triggers. A trigger is a single token that indicates the presence of a relation. A relation may present one or two triggers. For instance, the relation in our running example has two triggers. EXEMPLAR also uses triggers to determine the relation name, as discussed later. A trigger can be any noun or verb that was not tagged as being part of an entity m</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL’05, pages 363–370, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based semantic role labeling of propbank.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP’08,</booktitle>
<pages>69--78</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2029" citStr="Johansson and Nugues, 2008" startWordPosition="299" endWordPosition="303">ole labelling (SRL). Shallow methods annotate the sentences with part-of-speech (POS) tags and the ORE approaches in this category, such as ReVerb (Fader et al., 2011) and SONEX (Merhav et al., 2012), identify relations by matching patterns over such tags. Dependency parsing gives unambiguous relations among each word in the sentence, and the ORE approaches in this category such as PATTY (Nakashole et al., 2012), OLLIE (Mausam et al., 2012), and TreeKernel (Xu et al., 2013) identify whole subtrees connecting the relation predicate and its arguments. Finally, semantic annotators, such as Lund (Johansson and Nugues, 2008) and SwiRL (Surdeanu et al., 2003), add roles to each node in a parse tree, enabling ORE approaches that identify the precise connection between each argument and the predicate in a relation, independently. The first contribution of the paper is an objective and fair experimental comparison of the stateof-the-art in ORE, on 5 datasets with varying degree of “difficulty”. Of these, 4 datasets were annotated manually, covering both well-formed sentences, from the New York Times (NYT) and the Penn Treebank, as well as mixed-quality sentences from a popular Web corpus. A much larger corpus with 12</context>
<context position="7277" citStr="Johansson and Nugues, 2008" startWordPosition="1147" endWordPosition="1150">fy whether candidate tree paths are indeed instances of relations. The shortest path between the two entities along with the shortest path between relational words and an entity are used as input to the tree kernel. An expanded set of syntactic patterns based on those from ReVerb are used to generate relation candidates. ORE via semantic parsing. Recently, a method based on SRL, called SRL-IE, has shown that the effectiveness of ORE methods can be improved with semantic features (Christensen et al., 2011). We implemented our version of SRL-IE by relying on the output of two SRL systems: Lund (Johansson and Nugues, 2008) and SwiRL (Surdeanu et al., 2003). SwiRL is trained on PropBank and expands upon the syntactic features used in previous work. One of its major limitations is that it is only able to label arguments with verb predicates. Lund, on the other hand, is based on dependency parsing and is trained on both PropBank and NomBank, making it able to extract relations with both verb and noun predicates. 3 Experimental Study This section compares the effectiveness and efficiency of the following ORE methods: ReVerb, 448 Method NYT-500 WEB-500 PENN-100 Time P R F Time P R F Time P R F ReVerb 0.02 0.70 0.11 </context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based semantic role labeling of propbank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP’08, pages 69–78, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL’03,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8839" citStr="Klein and Manning, 2003" startWordPosition="1431" endWordPosition="1434">.23 0.32 0.48 0.71 0.48 0.57 0.66 0.46 0.24 0.31 SwiRL 2.96 0.63 0.10 0.17 1.73 0.97 0.34 0.50 2.17 0.89 0.16 0.27 Lund 11.40 0.78 0.24 0.37 2.69 0.91 0.37 0.52 5.21 0.86 0.35 0.50 TreeKernel – – – – – – – – 0.85 0.85 0.33 0.48 Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity. SONEX, OLLIE, PATTY, TreeKernel, SwiRL, Lund and our two variants of our method, EXEMPLAR, explained in detail in Appendix A: (1) EXEMPLAR[S] uses the Stanford parser (Klein and Manning, 2003) and (2) EXEMPLAR[M] uses the Malt parser (Nivre and Nilsson, 2004). 3.1 Binary Relations – Setup We start by evaluating the extraction of binary relations. Table 1 shows our experimental datasets. WEB-500 is a commonly used dataset, developed for the TextRunner experiments (Banko and Etzioni, 2008). These sentences are often incomplete and grammatically unsound, representing the challenges of dealing with web text. NYT-500 represents the other end of the spectrum with formal, well written new stories from the New York Times Corpus (Sandhaus, 2008). PENN-100 contains sentences from the Penn Tr</context>
<context position="30256" citStr="Klein and Manning, 2003" startWordPosition="4922" endWordPosition="4925">m in Atlanta. dobj prep_in Figure 4: A input sentence after pre-processing. Entities are in bold, triggers are underline and arrows represent dependencies. syntactic roles correspond to the same semantic role across different relations (Chambers and Jurafsky, 2011). However, this problem is out of the scope of this work. A.1 The method EXEMPLAR takes a stream of textual documents and extracts instances of n-ary relations as illustrated in Figure 3. A.2 Preprocessing Given a document, EXEMPLAR extracts its syntactic structure by applying a pipeline of NLP tools provided by the Stanford Parser (Klein and Manning, 2003). Our method converts the original text into sentences, each containing a list of tokens. Each token is tagged with part of speech, lemma and dependencies. EXEMPLAR also works with other dependency parsers based on Stanford’s dependencies, 454 such as the Malt parser (Nivre and Nilsson, 2004). Figure 4 illustrates our running example where each word is a token and arrows represent dependencies among tokens. In this example, “stadium” depends on “approves” and the arrow connecting them can be read as “the direct object of approves is stadium”. Extracting Named Entities. EXEMPLAR employs the Sta</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL’03, pages 423–430, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Schmitz Mausam</author>
<author>Robert Bart</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Open language learning for information extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL’12.</booktitle>
<contexts>
<context position="1846" citStr="Mausam et al., 2012" startWordPosition="270" endWordPosition="273">ting ORE approaches can be grouped according to the level of sophistication of the NLP techniques they rely upon: (1) shallow parsing, (2) dependency parsing and (3) semantic role labelling (SRL). Shallow methods annotate the sentences with part-of-speech (POS) tags and the ORE approaches in this category, such as ReVerb (Fader et al., 2011) and SONEX (Merhav et al., 2012), identify relations by matching patterns over such tags. Dependency parsing gives unambiguous relations among each word in the sentence, and the ORE approaches in this category such as PATTY (Nakashole et al., 2012), OLLIE (Mausam et al., 2012), and TreeKernel (Xu et al., 2013) identify whole subtrees connecting the relation predicate and its arguments. Finally, semantic annotators, such as Lund (Johansson and Nugues, 2008) and SwiRL (Surdeanu et al., 2003), add roles to each node in a parse tree, enabling ORE approaches that identify the precise connection between each argument and the predicate in a relation, independently. The first contribution of the paper is an objective and fair experimental comparison of the stateof-the-art in ORE, on 5 datasets with varying degree of “difficulty”. Of these, 4 datasets were annotated manuall</context>
<context position="6098" citStr="Mausam et al., 2012" startWordPosition="951" endWordPosition="954">by detecting patterns with appositions and possessives. ORE via dependency parsing. PATTY (Nakashole et al., 2012) extracts textual patterns from sentences based on paths in the dependency graph. For all pairs of named entities, PATTY finds the shortest Dataset Source # Sentences # Relations WEB-500 Search Snippets 500 461 NYT-500 New York Times 500 150 PENN-100 Penn Treebank 100 51 Table 1: Binary relation datasets. path in the dependency graph that connects the two named entities. They limit the search to only paths that start with one of these dependencies: nsubj, rcmod and partmod. OLLIE (Mausam et al., 2012) also extracts relations between two entities. It applies pattern templates over the dependency subtree containing pairs of entities. Pattern templates are learned automatically from a large training set that is bootstrapped from high confidence extractions from ReVerb. OLLIE merges binary relations that differ only in the preposition and second argument to produce n-ary extractions, as in: (A, “met with”, B) and (A, “met in”, C) leading to (A, “met”, [with B, in C]). The TreeKernel (Xu et al., 2013) method uses a dependency tree kernel to classify whether candidate tree paths are indeed insta</context>
</contexts>
<marker>Mausam, Bart, Soderland, Etzioni, 2012</marker>
<rawString>Mausam, Michael Schmitz, Robert Bart, Stephen Soderland, and Oren Etzioni. 2012. Open language learning for information extraction. In Proceedings of Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Merhav</author>
</authors>
<title>Filipe de S´a Mesquita, Denilson Barbosa, Wai Gen Yee, and Ophir Frieder.</title>
<date>2012</date>
<journal>TWEB,</journal>
<volume>6</volume>
<issue>3</issue>
<marker>Merhav, 2012</marker>
<rawString>Yuval Merhav, Filipe de S´a Mesquita, Denilson Barbosa, Wai Gen Yee, and Ophir Frieder. 2012. Extracting information networks from the blogosphere. TWEB, 6(3):11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ndapandula Nakashole</author>
<author>Gerhard Weikum</author>
<author>Fabian Suchanek</author>
</authors>
<title>Patty: a taxonomy of relational patterns with semantic types.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL’12,</booktitle>
<pages>1135--1145</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1817" citStr="Nakashole et al., 2012" startWordPosition="265" endWordPosition="268">relation. Broadly speaking, existing ORE approaches can be grouped according to the level of sophistication of the NLP techniques they rely upon: (1) shallow parsing, (2) dependency parsing and (3) semantic role labelling (SRL). Shallow methods annotate the sentences with part-of-speech (POS) tags and the ORE approaches in this category, such as ReVerb (Fader et al., 2011) and SONEX (Merhav et al., 2012), identify relations by matching patterns over such tags. Dependency parsing gives unambiguous relations among each word in the sentence, and the ORE approaches in this category such as PATTY (Nakashole et al., 2012), OLLIE (Mausam et al., 2012), and TreeKernel (Xu et al., 2013) identify whole subtrees connecting the relation predicate and its arguments. Finally, semantic annotators, such as Lund (Johansson and Nugues, 2008) and SwiRL (Surdeanu et al., 2003), add roles to each node in a parse tree, enabling ORE approaches that identify the precise connection between each argument and the predicate in a relation, independently. The first contribution of the paper is an objective and fair experimental comparison of the stateof-the-art in ORE, on 5 datasets with varying degree of “difficulty”. Of these, 4 da</context>
<context position="5592" citStr="Nakashole et al., 2012" startWordPosition="865" endWordPosition="869">versimplification. This paper sheds more light into the debate by comparing the state-of-the-art from three broad classes of approaches. Shallow ORE. TextRunner (Banko and Etzioni, 2008) and its successor ReVerb (Fader et al., 2011) are based on the idea that most relations are expressed using few syntactic patterns. ReVerb, for example, detects only three types of relations (“verb”, “verb+preposition” and “verb+noun+preposition”). Following a similar approach, SONEX (Merhav et al., 2012) extends ReVerb by detecting patterns with appositions and possessives. ORE via dependency parsing. PATTY (Nakashole et al., 2012) extracts textual patterns from sentences based on paths in the dependency graph. For all pairs of named entities, PATTY finds the shortest Dataset Source # Sentences # Relations WEB-500 Search Snippets 500 461 NYT-500 New York Times 500 150 PENN-100 Penn Treebank 100 51 Table 1: Binary relation datasets. path in the dependency graph that connects the two named entities. They limit the search to only paths that start with one of these dependencies: nsubj, rcmod and partmod. OLLIE (Mausam et al., 2012) also extracts relations between two entities. It applies pattern templates over the dependenc</context>
</contexts>
<marker>Nakashole, Weikum, Suchanek, 2012</marker>
<rawString>Ndapandula Nakashole, Gerhard Weikum, and Fabian Suchanek. 2012. Patty: a taxonomy of relational patterns with semantic types. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL’12, pages 1135–1145, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of Computational Natural Language Learning, CoNLL’04.</booktitle>
<contexts>
<context position="8906" citStr="Nivre and Nilsson, 2004" startWordPosition="1442" endWordPosition="1445">10 0.17 1.73 0.97 0.34 0.50 2.17 0.89 0.16 0.27 Lund 11.40 0.78 0.24 0.37 2.69 0.91 0.37 0.52 5.21 0.86 0.35 0.50 TreeKernel – – – – – – – – 0.85 0.85 0.33 0.48 Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity. SONEX, OLLIE, PATTY, TreeKernel, SwiRL, Lund and our two variants of our method, EXEMPLAR, explained in detail in Appendix A: (1) EXEMPLAR[S] uses the Stanford parser (Klein and Manning, 2003) and (2) EXEMPLAR[M] uses the Malt parser (Nivre and Nilsson, 2004). 3.1 Binary Relations – Setup We start by evaluating the extraction of binary relations. Table 1 shows our experimental datasets. WEB-500 is a commonly used dataset, developed for the TextRunner experiments (Banko and Etzioni, 2008). These sentences are often incomplete and grammatically unsound, representing the challenges of dealing with web text. NYT-500 represents the other end of the spectrum with formal, well written new stories from the New York Times Corpus (Sandhaus, 2008). PENN-100 contains sentences from the Penn Treebank recently used in an evaluation of the TreeKernel method (Xu </context>
<context position="30549" citStr="Nivre and Nilsson, 2004" startWordPosition="4971" endWordPosition="4974">ut of the scope of this work. A.1 The method EXEMPLAR takes a stream of textual documents and extracts instances of n-ary relations as illustrated in Figure 3. A.2 Preprocessing Given a document, EXEMPLAR extracts its syntactic structure by applying a pipeline of NLP tools provided by the Stanford Parser (Klein and Manning, 2003). Our method converts the original text into sentences, each containing a list of tokens. Each token is tagged with part of speech, lemma and dependencies. EXEMPLAR also works with other dependency parsers based on Stanford’s dependencies, 454 such as the Malt parser (Nivre and Nilsson, 2004). Figure 4 illustrates our running example where each word is a token and arrows represent dependencies among tokens. In this example, “stadium” depends on “approves” and the arrow connecting them can be read as “the direct object of approves is stadium”. Extracting Named Entities. EXEMPLAR employs the Stanford NER (Finkel et al., 2005) to recognize named entities. We consider these types of entities: people, organization, location, miscellaneous and date. Figure 4 shows entities highlighted in bold. A.3 Detecting triggers After recognizing entities, EXEMPLAR detects relation triggers. A trigg</context>
</contexts>
<marker>Nivre, Nilsson, 2004</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2004. Memory-based dependency parsing. In Proceedings of Computational Natural Language Learning, CoNLL’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<title>The new york times annotated corpus.</title>
<date>2008</date>
<note>http://www.ldc.upenn.edu/ Catalog/catalogEntry.jsp?catalogId= LDC2008T19.</note>
<contexts>
<context position="9393" citStr="Sandhaus, 2008" startWordPosition="1518" endWordPosition="1520">XEMPLAR[S] uses the Stanford parser (Klein and Manning, 2003) and (2) EXEMPLAR[M] uses the Malt parser (Nivre and Nilsson, 2004). 3.1 Binary Relations – Setup We start by evaluating the extraction of binary relations. Table 1 shows our experimental datasets. WEB-500 is a commonly used dataset, developed for the TextRunner experiments (Banko and Etzioni, 2008). These sentences are often incomplete and grammatically unsound, representing the challenges of dealing with web text. NYT-500 represents the other end of the spectrum with formal, well written new stories from the New York Times Corpus (Sandhaus, 2008). PENN-100 contains sentences from the Penn Treebank recently used in an evaluation of the TreeKernel method (Xu et al., 2013). We manually annotated the relations for WEB-500 and NYT500 and use the PENN-100 annotations provided by TreeKernel’s authors (Xu et al., 2013). We annotate each sentence manually as follows. We identify exactly two entities and a trigger (a single token indicating a relation–see Section A.3) for the relation between them, if one exists. In addition, we specify a window of tokens allowed to be in a relation, including modifiers of the trigger and prepositions connectin</context>
<context position="25869" citStr="Sandhaus, 2008" startWordPosition="4221" endWordPosition="4222">port. A EXEMPLAR ORE methods must recognize relations from the text alone. To do this, each method tries to model how relations are expressed in English. Banko and Etzioni claim that more than 90% of binary relations are expressed through a few syntactic patterns, such as “verb” and “noun+preposition” (Banko and Etzioni, 2008). It is unclear, however, whether n-ary relations follow. This section presents a study focusing on how nary relations (n &gt; 2) are expressed in English, based on 100 distinct relations manually annotated from a random sample of 514 sentences in the New York Times Corpus (Sandhaus, 2008). Table 5 shows six syntactic patterns that cover 86% of our n-ary relations. These patterns are slightly different from those used for binary relations. In binary relations, the pattern implicitly defines the roles of the two arguments. For instance, a relation “met with” indicates that the first argument is the subject and the second one is the object of “with”. In order to represent n-ary relations, our patterns do not contain prepositions, possessives or any other word connecting the relation to the argument. For instance, the sentence “Obama met with Putin in Russia” contains the relation</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus. 2008. The new york times annotated corpus. http://www.ldc.upenn.edu/ Catalog/catalogEntry.jsp?catalogId= LDC2008T19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sanda Harabagiu</author>
<author>John Williams</author>
<author>Paul Aarseth</author>
</authors>
<title>Using predicate-argument structures for information extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>8--15</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2063" citStr="Surdeanu et al., 2003" startWordPosition="306" endWordPosition="309">notate the sentences with part-of-speech (POS) tags and the ORE approaches in this category, such as ReVerb (Fader et al., 2011) and SONEX (Merhav et al., 2012), identify relations by matching patterns over such tags. Dependency parsing gives unambiguous relations among each word in the sentence, and the ORE approaches in this category such as PATTY (Nakashole et al., 2012), OLLIE (Mausam et al., 2012), and TreeKernel (Xu et al., 2013) identify whole subtrees connecting the relation predicate and its arguments. Finally, semantic annotators, such as Lund (Johansson and Nugues, 2008) and SwiRL (Surdeanu et al., 2003), add roles to each node in a parse tree, enabling ORE approaches that identify the precise connection between each argument and the predicate in a relation, independently. The first contribution of the paper is an objective and fair experimental comparison of the stateof-the-art in ORE, on 5 datasets with varying degree of “difficulty”. Of these, 4 datasets were annotated manually, covering both well-formed sentences, from the New York Times (NYT) and the Penn Treebank, as well as mixed-quality sentences from a popular Web corpus. A much larger corpus with 12,000 sentences from NYT, automatic</context>
<context position="7311" citStr="Surdeanu et al., 2003" startWordPosition="1153" endWordPosition="1156">eed instances of relations. The shortest path between the two entities along with the shortest path between relational words and an entity are used as input to the tree kernel. An expanded set of syntactic patterns based on those from ReVerb are used to generate relation candidates. ORE via semantic parsing. Recently, a method based on SRL, called SRL-IE, has shown that the effectiveness of ORE methods can be improved with semantic features (Christensen et al., 2011). We implemented our version of SRL-IE by relying on the output of two SRL systems: Lund (Johansson and Nugues, 2008) and SwiRL (Surdeanu et al., 2003). SwiRL is trained on PropBank and expands upon the syntactic features used in previous work. One of its major limitations is that it is only able to label arguments with verb predicates. Lund, on the other hand, is based on dependency parsing and is trained on both PropBank and NomBank, making it able to extract relations with both verb and noun predicates. 3 Experimental Study This section compares the effectiveness and efficiency of the following ORE methods: ReVerb, 448 Method NYT-500 WEB-500 PENN-100 Time P R F Time P R F Time P R F ReVerb 0.02 0.70 0.11 0.18 0.01 0.92 0.29 0.44 0.02 0.78</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Using predicate-argument structures for information extraction. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 8–15. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Xu</author>
<author>Mi-Young Kim</author>
<author>Kevin Quinn</author>
<author>Randy Goebel</author>
<author>Denilson Barbosa</author>
</authors>
<title>Open information extraction with tree kernels.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>868--877</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="1880" citStr="Xu et al., 2013" startWordPosition="277" endWordPosition="280">ording to the level of sophistication of the NLP techniques they rely upon: (1) shallow parsing, (2) dependency parsing and (3) semantic role labelling (SRL). Shallow methods annotate the sentences with part-of-speech (POS) tags and the ORE approaches in this category, such as ReVerb (Fader et al., 2011) and SONEX (Merhav et al., 2012), identify relations by matching patterns over such tags. Dependency parsing gives unambiguous relations among each word in the sentence, and the ORE approaches in this category such as PATTY (Nakashole et al., 2012), OLLIE (Mausam et al., 2012), and TreeKernel (Xu et al., 2013) identify whole subtrees connecting the relation predicate and its arguments. Finally, semantic annotators, such as Lund (Johansson and Nugues, 2008) and SwiRL (Surdeanu et al., 2003), add roles to each node in a parse tree, enabling ORE approaches that identify the precise connection between each argument and the predicate in a relation, independently. The first contribution of the paper is an objective and fair experimental comparison of the stateof-the-art in ORE, on 5 datasets with varying degree of “difficulty”. Of these, 4 datasets were annotated manually, covering both well-formed sente</context>
<context position="6603" citStr="Xu et al., 2013" startWordPosition="1034" endWordPosition="1037"> to only paths that start with one of these dependencies: nsubj, rcmod and partmod. OLLIE (Mausam et al., 2012) also extracts relations between two entities. It applies pattern templates over the dependency subtree containing pairs of entities. Pattern templates are learned automatically from a large training set that is bootstrapped from high confidence extractions from ReVerb. OLLIE merges binary relations that differ only in the preposition and second argument to produce n-ary extractions, as in: (A, “met with”, B) and (A, “met in”, C) leading to (A, “met”, [with B, in C]). The TreeKernel (Xu et al., 2013) method uses a dependency tree kernel to classify whether candidate tree paths are indeed instances of relations. The shortest path between the two entities along with the shortest path between relational words and an entity are used as input to the tree kernel. An expanded set of syntactic patterns based on those from ReVerb are used to generate relation candidates. ORE via semantic parsing. Recently, a method based on SRL, called SRL-IE, has shown that the effectiveness of ORE methods can be improved with semantic features (Christensen et al., 2011). We implemented our version of SRL-IE by r</context>
<context position="9519" citStr="Xu et al., 2013" startWordPosition="1537" endWordPosition="1540">04). 3.1 Binary Relations – Setup We start by evaluating the extraction of binary relations. Table 1 shows our experimental datasets. WEB-500 is a commonly used dataset, developed for the TextRunner experiments (Banko and Etzioni, 2008). These sentences are often incomplete and grammatically unsound, representing the challenges of dealing with web text. NYT-500 represents the other end of the spectrum with formal, well written new stories from the New York Times Corpus (Sandhaus, 2008). PENN-100 contains sentences from the Penn Treebank recently used in an evaluation of the TreeKernel method (Xu et al., 2013). We manually annotated the relations for WEB-500 and NYT500 and use the PENN-100 annotations provided by TreeKernel’s authors (Xu et al., 2013). We annotate each sentence manually as follows. We identify exactly two entities and a trigger (a single token indicating a relation–see Section A.3) for the relation between them, if one exists. In addition, we specify a window of tokens allowed to be in a relation, including modifiers of the trigger and prepositions connecting triggers to their arguments. For each sentence annotated with two entities, a system must extract a string representing the </context>
<context position="14275" citStr="Xu et al., 2013" startWordPosition="2328" endWordPosition="2331"> relation instances. The other two datasets, on the other hand, contain randomly chosen sentences. Although WEB-500 is a popular 0.01 0.1 1 10 seconds per sentence Figure 1: Average f-measure vs average time for NYT-500, WEB-500 and PEN-100. 0 0.1 0.2 0.3 0.4 0.5 0.6 recall Figure 2: Average precision vs average recall for NYT-500, WEB-500 and PEN-100. dataset, it perhaps does not represent the challenges found in web text. We were unable to run TreeKernel for NYT-500 and WEB-500 for lack of training data. We ran TreeKernel, as trained by its authors, on the same test set used in their paper (Xu et al., 2013). Comparing methods based on effectiveness (fmeasure) or efficiency (computational cost) alone can be misleading. To do so, we compare methods in terms of dominance. We say method A dominates method B if A is: (1) more effective and as efficient as B; (2) more efficient and as effective as B; or (3) both more effective and more efficient than B. The methods that are not dominated by any other form the state-of-the-art. Figure 1 plots the effectiveness and efficiency of SONEX EXEMPLAR[M] REVERB OLLIE LUND EXEMPLAR[S] PATTY SWIRL precision 0.9 0.8 0.7 0.6 0.5 1 REVERB EXEMPLAR[M] SWIRL LUND SONE</context>
</contexts>
<marker>Xu, Kim, Quinn, Goebel, Barbosa, 2013</marker>
<rawString>Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel, and Denilson Barbosa. 2013. Open information extraction with tree kernels. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 868–877, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
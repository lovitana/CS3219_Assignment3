<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000520">
<title confidence="0.876374">
Leveraging lexical cohesion and disruption for topic segmentation
</title>
<author confidence="0.683745">
Anca S¸imon Guillaume Gravier Pascale S´ebillot
</author>
<affiliation confidence="0.583333">
Universit´e de Rennes 1 CNRS INSA de Rennes
</affiliation>
<address confidence="0.353842">
IRISA &amp; INRIA Rennes IRISA &amp; INRIA Rennes IRISA &amp; INRIA Rennes
</address>
<email confidence="0.972036">
anca-roxana.simon@irisa.fr
guillaume.gravier@irisa.fr
pascale.sebillot@irisa.fr
</email>
<sectionHeader confidence="0.998544" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997293">
Topic segmentation classically relies on one
of two criteria, either finding areas with co-
herent vocabulary use or detecting discontinu-
ities. In this paper, we propose a segmenta-
tion criterion combining both lexical cohesion
and disruption, enabling a trade-off between
the two. We provide the mathematical formu-
lation of the criterion and an efficient graph
based decoding algorithm for topic segmenta-
tion. Experimental results on standard textual
data sets and on a more challenging corpus
of automatically transcribed broadcast news
shows demonstrate the benefit of such a com-
bination. Gains were observed in all condi-
tions, with segments of either regular or vary-
ing length and abrupt or smooth topic shifts.
Long segments benefit more than short seg-
ments. However the algorithm has proven ro-
bust on automatic transcripts with short seg-
ments and limited vocabulary reoccurrences.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999935543478261">
Topic segmentation consists in evidentiating the se-
mantic structure of a document: Algorithms devel-
oped for this task aim at automatically detecting
frontiers which define topically coherent segments
in a text.
Various methods for topic segmentation of tex-
tual data are described in the literature, e.g., (Rey-
nar, 1994; Hearst, 1997; Ferret et al., 1998; Choi,
2000; Moens and Busser, 2001; Utiyama and Isa-
hara, 2001), most of them relying on the notion of
lexical cohesion, i.e., identifying segments with a
consistent use of vocabulary, either based on words
or on semantic relations between words. Reoccur-
rences of words or related words and lexical chains
are two popular methods to evidence lexical cohe-
sion. This general principle of lexical cohesion is
further exploited for topic segmentation with two
radically different strategies. On the one hand, a
measure of the lexical cohesion can be used to deter-
mine coherent segments (Reynar, 1994; Moens and
Busser, 2001; Utiyama and Isahara, 2001). On the
other hand, shifts in the use of vocabulary can be
searched for to directly identify the segment fron-
tiers by measuring the lexical disruption (Hearst,
1997).
Techniques based on the first strategy yield more
accurate segmentation results, but face a problem of
over-segmentation which can, up to now, only be
solved by providing prior information regarding the
distribution of segment length or the expected num-
ber of segments. In this paper, we propose a segmen-
tation criterion combining both cohesion and dis-
ruption along with the corresponding algorithm for
topic segmentation. Such a criterion ensures a co-
herent use of vocabulary within each resulting seg-
ment, as well as a significant difference of vocabu-
lary between neighboring segments. Moreover, the
combination of these two strategies enables regular-
izing the number of segments found without resort-
ing to prior knowledge.
This piece of work uses the algorithm of Utiyama
and Isahara (2001) as a starting point, a versatile and
performing topic segmentation algorithm cast in a
statistical framework. Among the benefits of this al-
gorithm are its independency to any particular do-
main and its ability to cope with thematic segments
</bodyText>
<page confidence="0.948393">
1314
</page>
<note confidence="0.7316345">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1314–1324,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999950475">
of highly varying lengths, two interesting features
to obtain a generic solution to the problem of topic
segmentation. Moreover, the algorithm has proven
to be up to the state of the art in several studies, with
no need of a priori information about the number of
segments (contrary to algorithms in (Malioutov and
Barzilay, 2006; Eisenstein and Barzilay, 2008) that
can attain a higher segmentation accuracy). It also
provides an efficient graph based implementation of
which we take advantage.
To account both for cohesion and disruption, we
extend the formalism of Isahara and Utiyama using
a Markovian assumption between segments in place
of the independence assumption of the original algo-
rithm. Keeping unchanged their probabilistic mea-
sure of lexical cohesion, the Markovian assumption
enables to introduce the disruption between two con-
secutive segments. We propose an extended graph
based decoding strategy, which is both optimal and
efficient, exploiting the notion of generalized seg-
ment model or semi hidden Markov models. Tests
are performed on standard textual data sets and on
a more challenging corpus of automatically tran-
scribed broadcast news shows.
The seminal idea of this paper was partially pub-
lished in (Simon et al., 2013) in the French language.
The current paper significantly elaborates on the lat-
ter, with a more detailed description of the algo-
rithm and additional contrastive experiments includ-
ing more data sets. In particular, new experiments
clearly demonstrate the benefit of the method in a
realistic setting with statistically significant gains.
The organization of the article is as follows. Ex-
isting work on topic segmentation is presented in
Section 2, emphasizing the motivations of the model
we propose. Section 3 details the baseline method
of Utiyama and Isahara before introducing our algo-
rithm. Experimental protocol and results are given
in Section 4. Section 5 summarizes the finding and
concludes with a discussion of future work.
</bodyText>
<sectionHeader confidence="0.999901" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999802509433962">
Defining the concept of theme precisely is not trivial
and a large number of definitions have been given by
linguists. Brown and Yule (1983) discuss at length
the difficulty of defining a topic and note: ”The
notion of ’topic’ is clearly an intuitively satisfac-
tory way of describing the unifying principle which
makes one stretch of discourse ’about’ something
and the next stretch ’about’ something else, for it
is appealed to very frequently in the discourse anal-
ysis literature. Yet the basis for the identi�cation of
’topic’ is rarely made explicit”. To skirt the issue of
defining a topic, they suggest to focus on topic-shift
markers and to identify topic changes, what most
current topic segmentation methods do.
Various characteristics can be exploited to iden-
tify thematic changes in text data. The most popular
ones rely either on the lexical distribution informa-
tion to measure lexical cohesion (i.e., word reoccur-
rences, lexical chains) or on linguistic markers such
as discourse markers which indicate continuity or
discontinuity (Grosz and Sidner, 1986; Litman and
Passonneau, 1995). Linguistic markers are however
often specific to a type of text and cannot be consid-
ered in a versatile approach as the one we are target-
ing, where versatility is achieved relying on the sole
lexical cohesion.
The key point with lexical cohesion is that a sig-
nificant change in the use of vocabulary is consid-
ered to be a sign of topic shift. This general idea
translates into two families of methods, local ones
targeting a local detection of lexical disruptions and
global ones relying on a measure of the lexical cohe-
sion to globally find segments exhibiting coherence
in their lexical distribution.
Local methods (Hearst, 1997; Ferret et al., 1998;
Hernandez and Grau, 2002; Claveau and Lef`evre,
2011) locally compare adjacent fixed size regions,
claiming a boundary when the similarity between
the adjacent regions is small enough, thus identify-
ing points of high lexical disruption. In the seminal
work of Hearst (1997), a fixed size window divided
into two adjacent blocks is used, consecutively cen-
tered at each potential boundary. Similarity between
the adjacent blocks is computed at each point, the re-
sulting similarity profile being analyzed to find sig-
nificant valleys which are considered as topic bound-
aries.
On the contrary, global methods (Reynar, 1994;
Choi, 2000; Utiyama and Isahara, 2001; Ji and Zha,
2003; Malioutov and Barzilay, 2006; Misra et al.,
2009) seek to maximize the value of the lexical co-
hesion on each segment resulting from the segmen-
tation globally on the text. Several approaches have
</bodyText>
<page confidence="0.985154">
1315
</page>
<bodyText confidence="0.999985105263158">
been taken relying on self-similarity matrices, such
as dot plots, or on graphs. A typical and state-of-the-
art algorithm is that of Utiyama and Isahara (2001)
whose principle is to search globally for the best
path in a graph representing all possible segmenta-
tions and where edges are valued according to the
lexical cohesion measured in a probabilistic way.
When the lengths of the respective topic segments
in a text (or between two texts) are very differ-
ent from one another, local methods are challenged.
Finding out an appropriate window size and extract-
ing boundaries become critical with segments of
varying length, in particular when short segments
are present. Short windows will render compari-
son of adjacent blocks difficult and unreliable while
long windows cannot handle short segments. The
lack of a global vision also makes it difficult to nor-
malize properly the similarities between blocks and
to deal with statistics on segment length. While
global methods override these drawbacks, they face
the problem of over-segmentation due to the fact that
they mainly rely on the sole lexical cohesion. Short
segments are therefore very likely to be coherent
which calls for regularization introduced as priors
on the segments length.
These considerations naturally lead to the idea of
methods combining lexical cohesion and disruption
to make the best of both worlds. While the two cri-
teria rely on the same underlying principle of lex-
ical coherence (Grosz et al., 1995) and might ap-
pear as redundant, the resulting algorithms are quite
different in their philosophy. A first (and, to the
best of our knowledge, unique) attempt at captur-
ing a global view of the local dissimilarities is de-
scribed in Malioutov and Barzilay (2006). However,
this method assumes that the number of segments to
find is known beforehand which makes it difficult
for real-world usage.
</bodyText>
<sectionHeader confidence="0.987096" genericHeader="method">
3 Combining lexical cohesion and
disruption
</sectionHeader>
<bodyText confidence="0.999987">
We extend the graph-based formalism of Utiyama
and Isahara to jointly account for lexical cohesion
and disruption in a global approach. Clearly, other
formalisms than the graph-based one could have
been considered. However, graph-based probabilis-
tic topic segmentation has proven very accurate and
versatile, relying on very minimal prior knowledge
on the texts to segment. Good results at the state-of-
the-art have also been reported in difficult conditions
with this approach (Misra et al., 2009; Claveau and
Lef`evre, 2011; Guinaudeau et al., 2012).
We briefly recall the principle of probabilistic
graph-based segmentation before detailing a Marko-
vian extension to account for disruption.
</bodyText>
<subsectionHeader confidence="0.999919">
3.1 Probabilistic graph-based segmentation
</subsectionHeader>
<bodyText confidence="0.992575875">
The idea of the probabilistic graph-based segmen-
tation algorithm is to find the segmentation into the
most coherent segments constrained by a prior dis-
tribution on segments length. This problem is cast
into finding the most probable segmentation of a se-
quence of t basic units (i.e., sentences or utterances
composed of words) W = ut1 among all possible
segmentations, i.e.,
</bodyText>
<equation confidence="0.994564">
S� = arg maxP[W|S]P[S] � (1)
S
</equation>
<bodyText confidence="0.999701666666667">
Assuming that segments are mutually independent
and assuming that basic units within a segment are
also independent, the probability of a text W for a
</bodyText>
<equation confidence="0.905296">
segmentation S = Sm1 is given by
P[wij|Si] , (2)
</equation>
<bodyText confidence="0.9986898">
where ni is the number of words in the segment
Si, wij is the jth word in Si and m the number of
segments. The probability P[wij|Si] is given by a
Laplace law where the parameters are estimated on
Si, i.e.,
</bodyText>
<equation confidence="0.997419333333333">
j) + 1
P[wi j|Si] = fi(wi, (3)
ni + k
</equation>
<bodyText confidence="0.9999885">
where fi(wij) is the number of occurrences of wi j
in Si and k is the total number of distinct words in
W, i.e., the size of the vocabulary V. This probabil-
ity favors segments that are homogeneous, increas-
ing when words are repeated and decreasing consis-
tently when they are different. The prior distribu-
tion on segment length is given by a simple model,
P[Sm1 ] = n−m, where n is the total number of
words, exhibiting a large value for a small number
of segments and conversely.
The optimization of Eq. 1 can be efficiently im-
plemented as the search for the best path in a
</bodyText>
<equation confidence="0.996112">
P[W|Sm1 ] =
m
i=1
nz
H
j=1
</equation>
<page confidence="0.900012">
1316
</page>
<bodyText confidence="0.999942375">
weighted graph which represents all the possible
segmentations. Each node in the graph corresponds
to a possible frontier placed between two utterances
(i.e., we have a node between each pair of utter-
ances), the arc between nodes i and j representing a
segment containing utterances ui+1 to uj. The cor-
responding arc weight is the generalized probability
of the words within segment Si→j according to
</bodyText>
<equation confidence="0.6303125">
v(i, j) = X j ln(P[uk|Si→j]) − αln(n)
k=i+1
</equation>
<bodyText confidence="0.999981333333333">
where the probability is given as in Eq. 3. The factor
α is introduced to control the trade-off between the
segments length and the lexical cohesion.
</bodyText>
<subsectionHeader confidence="0.998991">
3.2 Introduction of the lexical disruption
</subsectionHeader>
<bodyText confidence="0.999849444444444">
Eq. 2 derives from the assumption that each segment
Si is independent from the others, which makes it
impossible to consider disruption between two con-
secutive segments. To do so, the weight of an arc
corresponding to a segment Si should take into ac-
count how different this segment is from Si−1. This
is typically handled using a Markovian assumption
of order 1. Under this assumption, Eq. 2 is reformu-
lated as
</bodyText>
<equation confidence="0.960953666666667">
m
P[W|Sm 1 ] = P[W|S1] Y P[W |Si, Si−1] ,
i=2
</equation>
<bodyText confidence="0.999921888888889">
where the notion of disruption can be embedded in
the term P[W |Si, Si−1] which explicitly mentions
both segments. Formally, P[W |Si, Si−1] is defined
as a probability. However, arbitrary scores which do
not correspond to probabilities can be used instead
as the search for the best path in the graph of possi-
ble segmentations makes no use of probability the-
ory. In this study, we define the score of a segment
Si given Si−1 as
</bodyText>
<equation confidence="0.99432">
lnP[W|Si,Si−1] = lnP[Wi|Si] − λΔ(Wi,Wi−1)
</equation>
<bodyText confidence="0.987563961538462">
(4)
where Wi designates the set of utterances in Si
and the rightmost part reflects the disruption be-
tween the content of Si and of Si−1. Eq. 4 clearly
combines the measure of lexical cohesion with a
measure of the disruption between consecutive seg-
ments: Δ(Wi, Wi−1) &gt; 0 measures the coherence
between Si and Si−1, the substraction thus account-
ing for disruption by penalizing consecutive coher-
ent segments. The underlying assumption is that the
bigger Δ(Wi, Wi−1), the weaker the disruption be-
tween the two segments. Parameter λ controls the
respective contributions of cohesion and disruption.
We initially adopted a probabilistic measure
of disruption based on cross probabilities, i.e.,
P[Wi|Si−1] and P[Wi−1|Si], which proved to have
limited impact on the segmentation. We therefore
prefer to rely on a cosine similarity measure be-
tween the word vectors representing two adjacent
segments, building upon a classical strategy of lo-
cal methods such as TextTiling (Hearst, 1997). The
cosine similarity measure is calculated between vec-
tors representing the content of resp. Si and Si−1,
denoted vi and vi−1, where vi is a vector contain-
ing the (tf-idf) weight of each term of V in Si. The
cosine similarity is classically defined as
</bodyText>
<equation confidence="0.9986846">
P vi−1(v) vi(v)
v∈V (5)
Δ(Wi, Wi−1) is calculated from the cosine similar-
ity measure as
Δ(Wi,Wi−1) = (1 − cos(vi−1,vi))−1 , (6)
</equation>
<bodyText confidence="0.999729333333333">
thus yielding a small penalty in Eq. 4 for highly dis-
rupting boundaries, i.e., corresponding to low simi-
larity measure.
Given the quantities defined above, the algorithm
boils down to finding the best scoring segmentation
as given by
</bodyText>
<equation confidence="0.9921048">
Sˆ = arg max
S
m
λX Δ(Wi,Wi−1) − αmln(n) . (7)
i=2
</equation>
<subsectionHeader confidence="0.998182">
3.3 Segmentation algorithm
</subsectionHeader>
<bodyText confidence="0.999970333333333">
Translating Eq. 7 into an efficient algorithm is not
straightforward since all possible combinations of
adjacent segments need be considered. To do so in a
graph based approach, one needs to keep separated
the paths of different lengths ending in a given node.
In other words, only paths of the same length ending
</bodyText>
<equation confidence="0.999716666666667">
cos(vi−1, vi) =
rP v2 i−1(v) P
v∈V v∈V
v2i (v)
Xm ln(P[Wi|Si]) −
i=1
</equation>
<page confidence="0.910005">
1317
</page>
<bodyText confidence="0.999234133333333">
at a given point, with different predecessors, should
be recombined so that disruption can be considered
properly in subsequent steps of the algorithm. Note
that, in standard decoding as in Utiyama and Isa-
hara’s algorithm, only one of such paths, the best
scoring one, would be retained. We employ a strat-
egy inspired from the decoding strategy of segment
models or semi-hidden Markov model with explicit
duration model (Ostendorf et al., 1996; Delakis et
al., 2008).
Search is performed through a lattice L =
{V, E}, with V the set of nodes representing poten-
tial boundaries and E the set of edges representing
segments, i.e., a set of consecutive utterances. The
set V is defined as
</bodyText>
<equation confidence="0.996354">
V = {nij|0 ≤ i, j ≤ N} ,
</equation>
<bodyText confidence="0.999916428571429">
where nij represents a boundary after utterance ui
reached by a segment of length j utterances and
N = t+1. In the lattice example of Fig. 1, it is trivial
to see that for a given node, all incoming edges cover
the same segment. For example, the node n42 is po-
sitioned after u4 and all incoming segments contain
the two utterances u� and u4. Edges are defined as
</bodyText>
<equation confidence="0.998362">
E = {eip,jl|0 ≤ i,p,j,l ≤ N;
i &lt; j;i = j − l;Lmin ≤ l ≤ Lmax} ,
</equation>
<bodyText confidence="0.987356772727273">
where eip,jl connects nip and njl with the constraint
that l = j − i and Lmin ≤ l ≤ Lmax. Thus, an edge
eip,jl represents a segment of length l containing ut-
terances from ui+1 to uj, denoted 5i,j. In Fig. 1,
e01,33 represents a segment of length 3 from n01 to
n33, covering utterances u1 to u3. To avoid explo-
sion of the lattice, a maximum segment length Lmax
is defined. Symmetrically, a minimum segment size
can be used.
The property of this lattice, where, by construc-
tion, all edges out of a node have the same segment
as a predecessor, makes it possible to weight each
edge in the lattice according to Eq. 4. Consider a
node nij for which all incoming edges encompass
utterances ui−j to ui. For each edge out of nij,
whatever the target node (i.e., the edge length), one
can therefore easily determine the lexical cohesion
as defined by the generalized probability of Eq. 3
and the disruption with respect to the previous seg-
ment as defined by Eq. 6.
Algorithm 1 Maximum probability segmentation
Step 0. Initialization
</bodyText>
<equation confidence="0.935099833333333">
q[0][j] = 0 ∀j ∈ [Lmin, Lmax]
q[i][j] = −∞ ∀i ∈ [1, N], j ∈ [Lmin, Lmax]
Step 1. Assign best score to each node
for i = 0 → t do
for j = Lmin → Lmax do
for k = Lmin → Lmax do
/* extend path ending after ui with a
segment of length j with an arc of length k */
{ q[i + k][k],
q[i][j]+
Cohesion(ui+1 → ui+k)−
AA(ui−j → ui; ui+1 → ui+k)
</equation>
<listItem confidence="0.808449">
end for
end for
end for
</listItem>
<bodyText confidence="0.903705555555556">
Step 2. Backtrack from nNj with best score
q[N][j]
Given the weighted decoding graph, the solution
to Eq. 7 is obtained by finding out the best path in
the decoding lattice, which can be done straightfor-
wardly by scanning nodes in topological order. The
decoding algorithm is summarized in Algorithm 1
with an efficient implementation in o(NL2 max) which
does not require explicit construction of the lattice.
</bodyText>
<sectionHeader confidence="0.999738" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9998404">
Experiments are performed on three distinct corpora
which exhibit different characteristics, two contain-
ing textual data and one spoken data. We first
present the corpora before presenting and discussing
results on each.
</bodyText>
<subsectionHeader confidence="0.965637">
4.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999864">
The artificial data set of Choi (2000) is widely used
in the literature and enables comparison of a new
segmentation method with existing ones. Choi’s
data set consist of 700 documents, each created by
concatenating the first z sentences of 10 articles ran-
domly chosen from the Brown corpus, assuming
each article is on a different topic. Table 1 provides
</bodyText>
<equation confidence="0.931278">
q[i+k][k] = max
</equation>
<page confidence="0.967621">
1318
</page>
<figureCaption confidence="0.997158">
Figure 1: An example of a lattice L.
</figureCaption>
<table confidence="0.947772">
z = 3–11 3–5 6–8 9–11
# samples 400 100 100 100
</table>
<tableCaption confidence="0.9791625">
Table 1: Number of documents in Choi’s corpus (Choi,
2000).
</tableCaption>
<bodyText confidence="0.999919">
the corpus statistics, where z=3–11 means z is ran-
domly chosen in the range [3,11]. Hence, Choi’s
corpus is adapted to test the ability of our model
to deal with variable segments length, z=3–11 be-
ing the most difficult condition. Moreover, Choi’s
corpus provides a direct comparison with results re-
ported in the literature.
One of the main criticism of Choi’s data set is the
presence of abrupt topic changes due to the artifi-
cial construction of the corpus. We therefore re-
port results on a textual corpus with more natural
topic changes, also used in (Eisenstein and Barzi-
lay, 2008). The data set consists of 277 chapters
selected from (Walker et al., 1990), a medical text-
book, where each chapter—considered here as a
document—was divided by its author into themat-
ically coherent sections. The data set has a total of
1,136 segments with an average of 5 segments per
document and an average of 28 sentences per seg-
ment. This data set is used to study the impact of
smooth, natural, topic changes.
Finally, results are reported on a corpus of au-
tomatic transcripts of TV news spoken data. The
data set consists of 56 news programs (Pt1/2 hour
each), broadcasted in February and March 2007 on
the French television channel France 2, and tran-
scribed by two different automatic speech recogni-
tion (ASR) systems, namely IRENE (Huet et al.,
2010) and LIMSI (Gauvain et al., 2002), with re-
spective word error rates (WER) around 36 % and
30 %. Each news program consists of successive
reports of short duration (2-3 min), possibly with
consecutive reports on different facets of the same
news. The reference segmentation was established
by associating a topic with each report, i.e., plac-
ing a boundary at the beginning of a report’s in-
troduction (and hence at the end of the closing re-
marks). The TV transcript data set, which corre-
sponds to some real-world use cases in the multi-
media field, is very challenging for several reasons.
On the one hand, segments are short, with a reduced
number of repetitions, synonyms being frequently
employed. Moreover, smooth topic shifts can be
found, in particular at the beginning of each pro-
gram with different reports dedicated to the head-
line. On the other hand, transcripts significantly dif-
fer from written texts: no punctuation signs or capi-
tal letters; no sentence structure but rather utterances
which are only loosely syntactically motivated; pres-
ence of transcription errors which may imply an ac-
centuated lack of word repetitions.
All data were preprocessed in the same way:
Words were tagged and lemmatized with TreeTag-
</bodyText>
<page confidence="0.991146">
1319
</page>
<bodyText confidence="0.999625833333333">
ger1 and only the nouns, non modal verbs and adjec-
tives were retained for segmentation. Inverse docu-
ment frequencies used to measure similarity in Eq. 5
are obtained on a per document basis, referring to
the number of sentences in textual data and of utter-
ances in spoken data.
</bodyText>
<subsectionHeader confidence="0.575601">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.998425578947368">
Performance is measured by comparison of hypoth-
esized frontiers with reference ones. Alignment as-
sumes a tolerance of 1 sentence on texts and of 10
seconds on transcripts, which corresponds to stan-
dard values in the literature. Results are reported us-
ing recall, precision and F1-measure. Recall refers
to the proportion of reference frontiers correctly de-
tected; Precision corresponds to the ratio of hypoth-
esized frontiers that belong to the reference seg-
mentation; F1-measure combines recall and preci-
sion in a single value. These evaluation measures
were selected because recall and precision are not
sensitive to variations of segment length contrary
to the Pk measure (Beeferman et al., 1997) and
do not favor segmentations with a few number of
frontiers as WindowDiff (Pevzner and Hearst, 2002)
(see (Niekrasz and Moore, 2010) for a rigorous an-
alytical explanation of the biases of Pk and Win-
dowDiff ).
Several configurations were considered in the ex-
periments; due to space constraints, only the most
salient experiments are presented here. In Eq. 7, the
parameter α, which controls the contribution of the
prior model with respect to the lexical cohesion and
disruption, allows for different trade-offs between
precision and recall. For any given value of A, α
is thus varied, providing the range of recall/precision
values attainable. Results are compared to a baseline
system corresponding to the application of the orig-
inal algorithm of Utiyama and Isahara (i.e., setting
A = 0). This baseline has been shown to be a high-
performance algorithm, in particular with respect to
local methods that exploit lexical disruption. Differ-
ences in F1-measure between this baseline and our
system presented below are all statistically signifi-
cant at the level of p &lt; 0.01 (paired t-test).
Choi’s corpus. Figure 2 reports results obtained
on Choi’s data set, each graphic corresponding to
</bodyText>
<footnote confidence="0.988881">
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
</footnote>
<table confidence="0.99981125">
z T F1 Confidence interval 95 %
gain UI Combined
3-5 0 -0.2 [66.6,74.26] [75.23,78.08]
3-5 1 0.7 [72.25,83.4] [87.88,92.13]
3-11 1 0.23 [68.5,79.3] [86.6,87.43]
6-8 1 0.4 [68.48,80.99] [76.9,85.17]
9-11 0 1.6 [64.35,75.16] [81.31,84.86]
9-11 1 1.4 [68.39,80.39] [84.37,88.9]
</table>
<tableCaption confidence="0.8194866">
Table 2: Gain in F1-measure for Choi’s corpus when us-
ing lexical cohesion and disruption, and the correspond-
ing 95 % confidence intervals for the F1-measure. Re-
sults are reported for different tolerance T. UI denotes
the baseline and Combined the proposed model.
</tableCaption>
<bodyText confidence="0.999460484848485">
a specific variation in the size of the thematic seg-
ments forming the documents (e.g., 9 to 11 sen-
tences for the top left graphic). Results are provided
for different values of A in terms of F1-measure
boxplots, i.e., variations of the F1-measure when α
varies (same range of variation for α considered for
each plot), where the leftmost boxplot, denoted by
UI, corresponds to the baseline. Box and whisker
plots graphically depicts the distribution of the F1-
measures that can be attained by varying α, plotting
the median value, the first and third quartile and the
extrema.
Figure 2 shows that, whatever the segments
length, results globally improve according to the im-
portance given to the disruption (A variable). More-
over, the variation in F1-measure diminishes when
disruption is considered, thus indicating the influ-
ence of the prior model diminishes. When the seg-
ments size decreases (see Figs. 2(b), 2(c), 2(d)), the
difference in the maximum F1-measure between our
results and that of the baseline lowers, however still
in favor of our model. This can be explained by the
fact that our approach is based on the distribution of
words, thus more words better help discriminate be-
tween potential thematic frontiers. Finally, using too
large values for A can lead to under-segmentation, as
can be seen in Fig. 2(d) where, for A = 3, the varia-
tion of F1-measure increases and the distribution be-
comes negatively skewed (i.e., the median is closer
to the third quartile than to the first).
These results are confirmed by Table 2 which
presents the gain in F1-measure (i.e., the differ-
ence between the highest F1-measure obtained when
</bodyText>
<page confidence="0.991727">
1320
</page>
<figureCaption confidence="0.8861865">
Figure 2: F1-measure variation obtained on Choi’s corpus. In each graphic, the leftmost boxplot UI corresponds to
results obtained by using the sole lexical cohesion (baseline), while the A value is the importance given to the lexical
disruption in our approach. Results are provided for the same range of variation of factor α, allowing a tolerance of 1
sentence between the hypothesized and reference frontiers.
</figureCaption>
<figure confidence="0.9753015">
(a) (b)
(c) (d)
</figure>
<bodyText confidence="0.997253">
combining lexical cohesion and disruption and the
highest value for the baseline) for each of the four
sets of documents in Choi’s corpus, together with
the 95 % confidence intervals: The effect of using
the disruption is higher when segment size is longer,
whether evaluation allows or not for a tolerance T
between the hypothesized frontiers and the reference
ones. A qualitative analysis of the segmentations
obtained confirmed that employing disruption helps
eliminate wrong hypothesis and shift hypothesized
frontiers closer to the reference ones (explaining the
higher gain at tolerance 0 for 9-11 data set). When
smaller segments—thus few word repetitions—and
no tolerance are considered (e.g., 3–5), disruption
cannot improve segmentation. Our model is glob-
ally stable with respect to segment length, with rel-
atively similar gain for 3–11 and 6–8 data sets in
which the average number of words (distinct or re-
peated) is close.
Results discussed up to now are optimistic as they
correspond to the best F1 value attainable computed
a posteriori. Stability of the results was confirmed
</bodyText>
<table confidence="0.993364666666667">
z = 3–5 3–11 6–8 9–11
UI 91.9 87.0 93.1 92.8
Combined 92.9 87.5 93.5 94.0
</table>
<tableCaption confidence="0.9856495">
Table 3: F1 results using cross-validation on Choi’s data
set.
</tableCaption>
<bodyText confidence="0.999585461538462">
using cross-validation with 5 folds (10 folds for
z=3–11): Parameters A and α maximizing the F1-
measure are determined on all but one fold, this last
fold being used for evaluation. Results, averaged
over all folds, are reported in Tab. 3 for the baseline
and the method combining cohesion and disruption.
Medical textbook corpus. The medical textbook
corpus was previously used for topic segmentation
by Eisenstein and Barzilay (2008) with their algo-
rithm BayesSeg2. We thus compare our results with
those obtained by BayesSeg and by the baseline.
When considering the best F1-measure (i.e., the best
F1-measure which can be achieved by varying α and
</bodyText>
<footnote confidence="0.9973255">
2The code and the data set are available at
http://groups.csail.mit.edu/rbg/code/bayesseg/
</footnote>
<page confidence="0.990696">
1321
</page>
<figure confidence="0.99908">
(a) (b)
</figure>
<figureCaption confidence="0.9985045">
Figure 3: Boxplots showing F1-measure variation on transcripts obtained using IRENE and LIMSI automatic speech
recognition systems.
</figureCaption>
<bodyText confidence="0.999986925925926">
A), we achieved an improvement of 2.2 with respect
to BayesSeg when no tolerance is allowed, and of
0.5 when the tolerance is of 1 sentence. The corre-
sponding figures with respect to the baseline are 0.6
and 0.4. When considering the F1-measure value
for which the number of hypothesized frontiers is
the closest to the number of reference boundaries,
improvement is of resp. 1.5 and 0.5 with respect to
BayesSeg, -0.1 and 0.4 with respect to the baseline.
These results show that our model combining lexi-
cal cohesion and disruption is also able to deal with
topic segmentation of corpora from a homogeneous
domain, with smooth topic changes and segments of
regular size.
One can argue that the higher number of free pa-
rameters in our method explains most of the gain
with respect to BayesSeg. While BayesSeg has only
one free parameter (as opposed to two in our case),
the number of segments is assumed to be provided
as prior knowledge. This assumption can be seen
as an additional free parameter, i.e., the number of
segments, and is a much stronger constraint than we
are using. Moreover, cross-validation experiments
on the Choi data set show that improvement is not
due to over-fitting of the development data thanks to
an additional parameter. Gains on development set
with parameters tuned on the development set itself
and with parameters tuned on a held-out set in cross-
validation experiments are in the same range.
TV news transcripts corpus Figure 3 provides
results, in terms of F1-measure variation, for TV
news transcripts obtained with the two ASR sys-
tems. On this highly challenging corpus, with short
segments, wrongly transcribed spoken words, and
thus few word repetitions, the capabilities of our
model to overcome the baseline system are reduced.
Yet, an improvement of the quality of the segmen-
tation of these noisy data is still observed, and
general conclusions are quite similar—though a bit
weaker—to those already made for Choi’s corpus.
Results are confirmed in Table 4 which presents the
gain in F1-measure of our model together with the
95 % confidence interval, where F1-measure values
correspond to that of segmentations with a num-
ber of hypothesized frontiers the closest to the ref-
erence. The two first lines show that the gain is
smaller for IRENE transcripts which have a higher
WER, thus fewer words available to discriminate be-
tween segments belonging to different topics. The
impact of transcription errors is illustrated in the
last three lines, when segmenting six TV news for
which manual reference transcripts were available
(line 3), where the higher the WER, the smaller the
F1-measure gain.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999977555555556">
We have proposed a method to combine lexical co-
hesion and disruption for topic segmentation. Ex-
perimental results on various data sets with various
characteristics demonstrate the impact of taking into
account disruption in addition to lexical cohesion.
We observed gains both on data sets with segments
of regular length and on data sets exhibiting seg-
ments of highly varying length within a document.
Unsurprisingly, bigger gains were observed on doc-
</bodyText>
<page confidence="0.96231">
1322
</page>
<table confidence="0.999629">
Corpus F1 Confidence interval 95 %
gain UI Combined
IRENE 0.3 [54.4,57.6] [56.92,59]
LIMSI 0.86 [56.7,60.2] [59.44,61.95]
MANUAL (6) 0.77 [70.39,72.29] [71.7,73.29]
IRENE (6) 0.2 [56.81,60.94] [59.51,63.43]
LIMSI (6) 0.5 [64.27,68.64] [67.7,71.56]
</table>
<tableCaption confidence="0.777962">
Table 4: Gain in F1-measure for TV news corpus auto-
matic and manual transcripts when using lexical cohesion
</tableCaption>
<bodyText confidence="0.997922428571429">
and disruption, and the corresponding 95 % confidence
intervals. Last three rows report results on only 6 shows
for which manual reference transcripts are available.
uments containing relatively long segments. How-
ever the segmentation algorithm has proven to be
robust on automatic transcripts with short segments
and limited vocabulary reoccurrences. Finally, we
tested both abrupt topic changes and smooth ones
with good results on both. Further work can be con-
sidered to improve segmentation of documents char-
acterized by small segments and few words repe-
titions, such as using semantic relations or vector-
ization techniques to better exploit implicit relations
not considered by lexical reoccurrence.
</bodyText>
<sectionHeader confidence="0.998925" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995620622641509">
Doug Beeferman, Adam Berger, and John Lafferty.
1997. Text segmentation using exponential models.
In 2nd Conference on Empirical Methods in Natural
Language Processing, pages 35–46.
Gillian Brown and George Yule. 1983. Discourse analy-
sis. Cambridge University Press.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In 1st International
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 26–33.
Vincent Claveau and S´ebastien Lef`evre. 2011. Topic
segmentation of TV-streams by mathematical mor-
phology and vectorization. In 12th International Con-
ference of the International Speech Communication
Association, pages 1105–1108.
Manolis Delakis, Guillaume Gravier, and Patrick Gros.
2008. Audiovisual integration with segment models
for tennis video parsing. Computer Vision and Image
Understanding, 111(2):142–154.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Conference on
Empirical Methods in Natural Language Processing,
pages 334–343.
Olivier Ferret, Brigitte Grau, and Nicolas Masson. 1998.
Thematic segmentation of texts: Two methods for two
kinds of texts. In 36th Annual Meeting of the As-
sociation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguistics,
pages 392–396.
Jean-Luc Gauvain, Lori Lamel, and Gilles Adda. 2002.
The LIMSI broadcast news transcription system.
Speech Communication, 37(1–2):89–108.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175–204.
Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi.
1995. Centering: a framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203–225, June.
Camille Guinaudeau, Guillaume Gravier, and Pascale
S´ebillot. 2012. Enhancing lexical cohesion measure
with confidence measures, semantic relations and lan-
guage model interpolation for multimedia spoken con-
tent topic segmentation. Computer Speech and Lan-
guage, 26(2):90–104.
Marti A. Hearst. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33–64.
Nicolas Hernandez and Brigitte Grau. 2002. Analyse
th´ematique du discours : segmentation, structuration,
description et repr´esentation. In 5e colloque interna-
tional sur le document ´electronique, pages 277–285.
St´ephane Huet, Guillaume Gravier, and Pascale S´ebillot.
2010. Morpho-syntactic post-processing of n-best lists
for improved French automatic speech recognition.
Computer Speech and Language, 24(4):663–684.
Xiang Ji and Hongyuan Zha. 2003. Domain-independent
text segmentation using anisotropic diffusion and dy-
namic programming. In 26th Annual International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, pages 322–329.
Diane J. Litman and Rebecca J. Passonneau. 1995. Com-
bining multiple knowledge sources for discourse seg-
mentation. In 33rd Annual Meeting of the Association
for Computational Linguistics, pages 108–115.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In 21st In-
ternational Conference on Computational Linguistics
and the 44th Annual Meeting of the Association for
Computational Linguistics, pages 25–32.
Hemant Misra, Franc¸ois Yvon, Joemon M. Jose, and
Olivier Cappe. 2009. Text segmentation via topic
modeling: an analytical study. In Proc. ACM con-
ference on Information and knowledge management,
pages 1553–1556.
Marie-Francine Moens and Rik De Busser. 2001.
Generic topic segmentation of document texts. In 24th
1323
International Conference on Research and Develope-
ment in Information Retrieval, pages 418–419.
John Niekrasz and Johanna D. Moore. 2010. Unbiased
discourse segmentation evaluation. In Spoken Lan-
guage Technology, pages 43–48.
Mari Ostendorf, Vassilios V. Digalakis, and Owen A.
Kimball. 1996. From HMM’s to segment models: a
unified view of stochastic modeling for speech recog-
nition. IEEE Transactions on Speech and Audio Pro-
cessing, 4(5):360–378.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28:19–36.
Jeffrey C. Reynar. 1994. An automatic method of finding
topic boundaries. In 32nd Annual Meeting on Associ-
ation for Computational Linguistics, pages 331–333.
Anca Simon, Guillaume Gravier, and Pascale S´ebillot.
2013. Un mod`ele segmental probabiliste combinant
coh´esion lexicale et rupture lexicale pour la segmen-
tation th´ematique. In 20e conf´erence Traitement Au-
tomatique des Langues Naturelles, pages 202–214.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
39th Annual Meeting on the Association for Computa-
tional Linguistics, pages 499–506.
Kenneth H. Walker, Dallas W. Hall, and Willis J. Hurst.
1990. Clinical Methods: The History, Physical, and
Laboratory Examinations. Butterworths.
</reference>
<page confidence="0.994999">
1324
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.824090">
<title confidence="0.999845">Leveraging lexical cohesion and disruption for topic segmentation</title>
<author confidence="0.995703">Guillaume Gravier Pascale S´ebillot</author>
<affiliation confidence="0.9431875">Universit´e de Rennes 1 CNRS INSA de Rennes IRISA &amp; INRIA Rennes IRISA &amp; INRIA Rennes IRISA &amp; INRIA</affiliation>
<email confidence="0.962053">pascale.sebillot@irisa.fr</email>
<abstract confidence="0.998135380952381">Topic segmentation classically relies on one of two criteria, either finding areas with coherent vocabulary use or detecting discontinuities. In this paper, we propose a segmentation criterion combining both lexical cohesion and disruption, enabling a trade-off between the two. We provide the mathematical formulation of the criterion and an efficient graph based decoding algorithm for topic segmentation. Experimental results on standard textual data sets and on a more challenging corpus of automatically transcribed broadcast news shows demonstrate the benefit of such a combination. Gains were observed in all conditions, with segments of either regular or varying length and abrupt or smooth topic shifts. Long segments benefit more than short segments. However the algorithm has proven robust on automatic transcripts with short segments and limited vocabulary reoccurrences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Text segmentation using exponential models.</title>
<date>1997</date>
<booktitle>In 2nd Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>35--46</pages>
<contexts>
<context position="23237" citStr="Beeferman et al., 1997" startWordPosition="3870" endWordPosition="3873">nce ones. Alignment assumes a tolerance of 1 sentence on texts and of 10 seconds on transcripts, which corresponds to standard values in the literature. Results are reported using recall, precision and F1-measure. Recall refers to the proportion of reference frontiers correctly detected; Precision corresponds to the ratio of hypothesized frontiers that belong to the reference segmentation; F1-measure combines recall and precision in a single value. These evaluation measures were selected because recall and precision are not sensitive to variations of segment length contrary to the Pk measure (Beeferman et al., 1997) and do not favor segmentations with a few number of frontiers as WindowDiff (Pevzner and Hearst, 2002) (see (Niekrasz and Moore, 2010) for a rigorous analytical explanation of the biases of Pk and WindowDiff ). Several configurations were considered in the experiments; due to space constraints, only the most salient experiments are presented here. In Eq. 7, the parameter α, which controls the contribution of the prior model with respect to the lexical cohesion and disruption, allows for different trade-offs between precision and recall. For any given value of A, α is thus varied, providing th</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1997</marker>
<rawString>Doug Beeferman, Adam Berger, and John Lafferty. 1997. Text segmentation using exponential models. In 2nd Conference on Empirical Methods in Natural Language Processing, pages 35–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gillian Brown</author>
<author>George Yule</author>
</authors>
<title>Discourse analysis.</title>
<date>1983</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5752" citStr="Brown and Yule (1983)" startWordPosition="887" endWordPosition="890"> method in a realistic setting with statistically significant gains. The organization of the article is as follows. Existing work on topic segmentation is presented in Section 2, emphasizing the motivations of the model we propose. Section 3 details the baseline method of Utiyama and Isahara before introducing our algorithm. Experimental protocol and results are given in Section 4. Section 5 summarizes the finding and concludes with a discussion of future work. 2 Related work Defining the concept of theme precisely is not trivial and a large number of definitions have been given by linguists. Brown and Yule (1983) discuss at length the difficulty of defining a topic and note: ”The notion of ’topic’ is clearly an intuitively satisfactory way of describing the unifying principle which makes one stretch of discourse ’about’ something and the next stretch ’about’ something else, for it is appealed to very frequently in the discourse analysis literature. Yet the basis for the identi�cation of ’topic’ is rarely made explicit”. To skirt the issue of defining a topic, they suggest to focus on topic-shift markers and to identify topic changes, what most current topic segmentation methods do. Various characteris</context>
</contexts>
<marker>Brown, Yule, 1983</marker>
<rawString>Gillian Brown and George Yule. 1983. Discourse analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Y Y Choi</author>
</authors>
<title>Advances in domain independent linear text segmentation.</title>
<date>2000</date>
<booktitle>In 1st International Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>26--33</pages>
<contexts>
<context position="1574" citStr="Choi, 2000" startWordPosition="232" endWordPosition="233">gular or varying length and abrupt or smooth topic shifts. Long segments benefit more than short segments. However the algorithm has proven robust on automatic transcripts with short segments and limited vocabulary reoccurrences. 1 Introduction Topic segmentation consists in evidentiating the semantic structure of a document: Algorithms developed for this task aim at automatically detecting frontiers which define topically coherent segments in a text. Various methods for topic segmentation of textual data are described in the literature, e.g., (Reynar, 1994; Hearst, 1997; Ferret et al., 1998; Choi, 2000; Moens and Busser, 2001; Utiyama and Isahara, 2001), most of them relying on the notion of lexical cohesion, i.e., identifying segments with a consistent use of vocabulary, either based on words or on semantic relations between words. Reoccurrences of words or related words and lexical chains are two popular methods to evidence lexical cohesion. This general principle of lexical cohesion is further exploited for topic segmentation with two radically different strategies. On the one hand, a measure of the lexical cohesion can be used to determine coherent segments (Reynar, 1994; Moens and Buss</context>
<context position="7993" citStr="Choi, 2000" startWordPosition="1249" endWordPosition="1250"> 2002; Claveau and Lef`evre, 2011) locally compare adjacent fixed size regions, claiming a boundary when the similarity between the adjacent regions is small enough, thus identifying points of high lexical disruption. In the seminal work of Hearst (1997), a fixed size window divided into two adjacent blocks is used, consecutively centered at each potential boundary. Similarity between the adjacent blocks is computed at each point, the resulting similarity profile being analyzed to find significant valleys which are considered as topic boundaries. On the contrary, global methods (Reynar, 1994; Choi, 2000; Utiyama and Isahara, 2001; Ji and Zha, 2003; Malioutov and Barzilay, 2006; Misra et al., 2009) seek to maximize the value of the lexical cohesion on each segment resulting from the segmentation globally on the text. Several approaches have 1315 been taken relying on self-similarity matrices, such as dot plots, or on graphs. A typical and state-of-theart algorithm is that of Utiyama and Isahara (2001) whose principle is to search globally for the best path in a graph representing all possible segmentations and where edges are valued according to the lexical cohesion measured in a probabilisti</context>
<context position="19193" citStr="Choi (2000)" startWordPosition="3190" endWordPosition="3191">, the solution to Eq. 7 is obtained by finding out the best path in the decoding lattice, which can be done straightforwardly by scanning nodes in topological order. The decoding algorithm is summarized in Algorithm 1 with an efficient implementation in o(NL2 max) which does not require explicit construction of the lattice. 4 Experiments Experiments are performed on three distinct corpora which exhibit different characteristics, two containing textual data and one spoken data. We first present the corpora before presenting and discussing results on each. 4.1 Corpora The artificial data set of Choi (2000) is widely used in the literature and enables comparison of a new segmentation method with existing ones. Choi’s data set consist of 700 documents, each created by concatenating the first z sentences of 10 articles randomly chosen from the Brown corpus, assuming each article is on a different topic. Table 1 provides q[i+k][k] = max 1318 Figure 1: An example of a lattice L. z = 3–11 3–5 6–8 9–11 # samples 400 100 100 100 Table 1: Number of documents in Choi’s corpus (Choi, 2000). the corpus statistics, where z=3–11 means z is randomly chosen in the range [3,11]. Hence, Choi’s corpus is adapted </context>
</contexts>
<marker>Choi, 2000</marker>
<rawString>Freddy Y. Y. Choi. 2000. Advances in domain independent linear text segmentation. In 1st International Conference of the North American Chapter of the Association for Computational Linguistics, pages 26–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Claveau</author>
<author>S´ebastien Lef`evre</author>
</authors>
<title>Topic segmentation of TV-streams by mathematical morphology and vectorization.</title>
<date>2011</date>
<booktitle>In 12th International Conference of the International Speech Communication Association,</booktitle>
<pages>1105--1108</pages>
<marker>Claveau, Lef`evre, 2011</marker>
<rawString>Vincent Claveau and S´ebastien Lef`evre. 2011. Topic segmentation of TV-streams by mathematical morphology and vectorization. In 12th International Conference of the International Speech Communication Association, pages 1105–1108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manolis Delakis</author>
<author>Guillaume Gravier</author>
<author>Patrick Gros</author>
</authors>
<title>Audiovisual integration with segment models for tennis video parsing.</title>
<date>2008</date>
<journal>Computer Vision and Image Understanding,</journal>
<volume>111</volume>
<issue>2</issue>
<contexts>
<context position="16445" citStr="Delakis et al., 2008" startWordPosition="2670" endWordPosition="2673">ths ending in a given node. In other words, only paths of the same length ending cos(vi−1, vi) = rP v2 i−1(v) P v∈V v∈V v2i (v) Xm ln(P[Wi|Si]) − i=1 1317 at a given point, with different predecessors, should be recombined so that disruption can be considered properly in subsequent steps of the algorithm. Note that, in standard decoding as in Utiyama and Isahara’s algorithm, only one of such paths, the best scoring one, would be retained. We employ a strategy inspired from the decoding strategy of segment models or semi-hidden Markov model with explicit duration model (Ostendorf et al., 1996; Delakis et al., 2008). Search is performed through a lattice L = {V, E}, with V the set of nodes representing potential boundaries and E the set of edges representing segments, i.e., a set of consecutive utterances. The set V is defined as V = {nij|0 ≤ i, j ≤ N} , where nij represents a boundary after utterance ui reached by a segment of length j utterances and N = t+1. In the lattice example of Fig. 1, it is trivial to see that for a given node, all incoming edges cover the same segment. For example, the node n42 is positioned after u4 and all incoming segments contain the two utterances u� and u4. Edges are defi</context>
</contexts>
<marker>Delakis, Gravier, Gros, 2008</marker>
<rawString>Manolis Delakis, Guillaume Gravier, and Patrick Gros. 2008. Audiovisual integration with segment models for tennis video parsing. Computer Vision and Image Understanding, 111(2):142–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Bayesian unsupervised topic segmentation.</title>
<date>2008</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>334--343</pages>
<contexts>
<context position="3978" citStr="Eisenstein and Barzilay, 2008" startWordPosition="609" endWordPosition="612">articular domain and its ability to cope with thematic segments 1314 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1314–1324, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics of highly varying lengths, two interesting features to obtain a generic solution to the problem of topic segmentation. Moreover, the algorithm has proven to be up to the state of the art in several studies, with no need of a priori information about the number of segments (contrary to algorithms in (Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008) that can attain a higher segmentation accuracy). It also provides an efficient graph based implementation of which we take advantage. To account both for cohesion and disruption, we extend the formalism of Isahara and Utiyama using a Markovian assumption between segments in place of the independence assumption of the original algorithm. Keeping unchanged their probabilistic measure of lexical cohesion, the Markovian assumption enables to introduce the disruption between two consecutive segments. We propose an extended graph based decoding strategy, which is both optimal and efficient, exploit</context>
<context position="20262" citStr="Eisenstein and Barzilay, 2008" startWordPosition="3375" endWordPosition="3379"> of documents in Choi’s corpus (Choi, 2000). the corpus statistics, where z=3–11 means z is randomly chosen in the range [3,11]. Hence, Choi’s corpus is adapted to test the ability of our model to deal with variable segments length, z=3–11 being the most difficult condition. Moreover, Choi’s corpus provides a direct comparison with results reported in the literature. One of the main criticism of Choi’s data set is the presence of abrupt topic changes due to the artificial construction of the corpus. We therefore report results on a textual corpus with more natural topic changes, also used in (Eisenstein and Barzilay, 2008). The data set consists of 277 chapters selected from (Walker et al., 1990), a medical textbook, where each chapter—considered here as a document—was divided by its author into thematically coherent sections. The data set has a total of 1,136 segments with an average of 5 segments per document and an average of 28 sentences per segment. This data set is used to study the impact of smooth, natural, topic changes. Finally, results are reported on a corpus of automatic transcripts of TV news spoken data. The data set consists of 56 news programs (Pt1/2 hour each), broadcasted in February and Marc</context>
<context position="28764" citStr="Eisenstein and Barzilay (2008)" startWordPosition="4756" endWordPosition="4759">ted a posteriori. Stability of the results was confirmed z = 3–5 3–11 6–8 9–11 UI 91.9 87.0 93.1 92.8 Combined 92.9 87.5 93.5 94.0 Table 3: F1 results using cross-validation on Choi’s data set. using cross-validation with 5 folds (10 folds for z=3–11): Parameters A and α maximizing the F1- measure are determined on all but one fold, this last fold being used for evaluation. Results, averaged over all folds, are reported in Tab. 3 for the baseline and the method combining cohesion and disruption. Medical textbook corpus. The medical textbook corpus was previously used for topic segmentation by Eisenstein and Barzilay (2008) with their algorithm BayesSeg2. We thus compare our results with those obtained by BayesSeg and by the baseline. When considering the best F1-measure (i.e., the best F1-measure which can be achieved by varying α and 2The code and the data set are available at http://groups.csail.mit.edu/rbg/code/bayesseg/ 1321 (a) (b) Figure 3: Boxplots showing F1-measure variation on transcripts obtained using IRENE and LIMSI automatic speech recognition systems. A), we achieved an improvement of 2.2 with respect to BayesSeg when no tolerance is allowed, and of 0.5 when the tolerance is of 1 sentence. The co</context>
</contexts>
<marker>Eisenstein, Barzilay, 2008</marker>
<rawString>Jacob Eisenstein and Regina Barzilay. 2008. Bayesian unsupervised topic segmentation. In Conference on Empirical Methods in Natural Language Processing, pages 334–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Ferret</author>
<author>Brigitte Grau</author>
<author>Nicolas Masson</author>
</authors>
<title>Thematic segmentation of texts: Two methods for two kinds of texts.</title>
<date>1998</date>
<booktitle>In 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>392--396</pages>
<contexts>
<context position="1562" citStr="Ferret et al., 1998" startWordPosition="228" endWordPosition="231">segments of either regular or varying length and abrupt or smooth topic shifts. Long segments benefit more than short segments. However the algorithm has proven robust on automatic transcripts with short segments and limited vocabulary reoccurrences. 1 Introduction Topic segmentation consists in evidentiating the semantic structure of a document: Algorithms developed for this task aim at automatically detecting frontiers which define topically coherent segments in a text. Various methods for topic segmentation of textual data are described in the literature, e.g., (Reynar, 1994; Hearst, 1997; Ferret et al., 1998; Choi, 2000; Moens and Busser, 2001; Utiyama and Isahara, 2001), most of them relying on the notion of lexical cohesion, i.e., identifying segments with a consistent use of vocabulary, either based on words or on semantic relations between words. Reoccurrences of words or related words and lexical chains are two popular methods to evidence lexical cohesion. This general principle of lexical cohesion is further exploited for topic segmentation with two radically different strategies. On the one hand, a measure of the lexical cohesion can be used to determine coherent segments (Reynar, 1994; Mo</context>
<context position="7362" citStr="Ferret et al., 1998" startWordPosition="1149" endWordPosition="1152">ften specific to a type of text and cannot be considered in a versatile approach as the one we are targeting, where versatility is achieved relying on the sole lexical cohesion. The key point with lexical cohesion is that a significant change in the use of vocabulary is considered to be a sign of topic shift. This general idea translates into two families of methods, local ones targeting a local detection of lexical disruptions and global ones relying on a measure of the lexical cohesion to globally find segments exhibiting coherence in their lexical distribution. Local methods (Hearst, 1997; Ferret et al., 1998; Hernandez and Grau, 2002; Claveau and Lef`evre, 2011) locally compare adjacent fixed size regions, claiming a boundary when the similarity between the adjacent regions is small enough, thus identifying points of high lexical disruption. In the seminal work of Hearst (1997), a fixed size window divided into two adjacent blocks is used, consecutively centered at each potential boundary. Similarity between the adjacent blocks is computed at each point, the resulting similarity profile being analyzed to find significant valleys which are considered as topic boundaries. On the contrary, global me</context>
</contexts>
<marker>Ferret, Grau, Masson, 1998</marker>
<rawString>Olivier Ferret, Brigitte Grau, and Nicolas Masson. 1998. Thematic segmentation of texts: Two methods for two kinds of texts. In 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 392–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Luc Gauvain</author>
<author>Lori Lamel</author>
<author>Gilles Adda</author>
</authors>
<title>The LIMSI broadcast news transcription system.</title>
<date>2002</date>
<journal>Speech Communication,</journal>
<pages>37--1</pages>
<contexts>
<context position="21054" citStr="Gauvain et al., 2002" startWordPosition="3513" endWordPosition="3516"> into thematically coherent sections. The data set has a total of 1,136 segments with an average of 5 segments per document and an average of 28 sentences per segment. This data set is used to study the impact of smooth, natural, topic changes. Finally, results are reported on a corpus of automatic transcripts of TV news spoken data. The data set consists of 56 news programs (Pt1/2 hour each), broadcasted in February and March 2007 on the French television channel France 2, and transcribed by two different automatic speech recognition (ASR) systems, namely IRENE (Huet et al., 2010) and LIMSI (Gauvain et al., 2002), with respective word error rates (WER) around 36 % and 30 %. Each news program consists of successive reports of short duration (2-3 min), possibly with consecutive reports on different facets of the same news. The reference segmentation was established by associating a topic with each report, i.e., placing a boundary at the beginning of a report’s introduction (and hence at the end of the closing remarks). The TV transcript data set, which corresponds to some real-world use cases in the multimedia field, is very challenging for several reasons. On the one hand, segments are short, with a re</context>
</contexts>
<marker>Gauvain, Lamel, Adda, 2002</marker>
<rawString>Jean-Luc Gauvain, Lori Lamel, and Gilles Adda. 2002. The LIMSI broadcast news transcription system. Speech Communication, 37(1–2):89–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="6679" citStr="Grosz and Sidner, 1986" startWordPosition="1033" endWordPosition="1036">in the discourse analysis literature. Yet the basis for the identi�cation of ’topic’ is rarely made explicit”. To skirt the issue of defining a topic, they suggest to focus on topic-shift markers and to identify topic changes, what most current topic segmentation methods do. Various characteristics can be exploited to identify thematic changes in text data. The most popular ones rely either on the lexical distribution information to measure lexical cohesion (i.e., word reoccurrences, lexical chains) or on linguistic markers such as discourse markers which indicate continuity or discontinuity (Grosz and Sidner, 1986; Litman and Passonneau, 1995). Linguistic markers are however often specific to a type of text and cannot be considered in a versatile approach as the one we are targeting, where versatility is achieved relying on the sole lexical cohesion. The key point with lexical cohesion is that a significant change in the use of vocabulary is considered to be a sign of topic shift. This general idea translates into two families of methods, local ones targeting a local detection of lexical disruptions and global ones relying on a measure of the lexical cohesion to globally find segments exhibiting cohere</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3):175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Scott Weinstein</author>
<author>Aravind K Joshi</author>
</authors>
<title>Centering: a framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="9720" citStr="Grosz et al., 1995" startWordPosition="1528" endWordPosition="1531">lize properly the similarities between blocks and to deal with statistics on segment length. While global methods override these drawbacks, they face the problem of over-segmentation due to the fact that they mainly rely on the sole lexical cohesion. Short segments are therefore very likely to be coherent which calls for regularization introduced as priors on the segments length. These considerations naturally lead to the idea of methods combining lexical cohesion and disruption to make the best of both worlds. While the two criteria rely on the same underlying principle of lexical coherence (Grosz et al., 1995) and might appear as redundant, the resulting algorithms are quite different in their philosophy. A first (and, to the best of our knowledge, unique) attempt at capturing a global view of the local dissimilarities is described in Malioutov and Barzilay (2006). However, this method assumes that the number of segments to find is known beforehand which makes it difficult for real-world usage. 3 Combining lexical cohesion and disruption We extend the graph-based formalism of Utiyama and Isahara to jointly account for lexical cohesion and disruption in a global approach. Clearly, other formalisms t</context>
</contexts>
<marker>Grosz, Weinstein, Joshi, 1995</marker>
<rawString>Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi. 1995. Centering: a framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Camille Guinaudeau</author>
<author>Guillaume Gravier</author>
<author>Pascale S´ebillot</author>
</authors>
<title>Enhancing lexical cohesion measure with confidence measures, semantic relations and language model interpolation for multimedia spoken content topic segmentation.</title>
<date>2012</date>
<journal>Computer Speech and Language,</journal>
<volume>26</volume>
<issue>2</issue>
<marker>Guinaudeau, Gravier, S´ebillot, 2012</marker>
<rawString>Camille Guinaudeau, Guillaume Gravier, and Pascale S´ebillot. 2012. Enhancing lexical cohesion measure with confidence measures, semantic relations and language model interpolation for multimedia spoken content topic segmentation. Computer Speech and Language, 26(2):90–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>TextTiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="1541" citStr="Hearst, 1997" startWordPosition="226" endWordPosition="227">ditions, with segments of either regular or varying length and abrupt or smooth topic shifts. Long segments benefit more than short segments. However the algorithm has proven robust on automatic transcripts with short segments and limited vocabulary reoccurrences. 1 Introduction Topic segmentation consists in evidentiating the semantic structure of a document: Algorithms developed for this task aim at automatically detecting frontiers which define topically coherent segments in a text. Various methods for topic segmentation of textual data are described in the literature, e.g., (Reynar, 1994; Hearst, 1997; Ferret et al., 1998; Choi, 2000; Moens and Busser, 2001; Utiyama and Isahara, 2001), most of them relying on the notion of lexical cohesion, i.e., identifying segments with a consistent use of vocabulary, either based on words or on semantic relations between words. Reoccurrences of words or related words and lexical chains are two popular methods to evidence lexical cohesion. This general principle of lexical cohesion is further exploited for topic segmentation with two radically different strategies. On the one hand, a measure of the lexical cohesion can be used to determine coherent segme</context>
<context position="7341" citStr="Hearst, 1997" startWordPosition="1147" endWordPosition="1148"> are however often specific to a type of text and cannot be considered in a versatile approach as the one we are targeting, where versatility is achieved relying on the sole lexical cohesion. The key point with lexical cohesion is that a significant change in the use of vocabulary is considered to be a sign of topic shift. This general idea translates into two families of methods, local ones targeting a local detection of lexical disruptions and global ones relying on a measure of the lexical cohesion to globally find segments exhibiting coherence in their lexical distribution. Local methods (Hearst, 1997; Ferret et al., 1998; Hernandez and Grau, 2002; Claveau and Lef`evre, 2011) locally compare adjacent fixed size regions, claiming a boundary when the similarity between the adjacent regions is small enough, thus identifying points of high lexical disruption. In the seminal work of Hearst (1997), a fixed size window divided into two adjacent blocks is used, consecutively centered at each potential boundary. Similarity between the adjacent blocks is computed at each point, the resulting similarity profile being analyzed to find significant valleys which are considered as topic boundaries. On th</context>
<context position="14896" citStr="Hearst, 1997" startWordPosition="2405" endWordPosition="2406">enalizing consecutive coherent segments. The underlying assumption is that the bigger Δ(Wi, Wi−1), the weaker the disruption between the two segments. Parameter λ controls the respective contributions of cohesion and disruption. We initially adopted a probabilistic measure of disruption based on cross probabilities, i.e., P[Wi|Si−1] and P[Wi−1|Si], which proved to have limited impact on the segmentation. We therefore prefer to rely on a cosine similarity measure between the word vectors representing two adjacent segments, building upon a classical strategy of local methods such as TextTiling (Hearst, 1997). The cosine similarity measure is calculated between vectors representing the content of resp. Si and Si−1, denoted vi and vi−1, where vi is a vector containing the (tf-idf) weight of each term of V in Si. The cosine similarity is classically defined as P vi−1(v) vi(v) v∈V (5) Δ(Wi, Wi−1) is calculated from the cosine similarity measure as Δ(Wi,Wi−1) = (1 − cos(vi−1,vi))−1 , (6) thus yielding a small penalty in Eq. 4 for highly disrupting boundaries, i.e., corresponding to low similarity measure. Given the quantities defined above, the algorithm boils down to finding the best scoring segmenta</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti A. Hearst. 1997. TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23(1):33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolas Hernandez</author>
<author>Brigitte Grau</author>
</authors>
<title>Analyse th´ematique du discours : segmentation, structuration, description et repr´esentation. In 5e colloque international sur le document ´electronique,</title>
<date>2002</date>
<pages>277--285</pages>
<contexts>
<context position="7388" citStr="Hernandez and Grau, 2002" startWordPosition="1153" endWordPosition="1156">pe of text and cannot be considered in a versatile approach as the one we are targeting, where versatility is achieved relying on the sole lexical cohesion. The key point with lexical cohesion is that a significant change in the use of vocabulary is considered to be a sign of topic shift. This general idea translates into two families of methods, local ones targeting a local detection of lexical disruptions and global ones relying on a measure of the lexical cohesion to globally find segments exhibiting coherence in their lexical distribution. Local methods (Hearst, 1997; Ferret et al., 1998; Hernandez and Grau, 2002; Claveau and Lef`evre, 2011) locally compare adjacent fixed size regions, claiming a boundary when the similarity between the adjacent regions is small enough, thus identifying points of high lexical disruption. In the seminal work of Hearst (1997), a fixed size window divided into two adjacent blocks is used, consecutively centered at each potential boundary. Similarity between the adjacent blocks is computed at each point, the resulting similarity profile being analyzed to find significant valleys which are considered as topic boundaries. On the contrary, global methods (Reynar, 1994; Choi,</context>
</contexts>
<marker>Hernandez, Grau, 2002</marker>
<rawString>Nicolas Hernandez and Brigitte Grau. 2002. Analyse th´ematique du discours : segmentation, structuration, description et repr´esentation. In 5e colloque international sur le document ´electronique, pages 277–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>St´ephane Huet</author>
<author>Guillaume Gravier</author>
<author>Pascale S´ebillot</author>
</authors>
<title>Morpho-syntactic post-processing of n-best lists for improved French automatic speech recognition.</title>
<date>2010</date>
<journal>Computer Speech and Language,</journal>
<volume>24</volume>
<issue>4</issue>
<marker>Huet, Gravier, S´ebillot, 2010</marker>
<rawString>St´ephane Huet, Guillaume Gravier, and Pascale S´ebillot. 2010. Morpho-syntactic post-processing of n-best lists for improved French automatic speech recognition. Computer Speech and Language, 24(4):663–684.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiang Ji</author>
<author>Hongyuan Zha</author>
</authors>
<title>Domain-independent text segmentation using anisotropic diffusion and dynamic programming.</title>
<date>2003</date>
<booktitle>In 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>322--329</pages>
<contexts>
<context position="8038" citStr="Ji and Zha, 2003" startWordPosition="1255" endWordPosition="1258">lly compare adjacent fixed size regions, claiming a boundary when the similarity between the adjacent regions is small enough, thus identifying points of high lexical disruption. In the seminal work of Hearst (1997), a fixed size window divided into two adjacent blocks is used, consecutively centered at each potential boundary. Similarity between the adjacent blocks is computed at each point, the resulting similarity profile being analyzed to find significant valleys which are considered as topic boundaries. On the contrary, global methods (Reynar, 1994; Choi, 2000; Utiyama and Isahara, 2001; Ji and Zha, 2003; Malioutov and Barzilay, 2006; Misra et al., 2009) seek to maximize the value of the lexical cohesion on each segment resulting from the segmentation globally on the text. Several approaches have 1315 been taken relying on self-similarity matrices, such as dot plots, or on graphs. A typical and state-of-theart algorithm is that of Utiyama and Isahara (2001) whose principle is to search globally for the best path in a graph representing all possible segmentations and where edges are valued according to the lexical cohesion measured in a probabilistic way. When the lengths of the respective top</context>
</contexts>
<marker>Ji, Zha, 2003</marker>
<rawString>Xiang Ji and Hongyuan Zha. 2003. Domain-independent text segmentation using anisotropic diffusion and dynamic programming. In 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 322–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Rebecca J Passonneau</author>
</authors>
<title>Combining multiple knowledge sources for discourse segmentation.</title>
<date>1995</date>
<booktitle>In 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>108--115</pages>
<contexts>
<context position="6709" citStr="Litman and Passonneau, 1995" startWordPosition="1037" endWordPosition="1040">s literature. Yet the basis for the identi�cation of ’topic’ is rarely made explicit”. To skirt the issue of defining a topic, they suggest to focus on topic-shift markers and to identify topic changes, what most current topic segmentation methods do. Various characteristics can be exploited to identify thematic changes in text data. The most popular ones rely either on the lexical distribution information to measure lexical cohesion (i.e., word reoccurrences, lexical chains) or on linguistic markers such as discourse markers which indicate continuity or discontinuity (Grosz and Sidner, 1986; Litman and Passonneau, 1995). Linguistic markers are however often specific to a type of text and cannot be considered in a versatile approach as the one we are targeting, where versatility is achieved relying on the sole lexical cohesion. The key point with lexical cohesion is that a significant change in the use of vocabulary is considered to be a sign of topic shift. This general idea translates into two families of methods, local ones targeting a local detection of lexical disruptions and global ones relying on a measure of the lexical cohesion to globally find segments exhibiting coherence in their lexical distribut</context>
</contexts>
<marker>Litman, Passonneau, 1995</marker>
<rawString>Diane J. Litman and Rebecca J. Passonneau. 1995. Combining multiple knowledge sources for discourse segmentation. In 33rd Annual Meeting of the Association for Computational Linguistics, pages 108–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
<author>Regina Barzilay</author>
</authors>
<title>Minimum cut model for spoken lecture segmentation.</title>
<date>2006</date>
<booktitle>In 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="3946" citStr="Malioutov and Barzilay, 2006" startWordPosition="605" endWordPosition="608"> are its independency to any particular domain and its ability to cope with thematic segments 1314 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1314–1324, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics of highly varying lengths, two interesting features to obtain a generic solution to the problem of topic segmentation. Moreover, the algorithm has proven to be up to the state of the art in several studies, with no need of a priori information about the number of segments (contrary to algorithms in (Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008) that can attain a higher segmentation accuracy). It also provides an efficient graph based implementation of which we take advantage. To account both for cohesion and disruption, we extend the formalism of Isahara and Utiyama using a Markovian assumption between segments in place of the independence assumption of the original algorithm. Keeping unchanged their probabilistic measure of lexical cohesion, the Markovian assumption enables to introduce the disruption between two consecutive segments. We propose an extended graph based decoding strategy, which is bot</context>
<context position="8068" citStr="Malioutov and Barzilay, 2006" startWordPosition="1259" endWordPosition="1262">nt fixed size regions, claiming a boundary when the similarity between the adjacent regions is small enough, thus identifying points of high lexical disruption. In the seminal work of Hearst (1997), a fixed size window divided into two adjacent blocks is used, consecutively centered at each potential boundary. Similarity between the adjacent blocks is computed at each point, the resulting similarity profile being analyzed to find significant valleys which are considered as topic boundaries. On the contrary, global methods (Reynar, 1994; Choi, 2000; Utiyama and Isahara, 2001; Ji and Zha, 2003; Malioutov and Barzilay, 2006; Misra et al., 2009) seek to maximize the value of the lexical cohesion on each segment resulting from the segmentation globally on the text. Several approaches have 1315 been taken relying on self-similarity matrices, such as dot plots, or on graphs. A typical and state-of-theart algorithm is that of Utiyama and Isahara (2001) whose principle is to search globally for the best path in a graph representing all possible segmentations and where edges are valued according to the lexical cohesion measured in a probabilistic way. When the lengths of the respective topic segments in a text (or betw</context>
<context position="9979" citStr="Malioutov and Barzilay (2006)" startWordPosition="1572" endWordPosition="1575">. Short segments are therefore very likely to be coherent which calls for regularization introduced as priors on the segments length. These considerations naturally lead to the idea of methods combining lexical cohesion and disruption to make the best of both worlds. While the two criteria rely on the same underlying principle of lexical coherence (Grosz et al., 1995) and might appear as redundant, the resulting algorithms are quite different in their philosophy. A first (and, to the best of our knowledge, unique) attempt at capturing a global view of the local dissimilarities is described in Malioutov and Barzilay (2006). However, this method assumes that the number of segments to find is known beforehand which makes it difficult for real-world usage. 3 Combining lexical cohesion and disruption We extend the graph-based formalism of Utiyama and Isahara to jointly account for lexical cohesion and disruption in a global approach. Clearly, other formalisms than the graph-based one could have been considered. However, graph-based probabilistic topic segmentation has proven very accurate and versatile, relying on very minimal prior knowledge on the texts to segment. Good results at the state-ofthe-art have also be</context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>Igor Malioutov and Regina Barzilay. 2006. Minimum cut model for spoken lecture segmentation. In 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hemant Misra</author>
<author>Franc¸ois Yvon</author>
<author>Joemon M Jose</author>
<author>Olivier Cappe</author>
</authors>
<title>Text segmentation via topic modeling: an analytical study.</title>
<date>2009</date>
<booktitle>In Proc. ACM conference on Information and knowledge management,</booktitle>
<pages>1553--1556</pages>
<contexts>
<context position="8089" citStr="Misra et al., 2009" startWordPosition="1263" endWordPosition="1266">g a boundary when the similarity between the adjacent regions is small enough, thus identifying points of high lexical disruption. In the seminal work of Hearst (1997), a fixed size window divided into two adjacent blocks is used, consecutively centered at each potential boundary. Similarity between the adjacent blocks is computed at each point, the resulting similarity profile being analyzed to find significant valleys which are considered as topic boundaries. On the contrary, global methods (Reynar, 1994; Choi, 2000; Utiyama and Isahara, 2001; Ji and Zha, 2003; Malioutov and Barzilay, 2006; Misra et al., 2009) seek to maximize the value of the lexical cohesion on each segment resulting from the segmentation globally on the text. Several approaches have 1315 been taken relying on self-similarity matrices, such as dot plots, or on graphs. A typical and state-of-theart algorithm is that of Utiyama and Isahara (2001) whose principle is to search globally for the best path in a graph representing all possible segmentations and where edges are valued according to the lexical cohesion measured in a probabilistic way. When the lengths of the respective topic segments in a text (or between two texts) are ve</context>
<context position="10653" citStr="Misra et al., 2009" startWordPosition="1674" endWordPosition="1677">ts to find is known beforehand which makes it difficult for real-world usage. 3 Combining lexical cohesion and disruption We extend the graph-based formalism of Utiyama and Isahara to jointly account for lexical cohesion and disruption in a global approach. Clearly, other formalisms than the graph-based one could have been considered. However, graph-based probabilistic topic segmentation has proven very accurate and versatile, relying on very minimal prior knowledge on the texts to segment. Good results at the state-ofthe-art have also been reported in difficult conditions with this approach (Misra et al., 2009; Claveau and Lef`evre, 2011; Guinaudeau et al., 2012). We briefly recall the principle of probabilistic graph-based segmentation before detailing a Markovian extension to account for disruption. 3.1 Probabilistic graph-based segmentation The idea of the probabilistic graph-based segmentation algorithm is to find the segmentation into the most coherent segments constrained by a prior distribution on segments length. This problem is cast into finding the most probable segmentation of a sequence of t basic units (i.e., sentences or utterances composed of words) W = ut1 among all possible segment</context>
</contexts>
<marker>Misra, Yvon, Jose, Cappe, 2009</marker>
<rawString>Hemant Misra, Franc¸ois Yvon, Joemon M. Jose, and Olivier Cappe. 2009. Text segmentation via topic modeling: an analytical study. In Proc. ACM conference on Information and knowledge management, pages 1553–1556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Francine Moens</author>
<author>Rik De Busser</author>
</authors>
<title>Generic topic segmentation of document texts.</title>
<date>2001</date>
<booktitle>In 24th</booktitle>
<pages>1323</pages>
<marker>Moens, De Busser, 2001</marker>
<rawString>Marie-Francine Moens and Rik De Busser. 2001. Generic topic segmentation of document texts. In 24th 1323</rawString>
</citation>
<citation valid="false">
<booktitle>International Conference on Research and Developement in Information Retrieval,</booktitle>
<pages>418--419</pages>
<marker></marker>
<rawString>International Conference on Research and Developement in Information Retrieval, pages 418–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Niekrasz</author>
<author>Johanna D Moore</author>
</authors>
<title>Unbiased discourse segmentation evaluation.</title>
<date>2010</date>
<booktitle>In Spoken Language Technology,</booktitle>
<pages>43--48</pages>
<contexts>
<context position="23372" citStr="Niekrasz and Moore, 2010" startWordPosition="3892" endWordPosition="3895"> in the literature. Results are reported using recall, precision and F1-measure. Recall refers to the proportion of reference frontiers correctly detected; Precision corresponds to the ratio of hypothesized frontiers that belong to the reference segmentation; F1-measure combines recall and precision in a single value. These evaluation measures were selected because recall and precision are not sensitive to variations of segment length contrary to the Pk measure (Beeferman et al., 1997) and do not favor segmentations with a few number of frontiers as WindowDiff (Pevzner and Hearst, 2002) (see (Niekrasz and Moore, 2010) for a rigorous analytical explanation of the biases of Pk and WindowDiff ). Several configurations were considered in the experiments; due to space constraints, only the most salient experiments are presented here. In Eq. 7, the parameter α, which controls the contribution of the prior model with respect to the lexical cohesion and disruption, allows for different trade-offs between precision and recall. For any given value of A, α is thus varied, providing the range of recall/precision values attainable. Results are compared to a baseline system corresponding to the application of the origin</context>
</contexts>
<marker>Niekrasz, Moore, 2010</marker>
<rawString>John Niekrasz and Johanna D. Moore. 2010. Unbiased discourse segmentation evaluation. In Spoken Language Technology, pages 43–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari Ostendorf</author>
<author>Vassilios V Digalakis</author>
<author>Owen A Kimball</author>
</authors>
<title>From HMM’s to segment models: a unified view of stochastic modeling for speech recognition.</title>
<date>1996</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>4</volume>
<issue>5</issue>
<contexts>
<context position="16422" citStr="Ostendorf et al., 1996" startWordPosition="2666" endWordPosition="2669"> paths of different lengths ending in a given node. In other words, only paths of the same length ending cos(vi−1, vi) = rP v2 i−1(v) P v∈V v∈V v2i (v) Xm ln(P[Wi|Si]) − i=1 1317 at a given point, with different predecessors, should be recombined so that disruption can be considered properly in subsequent steps of the algorithm. Note that, in standard decoding as in Utiyama and Isahara’s algorithm, only one of such paths, the best scoring one, would be retained. We employ a strategy inspired from the decoding strategy of segment models or semi-hidden Markov model with explicit duration model (Ostendorf et al., 1996; Delakis et al., 2008). Search is performed through a lattice L = {V, E}, with V the set of nodes representing potential boundaries and E the set of edges representing segments, i.e., a set of consecutive utterances. The set V is defined as V = {nij|0 ≤ i, j ≤ N} , where nij represents a boundary after utterance ui reached by a segment of length j utterances and N = t+1. In the lattice example of Fig. 1, it is trivial to see that for a given node, all incoming edges cover the same segment. For example, the node n42 is positioned after u4 and all incoming segments contain the two utterances u�</context>
</contexts>
<marker>Ostendorf, Digalakis, Kimball, 1996</marker>
<rawString>Mari Ostendorf, Vassilios V. Digalakis, and Owen A. Kimball. 1996. From HMM’s to segment models: a unified view of stochastic modeling for speech recognition. IEEE Transactions on Speech and Audio Processing, 4(5):360–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Pevzner</author>
<author>Marti A Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<pages>28--19</pages>
<contexts>
<context position="23340" citStr="Pevzner and Hearst, 2002" startWordPosition="3887" endWordPosition="3890">h corresponds to standard values in the literature. Results are reported using recall, precision and F1-measure. Recall refers to the proportion of reference frontiers correctly detected; Precision corresponds to the ratio of hypothesized frontiers that belong to the reference segmentation; F1-measure combines recall and precision in a single value. These evaluation measures were selected because recall and precision are not sensitive to variations of segment length contrary to the Pk measure (Beeferman et al., 1997) and do not favor segmentations with a few number of frontiers as WindowDiff (Pevzner and Hearst, 2002) (see (Niekrasz and Moore, 2010) for a rigorous analytical explanation of the biases of Pk and WindowDiff ). Several configurations were considered in the experiments; due to space constraints, only the most salient experiments are presented here. In Eq. 7, the parameter α, which controls the contribution of the prior model with respect to the lexical cohesion and disruption, allows for different trade-offs between precision and recall. For any given value of A, α is thus varied, providing the range of recall/precision values attainable. Results are compared to a baseline system corresponding </context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>Lev Pevzner and Marti A. Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, 28:19–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
</authors>
<title>An automatic method of finding topic boundaries.</title>
<date>1994</date>
<booktitle>In 32nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>331--333</pages>
<contexts>
<context position="1527" citStr="Reynar, 1994" startWordPosition="223" endWordPosition="225">ved in all conditions, with segments of either regular or varying length and abrupt or smooth topic shifts. Long segments benefit more than short segments. However the algorithm has proven robust on automatic transcripts with short segments and limited vocabulary reoccurrences. 1 Introduction Topic segmentation consists in evidentiating the semantic structure of a document: Algorithms developed for this task aim at automatically detecting frontiers which define topically coherent segments in a text. Various methods for topic segmentation of textual data are described in the literature, e.g., (Reynar, 1994; Hearst, 1997; Ferret et al., 1998; Choi, 2000; Moens and Busser, 2001; Utiyama and Isahara, 2001), most of them relying on the notion of lexical cohesion, i.e., identifying segments with a consistent use of vocabulary, either based on words or on semantic relations between words. Reoccurrences of words or related words and lexical chains are two popular methods to evidence lexical cohesion. This general principle of lexical cohesion is further exploited for topic segmentation with two radically different strategies. On the one hand, a measure of the lexical cohesion can be used to determine </context>
<context position="7981" citStr="Reynar, 1994" startWordPosition="1247" endWordPosition="1248">ndez and Grau, 2002; Claveau and Lef`evre, 2011) locally compare adjacent fixed size regions, claiming a boundary when the similarity between the adjacent regions is small enough, thus identifying points of high lexical disruption. In the seminal work of Hearst (1997), a fixed size window divided into two adjacent blocks is used, consecutively centered at each potential boundary. Similarity between the adjacent blocks is computed at each point, the resulting similarity profile being analyzed to find significant valleys which are considered as topic boundaries. On the contrary, global methods (Reynar, 1994; Choi, 2000; Utiyama and Isahara, 2001; Ji and Zha, 2003; Malioutov and Barzilay, 2006; Misra et al., 2009) seek to maximize the value of the lexical cohesion on each segment resulting from the segmentation globally on the text. Several approaches have 1315 been taken relying on self-similarity matrices, such as dot plots, or on graphs. A typical and state-of-theart algorithm is that of Utiyama and Isahara (2001) whose principle is to search globally for the best path in a graph representing all possible segmentations and where edges are valued according to the lexical cohesion measured in a </context>
</contexts>
<marker>Reynar, 1994</marker>
<rawString>Jeffrey C. Reynar. 1994. An automatic method of finding topic boundaries. In 32nd Annual Meeting on Association for Computational Linguistics, pages 331–333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anca Simon</author>
<author>Guillaume Gravier</author>
<author>Pascale S´ebillot</author>
</authors>
<title>Un mod`ele segmental probabiliste combinant coh´esion lexicale et rupture lexicale pour la segmentation th´ematique.</title>
<date>2013</date>
<booktitle>In 20e conf´erence Traitement Automatique des Langues Naturelles,</booktitle>
<pages>202--214</pages>
<marker>Simon, Gravier, S´ebillot, 2013</marker>
<rawString>Anca Simon, Guillaume Gravier, and Pascale S´ebillot. 2013. Un mod`ele segmental probabiliste combinant coh´esion lexicale et rupture lexicale pour la segmentation th´ematique. In 20e conf´erence Traitement Automatique des Langues Naturelles, pages 202–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A statistical model for domain-independent text segmentation.</title>
<date>2001</date>
<booktitle>In 39th Annual Meeting on the Association for Computational Linguistics,</booktitle>
<pages>499--506</pages>
<contexts>
<context position="1626" citStr="Utiyama and Isahara, 2001" startWordPosition="238" endWordPosition="242">r smooth topic shifts. Long segments benefit more than short segments. However the algorithm has proven robust on automatic transcripts with short segments and limited vocabulary reoccurrences. 1 Introduction Topic segmentation consists in evidentiating the semantic structure of a document: Algorithms developed for this task aim at automatically detecting frontiers which define topically coherent segments in a text. Various methods for topic segmentation of textual data are described in the literature, e.g., (Reynar, 1994; Hearst, 1997; Ferret et al., 1998; Choi, 2000; Moens and Busser, 2001; Utiyama and Isahara, 2001), most of them relying on the notion of lexical cohesion, i.e., identifying segments with a consistent use of vocabulary, either based on words or on semantic relations between words. Reoccurrences of words or related words and lexical chains are two popular methods to evidence lexical cohesion. This general principle of lexical cohesion is further exploited for topic segmentation with two radically different strategies. On the one hand, a measure of the lexical cohesion can be used to determine coherent segments (Reynar, 1994; Moens and Busser, 2001; Utiyama and Isahara, 2001). On the other h</context>
<context position="3171" citStr="Utiyama and Isahara (2001)" startWordPosition="485" endWordPosition="488">ing prior information regarding the distribution of segment length or the expected number of segments. In this paper, we propose a segmentation criterion combining both cohesion and disruption along with the corresponding algorithm for topic segmentation. Such a criterion ensures a coherent use of vocabulary within each resulting segment, as well as a significant difference of vocabulary between neighboring segments. Moreover, the combination of these two strategies enables regularizing the number of segments found without resorting to prior knowledge. This piece of work uses the algorithm of Utiyama and Isahara (2001) as a starting point, a versatile and performing topic segmentation algorithm cast in a statistical framework. Among the benefits of this algorithm are its independency to any particular domain and its ability to cope with thematic segments 1314 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1314–1324, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics of highly varying lengths, two interesting features to obtain a generic solution to the problem of topic segmentation. Moreover, the algorithm has proven </context>
<context position="8020" citStr="Utiyama and Isahara, 2001" startWordPosition="1251" endWordPosition="1254">au and Lef`evre, 2011) locally compare adjacent fixed size regions, claiming a boundary when the similarity between the adjacent regions is small enough, thus identifying points of high lexical disruption. In the seminal work of Hearst (1997), a fixed size window divided into two adjacent blocks is used, consecutively centered at each potential boundary. Similarity between the adjacent blocks is computed at each point, the resulting similarity profile being analyzed to find significant valleys which are considered as topic boundaries. On the contrary, global methods (Reynar, 1994; Choi, 2000; Utiyama and Isahara, 2001; Ji and Zha, 2003; Malioutov and Barzilay, 2006; Misra et al., 2009) seek to maximize the value of the lexical cohesion on each segment resulting from the segmentation globally on the text. Several approaches have 1315 been taken relying on self-similarity matrices, such as dot plots, or on graphs. A typical and state-of-theart algorithm is that of Utiyama and Isahara (2001) whose principle is to search globally for the best path in a graph representing all possible segmentations and where edges are valued according to the lexical cohesion measured in a probabilistic way. When the lengths of </context>
</contexts>
<marker>Utiyama, Isahara, 2001</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2001. A statistical model for domain-independent text segmentation. In 39th Annual Meeting on the Association for Computational Linguistics, pages 499–506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth H Walker</author>
<author>Dallas W Hall</author>
<author>Willis J Hurst</author>
</authors>
<title>Clinical Methods: The History, Physical, and Laboratory Examinations.</title>
<date>1990</date>
<publisher>Butterworths.</publisher>
<contexts>
<context position="20337" citStr="Walker et al., 1990" startWordPosition="3389" endWordPosition="3392">s z is randomly chosen in the range [3,11]. Hence, Choi’s corpus is adapted to test the ability of our model to deal with variable segments length, z=3–11 being the most difficult condition. Moreover, Choi’s corpus provides a direct comparison with results reported in the literature. One of the main criticism of Choi’s data set is the presence of abrupt topic changes due to the artificial construction of the corpus. We therefore report results on a textual corpus with more natural topic changes, also used in (Eisenstein and Barzilay, 2008). The data set consists of 277 chapters selected from (Walker et al., 1990), a medical textbook, where each chapter—considered here as a document—was divided by its author into thematically coherent sections. The data set has a total of 1,136 segments with an average of 5 segments per document and an average of 28 sentences per segment. This data set is used to study the impact of smooth, natural, topic changes. Finally, results are reported on a corpus of automatic transcripts of TV news spoken data. The data set consists of 56 news programs (Pt1/2 hour each), broadcasted in February and March 2007 on the French television channel France 2, and transcribed by two di</context>
</contexts>
<marker>Walker, Hall, Hurst, 1990</marker>
<rawString>Kenneth H. Walker, Dallas W. Hall, and Willis J. Hurst. 1990. Clinical Methods: The History, Physical, and Laboratory Examinations. Butterworths.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
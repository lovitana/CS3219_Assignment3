<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019915">
<title confidence="0.989788">
Using crowdsourcing to get representations based on regular expressions
</title>
<author confidence="0.995523">
Anders Søgaard and Hector Martinez and Jakob Elming and Anders Johannsen
</author>
<affiliation confidence="0.9984125">
Center for Language Technology
University of Copenhagen
</affiliation>
<address confidence="0.549761">
DK-2300 Copenhagen S
</address>
<email confidence="0.998774">
{soegaard|alonso|zmk867|ajohannsen}@hum.ku.dk
</email>
<sectionHeader confidence="0.998598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9982688125">
Often the bottleneck in document classifica-
tion is finding good representations that zoom
in on the most important aspects of the doc-
uments. Most research uses n-gram repre-
sentations, but relevant features often occur
discontinuously, e.g., not... good in sentiment
analysis. In this paper we present experi-
ments getting experts to provide regular ex-
pressions, as well as crowdsourced annota-
tion tasks from which regular expressions can
be derived. Somewhat surprisingly, it turns
out that these crowdsourced feature combina-
tions outperform automatic feature combina-
tion methods, as well as expert features, by a
very large margin and reduce error by 24-41%
over n-gram representations.
</bodyText>
<sectionHeader confidence="0.999522" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999869233333333">
Finding good representations of classification prob-
lems is often glossed over in the literature. Sev-
eral authors have emphasized the need to pay more
attention to finding such representations (Wagstaff,
2012; Domingos, 2012), but in document classifica-
tion most research still uses n-gram representations.
This paper considers two document classification
problems where such representations seem inade-
quate. The problems are answer scoring (Burstein
et al., 1998), on data from stackoverflow.com, and
multi-attribute sentiment analysis (McAuley et al.,
2012). We argue that in order to adequately repre-
sent such problems we need discontinuous features,
i.e., regular expressions.
The problem with using regular expressions as
features is of course that even with a finite vocab-
ulary we can generate infinitely many regular ex-
pressions that match our documents. We suggest to
use expert knowledge or crowdsourcing in the loop.
In particular we present experiments where standard
representations are augmented with features from a
few hours of manual work, by machine learning ex-
perts or by turkers.
Somewhat surprisingly, we find that features de-
rived from crowdsourced annotation tasks lead to the
best results across the three datasets. While crowd-
sourcing of annotation tasks has become increasing
popular in NLP, this is, to the best of our knowledge,
the first attempt to crowdsource the problem of find-
ing good representations.
</bodyText>
<sectionHeader confidence="0.910771" genericHeader="introduction">
1.1 Related work
</sectionHeader>
<bodyText confidence="0.999867470588235">
Musat et al. (2012) design a collaborative two-player
game for sentiment annotation and collecting a sen-
timent lexicon. One player guesses the sentiment of
a text and picks a word from it that is representative
of its sentiment. The other player also provides a
guess observing only this word. If the two guesses
agree, both players get a point. The idea of gam-
ifying the problem of finding good representations
goes beyond crowdsourcing, but is not considered
here. Boyd-Graber et al. (2012) crowdsource the
feature weighting problem, but using standard rep-
resentations. The work most similar to ours is prob-
ably Tamuz et al. (2011), who learn a ’crowd kernel’
by asking annotators to rate examples by similarity,
providing an embedding that promotes feature com-
binations deemed relative when measuring similar-
ity.
</bodyText>
<page confidence="0.899598">
1476
</page>
<note confidence="0.380287">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1476–1480,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<table confidence="0.9978752">
BoW Exp AMT
n P(1) m µX m µX m µX
STACKOVERFLOW 97,519 0.5013 30,716 0.00131 1,156 0.1380 172,691 0.00331
TASTE 152,390 0.5003 38,227 0.00095 666 0.10631 114,588 0.00285
APPEARANCE 152,331 0.5009 37,901 0.00097 650 0.14629 102,734 0.00289
</table>
<tableCaption confidence="0.999834">
Table 1: Characteristics of the n x m data sets
</tableCaption>
<sectionHeader confidence="0.996922" genericHeader="method">
2 Experiments
</sectionHeader>
<bodyText confidence="0.998934516129033">
Data The three datasets used in our experi-
ments come from two sources, namely stackover-
flow.com and ratebeer.com. The two beer review
datasets (TASTE and APPEARANCE) are described
in McAuley et al. (2012) and available for down-
load.1 Each input example is an unstructured review
text, and the associated label is the score assigned to
taste or appearance by the reviewer. We randomly
sample about 152k data points, as well as 500 exam-
ples for experiments with experts and turks.
We extracted the STACKOVERFLOW dataset from
a publicly available data dump,2, and we briefly de-
scribe our sampling process here. We select pairs of
answers, where one is ranked higher than the other
by stackoverflow.com users. Obviously the answers
submitted first have a better chance of being ranked
highly, so we also require that the highest ranked
answer was submitted last. From this set of answer
pairs, we randomly sample 97,519 pairs, as well as
500 examples for our experiments with experts and
turks.
Our experiments are classification experiments
using the same learning algorithm in all experi-
ments, namely Li-regularized logistic regression.
We don’t set any parameters The only differences
between our systems are in the feature sets. Results
are from 5-fold cross-validation. The four feature
sets are described below: BoW, HI, Exp and AMT.
For motivating using regular expressions, con-
sider the following sentence from a review of John
Harvard’s Grand Cru:
</bodyText>
<listItem confidence="0.961787">
(1) Could have been more flavorful.
</listItem>
<bodyText confidence="0.999471666666667">
The only word carrying direct sentiment in this
sentence is flavorful, which is positive, but the sen-
tence is a negative evaluation of the Grand Cru’s
</bodyText>
<footnote confidence="0.998594">
1http://snap.stanford.edu/data/web-RateBeer.html
2http://www.clearbits.net/torrents/2076-aug-2012
</footnote>
<bodyText confidence="0.996830885714286">
taste. The trigram been more flavorful seems neg-
ative at first, but in the context of negation or in a
comparative, it can become positive again. How-
ever, note that this trigram may occur discontinu-
ously, e.g., in been less watery and more flavorful.
In order to match such occurrences, we need simple
regular expressions, e.g.,:
been.*more.*flavorful
This is exactly the kind of regular expressions we
asked experts to submit, and that we derived from
the crowdsourced annotation tasks. Note that the
sentence says nothing about the beer’s appearance,
so this feature is only relevant in TASTE, not in
APPEARANCE.
BoW and BoW+HI Our most simple baseline ap-
proach is a bag-of-words model of unigram features
(BoW). We lower-case our data, but leave in stop
words. We also introduce a semantically enriched
unigram model (BoW)+HI, where in addition to
representing what words occur in a text, we also
represent what Harvard Inquirer (HI)3 word classes
occur in it. The HI classes are used to generate
features from the crowdsourced annotation tasks,
so the semantically enriched unigram model is an
important baseline in our experiments below.
BoW+Exp In order to collect regular expressions
from experts, we set up a web interface for query-
ing held-out portions of the datasets with regular ex-
pressions that reports how occurrences of the sub-
mitted regular expressions correlate with class. We
used the Python re syntax for regular expressions
after augmenting word forms with POS and seman-
tic classes from the HI. Few of the experts made use
of the POS tags, but many regular expressions in-
cluded references to HI classes.
</bodyText>
<footnote confidence="0.987913">
3http://www.wjh.harvard.edu/ inquirer/homecat.htm
</footnote>
<page confidence="0.992511">
1477
</page>
<bodyText confidence="0.999958644444445">
Regular expressions submitted by participants
were visible to other participants during the exper-
iment, and participants were allowed to work to-
gether. Participants had 15 minutes to familiarize
themselves with the syntax used in the experiments.
Each query was executed in 2-30 seconds.
Seven researchers and graduate students spent
five effective hours querying the datasets with
regular expressions. In particular, they spent three
hours on the Stack Exchange dataset, and one hour
on each of the two RateBeer datasets. One had to
leave an hour early. So, in total, we spent 20 person
hours on Stack Exchange, and seven person hours
on each of the RateBeer datasets. In the five hours,
we collected 1,156 regular expressions for the
STACKOVERFLOW dataset, and about 650 regular
expressions for each of the two RateBeer datasets.
Exp refers to these sets of regular expressions. In
our experiments below we concatenate these with
the BoW features to form BoW+Exp.
BoW+AMT For each dataset, we also had 500 held-
out examples annotated by three turkers each, using
Amazon Mechanical Turk,4 obtaining 1,500 HITs
for each dataset. The annotators were presented with
each text, a review or an answer, twice: once as run-
ning text, once word-by-word with bullets to tick off
words. The annotators were instructed to tick off
words or phrases that they found predictive of the
text’s sentiment or answer quality. They were not in-
formed about the class of the text. We chose this an-
notation task, because it is relatively easy for annota-
tors to mark spans of text with a particular attribute.
This set-up has been used in other applications, in-
cluding NER (Finin et al., 2010) and error detection
(Dahlmeier et al., 2013). The annotators were con-
strained to tick off at least three words, including
one closed class item (closed class items were col-
ored differently). Finally, we only used annotators
with a track record of providing high-quality anno-
tations in previous tasks. It was clear from the aver-
age time spent by annotators that annotating STACK-
OVERFLOW was harder than annotating the Rate-
beer datasets. The average time spent on a Rate-
beer HIT was 44s, while for STACKOVERFLOW it
was 3m:8s. The mean number of words ticked off
</bodyText>
<footnote confidence="0.989664">
4www.mturk.com
</footnote>
<table confidence="0.99651925">
BoW HI Exp AMT
STACKOVERF 0.655 0.654 0.683 0.739
TASTE 0.798 0.797 0.798 0.867
APPEARANCE 0.758 0.760 0.761 0.859
</table>
<tableCaption confidence="0.999571">
Table 2: Results using all features
</tableCaption>
<bodyText confidence="0.994976564102564">
was between 5.6 and 7, with more words ticked off
in STACKOVERFLOW. The maximum number of
words ticked off by an annotator was 41. We spent
$292.5 on the annotations, including a trial round.
This was supposed to match, roughly, the cost of the
experts consulted for BoW+Exp.
The features generated from the annotations were
constructed as follows: We use a sliding window of
size 3 to extract trigrams over the possibly discon-
tinuous words ticked off by the annotators. These
trigrams were converted into regular expressions by
placing Kleene stars between the words. This gives
us a manually selected subset of skip trigrams. For
each skip trigram, we add copies with one or more
words replaced by one of their HI classes.
Feature combinations This subsection introduces
some harder baselines for our experiments, consid-
ered in Experiment #2. The simplest possible way
of combining unigram features is by considering n-
gram models. An n-gram extracts features from a
sliding window (of size n) over the text. We call this
model BoW(N = n). Our BoW(N = 1) model
takes word forms as features, and there are obvi-
ously more advanced ways of automatically combin-
ing such features.
Kernel representations We experimented with ap-
plying an approximate feature map for the addi-
tive x2-kernel. We used two sample steps, result-
ing in 4N + 1 features. See Vedaldi and Zimmer-
man (2011) for details.
Deep features We also ran denoising autoen-
coders (Pascal et al., 2008), previously applied
to a wide range of NLP tasks (Ranganath et al.,
2009; Socher et al., 2011; Chen et al., 2012), with
2N nodes in the middle layer to obtain a deep
representation of our datasets from x2-BoW input.
The network was trained for 15 epochs. We set the
drop-out rate to 0.0 and 0.3.
Summary of feature sets The feature sets – BoW,
</bodyText>
<page confidence="0.974561">
1478
</page>
<figure confidence="0.997274264150944">
74
Acuray 72
07
68
90
85
80
64
68
66
62
60
74
72
70
102 103
N
StackOverflow
−
χ2BoW
χ2 −BoW
χ2 −+
χ2 −BoW+BoWz
Acuray
90
χ2 χ2 χ2 χ2
85
80
90
85
80
75
70
102 103
N
Taste
χ2 −BoW
χ2 −BoW +HI
χ2 −BoW +Exp
χ2 −BoW +AMT
Acuray
90
80
85
70
75
χ2 −BoW
χ2−BoW +HI
χ2 −BoW +Exp
χ2 −BoW +AMT
102 103
N
Appearance
</figure>
<figureCaption confidence="0.999572">
Figure 1: Results selecting N features using x2 (left to right): STACKOVERFLOW, TASTE, and APPEARANCE. The
x-axis is logarithmic scale.
</figureCaption>
<figure confidence="0.999022019607843">
90
Acuray
90
Acuray 74
72
85
07
68
80
Acura
74
72
85
70
68
80
66
64
75
�2 −���(� =1)
�2 −���(� =2)
f −kernel
�2 −�����
�2 −��� +u��
62
χ2 −BoW(N =1)
χ2 −BoW(N =2)
χ2−kernel
χ2 −dauto
χ2 −BoW +AMT
70
60
102 103
N
χ2 −BoW(N =1)
χ2 −BoW(N =2)
χ2 −kernel
χ2 −dauto
χ2 −BoW+AMT
84
82
80
78
76
74
72
70
102 103
N
102 103
�
</figure>
<figureCaption confidence="0.977897">
Figure 2: Results using different feature combination techniques (left to right): STACKOVERFLOW, TASTE, and
APPEARANCE. The x-axis is logarithmic scale.
</figureCaption>
<bodyText confidence="0.9998076">
Exp and AMT – are very different. Their character-
istics are presented in Table 1. P(1) is the class dis-
tribution, e.g., the prior probability of positive class.
n is the number of data points, m the number of
features. Finally, px is the average density of data
points. One observation is of course that the expert
feature set Exp is much smaller than BoW and AMT,
but note also that the expert features fire about 150
times more often on average than the BoW features.
HI is only a small set of additional features.
</bodyText>
<sectionHeader confidence="0.999982" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.99923925">
Experiment #1: BoW vs. Exp and AMT We present
results using all features, as well as results obtained
after selecting k features as ranked by a simple x2
test. The results using all collected features are pre-
sented in Table 2. The error reduction on STACK-
OVERFLOW when adding crowdsourced features to
our baseline model (BoW+AMT), is 24.3%. On
TASTE, it is 34.2%. On APPEARANCE, it is 41.0%.
The BoW+AMT feature set is bigger than those of
the other models. We therefore report results using
the top-k features as ranked by a simple k2 test.
The result curves are presented in the three plots in
</bodyText>
<figureCaption confidence="0.541268">
Fig. 1. With +500 features, BoW+AMT outperforms
the other models by a large margin.
</figureCaption>
<bodyText confidence="0.996137166666667">
Experiment #2: AMT vs. more baselines The
BoW baseline uses a standard representation that,
while widely used, is usually thought of as a weak
baseline. BoW+HIT did not provide a stronger base-
line. We also show that bigram features, kernel-
based decomposition and deep features do not pro-
vide much stronger baselines either. The result
curves are presented in the three plots in Fig. 2.
BoW+AMT is still significantly better than all other
models with +500 features. Since autoencoders
are consistently worse than denoising autoencoders
(drop-out 0.3), we only plot denoising autoencoders.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999959285714286">
We presented a new method for deriving feature
representations from crowdsourced annotation tasks
and showed how it leads to 24%-41% error reduc-
tions on answer scoring and multi-aspect sentiment
analysis problems. We saw no significant improve-
ments using features contributed by experts, kernel
representations or learned deep representations.
</bodyText>
<page confidence="0.994568">
1479
</page>
<sectionHeader confidence="0.998339" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999409239130434">
Jordan Boyd-Graber, Brianna Satinoff, He He, and Hal
Daume. 2012. Besting the quiz master: Crowdsourc-
ing incremental classification games. In NAACL.
Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu,
Martin Chodorow, Lisa Braden-Harder, and Mary Dee
Harris. 1998. Automated scoring using a hybrid fea-
ture identification technique. In ACL.
Minmin Chen, Zhixiang Xu, Kilian Weinberger, and Fei
Sha. 2012. Marginalized denoising autoencoders for
domain adaptation. In ICML.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
English. In Workshop on Innovative Use of NLP for
Building Educational Applications, NAACL.
Pedro Domingos. 2012. A few useful things to know
about machine learning. In CACM.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in Twitter data with crowd-
sourcing. In NAACL Workshop on Creating Speech
and Language Data with Amazon’s Mechanical Turk.
Julian McAuley, Jure Leskovec, and Dan Jurafsky. 2012.
Learning attitudes and attributes from multi-aspect re-
views. In ICDM.
Claudiu-Christian Musat, Alireza Ghasemi, and Boi Falt-
ings. 2012. Sentiment analysis using a novel human
computation game. In Workshop on the People’s Web
Meets NLP, ACL.
Vincent Pascal, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and com-
posing robust features with denoising autoencoders. In
ICML.
Rajesh Ranganath, Dan Jurafsky, and Dan McFarland.
2009. It’s not you, it’s me: detecting flirting and its
misperception in speed-dates. In NAACL.
Richard Socher, Eric Huan, Jeffrey Pennington, Andrew
Ng, and Christopher Manning. 2011. Dynamic pool-
ing and unfolding recursive autoencoders for para-
phrase detection. In NIPS.
Omer Tamuz, Ce Liu, Serge Belongie, Ohad Shamir, and
Adam Tauman Kalai. 2011. Adaptively learning the
crowd kernel. In ICML.
Andrea Vedaldi and Andrew Zisserman. 2011. Efficient
additive kernels via explicit feature maps. In CVPR.
Kiri Wagstaff. 2012. Machine learning that matters. In
ICML.
</reference>
<page confidence="0.990343">
1480
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.747783">
<title confidence="0.999422">Using crowdsourcing to get representations based on regular expressions</title>
<author confidence="0.95925">Anders Søgaard</author>
<author confidence="0.95925">Hector Martinez</author>
<author confidence="0.95925">Jakob Elming</author>
<author confidence="0.95925">Anders</author>
<affiliation confidence="0.9983445">Center for Language University of</affiliation>
<address confidence="0.868783">DK-2300 Copenhagen</address>
<abstract confidence="0.993885058823529">Often the bottleneck in document classification is finding good representations that zoom in on the most important aspects of the doc- Most research uses representations, but relevant features often occur e.g., sentiment analysis. In this paper we present experiments getting experts to provide regular expressions, as well as crowdsourced annotation tasks from which regular expressions can be derived. Somewhat surprisingly, it turns out that these crowdsourced feature combinations outperform automatic feature combination methods, as well as expert features, by a very large margin and reduce error by 24-41% representations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>Brianna Satinoff</author>
<author>He He</author>
<author>Hal Daume</author>
</authors>
<title>Besting the quiz master: Crowdsourcing incremental classification games.</title>
<date>2012</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="2921" citStr="Boyd-Graber et al. (2012)" startWordPosition="436" endWordPosition="439">his is, to the best of our knowledge, the first attempt to crowdsource the problem of finding good representations. 1.1 Related work Musat et al. (2012) design a collaborative two-player game for sentiment annotation and collecting a sentiment lexicon. One player guesses the sentiment of a text and picks a word from it that is representative of its sentiment. The other player also provides a guess observing only this word. If the two guesses agree, both players get a point. The idea of gamifying the problem of finding good representations goes beyond crowdsourcing, but is not considered here. Boyd-Graber et al. (2012) crowdsource the feature weighting problem, but using standard representations. The work most similar to ours is probably Tamuz et al. (2011), who learn a ’crowd kernel’ by asking annotators to rate examples by similarity, providing an embedding that promotes feature combinations deemed relative when measuring similarity. 1476 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1476–1480, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics BoW Exp AMT n P(1) m µX m µX m µX STACKOVERFLOW 97,519 0.5013 30,716 0.</context>
</contexts>
<marker>Boyd-Graber, Satinoff, He, Daume, 2012</marker>
<rawString>Jordan Boyd-Graber, Brianna Satinoff, He He, and Hal Daume. 2012. Besting the quiz master: Crowdsourcing incremental classification games. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jill Burstein</author>
<author>Karen Kukich</author>
<author>Susanne Wolff</author>
<author>Chi Lu</author>
<author>Martin Chodorow</author>
<author>Lisa Braden-Harder</author>
<author>Mary Dee Harris</author>
</authors>
<title>Automated scoring using a hybrid feature identification technique.</title>
<date>1998</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1442" citStr="Burstein et al., 1998" startWordPosition="200" endWordPosition="203">tomatic feature combination methods, as well as expert features, by a very large margin and reduce error by 24-41% over n-gram representations. 1 Introduction Finding good representations of classification problems is often glossed over in the literature. Several authors have emphasized the need to pay more attention to finding such representations (Wagstaff, 2012; Domingos, 2012), but in document classification most research still uses n-gram representations. This paper considers two document classification problems where such representations seem inadequate. The problems are answer scoring (Burstein et al., 1998), on data from stackoverflow.com, and multi-attribute sentiment analysis (McAuley et al., 2012). We argue that in order to adequately represent such problems we need discontinuous features, i.e., regular expressions. The problem with using regular expressions as features is of course that even with a finite vocabulary we can generate infinitely many regular expressions that match our documents. We suggest to use expert knowledge or crowdsourcing in the loop. In particular we present experiments where standard representations are augmented with features from a few hours of manual work, by machi</context>
</contexts>
<marker>Burstein, Kukich, Wolff, Lu, Chodorow, Braden-Harder, Harris, 1998</marker>
<rawString>Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu, Martin Chodorow, Lisa Braden-Harder, and Mary Dee Harris. 1998. Automated scoring using a hybrid feature identification technique. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minmin Chen</author>
<author>Zhixiang Xu</author>
<author>Kilian Weinberger</author>
<author>Fei Sha</author>
</authors>
<title>Marginalized denoising autoencoders for domain adaptation.</title>
<date>2012</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="11134" citStr="Chen et al., 2012" startWordPosition="1763" endWordPosition="1766">eatures from a sliding window (of size n) over the text. We call this model BoW(N = n). Our BoW(N = 1) model takes word forms as features, and there are obviously more advanced ways of automatically combining such features. Kernel representations We experimented with applying an approximate feature map for the additive x2-kernel. We used two sample steps, resulting in 4N + 1 features. See Vedaldi and Zimmerman (2011) for details. Deep features We also ran denoising autoencoders (Pascal et al., 2008), previously applied to a wide range of NLP tasks (Ranganath et al., 2009; Socher et al., 2011; Chen et al., 2012), with 2N nodes in the middle layer to obtain a deep representation of our datasets from x2-BoW input. The network was trained for 15 epochs. We set the drop-out rate to 0.0 and 0.3. Summary of feature sets The feature sets – BoW, 1478 74 Acuray 72 07 68 90 85 80 64 68 66 62 60 74 72 70 102 103 N StackOverflow − χ2BoW χ2 −BoW χ2 −+ χ2 −BoW+BoWz Acuray 90 χ2 χ2 χ2 χ2 85 80 90 85 80 75 70 102 103 N Taste χ2 −BoW χ2 −BoW +HI χ2 −BoW +Exp χ2 −BoW +AMT Acuray 90 80 85 70 75 χ2 −BoW χ2−BoW +HI χ2 −BoW +Exp χ2 −BoW +AMT 102 103 N Appearance Figure 1: Results selecting N features using x2 (left to rig</context>
</contexts>
<marker>Chen, Xu, Weinberger, Sha, 2012</marker>
<rawString>Minmin Chen, Zhixiang Xu, Kilian Weinberger, and Fei Sha. 2012. Marginalized denoising autoencoders for domain adaptation. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
</authors>
<title>Building a large annotated corpus of learner English.</title>
<date>2013</date>
<booktitle>In Workshop on Innovative Use of NLP for Building Educational Applications, NAACL.</booktitle>
<contexts>
<context position="8881" citStr="Dahlmeier et al., 2013" startWordPosition="1382" endWordPosition="1385">btaining 1,500 HITs for each dataset. The annotators were presented with each text, a review or an answer, twice: once as running text, once word-by-word with bullets to tick off words. The annotators were instructed to tick off words or phrases that they found predictive of the text’s sentiment or answer quality. They were not informed about the class of the text. We chose this annotation task, because it is relatively easy for annotators to mark spans of text with a particular attribute. This set-up has been used in other applications, including NER (Finin et al., 2010) and error detection (Dahlmeier et al., 2013). The annotators were constrained to tick off at least three words, including one closed class item (closed class items were colored differently). Finally, we only used annotators with a track record of providing high-quality annotations in previous tasks. It was clear from the average time spent by annotators that annotating STACKOVERFLOW was harder than annotating the Ratebeer datasets. The average time spent on a Ratebeer HIT was 44s, while for STACKOVERFLOW it was 3m:8s. The mean number of words ticked off 4www.mturk.com BoW HI Exp AMT STACKOVERF 0.655 0.654 0.683 0.739 TASTE 0.798 0.797 0</context>
</contexts>
<marker>Dahlmeier, Ng, Wu, 2013</marker>
<rawString>Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu. 2013. Building a large annotated corpus of learner English. In Workshop on Innovative Use of NLP for Building Educational Applications, NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Domingos</author>
</authors>
<title>A few useful things to know about machine learning.</title>
<date>2012</date>
<booktitle>In CACM.</booktitle>
<contexts>
<context position="1203" citStr="Domingos, 2012" startWordPosition="169" endWordPosition="170">s getting experts to provide regular expressions, as well as crowdsourced annotation tasks from which regular expressions can be derived. Somewhat surprisingly, it turns out that these crowdsourced feature combinations outperform automatic feature combination methods, as well as expert features, by a very large margin and reduce error by 24-41% over n-gram representations. 1 Introduction Finding good representations of classification problems is often glossed over in the literature. Several authors have emphasized the need to pay more attention to finding such representations (Wagstaff, 2012; Domingos, 2012), but in document classification most research still uses n-gram representations. This paper considers two document classification problems where such representations seem inadequate. The problems are answer scoring (Burstein et al., 1998), on data from stackoverflow.com, and multi-attribute sentiment analysis (McAuley et al., 2012). We argue that in order to adequately represent such problems we need discontinuous features, i.e., regular expressions. The problem with using regular expressions as features is of course that even with a finite vocabulary we can generate infinitely many regular e</context>
</contexts>
<marker>Domingos, 2012</marker>
<rawString>Pedro Domingos. 2012. A few useful things to know about machine learning. In CACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Finin</author>
<author>Will Murnane</author>
<author>Anand Karandikar</author>
<author>Nicholas Keller</author>
<author>Justin Martineau</author>
<author>Mark Dredze</author>
</authors>
<title>Annotating named entities in Twitter data with crowdsourcing.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="8836" citStr="Finin et al., 2010" startWordPosition="1375" endWordPosition="1378">rs each, using Amazon Mechanical Turk,4 obtaining 1,500 HITs for each dataset. The annotators were presented with each text, a review or an answer, twice: once as running text, once word-by-word with bullets to tick off words. The annotators were instructed to tick off words or phrases that they found predictive of the text’s sentiment or answer quality. They were not informed about the class of the text. We chose this annotation task, because it is relatively easy for annotators to mark spans of text with a particular attribute. This set-up has been used in other applications, including NER (Finin et al., 2010) and error detection (Dahlmeier et al., 2013). The annotators were constrained to tick off at least three words, including one closed class item (closed class items were colored differently). Finally, we only used annotators with a track record of providing high-quality annotations in previous tasks. It was clear from the average time spent by annotators that annotating STACKOVERFLOW was harder than annotating the Ratebeer datasets. The average time spent on a Ratebeer HIT was 44s, while for STACKOVERFLOW it was 3m:8s. The mean number of words ticked off 4www.mturk.com BoW HI Exp AMT STACKOVER</context>
</contexts>
<marker>Finin, Murnane, Karandikar, Keller, Martineau, Dredze, 2010</marker>
<rawString>Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Annotating named entities in Twitter data with crowdsourcing. In NAACL Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian McAuley</author>
<author>Jure Leskovec</author>
<author>Dan Jurafsky</author>
</authors>
<title>Learning attitudes and attributes from multi-aspect reviews.</title>
<date>2012</date>
<booktitle>In ICDM.</booktitle>
<contexts>
<context position="1537" citStr="McAuley et al., 2012" startWordPosition="212" endWordPosition="215">ce error by 24-41% over n-gram representations. 1 Introduction Finding good representations of classification problems is often glossed over in the literature. Several authors have emphasized the need to pay more attention to finding such representations (Wagstaff, 2012; Domingos, 2012), but in document classification most research still uses n-gram representations. This paper considers two document classification problems where such representations seem inadequate. The problems are answer scoring (Burstein et al., 1998), on data from stackoverflow.com, and multi-attribute sentiment analysis (McAuley et al., 2012). We argue that in order to adequately represent such problems we need discontinuous features, i.e., regular expressions. The problem with using regular expressions as features is of course that even with a finite vocabulary we can generate infinitely many regular expressions that match our documents. We suggest to use expert knowledge or crowdsourcing in the loop. In particular we present experiments where standard representations are augmented with features from a few hours of manual work, by machine learning experts or by turkers. Somewhat surprisingly, we find that features derived from cr</context>
<context position="3955" citStr="McAuley et al. (2012)" startWordPosition="595" endWordPosition="598">ng, pages 1476–1480, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics BoW Exp AMT n P(1) m µX m µX m µX STACKOVERFLOW 97,519 0.5013 30,716 0.00131 1,156 0.1380 172,691 0.00331 TASTE 152,390 0.5003 38,227 0.00095 666 0.10631 114,588 0.00285 APPEARANCE 152,331 0.5009 37,901 0.00097 650 0.14629 102,734 0.00289 Table 1: Characteristics of the n x m data sets 2 Experiments Data The three datasets used in our experiments come from two sources, namely stackoverflow.com and ratebeer.com. The two beer review datasets (TASTE and APPEARANCE) are described in McAuley et al. (2012) and available for download.1 Each input example is an unstructured review text, and the associated label is the score assigned to taste or appearance by the reviewer. We randomly sample about 152k data points, as well as 500 examples for experiments with experts and turks. We extracted the STACKOVERFLOW dataset from a publicly available data dump,2, and we briefly describe our sampling process here. We select pairs of answers, where one is ranked higher than the other by stackoverflow.com users. Obviously the answers submitted first have a better chance of being ranked highly, so we also requ</context>
</contexts>
<marker>McAuley, Leskovec, Jurafsky, 2012</marker>
<rawString>Julian McAuley, Jure Leskovec, and Dan Jurafsky. 2012. Learning attitudes and attributes from multi-aspect reviews. In ICDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudiu-Christian Musat</author>
<author>Alireza Ghasemi</author>
<author>Boi Faltings</author>
</authors>
<title>Sentiment analysis using a novel human computation game.</title>
<date>2012</date>
<booktitle>In Workshop on the People’s Web Meets NLP, ACL.</booktitle>
<contexts>
<context position="2448" citStr="Musat et al. (2012)" startWordPosition="358" endWordPosition="361">documents. We suggest to use expert knowledge or crowdsourcing in the loop. In particular we present experiments where standard representations are augmented with features from a few hours of manual work, by machine learning experts or by turkers. Somewhat surprisingly, we find that features derived from crowdsourced annotation tasks lead to the best results across the three datasets. While crowdsourcing of annotation tasks has become increasing popular in NLP, this is, to the best of our knowledge, the first attempt to crowdsource the problem of finding good representations. 1.1 Related work Musat et al. (2012) design a collaborative two-player game for sentiment annotation and collecting a sentiment lexicon. One player guesses the sentiment of a text and picks a word from it that is representative of its sentiment. The other player also provides a guess observing only this word. If the two guesses agree, both players get a point. The idea of gamifying the problem of finding good representations goes beyond crowdsourcing, but is not considered here. Boyd-Graber et al. (2012) crowdsource the feature weighting problem, but using standard representations. The work most similar to ours is probably Tamuz</context>
</contexts>
<marker>Musat, Ghasemi, Faltings, 2012</marker>
<rawString>Claudiu-Christian Musat, Alireza Ghasemi, and Boi Faltings. 2012. Sentiment analysis using a novel human computation game. In Workshop on the People’s Web Meets NLP, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Pascal</author>
<author>Hugo Larochelle</author>
<author>Yoshua Bengio</author>
<author>Pierre-Antoine Manzagol</author>
</authors>
<title>Extracting and composing robust features with denoising autoencoders.</title>
<date>2008</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="11020" citStr="Pascal et al., 2008" startWordPosition="1742" endWordPosition="1745">ent #2. The simplest possible way of combining unigram features is by considering ngram models. An n-gram extracts features from a sliding window (of size n) over the text. We call this model BoW(N = n). Our BoW(N = 1) model takes word forms as features, and there are obviously more advanced ways of automatically combining such features. Kernel representations We experimented with applying an approximate feature map for the additive x2-kernel. We used two sample steps, resulting in 4N + 1 features. See Vedaldi and Zimmerman (2011) for details. Deep features We also ran denoising autoencoders (Pascal et al., 2008), previously applied to a wide range of NLP tasks (Ranganath et al., 2009; Socher et al., 2011; Chen et al., 2012), with 2N nodes in the middle layer to obtain a deep representation of our datasets from x2-BoW input. The network was trained for 15 epochs. We set the drop-out rate to 0.0 and 0.3. Summary of feature sets The feature sets – BoW, 1478 74 Acuray 72 07 68 90 85 80 64 68 66 62 60 74 72 70 102 103 N StackOverflow − χ2BoW χ2 −BoW χ2 −+ χ2 −BoW+BoWz Acuray 90 χ2 χ2 χ2 χ2 85 80 90 85 80 75 70 102 103 N Taste χ2 −BoW χ2 −BoW +HI χ2 −BoW +Exp χ2 −BoW +AMT Acuray 90 80 85 70 75 χ2 −BoW χ2−B</context>
</contexts>
<marker>Pascal, Larochelle, Bengio, Manzagol, 2008</marker>
<rawString>Vincent Pascal, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoencoders. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajesh Ranganath</author>
<author>Dan Jurafsky</author>
<author>Dan McFarland</author>
</authors>
<title>It’s not you, it’s me: detecting flirting and its misperception in speed-dates.</title>
<date>2009</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="11093" citStr="Ranganath et al., 2009" startWordPosition="1755" endWordPosition="1758">onsidering ngram models. An n-gram extracts features from a sliding window (of size n) over the text. We call this model BoW(N = n). Our BoW(N = 1) model takes word forms as features, and there are obviously more advanced ways of automatically combining such features. Kernel representations We experimented with applying an approximate feature map for the additive x2-kernel. We used two sample steps, resulting in 4N + 1 features. See Vedaldi and Zimmerman (2011) for details. Deep features We also ran denoising autoencoders (Pascal et al., 2008), previously applied to a wide range of NLP tasks (Ranganath et al., 2009; Socher et al., 2011; Chen et al., 2012), with 2N nodes in the middle layer to obtain a deep representation of our datasets from x2-BoW input. The network was trained for 15 epochs. We set the drop-out rate to 0.0 and 0.3. Summary of feature sets The feature sets – BoW, 1478 74 Acuray 72 07 68 90 85 80 64 68 66 62 60 74 72 70 102 103 N StackOverflow − χ2BoW χ2 −BoW χ2 −+ χ2 −BoW+BoWz Acuray 90 χ2 χ2 χ2 χ2 85 80 90 85 80 75 70 102 103 N Taste χ2 −BoW χ2 −BoW +HI χ2 −BoW +Exp χ2 −BoW +AMT Acuray 90 80 85 70 75 χ2 −BoW χ2−BoW +HI χ2 −BoW +Exp χ2 −BoW +AMT 102 103 N Appearance Figure 1: Results s</context>
</contexts>
<marker>Ranganath, Jurafsky, McFarland, 2009</marker>
<rawString>Rajesh Ranganath, Dan Jurafsky, and Dan McFarland. 2009. It’s not you, it’s me: detecting flirting and its misperception in speed-dates. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric Huan</author>
<author>Jeffrey Pennington</author>
<author>Andrew Ng</author>
<author>Christopher Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="11114" citStr="Socher et al., 2011" startWordPosition="1759" endWordPosition="1762"> An n-gram extracts features from a sliding window (of size n) over the text. We call this model BoW(N = n). Our BoW(N = 1) model takes word forms as features, and there are obviously more advanced ways of automatically combining such features. Kernel representations We experimented with applying an approximate feature map for the additive x2-kernel. We used two sample steps, resulting in 4N + 1 features. See Vedaldi and Zimmerman (2011) for details. Deep features We also ran denoising autoencoders (Pascal et al., 2008), previously applied to a wide range of NLP tasks (Ranganath et al., 2009; Socher et al., 2011; Chen et al., 2012), with 2N nodes in the middle layer to obtain a deep representation of our datasets from x2-BoW input. The network was trained for 15 epochs. We set the drop-out rate to 0.0 and 0.3. Summary of feature sets The feature sets – BoW, 1478 74 Acuray 72 07 68 90 85 80 64 68 66 62 60 74 72 70 102 103 N StackOverflow − χ2BoW χ2 −BoW χ2 −+ χ2 −BoW+BoWz Acuray 90 χ2 χ2 χ2 χ2 85 80 90 85 80 75 70 102 103 N Taste χ2 −BoW χ2 −BoW +HI χ2 −BoW +Exp χ2 −BoW +AMT Acuray 90 80 85 70 75 χ2 −BoW χ2−BoW +HI χ2 −BoW +Exp χ2 −BoW +AMT 102 103 N Appearance Figure 1: Results selecting N features u</context>
</contexts>
<marker>Socher, Huan, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric Huan, Jeffrey Pennington, Andrew Ng, and Christopher Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Tamuz</author>
<author>Ce Liu</author>
<author>Serge Belongie</author>
<author>Ohad Shamir</author>
<author>Adam Tauman Kalai</author>
</authors>
<title>Adaptively learning the crowd kernel.</title>
<date>2011</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="3062" citStr="Tamuz et al. (2011)" startWordPosition="459" endWordPosition="462">2012) design a collaborative two-player game for sentiment annotation and collecting a sentiment lexicon. One player guesses the sentiment of a text and picks a word from it that is representative of its sentiment. The other player also provides a guess observing only this word. If the two guesses agree, both players get a point. The idea of gamifying the problem of finding good representations goes beyond crowdsourcing, but is not considered here. Boyd-Graber et al. (2012) crowdsource the feature weighting problem, but using standard representations. The work most similar to ours is probably Tamuz et al. (2011), who learn a ’crowd kernel’ by asking annotators to rate examples by similarity, providing an embedding that promotes feature combinations deemed relative when measuring similarity. 1476 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1476–1480, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics BoW Exp AMT n P(1) m µX m µX m µX STACKOVERFLOW 97,519 0.5013 30,716 0.00131 1,156 0.1380 172,691 0.00331 TASTE 152,390 0.5003 38,227 0.00095 666 0.10631 114,588 0.00285 APPEARANCE 152,331 0.5009 37,901 0.00097 6</context>
</contexts>
<marker>Tamuz, Liu, Belongie, Shamir, Kalai, 2011</marker>
<rawString>Omer Tamuz, Ce Liu, Serge Belongie, Ohad Shamir, and Adam Tauman Kalai. 2011. Adaptively learning the crowd kernel. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Vedaldi</author>
<author>Andrew Zisserman</author>
</authors>
<title>Efficient additive kernels via explicit feature maps.</title>
<date>2011</date>
<booktitle>In CVPR.</booktitle>
<marker>Vedaldi, Zisserman, 2011</marker>
<rawString>Andrea Vedaldi and Andrew Zisserman. 2011. Efficient additive kernels via explicit feature maps. In CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiri Wagstaff</author>
</authors>
<title>Machine learning that matters.</title>
<date>2012</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="1186" citStr="Wagstaff, 2012" startWordPosition="167" endWordPosition="168">esent experiments getting experts to provide regular expressions, as well as crowdsourced annotation tasks from which regular expressions can be derived. Somewhat surprisingly, it turns out that these crowdsourced feature combinations outperform automatic feature combination methods, as well as expert features, by a very large margin and reduce error by 24-41% over n-gram representations. 1 Introduction Finding good representations of classification problems is often glossed over in the literature. Several authors have emphasized the need to pay more attention to finding such representations (Wagstaff, 2012; Domingos, 2012), but in document classification most research still uses n-gram representations. This paper considers two document classification problems where such representations seem inadequate. The problems are answer scoring (Burstein et al., 1998), on data from stackoverflow.com, and multi-attribute sentiment analysis (McAuley et al., 2012). We argue that in order to adequately represent such problems we need discontinuous features, i.e., regular expressions. The problem with using regular expressions as features is of course that even with a finite vocabulary we can generate infinite</context>
</contexts>
<marker>Wagstaff, 2012</marker>
<rawString>Kiri Wagstaff. 2012. Machine learning that matters. In ICML.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
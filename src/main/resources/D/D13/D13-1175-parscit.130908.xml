<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.998884">
Boosting Cross-Language Retrieval by Learning
Bilingual Phrase Associations from Relevance Rankings
</title>
<author confidence="0.983654">
Artem Sokolov and Laura Jehl and Felix Hieber and Stefan Riezler
</author>
<affiliation confidence="0.788272">
Department of Computational Linguistics
Heidelberg University, 69120 Heidelberg, Germany
</affiliation>
<email confidence="0.996453">
{sokolov,jehl,hieber,riezler}@cl.uni-heidelberg.de
</email>
<sectionHeader confidence="0.99858" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99989825">
We present an approach to learning bilin-
gual n-gram correspondences from relevance
rankings of English documents for Japanese
queries. We show that directly optimizing
cross-lingual rankings rivals and complements
machine translation-based cross-language in-
formation retrieval (CLIR). We propose an ef-
ficient boosting algorithm that deals with very
large cross-product spaces of word correspon-
dences. We show in an experimental evalu-
ation on patent prior art search that our ap-
proach, and in particular a consensus-based
combination of boosting and translation-based
approaches, yields substantial improvements
in CLIR performance. Our training and test
data are made publicly available.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996806122449">
The central problem addressed in Cross-Language
Information Retrieval (CLIR) is that of translating
or projecting a query into the language of the docu-
ment repository across which retrieval is performed.
There are two main approaches to tackle this prob-
lem: The first approach leverages the standard Sta-
tistical Machine Translation (SMT) machinery to
produce a single best translation that is used as
search query in the target language. We will hence-
forth call this the direct translation approach. This
technique is particularly useful if large amounts of
data are available in domain-specific form.
Alternative approaches avoid to solve the hard
problem of word reordering, and instead rely on
token-to-token translations that are used to project
the query terms into the target language with a
probabilistic weighting of the standard term tf-idf
scheme. Darwish and Oard (2003) termed this
method the probabilistic structured query approach.
The advantage of this technique is an implicit query
expansion effect due to the use of probability distri-
butions over term translations (Xu et al., 2001). Re-
cent research has shown that leveraging query con-
text by extracting term translation probabilities from
n-best direct translations of queries instead of using
context-free translation tables outperforms both di-
rect translation and context-free projection (Ture et
al., 2012b; Ture et al., 2012a).
While direct translation as well as probabilistic
structured query approaches use machine learning to
optimize the SMT module, retrieval is done by stan-
dard search algorithms in both approaches. For ex-
ample, Google’s CLIR approach uses their standard
proprietary search engine (Chin et al., 2008). Ture et
al. (2012b; 2012a) use standard retrieval algorithms
such as BM25 (Robertson et al., 1998). That means,
machine learning in SMT-based approaches concen-
trates on the cross-language aspect of CLIR and is
agnostic of the ultimate ranking task.
In this paper, we present a method to project
search queries into the target language that is com-
plementary to SMT-based CLIR approaches. Our
method learns a table of n-gram correspondences by
direct optimization of a ranking objective on rele-
vance rankings of English documents for Japanese
queries. Our model is similar to the approach of
Bai et al. (2010) who characterize their technique as
“Learning to rank with (a Lot of) Word Features”.
Given a set of search queries q E IRQ and docu-
</bodyText>
<page confidence="0.936173">
1688
</page>
<note confidence="0.730562">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1688–1699,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.977434153846154">
ments d E IRD, where the jth dimension of a vector
indicates the occurrence of the jth word for dictio-
naries of size Q and D, we want to learn a score
f(q, d) between a query and a given document us-
ing the model1
f(q, d) = q&gt;Wd =
We take a pairwise ranking approach to optimiza-
tion. That is, given labeled data in the form of a
set R of tuples (q, d+, d−), where d+ is a relevant
(or higher ranked) document and d− an irrelevant
(or lower ranked) document for query q, the goal
is to find a weight matrix W E IRQ×D such that
f(q, d+) &gt; f(q, d−) for all data tuples from R.
The scoring model learns weights for all possible
correspondences of query terms and document terms
by directly optimizing the ranking objective at hand.
Such a phrase table contains domain-specific word
associations that are useful to discern relevant from
irrelevant documents, something that is orthogonal
and complementary to standard SMT models.
The challenge of our approach can be explained
by constructing a joint feature map 0 from the outer
product of the vectors q and d where
0((i−1)D+j)(q, d) = (q (9 d)ij = (qd&gt;)ij. (1)
Using this feature map, we see that the score func-
tion f can be written in the standard form of a lin-
ear model that computes the inner product between
a weight vector w and a feature vector 0 where
w,0 E IRQ×D and
f(q, d) = (w, 0(q, d)). (2)
While various standard algorithms exist to optimize
linear models, the difficulty lies in the memory foot-
print and capacity of the word-based model. A full-
sized model includes Q x D parameters which is
easily in the billions even for moderately sized dic-
tionaries. Clearly, an efficient implementation and
remedies against overfitting are essential.
The main contribution of our paper is the pre-
sentation of algorithms that make learning a phrase
</bodyText>
<footnote confidence="0.935388">
1With bold letters we denote vectors for query q and docu-
ment d. Vector components are denoted with normal font letters
and indices (e.g., qi).
</footnote>
<bodyText confidence="0.999803655172414">
table by direct rank optimization feasible, and an
experimental verification of the benefits of this ap-
proach, especially with regard to a combination
of the orthogonal information sources of ranking-
based and SMT-based CLIR approaches. Our ap-
proach builds upon a boosting framework for pair-
wise ranking (Freund et al., 2003) that allows the
model to grow incrementally, thus avoiding having
to deal with the full matrix W. Furthermore, we
present an implementation of boosting that utilizes
parallel estimation on bootstrap samples from the
training set for increased efficiency and reduced er-
ror (Breiman, 1996). Our “bagged boosting” ap-
proach allows to combine incremental feature selec-
tion, parallel training, and efficient management of
large data structures.
We show in an experimental evaluation on large-
scale retrieval on patent abstracts that our boosting
approach is comparable in MAP and improves sig-
nificantly by 13-15 PRES points over very competi-
tive translation-based CLIR systems that are trained
on 1.8 million parallel sentence pairs from Japanese-
English patent documents. Moreover, a combination
of the orthogonal information learned in ranking-
based and translation-based approaches improves
over 7 MAP points and over 15 PRES points over the
respective translation-based system in a consensus-
based voting approach following the Borda Count
technique (Aslam and Montague, 2001).
</bodyText>
<sectionHeader confidence="0.999892" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9988775">
Recent research in CLIR follows the two main
paradigms of direct translation and probabilistic
structured query approaches. An example for the
first approach is the work of Magdy and Jones
(2011) who presented an efficient technique to adapt
off-the-shelf SMT systems for CLIR by training
them on data pre-processed for retrieval (case fold-
ing, stopword removal, stemming). Nikoulina et al.
(2012) presented an approach to direct translation-
based CLIR where the n-best list of an SMT system
is re-ranked according to the MAP performance of
the translated queries. The probabilistic structured
query approach has seen a lot of work on context-
aware query expansion across languages, based on
various similarity statistics (Ballesteros and Croft,
1998; Gao et al., 2001; Lavrenko et al., 2002; Gao
</bodyText>
<equation confidence="0.950901333333333">
� Q
i=1
qiWijdj.
D
E
j=1
</equation>
<page confidence="0.969522">
1689
</page>
<bodyText confidence="0.999865066666667">
et al., 2007). At the time of writing this paper, the
most recent extension to this paradigm is Ture et
al. (2012a). In addition to projecting terms from
n-best translations, they propose a projection ex-
tracted from the hierarchical phrase- based grammar
models, and a scoring method based on multi-token
terms. Since the latter techniques achieved only
marginal improvements over the context-sensitive
query translation from n-best lists, we did not pur-
sue them in our work.
CLIR in the context of patent prior art search
was done as extrinsic evaluation at the NTCIR
PatentMT2 workshops until 2010, and has been on-
going in the CLEF-IP3 benchmarking workshops
since 2009. However, most workshop participants
did either not make use of automatic translation at
all, or they used an off-the-shelf translation tool.
This is due to the CLEF-IP data collection where
parts of patent documents are provided as man-
ual translations into three languages. In order to
evaluate CLIR in a truly cross-lingual scenario, we
created a large patent CLIR dataset where queries
and documents are Japanese and English patent ab-
stracts, respectively.
Ranking approaches to CLIR have been presented
by Guo and Gomes (2009) who use pairwise rank-
ing for patent retrieval. Their method is a classical
learning-to-rank setup where retrieval scores such as
tf-idf or BM25 are combined with domain knowl-
edge on patent class, inventor, date, location, etc.
into a dense feature vector of a few hundred fea-
tures. Methods to learn word-based translation cor-
respondences from supervised ranking signals have
been presented by Bai et al. (2010) and Chen et
al. (2010). These approaches tackle the problem of
complexity and capacity of the cross product matrix
of word correspondences from different directions.
The first proposes to learn a low rank representa-
tion of the matrix; the second deploys sparse online
learning under f, regularization to keep the matrix
small. Both approaches are mainly evaluated in a
monolingual setting. The cross-lingual evaluation
presented in Bai et al. (2010) uses weak translation-
based baselines and non-public data such that a di-
rect comparison is not possible.
</bodyText>
<footnote confidence="0.998647">
2http://research.nii.ac.jp/ntcir/ntcir/
3http://www.ifs.tuwien.ac.at/˜clef-ip/
</footnote>
<bodyText confidence="0.999887142857143">
A combination of bagging and boosting in the
context of retrieval has been presented by Pavlov et
al. (2010) and Ganjisaffar et al. (2011). This work
is done in a standard learning-to-rank setup using a
few hundred dense features trained on hundreds of
thousands of pairs. Our setup deals with billions of
sparse features (from the cross-product of the un-
restricted dictionaries) trained on millions of pairs
(sampled from a much larger space). Parallel boost-
ing where all feature weights are updated simultane-
ously has been presented by Collins et al. (2002) and
Canini et al. (2010). The first method distributes the
gradient calculation for different features among dif-
ferent compute nodes. This is not possible in our ap-
proach because we construct the cross-product ma-
trix on-the-fly. The second approach requires sub-
stantial efforts in changing the data representation
to use the MapReduce framework. Overall, one of
the goals of our work is sequential updating for im-
plicit feature selection, something that runs contrary
to parallel boosting.
</bodyText>
<sectionHeader confidence="0.99952" genericHeader="method">
3 CLIR Approaches
</sectionHeader>
<subsectionHeader confidence="0.999777">
3.1 Direct translation approach
</subsectionHeader>
<bodyText confidence="0.999843">
For direct translation, we use the SCFG decoder
cdec (Dyer et al., 2010)4 and build grammars us-
ing its implementation of the suffix array extraction
method described in Lopez (2007). Word align-
ments are built from all parallel data using mgiza5
and the Moses scripts6. SCFG models use the same
settings as described in Chiang (2007). Training
and querying of a modified Kneser-Ney smoothed 5-
gram language model are done on the English side
of the training data using KenLM (Heafield, 2011)7.
Model parameters were optimized using cdec’s im-
plementation of MERT (Och (2003)).
At retrieval time, all queries are translated
sentence-wise and subsequently re-joined to form
one query per patent. Our baseline retrieval system
uses the Okapi BM25 scores for document ranking.
</bodyText>
<footnote confidence="0.993762857142857">
4https://github.com/redpony/cdec
5http://www.kyloo.net/software/doku.php/
mgiza:overview
6http://www.statmt.org/moses/?n=Moses.
SupportTools
7http://kheafield.com/code/kenlm/
estimation/
</footnote>
<page confidence="0.984703">
1690
</page>
<subsectionHeader confidence="0.999304">
3.2 Probabilistic structured query approach
</subsectionHeader>
<bodyText confidence="0.999902904761905">
Early Probabilistic Structured Query approaches
(Xu et al., 2001; Darwish and Oard, 2003) represent
translation options by lexical, i.e., token-to-token
translation tables that are estimated using standard
word alignment techniques (Och and Ney, 2000).
Later approaches (Ture et al., 2012b; Ture et al.,
2012a) extract translation options from the decoder’s
n-best list for translating a particular query. The
central idea is to let the language model choose flu-
ent, context-aware translations for each query term
during decoding. This retains the desired query-
expansion effect of probabilistic structured models,
but it reduces query drift by filtering translations
with respect to the context of the full query.
A projection of source language query terms f E
F into the target language is achieved by repre-
senting each source token f by its probabilistically
weighted translations. The score of target document
E, given source language query F, is computed by
calculating the BM25 rank over projected term fre-
quency and document frequency weights as follows:
</bodyText>
<equation confidence="0.981982625">
Xscore(E|F) = BM25(tf(f,E),df(f)) (3)
fEF
X
tf(f, E) =
eEEf
X
df(f) =
eEEf
</equation>
<bodyText confidence="0.999609">
where Ef = {e E E|p(e|f) &gt; pL} is the set of
translation options for query term f with probability
greater than pL. We also use a cumulative threshold
pC so that only the most probable options are added
until pC is reached.
Ture et al. (2012b; 2012a) achieved best retrieval
performance by interpolating between (context-free)
lexical translation probabilities plex estimated on
symmetrized word alignments, and (context-aware)
translation probabilities pnbest estimated on the n-
best list of an SMT decoder:
</bodyText>
<equation confidence="0.998891">
p(e|f) = Apnbest(e|f) + (1 − A)plex(e|f) (4)
</equation>
<bodyText confidence="0.7022005">
pnbest(e|f) is estimated by calculating expectations
of term translations from k-best translations:
</bodyText>
<equation confidence="0.619857">
pnbest(e |f) = nPnk=1 ak(e, f)D(k, F)
Pk=1 Pe, ak(e , f)D(k, F)
</equation>
<bodyText confidence="0.999977666666667">
where ak(e, f) is a function indicating an alignment
of target term e to source term f in the kth derivation
of query F, and D(k, F) is the model score of the kth
derivation in the n-best list for query F.
We use the same hierarchical phrase-based sys-
tem that was used for direct translation to calcu-
late n-best translations for the probabilistic struc-
tured query approach. This allows us to extract
word alignments between source and target text for
F from the SCFG rules used in the derivation. The
concept of self-translation is covered by the de-
coder’s ability to use pass-through rules if words or
phrases cannot be translated.
Probabilistic structured queries that include
context-aware estimates of translation probabilities
require a preservation of sentence-wise context-
sensitivity also in retrieval. Thus, unlike the direct
translation approach, we compute weighted term
and document frequencies for each sentence s in
query F separately. The scoring (3) of a target doc-
ument for a multiple sentence query then becomes:
</bodyText>
<equation confidence="0.9305025">
Xscore(E|F) =
s in F
</equation>
<subsectionHeader confidence="0.9840835">
3.3 Direct Phrase Table Learning from
Relevance Rankings
</subsectionHeader>
<bodyText confidence="0.998448238095238">
Pairwise Ranking using Boosting The general
form of the RankBoost algorithm (Freund et al.,
2003; Collins and Koo, 2005) defines a scoring
function f(q, d) on query q and document d as a
weighted linear combination of T weak learners ht
such that f(q, d) = PTt=1 wtht(q, d). Weak learn-
ers can belong to an arbitrary family of functions,
but in our case they are restricted to the simplest
case of unparameterized indicator functions select-
ing components of the feature vector O(q, d) in (1)
such that f is of the standard linear form (2). In our
experiments, these features indicate the presence of
pairs of uni- and bi-grams from the source-side vo-
cabulary of query terms and the target-side vocabu-
lary of document-terms, respectively. Furthermore,
in order to simulate the pass-through behavior of
SMT, we introduce additional features to the model
that indicate the identity of terms in source and tar-
get. All identity features have the same fixed weight
6, which is found on the development set.
For training, we are given labeled data in the form
</bodyText>
<equation confidence="0.907735">
tf(e, E)p(e|f)
df(e)p(e|f)
X BM25(tf(f, E), df(f))
fEs
</equation>
<page confidence="0.882126">
1691
</page>
<bodyText confidence="0.999264714285714">
of a set R of tuples (q, d+, d−), where d+ is a rel-
evant (or higher ranked) document and d− an ir-
relevant (or lower ranked) document for query q.
RankBoost’s objective is to correctly rank query-
document pairs such that f(q, d+) &gt; f(q, d−) for
all data tuples from R. RankBoost achieves this by
optimizing the following convex exponential loss:
</bodyText>
<equation confidence="0.9910945">
�Lexp = D(q, d+, d−)ef(q,d−)−f(q,d+),
(q,d+,d−)∈R
</equation>
<bodyText confidence="0.98879072972973">
where D(q, d+, d−) is a non-negative importance
function on pairs of documents for a given q.
We optimize Lexp in a greedy iterative fash-
ion, which closely follows an efficient algorithm of
Collins and Koo (2005) for the case of binary-valued
h. In each step, the single feature h is selected that
provides the largest decrease of Lexp, i.e., that has
the largest projection on the direction of the gradi-
ent VhLexp. Because of the sequential nature of
the algorithm, RankBoost implicitly performs auto-
matic feature selection and regularization (Rosset et
al., 2004), which is crucial to reduce complexity and
capacity for our application.
Parallelization and Bagging To achieve paral-
lelization we use a variant of bagging (Breiman,
1996) on top of boosting, which has been observed
to improve performance, reduce variance and is
trivial to parallelize. The procedure is described
as part of Algorithm 1: From the set of prefer-
ence pairs R, draw S equal-sized samples with
replacement and distribute to nodes. Then, us-
ing each of the samples as a training set, sep-
arate boosting models {wst , hst}, s = 1... S are
trained that contain the same number of features
t = 1... T. Finally the models are averaged:
f (q, d) = s Et Es wst hst (q, d)
Algorithm The entire training procedure is out-
lined in Algorithm 1. For each possible feature h
we maintain auxiliary variables Wh+ and Wh− :
D(q, d+, d−),
which are the cumulative weights of correctly and
incorrectly ranked instances by a candidate feature
h. The absolute value of aLexp/ah can be ex-
pressed as I �W+ − �W−h I which is used as fea-
ture selection criterion (Collins and Koo, 2005).
The optimum of minimizing Lexp over w (with
fixed h) can be shown to be w = 1 ln Wh +EZ
</bodyText>
<equation confidence="0.851537">
2 +
Wh− +EZ,
</equation>
<bodyText confidence="0.999296833333333">
where E is a smoothing parameter to avoid prob-
lems with small Wh± (Schapire and Singer, 1999),
and Z = E(q,d+,d−)∈R D(q, d+, d−). Further-
more, for each step t of the learning process, values
of D are updated to concentrate on pairs that have
not been correctly ranked so far:
</bodyText>
<equation confidence="0.987723">
Dt+1 = Dt · ewt(ht(q,d−)−ht(q,d+)) . (5)
</equation>
<bodyText confidence="0.94499325">
Finally, to speed up learning, on iteration t we
recalculate Wh± only for those h that cooccur
with previously selected ht and keep the rest un-
changed (Collins and Koo, 2005).
</bodyText>
<figureCaption confidence="0.40759">
Algorithm 1: Bagged Boosting
</figureCaption>
<bodyText confidence="0.93230775">
Input: training tuples R, max number of features
T, initial D0, smoothing param. E ^_ 10−5
Initialize:
from R draw S samples with replacement and
distribute to nodes
Learn:
for all samples s = 1... S in parallel do
calculate Wh+,Wh− , Z on sample’s data
for all t = 1 ... T do
choose ht = arg maxh I . Wh+ − �W−h
h +~Z
and wt = 2 1 ln W +
</bodyText>
<equation confidence="0.670483777777778">
Wh− +EZ
update Dt according to (5)
update Wh± for all h that cooccur with ht
end
return to master {hst, wst }, t = 1... T
end
Bagging:
return scoring function
f (q, d) = s Et Es wst hst (q, d)
</equation>
<bodyText confidence="0.996268428571429">
Implementation Because of the total number of
features (billions) there are several obstacles for the
straight-forward implementation of Algorithm 1.
First, we cannot directly access all pairs (q, d)
containing a particular feature h needed for calcu-
lating Wh± . Building an inverted index is compli-
cated as it needs to fit into memory for fast fre-
</bodyText>
<equation confidence="0.576852333333333">
W± = E
h
(q,d+,d−):h(q,d+)−h(q,d−)=±1
</equation>
<page confidence="0.90255">
1692
</page>
<bodyText confidence="0.9991404">
quent access8. We resort to the on-the-fly creation of
the cross-product space of features, following prior
work by Grangier and Bengio (2008) and Goel et al.
(2008). That is, while processing a pair (q, d), we
update Wh� for all h found for the pair.
Second, even if the explicit representation of all
features is avoided by on-the-fly feature construc-
tion, we still need to keep all Wh� in addressable
RAM. To achieve that we use hash kernels (Shi et
al., 2009) and map original features into b-bit integer
hashes. The values Wh1� for new, “hashed”, features
h&apos; become Wh1� = Eh:HASH(h)=h1 Wh� . We used
the MurmurHash3 function on the UTF-8 represen-
tations of features and b = 30 (resulting in more
than 1 billion distinct hashes).
</bodyText>
<sectionHeader confidence="0.98356" genericHeader="method">
4 Model Combination by Borda Counts
</sectionHeader>
<bodyText confidence="0.9937084">
SMT-based approaches to CLIR and our boosting
approach have different strengths. The SMT-based
approaches produce fluent translations that are use-
ful for matching general passages written in natu-
ral language. Both baseline SMT-based approaches
presented above are agnostic of the ultimate retrieval
task and are not specifically adapted for it. The
boosting method, on the other hand, learns domain-
specific word associations that are useful to discern
relevant from irrelevant documents. In order to com-
bine these orthogonal sources of information in a
way that democratically respects each approach we
use Borda Counts, i.e., a consensus-based voting
procedure that has been successfully employed to
aggregate ranked lists of documents for metasearch
(Aslam and Montague, 2001).
We implemented a weighted version of the Borda
Count method where each voter has a fixed amount
of voting points which she is free to distribute among
the candidates to indicate the amount of preference
she is giving to each of them. In the case of retrieval,
for each q, the candidates are the scored documents
in the retrieved subset of the whole document set.
The aggregate score fagg for two rankings f1(q, d)
8It is possible to construct separate query and document in-
verted indices and intersect them on the fly to determine the
set of documents that contains some pair of words. In practice,
however, we found the overhead of set intersection during each
feature access prohibitive.
and f2(q, d) for all (q, d) in the test set is then:
</bodyText>
<equation confidence="0.997706">
fagg(q,d) = n f1(q,d) +(1−� f2(q,d)
Ed f1 (q, d) ) Ed f2(q, d)
</equation>
<bodyText confidence="0.9998656875">
In practice, the normalizations sum over the top K
retrieved documents. If a document is present only
in the top-K list of one system, its score is con-
sidered zero for the other system. The aggregated
scores fagg(q, d) are sorted in descending order and
top K scores are kept for evaluation.
Using the terminology proposed by Belkin et
al. (1995), combining several systems’ scores with
Borda Counts can be viewed as the “data fusion”
approach to IR, that merges outputs of the systems,
while the PSQ baseline is an example of the “query
combination” approach that extends the query at the
input. Both techniques were earlier found to have
similar performance in CLIR tasks based on direct
translation, with a preference for the data fusion ap-
proach (Jones and Lam-Adesina, 2002).
</bodyText>
<sectionHeader confidence="0.984652" genericHeader="method">
5 Translation and Ranking Data
</sectionHeader>
<subsectionHeader confidence="0.983066">
5.1 Parallel Translation Data
</subsectionHeader>
<bodyText confidence="0.999981095238095">
For Japanese-to-English patent translation we used
data provided by the organizers of the NTCIR9
workshop for the JP-EN PatentMT subtask. In par-
ticular, we used the data provided for NTCIR-7 (Fu-
jii et al., 2008), consisting of 1.8 million parallel
sentence pairs from the years 1993-2002 for train-
ing. For parameter tuning we used the develop-
ment set of the NTCIR-8 test collection, consisting
of 2,000 sentence pairs. The data were extracted
from the description section of patents published
by the Japanese Patent Office (JPO) and the United
States Patent and Trademark Office (USPTO) by the
method described in Utiyama and Isahara (2007).
Japanese text was segmented using the MeCab10
toolkit. Following Feng et al. (2011), we applied
a modified version of the compound splitter de-
scribed in Koehn and Knight (2003) to katakana
terms, which are often transliterations of English
compound words. As these are usually not split by
MeCab, they can cause a large number of out-of-
vocabulary terms.
</bodyText>
<footnote confidence="0.99955">
9http://research.nii.ac.jp/ntcir/ntcir/
10https://code.google.com/p/mecab/
</footnote>
<page confidence="0.64118">
1693
</page>
<table confidence="0.999742">
#queries #relevant #unique docs
train 107,061 1,422,253 888,127
dev 2,000 26,478 25,669
test 2,000 25,173 24,668
</table>
<tableCaption confidence="0.9999">
Table 1: Statistics of ranking data.
</tableCaption>
<bodyText confidence="0.999585">
For the English side of the training data, we ap-
plied a modified version of the tokenizer included in
the Moses scripts. This tokenizer relies on a list of
non-breaking prefixes which mark expressions that
are usually followed by a “.” (period). We cus-
tomized the list of prefixes by adding some abbrevi-
ations like “Chem”, “FIG” or “Pat”, which are spe-
cific to patent documents.
</bodyText>
<subsectionHeader confidence="0.999422">
5.2 Ranking Data from Patent Citations
</subsectionHeader>
<bodyText confidence="0.999993428571429">
Graf and Azzopardi (2008) describe a method to ex-
tract relevance judgements for patent retrieval from
patent citations. The key idea is to regard patent doc-
uments that are cited in a query patent, either by the
patent applicant, or by the patent examiner or in a
patent office’s search report, as relevant for the query
patent. Furthermore, patent documents that are re-
lated to the query patent via a patent family relation-
ship, i.e., patents granted by different patent author-
ities but related to the same invention, are regarded
as relevant. We assign three integer relevance levels
to these three categories of relationships, with high-
est relevance (3) for family patents, lower relevance
for patents cited in search reports by patent examin-
ers (2), and lowest relevance level (1) for applicants’
citations. We also include all patents which are in
the same patent family as an applicant or examiner
citation to avoid false negatives. This methodol-
ogy has been used to create patent retrieval data at
CLEF-IP11 and proved very useful to automatically
create a patent retrieval dataset for our experiments.
For the creation of our dataset, we used the
MAREC12 citation graph to extract patents in cita-
tion or family relation. Since the Japanese portion
of the MAREC corpus only contains English ab-
stracts, but not the Japanese full texts, we merged
the patent documents in the NTCIR-10 test collec-
tion described above with the Japanese (JP) section
</bodyText>
<footnote confidence="0.946531">
11http://www.ifs.tuwien.ac.at/˜clef-ip/
12http://www.ifs.tuwien.ac.at/imp/marec.
shtml
</footnote>
<bodyText confidence="0.999823555555556">
of MAREC. Title, abstract, description and claims
were added to the MAREC-JP data if the docu-
ment was available in NTCIR. In order to keep par-
allel data for SMT training separate from ranking
data, we used only data from the years 2003-2005
to extract training data for ranking, and two small
datasets of 2,000 queries each from the years 2006-
2007 for development and testing. Table 1 gives an
overview over the data used for ranking. For de-
velopment and test data, we randomly added irrele-
vant documents from the NTCIR-10 collection until
we obtained two pools of 100,000 documents. The
necessary information to reproduce the exact train,
development and test data samples is downloadable
from authors’ webpage13.
The experiments reported here use only the ab-
stract of the Japanese and English patents in our
training, development and test collection.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999254">
6.1 System Development
</subsectionHeader>
<bodyText confidence="0.99997980952381">
System development and evaluation in our exper-
iments was done on the ranking data described
in the previous section (see Table 1). We report
Mean Average Precision (MAP) scores, using the
trec eval (ver. 8.1) script from the TREC evalu-
ation campaign14, with a limit of top K = 1, 000 re-
trieved documents for each query. Furthermore, we
use the Patent Retrieval Evaluation Score (PRES)15
introduced by Magdy and Jones (2010). This met-
ric accounts for both precision and recall. In the
study by Magdy and Jones (2010), PRES agreed
with MAP in almost 80% of cases, and both agreed
on the ranks of the best and the worst IR system.
Both MAP and PRES scores are reported in the same
range [0, 1], and 0.01 stands for 1 MAP (PRES)
point. Statistical significance of pairwise system
comparisons was assessed using the paired random-
ization test (Noreen, 1989; Smucker et al., 2007).
For each system, optimal meta-parameter settings
were found by choosing the configuration with high-
est MAP score on the development set. These results
</bodyText>
<footnote confidence="0.9644026">
13http://www.cl.uni-heidelberg.de/
statnlpgroup/boostclir
14http://trec.nist.gov/trec_eval
15http://www.computing.dcu.ie/˜wmagdy/
Scripts/PRESeval.htm
</footnote>
<page confidence="0.980226">
1694
</page>
<table confidence="0.978080375">
MAP PRES
method
dev test dev test
1 DT 0.2636 0.2555 0.5669 0.5681
2 PSQ lexical table 0.2520 0.2444 0.5445 0.5498
3 PSQ n-best table 0.2698 0.2659 0.5789 0.5851
Boost-1g 0.2064 1230.1982 0.5850 120.6122
Boost-2g 0.2526 30.2474 0.6900 1230.7196
</table>
<tableCaption confidence="0.562497428571428">
Table 2: MAP and PRES scores for CLIR methods (best
configurations) on the development and test sets. Prefixed
numbers denote statistical significance of a pairwise com-
parison with the baseline indicated by the superscript. For
example, the bottom right result shows that Boost-2g is
significantly better than DT (method 1), PSQ lexical ta-
ble (method 2) and PSQ n-best table (method 3).
</tableCaption>
<bodyText confidence="0.999835411764706">
(together with PRES results) are shown in the sec-
ond and fourth column of Table 2.
The direct translation approach (DT) was devel-
oped in three configurations: no stopword filtering,
small stopword list (52 words) and a large stopword
list (543 words). The last configuration achieved the
highest score (MAP 0.2636).
The probabilistic structured query (PSQ) ap-
proach was developed using the lexical translation
table and the translation table estimated on the de-
coder’s n-best list, both optionally pruned with a
variable lower pL and cumulative pC threshold on
the word pair probability in the table (Section 3.2).
A further meta-parameter of PSQ was whether to use
standard or unique n-best lists. Finally, all variants
were coupled with the same stopword filters as in
the DT approach. The configurations that achieved
the highest scores were: MAP 0.2520 for PSQ with
a lexical table (pL = 0.01, pC = 0.95, no stop-
word filtering), and MAP 0.2698 for PSQ with a
translation table estimated on the n-best list (pL =
0.005, pC = 0.95, large stopword list). Interpolat-
ing between lexical and n-best tables did not im-
prove results in our experiments, thus we set A = 1
in equation (4).
Each SMT-based system was run with 4 different
MERT optimizations, leading to variations of less
than 1 MAP point for each system. The best con-
figurations for DT and PSQ on the development set
were fixed and used for evaluation on the test set.
Training of the boosting approach (Boost) was
done in parallel on bootstrap samples from the train-
ing data. First, a query q (i.e., a Japanese abstract)
was sampled uniformly from all training queries.
</bodyText>
<table confidence="0.999115888888889">
method MAP PRES
dev test dev test
DT + PSQ n-best 0.2778 *0.2726 0.5884 *0.5942
DT + Boost-1g 0.2778 *0.2728 0.6157 *0.6225
DT + Boost-2g 0.3309 *0.3300 0.7132 *0.7279
PSQ lexical + Boost-1g 0.2695 *0.2653 0.6068 *0.6131
PSQ lexical + Boost-2g 0.3215 *0.3187 0.7071 *0.7240
PSQ n-best + Boost-1g 0.2863 *0.2850 0.6309 *0.6402
PSQ n-best + Boost-2g 0.3439 *0.3416 0.7212 *0.7376
</table>
<tableCaption confidence="0.9526935">
Table 3: MAP and PRES scores of the aggregated mod-
els on the development and test sets. Development scores
correspond to peaks in Figures 1 and 3, respectively, for
MAP and PRES; test scores are given for the re’s deliv-
ering these peaks on the development set. Prefixed * in-
dicates statistical significance of the result difference be-
tween aggregated system and the respective translation-
based system used in the aggregation.
</tableCaption>
<bodyText confidence="0.999975033333333">
Then we sampled independently and uniformly a
relevant document d+ (i.e., an English abstract)
from the English patents marked relevant for the
Japanese patent, and a random document d− from
the whole pool of English patent abstracts. If d−
had a relevance score greater or equal to the rele-
vance score of d+, it was resampled. The initial im-
portance weight D0 for a triplet (q, d+, d−) was set
to the positive difference in relevance scores for d+
and d−. Each bootstrap sample consisted of 10 pairs
of documents for each of 10, 000 queries, resulting
in 100, 000 training instances per sample.
The Boost approach was developed for uni-gram
and combined uni- and bi-gram versions. We ob-
served that the performance of the Boost method
continuously improved with the number of iterations
T and with the number of samples S, but saturated
at about 15-20 samples without visible over-fitting
in the tested range of T. Therefore we arbitrarily
stopped training after obtaining 5, 000 features per
sample, and used 35 samples for uni-gram version
and 65 samples for the combined bi-gram version,
resulting in models with 104K and 172K unique fea-
tures, respectively. The optimal values for the pass-
through weight Q were found to be 0.3 and 0.2 for
the uni-gram and bi-gram models on the develop-
ment set. The best configuration of uni-gram and
bi-gram model achieved MAP scores of 0.2064 and
0.2526 the development set. Using stopword filters
during training did not improve the results here.
</bodyText>
<page confidence="0.974984">
1695
</page>
<figure confidence="0.982296">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
κ
</figure>
<figureCaption confidence="0.99771">
Figure 1: MAP rank aggregation for combinations of the
bi-gram boosting and the baselines on the dev set.
</figureCaption>
<figure confidence="0.9754345">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
κ
</figure>
<figureCaption confidence="0.99737">
Figure 3: PRES rank aggregation for combinations of the
bi-gram boosting and the baselines on the dev set.
</figureCaption>
<subsectionHeader confidence="0.999684">
6.2 Testing and Model Combination
</subsectionHeader>
<bodyText confidence="0.999515470588235">
The third and the fifth columns of Table 2 give a
comparison of the MAP scores of the baseline ap-
proaches and the Boost model evaluated individu-
ally on the test set. Each score corresponds to the
best configuration found on the development set. We
see that the PSQ approach using n-best lists for pro-
jection outperforms all other methods in terms of
MAP, but loses to both Boost approaches when eval-
uated with PRES. Direct translation is about 1 MAP
point lower than PSQ n-best; Boost with combined
uni- and bi-grams is another 0.8 MAP points worse,
but is better in terms of PRES, especially for the bi-
gram version. Given the fact that the complex SMT
system behind the direct translation and PSQ ap-
proach is trained and tuned on very large in-domain
datasets, the performance of the bare phrase table
induced by the Boost method is respectable.
</bodyText>
<figureCaption confidence="0.9964575">
Figure 2: MAP rank aggregation for the bi-gram boosting
and the “PSQ n-best table” approach on dev and test sets.
</figureCaption>
<bodyText confidence="0.999919633333333">
Our best results are obtained by a combination
of the orthogonal information sources of the SMT
and the Boost approaches. We evaluated the Borda
Count aggregation scheme on the development data
in order to find the optimal value for r. E [0, 1]. The
interpolation was done for the best combined uni-
and bi-gram boosting model with the best variants of
the DT and PSQ approaches. As can be seen from
Figures 1 and 3, rank aggregation by Borda Count
outperforms both individual approaches by a large
margin. Figure 2 verifies that the results are trans-
ferable from the development set to the test set. The
best performing system combination on the develop-
ment data is also optimal on the test data.
Table 3 shows the retrieval performance of the
best baseline model (PSQ n-best) combined with
the best Boost model (bi-gram), with an impres-
sive gain of over 7 MAP points (15 PRES points)
over the best individual baseline result from Table 2.
Even when, according to the PRES measure (Fig-
ure 3), the Boost-2g system is better on its own, in-
jecting complementary information from the PSQ or
DT approach still contributes several points. Simi-
lar gains are obtained by model combination of the
DT approach with the best Boost model. However,
a combination of the SMT-based CLIR approaches
DT and PSQ barely improved results over the best
input model. In summary, aggregating rankings is
helpful for orthogonal systems, but not for systems
including similar information.
</bodyText>
<table confidence="0.697290375">
DT + Boost-2g
PSQ lexical + Boost-2g
PSQ n-best + Boost-2g
DT, 0.2636
PSQ lexical, 0.2520
PSQ n-best, 0.2698
Boost-2g, 0.2526
PSQ n-best + DT
</table>
<figure confidence="0.986626490566038">
0.35
0.31
0.30
0.29
0.28
0.27
0.26
0.25
0.24
0.34
0.33
0.32
PRES
0.72
0.70
0.68
0.66
0.64
0.62
0.60
0.58
0.56
0.54
DT + Boost-2g
PSQ lexical + Boost-2g
PSQ n-best + Boost-2g
DT, 0.5669
PSQ lexical, 0.5445
PSQ n-best, 0.5789
Boost-2g, 0.6900
PSQ n-best + DT
0.35
0.34
0.33
dev, PSQ n-best + Boost-2g
test, PSQ n-best + Boost-2g
dev, PSQ n-best, 0.2698
test, PSQ n-best, 0.2659
dev, Boost-2g, 0.2526
test, Boost-2g, 0.2474
0.32
0.31
0.30
0.29
0.28
0.27
0.26
0.25
0.24
MAP
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
κ
MAP
</figure>
<page confidence="0.96814">
1696
</page>
<subsectionHeader confidence="0.99778">
6.3 Analysis
</subsectionHeader>
<bodyText confidence="0.999887384615385">
Table 4 lists some of the top-200 selected features
for the boosting approach (the most common trans-
lation of the Japanese term is put in subscript).
We see that the direct ranking approach is able
to penalize uni- and bi-gram cooccurrences that are
harmful for retrieval by assigning them a negative
weight, e.g., the pairing of f7Cresolution with image.
Pairs of uni- and bi-grams that are useful for re-
trieval are boosted by positive weights, e.g., the pair
Eñcompression,Ömachine and compressor captures an
important compound. Further examples, not shown
in the table, are matches of the same source (tar-
get) n-gram with several different target (source) n-
grams, e.g., the Japanese term UWllimage is paired
not only with its main translation, but also with
dozens of related notions: video, picture, scanning,
printing, photosensitive, pixel, background etc. This
has a query expansion effect that is not possible in
systems that use one translation or a small list of n-
best translations. In addition, associations of source
n-grams with overlapping target n-grams help boost
the final score: e.g., the same term UAllimage is pos-
itively paired with target bi-grams as {an,original},
{original,image} and {image,for}. This has the ef-
fect of compensating for the lack of handling phrase
overlaps in an SMT decoder.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999938555555556">
We presented a boosting approach to induce a table
of bilingual n-gram correspondences by direct pref-
erence learning on relevance rankings. This table
can be seen as a phrase table that encodes word-
based information that is orthogonal and comple-
mentary to the information in standard translation-
based CLIR approaches. We compared our boosting
approach to very competitive CLIR baselines that
use a complex SMT system trained and tuned on
large in-domain datasets. Furthermore, our patent
retrieval setup gives SMT-based approaches an ad-
vantage in that queries consist of several normal-
length sentences, as opposed to the short queries
common to web search. Despite this and despite the
tiny size (about 170K parameters) of the boosting
phrase table, compared to standard SMT phrase ta-
bles, this approach reached performance similar to
direct translation using a full SMT model in terms
</bodyText>
<table confidence="0.999083833333333">
t ht (uni- &amp; bi-grams) wt
1 player - layer 1.29
2 &apos;�– Ydata - data 1.13
t-
3 ®D jrcuit - circuit 1.13
76 Zin - voltage -0.39
77 4guide,�power - conductive 1.25
81 Pf Aresolution - image -0.25
99 ICAspeed - transmission 1.68
100 APIILCD - liquid,crystal 1.73
123 hpo{�wer - force 0.91
124 l�-. NcompressionArnachine - compressor 2.83
132 �--7�1&apos;4Lcable - cable 1.81
133 0—hyper,�9sound wave - ultrasonic 3.34
169 R-Tuparticle - particles 1.57
170 $`_&apos;,calculation - for,each 1.14
184 &apos; 3—Yrotor - rotor 2.01
185 Wõdetection,Ævessel - detector 1.43
</table>
<tableCaption confidence="0.999753">
Table 4: Examples of the features found by boosting.
</tableCaption>
<bodyText confidence="0.999984777777778">
of MAP, and was significantly better in terms of
PRES. Overall, we obtained the best results by a
model combination using consensus- based voting
where the best SMT-based approach was combined
with the boosting phrase table (gaining more than 7
MAP or 15 PRES points). We attribute this to the
fact that the boosting approach augments SMT ap-
proaches with valuable information that is hard to
get in approaches that are agnostic about the rank-
ing data and the ranking task at hand.
The experimental setup presented in this paper
uses relevance links between patent abstracts as
ranking data. While this technique is useful to de-
velop patent retrieval systems, it would be interest-
ing to see if our results transfer to patent retrieval
scenarios where full patent documents are used in-
stead of only abstracts, or to standard CLIR scenar-
ios that use short search queries in retrieval.
</bodyText>
<sectionHeader confidence="0.997751" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999653">
The research presented in this paper was supported
in part by DFG grant “Cross-language Learning-to-
Rank for Patent Retrieval”. We would like to thank
Eugen Ruppert for his contribution to the ranking
data construction.
</bodyText>
<page confidence="0.992183">
1697
</page>
<sectionHeader confidence="0.968291" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99625520952381">
Javed A. Aslam and Mark Montague. 2001. Models for
metasearch. In Proceedings of the ACM SIGIR Con-
ference on Research and Development in Information
Retrieval (SIGIR’01), New Orleans, LA.
Bing Bai, Jason Weston, David Grangier, Ronan
Collobert, Kunihiko Sadamasa, Yanjun Qi, Olivier
Chapelle, and Kilian Weinberger. 2010. Learning to
rank with (a lot of) word features. Information Re-
trieval Journal, 13(3):291–314.
Lisa Ballesteros and W. Bruce Croft. 1998. Resolving
ambiguity for cross-language retrieval. In Proceedings
of the ACM SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR’98), Mel-
bourne, Australia.
Nicholas J. Belkin, Paul Kantor, Edward A. Fox, and
Joseph A. Shaw. 1995. Combining the evidence
of multiple query representations for information re-
trieval. Inf. Process. Manage., 31(3):431–448.
Leo Breiman. 1996. Bagging predictors. Journal of Ma-
chine Learning Research, 24:123–140.
Kevin Canini, Tushar Chandra, Eugene Ie, Jim McFad-
den, Ken Goldman, Mike Gunter, Jeremiah Harm-
sen, Kristen LeFevre, Dmitry Lepikhin, Tomas Lloret
Llinares, Indraneel Mukherjee, Fernando Pereira, Josh
Redstone, Tal Shaked, and Yoram Singer. 2010.
Sibyl: A system for large scale machine learning.
In LADIS: The 4th ACM SIGOPS/SIGACT Workshop
on Large Scale Distributed Systems and Middleware,
Zurich, Switzerland.
Xi Chen, Bing Bai, Yanjun Qi, Qihang Ling, and Jaime
Carbonell. 2010. Learning preferences with millions
of parameters by enforcing sparsity. In Proceedings
of the IEEE International Conference on Data Mining
(ICDM’10), Sydney, Australia.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Jeffrey Chin, Maureen Heymans, Alexandre Kojoukhov,
Jocelyn Lin, and Hui Tan. 2008. Cross-language
information retrieval. Patent Application. US
2008/0288474 A1.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25–69.
Michael Collins, Robert E. Schapire, and Yoram Singer.
2002. Logistic regression, AdaBoost and Bregman
distances. Journal of Machine Learning Research,
48(1-3):253–285.
Kareem Darwish and Douglas W. Oard. 2003. Proba-
bilistic structured query methods. In Proceedings. of
the ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval (SIGIR’03), Toronto,
Canada.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, Upp-
sala, Sweden.
Minwei Feng, Christoph Schmidt, Joern Wuebker,
Stephan Peitz, Markus Freitag, and Hermann Ney.
2011. The RWTH Aachen system for NTCIR-9
PatentMT. In Proceedings of the NTCIR-9 Workshop,
Tokyo, Japan.
Yoav Freund, Ray Iyer, Robert E. Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm for
combining preferences. Journal of Machine Learning
Research, 4:933–969.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent trans-
lation task at the NTCIR-7 workshop. In Proceedings
of NTCIR-7 Workshop Meeting, Tokyo, Japan.
Yasser Ganjisaffar, Rich Caruana, and Cristina Videira
Lopes. 2011. Bagging gradient-boosted trees for
high precision, low variance ranking models. In Pro-
ceedings of the ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR’11),
Beijing, China.
Jianfeng Gao, Jian-Yun Nie, Endong Xun, Jian Zhang,
Ming Zhou, and Changning Huang. 2001. Improv-
ing query translation for cross-language information
retrieval using statistical models. In Proceedings of
the ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval (SIGIR’01), New Or-
leans, LA.
Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Jian Hu,
Kam-Fai Wong, and Hsiao-Wuen Hon. 2007. Cross-
lingual query suggestion using query logs of different
languages. In Proceedings of the ACM SIGIR Con-
ference on Research and Development in Information
Retrieval (SIGIR’07), Amsterdam, The Netherlands.
Sharad Goel, John Langford, and Alexander L. Strehl.
2008. Predictive indexing for fast search. In Advances
in Neural Information Processing Systems, Vancouver,
Canada.
Erik Graf and Leif Azzopardi. 2008. A methodology
for building a patent test collection for prior art search.
In Proceedings of the 2nd International Workshop on
Evaluating Information Access (EVIA), Tokyo, Japan.
David Grangier and Samy Bengio. 2008. A discrimi-
native kernel-based approach to rank images from text
queries. IEEE Transactions on Pattern Analysis and
Machine Intelligence (PAMI), 30(8):1371–1384.
Yunsong Guo and Carla Gomes. 2009. Ranking struc-
tured documents: A large margin based approach for
</reference>
<page confidence="0.820439">
1698
</page>
<reference confidence="0.999848596153846">
patent prior art search. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI’09), Pasadena, CA.
Kenneth Heafield. 2011. KenLM: faster and smaller lan-
guage model queries. In Proceedings of the EMNLP
2011 Sixth Workshop on Statistical Machine Transla-
tion (WMT’11), Edinburgh, UK.
Gareth J.F. Jones and Adenike M. Lam-Adesina. 2002.
Combination methods for improving the reliability of
machine translation based cross-language information
retrieval. In Proceedings of the 13th Irish Interna-
tional Conference on Artificial Intelligence and Cog-
nitive Science (AICS’02), Limerick, Ireland.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of the
Conference on European Chapter of the Association
for Computational Linguistics (EACL’03), Budapest,
Hungary.
Victor Lavrenko, Martin Choquette, and W. Bruce Croft.
2002. Cross-lingual relevance models. In Proceed-
ings of the ACM Conference on Research and Devel-
opment in Information Retrieval (SIGIR’02), Tampere,
Finland.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of the
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2007), Prague,
Czech Republic.
Walid Magdy and Gareth J.F. Jones. 2010. PRES: a
score metric for evaluating recall-oriented information
retrieval applications. In Proceedings of the ACM SI-
GIR conference on Research and development in in-
formation retrieval (SIGIR’10), New York, NY.
Walid Magdy and Gareth J. F. Jones. 2011. An efficient
method for using machine translation technologies in
cross-language patent search. In Proceedings of the
20th ACM Conference on Informationand Knowledge
Management (CIKM’11), Glasgow, Scotland, UK.
Vassilina Nikoulina, Bogomil Kovachev, Nikolaos Lagos,
and Christof Monz. 2012. Adaptation of statistical
machine translation model for cross-lingual informa-
tion retrieval in a service context. In Proceedings of
the 13th Conference of the European Chapter of the
Association for Computational Linguistics (EACL’12),
Avignon, France.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Meeting of the Association for Computational Linguis-
tics (ACL’00), Hongkong, China.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Meeting on Association for Computational Lin-
guistics (ACL’03), Sapporo, Japan.
Dmitry Pavlov, Alexey Gorodilov, and Cliff A. Brunk.
2010. Bagboo: a scalable hybrid bagging-the-
boosting model. In Proceedings of the 19th ACM
International Conference on Information and Knowl-
edge Management (CIKM’10), Toronto, Canada.
Stephen E. Robertson, Steve Walker, and Micheline
Hancock-Beaulieu. 1998. Okapi at TREC-7. In
Proceedings of the Seventh Text REtrieval Conference
(TREC-7), Gaithersburg, MD.
Saharon Rosset, Ji Zhu, and Trevor Hastie. 2004. Boost-
ing as a regularized path to a maximum margin clas-
sifier. Journal of Machine Learning Research, 5:941–
973.
Robert E. Schapire and Yoram Singer. 1999. Im-
proved boosting algorithms using confidence-rated
predictions. Journal of Machine Learning Research,
37(3):297–336.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alexander J. Smola, Alexander L. Strehl, and
Vishy Vishwanathan. 2009. Hash Kernels. In Pro-
ceedings of the 12th Int. Conference on Artificial In-
telligence and Statistics (AISTATS’09), Irvine, CA.
Mark D. Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests for
information retrieval evaluation. In Proceedings of the
16th ACM conference on Conference on Information
and Knowledge Management (CIKM ’07), New York,
NY.
Ferhan Ture, Jimmy Lin, and Douglas W. Oard. 2012a.
Combining statistical translation techniques for cross-
language information retrieval. In Proceedings of the
International Conference on Computational Linguis-
tics (COLING 2012), Bombay, India.
Ferhan Ture, Jimmy Lin, and Douglas W. Oard. 2012b.
Looking inside the box: Context-sensitive translation
for cross-language information retrieval. In Proceed-
ings of the ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR 2012),
Portland, OR.
Masao Utiyama and Hitoshi Isahara. 2007. A Japanese-
English patent parallel corpus. In Proceedings of MT
Summit XI, Copenhagen, Denmark.
Jinxi Xu, Ralph Weischedel, and Chanh Nguyen. 2001.
Evaluating a probabilistic model for cross-lingual in-
formation retrieval. In Proceedings of the ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval (SIGIR’01), New York, NY.
</reference>
<page confidence="0.997409">
1699
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.923983">
<title confidence="0.990438">Boosting Cross-Language Retrieval by Bilingual Phrase Associations from Relevance Rankings</title>
<author confidence="0.999725">Sokolov Jehl Hieber</author>
<affiliation confidence="0.973793">Department of Computational Heidelberg University, 69120 Heidelberg,</affiliation>
<abstract confidence="0.999610411764706">We present an approach to learning bilincorrespondences from relevance rankings of English documents for Japanese queries. We show that directly optimizing cross-lingual rankings rivals and complements machine translation-based cross-language information retrieval (CLIR). We propose an efficient boosting algorithm that deals with very large cross-product spaces of word correspondences. We show in an experimental evaluation on patent prior art search that our approach, and in particular a consensus-based combination of boosting and translation-based approaches, yields substantial improvements in CLIR performance. Our training and test data are made publicly available.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Javed A Aslam</author>
<author>Mark Montague</author>
</authors>
<title>Models for metasearch.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’01),</booktitle>
<location>New Orleans, LA.</location>
<contexts>
<context position="6996" citStr="Aslam and Montague, 2001" startWordPosition="1097" endWordPosition="1100">w in an experimental evaluation on largescale retrieval on patent abstracts that our boosting approach is comparable in MAP and improves significantly by 13-15 PRES points over very competitive translation-based CLIR systems that are trained on 1.8 million parallel sentence pairs from JapaneseEnglish patent documents. Moreover, a combination of the orthogonal information learned in rankingbased and translation-based approaches improves over 7 MAP points and over 15 PRES points over the respective translation-based system in a consensusbased voting approach following the Borda Count technique (Aslam and Montague, 2001). 2 Related Work Recent research in CLIR follows the two main paradigms of direct translation and probabilistic structured query approaches. An example for the first approach is the work of Magdy and Jones (2011) who presented an efficient technique to adapt off-the-shelf SMT systems for CLIR by training them on data pre-processed for retrieval (case folding, stopword removal, stemming). Nikoulina et al. (2012) presented an approach to direct translationbased CLIR where the n-best list of an SMT system is re-ranked according to the MAP performance of the translated queries. The probabilistic s</context>
<context position="21394" citStr="Aslam and Montague, 2001" startWordPosition="3452" endWordPosition="3455"> matching general passages written in natural language. Both baseline SMT-based approaches presented above are agnostic of the ultimate retrieval task and are not specifically adapted for it. The boosting method, on the other hand, learns domainspecific word associations that are useful to discern relevant from irrelevant documents. In order to combine these orthogonal sources of information in a way that democratically respects each approach we use Borda Counts, i.e., a consensus-based voting procedure that has been successfully employed to aggregate ranked lists of documents for metasearch (Aslam and Montague, 2001). We implemented a weighted version of the Borda Count method where each voter has a fixed amount of voting points which she is free to distribute among the candidates to indicate the amount of preference she is giving to each of them. In the case of retrieval, for each q, the candidates are the scored documents in the retrieved subset of the whole document set. The aggregate score fagg for two rankings f1(q, d) 8It is possible to construct separate query and document inverted indices and intersect them on the fly to determine the set of documents that contains some pair of words. In practice,</context>
</contexts>
<marker>Aslam, Montague, 2001</marker>
<rawString>Javed A. Aslam and Mark Montague. 2001. Models for metasearch. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’01), New Orleans, LA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Bai</author>
<author>Jason Weston</author>
<author>David Grangier</author>
<author>Ronan Collobert</author>
<author>Kunihiko Sadamasa</author>
<author>Yanjun Qi</author>
<author>Olivier Chapelle</author>
<author>Kilian Weinberger</author>
</authors>
<title>Learning to rank with (a lot of) word features.</title>
<date>2010</date>
<journal>Information Retrieval Journal,</journal>
<volume>13</volume>
<issue>3</issue>
<contexts>
<context position="3323" citStr="Bai et al. (2010)" startWordPosition="484" endWordPosition="487">., 2008). Ture et al. (2012b; 2012a) use standard retrieval algorithms such as BM25 (Robertson et al., 1998). That means, machine learning in SMT-based approaches concentrates on the cross-language aspect of CLIR and is agnostic of the ultimate ranking task. In this paper, we present a method to project search queries into the target language that is complementary to SMT-based CLIR approaches. Our method learns a table of n-gram correspondences by direct optimization of a ranking objective on relevance rankings of English documents for Japanese queries. Our model is similar to the approach of Bai et al. (2010) who characterize their technique as “Learning to rank with (a Lot of) Word Features”. Given a set of search queries q E IRQ and docu1688 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1688–1699, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics ments d E IRD, where the jth dimension of a vector indicates the occurrence of the jth word for dictionaries of size Q and D, we want to learn a score f(q, d) between a query and a given document using the model1 f(q, d) = q&gt;Wd = We take a pairwise ranking appro</context>
<context position="9451" citStr="Bai et al. (2010)" startWordPosition="1491" endWordPosition="1494">l scenario, we created a large patent CLIR dataset where queries and documents are Japanese and English patent abstracts, respectively. Ranking approaches to CLIR have been presented by Guo and Gomes (2009) who use pairwise ranking for patent retrieval. Their method is a classical learning-to-rank setup where retrieval scores such as tf-idf or BM25 are combined with domain knowledge on patent class, inventor, date, location, etc. into a dense feature vector of a few hundred features. Methods to learn word-based translation correspondences from supervised ranking signals have been presented by Bai et al. (2010) and Chen et al. (2010). These approaches tackle the problem of complexity and capacity of the cross product matrix of word correspondences from different directions. The first proposes to learn a low rank representation of the matrix; the second deploys sparse online learning under f, regularization to keep the matrix small. Both approaches are mainly evaluated in a monolingual setting. The cross-lingual evaluation presented in Bai et al. (2010) uses weak translationbased baselines and non-public data such that a direct comparison is not possible. 2http://research.nii.ac.jp/ntcir/ntcir/ 3http</context>
</contexts>
<marker>Bai, Weston, Grangier, Collobert, Sadamasa, Qi, Chapelle, Weinberger, 2010</marker>
<rawString>Bing Bai, Jason Weston, David Grangier, Ronan Collobert, Kunihiko Sadamasa, Yanjun Qi, Olivier Chapelle, and Kilian Weinberger. 2010. Learning to rank with (a lot of) word features. Information Retrieval Journal, 13(3):291–314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa Ballesteros</author>
<author>W Bruce Croft</author>
</authors>
<title>Resolving ambiguity for cross-language retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’98),</booktitle>
<location>Melbourne, Australia.</location>
<contexts>
<context position="7761" citStr="Ballesteros and Croft, 1998" startWordPosition="1215" endWordPosition="1218">s. An example for the first approach is the work of Magdy and Jones (2011) who presented an efficient technique to adapt off-the-shelf SMT systems for CLIR by training them on data pre-processed for retrieval (case folding, stopword removal, stemming). Nikoulina et al. (2012) presented an approach to direct translationbased CLIR where the n-best list of an SMT system is re-ranked according to the MAP performance of the translated queries. The probabilistic structured query approach has seen a lot of work on contextaware query expansion across languages, based on various similarity statistics (Ballesteros and Croft, 1998; Gao et al., 2001; Lavrenko et al., 2002; Gao � Q i=1 qiWijdj. D E j=1 1689 et al., 2007). At the time of writing this paper, the most recent extension to this paradigm is Ture et al. (2012a). In addition to projecting terms from n-best translations, they propose a projection extracted from the hierarchical phrase- based grammar models, and a scoring method based on multi-token terms. Since the latter techniques achieved only marginal improvements over the context-sensitive query translation from n-best lists, we did not pursue them in our work. CLIR in the context of patent prior art search </context>
</contexts>
<marker>Ballesteros, Croft, 1998</marker>
<rawString>Lisa Ballesteros and W. Bruce Croft. 1998. Resolving ambiguity for cross-language retrieval. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’98), Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas J Belkin</author>
<author>Paul Kantor</author>
<author>Edward A Fox</author>
<author>Joseph A Shaw</author>
</authors>
<title>Combining the evidence of multiple query representations for information retrieval.</title>
<date>1995</date>
<journal>Inf. Process. Manage.,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="22548" citStr="Belkin et al. (1995)" startWordPosition="3659" endWordPosition="3662">he set of documents that contains some pair of words. In practice, however, we found the overhead of set intersection during each feature access prohibitive. and f2(q, d) for all (q, d) in the test set is then: fagg(q,d) = n f1(q,d) +(1−� f2(q,d) Ed f1 (q, d) ) Ed f2(q, d) In practice, the normalizations sum over the top K retrieved documents. If a document is present only in the top-K list of one system, its score is considered zero for the other system. The aggregated scores fagg(q, d) are sorted in descending order and top K scores are kept for evaluation. Using the terminology proposed by Belkin et al. (1995), combining several systems’ scores with Borda Counts can be viewed as the “data fusion” approach to IR, that merges outputs of the systems, while the PSQ baseline is an example of the “query combination” approach that extends the query at the input. Both techniques were earlier found to have similar performance in CLIR tasks based on direct translation, with a preference for the data fusion approach (Jones and Lam-Adesina, 2002). 5 Translation and Ranking Data 5.1 Parallel Translation Data For Japanese-to-English patent translation we used data provided by the organizers of the NTCIR9 worksho</context>
</contexts>
<marker>Belkin, Kantor, Fox, Shaw, 1995</marker>
<rawString>Nicholas J. Belkin, Paul Kantor, Edward A. Fox, and Joseph A. Shaw. 1995. Combining the evidence of multiple query representations for information retrieval. Inf. Process. Manage., 31(3):431–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>24--123</pages>
<contexts>
<context position="6213" citStr="Breiman, 1996" startWordPosition="984" endWordPosition="985">. table by direct rank optimization feasible, and an experimental verification of the benefits of this approach, especially with regard to a combination of the orthogonal information sources of rankingbased and SMT-based CLIR approaches. Our approach builds upon a boosting framework for pairwise ranking (Freund et al., 2003) that allows the model to grow incrementally, thus avoiding having to deal with the full matrix W. Furthermore, we present an implementation of boosting that utilizes parallel estimation on bootstrap samples from the training set for increased efficiency and reduced error (Breiman, 1996). Our “bagged boosting” approach allows to combine incremental feature selection, parallel training, and efficient management of large data structures. We show in an experimental evaluation on largescale retrieval on patent abstracts that our boosting approach is comparable in MAP and improves significantly by 13-15 PRES points over very competitive translation-based CLIR systems that are trained on 1.8 million parallel sentence pairs from JapaneseEnglish patent documents. Moreover, a combination of the orthogonal information learned in rankingbased and translation-based approaches improves ov</context>
<context position="17398" citStr="Breiman, 1996" startWordPosition="2741" endWordPosition="2742">iterative fashion, which closely follows an efficient algorithm of Collins and Koo (2005) for the case of binary-valued h. In each step, the single feature h is selected that provides the largest decrease of Lexp, i.e., that has the largest projection on the direction of the gradient VhLexp. Because of the sequential nature of the algorithm, RankBoost implicitly performs automatic feature selection and regularization (Rosset et al., 2004), which is crucial to reduce complexity and capacity for our application. Parallelization and Bagging To achieve parallelization we use a variant of bagging (Breiman, 1996) on top of boosting, which has been observed to improve performance, reduce variance and is trivial to parallelize. The procedure is described as part of Algorithm 1: From the set of preference pairs R, draw S equal-sized samples with replacement and distribute to nodes. Then, using each of the samples as a training set, separate boosting models {wst , hst}, s = 1... S are trained that contain the same number of features t = 1... T. Finally the models are averaged: f (q, d) = s Et Es wst hst (q, d) Algorithm The entire training procedure is outlined in Algorithm 1. For each possible feature h </context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Leo Breiman. 1996. Bagging predictors. Journal of Machine Learning Research, 24:123–140.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Canini</author>
<author>Tushar Chandra</author>
<author>Eugene Ie</author>
<author>Jim McFadden</author>
<author>Ken Goldman</author>
<author>Mike Gunter</author>
<author>Jeremiah Harmsen</author>
<author>Kristen LeFevre</author>
</authors>
<title>Dmitry Lepikhin, Tomas Lloret Llinares, Indraneel Mukherjee, Fernando Pereira, Josh Redstone, Tal Shaked, and Yoram Singer.</title>
<date>2010</date>
<booktitle>In LADIS: The 4th ACM SIGOPS/SIGACT Workshop on Large Scale Distributed Systems and Middleware,</booktitle>
<location>Zurich, Switzerland.</location>
<contexts>
<context position="10669" citStr="Canini et al. (2010)" startWordPosition="1677" endWordPosition="1680">tp://www.ifs.tuwien.ac.at/˜clef-ip/ A combination of bagging and boosting in the context of retrieval has been presented by Pavlov et al. (2010) and Ganjisaffar et al. (2011). This work is done in a standard learning-to-rank setup using a few hundred dense features trained on hundreds of thousands of pairs. Our setup deals with billions of sparse features (from the cross-product of the unrestricted dictionaries) trained on millions of pairs (sampled from a much larger space). Parallel boosting where all feature weights are updated simultaneously has been presented by Collins et al. (2002) and Canini et al. (2010). The first method distributes the gradient calculation for different features among different compute nodes. This is not possible in our approach because we construct the cross-product matrix on-the-fly. The second approach requires substantial efforts in changing the data representation to use the MapReduce framework. Overall, one of the goals of our work is sequential updating for implicit feature selection, something that runs contrary to parallel boosting. 3 CLIR Approaches 3.1 Direct translation approach For direct translation, we use the SCFG decoder cdec (Dyer et al., 2010)4 and build </context>
</contexts>
<marker>Canini, Chandra, Ie, McFadden, Goldman, Gunter, Harmsen, LeFevre, 2010</marker>
<rawString>Kevin Canini, Tushar Chandra, Eugene Ie, Jim McFadden, Ken Goldman, Mike Gunter, Jeremiah Harmsen, Kristen LeFevre, Dmitry Lepikhin, Tomas Lloret Llinares, Indraneel Mukherjee, Fernando Pereira, Josh Redstone, Tal Shaked, and Yoram Singer. 2010. Sibyl: A system for large scale machine learning. In LADIS: The 4th ACM SIGOPS/SIGACT Workshop on Large Scale Distributed Systems and Middleware, Zurich, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xi Chen</author>
<author>Bing Bai</author>
<author>Yanjun Qi</author>
<author>Qihang Ling</author>
<author>Jaime Carbonell</author>
</authors>
<title>Learning preferences with millions of parameters by enforcing sparsity.</title>
<date>2010</date>
<booktitle>In Proceedings of the IEEE International Conference on Data Mining (ICDM’10),</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="9474" citStr="Chen et al. (2010)" startWordPosition="1496" endWordPosition="1499"> a large patent CLIR dataset where queries and documents are Japanese and English patent abstracts, respectively. Ranking approaches to CLIR have been presented by Guo and Gomes (2009) who use pairwise ranking for patent retrieval. Their method is a classical learning-to-rank setup where retrieval scores such as tf-idf or BM25 are combined with domain knowledge on patent class, inventor, date, location, etc. into a dense feature vector of a few hundred features. Methods to learn word-based translation correspondences from supervised ranking signals have been presented by Bai et al. (2010) and Chen et al. (2010). These approaches tackle the problem of complexity and capacity of the cross product matrix of word correspondences from different directions. The first proposes to learn a low rank representation of the matrix; the second deploys sparse online learning under f, regularization to keep the matrix small. Both approaches are mainly evaluated in a monolingual setting. The cross-lingual evaluation presented in Bai et al. (2010) uses weak translationbased baselines and non-public data such that a direct comparison is not possible. 2http://research.nii.ac.jp/ntcir/ntcir/ 3http://www.ifs.tuwien.ac.at</context>
</contexts>
<marker>Chen, Bai, Qi, Ling, Carbonell, 2010</marker>
<rawString>Xi Chen, Bing Bai, Yanjun Qi, Qihang Ling, and Jaime Carbonell. 2010. Learning preferences with millions of parameters by enforcing sparsity. In Proceedings of the IEEE International Conference on Data Mining (ICDM’10), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="11517" citStr="Chiang (2007)" startWordPosition="1813" endWordPosition="1814">ubstantial efforts in changing the data representation to use the MapReduce framework. Overall, one of the goals of our work is sequential updating for implicit feature selection, something that runs contrary to parallel boosting. 3 CLIR Approaches 3.1 Direct translation approach For direct translation, we use the SCFG decoder cdec (Dyer et al., 2010)4 and build grammars using its implementation of the suffix array extraction method described in Lopez (2007). Word alignments are built from all parallel data using mgiza5 and the Moses scripts6. SCFG models use the same settings as described in Chiang (2007). Training and querying of a modified Kneser-Ney smoothed 5- gram language model are done on the English side of the training data using KenLM (Heafield, 2011)7. Model parameters were optimized using cdec’s implementation of MERT (Och (2003)). At retrieval time, all queries are translated sentence-wise and subsequently re-joined to form one query per patent. Our baseline retrieval system uses the Okapi BM25 scores for document ranking. 4https://github.com/redpony/cdec 5http://www.kyloo.net/software/doku.php/ mgiza:overview 6http://www.statmt.org/moses/?n=Moses. SupportTools 7http://kheafield.c</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jeffrey Chin</author>
</authors>
<title>Maureen Heymans, Alexandre Kojoukhov,</title>
<marker>Chin, </marker>
<rawString>Jeffrey Chin, Maureen Heymans, Alexandre Kojoukhov,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jocelyn Lin</author>
<author>Hui Tan</author>
</authors>
<title>Cross-language information retrieval. Patent Application.</title>
<date>2008</date>
<journal>US</journal>
<volume>2008</volume>
<pages>1</pages>
<marker>Lin, Tan, 2008</marker>
<rawString>Jocelyn Lin, and Hui Tan. 2008. Cross-language information retrieval. Patent Application. US 2008/0288474 A1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="15275" citStr="Collins and Koo, 2005" startWordPosition="2380" endWordPosition="2383">ases cannot be translated. Probabilistic structured queries that include context-aware estimates of translation probabilities require a preservation of sentence-wise contextsensitivity also in retrieval. Thus, unlike the direct translation approach, we compute weighted term and document frequencies for each sentence s in query F separately. The scoring (3) of a target document for a multiple sentence query then becomes: Xscore(E|F) = s in F 3.3 Direct Phrase Table Learning from Relevance Rankings Pairwise Ranking using Boosting The general form of the RankBoost algorithm (Freund et al., 2003; Collins and Koo, 2005) defines a scoring function f(q, d) on query q and document d as a weighted linear combination of T weak learners ht such that f(q, d) = PTt=1 wtht(q, d). Weak learners can belong to an arbitrary family of functions, but in our case they are restricted to the simplest case of unparameterized indicator functions selecting components of the feature vector O(q, d) in (1) such that f is of the standard linear form (2). In our experiments, these features indicate the presence of pairs of uni- and bi-grams from the source-side vocabulary of query terms and the target-side vocabulary of document-term</context>
<context position="16873" citStr="Collins and Koo (2005)" startWordPosition="2656" endWordPosition="2659">df(f)) fEs 1691 of a set R of tuples (q, d+, d−), where d+ is a relevant (or higher ranked) document and d− an irrelevant (or lower ranked) document for query q. RankBoost’s objective is to correctly rank querydocument pairs such that f(q, d+) &gt; f(q, d−) for all data tuples from R. RankBoost achieves this by optimizing the following convex exponential loss: �Lexp = D(q, d+, d−)ef(q,d−)−f(q,d+), (q,d+,d−)∈R where D(q, d+, d−) is a non-negative importance function on pairs of documents for a given q. We optimize Lexp in a greedy iterative fashion, which closely follows an efficient algorithm of Collins and Koo (2005) for the case of binary-valued h. In each step, the single feature h is selected that provides the largest decrease of Lexp, i.e., that has the largest projection on the direction of the gradient VhLexp. Because of the sequential nature of the algorithm, RankBoost implicitly performs automatic feature selection and regularization (Rosset et al., 2004), which is crucial to reduce complexity and capacity for our application. Parallelization and Bagging To achieve parallelization we use a variant of bagging (Breiman, 1996) on top of boosting, which has been observed to improve performance, reduce</context>
<context position="18297" citStr="Collins and Koo, 2005" startWordPosition="2905" endWordPosition="2908"> using each of the samples as a training set, separate boosting models {wst , hst}, s = 1... S are trained that contain the same number of features t = 1... T. Finally the models are averaged: f (q, d) = s Et Es wst hst (q, d) Algorithm The entire training procedure is outlined in Algorithm 1. For each possible feature h we maintain auxiliary variables Wh+ and Wh− : D(q, d+, d−), which are the cumulative weights of correctly and incorrectly ranked instances by a candidate feature h. The absolute value of aLexp/ah can be expressed as I �W+ − �W−h I which is used as feature selection criterion (Collins and Koo, 2005). The optimum of minimizing Lexp over w (with fixed h) can be shown to be w = 1 ln Wh +EZ 2 + Wh− +EZ, where E is a smoothing parameter to avoid problems with small Wh± (Schapire and Singer, 1999), and Z = E(q,d+,d−)∈R D(q, d+, d−). Furthermore, for each step t of the learning process, values of D are updated to concentrate on pairs that have not been correctly ranked so far: Dt+1 = Dt · ewt(ht(q,d−)−ht(q,d+)) . (5) Finally, to speed up learning, on iteration t we recalculate Wh± only for those h that cooccur with previously selected ht and keep the rest unchanged (Collins and Koo, 2005). Algo</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Logistic regression, AdaBoost and Bregman distances.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>48--1</pages>
<contexts>
<context position="10644" citStr="Collins et al. (2002)" startWordPosition="1672" endWordPosition="1675">nii.ac.jp/ntcir/ntcir/ 3http://www.ifs.tuwien.ac.at/˜clef-ip/ A combination of bagging and boosting in the context of retrieval has been presented by Pavlov et al. (2010) and Ganjisaffar et al. (2011). This work is done in a standard learning-to-rank setup using a few hundred dense features trained on hundreds of thousands of pairs. Our setup deals with billions of sparse features (from the cross-product of the unrestricted dictionaries) trained on millions of pairs (sampled from a much larger space). Parallel boosting where all feature weights are updated simultaneously has been presented by Collins et al. (2002) and Canini et al. (2010). The first method distributes the gradient calculation for different features among different compute nodes. This is not possible in our approach because we construct the cross-product matrix on-the-fly. The second approach requires substantial efforts in changing the data representation to use the MapReduce framework. Overall, one of the goals of our work is sequential updating for implicit feature selection, something that runs contrary to parallel boosting. 3 CLIR Approaches 3.1 Direct translation approach For direct translation, we use the SCFG decoder cdec (Dyer </context>
</contexts>
<marker>Collins, Schapire, Singer, 2002</marker>
<rawString>Michael Collins, Robert E. Schapire, and Yoram Singer. 2002. Logistic regression, AdaBoost and Bregman distances. Journal of Machine Learning Research, 48(1-3):253–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kareem Darwish</author>
<author>Douglas W Oard</author>
</authors>
<title>Probabilistic structured query methods.</title>
<date>2003</date>
<booktitle>In Proceedings. of the ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’03),</booktitle>
<location>Toronto, Canada.</location>
<contexts>
<context position="1898" citStr="Darwish and Oard (2003)" startWordPosition="263" endWordPosition="266">The first approach leverages the standard Statistical Machine Translation (SMT) machinery to produce a single best translation that is used as search query in the target language. We will henceforth call this the direct translation approach. This technique is particularly useful if large amounts of data are available in domain-specific form. Alternative approaches avoid to solve the hard problem of word reordering, and instead rely on token-to-token translations that are used to project the query terms into the target language with a probabilistic weighting of the standard term tf-idf scheme. Darwish and Oard (2003) termed this method the probabilistic structured query approach. The advantage of this technique is an implicit query expansion effect due to the use of probability distributions over term translations (Xu et al., 2001). Recent research has shown that leveraging query context by extracting term translation probabilities from n-best direct translations of queries instead of using context-free translation tables outperforms both direct translation and context-free projection (Ture et al., 2012b; Ture et al., 2012a). While direct translation as well as probabilistic structured query approaches us</context>
<context position="12282" citStr="Darwish and Oard, 2003" startWordPosition="1904" endWordPosition="1907"> (Heafield, 2011)7. Model parameters were optimized using cdec’s implementation of MERT (Och (2003)). At retrieval time, all queries are translated sentence-wise and subsequently re-joined to form one query per patent. Our baseline retrieval system uses the Okapi BM25 scores for document ranking. 4https://github.com/redpony/cdec 5http://www.kyloo.net/software/doku.php/ mgiza:overview 6http://www.statmt.org/moses/?n=Moses. SupportTools 7http://kheafield.com/code/kenlm/ estimation/ 1690 3.2 Probabilistic structured query approach Early Probabilistic Structured Query approaches (Xu et al., 2001; Darwish and Oard, 2003) represent translation options by lexical, i.e., token-to-token translation tables that are estimated using standard word alignment techniques (Och and Ney, 2000). Later approaches (Ture et al., 2012b; Ture et al., 2012a) extract translation options from the decoder’s n-best list for translating a particular query. The central idea is to let the language model choose fluent, context-aware translations for each query term during decoding. This retains the desired queryexpansion effect of probabilistic structured models, but it reduces query drift by filtering translations with respect to the co</context>
</contexts>
<marker>Darwish, Oard, 2003</marker>
<rawString>Kareem Darwish and Douglas W. Oard. 2003. Probabilistic structured query methods. In Proceedings. of the ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’03), Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="11257" citStr="Dyer et al., 2010" startWordPosition="1768" endWordPosition="1771">2002) and Canini et al. (2010). The first method distributes the gradient calculation for different features among different compute nodes. This is not possible in our approach because we construct the cross-product matrix on-the-fly. The second approach requires substantial efforts in changing the data representation to use the MapReduce framework. Overall, one of the goals of our work is sequential updating for implicit feature selection, something that runs contrary to parallel boosting. 3 CLIR Approaches 3.1 Direct translation approach For direct translation, we use the SCFG decoder cdec (Dyer et al., 2010)4 and build grammars using its implementation of the suffix array extraction method described in Lopez (2007). Word alignments are built from all parallel data using mgiza5 and the Moses scripts6. SCFG models use the same settings as described in Chiang (2007). Training and querying of a modified Kneser-Ney smoothed 5- gram language model are done on the English side of the training data using KenLM (Heafield, 2011)7. Model parameters were optimized using cdec’s implementation of MERT (Och (2003)). At retrieval time, all queries are translated sentence-wise and subsequently re-joined to form o</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models. In Proceedings of the ACL 2010 System Demonstrations, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwei Feng</author>
<author>Christoph Schmidt</author>
<author>Joern Wuebker</author>
<author>Stephan Peitz</author>
<author>Markus Freitag</author>
<author>Hermann Ney</author>
</authors>
<title>The RWTH Aachen system for NTCIR-9 PatentMT.</title>
<date>2011</date>
<booktitle>In Proceedings of the NTCIR-9 Workshop,</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="23769" citStr="Feng et al. (2011)" startWordPosition="3855" endWordPosition="3858">or the JP-EN PatentMT subtask. In particular, we used the data provided for NTCIR-7 (Fujii et al., 2008), consisting of 1.8 million parallel sentence pairs from the years 1993-2002 for training. For parameter tuning we used the development set of the NTCIR-8 test collection, consisting of 2,000 sentence pairs. The data were extracted from the description section of patents published by the Japanese Patent Office (JPO) and the United States Patent and Trademark Office (USPTO) by the method described in Utiyama and Isahara (2007). Japanese text was segmented using the MeCab10 toolkit. Following Feng et al. (2011), we applied a modified version of the compound splitter described in Koehn and Knight (2003) to katakana terms, which are often transliterations of English compound words. As these are usually not split by MeCab, they can cause a large number of out-ofvocabulary terms. 9http://research.nii.ac.jp/ntcir/ntcir/ 10https://code.google.com/p/mecab/ 1693 #queries #relevant #unique docs train 107,061 1,422,253 888,127 dev 2,000 26,478 25,669 test 2,000 25,173 24,668 Table 1: Statistics of ranking data. For the English side of the training data, we applied a modified version of the tokenizer included </context>
</contexts>
<marker>Feng, Schmidt, Wuebker, Peitz, Freitag, Ney, 2011</marker>
<rawString>Minwei Feng, Christoph Schmidt, Joern Wuebker, Stephan Peitz, Markus Freitag, and Hermann Ney. 2011. The RWTH Aachen system for NTCIR-9 PatentMT. In Proceedings of the NTCIR-9 Workshop, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Ray Iyer</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>An efficient boosting algorithm for combining preferences.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>4--933</pages>
<contexts>
<context position="5925" citStr="Freund et al., 2003" startWordPosition="938" endWordPosition="941">tation and remedies against overfitting are essential. The main contribution of our paper is the presentation of algorithms that make learning a phrase 1With bold letters we denote vectors for query q and document d. Vector components are denoted with normal font letters and indices (e.g., qi). table by direct rank optimization feasible, and an experimental verification of the benefits of this approach, especially with regard to a combination of the orthogonal information sources of rankingbased and SMT-based CLIR approaches. Our approach builds upon a boosting framework for pairwise ranking (Freund et al., 2003) that allows the model to grow incrementally, thus avoiding having to deal with the full matrix W. Furthermore, we present an implementation of boosting that utilizes parallel estimation on bootstrap samples from the training set for increased efficiency and reduced error (Breiman, 1996). Our “bagged boosting” approach allows to combine incremental feature selection, parallel training, and efficient management of large data structures. We show in an experimental evaluation on largescale retrieval on patent abstracts that our boosting approach is comparable in MAP and improves significantly by </context>
<context position="15251" citStr="Freund et al., 2003" startWordPosition="2376" endWordPosition="2379">rules if words or phrases cannot be translated. Probabilistic structured queries that include context-aware estimates of translation probabilities require a preservation of sentence-wise contextsensitivity also in retrieval. Thus, unlike the direct translation approach, we compute weighted term and document frequencies for each sentence s in query F separately. The scoring (3) of a target document for a multiple sentence query then becomes: Xscore(E|F) = s in F 3.3 Direct Phrase Table Learning from Relevance Rankings Pairwise Ranking using Boosting The general form of the RankBoost algorithm (Freund et al., 2003; Collins and Koo, 2005) defines a scoring function f(q, d) on query q and document d as a weighted linear combination of T weak learners ht such that f(q, d) = PTt=1 wtht(q, d). Weak learners can belong to an arbitrary family of functions, but in our case they are restricted to the simplest case of unparameterized indicator functions selecting components of the feature vector O(q, d) in (1) such that f is of the standard linear form (2). In our experiments, these features indicate the presence of pairs of uni- and bi-grams from the source-side vocabulary of query terms and the target-side voc</context>
</contexts>
<marker>Freund, Iyer, Schapire, Singer, 2003</marker>
<rawString>Yoav Freund, Ray Iyer, Robert E. Schapire, and Yoram Singer. 2003. An efficient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4:933–969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Masao Utiyama</author>
<author>Mikio Yamamoto</author>
<author>Takehito Utsuro</author>
</authors>
<title>Overview of the patent translation task at the NTCIR-7 workshop.</title>
<date>2008</date>
<booktitle>In Proceedings of NTCIR-7 Workshop Meeting,</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="23255" citStr="Fujii et al., 2008" startWordPosition="3773" endWordPosition="3777"> approach to IR, that merges outputs of the systems, while the PSQ baseline is an example of the “query combination” approach that extends the query at the input. Both techniques were earlier found to have similar performance in CLIR tasks based on direct translation, with a preference for the data fusion approach (Jones and Lam-Adesina, 2002). 5 Translation and Ranking Data 5.1 Parallel Translation Data For Japanese-to-English patent translation we used data provided by the organizers of the NTCIR9 workshop for the JP-EN PatentMT subtask. In particular, we used the data provided for NTCIR-7 (Fujii et al., 2008), consisting of 1.8 million parallel sentence pairs from the years 1993-2002 for training. For parameter tuning we used the development set of the NTCIR-8 test collection, consisting of 2,000 sentence pairs. The data were extracted from the description section of patents published by the Japanese Patent Office (JPO) and the United States Patent and Trademark Office (USPTO) by the method described in Utiyama and Isahara (2007). Japanese text was segmented using the MeCab10 toolkit. Following Feng et al. (2011), we applied a modified version of the compound splitter described in Koehn and Knight</context>
</contexts>
<marker>Fujii, Utiyama, Yamamoto, Utsuro, 2008</marker>
<rawString>Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and Takehito Utsuro. 2008. Overview of the patent translation task at the NTCIR-7 workshop. In Proceedings of NTCIR-7 Workshop Meeting, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasser Ganjisaffar</author>
<author>Rich Caruana</author>
<author>Cristina Videira Lopes</author>
</authors>
<title>Bagging gradient-boosted trees for high precision, low variance ranking models.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’11),</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="10223" citStr="Ganjisaffar et al. (2011)" startWordPosition="1604" endWordPosition="1607">fferent directions. The first proposes to learn a low rank representation of the matrix; the second deploys sparse online learning under f, regularization to keep the matrix small. Both approaches are mainly evaluated in a monolingual setting. The cross-lingual evaluation presented in Bai et al. (2010) uses weak translationbased baselines and non-public data such that a direct comparison is not possible. 2http://research.nii.ac.jp/ntcir/ntcir/ 3http://www.ifs.tuwien.ac.at/˜clef-ip/ A combination of bagging and boosting in the context of retrieval has been presented by Pavlov et al. (2010) and Ganjisaffar et al. (2011). This work is done in a standard learning-to-rank setup using a few hundred dense features trained on hundreds of thousands of pairs. Our setup deals with billions of sparse features (from the cross-product of the unrestricted dictionaries) trained on millions of pairs (sampled from a much larger space). Parallel boosting where all feature weights are updated simultaneously has been presented by Collins et al. (2002) and Canini et al. (2010). The first method distributes the gradient calculation for different features among different compute nodes. This is not possible in our approach because</context>
</contexts>
<marker>Ganjisaffar, Caruana, Lopes, 2011</marker>
<rawString>Yasser Ganjisaffar, Rich Caruana, and Cristina Videira Lopes. 2011. Bagging gradient-boosted trees for high precision, low variance ranking models. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’11), Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Jian-Yun Nie</author>
<author>Endong Xun</author>
<author>Jian Zhang</author>
<author>Ming Zhou</author>
<author>Changning Huang</author>
</authors>
<title>Improving query translation for cross-language information retrieval using statistical models.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’01),</booktitle>
<location>New Orleans, LA.</location>
<contexts>
<context position="7779" citStr="Gao et al., 2001" startWordPosition="1219" endWordPosition="1222">pproach is the work of Magdy and Jones (2011) who presented an efficient technique to adapt off-the-shelf SMT systems for CLIR by training them on data pre-processed for retrieval (case folding, stopword removal, stemming). Nikoulina et al. (2012) presented an approach to direct translationbased CLIR where the n-best list of an SMT system is re-ranked according to the MAP performance of the translated queries. The probabilistic structured query approach has seen a lot of work on contextaware query expansion across languages, based on various similarity statistics (Ballesteros and Croft, 1998; Gao et al., 2001; Lavrenko et al., 2002; Gao � Q i=1 qiWijdj. D E j=1 1689 et al., 2007). At the time of writing this paper, the most recent extension to this paradigm is Ture et al. (2012a). In addition to projecting terms from n-best translations, they propose a projection extracted from the hierarchical phrase- based grammar models, and a scoring method based on multi-token terms. Since the latter techniques achieved only marginal improvements over the context-sensitive query translation from n-best lists, we did not pursue them in our work. CLIR in the context of patent prior art search was done as extrin</context>
</contexts>
<marker>Gao, Nie, Xun, Zhang, Zhou, Huang, 2001</marker>
<rawString>Jianfeng Gao, Jian-Yun Nie, Endong Xun, Jian Zhang, Ming Zhou, and Changning Huang. 2001. Improving query translation for cross-language information retrieval using statistical models. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’01), New Orleans, LA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Gao</author>
<author>Cheng Niu</author>
<author>Jian-Yun Nie</author>
<author>Ming Zhou</author>
<author>Jian Hu</author>
<author>Kam-Fai Wong</author>
<author>Hsiao-Wuen Hon</author>
</authors>
<title>Crosslingual query suggestion using query logs of different languages.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’07),</booktitle>
<location>Amsterdam, The Netherlands.</location>
<marker>Gao, Niu, Nie, Zhou, Hu, Wong, Hon, 2007</marker>
<rawString>Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Jian Hu, Kam-Fai Wong, and Hsiao-Wuen Hon. 2007. Crosslingual query suggestion using query logs of different languages. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’07), Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharad Goel</author>
<author>John Langford</author>
<author>Alexander L Strehl</author>
</authors>
<title>Predictive indexing for fast search.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="20010" citStr="Goel et al. (2008)" startWordPosition="3230" endWordPosition="3233">ng: return scoring function f (q, d) = s Et Es wst hst (q, d) Implementation Because of the total number of features (billions) there are several obstacles for the straight-forward implementation of Algorithm 1. First, we cannot directly access all pairs (q, d) containing a particular feature h needed for calculating Wh± . Building an inverted index is complicated as it needs to fit into memory for fast freW± = E h (q,d+,d−):h(q,d+)−h(q,d−)=±1 1692 quent access8. We resort to the on-the-fly creation of the cross-product space of features, following prior work by Grangier and Bengio (2008) and Goel et al. (2008). That is, while processing a pair (q, d), we update Wh� for all h found for the pair. Second, even if the explicit representation of all features is avoided by on-the-fly feature construction, we still need to keep all Wh� in addressable RAM. To achieve that we use hash kernels (Shi et al., 2009) and map original features into b-bit integer hashes. The values Wh1� for new, “hashed”, features h&apos; become Wh1� = Eh:HASH(h)=h1 Wh� . We used the MurmurHash3 function on the UTF-8 representations of features and b = 30 (resulting in more than 1 billion distinct hashes). 4 Model Combination by Borda C</context>
</contexts>
<marker>Goel, Langford, Strehl, 2008</marker>
<rawString>Sharad Goel, John Langford, and Alexander L. Strehl. 2008. Predictive indexing for fast search. In Advances in Neural Information Processing Systems, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Graf</author>
<author>Leif Azzopardi</author>
</authors>
<title>A methodology for building a patent test collection for prior art search.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2nd International Workshop on Evaluating Information Access (EVIA),</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="24713" citStr="Graf and Azzopardi (2008)" startWordPosition="4001" endWordPosition="4004">//code.google.com/p/mecab/ 1693 #queries #relevant #unique docs train 107,061 1,422,253 888,127 dev 2,000 26,478 25,669 test 2,000 25,173 24,668 Table 1: Statistics of ranking data. For the English side of the training data, we applied a modified version of the tokenizer included in the Moses scripts. This tokenizer relies on a list of non-breaking prefixes which mark expressions that are usually followed by a “.” (period). We customized the list of prefixes by adding some abbreviations like “Chem”, “FIG” or “Pat”, which are specific to patent documents. 5.2 Ranking Data from Patent Citations Graf and Azzopardi (2008) describe a method to extract relevance judgements for patent retrieval from patent citations. The key idea is to regard patent documents that are cited in a query patent, either by the patent applicant, or by the patent examiner or in a patent office’s search report, as relevant for the query patent. Furthermore, patent documents that are related to the query patent via a patent family relationship, i.e., patents granted by different patent authorities but related to the same invention, are regarded as relevant. We assign three integer relevance levels to these three categories of relationshi</context>
</contexts>
<marker>Graf, Azzopardi, 2008</marker>
<rawString>Erik Graf and Leif Azzopardi. 2008. A methodology for building a patent test collection for prior art search. In Proceedings of the 2nd International Workshop on Evaluating Information Access (EVIA), Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Grangier</author>
<author>Samy Bengio</author>
</authors>
<title>A discriminative kernel-based approach to rank images from text queries.</title>
<date>2008</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),</journal>
<volume>30</volume>
<issue>8</issue>
<contexts>
<context position="19987" citStr="Grangier and Bengio (2008)" startWordPosition="3225" endWordPosition="3228">st, wst }, t = 1... T end Bagging: return scoring function f (q, d) = s Et Es wst hst (q, d) Implementation Because of the total number of features (billions) there are several obstacles for the straight-forward implementation of Algorithm 1. First, we cannot directly access all pairs (q, d) containing a particular feature h needed for calculating Wh± . Building an inverted index is complicated as it needs to fit into memory for fast freW± = E h (q,d+,d−):h(q,d+)−h(q,d−)=±1 1692 quent access8. We resort to the on-the-fly creation of the cross-product space of features, following prior work by Grangier and Bengio (2008) and Goel et al. (2008). That is, while processing a pair (q, d), we update Wh� for all h found for the pair. Second, even if the explicit representation of all features is avoided by on-the-fly feature construction, we still need to keep all Wh� in addressable RAM. To achieve that we use hash kernels (Shi et al., 2009) and map original features into b-bit integer hashes. The values Wh1� for new, “hashed”, features h&apos; become Wh1� = Eh:HASH(h)=h1 Wh� . We used the MurmurHash3 function on the UTF-8 representations of features and b = 30 (resulting in more than 1 billion distinct hashes). 4 Model</context>
</contexts>
<marker>Grangier, Bengio, 2008</marker>
<rawString>David Grangier and Samy Bengio. 2008. A discriminative kernel-based approach to rank images from text queries. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 30(8):1371–1384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunsong Guo</author>
<author>Carla Gomes</author>
</authors>
<title>Ranking structured documents: A large margin based approach for patent prior art search.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI’09),</booktitle>
<location>Pasadena, CA.</location>
<contexts>
<context position="9040" citStr="Guo and Gomes (2009)" startWordPosition="1425" endWordPosition="1428">kshops until 2010, and has been ongoing in the CLEF-IP3 benchmarking workshops since 2009. However, most workshop participants did either not make use of automatic translation at all, or they used an off-the-shelf translation tool. This is due to the CLEF-IP data collection where parts of patent documents are provided as manual translations into three languages. In order to evaluate CLIR in a truly cross-lingual scenario, we created a large patent CLIR dataset where queries and documents are Japanese and English patent abstracts, respectively. Ranking approaches to CLIR have been presented by Guo and Gomes (2009) who use pairwise ranking for patent retrieval. Their method is a classical learning-to-rank setup where retrieval scores such as tf-idf or BM25 are combined with domain knowledge on patent class, inventor, date, location, etc. into a dense feature vector of a few hundred features. Methods to learn word-based translation correspondences from supervised ranking signals have been presented by Bai et al. (2010) and Chen et al. (2010). These approaches tackle the problem of complexity and capacity of the cross product matrix of word correspondences from different directions. The first proposes to </context>
</contexts>
<marker>Guo, Gomes, 2009</marker>
<rawString>Yunsong Guo and Carla Gomes. 2009. Ranking structured documents: A large margin based approach for patent prior art search. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI’09), Pasadena, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation (WMT’11),</booktitle>
<location>Edinburgh, UK.</location>
<contexts>
<context position="11676" citStr="Heafield, 2011" startWordPosition="1839" endWordPosition="1840">cit feature selection, something that runs contrary to parallel boosting. 3 CLIR Approaches 3.1 Direct translation approach For direct translation, we use the SCFG decoder cdec (Dyer et al., 2010)4 and build grammars using its implementation of the suffix array extraction method described in Lopez (2007). Word alignments are built from all parallel data using mgiza5 and the Moses scripts6. SCFG models use the same settings as described in Chiang (2007). Training and querying of a modified Kneser-Ney smoothed 5- gram language model are done on the English side of the training data using KenLM (Heafield, 2011)7. Model parameters were optimized using cdec’s implementation of MERT (Och (2003)). At retrieval time, all queries are translated sentence-wise and subsequently re-joined to form one query per patent. Our baseline retrieval system uses the Okapi BM25 scores for document ranking. 4https://github.com/redpony/cdec 5http://www.kyloo.net/software/doku.php/ mgiza:overview 6http://www.statmt.org/moses/?n=Moses. SupportTools 7http://kheafield.com/code/kenlm/ estimation/ 1690 3.2 Probabilistic structured query approach Early Probabilistic Structured Query approaches (Xu et al., 2001; Darwish and Oard,</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation (WMT’11), Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gareth J F Jones</author>
<author>Adenike M Lam-Adesina</author>
</authors>
<title>Combination methods for improving the reliability of machine translation based cross-language information retrieval.</title>
<date>2002</date>
<booktitle>In Proceedings of the 13th Irish International Conference on Artificial Intelligence and Cognitive Science (AICS’02),</booktitle>
<location>Limerick,</location>
<contexts>
<context position="22981" citStr="Jones and Lam-Adesina, 2002" startWordPosition="3730" endWordPosition="3733">sidered zero for the other system. The aggregated scores fagg(q, d) are sorted in descending order and top K scores are kept for evaluation. Using the terminology proposed by Belkin et al. (1995), combining several systems’ scores with Borda Counts can be viewed as the “data fusion” approach to IR, that merges outputs of the systems, while the PSQ baseline is an example of the “query combination” approach that extends the query at the input. Both techniques were earlier found to have similar performance in CLIR tasks based on direct translation, with a preference for the data fusion approach (Jones and Lam-Adesina, 2002). 5 Translation and Ranking Data 5.1 Parallel Translation Data For Japanese-to-English patent translation we used data provided by the organizers of the NTCIR9 workshop for the JP-EN PatentMT subtask. In particular, we used the data provided for NTCIR-7 (Fujii et al., 2008), consisting of 1.8 million parallel sentence pairs from the years 1993-2002 for training. For parameter tuning we used the development set of the NTCIR-8 test collection, consisting of 2,000 sentence pairs. The data were extracted from the description section of patents published by the Japanese Patent Office (JPO) and the </context>
</contexts>
<marker>Jones, Lam-Adesina, 2002</marker>
<rawString>Gareth J.F. Jones and Adenike M. Lam-Adesina. 2002. Combination methods for improving the reliability of machine translation based cross-language information retrieval. In Proceedings of the 13th Irish International Conference on Artificial Intelligence and Cognitive Science (AICS’02), Limerick, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical methods for compound splitting.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on European Chapter of the Association for Computational Linguistics (EACL’03),</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="23862" citStr="Koehn and Knight (2003)" startWordPosition="3871" endWordPosition="3874">ii et al., 2008), consisting of 1.8 million parallel sentence pairs from the years 1993-2002 for training. For parameter tuning we used the development set of the NTCIR-8 test collection, consisting of 2,000 sentence pairs. The data were extracted from the description section of patents published by the Japanese Patent Office (JPO) and the United States Patent and Trademark Office (USPTO) by the method described in Utiyama and Isahara (2007). Japanese text was segmented using the MeCab10 toolkit. Following Feng et al. (2011), we applied a modified version of the compound splitter described in Koehn and Knight (2003) to katakana terms, which are often transliterations of English compound words. As these are usually not split by MeCab, they can cause a large number of out-ofvocabulary terms. 9http://research.nii.ac.jp/ntcir/ntcir/ 10https://code.google.com/p/mecab/ 1693 #queries #relevant #unique docs train 107,061 1,422,253 888,127 dev 2,000 26,478 25,669 test 2,000 25,173 24,668 Table 1: Statistics of ranking data. For the English side of the training data, we applied a modified version of the tokenizer included in the Moses scripts. This tokenizer relies on a list of non-breaking prefixes which mark exp</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Philipp Koehn and Kevin Knight. 2003. Empirical methods for compound splitting. In Proceedings of the Conference on European Chapter of the Association for Computational Linguistics (EACL’03), Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>Martin Choquette</author>
<author>W Bruce Croft</author>
</authors>
<title>Cross-lingual relevance models.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR’02),</booktitle>
<location>Tampere, Finland.</location>
<contexts>
<context position="7802" citStr="Lavrenko et al., 2002" startWordPosition="1223" endWordPosition="1226">k of Magdy and Jones (2011) who presented an efficient technique to adapt off-the-shelf SMT systems for CLIR by training them on data pre-processed for retrieval (case folding, stopword removal, stemming). Nikoulina et al. (2012) presented an approach to direct translationbased CLIR where the n-best list of an SMT system is re-ranked according to the MAP performance of the translated queries. The probabilistic structured query approach has seen a lot of work on contextaware query expansion across languages, based on various similarity statistics (Ballesteros and Croft, 1998; Gao et al., 2001; Lavrenko et al., 2002; Gao � Q i=1 qiWijdj. D E j=1 1689 et al., 2007). At the time of writing this paper, the most recent extension to this paradigm is Ture et al. (2012a). In addition to projecting terms from n-best translations, they propose a projection extracted from the hierarchical phrase- based grammar models, and a scoring method based on multi-token terms. Since the latter techniques achieved only marginal improvements over the context-sensitive query translation from n-best lists, we did not pursue them in our work. CLIR in the context of patent prior art search was done as extrinsic evaluation at the N</context>
</contexts>
<marker>Lavrenko, Choquette, Croft, 2002</marker>
<rawString>Victor Lavrenko, Martin Choquette, and W. Bruce Croft. 2002. Cross-lingual relevance models. In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR’02), Tampere, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Hierarchical phrase-based translation with suffix arrays.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2007),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="11366" citStr="Lopez (2007)" startWordPosition="1787" endWordPosition="1788">different compute nodes. This is not possible in our approach because we construct the cross-product matrix on-the-fly. The second approach requires substantial efforts in changing the data representation to use the MapReduce framework. Overall, one of the goals of our work is sequential updating for implicit feature selection, something that runs contrary to parallel boosting. 3 CLIR Approaches 3.1 Direct translation approach For direct translation, we use the SCFG decoder cdec (Dyer et al., 2010)4 and build grammars using its implementation of the suffix array extraction method described in Lopez (2007). Word alignments are built from all parallel data using mgiza5 and the Moses scripts6. SCFG models use the same settings as described in Chiang (2007). Training and querying of a modified Kneser-Ney smoothed 5- gram language model are done on the English side of the training data using KenLM (Heafield, 2011)7. Model parameters were optimized using cdec’s implementation of MERT (Och (2003)). At retrieval time, all queries are translated sentence-wise and subsequently re-joined to form one query per patent. Our baseline retrieval system uses the Okapi BM25 scores for document ranking. 4https://</context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>Adam Lopez. 2007. Hierarchical phrase-based translation with suffix arrays. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2007), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walid Magdy</author>
<author>Gareth J F Jones</author>
</authors>
<title>PRES: a score metric for evaluating recall-oriented information retrieval applications.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACM SIGIR conference on Research and development in information retrieval (SIGIR’10),</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="27541" citStr="Magdy and Jones (2010)" startWordPosition="4458" endWordPosition="4461">rom authors’ webpage13. The experiments reported here use only the abstract of the Japanese and English patents in our training, development and test collection. 6 Experiments 6.1 System Development System development and evaluation in our experiments was done on the ranking data described in the previous section (see Table 1). We report Mean Average Precision (MAP) scores, using the trec eval (ver. 8.1) script from the TREC evaluation campaign14, with a limit of top K = 1, 000 retrieved documents for each query. Furthermore, we use the Patent Retrieval Evaluation Score (PRES)15 introduced by Magdy and Jones (2010). This metric accounts for both precision and recall. In the study by Magdy and Jones (2010), PRES agreed with MAP in almost 80% of cases, and both agreed on the ranks of the best and the worst IR system. Both MAP and PRES scores are reported in the same range [0, 1], and 0.01 stands for 1 MAP (PRES) point. Statistical significance of pairwise system comparisons was assessed using the paired randomization test (Noreen, 1989; Smucker et al., 2007). For each system, optimal meta-parameter settings were found by choosing the configuration with highest MAP score on the development set. These resul</context>
</contexts>
<marker>Magdy, Jones, 2010</marker>
<rawString>Walid Magdy and Gareth J.F. Jones. 2010. PRES: a score metric for evaluating recall-oriented information retrieval applications. In Proceedings of the ACM SIGIR conference on Research and development in information retrieval (SIGIR’10), New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walid Magdy</author>
<author>Gareth J F Jones</author>
</authors>
<title>An efficient method for using machine translation technologies in cross-language patent search.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM Conference on Informationand Knowledge Management (CIKM’11),</booktitle>
<location>Glasgow, Scotland, UK.</location>
<contexts>
<context position="7208" citStr="Magdy and Jones (2011)" startWordPosition="1131" endWordPosition="1134">R systems that are trained on 1.8 million parallel sentence pairs from JapaneseEnglish patent documents. Moreover, a combination of the orthogonal information learned in rankingbased and translation-based approaches improves over 7 MAP points and over 15 PRES points over the respective translation-based system in a consensusbased voting approach following the Borda Count technique (Aslam and Montague, 2001). 2 Related Work Recent research in CLIR follows the two main paradigms of direct translation and probabilistic structured query approaches. An example for the first approach is the work of Magdy and Jones (2011) who presented an efficient technique to adapt off-the-shelf SMT systems for CLIR by training them on data pre-processed for retrieval (case folding, stopword removal, stemming). Nikoulina et al. (2012) presented an approach to direct translationbased CLIR where the n-best list of an SMT system is re-ranked according to the MAP performance of the translated queries. The probabilistic structured query approach has seen a lot of work on contextaware query expansion across languages, based on various similarity statistics (Ballesteros and Croft, 1998; Gao et al., 2001; Lavrenko et al., 2002; Gao </context>
</contexts>
<marker>Magdy, Jones, 2011</marker>
<rawString>Walid Magdy and Gareth J. F. Jones. 2011. An efficient method for using machine translation technologies in cross-language patent search. In Proceedings of the 20th ACM Conference on Informationand Knowledge Management (CIKM’11), Glasgow, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vassilina Nikoulina</author>
<author>Bogomil Kovachev</author>
<author>Nikolaos Lagos</author>
<author>Christof Monz</author>
</authors>
<title>Adaptation of statistical machine translation model for cross-lingual information retrieval in a service context.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL’12),</booktitle>
<location>Avignon, France.</location>
<contexts>
<context position="7410" citStr="Nikoulina et al. (2012)" startWordPosition="1161" endWordPosition="1164">sed approaches improves over 7 MAP points and over 15 PRES points over the respective translation-based system in a consensusbased voting approach following the Borda Count technique (Aslam and Montague, 2001). 2 Related Work Recent research in CLIR follows the two main paradigms of direct translation and probabilistic structured query approaches. An example for the first approach is the work of Magdy and Jones (2011) who presented an efficient technique to adapt off-the-shelf SMT systems for CLIR by training them on data pre-processed for retrieval (case folding, stopword removal, stemming). Nikoulina et al. (2012) presented an approach to direct translationbased CLIR where the n-best list of an SMT system is re-ranked according to the MAP performance of the translated queries. The probabilistic structured query approach has seen a lot of work on contextaware query expansion across languages, based on various similarity statistics (Ballesteros and Croft, 1998; Gao et al., 2001; Lavrenko et al., 2002; Gao � Q i=1 qiWijdj. D E j=1 1689 et al., 2007). At the time of writing this paper, the most recent extension to this paradigm is Ture et al. (2012a). In addition to projecting terms from n-best translation</context>
</contexts>
<marker>Nikoulina, Kovachev, Lagos, Monz, 2012</marker>
<rawString>Vassilina Nikoulina, Bogomil Kovachev, Nikolaos Lagos, and Christof Monz. 2012. Adaptation of statistical machine translation model for cross-lingual information retrieval in a service context. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL’12), Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer Intensive Methods for Testing Hypotheses. An Introduction.</title>
<date>1989</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="27968" citStr="Noreen, 1989" startWordPosition="4537" endWordPosition="4538">on campaign14, with a limit of top K = 1, 000 retrieved documents for each query. Furthermore, we use the Patent Retrieval Evaluation Score (PRES)15 introduced by Magdy and Jones (2010). This metric accounts for both precision and recall. In the study by Magdy and Jones (2010), PRES agreed with MAP in almost 80% of cases, and both agreed on the ranks of the best and the worst IR system. Both MAP and PRES scores are reported in the same range [0, 1], and 0.01 stands for 1 MAP (PRES) point. Statistical significance of pairwise system comparisons was assessed using the paired randomization test (Noreen, 1989; Smucker et al., 2007). For each system, optimal meta-parameter settings were found by choosing the configuration with highest MAP score on the development set. These results 13http://www.cl.uni-heidelberg.de/ statnlpgroup/boostclir 14http://trec.nist.gov/trec_eval 15http://www.computing.dcu.ie/˜wmagdy/ Scripts/PRESeval.htm 1694 MAP PRES method dev test dev test 1 DT 0.2636 0.2555 0.5669 0.5681 2 PSQ lexical table 0.2520 0.2444 0.5445 0.5498 3 PSQ n-best table 0.2698 0.2659 0.5789 0.5851 Boost-1g 0.2064 1230.1982 0.5850 120.6122 Boost-2g 0.2526 30.2474 0.6900 1230.7196 Table 2: MAP and PRES s</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer Intensive Methods for Testing Hypotheses. An Introduction. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Meeting of the Association for Computational Linguistics (ACL’00), Hongkong,</booktitle>
<contexts>
<context position="12444" citStr="Och and Ney, 2000" startWordPosition="1925" endWordPosition="1928">ubsequently re-joined to form one query per patent. Our baseline retrieval system uses the Okapi BM25 scores for document ranking. 4https://github.com/redpony/cdec 5http://www.kyloo.net/software/doku.php/ mgiza:overview 6http://www.statmt.org/moses/?n=Moses. SupportTools 7http://kheafield.com/code/kenlm/ estimation/ 1690 3.2 Probabilistic structured query approach Early Probabilistic Structured Query approaches (Xu et al., 2001; Darwish and Oard, 2003) represent translation options by lexical, i.e., token-to-token translation tables that are estimated using standard word alignment techniques (Och and Ney, 2000). Later approaches (Ture et al., 2012b; Ture et al., 2012a) extract translation options from the decoder’s n-best list for translating a particular query. The central idea is to let the language model choose fluent, context-aware translations for each query term during decoding. This retains the desired queryexpansion effect of probabilistic structured models, but it reduces query drift by filtering translations with respect to the context of the full query. A projection of source language query terms f E F into the target language is achieved by representing each source token f by its probabi</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Meeting of the Association for Computational Linguistics (ACL’00), Hongkong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting on Association for Computational Linguistics (ACL’03),</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="11758" citStr="Och (2003)" startWordPosition="1851" endWordPosition="1852">ches 3.1 Direct translation approach For direct translation, we use the SCFG decoder cdec (Dyer et al., 2010)4 and build grammars using its implementation of the suffix array extraction method described in Lopez (2007). Word alignments are built from all parallel data using mgiza5 and the Moses scripts6. SCFG models use the same settings as described in Chiang (2007). Training and querying of a modified Kneser-Ney smoothed 5- gram language model are done on the English side of the training data using KenLM (Heafield, 2011)7. Model parameters were optimized using cdec’s implementation of MERT (Och (2003)). At retrieval time, all queries are translated sentence-wise and subsequently re-joined to form one query per patent. Our baseline retrieval system uses the Okapi BM25 scores for document ranking. 4https://github.com/redpony/cdec 5http://www.kyloo.net/software/doku.php/ mgiza:overview 6http://www.statmt.org/moses/?n=Moses. SupportTools 7http://kheafield.com/code/kenlm/ estimation/ 1690 3.2 Probabilistic structured query approach Early Probabilistic Structured Query approaches (Xu et al., 2001; Darwish and Oard, 2003) represent translation options by lexical, i.e., token-to-token translation </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Meeting on Association for Computational Linguistics (ACL’03), Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Pavlov</author>
<author>Alexey Gorodilov</author>
<author>Cliff A Brunk</author>
</authors>
<title>Bagboo: a scalable hybrid bagging-theboosting model.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM International Conference on Information and Knowledge Management (CIKM’10),</booktitle>
<location>Toronto, Canada.</location>
<contexts>
<context position="10193" citStr="Pavlov et al. (2010)" startWordPosition="1599" endWordPosition="1602">d correspondences from different directions. The first proposes to learn a low rank representation of the matrix; the second deploys sparse online learning under f, regularization to keep the matrix small. Both approaches are mainly evaluated in a monolingual setting. The cross-lingual evaluation presented in Bai et al. (2010) uses weak translationbased baselines and non-public data such that a direct comparison is not possible. 2http://research.nii.ac.jp/ntcir/ntcir/ 3http://www.ifs.tuwien.ac.at/˜clef-ip/ A combination of bagging and boosting in the context of retrieval has been presented by Pavlov et al. (2010) and Ganjisaffar et al. (2011). This work is done in a standard learning-to-rank setup using a few hundred dense features trained on hundreds of thousands of pairs. Our setup deals with billions of sparse features (from the cross-product of the unrestricted dictionaries) trained on millions of pairs (sampled from a much larger space). Parallel boosting where all feature weights are updated simultaneously has been presented by Collins et al. (2002) and Canini et al. (2010). The first method distributes the gradient calculation for different features among different compute nodes. This is not po</context>
</contexts>
<marker>Pavlov, Gorodilov, Brunk, 2010</marker>
<rawString>Dmitry Pavlov, Alexey Gorodilov, and Cliff A. Brunk. 2010. Bagboo: a scalable hybrid bagging-theboosting model. In Proceedings of the 19th ACM International Conference on Information and Knowledge Management (CIKM’10), Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen E Robertson</author>
<author>Steve Walker</author>
<author>Micheline Hancock-Beaulieu</author>
</authors>
<title>Okapi at TREC-7.</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventh Text REtrieval Conference (TREC-7),</booktitle>
<location>Gaithersburg, MD.</location>
<contexts>
<context position="2814" citStr="Robertson et al., 1998" startWordPosition="401" endWordPosition="404">rm translation probabilities from n-best direct translations of queries instead of using context-free translation tables outperforms both direct translation and context-free projection (Ture et al., 2012b; Ture et al., 2012a). While direct translation as well as probabilistic structured query approaches use machine learning to optimize the SMT module, retrieval is done by standard search algorithms in both approaches. For example, Google’s CLIR approach uses their standard proprietary search engine (Chin et al., 2008). Ture et al. (2012b; 2012a) use standard retrieval algorithms such as BM25 (Robertson et al., 1998). That means, machine learning in SMT-based approaches concentrates on the cross-language aspect of CLIR and is agnostic of the ultimate ranking task. In this paper, we present a method to project search queries into the target language that is complementary to SMT-based CLIR approaches. Our method learns a table of n-gram correspondences by direct optimization of a ranking objective on relevance rankings of English documents for Japanese queries. Our model is similar to the approach of Bai et al. (2010) who characterize their technique as “Learning to rank with (a Lot of) Word Features”. Give</context>
</contexts>
<marker>Robertson, Walker, Hancock-Beaulieu, 1998</marker>
<rawString>Stephen E. Robertson, Steve Walker, and Micheline Hancock-Beaulieu. 1998. Okapi at TREC-7. In Proceedings of the Seventh Text REtrieval Conference (TREC-7), Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saharon Rosset</author>
<author>Ji Zhu</author>
<author>Trevor Hastie</author>
</authors>
<title>Boosting as a regularized path to a maximum margin classifier.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>5</volume>
<pages>973</pages>
<contexts>
<context position="17226" citStr="Rosset et al., 2004" startWordPosition="2713" endWordPosition="2716">oss: �Lexp = D(q, d+, d−)ef(q,d−)−f(q,d+), (q,d+,d−)∈R where D(q, d+, d−) is a non-negative importance function on pairs of documents for a given q. We optimize Lexp in a greedy iterative fashion, which closely follows an efficient algorithm of Collins and Koo (2005) for the case of binary-valued h. In each step, the single feature h is selected that provides the largest decrease of Lexp, i.e., that has the largest projection on the direction of the gradient VhLexp. Because of the sequential nature of the algorithm, RankBoost implicitly performs automatic feature selection and regularization (Rosset et al., 2004), which is crucial to reduce complexity and capacity for our application. Parallelization and Bagging To achieve parallelization we use a variant of bagging (Breiman, 1996) on top of boosting, which has been observed to improve performance, reduce variance and is trivial to parallelize. The procedure is described as part of Algorithm 1: From the set of preference pairs R, draw S equal-sized samples with replacement and distribute to nodes. Then, using each of the samples as a training set, separate boosting models {wst , hst}, s = 1... S are trained that contain the same number of features t =</context>
</contexts>
<marker>Rosset, Zhu, Hastie, 2004</marker>
<rawString>Saharon Rosset, Ji Zhu, and Trevor Hastie. 2004. Boosting as a regularized path to a maximum margin classifier. Journal of Machine Learning Research, 5:941– 973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Improved boosting algorithms using confidence-rated predictions.</title>
<date>1999</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="18493" citStr="Schapire and Singer, 1999" startWordPosition="2947" endWordPosition="2950"> f (q, d) = s Et Es wst hst (q, d) Algorithm The entire training procedure is outlined in Algorithm 1. For each possible feature h we maintain auxiliary variables Wh+ and Wh− : D(q, d+, d−), which are the cumulative weights of correctly and incorrectly ranked instances by a candidate feature h. The absolute value of aLexp/ah can be expressed as I �W+ − �W−h I which is used as feature selection criterion (Collins and Koo, 2005). The optimum of minimizing Lexp over w (with fixed h) can be shown to be w = 1 ln Wh +EZ 2 + Wh− +EZ, where E is a smoothing parameter to avoid problems with small Wh± (Schapire and Singer, 1999), and Z = E(q,d+,d−)∈R D(q, d+, d−). Furthermore, for each step t of the learning process, values of D are updated to concentrate on pairs that have not been correctly ranked so far: Dt+1 = Dt · ewt(ht(q,d−)−ht(q,d+)) . (5) Finally, to speed up learning, on iteration t we recalculate Wh± only for those h that cooccur with previously selected ht and keep the rest unchanged (Collins and Koo, 2005). Algorithm 1: Bagged Boosting Input: training tuples R, max number of features T, initial D0, smoothing param. E ^_ 10−5 Initialize: from R draw S samples with replacement and distribute to nodes Learn</context>
</contexts>
<marker>Schapire, Singer, 1999</marker>
<rawString>Robert E. Schapire and Yoram Singer. 1999. Improved boosting algorithms using confidence-rated predictions. Journal of Machine Learning Research, 37(3):297–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qinfeng Shi</author>
<author>James Petterson</author>
<author>Gideon Dror</author>
<author>John Langford</author>
<author>Alexander J Smola</author>
<author>Alexander L Strehl</author>
<author>Vishy Vishwanathan</author>
</authors>
<title>Hash Kernels.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Int. Conference on Artificial Intelligence and Statistics (AISTATS’09),</booktitle>
<location>Irvine, CA.</location>
<contexts>
<context position="20308" citStr="Shi et al., 2009" startWordPosition="3285" endWordPosition="3288">ded for calculating Wh± . Building an inverted index is complicated as it needs to fit into memory for fast freW± = E h (q,d+,d−):h(q,d+)−h(q,d−)=±1 1692 quent access8. We resort to the on-the-fly creation of the cross-product space of features, following prior work by Grangier and Bengio (2008) and Goel et al. (2008). That is, while processing a pair (q, d), we update Wh� for all h found for the pair. Second, even if the explicit representation of all features is avoided by on-the-fly feature construction, we still need to keep all Wh� in addressable RAM. To achieve that we use hash kernels (Shi et al., 2009) and map original features into b-bit integer hashes. The values Wh1� for new, “hashed”, features h&apos; become Wh1� = Eh:HASH(h)=h1 Wh� . We used the MurmurHash3 function on the UTF-8 representations of features and b = 30 (resulting in more than 1 billion distinct hashes). 4 Model Combination by Borda Counts SMT-based approaches to CLIR and our boosting approach have different strengths. The SMT-based approaches produce fluent translations that are useful for matching general passages written in natural language. Both baseline SMT-based approaches presented above are agnostic of the ultimate ret</context>
</contexts>
<marker>Shi, Petterson, Dror, Langford, Smola, Strehl, Vishwanathan, 2009</marker>
<rawString>Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alexander J. Smola, Alexander L. Strehl, and Vishy Vishwanathan. 2009. Hash Kernels. In Proceedings of the 12th Int. Conference on Artificial Intelligence and Statistics (AISTATS’09), Irvine, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark D Smucker</author>
<author>James Allan</author>
<author>Ben Carterette</author>
</authors>
<title>A comparison of statistical significance tests for information retrieval evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th ACM conference on Conference on Information and Knowledge Management (CIKM ’07),</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="27991" citStr="Smucker et al., 2007" startWordPosition="4539" endWordPosition="4542"> with a limit of top K = 1, 000 retrieved documents for each query. Furthermore, we use the Patent Retrieval Evaluation Score (PRES)15 introduced by Magdy and Jones (2010). This metric accounts for both precision and recall. In the study by Magdy and Jones (2010), PRES agreed with MAP in almost 80% of cases, and both agreed on the ranks of the best and the worst IR system. Both MAP and PRES scores are reported in the same range [0, 1], and 0.01 stands for 1 MAP (PRES) point. Statistical significance of pairwise system comparisons was assessed using the paired randomization test (Noreen, 1989; Smucker et al., 2007). For each system, optimal meta-parameter settings were found by choosing the configuration with highest MAP score on the development set. These results 13http://www.cl.uni-heidelberg.de/ statnlpgroup/boostclir 14http://trec.nist.gov/trec_eval 15http://www.computing.dcu.ie/˜wmagdy/ Scripts/PRESeval.htm 1694 MAP PRES method dev test dev test 1 DT 0.2636 0.2555 0.5669 0.5681 2 PSQ lexical table 0.2520 0.2444 0.5445 0.5498 3 PSQ n-best table 0.2698 0.2659 0.5789 0.5851 Boost-1g 0.2064 1230.1982 0.5850 120.6122 Boost-2g 0.2526 30.2474 0.6900 1230.7196 Table 2: MAP and PRES scores for CLIR methods </context>
</contexts>
<marker>Smucker, Allan, Carterette, 2007</marker>
<rawString>Mark D. Smucker, James Allan, and Ben Carterette. 2007. A comparison of statistical significance tests for information retrieval evaluation. In Proceedings of the 16th ACM conference on Conference on Information and Knowledge Management (CIKM ’07), New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferhan Ture</author>
<author>Jimmy Lin</author>
<author>Douglas W Oard</author>
</authors>
<title>Combining statistical translation techniques for crosslanguage information retrieval.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING 2012),</booktitle>
<location>Bombay, India.</location>
<contexts>
<context position="2394" citStr="Ture et al., 2012" startWordPosition="336" endWordPosition="339">terms into the target language with a probabilistic weighting of the standard term tf-idf scheme. Darwish and Oard (2003) termed this method the probabilistic structured query approach. The advantage of this technique is an implicit query expansion effect due to the use of probability distributions over term translations (Xu et al., 2001). Recent research has shown that leveraging query context by extracting term translation probabilities from n-best direct translations of queries instead of using context-free translation tables outperforms both direct translation and context-free projection (Ture et al., 2012b; Ture et al., 2012a). While direct translation as well as probabilistic structured query approaches use machine learning to optimize the SMT module, retrieval is done by standard search algorithms in both approaches. For example, Google’s CLIR approach uses their standard proprietary search engine (Chin et al., 2008). Ture et al. (2012b; 2012a) use standard retrieval algorithms such as BM25 (Robertson et al., 1998). That means, machine learning in SMT-based approaches concentrates on the cross-language aspect of CLIR and is agnostic of the ultimate ranking task. In this paper, we present a m</context>
<context position="7951" citStr="Ture et al. (2012" startWordPosition="1254" endWordPosition="1257">etrieval (case folding, stopword removal, stemming). Nikoulina et al. (2012) presented an approach to direct translationbased CLIR where the n-best list of an SMT system is re-ranked according to the MAP performance of the translated queries. The probabilistic structured query approach has seen a lot of work on contextaware query expansion across languages, based on various similarity statistics (Ballesteros and Croft, 1998; Gao et al., 2001; Lavrenko et al., 2002; Gao � Q i=1 qiWijdj. D E j=1 1689 et al., 2007). At the time of writing this paper, the most recent extension to this paradigm is Ture et al. (2012a). In addition to projecting terms from n-best translations, they propose a projection extracted from the hierarchical phrase- based grammar models, and a scoring method based on multi-token terms. Since the latter techniques achieved only marginal improvements over the context-sensitive query translation from n-best lists, we did not pursue them in our work. CLIR in the context of patent prior art search was done as extrinsic evaluation at the NTCIR PatentMT2 workshops until 2010, and has been ongoing in the CLEF-IP3 benchmarking workshops since 2009. However, most workshop participants did </context>
<context position="12481" citStr="Ture et al., 2012" startWordPosition="1931" endWordPosition="1934">y per patent. Our baseline retrieval system uses the Okapi BM25 scores for document ranking. 4https://github.com/redpony/cdec 5http://www.kyloo.net/software/doku.php/ mgiza:overview 6http://www.statmt.org/moses/?n=Moses. SupportTools 7http://kheafield.com/code/kenlm/ estimation/ 1690 3.2 Probabilistic structured query approach Early Probabilistic Structured Query approaches (Xu et al., 2001; Darwish and Oard, 2003) represent translation options by lexical, i.e., token-to-token translation tables that are estimated using standard word alignment techniques (Och and Ney, 2000). Later approaches (Ture et al., 2012b; Ture et al., 2012a) extract translation options from the decoder’s n-best list for translating a particular query. The central idea is to let the language model choose fluent, context-aware translations for each query term during decoding. This retains the desired queryexpansion effect of probabilistic structured models, but it reduces query drift by filtering translations with respect to the context of the full query. A projection of source language query terms f E F into the target language is achieved by representing each source token f by its probabilistically weighted translations. The</context>
</contexts>
<marker>Ture, Lin, Oard, 2012</marker>
<rawString>Ferhan Ture, Jimmy Lin, and Douglas W. Oard. 2012a. Combining statistical translation techniques for crosslanguage information retrieval. In Proceedings of the International Conference on Computational Linguistics (COLING 2012), Bombay, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferhan Ture</author>
<author>Jimmy Lin</author>
<author>Douglas W Oard</author>
</authors>
<title>Looking inside the box: Context-sensitive translation for cross-language information retrieval.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2012),</booktitle>
<location>Portland, OR.</location>
<contexts>
<context position="2394" citStr="Ture et al., 2012" startWordPosition="336" endWordPosition="339">terms into the target language with a probabilistic weighting of the standard term tf-idf scheme. Darwish and Oard (2003) termed this method the probabilistic structured query approach. The advantage of this technique is an implicit query expansion effect due to the use of probability distributions over term translations (Xu et al., 2001). Recent research has shown that leveraging query context by extracting term translation probabilities from n-best direct translations of queries instead of using context-free translation tables outperforms both direct translation and context-free projection (Ture et al., 2012b; Ture et al., 2012a). While direct translation as well as probabilistic structured query approaches use machine learning to optimize the SMT module, retrieval is done by standard search algorithms in both approaches. For example, Google’s CLIR approach uses their standard proprietary search engine (Chin et al., 2008). Ture et al. (2012b; 2012a) use standard retrieval algorithms such as BM25 (Robertson et al., 1998). That means, machine learning in SMT-based approaches concentrates on the cross-language aspect of CLIR and is agnostic of the ultimate ranking task. In this paper, we present a m</context>
<context position="7951" citStr="Ture et al. (2012" startWordPosition="1254" endWordPosition="1257">etrieval (case folding, stopword removal, stemming). Nikoulina et al. (2012) presented an approach to direct translationbased CLIR where the n-best list of an SMT system is re-ranked according to the MAP performance of the translated queries. The probabilistic structured query approach has seen a lot of work on contextaware query expansion across languages, based on various similarity statistics (Ballesteros and Croft, 1998; Gao et al., 2001; Lavrenko et al., 2002; Gao � Q i=1 qiWijdj. D E j=1 1689 et al., 2007). At the time of writing this paper, the most recent extension to this paradigm is Ture et al. (2012a). In addition to projecting terms from n-best translations, they propose a projection extracted from the hierarchical phrase- based grammar models, and a scoring method based on multi-token terms. Since the latter techniques achieved only marginal improvements over the context-sensitive query translation from n-best lists, we did not pursue them in our work. CLIR in the context of patent prior art search was done as extrinsic evaluation at the NTCIR PatentMT2 workshops until 2010, and has been ongoing in the CLEF-IP3 benchmarking workshops since 2009. However, most workshop participants did </context>
<context position="12481" citStr="Ture et al., 2012" startWordPosition="1931" endWordPosition="1934">y per patent. Our baseline retrieval system uses the Okapi BM25 scores for document ranking. 4https://github.com/redpony/cdec 5http://www.kyloo.net/software/doku.php/ mgiza:overview 6http://www.statmt.org/moses/?n=Moses. SupportTools 7http://kheafield.com/code/kenlm/ estimation/ 1690 3.2 Probabilistic structured query approach Early Probabilistic Structured Query approaches (Xu et al., 2001; Darwish and Oard, 2003) represent translation options by lexical, i.e., token-to-token translation tables that are estimated using standard word alignment techniques (Och and Ney, 2000). Later approaches (Ture et al., 2012b; Ture et al., 2012a) extract translation options from the decoder’s n-best list for translating a particular query. The central idea is to let the language model choose fluent, context-aware translations for each query term during decoding. This retains the desired queryexpansion effect of probabilistic structured models, but it reduces query drift by filtering translations with respect to the context of the full query. A projection of source language query terms f E F into the target language is achieved by representing each source token f by its probabilistically weighted translations. The</context>
</contexts>
<marker>Ture, Lin, Oard, 2012</marker>
<rawString>Ferhan Ture, Jimmy Lin, and Douglas W. Oard. 2012b. Looking inside the box: Context-sensitive translation for cross-language information retrieval. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2012), Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A JapaneseEnglish patent parallel corpus.</title>
<date>2007</date>
<booktitle>In Proceedings of MT</booktitle>
<location>Summit XI, Copenhagen, Denmark.</location>
<contexts>
<context position="23684" citStr="Utiyama and Isahara (2007)" startWordPosition="3842" endWordPosition="3845">o-English patent translation we used data provided by the organizers of the NTCIR9 workshop for the JP-EN PatentMT subtask. In particular, we used the data provided for NTCIR-7 (Fujii et al., 2008), consisting of 1.8 million parallel sentence pairs from the years 1993-2002 for training. For parameter tuning we used the development set of the NTCIR-8 test collection, consisting of 2,000 sentence pairs. The data were extracted from the description section of patents published by the Japanese Patent Office (JPO) and the United States Patent and Trademark Office (USPTO) by the method described in Utiyama and Isahara (2007). Japanese text was segmented using the MeCab10 toolkit. Following Feng et al. (2011), we applied a modified version of the compound splitter described in Koehn and Knight (2003) to katakana terms, which are often transliterations of English compound words. As these are usually not split by MeCab, they can cause a large number of out-ofvocabulary terms. 9http://research.nii.ac.jp/ntcir/ntcir/ 10https://code.google.com/p/mecab/ 1693 #queries #relevant #unique docs train 107,061 1,422,253 888,127 dev 2,000 26,478 25,669 test 2,000 25,173 24,668 Table 1: Statistics of ranking data. For the Englis</context>
</contexts>
<marker>Utiyama, Isahara, 2007</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2007. A JapaneseEnglish patent parallel corpus. In Proceedings of MT Summit XI, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
<author>Chanh Nguyen</author>
</authors>
<title>Evaluating a probabilistic model for cross-lingual information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’01),</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="2117" citStr="Xu et al., 2001" startWordPosition="297" endWordPosition="300">ation approach. This technique is particularly useful if large amounts of data are available in domain-specific form. Alternative approaches avoid to solve the hard problem of word reordering, and instead rely on token-to-token translations that are used to project the query terms into the target language with a probabilistic weighting of the standard term tf-idf scheme. Darwish and Oard (2003) termed this method the probabilistic structured query approach. The advantage of this technique is an implicit query expansion effect due to the use of probability distributions over term translations (Xu et al., 2001). Recent research has shown that leveraging query context by extracting term translation probabilities from n-best direct translations of queries instead of using context-free translation tables outperforms both direct translation and context-free projection (Ture et al., 2012b; Ture et al., 2012a). While direct translation as well as probabilistic structured query approaches use machine learning to optimize the SMT module, retrieval is done by standard search algorithms in both approaches. For example, Google’s CLIR approach uses their standard proprietary search engine (Chin et al., 2008). T</context>
<context position="12257" citStr="Xu et al., 2001" startWordPosition="1900" endWordPosition="1903"> data using KenLM (Heafield, 2011)7. Model parameters were optimized using cdec’s implementation of MERT (Och (2003)). At retrieval time, all queries are translated sentence-wise and subsequently re-joined to form one query per patent. Our baseline retrieval system uses the Okapi BM25 scores for document ranking. 4https://github.com/redpony/cdec 5http://www.kyloo.net/software/doku.php/ mgiza:overview 6http://www.statmt.org/moses/?n=Moses. SupportTools 7http://kheafield.com/code/kenlm/ estimation/ 1690 3.2 Probabilistic structured query approach Early Probabilistic Structured Query approaches (Xu et al., 2001; Darwish and Oard, 2003) represent translation options by lexical, i.e., token-to-token translation tables that are estimated using standard word alignment techniques (Och and Ney, 2000). Later approaches (Ture et al., 2012b; Ture et al., 2012a) extract translation options from the decoder’s n-best list for translating a particular query. The central idea is to let the language model choose fluent, context-aware translations for each query term during decoding. This retains the desired queryexpansion effect of probabilistic structured models, but it reduces query drift by filtering translatio</context>
</contexts>
<marker>Xu, Weischedel, Nguyen, 2001</marker>
<rawString>Jinxi Xu, Ralph Weischedel, and Chanh Nguyen. 2001. Evaluating a probabilistic model for cross-lingual information retrieval. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’01), New York, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000056">
<title confidence="0.960785">
Automatic Extraction of Morphological Lexicons
from Morphologically Annotated Corpora
</title>
<author confidence="0.998439">
Ramy Eskander, Nizar Habash, Owen Rambow
</author>
<affiliation confidence="0.996603">
Center for Computational Learning Systems
Columbia University
</affiliation>
<email confidence="0.999383">
{reskander,habash,rambow}@ccls.columbia.edu
</email>
<sectionHeader confidence="0.997397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999776333333333">
We present a method for automatically learn-
ing inflectional classes and associated lem-
mas from morphologically annotated corpora.
The method consists of a core language-
independent algorithm, which can be opti-
mized for specific languages. The method is
demonstrated on Egyptian Arabic and Ger-
man, two morphologically rich languages.
Our best method for Egyptian Arabic pro-
vides an error reduction of 55.6% over a sim-
ple baseline; our best method for German
achieves a 66.7% error reduction.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999167049180328">
Morphological lexicons specify all inflected forms
for each lexeme; in a language with rich morphol-
ogy, such a resource can be important for natural lan-
guage processing (NLP) tasks in order to limit data
sparseness. For example, a morphological lexicon is
an important component of a morphological tagger
or of a part-of-speech (POS) tagger for languages
with rich morphology.&apos; Traditionally, a morpholog-
ical lexicon has been created through painstaking
lexicographic and morphological analysis of the lan-
guage, drawing on unannotated corpora. Recently,
new approaches have emerged. Fully or largely un-
supervised approaches cannot link surface forms to
morphosyntactic features and are thus not suited for
building morphological lexicons. This problem is
overcome by approaches that use explicit linguistic
knowledge. In this paper, we investigate using exist-
ing morphologically annotated corpora. In a mor-
phologically annotated corpus, the words in natu-
rally occurring texts or transcribed speech are anno-
tated for the correct morphological analysis (includ-
&apos;Note that a full-form morphological lexicon is functionally
equivalent to a morphological analyzer-generator, since one can
be used to create the other.
ing, of course, core POS) in context. While there
has been much work on computational morphology,
to our knowledge this is the first paper to study the
question of how to extract morphological lexicons
from morphologically annotated corpora, and how
to determine how much annotation is needed. In
this paper, we assume a corpus with each word an-
notated with morphosyntactic features and with a
lemma which tells us what lexeme the word form
is part of. The task is to predict the correspondence
between a word form and its lemma and morpholog-
ical features.
This paper makes two contributions. First, we
introduce an algorithm that learns unseen forms by
analogy. This algorithm is language-independent. It
incrementally merges complementary paradigm in-
formation about different lexemes into more abstract
and more informative inflectional classes. Second,
we explore how to model stems, and we propose
a generalization of the Semitic root-and-template
modeling. We use Egyptian Arabic (EGY), and Ger-
man (GER) as our test languages. We test on corpus
data, in order to simulate a standard real-world ap-
plication. The baseline just uses the word forms seen
in training and does not predict any unseen forms.
Our language-independent algorithm improves the
performance for both EGY and GER, with error re-
ductions over the baseline of 44.4% for EGY and of
66.7% for GER. By adding language-specific mod-
eling of the stem using templates, we obtain further
error reductions for EGY (up to 55.6%) but not for
GER.
Next, we review related work (Section 2) and
introduce the key linguistic concepts we use (Sec-
tion 3). We present our basic language-independent
method in Section 4, and our language-specific mod-
eling of stem variation in Section 5.
</bodyText>
<page confidence="0.966183">
1032
</page>
<note confidence="0.774236">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1032–1043,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.998526" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999383468750001">
Approaches to Morphological Modeling Much
work has been done in the area of computational
morphology ranging from systems painstakingly de-
signed by hand (Koskenniemi, 1983; Buckwalter,
2004; Habash and Rambow, 2006; D´etrez and
Ranta, 2012) to unsupervised methods that learn
morphology models from unannotated data (Creutz
and Lagus, 2007; Monson et al., 2008; Ham-
marstr¨om and Borin, 2011; Dreyer and Eisner,
2011). There is a large continuum between these two
approaches. Closer to one end, we find work on min-
imally supervised methods for morphology learning
that make use of available resources such as paral-
lel data, dictionaries or some additional morpholog-
ical annotations (Yarowsky and Wicentowski, 2000;
Cucerzan and Yarowsky, 2002; Neuvel and Fulop,
2002; Snyder and Barzilay, 2008). Closer to the
other end, we find work that focuses on defining
morphological models with limited lexicons that are
then extended using raw text (Cl´ement et al., 2004;
Forsberg et al., 2006). The work presented in this
paper falls in the middle of this continuum: we are
interested in learning complete morphological mod-
els using rich morphological annotations and, op-
tionally, limited linguistic knowledge. We compare
the value of different amounts of annotation and how
they relate to additional linguistic knowledge.
Morphological Paradigms Many traditional and
modern theories of inflectional morphology orga-
nize natural language morphology by paradigms
(Stump, 2001; Walther, 2011; Camilleri, 2011).
Within the continuum we discussed above, we find
hierarchical representations of paradigm knowledge
that have been used in manually constructed mor-
phological models (Finkel and Stump, 2002; Habash
et al., 2005). Furthermore, D´etrez and Ranta (2012)
introduce an implementation of Smart Paradigms
– heuristically organized paradigms minimizing the
number of forms needed to predict the full paradigm
of a particular lexeme.
Both Forsberg et al. (2006) and Cl´ement et al.
(2004) describe methods for automatically populat-
ing a lexicon from raw data given a set of morpho-
logical inflectional classes in a language. Our work
differs in that we use annotated data, but do not start
with a complete set of inflectional classes; thus, our
work is exactly complementary to this work.
The concept of a paradigm is also used in many
published efforts on unsupervised learning of mor-
phology, although not always in a way consistent
with its use in linguistics. For instance, Snover et
al. (2002) (and later on Can and Manandhar (2012))
define a paradigm as “a set of suffixes and the stems
that attach to those suffixes and no others”. This
definition is quite limited since it is not modeling
the notion of lexeme. Chan (2006) defines a simpler
concept of paradigms in his probabilistic paradigm
model, which has many limitations, such as not han-
dling syncretism or irregular morphology, nor dis-
tinguishing inflection and derivation.
Dreyer and Eisner (2011) learn complete Ger-
man verb paradigms from a small set of complete
seed paradigms (50 or 100), which they choose ran-
domly from all verbs in the language. They model
stem changes using letter-based models, and use
a large unannotated corpus in addition to the seed
paradigms. Durrett and DeNero (2013) attack the
same problem as Dreyer and Eisner (2011). Instead
of using unannotated text, they model explicit rules
for affixes and stem changes. The major difference
between these two efforts and our work is that the
problem is defined differently: we assume that the
training and test data is defined by a corpus, not
by complete paradigms. Our methods therefore are
more sensitive to frequency effects of tokens. We
believe that our way of stating the problem is more
relevant to actual computational challenges for lan-
guages with limited morphological resources. We
empirically compare our approach to that of Durrett
and DeNero (2013) in Section 5.
None of the unsupervised approaches mentioned
model inflectional classes, i.e., meta-paradigmatic
representation that cluster the various paradigms of
different lexemes into a set of general classes of
paradigms. There is no explicit notion of mor-
phosyntactic features. In this paper we target the
learning and completion of inflectional classes from
morphologically annotated data. Our approach does
not sacrifice details of what paradigms should in-
clude: we handle syncretism and stem changes, and
allow for the prediction of new word forms from
morphosyntactic features and lemmas, unlike the
largely unsupervised work. Our work also differs
from most previous work in that we investigate how
to model stem change explicitly. Whereas other
approaches model stem syncretism through letter-
</bodyText>
<page confidence="0.983221">
1033
</page>
<bodyText confidence="0.99965325">
based models (Yarowsky and Wicentowski, 2000;
Neuvel and Fulop, 2002; Dreyer and Eisner, 2011),
we explore the use of abstract stems.
In our previous work on the EGY morphological
analyzer CALIMA, we similarly used a lexicon of
annotated morphological forms and extended it au-
tomatically using a simpler approach to paradigm
completion (Habash et al., 2012).
</bodyText>
<sectionHeader confidence="0.966841" genericHeader="method">
3 Linguistic Terminology
</sectionHeader>
<bodyText confidence="0.999966054054054">
In this section, we review key concepts from mor-
phology, and introduce the terminology we will use
in this paper.2 We then introduce our own formal-
ization of stems using vocalic templates.
Morphology is the study of word forms and their
decomposition into elementary morphemes, which
are the smallest meaning-bearing units of a lan-
guage. There are two types of morphological pro-
cesses: inflectional and derivational morphology. In
inflectional morphology, a core meaning is retained
and different word forms reflect different types of
morphosyntactic features such as person, number, or
tense. In derivational morphology, the core meaning
of a word is changed, and perhaps even its part-of-
speech (POS). In this paper, we restrict our interest
to inflectional morphology. Furthermore, we take
the written form of the word to be primary, and base
all morphological analyses on the written form.
We will refer to the set of all word forms that
are related through inflectional morphology alone as
lexeme. We can refer to a lexeme with a lemma,
which we take to be a language-specific and conven-
tionalized choice of one of the inflected forms. For
example, in English the verb is conventionally cited
in the infinitive (often with to, which can be omit-
ted), while in Arabic it is conventionally cited in per-
fective third person masculine singular. The lemma
is sometimes referred to as a “citation form”. A
paradigm of a lexeme is a list of cells, where a cell
is a combination of a complete set of morphosyn-
tactic features (properties) and the corresponding in-
flected form of the lexeme. A paradigm is com-
plete if there are cells for all possible morphosyn-
tactic features (the list of possible morphosyntactic
features is of course language-dependent).
We can divide the word forms into affixes (i.e.,
prefixes and suffixes) and the stem. There is no sin-
</bodyText>
<footnote confidence="0.50539">
2We (roughly) base our terminology and our conceptualiza-
tion on the inferential-realizational theory of Stump (2001).
</footnote>
<bodyText confidence="0.999347111111111">
gle correct way to do this for the words of a lan-
guage. Each lexeme has its own paradigm. We
can abstract from paradigms by grouping together
paradigms which share the same affixes in corre-
sponding cells, and where stems in corresponding
cells differ in some restricted manner. We can de-
fine the inflectional class (IC) more formally as a
set of abstract cells, where an abstract cell is a com-
bination of a complete set of morphosyntactic fea-
tures (properties) and an abstract representation of a
stem (an abstract stem) along with fully specified
affixes. The abstract stems (and thus the abstract
cells) must have the property that, given a single in-
stantiated word form of a lexeme along with its as-
sociated IC, we can derive the complete paradigm
of the lexeme deterministically. In the first results
we present, we simply assume that the stem is ei-
ther shared entirely with other abstract cells, or it is
entirely lexically instantiated. We explore language-
specific approaches to defining an abstract stem in
Section 5, where we also discuss relevant morpho-
logical facts of EGY and GER.
Prefixes, suffixes, and stems can be the same for
different cells of a single paradigm or IC. This is
called syncretism. We will refer to the cells which
share a stem as a stem syncretism zone or “zone”
for short.
</bodyText>
<sectionHeader confidence="0.769784" genericHeader="method">
4 Language-Independent Inflectional
Class Construction Algorithm
</sectionHeader>
<bodyText confidence="0.99987225">
In this section, we present our language-independent
IC construction algorithm (LICA). LICA consists of
a core algorithm for building ICs from seen data and
models of soft-stem syncretism and affix prediction.
</bodyText>
<subsectionHeader confidence="0.99682">
4.1 Problem Definition
</subsectionHeader>
<bodyText confidence="0.999830125">
Starting with a corpus of words annotated as triples
of (prefix+stem+suffix, lemma, features), we want
to create a lexicon of complete ICs, with each
IC having an associated set of lemmas. The
following is an input example specifying the in-
flected form of the 3rd person plural imperfec-
tive inflection for the EGY lemma katab3 ‘write’:
(y+iktib+uwA, katab, I3UP).
</bodyText>
<footnote confidence="0.9954145">
3Arabic transliteration throughout the paper is presented in
the Habash-Soudi-Buckwalter scheme (Habash et al., 2007).
</footnote>
<page confidence="0.995844">
1034
</page>
<figure confidence="0.995738727272727">
1 2 3
IC1,2,3
IC1 IC1,3
IC1 CS = 2 IC2 L1 – L2 – L3 IC2 L1 – L2 – L3 IC1,2,3 L1 – L2 – L3
L6 – l7 L4 – L5
L6 – l7
IC2
IC2
L4 – L5
IC4
CS = 3 IC3
CS = 2 L4 – L5
IC1,3
L8
CS = 1 L6 – L7
IC4
IC4
IC4 IC4
L8
IC4
L8
IC3
</figure>
<figureCaption confidence="0.9718555">
Figure 1: This graph illustrates two merges that result in combining three ICs in two steps. The IC cells are represented
as solid (filled cells) or blank (empty cells) circles. CS is the compatibility score marking the number of matching
filled cells in an IC. The boxes to the right of the graph represent the lexicon which associates lemmas (L*) with ICs.
IC4 is not connected with any of the other ICs, because it has cell values that are incompatible with them.
</figureCaption>
<subsectionHeader confidence="0.991158">
4.2 Core Algorithm
</subsectionHeader>
<bodyText confidence="0.999014432432432">
Building the Initial Inflectional Classes An ini-
tial IC is constructed for each lemma found in the
input corpus using all the triplets involving said
lemma. Since the lemma itself is an inflected form
with an a priori fixed feature combination, we also
use the lemma to construct the initial IC, even if it
did not occur as a word form in the corpus. If all
inflected forms of a lemma appear in the training
data, the IC will be complete. However, typically
there are many unseen forms. If all seen forms re-
lated to one lexeme have the same stem, the abstract
stem chosen for the IC is a single stem variable;
the abstract cells of the new IC can of course dif-
fer in terms of affixes (which are always fully spec-
ified rather than represented as variables). In cases
when the seen forms of one lemma have more than
one stem, the IC created will simply be the same as
the paradigm, i.e., we have fully instantiated stems
as our abstract stems. We call such ICs suppletive
ICs. At this point, we have a repository of numerous
incomplete ICs and a lexicon consisting of a one-to-
one mapping between lemmas and these new ICs.
We then identify all ICs that are exactly the same (by
definition, these are not suppletive ICs), and merge
the associated sets of lemmas. We now have a new
lexicon in which some ICs are associated with more
than one lemma.
Constructing the Inflectional Class Graph We
construct an IC graph that connects all mergeable
ICs. Two ICs are mergeable if neither of the ICs
is suppletive, if they share at least one abstract cell
for some morphological feature, and if no morpho-
logical features are associated with different abstract
cells, i.e., there are no incompatibilities. Each point
in the graph represents a specific IC, while the edges
carry the following four scores that we use to deter-
mine mergeability order:
</bodyText>
<listItem confidence="0.978537222222222">
• The compatibility score is the number of non-
empty intersections between the two ICs.
• The originality score is the larger number of
previous merges of the two ICs. At the begin-
ning, this number is 0 for all edges.
• The completeness score is the size of the union
of non-empty rows from the two ICs
• The lexical size score is the sum of the number
of lemmas associated with the two ICs.
</listItem>
<bodyText confidence="0.999971818181818">
The edges of the graph are ranked according to the
compatibility score first (more is better). Any ties
are broken using the originality score (lower is bet-
ter), any remaining ties by the completeness score
(more is better) and then by the lexical size score
(more is better). We explored all possible orders of
tie breaking using EGY data, and determined the or-
der of the listing above to be best. We use it in all
experiments reported in this paper.
Merging the Inflectional Classes While the IC
graph is still connected, we repeat the following
merging procedure. Starting with the highest ranked
edge in the graph, we merge the two ICs connected
by the edge. In case of multiple edges that are
equally highly ranked (after tie breaking), we select
randomly among them. The merge creates a new IC
that has the union of the cells in the two ICs. The
lexicon is adjusted accordingly by associating with
the new IC the union of all the lemmas originally as-
sociated with the two ICs. The new IC inherits the
union of all the IC graph connections of its prede-
cessors. The IC graph edge scores between the new
</bodyText>
<page confidence="0.965902">
1035
</page>
<bodyText confidence="0.9997188">
IC and other ICs are recalculated. Graph edges that
become incompatible after the merge are removed.
The two original ICs are removed from the graph
and lexicon. See Figure 1 for an illustration of the
merge process.
</bodyText>
<subsectionHeader confidence="0.999959">
4.3 Completing Stems and Affixes
</subsectionHeader>
<bodyText confidence="0.999652714285714">
Soft Stem Syncretism Zones We extend the con-
cept of stem-syncretism zones into a statistical
model that computes the probability that the abstract
stems in two abstract cells are the same given their
feature combinations, i.e., that they belong to the
same syncretism zone. We refer to this approach
as “soft” stem-syncretism zones or “soft zones” for
short. The probabilities are computed for each
feature-combination pair as the ratio of the times the
abstract stem for the feature-combination pair are
equal divided by the times the abstract stem for the
feature-combination pair are not empty. The prob-
abilities can be computed after the initial IC graph
construction or after the merging process has con-
cluded; they can also be based on IC type counts
or weighted by the number of associated lemmas.
Applying soft zones to fully complete the ICs can
only be done after merging is completed. We try all
the possible alternatives for learning the SZs on the
EGY data, including experiments with small train-
ing data sizes. The best accuracy is obtained using
IC type weights learned after the merge. We use this
setting for all experiments.
When applying the soft zones to determine the
abstract stems of empty cells, we consider all filled
cells in the same IC and select the abstract stem from
the cell of the feature combination that has the high-
est soft-zone probability with the feature combina-
tion of the empty cell. Note that the copied abstract
stem can be either a stem variable, or, for a supple-
tive IC, a lexical form.
Predicting Affixes In building the ICs mentioned
above, some feature combinations and their corre-
sponding affixes (prefixes and suffixes) are missing
since they were not in the training data. We fill the
missing affixes in a particular IC i by copying them
from other ICs that we rank by overall seen affix
similarity to IC i. ICs with conflicting affixes for
any feature combinations are excluded completely.
For any remaining missing affix-feature combina-
tion, we use the most common affix for that feature
combination over all ICs.
</bodyText>
<subsectionHeader confidence="0.996454">
4.4 Model Application
</subsectionHeader>
<bodyText confidence="0.999996">
The complete ICs produced by the completion algo-
rithm, associated with their lemmas in the lexicon,
can now be used to predict the surface form of a
given lemma and morphological features. We use
the following procedure: If the given features are
the same as those used to define the lemma, then the
surface form is the same as the lemma form. If the
lemma was seen in training, we select its IC from the
model and generate the inflection associated with the
features. This may require instantiating a stem vari-
able (in case the IC is not suppletive), but this can be
done deterministically from the lemma.
If the lemma has not been seen, then we build
an IC on the fly using the lemma, i.e., an IC with
a single cell. We then pick an IC from the model
that does not conflict with that IC. The priority is
given to the IC with the largest lexicon size, which
is likely to be a result of several merges. If such an
IC does not exist, a backup mode returns the stem of
the lemma associated with the most frequent affixes
of the queried features.
</bodyText>
<subsectionHeader confidence="0.815861">
4.5 Results on Egyptian Arabic
</subsectionHeader>
<bodyText confidence="0.999600428571429">
Data and Metrics We use a morphologically an-
notated EGY corpus based on the CALLHOME
EGY (CHE) corpus (Gadalla et al., 1997).4 We di-
vide the corpus into three parts: training, develop-
ment and test, of about 75K, 36K and 41K words,
respectively. We conduct our experiments on verbs
only since they have a large number of possible mor-
phosyntactic feature combinations. The verbs in
these experiments are uncliticized. Clitics are eas-
ily handled using a few orthographic rules (El Kholy
and Habash, 2010). On average, uncliticized verbs
are about 12% of all words in our corpus. The data
is represented in triplets as described in Section 4.1.
There is a total of 19 feature combinations of as-
pect/mood (perfective,imperfective and imperative),
person (first, second and third), gender (masculine,
feminine and neutral) and number (singular and plu-
ral). Some combinations are invalid such as the first
and third persons with the imperative form. The
lemma we use is the Arabic citation form for verbs,
which is the perfective third person masculine sin-
</bodyText>
<footnote confidence="0.89379825">
4The corpus was automatically annotated using information
from the CHE transcripts (Gadalla et al., 1997) and the Egyp-
tian Colloquial Arabic Lexicon (Kilany et al., 2002). For more
details, see Habash et al. (2012) and Eskander et al. (2013).
</footnote>
<page confidence="0.992325">
1036
</page>
<bodyText confidence="0.998416161290323">
gular (P3MS) inflection. We use this fact to fill the
P3MS cells when building the initial ICs.
We evaluate the accuracy of the automatically
generated bidirectional lexicon by generating sur-
face forms from lemmas and morphosyntactic fea-
tures. We use the model application method de-
scribed in Section 4.4.
Baseline Our baseline system consists of two
steps. First, we check the features. All cases of cita-
tion form features (in the case of EGY, P3MS) return
the lemma as the inflected form. Otherwise, we look
up the lemma and feature pair among all triplets in
the training data and return the inflected form if such
a triplet was found. If not, the baseline does not re-
turn an answer.
Results The results on tokens are summarized in
Table 2 under the column heading BL (baseline) and
NOTMP (LICA with no stem template). Our sys-
tem consistently improves over the baseline for all
training data sizes explored.
Error Analysis We performed an error analysis
using the best settings found on the development
set. We found that in 46% of the error types the
lemma is unseen. Additional 35% of the cases are
due to stem templates that are unseen in the com-
plete ICs although their corresponding lemmas are
seen. About one tenth of the cases are because of
the existence of multiple forms of the same lemma
and feature combinations, where our system assigns
a form that is different from the gold form. Finally,
gold errors contribute to about 9% of all errors.
</bodyText>
<subsectionHeader confidence="0.838539">
4.6 Results on German
</subsectionHeader>
<bodyText confidence="0.999895787878788">
Data and Metrics For our experiments, we use
the TIGER corpus (Brants and Hansen, 2002). We
divide the corpus into three parts: training, develop-
ment and test, of about 709K, 143K and 37K words,
respectively. However, we use the first 75K words
in training and the first 36K words in development
to have the results comparable to EGY. We conduct
our experiments on verbs only. We disregard any
verbs with separable prefixes (as is common in work
in morphology learning). On average, verbs with-
out separable prefixes are about 9% of all words in
our corpus. The data is represented in triplets as de-
scribed in Section 4.1. There are 28 feature com-
binations of tense (present, past), person (first, sec-
ond and third), number (singular, plural), and mood
(indicative, subjunctive, imperative, past participle,
infinitive). Some combinations are invalid such as
any tense with the non-tensed participle or infinitive.
The infinitive is the lemma. We use the same metrics
as for EGY (Section 4.5).
Results The results are summarized in Table 5,
with the relevant results in the column NOTMP. We
see that our algorithm performs substantially better
than the baseline at all training set sizes, with greater
relative error reductions at smaller training sizes.
Error Analysis We inspected all error types in the
development set and found that nearly 8% of all er-
ror types are errors in the gold annotation of the de-
velopment set, and in 4% of cases, our predicted
form is correct because a lemma is shared by two
verbal paradigms (for example, werden has different
past participles depending on whether it is the pas-
sive auxiliary or the verb for ‘become’).
</bodyText>
<sectionHeader confidence="0.997274" genericHeader="method">
5 Language-Specific Modeling of Stems
</sectionHeader>
<subsectionHeader confidence="0.96541">
5.1 General Approach
</subsectionHeader>
<bodyText confidence="0.999859814814815">
We model the abstract stems using the notions of or-
thographic template and orthographic root. To
meet our definition of IC given above, we define
these two notions so that the root and template can
always be extracted deterministically. We define
them simply in terms of sets of letters: the set of
letters of the alphabet used to write the language
we are modeling is partitioned into the root let-
ters and the pattern letters. The orthographic tem-
plate is a string that specifies the template letters and
that has placeholders for the root letters, which we
write as ‘❑’. The orthographic root is a sequence
of strings that specifies the root letters that can fill
a vocalic template’s placeholders, in the order spec-
ified. Note that an IC along with a root is equiv-
alent to a paradigm, since the root can be inserted
deterministically into the abstract stem. This gives
us another way to specify a lexeme: since a com-
plete paradigm enumerates all inflected forms of a
lexeme, and since a complete IC along with a root
defines a complete paradigm, we can specify a lex-
eme to be a pair consisting of an IC along with a
root. This pair determines a complete mapping from
morphosyntactic features to surface word forms for
the lexeme.
The basic algorithm discussed earlier is modified
as follows: when we read in the training data, the
</bodyText>
<page confidence="0.981696">
1037
</page>
<table confidence="0.900361978723404">
EGY Inflectional Class (IC) Repository
P1US IC-1 IC-2 IC-3
P1UP
P2MS
P2FS
P2UP
P3MS
P3FS
P3UP
I1US
I1UP
I2MS
I2FS
I2UP
I3MS
I3FS
I3UP
C2MS
C2FS
C2UP
❑a❑a❑+t ❑a❑∼+ayt ❑u❑+t
❑a❑a❑+nA ❑a❑∼+aynA ❑u❑+nA
❑a❑a❑+t ❑a❑∼+ayt ❑u❑+t
❑a❑a❑+tiy ❑a❑∼+aytiy ❑u❑+tiy
❑a❑a❑+tuwA ❑a❑∼+aytuwA ❑u❑+tuwA
❑a❑a❑
❑a❑a❑+it
❑a❑a❑+uwA
❑a❑∼ ❑A❑
❑a❑∼+it ❑A❑+it
❑a❑∼+uwA ❑A❑+uwA
Aa+❑❑i❑ Aa+❑i❑∼ Aa+❑uw❑
ni+❑❑i❑ ni+❑i❑∼ ni+❑uw❑
ti+❑❑i❑ ti+❑i❑∼ ti+❑uw❑
ti+❑❑i❑+iy ti+❑i❑∼+iy ti+❑uw❑+iy
ti+❑❑i❑+uwA ti+❑i❑∼+uwA ti+❑uw❑+uwA
yi+❑❑i❑ yi+❑i❑∼ yi+❑uw❑
ti+❑❑i❑ ti+❑i❑∼ ti+❑uw❑
yi+❑❑i❑+uwA yi+❑i❑∼+uwA yi+❑uw❑+uwA
Ai❑❑i❑ ❑i❑∼ ❑uw❑
Ai❑❑i❑+iy ❑i❑∼+iy ❑uw❑+iy
Ai❑❑i❑+uwA ❑i❑∼+uwA ❑uw❑+uwA
EGY Lexicon
IC-1 IC-2 IC-3
katab faraD mad∼ Hal∼ ˇsAf qAl
kk,t,bk kf,r,Dk km,dk kH,lk kˇs,fk kq,lk
‘write’ ‘suppose’ ‘extend’ ‘solve’ ‘see’ ‘say’
</table>
<tableCaption confidence="0.859190333333333">
Table 1: Example of three inflectional classes and associ-
ated lemmas with their roots in EGY. The various blocks
in the IC table specify different syncretism zones.
</tableCaption>
<bodyText confidence="0.9998135">
root of the lemma is used to decompose the stem of
the inflected form into a root and a template.
</bodyText>
<subsectionHeader confidence="0.999902">
5.2 Choosing Root and Pattern Letters
</subsectionHeader>
<bodyText confidence="0.999965974358974">
Determining which letters count as root letters and
which count as pattern letters is language-specific.
In this paper, we use three approaches to choosing
root and pattern letters.
No Template (NOTMP) In this approach, we do
not model the stem at all, i.e., there are no pattern
letters. As a result, the ICs cannot express stem
changes; any verb with a stem change will be an
irregular verb and it will be modeled with a sup-
pletive IC. Note that in EGY, every verb manifests
a stem change between the perfective and imperfec-
tive forms, while in GER many verbs in fact do have
the same stem for all inflected forms. The results of
the NOTMP approach are the same basic approach
results already presented in Section 4.
Scholar (SCHLR) In this approach, we use lin-
guistic scholarship and intuition to choose a set of
letters to be the pattern letters. These are letters
which we know can change in stems within the
same IC. We discuss this approach for EGY in Sec-
tion 5.3.1 and for GER Section 5.4.1.
Empirical (EMPR) In this approach, we obtain
the set of pattern letters empirically. We align the
letters of all the stems in the development corpus
with the stem letters of their corresponding lemmas.
This allows us to identify which letters change be-
tween lemma stem and inflected stem. We order the
letters by the probability that a letter changes given
all occurrences of that letter, and by the probabil-
ity that a letter changes given all changes that oc-
cur. This gives us two rankings. For both rank-
ings and for each letter that changes, we construct
a set of letters by including all letters from the most
highly ranked letter down to the letter under con-
sideration. This gives us per ranking as many sets
as there are letters that change. We then choose the
best performing set among all sets generated by both
rankings. We present the results of this approach for
EGY in Section 5.3.1 and for GER in Section 5.4.1.
</bodyText>
<subsectionHeader confidence="0.988051">
5.3 Egyptian Arabic
5.3.1 Choice of Pattern Letters
</subsectionHeader>
<bodyText confidence="0.99965875">
Scholar-based Pattern Letters For the EGY
SCHLR approach, we selected the following pattern
II AAA�¯Awwyyy’aui—. These
letters cover all the so-called weak root radicals,
Hamzated forms and diacritics that are often used
to discuss different verbal paradigms in Arabic
(Gadalla, 2000). Table 1 shows three EGY inflec-
tional classes. We follow the standard practice in
modeling Arabic morphology of assuming that each
placeholder in an orthographic template corresponds
to exactly one letter from the orthographic root (i.e.,
the root is a sequence of strings each of length one).
We would like to stress that our orthographic tem-
plate and root differ from the notions of pattern and
root in Semitic morphology (for an overview, see
(Habash, 2010)): while for katab ‘write’ the or-
thographic root as well as the “real” root is 11k,t,b11,
the orthographic root for xal—a´y ‘let’ is 11x,l11 (as
opposed to the real root 11x,l,y11). Henceforth, we
will omit the adjective “orthographic”, since we will
not talk about the “real” root. To inflect a lemma
for a particular set of features, we combine the root
with the ❑ slots in the corresponding inflectional
class feature row, e.g., Hal— + P2FS ==&gt;- 11H,l11 +
</bodyText>
<equation confidence="0.584116">
�
letters: ��������� ����� ��� II
</equation>
<page confidence="0.968575">
1038
</page>
<bodyText confidence="0.970167375">
❑a❑-+aytiy ==&gt;- Hal-aytiy. Note that in this paper,
we do not use any rules; all regular phonological and
orthographic variation is “compiled” into the ICs.
We also see examples of stem syncretism zones in
Table 1; they are marked with horizontal lines. The
stems associated with P3MS, P3FS and P3UP hap-
pen to be the same within each IC (although differ-
ent across ICs). Different ICs have different zones:
e.g., IC-1 has one zone for the P*** features, while
IC-2 and IC-3 have two identical zones for P3** and
P[12]**.
Empirically Determined Pattern Letters For
EGY, the EMPR approach, using the algorithm
described in Section 5.2, yields the following
�
optimal set of pattern letters: ������ ��� ����� �I�
</bodyText>
<sectionHeader confidence="0.569235" genericHeader="method">
II
</sectionHeader>
<bodyText confidence="0.998380714285714">
AAAwyyy’Baui. This set is very similar to the
SCHLR set except in that it omits the lexically
constant (per lexeme) Shadda diacrtic = - (ortho-
graphic gemination marker), and some very infre-
quent Hamzated forms; and it also adds one letter
u 0 as a result of orthographic inconsistency in the
training data.
</bodyText>
<table confidence="0.999538888888889">
Corpus Verb BL NOTMP SCHLR EMPR EMPR
Size Count IIC+HZ
0K 0 19.3 20.0 20.0 20.0 58.8
1K 95 51.8 64.2 68.3 68.1 83.3
5K 630 64.2 77.0 83.9 83.9 91.1
10K 1,201 70.4 84.3 89.4 89.1 92.4
25K 2,994 79.6 91.5 94.4 94.8 95.6
50K 5,966 84.5 94.2 96.9 96.7 97.2
75K 8,690 85.6 94.9 97.5 97.6 97.2
</table>
<tableCaption confidence="0.96029">
Table 2: Learning curve comparing system performance
for EGY on tokens (development set).
</tableCaption>
<bodyText confidence="0.998633210526316">
Results The template approaches SCHLR and
EMPR consistently beat NOTMP which does not
make use of any templatic stem modeling (see Ta-
ble 2). However, SCHLR and EMPR perform about
equally. This is not surprising since the two sets of
pattern letters are quite similar.
Error Analysis We conducted an error analysis of
EMPR using the best settings found on the develop-
ment set. About 86% of the error types in NOTMP
where the lemma is unseen are solved after intro-
ducing EMPR. Additionally, 71% of the cases in
NOTMP where the stem template is unseen and the
lemma is seen are solved. However, 7% of the error
types in EMPR are not present in NOTMP, and they
are all cases where the stem template is unseen while
the lemma is seen. Errors due to multiple stem forms
and gold errors remain the same in both NOTMP and
EMPR, contributing to 26% and 23%, respectively,
of all the error types in EMPR.
</bodyText>
<subsectionHeader confidence="0.4463005">
5.3.2 Other Enhancements with Linguistic
Knowledge
</subsectionHeader>
<bodyText confidence="0.999960075">
Other linguistic knowledge can also help enrich
the process of IC learning, especially under limited
annotation conditions. We use the following two
types of linguistic knowledge, which seamlessly in-
tegrate into the core merging algorithm described
above.
Iconic Inflectional Classes (IICs) are ICs that are
manually fully annotated, i.e., they have all the tem-
plate cells for all morphosyntactic features specified.
IICs are treated like any other ICs when constructing
the initial IC graph. They are different from other
ICs in that they initially have no lemmas associated
with them in the lexicon.
Hard Stem-Syncretism Zones (HZ) are stem-
syncretism zones determined manually by linguists
to hold for all ICs. As such they can be more fine-
grained than is needed to describe individual ICs. A
hard zone is not applied in case of any partial dis-
agreement within it. Unlike soft zones, they could
be applied before or after the merge process, and
they do not guarantee that the ICs will be completely
filled.
We conducted experiments where we added
external linguistic knowledge to our training data.
We added 112 IICs which are extracted from all
EGY verb inflections listed in a reference grammar
of EGY (Gadalla, 2000). Also we added the
following eight HZs for EGY (with reference to
features &lt;tense, person, gender, number&gt;):
(P1US-P1UP-P2MS-P2FS-P2UP), (I1UP-I2MS-
I3MS-I3FS), (I2FS-I2UP-I3UP), (P3FS-P3UP),
(C2FS-C2UP), (P3MS), (I1US), and (C2MS).
Applying the HZs on the completed ICs (before soft
zone application) gives higher results than applying
them on initial ICs. We only report below on the
setting of applying HZs after IC merge completion.
We present the accuracies of IC learning for the
baseline, and for the best setup with and without
IICs and HZs, for different training sizes in Table 2.
The baseline with no training data is at 19.3%
</bodyText>
<page confidence="0.993126">
1039
</page>
<bodyText confidence="0.99977675">
because of all the cases with verbs appearing in
the citation form. As expected, IICs always help
improve accuracy, especially under limited (and
no) data conditions. However the benefits diminish
rapidly for larger training sets. When evaluating on
types only (results not presented in this paper), we
find that using IICs in our system with no data is
better than using the baseline with 75K words.
</bodyText>
<subsectionHeader confidence="0.835468">
5.3.3 Blind Test Set
</subsectionHeader>
<bodyText confidence="0.998290583333333">
Table 3 shows the accuracies the different sys-
tems on our blind test set. The results for the test
set are lower than those of the development set, but
the trends are the same. We also compare our re-
sults to those obtained using the system of Durrett
and DeNero (2013) on the same test data. Note that
we apply their system to our problem – predicting
unseen forms from annotated corpora (i.e., incom-
plete paradigms), not to the problem for which they
created their system – predicting unseen forms from
complete paradigms. Our best system outperforms
theirs by 2.8% absolute in accuracy.
</bodyText>
<table confidence="0.998062">
System Accuracy Error Reduction
Baseline 84.7
NOTMP 91.5 44.4
SCHLR 93.2 55.6
EMPR 93.2 55.6
Durrett &amp; DeNero 90.4 37.3
</table>
<tableCaption confidence="0.998427">
Table 3: Results for EGY on tokens using a blind test set.
</tableCaption>
<subsectionHeader confidence="0.973575">
5.4 German
5.4.1 Choice of Pattern Letters
</subsectionHeader>
<bodyText confidence="0.9746000625">
Scholar-based Pattern Letters We now discuss
our scholarship-based choice of pattern letters for
German. Like Arabic, German verb paradigms
can show stem changes which are typically vowel
changes. Furthermore, like Arabic, German has
prefixes, suffixes, and circumfixes. However, un-
like Arabic, German has many verbs (called “weak
verbs”) which are regular in the sense that they
show no stem change at all. The irregular verbs,
or “strong verbs”, show many different patterns of
stem changes. Another difference to Arabic is that
the affixes are not the same for all verb paradigms.
In particular, the weak verbs form several inflec-
tional classes (which, of course, differ only in af-
fixes). Finally, unlike Arabic, in the strong verbs
the orthographic root does not necessarily consist
</bodyText>
<table confidence="0.999236238095238">
GER Inflectional Class (IC) Repository
IC-1 IC-2 IC-3
PI1S ❑o❑ +e ❑e❑ +e ❑e❑ +e
PI2S ❑o❑ +st ❑ie❑ +st ❑i❑ +st
PI3S ❑o❑ +t ❑ie❑ +t ❑i❑ +t
PI1P ❑o❑ +en ❑e❑ +en ❑e❑ +en
PI2P ❑o❑ +t ❑e❑ +t ❑e❑ +t
PI3P ❑o❑ +en ❑e❑ +en ❑e❑ +en
PS1S ❑o❑ +e ❑e❑ +e ❑e❑ +e
PS2S ❑o❑ +est ❑e❑ +est ❑e❑ +est
XI1S ❑o❑ +te ❑a❑ + ❑a❑ +
XI2S ❑o❑ +est ❑a❑ +st ❑a❑ +st
XS1S ❑o❑ +te ❑¨a❑ +e ❑¨a❑ +e
XS2S ❑o❑ +test ❑¨a❑ +est ❑¨a❑ +est
PP ge+ ❑o❑ +t ge+ ❑e❑ +en ❑e❑ +en
INF ❑o❑ +en ❑e❑ +en ❑e❑ +en
GER Lexicon
IC-1 IC-2 IC-3
holen sohlen sehen lesen vergeben begeben
kh,lk ks,hlk ks,hk kl,sk kverg,bk kbeg,bk
‘fetch’ ‘sole’ ‘see’ ‘read’ ‘forgive’ ‘occur’
</table>
<tableCaption confidence="0.7407446">
Table 4: Example of three inflectional classes (some cells
omitted in the interest of space economy) and associated
lemmas with their roots in German (“X” stands for past
tense). The different blocks in the IC table specify differ-
ent stem syncretism zones.
</tableCaption>
<bodyText confidence="0.97827024">
of sequences of single letters: the strong verbs have
monosyllabic stems (plus perhaps derivational mor-
phology), with the vowel in this stem potentially un-
dergoing changes. However, the onset and coda of
the stem syllable can be any consonant cluster al-
lowed by German phonology. Thus, in German, we
model roots as pairs of strings of any length (which
represent the onset and coda of the stem syllable).
Table 4 shows a weak IC and two strong ICs.
Since German strong verbs can have any stem
vowel, we assume that all eight vowel letters of Ger-
man (aeioudadodu) are pattern letters, and all other con-
sonant letters are root letters. German weak verbs
show no stem changes at all; if we tailored our tem-
plates to them, we would define all letters to be root
letters, and there would be no pattern letters at all.
This is in fact the experiment we reported on in Sec-
tion 4.6, and whose results are shown as NOTMP
in Table 5. However, since we do not know dur-
ing training time whether a verb is weak or strong,
all verbs will be modeled with an orthographic tem-
plate, even though there is no stem change at all.
Empirically Determined Pattern Letters For
GER, the EMPR approach, using the algorithm de-
scribed in Section 5.2, yields oadaß as the optimal
</bodyText>
<page confidence="0.972257">
1040
</page>
<bodyText confidence="0.99990728125">
set of pattern letters. The EMPR pattern letters dif-
fer from the SCHLR pattern letters by omitting five
vowels, but including the ß variant of the s.
Results In table 5 we see that using all eight vow-
els as pattern letters (SCHLR column) in fact de-
creases performance at every training size (and rel-
atively more at smaller training sizes). However,
if we use the empirically obtained pattern letter set
oa¨aß, we see that we perform better than SCHLR
at almost all training sizes, and slightly better than
NOTMP at larger training sizes.
Error Analysis A manual inspection of all devel-
opment error types again revealed 8% development
set annotation errors and 4% acceptable variations.
To investigate why NOTMP outperforms SCHLR
for German on the development set, we performed
an oracle experiment: we assumed we knew for each
seen verb in training whether it is a weak or a strong
verb. If it is weak, we model it using NOTMP, and if
it is strong, using SCHLR. We observe as expected
that the performance on weak verbs is very similar to
that obtained using NOTMP on all verbs. However,
for the strong verbs, it is only when we have more
than 75,000 words of training data that the oracle
outperforms NOTMP. We assume that the reason is
that the highly frequent verbs are strong, but occur
frequently enough so that their IC can be learned di-
rectly from the training data. Using SCHLR simply
adds noise for these very frequent verbs. It is only
for the less frequent strong verbs that SCHLR can
contribute, and then only when a large amount of
training data is available.
</bodyText>
<subsectionHeader confidence="0.656111">
5.4.2 Blind Test Set
</subsectionHeader>
<bodyText confidence="0.995109733333333">
Table 6 shows the accuracies the different sys-
tems on our blind test set. The results for the test
set are lower than those of the development set, and
NOTMP, SCHLR,and EMPR produce very simi-
lar accuracy results. We also compare our results
to those obtained by running the system of Durrett
and DeNero (2013) on the same training and test
data. Our system outperforms Durrett and DeNero
(2013)’s system reducing the error of by 5%.5
5We also tested our system on Durrett and DeNero (2013)’s
problem definition and data, training on 200 GER paradigms,
and testing on 200 unseen paradigms. This is the case of testing
for unseen lemmas in our system. Our system gives an accu-
racy of 88.4% as opposed to 91.8% as reported by Durrett and
DeNero (2013). Our system was not designed for this task.
</bodyText>
<table confidence="0.999336333333333">
Corpus Verb BL NOTMP SCHLR EMPR
Size Count
0K 0 13.7 25.2 25.2 25.2
1K 81 43.3 72.0 67.0 69.4
5K 461 64.8 83.5 83.0 83.1
10K 929 72.1 89.0 87.4 88.6
25K 2,362.0 79.5 93.6 93.0 92.4
50K 4,527 86.2 95.6 95.1 95.6
75K 6,728 88.9 96.8 96.2 97.0
</table>
<tableCaption confidence="0.9165195">
Table 5: Learning curve comparing system performance
for GER on tokens on DEV corpus. BL=Baseline
</tableCaption>
<table confidence="0.999881">
System Accuracy Error Reduction
Baseline 89.5
NOTMP 96.5 66.7
SCHLR 96.5 66.7
EMPR 96.5 66.7
Durrett 96.3 64.8
</table>
<tableCaption confidence="0.985651">
Table 6: Results for GER on tokens using a blind test set.
</tableCaption>
<sectionHeader confidence="0.993632" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999974933333333">
We presented a method for automatically learn-
ing inflectional classes and associated lemmas from
morphologically annotated corpora. In the future,
we plan to improve several aspects of our mod-
els, in particular, using more powerful language-
independent template transformations to automati-
cally optimize for stem and affix modeling. We
plan to take the insights from this paper and ap-
ply them to new dialects and languages with lim-
ited resources. We are interested in extending our
approach to languages with different morphological
systems, e.g., agglutinative or reduplicative. We will
explore ideas from unsupervised morphology learn-
ing to minimize the need for morphological annota-
tions.
</bodyText>
<sectionHeader confidence="0.943512" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999007571428571">
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-12-C-0014.
Any opinions, findings and conclusions or recom-
mendations expressed in this paper are those of the
authors and do not necessarily reflect the views of
DARPA.
</bodyText>
<page confidence="0.9915">
1041
</page>
<sectionHeader confidence="0.996258" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999597735849057">
Sabine Brants and Silvia Hansen. 2002. Developments
in the tiger annotation scheme and their realization in
the corpus. In In Proceedings of the Third Conference
on Language Resources and Evaluation LREC-02. Las
Palmas de Gran Canaria, pages 1643–1649.
Tim Buckwalter. 2004. Buckwalter Arabic Morpho-
logical Analyzer Version 2.0. LDC catalog number
LDC2004L02, ISBN 1-58563-324-0.
Maris Camilleri. 2011. Island morphology: Morphol-
ogy’s interactions in the study of stem patterns. Lin-
guistica, 51:65–84. Internal and External Boundaries
of Morphology.
Burcu Can and Suresh Manandhar. 2012. Probabilistic
hierarchical clustering of morphological paradigms.
EACL 2012, page 654.
Erwin Chan. 2006. Learning probabilistic paradigms for
morphology in a latent class model. In Proceedings of
the Eighth Meeting of the ACL Special Interest Group
on Computational Phonology at HLT-NAACL, pages
69–78.
Lionel Cl´ement, Benoit Sagot, and Bernard Lang. 2004.
Morphology based automatic acquisition of large-
coverage lexica. In LREC 04, pages 1841–1844.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing (TSLP), 4(1).
Silviu Cucerzan and David Yarowsky. 2002. Boot-
strapping a multilingual part-of-speech tagger in one
person-day. In Proceedings of the 6th conference on
Natural language learning-Volume 20, pages 1–7.
Gr´egoire D´etrez and Aarne Ranta. 2012. Smart
paradigms and the predictability and complexity of in-
flectional morphology. EACL 2012, page 645.
Markus Dreyer and Jason Eisner. 2011. Discover-
ing morphological paradigms from plain text using a
dirichlet process mixture model. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 616–627.
Greg Durrett and John DeNero. 2013. Supervised Learn-
ing of Complete Morphological Paradigms. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies (NAACL-
HLT), Atlanta, GA.
Ahmed El Kholy and Nizar Habash. 2010. Techniques
for Arabic morphological detokenization and ortho-
graphic denormalization. In Proceedings of LREC-
2010, May.
Ramy Eskander, Nizar Habash, Ann Bies, Seth Kulick,
and Mohamed Maamouri. 2013. Automatic correc-
tion and extension of morphological annotations. In
Proceedings of the 7th Linguistic Annotation Work-
shop and Interoperability with Discourse, pages 1–
10, Sofia, Bulgaria, August. Association for Compu-
tational Linguistics.
Raphael Finkel and Gregory Stump. 2002. Generating
Hebrew Verb Morphology by Default Inheritance Hi-
erarchies. In Proceedings of the Workshop on Compu-
tational Approaches to Semitic Languages, pages 9–
18.
Markus Forsberg, Harald Hammarstr¨om, and Aarne
Ranta. 2006. Morphological lexicon extraction from
raw text data. Advances in Natural Language Process-
ing, pages 488–499.
Hassan Gadalla, Hanaa Kilany, Howaida Arram, Ashraf
Yacoub, Alaa El-Habashi, Amr Shalaby, Krisjanis
Karins, Everett Rowson, Robert MacIntyre, Paul
Kingsbury, David Graff, and Cynthia McLemore.
1997. CALLHOME Egyptian Arabic Transcripts. In
Linguistic Data Consortium, Philadelphia.
Hassan Gadalla. 2000. Comparative Morphology of
Standard and Egyptian Arabic. LINCOM EUROPA.
Nizar Habash and Owen Rambow. 2006. MAGEAD: A
morphological analyzer and generator for the Arabic
dialects. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 681–688, Sydney, Australia.
Nizar Habash, Owen Rambow, and George Kiraz. 2005.
Morphological Analysis and Generation for Arabic
Dialects. In Proceedings of the ACL Workshop
on Computational Approaches to Semitic Languages,
pages 17–24, Ann Arbor, Michigan.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den Bosch
and A. Soudi, editors, Arabic Computational Mor-
phology: Knowledge-based and Empirical Methods.
Springer.
Nizar Habash, Ramy Eskander, and Abdelati Hawwari.
2012. A Morphological Analyzer for Egyptian Ara-
bic. In Proceedings of the Twelfth Meeting of the Spe-
cial Interest Group on Computational Morphology and
Phonology, pages 1–9, Montr´eal, Canada.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan &amp; Claypool Publish-
ers.
Harald Hammarstr¨om and Lars Borin. 2011. Unsuper-
vised learning of morphology. Computational Lin-
guistics, 37(2):309–350.
H. Kilany, H. Gadalla, H. Arram, A. Yacoub, A. El-
Habashi, and C. McLemore. 2002. Egyptian
Colloquial Arabic Lexicon. LDC catalog number
LDC99L22.
Kimmo Koskenniemi. 1983. Two-Level Model for Mor-
phological Analysis. In Proceedings of the 8th In-
</reference>
<page confidence="0.805394">
1042
</page>
<reference confidence="0.9991756875">
ternational Joint Conference on Artificial Intelligence,
pages 683–685.
Christian Monson, Jaime Carbonell, Alon Lavie, and Lori
Levin. 2008. Paramor: Finding paradigms across
morphology. Advances in Multilingual and Multi-
modal Information Retrieval, pages 900–907.
Sylvain Neuvel and Sean A Fulop. 2002. Unsuper-
vised learning of morphology without morphemes. In
Proceedings of the ACL-02 workshop on Morphologi-
cal and phonological learning-Volume 6, pages 31–40.
Association for Computational Linguistics.
Matthew G Snover, Gaja E Jarosz, and Michael R Brent.
2002. Unsupervised learning of morphology using a
novel directed search algorithm: Taking the first step.
In Proceedings of the ACL-02 workshop on Morpho-
logical and phonological learning-Volume 6, pages
11–20.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. In Proceedings of ACL-08: HLT, pages 737–
745, Columbus, Ohio, June.
Gregory T. Stump. 2001. Inflectional Morphology. A
Theory of Paradigm Structure. Cambridge Studies in
Linguistics. Cambridge University Press.
G´eraldine Walther. 2011. Measuring morphological
canonicity. Linguistica, 51:157–180. Internal and Ex-
ternal Boundaries of Morphology.
David Yarowsky and Richard Wicentowski. 2000. Min-
imally supervised morphological analysis by multi-
modal alignment. In Proceedings of the 38th Annual
Meeting on Association for Computational Linguis-
tics, pages 207–216.
</reference>
<page confidence="0.972086">
1043
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.712052">
<title confidence="0.9574015">Automatic Extraction of Morphological from Morphologically Annotated Corpora</title>
<author confidence="0.995439">Ramy Eskander</author>
<author confidence="0.995439">Nizar Habash</author>
<author confidence="0.995439">Owen</author>
<affiliation confidence="0.999731">Center for Computational Learning</affiliation>
<address confidence="0.79448">Columbia</address>
<abstract confidence="0.998286615384615">We present a method for automatically learning inflectional classes and associated lemmas from morphologically annotated corpora. The method consists of a core languageindependent algorithm, which can be optimized for specific languages. The method is demonstrated on Egyptian Arabic and German, two morphologically rich languages. Our best method for Egyptian Arabic provides an error reduction of 55.6% over a simple baseline; our best method for German achieves a 66.7% error reduction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Silvia Hansen</author>
</authors>
<title>Developments in the tiger annotation scheme and their realization in the corpus. In</title>
<date>2002</date>
<booktitle>In Proceedings of the Third Conference on Language Resources and Evaluation LREC-02. Las Palmas de Gran Canaria,</booktitle>
<pages>1643--1649</pages>
<contexts>
<context position="23378" citStr="Brants and Hansen, 2002" startWordPosition="3880" endWordPosition="3883">error analysis using the best settings found on the development set. We found that in 46% of the error types the lemma is unseen. Additional 35% of the cases are due to stem templates that are unseen in the complete ICs although their corresponding lemmas are seen. About one tenth of the cases are because of the existence of multiple forms of the same lemma and feature combinations, where our system assigns a form that is different from the gold form. Finally, gold errors contribute to about 9% of all errors. 4.6 Results on German Data and Metrics For our experiments, we use the TIGER corpus (Brants and Hansen, 2002). We divide the corpus into three parts: training, development and test, of about 709K, 143K and 37K words, respectively. However, we use the first 75K words in training and the first 36K words in development to have the results comparable to EGY. We conduct our experiments on verbs only. We disregard any verbs with separable prefixes (as is common in work in morphology learning). On average, verbs without separable prefixes are about 9% of all words in our corpus. The data is represented in triplets as described in Section 4.1. There are 28 feature combinations of tense (present, past), perso</context>
</contexts>
<marker>Brants, Hansen, 2002</marker>
<rawString>Sabine Brants and Silvia Hansen. 2002. Developments in the tiger annotation scheme and their realization in the corpus. In In Proceedings of the Third Conference on Language Resources and Evaluation LREC-02. Las Palmas de Gran Canaria, pages 1643–1649.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Buckwalter</author>
</authors>
<date>2004</date>
<booktitle>Buckwalter Arabic Morphological Analyzer Version 2.0. LDC catalog number LDC2004L02, ISBN</booktitle>
<pages>1--58563</pages>
<contexts>
<context position="4108" citStr="Buckwalter, 2004" startWordPosition="617" endWordPosition="618">rk (Section 2) and introduce the key linguistic concepts we use (Section 3). We present our basic language-independent method in Section 4, and our language-specific modeling of stem variation in Section 5. 1032 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1032–1043, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Approaches to Morphological Modeling Much work has been done in the area of computational morphology ranging from systems painstakingly designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012) to unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Hammarstr¨om and Borin, 2011; Dreyer and Eisner, 2011). There is a large continuum between these two approaches. Closer to one end, we find work on minimally supervised methods for morphology learning that make use of available resources such as parallel data, dictionaries or some additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2</context>
</contexts>
<marker>Buckwalter, 2004</marker>
<rawString>Tim Buckwalter. 2004. Buckwalter Arabic Morphological Analyzer Version 2.0. LDC catalog number LDC2004L02, ISBN 1-58563-324-0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maris Camilleri</author>
</authors>
<title>Island morphology: Morphology’s interactions in the study of stem patterns.</title>
<date>2011</date>
<booktitle>Linguistica, 51:65–84. Internal and External Boundaries of Morphology.</booktitle>
<contexts>
<context position="5417" citStr="Camilleri, 2011" startWordPosition="814" endWordPosition="815">h limited lexicons that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). The work presented in this paper falls in the middle of this continuum: we are interested in learning complete morphological models using rich morphological annotations and, optionally, limited linguistic knowledge. We compare the value of different amounts of annotation and how they relate to additional linguistic knowledge. Morphological Paradigms Many traditional and modern theories of inflectional morphology organize natural language morphology by paradigms (Stump, 2001; Walther, 2011; Camilleri, 2011). Within the continuum we discussed above, we find hierarchical representations of paradigm knowledge that have been used in manually constructed morphological models (Finkel and Stump, 2002; Habash et al., 2005). Furthermore, D´etrez and Ranta (2012) introduce an implementation of Smart Paradigms – heuristically organized paradigms minimizing the number of forms needed to predict the full paradigm of a particular lexeme. Both Forsberg et al. (2006) and Cl´ement et al. (2004) describe methods for automatically populating a lexicon from raw data given a set of morphological inflectional classes</context>
</contexts>
<marker>Camilleri, 2011</marker>
<rawString>Maris Camilleri. 2011. Island morphology: Morphology’s interactions in the study of stem patterns. Linguistica, 51:65–84. Internal and External Boundaries of Morphology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burcu Can</author>
<author>Suresh Manandhar</author>
</authors>
<title>Probabilistic hierarchical clustering of morphological paradigms.</title>
<date>2012</date>
<booktitle>EACL 2012,</booktitle>
<pages>654</pages>
<contexts>
<context position="6444" citStr="Can and Manandhar (2012)" startWordPosition="977" endWordPosition="980">f a particular lexeme. Both Forsberg et al. (2006) and Cl´ement et al. (2004) describe methods for automatically populating a lexicon from raw data given a set of morphological inflectional classes in a language. Our work differs in that we use annotated data, but do not start with a complete set of inflectional classes; thus, our work is exactly complementary to this work. The concept of a paradigm is also used in many published efforts on unsupervised learning of morphology, although not always in a way consistent with its use in linguistics. For instance, Snover et al. (2002) (and later on Can and Manandhar (2012)) define a paradigm as “a set of suffixes and the stems that attach to those suffixes and no others”. This definition is quite limited since it is not modeling the notion of lexeme. Chan (2006) defines a simpler concept of paradigms in his probabilistic paradigm model, which has many limitations, such as not handling syncretism or irregular morphology, nor distinguishing inflection and derivation. Dreyer and Eisner (2011) learn complete German verb paradigms from a small set of complete seed paradigms (50 or 100), which they choose randomly from all verbs in the language. They model stem chang</context>
</contexts>
<marker>Can, Manandhar, 2012</marker>
<rawString>Burcu Can and Suresh Manandhar. 2012. Probabilistic hierarchical clustering of morphological paradigms. EACL 2012, page 654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erwin Chan</author>
</authors>
<title>Learning probabilistic paradigms for morphology in a latent class model.</title>
<date>2006</date>
<booktitle>In Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL,</booktitle>
<pages>69--78</pages>
<contexts>
<context position="6637" citStr="Chan (2006)" startWordPosition="1014" endWordPosition="1015">guage. Our work differs in that we use annotated data, but do not start with a complete set of inflectional classes; thus, our work is exactly complementary to this work. The concept of a paradigm is also used in many published efforts on unsupervised learning of morphology, although not always in a way consistent with its use in linguistics. For instance, Snover et al. (2002) (and later on Can and Manandhar (2012)) define a paradigm as “a set of suffixes and the stems that attach to those suffixes and no others”. This definition is quite limited since it is not modeling the notion of lexeme. Chan (2006) defines a simpler concept of paradigms in his probabilistic paradigm model, which has many limitations, such as not handling syncretism or irregular morphology, nor distinguishing inflection and derivation. Dreyer and Eisner (2011) learn complete German verb paradigms from a small set of complete seed paradigms (50 or 100), which they choose randomly from all verbs in the language. They model stem changes using letter-based models, and use a large unannotated corpus in addition to the seed paradigms. Durrett and DeNero (2013) attack the same problem as Dreyer and Eisner (2011). Instead of usi</context>
</contexts>
<marker>Chan, 2006</marker>
<rawString>Erwin Chan. 2006. Learning probabilistic paradigms for morphology in a latent class model. In Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL, pages 69–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lionel Cl´ement</author>
<author>Benoit Sagot</author>
<author>Bernard Lang</author>
</authors>
<title>Morphology based automatic acquisition of largecoverage lexica.</title>
<date>2004</date>
<booktitle>In LREC 04,</booktitle>
<pages>1841--1844</pages>
<marker>Cl´ement, Sagot, Lang, 2004</marker>
<rawString>Lionel Cl´ement, Benoit Sagot, and Bernard Lang. 2004. Morphology based automatic acquisition of largecoverage lexica. In LREC 04, pages 1841–1844.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised models for morpheme segmentation and morphology learning.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing (TSLP),</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="4258" citStr="Creutz and Lagus, 2007" startWordPosition="637" endWordPosition="640">our language-specific modeling of stem variation in Section 5. 1032 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1032–1043, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Approaches to Morphological Modeling Much work has been done in the area of computational morphology ranging from systems painstakingly designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012) to unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Hammarstr¨om and Borin, 2011; Dreyer and Eisner, 2011). There is a large continuum between these two approaches. Closer to one end, we find work on minimally supervised methods for morphology learning that make use of available resources such as parallel data, dictionaries or some additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). Closer to the other end, we find work that focuses on defining morphological models with limited lexicons that are then extended using raw text </context>
</contexts>
<marker>Creutz, Lagus, 2007</marker>
<rawString>Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. ACM Transactions on Speech and Language Processing (TSLP), 4(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
<author>David Yarowsky</author>
</authors>
<title>Bootstrapping a multilingual part-of-speech tagger in one person-day.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th conference on Natural language learning-Volume 20,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="4660" citStr="Cucerzan and Yarowsky, 2002" startWordPosition="700" endWordPosition="703">systems painstakingly designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012) to unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Hammarstr¨om and Borin, 2011; Dreyer and Eisner, 2011). There is a large continuum between these two approaches. Closer to one end, we find work on minimally supervised methods for morphology learning that make use of available resources such as parallel data, dictionaries or some additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). Closer to the other end, we find work that focuses on defining morphological models with limited lexicons that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). The work presented in this paper falls in the middle of this continuum: we are interested in learning complete morphological models using rich morphological annotations and, optionally, limited linguistic knowledge. We compare the value of different amounts of annotation and how they relate to additional linguistic knowledge. Morphological Paradigms Ma</context>
</contexts>
<marker>Cucerzan, Yarowsky, 2002</marker>
<rawString>Silviu Cucerzan and David Yarowsky. 2002. Bootstrapping a multilingual part-of-speech tagger in one person-day. In Proceedings of the 6th conference on Natural language learning-Volume 20, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gr´egoire D´etrez</author>
<author>Aarne Ranta</author>
</authors>
<title>Smart paradigms and the predictability and complexity of inflectional morphology.</title>
<date>2012</date>
<booktitle>EACL 2012,</booktitle>
<pages>645</pages>
<marker>D´etrez, Ranta, 2012</marker>
<rawString>Gr´egoire D´etrez and Aarne Ranta. 2012. Smart paradigms and the predictability and complexity of inflectional morphology. EACL 2012, page 645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Eisner</author>
</authors>
<title>Discovering morphological paradigms from plain text using a dirichlet process mixture model.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>616--627</pages>
<contexts>
<context position="4335" citStr="Dreyer and Eisner, 2011" startWordPosition="650" endWordPosition="653">ings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1032–1043, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Approaches to Morphological Modeling Much work has been done in the area of computational morphology ranging from systems painstakingly designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012) to unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Hammarstr¨om and Borin, 2011; Dreyer and Eisner, 2011). There is a large continuum between these two approaches. Closer to one end, we find work on minimally supervised methods for morphology learning that make use of available resources such as parallel data, dictionaries or some additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). Closer to the other end, we find work that focuses on defining morphological models with limited lexicons that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). The work presented in this pa</context>
<context position="6869" citStr="Dreyer and Eisner (2011)" startWordPosition="1046" endWordPosition="1049">ublished efforts on unsupervised learning of morphology, although not always in a way consistent with its use in linguistics. For instance, Snover et al. (2002) (and later on Can and Manandhar (2012)) define a paradigm as “a set of suffixes and the stems that attach to those suffixes and no others”. This definition is quite limited since it is not modeling the notion of lexeme. Chan (2006) defines a simpler concept of paradigms in his probabilistic paradigm model, which has many limitations, such as not handling syncretism or irregular morphology, nor distinguishing inflection and derivation. Dreyer and Eisner (2011) learn complete German verb paradigms from a small set of complete seed paradigms (50 or 100), which they choose randomly from all verbs in the language. They model stem changes using letter-based models, and use a large unannotated corpus in addition to the seed paradigms. Durrett and DeNero (2013) attack the same problem as Dreyer and Eisner (2011). Instead of using unannotated text, they model explicit rules for affixes and stem changes. The major difference between these two efforts and our work is that the problem is defined differently: we assume that the training and test data is define</context>
<context position="8711" citStr="Dreyer and Eisner, 2011" startWordPosition="1334" endWordPosition="1337">res. In this paper we target the learning and completion of inflectional classes from morphologically annotated data. Our approach does not sacrifice details of what paradigms should include: we handle syncretism and stem changes, and allow for the prediction of new word forms from morphosyntactic features and lemmas, unlike the largely unsupervised work. Our work also differs from most previous work in that we investigate how to model stem change explicitly. Whereas other approaches model stem syncretism through letter1033 based models (Yarowsky and Wicentowski, 2000; Neuvel and Fulop, 2002; Dreyer and Eisner, 2011), we explore the use of abstract stems. In our previous work on the EGY morphological analyzer CALIMA, we similarly used a lexicon of annotated morphological forms and extended it automatically using a simpler approach to paradigm completion (Habash et al., 2012). 3 Linguistic Terminology In this section, we review key concepts from morphology, and introduce the terminology we will use in this paper.2 We then introduce our own formalization of stems using vocalic templates. Morphology is the study of word forms and their decomposition into elementary morphemes, which are the smallest meaning-b</context>
</contexts>
<marker>Dreyer, Eisner, 2011</marker>
<rawString>Markus Dreyer and Jason Eisner. 2011. Discovering morphological paradigms from plain text using a dirichlet process mixture model. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 616–627.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>John DeNero</author>
</authors>
<title>Supervised Learning of Complete Morphological Paradigms.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT),</booktitle>
<location>Atlanta, GA.</location>
<contexts>
<context position="7169" citStr="Durrett and DeNero (2013)" startWordPosition="1097" endWordPosition="1100">. This definition is quite limited since it is not modeling the notion of lexeme. Chan (2006) defines a simpler concept of paradigms in his probabilistic paradigm model, which has many limitations, such as not handling syncretism or irregular morphology, nor distinguishing inflection and derivation. Dreyer and Eisner (2011) learn complete German verb paradigms from a small set of complete seed paradigms (50 or 100), which they choose randomly from all verbs in the language. They model stem changes using letter-based models, and use a large unannotated corpus in addition to the seed paradigms. Durrett and DeNero (2013) attack the same problem as Dreyer and Eisner (2011). Instead of using unannotated text, they model explicit rules for affixes and stem changes. The major difference between these two efforts and our work is that the problem is defined differently: we assume that the training and test data is defined by a corpus, not by complete paradigms. Our methods therefore are more sensitive to frequency effects of tokens. We believe that our way of stating the problem is more relevant to actual computational challenges for languages with limited morphological resources. We empirically compare our approac</context>
<context position="35549" citStr="Durrett and DeNero (2013)" startWordPosition="5933" endWordPosition="5936">m. As expected, IICs always help improve accuracy, especially under limited (and no) data conditions. However the benefits diminish rapidly for larger training sets. When evaluating on types only (results not presented in this paper), we find that using IICs in our system with no data is better than using the baseline with 75K words. 5.3.3 Blind Test Set Table 3 shows the accuracies the different systems on our blind test set. The results for the test set are lower than those of the development set, but the trends are the same. We also compare our results to those obtained using the system of Durrett and DeNero (2013) on the same test data. Note that we apply their system to our problem – predicting unseen forms from annotated corpora (i.e., incomplete paradigms), not to the problem for which they created their system – predicting unseen forms from complete paradigms. Our best system outperforms theirs by 2.8% absolute in accuracy. System Accuracy Error Reduction Baseline 84.7 NOTMP 91.5 44.4 SCHLR 93.2 55.6 EMPR 93.2 55.6 Durrett &amp; DeNero 90.4 37.3 Table 3: Results for EGY on tokens using a blind test set. 5.4 German 5.4.1 Choice of Pattern Letters Scholar-based Pattern Letters We now discuss our scholars</context>
<context position="40912" citStr="Durrett and DeNero (2013)" startWordPosition="6883" endWordPosition="6886">g, but occur frequently enough so that their IC can be learned directly from the training data. Using SCHLR simply adds noise for these very frequent verbs. It is only for the less frequent strong verbs that SCHLR can contribute, and then only when a large amount of training data is available. 5.4.2 Blind Test Set Table 6 shows the accuracies the different systems on our blind test set. The results for the test set are lower than those of the development set, and NOTMP, SCHLR,and EMPR produce very similar accuracy results. We also compare our results to those obtained by running the system of Durrett and DeNero (2013) on the same training and test data. Our system outperforms Durrett and DeNero (2013)’s system reducing the error of by 5%.5 5We also tested our system on Durrett and DeNero (2013)’s problem definition and data, training on 200 GER paradigms, and testing on 200 unseen paradigms. This is the case of testing for unseen lemmas in our system. Our system gives an accuracy of 88.4% as opposed to 91.8% as reported by Durrett and DeNero (2013). Our system was not designed for this task. Corpus Verb BL NOTMP SCHLR EMPR Size Count 0K 0 13.7 25.2 25.2 25.2 1K 81 43.3 72.0 67.0 69.4 5K 461 64.8 83.5 83.0 </context>
</contexts>
<marker>Durrett, DeNero, 2013</marker>
<rawString>Greg Durrett and John DeNero. 2013. Supervised Learning of Complete Morphological Paradigms. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT), Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed El Kholy</author>
<author>Nizar Habash</author>
</authors>
<title>Techniques for Arabic morphological detokenization and orthographic denormalization.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC2010,</booktitle>
<marker>El Kholy, Habash, 2010</marker>
<rawString>Ahmed El Kholy and Nizar Habash. 2010. Techniques for Arabic morphological detokenization and orthographic denormalization. In Proceedings of LREC2010, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramy Eskander</author>
<author>Nizar Habash</author>
<author>Ann Bies</author>
<author>Seth Kulick</author>
<author>Mohamed Maamouri</author>
</authors>
<title>Automatic correction and extension of morphological annotations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 1– 10,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="21805" citStr="Eskander et al. (2013)" startWordPosition="3607" endWordPosition="3610">ure combinations of aspect/mood (perfective,imperfective and imperative), person (first, second and third), gender (masculine, feminine and neutral) and number (singular and plural). Some combinations are invalid such as the first and third persons with the imperative form. The lemma we use is the Arabic citation form for verbs, which is the perfective third person masculine sin4The corpus was automatically annotated using information from the CHE transcripts (Gadalla et al., 1997) and the Egyptian Colloquial Arabic Lexicon (Kilany et al., 2002). For more details, see Habash et al. (2012) and Eskander et al. (2013). 1036 gular (P3MS) inflection. We use this fact to fill the P3MS cells when building the initial ICs. We evaluate the accuracy of the automatically generated bidirectional lexicon by generating surface forms from lemmas and morphosyntactic features. We use the model application method described in Section 4.4. Baseline Our baseline system consists of two steps. First, we check the features. All cases of citation form features (in the case of EGY, P3MS) return the lemma as the inflected form. Otherwise, we look up the lemma and feature pair among all triplets in the training data and return th</context>
</contexts>
<marker>Eskander, Habash, Bies, Kulick, Maamouri, 2013</marker>
<rawString>Ramy Eskander, Nizar Habash, Ann Bies, Seth Kulick, and Mohamed Maamouri. 2013. Automatic correction and extension of morphological annotations. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 1– 10, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Finkel</author>
<author>Gregory Stump</author>
</authors>
<title>Generating Hebrew Verb Morphology by Default Inheritance Hierarchies.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>9--18</pages>
<contexts>
<context position="5607" citStr="Finkel and Stump, 2002" startWordPosition="839" endWordPosition="842">interested in learning complete morphological models using rich morphological annotations and, optionally, limited linguistic knowledge. We compare the value of different amounts of annotation and how they relate to additional linguistic knowledge. Morphological Paradigms Many traditional and modern theories of inflectional morphology organize natural language morphology by paradigms (Stump, 2001; Walther, 2011; Camilleri, 2011). Within the continuum we discussed above, we find hierarchical representations of paradigm knowledge that have been used in manually constructed morphological models (Finkel and Stump, 2002; Habash et al., 2005). Furthermore, D´etrez and Ranta (2012) introduce an implementation of Smart Paradigms – heuristically organized paradigms minimizing the number of forms needed to predict the full paradigm of a particular lexeme. Both Forsberg et al. (2006) and Cl´ement et al. (2004) describe methods for automatically populating a lexicon from raw data given a set of morphological inflectional classes in a language. Our work differs in that we use annotated data, but do not start with a complete set of inflectional classes; thus, our work is exactly complementary to this work. The concep</context>
</contexts>
<marker>Finkel, Stump, 2002</marker>
<rawString>Raphael Finkel and Gregory Stump. 2002. Generating Hebrew Verb Morphology by Default Inheritance Hierarchies. In Proceedings of the Workshop on Computational Approaches to Semitic Languages, pages 9– 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Forsberg</author>
<author>Harald Hammarstr¨om</author>
<author>Aarne Ranta</author>
</authors>
<title>Morphological lexicon extraction from raw text data.</title>
<date>2006</date>
<booktitle>Advances in Natural Language Processing,</booktitle>
<pages>488--499</pages>
<marker>Forsberg, Hammarstr¨om, Ranta, 2006</marker>
<rawString>Markus Forsberg, Harald Hammarstr¨om, and Aarne Ranta. 2006. Morphological lexicon extraction from raw text data. Advances in Natural Language Processing, pages 488–499.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Gadalla</author>
</authors>
<title>Hanaa Kilany, Howaida Arram, Ashraf Yacoub, Alaa El-Habashi, Amr Shalaby, Krisjanis Karins,</title>
<date>1997</date>
<booktitle>In Linguistic Data Consortium,</booktitle>
<location>Everett Rowson, Robert MacIntyre, Paul Kingsbury, David</location>
<marker>Gadalla, 1997</marker>
<rawString>Hassan Gadalla, Hanaa Kilany, Howaida Arram, Ashraf Yacoub, Alaa El-Habashi, Amr Shalaby, Krisjanis Karins, Everett Rowson, Robert MacIntyre, Paul Kingsbury, David Graff, and Cynthia McLemore. 1997. CALLHOME Egyptian Arabic Transcripts. In Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Gadalla</author>
</authors>
<title>Comparative Morphology of Standard and Egyptian Arabic.</title>
<date>2000</date>
<publisher>LINCOM EUROPA.</publisher>
<contexts>
<context position="29693" citStr="Gadalla, 2000" startWordPosition="4941" endWordPosition="4942">etter under consideration. This gives us per ranking as many sets as there are letters that change. We then choose the best performing set among all sets generated by both rankings. We present the results of this approach for EGY in Section 5.3.1 and for GER in Section 5.4.1. 5.3 Egyptian Arabic 5.3.1 Choice of Pattern Letters Scholar-based Pattern Letters For the EGY SCHLR approach, we selected the following pattern II AAA�¯Awwyyy’aui—. These letters cover all the so-called weak root radicals, Hamzated forms and diacritics that are often used to discuss different verbal paradigms in Arabic (Gadalla, 2000). Table 1 shows three EGY inflectional classes. We follow the standard practice in modeling Arabic morphology of assuming that each placeholder in an orthographic template corresponds to exactly one letter from the orthographic root (i.e., the root is a sequence of strings each of length one). We would like to stress that our orthographic template and root differ from the notions of pattern and root in Semitic morphology (for an overview, see (Habash, 2010)): while for katab ‘write’ the orthographic root as well as the “real” root is 11k,t,b11, the orthographic root for xal—a´y ‘let’ is 11x,l1</context>
<context position="34217" citStr="Gadalla, 2000" startWordPosition="5715" endWordPosition="5716">Stem-Syncretism Zones (HZ) are stemsyncretism zones determined manually by linguists to hold for all ICs. As such they can be more finegrained than is needed to describe individual ICs. A hard zone is not applied in case of any partial disagreement within it. Unlike soft zones, they could be applied before or after the merge process, and they do not guarantee that the ICs will be completely filled. We conducted experiments where we added external linguistic knowledge to our training data. We added 112 IICs which are extracted from all EGY verb inflections listed in a reference grammar of EGY (Gadalla, 2000). Also we added the following eight HZs for EGY (with reference to features &lt;tense, person, gender, number&gt;): (P1US-P1UP-P2MS-P2FS-P2UP), (I1UP-I2MSI3MS-I3FS), (I2FS-I2UP-I3UP), (P3FS-P3UP), (C2FS-C2UP), (P3MS), (I1US), and (C2MS). Applying the HZs on the completed ICs (before soft zone application) gives higher results than applying them on initial ICs. We only report below on the setting of applying HZs after IC merge completion. We present the accuracies of IC learning for the baseline, and for the best setup with and without IICs and HZs, for different training sizes in Table 2. The baseli</context>
</contexts>
<marker>Gadalla, 2000</marker>
<rawString>Hassan Gadalla. 2000. Comparative Morphology of Standard and Egyptian Arabic. LINCOM EUROPA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>MAGEAD: A morphological analyzer and generator for the Arabic dialects.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>681--688</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="4133" citStr="Habash and Rambow, 2006" startWordPosition="619" endWordPosition="622"> introduce the key linguistic concepts we use (Section 3). We present our basic language-independent method in Section 4, and our language-specific modeling of stem variation in Section 5. 1032 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1032–1043, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Approaches to Morphological Modeling Much work has been done in the area of computational morphology ranging from systems painstakingly designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012) to unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Hammarstr¨om and Borin, 2011; Dreyer and Eisner, 2011). There is a large continuum between these two approaches. Closer to one end, we find work on minimally supervised methods for morphology learning that make use of available resources such as parallel data, dictionaries or some additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). Closer to the other</context>
</contexts>
<marker>Habash, Rambow, 2006</marker>
<rawString>Nizar Habash and Owen Rambow. 2006. MAGEAD: A morphological analyzer and generator for the Arabic dialects. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 681–688, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>George Kiraz</author>
</authors>
<title>Morphological Analysis and Generation for Arabic Dialects.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>17--24</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="5629" citStr="Habash et al., 2005" startWordPosition="843" endWordPosition="846">omplete morphological models using rich morphological annotations and, optionally, limited linguistic knowledge. We compare the value of different amounts of annotation and how they relate to additional linguistic knowledge. Morphological Paradigms Many traditional and modern theories of inflectional morphology organize natural language morphology by paradigms (Stump, 2001; Walther, 2011; Camilleri, 2011). Within the continuum we discussed above, we find hierarchical representations of paradigm knowledge that have been used in manually constructed morphological models (Finkel and Stump, 2002; Habash et al., 2005). Furthermore, D´etrez and Ranta (2012) introduce an implementation of Smart Paradigms – heuristically organized paradigms minimizing the number of forms needed to predict the full paradigm of a particular lexeme. Both Forsberg et al. (2006) and Cl´ement et al. (2004) describe methods for automatically populating a lexicon from raw data given a set of morphological inflectional classes in a language. Our work differs in that we use annotated data, but do not start with a complete set of inflectional classes; thus, our work is exactly complementary to this work. The concept of a paradigm is als</context>
</contexts>
<marker>Habash, Rambow, Kiraz, 2005</marker>
<rawString>Nizar Habash, Owen Rambow, and George Kiraz. 2005. Morphological Analysis and Generation for Arabic Dialects. In Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 17–24, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Abdelhadi Soudi</author>
<author>Tim Buckwalter</author>
</authors>
<title>On Arabic Transliteration.</title>
<date>2007</date>
<booktitle>Arabic Computational Morphology: Knowledge-based and Empirical Methods.</booktitle>
<editor>In A. van den Bosch and A. Soudi, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="13055" citStr="Habash et al., 2007" startWordPosition="2043" endWordPosition="2046">nsists of a core algorithm for building ICs from seen data and models of soft-stem syncretism and affix prediction. 4.1 Problem Definition Starting with a corpus of words annotated as triples of (prefix+stem+suffix, lemma, features), we want to create a lexicon of complete ICs, with each IC having an associated set of lemmas. The following is an input example specifying the inflected form of the 3rd person plural imperfective inflection for the EGY lemma katab3 ‘write’: (y+iktib+uwA, katab, I3UP). 3Arabic transliteration throughout the paper is presented in the Habash-Soudi-Buckwalter scheme (Habash et al., 2007). 1034 1 2 3 IC1,2,3 IC1 IC1,3 IC1 CS = 2 IC2 L1 – L2 – L3 IC2 L1 – L2 – L3 IC1,2,3 L1 – L2 – L3 L6 – l7 L4 – L5 L6 – l7 IC2 IC2 L4 – L5 IC4 CS = 3 IC3 CS = 2 L4 – L5 IC1,3 L8 CS = 1 L6 – L7 IC4 IC4 IC4 IC4 L8 IC4 L8 IC3 Figure 1: This graph illustrates two merges that result in combining three ICs in two steps. The IC cells are represented as solid (filled cells) or blank (empty cells) circles. CS is the compatibility score marking the number of matching filled cells in an IC. The boxes to the right of the graph represent the lexicon which associates lemmas (L*) with ICs. IC4 is not connected</context>
</contexts>
<marker>Habash, Soudi, Buckwalter, 2007</marker>
<rawString>Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter. 2007. On Arabic Transliteration. In A. van den Bosch and A. Soudi, editors, Arabic Computational Morphology: Knowledge-based and Empirical Methods. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Ramy Eskander</author>
<author>Abdelati Hawwari</author>
</authors>
<title>A Morphological Analyzer for Egyptian Arabic.</title>
<date>2012</date>
<booktitle>In Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology,</booktitle>
<pages>1--9</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="8974" citStr="Habash et al., 2012" startWordPosition="1376" endWordPosition="1379">word forms from morphosyntactic features and lemmas, unlike the largely unsupervised work. Our work also differs from most previous work in that we investigate how to model stem change explicitly. Whereas other approaches model stem syncretism through letter1033 based models (Yarowsky and Wicentowski, 2000; Neuvel and Fulop, 2002; Dreyer and Eisner, 2011), we explore the use of abstract stems. In our previous work on the EGY morphological analyzer CALIMA, we similarly used a lexicon of annotated morphological forms and extended it automatically using a simpler approach to paradigm completion (Habash et al., 2012). 3 Linguistic Terminology In this section, we review key concepts from morphology, and introduce the terminology we will use in this paper.2 We then introduce our own formalization of stems using vocalic templates. Morphology is the study of word forms and their decomposition into elementary morphemes, which are the smallest meaning-bearing units of a language. There are two types of morphological processes: inflectional and derivational morphology. In inflectional morphology, a core meaning is retained and different word forms reflect different types of morphosyntactic features such as perso</context>
<context position="21778" citStr="Habash et al. (2012)" startWordPosition="3602" endWordPosition="3605">ere is a total of 19 feature combinations of aspect/mood (perfective,imperfective and imperative), person (first, second and third), gender (masculine, feminine and neutral) and number (singular and plural). Some combinations are invalid such as the first and third persons with the imperative form. The lemma we use is the Arabic citation form for verbs, which is the perfective third person masculine sin4The corpus was automatically annotated using information from the CHE transcripts (Gadalla et al., 1997) and the Egyptian Colloquial Arabic Lexicon (Kilany et al., 2002). For more details, see Habash et al. (2012) and Eskander et al. (2013). 1036 gular (P3MS) inflection. We use this fact to fill the P3MS cells when building the initial ICs. We evaluate the accuracy of the automatically generated bidirectional lexicon by generating surface forms from lemmas and morphosyntactic features. We use the model application method described in Section 4.4. Baseline Our baseline system consists of two steps. First, we check the features. All cases of citation form features (in the case of EGY, P3MS) return the lemma as the inflected form. Otherwise, we look up the lemma and feature pair among all triplets in the </context>
</contexts>
<marker>Habash, Eskander, Hawwari, 2012</marker>
<rawString>Nizar Habash, Ramy Eskander, and Abdelati Hawwari. 2012. A Morphological Analyzer for Egyptian Arabic. In Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology, pages 1–9, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Introduction to Arabic Natural Language Processing.</title>
<date>2010</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="21016" citStr="Habash, 2010" startWordPosition="3482" endWordPosition="3483"> of the lemma associated with the most frequent affixes of the queried features. 4.5 Results on Egyptian Arabic Data and Metrics We use a morphologically annotated EGY corpus based on the CALLHOME EGY (CHE) corpus (Gadalla et al., 1997).4 We divide the corpus into three parts: training, development and test, of about 75K, 36K and 41K words, respectively. We conduct our experiments on verbs only since they have a large number of possible morphosyntactic feature combinations. The verbs in these experiments are uncliticized. Clitics are easily handled using a few orthographic rules (El Kholy and Habash, 2010). On average, uncliticized verbs are about 12% of all words in our corpus. The data is represented in triplets as described in Section 4.1. There is a total of 19 feature combinations of aspect/mood (perfective,imperfective and imperative), person (first, second and third), gender (masculine, feminine and neutral) and number (singular and plural). Some combinations are invalid such as the first and third persons with the imperative form. The lemma we use is the Arabic citation form for verbs, which is the perfective third person masculine sin4The corpus was automatically annotated using inform</context>
<context position="30154" citStr="Habash, 2010" startWordPosition="5017" endWordPosition="5018"> all the so-called weak root radicals, Hamzated forms and diacritics that are often used to discuss different verbal paradigms in Arabic (Gadalla, 2000). Table 1 shows three EGY inflectional classes. We follow the standard practice in modeling Arabic morphology of assuming that each placeholder in an orthographic template corresponds to exactly one letter from the orthographic root (i.e., the root is a sequence of strings each of length one). We would like to stress that our orthographic template and root differ from the notions of pattern and root in Semitic morphology (for an overview, see (Habash, 2010)): while for katab ‘write’ the orthographic root as well as the “real” root is 11k,t,b11, the orthographic root for xal—a´y ‘let’ is 11x,l11 (as opposed to the real root 11x,l,y11). Henceforth, we will omit the adjective “orthographic”, since we will not talk about the “real” root. To inflect a lemma for a particular set of features, we combine the root with the ❑ slots in the corresponding inflectional class feature row, e.g., Hal— + P2FS ==&gt;- 11H,l11 + � letters: ��������� ����� ��� II 1038 ❑a❑-+aytiy ==&gt;- Hal-aytiy. Note that in this paper, we do not use any rules; all regular phonological </context>
</contexts>
<marker>Habash, 2010</marker>
<rawString>Nizar Habash. 2010. Introduction to Arabic Natural Language Processing. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Hammarstr¨om</author>
<author>Lars Borin</author>
</authors>
<title>Unsupervised learning of morphology.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<marker>Hammarstr¨om, Borin, 2011</marker>
<rawString>Harald Hammarstr¨om and Lars Borin. 2011. Unsupervised learning of morphology. Computational Linguistics, 37(2):309–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kilany</author>
<author>H Gadalla</author>
<author>H Arram</author>
<author>A Yacoub</author>
<author>A ElHabashi</author>
<author>C McLemore</author>
</authors>
<date>2002</date>
<booktitle>Egyptian Colloquial Arabic Lexicon. LDC catalog number LDC99L22.</booktitle>
<contexts>
<context position="21734" citStr="Kilany et al., 2002" startWordPosition="3594" endWordPosition="3597"> in triplets as described in Section 4.1. There is a total of 19 feature combinations of aspect/mood (perfective,imperfective and imperative), person (first, second and third), gender (masculine, feminine and neutral) and number (singular and plural). Some combinations are invalid such as the first and third persons with the imperative form. The lemma we use is the Arabic citation form for verbs, which is the perfective third person masculine sin4The corpus was automatically annotated using information from the CHE transcripts (Gadalla et al., 1997) and the Egyptian Colloquial Arabic Lexicon (Kilany et al., 2002). For more details, see Habash et al. (2012) and Eskander et al. (2013). 1036 gular (P3MS) inflection. We use this fact to fill the P3MS cells when building the initial ICs. We evaluate the accuracy of the automatically generated bidirectional lexicon by generating surface forms from lemmas and morphosyntactic features. We use the model application method described in Section 4.4. Baseline Our baseline system consists of two steps. First, we check the features. All cases of citation form features (in the case of EGY, P3MS) return the lemma as the inflected form. Otherwise, we look up the lemma</context>
</contexts>
<marker>Kilany, Gadalla, Arram, Yacoub, ElHabashi, McLemore, 2002</marker>
<rawString>H. Kilany, H. Gadalla, H. Arram, A. Yacoub, A. ElHabashi, and C. McLemore. 2002. Egyptian Colloquial Arabic Lexicon. LDC catalog number LDC99L22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Two-Level Model for Morphological Analysis.</title>
<date>1983</date>
<booktitle>In Proceedings of the 8th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>683--685</pages>
<contexts>
<context position="4090" citStr="Koskenniemi, 1983" startWordPosition="615" endWordPosition="616">e review related work (Section 2) and introduce the key linguistic concepts we use (Section 3). We present our basic language-independent method in Section 4, and our language-specific modeling of stem variation in Section 5. 1032 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1032–1043, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Approaches to Morphological Modeling Much work has been done in the area of computational morphology ranging from systems painstakingly designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012) to unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Hammarstr¨om and Borin, 2011; Dreyer and Eisner, 2011). There is a large continuum between these two approaches. Closer to one end, we find work on minimally supervised methods for morphology learning that make use of available resources such as parallel data, dictionaries or some additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyd</context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>Kimmo Koskenniemi. 1983. Two-Level Model for Morphological Analysis. In Proceedings of the 8th International Joint Conference on Artificial Intelligence, pages 683–685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Monson</author>
<author>Jaime Carbonell</author>
<author>Alon Lavie</author>
<author>Lori Levin</author>
</authors>
<title>Paramor: Finding paradigms across morphology. Advances in Multilingual and Multimodal Information Retrieval,</title>
<date>2008</date>
<pages>900--907</pages>
<contexts>
<context position="4279" citStr="Monson et al., 2008" startWordPosition="641" endWordPosition="644">deling of stem variation in Section 5. 1032 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1032–1043, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Approaches to Morphological Modeling Much work has been done in the area of computational morphology ranging from systems painstakingly designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012) to unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Hammarstr¨om and Borin, 2011; Dreyer and Eisner, 2011). There is a large continuum between these two approaches. Closer to one end, we find work on minimally supervised methods for morphology learning that make use of available resources such as parallel data, dictionaries or some additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). Closer to the other end, we find work that focuses on defining morphological models with limited lexicons that are then extended using raw text (Cl´ement et al., 200</context>
</contexts>
<marker>Monson, Carbonell, Lavie, Levin, 2008</marker>
<rawString>Christian Monson, Jaime Carbonell, Alon Lavie, and Lori Levin. 2008. Paramor: Finding paradigms across morphology. Advances in Multilingual and Multimodal Information Retrieval, pages 900–907.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Neuvel</author>
<author>Sean A Fulop</author>
</authors>
<title>Unsupervised learning of morphology without morphemes.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 workshop on Morphological and phonological learning-Volume 6,</booktitle>
<pages>31--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4684" citStr="Neuvel and Fulop, 2002" startWordPosition="704" endWordPosition="707">d by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012) to unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Hammarstr¨om and Borin, 2011; Dreyer and Eisner, 2011). There is a large continuum between these two approaches. Closer to one end, we find work on minimally supervised methods for morphology learning that make use of available resources such as parallel data, dictionaries or some additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). Closer to the other end, we find work that focuses on defining morphological models with limited lexicons that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). The work presented in this paper falls in the middle of this continuum: we are interested in learning complete morphological models using rich morphological annotations and, optionally, limited linguistic knowledge. We compare the value of different amounts of annotation and how they relate to additional linguistic knowledge. Morphological Paradigms Many traditional and moder</context>
<context position="8685" citStr="Neuvel and Fulop, 2002" startWordPosition="1330" endWordPosition="1333">of morphosyntactic features. In this paper we target the learning and completion of inflectional classes from morphologically annotated data. Our approach does not sacrifice details of what paradigms should include: we handle syncretism and stem changes, and allow for the prediction of new word forms from morphosyntactic features and lemmas, unlike the largely unsupervised work. Our work also differs from most previous work in that we investigate how to model stem change explicitly. Whereas other approaches model stem syncretism through letter1033 based models (Yarowsky and Wicentowski, 2000; Neuvel and Fulop, 2002; Dreyer and Eisner, 2011), we explore the use of abstract stems. In our previous work on the EGY morphological analyzer CALIMA, we similarly used a lexicon of annotated morphological forms and extended it automatically using a simpler approach to paradigm completion (Habash et al., 2012). 3 Linguistic Terminology In this section, we review key concepts from morphology, and introduce the terminology we will use in this paper.2 We then introduce our own formalization of stems using vocalic templates. Morphology is the study of word forms and their decomposition into elementary morphemes, which </context>
</contexts>
<marker>Neuvel, Fulop, 2002</marker>
<rawString>Sylvain Neuvel and Sean A Fulop. 2002. Unsupervised learning of morphology without morphemes. In Proceedings of the ACL-02 workshop on Morphological and phonological learning-Volume 6, pages 31–40. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew G Snover</author>
<author>Gaja E Jarosz</author>
<author>Michael R Brent</author>
</authors>
<title>Unsupervised learning of morphology using a novel directed search algorithm: Taking the first step.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 workshop on Morphological and phonological learning-Volume 6,</booktitle>
<pages>11--20</pages>
<contexts>
<context position="6405" citStr="Snover et al. (2002)" startWordPosition="970" endWordPosition="973">eded to predict the full paradigm of a particular lexeme. Both Forsberg et al. (2006) and Cl´ement et al. (2004) describe methods for automatically populating a lexicon from raw data given a set of morphological inflectional classes in a language. Our work differs in that we use annotated data, but do not start with a complete set of inflectional classes; thus, our work is exactly complementary to this work. The concept of a paradigm is also used in many published efforts on unsupervised learning of morphology, although not always in a way consistent with its use in linguistics. For instance, Snover et al. (2002) (and later on Can and Manandhar (2012)) define a paradigm as “a set of suffixes and the stems that attach to those suffixes and no others”. This definition is quite limited since it is not modeling the notion of lexeme. Chan (2006) defines a simpler concept of paradigms in his probabilistic paradigm model, which has many limitations, such as not handling syncretism or irregular morphology, nor distinguishing inflection and derivation. Dreyer and Eisner (2011) learn complete German verb paradigms from a small set of complete seed paradigms (50 or 100), which they choose randomly from all verbs</context>
</contexts>
<marker>Snover, Jarosz, Brent, 2002</marker>
<rawString>Matthew G Snover, Gaja E Jarosz, and Michael R Brent. 2002. Unsupervised learning of morphology using a novel directed search algorithm: Taking the first step. In Proceedings of the ACL-02 workshop on Morphological and phonological learning-Volume 6, pages 11–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for morphological segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>737--745</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="4712" citStr="Snyder and Barzilay, 2008" startWordPosition="708" endWordPosition="711">1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012) to unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Hammarstr¨om and Borin, 2011; Dreyer and Eisner, 2011). There is a large continuum between these two approaches. Closer to one end, we find work on minimally supervised methods for morphology learning that make use of available resources such as parallel data, dictionaries or some additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). Closer to the other end, we find work that focuses on defining morphological models with limited lexicons that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). The work presented in this paper falls in the middle of this continuum: we are interested in learning complete morphological models using rich morphological annotations and, optionally, limited linguistic knowledge. We compare the value of different amounts of annotation and how they relate to additional linguistic knowledge. Morphological Paradigms Many traditional and modern theories of inflectional m</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2008. Unsupervised multilingual learning for morphological segmentation. In Proceedings of ACL-08: HLT, pages 737– 745, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory T Stump</author>
</authors>
<title>Inflectional Morphology. A Theory of Paradigm Structure. Cambridge Studies in Linguistics.</title>
<date>2001</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5384" citStr="Stump, 2001" startWordPosition="810" endWordPosition="811">ing morphological models with limited lexicons that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). The work presented in this paper falls in the middle of this continuum: we are interested in learning complete morphological models using rich morphological annotations and, optionally, limited linguistic knowledge. We compare the value of different amounts of annotation and how they relate to additional linguistic knowledge. Morphological Paradigms Many traditional and modern theories of inflectional morphology organize natural language morphology by paradigms (Stump, 2001; Walther, 2011; Camilleri, 2011). Within the continuum we discussed above, we find hierarchical representations of paradigm knowledge that have been used in manually constructed morphological models (Finkel and Stump, 2002; Habash et al., 2005). Furthermore, D´etrez and Ranta (2012) introduce an implementation of Smart Paradigms – heuristically organized paradigms minimizing the number of forms needed to predict the full paradigm of a particular lexeme. Both Forsberg et al. (2006) and Cl´ement et al. (2004) describe methods for automatically populating a lexicon from raw data given a set of m</context>
<context position="10968" citStr="Stump (2001)" startWordPosition="1704" endWordPosition="1705">s sometimes referred to as a “citation form”. A paradigm of a lexeme is a list of cells, where a cell is a combination of a complete set of morphosyntactic features (properties) and the corresponding inflected form of the lexeme. A paradigm is complete if there are cells for all possible morphosyntactic features (the list of possible morphosyntactic features is of course language-dependent). We can divide the word forms into affixes (i.e., prefixes and suffixes) and the stem. There is no sin2We (roughly) base our terminology and our conceptualization on the inferential-realizational theory of Stump (2001). gle correct way to do this for the words of a language. Each lexeme has its own paradigm. We can abstract from paradigms by grouping together paradigms which share the same affixes in corresponding cells, and where stems in corresponding cells differ in some restricted manner. We can define the inflectional class (IC) more formally as a set of abstract cells, where an abstract cell is a combination of a complete set of morphosyntactic features (properties) and an abstract representation of a stem (an abstract stem) along with fully specified affixes. The abstract stems (and thus the abstract</context>
</contexts>
<marker>Stump, 2001</marker>
<rawString>Gregory T. Stump. 2001. Inflectional Morphology. A Theory of Paradigm Structure. Cambridge Studies in Linguistics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G´eraldine Walther</author>
</authors>
<date>2011</date>
<booktitle>Measuring morphological canonicity. Linguistica, 51:157–180. Internal and External Boundaries of Morphology.</booktitle>
<contexts>
<context position="5399" citStr="Walther, 2011" startWordPosition="812" endWordPosition="813">ical models with limited lexicons that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). The work presented in this paper falls in the middle of this continuum: we are interested in learning complete morphological models using rich morphological annotations and, optionally, limited linguistic knowledge. We compare the value of different amounts of annotation and how they relate to additional linguistic knowledge. Morphological Paradigms Many traditional and modern theories of inflectional morphology organize natural language morphology by paradigms (Stump, 2001; Walther, 2011; Camilleri, 2011). Within the continuum we discussed above, we find hierarchical representations of paradigm knowledge that have been used in manually constructed morphological models (Finkel and Stump, 2002; Habash et al., 2005). Furthermore, D´etrez and Ranta (2012) introduce an implementation of Smart Paradigms – heuristically organized paradigms minimizing the number of forms needed to predict the full paradigm of a particular lexeme. Both Forsberg et al. (2006) and Cl´ement et al. (2004) describe methods for automatically populating a lexicon from raw data given a set of morphological in</context>
</contexts>
<marker>Walther, 2011</marker>
<rawString>G´eraldine Walther. 2011. Measuring morphological canonicity. Linguistica, 51:157–180. Internal and External Boundaries of Morphology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Richard Wicentowski</author>
</authors>
<title>Minimally supervised morphological analysis by multimodal alignment.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>207--216</pages>
<contexts>
<context position="4631" citStr="Yarowsky and Wicentowski, 2000" startWordPosition="696" endWordPosition="699">ational morphology ranging from systems painstakingly designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012) to unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Hammarstr¨om and Borin, 2011; Dreyer and Eisner, 2011). There is a large continuum between these two approaches. Closer to one end, we find work on minimally supervised methods for morphology learning that make use of available resources such as parallel data, dictionaries or some additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). Closer to the other end, we find work that focuses on defining morphological models with limited lexicons that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). The work presented in this paper falls in the middle of this continuum: we are interested in learning complete morphological models using rich morphological annotations and, optionally, limited linguistic knowledge. We compare the value of different amounts of annotation and how they relate to additional linguistic knowledg</context>
<context position="8661" citStr="Yarowsky and Wicentowski, 2000" startWordPosition="1326" endWordPosition="1329">ms. There is no explicit notion of morphosyntactic features. In this paper we target the learning and completion of inflectional classes from morphologically annotated data. Our approach does not sacrifice details of what paradigms should include: we handle syncretism and stem changes, and allow for the prediction of new word forms from morphosyntactic features and lemmas, unlike the largely unsupervised work. Our work also differs from most previous work in that we investigate how to model stem change explicitly. Whereas other approaches model stem syncretism through letter1033 based models (Yarowsky and Wicentowski, 2000; Neuvel and Fulop, 2002; Dreyer and Eisner, 2011), we explore the use of abstract stems. In our previous work on the EGY morphological analyzer CALIMA, we similarly used a lexicon of annotated morphological forms and extended it automatically using a simpler approach to paradigm completion (Habash et al., 2012). 3 Linguistic Terminology In this section, we review key concepts from morphology, and introduce the terminology we will use in this paper.2 We then introduce our own formalization of stems using vocalic templates. Morphology is the study of word forms and their decomposition into elem</context>
</contexts>
<marker>Yarowsky, Wicentowski, 2000</marker>
<rawString>David Yarowsky and Richard Wicentowski. 2000. Minimally supervised morphological analysis by multimodal alignment. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 207–216.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
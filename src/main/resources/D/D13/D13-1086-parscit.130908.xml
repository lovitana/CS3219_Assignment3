<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015681">
<title confidence="0.998151">
Automatic Domain Partitioning for Multi-Domain Learning
</title>
<author confidence="0.999199">
Di Wang
</author>
<affiliation confidence="0.941022333333333">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.993242">
diwang@cs.cmu.edu
</email>
<author confidence="0.984959">
Chenyan Xiong
</author>
<affiliation confidence="0.891469333333333">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998357">
cx@cs.cmu.edu
</email>
<author confidence="0.998983">
William Yang Wang
</author>
<affiliation confidence="0.891484">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998572">
ww@cmu.edu
</email>
<sectionHeader confidence="0.9986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998706111111111">
Multi-Domain learning (MDL) assumes that
the domain labels in the dataset are known.
However, when there are multiple metadata at-
tributes available, it is not always straightfor-
ward to select a single best attribute for do-
main partition, and it is possible that combin-
ing more than one metadata attributes (includ-
ing continuous attributes) can lead to better
MDL performance. In this work, we propose
an automatic domain partitioning approach
that aims at providing better domain identi-
ties for MDL. We use a supervised clustering
approach that learns the domain distance be-
tween data instances , and then cluster the data
into better domains for MDL. Our experiment
on real multi-domain datasets shows that us-
ing our automatically generated domain parti-
tion improves over popular MDL methods.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999737574468085">
Instead of assuming data are 02, Multi-domain
learning (MDL) methods assumes that data come
from several domains and make use of domain la-
bels to improve modeling performance (Daum´e III,
2007). The motivation of using MDL is that datasets
from different domains could be different, in two
ways. First, the feature distribution p(x) could be
domain specific, meaning that the importance of
each feature is different across domains. Second,
the distribution of label Y given X, p(ylx), of dif-
ferent domains could be different. These differ-
ences could create problems for traditional machine
learning methods: models learned from one domain
might not be generalizable to other domains (Ben-
David et al., 2006; Ben-David et al., 2010).
One common assumption of MDL methods is that
the domain identities are pre-defined. For example,
in the multi-domain Amazon product review dataset
(Finkel and Manning, 2009), the product categories
are typically used as the domain identities. How-
ever, a question raised by Joshi et al. (2012) is that,
in real-world data sets, there could be many ways to
split data into domains, and it is hard to decide which
one to use. Consider the Amazon product reviews,
where we have multiple attributes attached to each
review: for example, product category, reviewer lo-
cation, price, and number of feedback. Which at-
tribute is the most informative domain label? Or we
should use all of these meta-data and partition the
data into many small domains?
In this paper, we investigate the problem of au-
tomatic domain partitioning. We propose an em-
pirical domain difference testing method to exam-
ine whether two groups of data are 02, or gener-
ated from different distributions, and how different
they are. Using this approach, we generate data pairs
that belong to the same distribution, and data pairs
that should be partitioned into different domains.
These pairs are then used as training data for a super-
vised clustering algorithm, which automatically par-
titions the dataset into several domains. In the eval-
uation, we show that our automatically-partitioned
domains improve the performances of two popular
MDL methods on real sentiment analysis data sets.
Note that Joshi et al. (2013) proposed a Multi-
Attribute Multi-Domain learning (MAMD) method,
which also exploited multiple dimensions of meta-
</bodyText>
<page confidence="0.978945">
869
</page>
<bodyText confidence="0.9112516">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 869–873,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
data and provided extensions to two traditional MDL
methods. However, extensions to the MAMD set-
ting may not be trivial for every MDL algorithm,
while our method serves as a pre-processing step and
can be easily used for all MDL approaches. In ad-
dition to this, MAMD only works with categorical
metadata, and can not fully utilize information in the
form of continuous metadata values.
</bodyText>
<sectionHeader confidence="0.974624" genericHeader="method">
2 Automatic Domain Partitioning
</sectionHeader>
<bodyText confidence="0.999944708333333">
In this section, we introduce the Automatic Domain
Partitioning (ADP) problem: given data X, meta-
data M and label Y , find a function g : M H I such
that the common MDL methods perform better with
data X and domain identity I in the prediction of Y .
For example, on Amazon sentiment analysis data, X
is the feature matrix extracted from reviews, Y is the
positive or negative label vector, and M is the meta-
data matrix associated with reviews (e.g. product
price and category).
Our approach works as follows: in training, we
first use an empirical domain difference testing
method to detect whether two groups of data should
be considered as different domains; after that we ap-
ply supervised clustering to learn the distance met-
ric between two data points, i.e. how different thay
are in MDL view, from training data generated by
our domain difference test method; finally, based on
the distance metric learned, we cluster our data into
several clusters, and train MDL models with those
clusters as domain labels; in testing, we assign data
instance to its nearest cluster and use that cluster as
its domain identity, and then apply the trained MDL
models for prediction.
</bodyText>
<subsectionHeader confidence="0.981718">
2.1 Empirical Domain Difference Test
</subsectionHeader>
<bodyText confidence="0.971893357142857">
The key motivation of MDL is that a model fits for
one domain may not fit well for other domains. Fol-
lowing the same motivation, we propose an empiri-
cal method for domain difference test called Domain
Model Loss (DML) that provides us the domain dif-
ference score d(G1, G2) between two groups of data
G1 = {X1, Y1I and G2 = {X2, Y2I.
Domain Model Loss If the mapping functions f1 :
X1 H Y1 and f2 : X2 H Y2 are different for
two data groups, we could directly use the disagree-
ment of f1 and f2 as domain difference score. More
specifically, if we train two classifiers �f1 : X1 ��
Y1, f2 : X2 H Y2 individually on G1 and G2, we
could have the K-fold empirical loss:
</bodyText>
<equation confidence="0.9898866">
�l(f1, G1) =1
K
i
l(f2, G2) = 1K
i
</equation>
<bodyText confidence="0.8903745">
And we could also apply the trained model f1 on
G2, and f2 on G1 to get:
�l(f1, G2) = Error of f1 on G2,
�l(f2, G1) = Error of f2 on G1.
Then, if G1 and G2 are actually the same with each
other, then both models will have same empirical
loss on either data set, but if they are not, we will
have a positive DML score:
</bodyText>
<equation confidence="0.986437375">
DML(G1, G2) = �(�L(f1, G2) + �L(f2, G1)),
1
where:
�l(f1, G2) − �l(f1, G1)
,
�l(f1, G1)
�l(f2, G1) − l(f2, G2)
�l(f2, G2) .
</equation>
<subsectionHeader confidence="0.980166">
2.2 Supervised Clustering for Domain
Partitioning
</subsectionHeader>
<bodyText confidence="0.994671785714286">
Our domain difference test method calculates the
distance between two partitioned data groups. How-
ever, to directly use it for domain partitioning, we
must go through all possible combinations of do-
main assignments in exponential time, which is in-
feasible. Our solution is to use a polynomial-time
supervised clustering method developed by Xing et
al. (2002) to learn a distance function that calculates
the distance between any two data points. Formally,
given a set of data pairs D, which belong to different
domains, and a set of data pairs S, which belong to
the same domain, it learns a distance metric A by:
Error of f1 on i-th fold of G1,
Error of f2 on i-th fold of G2.
</bodyText>
<equation confidence="0.859351">
�L(f1, G2) =
�L(f2, G1) =
11 &apos; (mi − mj)&apos;A(mi − mj)
maxg(A) =
A
(i,j)ED
s.t.f(A) = 11 (mi − mj)&apos;A(mi − mj) &lt; 1
(i,j)ES
A &gt;- 0,
</equation>
<page confidence="0.956187">
870
</page>
<bodyText confidence="0.999840133333333">
where mZ, mj are meta data of i and j.
The metadata M are preprocessed as follows: 1)
Each categorical attribute was converted to several
binary questions, one per category, and each bi-
nary question was considered as one metadata di-
mension in ADP method. For example, if categor-
ical attribute “Product Type” has two values “Mu-
sic” and “Electronics”, then there will be two dimen-
sions of metadata corresponding to “Product Type”
in ADP. Two metadata dimensions correspond to bi-
nary questions: “Is Product Type Music” and “Is
Product Type Electronics”. 2) Each continuous at-
tribute was normalized by scaling between 0 and 1.
The training data 5, D for metric learning are gen-
erated as follows:
</bodyText>
<listItem confidence="0.979588222222222">
1. For each dimension Mk of M, split data at
value 0.5, sample two equally sized groups, ap-
ply our domain difference testing method and
find the difference between these data groups.
2. Assign distance to each pair of instances by the
average distance of all partitions that partitions
the pair into different groups.
3. Select top n similar pairs as 5 and top n differ-
ent pairs as D.
</listItem>
<bodyText confidence="0.999911">
The learned distance metric A now conveys the
domain difference information obtained from our
domain distance test results: which meta attributes
are important for domain partitioning and which are
not as important. Following Xing et al. (2002), we
transfer the instance’s metadata feature M by MBT ,
where BTB = A. Then we use a clustering method
on MBT, and the output is our domain partitioning
result.
</bodyText>
<sectionHeader confidence="0.997913" genericHeader="method">
3 Experiment Methodology
</sectionHeader>
<bodyText confidence="0.999630053571429">
Datasets To evaluate our methods, we used two
subsets of Amazon review corpus (Jindal and Liu,
2008), which originally contain 5.8 million reviews
with a variety of metadata about products and users.
The first subset (BOOK) contains 20,000 reviews on
books published by eleven most popular publishers,
while the second (PROD) is reviews about products
within seven most common product categories. We
randomly split each dataset into training and testing
sets with equal size. The task is to predict a positive
or negative label for each review. Case insensitive
unigrams excluding stop words are used as features,
and all features appear less than 500 times are re-
moved for efficient experiment processing. Reviews
of 4 or 5 stars are considered positive and 1 or 2 stars
are considered negative, while 3 stars reviews are ex-
cluded. Each review has multiple metadata such as
book’s publisher, product’s type, user’s state loca-
tion, product price, review year, and number of other
user feedback. Reviews with missing metadata are
filtered out.
MDL Methods Our first MDL algorithm is the
Frustratingly Easy Domain Adaptation (FEDA)
(Daum´e III, 2007) which is easy to implement and
achieved competitive performance on many applica-
tions. It creates an augmented feature space as the
Cartesian product of the input features and the orig-
inal domains plus a shared domain. Then it uses a
SVM classifier over the augmented feature space to
obtain classification result. Specifically, our FEDA
methods use L2-regularized SVM with linear ker-
nel by LIBLINEAR package1. The parameters C =
0.01 was selected using five-fold cross-validation on
training set.
Our second MDL algorithm is Multi-Domain
Regularization (MDR) (Dredze and Crammer,
2008), which is a classifier combination ap-
proach based on Confidence-Weighted (CW) learn-
ing (Dredze et al., 2008). The CW learning is an on-
line update method that maintains probabilistic con-
fidence for each parameter by keeping track of its
variance. In our experiments, we use the CW im-
plementation provided by its authors and choose the
best performing configurations described in (Dredze
and Crammer, 2008).
Domain Partition Methods We evaluated the do-
main partition results provided by our ADP on the
two MDL methods (FEDA &amp; MDR). For simplic-
ity and efficiency, we use Naive Bayes as our base
prediction model f1 and f2 to generate the domain
model loss score, described in section 2.1. In train-
ing data generation, we choose top 10% similar pairs
as 5 and top 10% different pairs as D. And given
the learnt distance metric A, we use K-means to do
the clustering. The number of clusters is selected by
five-fold cross-validation on training set.
</bodyText>
<footnote confidence="0.993747">
1http://www.csie.ntu.edu.tw/—cjlin/liblinear
</footnote>
<page confidence="0.99818">
871
</page>
<bodyText confidence="0.999990785714286">
We compare our domain partition quality with
three other methods: 1) 1-Best chooses best per-
forming categorical metadata on a validation set as
domain indicators, where the original training set
was splitted equally to train and validate the per-
formance of each categorical attribute; 2) Random
partition that assigns domain identities to instances
randomly with same number of domains as 1-Best.
We run each random partition ten times and took the
average; 3) MAMD proposed by Joshi et al. (2013).
However, the original version of MAMD does not
support continuous attribute such as price. So we
made an extension that sorts these values to ten bins
and then treats them as categorical values.
</bodyText>
<sectionHeader confidence="0.998649" genericHeader="evaluation">
4 Results and Discussions
</sectionHeader>
<table confidence="0.999502">
Partition + MDL PROD BOOK
ADP +FEDA 82.02 ∗ 86.22 ‡∗
MAMD + FEDA 81.04 86.08
1-Best + FEDA 82.00 85.85
Random + FEDA 79.36 84.72
ADP + MDR 82.10� ‡ ∗ 86.62 # ‡ ∗
MAMD + MDR 80.17 84.37
1-Best + MDR 79.79 83.68
Random + MDR 74.65 81.16
</table>
<tableCaption confidence="0.9805884">
Table 1: Overall accuracies on PROD and BOOK
datasets. ADP results that are statistically significantly
better than MAMD are marked with �, and better than 1-
Best and Random are indicated by ‡ and ∗ respectively,
using a paired t-test, with p &lt; 0.05.
</tableCaption>
<bodyText confidence="0.999307903846154">
Table 1 shows the overall experimental results
of four domain partition methods with two MDL
methods on PROD and BOOK datasets. One could
see that when using MDR method, ADP could
significantly outperform all baselines on both data
sets, with relatively more than 2% gains. For
FEDA, on PROD data, ADP performs the same with
MAMD and 1-Best; on BOOK data, ADP outper-
forms 1-Best significantly, but is just slightly better
than MAMD. One possible reason is that the best
numbers of cluster selected by cross-validation are
around 150. With such large number of none-perfect
domains, FEDA will generate huge dimension of
features and perhaps require more training data to
provide better performances. Another possible rea-
son is that FEDA and the SVM underlying FEDA
are very robust against bad domain partition results.
This might be the reason of high FEDA baselines.
In general, our ADP method helps existing MDL
approaches achieve better performance, while bad
(Random) partitioning does hurt.
Figure 1(a) and 1(b) shows the performances of
applying FEDA on different domain partitioning
methods on PROD and BOOK, while Figure 1(c)
and 1(d) shows experiment results with MDR. The
x-axis is the size of the output domains (the K in
our K-means clustering), and y-axis is the accuracy
of models. With our domain partitioning approach,
MDR can perform consistently higher than all the
three baselines on both dataset when k &gt; 50. As
we discussed for Table 1, FEDA might be less sen-
sitive to domain partition results, which causes high
baseline performance and high ADP+FEDA perfor-
mance with small K. Since the performance trends
to increase along with k until 50 in three figures
(1(b), 1(c) and 1(d)), we believe that the ground-
truth domain size is likely larger than 50. These
results clearly indicate ADP does provide more de-
sirable domain assignments for MDL. The domain
selected by 1-Best such as publishers has only 11
domains, which limits the ability of 1-Best to com-
pletely express domain information. And our gener-
ated domains integrate multiple metadata attributes,
lead to more detailed domain partitions, and enhance
the ability of MDL methods to capture the difference
between different groups of data. Although accu-
racies are growing with k in general, we also see
that there are fluctuations on curves especially when
curves are zoomed to a small range. To get smoother
results, we can sample more data to calculate do-
main similarity and repeat the K-means clustering
with more different initializations.
</bodyText>
<sectionHeader confidence="0.996492" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999905222222222">
In this paper, we propose an Automatic Domain Par-
tition (ADP) method that provides better domain
identities for multi-domain learning methods. We
first propose a new approach to identify whether two
data groups should be considered as different do-
mains, by comparing the differences using Domain
Model Loss. We use a supervised clustering ap-
proach to train our model with labels generated by
domain difference tests, and cluster the re-weighted
</bodyText>
<page confidence="0.991505">
872
</page>
<bodyText confidence="0.99968225">
metadata as our domain partition by K-means. Ex-
periments on real world multi-domain data show
that the domain identities generated by our method
can improve the performance of MDL models.
</bodyText>
<figure confidence="0.876208">
(c) MDR results on PROD
Number of domains (K)
(d) MDR results on BOOK
</figure>
<figureCaption confidence="0.9904535">
Figure 1: Accuracies over different size of the output do-
mains (K)
</figureCaption>
<sectionHeader confidence="0.990372" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997090020408164">
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2006. Analysis of representations for
domain adaptation. In Advances in Neural Informa-
tion Processing Systems (NIPS), pages 137–144.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 79(1-2):151–175.
Mark Dredze and Koby Crammer. 2008. Online methods
for multi-domain learning and adaptation. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 689–697.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
Machine Learning, Proceedings of the Twenty-Fifth
International Conference (ICML), pages 264–271.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Proceed-
ings of the 2009 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies (NAACL-HLT),
pages 602–610.
Hal Daum´e III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics (ACL).
Nitin Jindal and Bing Liu. 2008. Opinion spam and anal-
ysis. In Proceedings of the International Conference
on Web Search and Web Data Mining (WSDM), pages
219–230.
Mahesh Joshi, Mark Dredze, William W. Cohen, and
Carolyn Penstein Ros´e. 2012. Multi-domain learn-
ing: When do domains matter? In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, (EMNLP-CoNLL), pages 1302–
1312.
Mahesh Joshi, Mark Dredze, William W. Cohen, and
Carolyn P. Ros´e. 2013. Whats in a domain? multi-
domain learning for multi-attribute data. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies (NAACL-HLT),
pages 685–690, Atlanta, Georgia, June. Association
for Computational Linguistics.
Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and Stu-
art J. Russell. 2002. Distance metric learning with
application to clustering with side-information. In
Advances in Neural Information Processing Systems
(NIPS), pages 505–512.
</reference>
<figure confidence="0.996966018181819">
82.5
82
81.5
81
ADP
1−Best
Random
MAMD
79.5
790 50 100 150 200
Number of domains (K)
(a) FEDA results on PROD
87
86.5
86
ADP
1−Best
Random
MAMD
84.50 50 100 150 200
Number of domains (K)
(b) FEDA results on BOOK
Accuracy %
Number of domains (K)
Accuracy %
Accuracy %
84
82
80
78
76
74
720 50
100 150 200
ADP
1−Best
Random
MAMD
80.5
80
85.5
85
0 50 100 150 200
81
87
86
Accuracy %
85
84
83
82
ADP
1−Best
Random
MAMD
</figure>
<page confidence="0.986273">
873
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.943758">
<title confidence="0.99979">Automatic Domain Partitioning for Multi-Domain Learning</title>
<author confidence="0.999977">Di_Wang</author>
<affiliation confidence="0.9999235">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.998953">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999779">diwang@cs.cmu.edu</email>
<author confidence="0.996313">Chenyan Xiong</author>
<affiliation confidence="0.9999535">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.997975">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999671">cx@cs.cmu.edu</email>
<author confidence="0.999877">William Yang Wang</author>
<affiliation confidence="0.9999235">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.997895">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999856">ww@cmu.edu</email>
<abstract confidence="0.997534578947369">Multi-Domain learning (MDL) assumes that the domain labels in the dataset are known. However, when there are multiple metadata attributes available, it is not always straightforward to select a single best attribute for domain partition, and it is possible that combining more than one metadata attributes (including continuous attributes) can lead to better MDL performance. In this work, we propose an automatic domain partitioning approach that aims at providing better domain identities for MDL. We use a supervised clustering approach that learns the domain distance between data instances , and then cluster the data into better domains for MDL. Our experiment on real multi-domain datasets shows that using our automatically generated domain partition improves over popular MDL methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shai Ben-David</author>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Analysis of representations for domain adaptation.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<pages>137--144</pages>
<marker>Ben-David, Blitzer, Crammer, Pereira, 2006</marker>
<rawString>Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. 2006. Analysis of representations for domain adaptation. In Advances in Neural Information Processing Systems (NIPS), pages 137–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Ben-David</author>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Alex Kulesza</author>
<author>Fernando Pereira</author>
<author>Jennifer Wortman Vaughan</author>
</authors>
<title>A theory of learning from different domains.</title>
<date>2010</date>
<booktitle>Machine Learning,</booktitle>
<pages>79--1</pages>
<contexts>
<context position="1927" citStr="Ben-David et al., 2010" startWordPosition="291" endWordPosition="294">mains and make use of domain labels to improve modeling performance (Daum´e III, 2007). The motivation of using MDL is that datasets from different domains could be different, in two ways. First, the feature distribution p(x) could be domain specific, meaning that the importance of each feature is different across domains. Second, the distribution of label Y given X, p(ylx), of different domains could be different. These differences could create problems for traditional machine learning methods: models learned from one domain might not be generalizable to other domains (BenDavid et al., 2006; Ben-David et al., 2010). One common assumption of MDL methods is that the domain identities are pre-defined. For example, in the multi-domain Amazon product review dataset (Finkel and Manning, 2009), the product categories are typically used as the domain identities. However, a question raised by Joshi et al. (2012) is that, in real-world data sets, there could be many ways to split data into domains, and it is hard to decide which one to use. Consider the Amazon product reviews, where we have multiple attributes attached to each review: for example, product category, reviewer location, price, and number of feedback</context>
</contexts>
<marker>Ben-David, Blitzer, Crammer, Kulesza, Pereira, Vaughan, 2010</marker>
<rawString>Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. 2010. A theory of learning from different domains. Machine Learning, 79(1-2):151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Koby Crammer</author>
</authors>
<title>Online methods for multi-domain learning and adaptation.</title>
<date>2008</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>689--697</pages>
<contexts>
<context position="10590" citStr="Dredze and Crammer, 2008" startWordPosition="1776" endWordPosition="1779">in Adaptation (FEDA) (Daum´e III, 2007) which is easy to implement and achieved competitive performance on many applications. It creates an augmented feature space as the Cartesian product of the input features and the original domains plus a shared domain. Then it uses a SVM classifier over the augmented feature space to obtain classification result. Specifically, our FEDA methods use L2-regularized SVM with linear kernel by LIBLINEAR package1. The parameters C = 0.01 was selected using five-fold cross-validation on training set. Our second MDL algorithm is Multi-Domain Regularization (MDR) (Dredze and Crammer, 2008), which is a classifier combination approach based on Confidence-Weighted (CW) learning (Dredze et al., 2008). The CW learning is an online update method that maintains probabilistic confidence for each parameter by keeping track of its variance. In our experiments, we use the CW implementation provided by its authors and choose the best performing configurations described in (Dredze and Crammer, 2008). Domain Partition Methods We evaluated the domain partition results provided by our ADP on the two MDL methods (FEDA &amp; MDR). For simplicity and efficiency, we use Naive Bayes as our base predict</context>
</contexts>
<marker>Dredze, Crammer, 2008</marker>
<rawString>Mark Dredze and Koby Crammer. 2008. Online methods for multi-domain learning and adaptation. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 689–697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Confidence-weighted linear classification.</title>
<date>2008</date>
<booktitle>In Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML),</booktitle>
<pages>264--271</pages>
<contexts>
<context position="10699" citStr="Dredze et al., 2008" startWordPosition="1793" endWordPosition="1796">plications. It creates an augmented feature space as the Cartesian product of the input features and the original domains plus a shared domain. Then it uses a SVM classifier over the augmented feature space to obtain classification result. Specifically, our FEDA methods use L2-regularized SVM with linear kernel by LIBLINEAR package1. The parameters C = 0.01 was selected using five-fold cross-validation on training set. Our second MDL algorithm is Multi-Domain Regularization (MDR) (Dredze and Crammer, 2008), which is a classifier combination approach based on Confidence-Weighted (CW) learning (Dredze et al., 2008). The CW learning is an online update method that maintains probabilistic confidence for each parameter by keeping track of its variance. In our experiments, we use the CW implementation provided by its authors and choose the best performing configurations described in (Dredze and Crammer, 2008). Domain Partition Methods We evaluated the domain partition results provided by our ADP on the two MDL methods (FEDA &amp; MDR). For simplicity and efficiency, we use Naive Bayes as our base prediction model f1 and f2 to generate the domain model loss score, described in section 2.1. In training data gener</context>
</contexts>
<marker>Dredze, Crammer, Pereira, 2008</marker>
<rawString>Mark Dredze, Koby Crammer, and Fernando Pereira. 2008. Confidence-weighted linear classification. In Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML), pages 264–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Hierarchical bayesian domain adaptation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),</booktitle>
<pages>602--610</pages>
<contexts>
<context position="2102" citStr="Finkel and Manning, 2009" startWordPosition="317" endWordPosition="320">t, in two ways. First, the feature distribution p(x) could be domain specific, meaning that the importance of each feature is different across domains. Second, the distribution of label Y given X, p(ylx), of different domains could be different. These differences could create problems for traditional machine learning methods: models learned from one domain might not be generalizable to other domains (BenDavid et al., 2006; Ben-David et al., 2010). One common assumption of MDL methods is that the domain identities are pre-defined. For example, in the multi-domain Amazon product review dataset (Finkel and Manning, 2009), the product categories are typically used as the domain identities. However, a question raised by Joshi et al. (2012) is that, in real-world data sets, there could be many ways to split data into domains, and it is hard to decide which one to use. Consider the Amazon product reviews, where we have multiple attributes attached to each review: for example, product category, reviewer location, price, and number of feedback. Which attribute is the most informative domain label? Or we should use all of these meta-data and partition the data into many small domains? In this paper, we investigate t</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2009. Hierarchical bayesian domain adaptation. In Proceedings of the 2009 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 602–610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
</authors>
<title>Opinion spam and analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Web Search and Web Data Mining (WSDM),</booktitle>
<pages>219--230</pages>
<contexts>
<context position="8956" citStr="Jindal and Liu, 2008" startWordPosition="1519" endWordPosition="1522">into different groups. 3. Select top n similar pairs as 5 and top n different pairs as D. The learned distance metric A now conveys the domain difference information obtained from our domain distance test results: which meta attributes are important for domain partitioning and which are not as important. Following Xing et al. (2002), we transfer the instance’s metadata feature M by MBT , where BTB = A. Then we use a clustering method on MBT, and the output is our domain partitioning result. 3 Experiment Methodology Datasets To evaluate our methods, we used two subsets of Amazon review corpus (Jindal and Liu, 2008), which originally contain 5.8 million reviews with a variety of metadata about products and users. The first subset (BOOK) contains 20,000 reviews on books published by eleven most popular publishers, while the second (PROD) is reviews about products within seven most common product categories. We randomly split each dataset into training and testing sets with equal size. The task is to predict a positive or negative label for each review. Case insensitive unigrams excluding stop words are used as features, and all features appear less than 500 times are removed for efficient experiment proce</context>
</contexts>
<marker>Jindal, Liu, 2008</marker>
<rawString>Nitin Jindal and Bing Liu. 2008. Opinion spam and analysis. In Proceedings of the International Conference on Web Search and Web Data Mining (WSDM), pages 219–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mahesh Joshi</author>
<author>Mark Dredze</author>
<author>William W Cohen</author>
<author>Carolyn Penstein Ros´e</author>
</authors>
<title>Multi-domain learning: When do domains matter?</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<volume>(EMNLP-CoNLL),</volume>
<pages>1302--1312</pages>
<marker>Joshi, Dredze, Cohen, Ros´e, 2012</marker>
<rawString>Mahesh Joshi, Mark Dredze, William W. Cohen, and Carolyn Penstein Ros´e. 2012. Multi-domain learning: When do domains matter? In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, (EMNLP-CoNLL), pages 1302– 1312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mahesh Joshi</author>
<author>Mark Dredze</author>
<author>William W Cohen</author>
<author>Carolyn P Ros´e</author>
</authors>
<title>Whats in a domain? multidomain learning for multi-attribute data.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),</booktitle>
<pages>685--690</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<marker>Joshi, Dredze, Cohen, Ros´e, 2013</marker>
<rawString>Mahesh Joshi, Mark Dredze, William W. Cohen, and Carolyn P. Ros´e. 2013. Whats in a domain? multidomain learning for multi-attribute data. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 685–690, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric P Xing</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
<author>Stuart J Russell</author>
</authors>
<title>Distance metric learning with application to clustering with side-information.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<pages>505--512</pages>
<contexts>
<context position="6897" citStr="Xing et al. (2002)" startWordPosition="1149" endWordPosition="1152">ical loss on either data set, but if they are not, we will have a positive DML score: DML(G1, G2) = �(�L(f1, G2) + �L(f2, G1)), 1 where: �l(f1, G2) − �l(f1, G1) , �l(f1, G1) �l(f2, G1) − l(f2, G2) �l(f2, G2) . 2.2 Supervised Clustering for Domain Partitioning Our domain difference test method calculates the distance between two partitioned data groups. However, to directly use it for domain partitioning, we must go through all possible combinations of domain assignments in exponential time, which is infeasible. Our solution is to use a polynomial-time supervised clustering method developed by Xing et al. (2002) to learn a distance function that calculates the distance between any two data points. Formally, given a set of data pairs D, which belong to different domains, and a set of data pairs S, which belong to the same domain, it learns a distance metric A by: Error of f1 on i-th fold of G1, Error of f2 on i-th fold of G2. �L(f1, G2) = �L(f2, G1) = 11 &apos; (mi − mj)&apos;A(mi − mj) maxg(A) = A (i,j)ED s.t.f(A) = 11 (mi − mj)&apos;A(mi − mj) &lt; 1 (i,j)ES A &gt;- 0, 870 where mZ, mj are meta data of i and j. The metadata M are preprocessed as follows: 1) Each categorical attribute was converted to several binary ques</context>
<context position="8669" citStr="Xing et al. (2002)" startWordPosition="1469" endWordPosition="1472"> dimension Mk of M, split data at value 0.5, sample two equally sized groups, apply our domain difference testing method and find the difference between these data groups. 2. Assign distance to each pair of instances by the average distance of all partitions that partitions the pair into different groups. 3. Select top n similar pairs as 5 and top n different pairs as D. The learned distance metric A now conveys the domain difference information obtained from our domain distance test results: which meta attributes are important for domain partitioning and which are not as important. Following Xing et al. (2002), we transfer the instance’s metadata feature M by MBT , where BTB = A. Then we use a clustering method on MBT, and the output is our domain partitioning result. 3 Experiment Methodology Datasets To evaluate our methods, we used two subsets of Amazon review corpus (Jindal and Liu, 2008), which originally contain 5.8 million reviews with a variety of metadata about products and users. The first subset (BOOK) contains 20,000 reviews on books published by eleven most popular publishers, while the second (PROD) is reviews about products within seven most common product categories. We randomly spli</context>
</contexts>
<marker>Xing, Ng, Jordan, Russell, 2002</marker>
<rawString>Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and Stuart J. Russell. 2002. Distance metric learning with application to clustering with side-information. In Advances in Neural Information Processing Systems (NIPS), pages 505–512.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
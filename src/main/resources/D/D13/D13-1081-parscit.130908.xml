<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008308">
<title confidence="0.984291">
What is Hidden among Translation Rules
</title>
<author confidence="0.950539">
Libin Shen
</author>
<affiliation confidence="0.815101">
Persado
</affiliation>
<address confidence="0.970049">
50 West 17th Street
New York, NY 10011
</address>
<email confidence="0.997237">
libin.shen@persado.com
</email>
<note confidence="0.5785285">
Bowen Zhou
IBM T. J. Watson Research Center
</note>
<address confidence="0.8675505">
1101 Kitchawan Road
Yorktown Heights, NY 10598
</address>
<email confidence="0.99816">
zhou@us.ibm.com
</email>
<sectionHeader confidence="0.993868" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999451272727273">
Most of the machine translation systems rely
on a large set of translation rules. These rules
are treated as discrete and independent events.
In this short paper, we propose a novel method
to model rules as observed generation output
of a compact hidden model, which leads to
better generalization capability. We present a
preliminary generative model to test this idea.
Experimental results show about one point im-
provement on TER-BLEU over a strong base-
line in Chinese-to-English translation.
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952525">
Most of the modern Statistical Machine Transla-
tion (SMT) systems, for example (Koehn et al.,
2003; Och and Ney, 2004; Chiang, 2005; Marcu et
al., 2006; Shen et al., 2008), employ a large rule
set that may contain tens of millions of translation
rules or even more. In these systems, each transla-
tion rule has about 20 dense features, which repre-
sent key statistics collected from the training data,
such as word translation probability, phrase transla-
tion probability etc. Except for these common fea-
tures, there is no connection among the translation
rules. The translation rules are treated as indepen-
dent events.
The use of sparse features as in (Arun and Koehn,
2007; Watanabe et al., 2007; Chiang et al., 2009) to
some extent mitigated this problem. In their work,
there are as many as 10,000 features defined on the
appearance of certain frequent words and Part of
Speech (POS) tags in rules. They provide signifi-
cant improvement in automatic evaluation metrics.
However, these sparse features fire quite randomly
and infrequently on each rule. Thus, there is still
plenty of space to better model translation rules.
In this paper, we will explore the relationship
among translation rules. We no longer view rules
as discrete or unrelated events. Instead, we view
rules, which are observed from training data, as ran-
dom variables generated by a hidden model. This
generative process itself is also hidden. All possible
generative processes can be represented with factor-
ized structures such as weighted hypergraphs and fi-
nite state machines. This approach leads to a com-
pact model that has better generalization capability
and allows translation rules not explicitly observed
in training date.
This paper reports work-in-progress to exploit
hidden relations among rules. Preliminary experi-
ments show about one point improvement on TER-
BLEU over a strong baseline in Chinese-to-English
translation.
</bodyText>
<sectionHeader confidence="0.984958" genericHeader="method">
2 Hidden Models
</sectionHeader>
<bodyText confidence="0.998078642857143">
Let !9 = {(r, f)} be a grammar observed from paral-
lel training data, where f is the frequency of a bilin-
gual translation rule r.
Let M be a hidden model that generates every
translation rule r. For example, M could be mod-
eled with a weighted hypergraph or finite state ma-
chine. For the sake of convenience, in this section
we assume M is a meta-grammar M = {m}, where
each m represents a meta-rule. For each translation
r, there exists a hypergraph Hr that represents all
possible derivations Dr = {d} that can generate rule
r. Here, each derivation d is a hyperpath using meta-
rules Md, where Md C M. Thus, we can use hy-
pergraph Hr to characterize r. Translation rules in !g
</bodyText>
<page confidence="0.980061">
839
</page>
<bodyText confidence="0.888858625">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 839–844,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
can share nodes and meta-rules in their hypergraphs,
so that M is more compact model than G.
In the rest of this section, we will introduce three
methods to quantify Hr as features of rule r. It
should be noted that there are more ways to exploit
the compact model of M than these three.
</bodyText>
<subsectionHeader confidence="0.965215">
2.1 Type 1 : A Generative Model
</subsectionHeader>
<bodyText confidence="0.9998152">
Let 0 be the parameters of a statistical model
Pr(m; 0) for meta-rules m in meta-grammar M es-
timated from the observed translation grammar G.
The probability of a translation rule r can be calcu-
lated as follows.
</bodyText>
<equation confidence="0.9016876">
Pr(r; 0) ∝ Pr(Hr; 0)
E= Pr(d; 0) (1)
dED,.
By assuming separability,
Pr(d; 0) = 11 Pr(m; 0) (2)
mEMd
we can further decompose rule probability Pr(r; 0)
as below.
Pr(r; 0) = � 11 Pr(m; 0) (3)
dED,. mEMd
</equation>
<bodyText confidence="0.995961111111111">
In practice, Pr(r; 0) in (3) can be calculated
through bottom-up dynamic programming on hyper-
graph Hr. Hypergraphs of different rules can share
nodes and meta-rules. This reveals the underlying
relationship among translation rules.
As a by-product of this generative model, we use
the log-likelihood of a translation rule, log Pr(r; 0),
as a new dense feature. We call it Type 1 in experi-
ments.
</bodyText>
<subsectionHeader confidence="0.988552">
2.2 Type 2 : Meta-Rules as Sparse Features
</subsectionHeader>
<bodyText confidence="0.990203846153846">
As given in (3), likelihood of a translation rule is
a function over Pr(m; 0), in which 0 is estimated
from the training data with a generative model. Pre-
vious work in (Chiang et al., 2009) showed the ad-
vantage of using a discriminative model to optimize
individual weights for these factors towards a better
automatic score.
Following this practice, we treat each meta-rule
m as a sparse feature. Feature value f(m) = 1 if
and only if m is used in hypergraph Hr. Otherwise,
its default value is 0. We call these features Type
2 in experiments. The Type 2 system contains the
log-likelihood feature in Type 1.
</bodyText>
<subsectionHeader confidence="0.802958">
2.3 Type 3 : Posterior as Feature Values
</subsectionHeader>
<bodyText confidence="0.9999513">
A natural question on the binary sparse features de-
fined above is why all the active features have the
same value of 1. We use these meta-rules to repre-
sent a translation rule in feature space. Intuitively,
for meta-rules with closer connection to the trans-
lation rules, we hope to use relatively larger feature
values to increase their effect.
We formalize this intuition with the posterior
probability that a meta-rule m is used to generate
r, as below.
</bodyText>
<equation confidence="0.9993695">
f(m) ≡ Pr(m|r; 0) (4)
Pr(m, r; 0)
=
Pr(r; 0)
= EdED,.,mEMd Pr(d; 0)
Pr(r; 0)
</equation>
<bodyText confidence="0.972048">
The posterior in (4) could be too sharp. Follow-
ing the common practice, we smooth the posterior
features with a scaling factor α.
</bodyText>
<equation confidence="0.70654">
f(m) ≡ Pr(m|r)α
</equation>
<bodyText confidence="0.924579">
We use Type 3(α) to represent the posterior model
with a scaling factor of α in experiments. The Type
3 systems also contain the log-likelihood feature in
Type 1.
</bodyText>
<subsectionHeader confidence="0.994473">
2.4 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999919785714286">
Now we explain how to obtain parameter 0. With
proper definition of the underlying model M, we
can estimate 0 with the traditional EM algorithm or
Bayesian methods.
In the next section, we will present an example
of the hidden model. We will employ the EM algo-
rithm to estimate the parameters in 0. Here, trans-
lation rules and their frequencies in G are observed
data, and derivation d for each rule r is hidden. At
the Expectation step, we search all derivations d in
Dr of each rule r and calculate their probabilities
according to equation (2). At the Maximization step,
we re-estimate 0 on all derivations in proportion to
their posterior probability.
</bodyText>
<page confidence="0.987827">
840
</page>
<sectionHeader confidence="0.935994" genericHeader="method">
3 Case Study
</sectionHeader>
<bodyText confidence="0.999894625">
In Section 2, we explored the use of meta-grammars
as the underlying model M and developed three
methods to define features. Similar techniques can
be applied to finite state machines and other underly-
ing models. Now, we introduce a POS-based under-
lying model to illustrate the generic model proposed
in Section 2. We will show experimental results in
Section 4.
</bodyText>
<subsectionHeader confidence="0.999787">
3.1 Meta-rules on POS tags
</subsectionHeader>
<bodyText confidence="0.9964389">
Let r E !g be a translation rule composed of a pair of
source and target word strings (F,,,, E,,,). Let Fp and
Ep be the POS tags for the source and target sides
respectively. For the sake of simplicity as the first
attempt, we treat non-terminal as a special word X
with POS tag X.
Suppose we have a Chinese-to-English translation
rule as below.
yuehan qu zhijiage ==&gt;. john leaves for chicago
We call
</bodyText>
<equation confidence="0.928466">
NR VV NR ==&gt;. NNP VBZ IN NNP (5)
</equation>
<bodyText confidence="0.9956754">
a translation rule in POS tags.
We will propose an underlying model M to gen-
erate translation rules in POS tags instead of trans-
lation rules themselves. For the rest of this section,
we take translation rules in POS tags as the target of
our generative model. We define meta-rules on pairs
of POS tag strings, e.g. NR VV ==&gt;. NNP VBZ.
We can decompose the probability of translation
rule in (5) into a product on meta-rule probabilities
via various derivations, such as
</bodyText>
<listItem confidence="0.99819">
• Pr(NR VV , NNP VBZ) x
Pr(NR, IN NNP), and
• Pr(NR, NNP) x Pr(VV, VBZ IN) x
Pr(NR, NNP).
</listItem>
<subsectionHeader confidence="0.996337">
3.2 The Underlying Model and Features
</subsectionHeader>
<bodyText confidence="0.999974166666667">
Now, we introduce a generative model M for trans-
lation rules in POS tags. We still use the example in
(5) as shown in Figure 1, where the top box repre-
sents the source side and the bottom box represents
the target side. Dotted lines represent word align-
ments on three pairs of words.
</bodyText>
<figureCaption confidence="0.999456">
Figure 1: An example
</figureCaption>
<bodyText confidence="0.988218333333333">
We first generate the number of source tokens of
a translation rule with a uniform distribution for up
to, for example, 7 tokens.
Then we split the source side into chunks with
a binomial distribution with a Bernoulli variable at
the gap between each two continuous words, which
splits the two words into two chunks with a proba-
bility of p. For example, the probability of obtaining
two chunks IR VV and IR is (1 − p)p, as shown in
Figure 1.
Suppose we split the target side into two parts,
IIP VBZ and II IIP, which respects the word
alignments. It generates two meta-rules IR VV ==&gt;.
IIP VBZ and IR ==&gt;. II IIP, as shown in Figure 1.
The probability for the first meta-rule is
</bodyText>
<equation confidence="0.995158">
Pr(|E |= 2  ||F |= 2) x
Pr(NR VV , NNP VBZ  ||F |= 2, |E |= 2),
</equation>
<bodyText confidence="0.999650333333333">
where |F |represents the number of source tokens,
and |E |the number of target tokens. Similarly, the
probability of the second one is as follows.
</bodyText>
<equation confidence="0.9884194">
Pr(|E |= 2  ||F |= 1) x
Pr(NR, IN NNP  ||F |= 1, |E |= 2).
To sum up, the probability of a derivation d for a
translation rule r : F ==&gt;. E is
Pr(d) Pz� Pro1(|F|)
x Pro2(F3)
11 x Pro,(|Em Fm|)
mEMd
11 x Pro,(m  ||Fm|, |Em|) (6)
mEMd
</equation>
<bodyText confidence="0.999595333333333">
where Fm and Em are source and target sides of a
meta-rule m used in derivation d, and F3 is a split-
ting of the source side. As for the distributions, we
</bodyText>
<page confidence="0.983733">
841
</page>
<table confidence="0.9519002">
have
01 ∼ Uniform
02 ∼ Binomial
03 ∼ Categorical
04 ∼ Categorical
</table>
<bodyText confidence="0.9998585">
where 01 and 02 have pre-selected hyperparameters,
and 03 and 04 are estimated with the EM algorithm.
As for sparse features, we will obtain 7 meta-rule
features as below.
</bodyText>
<listItem confidence="0.995557785714286">
• IR ⇒ IIP
• VV ⇒ VBZ
• VV ⇒ VBZ II
• IR VV ⇒ IIP VBZ
• IR VV ⇒ IIP VBZ II
• VVIR ⇒ VBZ II IIP
• IR VV IR ⇒ IIP VBZ II IIP
All of them respect the word alignment, which
means that
• there is no alignment that aligns one word in a
meta-rule with the other out of the same meta-
rule, and
• there is at least one alignment within a meta-
rule.
</listItem>
<subsectionHeader confidence="0.997927">
3.3 Implementation Details
</subsectionHeader>
<bodyText confidence="0.9998472">
Even though the size of all possible meta-rules is
much smaller than the space of translation rules,
it is still too large to work with existing optimiza-
tion methods for sparse features in MT, i.e. MIRA
(Chiang et al., 2009) or L-BFGS (Matsoukas et al.,
2009). In practice, we have to limit the feature space
to around 20,000 dimensions.
For this purpose, we first use a frequency based
method to filter meta-rule features. Specifically,
we first divide all the meta-rules into 100 bins,
(|F |, |E|), where |F  |is the number of words on the
source side, and |E |the target side, 0 &lt; |F |, |E |≤
10. For each bin, we keep the same top k-percentile
of the meta-rules such that we obtain a total of
20,000 meta-rules as features.
</bodyText>
<table confidence="0.999235285714286">
System BLEU% TER% T-B
Baseline 30.35 55.32 24.97
Type 1 30.74 55.48 24.74
Type 2 31.07 55.07 24.00
Type 3 (1) 30.93 55.34 24.41
Type 3 (0.1) 31.05 55.02 23.97
Type 3 (0.01) 31.09 54.96 23.87
</table>
<tableCaption confidence="0.999916">
Table 1: scores on test-1
</tableCaption>
<bodyText confidence="0.9999234">
A shortcoming of this filtering method is that all
these features are positive indicators, while low-
frequency negative indicators are discarded. In order
to keep the features of various level of frequency, we
define class features with a 3-tuple C(|F|, |E|, q),
where |F |and |E |are numbers of source and target
words as defined above, and q is the integer part of
the log2 value of the feature frequency in the training
data.
In this way, each meta-rule feature can be mapped
to one of these classes. The value of a class feature
equals the sum of the meta-rule features that mapped
into this class. We have about 2,000 class features
defined in this way. They are applied on both Type
2 and Type 3 features.
</bodyText>
<sectionHeader confidence="0.99965" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999981">
We carry out our experiments on web genre of
Chinese-to-English translation. The training set
contains about 10 million parallel sentences avail-
able to Phase 1 of the DARPA BOLT MT task. The
tune set contains 1275 sentences. Each has four ref-
erences. There are two test sets. Test-1 is from a
similar source of the tune set, and it contains 1239
sentences. Test-2 is the web part of the MT08 eval-
uation data.
Our baseline system is a home-made Hiero (Chi-
ang, 2005) style system. The baseline rule set con-
tains about 17 million rules. It contains about 40
dense features, including a 6-gram LM.
The sparse feature optimization algorithm is sim-
ilar to the MIRA recipe described in (Chiang et al.,
2009). We optimize on TER-BLEU (Snover et al.,
2006; Papineni et al., 2001).
The BLEU, TER and T-B scores on the two tests
are shown in Tables 1 and 2. It should be noted that,
even though our metric of tuning is T-B, the baseline
</bodyText>
<page confidence="0.993126">
842
</page>
<table confidence="0.999795142857143">
System BLEU% TER% T-B
Baseline 25.80 56.96 31.16
Type 1 26.18 57.09 30.91
Type 2 26.63 56.64 30.01
Type 3 (1) 26.30 57.00 30.70
Type 3 (0.1) 26.34 56.73 30.39
Type 3 (0.01) 26.50 56.73 30.23
</table>
<tableCaption confidence="0.999606">
Table 2: scores on test-2 (MT08-WB)
</tableCaption>
<bodyText confidence="0.999947">
system already provides a very competitive BLEU
score on MT08-WB as compared the best system in
the evaluation1, thanks to comprehensive features in
the baseline system and more data in training.
All the three types of systems provide consis-
tent improvement on both test sets in terms of T-B,
our optimization metric. Type 1 gives marginal im-
provement of 0.2. This shows the limitation of the
generative feature. When we use meta-rules as bi-
nary sparse features in Type 2, we obtain about one
point improvement on T-B on both sets. This shows
the advantage of tuning individual meta-rule weights
over a generative model. Type 3 (0.01) and Type 2
are at the same level. Proper smoothing is important
to Type 3.
</bodyText>
<sectionHeader confidence="0.998064" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999963210526316">
In the case study of Section 3, we use POS-based
rules as hidden states. However, it should be noted
that the hidden structures surely do not have to be
POS tags. For example, an alternative could be
unsupervised NT splitting similar to (Huang et al.,
2010).
The meta-grammar based approach was also mo-
tivated by the insight acquired on mono-lingual lin-
guistic grammar generation, especially in the TAG
related research (Xia, 2001; Prolo, 2002). Meta-
grammar was viewed as an effective way to remove
redundancy in grammars.
The link between Tree Adjoining Grammar
(TAG) (Joshi et al., 1975; Joshi and Schabes, 1997)
and MT was first introduced in (Shieber and Sch-
abes, 1990), a pioneer work in tree-to-tree transla-
tion. (DeNeefe and Knight, 2009) re-visited the use
of adjoining operation in the context of Statistical
MT, and reported encouraging results. On the other
</bodyText>
<footnote confidence="0.947838">
1http://www.itl.nist.gov/iad/mig/tests/mt/2008/
</footnote>
<bodyText confidence="0.9992906">
hand, (Dras, 1999) showed how a meta-level gram-
mar could help in modeling parallel operations in
(Shieber and Schabes, 1990). Our work is another
effort of statistical modeling of well-recognized lin-
guistic insight in NLP and MT.
</bodyText>
<sectionHeader confidence="0.996167" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9995102">
In this paper, we introduced a novel method to model
translation rules as observed generation output of a
compact hidden model. As a case study to capital-
ize this model, we presented three methods to en-
rich rule modeling with features defined on a hid-
den model. Preliminary experiments verified gain of
one point on TER-BLEU over a strong baseline in
Chinese-to-English translation.
As for future work, we plan to extend this work in
the following aspects.
</bodyText>
<listItem confidence="0.9983681">
• To try other prior distributions to generate the
number of source tokens.
• Unsupervised and semi-supervised learning of
hidden models.
• To incorporate rich models into the generative
process, e.g. reordering, non-terminals, struc-
tural information and lexical models.
• To improve the posterior model with better pa-
rameter estimation, e.g. Bayesian methods.
• To replace the exhaustive translation rule set
</listItem>
<bodyText confidence="0.96466225">
with a compact meta grammar that can create
and parameterize new translation rules dynam-
ically, which is the ultimate goal of this line of
work.
</bodyText>
<sectionHeader confidence="0.997483" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999788727272727">
We would like thank the anonymous reviewers for
their valuable comments. Haitao Mi and Martin
Cmejrek kindly helped on data preparation.
This work was done when the first author was
at IBM. The work was supported by DARPA un-
der Grant HR0011-12-C-0015 for funding part of
this work. The views, opinions, and/or findings con-
tained in this article/presentation are those of the au-
thor/presenter and should not be interpreted as repre-
senting the ofcial views or policies, either expressed
or implied, of the DARPA.
</bodyText>
<page confidence="0.998068">
843
</page>
<sectionHeader confidence="0.988708" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995402614457832">
Abhishek Arun and Philipp Koehn. 2007. Online
learning methods for discriminative training of phrase
based statistical machine translation. In Proceedings
ofMT Summit XI.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
Proceedings of the 2009 Human Language Technol-
ogy Conference of the North American Chapter of the
Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 263–270, Ann Ar-
bor, MI.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proceedings of
the 2009 Conference of Empirical Methods in Natural
Language Processing, pages 727–736, Singapore.
Mark Dras. 1999. A meta-level grammar: redefining
synchronous tag for translation and paraphrase. In
Proceedings of the 37th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL).
Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of the 2010 Conference of
Empirical Methods in Natural Language Processing.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages, vol-
ume 3, pages 69–124. Springer-Verlag.
Aravind K. Joshi, Leon S. Levy, and Masako Takahashi.
1975. Tree adjunct grammars. Journal of Computer
and System Sciences, 10(1):136–163.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase based translation. In Proceedings
of the 2003 Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 48–54, Edmonton,
Canada.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of the 2006 Conference of Empirical
Methods in Natural Language Processing, pages 44–
52, Sydney, Australia.
Spyros Matsoukas, Antti-Veikko Rosti, and Bing Zhang.
2009. Discriminative corpus weight estimation for
machine translation. In Proceedings of the 2009 Con-
ference of Empirical Methods in Natural Language
Processing.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4).
Kishore Papineni, Salim Roukos, and Todd Ward. 2001.
Bleu: a method for automatic evaluation of machine
translation. IBM Research Report, RC22176.
Carlos Prolo. 2002. Generating the xtag english gram-
mar using metarules. In Proceedings of the 19th in-
ternational conference on Computational linguistics
(COLING).
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL).
Stuart Shieber and Yves Schabes. 1990. Synchronous
tree adjoining grammars. In Proceedings of COLING
’90: The 13th Int. Conf. on Computational Linguistics,
pages 253–258, Helsinki, Finland.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings ofAssociation for Machine Translation
in the Americas, pages 223–231, Cambridge, MA.
T. Watanabe, J. Suzuki, H. Tsukuda, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. In Proceedings of the 2007 Confer-
ence of Empirical Methods in Natural Language Pro-
cessing.
F. Xia. 2001. Automatic Grammar Generation From
Two Different Perspectives. Ph.D. thesis, University
of Pennsylvania.
</reference>
<page confidence="0.998699">
844
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.154889">
<title confidence="0.938766">What is Hidden among Translation Rules</title>
<affiliation confidence="0.514694">Libin</affiliation>
<address confidence="0.892964">50 West 17th New York, NY</address>
<email confidence="0.999125">libin.shen@persado.com</email>
<author confidence="0.938532">Bowen</author>
<affiliation confidence="0.979908">IBM T. J. Watson Research</affiliation>
<address confidence="0.7045455">1101 Kitchawan Yorktown Heights, NY</address>
<email confidence="0.99936">zhou@us.ibm.com</email>
<abstract confidence="0.988005">Most of the machine translation systems rely on a large set of translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abhishek Arun</author>
<author>Philipp Koehn</author>
</authors>
<title>Online learning methods for discriminative training of phrase based statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofMT Summit XI.</booktitle>
<contexts>
<context position="1416" citStr="Arun and Koehn, 2007" startWordPosition="224" endWordPosition="227"> (SMT) systems, for example (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), employ a large rule set that may contain tens of millions of translation rules or even more. In these systems, each translation rule has about 20 dense features, which represent key statistics collected from the training data, such as word translation probability, phrase translation probability etc. Except for these common features, there is no connection among the translation rules. The translation rules are treated as independent events. The use of sparse features as in (Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2009) to some extent mitigated this problem. In their work, there are as many as 10,000 features defined on the appearance of certain frequent words and Part of Speech (POS) tags in rules. They provide significant improvement in automatic evaluation metrics. However, these sparse features fire quite randomly and infrequently on each rule. Thus, there is still plenty of space to better model translation rules. In this paper, we will explore the relationship among translation rules. We no longer view rules as discrete or unrelated events. Instead, we view </context>
</contexts>
<marker>Arun, Koehn, 2007</marker>
<rawString>Abhishek Arun and Philipp Koehn. 2007. Online learning methods for discriminative training of phrase based statistical machine translation. In Proceedings ofMT Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>K Knight</author>
<author>W Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1461" citStr="Chiang et al., 2009" startWordPosition="232" endWordPosition="235">03; Och and Ney, 2004; Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), employ a large rule set that may contain tens of millions of translation rules or even more. In these systems, each translation rule has about 20 dense features, which represent key statistics collected from the training data, such as word translation probability, phrase translation probability etc. Except for these common features, there is no connection among the translation rules. The translation rules are treated as independent events. The use of sparse features as in (Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2009) to some extent mitigated this problem. In their work, there are as many as 10,000 features defined on the appearance of certain frequent words and Part of Speech (POS) tags in rules. They provide significant improvement in automatic evaluation metrics. However, these sparse features fire quite randomly and infrequently on each rule. Thus, there is still plenty of space to better model translation rules. In this paper, we will explore the relationship among translation rules. We no longer view rules as discrete or unrelated events. Instead, we view rules, which are observed from training data,</context>
<context position="4903" citStr="Chiang et al., 2009" startWordPosition="822" endWordPosition="825">tice, Pr(r; 0) in (3) can be calculated through bottom-up dynamic programming on hypergraph Hr. Hypergraphs of different rules can share nodes and meta-rules. This reveals the underlying relationship among translation rules. As a by-product of this generative model, we use the log-likelihood of a translation rule, log Pr(r; 0), as a new dense feature. We call it Type 1 in experiments. 2.2 Type 2 : Meta-Rules as Sparse Features As given in (3), likelihood of a translation rule is a function over Pr(m; 0), in which 0 is estimated from the training data with a generative model. Previous work in (Chiang et al., 2009) showed the advantage of using a discriminative model to optimize individual weights for these factors towards a better automatic score. Following this practice, we treat each meta-rule m as a sparse feature. Feature value f(m) = 1 if and only if m is used in hypergraph Hr. Otherwise, its default value is 0. We call these features Type 2 in experiments. The Type 2 system contains the log-likelihood feature in Type 1. 2.3 Type 3 : Posterior as Feature Values A natural question on the binary sparse features defined above is why all the active features have the same value of 1. We use these meta-</context>
<context position="10737" citStr="Chiang et al., 2009" startWordPosition="1915" endWordPosition="1918">7 meta-rule features as below. • IR ⇒ IIP • VV ⇒ VBZ • VV ⇒ VBZ II • IR VV ⇒ IIP VBZ • IR VV ⇒ IIP VBZ II • VVIR ⇒ VBZ II IIP • IR VV IR ⇒ IIP VBZ II IIP All of them respect the word alignment, which means that • there is no alignment that aligns one word in a meta-rule with the other out of the same metarule, and • there is at least one alignment within a metarule. 3.3 Implementation Details Even though the size of all possible meta-rules is much smaller than the space of translation rules, it is still too large to work with existing optimization methods for sparse features in MT, i.e. MIRA (Chiang et al., 2009) or L-BFGS (Matsoukas et al., 2009). In practice, we have to limit the feature space to around 20,000 dimensions. For this purpose, we first use a frequency based method to filter meta-rule features. Specifically, we first divide all the meta-rules into 100 bins, (|F |, |E|), where |F |is the number of words on the source side, and |E |the target side, 0 &lt; |F |, |E |≤ 10. For each bin, we keep the same top k-percentile of the meta-rules such that we obtain a total of 20,000 meta-rules as features. System BLEU% TER% T-B Baseline 30.35 55.32 24.97 Type 1 30.74 55.48 24.74 Type 2 31.07 55.07 24.0</context>
<context position="12882" citStr="Chiang et al., 2009" startWordPosition="2300" endWordPosition="2303"> training set contains about 10 million parallel sentences available to Phase 1 of the DARPA BOLT MT task. The tune set contains 1275 sentences. Each has four references. There are two test sets. Test-1 is from a similar source of the tune set, and it contains 1239 sentences. Test-2 is the web part of the MT08 evaluation data. Our baseline system is a home-made Hiero (Chiang, 2005) style system. The baseline rule set contains about 17 million rules. It contains about 40 dense features, including a 6-gram LM. The sparse feature optimization algorithm is similar to the MIRA recipe described in (Chiang et al., 2009). We optimize on TER-BLEU (Snover et al., 2006; Papineni et al., 2001). The BLEU, TER and T-B scores on the two tests are shown in Tables 1 and 2. It should be noted that, even though our metric of tuning is T-B, the baseline 842 System BLEU% TER% T-B Baseline 25.80 56.96 31.16 Type 1 26.18 57.09 30.91 Type 2 26.63 56.64 30.01 Type 3 (1) 26.30 57.00 30.70 Type 3 (0.1) 26.34 56.73 30.39 Type 3 (0.01) 26.50 56.73 30.23 Table 2: scores on test-2 (MT08-WB) system already provides a very competitive BLEU score on MT08-WB as compared the best system in the evaluation1, thanks to comprehensive featur</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new features for statistical machine translation. In Proceedings of the 2009 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="876" citStr="Chiang, 2005" startWordPosition="135" endWordPosition="136">ion systems rely on a large set of translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation. 1 Introduction Most of the modern Statistical Machine Translation (SMT) systems, for example (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), employ a large rule set that may contain tens of millions of translation rules or even more. In these systems, each translation rule has about 20 dense features, which represent key statistics collected from the training data, such as word translation probability, phrase translation probability etc. Except for these common features, there is no connection among the translation rules. The translation rules are treated as independent events. The use of sparse features as in (Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2009) to some extent</context>
<context position="12646" citStr="Chiang, 2005" startWordPosition="2261" endWordPosition="2263">t mapped into this class. We have about 2,000 class features defined in this way. They are applied on both Type 2 and Type 3 features. 4 Experiments We carry out our experiments on web genre of Chinese-to-English translation. The training set contains about 10 million parallel sentences available to Phase 1 of the DARPA BOLT MT task. The tune set contains 1275 sentences. Each has four references. There are two test sets. Test-1 is from a similar source of the tune set, and it contains 1239 sentences. Test-2 is the web part of the MT08 evaluation data. Our baseline system is a home-made Hiero (Chiang, 2005) style system. The baseline rule set contains about 17 million rules. It contains about 40 dense features, including a 6-gram LM. The sparse feature optimization algorithm is similar to the MIRA recipe described in (Chiang et al., 2009). We optimize on TER-BLEU (Snover et al., 2006; Papineni et al., 2001). The BLEU, TER and T-B scores on the two tests are shown in Tables 1 and 2. It should be noted that, even though our metric of tuning is T-B, the baseline 842 System BLEU% TER% T-B Baseline 25.80 56.96 31.16 Type 1 26.18 57.09 30.91 Type 2 26.63 56.64 30.01 Type 3 (1) 26.30 57.00 30.70 Type 3</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL), pages 263–270, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve DeNeefe</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous tree adjoining machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference of Empirical Methods in Natural Language Processing,</booktitle>
<pages>727--736</pages>
<contexts>
<context position="14807" citStr="DeNeefe and Knight, 2009" startWordPosition="2635" endWordPosition="2638">uctures surely do not have to be POS tags. For example, an alternative could be unsupervised NT splitting similar to (Huang et al., 2010). The meta-grammar based approach was also motivated by the insight acquired on mono-lingual linguistic grammar generation, especially in the TAG related research (Xia, 2001; Prolo, 2002). Metagrammar was viewed as an effective way to remove redundancy in grammars. The link between Tree Adjoining Grammar (TAG) (Joshi et al., 1975; Joshi and Schabes, 1997) and MT was first introduced in (Shieber and Schabes, 1990), a pioneer work in tree-to-tree translation. (DeNeefe and Knight, 2009) re-visited the use of adjoining operation in the context of Statistical MT, and reported encouraging results. On the other 1http://www.itl.nist.gov/iad/mig/tests/mt/2008/ hand, (Dras, 1999) showed how a meta-level grammar could help in modeling parallel operations in (Shieber and Schabes, 1990). Our work is another effort of statistical modeling of well-recognized linguistic insight in NLP and MT. 6 Conclusions and Future Work In this paper, we introduced a novel method to model translation rules as observed generation output of a compact hidden model. As a case study to capitalize this model</context>
</contexts>
<marker>DeNeefe, Knight, 2009</marker>
<rawString>Steve DeNeefe and Kevin Knight. 2009. Synchronous tree adjoining machine translation. In Proceedings of the 2009 Conference of Empirical Methods in Natural Language Processing, pages 727–736, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dras</author>
</authors>
<title>A meta-level grammar: redefining synchronous tag for translation and paraphrase.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="14997" citStr="Dras, 1999" startWordPosition="2660" endWordPosition="2661"> acquired on mono-lingual linguistic grammar generation, especially in the TAG related research (Xia, 2001; Prolo, 2002). Metagrammar was viewed as an effective way to remove redundancy in grammars. The link between Tree Adjoining Grammar (TAG) (Joshi et al., 1975; Joshi and Schabes, 1997) and MT was first introduced in (Shieber and Schabes, 1990), a pioneer work in tree-to-tree translation. (DeNeefe and Knight, 2009) re-visited the use of adjoining operation in the context of Statistical MT, and reported encouraging results. On the other 1http://www.itl.nist.gov/iad/mig/tests/mt/2008/ hand, (Dras, 1999) showed how a meta-level grammar could help in modeling parallel operations in (Shieber and Schabes, 1990). Our work is another effort of statistical modeling of well-recognized linguistic insight in NLP and MT. 6 Conclusions and Future Work In this paper, we introduced a novel method to model translation rules as observed generation output of a compact hidden model. As a case study to capitalize this model, we presented three methods to enrich rule modeling with features defined on a hidden model. Preliminary experiments verified gain of one point on TER-BLEU over a strong baseline in Chinese</context>
</contexts>
<marker>Dras, 1999</marker>
<rawString>Mark Dras. 1999. A meta-level grammar: redefining synchronous tag for translation and paraphrase. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Martin Cmejrek</author>
<author>Bowen Zhou</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="14319" citStr="Huang et al., 2010" startWordPosition="2557" endWordPosition="2560">his shows the limitation of the generative feature. When we use meta-rules as binary sparse features in Type 2, we obtain about one point improvement on T-B on both sets. This shows the advantage of tuning individual meta-rule weights over a generative model. Type 3 (0.01) and Type 2 are at the same level. Proper smoothing is important to Type 3. 5 Discussion In the case study of Section 3, we use POS-based rules as hidden states. However, it should be noted that the hidden structures surely do not have to be POS tags. For example, an alternative could be unsupervised NT splitting similar to (Huang et al., 2010). The meta-grammar based approach was also motivated by the insight acquired on mono-lingual linguistic grammar generation, especially in the TAG related research (Xia, 2001; Prolo, 2002). Metagrammar was viewed as an effective way to remove redundancy in grammars. The link between Tree Adjoining Grammar (TAG) (Joshi et al., 1975; Joshi and Schabes, 1997) and MT was first introduced in (Shieber and Schabes, 1990), a pioneer work in tree-to-tree translation. (DeNeefe and Knight, 2009) re-visited the use of adjoining operation in the context of Statistical MT, and reported encouraging results. O</context>
</contexts>
<marker>Huang, Cmejrek, Zhou, 2010</marker>
<rawString>Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou. 2010. Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions. In Proceedings of the 2010 Conference of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>Treeadjoining grammars.</title>
<date>1997</date>
<booktitle>Handbook of Formal Languages,</booktitle>
<volume>3</volume>
<pages>69--124</pages>
<editor>In G. Rozenberg and A. Salomaa, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="14676" citStr="Joshi and Schabes, 1997" startWordPosition="2613" endWordPosition="2616">iscussion In the case study of Section 3, we use POS-based rules as hidden states. However, it should be noted that the hidden structures surely do not have to be POS tags. For example, an alternative could be unsupervised NT splitting similar to (Huang et al., 2010). The meta-grammar based approach was also motivated by the insight acquired on mono-lingual linguistic grammar generation, especially in the TAG related research (Xia, 2001; Prolo, 2002). Metagrammar was viewed as an effective way to remove redundancy in grammars. The link between Tree Adjoining Grammar (TAG) (Joshi et al., 1975; Joshi and Schabes, 1997) and MT was first introduced in (Shieber and Schabes, 1990), a pioneer work in tree-to-tree translation. (DeNeefe and Knight, 2009) re-visited the use of adjoining operation in the context of Statistical MT, and reported encouraging results. On the other 1http://www.itl.nist.gov/iad/mig/tests/mt/2008/ hand, (Dras, 1999) showed how a meta-level grammar could help in modeling parallel operations in (Shieber and Schabes, 1990). Our work is another effort of statistical modeling of well-recognized linguistic insight in NLP and MT. 6 Conclusions and Future Work In this paper, we introduced a novel </context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>Aravind K. Joshi and Yves Schabes. 1997. Treeadjoining grammars. In G. Rozenberg and A. Salomaa, editors, Handbook of Formal Languages, volume 3, pages 69–124. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Leon S Levy</author>
<author>Masako Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="14650" citStr="Joshi et al., 1975" startWordPosition="2609" endWordPosition="2612">rtant to Type 3. 5 Discussion In the case study of Section 3, we use POS-based rules as hidden states. However, it should be noted that the hidden structures surely do not have to be POS tags. For example, an alternative could be unsupervised NT splitting similar to (Huang et al., 2010). The meta-grammar based approach was also motivated by the insight acquired on mono-lingual linguistic grammar generation, especially in the TAG related research (Xia, 2001; Prolo, 2002). Metagrammar was viewed as an effective way to remove redundancy in grammars. The link between Tree Adjoining Grammar (TAG) (Joshi et al., 1975; Joshi and Schabes, 1997) and MT was first introduced in (Shieber and Schabes, 1990), a pioneer work in tree-to-tree translation. (DeNeefe and Knight, 2009) re-visited the use of adjoining operation in the context of Statistical MT, and reported encouraging results. On the other 1http://www.itl.nist.gov/iad/mig/tests/mt/2008/ hand, (Dras, 1999) showed how a meta-level grammar could help in modeling parallel operations in (Shieber and Schabes, 1990). Our work is another effort of statistical modeling of well-recognized linguistic insight in NLP and MT. 6 Conclusions and Future Work In this pap</context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Aravind K. Joshi, Leon S. Levy, and Masako Takahashi. 1975. Tree adjunct grammars. Journal of Computer and System Sciences, 10(1):136–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="843" citStr="Koehn et al., 2003" startWordPosition="127" endWordPosition="130">m Abstract Most of the machine translation systems rely on a large set of translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation. 1 Introduction Most of the modern Statistical Machine Translation (SMT) systems, for example (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), employ a large rule set that may contain tens of millions of translation rules or even more. In these systems, each translation rule has about 20 dense features, which represent key statistics collected from the training data, such as word translation probability, phrase translation probability etc. Except for these common features, there is no connection among the translation rules. The translation rules are treated as independent events. The use of sparse features as in (Arun and Koehn, 2007; Watanabe et al., 2007; Ch</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase based translation. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 48–54, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference of Empirical Methods in Natural Language Processing,</booktitle>
<pages>44--52</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="896" citStr="Marcu et al., 2006" startWordPosition="137" endWordPosition="140">ly on a large set of translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation. 1 Introduction Most of the modern Statistical Machine Translation (SMT) systems, for example (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), employ a large rule set that may contain tens of millions of translation rules or even more. In these systems, each translation rule has about 20 dense features, which represent key statistics collected from the training data, such as word translation probability, phrase translation probability etc. Except for these common features, there is no connection among the translation rules. The translation rules are treated as independent events. The use of sparse features as in (Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2009) to some extent mitigated this prob</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proceedings of the 2006 Conference of Empirical Methods in Natural Language Processing, pages 44– 52, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyros Matsoukas</author>
<author>Antti-Veikko Rosti</author>
<author>Bing Zhang</author>
</authors>
<title>Discriminative corpus weight estimation for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="10772" citStr="Matsoukas et al., 2009" startWordPosition="1921" endWordPosition="1924"> IR ⇒ IIP • VV ⇒ VBZ • VV ⇒ VBZ II • IR VV ⇒ IIP VBZ • IR VV ⇒ IIP VBZ II • VVIR ⇒ VBZ II IIP • IR VV IR ⇒ IIP VBZ II IIP All of them respect the word alignment, which means that • there is no alignment that aligns one word in a meta-rule with the other out of the same metarule, and • there is at least one alignment within a metarule. 3.3 Implementation Details Even though the size of all possible meta-rules is much smaller than the space of translation rules, it is still too large to work with existing optimization methods for sparse features in MT, i.e. MIRA (Chiang et al., 2009) or L-BFGS (Matsoukas et al., 2009). In practice, we have to limit the feature space to around 20,000 dimensions. For this purpose, we first use a frequency based method to filter meta-rule features. Specifically, we first divide all the meta-rules into 100 bins, (|F |, |E|), where |F |is the number of words on the source side, and |E |the target side, 0 &lt; |F |, |E |≤ 10. For each bin, we keep the same top k-percentile of the meta-rules such that we obtain a total of 20,000 meta-rules as features. System BLEU% TER% T-B Baseline 30.35 55.32 24.97 Type 1 30.74 55.48 24.74 Type 2 31.07 55.07 24.00 Type 3 (1) 30.93 55.34 24.41 Type</context>
</contexts>
<marker>Matsoukas, Rosti, Zhang, 2009</marker>
<rawString>Spyros Matsoukas, Antti-Veikko Rosti, and Bing Zhang. 2009. Discriminative corpus weight estimation for machine translation. In Proceedings of the 2009 Conference of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="862" citStr="Och and Ney, 2004" startWordPosition="131" endWordPosition="134">he machine translation systems rely on a large set of translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation. 1 Introduction Most of the modern Statistical Machine Translation (SMT) systems, for example (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), employ a large rule set that may contain tens of millions of translation rules or even more. In these systems, each translation rule has about 20 dense features, which represent key statistics collected from the training data, such as word translation probability, phrase translation probability etc. Except for these common features, there is no connection among the translation rules. The translation rules are treated as independent events. The use of sparse features as in (Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2009) </context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<journal>IBM Research</journal>
<tech>Report, RC22176.</tech>
<contexts>
<context position="12952" citStr="Papineni et al., 2001" startWordPosition="2312" endWordPosition="2315"> to Phase 1 of the DARPA BOLT MT task. The tune set contains 1275 sentences. Each has four references. There are two test sets. Test-1 is from a similar source of the tune set, and it contains 1239 sentences. Test-2 is the web part of the MT08 evaluation data. Our baseline system is a home-made Hiero (Chiang, 2005) style system. The baseline rule set contains about 17 million rules. It contains about 40 dense features, including a 6-gram LM. The sparse feature optimization algorithm is similar to the MIRA recipe described in (Chiang et al., 2009). We optimize on TER-BLEU (Snover et al., 2006; Papineni et al., 2001). The BLEU, TER and T-B scores on the two tests are shown in Tables 1 and 2. It should be noted that, even though our metric of tuning is T-B, the baseline 842 System BLEU% TER% T-B Baseline 25.80 56.96 31.16 Type 1 26.18 57.09 30.91 Type 2 26.63 56.64 30.01 Type 3 (1) 26.30 57.00 30.70 Type 3 (0.1) 26.34 56.73 30.39 Type 3 (0.01) 26.50 56.73 30.23 Table 2: scores on test-2 (MT08-WB) system already provides a very competitive BLEU score on MT08-WB as compared the best system in the evaluation1, thanks to comprehensive features in the baseline system and more data in training. All the three typ</context>
</contexts>
<marker>Papineni, Roukos, Ward, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, and Todd Ward. 2001. Bleu: a method for automatic evaluation of machine translation. IBM Research Report, RC22176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Prolo</author>
</authors>
<title>Generating the xtag english grammar using metarules.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics (COLING).</booktitle>
<contexts>
<context position="14506" citStr="Prolo, 2002" startWordPosition="2587" endWordPosition="2588">e of tuning individual meta-rule weights over a generative model. Type 3 (0.01) and Type 2 are at the same level. Proper smoothing is important to Type 3. 5 Discussion In the case study of Section 3, we use POS-based rules as hidden states. However, it should be noted that the hidden structures surely do not have to be POS tags. For example, an alternative could be unsupervised NT splitting similar to (Huang et al., 2010). The meta-grammar based approach was also motivated by the insight acquired on mono-lingual linguistic grammar generation, especially in the TAG related research (Xia, 2001; Prolo, 2002). Metagrammar was viewed as an effective way to remove redundancy in grammars. The link between Tree Adjoining Grammar (TAG) (Joshi et al., 1975; Joshi and Schabes, 1997) and MT was first introduced in (Shieber and Schabes, 1990), a pioneer work in tree-to-tree translation. (DeNeefe and Knight, 2009) re-visited the use of adjoining operation in the context of Statistical MT, and reported encouraging results. On the other 1http://www.itl.nist.gov/iad/mig/tests/mt/2008/ hand, (Dras, 1999) showed how a meta-level grammar could help in modeling parallel operations in (Shieber and Schabes, 1990). O</context>
</contexts>
<marker>Prolo, 2002</marker>
<rawString>Carlos Prolo. 2002. Generating the xtag english grammar using metarules. In Proceedings of the 19th international conference on Computational linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="916" citStr="Shen et al., 2008" startWordPosition="141" endWordPosition="144"> translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation. 1 Introduction Most of the modern Statistical Machine Translation (SMT) systems, for example (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), employ a large rule set that may contain tens of millions of translation rules or even more. In these systems, each translation rule has about 20 dense features, which represent key statistics collected from the training data, such as word translation probability, phrase translation probability etc. Except for these common features, there is no connection among the translation rules. The translation rules are treated as independent events. The use of sparse features as in (Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2009) to some extent mitigated this problem. In their work, </context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Yves Schabes</author>
</authors>
<title>Synchronous tree adjoining grammars.</title>
<date>1990</date>
<booktitle>In Proceedings of COLING ’90: The 13th Int. Conf. on Computational Linguistics,</booktitle>
<pages>253--258</pages>
<location>Helsinki, Finland.</location>
<contexts>
<context position="14735" citStr="Shieber and Schabes, 1990" startWordPosition="2623" endWordPosition="2627">d rules as hidden states. However, it should be noted that the hidden structures surely do not have to be POS tags. For example, an alternative could be unsupervised NT splitting similar to (Huang et al., 2010). The meta-grammar based approach was also motivated by the insight acquired on mono-lingual linguistic grammar generation, especially in the TAG related research (Xia, 2001; Prolo, 2002). Metagrammar was viewed as an effective way to remove redundancy in grammars. The link between Tree Adjoining Grammar (TAG) (Joshi et al., 1975; Joshi and Schabes, 1997) and MT was first introduced in (Shieber and Schabes, 1990), a pioneer work in tree-to-tree translation. (DeNeefe and Knight, 2009) re-visited the use of adjoining operation in the context of Statistical MT, and reported encouraging results. On the other 1http://www.itl.nist.gov/iad/mig/tests/mt/2008/ hand, (Dras, 1999) showed how a meta-level grammar could help in modeling parallel operations in (Shieber and Schabes, 1990). Our work is another effort of statistical modeling of well-recognized linguistic insight in NLP and MT. 6 Conclusions and Future Work In this paper, we introduced a novel method to model translation rules as observed generation ou</context>
</contexts>
<marker>Shieber, Schabes, 1990</marker>
<rawString>Stuart Shieber and Yves Schabes. 1990. Synchronous tree adjoining grammars. In Proceedings of COLING ’90: The 13th Int. Conf. on Computational Linguistics, pages 253–258, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="12928" citStr="Snover et al., 2006" startWordPosition="2308" endWordPosition="2311">l sentences available to Phase 1 of the DARPA BOLT MT task. The tune set contains 1275 sentences. Each has four references. There are two test sets. Test-1 is from a similar source of the tune set, and it contains 1239 sentences. Test-2 is the web part of the MT08 evaluation data. Our baseline system is a home-made Hiero (Chiang, 2005) style system. The baseline rule set contains about 17 million rules. It contains about 40 dense features, including a 6-gram LM. The sparse feature optimization algorithm is similar to the MIRA recipe described in (Chiang et al., 2009). We optimize on TER-BLEU (Snover et al., 2006; Papineni et al., 2001). The BLEU, TER and T-B scores on the two tests are shown in Tables 1 and 2. It should be noted that, even though our metric of tuning is T-B, the baseline 842 System BLEU% TER% T-B Baseline 25.80 56.96 31.16 Type 1 26.18 57.09 30.91 Type 2 26.63 56.64 30.01 Type 3 (1) 26.30 57.00 30.70 Type 3 (0.1) 26.34 56.73 30.39 Type 3 (0.01) 26.50 56.73 30.23 Table 2: scores on test-2 (MT08-WB) system already provides a very competitive BLEU score on MT08-WB as compared the best system in the evaluation1, thanks to comprehensive features in the baseline system and more data in tra</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings ofAssociation for Machine Translation in the Americas, pages 223–231, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Watanabe</author>
<author>J Suzuki</author>
<author>H Tsukuda</author>
<author>H Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Conference of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1439" citStr="Watanabe et al., 2007" startWordPosition="228" endWordPosition="231">ample (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), employ a large rule set that may contain tens of millions of translation rules or even more. In these systems, each translation rule has about 20 dense features, which represent key statistics collected from the training data, such as word translation probability, phrase translation probability etc. Except for these common features, there is no connection among the translation rules. The translation rules are treated as independent events. The use of sparse features as in (Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2009) to some extent mitigated this problem. In their work, there are as many as 10,000 features defined on the appearance of certain frequent words and Part of Speech (POS) tags in rules. They provide significant improvement in automatic evaluation metrics. However, these sparse features fire quite randomly and infrequently on each rule. Thus, there is still plenty of space to better model translation rules. In this paper, we will explore the relationship among translation rules. We no longer view rules as discrete or unrelated events. Instead, we view rules, which are observ</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukuda, Isozaki, 2007</marker>
<rawString>T. Watanabe, J. Suzuki, H. Tsukuda, and H. Isozaki. 2007. Online large-margin training for statistical machine translation. In Proceedings of the 2007 Conference of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Xia</author>
</authors>
<title>Automatic Grammar Generation From Two Different Perspectives.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="14492" citStr="Xia, 2001" startWordPosition="2585" endWordPosition="2586">he advantage of tuning individual meta-rule weights over a generative model. Type 3 (0.01) and Type 2 are at the same level. Proper smoothing is important to Type 3. 5 Discussion In the case study of Section 3, we use POS-based rules as hidden states. However, it should be noted that the hidden structures surely do not have to be POS tags. For example, an alternative could be unsupervised NT splitting similar to (Huang et al., 2010). The meta-grammar based approach was also motivated by the insight acquired on mono-lingual linguistic grammar generation, especially in the TAG related research (Xia, 2001; Prolo, 2002). Metagrammar was viewed as an effective way to remove redundancy in grammars. The link between Tree Adjoining Grammar (TAG) (Joshi et al., 1975; Joshi and Schabes, 1997) and MT was first introduced in (Shieber and Schabes, 1990), a pioneer work in tree-to-tree translation. (DeNeefe and Knight, 2009) re-visited the use of adjoining operation in the context of Statistical MT, and reported encouraging results. On the other 1http://www.itl.nist.gov/iad/mig/tests/mt/2008/ hand, (Dras, 1999) showed how a meta-level grammar could help in modeling parallel operations in (Shieber and Sch</context>
</contexts>
<marker>Xia, 2001</marker>
<rawString>F. Xia. 2001. Automatic Grammar Generation From Two Different Perspectives. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
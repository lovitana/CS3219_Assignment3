<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000038">
<title confidence="0.994244">
Recursive Autoencoders for ITG-based Translation
</title>
<author confidence="0.999386">
Peng Li, Yang Liu and Maosong Sun
</author>
<affiliation confidence="0.998868666666667">
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China
</affiliation>
<email confidence="0.990872">
pengli09@gmail.com, {liuyang2011,sms}@tsinghua.edu.cn
</email>
<sectionHeader confidence="0.998545" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999158277777778">
While inversion transduction grammar (ITG)
is well suited for modeling ordering shifts
between languages, how to make applying
the two reordering rules (i.e., straight and
inverted) dependent on actual blocks being
merged remains a challenge. Unlike previous
work that only uses boundary words, we pro-
pose to use recursive autoencoders to make
full use of the entire merging blocks alter-
natively. The recursive autoencoders are ca-
pable of generating vector space representa-
tions for variable-sized phrases, which enable
predicting orders to exploit syntactic and se-
mantic information from a neural language
modeling’s perspective. Experiments on the
NIST 2008 dataset show that our system sig-
nificantly improves over the MaxEnt classifier
by 1.07 BLEU points.
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999846659574468">
Phrase-based models (Koehn et al., 2003; Och and
Ney, 2004) have been widely used in practical ma-
chine translation (MT) systems due to their effec-
tiveness, simplicity, and applicability. First, as se-
quences of consecutive words, phrases are capable
of memorizing local word selection and reorder-
ing, making them an effective mechanism for trans-
lating idioms or translations with word insertions
or omissions. Moreover, n-gram language models
can be seamlessly integrated into phrase-based de-
coders since partial translations grow left to right
in decoding. Finally, phrase-based systems can be
applicable to most domains and languages, espe-
cially for resource-scarce languages without high-
accuracy parsers.
However, as phrase-based decoding casts transla-
tion as a string concatenation problem and permits
arbitrary permutations, it proves to be NP-complete
(Knight, 1999). Therefore, phrase reordering mod-
eling has attracted intensive attention in the past
decade (e.g., Och et al., 2004; Tillman, 2004; Zens
et al., 2004; Al-Onaizan and Papineni, 2006; Xiong
et al., 2006; Koehn et al., 2007; Galley and Man-
ning, 2008; Feng et al., 2010; Green et al., 2010;
Bisazza and Federico, 2012; Cherry, 2013).
Among them, reordering models based on inver-
sion transduction grammar (ITG) (Wu, 1997) are
one of the important ongoing research directions.
As a formalism for bilingual modeling of sentence
pairs, ITG is particularly well suited to predicting
ordering shifts between languages. As a result, a
number of authors have incorporated ITG into left-
to-right decoding to constrain the reordering space
and reported significant improvements (e.g., Zens et
al., 2004; Feng et al., 2010). Along another line,
Xiong et al. (2006) propose a maximum entropy
(MaxEnt) reordering model based on ITG. They use
the CKY algorithm to recursively merge two blocks
(i.e., a pair of source and target strings) into larger
blocks, either in a straight or an inverted order. Un-
like lexicalized reordering models (Tillman, 2004;
Koehn et al., 2007; Galley and Manning, 2008) that
are defined on individual bilingual phrases, the Max-
Ent ITG reordering model is a two-category classi-
fier (i.e., straight or inverted) for two arbitrary bilin-
gual phrases of which the source phrases are adja-
cent. This potentially alleviates the data sparseness
</bodyText>
<page confidence="0.940135">
567
</page>
<note confidence="0.7352115">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567–577,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.998111176470588">
problem since there are usually a large number of
reordering training examples available (Xiong et al.,
2006). As a result, the MaxEnt ITG model and its
extensions (Xiong et al., 2008; Xiong et al., 2010)
have achieved competing performance as compared
with state-of-the-art phrase-based systems.
Despite these successful efforts, the ITG reorder-
ing classifiers still face a major challenge: how to
extract features from training examples (i.e., a pair
of bilingual strings). It is hard to decide which words
are representative for predicting reordering, either
manually or automatically, especially for long sen-
tences. As a result, Xiong et al. (2006) only use
boundary words (i.e., the first and the last words in
a string) to predict the ordering. What if we look
inside? Is it possible to avoid manual feature engi-
neering and learn semantic representations from the
data?
Fortunately, the rapid development of intersect-
ing deep learning with natural language processing
(Bengio et al., 2003; Collobert and Weston, 2008;
Collobert et al., 2011; Glorot et al., 2011; Bordes et
al., 2011; Socher et al., 2011a; Socher et al., 2011b;
Socher et al., 2011c; Socher et al., 2012; Bordes et
al., 2012; Huang et al., 2012; Socher et al., 2013;
Hermann and Blunsom, 2013) brings hope for alle-
viating this problem. In these efforts, natural lan-
guage words are represented as real-valued vectors,
which can be naturally fed to neural networks as in-
put. More importantly, it is possible to learn vec-
tor space representations for multi-word phrases us-
ing recursive autoencoders (Socher et al., 2011c),
which opens the door to leveraging semantic repre-
sentations of phrases in reordering models from a
neural language modeling point of view.
In this work, we propose an ITG reordering clas-
sifier based on recursive autoencoders. The neu-
ral network consists of four autoencoders (i.e., the
first source phrase, the first target phrase, the sec-
ond source phrase, and the second target phrase)
and a softmax layer. The recursive autoencoders,
which are trained on reordering examples extracted
from word-aligned bilingual corpus, are capable
of producing vector space representations for arbi-
trary multi-word strings in decoding. Therefore,
our model takes the whole phrases rather than only
boundary words into consideration when predict-
ing phrase permutations. Experiments on the NIST
2008 dataset show that our system significantly im-
proves over the MaxEnt classifier by 1.07 in terms
of case-insensitive BLEU score.
</bodyText>
<sectionHeader confidence="0.995747" genericHeader="method">
2 Recursive Autoencoders for ITG-based
Translation
</sectionHeader>
<subsectionHeader confidence="0.993455">
2.1 Inversion Transduction Grammar
</subsectionHeader>
<bodyText confidence="0.999979">
Inversion transduction grammar (ITG) (Wu, 1997)
is a formalism for synchronous parsing of bilingual
sentence pairs. Xiong et al. (2006) apply bracketing
transduction grammar (BTG), which is a simplified
version of ITG, to phrase-based translation using the
following production rules:
</bodyText>
<equation confidence="0.999739333333333">
X → [X1, X2]
X → hX1, X2i
X → f/e
</equation>
<bodyText confidence="0.9962641875">
where X is a block that consists of a pair of source
and target strings, f is a source phrase, and e is a tar-
get phrase. X1 and X2 are two neighboring blocks
of which the two source phrases are adjacent. While
rule (1) merges two target phrases in a straight or-
der, rule (2) merges in an inverted order. Besides
these two reordering rules, rule (3) is a lexical rule
that translates a source phrase f into a target phrase
e. This is exactly a bilingual phrase used in conven-
tional phrase-based systems.
An ITG derivation, which consists of a sequence
of production rules, explains how a sentence pair is
generated simultaneously. Figure 1 shows an ITG
derivation for a Chinese sentence and its English
translation. We distinguish between two types of
blocks:
</bodyText>
<listItem confidence="0.9988135">
1. atomic blocks: blocks generated by applying
lexical rules,
2. composed blocks: blocks generated by apply-
ing reordering rules.
</listItem>
<bodyText confidence="0.966502">
In Figure 1, the sentence pair is segmented into
five atomic blocks:
</bodyText>
<equation confidence="0.952907">
X0,3,0,3 : wo you yi ge ↔ I have a
X3,5,5,6 : cong mei you ↔ never
X5,8,6,8 : jian guo de ↔ seen before
X8,10,3,5 : nv xing peng you ↔ female friend
X10,11,8,9 : . ↔ .
568
X0,11,0,9 → [X0,10,0,8, X10,11,8,9]
X0,10,0,8 → [X0,3,0,3, X3,10,3,8]
X0,3,0,3 → wo you yi ge / I have a
X3,10,3,8 → hX3,8,5,8, X8,10,3,5i
X3,8,5,8 → [X3,5,5,6, X5,8,6,8]
X3,5,5,6 → cong mei you / never
X5,8,6,8 → juan guo de / seen before
X8,10,3,5 → nv xing peng you/ female friend
X10,11,8,9 → . / .
</equation>
<figureCaption confidence="0.964689">
Figure 1: An ITG derivation for a Chinese sentence and its translation. We use Xz,j,k,l = hfj� , el ki to represent a block.
</figureCaption>
<bodyText confidence="0.8423335">
Our neural ITG reordering model first assigns vector space representations to single words and then produces vectors
for phrases using recursive autoencoders, which form atomic blocks. The atomic blocks are recursively merged into
composed blocks, the vector space representations of which are produced by recursive autoencoders simultaneously.
The neural classifier makes decisions at each node using the vectors of all its descendants.
</bodyText>
<page confidence="0.997102">
569
</page>
<bodyText confidence="0.999841777777778">
where X3,5,5,6 indicates that the block consists of a
source phrase spanning from position 3 to position 5
(i.e., “cong mei you”) and a target phrase spanning
from position 5 to position 6 (i.e., “never”). More
formally, a block Xi,j,k,l = (fji , elk) is a pair of a
source phrase fji = fi+1 ... fj and a target phrase
elk = ek+1 ... el. Obviously, these atomic blocks
are generated by lexical rules.
Two blocks of which the source phrases are adja-
cent can be merged into a larger one in two ways:
concatenating the target phrases in a straight order
using rule (1) or in an inverted order using rule (2).
For example, atomic blocks X3,5,5,6 and X5,8,6,8 are
merged into a composed block X3,8,5,8 in a straight
order, which is further merged with an atomic block
X8,10,3,5 into another composed block X3,10,3,8 in
an inverted order. This process recursively proceeds
until the entire sentence pair is generated.
The major challenge of applying ITG to machine
translation is to decide when to merge two blocks
in a straight order and when in an inverted order.
Therefore, the ITG reordering model can be seen as
a two-category classifier P(o|X1,X2), where o E
{straight, inverted}.
A naive way is to assign fixed probabilities to two
reordering rules, which is referred to as flat model
by Xiong et al. (2006):
</bodyText>
<equation confidence="0.980739">
� p o = straight
P(o|X1, X2) = 1 − p o = inverted (4)
</equation>
<bodyText confidence="0.9998375">
The drawback of the flat model is ignoring the
actual blocks being merged. Intuitively, different
blocks should have different preferences between
the two orders.
To alleviate this problem, Xiong et al. (2006) pro-
pose a maximum entropy (MaxEnt) classifier:
</bodyText>
<equation confidence="0.999532">
P(o|X1,X2) =exp(θ · h(o, X1, X2))
� �� exp(θ · h(o�, X1, X2)) (5)
</equation>
<bodyText confidence="0.9998965">
where h(·) is a vector of features defined on the
blocks and the order, θ is a vector of feature weights.
While MaxEnt is a flexible and powerful frame-
work for including arbitrary features, feature engi-
neering becomes a major challenge for the MaxEnt
classifier. Xiong et al. (2006) find that boundary
words (i.e., the first and the last words in a string)
are informative for predicting reordering. Actually,
</bodyText>
<figureCaption confidence="0.83041875">
Figure 2: A recursive autoencoder for multi-word strings.
The example is adapted from (Socher et al., 2011c). Blue
and grey nodes are original and reconstructed ones, re-
spectively.
</figureCaption>
<bodyText confidence="0.987610352941176">
it is hard to decide which internal words in a long
composed blocks are representative and informa-
tive. Therefore, they only use boundary words as
the main features.
However, it seems not enough to just consider
boundary words and ignore all internal words when
making order predictions, especially for long sen-
tences.1 Indeed, Xiong et al. (2008) find that the
MaxEnt classifier with boundary words as features
is prone to make wrong predictions for long com-
posed blocks. As a result, they have to impose a hard
constraint to always prefer merging long composed
blocks in a monotonic way.
Therefore, it is important to consider more than
boundary words to make more accurate reordering
predictions. We need a new mechanism to achieve
this goal.
</bodyText>
<subsectionHeader confidence="0.821135">
2.2 Recursive Autoencoders
2.2.1 Vector Space Representations for Words
</subsectionHeader>
<bodyText confidence="0.9996995">
In neural networks, a natural language word is
represented as a real-valued vector (Bengio et al.,
2003; Collobert and Weston, 2008). For example,
we can use [0.1 0.8 0.4]T to represent “female” and
</bodyText>
<footnote confidence="0.830646">
1Strictly speaking, the ITG reordering model is not a phrase
reordering model since phrase pairs are only the atomic blocks.
Instead, it is defined to work on arbitrarily long strings because
composed blocks become larger and larger until the entire sen-
tence pair is generated.
</footnote>
<page confidence="0.991491">
570
</page>
<figureCaption confidence="0.979469">
Figure 3: A neural ITG reordering model. The binary classifier makes decisions based on the vector space representa-
tions of the source and target sides of merging blocks.
</figureCaption>
<bodyText confidence="0.983482666666667">
[0.7 0.1 0.5]T to represent “friend”. Such vector
space representations enable natural language words
to be fed to neural networks as input.
Formally, we denote each word as a vector x E
Rn. These word vectors are then stacked into a word
embedding matrix L E Rn×|V |, where |V  |is the vo-
cabulary size. Given a sentence that is an ordered list
of m words, each word has an associated vocabulary
index k into the word embedding matrix L that we
use to retrieve the word’s vector space representa-
tion. This look-up operation can be seen as a simple
projection layer:
xi = Lbk E Rn (6)
where bk is a binary vector which is zero in all posi-
tions except for the kth index.
In Figure 1, we assume n = 3 for simplicity and
can retrieve vectors for Chinese and English words
from two embedding matrices, respectively.
</bodyText>
<subsectionHeader confidence="0.8104115">
2.2.2 Vector Space Representations for
Multi-Word Strings
</subsectionHeader>
<bodyText confidence="0.9987468">
To apply neural networks to ITG-based transla-
tion, it is important to generate vector space repre-
sentations for atomic and composed blocks.
For example, since the vector of “female” is
[0.1 0.8 0.4]T and the vector of “friend” is
[0.7 0.1 0.5]T, what is the vector of the phrase “fe-
male friend”? If we denote “female friend” as p
(i.e., parent), “female” as c1 (i.e., the first child),
and “friend” as c2 (i.e., the second child), this can
be done by applying a function f(1):
</bodyText>
<equation confidence="0.919176">
p = f(1)(W(1)[c1; c2] + b(1)) (7)
</equation>
<bodyText confidence="0.999701733333333">
where [c1; c2] E R2n×1 is the concatenation of c1
and c2, W(1) E Rn×2n is a parameter matrix, b(1) E
Rn×1 is a bias term, and f(1) is an element-wise ac-
tivation function such as tanh(·), which is used in
our experiments.
Note that the resulting vector for the parent is also
an n-dimensional vector, e.g, [0.6 0.9 0.2]T. The
same neural network can be recursively applied to
two strings until the vector of the entire sentence is
generated. As ITG derivation builds a binary parse
tree, the neural network can be naturally integrated
into CKY parsing.
To assess how well the learned vector p represents
its children, we can reconstruct the children in a
reconstruction layer:
</bodyText>
<equation confidence="0.79724">
[c01; c02] = f(2)(W (2)p + b(2)) (8)
</equation>
<bodyText confidence="0.9993613">
where c01 and c02 are the reconstructed children, W(2)
is a parameter matrix for reconstruction, b(2) is a bias
term for reconstruction, and f(2) is an element-wise
activation function, which is also set as tanh(·) in
our experiments. Similarly, the same reconstruction
neural network can be applied to each node in an
ITG parse.
These neural networks are called recursive au-
toencoders (Socher et al., 2011c). Figure 2 illus-
trates an application of a recursive autoencoder to a
</bodyText>
<page confidence="0.986489">
571
</page>
<bodyText confidence="0.965931857142857">
binary tree. The blue and grey nodes are the original
and reconstructed nodes, respectively. The autoen-
coder is re-used at each node of the tree. The bi-
nary tree is composed of a set of triplets in the form
of (p —* c1 c2), where p is a parent vector and c1
and c2 are children vectors of p. Each child can be
either an input word vector or a multi-word vector.
Therefore, the tree in Figure 2 can be represented as
three triplets: (y1 — x1 x2), (y2 — y1 x3), and
(y3 — y2 x4).
In Figure 1, we use recursive autoencoders to gen-
erate vector space representations for Chinese and
English phrases, which form the atomic blocks for
further block merging.
</bodyText>
<subsectionHeader confidence="0.55149">
2.2.3 A Neural ITG Reordering Model
</subsectionHeader>
<bodyText confidence="0.9999791">
Once the vectors for blocks are generated, it is
straightforward to introduce a neural ITG reorder-
ing model. As shown in Figure 3, the neural net-
work consists of an input layer and a softmax layer.
The input layer is composed of the vectors of the
first source phrase, the first target phrase, the second
source phrase, and the second target phrase. Note
that all phrases in the same language use the the
same recursive autoencoder. The softmax layer out-
puts the probabilities of the two merging orders:
</bodyText>
<equation confidence="0.999858333333333">
P(o|X1,X2) =�d ex ( 1
p(g(o XX X) )) (9)
g(o, X1, X2) = f(Woc(X1, X2) + bo) (10)
</equation>
<bodyText confidence="0.99983025">
where o E {straight, inverted}, Wo E R1x4n
is a parameter matrix, bo E R is a bias term, and
c(X1, X2) E R4nx1 is the concatenation of the vec-
tors of the four phrases.
</bodyText>
<sectionHeader confidence="0.997582" genericHeader="method">
3 Training
</sectionHeader>
<bodyText confidence="0.991615">
There are three sets of parameters in our recursive
autoencoders:
</bodyText>
<listItem confidence="0.998605714285714">
1. θL: word embedding matrix L for both source
and target languages (Section 2.2.1);
2. θrec: recursive autoencoder parameter matrices
W(1), W(2) and bias terms b(1), b(2) for both
source and target languages (Section 2.2.2);
3. θreo: neural ITG reordering model parameter
matrix Wo and bias term bo (Section 2.2.3).
</listItem>
<bodyText confidence="0.999204777777778">
All these parameters are learned automatically from
the training data. For clarity, we will use θ to denote
all these parameters in the rest of the paper.
For training word embedding matrix, there are
two settings commonly used. In the first setting,
the word embedding matrix is initialized randomly.
This works well in a supervised scenario, in which
a neural network updates the matrix in order to op-
timize some task-specific objectives (Collobert et
al., 2011; Socher et al., 2011c). In the second set-
ting, the word embedding matrix is pre-trained us-
ing an unsupervised neural language model (Bengio
et al., 2003; Collobert and Weston, 2008) with huge
amount of unlabeled data. In this work, we prefer to
the first setting because the word embedding matri-
ces can be trained to minimize errors with respect to
reordering modeling.
There are two kinds of errors involved
</bodyText>
<listItem confidence="0.9946574">
1. reconstruction error: how well the learned
vector space representations represent the cor-
responding strings?
2. reordering error: how well the classifier pre-
dicts the merging order?
</listItem>
<bodyText confidence="0.988854190476191">
As described in Section 2.2.2, the input vector
c1 and c2 of a recursive autoencoder can be recon-
structed using Eq. 8 as ci and c2. We use Euclidean
distance between the input and the reconstructed
vectors to measure the reconstruction error:
1 Erec( [c1; c2]; θ) = 2 II [c1; c2] — [cl; c�2] II2 . (11)
Given a sentence, there are exponentially many
ways to obtain its vector space representation. Note
that each way corresponds to a binary tree like Fig-
ure 2. To find a binary tree with minimal reconstruc-
tion error, we follow Socher et al. (2011c) to use a
greedy algorithm. Taking Figure 2 as an example,
the greedy algorithm begins with computing the re-
construction error Erec(�) for each pair of consecu-
tive vectors, i.e., Erec([x1; x2]; θ), Erec([x2; x3]; θ)
and Erec([x3; x4]; θ). Suppose Erec([x1; x2]; θ) is
the smallest, the algorithm will replace x1 and x2
with their vector representation y1 produced by the
recursive autoencoder. Then, the algorithm evalu-
ates Erec([y1; x3]; θ) and Erec([x3; x4]; θ) and re-
peats the above replacing steps until only one vector
</bodyText>
<page confidence="0.986575">
572
</page>
<bodyText confidence="0.994156666666667">
remains. Socher et al. (2011c) find that the greedy
algorithm runs fast without significant loss in perfor-
mance as compared with CKY-style algorithms.
Given a training example set S = {ti =
(oi, X1i , X2i )}, the average reconstruction error on
the source side on the training set is defined as
</bodyText>
<equation confidence="0.9261795">
1E E
Erec,s(S; θ) = Erec([p.c1,p.c2]; θ)
Ns i p∈TOR(ti,s)
(12)
</equation>
<bodyText confidence="0.998698428571429">
where Tθ R(ti, s) denotes all the intermediate nodes
on the source side in binary trees, Ns is the num-
ber of these intermediate nodes, and p.ck is the kth
child vector of p. The average reconstruction error
on the target side, denoted by Erec,t(S; θ), can be
computed in a similar way.
Therefore, the reconstruction error is defined as
</bodyText>
<equation confidence="0.995438">
Erec(S; θ) = Erec,s(S; θ) + Erec,t(S; θ). (13)
</equation>
<bodyText confidence="0.9997855">
Given a training example ti = (oi, X1i , X2i ), we
assume the probability distribution dti for its label
is [1, 0] when oi = straight, and [0, 1] when oi =
inverted. Then the cross-entropy error is
</bodyText>
<equation confidence="0.9910945">
Ec(ti; θ) = − E dti(o)log (Pθ(o|X1,X2)) (14)
o
</equation>
<bodyText confidence="0.995395">
where o E {straight, inverted}. As a result, the
reordering error is defined as
</bodyText>
<equation confidence="0.82538325">
Ereo(S; θ) = |1  |E Ec(ti; θ). (15)
i
Therefore, the joint training objective function is
J = αErec(S; θ)+(1−α)Ereo(S; θ)+R(θ) (16)
</equation>
<bodyText confidence="0.971171666666667">
where α is a parameter used to balance the prefer-
ence between reconstruction error and reordering er-
ror, R(θ) is the regularizer and defined as 2
</bodyText>
<equation confidence="0.774333666666667">
λL 2 λrec 2 λreo 2
R(θ) = 2 11θL 11 + 2 Ilθrec11 + 2 Ilθreo11 .
(17)
</equation>
<bodyText confidence="0.989972333333333">
As Socher et al. (2011c) stated, a naive way for
lowering the reconstruction error is to make the
magnitude of the hidden layer very small, which is
</bodyText>
<footnote confidence="0.791249">
2The bias terms b(1), b(2) and b� are not regularized. We do
not exclude them from the equation explicitly just for clarity.
</footnote>
<bodyText confidence="0.994556714285714">
not desirable. In order to prevent such behavior, we
normalize all the output vectors of the hidden layers
to have length 1 in the same way as (Socher et al.,
2011c). Namely we set p = p
||p ||after computing p
as in Eq. 7, and c01 = c� 1||ci ||,c02=c�2
||4 ||in Eq. 8.
Following Socher et al. (2011c), we use L-BFGS
to estimate the parameters with respect to the joint
training objective. Given a set of parameters, we
construct binary trees for all the phrases using the
greedy algorithm. The derivatives for these fixed
binary trees can be computed via backpropagation
through structures (Goller and Kuchler, 1996).
</bodyText>
<sectionHeader confidence="0.999954" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998395">
4.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999925875">
We evaluated our system on Chinese-English trans-
lation. The training corpus contains 1.23M sen-
tence pairs with 32.1M Chinese words and 35.4M
English words. We used SRILM (Stolcke, 2002)
to train a 4-gram language model on the Xinhua
portion of the GIGAWORD corpus, which con-
tains 398.6M words. We used the NIST 2006 MT
Chinese-English dataset as the development set and
NIST 2008 dataset as the test set. The evaluation
metric is case-insensitive BLEU. Because of the ex-
pensive computational cost for training our neural
ITG reordering model, only the reordering exam-
ples extracted from about 1/5 of the entire parallel
training corpus were used to train our neural ITG re-
ordering model.
For the neural ITG reordering model, we set the
dimension of the word embedding vectors to 25 em-
pirically, which is a trade-off between computational
cost and expressive power. We use the early stop-
ping principle to determine when to stop L-BFGS.
The hyper-parameters α, λL, λrec and λreo are op-
timized by random search (Bergstra and Bengio,
2012). As preliminary experiments show that classi-
fication accuracy has a high correlation with BLEU
score, we optimize these hyper-parameters with re-
spect to classification accuracy instead of BLEU
to reduce computational cost. We randomly select
400,000 reordering examples as training set, 500 as
development set, and another 500 as test set. The
numbers of straight and inverted reordering exam-
ples in the development/test set are set to be equal
to avoid biases. We draw α uniformly from 0.05
</bodyText>
<page confidence="0.999573">
573
</page>
<tableCaption confidence="0.96096325">
Table 1: BLEU scores on the NIST 2006 and 2008
datasets. *: significantly better (p &lt; 0.01). “maxent”
denotes the baseline maximum entropy system and “neu-
ral” denotes our recursive autoencoder system.
</tableCaption>
<table confidence="0.999737428571429">
length &gt; &lt;
[1,10] 43 121 57
[11,20] 181 67 164
[21,30] 170 11 152
[31,40] 105 3 90
[41,50] 69 1 53
[51,119] 40 0 30
</table>
<tableCaption confidence="0.951912666666667">
Table 2: Number of sentences that our system has a
higher (&gt;), equal (=) or lower (&lt;) sentence-level BLEU-
4 score on the NIST 2008 dataset.
</tableCaption>
<bodyText confidence="0.9616465">
to 0.3, and AL, Arec, Areo exponentially from 10−8
to 10−2. We use the following hyper-parameters in
our experiments: α = 0.11764, AL = 7.59 x 10−5,
Arec = 1.30 x 10−5 and Areo = 3.80 x 10−4. 3
The baseline system is a re-implementation of
(Xiong et al., 2006). Our system is different from the
baseline by replacing the MaxEnt reordering model
with a neural model. Both the systems have the same
pruning settings: the threshold pruning parameter is
set to 0.5 and the histogram pruning parameter to
40. For minimum-error-rate training, both systems
generate 200-best lists.
</bodyText>
<subsectionHeader confidence="0.992107">
4.2 MT Evaluation
</subsectionHeader>
<bodyText confidence="0.999388454545455">
Table 1 shows the case-insensitive BLEU-4 scores
of the baseline system and our system on the devel-
opment and test sets. Our system outperforms the
baseline system by 1.21 BLEU points on the de-
velopment set and 1.07 on the test set. Both the
differences are statistically significant at p = 0.01
level (Riezler and Maxwell, 2005).
Table 2 shows the number of sentences that our
system has a higher (&gt;), equal (=) or lower (&lt;)
BLEU score on the NIST 2008 dataset. We find that
our system is superior to the baseline system for long
</bodyText>
<footnote confidence="0.826850333333333">
3The choice of α is very important for achieving high BLEU
scores. We tried a number of intervals and found that the clas-
sification accuracy is most stable in the interval [0.100,0.125].
</footnote>
<figure confidence="0.990294142857143">
100
neural
maxent
95
90
5 10 15 20 25 30 35
Length
</figure>
<figureCaption confidence="0.975194">
Figure 4: Comparison of reordering classification accu-
racies between the MaxEnt and neural classifiers over
varying phrase lengths. “Length” denotes the sum of the
lengths of two source phrases in a reordering example.
Our classifier (neural) outperforms the MaxEnt classi-
fier (maxent) consistently, especially for predicting long-
distance reordering.
</figureCaption>
<figure confidence="0.414745">
6,004,441 31.61 24.82
</figure>
<figureCaption confidence="0.4880104">
Table 3: The effect of reordering training data size on
BLEU scores. The BLEU scores rise with the increase of
training data size. Due to the computational cost, we only
used 1/5 of the entire bilingual corpus to train our neural
reordering model.
</figureCaption>
<bodyText confidence="0.991333647058824">
sentences.
Figure 4 compares classification accuracies of the
neural and MaxEnt classifiers. “Length” denotes the
sum of the lengths of two source phrases in a re-
ordering example. For each length, we randomly se-
lect 200 unseen reordering examples to calculate the
classification accuracy. Our classifier outperforms
the baseline consistently, especially for long com-
posed blocks.
Xiong et al. (2008) find that the performance of
the baseline system can be improved by forbidding
inverted reordering if the phrase length exceeds a
pre-defined distortion limit. This heuristic increases
the BLEU score of the baseline system significantly
to 24.46 but is still significantly worse (p &lt; 0.05)
than our system without the heuristic. We find that
imposing this heuristic fails to improve our system
</bodyText>
<figure confidence="0.996677888888889">
System
maxent
neural
NIST 2006 (tune) NIST 2008
30.40 23.75
31.61* 24.82*
# of examples
NIST 2006 (tune) NIST 2008
100,000
200,000
400,000
800,000
30.88 23.78
30.75 23.89
30.80 24.35
31.01 24.45
Accuracy (%)
88 1
</figure>
<page confidence="0.992884">
574
</page>
<table confidence="0.551595">
cluster 1 cluster 2 cluster 3 cluster 4 cluster 5
1.18 works for alternative duties these people who of the three
accessibility verify on one-day conference the reasons why on the fundamental
wheelchair tunnels from armed groups the story of how over the entire
candies transparency in chinese language works the system which through its own
cough opinion at eating habits the trend towards with the best
</table>
<tableCaption confidence="0.916154">
Table 4: Words and phrases that are close in the Euclidean space. The words and phrases in the same cluster have
similar behaviors from a reordering point of view rather than relatedness, suggesting that the vector representations
produced by the recursive autoencoders are helpful for capturing reordering regularities.
</tableCaption>
<bodyText confidence="0.999347555555556">
significantly. One possible reason is that there is
limited room for improvement as our system makes
fewer wrong predictions for long composed blocks.
The above results suggest that our system does go
beyond using boundary words and make a better use
of the merging blocks by using vector space repre-
sentations.
Table 3 shows the effect of training dataset size
on BLEU scores. We find that BLEU scores on both
the development and test sets rise with the increase
of the training dataset size. As the training process is
very time-consuming, only the reordering examples
extracted from 1/5 of the entire parallel training cor-
pus are used in our experiments to train our model.
Obviously, with more efficient training algorithms,
making full use of all the reordering examples ex-
tracted from the entire corpus will result in better
results. We leave this for future work.
</bodyText>
<subsectionHeader confidence="0.9916945">
4.3 Qualitative Analysis on Vector
Representations
</subsectionHeader>
<bodyText confidence="0.99998535">
Table 4 shows a number of words and phrases that
are close (measured by Euclidean distance) in the
n-dimensional space. We randomly select about
370K target side phrases used in our experiments
and cluster them into 983 clusters using k-means al-
gorithm (MacQueen, 1967). The distance between
two phrases are measured by the Euclidean distance
between their vector representations. As shown in
Table 4, cluster 1 mainly consists of nouns, clus-
ter 2 mainly contains verb/noun+preposition struc-
tures, cluster 3 contains compound phrases, cluster
4 consists of phrases which should be followed by
a clause, and cluster 5 mainly contains the begin-
ning parts of prepositional phrases that tend to be
followed by a noun phrase or word. We find that
the words and phrases in the same cluster have sim-
ilar behaviors from a reordering point of view rather
than relatedness. This indicates that the vector rep-
resentations produced by the recursive autoencoders
are helpful for capturing reordering regularities.
</bodyText>
<sectionHeader confidence="0.997771" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99998725">
We have presented an ITG reordering classifier
based on recursive autoencoders. As recursive au-
toencoders are capable of producing vector space
representations for arbitrary multi-word strings in
decoding, our neural ITG system achieves an ab-
solute improvement of 1.07 BLEU points over the
baseline on the NIST 2008 Chinese-English dataset.
There are a number of interesting directions we
would like to pursue in the near future. First, re-
placing the MaxEnt classifier with a neural one re-
defines the conditions for risk-free hypothesis re-
combination. We find that the number of hypothe-
ses that can be recombined reduces in our system.
Therefore, we plan to use forest reranking (Huang,
2008) to alleviate this problem. Second, it is in-
teresting to follow Socher et al. (2013) to combine
linguistically-motivated labels with recursive neural
networks. Another problem with our system is that
the decoding speed is much slower than the baseline
system because of the computational overhead intro-
duced by RAEs. It is necessary to investigate more
efficient decoding algorithms. Finally, it is possible
to apply our method to other phrase-based and even
syntax-based systems.
</bodyText>
<sectionHeader confidence="0.99882" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.76174875">
This research is supported by the 863 Program un-
der the grant No. 2012AA011102, by the Boeing
Tsinghua Joint Research Project on Language Pro-
cessing (Agreement TBRC-008-SDB-2011 Phase 3
</bodyText>
<page confidence="0.994719">
575
</page>
<bodyText confidence="0.9998353">
(2013)), by the Singapore National Research Foun-
dation under its International Research Centre @
Singapore Funding Initiative and administered by
the IDM Programme Office, and by a Research Fund
No. 20123000007 from Tsinghua MOE-Microsoft
Joint Laboratory. Many thanks go to Chunyang Liu
and Chong Kuang for their great help for setting up
the computing platform. We also thank Min-Yen
Kan, Meng Zhang and Yu Zhao for their insightful
discussions.
</bodyText>
<sectionHeader confidence="0.998609" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998767195652174">
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion models for statistical machine translation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
529–536, Sydney, Australia, July.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137–1155, March.
James Bergstra and Yoshua Bengio. 2012. Random
search for hyper-parameter optimization. The Jour-
nal of Machine Learning Research, 13(1):281–305,
February.
Arianna Bisazza and Marcello Federico. 2012. Modi-
fied distortion matrices for phrase-based statistical ma-
chine translation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 478–487, Jeju Is-
land, Korea, July.
Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning structured embed-
dings of knowledge bases. In Proceedings of the
Twenty-Fifth AAAI Conference on Artificial Intelli-
gence, pages 301–306.
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words and
meaning representations for open-text semantic pars-
ing. In International Conference on Artificial Intelli-
gence and Statistics, pages 127–135.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 22–31,
Atlanta, Georgia, June.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: Deep neu-
ral networks with multitask learning. In Proceed-
ings of the 25th International Conference on Machine
Learning, pages 160–167.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493–2537.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An
efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics: Posters, pages 285–293, Beijing, China, August.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 848–856, Honolulu, Hawaii, October.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Proceed-
ings of the 28th International Conference on Machine
Learning (ICML-11), pages 513–520.
Christoph Goller and Andreas Kuchler. 1996. Learning
task-dependent distributed representations by back-
propagation through structure. In Proceedings of 1996
IEEE International Conference on Neural Networks
(Volume:1), volume 1, pages 347–352.
Spence Green, Michel Galley, and Christopher D. Man-
ning. 2010. Improved models of distortion cost for
statistical machine translation. In Proceedings of Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 867–875,
Los Angeles, California, June.
Karl Moritz Hermann and Phil Blunsom. 2013. The role
of syntax in vector space models of compositional se-
mantics. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 894–904, Sofia, Bulgaria,
August.
Eric Huang, Richard Socher, Christopher Manning, and
Andrew Ng. 2012. Improving word representations
via global context and multiple word prototypes. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long
Papers), pages 873–882, Jeju Island, Korea, July.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 586–594, Columbus, Ohio, June.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607–615, December.
</reference>
<page confidence="0.982997">
576
</page>
<reference confidence="0.999849933962264">
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology-Volume 1, pages
48–54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177–
180, Prague, Czech Republic, June.
James MacQueen. 1967. Some methods for classifica-
tion and analysis of multivariate observations. In Pro-
ceedings of the Fifth Berkeley Symposium on Mathe-
matical Statistics and Probability, volume 1.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417–449.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics: HLT-
NAACL 2004: Main Proceedings, pages 161–168,
Boston, Massachusetts, USA, May.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 57–
64, Ann Arbor, Michigan, June.
Richard Socher, Eric H. Huang, Jeffrey Pennin, An-
drew Y. Ng, and Christopher D. Manning. 2011a. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Proceedings of Advances
in Neural Information Processing Systems 24, pages
801–809.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christo-
pher D. Manning. 2011b. Parsing natural scenes and
natural language with recursive neural networks. In
Proceedings of the 26th International Conference on
Machine Learning (ICML), pages 129–136.
Richard Socher, Jeffrey Pennington, Eric H. Huang, An-
drew Y. Ng, and Christopher D. Manning. 2011c.
Semi-supervised recursive autoencoders for predicting
sentiment distributions. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161, Edinburgh, Scot-
land, UK., July.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1201–
1211, Jeju Island, Korea, July.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with compositional
vector grammars. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 455–465, Sofia,
Bulgaria, August.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
vol. 2, pages 901–904, September.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics: HLT-NAACL 2004: Short Pa-
pers, pages 101–104, Boston, Massachusetts, USA,
May.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 521–528, Sydney,
Australia, July.
Deyi Xiong, Min Zhang, Aiti Aw, Haitao Mi, Qun Liu,
and Shouxun Lin. 2008. Refinements in BTG-based
statistical machine translation. In Proceedings of the
Third International Joint Conference on Natural Lan-
guage Processing: Volume-I, pages 505–512.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2010.
Linguistically annotated reordering: Evaluation and
analysis. Computational Linguistics, 36(3):535–568,
September.
Richard Zens, Hermann Ney, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Reordering constraints for
phrase-based statistical machine translation. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics, pages 205–211, Geneva,
Switzerland, Aug 23–Aug 27.
</reference>
<page confidence="0.997296">
577
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.780817">
<title confidence="0.999253">Recursive Autoencoders for ITG-based Translation</title>
<author confidence="0.973836">Yang Liu Li</author>
<affiliation confidence="0.910576333333333">State Key Laboratory of Intelligent Technology and Tsinghua National Laboratory for Information Science and Department of Computer Science and Technology, Tsinghua University, Beijing 100084,</affiliation>
<abstract confidence="0.997339105263158">While inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge. Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kishore Papineni</author>
</authors>
<title>Distortion models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>529--536</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="2190" citStr="Al-Onaizan and Papineni, 2006" startWordPosition="309" endWordPosition="312">am language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, especially for resource-scarce languages without highaccuracy parsers. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Among them, reordering models based on inversion transduction grammar (ITG) (Wu, 1997) are one of the important ongoing research directions. As a formalism for bilingual modeling of sentence pairs, ITG is particularly well suited to predicting ordering shifts between languages. As a result, a number of authors have incorporated ITG into leftto-right decoding to constrain the reordering space and reported significant improvements (e.g., Zens et al</context>
</contexts>
<marker>Al-Onaizan, Papineni, 2006</marker>
<rawString>Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion models for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 529–536, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="4652" citStr="Bengio et al., 2003" startWordPosition="694" endWordPosition="697">major challenge: how to extract features from training examples (i.e., a pair of bilingual strings). It is hard to decide which words are representative for predicting reordering, either manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), wh</context>
<context position="11767" citStr="Bengio et al., 2003" startWordPosition="1885" endWordPosition="1888"> long sentences.1 Indeed, Xiong et al. (2008) find that the MaxEnt classifier with boundary words as features is prone to make wrong predictions for long composed blocks. As a result, they have to impose a hard constraint to always prefer merging long composed blocks in a monotonic way. Therefore, it is important to consider more than boundary words to make more accurate reordering predictions. We need a new mechanism to achieve this goal. 2.2 Recursive Autoencoders 2.2.1 Vector Space Representations for Words In neural networks, a natural language word is represented as a real-valued vector (Bengio et al., 2003; Collobert and Weston, 2008). For example, we can use [0.1 0.8 0.4]T to represent “female” and 1Strictly speaking, the ITG reordering model is not a phrase reordering model since phrase pairs are only the atomic blocks. Instead, it is defined to work on arbitrarily long strings because composed blocks become larger and larger until the entire sentence pair is generated. 570 Figure 3: A neural ITG reordering model. The binary classifier makes decisions based on the vector space representations of the source and target sides of merging blocks. [0.7 0.1 0.5]T to represent “friend”. Such vector s</context>
<context position="17340" citStr="Bengio et al., 2003" startWordPosition="2865" endWordPosition="2868">). All these parameters are learned automatically from the training data. For clarity, we will use θ to denote all these parameters in the rest of the paper. For training word embedding matrix, there are two settings commonly used. In the first setting, the word embedding matrix is initialized randomly. This works well in a supervised scenario, in which a neural network updates the matrix in order to optimize some task-specific objectives (Collobert et al., 2011; Socher et al., 2011c). In the second setting, the word embedding matrix is pre-trained using an unsupervised neural language model (Bengio et al., 2003; Collobert and Weston, 2008) with huge amount of unlabeled data. In this work, we prefer to the first setting because the word embedding matrices can be trained to minimize errors with respect to reordering modeling. There are two kinds of errors involved 1. reconstruction error: how well the learned vector space representations represent the corresponding strings? 2. reordering error: how well the classifier predicts the merging order? As described in Section 2.2.2, the input vector c1 and c2 of a recursive autoencoder can be reconstructed using Eq. 8 as ci and c2. We use Euclidean distance </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Bergstra</author>
<author>Yoshua Bengio</author>
</authors>
<title>Random search for hyper-parameter optimization.</title>
<date>2012</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="22234" citStr="Bergstra and Bengio, 2012" startWordPosition="3720" endWordPosition="3723">evaluation metric is case-insensitive BLEU. Because of the expensive computational cost for training our neural ITG reordering model, only the reordering examples extracted from about 1/5 of the entire parallel training corpus were used to train our neural ITG reordering model. For the neural ITG reordering model, we set the dimension of the word embedding vectors to 25 empirically, which is a trade-off between computational cost and expressive power. We use the early stopping principle to determine when to stop L-BFGS. The hyper-parameters α, λL, λrec and λreo are optimized by random search (Bergstra and Bengio, 2012). As preliminary experiments show that classification accuracy has a high correlation with BLEU score, we optimize these hyper-parameters with respect to classification accuracy instead of BLEU to reduce computational cost. We randomly select 400,000 reordering examples as training set, 500 as development set, and another 500 as test set. The numbers of straight and inverted reordering examples in the development/test set are set to be equal to avoid biases. We draw α uniformly from 0.05 573 Table 1: BLEU scores on the NIST 2006 and 2008 datasets. *: significantly better (p &lt; 0.01). “maxent” d</context>
</contexts>
<marker>Bergstra, Bengio, 2012</marker>
<rawString>James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization. The Journal of Machine Learning Research, 13(1):281–305, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arianna Bisazza</author>
<author>Marcello Federico</author>
</authors>
<title>Modified distortion matrices for phrase-based statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>478--487</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="2323" citStr="Bisazza and Federico, 2012" startWordPosition="334" endWordPosition="337">nally, phrase-based systems can be applicable to most domains and languages, especially for resource-scarce languages without highaccuracy parsers. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Among them, reordering models based on inversion transduction grammar (ITG) (Wu, 1997) are one of the important ongoing research directions. As a formalism for bilingual modeling of sentence pairs, ITG is particularly well suited to predicting ordering shifts between languages. As a result, a number of authors have incorporated ITG into leftto-right decoding to constrain the reordering space and reported significant improvements (e.g., Zens et al., 2004; Feng et al., 2010). Along another line, Xiong et al. (2006) propose a maximum entropy (MaxEnt) reordering model based on ITG</context>
</contexts>
<marker>Bisazza, Federico, 2012</marker>
<rawString>Arianna Bisazza and Marcello Federico. 2012. Modified distortion matrices for phrase-based statistical machine translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 478–487, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Jason Weston</author>
<author>Ronan Collobert</author>
<author>Yoshua Bengio</author>
</authors>
<title>Learning structured embeddings of knowledge bases.</title>
<date>2011</date>
<booktitle>In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence,</booktitle>
<pages>301--306</pages>
<contexts>
<context position="4746" citStr="Bordes et al., 2011" startWordPosition="710" endWordPosition="713">ings). It is hard to decide which words are representative for predicting reordering, either manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from</context>
</contexts>
<marker>Bordes, Weston, Collobert, Bengio, 2011</marker>
<rawString>Antoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio. 2011. Learning structured embeddings of knowledge bases. In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, pages 301–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Xavier Glorot</author>
<author>Jason Weston</author>
<author>Yoshua Bengio</author>
</authors>
<title>Joint learning of words and meaning representations for open-text semantic parsing.</title>
<date>2012</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>127--135</pages>
<contexts>
<context position="4854" citStr="Bordes et al., 2012" startWordPosition="730" endWordPosition="733">omatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language modeling point of view. In this work, we propose an ITG reordering classifier based on re</context>
</contexts>
<marker>Bordes, Glorot, Weston, Bengio, 2012</marker>
<rawString>Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. 2012. Joint learning of words and meaning representations for open-text semantic parsing. In International Conference on Artificial Intelligence and Statistics, pages 127–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
</authors>
<title>Improved reordering for phrasebased translation using sparse features.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>22--31</pages>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="2338" citStr="Cherry, 2013" startWordPosition="338" endWordPosition="339">can be applicable to most domains and languages, especially for resource-scarce languages without highaccuracy parsers. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Among them, reordering models based on inversion transduction grammar (ITG) (Wu, 1997) are one of the important ongoing research directions. As a formalism for bilingual modeling of sentence pairs, ITG is particularly well suited to predicting ordering shifts between languages. As a result, a number of authors have incorporated ITG into leftto-right decoding to constrain the reordering space and reported significant improvements (e.g., Zens et al., 2004; Feng et al., 2010). Along another line, Xiong et al. (2006) propose a maximum entropy (MaxEnt) reordering model based on ITG. They use the </context>
</contexts>
<marker>Cherry, 2013</marker>
<rawString>Colin Cherry. 2013. Improved reordering for phrasebased translation using sparse features. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 22–31, Atlanta, Georgia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="4680" citStr="Collobert and Weston, 2008" startWordPosition="698" endWordPosition="701">to extract features from training examples (i.e., a pair of bilingual strings). It is hard to decide which words are representative for predicting reordering, either manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to levera</context>
<context position="11796" citStr="Collobert and Weston, 2008" startWordPosition="1889" endWordPosition="1892">eed, Xiong et al. (2008) find that the MaxEnt classifier with boundary words as features is prone to make wrong predictions for long composed blocks. As a result, they have to impose a hard constraint to always prefer merging long composed blocks in a monotonic way. Therefore, it is important to consider more than boundary words to make more accurate reordering predictions. We need a new mechanism to achieve this goal. 2.2 Recursive Autoencoders 2.2.1 Vector Space Representations for Words In neural networks, a natural language word is represented as a real-valued vector (Bengio et al., 2003; Collobert and Weston, 2008). For example, we can use [0.1 0.8 0.4]T to represent “female” and 1Strictly speaking, the ITG reordering model is not a phrase reordering model since phrase pairs are only the atomic blocks. Instead, it is defined to work on arbitrarily long strings because composed blocks become larger and larger until the entire sentence pair is generated. 570 Figure 3: A neural ITG reordering model. The binary classifier makes decisions based on the vector space representations of the source and target sides of merging blocks. [0.7 0.1 0.5]T to represent “friend”. Such vector space representations enable n</context>
<context position="17369" citStr="Collobert and Weston, 2008" startWordPosition="2869" endWordPosition="2872">rs are learned automatically from the training data. For clarity, we will use θ to denote all these parameters in the rest of the paper. For training word embedding matrix, there are two settings commonly used. In the first setting, the word embedding matrix is initialized randomly. This works well in a supervised scenario, in which a neural network updates the matrix in order to optimize some task-specific objectives (Collobert et al., 2011; Socher et al., 2011c). In the second setting, the word embedding matrix is pre-trained using an unsupervised neural language model (Bengio et al., 2003; Collobert and Weston, 2008) with huge amount of unlabeled data. In this work, we prefer to the first setting because the word embedding matrices can be trained to minimize errors with respect to reordering modeling. There are two kinds of errors involved 1. reconstruction error: how well the learned vector space representations represent the corresponding strings? 2. reordering error: how well the classifier predicts the merging order? As described in Section 2.2.2, the input vector c1 and c2 of a recursive autoencoder can be reconstructed using Eq. 8 as ci and c2. We use Euclidean distance between the input and the rec</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
<author>L Bottou</author>
<author>M Karlen</author>
<author>K Kavukcuoglu</author>
<author>P Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="4704" citStr="Collobert et al., 2011" startWordPosition="702" endWordPosition="705">ining examples (i.e., a pair of bilingual strings). It is hard to decide which words are representative for predicting reordering, either manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representa</context>
<context position="17187" citStr="Collobert et al., 2011" startWordPosition="2839" endWordPosition="2842">ms b(1), b(2) for both source and target languages (Section 2.2.2); 3. θreo: neural ITG reordering model parameter matrix Wo and bias term bo (Section 2.2.3). All these parameters are learned automatically from the training data. For clarity, we will use θ to denote all these parameters in the rest of the paper. For training word embedding matrix, there are two settings commonly used. In the first setting, the word embedding matrix is initialized randomly. This works well in a supervised scenario, in which a neural network updates the matrix in order to optimize some task-specific objectives (Collobert et al., 2011; Socher et al., 2011c). In the second setting, the word embedding matrix is pre-trained using an unsupervised neural language model (Bengio et al., 2003; Collobert and Weston, 2008) with huge amount of unlabeled data. In this work, we prefer to the first setting because the word embedding matrices can be trained to minimize errors with respect to reordering modeling. There are two kinds of errors involved 1. reconstruction error: how well the learned vector space representations represent the corresponding strings? 2. reordering error: how well the classifier predicts the merging order? As de</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Feng</author>
<author>Haitao Mi</author>
<author>Yang Liu</author>
<author>Qun Liu</author>
</authors>
<title>An efficient shift-reduce decoding algorithm for phrasedbased machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>285--293</pages>
<location>Beijing, China,</location>
<contexts>
<context position="2275" citStr="Feng et al., 2010" startWordPosition="326" endWordPosition="329">ions grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, especially for resource-scarce languages without highaccuracy parsers. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Among them, reordering models based on inversion transduction grammar (ITG) (Wu, 1997) are one of the important ongoing research directions. As a formalism for bilingual modeling of sentence pairs, ITG is particularly well suited to predicting ordering shifts between languages. As a result, a number of authors have incorporated ITG into leftto-right decoding to constrain the reordering space and reported significant improvements (e.g., Zens et al., 2004; Feng et al., 2010). Along another line, Xiong et al. (2006) propose a maximu</context>
</contexts>
<marker>Feng, Mi, Liu, Liu, 2010</marker>
<rawString>Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An efficient shift-reduce decoding algorithm for phrasedbased machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 285–293, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>848--856</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="2256" citStr="Galley and Manning, 2008" startWordPosition="321" endWordPosition="325">ers since partial translations grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, especially for resource-scarce languages without highaccuracy parsers. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Among them, reordering models based on inversion transduction grammar (ITG) (Wu, 1997) are one of the important ongoing research directions. As a formalism for bilingual modeling of sentence pairs, ITG is particularly well suited to predicting ordering shifts between languages. As a result, a number of authors have incorporated ITG into leftto-right decoding to constrain the reordering space and reported significant improvements (e.g., Zens et al., 2004; Feng et al., 2010). Along another line, Xiong et al. (200</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 848–856, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Domain adaptation for large-scale sentiment classification: A deep learning approach.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>513--520</pages>
<contexts>
<context position="4725" citStr="Glorot et al., 2011" startWordPosition="706" endWordPosition="709">pair of bilingual strings). It is hard to decide which words are representative for predicting reordering, either manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in r</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 513–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Goller</author>
<author>Andreas Kuchler</author>
</authors>
<title>Learning task-dependent distributed representations by backpropagation through structure.</title>
<date>1996</date>
<booktitle>In Proceedings of 1996 IEEE International Conference on Neural Networks (Volume:1),</booktitle>
<volume>1</volume>
<pages>347--352</pages>
<contexts>
<context position="21162" citStr="Goller and Kuchler, 1996" startWordPosition="3543" endWordPosition="3546">larity. not desirable. In order to prevent such behavior, we normalize all the output vectors of the hidden layers to have length 1 in the same way as (Socher et al., 2011c). Namely we set p = p ||p ||after computing p as in Eq. 7, and c01 = c� 1||ci ||,c02=c�2 ||4 ||in Eq. 8. Following Socher et al. (2011c), we use L-BFGS to estimate the parameters with respect to the joint training objective. Given a set of parameters, we construct binary trees for all the phrases using the greedy algorithm. The derivatives for these fixed binary trees can be computed via backpropagation through structures (Goller and Kuchler, 1996). 4 Experiments 4.1 Data Preparation We evaluated our system on Chinese-English translation. The training corpus contains 1.23M sentence pairs with 32.1M Chinese words and 35.4M English words. We used SRILM (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus, which contains 398.6M words. We used the NIST 2006 MT Chinese-English dataset as the development set and NIST 2008 dataset as the test set. The evaluation metric is case-insensitive BLEU. Because of the expensive computational cost for training our neural ITG reordering model, only the reordering </context>
</contexts>
<marker>Goller, Kuchler, 1996</marker>
<rawString>Christoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backpropagation through structure. In Proceedings of 1996 IEEE International Conference on Neural Networks (Volume:1), volume 1, pages 347–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Improved models of distortion cost for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>867--875</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="2295" citStr="Green et al., 2010" startWordPosition="330" endWordPosition="333">ight in decoding. Finally, phrase-based systems can be applicable to most domains and languages, especially for resource-scarce languages without highaccuracy parsers. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Among them, reordering models based on inversion transduction grammar (ITG) (Wu, 1997) are one of the important ongoing research directions. As a formalism for bilingual modeling of sentence pairs, ITG is particularly well suited to predicting ordering shifts between languages. As a result, a number of authors have incorporated ITG into leftto-right decoding to constrain the reordering space and reported significant improvements (e.g., Zens et al., 2004; Feng et al., 2010). Along another line, Xiong et al. (2006) propose a maximum entropy (MaxEnt) r</context>
</contexts>
<marker>Green, Galley, Manning, 2010</marker>
<rawString>Spence Green, Michel Galley, and Christopher D. Manning. 2010. Improved models of distortion cost for statistical machine translation. In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 867–875, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>The role of syntax in vector space models of compositional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>894--904</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="4923" citStr="Hermann and Blunsom, 2013" startWordPosition="742" endWordPosition="745">et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language modeling point of view. In this work, we propose an ITG reordering classifier based on recursive autoencoders. The neural network consists of four autoencoder</context>
</contexts>
<marker>Hermann, Blunsom, 2013</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2013. The role of syntax in vector space models of compositional semantics. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 894–904, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Huang</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>873--882</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="4874" citStr="Huang et al., 2012" startWordPosition="734" endWordPosition="737">y for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language modeling point of view. In this work, we propose an ITG reordering classifier based on recursive autoencoders</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric Huang, Richard Socher, Christopher Manning, and Andrew Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 873–882, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>586--594</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="29535" citStr="Huang, 2008" startWordPosition="4920" endWordPosition="4921">As recursive autoencoders are capable of producing vector space representations for arbitrary multi-word strings in decoding, our neural ITG system achieves an absolute improvement of 1.07 BLEU points over the baseline on the NIST 2008 Chinese-English dataset. There are a number of interesting directions we would like to pursue in the near future. First, replacing the MaxEnt classifier with a neural one redefines the conditions for risk-free hypothesis recombination. We find that the number of hypotheses that can be recombined reduces in our system. Therefore, we plan to use forest reranking (Huang, 2008) to alleviate this problem. Second, it is interesting to follow Socher et al. (2013) to combine linguistically-motivated labels with recursive neural networks. Another problem with our system is that the decoding speed is much slower than the baseline system because of the computational overhead introduced by RAEs. It is necessary to investigate more efficient decoding algorithms. Finally, it is possible to apply our method to other phrase-based and even syntax-based systems. Acknowledgments This research is supported by the 863 Program under the grant No. 2012AA011102, by the Boeing Tsinghua </context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 586–594, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Decoding complexity in wordreplacement translation models.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="2009" citStr="Knight, 1999" startWordPosition="283" endWordPosition="284">zing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, especially for resource-scarce languages without highaccuracy parsers. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Among them, reordering models based on inversion transduction grammar (ITG) (Wu, 1997) are one of the important ongoing research directions. As a formalism for bilingual modeling of sentence pairs, ITG is particularly well suited to predicting ordering shifts between l</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. Decoding complexity in wordreplacement translation models. Computational Linguistics, 25(4):607–615, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume</booktitle>
<volume>1</volume>
<pages>48--54</pages>
<contexts>
<context position="1177" citStr="Koehn et al., 2003" startWordPosition="160" endWordPosition="163">ctual blocks being merged remains a challenge. Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. 1 Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, esp</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 48–54.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2230" citStr="Koehn et al., 2007" startWordPosition="317" endWordPosition="320">o phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, especially for resource-scarce languages without highaccuracy parsers. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Among them, reordering models based on inversion transduction grammar (ITG) (Wu, 1997) are one of the important ongoing research directions. As a formalism for bilingual modeling of sentence pairs, ITG is particularly well suited to predicting ordering shifts between languages. As a result, a number of authors have incorporated ITG into leftto-right decoding to constrain the reordering space and reported significant improvements (e.g., Zens et al., 2004; Feng et al., 2010). Along anoth</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177– 180, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James MacQueen</author>
</authors>
<title>Some methods for classification and analysis of multivariate observations.</title>
<date>1967</date>
<booktitle>In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability,</booktitle>
<volume>1</volume>
<contexts>
<context position="28098" citStr="MacQueen, 1967" startWordPosition="4692" endWordPosition="4693">cted from 1/5 of the entire parallel training corpus are used in our experiments to train our model. Obviously, with more efficient training algorithms, making full use of all the reordering examples extracted from the entire corpus will result in better results. We leave this for future work. 4.3 Qualitative Analysis on Vector Representations Table 4 shows a number of words and phrases that are close (measured by Euclidean distance) in the n-dimensional space. We randomly select about 370K target side phrases used in our experiments and cluster them into 983 clusters using k-means algorithm (MacQueen, 1967). The distance between two phrases are measured by the Euclidean distance between their vector representations. As shown in Table 4, cluster 1 mainly consists of nouns, cluster 2 mainly contains verb/noun+preposition structures, cluster 3 contains compound phrases, cluster 4 consists of phrases which should be followed by a clause, and cluster 5 mainly contains the beginning parts of prepositional phrases that tend to be followed by a noun phrase or word. We find that the words and phrases in the same cluster have similar behaviors from a reordering point of view rather than relatedness. This </context>
</contexts>
<marker>MacQueen, 1967</marker>
<rawString>James MacQueen. 1967. Some methods for classification and analysis of multivariate observations. In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1197" citStr="Och and Ney, 2004" startWordPosition="164" endWordPosition="167">erged remains a challenge. Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points. 1 Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability. First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, especially for resource</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
<author>Sanjeev Khudanpur</author>
<author>Anoop Sarkar</author>
<author>Kenji Yamada</author>
<author>Alex Fraser</author>
<author>Shankar Kumar</author>
<author>Libin Shen</author>
<author>David Smith</author>
<author>Katherine Eng</author>
<author>Viren Jain</author>
<author>Zhen Jin</author>
<author>Dragomir Radev</author>
</authors>
<title>A smorgasbord of features for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLTNAACL 2004: Main Proceedings,</booktitle>
<pages>161--168</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="2125" citStr="Och et al., 2004" startWordPosition="299" endWordPosition="302">ns with word insertions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, especially for resource-scarce languages without highaccuracy parsers. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Among them, reordering models based on inversion transduction grammar (ITG) (Wu, 1997) are one of the important ongoing research directions. As a formalism for bilingual modeling of sentence pairs, ITG is particularly well suited to predicting ordering shifts between languages. As a result, a number of authors have incorporated ITG into leftto-right decoding to constrain the reorder</context>
</contexts>
<marker>Och, Gildea, Khudanpur, Sarkar, Yamada, Fraser, Kumar, Shen, Smith, Eng, Jain, Jin, Radev, 2004</marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A smorgasbord of features for statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLTNAACL 2004: Main Proceedings, pages 161–168, Boston, Massachusetts, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John T Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for MT.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>57--64</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="24109" citStr="Riezler and Maxwell, 2005" startWordPosition="4042" endWordPosition="4045">aseline by replacing the MaxEnt reordering model with a neural model. Both the systems have the same pruning settings: the threshold pruning parameter is set to 0.5 and the histogram pruning parameter to 40. For minimum-error-rate training, both systems generate 200-best lists. 4.2 MT Evaluation Table 1 shows the case-insensitive BLEU-4 scores of the baseline system and our system on the development and test sets. Our system outperforms the baseline system by 1.21 BLEU points on the development set and 1.07 on the test set. Both the differences are statistically significant at p = 0.01 level (Riezler and Maxwell, 2005). Table 2 shows the number of sentences that our system has a higher (&gt;), equal (=) or lower (&lt;) BLEU score on the NIST 2008 dataset. We find that our system is superior to the baseline system for long 3The choice of α is very important for achieving high BLEU scores. We tried a number of intervals and found that the classification accuracy is most stable in the interval [0.100,0.125]. 100 neural maxent 95 90 5 10 15 20 25 30 35 Length Figure 4: Comparison of reordering classification accuracies between the MaxEnt and neural classifiers over varying phrase lengths. “Length” denotes the sum of </context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Stefan Riezler and John T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 57– 64, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennin</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems 24,</booktitle>
<pages>801--809</pages>
<contexts>
<context position="4767" citStr="Socher et al., 2011" startWordPosition="714" endWordPosition="717">decide which words are representative for predicting reordering, either manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language mo</context>
<context position="10771" citStr="Socher et al., 2011" startWordPosition="1724" endWordPosition="1727">maximum entropy (MaxEnt) classifier: P(o|X1,X2) =exp(θ · h(o, X1, X2)) � �� exp(θ · h(o�, X1, X2)) (5) where h(·) is a vector of features defined on the blocks and the order, θ is a vector of feature weights. While MaxEnt is a flexible and powerful framework for including arbitrary features, feature engineering becomes a major challenge for the MaxEnt classifier. Xiong et al. (2006) find that boundary words (i.e., the first and the last words in a string) are informative for predicting reordering. Actually, Figure 2: A recursive autoencoder for multi-word strings. The example is adapted from (Socher et al., 2011c). Blue and grey nodes are original and reconstructed ones, respectively. it is hard to decide which internal words in a long composed blocks are representative and informative. Therefore, they only use boundary words as the main features. However, it seems not enough to just consider boundary words and ignore all internal words when making order predictions, especially for long sentences.1 Indeed, Xiong et al. (2008) find that the MaxEnt classifier with boundary words as features is prone to make wrong predictions for long composed blocks. As a result, they have to impose a hard constraint t</context>
<context position="14814" citStr="Socher et al., 2011" startWordPosition="2416" endWordPosition="2419">twork can be naturally integrated into CKY parsing. To assess how well the learned vector p represents its children, we can reconstruct the children in a reconstruction layer: [c01; c02] = f(2)(W (2)p + b(2)) (8) where c01 and c02 are the reconstructed children, W(2) is a parameter matrix for reconstruction, b(2) is a bias term for reconstruction, and f(2) is an element-wise activation function, which is also set as tanh(·) in our experiments. Similarly, the same reconstruction neural network can be applied to each node in an ITG parse. These neural networks are called recursive autoencoders (Socher et al., 2011c). Figure 2 illustrates an application of a recursive autoencoder to a 571 binary tree. The blue and grey nodes are the original and reconstructed nodes, respectively. The autoencoder is re-used at each node of the tree. The binary tree is composed of a set of triplets in the form of (p —* c1 c2), where p is a parent vector and c1 and c2 are children vectors of p. Each child can be either an input word vector or a multi-word vector. Therefore, the tree in Figure 2 can be represented as three triplets: (y1 — x1 x2), (y2 — y1 x3), and (y3 — y2 x4). In Figure 1, we use recursive autoencoders to </context>
<context position="17208" citStr="Socher et al., 2011" startWordPosition="2843" endWordPosition="2846">ource and target languages (Section 2.2.2); 3. θreo: neural ITG reordering model parameter matrix Wo and bias term bo (Section 2.2.3). All these parameters are learned automatically from the training data. For clarity, we will use θ to denote all these parameters in the rest of the paper. For training word embedding matrix, there are two settings commonly used. In the first setting, the word embedding matrix is initialized randomly. This works well in a supervised scenario, in which a neural network updates the matrix in order to optimize some task-specific objectives (Collobert et al., 2011; Socher et al., 2011c). In the second setting, the word embedding matrix is pre-trained using an unsupervised neural language model (Bengio et al., 2003; Collobert and Weston, 2008) with huge amount of unlabeled data. In this work, we prefer to the first setting because the word embedding matrices can be trained to minimize errors with respect to reordering modeling. There are two kinds of errors involved 1. reconstruction error: how well the learned vector space representations represent the corresponding strings? 2. reordering error: how well the classifier predicts the merging order? As described in Section 2.</context>
<context position="18887" citStr="Socher et al. (2011" startWordPosition="3129" endWordPosition="3132">h minimal reconstruction error, we follow Socher et al. (2011c) to use a greedy algorithm. Taking Figure 2 as an example, the greedy algorithm begins with computing the reconstruction error Erec(�) for each pair of consecutive vectors, i.e., Erec([x1; x2]; θ), Erec([x2; x3]; θ) and Erec([x3; x4]; θ). Suppose Erec([x1; x2]; θ) is the smallest, the algorithm will replace x1 and x2 with their vector representation y1 produced by the recursive autoencoder. Then, the algorithm evaluates Erec([y1; x3]; θ) and Erec([x3; x4]; θ) and repeats the above replacing steps until only one vector 572 remains. Socher et al. (2011c) find that the greedy algorithm runs fast without significant loss in performance as compared with CKY-style algorithms. Given a training example set S = {ti = (oi, X1i , X2i )}, the average reconstruction error on the source side on the training set is defined as 1E E Erec,s(S; θ) = Erec([p.c1,p.c2]; θ) Ns i p∈TOR(ti,s) (12) where Tθ R(ti, s) denotes all the intermediate nodes on the source side in binary trees, Ns is the number of these intermediate nodes, and p.ck is the kth child vector of p. The average reconstruction error on the target side, denoted by Erec,t(S; θ), can be computed in</context>
<context position="20293" citStr="Socher et al. (2011" startWordPosition="3390" endWordPosition="3393">distribution dti for its label is [1, 0] when oi = straight, and [0, 1] when oi = inverted. Then the cross-entropy error is Ec(ti; θ) = − E dti(o)log (Pθ(o|X1,X2)) (14) o where o E {straight, inverted}. As a result, the reordering error is defined as Ereo(S; θ) = |1 |E Ec(ti; θ). (15) i Therefore, the joint training objective function is J = αErec(S; θ)+(1−α)Ereo(S; θ)+R(θ) (16) where α is a parameter used to balance the preference between reconstruction error and reordering error, R(θ) is the regularizer and defined as 2 λL 2 λrec 2 λreo 2 R(θ) = 2 11θL 11 + 2 Ilθrec11 + 2 Ilθreo11 . (17) As Socher et al. (2011c) stated, a naive way for lowering the reconstruction error is to make the magnitude of the hidden layer very small, which is 2The bias terms b(1), b(2) and b� are not regularized. We do not exclude them from the equation explicitly just for clarity. not desirable. In order to prevent such behavior, we normalize all the output vectors of the hidden layers to have length 1 in the same way as (Socher et al., 2011c). Namely we set p = p ||p ||after computing p as in Eq. 7, and c01 = c� 1||ci ||,c02=c�2 ||4 ||in Eq. 8. Following Socher et al. (2011c), we use L-BFGS to estimate the parameters with</context>
</contexts>
<marker>Socher, Huang, Pennin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennin, Andrew Y. Ng, and Christopher D. Manning. 2011a. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Proceedings of Advances in Neural Information Processing Systems 24, pages 801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 26th International Conference on Machine Learning (ICML),</booktitle>
<pages>129--136</pages>
<contexts>
<context position="4767" citStr="Socher et al., 2011" startWordPosition="714" endWordPosition="717">decide which words are representative for predicting reordering, either manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language mo</context>
<context position="10771" citStr="Socher et al., 2011" startWordPosition="1724" endWordPosition="1727">maximum entropy (MaxEnt) classifier: P(o|X1,X2) =exp(θ · h(o, X1, X2)) � �� exp(θ · h(o�, X1, X2)) (5) where h(·) is a vector of features defined on the blocks and the order, θ is a vector of feature weights. While MaxEnt is a flexible and powerful framework for including arbitrary features, feature engineering becomes a major challenge for the MaxEnt classifier. Xiong et al. (2006) find that boundary words (i.e., the first and the last words in a string) are informative for predicting reordering. Actually, Figure 2: A recursive autoencoder for multi-word strings. The example is adapted from (Socher et al., 2011c). Blue and grey nodes are original and reconstructed ones, respectively. it is hard to decide which internal words in a long composed blocks are representative and informative. Therefore, they only use boundary words as the main features. However, it seems not enough to just consider boundary words and ignore all internal words when making order predictions, especially for long sentences.1 Indeed, Xiong et al. (2008) find that the MaxEnt classifier with boundary words as features is prone to make wrong predictions for long composed blocks. As a result, they have to impose a hard constraint t</context>
<context position="14814" citStr="Socher et al., 2011" startWordPosition="2416" endWordPosition="2419">twork can be naturally integrated into CKY parsing. To assess how well the learned vector p represents its children, we can reconstruct the children in a reconstruction layer: [c01; c02] = f(2)(W (2)p + b(2)) (8) where c01 and c02 are the reconstructed children, W(2) is a parameter matrix for reconstruction, b(2) is a bias term for reconstruction, and f(2) is an element-wise activation function, which is also set as tanh(·) in our experiments. Similarly, the same reconstruction neural network can be applied to each node in an ITG parse. These neural networks are called recursive autoencoders (Socher et al., 2011c). Figure 2 illustrates an application of a recursive autoencoder to a 571 binary tree. The blue and grey nodes are the original and reconstructed nodes, respectively. The autoencoder is re-used at each node of the tree. The binary tree is composed of a set of triplets in the form of (p —* c1 c2), where p is a parent vector and c1 and c2 are children vectors of p. Each child can be either an input word vector or a multi-word vector. Therefore, the tree in Figure 2 can be represented as three triplets: (y1 — x1 x2), (y2 — y1 x3), and (y3 — y2 x4). In Figure 1, we use recursive autoencoders to </context>
<context position="17208" citStr="Socher et al., 2011" startWordPosition="2843" endWordPosition="2846">ource and target languages (Section 2.2.2); 3. θreo: neural ITG reordering model parameter matrix Wo and bias term bo (Section 2.2.3). All these parameters are learned automatically from the training data. For clarity, we will use θ to denote all these parameters in the rest of the paper. For training word embedding matrix, there are two settings commonly used. In the first setting, the word embedding matrix is initialized randomly. This works well in a supervised scenario, in which a neural network updates the matrix in order to optimize some task-specific objectives (Collobert et al., 2011; Socher et al., 2011c). In the second setting, the word embedding matrix is pre-trained using an unsupervised neural language model (Bengio et al., 2003; Collobert and Weston, 2008) with huge amount of unlabeled data. In this work, we prefer to the first setting because the word embedding matrices can be trained to minimize errors with respect to reordering modeling. There are two kinds of errors involved 1. reconstruction error: how well the learned vector space representations represent the corresponding strings? 2. reordering error: how well the classifier predicts the merging order? As described in Section 2.</context>
<context position="18887" citStr="Socher et al. (2011" startWordPosition="3129" endWordPosition="3132">h minimal reconstruction error, we follow Socher et al. (2011c) to use a greedy algorithm. Taking Figure 2 as an example, the greedy algorithm begins with computing the reconstruction error Erec(�) for each pair of consecutive vectors, i.e., Erec([x1; x2]; θ), Erec([x2; x3]; θ) and Erec([x3; x4]; θ). Suppose Erec([x1; x2]; θ) is the smallest, the algorithm will replace x1 and x2 with their vector representation y1 produced by the recursive autoencoder. Then, the algorithm evaluates Erec([y1; x3]; θ) and Erec([x3; x4]; θ) and repeats the above replacing steps until only one vector 572 remains. Socher et al. (2011c) find that the greedy algorithm runs fast without significant loss in performance as compared with CKY-style algorithms. Given a training example set S = {ti = (oi, X1i , X2i )}, the average reconstruction error on the source side on the training set is defined as 1E E Erec,s(S; θ) = Erec([p.c1,p.c2]; θ) Ns i p∈TOR(ti,s) (12) where Tθ R(ti, s) denotes all the intermediate nodes on the source side in binary trees, Ns is the number of these intermediate nodes, and p.ck is the kth child vector of p. The average reconstruction error on the target side, denoted by Erec,t(S; θ), can be computed in</context>
<context position="20293" citStr="Socher et al. (2011" startWordPosition="3390" endWordPosition="3393">distribution dti for its label is [1, 0] when oi = straight, and [0, 1] when oi = inverted. Then the cross-entropy error is Ec(ti; θ) = − E dti(o)log (Pθ(o|X1,X2)) (14) o where o E {straight, inverted}. As a result, the reordering error is defined as Ereo(S; θ) = |1 |E Ec(ti; θ). (15) i Therefore, the joint training objective function is J = αErec(S; θ)+(1−α)Ereo(S; θ)+R(θ) (16) where α is a parameter used to balance the preference between reconstruction error and reordering error, R(θ) is the regularizer and defined as 2 λL 2 λrec 2 λreo 2 R(θ) = 2 11θL 11 + 2 Ilθrec11 + 2 Ilθreo11 . (17) As Socher et al. (2011c) stated, a naive way for lowering the reconstruction error is to make the magnitude of the hidden layer very small, which is 2The bias terms b(1), b(2) and b� are not regularized. We do not exclude them from the equation explicitly just for clarity. not desirable. In order to prevent such behavior, we normalize all the output vectors of the hidden layers to have length 1 in the same way as (Socher et al., 2011c). Namely we set p = p ||p ||after computing p as in Eq. 7, and c01 = c� 1||ci ||,c02=c�2 ||4 ||in Eq. 8. Following Socher et al. (2011c), we use L-BFGS to estimate the parameters with</context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. 2011b. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 26th International Conference on Machine Learning (ICML), pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="4767" citStr="Socher et al., 2011" startWordPosition="714" endWordPosition="717">decide which words are representative for predicting reordering, either manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language mo</context>
<context position="10771" citStr="Socher et al., 2011" startWordPosition="1724" endWordPosition="1727">maximum entropy (MaxEnt) classifier: P(o|X1,X2) =exp(θ · h(o, X1, X2)) � �� exp(θ · h(o�, X1, X2)) (5) where h(·) is a vector of features defined on the blocks and the order, θ is a vector of feature weights. While MaxEnt is a flexible and powerful framework for including arbitrary features, feature engineering becomes a major challenge for the MaxEnt classifier. Xiong et al. (2006) find that boundary words (i.e., the first and the last words in a string) are informative for predicting reordering. Actually, Figure 2: A recursive autoencoder for multi-word strings. The example is adapted from (Socher et al., 2011c). Blue and grey nodes are original and reconstructed ones, respectively. it is hard to decide which internal words in a long composed blocks are representative and informative. Therefore, they only use boundary words as the main features. However, it seems not enough to just consider boundary words and ignore all internal words when making order predictions, especially for long sentences.1 Indeed, Xiong et al. (2008) find that the MaxEnt classifier with boundary words as features is prone to make wrong predictions for long composed blocks. As a result, they have to impose a hard constraint t</context>
<context position="14814" citStr="Socher et al., 2011" startWordPosition="2416" endWordPosition="2419">twork can be naturally integrated into CKY parsing. To assess how well the learned vector p represents its children, we can reconstruct the children in a reconstruction layer: [c01; c02] = f(2)(W (2)p + b(2)) (8) where c01 and c02 are the reconstructed children, W(2) is a parameter matrix for reconstruction, b(2) is a bias term for reconstruction, and f(2) is an element-wise activation function, which is also set as tanh(·) in our experiments. Similarly, the same reconstruction neural network can be applied to each node in an ITG parse. These neural networks are called recursive autoencoders (Socher et al., 2011c). Figure 2 illustrates an application of a recursive autoencoder to a 571 binary tree. The blue and grey nodes are the original and reconstructed nodes, respectively. The autoencoder is re-used at each node of the tree. The binary tree is composed of a set of triplets in the form of (p —* c1 c2), where p is a parent vector and c1 and c2 are children vectors of p. Each child can be either an input word vector or a multi-word vector. Therefore, the tree in Figure 2 can be represented as three triplets: (y1 — x1 x2), (y2 — y1 x3), and (y3 — y2 x4). In Figure 1, we use recursive autoencoders to </context>
<context position="17208" citStr="Socher et al., 2011" startWordPosition="2843" endWordPosition="2846">ource and target languages (Section 2.2.2); 3. θreo: neural ITG reordering model parameter matrix Wo and bias term bo (Section 2.2.3). All these parameters are learned automatically from the training data. For clarity, we will use θ to denote all these parameters in the rest of the paper. For training word embedding matrix, there are two settings commonly used. In the first setting, the word embedding matrix is initialized randomly. This works well in a supervised scenario, in which a neural network updates the matrix in order to optimize some task-specific objectives (Collobert et al., 2011; Socher et al., 2011c). In the second setting, the word embedding matrix is pre-trained using an unsupervised neural language model (Bengio et al., 2003; Collobert and Weston, 2008) with huge amount of unlabeled data. In this work, we prefer to the first setting because the word embedding matrices can be trained to minimize errors with respect to reordering modeling. There are two kinds of errors involved 1. reconstruction error: how well the learned vector space representations represent the corresponding strings? 2. reordering error: how well the classifier predicts the merging order? As described in Section 2.</context>
<context position="18887" citStr="Socher et al. (2011" startWordPosition="3129" endWordPosition="3132">h minimal reconstruction error, we follow Socher et al. (2011c) to use a greedy algorithm. Taking Figure 2 as an example, the greedy algorithm begins with computing the reconstruction error Erec(�) for each pair of consecutive vectors, i.e., Erec([x1; x2]; θ), Erec([x2; x3]; θ) and Erec([x3; x4]; θ). Suppose Erec([x1; x2]; θ) is the smallest, the algorithm will replace x1 and x2 with their vector representation y1 produced by the recursive autoencoder. Then, the algorithm evaluates Erec([y1; x3]; θ) and Erec([x3; x4]; θ) and repeats the above replacing steps until only one vector 572 remains. Socher et al. (2011c) find that the greedy algorithm runs fast without significant loss in performance as compared with CKY-style algorithms. Given a training example set S = {ti = (oi, X1i , X2i )}, the average reconstruction error on the source side on the training set is defined as 1E E Erec,s(S; θ) = Erec([p.c1,p.c2]; θ) Ns i p∈TOR(ti,s) (12) where Tθ R(ti, s) denotes all the intermediate nodes on the source side in binary trees, Ns is the number of these intermediate nodes, and p.ck is the kth child vector of p. The average reconstruction error on the target side, denoted by Erec,t(S; θ), can be computed in</context>
<context position="20293" citStr="Socher et al. (2011" startWordPosition="3390" endWordPosition="3393">distribution dti for its label is [1, 0] when oi = straight, and [0, 1] when oi = inverted. Then the cross-entropy error is Ec(ti; θ) = − E dti(o)log (Pθ(o|X1,X2)) (14) o where o E {straight, inverted}. As a result, the reordering error is defined as Ereo(S; θ) = |1 |E Ec(ti; θ). (15) i Therefore, the joint training objective function is J = αErec(S; θ)+(1−α)Ereo(S; θ)+R(θ) (16) where α is a parameter used to balance the preference between reconstruction error and reordering error, R(θ) is the regularizer and defined as 2 λL 2 λrec 2 λreo 2 R(θ) = 2 11θL 11 + 2 Ilθrec11 + 2 Ilθreo11 . (17) As Socher et al. (2011c) stated, a naive way for lowering the reconstruction error is to make the magnitude of the hidden layer very small, which is 2The bias terms b(1), b(2) and b� are not regularized. We do not exclude them from the equation explicitly just for clarity. not desirable. In order to prevent such behavior, we normalize all the output vectors of the hidden layers to have length 1 in the same way as (Socher et al., 2011c). Namely we set p = p ||p ||after computing p as in Eq. 7, and c01 = c� 1||ci ||,c02=c�2 ||4 ||in Eq. 8. Following Socher et al. (2011c), we use L-BFGS to estimate the parameters with</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011c. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 151–161, Edinburgh, Scotland, UK., July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="4833" citStr="Socher et al., 2012" startWordPosition="726" endWordPosition="729">ither manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language modeling point of view. In this work, we propose an ITG reordering c</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201– 1211, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Ng Andrew Y</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>455--465</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="4895" citStr="Socher et al., 2013" startWordPosition="738" endWordPosition="741">. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manual feature engineering and learn semantic representations from the data? Fortunately, the rapid development of intersecting deep learning with natural language processing (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Glorot et al., 2011; Bordes et al., 2011; Socher et al., 2011a; Socher et al., 2011b; Socher et al., 2011c; Socher et al., 2012; Bordes et al., 2012; Huang et al., 2012; Socher et al., 2013; Hermann and Blunsom, 2013) brings hope for alleviating this problem. In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input. More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders (Socher et al., 2011c), which opens the door to leveraging semantic representations of phrases in reordering models from a neural language modeling point of view. In this work, we propose an ITG reordering classifier based on recursive autoencoders. The neural network </context>
<context position="29619" citStr="Socher et al. (2013)" startWordPosition="4933" endWordPosition="4936">ons for arbitrary multi-word strings in decoding, our neural ITG system achieves an absolute improvement of 1.07 BLEU points over the baseline on the NIST 2008 Chinese-English dataset. There are a number of interesting directions we would like to pursue in the near future. First, replacing the MaxEnt classifier with a neural one redefines the conditions for risk-free hypothesis recombination. We find that the number of hypotheses that can be recombined reduces in our system. Therefore, we plan to use forest reranking (Huang, 2008) to alleviate this problem. Second, it is interesting to follow Socher et al. (2013) to combine linguistically-motivated labels with recursive neural networks. Another problem with our system is that the decoding speed is much slower than the baseline system because of the computational overhead introduced by RAEs. It is necessary to investigate more efficient decoding algorithms. Finally, it is possible to apply our method to other phrase-based and even syntax-based systems. Acknowledgments This research is supported by the 863 Program under the grant No. 2012AA011102, by the Boeing Tsinghua Joint Research Project on Language Processing (Agreement TBRC-008-SDB-2011 Phase 3 5</context>
</contexts>
<marker>Socher, Bauer, Manning, Y, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y. 2013. Parsing with compositional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 455–465, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="21384" citStr="Stolcke, 2002" startWordPosition="3579" endWordPosition="3580">nd c01 = c� 1||ci ||,c02=c�2 ||4 ||in Eq. 8. Following Socher et al. (2011c), we use L-BFGS to estimate the parameters with respect to the joint training objective. Given a set of parameters, we construct binary trees for all the phrases using the greedy algorithm. The derivatives for these fixed binary trees can be computed via backpropagation through structures (Goller and Kuchler, 1996). 4 Experiments 4.1 Data Preparation We evaluated our system on Chinese-English translation. The training corpus contains 1.23M sentence pairs with 32.1M Chinese words and 35.4M English words. We used SRILM (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus, which contains 398.6M words. We used the NIST 2006 MT Chinese-English dataset as the development set and NIST 2008 dataset as the test set. The evaluation metric is case-insensitive BLEU. Because of the expensive computational cost for training our neural ITG reordering model, only the reordering examples extracted from about 1/5 of the entire parallel training corpus were used to train our neural ITG reordering model. For the neural ITG reordering model, we set the dimension of the word embedding vectors to 25 emp</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of International Conference on Spoken Language Processing, vol. 2, pages 901–904, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillman</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004: Short Papers,</booktitle>
<pages>101--104</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="2140" citStr="Tillman, 2004" startWordPosition="303" endWordPosition="304">tions or omissions. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, especially for resource-scarce languages without highaccuracy parsers. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Among them, reordering models based on inversion transduction grammar (ITG) (Wu, 1997) are one of the important ongoing research directions. As a formalism for bilingual modeling of sentence pairs, ITG is particularly well suited to predicting ordering shifts between languages. As a result, a number of authors have incorporated ITG into leftto-right decoding to constrain the reordering space and r</context>
</contexts>
<marker>Tillman, 2004</marker>
<rawString>Christoph Tillman. 2004. A unigram orientation model for statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004: Short Papers, pages 101–104, Boston, Massachusetts, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="2426" citStr="Wu, 1997" startWordPosition="351" endWordPosition="352">thout highaccuracy parsers. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Among them, reordering models based on inversion transduction grammar (ITG) (Wu, 1997) are one of the important ongoing research directions. As a formalism for bilingual modeling of sentence pairs, ITG is particularly well suited to predicting ordering shifts between languages. As a result, a number of authors have incorporated ITG into leftto-right decoding to constrain the reordering space and reported significant improvements (e.g., Zens et al., 2004; Feng et al., 2010). Along another line, Xiong et al. (2006) propose a maximum entropy (MaxEnt) reordering model based on ITG. They use the CKY algorithm to recursively merge two blocks (i.e., a pair of source and target strings</context>
<context position="6297" citStr="Wu, 1997" startWordPosition="951" endWordPosition="952">hich are trained on reordering examples extracted from word-aligned bilingual corpus, are capable of producing vector space representations for arbitrary multi-word strings in decoding. Therefore, our model takes the whole phrases rather than only boundary words into consideration when predicting phrase permutations. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 in terms of case-insensitive BLEU score. 2 Recursive Autoencoders for ITG-based Translation 2.1 Inversion Transduction Grammar Inversion transduction grammar (ITG) (Wu, 1997) is a formalism for synchronous parsing of bilingual sentence pairs. Xiong et al. (2006) apply bracketing transduction grammar (BTG), which is a simplified version of ITG, to phrase-based translation using the following production rules: X → [X1, X2] X → hX1, X2i X → f/e where X is a block that consists of a pair of source and target strings, f is a source phrase, and e is a target phrase. X1 and X2 are two neighboring blocks of which the two source phrases are adjacent. While rule (1) merges two target phrases in a straight order, rule (2) merges in an inverted order. Besides these two reorde</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>521--528</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="2210" citStr="Xiong et al., 2006" startWordPosition="313" endWordPosition="316">essly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, especially for resource-scarce languages without highaccuracy parsers. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Among them, reordering models based on inversion transduction grammar (ITG) (Wu, 1997) are one of the important ongoing research directions. As a formalism for bilingual modeling of sentence pairs, ITG is particularly well suited to predicting ordering shifts between languages. As a result, a number of authors have incorporated ITG into leftto-right decoding to constrain the reordering space and reported significant improvements (e.g., Zens et al., 2004; Feng et al.</context>
<context position="3767" citStr="Xiong et al., 2006" startWordPosition="555" endWordPosition="558">n et al., 2007; Galley and Manning, 2008) that are defined on individual bilingual phrases, the MaxEnt ITG reordering model is a two-category classifier (i.e., straight or inverted) for two arbitrary bilingual phrases of which the source phrases are adjacent. This potentially alleviates the data sparseness 567 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567–577, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics problem since there are usually a large number of reordering training examples available (Xiong et al., 2006). As a result, the MaxEnt ITG model and its extensions (Xiong et al., 2008; Xiong et al., 2010) have achieved competing performance as compared with state-of-the-art phrase-based systems. Despite these successful efforts, the ITG reordering classifiers still face a major challenge: how to extract features from training examples (i.e., a pair of bilingual strings). It is hard to decide which words are representative for predicting reordering, either manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last wo</context>
<context position="6385" citStr="Xiong et al. (2006)" startWordPosition="963" endWordPosition="966">corpus, are capable of producing vector space representations for arbitrary multi-word strings in decoding. Therefore, our model takes the whole phrases rather than only boundary words into consideration when predicting phrase permutations. Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 in terms of case-insensitive BLEU score. 2 Recursive Autoencoders for ITG-based Translation 2.1 Inversion Transduction Grammar Inversion transduction grammar (ITG) (Wu, 1997) is a formalism for synchronous parsing of bilingual sentence pairs. Xiong et al. (2006) apply bracketing transduction grammar (BTG), which is a simplified version of ITG, to phrase-based translation using the following production rules: X → [X1, X2] X → hX1, X2i X → f/e where X is a block that consists of a pair of source and target strings, f is a source phrase, and e is a target phrase. X1 and X2 are two neighboring blocks of which the two source phrases are adjacent. While rule (1) merges two target phrases in a straight order, rule (2) merges in an inverted order. Besides these two reordering rules, rule (3) is a lexical rule that translates a source phrase f into a target p</context>
<context position="9876" citStr="Xiong et al. (2006)" startWordPosition="1570" endWordPosition="1573">3,8,5,8 in a straight order, which is further merged with an atomic block X8,10,3,5 into another composed block X3,10,3,8 in an inverted order. This process recursively proceeds until the entire sentence pair is generated. The major challenge of applying ITG to machine translation is to decide when to merge two blocks in a straight order and when in an inverted order. Therefore, the ITG reordering model can be seen as a two-category classifier P(o|X1,X2), where o E {straight, inverted}. A naive way is to assign fixed probabilities to two reordering rules, which is referred to as flat model by Xiong et al. (2006): � p o = straight P(o|X1, X2) = 1 − p o = inverted (4) The drawback of the flat model is ignoring the actual blocks being merged. Intuitively, different blocks should have different preferences between the two orders. To alleviate this problem, Xiong et al. (2006) propose a maximum entropy (MaxEnt) classifier: P(o|X1,X2) =exp(θ · h(o, X1, X2)) � �� exp(θ · h(o�, X1, X2)) (5) where h(·) is a vector of features defined on the blocks and the order, θ is a vector of feature weights. While MaxEnt is a flexible and powerful framework for including arbitrary features, feature engineering becomes a m</context>
<context position="23447" citStr="Xiong et al., 2006" startWordPosition="3934" endWordPosition="3937">ent” denotes the baseline maximum entropy system and “neural” denotes our recursive autoencoder system. length &gt; &lt; [1,10] 43 121 57 [11,20] 181 67 164 [21,30] 170 11 152 [31,40] 105 3 90 [41,50] 69 1 53 [51,119] 40 0 30 Table 2: Number of sentences that our system has a higher (&gt;), equal (=) or lower (&lt;) sentence-level BLEU4 score on the NIST 2008 dataset. to 0.3, and AL, Arec, Areo exponentially from 10−8 to 10−2. We use the following hyper-parameters in our experiments: α = 0.11764, AL = 7.59 x 10−5, Arec = 1.30 x 10−5 and Areo = 3.80 x 10−4. 3 The baseline system is a re-implementation of (Xiong et al., 2006). Our system is different from the baseline by replacing the MaxEnt reordering model with a neural model. Both the systems have the same pruning settings: the threshold pruning parameter is set to 0.5 and the histogram pruning parameter to 40. For minimum-error-rate training, both systems generate 200-best lists. 4.2 MT Evaluation Table 1 shows the case-insensitive BLEU-4 scores of the baseline system and our system on the development and test sets. Our system outperforms the baseline system by 1.21 BLEU points on the development set and 1.07 on the test set. Both the differences are statistic</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 521–528, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Haitao Mi</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Refinements in BTG-based statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-I,</booktitle>
<pages>505--512</pages>
<contexts>
<context position="3841" citStr="Xiong et al., 2008" startWordPosition="569" endWordPosition="572">lingual phrases, the MaxEnt ITG reordering model is a two-category classifier (i.e., straight or inverted) for two arbitrary bilingual phrases of which the source phrases are adjacent. This potentially alleviates the data sparseness 567 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567–577, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics problem since there are usually a large number of reordering training examples available (Xiong et al., 2006). As a result, the MaxEnt ITG model and its extensions (Xiong et al., 2008; Xiong et al., 2010) have achieved competing performance as compared with state-of-the-art phrase-based systems. Despite these successful efforts, the ITG reordering classifiers still face a major challenge: how to extract features from training examples (i.e., a pair of bilingual strings). It is hard to decide which words are representative for predicting reordering, either manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it po</context>
<context position="11193" citStr="Xiong et al. (2008)" startWordPosition="1792" endWordPosition="1795">first and the last words in a string) are informative for predicting reordering. Actually, Figure 2: A recursive autoencoder for multi-word strings. The example is adapted from (Socher et al., 2011c). Blue and grey nodes are original and reconstructed ones, respectively. it is hard to decide which internal words in a long composed blocks are representative and informative. Therefore, they only use boundary words as the main features. However, it seems not enough to just consider boundary words and ignore all internal words when making order predictions, especially for long sentences.1 Indeed, Xiong et al. (2008) find that the MaxEnt classifier with boundary words as features is prone to make wrong predictions for long composed blocks. As a result, they have to impose a hard constraint to always prefer merging long composed blocks in a monotonic way. Therefore, it is important to consider more than boundary words to make more accurate reordering predictions. We need a new mechanism to achieve this goal. 2.2 Recursive Autoencoders 2.2.1 Vector Space Representations for Words In neural networks, a natural language word is represented as a real-valued vector (Bengio et al., 2003; Collobert and Weston, 20</context>
<context position="25569" citStr="Xiong et al. (2008)" startWordPosition="4281" endWordPosition="4284">ering training data size on BLEU scores. The BLEU scores rise with the increase of training data size. Due to the computational cost, we only used 1/5 of the entire bilingual corpus to train our neural reordering model. sentences. Figure 4 compares classification accuracies of the neural and MaxEnt classifiers. “Length” denotes the sum of the lengths of two source phrases in a reordering example. For each length, we randomly select 200 unseen reordering examples to calculate the classification accuracy. Our classifier outperforms the baseline consistently, especially for long composed blocks. Xiong et al. (2008) find that the performance of the baseline system can be improved by forbidding inverted reordering if the phrase length exceeds a pre-defined distortion limit. This heuristic increases the BLEU score of the baseline system significantly to 24.46 but is still significantly worse (p &lt; 0.05) than our system without the heuristic. We find that imposing this heuristic fails to improve our system System maxent neural NIST 2006 (tune) NIST 2008 30.40 23.75 31.61* 24.82* # of examples NIST 2006 (tune) NIST 2008 100,000 200,000 400,000 800,000 30.88 23.78 30.75 23.89 30.80 24.35 31.01 24.45 Accuracy (</context>
</contexts>
<marker>Xiong, Zhang, Aw, Mi, Liu, Lin, 2008</marker>
<rawString>Deyi Xiong, Min Zhang, Aiti Aw, Haitao Mi, Qun Liu, and Shouxun Lin. 2008. Refinements in BTG-based statistical machine translation. In Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-I, pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
</authors>
<title>Linguistically annotated reordering: Evaluation and analysis.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="3862" citStr="Xiong et al., 2010" startWordPosition="573" endWordPosition="576"> MaxEnt ITG reordering model is a two-category classifier (i.e., straight or inverted) for two arbitrary bilingual phrases of which the source phrases are adjacent. This potentially alleviates the data sparseness 567 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567–577, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics problem since there are usually a large number of reordering training examples available (Xiong et al., 2006). As a result, the MaxEnt ITG model and its extensions (Xiong et al., 2008; Xiong et al., 2010) have achieved competing performance as compared with state-of-the-art phrase-based systems. Despite these successful efforts, the ITG reordering classifiers still face a major challenge: how to extract features from training examples (i.e., a pair of bilingual strings). It is hard to decide which words are representative for predicting reordering, either manually or automatically, especially for long sentences. As a result, Xiong et al. (2006) only use boundary words (i.e., the first and the last words in a string) to predict the ordering. What if we look inside? Is it possible to avoid manua</context>
</contexts>
<marker>Xiong, Zhang, Aw, Li, 2010</marker>
<rawString>Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2010. Linguistically annotated reordering: Evaluation and analysis. Computational Linguistics, 36(3):535–568, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Reordering constraints for phrase-based statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>205--211</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="2159" citStr="Zens et al., 2004" startWordPosition="305" endWordPosition="308">ons. Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding. Finally, phrase-based systems can be applicable to most domains and languages, especially for resource-scarce languages without highaccuracy parsers. However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete (Knight, 1999). Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g., Och et al., 2004; Tillman, 2004; Zens et al., 2004; Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Koehn et al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012; Cherry, 2013). Among them, reordering models based on inversion transduction grammar (ITG) (Wu, 1997) are one of the important ongoing research directions. As a formalism for bilingual modeling of sentence pairs, ITG is particularly well suited to predicting ordering shifts between languages. As a result, a number of authors have incorporated ITG into leftto-right decoding to constrain the reordering space and reported significant</context>
</contexts>
<marker>Zens, Ney, Watanabe, Sumita, 2004</marker>
<rawString>Richard Zens, Hermann Ney, Taro Watanabe, and Eiichiro Sumita. 2004. Reordering constraints for phrase-based statistical machine translation. In Proceedings of the 20th International Conference on Computational Linguistics, pages 205–211, Geneva, Switzerland, Aug 23–Aug 27.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
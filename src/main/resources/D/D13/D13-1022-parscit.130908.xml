<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.948315">
Optimal Beam Search for Machine Translation
</title>
<author confidence="0.669392">
Alexander M. Rush Yin-Wen Chang
</author>
<note confidence="0.598161">
MIT CSAIL,
Cambridge, MA 02139, USA
</note>
<email confidence="0.996075">
{srush, yinwen}@csail.mit.edu
</email>
<author confidence="0.983211">
Michael Collins
</author>
<affiliation confidence="0.900153">
Department of Computer Science,
Columbia University,
</affiliation>
<address confidence="0.988151">
New York, NY 10027, USA
</address>
<email confidence="0.999354">
mcollins@cs.columbia.edu
</email>
<sectionHeader confidence="0.99666" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999165">
Beam search is a fast and empirically effective
method for translation decoding, but it lacks
formal guarantees about search error. We de-
velop a new decoding algorithm that combines
the speed of beam search with the optimal cer-
tificate property of Lagrangian relaxation, and
apply it to phrase- and syntax-based transla-
tion decoding. The new method is efficient,
utilizes standard MT algorithms, and returns
an exact solution on the majority of transla-
tion examples in our test data. The algorithm
is 3.5 times faster than an optimized incremen-
tal constraint-based decoder for phrase-based
translation and 4 times faster for syntax-based
translation.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999621">
Beam search (Koehn et al., 2003) and cube prun-
ing (Chiang, 2007) have become the de facto decod-
ing algorithms for phrase- and syntax-based trans-
lation. The algorithms are central to large-scale
machine translation systems due to their efficiency
and tendency to produce high-quality translations
(Koehn, 2004; Koehn et al., 2007; Dyer et al., 2010).
However despite practical effectiveness, neither al-
gorithm provides any bound on possible decoding
error.
In this work we present a variant of beam search
decoding for phrase- and syntax-based translation.
The motivation is to exploit the effectiveness and ef-
ficiency of beam search, but still maintain formal
guarantees. The algorithm has the following bene-
fits:
</bodyText>
<listItem confidence="0.998922">
• In theory, it can provide a certificate of optimal-
ity; in practice, we show that it produces opti-
mal hypotheses, with certificates of optimality,
on the vast majority of examples.
• It utilizes well-studied algorithms and extends
off-the-shelf beam search decoders.
• Empirically it is very fast, results show that it is
3.5 times faster than an optimized incremental
constraint-based solver.
</listItem>
<bodyText confidence="0.999841130434783">
While our focus is on fast decoding for machine
translation, the algorithm we present can be applied
to a variety of dynamic programming-based decod-
ing problems. The method only relies on having a
constrained beam search algorithm and a fast uncon-
strained search algorithm. Similar algorithms exist
for many NLP tasks.
We begin in Section 2 by describing constrained
hypergraph search and showing how it generalizes
translation decoding. Section 3 introduces a variant
of beam search that is, in theory, able to produce
a certificate of optimality. Section 4 shows how to
improve the effectiveness of beam search by using
weights derived from Lagrangian relaxation. Sec-
tion 5 puts everything together to derive a fast beam
search algorithm that is often optimal in practice.
Experiments compare the new algorithm with
several variants of beam search, cube pruning, A*
search, and relaxation-based decoders on two trans-
lation tasks. The optimal beam search algorithm is
able to find exact solutions with certificates of opti-
mality on 99% of translation examples, significantly
more than other baselines. Additionally the optimal
</bodyText>
<page confidence="0.982029">
210
</page>
<note confidence="0.752058">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 210–221,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.8918755">
beam search algorithm is much faster than other ex-
act methods.
</bodyText>
<sectionHeader confidence="0.972527" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999969888888889">
The focus of this work is decoding for statistical ma-
chine translation. Given a source sentence, the goal
is to find the target sentence that maximizes a com-
bination of translation model and language model
scores. In order to analyze this decoding problem,
we first abstract away from the specifics of transla-
tion into a general form, known as a hypergraph. In
this section, we describe the hypergraph formalism
and its relation to machine translation.
</bodyText>
<subsectionHeader confidence="0.571264">
2.1 Notation
</subsectionHeader>
<bodyText confidence="0.998681333333333">
Throughout the paper, scalars and vectors are writ-
ten in lowercase, matrices are written in uppercase,
and sets are written in script-case, e.g. X. All vec-
tors are assumed to be column vectors. The function
δ(j) yields an indicator vector with δ(j)j = 1 and
δ(j)i = 0 for all i =6 j.
</bodyText>
<subsectionHeader confidence="0.996097">
2.2 Hypergraphs and Search
</subsectionHeader>
<bodyText confidence="0.997467083333333">
A directed hypergraph is a pair (V, E) where V =
{1 ... |V|} is a set of vertices, and E is a set of di-
rected hyperedges. Each hyperedge e ∈ E is a tuple
�hv2,...,v|v|i,v1) where vi ∈ V for i ∈ {1... |v|}.
The head of the hyperedge is h(e) = v1. The tail
of the hyperedge is the ordered sequence t(e) =
hv2, ... , v|v|i. The size of the tail |t(e) |may vary
across different hyperedges, but |t(e) |≥ 1 for all
edges and is bounded by a constant. A directed
graph is a directed hypergraph with |t(e) |= 1 for
all edges e ∈ E.
Each vertex v ∈ V is either a non-terminal or a
terminal in the hypergraph. The set of non-terminals
is N = {v ∈ V : h(e) = v for some e ∈ E}. Con-
versely, the set of terminals is defined as T = V \N.
All directed hypergraphs used in this work are
acyclic: informally this implies that no hyperpath (as
defined below) contains the same vertex more than
once (see Martin et al. (1990) for a full definition).
Acyclicity implies a partial topological ordering of
the vertices. We also assume there is a distinguished
root vertex 1 where for all e ∈ E, 1 ∈6 t(e).
Next we define a hyperpath as x ∈ {0,1}|£ |where
x(e) = 1 if hyperedge e is used in the hyperpath,
</bodyText>
<equation confidence="0.84351175">
procedure BESTPATHSCORE(θ, τ)
π[v] ← 0 for all v ∈ T
for e ∈ E in topological order do
hhv2, ... ,v|v|i,v1i ← e
</equation>
<figureCaption confidence="0.98665625">
Figure 1: Dynamic programming algorithm for uncon-
strained hypergraph search. Note that this version only
returns the highest score: maxxEX θTx+τ. The optimal
hyperpath can be found by including back-pointers.
</figureCaption>
<bodyText confidence="0.995903">
x(e) = 0 otherwise. The set of valid hyperpaths is
defined as
The first problem we consider is unconstrained hy-
pergraph search. Let θ ∈ R|£ |be the weight vector
for the hypergraph and let τ ∈ R be a weight offset.1
The unconstrained search problem is to find
</bodyText>
<equation confidence="0.9896345">
θ(e)x(e) + τ = max
xEX
</equation>
<bodyText confidence="0.999828333333333">
This maximization can be computed for any
weights and directed acyclic hypergraph in time
O(|E|) using dynamic programming. Figure 1
shows this algorithm which is simply a version of
the CKY algorithm.
Next consider a variant of this problem: con-
strained hypergraph search. Constraints will be nec-
essary for both phrase- and syntax-based decoding.
In phrase-based models, the constraints will ensure
that each source word is translated exactly once. In
syntax-based models, the constraints will be used to
intersect a translation forest with a language model.
In the constrained hypergraph problem, hyper-
paths must fulfill additional linear hyperedge con-
straints. Define the set of constrained hyperpaths as
</bodyText>
<equation confidence="0.757186">
X&apos; = {x ∈ X : Ax = b}
</equation>
<footnote confidence="0.9977545">
1The purpose of the offset will be clear in later sections. For
this section, the value of T can be taken as 0.
</footnote>
<equation confidence="0.996377421052632">
E
e:h(e)=v
Ex(e) =
e:vEt(e)
x(e) ∀ v ∈ N \ {1} }
x : E x(e) = 1,
eEE:h(e)=1
�
���� �
�����
X=
s ← θ(e) + |v |π[vi]
E
i=2
if s &gt; π[v1] then π[v1] ← s
return π[1] + τ
Emax
xEX eE£
θTx + τ
</equation>
<page confidence="0.994168">
211
</page>
<bodyText confidence="0.9175805">
where we have a constraint matrix A E RIA X61
1£1
and vector b E R|b |encoding |b |constraints.
The optimal constrained hyperpath is x* =
arg ma xEX&apos; θTx + τ.
Note that the constrained hypergraph search prob-
lem may be NP-Hard. Crucially this is true even
when the corresponding unconstrained search prob-
lem is solvable in polynomial time. For instance,
phrase-based decoding is known to be NP-Hard
(Knight, 1999), but we will see that it can be ex-
pressed as a polynomial-sized hypergraph with con-
straints.
Example: Phrase-Based Machine Translation
Consider translating a source sentence w1 ... w|w |to
a target sentence in a language with vocabulary E. A
simple phrase-based translation model consists of a
tuple (P, ω, σ) with
</bodyText>
<listItem confidence="0.998320222222222">
• P; a set of pairs (q, r) where q1 ... q|q |is a se-
quence of source-language words and r1 ... r|r|
is a sequence of target-language words drawn
from the target vocabulary E.
• ω : R|P|; parameters for the translation model
mapping each pair in P to a real-valued score.
• σ : R|ExE|; parameters of the language model
mapping a bigram of target-language words to
a real-valued score.
</listItem>
<bodyText confidence="0.999882777777778">
The translation decoding problem is to find the
best derivation for a given source sentence. A
derivation consists of a sequence of phrases p =
p1 ... pn. Define a phrase as a tuple (q, r, j, k)
consisting of a span in the source sentence q =
wi ... wk and a sequence of target words r1 ... r|r|,
with (q, r) E P. We say the source words wi ... wk
are translated to r.
The score of a derivation, f(p), is the sum of the
translation score of each phrase plus the language
model score of the target sentence
Crucially for a derivation to be valid it must sat-
isfy an additional condition: it must translate every
source word exactly once. The decoding problem
for phrase-based translation is to find the highest-
scoring derivation satisfying this property.
We can represent this decoding problem as a con-
strained hypergraph using the construction of Chang
and Collins (2011). The hypergraph weights en-
code the translation and language model scores, and
its structure ensures that the count of source words
translated is |w|, i.e. the length of the source sen-
tence. Each vertex will remember the preceding
target-language word and the count of source words
translated so far.
The hypergraph, which for this problem is also a
directed graph, takes the following form.
</bodyText>
<listItem confidence="0.8552525">
• Vertices v E V are labeled (c, u) where c E
11 ... |w|} is the count of source words trans-
lated and u E E is the last target-language word
produced by a partial hypothesis at this vertex.
Additionally there is an initial terminal vertex
labeled (0, &lt;s&gt;).
• There is a hyperedge e E £ with head (c&apos;, u&apos;)
and tail ((c, u)) if there is a valid corresponding
phrase (q, r, j, k) such that c&apos; = c + |q |and
u&apos; = r|r|, i.e. c&apos; is the count of words translated
and u&apos; is the last word of target phrase r. We
call this phrase p(e).
</listItem>
<bodyText confidence="0.999099">
The weight of this hyperedge, θ(e), is the trans-
lation model score of the pair plus its language
model score
</bodyText>
<equation confidence="0.990051333333333">
⎛ ⎞
|r|
θ(e) = ω(q, r)+Xσ(ri−1, ri) ⎠+σ(u, r1)
i=2
edges with head 1 and tail
u)) for all
</equation>
<bodyText confidence="0.707058">
u E E. The weight of these edges is the cost of
the stop bigram foll
</bodyText>
<equation confidence="0.98763">
((|w|,
owing u, i.e. σ(u, &lt;/s&gt;).
f(p) = Xn ω(q(pi), r(pi)) + |u|+1 σ(ui−1, ui)
i=1 X
i=0
</equation>
<bodyText confidence="0.998263666666667">
where u is the sequence of words in E formed
by concatenating the phrases r(p1) ... r(pn), with
boundary cases u0 = &lt;s&gt; and u|u|+1 = &lt;/s&gt;.
</bodyText>
<listItem confidence="0.98126">
• To handle the end boundary, there are hyper-
</listItem>
<bodyText confidence="0.9986386">
While any valid derivation corresponds to a hy-
perpath in this graph, a hyperpath may not corre-
spond to a valid derivation. For instance, ahyper-
path may translate some source words more than
once or not at all.
</bodyText>
<page confidence="0.997179">
212
</page>
<figureCaption confidence="0.8150925">
Figure 2: Hypergraph for translating the sentence w = les1 pauvres2 sont3 demunis4 with set of pairs P =
{(les, the), (pauvres, poor), (sont demunis, don’t have any money)}. Hyperedges are color-coded
by source words translated: orange for les1, green for pauvres2, and red for sont3 demunis4. The dotted lines
show an invalid hyperpath x that has signature Ax = h0, 0, 2, 2i =6 h1,1,1,1i .
</figureCaption>
<bodyText confidence="0.999973666666667">
We handle this problem by adding additional con-
straints. For all source words i ∈ {1 ... |w|}, define
p as the set of hyperedges that translate wi
</bodyText>
<equation confidence="0.799569">
p(i) = {e ∈ E : j(p(e)) ≤ i ≤ k(p(e))}
</equation>
<bodyText confidence="0.996821">
Next define |w |constraints enforcing that each word
in the source sentence is translated exactly once
</bodyText>
<equation confidence="0.987119">
E x(e) = 1 ∀ i ∈ {1 ... |w|}
eEp(i)
</equation>
<bodyText confidence="0.989030545454545">
These linear constraints can be represented with
a matrix A ∈ {0,1}|w|&amp;quot;|£ |where the rows corre-
spond to source indices and the columns correspond
to edges. We call the product Ax the signature,
where in this case (Ax)i is the number of times word
i has been translated. The full set of constrained hy-
perpaths is X&apos; = {x ∈ X : Ax = 1 }, and the best
derivation under this phrase-based translation model
has score maxxEX&apos; BTx + T.
Figure 2.2 shows an example hypergraph
with constraints for translating the sentence les
pauvres sont demunis into English using
a simple set of phrases. Even in this small exam-
ple, many of the possible hyperpaths violate the
constraints and correspond to invalid derivations.
Example: Syntax-Based Machine Translation
Syntax-based machine translation with a language
model can also be expressed as a constrained hyper-
graph problem. For the sake of space, we omit the
definition. See Rush and Collins (2011) for an in-
depth description of the constraint matrix used for
syntax-based translation.
</bodyText>
<sectionHeader confidence="0.987778" genericHeader="method">
3 A Variant of Beam Search
</sectionHeader>
<bodyText confidence="0.99998195">
This section describes a variant of the beam
search algorithm for finding the highest-scoring con-
strained hyperpath. The algorithm uses three main
techniques: (1) dynamic programming with ad-
ditional signature information to satisfy the con-
straints, (2) beam pruning where some, possibly op-
timal, hypotheses are discarded, and (3) branch-and-
bound-style application of upper and lower bounds
to discard provably non-optimal hypotheses.
Any solution returned by the algorithm will be a
valid constrained hyperpath and a member of X&apos;.
Additionally the algorithm returns a certificate flag
opt that, if true, indicates that no beam pruning
was used, implying the solution returned is opti-
mal. Generally it will be hard to produce a certificate
even by reducing the amount of beam pruning; how-
ever in the next section we will introduce a method
based on Lagrangian relaxation to tighten the upper
bounds. These bounds will help eliminate most so-
lutions before they trigger pruning.
</bodyText>
<subsectionHeader confidence="0.997531">
3.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.99998875">
Figure 3 shows the complete beam search algorithm.
At its core it is a dynamic programming algorithm
filling in the chart 7r. The beam search chart indexes
hypotheses by vertex v ∈ V as well as a signature
sig ∈ 8|6 |where |b |is the number of constraints. A
new hypothesis is constructed from each hyperedge
and all possible signatures of tail nodes. We define
the function SIGS to take the tail of an edge and re-
</bodyText>
<page confidence="0.994582">
213
</page>
<bodyText confidence="0.999054170731707">
turn the set of possible signature combinations
SIGS(v2, ... v|v|) = |v |ri2 {sig : π[vi, sig] =� −oo}
where the product is the Cartesian product over sets.
Line 8 loops over this entire set.2 For hypothesis x,
the algorithm ensures that its signature sig is equal
to Ax. This property is updated on line 9.
The signature provides proof that a hypothesis is
still valid. Let the function CHECK(sig) return true
if the hypothesis can still fulfill the constraints. For
example, in phrase-based decoding, we will define
CHECK(sig) = (sig &lt; 1); this ensures that each
word has been translated 0 or 1 times. This check is
applied on line 11.
Unfortunately maintaining all signatures is inef-
ficient. For example we will see that in phrase-
based decoding the signature is a bit-string recording
which source words have been translated; the num-
ber of possible bit-strings is exponential in the length
of the sentence. The algorithm includes two meth-
ods for removing hypotheses, bounding and prun-
ing.
Bounding allows us to discard provably non-
optimal solutions. The algorithm takes as arguments
a lower bound on the optimal score lb &lt; θTx* + τ,
and computes upper bounds on the outside score
for all vertices v: ubs[v], i.e. an overestimate of
the score for completing the hyperpath from v. If
a hypothesis has score s, it can only be optimal if
s + ubs[v] &gt; lb. This bound check is performed on
line 11.
Pruning removes weak partial solutions based on
problem-specific checks. The algorithm invokes the
black-box function, PRUNE, on line 13, passing it
a pruning parameter β and a vertex-signature pair.
The parameter β controls a threshold for pruning.
For instance for phrase-based translation, it specifies
a hard-limit on the number of hypotheses to retain.
The function returns true if it prunes from the chart.
Note that pruning may remove optimal hypotheses,
so we set the certificate flag opt to false if the chart
is modified.
</bodyText>
<footnote confidence="0.869834666666667">
2For simplicity we write this loop over the entire set. In
practice it is important to use data structures to optimize look-
up. See Tillmann (2006) and Huang and Chiang (2005).
</footnote>
<listItem confidence="0.9771505">
1: procedure BEAMSEARCH(θ, τ, lb, β)
2: ubs +— OUTSIDE(θ, τ)
3: opt +— true
4: π[v, sig] +— −oo for all v E V, sig E R|b|
5: π[v, 0] +— 0 for all v E T
6: for e E E in topological order do
7: ((v2,... , v|v|), v1) +— e
8: for sig(2) ... sig(|v|) E SIGS(v2, ... , v|v|) do
</listItem>
<table confidence="0.687876928571429">
�
s &gt; π[v1, sig] n
11: if �CHECK(sig) n
s + ubs[v1] &gt; lb
12: π[v1, sig] +— s
13: if PRUNE(π, v1, sig,β) then opt +— false
14: lb0 +— π[1,c] + τ
15: return lb0, opt
(V, E, θ, τ) hypergraph with weights
(A, b) matrix and vector for constraints
lb E R lower bound
β a pruning parameter
� lb0 resulting lower bound score
Output: opt certificate of optimality
</table>
<figureCaption confidence="0.9995088">
Figure 3: A variant of the beam search algorithm. Uses
dynamic programming to produce a lower bound on the
optimal constrained solution and, possibly, a certificate of
optimality. Function OUTSIDE computes upper bounds
on outside scores. Function SIGS enumerates all possi-
ble tail signatures. Function CHECK identifies signatures
that do not violate constraints. Bounds lb and ubs are
used to remove provably non-optimal solutions. Func-
tion PRUNE, taking parameter β, returns true if it prunes
hypotheses from π that could be optimal.
</figureCaption>
<bodyText confidence="0.99759575">
This variant on beam search satisfies the follow-
ing two properties (recall x* is the optimal con-
strained solution)
Property 3.1 (Primal Feasibility). The returned
score lb&apos; lower bounds the optimal constrained
score, that is lb&apos; &lt; θTx* + τ.
Property 3.2 (Dual Certificate). If beam search re-
turns with opt = true, then the returned score is
optimal, i.e. lb&apos; = θTx* + τ.
An immediate consequence of Property 3.1 is that
the output of beam search, lb&apos;, can be used as the in-
put lb for future runs of the algorithm. Furthermore,
</bodyText>
<figure confidence="0.998525">
9: sig +— Aδ(e) +
|v|
10: s +— θ(e) + L π[vi, sig(i)]
i=2
|v|
L
i=2
sig(i)
�
�then
�
� � �
Input:
</figure>
<page confidence="0.95969">
214
</page>
<equation confidence="0.7132546">
procedure PRUNE(π, v, sig, β)
C +— {(v&apos;, sig&apos;) : ||sig&apos;||1 = ||sig||1,
π[v&apos;, sig&apos;] =� −oo}
D +— C \ mBEST(β, C, π)
π[v&apos;, sig&apos;] +— −oo for all v&apos;, sig&apos; E D
if D = 0 then return true
else return false
Input:f (v, sig) the last hypothesis added to the chart
L β E Z # of hypotheses to retain
Output: true, if π is modified
</equation>
<figureCaption confidence="0.99946475">
Figure 4: Pruning function for phrase-based translation.
Set C contains all hypotheses with ||sig||1 source words
translated. The function prunes all but the top-β scoring
hypotheses in this set.
</figureCaption>
<bodyText confidence="0.99505075">
if we loosen the amount of beam pruning by adjust-
ing the pruning parameter β we can produce tighter
lower bounds and discard more hypotheses. We can
then iteratively apply this idea with a sequence of
parameters β1 ... βK producing lower bounds lb(1)
through lb(K). We return to this idea in Section 5.
Example: Phrase-based Beam Search. Recall
that the constraints for phrase-based translation con-
sist of a binary matrix A E {0,1}|w|x|e |and vec-
tor b = 1. The value sigi is therefore the num-
ber of times source word i has been translated in
the hypothesis. We define the predicate CHECK as
CHECK(sig) = (sig &lt; 1) in order to remove hy-
potheses that already translate a source word more
than once, and are therefore invalid. For this reason,
phrase-based signatures are called bit-strings.
A common beam pruning strategy is to group
together items into a set C and retain a (possibly
complete) subset. An example phrase-based beam
pruner is given in Figure 4. It groups together
hypotheses based on ||sigi||1, i.e. the number of
source words translated, and applies a hard pruning
filter that retains only the β highest-scoring items
(v, sig) E C based on π[v, sig].
</bodyText>
<subsectionHeader confidence="0.999902">
3.2 Computing Upper Bounds
</subsectionHeader>
<bodyText confidence="0.9996088">
Define the set 0(v, x) to contain all outside edges of
vertex v in hyperpath x (informally, hyperedges that
do not have v as an ancestor). For all v E V, we set
the upper bounds, ubs, to be the best unconstrained
outside score
</bodyText>
<equation confidence="0.988168666666667">
�ubs[v] = max
xEX:vEx
eEO(v,x)
</equation>
<bodyText confidence="0.999883111111111">
This upper bound can be efficiently computed for
all vertices using the standard outside dynamic pro-
gramming algorithm. We will refer to this algorithm
as OUTSIDE(θ, τ).
Unfortunately, as we will see, these upper bounds
are often quite loose. The issue is that unconstrained
outside paths are able to violate the constraints with-
out being penalized, and therefore greatly overesti-
mate the score.
</bodyText>
<sectionHeader confidence="0.9148775" genericHeader="method">
4 Finding Tighter Bounds with
Lagrangian Relaxation
</sectionHeader>
<bodyText confidence="0.999972769230769">
Beam search produces a certificate only if beam
pruning is never used. In the case of phrase-based
translation, the certificate is dependent on all groups
C having β or less hypotheses. The only way to en-
sure this is to bound out enough hypotheses to avoid
pruning. The effectiveness of the bounding inequal-
ity, s + ubs[v] &lt; lb, in removing hypotheses is di-
rectly dependent on the tightness of the bounds.
In this section we propose using Lagrangian re-
laxation to improve these bounds. We first give a
brief overview of the method and then apply it to
computing bounds. Our experiments show that this
approach is very effective at finding certificates.
</bodyText>
<subsectionHeader confidence="0.969058">
4.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.9835005625">
In Lagrangian relaxation, instead of solving the con-
strained search problem, we relax the constraints
and solve an unconstrained hypergraph problem
with modified weights. Recall the constrained hy-
pergraph problem: max
xEX:Ax=b
grangian dual of this optimization problem is
where λ E R|b |is a vector of dual variables and
define θ&apos; = θ − ATλ and τ&apos; = τ + λTb. This
maximization is over X, so for any value of λ, L(λ)
can be calculated as BestPathScore(θ&apos;, τ&apos;).
Note that for all valid constrained hyperpaths x E
X&apos; the term Ax−b equals 0, which implies that these
hyperpaths have the same score under the modified
weights as under the original weights, θTx + τ =
θ&apos;Tx+τ&apos;. This leads to the following two properties,
</bodyText>
<equation confidence="0.92627225">
θ(e) + τ
θTx + τ. The La-
L(λ) = max θTx + τ − λT(Ax − b)
XEX
Cmax(θ − AT λ)T x� + τ + λT b
XEX
= max θ&apos;Tx + τ&apos;
XEX
</equation>
<page confidence="0.767912">
215
</page>
<equation confidence="0.9539774">
procedure LRROUND(αk, A)
OTx + T − AT(Ax − b)
A&apos; ← A − αk(Ax − b)
opt ← Ax = b
ub ← OTx + T
return A&apos;, ub, opt
procedure LAGRANGIANRELAXATION(α)
A(0) ← 0
for k in 1 ... K do
A(k), ub, opt ← LRROUND(αk, A(k−1))
</equation>
<bodyText confidence="0.301972666666667">
if opt then return A(k), ub, opt
return A(K), ub, opt
Input: α1 ... αK sequence of subgradient rates
</bodyText>
<figure confidence="0.527465666666667">
⎡A final dual vector
Output: ⎣ub upper bound on optimal constrained solution
opt certificate of optimality
</figure>
<figureCaption confidence="0.994073">
Figure 5: Lagrangian relaxation algorithm. The algo-
rithm repeatedly calls LRROUND to compute the subgra-
dient, update the dual vector, and check for a certificate.
</figureCaption>
<bodyText confidence="0.93808275">
where x ∈ X is the hyperpath computed within the
max,
Property 4.1 (Dual Feasibility). The value L(A) up-
per bounds the optimal solution, that is L(A) ≥
</bodyText>
<equation confidence="0.964612">
OTx* + T
Property 4.2 (Primal Certificate). If the hyperpath
x is a member of X&apos;, i.e. Ax = b, then L(A) =
OTx* + T.
</equation>
<bodyText confidence="0.985635652173913">
Property 4.1 states that L(A) always produces
some upper bound; however, to help beam search,
we want as tight a bound as possible: mina L(A).
The Lagrangian relaxation algorithm, shown in
Figure 5, uses subgradient descent to find this min-
imum. The subgradient of L(A) is Ax − b where
x is the argmax of the modified objective x =
arg maxxEX O&apos;Tx + T&apos;. Subgradient descent itera-
tively solves unconstrained hypergraph search prob-
lems to compute these subgradients and updates A.
See Rush and Collins (2012) for an extensive discus-
sion of this style of optimization in natural language
processing.
Example: Phrase-based Relaxation. For phrase-
based translation, we expand out the Lagrangian to
The weight of each edge O(e) is modified by the
dual variables Ai for each source word translated by
the edge, i.e. if (q, r, j, k) = p(e), then the score
is modified by Pke Ai. A solution under these
weights may use source words multiple times or not
at all. However if the solution uses each source word
exactly once (Ax = 1), then we have a certificate
and the solution is optimal.
</bodyText>
<subsectionHeader confidence="0.999757">
4.2 Utilizing Upper Bounds in Beam Search
</subsectionHeader>
<bodyText confidence="0.999192125">
For many problems, it may not be possible to satisfy
Property 4.2 by running the subgradient algorithm
alone. Yet even for these problems, applying sub-
gradient descent will produce an improved estimate
of the upper bound, mina L(A).
To utilize these improved bounds, we simply re-
place the weights in beam search and the outside al-
gorithm with the modified weights from Lagrangian
relaxation, O&apos; and T&apos;. Since the result of beam search
must be a valid constrained hyperpath x ∈ X&apos;, and
for all x ∈ X&apos;, OTx + T = O&apos;Tx + T&apos;, this sub-
stitution does not alter the necessary properties of
the algorithm; i.e. if the algorithm returns with opt
equal to true, then the solution is optimal.
Additionally the computation of upper bounds
now becomes
</bodyText>
<equation confidence="0.996809">
Xubs[v] = max
xEX:vEx
eEO(v,x)
</equation>
<bodyText confidence="0.999881666666667">
These outside paths may still violate constraints, but
the modified weights now include penalty terms to
discourage common violations.
</bodyText>
<sectionHeader confidence="0.985903" genericHeader="method">
5 Optimal Beam Search
</sectionHeader>
<bodyText confidence="0.999979923076923">
The optimality of the beam search algorithm is de-
pendent on the tightness of the upper and lower
bounds. We can produce better lower bounds by
varying the pruning parameter 0; we can produce
better upper bounds by running Lagrangian relax-
ation. In this section we combine these two ideas
and present a complete optimal beam search algo-
rithm.
Our general strategy will be to use Lagrangian
relaxation to compute modified weights and to use
beam search over these modified weights to attempt
to find an optimal solution. One simple method for
doing this, shown at the top of Figure 6, is to run
</bodyText>
<equation confidence="0.512918444444444">
x ← arg max
xEX
L(A) = max OTx + T − AT(Ax − b) =
xEX
X ⎛ k(p(e)) ⎞Ai ⎠x(e) + T + X ��� Ai
max ⎝O(e) − X i=1
xEX i=Xp(e))
eEE
O&apos;(e) + T&apos;
</equation>
<page confidence="0.997708">
216
</page>
<bodyText confidence="0.999957166666667">
in stages. The algorithm first runs Lagrangian relax-
ation to compute the best A vector. The algorithm
then iteratively runs beam search using the parame-
ter sequence 3k. These parameters allow the algo-
rithm to loosen the amount of beam pruning. For
example in phrase based pruning, we would raise
the number of hypotheses stored per group until no
beam pruning occurs.
A clear disadvantage of the staged approach is
that it needs to wait until Lagrangian relaxation is
completed before even running beam search. Of-
ten beam search will be able to quickly find an opti-
mal solution even with good but non-optimal A. In
other cases, beam search may still improve the lower
bound lb.
This motivates the alternating algorithm OPT-
BEAM shown Figure 6. In each round, the algo-
rithm alternates between computing subgradients to
tighten ubs and running beam search to maximize
lb. In early rounds we set 3 for aggressive beam
pruning, and as the upper bounds get tighter, we
loosen pruning to try to get a certificate. If at any
point either a primal or dual certificate is found, the
algorithm returns the optimal solution.
</bodyText>
<sectionHeader confidence="0.999842" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.984685416666667">
Approximate methods based on beam search and
cube-pruning have been widely studied for phrase-
based (Koehn et al., 2003; Tillmann and Ney, 2003;
Tillmann, 2006) and syntax-based translation mod-
els (Chiang, 2007; Huang and Chiang, 2007; Watan-
abe et al., 2006; Huang and Mi, 2010).
There is a line of work proposing exact algorithms
for machine translation decoding. Exact decoders
are often slow in practice, but help quantify the er-
rors made by other methods. Exact algorithms pro-
posed for IBM model 4 include ILP (Germann et al.,
2001), cutting plane (Riedel and Clarke, 2009), and
multi-pass A* search (Och et al., 2001). Zaslavskiy
et al. (2009) formulate phrase-based decoding as a
traveling salesman problem (TSP) and use a TSP
decoder. Exact decoding algorithms based on finite
state transducers (FST) (Iglesias et al., 2009) have
been studied on phrase-based models with limited
reordering (Kumar and Byrne, 2005). Exact decod-
ing based on FST is also feasible for certain hier-
archical grammars (de Gispert et al., 2010). Chang
procedure OPTBEAMSTAGED(α, 3)
A, ub, opt ←LAGRANGIANRELAXATION(α)
if opt then return ub
</bodyText>
<equation confidence="0.9283395">
0&apos; ← 0 − ATA
T&apos; ← T + ATb
lb(0) ← −∞
for k in 1 ... K do
lb(k), opt ← BEAMSEARCH(0&apos;, T&apos;, lb(k−1), 3k)
if opt then return lb(k)
return maxkE{1...K} lb(k)
procedure OPTBEAM(α, 3)
A(0) ← 0
lb(0) ← −∞
for k in 1 ... K do
A(k), ub(k), opt ← LRROUND(αk, A(k−1))
if opt then return ub(k)
0&apos; ← 0 − ATA(k)
T&apos; ← T + A(k)Tb
lb(k), opt ← BEAMSEARCH(0&apos;, T&apos;, lb(k−1), 3k)
if opt then return lb(k)
return maxkE{1...K} lb(k)
� α1 ... αK sequence of subgradient rates
Input:
</equation>
<figureCaption confidence="0.9163407">
31 ... 3K sequence of pruning parameters
Output: optimal constrained score or lower bound
Figure 6: Two versions of optimal beam search: staged
and alternating. Staged runs Lagrangian relaxation to
find the optimal A, uses A to compute upper bounds, and
then repeatedly runs beam search with pruning sequence
31 ... 3k. Alternating switches between running a round
of Lagrangian relaxation and a round of beam search with
the updated A. If either produces a certificate it returns the
result.
</figureCaption>
<bodyText confidence="0.9870189">
and Collins (2011) and Rush and Collins (2011) de-
velop Lagrangian relaxation-based approaches for
exact machine translation.
Apart from translation decoding, this paper is
closely related to work on column generation for
NLP. Riedel et al. (2012) and Belanger et al. (2012)
relate column generation to beam search and pro-
duce exact solutions for parsing and tagging prob-
lems. The latter work also gives conditions for when
beam search-style decoding is optimal.
</bodyText>
<sectionHeader confidence="0.999751" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999939">
To evaluate the effectiveness of optimal beam search
for translation decoding, we implemented decoders
for phrase- and syntax-based models. In this sec-
tion we compare the speed and optimality of these
</bodyText>
<page confidence="0.993935">
217
</page>
<bodyText confidence="0.676915">
decoders to several baseline methods.
</bodyText>
<subsectionHeader confidence="0.9551">
7.1 Setup and Implementation
</subsectionHeader>
<bodyText confidence="0.996801333333334">
For phrase-based translation we used a German-to-
English data set taken from Europarl (Koehn, 2005).
We tested on 1,824 sentences of length at most 50
words. For experiments the phrase-based systems
uses a trigram language model and includes standard
distortion penalties. Additionally the unconstrained
hypergraph includes further derivation information
similar to the graph described in Chang and Collins
(2011).
For syntax-based translation we used a Chinese-
to-English data set. The model and hypergraphs
come from the work of Huang and Mi (2010). We
tested on 691 sentences from the newswire portion
of the 2008 NIST MT evaluation test set. For ex-
periments, the syntax-based model uses a trigram
language model. The translation model is tree-to-
string syntax-based model with a standard context-
free translation forest. The constraint matrix A
is based on the constraints described by Rush and
Collins (2011).
Our decoders use a two-pass architecture. The
first pass sets up the hypergraph in memory, and the
second pass runs search. When possible the base-
lines share optimized construction and search code.
The performance of optimal beam search is de-
pendent on the sequences α and Q. For the step-
size α we used a variant of Polyak’s rule (Polyak,
1987; Boyd and Mutapcic, 2007), substituting the
unknown optimal score for the last computed lower
bound: αk ← ub �Ax(k )-b��lb(JJ22 We adjust the order of
|
the pruning parameter Q based on a function p of
the current gap: Qk ← 10µ(ub(k)−lb(k))
</bodyText>
<subsectionHeader confidence="0.848393">
7.2 Baseline Methods
</subsectionHeader>
<bodyText confidence="0.9998495">
The experiments compare optimal beam search
(OPTBEAM) to several different decoding meth-
ods. For both systems we compare to: BEAM, the
beam search decoder from Figure 3 using the orig-
inal weights B and T, and Q ∈ {100,1000}; LR-
TIGHT, Lagrangian relaxation followed by incre-
</bodyText>
<figureCaption confidence="0.521248166666667">
Figure 7: Two graphs from phrase-based decoding.
Graph (a) shows the duality gap distribution for 1,824
sentences after 0, 5, and 10 rounds of LR. Graph (b)
shows the % of certificates found for sentences with dif-
fering gap sizes and beam search parameters Q. Duality
gap is defined as, ub - (BTx* + T).
</figureCaption>
<bodyText confidence="0.991331">
mental tightening constraints, which is a reimple-
mentation of Chang and Collins (2011) and Rush
and Collins (2011).
For phrase-based translation we compare with:
MOSES-GC, the standard Moses beam search de-
coder with Q ∈ {100,1000} (Koehn et al., 2007);
MOSES, a version of Moses without gap constraints
more similar to BEAM (see Chang and Collins
(2011)); ASTAR, an implementation of A∗ search
using original outside scores, i.e. OUTSIDE(B, T),
and capped at 20,000,000 queue pops.
For syntax-based translation we compare with:
ILP, a general-purpose integer linear program-
ming solver (Gurobi Optimization, 2013) and
CUBEPRUNING, an approximate decoding method
similar to beam search (Chiang, 2007), tested with
Q ∈ {100, 1000}.
</bodyText>
<subsectionHeader confidence="0.801969">
7.3 Experiments
</subsectionHeader>
<bodyText confidence="0.991913">
Table 1 shows the main results. For phrase-based
translation, OPTBEAM decodes the optimal trans-
lation with certificate in 99% of sentences with an
average time of 17.27 seconds per sentence. This
.
Previous work on these data sets has shown that
exact algorithms do not result in a significant in-
crease in translation accuracy. We focus on the effi-
ciency and model score of the algorithms.
</bodyText>
<page confidence="0.993426">
218
</page>
<table confidence="0.999941590909091">
Phrase-Based time 11-20 (558) exact time 21-30 (566) exact time 31-40 (347) exact time 41-50 (168) exact all (1824) exact
cert cert cert cert time cert
BEAM (100) 2.33 19.5 38.0 8.37 1.6 7.2 24.12 0.3 1.4 71.35 0.0 0.0 14.50 15.3 23.2
BEAM (1000) 2.33 37.8 66.3 8.42 3.4 18.9 21.60 0.6 3.2 53.99 0.6 1.2 12.44 22.6 36.9
BEAM (100000) 3.34 83.9 96.2 18.53 22.4 60.4 46.65 2.0 18.1 83.53 1.2 6.5 23.39 43.2 62.4
MOSES (100) 0.18 0.0 81.0 0.36 0.0 45.6 0.53 0.0 14.1 0.74 0.0 6.0 0.34 0.0 52.3
MOSES (1000) 2.29 0.0 97.8 4.39 0.0 78.8 6.52 0.0 43.5 9.00 0.0 19.6 4.20 0.0 74.6
ASTAR (cap) 11.11 99.3 99.3 91.39 53.9 53.9 122.67 7.8 7.8 139.61 1.2 1.2 67.99 58.8 58.8
LR-TIGHT 4.20 100.0 100.0 23.25 100.0 100.0 88.16 99.7 99.7 377.9 97.0 97.0 60.11 99.7 99.7
OPTBEAM 2.85 100.0 100.0 10.33 100.0 100.0 28.29 100.0 100.0 84.34 97.0 97.0 17.27 99.7 99.7
ChangCollins 10.90 100.0 100.0 57.20 100.0 100.0 203.4 99.7 99.7 679.9 97.0 97.0 120.9 99.7 99.7
MOSES-GC (100) 0.14 0.0 89.4 0.27 0.0 84.1 0.41 0.0 75.8 0.58 0.0 78.6 0.26 0.0 84.9
MOSES-GC (1000) 1.33 0.0 89.4 2.62 0.0 84.3 4.15 0.0 75.8 6.19 0.0 79.2 2.61 0.0 85.0
11-20 (192) 21-30 (159) 31-40 (136) 41-100 (123) all (691)
Syntax-Based time cert exact time cert exact time cert exact time cert exact time cert exact
BEAM (100) 0.40 4.7 75.9 0.40 0.0 66.0 0.75 0.0 43.4 1.66 0.0 25.8 0.68 5.72 58.7
BEAM (1000) 0.78 16.9 79.4 2.65 0.6 67.1 6.20 0.0 47.5 15.5 0.0 36.4 4.16 12.5 65.5
CUBE (100) 0.08 0.0 77.6 0.16 0.0 66.7 0.23 0.0 43.9 0.41 0.0 26.3 0.19 0.0 59.0
CUBE (1000) 1.76 0.0 91.7 4.06 0.0 95.0 5.71 0.0 82.9 10.69 0.0 60.9 4.66 0.0 85.0
LR-TIGHT 0.37 100.0 100.0 1.76 100.0 100.0 4.79 100.0 100.0 30.85 94.5 94.5 7.25 99.0 99.0
OPTBEAM 0.23 100.0 100.0 0.50 100.0 100.0 1.42 100.0 100.0 7.14 93.6 93.6 1.75 98.8 98.8
ILP 9.15 100.0 100.0 32.35 100.0 100.0 49.6 100.0 100.0 108.6 100.0 100.0 40.1 100.0 100.0
</table>
<tableCaption confidence="0.914940666666667">
Table 1: Experimental results for translation experiments. Column time is the mean time per sentence in seconds,
cert is the percentage of sentences solved with a certificate of optimality, exact is the percentage of sentences solved
exactly, i.e. BTx + T = BTx* + T. Results are grouped by sentence length (group 1-10 is omitted for space).
</tableCaption>
<bodyText confidence="0.999869275862069">
is seven times faster than the decoder of Chang and
Collins (2011) and 3.5 times faster then our reim-
plementation, LR-TIGHT. ASTAR performs poorly,
taking lots of time on difficult sentences. BEAM runs
quickly, but rarely finds an exact solution. MOSES
without gap constraints is also fast, but less exact
than OPTBEAM and unable to produce certificates.
For syntax-based translation. OPTBEAM finds a
certificate on 98.8% of solutions with an average
time of 1.75 seconds per sentence, and is four times
faster than LR-TIGHT. CUBE (100) is an order
of magnitude faster, but is rarely exact on longer
sentences. CUBE (1000) finds more exact solu-
tions, but is comparable in speed to optimal beam
search. BEAM performs better than in the phrase-
based model, but is not much faster than OPTBEAM.
Figure 7.2 shows the relationship between beam
search optimality and duality gap. Graph (a) shows
how a handful of LR rounds can significantly tighten
the upper bound score of many sentences. Graph (b)
shows how beam search is more likely to find opti-
mal solutions with tighter bounds. BEAM effectively
uses 0 rounds of LR, which may explain why it finds
so few optimal solutions compared to OPTBEAM.
Table 2 breaks down the time spent in each part
of the algorithm. For both methods, beam search has
the most time variance and uses more time on longer
sentences. For phrase-based sentences, Lagrangian
relaxation is fast, and hypergraph construction dom-
</bodyText>
<table confidence="0.997586625">
&gt; 30 all
mean median mean median
Hypergraph 56.6% 69.8% 59.6% 69.6%
PB Lag. Relaxation 10.0% 5.5% 9.4% 7.6%
Beam Search 33.4% 24.6% 30.9% 22.8%
Hypergraph 0.5% 1.6% 0.8% 2.4%
SB Lag. Relaxation 15.0% 35.2% 17.3% 41.4%
Beam Search 84.4% 63.1% 81.9 % 56.1%
</table>
<tableCaption confidence="0.6823072">
Table 2: Distribution of time within optimal beam search,
including: hypergraph construction, Lagrangian relax-
ation, and beam search. Mean is the percentage of total
time. Median is the distribution over the median values
for each row.
</tableCaption>
<bodyText confidence="0.9925005">
inates. If not for this cost, OPTBEAM might be com-
parable in speed to MOSES (1000).
</bodyText>
<sectionHeader confidence="0.997499" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999880125">
In this work we develop an optimal variant of beam
search and apply it to machine translation decod-
ing. The algorithm uses beam search to produce
constrained solutions and bounds from Lagrangian
relaxation to eliminate non-optimal solutions. Re-
sults show that this method can efficiently find exact
solutions for two important styles of machine trans-
lation.
</bodyText>
<sectionHeader confidence="0.697937" genericHeader="acknowledgments">
Acknowledgments Alexander Rush, Yin-Wen
</sectionHeader>
<note confidence="0.339638666666667">
Chang and Michael Collins were all supported by
NSF grant IIS-1161814. Alexander Rush was partially
supported by an NSF Graduate Research Fellowship.
</note>
<page confidence="0.998689">
219
</page>
<sectionHeader confidence="0.983541" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999644047619048">
David Belanger, Alexandre Passos, Sebastian Riedel, and
Andrew McCallum. 2012. Map inference in chains
using column generation. In NIPS, pages 1853–1861.
Stephen Boyd and Almir Mutapcic. 2007. Subgradient
methods.
Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through la-
grangian relaxation. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 26–37. Association for Computational Lin-
guistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. computational linguistics, 33(2):201–228.
Adria de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hierar-
chical Phrase-Based Translation with Weighted Finite-
State Transducers and Shallow-n Grammars. In Com-
putational linguistics, volume 36, pages 505–533.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathen
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vlad Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proceed-
ings of the 39th Annual Meeting on Association for
Computational Linguistics, ACL ’01, pages 228–235.
Inc. Gurobi Optimization. 2013. Gurobi optimizer refer-
ence manual.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technology, pages 53–64. As-
sociation for Computational Linguistics.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144–151,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 273–283, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 380–388, Athens, Greece,
March. Association for Computational Linguistics.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607–615.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology, NAACL ’03,
pages 48–54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL ’07,
pages 177–180.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. Machine translation: From real users to research,
pages 115–124.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in Nat-
ural Language Processing, pages 161–168, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
R. Kipp Martin, Rardin L. Rardin, and Brian A. Camp-
bell. 1990. Polyhedral characterization of dis-
crete dynamic programming. Operations research,
38(1):127–138.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for statisti-
cal machine translation. In Proceedings of the work-
shop on Data-driven methods in machine translation -
Volume 14, DMMT ’01, pages 1–8, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Boris Polyak. 1987. Introduction to Optimization. Opti-
mization Software, Inc.
Sebastian Riedel and James Clarke. 2009. Revisiting
optimal decoding for machine translation IBM model
4. In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Companion Volume: Short Papers, pages 5–8. As-
sociation for Computational Linguistics.
Sebastian Riedel, David Smith, and Andrew McCallum.
2012. Parse, price and cut: delayed column and row
generation for graph based parsers. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
</reference>
<page confidence="0.957659">
220
</page>
<reference confidence="0.999489513513514">
Natural Language Learning, pages 732–743. Associa-
tion for Computational Linguistics.
Alexander M Rush and Michael Collins. 2011. Exact
decoding of syntactic translation models through la-
grangian relaxation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, vol-
ume 1, pages 72–82.
Alexander M Rush and Michael Collins. 2012. A tutorial
on dual decomposition and lagrangian relaxation for
inference in natural language processing. Journal of
Artificial Intelligence Research, 45:305–362.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for statistical machine translation. Computa-
tional Linguistics, 29(1):97–133.
Christoph Tillmann. 2006. Efficient dynamic pro-
gramming search algorithms for phrase-based SMT.
In Proceedings of the Workshop on Computationally
Hard Problems and Joint Inference in Speech and Lan-
guage Processing, CHSLP ’06, pages 9–16.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics, ACL-44, pages 777–784,
Morristown, NJ, USA. Association for Computational
Linguistics.
Mikhail Zaslavskiy, Marc Dymetman, and Nicola Can-
cedda. 2009. Phrase-based statistical machine transla-
tion as a traveling salesman problem. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
1 - Volume 1, ACL ’09, pages 333–341, Stroudsburg,
PA, USA. Association for Computational Linguistics.
</reference>
<page confidence="0.998329">
221
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.592546">
<title confidence="0.999913">Optimal Beam Search for Machine Translation</title>
<author confidence="0.999977">Alexander M Rush Yin-Wen</author>
<affiliation confidence="0.996625">MIT</affiliation>
<address confidence="0.999112">Cambridge, MA 02139,</address>
<author confidence="0.828943">Michael</author>
<affiliation confidence="0.999959">Department of Computer</affiliation>
<address confidence="0.879628">Columbia New York, NY 10027,</address>
<email confidence="0.999668">mcollins@cs.columbia.edu</email>
<abstract confidence="0.995416125">Beam search is a fast and empirically effective method for translation decoding, but it lacks formal guarantees about search error. We develop a new decoding algorithm that combines the speed of beam search with the optimal certificate property of Lagrangian relaxation, and apply it to phraseand syntax-based translation decoding. The new method is efficient, utilizes standard MT algorithms, and returns an exact solution on the majority of translation examples in our test data. The algorithm is 3.5 times faster than an optimized incremental constraint-based decoder for phrase-based translation and 4 times faster for syntax-based translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Belanger</author>
<author>Alexandre Passos</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Map inference in chains using column generation.</title>
<date>2012</date>
<booktitle>In NIPS,</booktitle>
<pages>1853--1861</pages>
<contexts>
<context position="28791" citStr="Belanger et al. (2012)" startWordPosition="5069" endWordPosition="5072">and alternating. Staged runs Lagrangian relaxation to find the optimal A, uses A to compute upper bounds, and then repeatedly runs beam search with pruning sequence 31 ... 3k. Alternating switches between running a round of Lagrangian relaxation and a round of beam search with the updated A. If either produces a certificate it returns the result. and Collins (2011) and Rush and Collins (2011) develop Lagrangian relaxation-based approaches for exact machine translation. Apart from translation decoding, this paper is closely related to work on column generation for NLP. Riedel et al. (2012) and Belanger et al. (2012) relate column generation to beam search and produce exact solutions for parsing and tagging problems. The latter work also gives conditions for when beam search-style decoding is optimal. 7 Results To evaluate the effectiveness of optimal beam search for translation decoding, we implemented decoders for phrase- and syntax-based models. In this section we compare the speed and optimality of these 217 decoders to several baseline methods. 7.1 Setup and Implementation For phrase-based translation we used a German-toEnglish data set taken from Europarl (Koehn, 2005). We tested on 1,824 sentences </context>
</contexts>
<marker>Belanger, Passos, Riedel, McCallum, 2012</marker>
<rawString>David Belanger, Alexandre Passos, Sebastian Riedel, and Andrew McCallum. 2012. Map inference in chains using column generation. In NIPS, pages 1853–1861.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Boyd</author>
<author>Almir Mutapcic</author>
</authors>
<date>2007</date>
<note>Subgradient methods.</note>
<contexts>
<context position="30542" citStr="Boyd and Mutapcic, 2007" startWordPosition="5347" endWordPosition="5350">nts, the syntax-based model uses a trigram language model. The translation model is tree-tostring syntax-based model with a standard contextfree translation forest. The constraint matrix A is based on the constraints described by Rush and Collins (2011). Our decoders use a two-pass architecture. The first pass sets up the hypergraph in memory, and the second pass runs search. When possible the baselines share optimized construction and search code. The performance of optimal beam search is dependent on the sequences α and Q. For the stepsize α we used a variant of Polyak’s rule (Polyak, 1987; Boyd and Mutapcic, 2007), substituting the unknown optimal score for the last computed lower bound: αk ← ub �Ax(k )-b��lb(JJ22 We adjust the order of | the pruning parameter Q based on a function p of the current gap: Qk ← 10µ(ub(k)−lb(k)) 7.2 Baseline Methods The experiments compare optimal beam search (OPTBEAM) to several different decoding methods. For both systems we compare to: BEAM, the beam search decoder from Figure 3 using the original weights B and T, and Q ∈ {100,1000}; LRTIGHT, Lagrangian relaxation followed by increFigure 7: Two graphs from phrase-based decoding. Graph (a) shows the duality gap distribut</context>
</contexts>
<marker>Boyd, Mutapcic, 2007</marker>
<rawString>Stephen Boyd and Almir Mutapcic. 2007. Subgradient methods.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yin-Wen Chang</author>
<author>Michael Collins</author>
</authors>
<title>Exact decoding of phrase-based translation models through lagrangian relaxation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>26--37</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9029" citStr="Chang and Collins (2011)" startWordPosition="1549" endWordPosition="1552">... wk and a sequence of target words r1 ... r|r|, with (q, r) E P. We say the source words wi ... wk are translated to r. The score of a derivation, f(p), is the sum of the translation score of each phrase plus the language model score of the target sentence Crucially for a derivation to be valid it must satisfy an additional condition: it must translate every source word exactly once. The decoding problem for phrase-based translation is to find the highestscoring derivation satisfying this property. We can represent this decoding problem as a constrained hypergraph using the construction of Chang and Collins (2011). The hypergraph weights encode the translation and language model scores, and its structure ensures that the count of source words translated is |w|, i.e. the length of the source sentence. Each vertex will remember the preceding target-language word and the count of source words translated so far. The hypergraph, which for this problem is also a directed graph, takes the following form. • Vertices v E V are labeled (c, u) where c E 11 ... |w|} is the count of source words translated and u E E is the last target-language word produced by a partial hypothesis at this vertex. Additionally there</context>
<context position="29674" citStr="Chang and Collins (2011)" startWordPosition="5201" endWordPosition="5204">nslation decoding, we implemented decoders for phrase- and syntax-based models. In this section we compare the speed and optimality of these 217 decoders to several baseline methods. 7.1 Setup and Implementation For phrase-based translation we used a German-toEnglish data set taken from Europarl (Koehn, 2005). We tested on 1,824 sentences of length at most 50 words. For experiments the phrase-based systems uses a trigram language model and includes standard distortion penalties. Additionally the unconstrained hypergraph includes further derivation information similar to the graph described in Chang and Collins (2011). For syntax-based translation we used a Chineseto-English data set. The model and hypergraphs come from the work of Huang and Mi (2010). We tested on 691 sentences from the newswire portion of the 2008 NIST MT evaluation test set. For experiments, the syntax-based model uses a trigram language model. The translation model is tree-tostring syntax-based model with a standard contextfree translation forest. The constraint matrix A is based on the constraints described by Rush and Collins (2011). Our decoders use a two-pass architecture. The first pass sets up the hypergraph in memory, and the se</context>
<context position="31442" citStr="Chang and Collins (2011)" startWordPosition="5502" endWordPosition="5505">arch (OPTBEAM) to several different decoding methods. For both systems we compare to: BEAM, the beam search decoder from Figure 3 using the original weights B and T, and Q ∈ {100,1000}; LRTIGHT, Lagrangian relaxation followed by increFigure 7: Two graphs from phrase-based decoding. Graph (a) shows the duality gap distribution for 1,824 sentences after 0, 5, and 10 rounds of LR. Graph (b) shows the % of certificates found for sentences with differing gap sizes and beam search parameters Q. Duality gap is defined as, ub - (BTx* + T). mental tightening constraints, which is a reimplementation of Chang and Collins (2011) and Rush and Collins (2011). For phrase-based translation we compare with: MOSES-GC, the standard Moses beam search decoder with Q ∈ {100,1000} (Koehn et al., 2007); MOSES, a version of Moses without gap constraints more similar to BEAM (see Chang and Collins (2011)); ASTAR, an implementation of A∗ search using original outside scores, i.e. OUTSIDE(B, T), and capped at 20,000,000 queue pops. For syntax-based translation we compare with: ILP, a general-purpose integer linear programming solver (Gurobi Optimization, 2013) and CUBEPRUNING, an approximate decoding method similar to beam search (C</context>
<context position="34773" citStr="Chang and Collins (2011)" startWordPosition="6090" endWordPosition="6093">00.0 30.85 94.5 94.5 7.25 99.0 99.0 OPTBEAM 0.23 100.0 100.0 0.50 100.0 100.0 1.42 100.0 100.0 7.14 93.6 93.6 1.75 98.8 98.8 ILP 9.15 100.0 100.0 32.35 100.0 100.0 49.6 100.0 100.0 108.6 100.0 100.0 40.1 100.0 100.0 Table 1: Experimental results for translation experiments. Column time is the mean time per sentence in seconds, cert is the percentage of sentences solved with a certificate of optimality, exact is the percentage of sentences solved exactly, i.e. BTx + T = BTx* + T. Results are grouped by sentence length (group 1-10 is omitted for space). is seven times faster than the decoder of Chang and Collins (2011) and 3.5 times faster then our reimplementation, LR-TIGHT. ASTAR performs poorly, taking lots of time on difficult sentences. BEAM runs quickly, but rarely finds an exact solution. MOSES without gap constraints is also fast, but less exact than OPTBEAM and unable to produce certificates. For syntax-based translation. OPTBEAM finds a certificate on 98.8% of solutions with an average time of 1.75 seconds per sentence, and is four times faster than LR-TIGHT. CUBE (100) is an order of magnitude faster, but is rarely exact on longer sentences. CUBE (1000) finds more exact solutions, but is comparab</context>
</contexts>
<marker>Chang, Collins, 2011</marker>
<rawString>Yin-Wen Chang and Michael Collins. 2011. Exact decoding of phrase-based translation models through lagrangian relaxation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 26–37. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation. computational linguistics,</title>
<date>2007</date>
<pages>33--2</pages>
<contexts>
<context position="999" citStr="Chiang, 2007" startWordPosition="147" endWordPosition="148"> search error. We develop a new decoding algorithm that combines the speed of beam search with the optimal certificate property of Lagrangian relaxation, and apply it to phrase- and syntax-based translation decoding. The new method is efficient, utilizes standard MT algorithms, and returns an exact solution on the majority of translation examples in our test data. The algorithm is 3.5 times faster than an optimized incremental constraint-based decoder for phrase-based translation and 4 times faster for syntax-based translation. 1 Introduction Beam search (Koehn et al., 2003) and cube pruning (Chiang, 2007) have become the de facto decoding algorithms for phrase- and syntax-based translation. The algorithms are central to large-scale machine translation systems due to their efficiency and tendency to produce high-quality translations (Koehn, 2004; Koehn et al., 2007; Dyer et al., 2010). However despite practical effectiveness, neither algorithm provides any bound on possible decoding error. In this work we present a variant of beam search decoding for phrase- and syntax-based translation. The motivation is to exploit the effectiveness and efficiency of beam search, but still maintain formal guar</context>
<context position="26655" citStr="Chiang, 2007" startWordPosition="4706" endWordPosition="4707">M shown Figure 6. In each round, the algorithm alternates between computing subgradients to tighten ubs and running beam search to maximize lb. In early rounds we set 3 for aggressive beam pruning, and as the upper bounds get tighter, we loosen pruning to try to get a certificate. If at any point either a primal or dual certificate is found, the algorithm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (I</context>
<context position="32054" citStr="Chiang, 2007" startWordPosition="5596" endWordPosition="5597">) and Rush and Collins (2011). For phrase-based translation we compare with: MOSES-GC, the standard Moses beam search decoder with Q ∈ {100,1000} (Koehn et al., 2007); MOSES, a version of Moses without gap constraints more similar to BEAM (see Chang and Collins (2011)); ASTAR, an implementation of A∗ search using original outside scores, i.e. OUTSIDE(B, T), and capped at 20,000,000 queue pops. For syntax-based translation we compare with: ILP, a general-purpose integer linear programming solver (Gurobi Optimization, 2013) and CUBEPRUNING, an approximate decoding method similar to beam search (Chiang, 2007), tested with Q ∈ {100, 1000}. 7.3 Experiments Table 1 shows the main results. For phrase-based translation, OPTBEAM decodes the optimal translation with certificate in 99% of sentences with an average time of 17.27 seconds per sentence. This . Previous work on these data sets has shown that exact algorithms do not result in a significant increase in translation accuracy. We focus on the efficiency and model score of the algorithms. 218 Phrase-Based time 11-20 (558) exact time 21-30 (566) exact time 31-40 (347) exact time 41-50 (168) exact all (1824) exact cert cert cert cert time cert BEAM (1</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. computational linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adria de Gispert</author>
<author>Gonzalo Iglesias</author>
<author>Graeme Blackwood</author>
<author>Eduardo R Banga</author>
<author>William Byrne</author>
</authors>
<title>Hierarchical Phrase-Based Translation with Weighted FiniteState Transducers and Shallow-n Grammars.</title>
<date>2010</date>
<booktitle>In Computational linguistics,</booktitle>
<volume>36</volume>
<pages>505--533</pages>
<marker>de Gispert, Iglesias, Blackwood, Banga, Byrne, 2010</marker>
<rawString>Adria de Gispert, Gonzalo Iglesias, Graeme Blackwood, Eduardo R. Banga, and William Byrne. 2010. Hierarchical Phrase-Based Translation with Weighted FiniteState Transducers and Shallow-n Grammars. In Computational linguistics, volume 36, pages 505–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathen Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vlad Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models.</title>
<date>2010</date>
<contexts>
<context position="1283" citStr="Dyer et al., 2010" startWordPosition="188" endWordPosition="191">and returns an exact solution on the majority of translation examples in our test data. The algorithm is 3.5 times faster than an optimized incremental constraint-based decoder for phrase-based translation and 4 times faster for syntax-based translation. 1 Introduction Beam search (Koehn et al., 2003) and cube pruning (Chiang, 2007) have become the de facto decoding algorithms for phrase- and syntax-based translation. The algorithms are central to large-scale machine translation systems due to their efficiency and tendency to produce high-quality translations (Koehn, 2004; Koehn et al., 2007; Dyer et al., 2010). However despite practical effectiveness, neither algorithm provides any bound on possible decoding error. In this work we present a variant of beam search decoding for phrase- and syntax-based translation. The motivation is to exploit the effectiveness and efficiency of beam search, but still maintain formal guarantees. The algorithm has the following benefits: • In theory, it can provide a certificate of optimality; in practice, we show that it produces optimal hypotheses, with certificates of optimality, on the vast majority of examples. • It utilizes well-studied algorithms and extends of</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathen Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vlad Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, ACL ’01,</booktitle>
<pages>228--235</pages>
<contexts>
<context position="26981" citStr="Germann et al., 2001" startWordPosition="4761" endWordPosition="4764">dual certificate is found, the algorithm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phrase-based models with limited reordering (Kumar and Byrne, 2005). Exact decoding based on FST is also feasible for certain hierarchical grammars (de Gispert et al., 2010). Chang procedure OPTBEAMSTAGED(α, 3) A, ub, opt ←LAGRANGIANRELAXATION(α) if opt then return ub 0&apos; ← 0 − ATA T</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, ACL ’01, pages 228–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gurobi Optimization</author>
</authors>
<date>2013</date>
<note>Gurobi optimizer reference manual.</note>
<contexts>
<context position="31968" citStr="Optimization, 2013" startWordPosition="5584" endWordPosition="5585"> + T). mental tightening constraints, which is a reimplementation of Chang and Collins (2011) and Rush and Collins (2011). For phrase-based translation we compare with: MOSES-GC, the standard Moses beam search decoder with Q ∈ {100,1000} (Koehn et al., 2007); MOSES, a version of Moses without gap constraints more similar to BEAM (see Chang and Collins (2011)); ASTAR, an implementation of A∗ search using original outside scores, i.e. OUTSIDE(B, T), and capped at 20,000,000 queue pops. For syntax-based translation we compare with: ILP, a general-purpose integer linear programming solver (Gurobi Optimization, 2013) and CUBEPRUNING, an approximate decoding method similar to beam search (Chiang, 2007), tested with Q ∈ {100, 1000}. 7.3 Experiments Table 1 shows the main results. For phrase-based translation, OPTBEAM decodes the optimal translation with certificate in 99% of sentences with an average time of 17.27 seconds per sentence. This . Previous work on these data sets has shown that exact algorithms do not result in a significant increase in translation accuracy. We focus on the efficiency and model score of the algorithms. 218 Phrase-Based time 11-20 (558) exact time 21-30 (566) exact time 31-40 (34</context>
</contexts>
<marker>Optimization, 2013</marker>
<rawString>Inc. Gurobi Optimization. 2013. Gurobi optimizer reference manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technology,</booktitle>
<pages>53--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15979" citStr="Huang and Chiang (2005)" startWordPosition="2776" endWordPosition="2779">thm invokes the black-box function, PRUNE, on line 13, passing it a pruning parameter β and a vertex-signature pair. The parameter β controls a threshold for pruning. For instance for phrase-based translation, it specifies a hard-limit on the number of hypotheses to retain. The function returns true if it prunes from the chart. Note that pruning may remove optimal hypotheses, so we set the certificate flag opt to false if the chart is modified. 2For simplicity we write this loop over the entire set. In practice it is important to use data structures to optimize lookup. See Tillmann (2006) and Huang and Chiang (2005). 1: procedure BEAMSEARCH(θ, τ, lb, β) 2: ubs +— OUTSIDE(θ, τ) 3: opt +— true 4: π[v, sig] +— −oo for all v E V, sig E R|b| 5: π[v, 0] +— 0 for all v E T 6: for e E E in topological order do 7: ((v2,... , v|v|), v1) +— e 8: for sig(2) ... sig(|v|) E SIGS(v2, ... , v|v|) do � s &gt; π[v1, sig] n 11: if �CHECK(sig) n s + ubs[v1] &gt; lb 12: π[v1, sig] +— s 13: if PRUNE(π, v1, sig,β) then opt +— false 14: lb0 +— π[1,c] + τ 15: return lb0, opt (V, E, θ, τ) hypergraph with weights (A, b) matrix and vector for constraints lb E R lower bound β a pruning parameter � lb0 resulting lower bound score Output: o</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of the Ninth International Workshop on Parsing Technology, pages 53–64. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>144--151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="26679" citStr="Huang and Chiang, 2007" startWordPosition="4708" endWordPosition="4711"> 6. In each round, the algorithm alternates between computing subgradients to tighten ubs and running beam search to maximize lb. In early rounds we set 3 for aggressive beam pruning, and as the upper bounds get tighter, we loosen pruning to try to get a certificate. If at any point either a primal or dual certificate is found, the algorithm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) ha</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144–151, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Haitao Mi</author>
</authors>
<title>Efficient incremental decoding for tree-to-string translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>273--283</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="26723" citStr="Huang and Mi, 2010" startWordPosition="4717" endWordPosition="4720">ween computing subgradients to tighten ubs and running beam search to maximize lb. In early rounds we set 3 for aggressive beam pruning, and as the upper bounds get tighter, we loosen pruning to try to get a certificate. If at any point either a primal or dual certificate is found, the algorithm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phrase-based models with </context>
<context position="29810" citStr="Huang and Mi (2010)" startWordPosition="5224" endWordPosition="5227">217 decoders to several baseline methods. 7.1 Setup and Implementation For phrase-based translation we used a German-toEnglish data set taken from Europarl (Koehn, 2005). We tested on 1,824 sentences of length at most 50 words. For experiments the phrase-based systems uses a trigram language model and includes standard distortion penalties. Additionally the unconstrained hypergraph includes further derivation information similar to the graph described in Chang and Collins (2011). For syntax-based translation we used a Chineseto-English data set. The model and hypergraphs come from the work of Huang and Mi (2010). We tested on 691 sentences from the newswire portion of the 2008 NIST MT evaluation test set. For experiments, the syntax-based model uses a trigram language model. The translation model is tree-tostring syntax-based model with a standard contextfree translation forest. The constraint matrix A is based on the constraints described by Rush and Collins (2011). Our decoders use a two-pass architecture. The first pass sets up the hypergraph in memory, and the second pass runs search. When possible the baselines share optimized construction and search code. The performance of optimal beam search </context>
</contexts>
<marker>Huang, Mi, 2010</marker>
<rawString>Liang Huang and Haitao Mi. 2010. Efficient incremental decoding for tree-to-string translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 273–283, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Iglesias</author>
<author>Adri`a de Gispert</author>
<author>Eduardo R Banga</author>
<author>William Byrne</author>
</authors>
<title>Rule filtering by pattern for efficient hierarchical translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>380--388</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<marker>Iglesias, de Gispert, Banga, Byrne, 2009</marker>
<rawString>Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga, and William Byrne. 2009. Rule filtering by pattern for efficient hierarchical translation. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 380–388, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Decoding complexity in wordreplacement translation models.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="7459" citStr="Knight, 1999" startWordPosition="1268" endWordPosition="1269">aken as 0. E e:h(e)=v Ex(e) = e:vEt(e) x(e) ∀ v ∈ N \ {1} } x : E x(e) = 1, eEE:h(e)=1 � ���� � ����� X= s ← θ(e) + |v |π[vi] E i=2 if s &gt; π[v1] then π[v1] ← s return π[1] + τ Emax xEX eE£ θTx + τ 211 where we have a constraint matrix A E RIA X61 1£1 and vector b E R|b |encoding |b |constraints. The optimal constrained hyperpath is x* = arg ma xEX&apos; θTx + τ. Note that the constrained hypergraph search problem may be NP-Hard. Crucially this is true even when the corresponding unconstrained search problem is solvable in polynomial time. For instance, phrase-based decoding is known to be NP-Hard (Knight, 1999), but we will see that it can be expressed as a polynomial-sized hypergraph with constraints. Example: Phrase-Based Machine Translation Consider translating a source sentence w1 ... w|w |to a target sentence in a language with vocabulary E. A simple phrase-based translation model consists of a tuple (P, ω, σ) with • P; a set of pairs (q, r) where q1 ... q|q |is a sequence of source-language words and r1 ... r|r| is a sequence of target-language words drawn from the target vocabulary E. • ω : R|P|; parameters for the translation model mapping each pair in P to a real-valued score. • σ : R|ExE|;</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. Decoding complexity in wordreplacement translation models. Computational Linguistics, 25(4):607–615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, NAACL ’03,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="967" citStr="Koehn et al., 2003" startWordPosition="139" endWordPosition="142">, but it lacks formal guarantees about search error. We develop a new decoding algorithm that combines the speed of beam search with the optimal certificate property of Lagrangian relaxation, and apply it to phrase- and syntax-based translation decoding. The new method is efficient, utilizes standard MT algorithms, and returns an exact solution on the majority of translation examples in our test data. The algorithm is 3.5 times faster than an optimized incremental constraint-based decoder for phrase-based translation and 4 times faster for syntax-based translation. 1 Introduction Beam search (Koehn et al., 2003) and cube pruning (Chiang, 2007) have become the de facto decoding algorithms for phrase- and syntax-based translation. The algorithms are central to large-scale machine translation systems due to their efficiency and tendency to produce high-quality translations (Koehn, 2004; Koehn et al., 2007; Dyer et al., 2010). However despite practical effectiveness, neither algorithm provides any bound on possible decoding error. In this work we present a variant of beam search decoding for phrase- and syntax-based translation. The motivation is to exploit the effectiveness and efficiency of beam search</context>
<context position="26564" citStr="Koehn et al., 2003" startWordPosition="4691" endWordPosition="4694">beam search may still improve the lower bound lb. This motivates the alternating algorithm OPTBEAM shown Figure 6. In each round, the algorithm alternates between computing subgradients to tighten ubs and running beam search to maximize lb. In early rounds we set 3 for aggressive beam pruning, and as the upper bounds get tighter, we loosen pruning to try to get a certificate. If at any point either a primal or dual certificate is found, the algorithm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, NAACL ’03, pages 48–54.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondˇrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="1263" citStr="Koehn et al., 2007" startWordPosition="184" endWordPosition="187">dard MT algorithms, and returns an exact solution on the majority of translation examples in our test data. The algorithm is 3.5 times faster than an optimized incremental constraint-based decoder for phrase-based translation and 4 times faster for syntax-based translation. 1 Introduction Beam search (Koehn et al., 2003) and cube pruning (Chiang, 2007) have become the de facto decoding algorithms for phrase- and syntax-based translation. The algorithms are central to large-scale machine translation systems due to their efficiency and tendency to produce high-quality translations (Koehn, 2004; Koehn et al., 2007; Dyer et al., 2010). However despite practical effectiveness, neither algorithm provides any bound on possible decoding error. In this work we present a variant of beam search decoding for phrase- and syntax-based translation. The motivation is to exploit the effectiveness and efficiency of beam search, but still maintain formal guarantees. The algorithm has the following benefits: • In theory, it can provide a certificate of optimality; in practice, we show that it produces optimal hypotheses, with certificates of optimality, on the vast majority of examples. • It utilizes well-studied algor</context>
<context position="31607" citStr="Koehn et al., 2007" startWordPosition="5529" endWordPosition="5532">Q ∈ {100,1000}; LRTIGHT, Lagrangian relaxation followed by increFigure 7: Two graphs from phrase-based decoding. Graph (a) shows the duality gap distribution for 1,824 sentences after 0, 5, and 10 rounds of LR. Graph (b) shows the % of certificates found for sentences with differing gap sizes and beam search parameters Q. Duality gap is defined as, ub - (BTx* + T). mental tightening constraints, which is a reimplementation of Chang and Collins (2011) and Rush and Collins (2011). For phrase-based translation we compare with: MOSES-GC, the standard Moses beam search decoder with Q ∈ {100,1000} (Koehn et al., 2007); MOSES, a version of Moses without gap constraints more similar to BEAM (see Chang and Collins (2011)); ASTAR, an implementation of A∗ search using original outside scores, i.e. OUTSIDE(B, T), and capped at 20,000,000 queue pops. For syntax-based translation we compare with: ILP, a general-purpose integer linear programming solver (Gurobi Optimization, 2013) and CUBEPRUNING, an approximate decoding method similar to beam search (Chiang, 2007), tested with Q ∈ {100, 1000}. 7.3 Experiments Table 1 shows the main results. For phrase-based translation, OPTBEAM decodes the optimal translation with</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models. Machine translation: From real users to research,</title>
<date>2004</date>
<pages>115--124</pages>
<contexts>
<context position="1243" citStr="Koehn, 2004" startWordPosition="182" endWordPosition="183">utilizes standard MT algorithms, and returns an exact solution on the majority of translation examples in our test data. The algorithm is 3.5 times faster than an optimized incremental constraint-based decoder for phrase-based translation and 4 times faster for syntax-based translation. 1 Introduction Beam search (Koehn et al., 2003) and cube pruning (Chiang, 2007) have become the de facto decoding algorithms for phrase- and syntax-based translation. The algorithms are central to large-scale machine translation systems due to their efficiency and tendency to produce high-quality translations (Koehn, 2004; Koehn et al., 2007; Dyer et al., 2010). However despite practical effectiveness, neither algorithm provides any bound on possible decoding error. In this work we present a variant of beam search decoding for phrase- and syntax-based translation. The motivation is to exploit the effectiveness and efficiency of beam search, but still maintain formal guarantees. The algorithm has the following benefits: • In theory, it can provide a certificate of optimality; in practice, we show that it produces optimal hypotheses, with certificates of optimality, on the vast majority of examples. • It utilize</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. Machine translation: From real users to research, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Local phrase reordering models for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>161--168</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="27365" citStr="Kumar and Byrne, 2005" startWordPosition="4819" endWordPosition="4822"> work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phrase-based models with limited reordering (Kumar and Byrne, 2005). Exact decoding based on FST is also feasible for certain hierarchical grammars (de Gispert et al., 2010). Chang procedure OPTBEAMSTAGED(α, 3) A, ub, opt ←LAGRANGIANRELAXATION(α) if opt then return ub 0&apos; ← 0 − ATA T&apos; ← T + ATb lb(0) ← −∞ for k in 1 ... K do lb(k), opt ← BEAMSEARCH(0&apos;, T&apos;, lb(k−1), 3k) if opt then return lb(k) return maxkE{1...K} lb(k) procedure OPTBEAM(α, 3) A(0) ← 0 lb(0) ← −∞ for k in 1 ... K do A(k), ub(k), opt ← LRROUND(αk, A(k−1)) if opt then return ub(k) 0&apos; ← 0 − ATA(k) T&apos; ← T + A(k)Tb lb(k), opt ← BEAMSEARCH(0&apos;, T&apos;, lb(k−1), 3k) if opt then return lb(k) return maxkE{1.</context>
</contexts>
<marker>Kumar, Byrne, 2005</marker>
<rawString>Shankar Kumar and William Byrne. 2005. Local phrase reordering models for statistical machine translation. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 161–168, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kipp Martin</author>
<author>Rardin L Rardin</author>
<author>Brian A Campbell</author>
</authors>
<title>Polyhedral characterization of discrete dynamic programming.</title>
<date>1990</date>
<journal>Operations research,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="5134" citStr="Martin et al. (1990)" startWordPosition="842" endWordPosition="845">sequence t(e) = hv2, ... , v|v|i. The size of the tail |t(e) |may vary across different hyperedges, but |t(e) |≥ 1 for all edges and is bounded by a constant. A directed graph is a directed hypergraph with |t(e) |= 1 for all edges e ∈ E. Each vertex v ∈ V is either a non-terminal or a terminal in the hypergraph. The set of non-terminals is N = {v ∈ V : h(e) = v for some e ∈ E}. Conversely, the set of terminals is defined as T = V \N. All directed hypergraphs used in this work are acyclic: informally this implies that no hyperpath (as defined below) contains the same vertex more than once (see Martin et al. (1990) for a full definition). Acyclicity implies a partial topological ordering of the vertices. We also assume there is a distinguished root vertex 1 where for all e ∈ E, 1 ∈6 t(e). Next we define a hyperpath as x ∈ {0,1}|£ |where x(e) = 1 if hyperedge e is used in the hyperpath, procedure BESTPATHSCORE(θ, τ) π[v] ← 0 for all v ∈ T for e ∈ E in topological order do hhv2, ... ,v|v|i,v1i ← e Figure 1: Dynamic programming algorithm for unconstrained hypergraph search. Note that this version only returns the highest score: maxxEX θTx+τ. The optimal hyperpath can be found by including back-pointers. x(</context>
</contexts>
<marker>Martin, Rardin, Campbell, 1990</marker>
<rawString>R. Kipp Martin, Rardin L. Rardin, and Brian A. Campbell. 1990. Polyhedral characterization of discrete dynamic programming. Operations research, 38(1):127–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>An efficient A* search algorithm for statistical machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the workshop on Data-driven methods in machine translation -Volume 14, DMMT ’01,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="27067" citStr="Och et al., 2001" startWordPosition="4775" endWordPosition="4778">oximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phrase-based models with limited reordering (Kumar and Byrne, 2005). Exact decoding based on FST is also feasible for certain hierarchical grammars (de Gispert et al., 2010). Chang procedure OPTBEAMSTAGED(α, 3) A, ub, opt ←LAGRANGIANRELAXATION(α) if opt then return ub 0&apos; ← 0 − ATA T&apos; ← T + ATb lb(0) ← −∞ for k in 1 ... K do lb(k), opt ← BEAMSEARCH(0&apos;, T&apos;, lb(k−1), 3k</context>
</contexts>
<marker>Och, Ueffing, Ney, 2001</marker>
<rawString>Franz Josef Och, Nicola Ueffing, and Hermann Ney. 2001. An efficient A* search algorithm for statistical machine translation. In Proceedings of the workshop on Data-driven methods in machine translation -Volume 14, DMMT ’01, pages 1–8, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boris Polyak</author>
</authors>
<title>Introduction to Optimization.</title>
<date>1987</date>
<publisher>Optimization Software, Inc.</publisher>
<contexts>
<context position="30516" citStr="Polyak, 1987" startWordPosition="5345" endWordPosition="5346">. For experiments, the syntax-based model uses a trigram language model. The translation model is tree-tostring syntax-based model with a standard contextfree translation forest. The constraint matrix A is based on the constraints described by Rush and Collins (2011). Our decoders use a two-pass architecture. The first pass sets up the hypergraph in memory, and the second pass runs search. When possible the baselines share optimized construction and search code. The performance of optimal beam search is dependent on the sequences α and Q. For the stepsize α we used a variant of Polyak’s rule (Polyak, 1987; Boyd and Mutapcic, 2007), substituting the unknown optimal score for the last computed lower bound: αk ← ub �Ax(k )-b��lb(JJ22 We adjust the order of | the pruning parameter Q based on a function p of the current gap: Qk ← 10µ(ub(k)−lb(k)) 7.2 Baseline Methods The experiments compare optimal beam search (OPTBEAM) to several different decoding methods. For both systems we compare to: BEAM, the beam search decoder from Figure 3 using the original weights B and T, and Q ∈ {100,1000}; LRTIGHT, Lagrangian relaxation followed by increFigure 7: Two graphs from phrase-based decoding. Graph (a) shows</context>
</contexts>
<marker>Polyak, 1987</marker>
<rawString>Boris Polyak. 1987. Introduction to Optimization. Optimization Software, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>James Clarke</author>
</authors>
<title>Revisiting optimal decoding for machine translation IBM model 4. In</title>
<date>2009</date>
<booktitle>Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,</booktitle>
<pages>5--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="27022" citStr="Riedel and Clarke, 2009" startWordPosition="4767" endWordPosition="4770">hm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phrase-based models with limited reordering (Kumar and Byrne, 2005). Exact decoding based on FST is also feasible for certain hierarchical grammars (de Gispert et al., 2010). Chang procedure OPTBEAMSTAGED(α, 3) A, ub, opt ←LAGRANGIANRELAXATION(α) if opt then return ub 0&apos; ← 0 − ATA T&apos; ← T + ATb lb(0) ← −∞ for k in 1 ... K d</context>
</contexts>
<marker>Riedel, Clarke, 2009</marker>
<rawString>Sebastian Riedel and James Clarke. 2009. Revisiting optimal decoding for machine translation IBM model 4. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 5–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>David Smith</author>
<author>Andrew McCallum</author>
</authors>
<title>Parse, price and cut: delayed column and row generation for graph based parsers.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>732--743</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="28764" citStr="Riedel et al. (2012)" startWordPosition="5064" endWordPosition="5067">imal beam search: staged and alternating. Staged runs Lagrangian relaxation to find the optimal A, uses A to compute upper bounds, and then repeatedly runs beam search with pruning sequence 31 ... 3k. Alternating switches between running a round of Lagrangian relaxation and a round of beam search with the updated A. If either produces a certificate it returns the result. and Collins (2011) and Rush and Collins (2011) develop Lagrangian relaxation-based approaches for exact machine translation. Apart from translation decoding, this paper is closely related to work on column generation for NLP. Riedel et al. (2012) and Belanger et al. (2012) relate column generation to beam search and produce exact solutions for parsing and tagging problems. The latter work also gives conditions for when beam search-style decoding is optimal. 7 Results To evaluate the effectiveness of optimal beam search for translation decoding, we implemented decoders for phrase- and syntax-based models. In this section we compare the speed and optimality of these 217 decoders to several baseline methods. 7.1 Setup and Implementation For phrase-based translation we used a German-toEnglish data set taken from Europarl (Koehn, 2005). We</context>
</contexts>
<marker>Riedel, Smith, McCallum, 2012</marker>
<rawString>Sebastian Riedel, David Smith, and Andrew McCallum. 2012. Parse, price and cut: delayed column and row generation for graph based parsers. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 732–743. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
</authors>
<title>Exact decoding of syntactic translation models through lagrangian relaxation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<volume>1</volume>
<pages>72--82</pages>
<contexts>
<context position="12354" citStr="Rush and Collins (2011)" startWordPosition="2165" endWordPosition="2168">&apos; = {x ∈ X : Ax = 1 }, and the best derivation under this phrase-based translation model has score maxxEX&apos; BTx + T. Figure 2.2 shows an example hypergraph with constraints for translating the sentence les pauvres sont demunis into English using a simple set of phrases. Even in this small example, many of the possible hyperpaths violate the constraints and correspond to invalid derivations. Example: Syntax-Based Machine Translation Syntax-based machine translation with a language model can also be expressed as a constrained hypergraph problem. For the sake of space, we omit the definition. See Rush and Collins (2011) for an indepth description of the constraint matrix used for syntax-based translation. 3 A Variant of Beam Search This section describes a variant of the beam search algorithm for finding the highest-scoring constrained hyperpath. The algorithm uses three main techniques: (1) dynamic programming with additional signature information to satisfy the constraints, (2) beam pruning where some, possibly optimal, hypotheses are discarded, and (3) branch-andbound-style application of upper and lower bounds to discard provably non-optimal hypotheses. Any solution returned by the algorithm will be a va</context>
<context position="28564" citStr="Rush and Collins (2011)" startWordPosition="5035" endWordPosition="5038">rn lb(k) return maxkE{1...K} lb(k) � α1 ... αK sequence of subgradient rates Input: 31 ... 3K sequence of pruning parameters Output: optimal constrained score or lower bound Figure 6: Two versions of optimal beam search: staged and alternating. Staged runs Lagrangian relaxation to find the optimal A, uses A to compute upper bounds, and then repeatedly runs beam search with pruning sequence 31 ... 3k. Alternating switches between running a round of Lagrangian relaxation and a round of beam search with the updated A. If either produces a certificate it returns the result. and Collins (2011) and Rush and Collins (2011) develop Lagrangian relaxation-based approaches for exact machine translation. Apart from translation decoding, this paper is closely related to work on column generation for NLP. Riedel et al. (2012) and Belanger et al. (2012) relate column generation to beam search and produce exact solutions for parsing and tagging problems. The latter work also gives conditions for when beam search-style decoding is optimal. 7 Results To evaluate the effectiveness of optimal beam search for translation decoding, we implemented decoders for phrase- and syntax-based models. In this section we compare the spe</context>
<context position="30171" citStr="Rush and Collins (2011)" startWordPosition="5282" endWordPosition="5285">he unconstrained hypergraph includes further derivation information similar to the graph described in Chang and Collins (2011). For syntax-based translation we used a Chineseto-English data set. The model and hypergraphs come from the work of Huang and Mi (2010). We tested on 691 sentences from the newswire portion of the 2008 NIST MT evaluation test set. For experiments, the syntax-based model uses a trigram language model. The translation model is tree-tostring syntax-based model with a standard contextfree translation forest. The constraint matrix A is based on the constraints described by Rush and Collins (2011). Our decoders use a two-pass architecture. The first pass sets up the hypergraph in memory, and the second pass runs search. When possible the baselines share optimized construction and search code. The performance of optimal beam search is dependent on the sequences α and Q. For the stepsize α we used a variant of Polyak’s rule (Polyak, 1987; Boyd and Mutapcic, 2007), substituting the unknown optimal score for the last computed lower bound: αk ← ub �Ax(k )-b��lb(JJ22 We adjust the order of | the pruning parameter Q based on a function p of the current gap: Qk ← 10µ(ub(k)−lb(k)) 7.2 Baseline </context>
<context position="31470" citStr="Rush and Collins (2011)" startWordPosition="5507" endWordPosition="5510">ferent decoding methods. For both systems we compare to: BEAM, the beam search decoder from Figure 3 using the original weights B and T, and Q ∈ {100,1000}; LRTIGHT, Lagrangian relaxation followed by increFigure 7: Two graphs from phrase-based decoding. Graph (a) shows the duality gap distribution for 1,824 sentences after 0, 5, and 10 rounds of LR. Graph (b) shows the % of certificates found for sentences with differing gap sizes and beam search parameters Q. Duality gap is defined as, ub - (BTx* + T). mental tightening constraints, which is a reimplementation of Chang and Collins (2011) and Rush and Collins (2011). For phrase-based translation we compare with: MOSES-GC, the standard Moses beam search decoder with Q ∈ {100,1000} (Koehn et al., 2007); MOSES, a version of Moses without gap constraints more similar to BEAM (see Chang and Collins (2011)); ASTAR, an implementation of A∗ search using original outside scores, i.e. OUTSIDE(B, T), and capped at 20,000,000 queue pops. For syntax-based translation we compare with: ILP, a general-purpose integer linear programming solver (Gurobi Optimization, 2013) and CUBEPRUNING, an approximate decoding method similar to beam search (Chiang, 2007), tested with Q </context>
</contexts>
<marker>Rush, Collins, 2011</marker>
<rawString>Alexander M Rush and Michael Collins. 2011. Exact decoding of syntactic translation models through lagrangian relaxation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, volume 1, pages 72–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
</authors>
<title>A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>45--305</pages>
<contexts>
<context position="23044" citStr="Rush and Collins (2012)" startWordPosition="4067" endWordPosition="4070">OTx* + T Property 4.2 (Primal Certificate). If the hyperpath x is a member of X&apos;, i.e. Ax = b, then L(A) = OTx* + T. Property 4.1 states that L(A) always produces some upper bound; however, to help beam search, we want as tight a bound as possible: mina L(A). The Lagrangian relaxation algorithm, shown in Figure 5, uses subgradient descent to find this minimum. The subgradient of L(A) is Ax − b where x is the argmax of the modified objective x = arg maxxEX O&apos;Tx + T&apos;. Subgradient descent iteratively solves unconstrained hypergraph search problems to compute these subgradients and updates A. See Rush and Collins (2012) for an extensive discussion of this style of optimization in natural language processing. Example: Phrase-based Relaxation. For phrasebased translation, we expand out the Lagrangian to The weight of each edge O(e) is modified by the dual variables Ai for each source word translated by the edge, i.e. if (q, r, j, k) = p(e), then the score is modified by Pke Ai. A solution under these weights may use source words multiple times or not at all. However if the solution uses each source word exactly once (Ax = 1), then we have a certificate and the solution is optimal. 4.2 Utilizing Upper Bounds in</context>
</contexts>
<marker>Rush, Collins, 2012</marker>
<rawString>Alexander M Rush and Michael Collins. 2012. A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing. Journal of Artificial Intelligence Research, 45:305–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Word reordering and a dynamic programming beam search algorithm for statistical machine translation.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="26588" citStr="Tillmann and Ney, 2003" startWordPosition="4695" endWordPosition="4698">l improve the lower bound lb. This motivates the alternating algorithm OPTBEAM shown Figure 6. In each round, the algorithm alternates between computing subgradients to tighten ubs and running beam search to maximize lb. In early rounds we set 3 for aggressive beam pruning, and as the upper bounds get tighter, we loosen pruning to try to get a certificate. If at any point either a primal or dual certificate is found, the algorithm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. E</context>
</contexts>
<marker>Tillmann, Ney, 2003</marker>
<rawString>Christoph Tillmann and Hermann Ney. 2003. Word reordering and a dynamic programming beam search algorithm for statistical machine translation. Computational Linguistics, 29(1):97–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>Efficient dynamic programming search algorithms for phrase-based SMT.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing, CHSLP ’06,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="15951" citStr="Tillmann (2006)" startWordPosition="2773" endWordPosition="2774">c checks. The algorithm invokes the black-box function, PRUNE, on line 13, passing it a pruning parameter β and a vertex-signature pair. The parameter β controls a threshold for pruning. For instance for phrase-based translation, it specifies a hard-limit on the number of hypotheses to retain. The function returns true if it prunes from the chart. Note that pruning may remove optimal hypotheses, so we set the certificate flag opt to false if the chart is modified. 2For simplicity we write this loop over the entire set. In practice it is important to use data structures to optimize lookup. See Tillmann (2006) and Huang and Chiang (2005). 1: procedure BEAMSEARCH(θ, τ, lb, β) 2: ubs +— OUTSIDE(θ, τ) 3: opt +— true 4: π[v, sig] +— −oo for all v E V, sig E R|b| 5: π[v, 0] +— 0 for all v E T 6: for e E E in topological order do 7: ((v2,... , v|v|), v1) +— e 8: for sig(2) ... sig(|v|) E SIGS(v2, ... , v|v|) do � s &gt; π[v1, sig] n 11: if �CHECK(sig) n s + ubs[v1] &gt; lb 12: π[v1, sig] +— s 13: if PRUNE(π, v1, sig,β) then opt +— false 14: lb0 +— π[1,c] + τ 15: return lb0, opt (V, E, θ, τ) hypergraph with weights (A, b) matrix and vector for constraints lb E R lower bound β a pruning parameter � lb0 resulting</context>
<context position="26605" citStr="Tillmann, 2006" startWordPosition="4699" endWordPosition="4700">d lb. This motivates the alternating algorithm OPTBEAM shown Figure 6. In each round, the algorithm alternates between computing subgradients to tighten ubs and running beam search to maximize lb. In early rounds we set 3 for aggressive beam pruning, and as the upper bounds get tighter, we loosen pruning to try to get a certificate. If at any point either a primal or dual certificate is found, the algorithm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding alg</context>
</contexts>
<marker>Tillmann, 2006</marker>
<rawString>Christoph Tillmann. 2006. Efficient dynamic programming search algorithms for phrase-based SMT. In Proceedings of the Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing, CHSLP ’06, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Left-to-right target generation for hierarchical phrase-based translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>777--784</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="26702" citStr="Watanabe et al., 2006" startWordPosition="4712" endWordPosition="4716">lgorithm alternates between computing subgradients to tighten ubs and running beam search to maximize lb. In early rounds we set 3 for aggressive beam pruning, and as the upper bounds get tighter, we loosen pruning to try to get a certificate. If at any point either a primal or dual certificate is found, the algorithm returns the optimal solution. 6 Related Work Approximate methods based on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phra</context>
</contexts>
<marker>Watanabe, Tsukada, Isozaki, 2006</marker>
<rawString>Taro Watanabe, Hajime Tsukada, and Hideki Isozaki. 2006. Left-to-right target generation for hierarchical phrase-based translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 777–784, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mikhail Zaslavskiy</author>
<author>Marc Dymetman</author>
<author>Nicola Cancedda</author>
</authors>
<title>Phrase-based statistical machine translation as a traveling salesman problem.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ’09,</booktitle>
<pages>333--341</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="27093" citStr="Zaslavskiy et al. (2009)" startWordPosition="4779" endWordPosition="4782">ed on beam search and cube-pruning have been widely studied for phrasebased (Koehn et al., 2003; Tillmann and Ney, 2003; Tillmann, 2006) and syntax-based translation models (Chiang, 2007; Huang and Chiang, 2007; Watanabe et al., 2006; Huang and Mi, 2010). There is a line of work proposing exact algorithms for machine translation decoding. Exact decoders are often slow in practice, but help quantify the errors made by other methods. Exact algorithms proposed for IBM model 4 include ILP (Germann et al., 2001), cutting plane (Riedel and Clarke, 2009), and multi-pass A* search (Och et al., 2001). Zaslavskiy et al. (2009) formulate phrase-based decoding as a traveling salesman problem (TSP) and use a TSP decoder. Exact decoding algorithms based on finite state transducers (FST) (Iglesias et al., 2009) have been studied on phrase-based models with limited reordering (Kumar and Byrne, 2005). Exact decoding based on FST is also feasible for certain hierarchical grammars (de Gispert et al., 2010). Chang procedure OPTBEAMSTAGED(α, 3) A, ub, opt ←LAGRANGIANRELAXATION(α) if opt then return ub 0&apos; ← 0 − ATA T&apos; ← T + ATb lb(0) ← −∞ for k in 1 ... K do lb(k), opt ← BEAMSEARCH(0&apos;, T&apos;, lb(k−1), 3k) if opt then return lb(k)</context>
</contexts>
<marker>Zaslavskiy, Dymetman, Cancedda, 2009</marker>
<rawString>Mikhail Zaslavskiy, Marc Dymetman, and Nicola Cancedda. 2009. Phrase-based statistical machine translation as a traveling salesman problem. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ’09, pages 333–341, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
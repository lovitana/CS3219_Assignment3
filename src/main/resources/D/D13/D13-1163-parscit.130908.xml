<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9972195">
Lexical Chain Based Cohesion Models for
Document-Level Statistical Machine Translation
</title>
<author confidence="0.999751">
Deyi Xiong1, Yang Ding2, Min Zhang1* and Chew Lim Tan2
</author>
<affiliation confidence="0.999873">
1School of Computer Science and Technology, Soochow University, Suzhou, China 215006
</affiliation>
<email confidence="0.865944">
{dyxiong, minzhang}@suda.edu.cn
</email>
<affiliation confidence="0.989557">
2School of Computing, National University of Singapore, Singapore 117417
</affiliation>
<email confidence="0.997075">
{a0082379, tancl}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.995628" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9944892">
Lexical chains provide a representation of the
lexical cohesion structure of a text. In this pa-
per, we propose two lexical chain based co-
hesion models to incorporate lexical cohesion
into document-level statistical machine trans-
lation: 1) a count cohesion model that rewards
a hypothesis whenever a chain word occurs in
the hypothesis, 2) and a probability cohesion
model that further takes chain word transla-
tion probabilities into account. We compute
lexical chains for each source document to be
translated and generate target lexical chains
based on the computed source chains via max-
imum entropy classifiers. We then use the
generated target chains to provide constraints
for word selection in document-level machine
translation through the two proposed lexical
chain based cohesion models. We verify the
effectiveness of the two models using a hier-
archical phrase-based translation system. Ex-
periments on large-scale training data show
that they can substantially improve translation
quality in terms of BLEU and that the prob-
ability cohesion model outperforms previous
models based on lexical cohesion devices.
</bodyText>
<sectionHeader confidence="0.999317" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.985790951219513">
Given a source document, traditionally most statisti-
cal machine translation (SMT) systems translate the
document sentence by sentence. In such a transla-
tion scheme, sentences are translated independent
of any other sentences. However, a text is normally
written cohesively, in which sentences are connected
*Corresponding author
to each other via syntactic and lexical devices. This
linguistic phenomenon is called as textual cohesion
(Halliday and Hasan, 1976).
Cohesion is a surface-level property of well-
formed texts. It deals with five categories of rela-
tionships between text units, namely co-reference,
ellipsis, substitution, conjunction and lexical cohe-
sion that is realized via semantically related words.
The former four cohesion relations can be grouped
as grammatical cohesion. Generally speaking,
grammatical cohesion is less common and harder
to identify than lexical cohesion (Barzilay and El-
hadad, 1997).
As most SMT systems translate a text in a
sentence-by-sentence fashion, they tend to build less
lexical cohesion than human translators (Wong and
Kit, 2012). We therefore study lexical cohesion for
document-level translation. We use lexical chains
(Morris and Hirst, 1991) to capture lexical cohe-
sion in a text. Lexical chains are connected graphs
that represent the lexical cohesion structure of a text.
They have been successfully used for information
retrieval (Stairmand, 1996), document summariza-
tion (Barzilay and Elhadad, 1997) and so on. In this
paper, we investigate how lexical chains can be used
to incorporate lexical cohesion into document-level
translation.
Our basic assumption is that the lexical chains of
a target document are direct correspondences of the
lexical chains of its counterpart source document.
This assumption is reasonable as the target docu-
ment translation should be faithful to the source doc-
ument in terms of both text meaning and structure.
Based on this assumption, we propose a framework
</bodyText>
<page confidence="0.739482">
1563
</page>
<note confidence="0.7261205">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1563–1573,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.987023666666667">
to incorporate lexical cohesion into target document
translation via lexical chains, which works as fol-
lows.
</bodyText>
<listItem confidence="0.937017888888889">
• Compute lexical chains for each source docu-
ment that is to be translated;
• Project the computed source lexical chains onto
the corresponding target document by translat-
ing source chain words into target chain words
using maximum entropy classifiers;
• Incorporate lexical cohesion into the target doc-
ument translation via cohesion models built on
the projected target lexical chains .
</listItem>
<bodyText confidence="0.999965363636364">
We build two lexical chain based cohesion mod-
els. The first model is a count model that rewards a
hypothesis whenever a word in the projected target
lexical chains occur in the hypothesis. As a source
chain word may be translated into many different
target words, we further extend the count model to
a second cohesion model: a probability model that
takes chain word translation probabilities into ac-
count.
We test the two lexical chain based cohesion mod-
els on a hierarchical phrase-based SMT system that
is trained with large-scale Chinese-English bilin-
gual data. Experiment results show that our lexi-
cal chain based cohesion models can achieve sub-
stantial improvements over the baseline. Further-
more, the probability cohesion model is better than
the count model and it also outperforms previous
cohesion models based on lexical cohesion devices
(Xiong et al., 2013).
To the best of our knowledge, this is the first at-
tempt to explore lexical chains for statistical ma-
chine translation. The remainder of this paper is or-
ganized as follows. Section 2 discusses related work
and highlights the differences between our method
and previous work. Section 3 briefly introduces
lexical chains and algorithms that compute lexical
chains. Section 4 elaborates the proposed lexical
chain based framework, including details on source
lexical chain computation, target lexical chain gen-
eration and the two lexical chain based cohesion
models. Section 5 presents our large-scale experi-
ments and results. Finally, we conclude with future
directions in Section 6.
</bodyText>
<sectionHeader confidence="0.999345" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999887692307692">
Recent years have witnessed growing research in-
terests in document-level statistical machine trans-
lation. Such research efforts can be roughly di-
vided into two groups: 1) general document-level
machine translation that does not explore or ex-
plores very little linguistic discourse information;
2) linguistically-motivated document-level machine
translation that incorporates discourse information
such as cohesion and coherence into SMT. Recent
studies (Guillou, 2013; Beigman Klebanov and
Flor, 2013) show that this discourse information is
very important for document-level machine transla-
tion.
</bodyText>
<sectionHeader confidence="0.439365" genericHeader="method">
General Document-Level Machine Translation
</sectionHeader>
<bodyText confidence="0.999282">
Tiedemann (2010) propose cache-based language
and translation models for document-level machine
translation. These models are built on recently trans-
lated sentences. Following this cache-based ap-
proach, Gong et al. (2011) further introduce two
additional caches. They use a static cache to store
bilingual phrases extracted from documents in train-
ing data that are similar to the document being trans-
lated. They also adopt a topic cache with target
language topic words. Xiao et al. (2011) study
the translation consistency issue in document-level
machine translation. They use a hard constraint to
consistently translate ambiguous source words into
the most frequent translation options. Ture et al.
(2012) soften this consistency constraint by integrat-
ing three counting features into decoder.
</bodyText>
<subsectionHeader confidence="0.723643">
Using Lexical Cohesion Devices in Document-
</subsectionHeader>
<bodyText confidence="0.999505133333333">
Level SMT Lexical cohesion devices are seman-
tically related words, including word repetition,
synonyms/near-synonyms, hyponyms and so on.
They are also the cohesion-building elements in lex-
ical chains.
Wong and Kit (2012) use lexical cohesion device
based metrics to improve machine translation evalu-
ation at the document level. These metrics measure
the proportion of content words that are used as lex-
ical cohesion devices in machine-generated transla-
tions. Hardmeier et al. (2012) propose a document-
wide phrase-based decoder and integrate a semantic
language model into the decoder. They argue that
their semantic language model can capture lexical
cohesion by exploring n-grams that cross sentence
</bodyText>
<page confidence="0.990576">
1564
</page>
<bodyText confidence="0.999215368421053">
boundaries.
Most recently Xiong et al. (2013) integrate
three categories of lexical cohesion devices into
document-level machine translation. They define
three cohesion models based on lexical cohesion de-
vices: a direct reward model, a conditional probabil-
ity model and a mutual information trigger model.
The latter two models measure the strength of lexical
cohesion relation between two lexical items. They
are incorporated into SMT to calculate how appro-
priately lexical cohesion devices are used in doc-
ument translation. As lexical chains capture lexi-
cal cohesion relations among sequences of related
words rather than those only between two words, ex-
periments in Section 5 show that our lexical chain
based probability cohesion model is better than the
lexical cohesion device based trigger model, which
is the best among the three cohesion models pro-
posed by Xiong et al. (2013).
</bodyText>
<subsectionHeader confidence="0.464263">
Modeling Coherence in Document-Level SMT
</subsectionHeader>
<bodyText confidence="0.99989925">
In discourse analysis, cohesion is often studied to-
gether with coherence which is another dimension
of the linguistic structure of a text (Barzilay and
Elhadad, 1997). Cohesion is related to the sur-
face structure of a text while coherence is concerned
with the underlying meaning connectedness in a text
(Vasconcellos, 1989). Compared with cohesion, co-
herence is not easy to be detected. Even so, various
models have been proposed to explore coherence for
document summarization and generation (Barzilay
and Lapata, 2008; Louis and Nenkova, 2012). Fol-
lowing this line, Xiong and Zhang (2013) integrate
a topic-based coherence model into document-level
machine translation, where coherence is defined as a
continuous sentence topic transition.
Our lexical chain based cohesion models are also
related to previous work on using word and phrase
sense disambiguation for lexical choice in SMT
(Carpuat and Wu, 2007b; Carpuat and Wu, 2007a;
Chan et al., 2007). The difference is that we use
document-wide lexical chains to build our cohesion
models rather than sentence-level context features.
In our framework, lexical choice is performed to
make the selected words consistent with the lexical
cohesion structure of a document.
Carpuat (2009) explores the principle of one sense
per discourse (Gale et al., 1992) in the context of
SMT and imposes the constraint of one translation
per discourse on document translation. We also
use the one sense per discourse principle to perform
word sense disambiguation on the source side in our
lexical chaining algorithm (See Section 4.1).
</bodyText>
<sectionHeader confidence="0.9714235" genericHeader="method">
3 Background: Lexical Chain and Chain
Computation
</sectionHeader>
<bodyText confidence="0.999068363636364">
Lexical chains are sequences of semantically related
words (Morris and Hirst, 1991). They represent the
lexical cohesion structure of a text. Figure 2 displays
six lexical chains computed from the Chinese news
article shown in Figure 1. Words in these lexical
chains have lexical cohesion relations such as rep-
etition, synonym, which may range over the entire
text. For example, in the lexical chain LC1 of Fig-
ure 2, the same word “d6gu6” (Germany) repeats
9 times. In the lexical chain LC3, the two words
“z6ngc6i” (president) and “zhuxi” (chairman) are
synonym words. Generally, a text can have many
different lexical chains, each of which represents a
thread of cohesion through the text.
Several lexical chaining algorithms have been
proposed to compute lexical chains from texts. Nor-
mally they need an ontology to obtain semantic re-
lations between words. Word sense disambiguation
(WSD) is also used to determine the sense of each
word in a text. Generally a lexical chain compu-
tation algorithm completes the following three sub-
tasks:
</bodyText>
<listItem confidence="0.989526">
• Building a representation of a text with a set
of candidate words and assigning semantic re-
lations between the candidate words according
to the ontology;
• Choosing the right sense for each candidate
word via WSD;
• Building chains over the semantically related
and disambiguated candidate words.
</listItem>
<bodyText confidence="0.993326777777778">
These three sub-tasks can be done separately or si-
multaneously.
Morris and Hirst (Morris and Hirst, 1991) de-
fine the first lexical chain computation algorithm
that adopts a greedy strategy to immediately disam-
biguate a word at its first occurrence. This algo-
rithm runs in linear time but suffers from inaccu-
rate disambiguation. Barzilay and Elhadad (Barzi-
lay and Elhadad, 1997) significantly improve WSD
</bodyText>
<page confidence="0.650109">
1565
</page>
<bodyText confidence="0.959444625">
dWgu_ diUnx]n g^ngsZ z`ngcSi su`ma c[zh[
dWgu_ diUnx]n g^ngsZ xuRnbe , qiSn jiRnsh]hu] zhdx[ qZsh[Yr su] de xZlYXr jiRng dRnrYn gRi
g^ngsZ de l[nsh[ z`ngcSi , wWiqZ lie gY yuY , zh[dUo su`ma de j]rYn rWnxuTn jiVrYn wWizh\ o
( fTxZnshY b^ Sng diUn ) dWgu_ diUnx]n g^ngsZ z`ngcSi su`ma jZntiRn c[qe tR de zh[we , tR
shu^ , y_uyc tR xiTnrSn bc zUi shaudUo dWgu_ diUnx]n g^ngsZ jiRnsh]hu] de ch^ngfYn x]nrYn ,
c[zh[ sh] tR wWiyZ de xuTnzW o
t_uzZrWn huRny[ng zhYxiUng xuRnbe , dWgu_ diUnx]n g^ngsZ de gdpiUo yZnc\ zUi fTlSnkYfc
gdpiUo jiRoy] sh]chTng shUng zhTng bTifYnzhZsh[yZ y\shUng o
su`ma zUi dWgu_ diUnx]n g^ngsZ b^ Sng z`ngbe zhUokRi jiRnsh]hu] tYbiW hu]y] zh^ng fRbiTo
yZ xiUng shVngm[ng , tR shu^ : [ w` y\ yUoqic jiRnsh]hu] jiXchc w` de zh[we o I
y_uyc liTng gY yuY hau jiRng jdx[ng dUxuTn , dUn liSnhWzhYngfd zUi m[n diUo zh^ng shVngwUng
luahau , dWgu_ z`ngl\ shZ ruadW s]hb xZwUng zUi gdjiU xiUcua zh] xZn dZ sh[ , dWgu_ diUnx]n
g^ngsZ shebTiwUn m[ng xiTo gdd^ng de zZjZn b]ng wYi xiRoshZ , Wr zhZch[ tR o
dWgu_ diUnx]n gdjiU haulSi hu[ wXn , y\ sh[yZdiTnyZbR ^uyuSn zua sh^u , shUngzhTng
bTifYnzhZbRdiTnwds] o
dWgu_ cSizhYngbe huRny[ng su`ma c[zh[ de juWd]ng o
</bodyText>
<figureCaption confidence="0.995644">
Figure 1: An example of a Chinese news article (written in pinyin).
</figureCaption>
<table confidence="0.668732375">
LC1: {dWgu_, dWgu_, b^, dWgu_, dWgu_, dWgu_,
dWgu_, b^, dWgu_, dWgu_, dWgu_}
LC2:{jiRnsh]hu], fTxZnshY, jiRnsh]hu], z`ngbe,
jiRnsh]hu], jiRnsh]hu]}
LC3: {z`ngcSi, zhdx[, z`ngcSi, z`ngl\}
LC4: {c[zh[, c[qe, c[zh[, c[zh[}
LC5: {zhTng, xiUcua, shUngzhTng}
LC6: {xuRnbe, xuRnbe, fRbiTo}
</table>
<figureCaption confidence="0.9832975">
Figure 2: Six lexical chains from the example in Figure
1.
</figureCaption>
<bodyText confidence="0.999869777777778">
accuracy by processing all possible combinations of
word senses in a text to disambiguate words. Un-
fortunately, their algorithm runs slowly in quadratic
time. Galley and Mckeown (2003) present an algo-
rithm that are better than the former two algorithms
both in terms of running efficiency and WSD accu-
racy. They separate the WSD sub-task from the task
of lexical chain building and impose a “one sense
per discourse” constraint in the WSD step.
</bodyText>
<sectionHeader confidence="0.975324" genericHeader="method">
4 Translating Documents Using Lexical
Chains
</sectionHeader>
<bodyText confidence="0.999931333333333">
In this section, we describe how we incorporate lex-
ical cohesion into document-level machine transla-
tion using lexical chains. We divide the lexical chain
based document-level machine translation process
into three steps: (1) computing lexical chains for
source documents with a source language ontology,
(2) generating target lexical chains from the com-
puted source lexical chains, and finally (3) incorpo-
rating lexical cohesion encoded in the generated tar-
get lexical chains into document-level translation via
lexical chain based cohesion models. The remainder
of this section will elaborate these three steps.
</bodyText>
<subsectionHeader confidence="0.998767">
4.1 Source Lexical Chains Computation
</subsectionHeader>
<bodyText confidence="0.999982055555556">
We follow the chain computation algorithm intro-
duced by Galley and McKeown (2003) to build lex-
ical chains on source (Chinese) documents. In the
algorithm, the chaining process includes three steps:
choosing candidate words to build a disambiguation
graph (Galley and McKeown, 2003) for each doc-
ument, disambiguating the candidate words and fi-
nally building lexical chains over the disambiguated
candidate words.
The disambiguation graph can be considered as
a representation of all possible interpretations of its
corresponding text. In the graph, nodes are candi-
date words with different senses and edges between
word senses are weighted according to their seman-
tic relations, such as synonym, hypernym and so on.
We use an extended version of a Chinese thesaurus
Tongyici Cilin (Cilin for short) to define word senses
and semantic relations between senses. The ex-
</bodyText>
<page confidence="0.955419">
1566
</page>
<bodyText confidence="0.9994192">
level 1
level 2
level 3
level 4
level 5
</bodyText>
<figureCaption confidence="0.989635">
Figure 3: The architecture of the extended Cilin. For sim-
plicity, we only draw a binary tree to represent the hier-
archical structure of Cilin. This doesn’t mean that each
semantic class at level i has only two sub-classes at level
i + 1. Actually, they have multiple sub-classes.
</figureCaption>
<bodyText confidence="0.998185916666667">
tended Cilin contains 77,343 Chinese words, which
are organized in a hierarchical structure containing
5 levels as shown in Figure 3. In the 5th level, each
node represents an atomic concept which consists of
a set of synonyms. These atomic concepts are just
like synsets in WordNet. We use them to represent
senses of words in the disambiguation graph.
We select nouns, verbs, abbreviations and idioms
as candidate words for the disambiguation graph.
These words are identified by a Chinese part-to-
speech tagger LTP (Che et al., 2010) in a preprocess-
ing step. In order to build the disambiguation graph,
we first build an array indexed by the atomic con-
cepts of Cilin, then insert a copy of each candidate
word into its all concept (sense) entries in the array.
After that, we create all semantic links among senses
of different candidate words in the disambiguation
graph following Galley and McKeown (2003).
In the second step, we use the principle of one
sense per discourse to perform WSD for each can-
didate word in the disambiguation graph. We sum
the weights of all semantic links under the different
senses of the candidate word in question. The sense
with the highest sum of weights is considered as the
most probable sense for this word. We then assign
this sense to all occurrences of the word in the doc-
ument by adopting the constraint of one sense per
discourse.
Once all candidate words are disambiguated, we
can build lexical chains over these words by remov-
ing all semantic links that connect those unselected
word senses. The six lexical chains shown in Fig-
ure 2 are computed from the Chinese document in
Figure 1 exactly following the algorithm of Galley
and McKeown (2003). The only difference is that
we use Cilin rather than WordNet as the ontology.
</bodyText>
<subsectionHeader confidence="0.989518">
4.2 Target Lexical Chains Generation
</subsectionHeader>
<bodyText confidence="0.999841916666667">
Since a faithful target document translation should
follow the same cohesion structure as that in its cor-
responding source document, we generate target lex-
ical chains from the computed source lexical chains.
Given a source lexical chain LCs = {sji } where the
ith chain word sji is from the jth sentence of the
source document Ds, we generate a target lexical
chain LCt = {tji } using maximum entropy (Max-
Ent) classifiers. Particularly, we translate a word sji
in the source lexical chain into a target word tji in
the target lexical chain using a corresponding Max-
Ent classifier as follows1.
</bodyText>
<equation confidence="0.989800333333333">
exp(Ek θkfk(tji,C(sz)))
P( |C(sji )) =E (1)
t exp(Ek θkfk(t, C(sji)))
</equation>
<bodyText confidence="0.989293863636364">
where fk are binary features, θk are weights of these
features, and C(sji) is the surrounding context of
chain word sji .
We train one MaxEnt classifier per unique source
chain word. For each classifier, we define two
groups of binary features: 1) the preceding and
succeeding two words of sji in the jth sentence
({w_2, w_1, sji, w+1, w+2}); 2) the preceding and
succeeding one word of sji in the lexical chain LCs
({spi_1, sji, sqi+1}). All features are in the following
binary form.
f(ti,C(sji)) =� 1, if tji = 46 and C(sz).CG= �
0, else
(2)
where the symbol 46 is a placeholder for a possible
target word, the symbol CG indicates a contextual ele-
ment for the chain word sj i (e.g., the preceding word
in the jth sentence or the succeeding word in the
lexical chain LCs), and the symbol 4 represents the
value of Q.
Given a source document Ds and its N lexical
chains {LCks }Nk=1 computed from the document as
</bodyText>
<footnote confidence="0.996974">
1We collect training instances from word-aligned bilingual
data to train the MaxEnt classifier.
</footnote>
<page confidence="0.991552">
1567
</page>
<bodyText confidence="0.999947857142857">
described in Section 4.1, we can generate the N
target lexical chains {LCkt }Nk�1 using our MaxEnt
classifiers. Each target word tji in the target lexi-
cal chain LCkt is the translation of its corresponding
source word sji in the source lexical chain LCks with
the highest probability P(tji |C(sji )) according to Eq.
(1).
As we know, the MaxEnt classifier can gen-
erate multiple translations for each source word.
In order to incorporate these multiple chain word
translations, we can generate a super target lexi-
cal chain ELCt from a source lexical chain LCs,
where E is a pre-defined threshold used to se-
lect multiple translations. For example, given a
source lexical chain LCs = {a, b, c}, we can
have the corresponding super target lexical chain
ELCt = {{a1t, at...}, {b1t, bt...}, {c1t, ct...}}, where
xit is the translation of x with a translation probabil-
ity P(xit|C(x)) &gt; E according to Eq. (1). Integrat-
ing multiple translations for each source chain word,
we can reduce the error propagation of the MaxEnt
classifier to some extent. Our experiments also con-
firm that the super target lexical chains with multi-
ple translation options for each chain word are better
than the target lexical chains with only one transla-
tion per chain word. Therefore we build our cohe-
sion models based on the super target lexical chains,
which will be described in the next section.
</bodyText>
<subsectionHeader confidence="0.997595">
4.3 Lexical Chain Based Cohesion Models
</subsectionHeader>
<bodyText confidence="0.998562857142857">
Once we generate the super target lexical chains
{ELCkt }Nk�1 for the target document Dt, we can use
them to provide constraints for the target document
translation. Our key interest is to make the target
document translation TDt as cohesive as possible.
We therefore propose lexical chain based cohesion
models to measure the cohesion of the target docu-
ment translation. The basic idea is to reward a trans-
lation hypothesis if a word from the super target lexi-
cal chains occurs in the hypothesis. According to the
difference in the reward strategy, we have two cohe-
sion models: a count cohesion model and a proba-
bility cohesion model.
Count Cohesion Model Mc(TDt, {ELCkt }Nk�1):
This model rewards a translation hypothesis of the
jth sentence in the document whenever a lexical
chain word tji occurs in the hypothesis. The model
maintains a counter and accumulates the counter
when necessary. It is factorized into the sentence
cohesion metric Mc(Tj, {ELCkt }Nk�1), where Tj is
the translation of the jth sentence in the target docu-
</bodyText>
<equation confidence="0.7906265">
ment. Mc(Tj, {ELCkt }Nk�1) is formulated as follows.
ri
Mc(Tj, {�LCk t }N k�1) =
wETj
</equation>
<bodyText confidence="0.957132">
where C represents {ELCkt }Nk�1, and the δ function
is defined as follows.
</bodyText>
<equation confidence="0.922098">
δw j 1, if tz = w 4
( ti) _ { 0, otherwise ( )
</equation>
<sectionHeader confidence="0.490376" genericHeader="method">
Probability Cohesion Model
</sectionHeader>
<bodyText confidence="0.999152">
Mp(TDt, {ELCkt }Nk�1): This model rewards a
translation hypothesis according to the translation
probability of a chain word that occurs in the
hypothesis. The translation probability is computed
by Eq. (1). The model is also factorized into the
sentence cohesion metric Mp(Tj, {ELCkt }Nk�1)
which is formulated as follows.
</bodyText>
<equation confidence="0.997121">
Mp(Tj,{ELCkt }Mk�1) =
eδ(w,tji) x P(tj i |C(sj i )) (5)
</equation>
<bodyText confidence="0.9995365">
where P(tji |C(sji ) is the translation probability com-
puted according to Eq. (1).
</bodyText>
<subsectionHeader confidence="0.992547">
4.4 Decoding
</subsectionHeader>
<bodyText confidence="0.999987473684211">
The proposed lexical chain based cohesion models
are integrated into the log-linear translation frame-
work of SMT as a cohesion feature. Before translat-
ing a source document, we compute lexical chains
for the source document as described in Section 4.1.
We then generate the super target lexical chains. In
order to efficiently calculate our lexical chain based
cohesion models, we reorganize words in the super
target lexical chains into vectors. We associate each
source sentence Sj a vector to store target lexical
chain words that are to occur in the corresponding
target sentence Tj.
Although we still translate a source document
sentence by sentence, we capture the global cohe-
sion structure of the document via lexical chains and
use the lexical chain based cohesion models to con-
strain word selection in document translation. Fig-
ure 4 shows the architecture of an SMT system with
the lexical chain based cohesion model.
</bodyText>
<equation confidence="0.932366">
ri eδ(w,tji) (3)
tji EC
ri
tji EC
ri
wETj
</equation>
<page confidence="0.959035">
1568
</page>
<figureCaption confidence="0.997533">
Figure 4: Architecture of an SMT system with the lexical
chain based cohesion model.
</figureCaption>
<sectionHeader confidence="0.998172" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.979746">
In this section, we conducted a series of experiments
to validate the effectiveness of the proposed lexical
chain based cohesion models for Chinese-to-English
document-level machine translation. We used a hier-
archical phrased-based SMT system (Chiang, 2007)
trained on large-scale data. In particular, we aim at:
</bodyText>
<listItem confidence="0.98798825">
• Measuring the impact of the threshold c on the
probability cohesion model and selecting the
best threshold on a development test set.
• Investigating the effect of the two lexical-chain
based cohesion models.
• Comparing our lexical chain based cohesion
models against the previous lexical cohesion
device based models (Xiong et al., 2013).
</listItem>
<subsectionHeader confidence="0.976273">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.999798083333333">
We collected our bilingual training data from
LDC, which includes the corpus LDC2002E18,
LDC2003E07, LDC2003E14, LDC2004E12,
LDC2004T07, LDC2004T08 (Only Hong Kong
News), LDC2005T06 and LDC2005T10. The
collected bilingual training data contains 3.8M
sentence pairs with 96.9M Chinese words and
109.5M English words. We trained a 4-gram
language model on the Xinhua portion of the
English Gigaword corpus (306 million words) via
the SRILM toolkit (Stolcke, 2002) with Kneser-Ney
smoothing.
</bodyText>
<table confidence="0.977698">
Training MT05 MT06 MT08
#Doc 103,236 100 79 109
#Sent 2.80M 1,082 1,664 1,357
#Chain 3.52M 1700 2172 1693
#AvgC 35.72 17 27.49 15.53
#AvgW 14.81 5.89 6.89 5.63
</table>
<tableCaption confidence="0.998959">
Table 1: Statistics of the training, development and test
</tableCaption>
<bodyText confidence="0.9966612">
sets, which show the number of documents (#Doc) and
sentences (#Sent), the number of lexical chains extracted
from the source documents (#Chain), the average number
of lexical chains per document (#AvgC) and the average
number of words per lexical chain (#AvgW).
In order to build the lexical chain based cohesion
models, we selected corpora with document bound-
aries explicitly provided from the bilingual training
data together with the whole Hong Kong parallel
text corpus as the cohesion model training data2. We
show the statistics of these selected corpora in Table
1. They contain 103,236 documents and 2.80M sen-
tences. Averagely, each document consists of 28.4
sentences. From the source documents of the se-
lected corpora, we extract 3.52M lexical chains. On
average, there are 35.72 lexical chains per document
and 14.81 words per lexical chain.
We used the off-the-shelf MaxEnt toolkit3 to train
one MaxEnt classifier per unique source lexical
chain word (61,121 different source chain words in
total). We performed 100 iterations of the L-BFGS
algorithm implemented in the training toolkit for
each chain word with both Gaussian prior and event
cutoff set to 1 to avoid overfitting. After event cutoff,
we have an average of 17.75 different classes (target
translations) per source chain word.
We used the NIST MT05 as the tuning set for the
minimum error rate training (MERT) [Och, 2003],
the NIST MT06 as the development test set and the
MT08 as the final test set. The numbers of doc-
uments/sentences in the NIST MT05, MT06 and
MT08 are 100/1082, 79/1664 and 109/1357 respec-
tively. They contain 17, 27.49 and 15.53 lexical
chains per document respectively.
We used the case-insensitive BLEU-4 (Papineni
</bodyText>
<footnote confidence="0.9959426">
2The training data includes LDC2003E14, LDC2004T07,
LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong
Hansards/Laws/News).
3Available at: http://homepages.inf.ed.ac.uk/lzhang10/
maxent toolkit.html
</footnote>
<page confidence="0.991429">
1569
</page>
<figure confidence="0.682833166666667">
c MT06
0.05 30.53
0.1 31.64
0.2 31.45
0.3 30.73
0.4 31.01
</figure>
<tableCaption confidence="0.816415666666667">
Table 2: BLEU scores of the probability cohesion
model MP(TDt, {,LCt }N 1) with different values for
the threshold c.
</tableCaption>
<table confidence="0.9999452">
System MT06 MT08 Avg
Baseline 30.43 23.32 26.88
LexChainCount(top 1) 30.46 23.52 26.99
LexChainCount 30.79 23.34 27.07
LexChainProb 31.64 24.54 28.09
</table>
<tableCaption confidence="0.9880585">
Table 3: Effects of the lexical chain based count and
probability cohesion models. LexChainCount: the count
model defined in Eq. (3). LexChainProb: the probability
model defined in Eq. (5).
</tableCaption>
<bodyText confidence="0.998575">
et al., 2002) as our evaluation metric. As MERT is
normally instable, we ran the tuning process three
times for all our experiments and presented the av-
erage BLEU scores on the three MERT runs as sug-
gested by Clark et al (2011).
</bodyText>
<subsectionHeader confidence="0.999622">
5.2 Setting the Threshold c
</subsectionHeader>
<bodyText confidence="0.999962956521739">
As the two lexical chain based cohesion models are
built on the super target lexical chains that are asso-
ciated with a parameter c, we need to tune the thresh-
old parameter c on the development test set NIST
MT06. We conducted a group of experiments using
the probability cohesion model defined in Eq. (5)
to find the best threshold. Experiment results are
shown in Table 2.
If we set the threshold too small (e.g., 0.05), the
super target lexical chains may contain too many
noisy words that are not the translations of source
lexical chain words, which may jeopardise the qual-
ity of the super target lexical chains. The cohesion
model built on these noisy super target lexical chains
may select incorrect words rather than the proper
lexical chain words. On the other hand, if we set the
threshold too large (e.g., 0.3 or 0.4), we may take
the risk of not selecting the appropriate chain word
translations into the super target lexical chains. It
seems that the best threshold is 0.1 as we obtained
the highest BLEU score 31.64 on the NIST MT06
with this threshold. Therefore we set the threshold c
to 0.1 in all experiments thereafter.
</bodyText>
<subsectionHeader confidence="0.9986395">
5.3 Effect of the Count and Probability
Cohesion Model
</subsectionHeader>
<bodyText confidence="0.996472583333333">
After we found the best threshold, we carried out ex-
periments to test the effect of the two lexical chain
based cohesion models: the count and probability
cohesion model. We compared them against the
baseline system that does not integrate any lexical
chain information. We also compared the count co-
hesion model (LexChainCount(top1)) built on the
target lexical chains where each target chain word is
the best translation of its corresponding source lex-
ical chain word according to Eq. (1). Experiment
results are shown in Table 3.
From Table 3, we can observe that
</bodyText>
<listItem confidence="0.940646285714286">
• Our lexical chain based cohesion models are
able to substantially improve the translation
quality in terms of BLEU score. We achieve
an average improvement of up to 1.21 BLEU
points over the baseline on the two test sets
MT06 and MT08.
• The count cohesion model built on the super
</listItem>
<bodyText confidence="0.5900324">
target lexical chains is better than that based
on the target lexical chains only with top one
translations (27.07 vs. 26.99). This shows
the advantage of the super target lexical chains
{,LCkt } 1 over the standard target lexical chi-
</bodyText>
<subsectionHeader confidence="0.497294">
k=ans {LCkt }N
</subsectionHeader>
<bodyText confidence="0.624440142857143">
k=1.
• Finally, the probability cohesion model is much
better than the count cohesion model (28.09
vs. 27.07). This suggests that we should take
into account chain word translation probabili-
ties when we reward hypotheses where target
lexical chain words occur.
</bodyText>
<subsectionHeader confidence="0.989915">
5.4 Lexical Chains vs. Lexical Cohesion
Devices
</subsectionHeader>
<bodyText confidence="0.999881166666667">
As we have mentioned in Section 2, lexical cohe-
sion devices can be also used to build lexical cohe-
sion models to capture lexical cohesion relations in a
text. We therefore want to compare our lexical chain
based cohesion models with the lexical cohesion de-
vice based cohesion models.
</bodyText>
<page confidence="0.951416">
1570
</page>
<table confidence="0.9994495">
System MT06 MT08 Avg
Baseline 30.43 23.32 26.88
LexDeviceTrigger 31.35 24.11 27.73
LexChainProb 31.64 24.54 28.09
</table>
<tableCaption confidence="0.796415333333333">
Table 4: The lexical chain based probability cohesion
model (LexChainProb) vs. the lexical cohesion device
based trigger model (LexDeviceTrigger).
</tableCaption>
<bodyText confidence="0.999985375">
We re-implemented the mutual information trig-
ger model that is the best lexical cohesion model
based on lexical cohesion devices among the three
models proposed by Xiong et al. (2013). The mu-
tual information trigger model measures the associ-
ation strength of two lexical cohesion items x and y
in a lexical cohesion relation xRy. In the model, it
is required that x occurs in a sentence preceding the
sentence where y occurs and that the two items have
a lexical cohesion relation such as word repetition,
synonym. The model treats x as the trigger and y as
the triggered item. The mutual information between
the trigger x and the triggered item y estimates how
possible y will occur given x is mentioned in a text.
The comparison results are reported in Table 4.
Our lexical chain based probability cohesion model
outperforms the lexical cohesion device based trig-
ger model by 0.36 BLEU points. The reason for this
superiority of our cohesion model over the trigger
model may be that the former model captures lex-
ical cohesion relations among sequences of words
through lexical chains while the latter model cap-
tures lexical cohesion relations only between two re-
lated words.
</bodyText>
<sectionHeader confidence="0.999499" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9997725">
We have presented two lexical chain based cohesion
models that incorporate the lexical cohesion struc-
ture of a text into document-level machine transla-
tion. We project the lexical chains of a source docu-
ment to the corresponding target document by trans-
lating each word in each source lexical chain into
their counterparts via MaxEnt classifiers. The pro-
jected target lexical chains provide a representation
of the lexical cohesion structure of the target doc-
ument that is to be generated. We build two co-
hesion models based on the projected target lexi-
cal chains: a count model that rewards a hypothesis
according to the time of occurrence of target lexi-
cal chain words in the hypothesis and a probability
model that further takes translation probabilities into
account when rewarding hypotheses. These two co-
hesion models are used to constrain word selection
for document translation so that the generated doc-
ument is consistent with the projected lexical cohe-
sion structure.
We have integrated the two proposed cohesion
models into a hierarchical phrase-based SMT sys-
tem. Experiment results on large-scale data validate
that
</bodyText>
<listItem confidence="0.826902166666667">
• The lexical chain based cohesion models are
able to substantially improve translation qual-
ity in terms of BLEU.
• The probability cohesion model is better than
the count cohesion model.
• The lexical chain based probability cohesion
</listItem>
<bodyText confidence="0.997803454545454">
model is better than the previous mutual infor-
mation trigger model that adopts lexical cohe-
sion devices to capture lexical cohesion rela-
tions between two related words.
As we mentioned in Section 2, cohesion is closely
connected to coherence. It provides a surface indi-
cator for coherence identification (Barzilay and El-
hadad, 1997). In the future, we would like to use
lexical chains to identify coherence and incorporate
both cohesion and coherence into document-level
machine translation.
</bodyText>
<sectionHeader confidence="0.997107" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.920170625">
Regina Barzilay and Michael Elhadad. 1997. Using lex-
ical chains for text summarization. In In Proceedings
of the ACL Workshop on Intelligent Scalable Text Sum-
marization, pages 10–17.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1–34.
Beata Beigman Klebanov and Michael Flor. 2013. As-
sociative texture is lost in translation. In Proceedings
of the Workshop on Discourse in Machine Translation,
pages 27–32, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Marine Carpuat and Dekai Wu. 2007a. How phrase
sense disambiguation outperforms word sense disam-
biguation for statistical machine translation. In Pro-
ceedings of the 11th Conference on Theoretical and
</reference>
<page confidence="0.933555">
1571
</page>
<reference confidence="0.99948125">
Methodological Issues in Machine Translation, pages
43–52.
Marine Carpuat and Dekai Wu. 2007b. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 61–72, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Marine Carpuat. 2009. One translation per discourse.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW-2009), pages 19–27, Boulder, Colorado, June.
Association for Computational Linguistics.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 33–40, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
a chinese language technology platform. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics: Demonstrations, COLING ’10,
pages 13–16, Stroudsburg, PA, USA. Association for
Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: Controlling for optimizer in-
stability. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 176–181, Port-
land, Oregon, USA, June.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural Lan-
guage, Harriman, NY, February.
Michel Galley and Kathleen McKeown. 2003. Improv-
ing word sense disambiguation in lexical chaining. In
Proceedings of the 18th international joint conference
on Artificial intelligence, IJCAI’03, pages 1486–1488,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011.
Cache-based document-level statistical machine trans-
lation. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 909–919, Edinburgh, Scotland, UK., July.
Liane Guillou. 2013. Analysing lexical consistency in
translation. In Proceedings of the Workshop on Dis-
course in Machine Translation, pages 10–18, Sofia,
Bulgaria, August. Association for Computational Lin-
guistics.
M.A.K Halliday and Ruqayia Hasan. 1976. Cohesion in
English. London: Longman.
Christian Hardmeier, Joakim Nivre, and J¨org Tiedemann.
2012. Document-wide decoding for phrase-based sta-
tistical machine translation. In Proceedings of the
2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning, pages 1179–1190, Jeju Island,
Korea, July.
Annie Louis and Ani Nenkova. 2012. A coherence
model based on syntactic patterns. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1157–1168, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator of
the structure of text. Comput. Linguist., 17(1):21–48,
March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, Pennsylva-
nia, USA, July.
M.A. Stairmand. 1996. A computational analysis of
lexical cohesion with applications in information re-
trieval. UMIST.
Andreas Stolcke. 2002. Srilm–an extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing,
pages 901–904, Denver, Colorado, USA, September.
J¨org Tiedemann. 2010. Context adaptation in statistical
machine translation using models with exponentially
decaying cache. In Proceedings of the 2010 Workshop
on Domain Adaptation for Natural Language Process-
ing, pages 8–15, Uppsala, Sweden, July.
Ferhan Ture, Douglas W. Oard, and Philip Resnik. 2012.
Encouraging consistent translation choices. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 417–
426, Montr´eal, Canada, June.
Muriel Vasconcellos. 1989. Cohesion and coherence in
the presentation of machine translation products. In
James E.Alatis, editor, Geogetown University Round
Table on Languages and Linguistics 1989, pages 89–
105, Washington, D.C. Georgetown University Press.
Billy T. M. Wong and Chunyu Kit. 2012. Extend-
ing machine translation evaluation metrics with lexi-
cal cohesion to document level. In Proceedings of the
</reference>
<page confidence="0.848366">
1572
</page>
<reference confidence="0.999010473684211">
2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning, pages 1060–1068, Jeju Island,
Korea, July.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in ma-
chine translation. In Proceedings of the 2011 MT sum-
mit XIII, pages 131–138, Xiamen, China, September.
Deyi Xiong and Min Zhang. 2013. A topic-based co-
herence model for statistical machine translation. In
Proceedings of the Twenty-Seventh AAAI Conference
on Artificial Intelligence (AAAI-13), Bellevue, Wash-
ington, USA, July.
Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan Lv,
and Qun Liu. 2013. Modeling lexical cohesion for
document-level machine translation. In Proceedings
of the Twenty-Third International Joint Conference on
Artificial Intelligence (IJCAI-13), Beijing, China, Au-
gust.
</reference>
<page confidence="0.984183">
1573
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.499874">
<title confidence="0.986353">Lexical Chain Based Cohesion Models Document-Level Statistical Machine Translation</title>
<author confidence="0.993195">Yang Min</author>
<author confidence="0.993195">Chew Lim</author>
<affiliation confidence="0.717964">of Computer Science and Technology, Soochow University, Suzhou, China</affiliation>
<address confidence="0.67667">of Computing, National University of Singapore, Singapore</address>
<abstract confidence="0.9998295">Lexical chains provide a representation of the lexical cohesion structure of a text. In this paper, we propose two lexical chain based cohesion models to incorporate lexical cohesion into document-level statistical machine translation: 1) a count cohesion model that rewards a hypothesis whenever a chain word occurs in the hypothesis, 2) and a probability cohesion model that further takes chain word translation probabilities into account. We compute lexical chains for each source document to be translated and generate target lexical chains based on the computed source chains via maximum entropy classifiers. We then use the generated target chains to provide constraints for word selection in document-level machine translation through the two proposed lexical chain based cohesion models. We verify the effectiveness of the two models using a hierarchical phrase-based translation system. Experiments on large-scale training data show that they can substantially improve translation quality in terms of BLEU and that the probability cohesion model outperforms previous models based on lexical cohesion devices.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Using lexical chains for text summarization. In</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>10--17</pages>
<contexts>
<context position="2427" citStr="Barzilay and Elhadad, 1997" startWordPosition="344" endWordPosition="348"> are connected *Corresponding author to each other via syntactic and lexical devices. This linguistic phenomenon is called as textual cohesion (Halliday and Hasan, 1976). Cohesion is a surface-level property of wellformed texts. It deals with five categories of relationships between text units, namely co-reference, ellipsis, substitution, conjunction and lexical cohesion that is realized via semantically related words. The former four cohesion relations can be grouped as grammatical cohesion. Generally speaking, grammatical cohesion is less common and harder to identify than lexical cohesion (Barzilay and Elhadad, 1997). As most SMT systems translate a text in a sentence-by-sentence fashion, they tend to build less lexical cohesion than human translators (Wong and Kit, 2012). We therefore study lexical cohesion for document-level translation. We use lexical chains (Morris and Hirst, 1991) to capture lexical cohesion in a text. Lexical chains are connected graphs that represent the lexical cohesion structure of a text. They have been successfully used for information retrieval (Stairmand, 1996), document summarization (Barzilay and Elhadad, 1997) and so on. In this paper, we investigate how lexical chains can</context>
<context position="9005" citStr="Barzilay and Elhadad, 1997" startWordPosition="1337" endWordPosition="1340">lexical cohesion devices are used in document translation. As lexical chains capture lexical cohesion relations among sequences of related words rather than those only between two words, experiments in Section 5 show that our lexical chain based probability cohesion model is better than the lexical cohesion device based trigger model, which is the best among the three cohesion models proposed by Xiong et al. (2013). Modeling Coherence in Document-Level SMT In discourse analysis, cohesion is often studied together with coherence which is another dimension of the linguistic structure of a text (Barzilay and Elhadad, 1997). Cohesion is related to the surface structure of a text while coherence is concerned with the underlying meaning connectedness in a text (Vasconcellos, 1989). Compared with cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based</context>
<context position="12180" citStr="Barzilay and Elhadad, 1997" startWordPosition="1840" endWordPosition="1844">of candidate words and assigning semantic relations between the candidate words according to the ontology; • Choosing the right sense for each candidate word via WSD; • Building chains over the semantically related and disambiguated candidate words. These three sub-tasks can be done separately or simultaneously. Morris and Hirst (Morris and Hirst, 1991) define the first lexical chain computation algorithm that adopts a greedy strategy to immediately disambiguate a word at its first occurrence. This algorithm runs in linear time but suffers from inaccurate disambiguation. Barzilay and Elhadad (Barzilay and Elhadad, 1997) significantly improve WSD 1565 dWgu_ diUnx]n g^ngsZ z`ngcSi su`ma c[zh[ dWgu_ diUnx]n g^ngsZ xuRnbe , qiSn jiRnsh]hu] zhdx[ qZsh[Yr su] de xZlYXr jiRng dRnrYn gRi g^ngsZ de l[nsh[ z`ngcSi , wWiqZ lie gY yuY , zh[dUo su`ma de j]rYn rWnxuTn jiVrYn wWizh\ o ( fTxZnshY b^ Sng diUn ) dWgu_ diUnx]n g^ngsZ z`ngcSi su`ma jZntiRn c[qe tR de zh[we , tR shu^ , y_uyc tR xiTnrSn bc zUi shaudUo dWgu_ diUnx]n g^ngsZ jiRnsh]hu] de ch^ngfYn x]nrYn , c[zh[ sh] tR wWiyZ de xuTnzW o t_uzZrWn huRny[ng zhYxiUng xuRnbe , dWgu_ diUnx]n g^ngsZ de gdpiUo yZnc\ zUi fTlSnkYfc gdpiUo jiRoy] sh]chTng shUng zhTng bTifYnzhZ</context>
</contexts>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>Regina Barzilay and Michael Elhadad. 1997. Using lexical chains for text summarization. In In Proceedings of the ACL Workshop on Intelligent Scalable Text Summarization, pages 10–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="9359" citStr="Barzilay and Lapata, 2008" startWordPosition="1392" endWordPosition="1395">mong the three cohesion models proposed by Xiong et al. (2013). Modeling Coherence in Document-Level SMT In discourse analysis, cohesion is often studied together with coherence which is another dimension of the linguistic structure of a text (Barzilay and Elhadad, 1997). Cohesion is related to the surface structure of a text while coherence is concerned with the underlying meaning connectedness in a text (Vasconcellos, 1989). Compared with cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choic</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beata Beigman Klebanov</author>
<author>Michael Flor</author>
</authors>
<title>Associative texture is lost in translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Discourse in Machine Translation,</booktitle>
<pages>27--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="6227" citStr="Klebanov and Flor, 2013" startWordPosition="923" endWordPosition="926">presents our large-scale experiments and results. Finally, we conclude with future directions in Section 6. 2 Related Work Recent years have witnessed growing research interests in document-level statistical machine translation. Such research efforts can be roughly divided into two groups: 1) general document-level machine translation that does not explore or explores very little linguistic discourse information; 2) linguistically-motivated document-level machine translation that incorporates discourse information such as cohesion and coherence into SMT. Recent studies (Guillou, 2013; Beigman Klebanov and Flor, 2013) show that this discourse information is very important for document-level machine translation. General Document-Level Machine Translation Tiedemann (2010) propose cache-based language and translation models for document-level machine translation. These models are built on recently translated sentences. Following this cache-based approach, Gong et al. (2011) further introduce two additional caches. They use a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated. They also adopt a topic cache with target language top</context>
</contexts>
<marker>Klebanov, Flor, 2013</marker>
<rawString>Beata Beigman Klebanov and Michael Flor. 2013. Associative texture is lost in translation. In Proceedings of the Workshop on Discourse in Machine Translation, pages 27–32, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>How phrase sense disambiguation outperforms word sense disambiguation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>43--52</pages>
<contexts>
<context position="9749" citStr="Carpuat and Wu, 2007" startWordPosition="1451" endWordPosition="1454">ess in a text (Vasconcellos, 1989). Compared with cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on the </context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007a. How phrase sense disambiguation outperforms word sense disambiguation for statistical machine translation. In Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation, pages 43–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>61--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="9749" citStr="Carpuat and Wu, 2007" startWordPosition="1451" endWordPosition="1454">ess in a text (Vasconcellos, 1989). Compared with cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on the </context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007b. Improving statistical machine translation using word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 61–72, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
</authors>
<title>One translation per discourse.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009),</booktitle>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="10077" citStr="Carpuat (2009)" startWordPosition="1503" endWordPosition="1504">e model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on the source side in our lexical chaining algorithm (See Section 4.1). 3 Background: Lexical Chain and Chain Computation Lexical chains are sequences of semantically related words (Morris and Hirst, 1991). They represent the lexical cohesion structure of a text. Figure 2 displays six lexical chains computed from the Chinese news art</context>
</contexts>
<marker>Carpuat, 2009</marker>
<rawString>Marine Carpuat. 2009. One translation per discourse. In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009), pages 19–27, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>David Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>33--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="9793" citStr="Chan et al., 2007" startWordPosition="1459" endWordPosition="1462">ith cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on the source side in our lexical chaining algorith</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 33–40, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Zhenghua Li</author>
<author>Ting Liu</author>
</authors>
<title>Ltp: a chinese language technology platform.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations, COLING ’10,</booktitle>
<pages>13--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="16659" citStr="Che et al., 2010" startWordPosition="2568" endWordPosition="2571">as only two sub-classes at level i + 1. Actually, they have multiple sub-classes. tended Cilin contains 77,343 Chinese words, which are organized in a hierarchical structure containing 5 levels as shown in Figure 3. In the 5th level, each node represents an atomic concept which consists of a set of synonyms. These atomic concepts are just like synsets in WordNet. We use them to represent senses of words in the disambiguation graph. We select nouns, verbs, abbreviations and idioms as candidate words for the disambiguation graph. These words are identified by a Chinese part-tospeech tagger LTP (Che et al., 2010) in a preprocessing step. In order to build the disambiguation graph, we first build an array indexed by the atomic concepts of Cilin, then insert a copy of each candidate word into its all concept (sense) entries in the array. After that, we create all semantic links among senses of different candidate words in the disambiguation graph following Galley and McKeown (2003). In the second step, we use the principle of one sense per discourse to perform WSD for each candidate word in the disambiguation graph. We sum the weights of all semantic links under the different senses of the candidate wor</context>
</contexts>
<marker>Che, Li, Liu, 2010</marker>
<rawString>Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp: a chinese language technology platform. In Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations, COLING ’10, pages 13–16, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="24094" citStr="Chiang, 2007" startWordPosition="3834" endWordPosition="3835">lexical chains and use the lexical chain based cohesion models to constrain word selection in document translation. Figure 4 shows the architecture of an SMT system with the lexical chain based cohesion model. ri eδ(w,tji) (3) tji EC ri tji EC ri wETj 1568 Figure 4: Architecture of an SMT system with the lexical chain based cohesion model. 5 Experiments In this section, we conducted a series of experiments to validate the effectiveness of the proposed lexical chain based cohesion models for Chinese-to-English document-level machine translation. We used a hierarchical phrased-based SMT system (Chiang, 2007) trained on large-scale data. In particular, we aim at: • Measuring the impact of the threshold c on the probability cohesion model and selecting the best threshold on a development test set. • Investigating the effect of the two lexical-chain based cohesion models. • Comparing our lexical chain based cohesion models against the previous lexical cohesion device based models (Xiong et al., 2013). 5.1 Setup We collected our bilingual training data from LDC, which includes the corpus LDC2002E18, LDC2003E07, LDC2003E14, LDC2004E12, LDC2004T07, LDC2004T08 (Only Hong Kong News), LDC2005T06 and LDC20</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>176--181</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="27870" citStr="Clark et al (2011)" startWordPosition="4424" endWordPosition="4427">t }N 1) with different values for the threshold c. System MT06 MT08 Avg Baseline 30.43 23.32 26.88 LexChainCount(top 1) 30.46 23.52 26.99 LexChainCount 30.79 23.34 27.07 LexChainProb 31.64 24.54 28.09 Table 3: Effects of the lexical chain based count and probability cohesion models. LexChainCount: the count model defined in Eq. (3). LexChainProb: the probability model defined in Eq. (5). et al., 2002) as our evaluation metric. As MERT is normally instable, we ran the tuning process three times for all our experiments and presented the average BLEU scores on the three MERT runs as suggested by Clark et al (2011). 5.2 Setting the Threshold c As the two lexical chain based cohesion models are built on the super target lexical chains that are associated with a parameter c, we need to tune the threshold parameter c on the development test set NIST MT06. We conducted a group of experiments using the probability cohesion model defined in Eq. (5) to find the best threshold. Experiment results are shown in Table 2. If we set the threshold too small (e.g., 0.05), the super target lexical chains may contain too many noisy words that are not the translations of source lexical chain words, which may jeopardise t</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 176–181, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>One sense per discourse.</title>
<date>1992</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language,</booktitle>
<location>Harriman, NY,</location>
<contexts>
<context position="10147" citStr="Gale et al., 1992" startWordPosition="1513" endWordPosition="1516">s defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on the source side in our lexical chaining algorithm (See Section 4.1). 3 Background: Lexical Chain and Chain Computation Lexical chains are sequences of semantically related words (Morris and Hirst, 1991). They represent the lexical cohesion structure of a text. Figure 2 displays six lexical chains computed from the Chinese news article shown in Figure 1. Words in these lexical chains have lexical coh</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William A. Gale, Kenneth W. Church, and David Yarowsky. 1992. One sense per discourse. In Proceedings of the workshop on Speech and Natural Language, Harriman, NY, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
</authors>
<title>Improving word sense disambiguation in lexical chaining.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th international joint conference on Artificial intelligence, IJCAI’03,</booktitle>
<pages>1486--1488</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="15015" citStr="Galley and McKeown (2003)" startWordPosition="2298" endWordPosition="2301">ng lexical chains. We divide the lexical chain based document-level machine translation process into three steps: (1) computing lexical chains for source documents with a source language ontology, (2) generating target lexical chains from the computed source lexical chains, and finally (3) incorporating lexical cohesion encoded in the generated target lexical chains into document-level translation via lexical chain based cohesion models. The remainder of this section will elaborate these three steps. 4.1 Source Lexical Chains Computation We follow the chain computation algorithm introduced by Galley and McKeown (2003) to build lexical chains on source (Chinese) documents. In the algorithm, the chaining process includes three steps: choosing candidate words to build a disambiguation graph (Galley and McKeown, 2003) for each document, disambiguating the candidate words and finally building lexical chains over the disambiguated candidate words. The disambiguation graph can be considered as a representation of all possible interpretations of its corresponding text. In the graph, nodes are candidate words with different senses and edges between word senses are weighted according to their semantic relations, suc</context>
<context position="17033" citStr="Galley and McKeown (2003)" startWordPosition="2632" endWordPosition="2635">We use them to represent senses of words in the disambiguation graph. We select nouns, verbs, abbreviations and idioms as candidate words for the disambiguation graph. These words are identified by a Chinese part-tospeech tagger LTP (Che et al., 2010) in a preprocessing step. In order to build the disambiguation graph, we first build an array indexed by the atomic concepts of Cilin, then insert a copy of each candidate word into its all concept (sense) entries in the array. After that, we create all semantic links among senses of different candidate words in the disambiguation graph following Galley and McKeown (2003). In the second step, we use the principle of one sense per discourse to perform WSD for each candidate word in the disambiguation graph. We sum the weights of all semantic links under the different senses of the candidate word in question. The sense with the highest sum of weights is considered as the most probable sense for this word. We then assign this sense to all occurrences of the word in the document by adopting the constraint of one sense per discourse. Once all candidate words are disambiguated, we can build lexical chains over these words by removing all semantic links that connect </context>
</contexts>
<marker>Galley, McKeown, 2003</marker>
<rawString>Michel Galley and Kathleen McKeown. 2003. Improving word sense disambiguation in lexical chaining. In Proceedings of the 18th international joint conference on Artificial intelligence, IJCAI’03, pages 1486–1488, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengxian Gong</author>
<author>Min Zhang</author>
<author>Guodong Zhou</author>
</authors>
<title>Cache-based document-level statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>909--919</pages>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="6587" citStr="Gong et al. (2011)" startWordPosition="970" endWordPosition="973">ores very little linguistic discourse information; 2) linguistically-motivated document-level machine translation that incorporates discourse information such as cohesion and coherence into SMT. Recent studies (Guillou, 2013; Beigman Klebanov and Flor, 2013) show that this discourse information is very important for document-level machine translation. General Document-Level Machine Translation Tiedemann (2010) propose cache-based language and translation models for document-level machine translation. These models are built on recently translated sentences. Following this cache-based approach, Gong et al. (2011) further introduce two additional caches. They use a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated. They also adopt a topic cache with target language topic words. Xiao et al. (2011) study the translation consistency issue in document-level machine translation. They use a hard constraint to consistently translate ambiguous source words into the most frequent translation options. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Using Lexical Cohesion De</context>
</contexts>
<marker>Gong, Zhang, Zhou, 2011</marker>
<rawString>Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011. Cache-based document-level statistical machine translation. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 909–919, Edinburgh, Scotland, UK., July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liane Guillou</author>
</authors>
<title>Analysing lexical consistency in translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Discourse in Machine Translation,</booktitle>
<pages>10--18</pages>
<location>Sofia,</location>
<contexts>
<context position="6193" citStr="Guillou, 2013" startWordPosition="920" endWordPosition="921">sion models. Section 5 presents our large-scale experiments and results. Finally, we conclude with future directions in Section 6. 2 Related Work Recent years have witnessed growing research interests in document-level statistical machine translation. Such research efforts can be roughly divided into two groups: 1) general document-level machine translation that does not explore or explores very little linguistic discourse information; 2) linguistically-motivated document-level machine translation that incorporates discourse information such as cohesion and coherence into SMT. Recent studies (Guillou, 2013; Beigman Klebanov and Flor, 2013) show that this discourse information is very important for document-level machine translation. General Document-Level Machine Translation Tiedemann (2010) propose cache-based language and translation models for document-level machine translation. These models are built on recently translated sentences. Following this cache-based approach, Gong et al. (2011) further introduce two additional caches. They use a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated. They also adopt a to</context>
</contexts>
<marker>Guillou, 2013</marker>
<rawString>Liane Guillou. 2013. Analysing lexical consistency in translation. In Proceedings of the Workshop on Discourse in Machine Translation, pages 10–18, Sofia,</rawString>
</citation>
<citation valid="false">
<authors>
<author>August Bulgaria</author>
</authors>
<title>Association for Computational Linguistics.</title>
<marker>Bulgaria, </marker>
<rawString>Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>Ruqayia Hasan</author>
</authors>
<date>1976</date>
<booktitle>Cohesion in English.</booktitle>
<publisher>Longman.</publisher>
<location>London:</location>
<contexts>
<context position="1969" citStr="Halliday and Hasan, 1976" startWordPosition="279" endWordPosition="282"> improve translation quality in terms of BLEU and that the probability cohesion model outperforms previous models based on lexical cohesion devices. 1 Introduction Given a source document, traditionally most statistical machine translation (SMT) systems translate the document sentence by sentence. In such a translation scheme, sentences are translated independent of any other sentences. However, a text is normally written cohesively, in which sentences are connected *Corresponding author to each other via syntactic and lexical devices. This linguistic phenomenon is called as textual cohesion (Halliday and Hasan, 1976). Cohesion is a surface-level property of wellformed texts. It deals with five categories of relationships between text units, namely co-reference, ellipsis, substitution, conjunction and lexical cohesion that is realized via semantically related words. The former four cohesion relations can be grouped as grammatical cohesion. Generally speaking, grammatical cohesion is less common and harder to identify than lexical cohesion (Barzilay and Elhadad, 1997). As most SMT systems translate a text in a sentence-by-sentence fashion, they tend to build less lexical cohesion than human translators (Won</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>M.A.K Halliday and Ruqayia Hasan. 1976. Cohesion in English. London: Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Hardmeier</author>
<author>Joakim Nivre</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Document-wide decoding for phrase-based statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1179--1190</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="7687" citStr="Hardmeier et al. (2012)" startWordPosition="1135" endWordPosition="1138">l. (2012) soften this consistency constraint by integrating three counting features into decoder. Using Lexical Cohesion Devices in DocumentLevel SMT Lexical cohesion devices are semantically related words, including word repetition, synonyms/near-synonyms, hyponyms and so on. They are also the cohesion-building elements in lexical chains. Wong and Kit (2012) use lexical cohesion device based metrics to improve machine translation evaluation at the document level. These metrics measure the proportion of content words that are used as lexical cohesion devices in machine-generated translations. Hardmeier et al. (2012) propose a documentwide phrase-based decoder and integrate a semantic language model into the decoder. They argue that their semantic language model can capture lexical cohesion by exploring n-grams that cross sentence 1564 boundaries. Most recently Xiong et al. (2013) integrate three categories of lexical cohesion devices into document-level machine translation. They define three cohesion models based on lexical cohesion devices: a direct reward model, a conditional probability model and a mutual information trigger model. The latter two models measure the strength of lexical cohesion relatio</context>
</contexts>
<marker>Hardmeier, Nivre, Tiedemann, 2012</marker>
<rawString>Christian Hardmeier, Joakim Nivre, and J¨org Tiedemann. 2012. Document-wide decoding for phrase-based statistical machine translation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1179–1190, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>A coherence model based on syntactic patterns.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1157--1168</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="9385" citStr="Louis and Nenkova, 2012" startWordPosition="1396" endWordPosition="1399">els proposed by Xiong et al. (2013). Modeling Coherence in Document-Level SMT In discourse analysis, cohesion is often studied together with coherence which is another dimension of the linguistic structure of a text (Barzilay and Elhadad, 1997). Cohesion is related to the surface structure of a text while coherence is concerned with the underlying meaning connectedness in a text (Vasconcellos, 1989). Compared with cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the</context>
</contexts>
<marker>Louis, Nenkova, 2012</marker>
<rawString>Annie Louis and Ani Nenkova. 2012. A coherence model based on syntactic patterns. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1157–1168, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Comput. Linguist.,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="2701" citStr="Morris and Hirst, 1991" startWordPosition="386" endWordPosition="389">tween text units, namely co-reference, ellipsis, substitution, conjunction and lexical cohesion that is realized via semantically related words. The former four cohesion relations can be grouped as grammatical cohesion. Generally speaking, grammatical cohesion is less common and harder to identify than lexical cohesion (Barzilay and Elhadad, 1997). As most SMT systems translate a text in a sentence-by-sentence fashion, they tend to build less lexical cohesion than human translators (Wong and Kit, 2012). We therefore study lexical cohesion for document-level translation. We use lexical chains (Morris and Hirst, 1991) to capture lexical cohesion in a text. Lexical chains are connected graphs that represent the lexical cohesion structure of a text. They have been successfully used for information retrieval (Stairmand, 1996), document summarization (Barzilay and Elhadad, 1997) and so on. In this paper, we investigate how lexical chains can be used to incorporate lexical cohesion into document-level translation. Our basic assumption is that the lexical chains of a target document are direct correspondences of the lexical chains of its counterpart source document. This assumption is reasonable as the target do</context>
<context position="10547" citStr="Morris and Hirst, 1991" startWordPosition="1575" endWordPosition="1578">es. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on the source side in our lexical chaining algorithm (See Section 4.1). 3 Background: Lexical Chain and Chain Computation Lexical chains are sequences of semantically related words (Morris and Hirst, 1991). They represent the lexical cohesion structure of a text. Figure 2 displays six lexical chains computed from the Chinese news article shown in Figure 1. Words in these lexical chains have lexical cohesion relations such as repetition, synonym, which may range over the entire text. For example, in the lexical chain LC1 of Figure 2, the same word “d6gu6” (Germany) repeats 9 times. In the lexical chain LC3, the two words “z6ngc6i” (president) and “zhuxi” (chairman) are synonym words. Generally, a text can have many different lexical chains, each of which represents a thread of cohesion through t</context>
<context position="11908" citStr="Morris and Hirst, 1991" startWordPosition="1797" endWordPosition="1800">tain semantic relations between words. Word sense disambiguation (WSD) is also used to determine the sense of each word in a text. Generally a lexical chain computation algorithm completes the following three subtasks: • Building a representation of a text with a set of candidate words and assigning semantic relations between the candidate words according to the ontology; • Choosing the right sense for each candidate word via WSD; • Building chains over the semantically related and disambiguated candidate words. These three sub-tasks can be done separately or simultaneously. Morris and Hirst (Morris and Hirst, 1991) define the first lexical chain computation algorithm that adopts a greedy strategy to immediately disambiguate a word at its first occurrence. This algorithm runs in linear time but suffers from inaccurate disambiguation. Barzilay and Elhadad (Barzilay and Elhadad, 1997) significantly improve WSD 1565 dWgu_ diUnx]n g^ngsZ z`ngcSi su`ma c[zh[ dWgu_ diUnx]n g^ngsZ xuRnbe , qiSn jiRnsh]hu] zhdx[ qZsh[Yr su] de xZlYXr jiRng dRnrYn gRi g^ngsZ de l[nsh[ z`ngcSi , wWiqZ lie gY yuY , zh[dUo su`ma de j]rYn rWnxuTn jiVrYn wWizh\ o ( fTxZnshY b^ Sng diUn ) dWgu_ diUnx]n g^ngsZ z`ngcSi su`ma jZntiRn c[qe</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Jane Morris and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Comput. Linguist., 17(1):21–48, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Stairmand</author>
</authors>
<title>A computational analysis of lexical cohesion with applications in information retrieval.</title>
<date>1996</date>
<publisher>UMIST.</publisher>
<contexts>
<context position="2910" citStr="Stairmand, 1996" startWordPosition="420" endWordPosition="421">on. Generally speaking, grammatical cohesion is less common and harder to identify than lexical cohesion (Barzilay and Elhadad, 1997). As most SMT systems translate a text in a sentence-by-sentence fashion, they tend to build less lexical cohesion than human translators (Wong and Kit, 2012). We therefore study lexical cohesion for document-level translation. We use lexical chains (Morris and Hirst, 1991) to capture lexical cohesion in a text. Lexical chains are connected graphs that represent the lexical cohesion structure of a text. They have been successfully used for information retrieval (Stairmand, 1996), document summarization (Barzilay and Elhadad, 1997) and so on. In this paper, we investigate how lexical chains can be used to incorporate lexical cohesion into document-level translation. Our basic assumption is that the lexical chains of a target document are direct correspondences of the lexical chains of its counterpart source document. This assumption is reasonable as the target document translation should be faithful to the source document in terms of both text meaning and structure. Based on this assumption, we propose a framework 1563 Proceedings of the 2013 Conference on Empirical M</context>
</contexts>
<marker>Stairmand, 1996</marker>
<rawString>M.A. Stairmand. 1996. A computational analysis of lexical cohesion with applications in information retrieval. UMIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm–an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, Colorado, USA,</location>
<contexts>
<context position="24964" citStr="Stolcke, 2002" startWordPosition="3965" endWordPosition="3966">on models. • Comparing our lexical chain based cohesion models against the previous lexical cohesion device based models (Xiong et al., 2013). 5.1 Setup We collected our bilingual training data from LDC, which includes the corpus LDC2002E18, LDC2003E07, LDC2003E14, LDC2004E12, LDC2004T07, LDC2004T08 (Only Hong Kong News), LDC2005T06 and LDC2005T10. The collected bilingual training data contains 3.8M sentence pairs with 96.9M Chinese words and 109.5M English words. We trained a 4-gram language model on the Xinhua portion of the English Gigaword corpus (306 million words) via the SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing. Training MT05 MT06 MT08 #Doc 103,236 100 79 109 #Sent 2.80M 1,082 1,664 1,357 #Chain 3.52M 1700 2172 1693 #AvgC 35.72 17 27.49 15.53 #AvgW 14.81 5.89 6.89 5.63 Table 1: Statistics of the training, development and test sets, which show the number of documents (#Doc) and sentences (#Sent), the number of lexical chains extracted from the source documents (#Chain), the average number of lexical chains per document (#AvgC) and the average number of words per lexical chain (#AvgW). In order to build the lexical chain based cohesion models, we selected corpora with documen</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm–an extensible language modeling toolkit. In Proceedings of the 7th International Conference on Spoken Language Processing, pages 901–904, Denver, Colorado, USA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Context adaptation in statistical machine translation using models with exponentially decaying cache.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing,</booktitle>
<pages>8--15</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="6382" citStr="Tiedemann (2010)" startWordPosition="944" endWordPosition="945">arch interests in document-level statistical machine translation. Such research efforts can be roughly divided into two groups: 1) general document-level machine translation that does not explore or explores very little linguistic discourse information; 2) linguistically-motivated document-level machine translation that incorporates discourse information such as cohesion and coherence into SMT. Recent studies (Guillou, 2013; Beigman Klebanov and Flor, 2013) show that this discourse information is very important for document-level machine translation. General Document-Level Machine Translation Tiedemann (2010) propose cache-based language and translation models for document-level machine translation. These models are built on recently translated sentences. Following this cache-based approach, Gong et al. (2011) further introduce two additional caches. They use a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated. They also adopt a topic cache with target language topic words. Xiao et al. (2011) study the translation consistency issue in document-level machine translation. They use a hard constraint to consistently tran</context>
</contexts>
<marker>Tiedemann, 2010</marker>
<rawString>J¨org Tiedemann. 2010. Context adaptation in statistical machine translation using models with exponentially decaying cache. In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, pages 8–15, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferhan Ture</author>
<author>Douglas W Oard</author>
<author>Philip Resnik</author>
</authors>
<title>Encouraging consistent translation choices.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>417--426</pages>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="7073" citStr="Ture et al. (2012)" startWordPosition="1045" endWordPosition="1048">achine translation. These models are built on recently translated sentences. Following this cache-based approach, Gong et al. (2011) further introduce two additional caches. They use a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated. They also adopt a topic cache with target language topic words. Xiao et al. (2011) study the translation consistency issue in document-level machine translation. They use a hard constraint to consistently translate ambiguous source words into the most frequent translation options. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Using Lexical Cohesion Devices in DocumentLevel SMT Lexical cohesion devices are semantically related words, including word repetition, synonyms/near-synonyms, hyponyms and so on. They are also the cohesion-building elements in lexical chains. Wong and Kit (2012) use lexical cohesion device based metrics to improve machine translation evaluation at the document level. These metrics measure the proportion of content words that are used as lexical cohesion devices in machine-generated translations. Hardmeier</context>
</contexts>
<marker>Ture, Oard, Resnik, 2012</marker>
<rawString>Ferhan Ture, Douglas W. Oard, and Philip Resnik. 2012. Encouraging consistent translation choices. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 417– 426, Montr´eal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muriel Vasconcellos</author>
</authors>
<title>Cohesion and coherence in the presentation of machine translation products.</title>
<date>1989</date>
<booktitle>Geogetown University Round Table on Languages and Linguistics</booktitle>
<pages>89--105</pages>
<editor>In James E.Alatis, editor,</editor>
<publisher>Georgetown University Press.</publisher>
<location>Washington, D.C.</location>
<contexts>
<context position="9163" citStr="Vasconcellos, 1989" startWordPosition="1364" endWordPosition="1365">y between two words, experiments in Section 5 show that our lexical chain based probability cohesion model is better than the lexical cohesion device based trigger model, which is the best among the three cohesion models proposed by Xiong et al. (2013). Modeling Coherence in Document-Level SMT In discourse analysis, cohesion is often studied together with coherence which is another dimension of the linguistic structure of a text (Barzilay and Elhadad, 1997). Cohesion is related to the surface structure of a text while coherence is concerned with the underlying meaning connectedness in a text (Vasconcellos, 1989). Compared with cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and</context>
</contexts>
<marker>Vasconcellos, 1989</marker>
<rawString>Muriel Vasconcellos. 1989. Cohesion and coherence in the presentation of machine translation products. In James E.Alatis, editor, Geogetown University Round Table on Languages and Linguistics 1989, pages 89– 105, Washington, D.C. Georgetown University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Billy T M Wong</author>
<author>Chunyu Kit</author>
</authors>
<title>Extending machine translation evaluation metrics with lexical cohesion to document level.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1060--1068</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="2585" citStr="Wong and Kit, 2012" startWordPosition="370" endWordPosition="373">76). Cohesion is a surface-level property of wellformed texts. It deals with five categories of relationships between text units, namely co-reference, ellipsis, substitution, conjunction and lexical cohesion that is realized via semantically related words. The former four cohesion relations can be grouped as grammatical cohesion. Generally speaking, grammatical cohesion is less common and harder to identify than lexical cohesion (Barzilay and Elhadad, 1997). As most SMT systems translate a text in a sentence-by-sentence fashion, they tend to build less lexical cohesion than human translators (Wong and Kit, 2012). We therefore study lexical cohesion for document-level translation. We use lexical chains (Morris and Hirst, 1991) to capture lexical cohesion in a text. Lexical chains are connected graphs that represent the lexical cohesion structure of a text. They have been successfully used for information retrieval (Stairmand, 1996), document summarization (Barzilay and Elhadad, 1997) and so on. In this paper, we investigate how lexical chains can be used to incorporate lexical cohesion into document-level translation. Our basic assumption is that the lexical chains of a target document are direct corr</context>
<context position="7425" citStr="Wong and Kit (2012)" startWordPosition="1095" endWordPosition="1098">h target language topic words. Xiao et al. (2011) study the translation consistency issue in document-level machine translation. They use a hard constraint to consistently translate ambiguous source words into the most frequent translation options. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Using Lexical Cohesion Devices in DocumentLevel SMT Lexical cohesion devices are semantically related words, including word repetition, synonyms/near-synonyms, hyponyms and so on. They are also the cohesion-building elements in lexical chains. Wong and Kit (2012) use lexical cohesion device based metrics to improve machine translation evaluation at the document level. These metrics measure the proportion of content words that are used as lexical cohesion devices in machine-generated translations. Hardmeier et al. (2012) propose a documentwide phrase-based decoder and integrate a semantic language model into the decoder. They argue that their semantic language model can capture lexical cohesion by exploring n-grams that cross sentence 1564 boundaries. Most recently Xiong et al. (2013) integrate three categories of lexical cohesion devices into document</context>
</contexts>
<marker>Wong, Kit, 2012</marker>
<rawString>Billy T. M. Wong and Chunyu Kit. 2012. Extending machine translation evaluation metrics with lexical cohesion to document level. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1060–1068, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Xiao</author>
<author>Jingbo Zhu</author>
<author>Shujie Yao</author>
<author>Hao Zhang</author>
</authors>
<title>Document-level consistency verification in machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 MT summit XIII,</booktitle>
<pages>131--138</pages>
<location>Xiamen, China,</location>
<contexts>
<context position="6855" citStr="Xiao et al. (2011)" startWordPosition="1015" endWordPosition="1018"> this discourse information is very important for document-level machine translation. General Document-Level Machine Translation Tiedemann (2010) propose cache-based language and translation models for document-level machine translation. These models are built on recently translated sentences. Following this cache-based approach, Gong et al. (2011) further introduce two additional caches. They use a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated. They also adopt a topic cache with target language topic words. Xiao et al. (2011) study the translation consistency issue in document-level machine translation. They use a hard constraint to consistently translate ambiguous source words into the most frequent translation options. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Using Lexical Cohesion Devices in DocumentLevel SMT Lexical cohesion devices are semantically related words, including word repetition, synonyms/near-synonyms, hyponyms and so on. They are also the cohesion-building elements in lexical chains. Wong and Kit (2012) use lexical cohesion device b</context>
</contexts>
<marker>Xiao, Zhu, Yao, Zhang, 2011</marker>
<rawString>Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang. 2011. Document-level consistency verification in machine translation. In Proceedings of the 2011 MT summit XIII, pages 131–138, Xiamen, China, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
</authors>
<title>A topic-based coherence model for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence (AAAI-13),</booktitle>
<location>Bellevue, Washington, USA,</location>
<contexts>
<context position="9430" citStr="Xiong and Zhang (2013)" startWordPosition="1404" endWordPosition="1407">oherence in Document-Level SMT In discourse analysis, cohesion is often studied together with coherence which is another dimension of the linguistic structure of a text (Barzilay and Elhadad, 1997). Cohesion is related to the surface structure of a text while coherence is concerned with the underlying meaning connectedness in a text (Vasconcellos, 1989). Compared with cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical c</context>
</contexts>
<marker>Xiong, Zhang, 2013</marker>
<rawString>Deyi Xiong and Min Zhang. 2013. A topic-based coherence model for statistical machine translation. In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence (AAAI-13), Bellevue, Washington, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Guosheng Ben</author>
<author>Min Zhang</author>
<author>Yajuan Lv</author>
<author>Qun Liu</author>
</authors>
<title>Modeling lexical cohesion for document-level machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence (IJCAI-13),</booktitle>
<location>Beijing, China,</location>
<contexts>
<context position="5028" citStr="Xiong et al., 2013" startWordPosition="748" endWordPosition="751">arget words, we further extend the count model to a second cohesion model: a probability model that takes chain word translation probabilities into account. We test the two lexical chain based cohesion models on a hierarchical phrase-based SMT system that is trained with large-scale Chinese-English bilingual data. Experiment results show that our lexical chain based cohesion models can achieve substantial improvements over the baseline. Furthermore, the probability cohesion model is better than the count model and it also outperforms previous cohesion models based on lexical cohesion devices (Xiong et al., 2013). To the best of our knowledge, this is the first attempt to explore lexical chains for statistical machine translation. The remainder of this paper is organized as follows. Section 2 discusses related work and highlights the differences between our method and previous work. Section 3 briefly introduces lexical chains and algorithms that compute lexical chains. Section 4 elaborates the proposed lexical chain based framework, including details on source lexical chain computation, target lexical chain generation and the two lexical chain based cohesion models. Section 5 presents our large-scale </context>
<context position="7956" citStr="Xiong et al. (2013)" startWordPosition="1175" endWordPosition="1178"> on. They are also the cohesion-building elements in lexical chains. Wong and Kit (2012) use lexical cohesion device based metrics to improve machine translation evaluation at the document level. These metrics measure the proportion of content words that are used as lexical cohesion devices in machine-generated translations. Hardmeier et al. (2012) propose a documentwide phrase-based decoder and integrate a semantic language model into the decoder. They argue that their semantic language model can capture lexical cohesion by exploring n-grams that cross sentence 1564 boundaries. Most recently Xiong et al. (2013) integrate three categories of lexical cohesion devices into document-level machine translation. They define three cohesion models based on lexical cohesion devices: a direct reward model, a conditional probability model and a mutual information trigger model. The latter two models measure the strength of lexical cohesion relation between two lexical items. They are incorporated into SMT to calculate how appropriately lexical cohesion devices are used in document translation. As lexical chains capture lexical cohesion relations among sequences of related words rather than those only between tw</context>
<context position="24491" citStr="Xiong et al., 2013" startWordPosition="3895" endWordPosition="3898">ies of experiments to validate the effectiveness of the proposed lexical chain based cohesion models for Chinese-to-English document-level machine translation. We used a hierarchical phrased-based SMT system (Chiang, 2007) trained on large-scale data. In particular, we aim at: • Measuring the impact of the threshold c on the probability cohesion model and selecting the best threshold on a development test set. • Investigating the effect of the two lexical-chain based cohesion models. • Comparing our lexical chain based cohesion models against the previous lexical cohesion device based models (Xiong et al., 2013). 5.1 Setup We collected our bilingual training data from LDC, which includes the corpus LDC2002E18, LDC2003E07, LDC2003E14, LDC2004E12, LDC2004T07, LDC2004T08 (Only Hong Kong News), LDC2005T06 and LDC2005T10. The collected bilingual training data contains 3.8M sentence pairs with 96.9M Chinese words and 109.5M English words. We trained a 4-gram language model on the Xinhua portion of the English Gigaword corpus (306 million words) via the SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing. Training MT05 MT06 MT08 #Doc 103,236 100 79 109 #Sent 2.80M 1,082 1,664 1,357 #Chain 3.52M 1700 217</context>
<context position="31235" citStr="Xiong et al. (2013)" startWordPosition="4989" endWordPosition="4992">apture lexical cohesion relations in a text. We therefore want to compare our lexical chain based cohesion models with the lexical cohesion device based cohesion models. 1570 System MT06 MT08 Avg Baseline 30.43 23.32 26.88 LexDeviceTrigger 31.35 24.11 27.73 LexChainProb 31.64 24.54 28.09 Table 4: The lexical chain based probability cohesion model (LexChainProb) vs. the lexical cohesion device based trigger model (LexDeviceTrigger). We re-implemented the mutual information trigger model that is the best lexical cohesion model based on lexical cohesion devices among the three models proposed by Xiong et al. (2013). The mutual information trigger model measures the association strength of two lexical cohesion items x and y in a lexical cohesion relation xRy. In the model, it is required that x occurs in a sentence preceding the sentence where y occurs and that the two items have a lexical cohesion relation such as word repetition, synonym. The model treats x as the trigger and y as the triggered item. The mutual information between the trigger x and the triggered item y estimates how possible y will occur given x is mentioned in a text. The comparison results are reported in Table 4. Our lexical chain b</context>
</contexts>
<marker>Xiong, Ben, Zhang, Lv, Liu, 2013</marker>
<rawString>Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan Lv, and Qun Liu. 2013. Modeling lexical cohesion for document-level machine translation. In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence (IJCAI-13), Beijing, China, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
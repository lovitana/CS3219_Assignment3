<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003147">
<title confidence="0.992564">
A Corpus Level MIRA Tuning Strategy for Machine Translation
</title>
<author confidence="0.995541">
Ming Tan, Tian Xia, Shaojun Wang
</author>
<affiliation confidence="0.992748">
Wright State University
</affiliation>
<address confidence="0.816401">
3640 Colonel Glenn Hwy,
Dayton, OH 45435 USA
{tan.6, xia.7, shaojun.wang}
</address>
<email confidence="0.996703">
@wright.edu
</email>
<note confidence="0.8846535">
Bowen Zhou
IBM T.J. Watson Research Center
</note>
<address confidence="0.5346315">
1101 Kitchawan Rd,
Yorktown Heights, NY 10598 USA
</address>
<email confidence="0.98671">
zhou@us.ibm.com
</email>
<sectionHeader confidence="0.995375" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997799384615385">
MIRA based tuning methods have been
widely used in statistical machine translation
(SMT) system with a large number of fea-
tures. Since the corpus-level BLEU is not de-
composable, these MIRA approaches usually
define a variety of heuristic-driven sentence-
level BLEUs in their model losses. Instead,
we present a new MIRA method, which em-
ploys an exact corpus-level BLEU to com-
pute the model loss. Our method is simpler in
implementation. Experiments on Chinese-to-
English translation show its effectiveness over
two state-of-the-art MIRA implementations.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999894571428571">
Margin infused relaxed algorithm (MIRA) has been
widely adopted for the parameter optimization in
SMT with a large feature size (Watanabe et al., 2007;
Chiang et al., 2008; Chiang et al., 2009; Chiang,
2012; Eidelman, 2012; Cherry and Foster, 2012).
Since BLEU is defined on the corpus, and not de-
composed into sentences, most MIRA approaches
consider a variety of sentence-level BLEUs for the
model losses, many of which are heuristic-driven
(Watanabe et al., 2007; Chiang et al., 2008; Chi-
ang et al., 2009; Chiang, 2012; Cherry and Foster,
2012). The sentence-level BLEU appearing in the
objective is generally based on a pseudo-document,
which may not precisely reflect the corpus-level
BLEU. We believe that this mismatch could poten-
tially harm the performance. To avoid the sentence
BLEU, the work in (Haddow et al., 2011) proposed
to process sentences in small batches. The authors
adopted a Gibbs sampling (Arun et al., 2009) tech-
nique to search the hope and fear hypotheses, and
they did not compare with MIRA. Watanabe (2012)
also tuned the parameters with small batches of sen-
tences and optimized a hinge loss not explicitly re-
lated to BLEU using stochastic gradient descent.
Both approaches introduced additional complexities
over baseline MIRA approaches.
In contrast, we propose a remarkably simple but
efficient batch MIRA approach which exploits the
exact corpus-level BLEU to compute model losses.
We search for a hope and a fear hypotheses for the
corpus with a straightforward approach and mini-
mize the structured hinge loss defined on them. The
experiments show that our method consistently out-
performs two state-of-the-art MIRAs in Chinese-to-
English translation tasks with a moderate margin.
</bodyText>
<sectionHeader confidence="0.982026" genericHeader="method">
2 Margin Infused Relaxed Algorithm
</sectionHeader>
<bodyText confidence="0.99995875">
We optimize the model parameters based on N-best
lists. Our development (dev) set is a set of triples
{(fi, ei, ri)}Mi=1, where fi is a source-language sen-
tence, corresponded by a list of target-language hy-
</bodyText>
<equation confidence="0.947276">
potheses ei = {eij}N(f�)
j=1 , with a number of refer-
</equation>
<bodyText confidence="0.9494275">
ences ri. h(eij) is a feature vector. Generally, most
decoders return a top-1 candidate as the transla-
tion result, such that ei(w) = arg maxj w · h(eij ),
where w are the model parameters. In this paper, we
aim at optimizing the BLEU score (Papineni et al.,
2002).
MIRA is an instance of online learning which as-
sumes an overlap of the decoding procedure and the
parameter optimization procedure. For example in
(Crammer et al., 2006; Chiang et al., 2008), MIRA
</bodyText>
<page confidence="0.974205">
851
</page>
<bodyText confidence="0.8217394">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 851–856,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
is performed after an input sentence are decoded,
and the next sentence is decoded with the updated
parameters. The objective for each sentence i is,
</bodyText>
<equation confidence="0.93082475">
2008), c-MIRA repeatedly optimizes,
2||w − w 0||2
+ C C. lcorpus(w) (3)
min
w
min �||w − w0||2 + C · li(w) (1) loo�ug(w) = max{B(E∗) − B(E)
w 1 E
−w · [H(E∗) − H(E)]} (4)
</equation>
<bodyText confidence="0.999885666666667">
where e∗i ∈ ei is a hope candidate, w0 is the pa-
rameter vector from the last sentence. Since MIRA
defines its objective only based on the current sen-
tence, b(·) is a sentence-level BLEU.
Most MIRA algorithms need a deliberate defini-
tion of b(·), since BLEU cannot be decomposed into
sentences. The types of the sentence BLEU calcula-
tion includes: (a) a smoothed version of BLEU for
ei9 (Liang et al., 2006), (b) fit ei9 into a pseudo-
document considering the history (Chiang et al.,
2008; Chiang, 2012), (c) use ei9 to replace the corre-
sponding hypothesis in the oracles (Watanabe et al.,
2007). The sentence-level BLEU sometimes per-
plexes the algorithms and results in a mismatch with
the corpus-level BLEU.
</bodyText>
<sectionHeader confidence="0.998586" genericHeader="method">
3 Corpus-level MIRA
</sectionHeader>
<subsectionHeader confidence="0.991945">
3.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.998867833333333">
We propose a batch tuning strategy, corpus-level
MIRA (c-MIRA), in which an objective is not built
upon a hinge loss of a single sentence, but upon that
of the entire corpus.
The online MIRAs are difficult to parallelize.
Therefore, similar to the batch MIRA in (Cherry and
Foster, 2012), we conduct the batch tuning by re-
peating the following steps: (a) Decode source sen-
tences (in parallel) and obtain {ei}M1, (b) Merge
{ei}M1 with the one from the previous iteration, (c)
Invoke Algorithm 1.
We define E = (eE,1, eE,2, ..., eE,M) as a corpus
</bodyText>
<equation confidence="0.99283025">
EM
1
hypothesis, with H (E) = M
i=1
</equation>
<bodyText confidence="0.886288888888889">
hypothesis of the source sentence fi covered by E.
E is corresponded to a corpus-level BLEU, which
we ultimately want to optimize. Following MIRA
formulated in (Crammer et al., 2006; Chiang et al.,
where B(·) is a corpus-level BLEU. E∗ is a hope
hypothesis. E ∈ L, where L is the hypothesis space
of the entire corpus, and |L |= |e1 |· · · |eM|.
Algorithm 1 Corpus-Level MIRA
Require: {(fi, ei, ri)}Mi=1, w0, C
</bodyText>
<listItem confidence="0.937717588235294">
1: for t = 1···T do
2: E∗ = {} ,E0 = {} &gt; Initialize the hope and fear
3: for i = 1···M do
4: eE*,i = arg max
eij
5: eE,,i = arg max
eij
6: E∗ ← E∗ + {eE+,i} &gt; Build the hope
7: E0 ← E0 + {eE�,i} &gt; Build the fear
8: end for
9: 4B = B(E∗) − B(E0) &gt; the BLEU difference
10: 4H = H(E0) − H(E∗) &gt; the feature difference
� C, 4�+wt−1·4H
11: α = min ||4H||2
12: wt = wt−1 − α · 4H
13: end for
14: return wt with the optimal BLEU on the dev set.
</listItem>
<bodyText confidence="0.99971575">
c-MIRA can be regarded as a standard MIRA,
in which there is only one single triple (F, L, R),
where F and R are the source and reference of
the corpus respectively. Eq. 3 is equivalent to a
quadratic programming with |L |constraints. Cram-
mer et al. (2006) show that a single constraint with
one hope E∗ and one fear E0 admits a closed-form
update and performs well. We denote one execution
of the outer loop as an epoch. The hope and fear
are updated in each epoch. Similar to (Chiang et al.,
2008), the hope and fear hypotheses are defined as
following,
</bodyText>
<equation confidence="0.99873325">
E∗ = max [w · H(E) + B(E)] (5)
E
E0 = max[w · H(E) − B(E)] (6)
E
</equation>
<bodyText confidence="0.99981325">
Eq. 5 and 6 find the hypotheses with the best and
worse BLEU that the decoder can easily achieve. It
is unnecessary to search the entire space of L for
precise solution E∗and E0, because MIRA only at-
</bodyText>
<equation confidence="0.98097195">
1
wt
15: wt =
t + 1
t
E
t=0
l;(w) = max {b(e∗i ) − b(eij)
eij
−w · [h(e∗i ) − h(eij)]} (2)
h(eE,i). eE,i is the
[wt−1 · h(eij) + b0(eij)]
[wt−1 · h(eij) − b0(eij)]
852
loss of c-MIRA in Eq. 4 is,
lcorpus(w) ∝ max
E0
M
[B(ei,kE∗) − B(ei,kE0)
i=1
</equation>
<bodyText confidence="0.99249175">
tempts to separate the hope from the fear by a mar-
gin proportional to their BLEU differentials (Cherry
and Foster, 2012). We just construct E*and E&apos; re-
spectively by,
</bodyText>
<equation confidence="0.9988096">
−w·h(ei,kE∗) + w · h(ei,kE0 )]
e£*,i = max [w · h(ei,j) + b&apos;(ei,j)] = M max [B(ei,kE∗) − B(eij)
ei,j i=1 eii
e£,,i = max [w · h(ei,j) − b&apos;(ei,j)] −w·h(ei,kE∗) + w · h(eij)] = M li(w)
ei,j i=1
</equation>
<bodyText confidence="0.9998758">
where b&apos; is simply a BLEU with add one smoothing
(Lin and Och, 2004). A smoothed BLEU is good
enough to pick up a “satisfying” pair of hope and
fear. However, the updating step (Line 11) uses the
corpus-level BLEU.
</bodyText>
<subsectionHeader confidence="0.991628">
3.2 Justification
</subsectionHeader>
<bodyText confidence="0.999912">
c-MIRA treats a corpus as one sentence for decod-
ing, while conventional decoders process sentences
one by one. We show the optimal solutions from the
two methods are equivalent theoretically.
We follow the notations in (Och and Ney,
2002). We search a hypothesis on corpus E =
{e1,ki , e2,k2, ..., eM,kM } with the highest probabil-
ity given the source corpus F = {f1, f2, ..., fm},
</bodyText>
<equation confidence="0.9993805">
E = arg max
E
= arg max
E
w · h(ei,ki)}M (8)
i=1
</equation>
<bodyText confidence="0.999826307692308">
where Zi = E (fi) j�1exp(w · h(ei,j)), which is a
constant with respective to E. Eq. 7 shows that
the feature vector of E is determined by the sum of
each candidate’s feature vectors. Also, the model
score can be decomposed into each sentence in Eq.
8, which shows that decoding all sentences together
equals to decoding one by one.
We also show that if the metric is decomposable,
the loss in c-MIRA is actually the sum of the hinge
loss li(w) in structural SVM (Tsochantaridis et al.,
2004; Cherry and Foster, 2012). We assume B(eij)
to be the metric of a sentence hypothesis, then the
Instead of adopting a cutting-plane algorithm
(Tsochantaridis et al., 2004), we optimize the same
loss with a MIRA pattern in a simpler way. How-
ever, since BLEU is not decomposable, the struc-
tural SVM (Cherry and Foster, 2012) uses an inter-
polated sentence BLEU (Liang et al., 2006). Al-
though Algorithm 1 has an outlook similar to the
batch-MIRA algorithm in (Cherry and Foster, 2012),
their loss definitions differ fundamentally. Batch
MIRA basically uses a sentence-level loss, and they
also follow the sentence-by-sentence tuning pattern.
In the future work, we will compare structural SVM
and c-MIRA under decomposable metrics like WER
or SSER (Och and Ney, 2002).
</bodyText>
<sectionHeader confidence="0.996458" genericHeader="evaluation">
4 Experiments and Analysis
</sectionHeader>
<bodyText confidence="0.999963285714286">
We first evaluate c-MIRA in a iterative batch tuning
procedure in a Chinese-to-English machine transla-
tion system with 228 features. Second, we show c-
MIRA is also effective in the re-ranking task with
more than 50,000 features.
In both experiments, we compare c-MIRA and
three baselines: (1) MERT (Och, 2003), (2) Chiang
et al.’s MIRA (MIRA1) in (Chiang et al., 2008). (3)
batch-MIRA (MIRA2) in (Cherry and Foster, 2012).
Here, we roughly choose C with the best BLEU on
dev set, from {0.1, 0.01, 0.001, 0.0001, 0.00001}.
We convert Chiang et al.’s MIRA to the batch mode
described in section 3.1. So the only difference be-
tween MIRA1 and MIRA2 is: MIRA1 obtains mul-
tiple constraints before optimization, while MIRA2
only uses one constraint. We implement MERT
and MIRA1, and directly use MIRA2 from Moses
(Koehn et al., 2007). We conduct experiments in a
server of 8-cores with 2.5GHz Opteron. We set the
maximum number of epochs as we generally do not
observe an obvious increase on the dev set BLEU.
</bodyText>
<equation confidence="0.9861762">
loge(E|F)
w · M h(ei,ki) − M �log(Zi) (7)
i=1 i=1
= {arg max
ei,ki
</equation>
<page confidence="0.997202">
853
</page>
<table confidence="0.999875384615385">
MERT MIRA1 MIRA2 c-MIRA
C 0.0001 0.001 0.0001
8 dev 34.80 34.70 34.73 34.70
feat. 04 31.92 31.81 31.73 31.83
05 28.85 28.94 28.71 28.92
C 0.001 0.001 0.001
all dev 34.61 35.24 35.14 35.56
feat. 04 31.76 32.25 32.04 32.57+
05 28.85 29.43 29.37 29.41
06news 30.91 31.43 31.24 31.82+
06others 27.43 28.01 28.13 28.45
08news 25.62 26.11 26.03 26.40
08others 16.22 16.66 16.46 17.10+
</table>
<tableCaption confidence="0.996918">
Table 1: BLEUs (%) on the dev and test sets with 8 dense
features only and all features. The significant symbols (+
at 0.05 level) are compared with MIRA2
</tableCaption>
<bodyText confidence="0.999961">
The epoch size for MIRA1 and MIRA2 is 40, while
the one for c-MIRA is 400. c-MIRA runs more
epochs, because we update the parameters by much
fewer times. However, we can implement Line 3-8
in Algorithm 1 in multi-thread (we use eight threads
in the following experiments), which makes our al-
gorithm much faster. Also, we increase the epoch
sizes of MIRA1 and MIRA2 to 400, and find there
is no improvement on their performance.
</bodyText>
<subsectionHeader confidence="0.938934">
4.1 Iterative Batch Training
</subsectionHeader>
<bodyText confidence="0.995726941176471">
In this experiment, we conduct the batch tuning pro-
cedure shown in section 3. We align the FBIS data
including about 230K sentence pairs with GIZA++
for extracting grammar, and train a 4-gram language
model on the Xinhua portion of Gigaword corpus. A
hierarchical phrase-based model (Chiang, 2007) is
tuned on NIST MT 2002, which has 878 sentences,
and tested on MT 2004, 2005, 2006, and 2008. All
features used here, besides eight basic ones in (Chi-
ang, 2007), consists of an extra 220 group features.
We design such feature templates to group gram-
mar by the length of source side and target side,
(feat type, a &lt; src side &lt; b, c &lt; tgt side &lt; d) ,
where feat type denotes any of relative frequency,
reversed relative frequency, lexical probability and
reversed lexical probability, and [a, b], [c, d] enumer-
ate all possible subranges of [1,10], as the maximum
</bodyText>
<table confidence="0.9770895">
MERT MIRA1 MIRA2 c-MIRA
R. T. 25.8min 16.0min 7.3min 7.8min
</table>
<tableCaption confidence="0.999344">
Table 2: Running time.
</tableCaption>
<bodyText confidence="0.999990652173913">
length on each side of a hierarchical grammar is lim-
ited to 10. There are 4 x 55 extra group features. We
also set the size of N-best list per sentence before
merge as 200.
All methods use 30 decoding iterations. We se-
lect the iteration with the best BLEU of the dev set
for testing. We present the BLEU scores in Table 1
on two feature settings: (1) 8 basic features only, and
(2) all 228 features. In the first case, due to the small
feature size, MERT can get a better BLEU of the
dev set, and all MIRA algorithms fails to generally
beat MERT on the test set. However, as the feature
size increase to 228, MERT degrades on the dev-set
BLEU, and also become worse on test sets, while
MIRA algorithms improve on the dev set expect-
edly. MIRA1 performs better than MIRA2, proba-
bly because of more constraints. c-MIRA can mod-
erately improve BLEU by 0.2-0.4 from MIRA1
and 0.2-0.6 from MIRA2. This might indicate that
a loss defined on corpus is more accurate than the
one defined on sentence. Table 2 lists the running
time. Only MIRA2 is fairly faster than c-MIRA be-
cause of more epochs in c-MIRA.
</bodyText>
<subsectionHeader confidence="0.992742">
4.2 Re-ranking Experiments
</subsectionHeader>
<bodyText confidence="0.999926235294118">
The baseline system is a state-of-the-art hierarchi-
cal phrase-based system, and trained on six million
parallel sentences corpora available to the DARPA
BOLT Chinese-English task. This system includes
51 dense features (including translation probabili-
ties, provenance features, etc.) and about 50k sparse
features (mostly lexical and fertility-based). The
language model is a six-gram model trained on a
10 billion words monolingual corpus, including the
English side of our parallel corpora plus other cor-
pora such as Gigaword (LDC2011T07) and Google
News. We use 1275 sentences for tuning and 1239
sentences for testing from the LDC2010E30 corpus
respectively. There are four reference translations
for each input sentence in both tuning and testing
datasets.
We use a N-best list which is an intermediate out-
</bodyText>
<page confidence="0.99702">
854
</page>
<table confidence="0.9988424">
MIRA, MIRA2 c-MIRA
dense dev 31.90 31.78 32.00
only test 30.89 30.89 31.07
dense dev 32.29 32.20 32.49
+sparse test 31.12 31.00 31.39
</table>
<tableCaption confidence="0.965459">
Table 3: BLEUs (%) on re-ranking experiments.
</tableCaption>
<table confidence="0.8178995">
MIRA, MIRA2 c-MIRA
about 1,966,720 35,120 400
</table>
<tableCaption confidence="0.999447">
Table 4: Times of updating model parameters.
</tableCaption>
<bodyText confidence="0.9999574">
put of the baseline system optimized on TER-BLEU
instead of BLEU. Before the re-ranking task, the ini-
tial BLEUs of the top-1 hypotheses on the tuning
and testing set are 31.45 and 30.56. The average
numbers of hypotheses per sentence are about 200
and 500, respectively for the tuning and testing sets.
Again, we use the best epoch on the tuning set for
testing. The BLEUs on dev and test sets are reported
in Table 3. We observe that the effectiveness of c-
MIRA is not harmed as the feature size is scaled up.
</bodyText>
<subsectionHeader confidence="0.999548">
4.3 Analysis
</subsectionHeader>
<bodyText confidence="0.99994590625">
To examine the simple search for hopes and fears
(Line 3∼8 in Alg. 1), we use two hope/fear building
strategies to get £* and £&apos; : (1) simply connect each
ez and ez in Line 4∼5 of Algorithm 1, (2) conduct a
slow beam search among the N-best lists of all for-
eign sentences from el to em and use Eq. 5 and
6 to prune the stack. The stack size is 10. We ob-
serve that there is no significant difference between
the two strategies on the BLEU of the dev set. But
the second strategy is about 10 times slower.
We also consider more constraints in Eq. 3. By
beam search, we obtain one corpus-level oracle and
29 other hypotheses similar to (Chiang et al., 2008),
and optimize with SMO (Platt, 1998). Unfortu-
nately, experiments show that more constraints lead
to an overfitting and no improved performance.
As shown in Table 4, in one execution, our
method updates the parameters by only 400 times;
MIRA2 updates by 40 x 878 = 35120 times; and
MIRA1 updates much more (about 1,966,720 times)
due to the SMO procedure. We are surprised to find
c-MIRA gets a higher training BLEU with such few
parameter updates. This probably suggests that there
is a gap between sentence-level BLEU and corpus-
level BLEU, so standard MIRAs need to update the
parameters more often.
Regarding simplicity, MIRA1 uses a strongly-
heuristic definition of a sentence BLEU, and
MIRA2 needs a pseudo-document with a decay rate
of -y = 0.9. In comparison, c-MIRA avoids both
the sentence level BLEU and the pseudo-document,
thus needs fewer variables.
</bodyText>
<sectionHeader confidence="0.998864" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999882538461539">
We present a simple and effective MIRA batch tun-
ing algorithm without the heuristic-driven calcula-
tion of sentence-level BLEU, due to the indecom-
posability of a corpus-level BLEU. Our optimiza-
tion objective is directly defined on the corpus-level
hypotheses. This work simplifies the tuning pro-
cess, and avoid the mismatch between the sentence-
level BLEU and the corpus-level BLEU. This strat-
egy can be potentially applied to other optimiza-
tion paradigms, such as the structural SVM (Cherry
and Foster, 2012), SGD and AROW (Chiang, 2012),
and other forms of samples, such as forests (Chiang,
2012) and lattice (Cherry and Foster, 2012).
</bodyText>
<sectionHeader confidence="0.999435" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.99968725">
The key idea and a part of the experimental work
of this paper were developed in collaboration with
the IBM researcher when the first author was an in-
tern at IBM T.J. Watson Research Center. This re-
search is partially supported by Air Force Office of
Scientific Research under grant FA9550-10-1-0335,
the National Science Foundation under grant IIS RI-
small 1218863 and a Google research award.
</bodyText>
<sectionHeader confidence="0.998247" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9920802">
A. Arun, C. Dyer, B. Haddow, P. Blunsom, A. Lopez,
and P. Koehn. 2009. Monte Carlo inference and maxi-
mization for phrase-based translation. In Proceedings
of the Thirteenth Conference on Computational Natu-
ral Language Learning (CoNLL), 102-110.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (NAACL-HLT), 427-436.
</reference>
<page confidence="0.994647">
855
</page>
<reference confidence="0.999869506493506">
D. Chiang. 2012. Hope and fear for discriminative train-
ing of statistical translation models. Journal of Ma-
chine Learning Research (JMLR), 1159-1187.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies (NAACL-HLT), 218-226.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. of Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), 224-
233.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201-228.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research
(JMLR), 7:551-585.
V. Eidelman. 2012. Optimization strategies for online
large-margin learning in machine translation. Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, 480-489.
B. Haddow, A. Arun, and P. Koehn. 2011. SampleRank
training for phrase-based machine translation. Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation. Association for Computational Linguis-
tics, 261-271.
P. Koehn, H. Hoang, A. Birch, C. Burch, M. Federico, N.
Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C.
Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. Proceedings of the Annual Meeting of
the Association for Computational Linguistics (ACL),
177-180.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Com-
putational Linguistics, 761-768.
C. Lin and F. Och. 2004. Orange: a method for evaluat-
ing automatic evaluation metrics for machine transla-
tion. In Proc. of International Conference on Compu-
tational Linguistics (COLING), No. 501.
F. Och. 2003. Minimum error rate training in statistical
machine translation. Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics
(ACL), 160-167.
F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics (ACL),
295-302.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. Proceedings of the 40th annual meeting on
association for computational linguistics. Association
for Computational Linguistics (ACL), 311-318.
J. Platt. 1998. Sequetial minimal optimization: A fast al-
gorithm for training support vector machines. In Tech-
nical Report MST-TR-98-14. Microsoft Research.
I. Tsochantaridis, T. Hofman, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interde-
pendent and structured output spaces. International
Conference on Machine Learning (ICML), 823-830.
T. Watanabe. 2012. Optimized online rank learning for
machine translation. Proceedings of Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (NAACL-HLT), 253-262.
T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. Proceedings of Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), 764-773.
</reference>
<page confidence="0.999005">
856
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.334073">
<title confidence="0.999768">A Corpus Level MIRA Tuning Strategy for Machine Translation</title>
<author confidence="0.8038975">Ming Tan</author>
<author confidence="0.8038975">Tian Xia</author>
<author confidence="0.8038975">Shaojun Wright State</author>
<address confidence="0.9337515">3640 Colonel Glenn Dayton, OH 45435</address>
<email confidence="0.9047155">xia.7,@wright.edu</email>
<author confidence="0.884103">Bowen</author>
<affiliation confidence="0.994358">IBM T.J. Watson Research</affiliation>
<address confidence="0.9700225">1101 Kitchawan Yorktown Heights, NY 10598</address>
<email confidence="0.999586">zhou@us.ibm.com</email>
<abstract confidence="0.998046357142857">MIRA based tuning methods have been widely used in statistical machine translation (SMT) system with a large number of features. Since the corpus-level BLEU is not decomposable, these MIRA approaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses. Instead, we present a new MIRA method, which employs an exact corpus-level BLEU to compute the model loss. Our method is simpler in implementation. Experiments on Chinese-to- English translation show its effectiveness over two state-of-the-art MIRA implementations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Arun</author>
<author>C Dyer</author>
<author>B Haddow</author>
<author>P Blunsom</author>
<author>A Lopez</author>
<author>P Koehn</author>
</authors>
<title>Monte Carlo inference and maximization for phrase-based translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>102--110</pages>
<contexts>
<context position="1820" citStr="Arun et al., 2009" startWordPosition="280" endWordPosition="283">tences, most MIRA approaches consider a variety of sentence-level BLEUs for the model losses, many of which are heuristic-driven (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Cherry and Foster, 2012). The sentence-level BLEU appearing in the objective is generally based on a pseudo-document, which may not precisely reflect the corpus-level BLEU. We believe that this mismatch could potentially harm the performance. To avoid the sentence BLEU, the work in (Haddow et al., 2011) proposed to process sentences in small batches. The authors adopted a Gibbs sampling (Arun et al., 2009) technique to search the hope and fear hypotheses, and they did not compare with MIRA. Watanabe (2012) also tuned the parameters with small batches of sentences and optimized a hinge loss not explicitly related to BLEU using stochastic gradient descent. Both approaches introduced additional complexities over baseline MIRA approaches. In contrast, we propose a remarkably simple but efficient batch MIRA approach which exploits the exact corpus-level BLEU to compute model losses. We search for a hope and a fear hypotheses for the corpus with a straightforward approach and minimize the structured </context>
</contexts>
<marker>Arun, Dyer, Haddow, Blunsom, Lopez, Koehn, 2009</marker>
<rawString>A. Arun, C. Dyer, B. Haddow, P. Blunsom, A. Lopez, and P. Koehn. 2009. Monte Carlo inference and maximization for phrase-based translation. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), 102-110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cherry</author>
<author>G Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),</booktitle>
<pages>427--436</pages>
<contexts>
<context position="1136" citStr="Cherry and Foster, 2012" startWordPosition="169" endWordPosition="172">proaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses. Instead, we present a new MIRA method, which employs an exact corpus-level BLEU to compute the model loss. Our method is simpler in implementation. Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations. 1 Introduction Margin infused relaxed algorithm (MIRA) has been widely adopted for the parameter optimization in SMT with a large feature size (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Eidelman, 2012; Cherry and Foster, 2012). Since BLEU is defined on the corpus, and not decomposed into sentences, most MIRA approaches consider a variety of sentence-level BLEUs for the model losses, many of which are heuristic-driven (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Cherry and Foster, 2012). The sentence-level BLEU appearing in the objective is generally based on a pseudo-document, which may not precisely reflect the corpus-level BLEU. We believe that this mismatch could potentially harm the performance. To avoid the sentence BLEU, the work in (Haddow et al., 2011) proposed to process </context>
<context position="4908" citStr="Cherry and Foster, 2012" startWordPosition="799" endWordPosition="802">), (b) fit ei9 into a pseudodocument considering the history (Chiang et al., 2008; Chiang, 2012), (c) use ei9 to replace the corresponding hypothesis in the oracles (Watanabe et al., 2007). The sentence-level BLEU sometimes perplexes the algorithms and results in a mismatch with the corpus-level BLEU. 3 Corpus-level MIRA 3.1 Algorithm We propose a batch tuning strategy, corpus-level MIRA (c-MIRA), in which an objective is not built upon a hinge loss of a single sentence, but upon that of the entire corpus. The online MIRAs are difficult to parallelize. Therefore, similar to the batch MIRA in (Cherry and Foster, 2012), we conduct the batch tuning by repeating the following steps: (a) Decode source sentences (in parallel) and obtain {ei}M1, (b) Merge {ei}M1 with the one from the previous iteration, (c) Invoke Algorithm 1. We define E = (eE,1, eE,2, ..., eE,M) as a corpus EM 1 hypothesis, with H (E) = M i=1 hypothesis of the source sentence fi covered by E. E is corresponded to a corpus-level BLEU, which we ultimately want to optimize. Following MIRA formulated in (Crammer et al., 2006; Chiang et al., where B(·) is a corpus-level BLEU. E∗ is a hope hypothesis. E ∈ L, where L is the hypothesis space of the en</context>
<context position="7237" citStr="Cherry and Foster, 2012" startWordPosition="1277" endWordPosition="1280">max [w · H(E) + B(E)] (5) E E0 = max[w · H(E) − B(E)] (6) E Eq. 5 and 6 find the hypotheses with the best and worse BLEU that the decoder can easily achieve. It is unnecessary to search the entire space of L for precise solution E∗and E0, because MIRA only at1 wt 15: wt = t + 1 t E t=0 l;(w) = max {b(e∗i ) − b(eij) eij −w · [h(e∗i ) − h(eij)]} (2) h(eE,i). eE,i is the [wt−1 · h(eij) + b0(eij)] [wt−1 · h(eij) − b0(eij)] 852 loss of c-MIRA in Eq. 4 is, lcorpus(w) ∝ max E0 M [B(ei,kE∗) − B(ei,kE0) i=1 tempts to separate the hope from the fear by a margin proportional to their BLEU differentials (Cherry and Foster, 2012). We just construct E*and E&apos; respectively by, −w·h(ei,kE∗) + w · h(ei,kE0 )] e£*,i = max [w · h(ei,j) + b&apos;(ei,j)] = M max [B(ei,kE∗) − B(eij) ei,j i=1 eii e£,,i = max [w · h(ei,j) − b&apos;(ei,j)] −w·h(ei,kE∗) + w · h(eij)] = M li(w) ei,j i=1 where b&apos; is simply a BLEU with add one smoothing (Lin and Och, 2004). A smoothed BLEU is good enough to pick up a “satisfying” pair of hope and fear. However, the updating step (Line 11) uses the corpus-level BLEU. 3.2 Justification c-MIRA treats a corpus as one sentence for decoding, while conventional decoders process sentences one by one. We show the optima</context>
<context position="8656" citStr="Cherry and Foster, 2012" startWordPosition="1544" endWordPosition="1547">robability given the source corpus F = {f1, f2, ..., fm}, E = arg max E = arg max E w · h(ei,ki)}M (8) i=1 where Zi = E (fi) j�1exp(w · h(ei,j)), which is a constant with respective to E. Eq. 7 shows that the feature vector of E is determined by the sum of each candidate’s feature vectors. Also, the model score can be decomposed into each sentence in Eq. 8, which shows that decoding all sentences together equals to decoding one by one. We also show that if the metric is decomposable, the loss in c-MIRA is actually the sum of the hinge loss li(w) in structural SVM (Tsochantaridis et al., 2004; Cherry and Foster, 2012). We assume B(eij) to be the metric of a sentence hypothesis, then the Instead of adopting a cutting-plane algorithm (Tsochantaridis et al., 2004), we optimize the same loss with a MIRA pattern in a simpler way. However, since BLEU is not decomposable, the structural SVM (Cherry and Foster, 2012) uses an interpolated sentence BLEU (Liang et al., 2006). Although Algorithm 1 has an outlook similar to the batch-MIRA algorithm in (Cherry and Foster, 2012), their loss definitions differ fundamentally. Batch MIRA basically uses a sentence-level loss, and they also follow the sentence-by-sentence tun</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>C. Cherry and G. Foster. 2012. Batch tuning strategies for statistical machine translation. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 427-436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hope and fear for discriminative training of statistical translation models.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>1159--1187</pages>
<contexts>
<context position="1094" citStr="Chiang, 2012" startWordPosition="165" endWordPosition="166">ot decomposable, these MIRA approaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses. Instead, we present a new MIRA method, which employs an exact corpus-level BLEU to compute the model loss. Our method is simpler in implementation. Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations. 1 Introduction Margin infused relaxed algorithm (MIRA) has been widely adopted for the parameter optimization in SMT with a large feature size (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Eidelman, 2012; Cherry and Foster, 2012). Since BLEU is defined on the corpus, and not decomposed into sentences, most MIRA approaches consider a variety of sentence-level BLEUs for the model losses, many of which are heuristic-driven (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Cherry and Foster, 2012). The sentence-level BLEU appearing in the objective is generally based on a pseudo-document, which may not precisely reflect the corpus-level BLEU. We believe that this mismatch could potentially harm the performance. To avoid the sentence BLEU, the work in </context>
<context position="4380" citStr="Chiang, 2012" startWordPosition="714" endWordPosition="715">orpus(w) (3) min w min �||w − w0||2 + C · li(w) (1) loo�ug(w) = max{B(E∗) − B(E) w 1 E −w · [H(E∗) − H(E)]} (4) where e∗i ∈ ei is a hope candidate, w0 is the parameter vector from the last sentence. Since MIRA defines its objective only based on the current sentence, b(·) is a sentence-level BLEU. Most MIRA algorithms need a deliberate definition of b(·), since BLEU cannot be decomposed into sentences. The types of the sentence BLEU calculation includes: (a) a smoothed version of BLEU for ei9 (Liang et al., 2006), (b) fit ei9 into a pseudodocument considering the history (Chiang et al., 2008; Chiang, 2012), (c) use ei9 to replace the corresponding hypothesis in the oracles (Watanabe et al., 2007). The sentence-level BLEU sometimes perplexes the algorithms and results in a mismatch with the corpus-level BLEU. 3 Corpus-level MIRA 3.1 Algorithm We propose a batch tuning strategy, corpus-level MIRA (c-MIRA), in which an objective is not built upon a hinge loss of a single sentence, but upon that of the entire corpus. The online MIRAs are difficult to parallelize. Therefore, similar to the batch MIRA in (Cherry and Foster, 2012), we conduct the batch tuning by repeating the following steps: (a) Deco</context>
</contexts>
<marker>Chiang, 2012</marker>
<rawString>D. Chiang. 2012. Hope and fear for discriminative training of statistical translation models. Journal of Machine Learning Research (JMLR), 1159-1187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>K Knight</author>
<author>W Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),</booktitle>
<pages>218--226</pages>
<contexts>
<context position="1080" citStr="Chiang et al., 2009" startWordPosition="161" endWordPosition="164">orpus-level BLEU is not decomposable, these MIRA approaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses. Instead, we present a new MIRA method, which employs an exact corpus-level BLEU to compute the model loss. Our method is simpler in implementation. Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations. 1 Introduction Margin infused relaxed algorithm (MIRA) has been widely adopted for the parameter optimization in SMT with a large feature size (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Eidelman, 2012; Cherry and Foster, 2012). Since BLEU is defined on the corpus, and not decomposed into sentences, most MIRA approaches consider a variety of sentence-level BLEUs for the model losses, many of which are heuristic-driven (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Cherry and Foster, 2012). The sentence-level BLEU appearing in the objective is generally based on a pseudo-document, which may not precisely reflect the corpus-level BLEU. We believe that this mismatch could potentially harm the performance. To avoid the sentence BLEU</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new features for statistical machine translation. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 218-226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>Y Marton</author>
<author>P Resnik</author>
</authors>
<title>Online largemargin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proc. of Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>224--233</pages>
<contexts>
<context position="1059" citStr="Chiang et al., 2008" startWordPosition="157" endWordPosition="160">features. Since the corpus-level BLEU is not decomposable, these MIRA approaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses. Instead, we present a new MIRA method, which employs an exact corpus-level BLEU to compute the model loss. Our method is simpler in implementation. Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations. 1 Introduction Margin infused relaxed algorithm (MIRA) has been widely adopted for the parameter optimization in SMT with a large feature size (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Eidelman, 2012; Cherry and Foster, 2012). Since BLEU is defined on the corpus, and not decomposed into sentences, most MIRA approaches consider a variety of sentence-level BLEUs for the model losses, many of which are heuristic-driven (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Cherry and Foster, 2012). The sentence-level BLEU appearing in the objective is generally based on a pseudo-document, which may not precisely reflect the corpus-level BLEU. We believe that this mismatch could potentially harm the performance. To av</context>
<context position="3349" citStr="Chiang et al., 2008" startWordPosition="533" endWordPosition="536">triples {(fi, ei, ri)}Mi=1, where fi is a source-language sentence, corresponded by a list of target-language hypotheses ei = {eij}N(f�) j=1 , with a number of references ri. h(eij) is a feature vector. Generally, most decoders return a top-1 candidate as the translation result, such that ei(w) = arg maxj w · h(eij ), where w are the model parameters. In this paper, we aim at optimizing the BLEU score (Papineni et al., 2002). MIRA is an instance of online learning which assumes an overlap of the decoding procedure and the parameter optimization procedure. For example in (Crammer et al., 2006; Chiang et al., 2008), MIRA 851 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 851–856, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics is performed after an input sentence are decoded, and the next sentence is decoded with the updated parameters. The objective for each sentence i is, 2008), c-MIRA repeatedly optimizes, 2||w − w 0||2 + C C. lcorpus(w) (3) min w min �||w − w0||2 + C · li(w) (1) loo�ug(w) = max{B(E∗) − B(E) w 1 E −w · [H(E∗) − H(E)]} (4) where e∗i ∈ ei is a hope candidate, w0 is the parameter vector from th</context>
<context position="6551" citStr="Chiang et al., 2008" startWordPosition="1133" endWordPosition="1136">C, 4�+wt−1·4H 11: α = min ||4H||2 12: wt = wt−1 − α · 4H 13: end for 14: return wt with the optimal BLEU on the dev set. c-MIRA can be regarded as a standard MIRA, in which there is only one single triple (F, L, R), where F and R are the source and reference of the corpus respectively. Eq. 3 is equivalent to a quadratic programming with |L |constraints. Crammer et al. (2006) show that a single constraint with one hope E∗ and one fear E0 admits a closed-form update and performs well. We denote one execution of the outer loop as an epoch. The hope and fear are updated in each epoch. Similar to (Chiang et al., 2008), the hope and fear hypotheses are defined as following, E∗ = max [w · H(E) + B(E)] (5) E E0 = max[w · H(E) − B(E)] (6) E Eq. 5 and 6 find the hypotheses with the best and worse BLEU that the decoder can easily achieve. It is unnecessary to search the entire space of L for precise solution E∗and E0, because MIRA only at1 wt 15: wt = t + 1 t E t=0 l;(w) = max {b(e∗i ) − b(eij) eij −w · [h(e∗i ) − h(eij)]} (2) h(eE,i). eE,i is the [wt−1 · h(eij) + b0(eij)] [wt−1 · h(eij) − b0(eij)] 852 loss of c-MIRA in Eq. 4 is, lcorpus(w) ∝ max E0 M [B(ei,kE∗) − B(ei,kE0) i=1 tempts to separate the hope from t</context>
<context position="9790" citStr="Chiang et al., 2008" startWordPosition="1729" endWordPosition="1732">asically uses a sentence-level loss, and they also follow the sentence-by-sentence tuning pattern. In the future work, we will compare structural SVM and c-MIRA under decomposable metrics like WER or SSER (Och and Ney, 2002). 4 Experiments and Analysis We first evaluate c-MIRA in a iterative batch tuning procedure in a Chinese-to-English machine translation system with 228 features. Second, we show cMIRA is also effective in the re-ranking task with more than 50,000 features. In both experiments, we compare c-MIRA and three baselines: (1) MERT (Och, 2003), (2) Chiang et al.’s MIRA (MIRA1) in (Chiang et al., 2008). (3) batch-MIRA (MIRA2) in (Cherry and Foster, 2012). Here, we roughly choose C with the best BLEU on dev set, from {0.1, 0.01, 0.001, 0.0001, 0.00001}. We convert Chiang et al.’s MIRA to the batch mode described in section 3.1. So the only difference between MIRA1 and MIRA2 is: MIRA1 obtains multiple constraints before optimization, while MIRA2 only uses one constraint. We implement MERT and MIRA1, and directly use MIRA2 from Moses (Koehn et al., 2007). We conduct experiments in a server of 8-cores with 2.5GHz Opteron. We set the maximum number of epochs as we generally do not observe an obv</context>
<context position="15818" citStr="Chiang et al., 2008" startWordPosition="2794" endWordPosition="2797">opes and fears (Line 3∼8 in Alg. 1), we use two hope/fear building strategies to get £* and £&apos; : (1) simply connect each ez and ez in Line 4∼5 of Algorithm 1, (2) conduct a slow beam search among the N-best lists of all foreign sentences from el to em and use Eq. 5 and 6 to prune the stack. The stack size is 10. We observe that there is no significant difference between the two strategies on the BLEU of the dev set. But the second strategy is about 10 times slower. We also consider more constraints in Eq. 3. By beam search, we obtain one corpus-level oracle and 29 other hypotheses similar to (Chiang et al., 2008), and optimize with SMO (Platt, 1998). Unfortunately, experiments show that more constraints lead to an overfitting and no improved performance. As shown in Table 4, in one execution, our method updates the parameters by only 400 times; MIRA2 updates by 40 x 878 = 35120 times; and MIRA1 updates much more (about 1,966,720 times) due to the SMO procedure. We are surprised to find c-MIRA gets a higher training BLEU with such few parameter updates. This probably suggests that there is a gap between sentence-level BLEU and corpuslevel BLEU, so standard MIRAs need to update the parameters more often</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>D. Chiang, Y. Marton, and P. Resnik. 2008. Online largemargin training of syntactic and structural translation features. In Proc. of Conference on Empirical Methods in Natural Language Processing (EMNLP), 224-233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>33--2</pages>
<contexts>
<context position="11784" citStr="Chiang, 2007" startWordPosition="2080" endWordPosition="2081">fewer times. However, we can implement Line 3-8 in Algorithm 1 in multi-thread (we use eight threads in the following experiments), which makes our algorithm much faster. Also, we increase the epoch sizes of MIRA1 and MIRA2 to 400, and find there is no improvement on their performance. 4.1 Iterative Batch Training In this experiment, we conduct the batch tuning procedure shown in section 3. We align the FBIS data including about 230K sentence pairs with GIZA++ for extracting grammar, and train a 4-gram language model on the Xinhua portion of Gigaword corpus. A hierarchical phrase-based model (Chiang, 2007) is tuned on NIST MT 2002, which has 878 sentences, and tested on MT 2004, 2005, 2006, and 2008. All features used here, besides eight basic ones in (Chiang, 2007), consists of an extra 220 group features. We design such feature templates to group grammar by the length of source side and target side, (feat type, a &lt; src side &lt; b, c &lt; tgt side &lt; d) , where feat type denotes any of relative frequency, reversed relative frequency, lexical probability and reversed lexical probability, and [a, b], [c, d] enumerate all possible subranges of [1,10], as the maximum MERT MIRA1 MIRA2 c-MIRA R. T. 25.8mi</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>7--551</pages>
<contexts>
<context position="3327" citStr="Crammer et al., 2006" startWordPosition="529" endWordPosition="532">(dev) set is a set of triples {(fi, ei, ri)}Mi=1, where fi is a source-language sentence, corresponded by a list of target-language hypotheses ei = {eij}N(f�) j=1 , with a number of references ri. h(eij) is a feature vector. Generally, most decoders return a top-1 candidate as the translation result, such that ei(w) = arg maxj w · h(eij ), where w are the model parameters. In this paper, we aim at optimizing the BLEU score (Papineni et al., 2002). MIRA is an instance of online learning which assumes an overlap of the decoding procedure and the parameter optimization procedure. For example in (Crammer et al., 2006; Chiang et al., 2008), MIRA 851 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 851–856, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics is performed after an input sentence are decoded, and the next sentence is decoded with the updated parameters. The objective for each sentence i is, 2008), c-MIRA repeatedly optimizes, 2||w − w 0||2 + C C. lcorpus(w) (3) min w min �||w − w0||2 + C · li(w) (1) loo�ug(w) = max{B(E∗) − B(E) w 1 E −w · [H(E∗) − H(E)]} (4) where e∗i ∈ ei is a hope candidate, w0 is the pa</context>
<context position="5383" citStr="Crammer et al., 2006" startWordPosition="884" endWordPosition="887"> upon that of the entire corpus. The online MIRAs are difficult to parallelize. Therefore, similar to the batch MIRA in (Cherry and Foster, 2012), we conduct the batch tuning by repeating the following steps: (a) Decode source sentences (in parallel) and obtain {ei}M1, (b) Merge {ei}M1 with the one from the previous iteration, (c) Invoke Algorithm 1. We define E = (eE,1, eE,2, ..., eE,M) as a corpus EM 1 hypothesis, with H (E) = M i=1 hypothesis of the source sentence fi covered by E. E is corresponded to a corpus-level BLEU, which we ultimately want to optimize. Following MIRA formulated in (Crammer et al., 2006; Chiang et al., where B(·) is a corpus-level BLEU. E∗ is a hope hypothesis. E ∈ L, where L is the hypothesis space of the entire corpus, and |L |= |e1 |· · · |eM|. Algorithm 1 Corpus-Level MIRA Require: {(fi, ei, ri)}Mi=1, w0, C 1: for t = 1···T do 2: E∗ = {} ,E0 = {} &gt; Initialize the hope and fear 3: for i = 1···M do 4: eE*,i = arg max eij 5: eE,,i = arg max eij 6: E∗ ← E∗ + {eE+,i} &gt; Build the hope 7: E0 ← E0 + {eE�,i} &gt; Build the fear 8: end for 9: 4B = B(E∗) − B(E0) &gt; the BLEU difference 10: 4H = H(E0) − H(E∗) &gt; the feature difference � C, 4�+wt−1·4H 11: α = min ||4H||2 12: wt = wt−1 − α </context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research (JMLR), 7:551-585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Eidelman</author>
</authors>
<title>Optimization strategies for online large-margin learning in machine translation.</title>
<date>2012</date>
<booktitle>Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>480--489</pages>
<contexts>
<context position="1110" citStr="Eidelman, 2012" startWordPosition="167" endWordPosition="168">e, these MIRA approaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses. Instead, we present a new MIRA method, which employs an exact corpus-level BLEU to compute the model loss. Our method is simpler in implementation. Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations. 1 Introduction Margin infused relaxed algorithm (MIRA) has been widely adopted for the parameter optimization in SMT with a large feature size (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Eidelman, 2012; Cherry and Foster, 2012). Since BLEU is defined on the corpus, and not decomposed into sentences, most MIRA approaches consider a variety of sentence-level BLEUs for the model losses, many of which are heuristic-driven (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Cherry and Foster, 2012). The sentence-level BLEU appearing in the objective is generally based on a pseudo-document, which may not precisely reflect the corpus-level BLEU. We believe that this mismatch could potentially harm the performance. To avoid the sentence BLEU, the work in (Haddow et al., </context>
</contexts>
<marker>Eidelman, 2012</marker>
<rawString>V. Eidelman. 2012. Optimization strategies for online large-margin learning in machine translation. Proceedings of the Seventh Workshop on Statistical Machine Translation, 480-489.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Haddow</author>
<author>A Arun</author>
<author>P Koehn</author>
</authors>
<title>SampleRank training for phrase-based machine translation.</title>
<date>2011</date>
<booktitle>Proceedings of the Sixth Workshop on Statistical Machine Translation. Association for Computational Linguistics,</booktitle>
<pages>261--271</pages>
<contexts>
<context position="1715" citStr="Haddow et al., 2011" startWordPosition="263" endWordPosition="266"> Eidelman, 2012; Cherry and Foster, 2012). Since BLEU is defined on the corpus, and not decomposed into sentences, most MIRA approaches consider a variety of sentence-level BLEUs for the model losses, many of which are heuristic-driven (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Cherry and Foster, 2012). The sentence-level BLEU appearing in the objective is generally based on a pseudo-document, which may not precisely reflect the corpus-level BLEU. We believe that this mismatch could potentially harm the performance. To avoid the sentence BLEU, the work in (Haddow et al., 2011) proposed to process sentences in small batches. The authors adopted a Gibbs sampling (Arun et al., 2009) technique to search the hope and fear hypotheses, and they did not compare with MIRA. Watanabe (2012) also tuned the parameters with small batches of sentences and optimized a hinge loss not explicitly related to BLEU using stochastic gradient descent. Both approaches introduced additional complexities over baseline MIRA approaches. In contrast, we propose a remarkably simple but efficient batch MIRA approach which exploits the exact corpus-level BLEU to compute model losses. We search for</context>
</contexts>
<marker>Haddow, Arun, Koehn, 2011</marker>
<rawString>B. Haddow, A. Arun, and P. Koehn. 2011. SampleRank training for phrase-based machine translation. Proceedings of the Sixth Workshop on Statistical Machine Translation. Association for Computational Linguistics, 261-271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>177--180</pages>
<contexts>
<context position="10248" citStr="Koehn et al., 2007" startWordPosition="1807" endWordPosition="1810">than 50,000 features. In both experiments, we compare c-MIRA and three baselines: (1) MERT (Och, 2003), (2) Chiang et al.’s MIRA (MIRA1) in (Chiang et al., 2008). (3) batch-MIRA (MIRA2) in (Cherry and Foster, 2012). Here, we roughly choose C with the best BLEU on dev set, from {0.1, 0.01, 0.001, 0.0001, 0.00001}. We convert Chiang et al.’s MIRA to the batch mode described in section 3.1. So the only difference between MIRA1 and MIRA2 is: MIRA1 obtains multiple constraints before optimization, while MIRA2 only uses one constraint. We implement MERT and MIRA1, and directly use MIRA2 from Moses (Koehn et al., 2007). We conduct experiments in a server of 8-cores with 2.5GHz Opteron. We set the maximum number of epochs as we generally do not observe an obvious increase on the dev set BLEU. loge(E|F) w · M h(ei,ki) − M �log(Zi) (7) i=1 i=1 = {arg max ei,ki 853 MERT MIRA1 MIRA2 c-MIRA C 0.0001 0.001 0.0001 8 dev 34.80 34.70 34.73 34.70 feat. 04 31.92 31.81 31.73 31.83 05 28.85 28.94 28.71 28.92 C 0.001 0.001 0.001 all dev 34.61 35.24 35.14 35.56 feat. 04 31.76 32.25 32.04 32.57+ 05 28.85 29.43 29.37 29.41 06news 30.91 31.43 31.24 31.82+ 06others 27.43 28.01 28.13 28.45 08news 25.62 26.11 26.03 26.40 08other</context>
</contexts>
<marker>Koehn, Hoang, Birch, Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 177-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>A Bouchard-Cote</author>
<author>D Klein</author>
<author>B Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>761--768</pages>
<contexts>
<context position="4285" citStr="Liang et al., 2006" startWordPosition="696" endWordPosition="699">rs. The objective for each sentence i is, 2008), c-MIRA repeatedly optimizes, 2||w − w 0||2 + C C. lcorpus(w) (3) min w min �||w − w0||2 + C · li(w) (1) loo�ug(w) = max{B(E∗) − B(E) w 1 E −w · [H(E∗) − H(E)]} (4) where e∗i ∈ ei is a hope candidate, w0 is the parameter vector from the last sentence. Since MIRA defines its objective only based on the current sentence, b(·) is a sentence-level BLEU. Most MIRA algorithms need a deliberate definition of b(·), since BLEU cannot be decomposed into sentences. The types of the sentence BLEU calculation includes: (a) a smoothed version of BLEU for ei9 (Liang et al., 2006), (b) fit ei9 into a pseudodocument considering the history (Chiang et al., 2008; Chiang, 2012), (c) use ei9 to replace the corresponding hypothesis in the oracles (Watanabe et al., 2007). The sentence-level BLEU sometimes perplexes the algorithms and results in a mismatch with the corpus-level BLEU. 3 Corpus-level MIRA 3.1 Algorithm We propose a batch tuning strategy, corpus-level MIRA (c-MIRA), in which an objective is not built upon a hinge loss of a single sentence, but upon that of the entire corpus. The online MIRAs are difficult to parallelize. Therefore, similar to the batch MIRA in (C</context>
<context position="9009" citStr="Liang et al., 2006" startWordPosition="1605" endWordPosition="1608">, which shows that decoding all sentences together equals to decoding one by one. We also show that if the metric is decomposable, the loss in c-MIRA is actually the sum of the hinge loss li(w) in structural SVM (Tsochantaridis et al., 2004; Cherry and Foster, 2012). We assume B(eij) to be the metric of a sentence hypothesis, then the Instead of adopting a cutting-plane algorithm (Tsochantaridis et al., 2004), we optimize the same loss with a MIRA pattern in a simpler way. However, since BLEU is not decomposable, the structural SVM (Cherry and Foster, 2012) uses an interpolated sentence BLEU (Liang et al., 2006). Although Algorithm 1 has an outlook similar to the batch-MIRA algorithm in (Cherry and Foster, 2012), their loss definitions differ fundamentally. Batch MIRA basically uses a sentence-level loss, and they also follow the sentence-by-sentence tuning pattern. In the future work, we will compare structural SVM and c-MIRA under decomposable metrics like WER or SSER (Och and Ney, 2002). 4 Experiments and Analysis We first evaluate c-MIRA in a iterative batch tuning procedure in a Chinese-to-English machine translation system with 228 features. Second, we show cMIRA is also effective in the re-ran</context>
</contexts>
<marker>Liang, Bouchard-Cote, Klein, Taskar, 2006</marker>
<rawString>P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, 761-768.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
<author>F Och</author>
</authors>
<title>Orange: a method for evaluating automatic evaluation metrics for machine translation.</title>
<date>2004</date>
<booktitle>In Proc. of International Conference on Computational Linguistics (COLING), No. 501.</booktitle>
<contexts>
<context position="7543" citStr="Lin and Och, 2004" startWordPosition="1340" endWordPosition="1343">eij) eij −w · [h(e∗i ) − h(eij)]} (2) h(eE,i). eE,i is the [wt−1 · h(eij) + b0(eij)] [wt−1 · h(eij) − b0(eij)] 852 loss of c-MIRA in Eq. 4 is, lcorpus(w) ∝ max E0 M [B(ei,kE∗) − B(ei,kE0) i=1 tempts to separate the hope from the fear by a margin proportional to their BLEU differentials (Cherry and Foster, 2012). We just construct E*and E&apos; respectively by, −w·h(ei,kE∗) + w · h(ei,kE0 )] e£*,i = max [w · h(ei,j) + b&apos;(ei,j)] = M max [B(ei,kE∗) − B(eij) ei,j i=1 eii e£,,i = max [w · h(ei,j) − b&apos;(ei,j)] −w·h(ei,kE∗) + w · h(eij)] = M li(w) ei,j i=1 where b&apos; is simply a BLEU with add one smoothing (Lin and Och, 2004). A smoothed BLEU is good enough to pick up a “satisfying” pair of hope and fear. However, the updating step (Line 11) uses the corpus-level BLEU. 3.2 Justification c-MIRA treats a corpus as one sentence for decoding, while conventional decoders process sentences one by one. We show the optimal solutions from the two methods are equivalent theoretically. We follow the notations in (Och and Ney, 2002). We search a hypothesis on corpus E = {e1,ki , e2,k2, ..., eM,kM } with the highest probability given the source corpus F = {f1, f2, ..., fm}, E = arg max E = arg max E w · h(ei,ki)}M (8) i=1 wher</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>C. Lin and F. Och. 2004. Orange: a method for evaluating automatic evaluation metrics for machine translation. In Proc. of International Conference on Computational Linguistics (COLING), No. 501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<contexts>
<context position="9731" citStr="Och, 2003" startWordPosition="1720" endWordPosition="1721">ss definitions differ fundamentally. Batch MIRA basically uses a sentence-level loss, and they also follow the sentence-by-sentence tuning pattern. In the future work, we will compare structural SVM and c-MIRA under decomposable metrics like WER or SSER (Och and Ney, 2002). 4 Experiments and Analysis We first evaluate c-MIRA in a iterative batch tuning procedure in a Chinese-to-English machine translation system with 228 features. Second, we show cMIRA is also effective in the re-ranking task with more than 50,000 features. In both experiments, we compare c-MIRA and three baselines: (1) MERT (Och, 2003), (2) Chiang et al.’s MIRA (MIRA1) in (Chiang et al., 2008). (3) batch-MIRA (MIRA2) in (Cherry and Foster, 2012). Here, we roughly choose C with the best BLEU on dev set, from {0.1, 0.01, 0.001, 0.0001, 0.00001}. We convert Chiang et al.’s MIRA to the batch mode described in section 3.1. So the only difference between MIRA1 and MIRA2 is: MIRA1 obtains multiple constraints before optimization, while MIRA2 only uses one constraint. We implement MERT and MIRA1, and directly use MIRA2 from Moses (Koehn et al., 2007). We conduct experiments in a server of 8-cores with 2.5GHz Opteron. We set the max</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och. 2003. Minimum error rate training in statistical machine translation. Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL), 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL),</booktitle>
<pages>295--302</pages>
<contexts>
<context position="7946" citStr="Och and Ney, 2002" startWordPosition="1407" endWordPosition="1410"> · h(ei,j) + b&apos;(ei,j)] = M max [B(ei,kE∗) − B(eij) ei,j i=1 eii e£,,i = max [w · h(ei,j) − b&apos;(ei,j)] −w·h(ei,kE∗) + w · h(eij)] = M li(w) ei,j i=1 where b&apos; is simply a BLEU with add one smoothing (Lin and Och, 2004). A smoothed BLEU is good enough to pick up a “satisfying” pair of hope and fear. However, the updating step (Line 11) uses the corpus-level BLEU. 3.2 Justification c-MIRA treats a corpus as one sentence for decoding, while conventional decoders process sentences one by one. We show the optimal solutions from the two methods are equivalent theoretically. We follow the notations in (Och and Ney, 2002). We search a hypothesis on corpus E = {e1,ki , e2,k2, ..., eM,kM } with the highest probability given the source corpus F = {f1, f2, ..., fm}, E = arg max E = arg max E w · h(ei,ki)}M (8) i=1 where Zi = E (fi) j�1exp(w · h(ei,j)), which is a constant with respective to E. Eq. 7 shows that the feature vector of E is determined by the sum of each candidate’s feature vectors. Also, the model score can be decomposed into each sentence in Eq. 8, which shows that decoding all sentences together equals to decoding one by one. We also show that if the metric is decomposable, the loss in c-MIRA is act</context>
<context position="9394" citStr="Och and Ney, 2002" startWordPosition="1664" endWordPosition="1667">ochantaridis et al., 2004), we optimize the same loss with a MIRA pattern in a simpler way. However, since BLEU is not decomposable, the structural SVM (Cherry and Foster, 2012) uses an interpolated sentence BLEU (Liang et al., 2006). Although Algorithm 1 has an outlook similar to the batch-MIRA algorithm in (Cherry and Foster, 2012), their loss definitions differ fundamentally. Batch MIRA basically uses a sentence-level loss, and they also follow the sentence-by-sentence tuning pattern. In the future work, we will compare structural SVM and c-MIRA under decomposable metrics like WER or SSER (Och and Ney, 2002). 4 Experiments and Analysis We first evaluate c-MIRA in a iterative batch tuning procedure in a Chinese-to-English machine translation system with 228 features. Second, we show cMIRA is also effective in the re-ranking task with more than 50,000 features. In both experiments, we compare c-MIRA and three baselines: (1) MERT (Och, 2003), (2) Chiang et al.’s MIRA (MIRA1) in (Chiang et al., 2008). (3) batch-MIRA (MIRA2) in (Cherry and Foster, 2012). Here, we roughly choose C with the best BLEU on dev set, from {0.1, 0.01, 0.001, 0.0001, 0.00001}. We convert Chiang et al.’s MIRA to the batch mode </context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL), 295-302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<contexts>
<context position="3157" citStr="Papineni et al., 2002" startWordPosition="501" endWordPosition="504">s in Chinese-toEnglish translation tasks with a moderate margin. 2 Margin Infused Relaxed Algorithm We optimize the model parameters based on N-best lists. Our development (dev) set is a set of triples {(fi, ei, ri)}Mi=1, where fi is a source-language sentence, corresponded by a list of target-language hypotheses ei = {eij}N(f�) j=1 , with a number of references ri. h(eij) is a feature vector. Generally, most decoders return a top-1 candidate as the translation result, such that ei(w) = arg maxj w · h(eij ), where w are the model parameters. In this paper, we aim at optimizing the BLEU score (Papineni et al., 2002). MIRA is an instance of online learning which assumes an overlap of the decoding procedure and the parameter optimization procedure. For example in (Crammer et al., 2006; Chiang et al., 2008), MIRA 851 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 851–856, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics is performed after an input sentence are decoded, and the next sentence is decoded with the updated parameters. The objective for each sentence i is, 2008), c-MIRA repeatedly optimizes, 2||w − w 0||2</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics (ACL), 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Platt</author>
</authors>
<title>Sequetial minimal optimization: A fast algorithm for training support vector machines. In</title>
<date>1998</date>
<tech>Technical Report MST-TR-98-14. Microsoft Research.</tech>
<contexts>
<context position="15855" citStr="Platt, 1998" startWordPosition="2802" endWordPosition="2803">wo hope/fear building strategies to get £* and £&apos; : (1) simply connect each ez and ez in Line 4∼5 of Algorithm 1, (2) conduct a slow beam search among the N-best lists of all foreign sentences from el to em and use Eq. 5 and 6 to prune the stack. The stack size is 10. We observe that there is no significant difference between the two strategies on the BLEU of the dev set. But the second strategy is about 10 times slower. We also consider more constraints in Eq. 3. By beam search, we obtain one corpus-level oracle and 29 other hypotheses similar to (Chiang et al., 2008), and optimize with SMO (Platt, 1998). Unfortunately, experiments show that more constraints lead to an overfitting and no improved performance. As shown in Table 4, in one execution, our method updates the parameters by only 400 times; MIRA2 updates by 40 x 878 = 35120 times; and MIRA1 updates much more (about 1,966,720 times) due to the SMO procedure. We are surprised to find c-MIRA gets a higher training BLEU with such few parameter updates. This probably suggests that there is a gap between sentence-level BLEU and corpuslevel BLEU, so standard MIRAs need to update the parameters more often. Regarding simplicity, MIRA1 uses a </context>
</contexts>
<marker>Platt, 1998</marker>
<rawString>J. Platt. 1998. Sequetial minimal optimization: A fast algorithm for training support vector machines. In Technical Report MST-TR-98-14. Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Hofman</author>
<author>T Joachims</author>
<author>Y Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>International Conference on Machine Learning (ICML),</booktitle>
<pages>823--830</pages>
<contexts>
<context position="8630" citStr="Tsochantaridis et al., 2004" startWordPosition="1540" endWordPosition="1543">., eM,kM } with the highest probability given the source corpus F = {f1, f2, ..., fm}, E = arg max E = arg max E w · h(ei,ki)}M (8) i=1 where Zi = E (fi) j�1exp(w · h(ei,j)), which is a constant with respective to E. Eq. 7 shows that the feature vector of E is determined by the sum of each candidate’s feature vectors. Also, the model score can be decomposed into each sentence in Eq. 8, which shows that decoding all sentences together equals to decoding one by one. We also show that if the metric is decomposable, the loss in c-MIRA is actually the sum of the hinge loss li(w) in structural SVM (Tsochantaridis et al., 2004; Cherry and Foster, 2012). We assume B(eij) to be the metric of a sentence hypothesis, then the Instead of adopting a cutting-plane algorithm (Tsochantaridis et al., 2004), we optimize the same loss with a MIRA pattern in a simpler way. However, since BLEU is not decomposable, the structural SVM (Cherry and Foster, 2012) uses an interpolated sentence BLEU (Liang et al., 2006). Although Algorithm 1 has an outlook similar to the batch-MIRA algorithm in (Cherry and Foster, 2012), their loss definitions differ fundamentally. Batch MIRA basically uses a sentence-level loss, and they also follow th</context>
</contexts>
<marker>Tsochantaridis, Hofman, Joachims, Altun, 2004</marker>
<rawString>I. Tsochantaridis, T. Hofman, T. Joachims, and Y. Altun. 2004. Support vector machine learning for interdependent and structured output spaces. International Conference on Machine Learning (ICML), 823-830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Watanabe</author>
</authors>
<title>Optimized online rank learning for machine translation.</title>
<date>2012</date>
<booktitle>Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),</booktitle>
<pages>253--262</pages>
<contexts>
<context position="1922" citStr="Watanabe (2012)" startWordPosition="300" endWordPosition="301">ch are heuristic-driven (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Cherry and Foster, 2012). The sentence-level BLEU appearing in the objective is generally based on a pseudo-document, which may not precisely reflect the corpus-level BLEU. We believe that this mismatch could potentially harm the performance. To avoid the sentence BLEU, the work in (Haddow et al., 2011) proposed to process sentences in small batches. The authors adopted a Gibbs sampling (Arun et al., 2009) technique to search the hope and fear hypotheses, and they did not compare with MIRA. Watanabe (2012) also tuned the parameters with small batches of sentences and optimized a hinge loss not explicitly related to BLEU using stochastic gradient descent. Both approaches introduced additional complexities over baseline MIRA approaches. In contrast, we propose a remarkably simple but efficient batch MIRA approach which exploits the exact corpus-level BLEU to compute model losses. We search for a hope and a fear hypotheses for the corpus with a straightforward approach and minimize the structured hinge loss defined on them. The experiments show that our method consistently outperforms two state-of</context>
</contexts>
<marker>Watanabe, 2012</marker>
<rawString>T. Watanabe. 2012. Optimized online rank learning for machine translation. Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 253-262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Watanabe</author>
<author>J Suzuki</author>
<author>H Tsukada</author>
<author>H Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>Proceedings of Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>764--773</pages>
<contexts>
<context position="1038" citStr="Watanabe et al., 2007" startWordPosition="153" endWordPosition="156">with a large number of features. Since the corpus-level BLEU is not decomposable, these MIRA approaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses. Instead, we present a new MIRA method, which employs an exact corpus-level BLEU to compute the model loss. Our method is simpler in implementation. Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations. 1 Introduction Margin infused relaxed algorithm (MIRA) has been widely adopted for the parameter optimization in SMT with a large feature size (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Eidelman, 2012; Cherry and Foster, 2012). Since BLEU is defined on the corpus, and not decomposed into sentences, most MIRA approaches consider a variety of sentence-level BLEUs for the model losses, many of which are heuristic-driven (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Cherry and Foster, 2012). The sentence-level BLEU appearing in the objective is generally based on a pseudo-document, which may not precisely reflect the corpus-level BLEU. We believe that this mismatch could potentially harm t</context>
<context position="4472" citStr="Watanabe et al., 2007" startWordPosition="728" endWordPosition="731">1 E −w · [H(E∗) − H(E)]} (4) where e∗i ∈ ei is a hope candidate, w0 is the parameter vector from the last sentence. Since MIRA defines its objective only based on the current sentence, b(·) is a sentence-level BLEU. Most MIRA algorithms need a deliberate definition of b(·), since BLEU cannot be decomposed into sentences. The types of the sentence BLEU calculation includes: (a) a smoothed version of BLEU for ei9 (Liang et al., 2006), (b) fit ei9 into a pseudodocument considering the history (Chiang et al., 2008; Chiang, 2012), (c) use ei9 to replace the corresponding hypothesis in the oracles (Watanabe et al., 2007). The sentence-level BLEU sometimes perplexes the algorithms and results in a mismatch with the corpus-level BLEU. 3 Corpus-level MIRA 3.1 Algorithm We propose a batch tuning strategy, corpus-level MIRA (c-MIRA), in which an objective is not built upon a hinge loss of a single sentence, but upon that of the entire corpus. The online MIRAs are difficult to parallelize. Therefore, similar to the batch MIRA in (Cherry and Foster, 2012), we conduct the batch tuning by repeating the following steps: (a) Decode source sentences (in parallel) and obtain {ei}M1, (b) Merge {ei}M1 with the one from the </context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki. 2007. Online large-margin training for statistical machine translation. Proceedings of Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), 764-773.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
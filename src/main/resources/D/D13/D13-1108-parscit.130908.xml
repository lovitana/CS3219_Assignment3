<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000024">
<title confidence="0.957492">
Translation with Source Constituency and Dependency Trees
</title>
<author confidence="0.991522">
Fandong Meng†§ Jun Xie† Linfeng Song†§ Yajuan L¨u† Qun Liul††Key Laboratory of Intelligent Information Processing
</author>
<affiliation confidence="0.9979365">
Institute of Computing Technology, Chinese Academy of Sciences
§University of Chinese Academy of Sciences
</affiliation>
<email confidence="0.975929">
{mengfandong,xiejun,songlinfeng,lvyajuan}@ict.ac.cn
</email>
<affiliation confidence="0.977669">
‡Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
</affiliation>
<email confidence="0.986735">
qliu@computing.dcu.ie
</email>
<sectionHeader confidence="0.998523" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.98912205">
We present a novel translation model, which
simultaneously exploits the constituency and
dependency trees on the source side, to com-
bine the advantages of two types of trees. We
take head-dependents relations of dependency
trees as backbone and incorporate phrasal n-
odes of constituency trees as the source side
of our translation rules, and the target side as
strings. Our rules hold the property of long
distance reorderings and the compatibility
with phrases. Large-scale experimental result-
s show that our model achieves significantly
improvements over the constituency-to-string
(+2.45 BLEU on average) and dependency-
to-string (+0.91 BLEU on average) model-
s, which only employ single type of trees,
and significantly outperforms the state-of-the-
art hierarchical phrase-based model (+1.12
BLEU on average), on three Chinese-English
NIST test sets.
</bodyText>
<sectionHeader confidence="0.999468" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999934304347826">
In recent years, syntax-based models have become a
hot topic in statistical machine translation. Accord-
ing to the linguistic structures, these models can be
broadly divided into two categories: constituency-
based models (Yamada and Knight, 2001; Graehl
and Knight, 2004; Liu et al., 2006; Huang et al.,
2006), and dependency-based models (Lin, 2004;
Ding and Palmer, 2005; Quirk et al., 2005; Xiong
et al., 2007; Shen et al., 2008; Xie et al., 2011).
These two kinds of models have their own advan-
tages, as they capture different linguistic phenome-
na. Constituency trees describe how words and se-
quences of words combine to form constituents, and
constituency-based models show better compatibil-
ity with phrases. However, dependency trees de-
scribe the grammatical relation between words of
the sentence, and represent long distance dependen-
cies in a concise manner. Dependency-based mod-
els, such as dependency-to-string model (Xie et al.,
2011), exhibit better capability of long distance re-
orderings.
In this paper, we propose to combine the advan-
tages of source side constituency and dependency
trees. Since the dependency tree is structurally sim-
pler and directly represents long distance depen-
dencies, we take dependency trees as the backbone
and incorporate constituents to them. Our mod-
el employs rules that represent the source side as
head-dependents relations which are incorporated
with constituency phrasal nodes, and the target side
as strings. A head-dependents relation (Xie et al.,
2011) is composed of a head and all its dependents in
dependency trees, and it encodes phrase pattern and
sentence pattern (typically long distance reordering
relations). With the advantages of head-dependents
relations, the translation rules of our model hold the
property of long distance reorderings and the com-
patibility with phrases.
Our new model (Section 2) extracts rules from
word-aligned pairs of source trees (constituency
and dependency) and target strings (Section 3), and
translate source trees into target strings by employ-
ing a bottom-up chart-based algorithm (Section 4).
Compared with the constituency-to-string (Liu et al.,
2006) and dependency-to-string (Xie et al., 2011)
models that only employ a single type of trees, our
</bodyText>
<page confidence="0.907355">
1066
</page>
<note confidence="0.799964">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1066–1076,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.988764423076923">
VP3
VP3
᧘ࠪ/VV
ADVP NP
IP
VP2
VP2
NP
㤡⢩ቄ/NR ሶ/AD ㅄ䇠ᵜ/NN
QP
NP1
NP1
ӊ⍢/NR Ⅾ/M
NP
CLP
NP
ADVP
䎵㓗/JJ
NR AD VV NR OD M JJ NN
ㅜа/OD
rz*fi * #viii ]E;&apos;)hl M— a Ma lit*
(a) (b)
Intel will launch Asia first super laptop
Chinese: XWF * Adi RM 34— 0-9 lgi2*
English: Intel will launch the first Ultrabook in Asia
(c)
</figure>
<figureCaption confidence="0.7651795">
Figure 1: Illustration of phrases that can not be captured by a dependency tree (b) while captured by a constituency tree
(a), where the bold phrasal nodes NP1, VP2, VP3 indicate the phrases which can not be captured by dependency syn-
tactic phrases. (c) is the corresponding bilingual sentences. The subscripts of phrasal nodes are used for distinguishing
the nodes with same phrasal categories.
</figureCaption>
<bodyText confidence="0.9992209">
approach yields encouraging results by exploiting t-
wo types of trees. Large-scale experiments (Sec-
tion 5) on Chinese-English translation show that
our model significantly outperforms the state-of-
the-art single constituency-to-string model by av-
eraged +2.45 BLEU points, dependency-to-string
model by averaged +0.91 BLEU points, and hierar-
chical phrase-based model (Chiang, 2005) by aver-
aged +1.12 BLEU points, on three Chinese-English
NIST test sets.
</bodyText>
<sectionHeader confidence="0.998679" genericHeader="introduction">
2 Grammar
</sectionHeader>
<bodyText confidence="0.99997647368421">
We take head-dependents relations of dependency
trees as backbone and incorporate phrasal nodes of
constituency trees as the source side of our transla-
tion rules, and the target side as strings. A head-
dependents relation consists of a head and all its de-
pendents in dependency trees, and it can represent
long distance dependencies. Incorporating phrasal
nodes of constituency trees into head-dependents
relations further enhances the compatibility with
phrases of our rules. Figure 1 shows an example of
phrases which can not be captured by a dependen-
cy tree while captured by a constituency tree, such
as the bold phrasal nodes NP1,VP2 and VP3. The
phrasal node NP1 in the constituency tree indicates
that “�&amp;Ii�,*” is a noun phrase and it should
be translated as a basic unit, while in the depen-
dency tree it is a non-syntactic phrase. The head-
dependents relation in the top level of the dependen-
cy tree presents long distance dependencies of the
words “ATT/ ”, “,I4”, “ [r”, and “Ii�,*” in a
concise manner, which is useful for long distance re-
ordering. We adopt this kind of rule representation
to hold the property of long distance reorderings and
the compatibility with phrases.
Figure 2 shows two examples of our translation
rules corresponding to the top level of Figure 1-(b).
We can see that r1 captures a head-dependents rela-
tion, while r2 extends r1 by incorporating a phrasal
node VP2 to replace the two nodes “íÑ/VV” and
“Iit,*/NN”. As shown in Figure 1-(b), VP2 con-
sists of two parts, a head node “íÑ/VV” and a
subtree rooted at the dependent node “Iii,*/NN”.
Therefore, we use VP2 and the POS tags of the t-
wo nodes VV and NN to denote the part covered
by VP2 in r2, to indicate that the source sequence
covered by VP2 can be translated by a bilingual
phrase. Since VP2 covers a head node “íÑ /VV”,
we represent r2 by constructing a new head node
</bodyText>
<page confidence="0.99532">
1067
</page>
<figureCaption confidence="0.7865167">
Figure 2: Two examples of our translation rules corre-
sponding to the top level of Figure 1-(b). r1 captures a
head-dependents relation, and r2 extends r1 by incorpo-
rating a phrasal node VP2. “x1:NN” indicates a substitu-
tion site which can be replaced by a subtree whose root
has POS tag “NN”. “x1:VP2 VV NN” indicates a sub-
stitution site which can be replaced by a source phrase
covered by a phrasal node VP (the phrasal node consist-
s of two dependency nodes with POS tag VV and NN,
respectively). The underline denotes a leaf node.
</figureCaption>
<bodyText confidence="0.620462909090909">
VP2 VV NN. For simplicity, we use a shorten for-
m CHDR to represent the head-dependents relations
with/without constituency phrasal nodes.
Formally, our grammar G is defined as a 5-tuple
G = (E, Nc, Nd, A, R), where E is a set of source
language terminals, Nc is a set of constituency
phrasal categories, Nd is a set of categories (POS
tags) for the terminals in E, A is a set of target lan-
guage terminals, and R is a set of translation rules
that include bilingual phrases for translating source
language terminals and CHDR rules for translation
</bodyText>
<listItem confidence="0.9781675">
and reordering. A CHDR rule is represented as a
triple (t, s, —), where:
• t is CHDR with each node labeled by a ter-
minal from E or a variable from a set X =
Ix1, x2, · · · } constrained by a terminal from E
or a category from Nd or a joint category (con-
structed by the categories from Nc and Nd);
• s E (X U A) denotes the target side string;
• — denotes one-to-one links between nontermi-
nals in t and variables in s.
</listItem>
<bodyText confidence="0.9999012">
We use the lexicon dependency grammar (Hellwig,
2006) which adopts a bracket representation to ex-
press the head-dependents relation and CHDR. For
example, the left-hand sides of r1 and r2 in Figure 2
can be respectively represented as follows:
</bodyText>
<equation confidence="0.991931363636364">
(.1) (411)A[h(x1:NN)
(A*&amp;quot;T.1) (4) x1:VP2 VV NN
ㅜа/OD
Intel will launch the first Ultrabook in Asia
Translation Rules
r3 (x1:NR) (x2:AD) ᧘ࠪ (x3:ㅄ䇠ᵜ) x1 x2 launch x3
r4 AWቄ Intel
r5 ሶ will
r6 (ӊ⍢)(x1:M)x2:NP1 JJ_NN x1 X2 in Aisa
䎵㓗 liaᵜ Ultrabook
r8 ㅜа (Ⅾ) the first
</equation>
<figureCaption confidence="0.9516758">
Figure 3: An example derivation of translation. (g) lists
all the translation rules. r3, rs and r8 are CHDR rules,
while r4, r5 and r7 are bilingual phrases, which are used
for translating source terminals. The dash lines indicate
the reordering when employing a translation rule.
</figureCaption>
<bodyText confidence="0.989285">
The formalized presentation of r2 in Figure 2-(b):
</bodyText>
<equation confidence="0.998077333333333">
t = (At&amp;quot;T.1) (44) x1:VP2 VV NN
s = Intel will x1
—= x1:VP2 VV NN +-+ x1
</equation>
<bodyText confidence="0.997266666666667">
where the underline indicates a leaf node.
Figure 3 gives an example of the translation
derivation in our model, with the translation rules
</bodyText>
<figure confidence="0.955993973684211">
ㅄ䇠ᵜ/NN
㤡⢩ቄ/NR ሶ/AD
r4
r5
(a)
A*;TN * *ffii RM %_ a OW lia*
㤡⢩ቄ/NR ሶ/AD launch
Parseing
Labelling
VP2
VP3
᧘ࠪ/VV
NP1
ㅜа/OD
r3
ㅄ䇠ᵜ/NN
ӊ⍢/NR Ⅾ/M 䎵㓗/JJ
NP1
ㅜа/OD
r6
Ⅾ/M
Intel will launch 䎵㓗 IiEᵜ in Asia
NP1
ㅜа/OD
r7
Ⅾ/M
Intel will launch Ultrabook in Asia
ӊ⍢/NR Ⅾ/M 䎵㓗/JJ
r8
᧘ࠪ
1
㤡⢩ቄ ሶ 1
1
1 2
2
1
㤡⢩ቄ ሶ
r7
</figure>
<page confidence="0.945244">
1068
</page>
<bodyText confidence="0.995741555555556">
listed in (g). r3, r6 and r8 are CHDR rules, while
r4, r5 and r7 are bilingual phrases, which are used
for translating source language terminals. Given a
sentence to translate in (a), we first parse it into a
constituency tree and a dependency tree, then label
the phrasal nodes from the constituency tree to the
dependency tree, and yield (b). Then, we translate
it into a target string by the following steps. At the
root node, we apply rule r3 to translate the top level
head-dependents relation and results in four unfin-
ished substructures and target strings in (c). From
(c) to (d), there are three steps (one rule for one step).
We use r4 to translate “ma to “Intel”, r5 to
translate “44” to “will”, and r6 to translate the right-
most unfinished part. Then, we apply r7 to translate
the phrase “Mr&amp;Ii�,V” to “Ultrabook”, and yield
(e). Finally, we apply r8 to translate the last frag-
ment to “the first”, and get the final result (f).
</bodyText>
<sectionHeader confidence="0.98968" genericHeader="method">
3 Rule Extraction
</sectionHeader>
<bodyText confidence="0.999187428571429">
In this section, we describe how to extract rules from
a set of 4-tuples (C, T, S, A), where C is a source
constituency tree, T is a source dependency tree, S
is a target side sentence, and A is an word alignmen-
t relation between T/C and S. We extract CHDR
rules from each 4-tuple (C, T, S, A) based on GHK-
M algorithm (Galley et al., 2004) with three steps:
</bodyText>
<listItem confidence="0.9885986">
1. Label the dependency tree with phrasal nodes
from the constituency tree, and annotate align-
ment information to the phrasal nodes labeled
dependency tree (Section 3.1).
2. Identify acceptable CHDR fragments from the
annotated dependency tree for rule induction
(Section 3.2).
3. Induce a set of lexicalized and generalized
CHDR rules from the acceptable fragments
(Section 3.3).
</listItem>
<subsectionHeader confidence="0.998048">
3.1 Annotation
</subsectionHeader>
<bodyText confidence="0.999900666666667">
Given a 4-tuple (C, T, S, A), we first label phrasal
nodes from the constituency tree C to the depen-
dency tree T, which can be easily accomplished by
phrases mapping according to the common covered
source sequences. As dependency trees can capture
some phrasal information by dependency syntactic
</bodyText>
<figureCaption confidence="0.9944764">
Figure 4: An annotated dependency tree. Each node is
annotated with two spans, the former is node span and
the latter subtree span. The fragments covered by phrasal
nodes are annotated with phrasal spans. The nodes de-
noted by the solid line box are not nsp consistent.
</figureCaption>
<bodyText confidence="0.999818333333333">
phrases, in order to complement the information that
dependency trees can not capture, we only label the
phrasal nodes that cover dependency non-syntactic
phrases.
Then, we annotate alignment information to the
phrasal nodes labeled dependency tree T, as shown
in Figure 4. For description convenience, we make
use of the notion of spans (Fox, 2002; Lin, 2004).
Given a node n in the source phrasal nodes labeled
T with word alignment information, the spans of n
induced by the word alignment are consecutive se-
quences of words in the target sentence. As shown
in Figure 4, we annotate each node n of phrasal n-
odes labeled T with two attributes: node span and
subtree span; besides, we annotate phrasal span to
the parts covered by phrasal nodes in each subtree
rooted at n. The three types of spans are defined as
follows:
</bodyText>
<construct confidence="0.542483333333333">
Definition 1 Given a node n, its node span nsp(n)
is the consecutive target word sequence aligned with
the node n.
</construct>
<bodyText confidence="0.9779525">
Take the node “AVtI/NR” in Figure 4 for example,
nsp(A0t1/NR)={7-8}, which corresponds to the tar-
get words “in” and “Asia”.
Definition 2 Given a subtree T′ rooted at n, the
subtree span tsp(n) of n is the consecutive target
word sequence from the lower bound of the nsp of
</bodyText>
<figure confidence="0.997330772727273">
VP3
&lt;2-8&gt;
᧘ࠪ/VV
{3-3}{1-8}
VP2
&lt;3-8&gt;
㤡⢩ቄ/NR
{1-1}{1-1}
ሶ/AD
{2-2}{2-2}
ㅄ䇠ᵜ/NN
{6-6}{4-8}
NP1
&lt;6-6&gt;
ㅜа/OD
{4-5}{4-5}
ӊ⍢/NR
{7-8}{7-8}
Ⅾ/M
{null}{4-5}
䎵㓗/JJ
{6-6}{6-6}
</figure>
<page confidence="0.97829">
1069
</page>
<bodyText confidence="0.997702407407407">
all nodes in T′ to the upper bound of the same set of
spans.
For instance, tsp()i�,*/NN)={4-8}, which corre-
sponds to the target words “the first Ultrabook in A-
sia”, whose indexes are from 4 to 8.
Definition 3 Given a fragment f covered by a
phrasal node, the phrasal span psp(f) of f is
the consecutive target word sequence aligned with
source string covered by f.
For example, psp(VP2)=(3-8), which corresponds
to the target word sequence “launch the first Ultra-
book in Asia”.
We say nsp, tsp and psp are consistent according
to the notion in the phrase-based model (Koehn et
al., 2003). For example, nsp(AVtI/NR), tsp()i�,
*/NN) and psp(NP1) are consistent while nsp( a
&amp;/JJ) and nsp()i�,*/NN) are not consistent.
The annotation can be achieved by a single pos-
torder transversal of the phrasal nodes labeled de-
pendency tree. For simplicity, we call the annotat-
ed phrasal nodes labeled dependency tree annotated
dependency tree. The extraction of bilingual phrases
(including the translation of head node, dependen-
cy syntactic phrases and the fragment covered by a
phrasal node) can be readily achieved by the algo-
rithm described in Koehn et al., (2003). In the fol-
lowing, we focus on CHDR rules extraction.
</bodyText>
<subsectionHeader confidence="0.999265">
3.2 Acceptable Fragments Identification
</subsectionHeader>
<bodyText confidence="0.999894764705882">
Before present the method of acceptable fragments
identification, we give a brief description of CHDR
fragments. A CHDR fragment is an annotated frag-
ment that consists of a source head-dependents rela-
tion with/without constituency phrasal nodes, a tar-
get string and the word alignment information be-
tween the source and target side. We identify the ac-
ceptable CHDR fragments that are suitable for rule
induction from the annotated dependency tree. We
divide the acceptable CHDR fragments into two cat-
egories depending on whether the fragments con-
tain phrasal nodes. If an acceptable CHDR frag-
ment does not contain phrasal nodes, we call it
CHDR-normal fragment, otherwise CHDR-phrasal
fragment. Given a CHDR fragment F rooted at n,
we say F is acceptable if it satisfies any one of the
following properties:
</bodyText>
<subsectionHeader confidence="0.453571">
CHDR-normal Rules
</subsectionHeader>
<equation confidence="0.984157857142857">
r1: (㤡⢩ቄ) (ሶ) ᧘ࠪ (x1:ㅄ䇠ᵜ) Intel will launch x1
r2: (x1:NR) (x2:AD) ᧘ࠪ (x3:ㅄ䇠ᵜ)
r3: (㤡⢩ቄ) (ሶ) ᧘ࠪ (x1:NN)
r4: (x1:NR) (x2:AD) ᧘ࠪ (x3:NN) x1 x2 launch x3
r5: (㤡⢩ቄ) (ሶ) x1:VV (x2:ㅄ䇠ᵜ) Intel will x1 x2
r6: (x1:NR) (x2:AD) x3:VV (x4:ㅄ䇠ᵜ)
r7: (㤡⢩ቄ) (ሶ) x1:VV (x2:NN)
</equation>
<table confidence="0.8239308">
CHDR-phrasal Rules
r9: (㤡⢩ቄ)(ሶ)x1:VP2 VV_NN Intel will x1
r10: (x1:NR)(x2:AD)x3:VP2 VV_NN x1 x2 x3
r11: (㤡⢩ቄ)x1:VP3 AD_VV_NN Intel x1
r12: (x1:NR)x2:VP3 AD_VV_NN x1 x2
</table>
<figureCaption confidence="0.99945">
Figure 5: Examples of a CHDR-normal fragment (a), two
CHDR-phrasal fragments (b) and (c) that are identified
from the top level of the annotated dependency tree in
Figure 4, and the corresponding CHDR rules (d) induced
from (a), (b) and (c). The underline denotes a leaf node.
</figureCaption>
<bodyText confidence="0.976451">
1. Without phrasal nodes, the node span of the
root n is consistent and the subtree spans of
n’s all dependents are consistent. For example,
Figure 5-(a) shows a CHDR-normal fragmen-
t that identified from the top level of the an-
notated dependency tree in Figure 4, since the
nsp( [h/VV), tsp(A*,T.f/NR), tsp(411/AD)
and tsp()it,*/NN) are consistent.
</bodyText>
<figure confidence="0.9980495">
᧘ࠪ/VV
㤡⢩ቄ/NR ሶ/AD ㅄ䇠ᵜ/NN
will launch
2 3
the first Ultrabook in Asia
4-8
Intel
1
2
3-8
᧘ࠪ/VV
VP2 VV_NN
㤡⢩ቄ/NR ሶ/AD
ㅄ䇠ᵜ/NN
will launch the first Ultrabook in Asia
Intel
1
VP2
᧘ࠪ/VV
ሶ/AD ㅄ䇠ᵜ/NN
㤡⢩ቄ/NR
will launch the first Ultrabook in Asia
2-8
Intel
1
VP3
VP3 AD_VV_NN
x1 x2 launch x3
Intel will launch x1
x1 x2 x3 x4
Intel will x1 x2
r8: (x1:NR) (x2:AD) x3:VV (x4:NN) x1 x2 x3 x4
</figure>
<page confidence="0.935955">
1070
</page>
<bodyText confidence="0.984616611111111">
2. With phrasal nodes, the phrasal spans of
phrasal nodes are consistent; and for the other
nodes, the node span of head (if it is not cov-
ered by any phrasal node) is consistent, and the
subtree spans of dependents are consistent. For
instance, Figure 5-(b) and (c) show two CHDR-
phrasal fragments identified from the top level
of Figure 4. In Figure 5-(b), psp(VP2), tsp(A
ty � /NR) and tsp(44/AD) are consistent. In
Figure 5-(c), psp(VP3) and tsp(Aty �f /NR)
are consistent.
The identification of acceptable fragments can be
achieved by a single postorder transversal of the an-
notated dependency tree. Typically, each acceptable
fragment contains at most three types of nodes: head
node, head of the related CHDR; internal nodes, in-
ternal nodes of the related CHDR except head node;
leaf nodes, leaf nodes of the related CHDR.
</bodyText>
<subsectionHeader confidence="0.994537">
3.3 Rule Induction
</subsectionHeader>
<bodyText confidence="0.999697571428571">
From each acceptable CHDR fragment, we induce
a set of lexicalized and generalized CHDR rules.
We induce CHDR-normal rules and CHDR-phrasal
rules from CHDR-normal fragments and CHDR-
phrasal fragments, respectively.
We first induce a lexicalized form of CHDR rule
from an acceptable CHDR fragment:
</bodyText>
<listItem confidence="0.850747111111111">
1. For a CHDR-normal fragment, we first mark
the internal nodes as substitution sites. This
forms the input of a CHDR-normal rule. Then
we generate the target string according to the
node span of the head and the subtree spans of
the dependents, and turn the word sequences
covered by the internal nodes into variables.
This forms the output of a lexicalized CHDR-
normal rule.
2. For a CHDR-phrasal fragment, we first mark
the internal nodes and the phrasal nodes as sub-
stitution sites. This forms the input of a CHDR-
phrasal rule. Then we construct the output of
the CHDR-phrasal rule in almost the same way
with constructing CHDR-normal rules, except
that we replace the target sequences covered by
the internal nodes and the phrasal nodes with
variables.
</listItem>
<bodyText confidence="0.999578488372093">
For example, rule r1 in Figure 5-(d) is a lexicalized
CHDR-normal rule induced from the CHDR-normal
fragment in Figure 5-(a). r9 and r11 are CHDR-
phrasal rules induced from the CHDR-phrasal frag-
ment in Figure 5-(b) and Figure 5-(c) respectively.
As we can see, these CHDR-phrasal rules are par-
tially unlexicalized.
To alleviate the sparseness problem, we gener-
alize the lexicalized CHDR-normal rules and par-
tially unlexicalized CHDR-phrasal rules with un-
lexicalized nodes by the method proposed in Xie
et al., (2011). As the modification relations be-
tween head and dependents are determined by the
edges, we can replace the lexical word of each n-
ode with its category (POS tag) and obtain new
head-dependents relations with unlexicalized nodes
keeping the same modification relations. We gen-
eralize the rule by simultaneously turn the nodes of
the same type (head, internal, leaf) into their cate-
gories. For example, CHDR-normal rules r2 ∼ r7
are generalized from r1 in Figure 5-(d). Besides, r10
and r12 are the corresponding generalized CHDR-
phrasal rules. Actually, our CHDR rules are the su-
perset of head-dependents relation rules in Xie et
al., (2011). CHDR-normal rules are equivalent with
the head-dependents relation rules and the CHDR-
phrasal rules are the extension of these rules. For
convenience of description, we use the subscript to
distinguish the phrasal nodes with the same catego-
ry, such as VP2 and VP3. In actual operation, we use
VP instead of VP2 and VP3.
We handle the unaligned words of the target side
by extending the node spans of the lexicalized head
and leaf nodes, and the subtree spans of the lexical-
ized dependents, on both left and right directions.
This procedure is similar with the method of Och
and Ney, (2004). During this process, we might ob-
tain m(m ≥ 1) CHDR rules from an acceptable
fragment. Each of these rules is assigned with a frac-
tional count 1/m. We take the extracted rule set as
observed data and make use of relative frequency es-
timator to obtain the translation probabilities P(t|s)
and P(s|t).
</bodyText>
<sectionHeader confidence="0.964426" genericHeader="method">
4 Decoding and the Model
</sectionHeader>
<bodyText confidence="0.943117">
Following Och and Ney, (2002), we adopt a general
loglinear model. Let d be a derivation that convert a
</bodyText>
<page confidence="0.990601">
1071
</page>
<bodyText confidence="0.9996645">
source phrasal nodes labeled dependency tree into a
target string e. The probability of d is defined as:
</bodyText>
<equation confidence="0.9717265">
P(d) a � Oi(d)&amp;quot;z (1)
i
</equation>
<bodyText confidence="0.998816333333333">
where Oi are features defined on derivations and Ai
are feature weights. In our experiments of this paper,
the features are used as follows:
</bodyText>
<listItem confidence="0.999692272727273">
• CHDR rules translation probabilities P(t|s)
and P(s|t), and CHDR rules lexical translation
probabilities Plex(t|s) and Plex(s|t);
• bilingual phrases translation probabilities
Pbp(t|s) and Pbp(s|t), and bilingual phrases
lexical translation probabilities Pbplex(t|s) and
Pbplex(s|t);
• rule penalty exp(−1);
• pseudo translation rule penalty exp(−1);
• target word penalty exp(|e|);
• language model Pl„t(e).
</listItem>
<bodyText confidence="0.999981810810811">
We have twelve features in our model. The values of
the first four features are accumulated on the CHDR
rules and the next four features are accumulated on
the bilingual phrases. We also use a pseudo transla-
tion rule (constructed according to the word order of
head-dependents relation) as a feature to guarantee
the complete translation when no matched rules can
be found during decoding.
Our decoder is based on bottom-up chart-based
algorithm. It finds the best derivation that convert
the input phrasal nodes labeled dependency tree into
a target string among all possible derivations. Giv-
en the source constituency tree and dependency tree,
we first generate phrasal nodes labeled dependency
tree T as described in Section 3.1, then the decoder
transverses each node in T by postorder. For each
node n, it enumerates all instances of CHDR rooted
at n, and checks the rule set for matched translation
rules. A larger translation is generated by substitut-
ing the variables in the target side of a translation
rule with the translations of the corresponding de-
pendents. Cube pruning (Chiang, 2007; Huang and
Chiang, 2007) is used to find the k-best items with
integrated language model for each node.
To balance the performance and speed of the de-
coder, we limit the search space by reducing the
number of translation rules used for each node.
There are two ways to limit the rule table size: by
a fixed limit (rule-limit) of how many rules are re-
trieved for each input node, and by a threshold (rule-
threshold) to specify that the rule with a score low-
er than Q times of the best score should be discard-
ed. On the other hand, instead of keeping the full
list of candidates for a given node, we keep a top-
scoring subset of the candidates. This can also be
done by a fixed limit (stack-limit) and a threshold
(stack-threshold).
</bodyText>
<sectionHeader confidence="0.999609" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999111">
We evaluated the performance of our model by com-
paring with hierarchical phrase-based model (Chi-
ang, 2007), constituency-to-string model (Liu et al.,
2006) and dependency-to-string model (Xie et al.,
2011) on Chinese-English translation. First, we de-
scribe data preparation (Section 5.1) and systems
(Section 5.2). Then, we validate that our model sig-
nificantly outperforms all the other baseline models
(Section 5.3). Finally, we give detail analysis (Sec-
tion 5.4).
</bodyText>
<subsectionHeader confidence="0.980514">
5.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999961111111111">
Our training data consists of 1.25M sentence pairs
extracted from LDC 1 data. We choose NIST MT
Evaluation test set 2002 as our development set,
NIST MT Evaluation test sets 2003 (MT03), 2004
(MT04) and 2005 (MT05) as our test sets. The qual-
ity of translations is evaluated by the case insensitive
NIST BLEU-4 metric 2.
We parse the source sentences to constituency
trees (without binarization) and projective depen-
dency trees with Stanford Parser (Klein and Man-
ning, 2002). The word alignments are obtained by
running GIZA++ (Och and Ney, 2003) on the corpus
in both directions and using the “grow-diag-final-
and” balance strategy (Koehn et al., 2003). We get
bilingual phrases from word-aligned data with algo-
rithm described in Koehn et al. (2003) by running
Moses Toolkit 3. We apply SRI Language Modeling
Toolkit (Stolcke and others, 2002) to train a 4-gram
</bodyText>
<footnote confidence="0.9085178">
1Including LDC2002E18, LDC2003E07, LDC2003E14,
Hansards portion of LDC2004T07, LDC2004T08 and LD-
C2005T06.
2ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
3http://www.statmt.org/moses/
</footnote>
<page confidence="0.952096">
1072
</page>
<table confidence="0.9999294">
System Rule # MT03 MT04 MT05 Average
Moses-chart 116.4M 34.65 36.47 34.39 35.17
cons2str 25.4M+32.5M 33.14 35.12 33.27 33.84
dep2str 19.6M+32.5M 34.85 36.57 34.72 35.38
consdep2str 23.3M+32.5M 35.57* 37.68* 35.62* 36.29
</table>
<tableCaption confidence="0.811212">
Table 1: Statistics of the extracted rules on training data and the BLEU scores (%) on the test sets of different systems.
The “+” denotes that the rules are composed of syntactic translation rules and bilingual phrases (32.5M). The “*”
denotes that the results are significantly better than all the other systems (p&lt;0.01).
</tableCaption>
<bodyText confidence="0.999894428571429">
language model with modified Kneser-Ney smooth-
ing on the Xinhua portion of the English Gigaword
corpus. We make use of the standard MERT (Och,
2003) to tune the feature weights in order to maxi-
mize the system’s BLEU score on the development
set. The statistical significance test is performed by
sign-test (Collins et al., 2005).
</bodyText>
<subsectionHeader confidence="0.995861">
5.2 Systems
</subsectionHeader>
<bodyText confidence="0.9999835">
We take the open source hierarchical phrase-based
system Moses-chart (with default configuration),
our in-house constituency-to-string system cons2str
and dependency-to-string system dep2str as our
baseline systems.
For cons2str, we follow Liu et al., (Liu et al.,
2006) to strict that the height of a rule tree is no
greater than 3 and phrase length is no greater than
7. To keep consistent with our proposed model,
we implement the dependency-to-string model (X-
ie et al., 2011) with GHKM (Galley et al., 2004)
rule extraction algorithm and utilize bilingual phras-
es to translate source head node and dependency
syntactic phrases. Our dep2str shows comparable
performance with Xie et al., (2011), which can be
seen by comparing with the results of hierarchical
phrase-based model in our experiments. For dep2str
and our proposed model consdep2str, we set rule-
threshold and stack-threshold to 10−3, rule-limit to
100, stack-limit to 300, and phrase length limit to 7.
</bodyText>
<subsectionHeader confidence="0.997378">
5.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999441318181818">
Table 1 illustrates the translation results of our ex-
periments. As we can see, our consdep2str sys-
tem has gained the best results on all test sets, with
+1.12 BLEU points higher than Moses-chart, +2.45
BLEU points higher than cons2str, and +0.91 BLEU
points higher than dep2str, averagely on MT03,
MT04 and MT05. Our model significantly outper-
forms all the other baseline models, with p&lt;0.01
on statistical significance test sign-test (Collins et
al., 2005). By exploiting two types of trees on
source side, our model gains significant improve-
ments over constituency-to-string and dependency-
to-string models, which employ single type of trees.
Table 1 also lists the statistical results of rules ex-
tracted from training data by different systems. Ac-
cording to our statistics, the number of rules extract-
ed by our consdep2str system is about 18.88% larger
than dep2str, without regard to the 32.5M bilingual
phrases. The extra rules are CHDR-phrasal rules,
which can bring in BLEU improvements by enhanc-
ing the compatibility with phrases. We will conduct
a deep analysis in the next sub-section.
</bodyText>
<subsectionHeader confidence="0.996874">
5.4 Analysis
</subsectionHeader>
<bodyText confidence="0.999858523809524">
In this section, we first illustrate the influence of
CHDR-phrasal rules in our consdep2str model. We
calculate the proportion of 1-best translations in test
sets that employ CHDR-phrasal rules, and we cal-
l this proportion “CHDR-phrasal Sent.”. Besides,
the proportion of CHDR-phrasal rules in all CHDR
rules is calculated in these translations, and we cal-
l this proportion “CHDR-phrasal Rule”. Table 2
lists the using of CHDR-phrasal rules on test sets,
showing that CHDR-phrasal Sent. on all test sets
are higher than 50%, and CHDR-phrasal Rule on al-
l three test sets are higher than 10%. These results
indicate that CHDR-phrasal rules do play a role in
decoding.
Furthermore, we compare some actual transla-
tions of our test sets generated by cons2str, de-
p2str and consdep2str systems, as shown in Fig-
ure 6. In the first example, the Chinese input hold-
s long distance dependencies “ [QE XJ
...�Hi?�T...�KT)�t)j”, which correspond
to the sentence pattern “noun+adverb+prepositional
</bodyText>
<page confidence="0.937728">
1073
</page>
<table confidence="0.999643333333333">
System MT03 MT04 MT05
CHDR-phrasal Sent. 50.71 61.80 56.19
CHDR-phrasal Rule 10.53 13.55 10.83
</table>
<tableCaption confidence="0.9867865">
Table 2: The proportion (%) of 1-best translations that
employs CHDR-phrasal rules (CHDR-phrasal Sent.) and
the proportion (%) of CHDR-phrasal rules in all CHDR
rules in these translations (CHDR-phrasal Rule).
</tableCaption>
<bodyText confidence="0.999930368421053">
phrase+verb+noun”. Cons2str gives a bad result
with wrong global reordering, while our consdep2str
system gains an almost correct result since we cap-
ture this pattern by CHDR-normal rules. In the sec-
ond example, we can see that the Chinese phrase
“KZ1, [h1)IIL” is a non-syntactic phrase in the depen-
dency tree, and this phrase can not be captured by
head-dependents relation rules in Xie et al., (2011),
thus can not be translated as one unit. Since we en-
code constituency phrasal nodes to the dependency
tree, “K Z1, [h 1)IIL” is labeled by a phrasal node “VP”
(means verb phrase), which can be captured by our
CHDR-phrasal rules and translated into the correct
result “reemergence” with bilingual phrases.
By combining the merits of constituency and
dependency trees, our consdep2str model learns
CHDR-normal rules to acquire the property of long
distance reorderings and CHDR-phrasal rules to ob-
tain good compatibility with phrases.
</bodyText>
<sectionHeader confidence="0.99994" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999942567567568">
In recent years, syntax-based models have witnessed
promising improvements. Some researchers make
efforts on constituency-based models (Graehl and
Knight, 2004; Liu et al., 2006; Huang et al., 2006;
Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009;
Liu et al., 2011; Zhai et al., 2012). Some works pay
attention to dependency-based models (Lin, 2004;
Ding and Palmer, 2005; Quirk et al., 2005; Xiong et
al., 2007; Shen et al., 2008; Xie et al., 2011). These
models are based on single type of trees.
There are also some approaches combining mer-
its of different structures. Marton and Resnik (2008)
took the source constituency tree into account and
added soft constraints to the hierarchical phrase-
based model (Chiang, 2005). Cherry (2008) u-
tilized dependency tree to add syntactic cohesion
to the phrased-based model. Mi and Liu, (2010)
proposed a constituency-to-dependency translation
model, which utilizes constituency forests on the
source side to direct the translation, and depen-
dency trees on the target side to ensure grammati-
cality. Feng et al. (2012) presented a hierarchical
chunk-to-string translation model, which is a com-
promise between the hierarchical phrase-based mod-
el and the constituency-to-string model. Most work-
s make effort to introduce linguistic knowledge in-
to the phrase-based model and hierarchical phrase-
based model with constituency trees. Only the work
proposed by Mi and Liu, (2010) utilized constituen-
cy and dependency trees, while their work applied
two types of trees on two sides.
Instead, our model simultaneously utilizes con-
stituency and dependency trees on the source side to
direct the translation, which is concerned with com-
bining the advantages of two types of trees in trans-
lation rules to advance the state-of-the-art machine
translation.
</bodyText>
<sectionHeader confidence="0.999009" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999984142857143">
In this paper, we present a novel model that si-
multaneously utilizes constituency and dependency
trees on the source side to direct the translation. To
combine the merits of constituency and dependen-
cy trees, our model employs head-dependents rela-
tions incorporating with constituency phrasal nodes.
Experimental results show that our model exhibits
good performance and significantly outperforms the
state-of-the-art constituency-to-string, dependency-
to-string and hierarchical phrase-based models. For
the first time, source side constituency and depen-
dency trees are simultaneously utilized to direct the
translation, and the model surpasses the state-of-the-
art translation models.
Since constituency tree binarization can lead
to more constituency-to-string rules and syntactic
phrases in rule extraction and decoding, which im-
prove the performance of constituency-to-string sys-
tems, for future work, we would like to do research
on encoding binarized constituency trees to depen-
dency trees to improve translation performance.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.970049">
The authors were supported by National Natural Sci-
ence Foundation of China (Contracts 61202216),
</bodyText>
<page confidence="0.990433">
1074
</page>
<note confidence="0.8213245">
MT05 ---- segment 448
LAK Big-; At EPP 09? �P g+ *R AiiR 0 MIR ;*T )`f)i.
</note>
<tableCaption confidence="0.525002666666667">
reference: The United Nations has expressed concern over the deadline the Indonesian government imposed on foreign troops.
cons2srt: united nations with the indonesian government have expressed concern over the time limit for foreign troops .
consdep2srt: the united nations has expressed concern over the deadline of the indonesian government on foreign troops .
</tableCaption>
<figure confidence="0.819659">
nsubj
advmod
MT04 ---- segment 194
WYX the 0 P&apos;A Sit OTIR31 tENRO ( SARS ) 9011
</figure>
<figureCaption confidence="0.86819425">
reference: •••••• the reemergence of a severe acute respiratory syndrome (SARS) case
dep2srt: ĂĂ again severe acute respiratory syndrome ( SARS ) case ĂĂ
consdep2srt: ĂĂ reemergence of a severe acute respiratory syndrome ( SARS ) case
Figure 6: Actual examples translated by the cons2str, dep2str and consdep2str systems.
</figureCaption>
<figure confidence="0.984993">
prep
pnuct
pobj
dobj
Af)U
V�m
*T
F_4-5 ;_
At
�
FP)E 99&apos;T MIT Mm MR a1 MPQ
the united nations
has expressed
the deadline of the indonesian government on foreign troops
over
concern
.
બ੨䚃/NN
બ੨䚃/NN
ࠪ⧠/VV
ѕ䟽/JJ ᙕᙗ/JJ
again
reemergence
޽⅑/AD Ⲵ/DEG
dep cons &amp; dep
ѕ䟽/JJ ᙕᙗ/JJ
Ⲵ/DEG
޽⅑/AD
ࠪ⧠/VV
VP
863 State Key Project (No. 2011AA01A207),
</figure>
<bodyText confidence="0.997267545454545">
and National Key Technology R&amp;D Program (No.
2012BAH39B03), Key Project of Knowledge Inno-
vation Program of Chinese Academy of Sciences
(No. KGZD-EW-501). Qun Liu&apos;s work was
partially supported by Science Foundation Ireland
(Grant No. 07/CE/I1142) as part of the CNGL
at Dublin City University. Sincere thanks to the
anonymous reviewers for their thorough reviewing
and valuable suggestions. We appreciate Haitao Mi,
Zhaopeng Tu and Anbang Zhao for insightful ad-
vices in writing.
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99989024137931">
Colin Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In ACL, pages 72–80.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263–270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 531–540.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguistic-
s, pages 541–548.
Yang Feng, Dongdong Zhang, Mu Li, Ming Zhou, and
Qun Liu. 2012. Hierarchical chunk-to-string transla-
tion. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics: Long
Papers-Volume 1, pages 950–958.
Heidi J Fox. 2002. Phrasal cohesion and statistical
machine translation. In Proceedings of the ACL-02
conference on Empirical methods in natural language
processing-Volume 10, pages 304–3111.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule. In Pro-
</reference>
<page confidence="0.842788">
1075
</page>
<reference confidence="0.999891301886793">
ceedings of HLT/NAACL, volume 4, pages 273–280.
Boston.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proc. HLT-NAACL, pages 105–112.
Peter Hellwig. 2006. Parsing with dependency gram-
mars. An International Handbook of Contemporary
Research, 2:1081–1109.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Annual Meeting-Association For Computational Lin-
guistics, volume 45, pages 144–151.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. S-
tatistical syntax-directed translation with extended do-
main of locality. In Proceedings ofAMTA, pages 66–
73.
Dan Klein and Christopher D Manning. 2002. Fast exact
inference with a factored model for natural language
parsing. In Advances in neural information processing
systems, volume 15, pages 3–10.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology-Volume 1, pages
48–54.
Dekang Lin. 2004. A path-based transfer model for ma-
chine translation. In Proceedings of the 20th interna-
tional conference on Computational Linguistics, pages
625–630.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 609–616.
Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Join-
t Conference on Natural Language Processing of the
AFNLP: Volume 2-Volume 2, pages 558–566.
Yang Liu, Qun Liu, and Yajuan L¨u. 2011. Adjoining
tree-to-string translation. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 1278–1287.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings ofACL-08: HLT, pages 1003–1011.
Haitao Mi and Qun Liu. 2010. Constituency to depen-
dency translation with forests. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, pages 1433–1442.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192–199.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for s-
tatistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computation-
al Linguistics, pages 295–302.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment models.
Computational linguistics, 29(1):19–51.
Franz Josef Och and Hermann Ney. 2004. The alignmen-
t template approach to statistical machine translation.
Computational linguistics, 30(4):417–449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computation-
al Linguistics-Volume 1, pages 160–167.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguistic-
s, pages 271–279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings ofACL-08: HLT, pages 577–585.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In Proceedings of the inter-
national conference on spoken language processing,
volume 2, pages 901–904.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A nov-
el dependency-to-string model for statistical machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 216–226.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A de-
pendency treelet string correspondence model for s-
tatistical machine translation. In Proceedings of the
Second Workshop on Statistical Machine Translation,
pages 40–47.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting on Association for Computation-
al Linguistics, pages 523–530.
Feifei Zhai, Jiajun Zhang, Yu Zhou, and Chengqing
Zong. 2012. Tree-based translation without using
parse trees. In Proceedings of COLING 2012, pages
3037–3054.
Min Zhang, Hongfei Jiang, AiTi Aw, Jun Sun, Sheng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. MT-
Summit-07, pages 535–542.
</reference>
<page confidence="0.995269">
1076
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.346069">
<title confidence="0.8595454">Translation with Source Constituency and Dependency Trees Laboratory of Intelligent Information Institute of Computing Technology, Chinese Academy of of Chinese Academy of for Next Generation</title>
<author confidence="0.531561">Faculty of Engineering</author>
<author confidence="0.531561">Dublin City Computing</author>
<email confidence="0.899598">qliu@computing.dcu.ie</email>
<abstract confidence="0.997185904761905">We present a novel translation model, which simultaneously exploits the constituency and dependency trees on the source side, to combine the advantages of two types of trees. We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. Our rules hold the property of long distance reorderings and the compatibility with phrases. Large-scale experimental results show that our model achieves significantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
</authors>
<title>Cohesive phrase-based decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>72--80</pages>
<contexts>
<context position="30961" citStr="Cherry (2008)" startWordPosition="5109" endWordPosition="5110">els (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target side to ensure grammaticality. Feng et al. (2012) presented a hierarchical chunk-to-string translation model, which is a compromise between the hierarchical phrase-based model and the constituency-to-string model. Most works make effort to introduce linguistic knowledge into the phrase-based model and hierarchical phrasebased</context>
</contexts>
<marker>Cherry, 2008</marker>
<rawString>Colin Cherry. 2008. Cohesive phrase-based decoding for statistical machine translation. In ACL, pages 72–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="4856" citStr="Chiang, 2005" startWordPosition="728" endWordPosition="729">VP2, VP3 indicate the phrases which can not be captured by dependency syntactic phrases. (c) is the corresponding bilingual sentences. The subscripts of phrasal nodes are used for distinguishing the nodes with same phrasal categories. approach yields encouraging results by exploiting two types of trees. Large-scale experiments (Section 5) on Chinese-English translation show that our model significantly outperforms the state-ofthe-art single constituency-to-string model by averaged +2.45 BLEU points, dependency-to-string model by averaged +0.91 BLEU points, and hierarchical phrase-based model (Chiang, 2005) by averaged +1.12 BLEU points, on three Chinese-English NIST test sets. 2 Grammar We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. A headdependents relation consists of a head and all its dependents in dependency trees, and it can represent long distance dependencies. Incorporating phrasal nodes of constituency trees into head-dependents relations further enhances the compatibility with phrases of our rules. Figure 1 shows an example of phrases whic</context>
<context position="30946" citStr="Chiang, 2005" startWordPosition="5107" endWordPosition="5108">uency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target side to ensure grammaticality. Feng et al. (2012) presented a hierarchical chunk-to-string translation model, which is a compromise between the hierarchical phrase-based model and the constituency-to-string model. Most works make effort to introduce linguistic knowledge into the phrase-based model and hierarchi</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="22695" citStr="Chiang, 2007" startWordPosition="3792" endWordPosition="3793">rt the input phrasal nodes labeled dependency tree into a target string among all possible derivations. Given the source constituency tree and dependency tree, we first generate phrasal nodes labeled dependency tree T as described in Section 3.1, then the decoder transverses each node in T by postorder. For each node n, it enumerates all instances of CHDR rooted at n, and checks the rule set for matched translation rules. A larger translation is generated by substituting the variables in the target side of a translation rule with the translations of the corresponding dependents. Cube pruning (Chiang, 2007; Huang and Chiang, 2007) is used to find the k-best items with integrated language model for each node. To balance the performance and speed of the decoder, we limit the search space by reducing the number of translation rules used for each node. There are two ways to limit the rule table size: by a fixed limit (rule-limit) of how many rules are retrieved for each input node, and by a threshold (rulethreshold) to specify that the rule with a score lower than Q times of the best score should be discarded. On the other hand, instead of keeping the full list of candidates for a given node, we ke</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>531--540</pages>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 531–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>541--548</pages>
<contexts>
<context position="1700" citStr="Ding and Palmer, 2005" startWordPosition="233" endWordPosition="236">ndencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and sequences of words combine to form constituents, and constituency-based models show better compatibility with phrases. However, dependency trees describe the grammatical relation between words of the sentence, and represent long distance dependencies in a concise manner. Dependency-based models, such as dependency-to-string model (Xie et al., 2011), exhibit better capability</context>
<context position="30594" citStr="Ding and Palmer, 2005" startWordPosition="5046" endWordPosition="5049"> By combining the merits of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translat</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 541–548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Feng</author>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Qun Liu</author>
</authors>
<title>Hierarchical chunk-to-string translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume</booktitle>
<volume>1</volume>
<pages>950--958</pages>
<contexts>
<context position="31283" citStr="Feng et al. (2012)" startWordPosition="5156" endWordPosition="5159">, 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target side to ensure grammaticality. Feng et al. (2012) presented a hierarchical chunk-to-string translation model, which is a compromise between the hierarchical phrase-based model and the constituency-to-string model. Most works make effort to introduce linguistic knowledge into the phrase-based model and hierarchical phrasebased model with constituency trees. Only the work proposed by Mi and Liu, (2010) utilized constituency and dependency trees, while their work applied two types of trees on two sides. Instead, our model simultaneously utilizes constituency and dependency trees on the source side to direct the translation, which is concerned w</context>
</contexts>
<marker>Feng, Zhang, Li, Zhou, Liu, 2012</marker>
<rawString>Yang Feng, Dongdong Zhang, Mu Li, Ming Zhou, and Qun Liu. 2012. Hierarchical chunk-to-string translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 950–958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi J Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>304--3111</pages>
<contexts>
<context position="12290" citStr="Fox, 2002" startWordPosition="2057" endWordPosition="2058">: An annotated dependency tree. Each node is annotated with two spans, the former is node span and the latter subtree span. The fragments covered by phrasal nodes are annotated with phrasal spans. The nodes denoted by the solid line box are not nsp consistent. phrases, in order to complement the information that dependency trees can not capture, we only label the phrasal nodes that cover dependency non-syntactic phrases. Then, we annotate alignment information to the phrasal nodes labeled dependency tree T, as shown in Figure 4. For description convenience, we make use of the notion of spans (Fox, 2002; Lin, 2004). Given a node n in the source phrasal nodes labeled T with word alignment information, the spans of n induced by the word alignment are consecutive sequences of words in the target sentence. As shown in Figure 4, we annotate each node n of phrasal nodes labeled T with two attributes: node span and subtree span; besides, we annotate phrasal span to the parts covered by phrasal nodes in each subtree rooted at n. The three types of spans are defined as follows: Definition 1 Given a node n, its node span nsp(n) is the consecutive target word sequence aligned with the node n. Take the </context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi J Fox. 2002. Phrasal cohesion and statistical machine translation. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 304–3111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<volume>4</volume>
<pages>273--280</pages>
<location>Boston.</location>
<contexts>
<context position="10961" citStr="Galley et al., 2004" startWordPosition="1842" endWordPosition="1845"> to translate “44” to “will”, and r6 to translate the rightmost unfinished part. Then, we apply r7 to translate the phrase “Mr&amp;Ii�,V” to “Ultrabook”, and yield (e). Finally, we apply r8 to translate the last fragment to “the first”, and get the final result (f). 3 Rule Extraction In this section, we describe how to extract rules from a set of 4-tuples (C, T, S, A), where C is a source constituency tree, T is a source dependency tree, S is a target side sentence, and A is an word alignment relation between T/C and S. We extract CHDR rules from each 4-tuple (C, T, S, A) based on GHKM algorithm (Galley et al., 2004) with three steps: 1. Label the dependency tree with phrasal nodes from the constituency tree, and annotate alignment information to the phrasal nodes labeled dependency tree (Section 3.1). 2. Identify acceptable CHDR fragments from the annotated dependency tree for rule induction (Section 3.2). 3. Induce a set of lexicalized and generalized CHDR rules from the acceptable fragments (Section 3.3). 3.1 Annotation Given a 4-tuple (C, T, S, A), we first label phrasal nodes from the constituency tree C to the dependency tree T, which can be easily accomplished by phrases mapping according to the co</context>
<context position="26382" citStr="Galley et al., 2004" startWordPosition="4380" endWordPosition="4383">lopment set. The statistical significance test is performed by sign-test (Collins et al., 2005). 5.2 Systems We take the open source hierarchical phrase-based system Moses-chart (with default configuration), our in-house constituency-to-string system cons2str and dependency-to-string system dep2str as our baseline systems. For cons2str, we follow Liu et al., (Liu et al., 2006) to strict that the height of a rule tree is no greater than 3 and phrase length is no greater than 7. To keep consistent with our proposed model, we implement the dependency-to-string model (Xie et al., 2011) with GHKM (Galley et al., 2004) rule extraction algorithm and utilize bilingual phrases to translate source head node and dependency syntactic phrases. Our dep2str shows comparable performance with Xie et al., (2011), which can be seen by comparing with the results of hierarchical phrase-based model in our experiments. For dep2str and our proposed model consdep2str, we set rulethreshold and stack-threshold to 10−3, rule-limit to 100, stack-limit to 300, and phrase length limit to 7. 5.3 Experimental Results Table 1 illustrates the translation results of our experiments. As we can see, our consdep2str system has gained the b</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule. In Proceedings of HLT/NAACL, volume 4, pages 273–280. Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
</authors>
<title>Training tree transducers.</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>105--112</pages>
<contexts>
<context position="1598" citStr="Graehl and Knight, 2004" startWordPosition="216" endWordPosition="219">del achieves significantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and sequences of words combine to form constituents, and constituency-based models show better compatibility with phrases. However, dependency trees describe the grammatical relation between words of the sentence, and represent long distance dependencies in a concise manner. Dep</context>
<context position="30376" citStr="Graehl and Knight, 2004" startWordPosition="5005" endWordPosition="5008">to the dependency tree, “K Z1, [h 1)IIL” is labeled by a phrasal node “VP” (means verb phrase), which can be captured by our CHDR-phrasal rules and translated into the correct result “reemergence” with bilingual phrases. By combining the merits of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized depen</context>
</contexts>
<marker>Graehl, Knight, 2004</marker>
<rawString>Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In Proc. HLT-NAACL, pages 105–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Hellwig</author>
</authors>
<title>Parsing with dependency grammars.</title>
<date>2006</date>
<booktitle>An International Handbook of Contemporary Research,</booktitle>
<pages>2--1081</pages>
<contexts>
<context position="8358" citStr="Hellwig, 2006" startWordPosition="1361" endWordPosition="1362">, and R is a set of translation rules that include bilingual phrases for translating source language terminals and CHDR rules for translation and reordering. A CHDR rule is represented as a triple (t, s, —), where: • t is CHDR with each node labeled by a terminal from E or a variable from a set X = Ix1, x2, · · · } constrained by a terminal from E or a category from Nd or a joint category (constructed by the categories from Nc and Nd); • s E (X U A) denotes the target side string; • — denotes one-to-one links between nonterminals in t and variables in s. We use the lexicon dependency grammar (Hellwig, 2006) which adopts a bracket representation to express the head-dependents relation and CHDR. For example, the left-hand sides of r1 and r2 in Figure 2 can be respectively represented as follows: (.1) (411)A[h(x1:NN) (A*&amp;quot;T.1) (4) x1:VP2 VV NN ㅜа/OD Intel will launch the first Ultrabook in Asia Translation Rules r3 (x1:NR) (x2:AD) ᧘ (x3:ㅄ䇠ᵜ) x1 x2 launch x3 r4 AWቄ Intel r5 ሶ will r6 (ӊ⍢)(x1:M)x2:NP1 JJ_NN x1 X2 in Aisa 䎵㓗 liaᵜ Ultrabook r8 ㅜа () the first Figure 3: An example derivation of translation. (g) lists all the translation rules. r3, rs and r8 are CHDR rules, while r4, r5 and r7 are bilin</context>
</contexts>
<marker>Hellwig, 2006</marker>
<rawString>Peter Hellwig. 2006. Parsing with dependency grammars. An International Handbook of Contemporary Research, 2:1081–1109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Annual Meeting-Association For Computational Linguistics,</booktitle>
<volume>45</volume>
<pages>144--151</pages>
<contexts>
<context position="22720" citStr="Huang and Chiang, 2007" startWordPosition="3794" endWordPosition="3797">hrasal nodes labeled dependency tree into a target string among all possible derivations. Given the source constituency tree and dependency tree, we first generate phrasal nodes labeled dependency tree T as described in Section 3.1, then the decoder transverses each node in T by postorder. For each node n, it enumerates all instances of CHDR rooted at n, and checks the rule set for matched translation rules. A larger translation is generated by substituting the variables in the target side of a translation rule with the translations of the corresponding dependents. Cube pruning (Chiang, 2007; Huang and Chiang, 2007) is used to find the k-best items with integrated language model for each node. To balance the performance and speed of the decoder, we limit the search space by reducing the number of translation rules used for each node. There are two ways to limit the rule table size: by a fixed limit (rule-limit) of how many rules are retrieved for each input node, and by a threshold (rulethreshold) to specify that the rule with a score lower than Q times of the best score should be discarded. On the other hand, instead of keeping the full list of candidates for a given node, we keep a topscoring subset of</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Annual Meeting-Association For Computational Linguistics, volume 45, pages 144–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings ofAMTA,</booktitle>
<pages>66--73</pages>
<contexts>
<context position="1637" citStr="Huang et al., 2006" startWordPosition="224" endWordPosition="227">r the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and sequences of words combine to form constituents, and constituency-based models show better compatibility with phrases. However, dependency trees describe the grammatical relation between words of the sentence, and represent long distance dependencies in a concise manner. Dependency-based models, such as dependenc</context>
<context position="30414" citStr="Huang et al., 2006" startWordPosition="5013" endWordPosition="5016"> labeled by a phrasal node “VP” (means verb phrase), which can be captured by our CHDR-phrasal rules and translated into the correct result “reemergence” with bilingual phrases. By combining the merits of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion t</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings ofAMTA, pages 66– 73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2002</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<volume>15</volume>
<pages>3--10</pages>
<contexts>
<context position="24400" citStr="Klein and Manning, 2002" startWordPosition="4078" endWordPosition="4082"> our model significantly outperforms all the other baseline models (Section 5.3). Finally, we give detail analysis (Section 5.4). 5.1 Data Preparation Our training data consists of 1.25M sentence pairs extracted from LDC 1 data. We choose NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test sets 2003 (MT03), 2004 (MT04) and 2005 (MT05) as our test sets. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric 2. We parse the source sentences to constituency trees (without binarization) and projective dependency trees with Stanford Parser (Klein and Manning, 2002). The word alignments are obtained by running GIZA++ (Och and Ney, 2003) on the corpus in both directions and using the “grow-diag-finaland” balance strategy (Koehn et al., 2003). We get bilingual phrases from word-aligned data with algorithm described in Koehn et al. (2003) by running Moses Toolkit 3. We apply SRI Language Modeling Toolkit (Stolcke and others, 2002) to train a 4-gram 1Including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 2ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 3http://www.statmt.org/moses/ 1072 System Rule # M</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D Manning. 2002. Fast exact inference with a factored model for natural language parsing. In Advances in neural information processing systems, volume 15, pages 3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume</booktitle>
<volume>1</volume>
<pages>48--54</pages>
<contexts>
<context position="13912" citStr="Koehn et al., 2003" startWordPosition="2339" endWordPosition="2342">ull}{4-5} 䎵㓗/JJ {6-6}{6-6} 1069 all nodes in T′ to the upper bound of the same set of spans. For instance, tsp()i�,*/NN)={4-8}, which corresponds to the target words “the first Ultrabook in Asia”, whose indexes are from 4 to 8. Definition 3 Given a fragment f covered by a phrasal node, the phrasal span psp(f) of f is the consecutive target word sequence aligned with source string covered by f. For example, psp(VP2)=(3-8), which corresponds to the target word sequence “launch the first Ultrabook in Asia”. We say nsp, tsp and psp are consistent according to the notion in the phrase-based model (Koehn et al., 2003). For example, nsp(AVtI/NR), tsp()i�, */NN) and psp(NP1) are consistent while nsp( a &amp;/JJ) and nsp()i�,*/NN) are not consistent. The annotation can be achieved by a single postorder transversal of the phrasal nodes labeled dependency tree. For simplicity, we call the annotated phrasal nodes labeled dependency tree annotated dependency tree. The extraction of bilingual phrases (including the translation of head node, dependency syntactic phrases and the fragment covered by a phrasal node) can be readily achieved by the algorithm described in Koehn et al., (2003). In the following, we focus on C</context>
<context position="24578" citStr="Koehn et al., 2003" startWordPosition="4108" endWordPosition="4111">25M sentence pairs extracted from LDC 1 data. We choose NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test sets 2003 (MT03), 2004 (MT04) and 2005 (MT05) as our test sets. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric 2. We parse the source sentences to constituency trees (without binarization) and projective dependency trees with Stanford Parser (Klein and Manning, 2002). The word alignments are obtained by running GIZA++ (Och and Ney, 2003) on the corpus in both directions and using the “grow-diag-finaland” balance strategy (Koehn et al., 2003). We get bilingual phrases from word-aligned data with algorithm described in Koehn et al. (2003) by running Moses Toolkit 3. We apply SRI Language Modeling Toolkit (Stolcke and others, 2002) to train a 4-gram 1Including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 2ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 3http://www.statmt.org/moses/ 1072 System Rule # MT03 MT04 MT05 Average Moses-chart 116.4M 34.65 36.47 34.39 35.17 cons2str 25.4M+32.5M 33.14 35.12 33.27 33.84 dep2str 19.6M+32.5M 34.85 36.57 34.72 35.38 consdep2str 23.3M+32.5M </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A path-based transfer model for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>625--630</pages>
<contexts>
<context position="1677" citStr="Lin, 2004" startWordPosition="231" endWordPosition="232">e) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and sequences of words combine to form constituents, and constituency-based models show better compatibility with phrases. However, dependency trees describe the grammatical relation between words of the sentence, and represent long distance dependencies in a concise manner. Dependency-based models, such as dependency-to-string model (Xie et al., 2011), ex</context>
<context position="12302" citStr="Lin, 2004" startWordPosition="2059" endWordPosition="2060">ted dependency tree. Each node is annotated with two spans, the former is node span and the latter subtree span. The fragments covered by phrasal nodes are annotated with phrasal spans. The nodes denoted by the solid line box are not nsp consistent. phrases, in order to complement the information that dependency trees can not capture, we only label the phrasal nodes that cover dependency non-syntactic phrases. Then, we annotate alignment information to the phrasal nodes labeled dependency tree T, as shown in Figure 4. For description convenience, we make use of the notion of spans (Fox, 2002; Lin, 2004). Given a node n in the source phrasal nodes labeled T with word alignment information, the spans of n induced by the word alignment are consecutive sequences of words in the target sentence. As shown in Figure 4, we annotate each node n of phrasal nodes labeled T with two attributes: node span and subtree span; besides, we annotate phrasal span to the parts covered by phrasal nodes in each subtree rooted at n. The three types of spans are defined as follows: Definition 1 Given a node n, its node span nsp(n) is the consecutive target word sequence aligned with the node n. Take the node “AVtI/N</context>
<context position="30571" citStr="Lin, 2004" startWordPosition="5044" endWordPosition="5045">al phrases. By combining the merits of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Dekang Lin. 2004. A path-based transfer model for machine translation. In Proceedings of the 20th international conference on Computational Linguistics, pages 625–630.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="1616" citStr="Liu et al., 2006" startWordPosition="220" endWordPosition="223">y improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and sequences of words combine to form constituents, and constituency-based models show better compatibility with phrases. However, dependency trees describe the grammatical relation between words of the sentence, and represent long distance dependencies in a concise manner. Dependency-based mode</context>
<context position="3474" citStr="Liu et al., 2006" startWordPosition="503" endWordPosition="506"> all its dependents in dependency trees, and it encodes phrase pattern and sentence pattern (typically long distance reordering relations). With the advantages of head-dependents relations, the translation rules of our model hold the property of long distance reorderings and the compatibility with phrases. Our new model (Section 2) extracts rules from word-aligned pairs of source trees (constituency and dependency) and target strings (Section 3), and translate source trees into target strings by employing a bottom-up chart-based algorithm (Section 4). Compared with the constituency-to-string (Liu et al., 2006) and dependency-to-string (Xie et al., 2011) models that only employ a single type of trees, our 1066 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1066–1076, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics VP3 VP3 ᧘/VV ADVP NP IP VP2 VP2 NP 㤡⢩ቄ/NR ሶ/AD ㅄ䇠ᵜ/NN QP NP1 NP1 ӊ⍢/NR /M NP CLP NP ADVP 䎵㓗/JJ NR AD VV NR OD M JJ NN ㅜа/OD rz*fi * #viii ]E;&apos;)hl M— a Ma lit* (a) (b) Intel will launch Asia first super laptop Chinese: XWF * Adi RM 34— 0-9 lgi2* English: Intel will launch the first Ultrabook in A</context>
<context position="23594" citStr="Liu et al., 2006" startWordPosition="3951" endWordPosition="3954">le table size: by a fixed limit (rule-limit) of how many rules are retrieved for each input node, and by a threshold (rulethreshold) to specify that the rule with a score lower than Q times of the best score should be discarded. On the other hand, instead of keeping the full list of candidates for a given node, we keep a topscoring subset of the candidates. This can also be done by a fixed limit (stack-limit) and a threshold (stack-threshold). 5 Experiments We evaluated the performance of our model by comparing with hierarchical phrase-based model (Chiang, 2007), constituency-to-string model (Liu et al., 2006) and dependency-to-string model (Xie et al., 2011) on Chinese-English translation. First, we describe data preparation (Section 5.1) and systems (Section 5.2). Then, we validate that our model significantly outperforms all the other baseline models (Section 5.3). Finally, we give detail analysis (Section 5.4). 5.1 Data Preparation Our training data consists of 1.25M sentence pairs extracted from LDC 1 data. We choose NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test sets 2003 (MT03), 2004 (MT04) and 2005 (MT05) as our test sets. The quality of translations is eva</context>
<context position="26141" citStr="Liu et al., 2006" startWordPosition="4335" endWordPosition="4338">01). language model with modified Kneser-Ney smoothing on the Xinhua portion of the English Gigaword corpus. We make use of the standard MERT (Och, 2003) to tune the feature weights in order to maximize the system’s BLEU score on the development set. The statistical significance test is performed by sign-test (Collins et al., 2005). 5.2 Systems We take the open source hierarchical phrase-based system Moses-chart (with default configuration), our in-house constituency-to-string system cons2str and dependency-to-string system dep2str as our baseline systems. For cons2str, we follow Liu et al., (Liu et al., 2006) to strict that the height of a rule tree is no greater than 3 and phrase length is no greater than 7. To keep consistent with our proposed model, we implement the dependency-to-string model (Xie et al., 2011) with GHKM (Galley et al., 2004) rule extraction algorithm and utilize bilingual phrases to translate source head node and dependency syntactic phrases. Our dep2str shows comparable performance with Xie et al., (2011), which can be seen by comparing with the results of hierarchical phrase-based model in our experiments. For dep2str and our proposed model consdep2str, we set rulethreshold </context>
<context position="30394" citStr="Liu et al., 2006" startWordPosition="5009" endWordPosition="5012">K Z1, [h 1)IIL” is labeled by a phrasal node “VP” (means verb phrase), which can be captured by our CHDR-phrasal rules and translated into the correct result “reemergence” with bilingual phrases. By combining the merits of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add </context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 609–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>Improving tree-to-tree translation with packed forests.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP:</booktitle>
<volume>2</volume>
<pages>558--566</pages>
<marker>Liu, L¨u, Liu, 2009</marker>
<rawString>Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 558–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Yajuan L¨u</author>
</authors>
<title>Adjoining tree-to-string translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume</booktitle>
<volume>1</volume>
<pages>1278--1287</pages>
<marker>Liu, Liu, L¨u, 2011</marker>
<rawString>Yang Liu, Qun Liu, and Yajuan L¨u. 2011. Adjoining tree-to-string translation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 1278–1287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="30819" citStr="Marton and Resnik (2008)" startWordPosition="5086" endWordPosition="5089">rases. 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target side to ensure grammaticality. Feng et al. (2012) presented a hierarchical chunk-to-string translation model, which is a compromise between the hierarchical phrase-based model and the c</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In Proceedings ofACL-08: HLT, pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Qun Liu</author>
</authors>
<title>Constituency to dependency translation with forests.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1433--1442</pages>
<contexts>
<context position="31059" citStr="Mi and Liu, (2010)" startWordPosition="5123" endWordPosition="5126"> al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target side to ensure grammaticality. Feng et al. (2012) presented a hierarchical chunk-to-string translation model, which is a compromise between the hierarchical phrase-based model and the constituency-to-string model. Most works make effort to introduce linguistic knowledge into the phrase-based model and hierarchical phrasebased model with constituency trees. Only the work proposed by Mi and Liu, (2010) utilized constituency</context>
</contexts>
<marker>Mi, Liu, 2010</marker>
<rawString>Haitao Mi and Qun Liu. 2010. Constituency to dependency translation with forests. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433–1442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="30451" citStr="Mi et al., 2008" startWordPosition="5021" endWordPosition="5024">erb phrase), which can be captured by our CHDR-phrasal rules and translated into the correct result “reemergence” with bilingual phrases. By combining the merits of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings of ACL-08: HLT, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="20837" citStr="Och and Ney, (2002)" startWordPosition="3494" endWordPosition="3497">ndle the unaligned words of the target side by extending the node spans of the lexicalized head and leaf nodes, and the subtree spans of the lexicalized dependents, on both left and right directions. This procedure is similar with the method of Och and Ney, (2004). During this process, we might obtain m(m ≥ 1) CHDR rules from an acceptable fragment. Each of these rules is assigned with a fractional count 1/m. We take the extracted rule set as observed data and make use of relative frequency estimator to obtain the translation probabilities P(t|s) and P(s|t). 4 Decoding and the Model Following Och and Ney, (2002), we adopt a general loglinear model. Let d be a derivation that convert a 1071 source phrasal nodes labeled dependency tree into a target string e. The probability of d is defined as: P(d) a � Oi(d)&amp;quot;z (1) i where Oi are features defined on derivations and Ai are feature weights. In our experiments of this paper, the features are used as follows: • CHDR rules translation probabilities P(t|s) and P(s|t), and CHDR rules lexical translation probabilities Plex(t|s) and Plex(s|t); • bilingual phrases translation probabilities Pbp(t|s) and Pbp(s|t), and bilingual phrases lexical translation probabil</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="24472" citStr="Och and Ney, 2003" startWordPosition="4091" endWordPosition="4094">3). Finally, we give detail analysis (Section 5.4). 5.1 Data Preparation Our training data consists of 1.25M sentence pairs extracted from LDC 1 data. We choose NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test sets 2003 (MT03), 2004 (MT04) and 2005 (MT05) as our test sets. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric 2. We parse the source sentences to constituency trees (without binarization) and projective dependency trees with Stanford Parser (Klein and Manning, 2002). The word alignments are obtained by running GIZA++ (Och and Ney, 2003) on the corpus in both directions and using the “grow-diag-finaland” balance strategy (Koehn et al., 2003). We get bilingual phrases from word-aligned data with algorithm described in Koehn et al. (2003) by running Moses Toolkit 3. We apply SRI Language Modeling Toolkit (Stolcke and others, 2002) to train a 4-gram 1Including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 2ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 3http://www.statmt.org/moses/ 1072 System Rule # MT03 MT04 MT05 Average Moses-chart 116.4M 34.65 36.47 34.39 35.17 cons2st</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="20482" citStr="Och and Ney, (2004)" startWordPosition="3431" endWordPosition="3434">n rules in Xie et al., (2011). CHDR-normal rules are equivalent with the head-dependents relation rules and the CHDRphrasal rules are the extension of these rules. For convenience of description, we use the subscript to distinguish the phrasal nodes with the same category, such as VP2 and VP3. In actual operation, we use VP instead of VP2 and VP3. We handle the unaligned words of the target side by extending the node spans of the lexicalized head and leaf nodes, and the subtree spans of the lexicalized dependents, on both left and right directions. This procedure is similar with the method of Och and Ney, (2004). During this process, we might obtain m(m ≥ 1) CHDR rules from an acceptable fragment. Each of these rules is assigned with a fractional count 1/m. We take the extracted rule set as observed data and make use of relative frequency estimator to obtain the translation probabilities P(t|s) and P(s|t). 4 Decoding and the Model Following Och and Ney, (2002), we adopt a general loglinear model. Let d be a derivation that convert a 1071 source phrasal nodes labeled dependency tree into a target string e. The probability of d is defined as: P(d) a � Oi(d)&amp;quot;z (1) i where Oi are features defined on deri</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="25677" citStr="Och, 2003" startWordPosition="4269" endWordPosition="4270">ns2str 25.4M+32.5M 33.14 35.12 33.27 33.84 dep2str 19.6M+32.5M 34.85 36.57 34.72 35.38 consdep2str 23.3M+32.5M 35.57* 37.68* 35.62* 36.29 Table 1: Statistics of the extracted rules on training data and the BLEU scores (%) on the test sets of different systems. The “+” denotes that the rules are composed of syntactic translation rules and bilingual phrases (32.5M). The “*” denotes that the results are significantly better than all the other systems (p&lt;0.01). language model with modified Kneser-Ney smoothing on the Xinhua portion of the English Gigaword corpus. We make use of the standard MERT (Och, 2003) to tune the feature weights in order to maximize the system’s BLEU score on the development set. The statistical significance test is performed by sign-test (Collins et al., 2005). 5.2 Systems We take the open source hierarchical phrase-based system Moses-chart (with default configuration), our in-house constituency-to-string system cons2str and dependency-to-string system dep2str as our baseline systems. For cons2str, we follow Liu et al., (Liu et al., 2006) to strict that the height of a rule tree is no greater than 3 and phrase length is no greater than 7. To keep consistent with our propo</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal smt.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>271--279</pages>
<contexts>
<context position="1720" citStr="Quirk et al., 2005" startWordPosition="237" endWordPosition="240">BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and sequences of words combine to form constituents, and constituency-based models show better compatibility with phrases. However, dependency trees describe the grammatical relation between words of the sentence, and represent long distance dependencies in a concise manner. Dependency-based models, such as dependency-to-string model (Xie et al., 2011), exhibit better capability of long distance re</context>
<context position="30614" citStr="Quirk et al., 2005" startWordPosition="5050" endWordPosition="5053">s of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency </context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal smt. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 271–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>577--585</pages>
<contexts>
<context position="1759" citStr="Shen et al., 2008" startWordPosition="245" endWordPosition="248">oy single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and sequences of words combine to form constituents, and constituency-based models show better compatibility with phrases. However, dependency trees describe the grammatical relation between words of the sentence, and represent long distance dependencies in a concise manner. Dependency-based models, such as dependency-to-string model (Xie et al., 2011), exhibit better capability of long distance reorderings. In this paper, we propose to</context>
<context position="30653" citStr="Shen et al., 2008" startWordPosition="5058" endWordPosition="5061">our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target side to ensure gram</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings ofACL-08: HLT, pages 577–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the international conference on spoken language processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke et al. 2002. Srilm-an extensible language modeling toolkit. In Proceedings of the international conference on spoken language processing, volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Xie</author>
<author>Haitao Mi</author>
<author>Qun Liu</author>
</authors>
<title>A novel dependency-to-string model for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>216--226</pages>
<contexts>
<context position="1778" citStr="Xie et al., 2011" startWordPosition="249" endWordPosition="252">rees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and sequences of words combine to form constituents, and constituency-based models show better compatibility with phrases. However, dependency trees describe the grammatical relation between words of the sentence, and represent long distance dependencies in a concise manner. Dependency-based models, such as dependency-to-string model (Xie et al., 2011), exhibit better capability of long distance reorderings. In this paper, we propose to combine the advant</context>
<context position="3518" citStr="Xie et al., 2011" startWordPosition="509" endWordPosition="512"> it encodes phrase pattern and sentence pattern (typically long distance reordering relations). With the advantages of head-dependents relations, the translation rules of our model hold the property of long distance reorderings and the compatibility with phrases. Our new model (Section 2) extracts rules from word-aligned pairs of source trees (constituency and dependency) and target strings (Section 3), and translate source trees into target strings by employing a bottom-up chart-based algorithm (Section 4). Compared with the constituency-to-string (Liu et al., 2006) and dependency-to-string (Xie et al., 2011) models that only employ a single type of trees, our 1066 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1066–1076, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics VP3 VP3 ᧘/VV ADVP NP IP VP2 VP2 NP 㤡⢩ቄ/NR ሶ/AD ㅄ䇠ᵜ/NN QP NP1 NP1 ӊ⍢/NR /M NP CLP NP ADVP 䎵㓗/JJ NR AD VV NR OD M JJ NN ㅜа/OD rz*fi * #viii ]E;&apos;)hl M— a Ma lit* (a) (b) Intel will launch Asia first super laptop Chinese: XWF * Adi RM 34— 0-9 lgi2* English: Intel will launch the first Ultrabook in Asia (c) Figure 1: Illustration of phrases th</context>
<context position="19253" citStr="Xie et al., (2011)" startWordPosition="3224" endWordPosition="3227">at we replace the target sequences covered by the internal nodes and the phrasal nodes with variables. For example, rule r1 in Figure 5-(d) is a lexicalized CHDR-normal rule induced from the CHDR-normal fragment in Figure 5-(a). r9 and r11 are CHDRphrasal rules induced from the CHDR-phrasal fragment in Figure 5-(b) and Figure 5-(c) respectively. As we can see, these CHDR-phrasal rules are partially unlexicalized. To alleviate the sparseness problem, we generalize the lexicalized CHDR-normal rules and partially unlexicalized CHDR-phrasal rules with unlexicalized nodes by the method proposed in Xie et al., (2011). As the modification relations between head and dependents are determined by the edges, we can replace the lexical word of each node with its category (POS tag) and obtain new head-dependents relations with unlexicalized nodes keeping the same modification relations. We generalize the rule by simultaneously turn the nodes of the same type (head, internal, leaf) into their categories. For example, CHDR-normal rules r2 ∼ r7 are generalized from r1 in Figure 5-(d). Besides, r10 and r12 are the corresponding generalized CHDRphrasal rules. Actually, our CHDR rules are the superset of head-dependen</context>
<context position="23644" citStr="Xie et al., 2011" startWordPosition="3958" endWordPosition="3961">w many rules are retrieved for each input node, and by a threshold (rulethreshold) to specify that the rule with a score lower than Q times of the best score should be discarded. On the other hand, instead of keeping the full list of candidates for a given node, we keep a topscoring subset of the candidates. This can also be done by a fixed limit (stack-limit) and a threshold (stack-threshold). 5 Experiments We evaluated the performance of our model by comparing with hierarchical phrase-based model (Chiang, 2007), constituency-to-string model (Liu et al., 2006) and dependency-to-string model (Xie et al., 2011) on Chinese-English translation. First, we describe data preparation (Section 5.1) and systems (Section 5.2). Then, we validate that our model significantly outperforms all the other baseline models (Section 5.3). Finally, we give detail analysis (Section 5.4). 5.1 Data Preparation Our training data consists of 1.25M sentence pairs extracted from LDC 1 data. We choose NIST MT Evaluation test set 2002 as our development set, NIST MT Evaluation test sets 2003 (MT03), 2004 (MT04) and 2005 (MT05) as our test sets. The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric </context>
<context position="26350" citStr="Xie et al., 2011" startWordPosition="4373" endWordPosition="4377">stem’s BLEU score on the development set. The statistical significance test is performed by sign-test (Collins et al., 2005). 5.2 Systems We take the open source hierarchical phrase-based system Moses-chart (with default configuration), our in-house constituency-to-string system cons2str and dependency-to-string system dep2str as our baseline systems. For cons2str, we follow Liu et al., (Liu et al., 2006) to strict that the height of a rule tree is no greater than 3 and phrase length is no greater than 7. To keep consistent with our proposed model, we implement the dependency-to-string model (Xie et al., 2011) with GHKM (Galley et al., 2004) rule extraction algorithm and utilize bilingual phrases to translate source head node and dependency syntactic phrases. Our dep2str shows comparable performance with Xie et al., (2011), which can be seen by comparing with the results of hierarchical phrase-based model in our experiments. For dep2str and our proposed model consdep2str, we set rulethreshold and stack-threshold to 10−3, rule-limit to 100, stack-limit to 300, and phrase length limit to 7. 5.3 Experimental Results Table 1 illustrates the translation results of our experiments. As we can see, our con</context>
<context position="29668" citStr="Xie et al., (2011)" startWordPosition="4898" endWordPosition="4901">0.83 Table 2: The proportion (%) of 1-best translations that employs CHDR-phrasal rules (CHDR-phrasal Sent.) and the proportion (%) of CHDR-phrasal rules in all CHDR rules in these translations (CHDR-phrasal Rule). phrase+verb+noun”. Cons2str gives a bad result with wrong global reordering, while our consdep2str system gains an almost correct result since we capture this pattern by CHDR-normal rules. In the second example, we can see that the Chinese phrase “KZ1, [h1)IIL” is a non-syntactic phrase in the dependency tree, and this phrase can not be captured by head-dependents relation rules in Xie et al., (2011), thus can not be translated as one unit. Since we encode constituency phrasal nodes to the dependency tree, “K Z1, [h 1)IIL” is labeled by a phrasal node “VP” (means verb phrase), which can be captured by our CHDR-phrasal rules and translated into the correct result “reemergence” with bilingual phrases. By combining the merits of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 6 Related Work In recent years, syntax-based models have witnessed</context>
</contexts>
<marker>Xie, Mi, Liu, 2011</marker>
<rawString>Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel dependency-to-string model for statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 216–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>A dependency treelet string correspondence model for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="1740" citStr="Xiong et al., 2007" startWordPosition="241" endWordPosition="244">els, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and sequences of words combine to form constituents, and constituency-based models show better compatibility with phrases. However, dependency trees describe the grammatical relation between words of the sentence, and represent long distance dependencies in a concise manner. Dependency-based models, such as dependency-to-string model (Xie et al., 2011), exhibit better capability of long distance reorderings. In this p</context>
<context position="30634" citStr="Xiong et al., 2007" startWordPosition="5054" endWordPosition="5057">d dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) proposed a constituency-to-dependency translation model, which utilizes constituency forests on the source side to direct the translation, and dependency trees on the target </context>
</contexts>
<marker>Xiong, Liu, Lin, 2007</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A dependency treelet string correspondence model for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="1573" citStr="Yamada and Knight, 2001" startWordPosition="212" endWordPosition="215"> results show that our model achieves significantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and significantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 1 Introduction In recent years, syntax-based models have become a hot topic in statistical machine translation. According to the linguistic structures, these models can be broadly divided into two categories: constituencybased models (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006), and dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These two kinds of models have their own advantages, as they capture different linguistic phenomena. Constituency trees describe how words and sequences of words combine to form constituents, and constituency-based models show better compatibility with phrases. However, dependency trees describe the grammatical relation between words of the sentence, and represent long distance dependencies</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feifei Zhai</author>
<author>Jiajun Zhang</author>
<author>Yu Zhou</author>
<author>Chengqing Zong</author>
</authors>
<title>Tree-based translation without using parse trees.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>3037--3054</pages>
<contexts>
<context position="30507" citStr="Zhai et al., 2012" startWordPosition="5033" endWordPosition="5036">l rules and translated into the correct result “reemergence” with bilingual phrases. By combining the merits of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based model. Mi and Liu, (2010) proposed a constituency-to-dependency translati</context>
</contexts>
<marker>Zhai, Zhang, Zhou, Zong, 2012</marker>
<rawString>Feifei Zhai, Jiajun Zhang, Yu Zhou, and Chengqing Zong. 2012. Tree-based translation without using parse trees. In Proceedings of COLING 2012, pages 3037–3054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>AiTi Aw</author>
<author>Jun Sun</author>
<author>Sheng Li</author>
<author>Chew Lim Tan</author>
</authors>
<title>A tree-to-tree alignmentbased model for statistical machine translation. MTSummit-07,</title>
<date>2007</date>
<pages>535--542</pages>
<contexts>
<context position="30434" citStr="Zhang et al., 2007" startWordPosition="5017" endWordPosition="5020">l node “VP” (means verb phrase), which can be captured by our CHDR-phrasal rules and translated into the correct result “reemergence” with bilingual phrases. By combining the merits of constituency and dependency trees, our consdep2str model learns CHDR-normal rules to acquire the property of long distance reorderings and CHDR-phrasal rules to obtain good compatibility with phrases. 6 Related Work In recent years, syntax-based models have witnessed promising improvements. Some researchers make efforts on constituency-based models (Graehl and Knight, 2004; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007; Mi et al., 2008; Liu et al., 2009; Liu et al., 2011; Zhai et al., 2012). Some works pay attention to dependency-based models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Xie et al., 2011). These models are based on single type of trees. There are also some approaches combining merits of different structures. Marton and Resnik (2008) took the source constituency tree into account and added soft constraints to the hierarchical phrasebased model (Chiang, 2005). Cherry (2008) utilized dependency tree to add syntactic cohesion to the phrased-based </context>
</contexts>
<marker>Zhang, Jiang, Aw, Sun, Li, Tan, 2007</marker>
<rawString>Min Zhang, Hongfei Jiang, AiTi Aw, Jun Sun, Sheng Li, and Chew Lim Tan. 2007. A tree-to-tree alignmentbased model for statistical machine translation. MTSummit-07, pages 535–542.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.998428">
Discourse Level Explanatory Relation Extraction from Product Reviews
Using First-order Logic
</title>
<author confidence="0.99883">
Qi Zhang, Jin Qian, Huan Chen, Jihua Kang, Xuanjing Huang
</author>
<affiliation confidence="0.985017666666667">
School of Computer Science
Fudan University
Shanghai, P.R. China
</affiliation>
<email confidence="0.974537">
{qz, 12110240030, 12210240054, 12210240059, xjhuang}@fudan.edu.cn
</email>
<sectionHeader confidence="0.996987" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999579210526316">
Explanatory sentences are employed to clarify
reasons, details, facts, and so on. High quality
online product reviews usually include not
only positive or negative opinions, but also a
variety of explanations of why these opinions
were given. These explanations can help
readers get easily comprehensible informa-
tion of the discussed products and aspect-
s. Moreover, explanatory relations can also
benefit sentiment analysis applications. In
this work, we focus on the task of identi-
fying subjective text segments and extracting
their corresponding explanations from prod-
uct reviews in discourse level. We propose
a novel joint extraction method using first-
order logic to model rich linguistic features
and long distance constraints. Experimental
results demonstrate the effectiveness of the
proposed method.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.962462104166667">
Through analyzing product reviews with high help-
fulness ratings assigned by readers, we find that a
large number of explanatory sentences are used to
clarify the causes, details, or consequences of opin-
ions. According to the statistic based on the dataset
we crawled from a popular product review website,
more than 56.1% opinion expressions are further
explained by other sentences. Since most consumers
are not experts, these explanations would bring lots
of helpful and easy comprehension information for
them. Suggestions about writing a product review
also advise authors to include not only whether they
like or dislike a product, but also why.&apos;
lhttp://www.reviewpips.com/
http://www.amazon.com/gp/community-help/customer-
For example, let us consider the following snip-
pets extracted from online reviews:
Example 1: TVs with lower refresh rates may
suffer from motion blur. If you’re watching
a fast-paced football game, for example, you
may notice a bit of blurring as the players run
around the field.
Example 2: The LED screen is highly reflec-
tive. The reflection of my own face makes it very
hard to see the subject I am trying to shoot.
The first sentence of example 1 expresses negative
opinion about refresh rate, which is one of the most
important attributes of TV. The second sentence
describes the consequence of it through an example.
In example 2, detail descriptions are used to explain
the reflection problem of the camera screen.
Although, explanations provide valuable infor-
mation, to the best of our knowledge, there is no
existing work that deals with explanation extraction
for opinions in discourse level. We think that if
explanatory relations can be automatically identified
from reviews, sentiment analysis applications may
benefit from it. Existing opinion mining approaches
mainly focus on subjective text. They try to de-
termine the subjectivity and polarity of fragments
of documents (e.g. a paragraph, a sentence, a
phrase and a word) (Pang et al., 2002; Riloff et
al., 2003; Takamura et al., 2005; Mihalcea et al.,
2007; Dasgupta and Ng, ; Hassan and Radev, 2010;
Meng et al., 2012; Dragut et al., 2012). Fine-grained
methods were also introduced to extract opinion
holder, opinion expression, opinion target, and other
opinion elements (Kobayashi et al., 2007; Wu et al.,
</bodyText>
<page confidence="0.767399">
reviews-guidelines
946
</page>
<note confidence="0.805157">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.967186842105263">
2011; Xu et al., 2013; Yang and Cardie, 2013). Ma-
jor research directions and challenges of sentiment
analysis can also be found in surveys (Pang and Lee,
2008; Liu, 2012).
In this work, we aim to identify subjective tex-
t segments and extract their corresponding expla-
nations from product reviews in discourse level.
We propose to use Markov Logic Networks (ML-
N) (Richardson and Domingos, 2006) to learn the
joint model for subjective classification and explana-
tory relation extraction. MLN has been applied in
several natural language processing tasks (Singla
and Domingos, 2006; Poon and Domingos, 2008;
Yoshikawa et al., 2009; Andrzejewski et al., 2011;
Song et al., 2012) and demonstrated its advantages.
It can easily incorporate rich linguistic features and
global constraints by designing various logic for-
mulas, which can also be viewed as templates or
rules. Logic formulas are combined in a proba-
bilistic framework to model soft constraints. Hence,
the proposed approach can benefit a lot from this
framework.
To evaluate the proposed method, we crawled a
large number of product reviews and constructed a
labeled corpus through Amazon’s Mechanical Turk.
Two tasks were deployed for labeling the corpus.
We compared the proposed method with state-of-
the-art methods on the dataset. Experimental results
demonstrate that the proposed approach can achieve
better performance than state-of-the-art methods.
The remaining part of this paper is organized as
follows: In Section 2, we define the problem and
give some examples to show the challenges of this
task. Section 3 describes the proposed MLN based
method. Dataset construction, experimental results
and analyses are given in Section 4. In Section 5, we
present the related work and Section 6 concludes the
paper.
</bodyText>
<sectionHeader confidence="0.987455" genericHeader="method">
2 Problem Statement
</sectionHeader>
<bodyText confidence="0.994231111111111">
Motivated by the argument structure of discourse
relations used in Penn Discourse Treebank (Rash-
mi Prasad and Webber, 2008), in this work, we
adopt the clause unit-based definition. It means that
clauses are treated as the basic units of opinion ex-
pressions and explanations. Let d = {c1, c2, ...cn}
be the clauses of document d. Directed graph
G = (V, E) is used to represent the subjectivity
of clauses and explanatory relationships between
them. In the graph, vertices represent clauses,
whose categories are specified by the vertex at-
tributes. Directed edges describe the explanatory
relationships between them, of which the heads are
explanatory clauses. If clause ca describes a set of
facts which clarify the causes, context, situation, or
consequences of another clause cb, ca −→ cb is used
to indicate that clause ca explains cb.
Adopting clause unit-based definition is based
on the following reasons: 1) clause is normally
considered as the smallest grammatical unit which
can express a complete proposition (Kroeger, 2005);
2) from analyzing online reviews, we observe that
a clause can express a complete opinion about one
aspect in most of cases; 3) in Penn Discourse
Treebank, the basic unit of discourse relations (with
a few exceptions) is also taken to be a clause (Rash-
mi Prasad and Webber, 2008).
Figure 1(a) illustrates a sample document. Figure
1(b) is the corresponding output of the given docu-
ment. In the graph, vertices whose color are black
stand for subjective clauses. The other clauses are
represented by white vertices. Edges describe the
explanatory relationships between them, of which
the heads are explanatory clauses.
Although the explanatory relation extraction task
has been studied from the view of linguistic and
discourse representation by existing works (Carston,
1993; Lascarides and Asher, 1993), the automatic
extraction task is still an open question. Consider the
following examples extracting from online reviews:
Example 3: It takes great pictures. Color ren-
ditions, skin tones, exposure levels are all first rate.
From the example, we can observe that the second
sentence explains the first one. However, the second
sentence itself also expresses opinion on various
opinion targets. In other words, both subjective and
objective sentences can be used as explanations.
Example 4: When we called their service center
they made us wait for them the whole day and no
one turned up. This level of service is simply not
acceptable. The first sentence in example 4 explains
the second one. Hence, the feature of relative
location between two sentences does not always
work well in all cases.
</bodyText>
<footnote confidence="0.491614">
Example 5: This backpack is great! its very big
</footnote>
<page confidence="0.995476">
947
</page>
<figureCaption confidence="0.589724571428571">
(c1) I have both the Panasonic LX3 and the Canon
S90. (c2) Both cameras are quite different but truly
excellent. (c�) The S90 is a true pocket camera.
(c4) It is very compact. (c�) The build quality is
also top notch. (c6) It feels solid and it is easy to
grip. (c7) It is so small and convenient, (c8) you
will find that you will always carry it with you.
</figureCaption>
<figure confidence="0.995831">
C3
C4
C2
C,
Cs
C5
C,
Cs
(a) Example Review (b) Directed Graph Representation
</figure>
<figureCaption confidence="0.999985">
Figure 1: Directed graph representation of a sample document.
</figureCaption>
<bodyText confidence="0.999775916666667">
and fits more than enough stuff. Many sentences,
which express explanatory relation, do not contain
any connectives (e.g. “because”, “the reason is”,
and so on). Lin et al.(2009) generalized four chal-
lenges (include ambiguity, inference, context, and
world knowledge) to automated implicit discourse
relation recognition. In this task, we also need to
address those challenges.
From the these examples, we can observe that ex-
tracting explanatory relations from product reviews
is a challenging task. Both linguistic and global
constraints should be carefully studied.
</bodyText>
<sectionHeader confidence="0.99293" genericHeader="method">
3 The Proposed Approach
</sectionHeader>
<bodyText confidence="0.999975111111111">
In this section, we present our method for jointly
classifying the subjectivity of text segments and
extracting explanatory relations. Firstly, we briefly
describe the framework of Markov Logic Networks.
Then, we introduce the clause extraction method
based on the definition described in the Section 2.
Finally, we present the first-order logic formulas
including local formulas and global formulas used
for joint modeling in this work.
</bodyText>
<subsectionHeader confidence="0.999236">
3.1 Markov Logic Networks
</subsectionHeader>
<bodyText confidence="0.999604461538462">
A MLN consists of a set of logic formulas that
describe first-order knowledge base. Each formula
consists of a set of first-order predicates, logical
connectors and variables. Different with first-order
logic, these hard logic formulas are softened and
can be violated with some penalty (the weight of
formula) in MLN.
We use M to represent a MLN and {(ϕi, wi)}
to represent formula ϕi and its weight wi. These
weighted formulas define a probability distribution
over sets of possible worlds. Let y denote a possible
world, the p(y) is defined as follows (Richardson
and Domingos, 2006):
</bodyText>
<equation confidence="0.801834">
p(y) = Z exp wi ∑f ϕi
c (y)
((ϕi,wi)EM cECnϕi
where each c is a binding of free variable in ϕi to
constraints; fϕi
c (y) is a binary feature function that
</equation>
<bodyText confidence="0.998672230769231">
returns 1 if the true value is obtained in the ground
formula we get by replacing the free variables in
ϕi with the constants in c under the given possible
world y, and 0 otherwise; Cnϕi is all possible
bindings of variables to constants, and Z is a nor-
malization constant.
Many methods have been proposed to learn the
weights of MLN using both generative and dis-
criminative approaches (Richardson and Domingos,
2006; Singla and Domingos, 2006). There are
also several MLN learning packages available online
such as thebeast2, Tuffy3, PyMLNs4, Alchemy5, and
so on.
</bodyText>
<footnote confidence="0.99993825">
2http://code.google.com/p/thebeast
3http://hazy.cs.wisc.edu/hazy/tuffy/
4http://www9-old.in.tum.de/people/jain/mlns/
5http://alchemy.cs.washington.edu/
</footnote>
<page confidence="0.993001">
948
</page>
<bodyText confidence="0.739465375">
Describing the attributes of words
Describing the attributes of the clause ci
word(i, w) The clause ci has word w.
firstWord(i, w) The first word of clause ci is word w.
pos(i, w, t) The POS tag of word w is t in clause ci.
dep(i, h, m) Word m and h are governor and dependent of a dependency
relation in clause ci.
Describing the attributes of relations between clause ci and clause cj
</bodyText>
<tableCaption confidence="0.995873">
Table 1: Descriptions of observed predicates.
</tableCaption>
<figure confidence="0.922726">
subjLexicon(w)
The word w belongs to the subjective lexicon (Baccianella et al.,
2010).
The word w belongs to the lexicon of explanation relation
connectives (Pitler and Nenkova, 2009).
relationLexicon(w)
clauseDistance(i, j, m)
Distance between clause ci and clause cj in clauses is m.
sentenceDistance(i, j, n)
Distance between clause ci and clause cj in sentences is n.
</figure>
<subsectionHeader confidence="0.995775">
3.2 Clause Identification
</subsectionHeader>
<bodyText confidence="0.9999922">
We model the clause boundary identification prob-
lem through sequence labeling and use Conditional
Random Fields (CRFs) to identify clause bound-
aries. Words and part-of-speech (POS) tags are used
as feature sets. Since we do not allow embedded
segments, the performance of our method is promis-
ing, which achieves the F1 score of 92.8%. The
result is comparable with the best results obtained
during the CoNLL-2001 campaign (Tjong et al.,
2001).
</bodyText>
<subsectionHeader confidence="0.988399">
3.3 Formulas
</subsectionHeader>
<bodyText confidence="0.999923428571429">
In this work, we propose to use predicate subj(i)
to indicate that the ith clause is subjective and
explain(i, j) to indicate that the jth clause explains
the ith clause. Both subj and explain are hidden
predicates and jointly modeled by MLN. We use
local and global formulas to model rich linguistic
features and long distance constraints.
</bodyText>
<subsectionHeader confidence="0.918118">
3.3.1 Local Formulas
</subsectionHeader>
<bodyText confidence="0.99929306060606">
The local formulas relate one or more observed
predicates to exactly one hidden predicate. In this
work, we define a list of observed predicates to
describe the properties of individual clauses and
attributes of relations between two clauses. The
observed predicates and descriptions are shown in
Table 1. The observed predicates can be categorized
into 3 groups: words, clauses, and relations between
clauses. We use two lexicons to capture background
knowledge of words. Lexical, part-of-speech tag,
and dependency relation are used to describe a single
clause. We also propose two predicates to model
distance between clauses.
Table 2 lists the local formulas used in this work.
The “+” notation in the formulas indicates that each
constant of the logic variable should be weighted
separately. For subjective classification and relation
extraction, we construct a number of formulas re-
spectively.
For subjective classification, the first two formu-
las model the influence of lexical and POS tag. It
is similar as the bag-of-words model, which is a
simplifying representation and has been successfully
used for various natural language processing tasks.
Since words which provide positive or negative
opinions may provide important information for
subjectivity classification, we combine predicates of
words and lexicon of opinion words. Bigrams are
also proved to be useful for textual classification in
several NLP tasks. Hence, we also combine predi-
cates about individual word and POS tag to capture
this kind of information. Word-level relations are
explicitly presented at the dependency trees, we
</bodyText>
<page confidence="0.994552">
949
</page>
<bodyText confidence="0.772278">
Formulas for subjective classification
</bodyText>
<equation confidence="0.999720545454545">
word(i,w+) ⇒ subj(i)
pos(i,w+,t+) ⇒ subj(i)
word(i,w+) ∧ subjLexicon(w) ⇒ subj(i)
pos(i,w+,t+) ∧ subjLexicon(w) ⇒ subj(i)
word(i,w1+) ∧ word(i,w2+) ⇒ subj(i)
pos(i,w1+,t+) ∧ pos(i,w2+,t+) ⇒ subj(i)
word(i,w1+) ∧ word(i,w2+) ∧ subjLexicon(w1) ⇒ subj(i)
word(i,w1+) ∧ word(i,w2+) ∧ subjLexicon(w2) ⇒ subj(i)
dep(i,w1+,w2+) ⇒ subj(i)
dep(i,w1+,w2+) ∧ subjLexicon(w1) ⇒ subj(i)
dep(i,w1+,w2+) ∧ subjLexicon(w2) ⇒ subj(i)
</equation>
<bodyText confidence="0.536257">
Formulas for explanatory relation extraction
</bodyText>
<equation confidence="0.999936652173913">
word(i,w1+) ∧ word(j,w2+) ∧ j̸�i ⇒ explain(i,j)
pos(i,w1+,t+) ∧ pos(j,w2+,t+) ∧ j̸�i ⇒ explain(i,j)
dep(i,h1+,m1+) ∧ dep(j,h2+,m2+)∧ j̸�i ⇒ explain(i,j)
word(i,w1+) ∧ word(j,w2+) ∧ clauseDistance(i,j,m+) ∧ j̸�i ⇒ explain(i,j)
pos(i,w1+,t+) ∧ pos(j,w2+,t+) ∧ clauseDistance(i,j,m+) ∧ j̸�i ⇒ explain(i,j)
dep(i,h1+,m1+) ∧ dep(j,h2+,m2+) ∧ clauseDistance(i,j,m+) ∧ j̸�i ⇒ explain(i,j)
word(i,w1+) ∧ word(j,w2+) ∧ sentenceDistance(i,j,n+) ∧ j̸�i ⇒ explain(i,j)
pos(i,w1+,t+) ∧ pos(j,w2+,t+) ∧ sentenceDistance(i,j,n+) ∧ j̸�i ⇒ explain(i,j)
dep(i,h1+,m1+) ∧ dep(j,h2+,m2+) ∧ sentenceDistance(i,j,n+) ∧ j̸�i ⇒ explain(i,j)
word(i,w1+) ∧ word(j,w2+) ∧ firstWord(j,w+) ∧ j̸�i ⇒ explain(i,j)
pos(i,w1+,t+) ∧ pos(j,w2+,t+) ∧ firstWord(j,w+) ∧ j̸�i ⇒ explain(i,j)
dep(i,h1+,m1+) ∧ dep(j,h2+,m2+) ∧ firstWord(j,w+) ∧ j̸�i ⇒ explain(i,j)
word(i,w1+) ∧ word(j,w2+) ∧ subjLexicon(w1) ∧ j̸�i ⇒ explain(i,j)
pos(i,w1+,t+) ∧ pos(j,w2+,t+) ∧ subjLexicon(w1) ∧ j̸�i ⇒ explain(i,j)
dep(i,h1+,m1+) ∧ dep(j,h2+,m2+) ∧ subjLexicon(m1) ∧ j̸�i ⇒ explain(i,j)
firstWord(j,w+) ∧ relationLexicon(w) ∧ clauseDistance(i,j,m+) ∧ j̸�i ⇒ explain(i,j)
firstWord(j,w+) ∧ relationLexicon(w) ∧ sentenceDistance(i,j,n+) ∧ j̸�i ⇒ explain(i,j)
firstWord(j,w+) ∧ relationLexicon(w) ∧ pos(j,w,t+) ∧ clauseDistance(i,j,m+) ∧ j̸�i ⇒ explain(i,j)
firstWord(j,w+) ∧ relationLexicon(w) ∧ pos(j,w,t+) ∧ sentenceDistance(i,j,n+) ∧ j̸�i ⇒ explain(i,j)
firstWord(j,w+) ∧ relationLexicon(w) ∧ word(i,w1+) ∧ clauseDistance(i,j,m+) ∧ j̸�i ⇒ explain(i,j)
firstWord(j,w+) ∧ relationLexicon(w) ∧ word(i,w1+) ∧ sentenceDistance(i,j,n+) ∧ j̸�i ⇒ explain(i,j)
firstWord(j,w+) ∧ relationLexicon(w) ∧ word(j,w1+) ∧ clauseDistance(i,j,m+) ∧ j̸�i ⇒ explain(i,j)
firstWord(j,w+) ∧ relationLexicon(w) ∧ word(j,w1+) ∧ sentenceDistance(i,j,n+) ∧ j̸�i ⇒ explain(i,j)
</equation>
<tableCaption confidence="0.985615">
Table 2: Descriptions of local formulas.
</tableCaption>
<page confidence="0.992289">
950
</page>
<bodyText confidence="0.998914266666667">
also construct local formulas based on predicates
extracted from dependency trees of clauses.
For explanatory relation extraction, we firstly use
formulas to capture lexical and syntactic information
from both of the clauses. Since distances between
clauses are helpful in determining the relation, we
incorporate two kinds of distance features with lex-
ical and syntactic predicates. Connective words
such as for example, since, explicitly signal the
presence of the explanation relation. Although some
connective words are ambiguous in terms of relation
they mark (Pitler and Nenkova, 2009), they may still
be useful for explanation relation extraction. Hence,
we construct local formulas with relation lexicon
and other predicates.
</bodyText>
<subsectionHeader confidence="0.592897">
3.3.2 Global Formulas
</subsectionHeader>
<bodyText confidence="0.999499">
Local formulas are designed to deal with sub-
jective classification of a single clause or relation
determination of a single pair of clauses. Global
formulas are designed to handle global constraints
of multiple clauses. From the definition of explana-
tory relation and corpus statistics, we observe the
following properties:
</bodyText>
<figureCaption confidence="0.774867333333333">
Property 1: One clause can only serve as the
explanation of one subjective clause.
Property 2: Explanatory clauses occur immedi-
ately before or after their corresponding subjective
clauses.
Property 3: The positions of explanatory clauses
are consecutive. In other words, if clause ck and
ck+2 explain clause cj, the clause ck+1 would also
be explanatory clause of cj.
For property 1, we use the following global for-
mula to make sure that one clause only explains at
most one another clause.
</figureCaption>
<bodyText confidence="0.926053">
explain(i, j) ==&gt;. -,explain(k, j) bk 7� i, j (1)
Based on the property 2 and 3, explanatory claus-
es are consecutive and immediately before or after
their corresponding subjective clauses. We use the
following formulas to guarantee the property:
</bodyText>
<equation confidence="0.69211375">
explain(i, i + k) ==&gt;. explain(i, i + m),
1 &lt; m &lt; k − 1
explain(i, i − k) ==&gt;. explain(i, i − m),
1 &lt; m &lt; k − 1
</equation>
<page confidence="0.963049">
951
</page>
<bodyText confidence="0.9996315">
Since our aim is to extract explanatory for subjec-
tive clauses, we also use the following formulas to
make sure that the clauses which are explained are
subjective ones.
</bodyText>
<equation confidence="0.522277">
explain(i, j) ==&gt;. subj(i) (4)
</equation>
<sectionHeader confidence="0.997947" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.990717">
4.1 Data Set
</subsectionHeader>
<bodyText confidence="0.999934263157895">
We crawled a number of reviews about digital cam-
eras from Buzzillions6, which is a product review
site and contains more than 16 million reviews.
We randomly select 100 reviews whose usefulness
ratings are 5 on a 5-point scale. They contain 1137
sentences, which are composed by 1665 clauses.
Amazon’s Mechanical Turk is used to deploy two
tasks for labeling the corpus. 694 clauses are labeled
subjective and 478 clauses explain other ones. More
than 56.1% opinion expressions are explained by
their corresponding explanatory sentences.
The two projects we deployed on Amazon’s Me-
chanical Turk are: 1) Determine whether a clause
contains opinion expressions or not; 2) Determine
whether a clause clarifies causes, reasons, or conse-
quences of another given clause. In order to control
the labeling quality, we configured parameters of
the project to make sure that all the tasks should
be judged by at least 20 annotators. Most of the
annotators can complete a task within 25 seconds.
Figure 2 shows the screenshots of the two projects.
Over all, 127 workers participated in the project.
About 72% of them submitted more than 5 tasks.
Although we listed several examples on the project
descriptions, different people may have their own
understanding and criteria for those tasks. In order
to measure the quality of the labeling task, we use
perplexity to evaluate each task. If the perplexity
of a task is below 0.51, which means that more than
80% of the workers submitted the same decision, the
result of the task will be used as training or testing
data. From the statistic of the corpus, we observe
that only 6.2% of the clauses’ subjectiveness and
15.6% of explanation relations can not be certainly
decided. For the first project, we treated those claus-
es as objective one. And, those clause pairs in the
second project were not considered as explanation
relations.
</bodyText>
<footnote confidence="0.989814">
6www.buzzillions.com
</footnote>
<table confidence="0.826131454545455">
Task7: Help us determine whether a sentence is
subjective or objective.
The following sentences are extracted from product reviews. Please
help us check whether the following sentences expressing opinion
towards some attributes/parts of a product.
The battery life is something I come to expect from this line of camera.
()Subjective
()Objective
I have the camera set to shut off the sensor after about 30 seconds
()Subjective
()Objective
</table>
<tableCaption confidence="0.503202">
Task2: Help us check whether a sentence is an
explanation of the opinion sentence.
</tableCaption>
<bodyText confidence="0.796598285714286">
The opinion sentence (red one) is extracted from product reviews and
express opinion towards some attributes/parts of a product. Please
help us check whether the following blue sentences describe a set of
facts which clarifies the causes, reason, and consequences of the
opinion given in the opinion sentence.
click &amp;quot;yes&amp;quot; if there is an explanation relation between them, &amp;quot;no&amp;quot;
otherwise.
</bodyText>
<figure confidence="0.94377925">
The battery life is something I come to expect from this line of camera.
I can leave the camera on for better than 8 hours shooting
()YES
()NO
The batery life is something I come to expect from this line of camera.
and I have the camera set to shut off the sensor after about 30 seconds
()YES
()NO
</figure>
<figureCaption confidence="0.9966025">
Figure 2: Screenshots of the two tasks on Amazon
Mechanical Turk.
</figureCaption>
<subsectionHeader confidence="0.900882">
4.2 Experiments Configurations
</subsectionHeader>
<bodyText confidence="0.999919555555556">
Stanford parser (Klein and Manning, 2003) is used
for extracting features from dependency parse trees.
For resolving Markov logic network, we use the
toolkit thebeast 7. The detailed setting of thebeast
engine is as follows: The inference algorithm is
the MAP inference with a cutting plane approach.
For parameter learning, the weights for formulas are
updated by an online learning algorithm with MIRA
update rule. All the initial weights are set to zeros.
The number of iterations is set to 10 epochs.
Evaluation metrics used for subjectivity classifi-
cation and relation extraction throughout the experi-
ments include: Precision, Recall, and F1-score. We
randomly select 80% reviews as training set and the
others as testing set.
Since the dataset is newly created for this task, to
compare the performance of the proposed method to
other models, we also reimplemented several state-
</bodyText>
<footnote confidence="0.764741">
7http://code.google.com/p/thebeast
</footnote>
<bodyText confidence="0.653598">
of-the-art methods for comparison.
</bodyText>
<listItem confidence="0.958161692307692">
• CRF-Subj: We follow the method proposed by
Zhao et al. (2008), which regard the subjec-
tivity of all clauses throughout a paragraph as
a sequential flow of sentiments and use CRFs
to model it. The feature sets are similar as
the local formulas for MLN including words,
POS tags, dependency relations, and opinion
lexicon.
• RAE-Subj: Socher et al. (2011) proposed to
use recursive autoencoders for sentence-level
predication of sentiment label distributions. To
compare with it, we also reimplement their
method without any hand designed lexicon.
• PDTB-Rel: For discourse relation extraction,
we use “PDTB-Styled End-to-End Discourse
Parser” (Lin et al., 2010) to extract discourse
level relations as baseline. Since it is a gener-
al discourse relations identification algorithms,
“Cause”, “Pragmatic Cause”, “Instantiation”,
and “Restatement” relation types are treated as
explanatory relation in this work.
• SVM-Rel: We also use LibSVM (Chang and
Lin, 2011) to classify the relations between
clauses. Following the configurations reported
by Feng and Hirst (2012), we use linear kernel
and probability estimation to model it.
</listItem>
<subsectionHeader confidence="0.808759">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.9995176875">
Table 3 shows the comparisons of the proposed
method with the state-of-the-art systems on subjec-
tivity classification and explanatory relation extrac-
tion. From the results, we can observe that recur-
sive autoencoders based subjectivity classification
method achieves slightly better performance than
our method and conditional random fields based
method. The performances of the proposed method
are similar as CRFs’. We think that the main reason
is that only lexical features are used in MLN models
for subjective classification. However, conditional
random fields consider not only lexical information
but also inference of the contexts of sentences.
RAE method learns vector space representations for
multi-word phrases and uses compositional seman-
tics to understand sentiment.
</bodyText>
<page confidence="0.994251">
952
</page>
<table confidence="0.999222818181818">
Methods Subjective Classification
P R F1
CRF-Subj 83.5% 76.9% 80.1%
RAE-Subj 85.3% 79.1% 82.1%
MLN 79.2% 80.6% 79.9%
Relation Extraction
Methods
P R F1
RAE-Subj + PDTB-Rel 28.5% 38.6% 32.8%
RAE-Subj + SVM-Rel 32.4% 89.7% 47.6%
MLN 56.2% 72.9% 63.5%
</table>
<tableCaption confidence="0.998823666666667">
Table 3: Performance comparisons between the proposed
method and state-of-the-art methods. “MLN” represents
the method proposed in this work.
</tableCaption>
<bodyText confidence="0.999981725">
For evaluating the performance of relation extrac-
tion, we combine the results of RAE with PDTB-
Rel and SVM-Rel. For all the subjective clauses
identified by RAE, PDTB-Rel and SVM-Rel are
used to extract corresponding explanatory clauses.
The results are shown in the last three rows in
the Table 3. From the results, we can observe
that the proposed joint model achieves best F1
score and precision among all methods. Although
the proposed method achieve slightly worse result
in processing subjectivity classification. We think
that the error propagation is the main reason for
worse results of cascaded methods. The relative
improvement of MLN over SVM-Rel is more than
33.4%.
To show the effectiveness of different observed
predicates, we evaluate the performances of the
proposed method with different predicate sets. We
subtract one observed predicate and its correspond-
ing local formulas from the original sets at a time.
The results of both subjectivity classification and
relation extraction are shown in Table 4. The first
row shows the result of the MLN based method with
all observed predicates and local formulas. From the
results we can observe that the observed predicates
which are not used in the local formulas for sub-
jectivity classification also impact the performance
of subjectivity classification. We think that the per-
formance is effected by the global formulas, which
combine the procedure of subjectivity classification
and relation extraction. Among all predicates, we
observe that words and dependency relations play
the most important roles. Without word predicate,
the F1 score of subjectivity classification and re-
lation extraction significantly drop to 51.2% and
42.9% respectively. For subjectivity classification,
subjective lexicon contributes a lot for recall. For
relation extraction, the impacts of clause distance
and sentence distance are not as significant as the
other features.
</bodyText>
<sectionHeader confidence="0.999915" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999935371428571">
Our work relates to three research areas: sentiment
analysis/opinion mining, discourse-level relation ex-
traction, and Markov logic networks. Along with the
increasing requirement, subjectivity classification
has recently received considerable attention from
both the industry and researchers. A variety of
approaches and methods have been proposed for
this task from different aspects. Among them, a
number of approaches focus on classifying senti-
ments of text in different levels (e.g. words (Kim
and Hovy, 2004), phrases (Wilson et al., 2005),
sentences (Zhao et al., 2008), documents (Pang et
al., 2002) and so on.), and detecting the overall
polarity of them.
Another research direction tries to convert the
sentiment analysis task into entity identification and
relation extraction. Hu and Liu (2004) proposed
to use a set of methods to produce feature-based
summary of a large number of customer reviews.
Kobayashi et al. (2007) assumed that evaluative
opinions could be structured as a frame which is
composed by opinion holder, subject, aspect, and
evaluation. They converted the task to two kinds
of relation extraction tasks and proposed a machine
learning-based method which used both contextual
and statistical clues.
Analysis of some special types of sentences were
also introduced in recent years. Jindal and Li-
u (2006) studied the problem of identifying com-
parative sentences. They analyzed different types
of comparative sentences and proposed learning
approaches to identify them. Conditional sentences
were studied by Narayanan et al (2009). They
analyzed the conditional sentences in both linguistic
and computitional perspectives and used learning
</bodyText>
<page confidence="0.996785">
953
</page>
<table confidence="0.999866454545455">
Subjective Classification Relation Extraction
P R F1 P R F1
MLN 79.2% 80.6% 79.9% 56.2% 72.9% 63.5%
−subjLexicon(w) 76.6% 70.4% 73.4 % 52.3% 68.6% 59.4%
−relationLexicon(w) 78.2% 79.4% 78.8% 53.6% 70.8% 61.0%
−word(i, w) 52.8% 49.6% 51.2 % 36.4% 52.1% 42.9%
−firstWord(i, w) 76.3% 80.1% 78.2% 56.9% 69.8% 62.7%
−pos(i, w, t) 72.6% 76.8% 74.6 % 52.4% 60.2% 56.0%
−dep(i, h, m) 57.6% 70.6% 63.4% 41.2% 56.8% 47.8%
−clauseDistance(i, j, m) 78.9% 80.2% 79.5% 52.6% 70.6% 60.3%
−sentenceDistance(i, j, n) 78.6% 80.3% 79.4% 52.4% 70.8% 60.2%
</table>
<tableCaption confidence="0.999908">
Table 4: Performance comparisons of different observed predicates
</tableCaption>
<bodyText confidence="0.999842941176471">
method to do it. They followed the feature-based
sentiment analysis model (Hu and Liu, 2004), which
also use flat frames to represent evaluations.
Since the cross sentences relations are considered
in this work, the discourse-level relation extrac-
tion methods are also related to ours. Marcu and
Echihabi (2002) proposed to use an unsupervised
approach to recognizing discourse relations. Lin et
al.(2009) analyzed the impacts of features extracted
from contextual information, constituent parse trees,
dependency parse trees, and word pairs. Asher
et al.(2009) studied discourse segments containing
opinion expressions from the perspective of linguis-
tics. Chen et al. (2010) introduced a multi-label
model to detect emotion causes. They developed
two sets of linguistic features for this task base on
linguistic cues. Zirn et al. (2011) proposed to use
MLN framework to capture the context information
in analysing (sub-)sentences.
The most similar work to ours was proposed by
Somasundaran et al.(2009). They proposed to use it-
erative classification algorithm to capture discourse-
level associations. However different to us, they
focused on pairwise relationships between opinion
expressions. In this paper, we used MLN framework
to capture another different discourse-level relation,
which exists between subject clauses or subject
clause and objective clause.
Richardson and Domingos (2006) proposed
Markov Logic Networks, which combines first-
order logic and probabilistic graphical models. In
recent years, MLN has been adopted for several
natural language processing tasks and achieved
a certain level of success (Singla and Domingos,
2006; Riedel and Meza-Ruiz, 2008; Yoshikawa
et al., 2009; Andrzejewski et al., 2011; Jiang
et al., 2012; Huang et al., 2012). Singla and
Domingos (2006) modeled the entity resolution
problem with MLN. They demonstrated the
capability of MLN to seamlessly combine a number
of previous approaches. Poon and Domingos (2008)
proposed to use MLN for joint unsupervised
coreference resolution. Yoshikawa et al. (2009)
proposed to use Markov logic to incorporate both
local features and global constraints that hold
between temporal relations. Andrzejewski et
al. (2011) introduced a framework for incorporating
general domain knowledge, which is represented by
First-Order Logic (FOL) rules, into LDA inference
to produce topics shaped by both the data and the
rules.
</bodyText>
<sectionHeader confidence="0.996775" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999675444444444">
In this paper, we propose to use Markov logic
networks to identify subjective text segments and ex-
tract their corresponding explanations in discourse
level. We use MLN to jointly model subjectivity
classification and explanatory relation extraction.
Rich linguistic features and global constraints are
incorporated by various logic formulas and global
formulas. To evaluate the proposed method, we
collected a large number of product reviews and
</bodyText>
<page confidence="0.995846">
954
</page>
<bodyText confidence="0.99161125">
constructed a labeled corpus through Amazon’s Me-
chanical Turk. Experimental results demonstrate
that the proposed approach achieve better perfor-
mance than state-of-the-art methods.
</bodyText>
<sectionHeader confidence="0.996785" genericHeader="acknowledgments">
7 Acknowledgement
</sectionHeader>
<bodyText confidence="0.943937307692308">
The authors wish to thank the anonymous reviewers
for their helpful comments and Kang Han for
preparing the corpus. This work was partially
funded by National Natural Science Foundation
of China (61003092, 61073069), Key Projects
in the National Science &amp; Technology Pillar
Program(2012BAH18B01), National Major
Science and Technology Special Project of China
(2014ZX03006005), Shanghai Municipal Science
and Technology Commission (12511504502) and
“Chen Guang” project supported by Shanghai
Municipal Education Commission and Shanghai
Education Development Foundation(11CG05).
</bodyText>
<sectionHeader confidence="0.997" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998152722891566">
David Andrzejewski, Xiaojin Zhu, Mark Craven, and
Benjamin Recht. 2011. A framework for incorpo-
rating general domain knowledge into latent dirichlet
allocation using first-order logic. In Proceedings of
the Twenty-Second international joint conference on
Artificial Intelligence - Volume Volume Two, IJCAI’11,
pages 1171–1177. AAAI Press.
Nicholas Asher, Farah Benamara, and Yannick Mathieu.
2009. Appraisal of opinion expressions in discourse.
Lingvisticte Investigationes.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk,
Stelios Piperidis, Mike Rosner, and Daniel Tapias,
editors, Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC’10), Valletta, Malta, may. European Language
Resources Association (ELRA).
R. Carston. 1993. Conjunction, explanation and
relevance. Lingua 90, pages 27–48.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3):27:1–27:27, May.
Ying Chen, Sophia Yat Mei Lee, Shoushan Li, and
Chu-Ren Huang. 2010. Emotion cause detection
with linguistic constructions. In Proceedings ofColing
2010.
Sajib Dasgupta and Vincent Ng. Mine the easy, classify
the hard: A semi-supervised approach to automatic
sentiment classification. In Proceedings of ACL-
IJCNLP 2009.
Eduard Dragut, Hong Wang, Clement Yu, Prasad Sistla,
and Weiyi Meng. 2012. Polarity consistency
checking for sentiment dictionaries. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 997–1005, Jeju Island, Korea, July. Association
for Computational Linguistics.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-
level discourse parsing with rich linguistic features.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 60–68, Jeju Island, Korea, July.
Association for Computational Linguistics.
Ahmed Hassan and Dragomir R. Radev. 2010.
Identifying text polarity using random walks. In
Proceedings of ACL 2010, Uppsala, Sweden, July.
Minqing Hu and Bing Liu. 2004. Mining and
summarizing customer reviews. In Proceedings of
SIGKDD 2004.
Minlie Huang, Xing Shi, Feng Jin, and Xiaoyan Zhu.
2012. Using first-order logic to compress sentences.
In Proceedings of the Twenty-Sixth AAAI Conference
on Artificial Intelligence.
Shangpu Jiang, D. Lowd, and Dejing Dou. 2012. Learn-
ing to refine an automatically extracted knowledge
base using markov logic. In Data Mining (ICDM),
2012 IEEE 12th International Conference on, pages
912–917.
Nitin Jindal and Bing Liu. 2006. Identifying comparative
sentences in text documents. In Proceedings of SIGIR
2006.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of COLING
2004.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Proceedings of NIPS 2003.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of
relations in opinion mining. In Proceedings of
EMNLP-CoNLL 2007.
Paul Kroeger. 2005. Analyzing Grammar: An
Introduction. Cambridge.
Alex Lascarides and Nicholas Asher. 1993. Temporal
interpretation, discourse relations, and common sense
entailment. Linguistics and Philosophy, 16(5):437–
493.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of EMNLP 2009.
</reference>
<page confidence="0.993623">
955
</page>
<reference confidence="0.994398224299066">
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
A pdtb-styled end-to-end discourse parser. CoRR,
abs/1011.0835.
Bing Liu. 2012. Sentiment Analysis and Opinion
Mining. Morgan &amp; Claypool Publishers.
Daniel Marcu and Abdessamad Echihabi. 2002.
An unsupervised approach to recognizing discourse
relations. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
’02, pages 368–375.
Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou,
Ge Xu, and Houfeng Wang. 2012. Cross-
lingual mixture model for sentiment classification.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 572–581, Jeju Island, Korea, July.
Association for Computational Linguistics.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2007.
Learning multilingual subjective language via cross-
lingual projections. In Proceedings of ACL 2007.
Ramanathan Narayanan, Bing Liu, and Alok Choudhary.
2009. Sentiment analysis of conditional sentences. In
Proceedings of EMNLP 2009.
Bo Pang and Lillian Lee. 2008. Opinion mining
and sentiment analysis. Foundations and Trends in
Information Retrieval, 2:1–135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of
EMNLP 2002.
Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text. In
Proceedings of the ACL-IJCNLP 2009.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with markov logic. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ’08, pages
650–659, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Alan Lee Eleni Miltsakaki Livio Robaldo Aravind Joshi
Rashmi Prasad, Nikhil Dinesh and Bonnie Webber.
2008. The penn discourse treebank 2.0. In
Bente Maegaard Joseph Mariani Jan Odjik Stelios
Piperidis Daniel Tapias Nicoletta Calzolari (Confer-
ence Chair), Khalid Choukri, editor, Proceedings of L-
REC’08, Marrakech, Morocco, may. http://www.lrec-
conf.org/proceedings/lrec2008/.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107–136.
Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collective
semantic role labelling with markov logic. In
Proceedings of the Twelfth Conference on Compu-
tational Natural Language Learning, CoNLL ’08,
pages 193–197, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of HLT-NAACL 2003.
P. Singla and P. Domingos. 2006. Entity resolution with
markov logic. In Data Mining, 2006. ICDM ’06. Sixth
International Conference on, pages 572–582.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ’11, pages 151–161,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Swapna Somasundaran, Galileo Namata, Lise Getoor,
and Janyce Wiebe. 2009. Opinion graphs for
polarity and discourse classification. In Proceedings
of TextGraphs-4.
Yang Song, Jing Jiang, Wayne Xin Zhao, Sujian Li, and
Houfeng Wang. 2012. Joint learning for coreference
resolution with markov logic. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1245–1254, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In Proceedings of ACL 2005.
Erik F. Tjong, Kim Sang, and Herv´e D´ejean. 2001.
Introduction to the conll-2001 shared task: clause
identification. In Proceedings of the 2001 workshop
on Computational Natural Language Learning -
Volume 7, ConLL ’01, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT-EMNLP
2005.
Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide
Wu. 2011. Structural opinion mining for graph-based
sentiment representation. In Proceedings of EMNLP
2011.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and
Jun Zhao. 2013. Walk and learn: a two-stage
approach for opinion words and opinion targets co-
extraction. In Proceedings of the 22nd international
conference on World Wide Web companion, WWW
’13 Companion, pages 95–96, Republic and Canton of
Geneva, Switzerland. International World Wide Web
Conferences Steering Committee.
</reference>
<page confidence="0.985642">
956
</page>
<reference confidence="0.999552727272727">
Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In Proceedings of
ACL 2013.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009. Jointly
identifying temporal relations with markov logic. In
Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1 - Volume 1, ACL ’09,
pages 405–413, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jun Zhao, Kang Liu, and Gen Wang. 2008. Adding
redundant features for crfs-based sentence sentiment
classification. In Proceedings of EMNLP 2008.
C¨acilia Zirn, Mathias Niepert, Heiner Stuckenschmidt,
and Michael Strube. 2011. Fine-grained sentiment
analysis with structural features. In Proceedings of 5th
International Joint Conference on Natural Language
Processing, pages 336–344, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.
</reference>
<page confidence="0.997564">
957
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.490523">
<title confidence="0.994102">Discourse Level Explanatory Relation Extraction from Product Using First-order Logic</title>
<author confidence="0.99291">Qi Zhang</author>
<author confidence="0.99291">Jin Qian</author>
<author confidence="0.99291">Huan Chen</author>
<author confidence="0.99291">Jihua Kang</author>
<author confidence="0.99291">Xuanjing</author>
<affiliation confidence="0.880198">School of Computer Fudan</affiliation>
<address confidence="0.745186">Shanghai, P.R. 12110240030, 12210240054, 12210240059,</address>
<abstract confidence="0.9984487">Explanatory sentences are employed to clarify reasons, details, facts, and so on. High quality online product reviews usually include not only positive or negative opinions, but also a variety of explanations of why these opinions were given. These explanations can help readers get easily comprehensible information of the discussed products and aspects. Moreover, explanatory relations can also benefit sentiment analysis applications. In this work, we focus on the task of identifying subjective text segments and extracting their corresponding explanations from product reviews in discourse level. We propose a novel joint extraction method using firstorder logic to model rich linguistic features and long distance constraints. Experimental results demonstrate the effectiveness of the proposed method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Andrzejewski</author>
<author>Xiaojin Zhu</author>
<author>Mark Craven</author>
<author>Benjamin Recht</author>
</authors>
<title>A framework for incorporating general domain knowledge into latent dirichlet allocation using first-order logic.</title>
<date>2011</date>
<booktitle>In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence - Volume Volume Two, IJCAI’11,</booktitle>
<pages>1171--1177</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="4296" citStr="Andrzejewski et al., 2011" startWordPosition="647" endWordPosition="650">nd Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Song et al., 2012) and demonstrated its advantages. It can easily incorporate rich linguistic features and global constraints by designing various logic formulas, which can also be viewed as templates or rules. Logic formulas are combined in a probabilistic framework to model soft constraints. Hence, the proposed approach can benefit a lot from this framework. To evaluate the proposed method, we crawled a large number of product reviews and constructed a labeled corpus through Amazon’s Mechanical Turk. Two tasks were deployed for labeling the corpus. We compared the proposed method with stat</context>
<context position="31303" citStr="Andrzejewski et al., 2011" startWordPosition="4790" endWordPosition="4793">s. However different to us, they focused on pairwise relationships between opinion expressions. In this paper, we used MLN framework to capture another different discourse-level relation, which exists between subject clauses or subject clause and objective clause. Richardson and Domingos (2006) proposed Markov Logic Networks, which combines firstorder logic and probabilistic graphical models. In recent years, MLN has been adopted for several natural language processing tasks and achieved a certain level of success (Singla and Domingos, 2006; Riedel and Meza-Ruiz, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Jiang et al., 2012; Huang et al., 2012). Singla and Domingos (2006) modeled the entity resolution problem with MLN. They demonstrated the capability of MLN to seamlessly combine a number of previous approaches. Poon and Domingos (2008) proposed to use MLN for joint unsupervised coreference resolution. Yoshikawa et al. (2009) proposed to use Markov logic to incorporate both local features and global constraints that hold between temporal relations. Andrzejewski et al. (2011) introduced a framework for incorporating general domain knowledge, which is represented by First-Order Logic (FOL) rule</context>
</contexts>
<marker>Andrzejewski, Zhu, Craven, Recht, 2011</marker>
<rawString>David Andrzejewski, Xiaojin Zhu, Mark Craven, and Benjamin Recht. 2011. A framework for incorporating general domain knowledge into latent dirichlet allocation using first-order logic. In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence - Volume Volume Two, IJCAI’11, pages 1171–1177. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
<author>Farah Benamara</author>
<author>Yannick Mathieu</author>
</authors>
<title>Appraisal of opinion expressions in discourse. Lingvisticte Investigationes.</title>
<date>2009</date>
<marker>Asher, Benamara, Mathieu, 2009</marker>
<rawString>Nicholas Asher, Farah Benamara, and Yannick Mathieu. 2009. Appraisal of opinion expressions in discourse. Lingvisticte Investigationes.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<contexts>
<context position="11658" citStr="Baccianella et al., 2010" startWordPosition="1828" endWordPosition="1831">.wisc.edu/hazy/tuffy/ 4http://www9-old.in.tum.de/people/jain/mlns/ 5http://alchemy.cs.washington.edu/ 948 Describing the attributes of words Describing the attributes of the clause ci word(i, w) The clause ci has word w. firstWord(i, w) The first word of clause ci is word w. pos(i, w, t) The POS tag of word w is t in clause ci. dep(i, h, m) Word m and h are governor and dependent of a dependency relation in clause ci. Describing the attributes of relations between clause ci and clause cj Table 1: Descriptions of observed predicates. subjLexicon(w) The word w belongs to the subjective lexicon (Baccianella et al., 2010). The word w belongs to the lexicon of explanation relation connectives (Pitler and Nenkova, 2009). relationLexicon(w) clauseDistance(i, j, m) Distance between clause ci and clause cj in clauses is m. sentenceDistance(i, j, n) Distance between clause ci and clause cj in sentences is n. 3.2 Clause Identification We model the clause boundary identification problem through sequence labeling and use Conditional Random Fields (CRFs) to identify clause boundaries. Words and part-of-speech (POS) tags are used as feature sets. Since we do not allow embedded segments, the performance of our method is p</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Carston</author>
</authors>
<title>Conjunction, explanation and relevance. Lingua 90,</title>
<date>1993</date>
<pages>27--48</pages>
<contexts>
<context position="7246" citStr="Carston, 1993" startWordPosition="1114" endWordPosition="1115">asic unit of discourse relations (with a few exceptions) is also taken to be a clause (Rashmi Prasad and Webber, 2008). Figure 1(a) illustrates a sample document. Figure 1(b) is the corresponding output of the given document. In the graph, vertices whose color are black stand for subjective clauses. The other clauses are represented by white vertices. Edges describe the explanatory relationships between them, of which the heads are explanatory clauses. Although the explanatory relation extraction task has been studied from the view of linguistic and discourse representation by existing works (Carston, 1993; Lascarides and Asher, 1993), the automatic extraction task is still an open question. Consider the following examples extracting from online reviews: Example 3: It takes great pictures. Color renditions, skin tones, exposure levels are all first rate. From the example, we can observe that the second sentence explains the first one. However, the second sentence itself also expresses opinion on various opinion targets. In other words, both subjective and objective sentences can be used as explanations. Example 4: When we called their service center they made us wait for them the whole day and </context>
</contexts>
<marker>Carston, 1993</marker>
<rawString>R. Carston. 1993. Conjunction, explanation and relevance. Lingua 90, pages 27–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Libsvm: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Trans. Intell. Syst. Technol.,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="24015" citStr="Chang and Lin, 2011" startWordPosition="3701" endWordPosition="3704"> et al. (2011) proposed to use recursive autoencoders for sentence-level predication of sentiment label distributions. To compare with it, we also reimplement their method without any hand designed lexicon. • PDTB-Rel: For discourse relation extraction, we use “PDTB-Styled End-to-End Discourse Parser” (Lin et al., 2010) to extract discourse level relations as baseline. Since it is a general discourse relations identification algorithms, “Cause”, “Pragmatic Cause”, “Instantiation”, and “Restatement” relation types are treated as explanatory relation in this work. • SVM-Rel: We also use LibSVM (Chang and Lin, 2011) to classify the relations between clauses. Following the configurations reported by Feng and Hirst (2012), we use linear kernel and probability estimation to model it. 4.3 Results Table 3 shows the comparisons of the proposed method with the state-of-the-art systems on subjectivity classification and explanatory relation extraction. From the results, we can observe that recursive autoencoders based subjectivity classification method achieves slightly better performance than our method and conditional random fields based method. The performances of the proposed method are similar as CRFs’. We </context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm: A library for support vector machines. ACM Trans. Intell. Syst. Technol., 2(3):27:1–27:27, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Chen</author>
<author>Sophia Yat Mei Lee</author>
<author>Shoushan Li</author>
<author>Chu-Ren Huang</author>
</authors>
<title>Emotion cause detection with linguistic constructions.</title>
<date>2010</date>
<booktitle>In Proceedings ofColing</booktitle>
<contexts>
<context position="30254" citStr="Chen et al. (2010)" startWordPosition="4637" endWordPosition="4640">t analysis model (Hu and Liu, 2004), which also use flat frames to represent evaluations. Since the cross sentences relations are considered in this work, the discourse-level relation extraction methods are also related to ours. Marcu and Echihabi (2002) proposed to use an unsupervised approach to recognizing discourse relations. Lin et al.(2009) analyzed the impacts of features extracted from contextual information, constituent parse trees, dependency parse trees, and word pairs. Asher et al.(2009) studied discourse segments containing opinion expressions from the perspective of linguistics. Chen et al. (2010) introduced a multi-label model to detect emotion causes. They developed two sets of linguistic features for this task base on linguistic cues. Zirn et al. (2011) proposed to use MLN framework to capture the context information in analysing (sub-)sentences. The most similar work to ours was proposed by Somasundaran et al.(2009). They proposed to use iterative classification algorithm to capture discourselevel associations. However different to us, they focused on pairwise relationships between opinion expressions. In this paper, we used MLN framework to capture another different discourse-leve</context>
</contexts>
<marker>Chen, Lee, Li, Huang, 2010</marker>
<rawString>Ying Chen, Sophia Yat Mei Lee, Shoushan Li, and Chu-Ren Huang. 2010. Emotion cause detection with linguistic constructions. In Proceedings ofColing 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sajib Dasgupta</author>
<author>Vincent Ng</author>
</authors>
<title>Mine the easy, classify the hard: A semi-supervised approach to automatic sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of ACLIJCNLP</booktitle>
<marker>Dasgupta, Ng, 2009</marker>
<rawString>Sajib Dasgupta and Vincent Ng. Mine the easy, classify the hard: A semi-supervised approach to automatic sentiment classification. In Proceedings of ACLIJCNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Dragut</author>
<author>Hong Wang</author>
<author>Clement Yu</author>
<author>Prasad Sistla</author>
<author>Weiyi Meng</author>
</authors>
<title>Polarity consistency checking for sentiment dictionaries.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>997--1005</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="3251" citStr="Dragut et al., 2012" startWordPosition="491" endWordPosition="494">f our knowledge, there is no existing work that deals with explanation extraction for opinions in discourse level. We think that if explanatory relations can be automatically identified from reviews, sentiment analysis applications may benefit from it. Existing opinion mining approaches mainly focus on subjective text. They try to determine the subjectivity and polarity of fragments of documents (e.g. a paragraph, a sentence, a phrase and a word) (Pang et al., 2002; Riloff et al., 2003; Takamura et al., 2005; Mihalcea et al., 2007; Dasgupta and Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subj</context>
</contexts>
<marker>Dragut, Wang, Yu, Sistla, Meng, 2012</marker>
<rawString>Eduard Dragut, Hong Wang, Clement Yu, Prasad Sistla, and Weiyi Meng. 2012. Polarity consistency checking for sentiment dictionaries. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 997–1005, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Wei Feng</author>
<author>Graeme Hirst</author>
</authors>
<title>Textlevel discourse parsing with rich linguistic features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>60--68</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="24121" citStr="Feng and Hirst (2012)" startWordPosition="3716" endWordPosition="3719">istributions. To compare with it, we also reimplement their method without any hand designed lexicon. • PDTB-Rel: For discourse relation extraction, we use “PDTB-Styled End-to-End Discourse Parser” (Lin et al., 2010) to extract discourse level relations as baseline. Since it is a general discourse relations identification algorithms, “Cause”, “Pragmatic Cause”, “Instantiation”, and “Restatement” relation types are treated as explanatory relation in this work. • SVM-Rel: We also use LibSVM (Chang and Lin, 2011) to classify the relations between clauses. Following the configurations reported by Feng and Hirst (2012), we use linear kernel and probability estimation to model it. 4.3 Results Table 3 shows the comparisons of the proposed method with the state-of-the-art systems on subjectivity classification and explanatory relation extraction. From the results, we can observe that recursive autoencoders based subjectivity classification method achieves slightly better performance than our method and conditional random fields based method. The performances of the proposed method are similar as CRFs’. We think that the main reason is that only lexical features are used in MLN models for subjective classificat</context>
</contexts>
<marker>Feng, Hirst, 2012</marker>
<rawString>Vanessa Wei Feng and Graeme Hirst. 2012. Textlevel discourse parsing with rich linguistic features. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 60–68, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Hassan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Identifying text polarity using random walks.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL 2010,</booktitle>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="3210" citStr="Hassan and Radev, 2010" startWordPosition="483" endWordPosition="486">provide valuable information, to the best of our knowledge, there is no existing work that deals with explanation extraction for opinions in discourse level. We think that if explanatory relations can be automatically identified from reviews, sentiment analysis applications may benefit from it. Existing opinion mining approaches mainly focus on subjective text. They try to determine the subjectivity and polarity of fragments of documents (e.g. a paragraph, a sentence, a phrase and a word) (Pang et al., 2002; Riloff et al., 2003; Takamura et al., 2005; Mihalcea et al., 2007; Dasgupta and Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 201</context>
</contexts>
<marker>Hassan, Radev, 2010</marker>
<rawString>Ahmed Hassan and Dragomir R. Radev. 2010. Identifying text polarity using random walks. In Proceedings of ACL 2010, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of SIGKDD</booktitle>
<contexts>
<context position="28110" citStr="Hu and Liu (2004)" startWordPosition="4315" endWordPosition="4318">vity classification has recently received considerable attention from both the industry and researchers. A variety of approaches and methods have been proposed for this task from different aspects. Among them, a number of approaches focus on classifying sentiments of text in different levels (e.g. words (Kim and Hovy, 2004), phrases (Wilson et al., 2005), sentences (Zhao et al., 2008), documents (Pang et al., 2002) and so on.), and detecting the overall polarity of them. Another research direction tries to convert the sentiment analysis task into entity identification and relation extraction. Hu and Liu (2004) proposed to use a set of methods to produce feature-based summary of a large number of customer reviews. Kobayashi et al. (2007) assumed that evaluative opinions could be structured as a frame which is composed by opinion holder, subject, aspect, and evaluation. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which used both contextual and statistical clues. Analysis of some special types of sentences were also introduced in recent years. Jindal and Liu (2006) studied the problem of identifying comparative sentences. They analyzed</context>
<context position="29671" citStr="Hu and Liu, 2004" startWordPosition="4554" endWordPosition="4557">9.9% 56.2% 72.9% 63.5% −subjLexicon(w) 76.6% 70.4% 73.4 % 52.3% 68.6% 59.4% −relationLexicon(w) 78.2% 79.4% 78.8% 53.6% 70.8% 61.0% −word(i, w) 52.8% 49.6% 51.2 % 36.4% 52.1% 42.9% −firstWord(i, w) 76.3% 80.1% 78.2% 56.9% 69.8% 62.7% −pos(i, w, t) 72.6% 76.8% 74.6 % 52.4% 60.2% 56.0% −dep(i, h, m) 57.6% 70.6% 63.4% 41.2% 56.8% 47.8% −clauseDistance(i, j, m) 78.9% 80.2% 79.5% 52.6% 70.6% 60.3% −sentenceDistance(i, j, n) 78.6% 80.3% 79.4% 52.4% 70.8% 60.2% Table 4: Performance comparisons of different observed predicates method to do it. They followed the feature-based sentiment analysis model (Hu and Liu, 2004), which also use flat frames to represent evaluations. Since the cross sentences relations are considered in this work, the discourse-level relation extraction methods are also related to ours. Marcu and Echihabi (2002) proposed to use an unsupervised approach to recognizing discourse relations. Lin et al.(2009) analyzed the impacts of features extracted from contextual information, constituent parse trees, dependency parse trees, and word pairs. Asher et al.(2009) studied discourse segments containing opinion expressions from the perspective of linguistics. Chen et al. (2010) introduced a mul</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of SIGKDD 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minlie Huang</author>
<author>Xing Shi</author>
<author>Feng Jin</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Using first-order logic to compress sentences.</title>
<date>2012</date>
<booktitle>In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="31344" citStr="Huang et al., 2012" startWordPosition="4798" endWordPosition="4801">rwise relationships between opinion expressions. In this paper, we used MLN framework to capture another different discourse-level relation, which exists between subject clauses or subject clause and objective clause. Richardson and Domingos (2006) proposed Markov Logic Networks, which combines firstorder logic and probabilistic graphical models. In recent years, MLN has been adopted for several natural language processing tasks and achieved a certain level of success (Singla and Domingos, 2006; Riedel and Meza-Ruiz, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Jiang et al., 2012; Huang et al., 2012). Singla and Domingos (2006) modeled the entity resolution problem with MLN. They demonstrated the capability of MLN to seamlessly combine a number of previous approaches. Poon and Domingos (2008) proposed to use MLN for joint unsupervised coreference resolution. Yoshikawa et al. (2009) proposed to use Markov logic to incorporate both local features and global constraints that hold between temporal relations. Andrzejewski et al. (2011) introduced a framework for incorporating general domain knowledge, which is represented by First-Order Logic (FOL) rules, into LDA inference to produce topics s</context>
</contexts>
<marker>Huang, Shi, Jin, Zhu, 2012</marker>
<rawString>Minlie Huang, Xing Shi, Feng Jin, and Xiaoyan Zhu. 2012. Using first-order logic to compress sentences. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shangpu Jiang</author>
<author>D Lowd</author>
<author>Dejing Dou</author>
</authors>
<title>Learning to refine an automatically extracted knowledge base using markov logic.</title>
<date>2012</date>
<booktitle>In Data Mining (ICDM), 2012 IEEE 12th International Conference on,</booktitle>
<pages>912--917</pages>
<contexts>
<context position="31323" citStr="Jiang et al., 2012" startWordPosition="4794" endWordPosition="4797"> they focused on pairwise relationships between opinion expressions. In this paper, we used MLN framework to capture another different discourse-level relation, which exists between subject clauses or subject clause and objective clause. Richardson and Domingos (2006) proposed Markov Logic Networks, which combines firstorder logic and probabilistic graphical models. In recent years, MLN has been adopted for several natural language processing tasks and achieved a certain level of success (Singla and Domingos, 2006; Riedel and Meza-Ruiz, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Jiang et al., 2012; Huang et al., 2012). Singla and Domingos (2006) modeled the entity resolution problem with MLN. They demonstrated the capability of MLN to seamlessly combine a number of previous approaches. Poon and Domingos (2008) proposed to use MLN for joint unsupervised coreference resolution. Yoshikawa et al. (2009) proposed to use Markov logic to incorporate both local features and global constraints that hold between temporal relations. Andrzejewski et al. (2011) introduced a framework for incorporating general domain knowledge, which is represented by First-Order Logic (FOL) rules, into LDA inferenc</context>
</contexts>
<marker>Jiang, Lowd, Dou, 2012</marker>
<rawString>Shangpu Jiang, D. Lowd, and Dejing Dou. 2012. Learning to refine an automatically extracted knowledge base using markov logic. In Data Mining (ICDM), 2012 IEEE 12th International Conference on, pages 912–917.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
</authors>
<title>Identifying comparative sentences in text documents.</title>
<date>2006</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<contexts>
<context position="28638" citStr="Jindal and Liu (2006)" startWordPosition="4398" endWordPosition="4402">sentiment analysis task into entity identification and relation extraction. Hu and Liu (2004) proposed to use a set of methods to produce feature-based summary of a large number of customer reviews. Kobayashi et al. (2007) assumed that evaluative opinions could be structured as a frame which is composed by opinion holder, subject, aspect, and evaluation. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which used both contextual and statistical clues. Analysis of some special types of sentences were also introduced in recent years. Jindal and Liu (2006) studied the problem of identifying comparative sentences. They analyzed different types of comparative sentences and proposed learning approaches to identify them. Conditional sentences were studied by Narayanan et al (2009). They analyzed the conditional sentences in both linguistic and computitional perspectives and used learning 953 Subjective Classification Relation Extraction P R F1 P R F1 MLN 79.2% 80.6% 79.9% 56.2% 72.9% 63.5% −subjLexicon(w) 76.6% 70.4% 73.4 % 52.3% 68.6% 59.4% −relationLexicon(w) 78.2% 79.4% 78.8% 53.6% 70.8% 61.0% −word(i, w) 52.8% 49.6% 51.2 % 36.4% 52.1% 42.9% −fi</context>
</contexts>
<marker>Jindal, Liu, 2006</marker>
<rawString>Nitin Jindal and Bing Liu. 2006. Identifying comparative sentences in text documents. In Proceedings of SIGIR 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="27818" citStr="Kim and Hovy, 2004" startWordPosition="4270" endWordPosition="4273">cts of clause distance and sentence distance are not as significant as the other features. 5 Related Work Our work relates to three research areas: sentiment analysis/opinion mining, discourse-level relation extraction, and Markov logic networks. Along with the increasing requirement, subjectivity classification has recently received considerable attention from both the industry and researchers. A variety of approaches and methods have been proposed for this task from different aspects. Among them, a number of approaches focus on classifying sentiments of text in different levels (e.g. words (Kim and Hovy, 2004), phrases (Wilson et al., 2005), sentences (Zhao et al., 2008), documents (Pang et al., 2002) and so on.), and detecting the overall polarity of them. Another research direction tries to convert the sentiment analysis task into entity identification and relation extraction. Hu and Liu (2004) proposed to use a set of methods to produce feature-based summary of a large number of customer reviews. Kobayashi et al. (2007) assumed that evaluative opinions could be structured as a frame which is composed by opinion holder, subject, aspect, and evaluation. They converted the task to two kinds of rela</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of COLING 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of NIPS</booktitle>
<contexts>
<context position="22141" citStr="Klein and Manning, 2003" startWordPosition="3416" endWordPosition="3419">t of facts which clarifies the causes, reason, and consequences of the opinion given in the opinion sentence. click &amp;quot;yes&amp;quot; if there is an explanation relation between them, &amp;quot;no&amp;quot; otherwise. The battery life is something I come to expect from this line of camera. I can leave the camera on for better than 8 hours shooting ()YES ()NO The batery life is something I come to expect from this line of camera. and I have the camera set to shut off the sensor after about 30 seconds ()YES ()NO Figure 2: Screenshots of the two tasks on Amazon Mechanical Turk. 4.2 Experiments Configurations Stanford parser (Klein and Manning, 2003) is used for extracting features from dependency parse trees. For resolving Markov logic network, we use the toolkit thebeast 7. The detailed setting of thebeast engine is as follows: The inference algorithm is the MAP inference with a cutting plane approach. For parameter learning, the weights for formulas are updated by an online learning algorithm with MIRA update rule. All the initial weights are set to zeros. The number of iterations is set to 10 epochs. Evaluation metrics used for subjectivity classification and relation extraction throughout the experiments include: Precision, Recall, a</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In Proceedings of NIPS 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nozomi Kobayashi</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Extracting aspect-evaluation and aspect-of relations in opinion mining.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL</booktitle>
<contexts>
<context position="3408" citStr="Kobayashi et al., 2007" startWordPosition="512" endWordPosition="515">an be automatically identified from reviews, sentiment analysis applications may benefit from it. Existing opinion mining approaches mainly focus on subjective text. They try to determine the subjectivity and polarity of fragments of documents (e.g. a paragraph, a sentence, a phrase and a word) (Pang et al., 2002; Riloff et al., 2003; Takamura et al., 2005; Mihalcea et al., 2007; Dasgupta and Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Ric</context>
<context position="28239" citStr="Kobayashi et al. (2007)" startWordPosition="4337" endWordPosition="4340">aches and methods have been proposed for this task from different aspects. Among them, a number of approaches focus on classifying sentiments of text in different levels (e.g. words (Kim and Hovy, 2004), phrases (Wilson et al., 2005), sentences (Zhao et al., 2008), documents (Pang et al., 2002) and so on.), and detecting the overall polarity of them. Another research direction tries to convert the sentiment analysis task into entity identification and relation extraction. Hu and Liu (2004) proposed to use a set of methods to produce feature-based summary of a large number of customer reviews. Kobayashi et al. (2007) assumed that evaluative opinions could be structured as a frame which is composed by opinion holder, subject, aspect, and evaluation. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which used both contextual and statistical clues. Analysis of some special types of sentences were also introduced in recent years. Jindal and Liu (2006) studied the problem of identifying comparative sentences. They analyzed different types of comparative sentences and proposed learning approaches to identify them. Conditional sentences were studied b</context>
</contexts>
<marker>Kobayashi, Inui, Matsumoto, 2007</marker>
<rawString>Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto. 2007. Extracting aspect-evaluation and aspect-of relations in opinion mining. In Proceedings of EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kroeger</author>
</authors>
<title>Analyzing Grammar: An Introduction.</title>
<date>2005</date>
<location>Cambridge.</location>
<contexts>
<context position="6470" citStr="Kroeger, 2005" startWordPosition="991" endWordPosition="992"> relationships between them. In the graph, vertices represent clauses, whose categories are specified by the vertex attributes. Directed edges describe the explanatory relationships between them, of which the heads are explanatory clauses. If clause ca describes a set of facts which clarify the causes, context, situation, or consequences of another clause cb, ca −→ cb is used to indicate that clause ca explains cb. Adopting clause unit-based definition is based on the following reasons: 1) clause is normally considered as the smallest grammatical unit which can express a complete proposition (Kroeger, 2005); 2) from analyzing online reviews, we observe that a clause can express a complete opinion about one aspect in most of cases; 3) in Penn Discourse Treebank, the basic unit of discourse relations (with a few exceptions) is also taken to be a clause (Rashmi Prasad and Webber, 2008). Figure 1(a) illustrates a sample document. Figure 1(b) is the corresponding output of the given document. In the graph, vertices whose color are black stand for subjective clauses. The other clauses are represented by white vertices. Edges describe the explanatory relationships between them, of which the heads are e</context>
</contexts>
<marker>Kroeger, 2005</marker>
<rawString>Paul Kroeger. 2005. Analyzing Grammar: An Introduction. Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lascarides</author>
<author>Nicholas Asher</author>
</authors>
<title>Temporal interpretation, discourse relations, and common sense entailment.</title>
<date>1993</date>
<journal>Linguistics and Philosophy,</journal>
<volume>16</volume>
<issue>5</issue>
<pages>493</pages>
<contexts>
<context position="7275" citStr="Lascarides and Asher, 1993" startWordPosition="1116" endWordPosition="1119">scourse relations (with a few exceptions) is also taken to be a clause (Rashmi Prasad and Webber, 2008). Figure 1(a) illustrates a sample document. Figure 1(b) is the corresponding output of the given document. In the graph, vertices whose color are black stand for subjective clauses. The other clauses are represented by white vertices. Edges describe the explanatory relationships between them, of which the heads are explanatory clauses. Although the explanatory relation extraction task has been studied from the view of linguistic and discourse representation by existing works (Carston, 1993; Lascarides and Asher, 1993), the automatic extraction task is still an open question. Consider the following examples extracting from online reviews: Example 3: It takes great pictures. Color renditions, skin tones, exposure levels are all first rate. From the example, we can observe that the second sentence explains the first one. However, the second sentence itself also expresses opinion on various opinion targets. In other words, both subjective and objective sentences can be used as explanations. Example 4: When we called their service center they made us wait for them the whole day and no one turned up. This level </context>
</contexts>
<marker>Lascarides, Asher, 1993</marker>
<rawString>Alex Lascarides and Nicholas Asher. 1993. Temporal interpretation, discourse relations, and common sense entailment. Linguistics and Philosophy, 16(5):437– 493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Min-Yen Kan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Recognizing implicit discourse relations in the penn discourse treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the penn discourse treebank. In Proceedings of EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>A pdtb-styled end-to-end discourse parser.</title>
<date>2010</date>
<location>CoRR, abs/1011.0835.</location>
<contexts>
<context position="23716" citStr="Lin et al., 2010" startWordPosition="3658" endWordPosition="3661">Zhao et al. (2008), which regard the subjectivity of all clauses throughout a paragraph as a sequential flow of sentiments and use CRFs to model it. The feature sets are similar as the local formulas for MLN including words, POS tags, dependency relations, and opinion lexicon. • RAE-Subj: Socher et al. (2011) proposed to use recursive autoencoders for sentence-level predication of sentiment label distributions. To compare with it, we also reimplement their method without any hand designed lexicon. • PDTB-Rel: For discourse relation extraction, we use “PDTB-Styled End-to-End Discourse Parser” (Lin et al., 2010) to extract discourse level relations as baseline. Since it is a general discourse relations identification algorithms, “Cause”, “Pragmatic Cause”, “Instantiation”, and “Restatement” relation types are treated as explanatory relation in this work. • SVM-Rel: We also use LibSVM (Chang and Lin, 2011) to classify the relations between clauses. Following the configurations reported by Feng and Hirst (2012), we use linear kernel and probability estimation to model it. 4.3 Results Table 3 shows the comparisons of the proposed method with the state-of-the-art systems on subjectivity classification an</context>
</contexts>
<marker>Lin, Ng, Kan, 2010</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A pdtb-styled end-to-end discourse parser. CoRR, abs/1011.0835.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining.</title>
<date>2012</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="3812" citStr="Liu, 2012" startWordPosition="574" endWordPosition="575">ev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Song et al., 2012) and demonstrated its advantages. It can easily incorporate rich linguistic features and global </context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>An unsupervised approach to recognizing discourse relations.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>368--375</pages>
<contexts>
<context position="29890" citStr="Marcu and Echihabi (2002)" startWordPosition="4587" endWordPosition="4590">8.2% 56.9% 69.8% 62.7% −pos(i, w, t) 72.6% 76.8% 74.6 % 52.4% 60.2% 56.0% −dep(i, h, m) 57.6% 70.6% 63.4% 41.2% 56.8% 47.8% −clauseDistance(i, j, m) 78.9% 80.2% 79.5% 52.6% 70.6% 60.3% −sentenceDistance(i, j, n) 78.6% 80.3% 79.4% 52.4% 70.8% 60.2% Table 4: Performance comparisons of different observed predicates method to do it. They followed the feature-based sentiment analysis model (Hu and Liu, 2004), which also use flat frames to represent evaluations. Since the cross sentences relations are considered in this work, the discourse-level relation extraction methods are also related to ours. Marcu and Echihabi (2002) proposed to use an unsupervised approach to recognizing discourse relations. Lin et al.(2009) analyzed the impacts of features extracted from contextual information, constituent parse trees, dependency parse trees, and word pairs. Asher et al.(2009) studied discourse segments containing opinion expressions from the perspective of linguistics. Chen et al. (2010) introduced a multi-label model to detect emotion causes. They developed two sets of linguistic features for this task base on linguistic cues. Zirn et al. (2011) proposed to use MLN framework to capture the context information in analy</context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>Daniel Marcu and Abdessamad Echihabi. 2002. An unsupervised approach to recognizing discourse relations. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 368–375.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Xinfan Meng</author>
</authors>
<title>Furu Wei,</title>
<location>Xiaohua Liu, Ming Zhou,</location>
<marker>Meng, </marker>
<rawString>Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ge Xu</author>
<author>Houfeng Wang</author>
</authors>
<title>Crosslingual mixture model for sentiment classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>572--581</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<marker>Xu, Wang, 2012</marker>
<rawString>Ge Xu, and Houfeng Wang. 2012. Crosslingual mixture model for sentiment classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 572–581, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carmen Banea</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning multilingual subjective language via crosslingual projections.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="3167" citStr="Mihalcea et al., 2007" startWordPosition="475" endWordPosition="478">the camera screen. Although, explanations provide valuable information, to the best of our knowledge, there is no existing work that deals with explanation extraction for opinions in discourse level. We think that if explanatory relations can be automatically identified from reviews, sentiment analysis applications may benefit from it. Existing opinion mining approaches mainly focus on subjective text. They try to determine the subjectivity and polarity of fragments of documents (e.g. a paragraph, a sentence, a phrase and a word) (Pang et al., 2002; Riloff et al., 2003; Takamura et al., 2005; Mihalcea et al., 2007; Dasgupta and Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be fou</context>
</contexts>
<marker>Mihalcea, Banea, Wiebe, 2007</marker>
<rawString>Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2007. Learning multilingual subjective language via crosslingual projections. In Proceedings of ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramanathan Narayanan</author>
<author>Bing Liu</author>
<author>Alok Choudhary</author>
</authors>
<title>Sentiment analysis of conditional sentences.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="28863" citStr="Narayanan et al (2009)" startWordPosition="4430" endWordPosition="4433">ssumed that evaluative opinions could be structured as a frame which is composed by opinion holder, subject, aspect, and evaluation. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which used both contextual and statistical clues. Analysis of some special types of sentences were also introduced in recent years. Jindal and Liu (2006) studied the problem of identifying comparative sentences. They analyzed different types of comparative sentences and proposed learning approaches to identify them. Conditional sentences were studied by Narayanan et al (2009). They analyzed the conditional sentences in both linguistic and computitional perspectives and used learning 953 Subjective Classification Relation Extraction P R F1 P R F1 MLN 79.2% 80.6% 79.9% 56.2% 72.9% 63.5% −subjLexicon(w) 76.6% 70.4% 73.4 % 52.3% 68.6% 59.4% −relationLexicon(w) 78.2% 79.4% 78.8% 53.6% 70.8% 61.0% −word(i, w) 52.8% 49.6% 51.2 % 36.4% 52.1% 42.9% −firstWord(i, w) 76.3% 80.1% 78.2% 56.9% 69.8% 62.7% −pos(i, w, t) 72.6% 76.8% 74.6 % 52.4% 60.2% 56.0% −dep(i, h, m) 57.6% 70.6% 63.4% 41.2% 56.8% 47.8% −clauseDistance(i, j, m) 78.9% 80.2% 79.5% 52.6% 70.6% 60.3% −sentenceDist</context>
</contexts>
<marker>Narayanan, Liu, Choudhary, 2009</marker>
<rawString>Ramanathan Narayanan, Bing Liu, and Alok Choudhary. 2009. Sentiment analysis of conditional sentences. In Proceedings of EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="3800" citStr="Pang and Lee, 2008" startWordPosition="570" endWordPosition="573">Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Song et al., 2012) and demonstrated its advantages. It can easily incorporate rich linguistic features</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2:1–135, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="3100" citStr="Pang et al., 2002" startWordPosition="463" endWordPosition="466">ail descriptions are used to explain the reflection problem of the camera screen. Although, explanations provide valuable information, to the best of our knowledge, there is no existing work that deals with explanation extraction for opinions in discourse level. We think that if explanatory relations can be automatically identified from reviews, sentiment analysis applications may benefit from it. Existing opinion mining approaches mainly focus on subjective text. They try to determine the subjectivity and polarity of fragments of documents (e.g. a paragraph, a sentence, a phrase and a word) (Pang et al., 2002; Riloff et al., 2003; Takamura et al., 2005; Mihalcea et al., 2007; Dasgupta and Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major resea</context>
<context position="27911" citStr="Pang et al., 2002" startWordPosition="4285" endWordPosition="4288">elated Work Our work relates to three research areas: sentiment analysis/opinion mining, discourse-level relation extraction, and Markov logic networks. Along with the increasing requirement, subjectivity classification has recently received considerable attention from both the industry and researchers. A variety of approaches and methods have been proposed for this task from different aspects. Among them, a number of approaches focus on classifying sentiments of text in different levels (e.g. words (Kim and Hovy, 2004), phrases (Wilson et al., 2005), sentences (Zhao et al., 2008), documents (Pang et al., 2002) and so on.), and detecting the overall polarity of them. Another research direction tries to convert the sentiment analysis task into entity identification and relation extraction. Hu and Liu (2004) proposed to use a set of methods to produce feature-based summary of a large number of customer reviews. Kobayashi et al. (2007) assumed that evaluative opinions could be structured as a frame which is composed by opinion holder, subject, aspect, and evaluation. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which used both contextual</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Using syntax to disambiguate explicit discourse connectives in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP</booktitle>
<contexts>
<context position="11756" citStr="Pitler and Nenkova, 2009" startWordPosition="1843" endWordPosition="1846">du/ 948 Describing the attributes of words Describing the attributes of the clause ci word(i, w) The clause ci has word w. firstWord(i, w) The first word of clause ci is word w. pos(i, w, t) The POS tag of word w is t in clause ci. dep(i, h, m) Word m and h are governor and dependent of a dependency relation in clause ci. Describing the attributes of relations between clause ci and clause cj Table 1: Descriptions of observed predicates. subjLexicon(w) The word w belongs to the subjective lexicon (Baccianella et al., 2010). The word w belongs to the lexicon of explanation relation connectives (Pitler and Nenkova, 2009). relationLexicon(w) clauseDistance(i, j, m) Distance between clause ci and clause cj in clauses is m. sentenceDistance(i, j, n) Distance between clause ci and clause cj in sentences is n. 3.2 Clause Identification We model the clause boundary identification problem through sequence labeling and use Conditional Random Fields (CRFs) to identify clause boundaries. Words and part-of-speech (POS) tags are used as feature sets. Since we do not allow embedded segments, the performance of our method is promising, which achieves the F1 score of 92.8%. The result is comparable with the best results obt</context>
<context position="17332" citStr="Pitler and Nenkova, 2009" startWordPosition="2620" endWordPosition="2623">scriptions of local formulas. 950 also construct local formulas based on predicates extracted from dependency trees of clauses. For explanatory relation extraction, we firstly use formulas to capture lexical and syntactic information from both of the clauses. Since distances between clauses are helpful in determining the relation, we incorporate two kinds of distance features with lexical and syntactic predicates. Connective words such as for example, since, explicitly signal the presence of the explanation relation. Although some connective words are ambiguous in terms of relation they mark (Pitler and Nenkova, 2009), they may still be useful for explanation relation extraction. Hence, we construct local formulas with relation lexicon and other predicates. 3.3.2 Global Formulas Local formulas are designed to deal with subjective classification of a single clause or relation determination of a single pair of clauses. Global formulas are designed to handle global constraints of multiple clauses. From the definition of explanatory relation and corpus statistics, we observe the following properties: Property 1: One clause can only serve as the explanation of one subjective clause. Property 2: Explanatory clau</context>
</contexts>
<marker>Pitler, Nenkova, 2009</marker>
<rawString>Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In Proceedings of the ACL-IJCNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint unsupervised coreference resolution with markov logic.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>650--659</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4245" citStr="Poon and Domingos, 2008" startWordPosition="639" endWordPosition="642">ational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Song et al., 2012) and demonstrated its advantages. It can easily incorporate rich linguistic features and global constraints by designing various logic formulas, which can also be viewed as templates or rules. Logic formulas are combined in a probabilistic framework to model soft constraints. Hence, the proposed approach can benefit a lot from this framework. To evaluate the proposed method, we crawled a large number of product reviews and constructed a labeled corpus through Amazon’s Mechanical Turk. Two tasks were deployed for labeling th</context>
<context position="31540" citStr="Poon and Domingos (2008)" startWordPosition="4827" endWordPosition="4830">ause and objective clause. Richardson and Domingos (2006) proposed Markov Logic Networks, which combines firstorder logic and probabilistic graphical models. In recent years, MLN has been adopted for several natural language processing tasks and achieved a certain level of success (Singla and Domingos, 2006; Riedel and Meza-Ruiz, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Jiang et al., 2012; Huang et al., 2012). Singla and Domingos (2006) modeled the entity resolution problem with MLN. They demonstrated the capability of MLN to seamlessly combine a number of previous approaches. Poon and Domingos (2008) proposed to use MLN for joint unsupervised coreference resolution. Yoshikawa et al. (2009) proposed to use Markov logic to incorporate both local features and global constraints that hold between temporal relations. Andrzejewski et al. (2011) introduced a framework for incorporating general domain knowledge, which is represented by First-Order Logic (FOL) rules, into LDA inference to produce topics shaped by both the data and the rules. 6 Conclusions In this paper, we propose to use Markov logic networks to identify subjective text segments and extract their corresponding explanations in disc</context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2008. Joint unsupervised coreference resolution with markov logic. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 650–659, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Lee</author>
</authors>
<title>Eleni Miltsakaki Livio Robaldo Aravind Joshi Rashmi Prasad, Nikhil Dinesh and Bonnie Webber.</title>
<date>2008</date>
<booktitle>In Bente Maegaard Joseph Mariani Jan Odjik Stelios Piperidis Daniel Tapias Nicoletta Calzolari (Conference Chair), Khalid Choukri, editor, Proceedings of LREC’08,</booktitle>
<location>Marrakech, Morocco,</location>
<note>http://www.lrecconf.org/proceedings/lrec2008/.</note>
<contexts>
<context position="3800" citStr="Lee, 2008" startWordPosition="572" endWordPosition="573">san and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Song et al., 2012) and demonstrated its advantages. It can easily incorporate rich linguistic features</context>
</contexts>
<marker>Lee, 2008</marker>
<rawString>Alan Lee Eleni Miltsakaki Livio Robaldo Aravind Joshi Rashmi Prasad, Nikhil Dinesh and Bonnie Webber. 2008. The penn discourse treebank 2.0. In Bente Maegaard Joseph Mariani Jan Odjik Stelios Piperidis Daniel Tapias Nicoletta Calzolari (Conference Chair), Khalid Choukri, editor, Proceedings of LREC’08, Marrakech, Morocco, may. http://www.lrecconf.org/proceedings/lrec2008/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>62--1</pages>
<contexts>
<context position="4035" citStr="Richardson and Domingos, 2006" startWordPosition="608" endWordPosition="611">007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Song et al., 2012) and demonstrated its advantages. It can easily incorporate rich linguistic features and global constraints by designing various logic formulas, which can also be viewed as templates or rules. Logic formulas are combined in a probabilistic framework to model soft constraints. Hence, the proposed approach can benefit a</context>
<context position="10261" citStr="Richardson and Domingos, 2006" startWordPosition="1602" endWordPosition="1605">ing in this work. 3.1 Markov Logic Networks A MLN consists of a set of logic formulas that describe first-order knowledge base. Each formula consists of a set of first-order predicates, logical connectors and variables. Different with first-order logic, these hard logic formulas are softened and can be violated with some penalty (the weight of formula) in MLN. We use M to represent a MLN and {(ϕi, wi)} to represent formula ϕi and its weight wi. These weighted formulas define a probability distribution over sets of possible worlds. Let y denote a possible world, the p(y) is defined as follows (Richardson and Domingos, 2006): p(y) = Z exp wi ∑f ϕi c (y) ((ϕi,wi)EM cECnϕi where each c is a binding of free variable in ϕi to constraints; fϕi c (y) is a binary feature function that returns 1 if the true value is obtained in the ground formula we get by replacing the free variables in ϕi with the constants in c under the given possible world y, and 0 otherwise; Cnϕi is all possible bindings of variables to constants, and Z is a normalization constant. Many methods have been proposed to learn the weights of MLN using both generative and discriminative approaches (Richardson and Domingos, 2006; Singla and Domingos, 2006</context>
<context position="30973" citStr="Richardson and Domingos (2006)" startWordPosition="4741" endWordPosition="4744">uistic features for this task base on linguistic cues. Zirn et al. (2011) proposed to use MLN framework to capture the context information in analysing (sub-)sentences. The most similar work to ours was proposed by Somasundaran et al.(2009). They proposed to use iterative classification algorithm to capture discourselevel associations. However different to us, they focused on pairwise relationships between opinion expressions. In this paper, we used MLN framework to capture another different discourse-level relation, which exists between subject clauses or subject clause and objective clause. Richardson and Domingos (2006) proposed Markov Logic Networks, which combines firstorder logic and probabilistic graphical models. In recent years, MLN has been adopted for several natural language processing tasks and achieved a certain level of success (Singla and Domingos, 2006; Riedel and Meza-Ruiz, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Jiang et al., 2012; Huang et al., 2012). Singla and Domingos (2006) modeled the entity resolution problem with MLN. They demonstrated the capability of MLN to seamlessly combine a number of previous approaches. Poon and Domingos (2008) proposed to use MLN for joint un</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Matthew Richardson and Pedro Domingos. 2006. Markov logic networks. Machine Learning, 62(1-2):107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Ivan Meza-Ruiz</author>
</authors>
<title>Collective semantic role labelling with markov logic.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08,</booktitle>
<contexts>
<context position="31252" citStr="Riedel and Meza-Ruiz, 2008" startWordPosition="4782" endWordPosition="4785">tion algorithm to capture discourselevel associations. However different to us, they focused on pairwise relationships between opinion expressions. In this paper, we used MLN framework to capture another different discourse-level relation, which exists between subject clauses or subject clause and objective clause. Richardson and Domingos (2006) proposed Markov Logic Networks, which combines firstorder logic and probabilistic graphical models. In recent years, MLN has been adopted for several natural language processing tasks and achieved a certain level of success (Singla and Domingos, 2006; Riedel and Meza-Ruiz, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Jiang et al., 2012; Huang et al., 2012). Singla and Domingos (2006) modeled the entity resolution problem with MLN. They demonstrated the capability of MLN to seamlessly combine a number of previous approaches. Poon and Domingos (2008) proposed to use MLN for joint unsupervised coreference resolution. Yoshikawa et al. (2009) proposed to use Markov logic to incorporate both local features and global constraints that hold between temporal relations. Andrzejewski et al. (2011) introduced a framework for incorporating general domain knowledge, w</context>
</contexts>
<marker>Riedel, Meza-Ruiz, 2008</marker>
<rawString>Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collective semantic role labelling with markov logic. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08,</rawString>
</citation>
<citation valid="false">
<pages>193--197</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker></marker>
<rawString>pages 193–197, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
</authors>
<title>Learning subjective nouns using extraction pattern bootstrapping.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<contexts>
<context position="3121" citStr="Riloff et al., 2003" startWordPosition="467" endWordPosition="470">e used to explain the reflection problem of the camera screen. Although, explanations provide valuable information, to the best of our knowledge, there is no existing work that deals with explanation extraction for opinions in discourse level. We think that if explanatory relations can be automatically identified from reviews, sentiment analysis applications may benefit from it. Existing opinion mining approaches mainly focus on subjective text. They try to determine the subjectivity and polarity of fragments of documents (e.g. a paragraph, a sentence, a phrase and a word) (Pang et al., 2002; Riloff et al., 2003; Takamura et al., 2005; Mihalcea et al., 2007; Dasgupta and Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and ch</context>
</contexts>
<marker>Riloff, Wiebe, Wilson, 2003</marker>
<rawString>Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003. Learning subjective nouns using extraction pattern bootstrapping. In Proceedings of HLT-NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Singla</author>
<author>P Domingos</author>
</authors>
<title>Entity resolution with markov logic.</title>
<date>2006</date>
<booktitle>In Data Mining,</booktitle>
<pages>572--582</pages>
<contexts>
<context position="4220" citStr="Singla and Domingos, 2006" startWordPosition="635" endWordPosition="638">2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Song et al., 2012) and demonstrated its advantages. It can easily incorporate rich linguistic features and global constraints by designing various logic formulas, which can also be viewed as templates or rules. Logic formulas are combined in a probabilistic framework to model soft constraints. Hence, the proposed approach can benefit a lot from this framework. To evaluate the proposed method, we crawled a large number of product reviews and constructed a labeled corpus through Amazon’s Mechanical Turk. Two tasks were</context>
<context position="10862" citStr="Singla and Domingos, 2006" startWordPosition="1712" endWordPosition="1715">rdson and Domingos, 2006): p(y) = Z exp wi ∑f ϕi c (y) ((ϕi,wi)EM cECnϕi where each c is a binding of free variable in ϕi to constraints; fϕi c (y) is a binary feature function that returns 1 if the true value is obtained in the ground formula we get by replacing the free variables in ϕi with the constants in c under the given possible world y, and 0 otherwise; Cnϕi is all possible bindings of variables to constants, and Z is a normalization constant. Many methods have been proposed to learn the weights of MLN using both generative and discriminative approaches (Richardson and Domingos, 2006; Singla and Domingos, 2006). There are also several MLN learning packages available online such as thebeast2, Tuffy3, PyMLNs4, Alchemy5, and so on. 2http://code.google.com/p/thebeast 3http://hazy.cs.wisc.edu/hazy/tuffy/ 4http://www9-old.in.tum.de/people/jain/mlns/ 5http://alchemy.cs.washington.edu/ 948 Describing the attributes of words Describing the attributes of the clause ci word(i, w) The clause ci has word w. firstWord(i, w) The first word of clause ci is word w. pos(i, w, t) The POS tag of word w is t in clause ci. dep(i, h, m) Word m and h are governor and dependent of a dependency relation in clause ci. Describ</context>
<context position="31224" citStr="Singla and Domingos, 2006" startWordPosition="4778" endWordPosition="4781">to use iterative classification algorithm to capture discourselevel associations. However different to us, they focused on pairwise relationships between opinion expressions. In this paper, we used MLN framework to capture another different discourse-level relation, which exists between subject clauses or subject clause and objective clause. Richardson and Domingos (2006) proposed Markov Logic Networks, which combines firstorder logic and probabilistic graphical models. In recent years, MLN has been adopted for several natural language processing tasks and achieved a certain level of success (Singla and Domingos, 2006; Riedel and Meza-Ruiz, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Jiang et al., 2012; Huang et al., 2012). Singla and Domingos (2006) modeled the entity resolution problem with MLN. They demonstrated the capability of MLN to seamlessly combine a number of previous approaches. Poon and Domingos (2008) proposed to use MLN for joint unsupervised coreference resolution. Yoshikawa et al. (2009) proposed to use Markov logic to incorporate both local features and global constraints that hold between temporal relations. Andrzejewski et al. (2011) introduced a framework for incorporating</context>
</contexts>
<marker>Singla, Domingos, 2006</marker>
<rawString>P. Singla and P. Domingos. 2006. Entity resolution with markov logic. In Data Mining, 2006. ICDM ’06. Sixth International Conference on, pages 572–582.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>151--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="23409" citStr="Socher et al. (2011)" startWordPosition="3616" endWordPosition="3619">raining set and the others as testing set. Since the dataset is newly created for this task, to compare the performance of the proposed method to other models, we also reimplemented several state7http://code.google.com/p/thebeast of-the-art methods for comparison. • CRF-Subj: We follow the method proposed by Zhao et al. (2008), which regard the subjectivity of all clauses throughout a paragraph as a sequential flow of sentiments and use CRFs to model it. The feature sets are similar as the local formulas for MLN including words, POS tags, dependency relations, and opinion lexicon. • RAE-Subj: Socher et al. (2011) proposed to use recursive autoencoders for sentence-level predication of sentiment label distributions. To compare with it, we also reimplement their method without any hand designed lexicon. • PDTB-Rel: For discourse relation extraction, we use “PDTB-Styled End-to-End Discourse Parser” (Lin et al., 2010) to extract discourse level relations as baseline. Since it is a general discourse relations identification algorithms, “Cause”, “Pragmatic Cause”, “Instantiation”, and “Restatement” relation types are treated as explanatory relation in this work. • SVM-Rel: We also use LibSVM (Chang and Lin,</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 151–161, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Galileo Namata</author>
<author>Lise Getoor</author>
<author>Janyce Wiebe</author>
</authors>
<title>Opinion graphs for polarity and discourse classification.</title>
<date>2009</date>
<booktitle>In Proceedings of TextGraphs-4.</booktitle>
<marker>Somasundaran, Namata, Getoor, Wiebe, 2009</marker>
<rawString>Swapna Somasundaran, Galileo Namata, Lise Getoor, and Janyce Wiebe. 2009. Opinion graphs for polarity and discourse classification. In Proceedings of TextGraphs-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Song</author>
<author>Jing Jiang</author>
<author>Wayne Xin Zhao</author>
<author>Sujian Li</author>
<author>Houfeng Wang</author>
</authors>
<title>Joint learning for coreference resolution with markov logic.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1245--1254</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="4316" citStr="Song et al., 2012" startWordPosition="651" endWordPosition="654">earch directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Song et al., 2012) and demonstrated its advantages. It can easily incorporate rich linguistic features and global constraints by designing various logic formulas, which can also be viewed as templates or rules. Logic formulas are combined in a probabilistic framework to model soft constraints. Hence, the proposed approach can benefit a lot from this framework. To evaluate the proposed method, we crawled a large number of product reviews and constructed a labeled corpus through Amazon’s Mechanical Turk. Two tasks were deployed for labeling the corpus. We compared the proposed method with state-ofthe-art methods </context>
</contexts>
<marker>Song, Jiang, Zhao, Li, Wang, 2012</marker>
<rawString>Yang Song, Jing Jiang, Wayne Xin Zhao, Sujian Li, and Houfeng Wang. 2012. Joint learning for coreference resolution with markov logic. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1245–1254, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Takashi Inui</author>
<author>Manabu Okumura</author>
</authors>
<title>Extracting semantic orientations of words using spin model.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="3144" citStr="Takamura et al., 2005" startWordPosition="471" endWordPosition="474"> reflection problem of the camera screen. Although, explanations provide valuable information, to the best of our knowledge, there is no existing work that deals with explanation extraction for opinions in discourse level. We think that if explanatory relations can be automatically identified from reviews, sentiment analysis applications may benefit from it. Existing opinion mining approaches mainly focus on subjective text. They try to determine the subjectivity and polarity of fragments of documents (e.g. a paragraph, a sentence, a phrase and a word) (Pang et al., 2002; Riloff et al., 2003; Takamura et al., 2005; Mihalcea et al., 2007; Dasgupta and Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment a</context>
</contexts>
<marker>Takamura, Inui, Okumura, 2005</marker>
<rawString>Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005. Extracting semantic orientations of words using spin model. In Proceedings of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong</author>
<author>Kim Sang</author>
<author>Herv´e D´ejean</author>
</authors>
<title>Introduction to the conll-2001 shared task: clause identification.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 workshop on Computational Natural Language Learning -Volume 7, ConLL ’01,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Tjong, Sang, D´ejean, 2001</marker>
<rawString>Erik F. Tjong, Kim Sang, and Herv´e D´ejean. 2001. Introduction to the conll-2001 shared task: clause identification. In Proceedings of the 2001 workshop on Computational Natural Language Learning -Volume 7, ConLL ’01, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP</booktitle>
<contexts>
<context position="27849" citStr="Wilson et al., 2005" startWordPosition="4275" endWordPosition="4278">tence distance are not as significant as the other features. 5 Related Work Our work relates to three research areas: sentiment analysis/opinion mining, discourse-level relation extraction, and Markov logic networks. Along with the increasing requirement, subjectivity classification has recently received considerable attention from both the industry and researchers. A variety of approaches and methods have been proposed for this task from different aspects. Among them, a number of approaches focus on classifying sentiments of text in different levels (e.g. words (Kim and Hovy, 2004), phrases (Wilson et al., 2005), sentences (Zhao et al., 2008), documents (Pang et al., 2002) and so on.), and detecting the overall polarity of them. Another research direction tries to convert the sentiment analysis task into entity identification and relation extraction. Hu and Liu (2004) proposed to use a set of methods to produce feature-based summary of a large number of customer reviews. Kobayashi et al. (2007) assumed that evaluative opinions could be structured as a frame which is composed by opinion holder, subject, aspect, and evaluation. They converted the task to two kinds of relation extraction tasks and propo</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of HLT-EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuanbin Wu</author>
<author>Qi Zhang</author>
<author>Xuangjing Huang</author>
<author>Lide Wu</author>
</authors>
<title>Structural opinion mining for graph-based sentiment representation.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<marker>Wu, Zhang, Huang, Wu, 2011</marker>
<rawString>Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide Wu. 2011. Structural opinion mining for graph-based sentiment representation. In Proceedings of EMNLP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liheng Xu</author>
<author>Kang Liu</author>
<author>Siwei Lai</author>
<author>Yubo Chen</author>
<author>Jun Zhao</author>
</authors>
<title>Walk and learn: a two-stage approach for opinion words and opinion targets coextraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd international conference on World Wide Web companion, WWW ’13 Companion,</booktitle>
<pages>95--96</pages>
<institution>Republic and Canton of Geneva, Switzerland. International World Wide Web Conferences Steering Committee.</institution>
<contexts>
<context position="3663" citStr="Xu et al., 2013" startWordPosition="547" endWordPosition="550"> a sentence, a phrase and a word) (Pang et al., 2002; Riloff et al., 2003; Takamura et al., 2005; Mihalcea et al., 2007; Dasgupta and Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al.</context>
</contexts>
<marker>Xu, Liu, Lai, Chen, Zhao, 2013</marker>
<rawString>Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun Zhao. 2013. Walk and learn: a two-stage approach for opinion words and opinion targets coextraction. In Proceedings of the 22nd international conference on World Wide Web companion, WWW ’13 Companion, pages 95–96, Republic and Canton of Geneva, Switzerland. International World Wide Web Conferences Steering Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Claire Cardie</author>
</authors>
<title>Joint inference for fine-grained opinion extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="3687" citStr="Yang and Cardie, 2013" startWordPosition="551" endWordPosition="554">rase and a word) (Pang et al., 2002; Riloff et al., 2003; Takamura et al., 2005; Mihalcea et al., 2007; Dasgupta and Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et </context>
</contexts>
<marker>Yang, Cardie, 2013</marker>
<rawString>Bishan Yang and Claire Cardie. 2013. Joint inference for fine-grained opinion extraction. In Proceedings of ACL 2013.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Katsumasa Yoshikawa</author>
<author>Sebastian Riedel</author>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Jointly identifying temporal relations with markov logic.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ’09,</booktitle>
<pages>405--413</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4269" citStr="Yoshikawa et al., 2009" startWordPosition="643" endWordPosition="646"> Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Song et al., 2012) and demonstrated its advantages. It can easily incorporate rich linguistic features and global constraints by designing various logic formulas, which can also be viewed as templates or rules. Logic formulas are combined in a probabilistic framework to model soft constraints. Hence, the proposed approach can benefit a lot from this framework. To evaluate the proposed method, we crawled a large number of product reviews and constructed a labeled corpus through Amazon’s Mechanical Turk. Two tasks were deployed for labeling the corpus. We compared th</context>
<context position="31276" citStr="Yoshikawa et al., 2009" startWordPosition="4786" endWordPosition="4789">scourselevel associations. However different to us, they focused on pairwise relationships between opinion expressions. In this paper, we used MLN framework to capture another different discourse-level relation, which exists between subject clauses or subject clause and objective clause. Richardson and Domingos (2006) proposed Markov Logic Networks, which combines firstorder logic and probabilistic graphical models. In recent years, MLN has been adopted for several natural language processing tasks and achieved a certain level of success (Singla and Domingos, 2006; Riedel and Meza-Ruiz, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Jiang et al., 2012; Huang et al., 2012). Singla and Domingos (2006) modeled the entity resolution problem with MLN. They demonstrated the capability of MLN to seamlessly combine a number of previous approaches. Poon and Domingos (2008) proposed to use MLN for joint unsupervised coreference resolution. Yoshikawa et al. (2009) proposed to use Markov logic to incorporate both local features and global constraints that hold between temporal relations. Andrzejewski et al. (2011) introduced a framework for incorporating general domain knowledge, which is represented by F</context>
</contexts>
<marker>Yoshikawa, Riedel, Asahara, Matsumoto, 2009</marker>
<rawString>Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asahara, and Yuji Matsumoto. 2009. Jointly identifying temporal relations with markov logic. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ’09, pages 405–413, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhao</author>
<author>Kang Liu</author>
<author>Gen Wang</author>
</authors>
<title>Adding redundant features for crfs-based sentence sentiment classification.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="23117" citStr="Zhao et al. (2008)" startWordPosition="3567" endWordPosition="3570">MIRA update rule. All the initial weights are set to zeros. The number of iterations is set to 10 epochs. Evaluation metrics used for subjectivity classification and relation extraction throughout the experiments include: Precision, Recall, and F1-score. We randomly select 80% reviews as training set and the others as testing set. Since the dataset is newly created for this task, to compare the performance of the proposed method to other models, we also reimplemented several state7http://code.google.com/p/thebeast of-the-art methods for comparison. • CRF-Subj: We follow the method proposed by Zhao et al. (2008), which regard the subjectivity of all clauses throughout a paragraph as a sequential flow of sentiments and use CRFs to model it. The feature sets are similar as the local formulas for MLN including words, POS tags, dependency relations, and opinion lexicon. • RAE-Subj: Socher et al. (2011) proposed to use recursive autoencoders for sentence-level predication of sentiment label distributions. To compare with it, we also reimplement their method without any hand designed lexicon. • PDTB-Rel: For discourse relation extraction, we use “PDTB-Styled End-to-End Discourse Parser” (Lin et al., 2010) </context>
<context position="27880" citStr="Zhao et al., 2008" startWordPosition="4280" endWordPosition="4283">cant as the other features. 5 Related Work Our work relates to three research areas: sentiment analysis/opinion mining, discourse-level relation extraction, and Markov logic networks. Along with the increasing requirement, subjectivity classification has recently received considerable attention from both the industry and researchers. A variety of approaches and methods have been proposed for this task from different aspects. Among them, a number of approaches focus on classifying sentiments of text in different levels (e.g. words (Kim and Hovy, 2004), phrases (Wilson et al., 2005), sentences (Zhao et al., 2008), documents (Pang et al., 2002) and so on.), and detecting the overall polarity of them. Another research direction tries to convert the sentiment analysis task into entity identification and relation extraction. Hu and Liu (2004) proposed to use a set of methods to produce feature-based summary of a large number of customer reviews. Kobayashi et al. (2007) assumed that evaluative opinions could be structured as a frame which is composed by opinion holder, subject, aspect, and evaluation. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based me</context>
</contexts>
<marker>Zhao, Liu, Wang, 2008</marker>
<rawString>Jun Zhao, Kang Liu, and Gen Wang. 2008. Adding redundant features for crfs-based sentence sentiment classification. In Proceedings of EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C¨acilia Zirn</author>
<author>Mathias Niepert</author>
<author>Heiner Stuckenschmidt</author>
<author>Michael Strube</author>
</authors>
<title>Fine-grained sentiment analysis with structural features.</title>
<date>2011</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>336--344</pages>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="30416" citStr="Zirn et al. (2011)" startWordPosition="4663" endWordPosition="4666">course-level relation extraction methods are also related to ours. Marcu and Echihabi (2002) proposed to use an unsupervised approach to recognizing discourse relations. Lin et al.(2009) analyzed the impacts of features extracted from contextual information, constituent parse trees, dependency parse trees, and word pairs. Asher et al.(2009) studied discourse segments containing opinion expressions from the perspective of linguistics. Chen et al. (2010) introduced a multi-label model to detect emotion causes. They developed two sets of linguistic features for this task base on linguistic cues. Zirn et al. (2011) proposed to use MLN framework to capture the context information in analysing (sub-)sentences. The most similar work to ours was proposed by Somasundaran et al.(2009). They proposed to use iterative classification algorithm to capture discourselevel associations. However different to us, they focused on pairwise relationships between opinion expressions. In this paper, we used MLN framework to capture another different discourse-level relation, which exists between subject clauses or subject clause and objective clause. Richardson and Domingos (2006) proposed Markov Logic Networks, which comb</context>
</contexts>
<marker>Zirn, Niepert, Stuckenschmidt, Strube, 2011</marker>
<rawString>C¨acilia Zirn, Mathias Niepert, Heiner Stuckenschmidt, and Michael Strube. 2011. Fine-grained sentiment analysis with structural features. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 336–344, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
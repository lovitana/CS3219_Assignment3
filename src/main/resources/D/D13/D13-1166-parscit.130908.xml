<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.725101">
Prior Disambiguation of Word Tensors
for Constructing Sentence Vectors
</title>
<author confidence="0.995627">
Dimitri Kartsaklis
</author>
<affiliation confidence="0.983294333333333">
University of Oxford
Department of
Computer Science
</affiliation>
<address confidence="0.9931635">
Wolfson Building, Parks Road
Oxford, OX1 3QD, UK
</address>
<email confidence="0.999036">
dimitri.kartsaklis@cs.ox.ac.uk
</email>
<author confidence="0.954432">
Mehrnoosh Sadrzadeh
</author>
<affiliation confidence="0.912725">
Queen Mary University of London
School of Electronic Engineering
and Computer Science
</affiliation>
<address confidence="0.9610745">
Mile End Road
London, E1 4NS, UK
</address>
<email confidence="0.999333">
mehrs@eecs.qmul.ac.uk
</email>
<sectionHeader confidence="0.996664" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999944789473684">
Recent work has shown that compositional-
distributional models using element-wise op-
erations on contextual word vectors benefit
from the introduction of a prior disambigua-
tion step. The purpose of this paper is to
generalise these ideas to tensor-based models,
where relational words such as verbs and ad-
jectives are represented by linear maps (higher
order tensors) acting on a number of argu-
ments (vectors). We propose disambiguation
algorithms for a number of tensor-based mod-
els, which we then test on a variety of tasks.
The results show that disambiguation can pro-
vide better compositional representation even
for the case of tensor-based models. Further-
more, we confirm previous findings regarding
the positive effect of disambiguation on vec-
tor mixture models, and we compare the ef-
fectiveness of the two approaches.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99987354">
Distributional models of meaning have been proved
extremely useful for a number of natural language
processing tasks, ranging from thesaurus extraction
(Curran, 2004) to topic modelling (Landauer and
Dumais, 1997) and information retrieval (Manning
et al., 2008), to name just a few. These models
are based on the distributional hypothesis of Har-
ris (1968), which states that the meaning of a word
depends on its context. This idea allows the words
to be represented by vectors of statistics collected
from a sufficiently large corpus of text; each ele-
ment of the vector reflects how many times a word
co-occurs in the same context with another word
of the vocabulary. However, due to the generative
power of natural language, which is able to pro-
duce infinite new structures from a finite set of re-
sources (words), no text corpus, regardless of its
size, can provide reliable distributional representa-
tions for anything longer than single words or per-
haps very short phrases consisting of two words; in
other words, this technique cannot scale up to the
phrase or sentence level.
Much research activity has been recently dedi-
cated to provide a solution to this problem: although
the direct construction of a sentence vector is not
possible, we might still be able to synthetically cre-
ate such a vectorial representation by somehow com-
posing the vectors of the words that comprise the
sentence. Towards this goal, researchers have em-
ployed a variety of approaches that roughly fall into
two general categories. Following an influential
work (Mitchell and Lapata, 2008), the models in the
first category compute a sentence vector as a mix-
ture of the original word vectors, using simple oper-
ations such as element-wise multiplication and ad-
dition; we refer to these models as vector mixtures.
The main characteristic of these models is that they
do not distinguish between the type-logical identi-
ties of the different words: an intransitive verb, for
example, is of the same order as its subject (a noun),
and both will contribute equally to the composite
sentence vector.
However, this symmetric treatment of composi-
tion seems unjustified from a formal semantics point
of view. Words with special meanings, such as verbs
and adjectives, are usually seen as functions acting
on, hence modifying, a number of arguments rather
than lexical units of the same order as them; an
adjective, for example, is a function that returns a
modified version of its input noun. Inspired from
</bodyText>
<page confidence="0.9133">
1590
</page>
<note confidence="0.730022">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1590–1601,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999957193548387">
this more-aligned-to-formal-semantics view, a sec-
ond research direction aims to represent relational
words as linear maps (tensors of various orders)
that can be applied to one or more arguments (vec-
tors). Baroni and Zamparelli (2010), for example,
model adjectives as matrices which, when matrix-
multiplied with a noun vector, will produce a vec-
torial representation of the specific adjective-noun
compound. The notion of a framework where re-
lational words are entities living in vector spaces of
higher order than nouns, which are simple vectors,
has been formalized by Coecke et al. (2010) in the
context of the abstract mathematical framework of
compact closed categories. We refer to this class of
models as tensor-based.
Regardless of the way they approach the repre-
sentation of relational words and their composition
operation, however, most current compositional-
distributional models do share a common feature:
they all rely on ambiguous vector representations,
where all the senses of a polysemous word, such as
the verb ‘file’ (which can mean register or smooth),
are merged into the same vector or tensor. At least
for the vector mixture approach, this practice has
been proved suboptimal: Reddy et al. (2011) and
Kartsaklis et al. (2013) test a number of simple mul-
tiplicative and additive models using disambiguated
vector representations on various tasks, showing that
the introduction of a disambiguation step prior to ac-
tual composition can indeed increase the quality of
the composite vectors. However, the fact that disam-
biguation can be beneficial for models based on vec-
tor mixtures is not very surprising. Both additive and
multiplicative compositions are but a kind of average
of the vectors of the words in the sentence, hence
can directly benefit from the provision of more ac-
curate starting points. Perhaps a more interesting
question, and one that the current paper aims to ad-
dress, is to what extent disambiguation can also pro-
vide benefits for tensor-based approaches, which in
general constitute more powerful models for natural
language (see discussion in Section 2).
Specifically, this paper aims to: (a) propose dis-
ambiguation algorithms for a number of tensor-
based distributional models; (b) examine the effect
of disambiguation on tensors for relational words;
and (c) meaningfully compare the effectiveness of
tensor-based against vector mixture models in a
number of tasks. Based on the generic procedure of
Sch¨utze (1998), we propose algorithms for a num-
ber of tensor-based models, where the composition
is modelled as the application of linear maps (ten-
sor contractions). Following Mitchell and Lapata
(2008) and many others, we test our models on
two disambiguation tasks similar to that of Kintsch
(2001), and on the phrase similarity task introduced
in (Mitchell and Lapata, 2010). In almost every
case, the results show that disambiguation can make
a great difference in the case of tensor-based models;
they also reconfirm previous findings regarding the
effectiveness of the method for simple vector mix-
ture models.
</bodyText>
<sectionHeader confidence="0.808964" genericHeader="introduction">
2 Vectors vs tensors
</sectionHeader>
<bodyText confidence="0.999983291666667">
The simple models of Mitchell and Lapata (2008)
constitute the easiest and perhaps the most intuitive
way of composing two or more vectors: each ele-
ment of the resulting vector is computed as the sum
or the product of the corresponding elements in the
input vectors (left part in Figure 1). In the case of
addition, the components of the output vector are
simply the cumulative scores of the corresponding
input components. So in a sense the output element
embraces both input elements, resembling a union of
the input features. On the other hand, the element-
wise multiplication of two vectors can be seen as the
intersection of their features: a zero element in one
of the input vectors will eliminate the corresponding
feature in the output, no matter how high the other
input component was. In addition to failing to iden-
tify the special roles of words in a sentence, vector
mixture models disregard grammar in another way:
the commutativity of operators make them a bag-
of-words approach, where the meaning of sentence
‘dog bites man’ is equated to that of ‘man bites dog’.
On the contrary to the above element-wise treat-
ment, a compositional approach based on linear
maps computes each element of the resulting vec-
</bodyText>
<subsectionHeader confidence="0.967896">
Vector mixture Tensor-based
</subsectionHeader>
<figureCaption confidence="0.78716075">
Figure 1: Vector mixture and tensor-based models for
composition. In the latter approach, the ith element of
the output vector is the linear combination of the input
vector with the ith row of the matrix.
</figureCaption>
<page confidence="0.983726">
1591
</page>
<bodyText confidence="0.999838166666667">
tor via a linear combination of all the elements of
the input vector (right part of Figure 1); in other
words, possible interdependencies between differ-
ent features are also taken into account, offering (in
principle) more power. Furthermore, by design, the
bag-of-words problem is not present here. Over-
all, tensor-based models offer a more complete and
linguistically motivated solution to the problem of
composition. For example, one can consider build-
ing linear maps for prepositions and logical words,
rather than treating them as noise and discard them,
as commonly done in the vector mixture models.
</bodyText>
<sectionHeader confidence="0.978057" genericHeader="method">
3 Disambiguation in vector mixtures
</sectionHeader>
<bodyText confidence="0.999921458333333">
For a compositional model based on vector mix-
tures, polysemy of words can be a critical factor.
Pulman (2013) and Kartsaklis et al. (2013) point out
that the element-wise combination of “ambiguous”
vectors produces results that are hard to interpret;
the composed vector is not a purely compositional
representation but a product of two tasks that take
place in parallel: composition and some amount of
disambiguation that emerges as a side-effect of the
compositional process, leaving the resulting vector
in an intermediate state.
This effect is demonstrated in Figure 2, which
shows the composition of the ambiguous verb ‘run’
(with meanings moving fast and dissolving) with the
subject ‘horse’. The first three components of our
toy vector space are related to the dissolving mean-
ing, while the last three of them to the moving fast
meaning. An ambiguous vector for ‘run’ will have
non-zero values for every component. On the other
hand, we would expect the vector for ‘horse’ to have
high values for the ‘race’, ‘gallop’, and ‘move’ com-
ponents, and very low values (but not necessarily
zero) for the dissolving-related ones—it is always
possible for the word ‘horse’ to appear in the same
</bodyText>
<figureCaption confidence="0.98144">
Figure 2: The effect of disambiguation on vector compo-
sition. The numbers are (artificial) co-occurrence counts
of each target word with the 6 basis words on the left.
</figureCaption>
<bodyText confidence="0.999988842105263">
context with the word ‘painting’, for example. The
left part of Figure 2 shows what happens when the
ambiguous ‘run’ vector is used; the multiplication
with the ‘horse’ vector will produce an impure re-
sult, half affected by composition and half by disam-
biguation. However, what we really want is a vec-
tor where all the dissolving-related components will
be eliminated, since they are irrelevant to the way
the word ‘run’ is used in the sentence. In order to
achieve this, we have to introduce a disambiguation
step prior to composition (right part of Figure 2).
These ideas are experimentally verified in the
works of Reddy et al. (2011) and Kartsaklis et al.
(2013); Pulman (2013) also presents a comprehen-
sive analysis of the problem. What remains to be
seen is if disambiguation can also provide bene-
fits for the linguistically motivated setting of tensor-
based models, the principles of which are shortly
discussed in the next section.
</bodyText>
<sectionHeader confidence="0.948977" genericHeader="method">
4 Tensors as multilinear maps
</sectionHeader>
<bodyText confidence="0.9992916">
A tensor is a geometric object that can be seen as the
generalization of the familiar notion of a vector in
higher dimensions. The order of a tensor is the num-
ber of its dimensions; in other words, the number of
indices we need to fully describe a random element
of the tensor. Hence, a vector is a tensor of order
1, a matrix is a tensor of order 2, and so on. Ten-
sors and multilinear maps stand in one-to-one cor-
respondence, as stated by the following well-known
“map-state” isomorphism (Bourbaki, 1989):
</bodyText>
<equation confidence="0.99284">
f : V1 → ... → V3 → Vk ∼� Vk⊗V3⊗...⊗V1 (1)
</equation>
<bodyText confidence="0.999925529411765">
This offers an elegant way to adopt a formal se-
mantics view of natural language in vector spaces.
Let nouns live in a basic vector space N ∈ RD; re-
turning to our previous example, an adjective then
can be seen as a map f : N → N which is isomor-
phic to N ⊗ N (that is, to a matrix). In general, the
order of the tensor is equal to the number of argu-
ments plus one dimension that carries the result; so
a unary function (e.g. adjectives, intransitive verbs)
is represented by a tensor of order 2 (a matrix), a bi-
nary function (e.g. a transitive verb) as an order 3
tensor, and so on. Due to the above isomorphism,
function application (and hence our compositional
operation) becomes a generalisation of matrix mul-
tiplication, formalised in terms of the inner product.
In the case of a unary relational word, such as an
adjective, this is nothing more than the usual notion
</bodyText>
<figure confidence="0.981672695652174">
Ambiguous Disambiguated
horse run horse run
colour
dissolve
painting
race
gallop
move
1
0
2
6
13
7
5
9
4
11
8
15
5
0
8
_
66
104
105
1
0
2
6
13
7
0
0
0
11
8
15
0
0
0
_
66
104
105
</figure>
<page confidence="0.987584">
1592
</page>
<bodyText confidence="0.999579">
of matrix multiplication between a matrix and a vec-
tor. The generalization of this process to tensors of
higher order is known as tensor contraction. Given
two tensors of orders n and m, the tensor contrac-
tion operation will always produce a tensor of order
n + m − 2.
Let us see an example of how this works for a
simple transitive sentence. Let V E RIxJxK be the
tensor of order 3 for the verb and S E RI, O E RK
the tensors of order 1 (vectors) for the subject and
the object of the verb, respectively. Then V x O will
return a new tensor living in RIxJ (i.e. a matrix)1;
a further interaction of this result with the subject
will return a vector for the whole transitive sentence
living in RJ. We should note that the order in which
the verb is applied to its arguments is not important;
so in general the meaning of a transitive sentence is
given by:
</bodyText>
<equation confidence="0.997573">
(V x O)T x S = (VT x S) x O (2)
</equation>
<bodyText confidence="0.999941">
where T denotes a transpose and makes indices
match, since subject precedes the verb.
</bodyText>
<sectionHeader confidence="0.864995" genericHeader="method">
5 Creating verb tensors
</sectionHeader>
<bodyText confidence="0.997568285714286">
In this section we review a number of proposals re-
garding concrete methods of constructing tensors for
relational words in the context of the frameworks
of Coecke et al. (2010) and Baroni and Zamparelli
(2010), which both comply to the setting of Section
4.2
Relational Following ideas from the set-theoretic
view of formal semantics, Grefenstette and
Sadrzadeh (2011a) suggest that the meaning of a
relational word should be represented as the sum
of its arguments. The meaning of adjective ‘red’,
for example, becomes the sum of the vectors of
all the nouns that ‘red’ modifies in the corpus; so
red= �
</bodyText>
<equation confidence="0.574595">
−� i −−−�
</equation>
<bodyText confidence="0.9360948">
nouni, where i iterates through all the
occurrences of ‘red’. This can be generalised to
relational words of any arity, by summing the tensor
product of their arguments. So for a transitive verb
we have:
</bodyText>
<equation confidence="0.929325">
�verb2 = (subji ® obji) (3)
i
1The symbol x denotes tensor contraction.
</equation>
<bodyText confidence="0.997286869565217">
2In what follows we use the case of a transitive verb as an
example; however the descriptions apply to any relational word
of any arity. A vector (an order-1 tensor) is denoted as x ; ten-
sors of order n &gt; 1 are shown as x&amp;quot; for clarity.
where i again iterates over all occurrences of the spe-
cific verb in the corpus and the superscript denotes
the order of the tensor.
In order to achieve a more expressive represen-
tation for the sentences, the authors used the con-
vention that the arity of the head word in a sentence
will also determine the order of the sentence space;
that is, the space of intransitive sentences will be of
order 1, of transitive ones will be of order 2, and
so on. Recall from Section 4 that for the transitive
case this increases the order of the verb tensor to 4
(2 dimensions for the arguments plus another 2 for
the result). In spite of this, however, note that the
method of Equation 3 produces a matrix. The other
two dimensions of the tensor remain empty (filled
with zeros), a fact that simplifies the calculations but
also considerably weakens the expressive power of
the model. This simplification transforms Equation
2 to the following:
</bodyText>
<equation confidence="0.9750865">
2 � � 2
subj verb obj = (subj ® obj) O verb (4)
</equation>
<bodyText confidence="0.999958333333333">
where ® denotes the tensor product and O element-
wise multiplication.
Kronecker In a subsequent work (Grefenstette
and Sadrzadeh, 2011b), the same team proposes the
creation of a verb matrix as the Kronecker product
of the verb’s contextual vector with itself:
</bodyText>
<equation confidence="0.97752475">
2
verb = −−�
verb ® −−�
verb (5)
</equation>
<bodyText confidence="0.999574444444444">
Again in this model the sentence space is of order
2, and the meaning of a transitive sentence is calcu-
lated using Equation 4.
Frobenius The previous models bring the impor-
tant limitation that only sentences of the same struc-
ture can be meaningfully compared; it is not pos-
sible, for example, to compare an intransitive sen-
tence (e.g. ‘kids play’) with a transitive one (‘chil-
dren play football’), since the former is a vector and
the latter a matrix. Using Frobenius algebras, Kart-
saklis et al. (2012) provide a unified sentence space
for every sentence regardless of its type. These mod-
els turn the matrix of Equation 3 to a tensor of or-
der 3 (as required by the type-logical identities) by
copying one of the existing dimensions. When the
dimension of rows (corresponding to subjects) is
copied, the calculation of a vector for a transitive
sentence becomes:
</bodyText>
<equation confidence="0.785476222222222">
−−−−−−−−−�
2
subj verb obj = subj O (verb x −�
−−� obj) (6)
1593
Copying the column dimension (objects) gives:
�
) � 2)T �
subj verb obj = obj O (verb) x subj (7)
</equation>
<bodyText confidence="0.99708625">
Linear regression None of the above models cre-
ate tensors that are fully populated: one or more
dimensions will always remain empty. Following
an idea first introduced by Baroni and Zamparelli
(2010) for the creation of adjective matrices, Grefen-
stette et al. (2013) use linear regression in order
to learn full tensors of order 3 for transitive verbs.
Linear regression is a supervised method of learn-
ing, so it needs a number of exemplar data points.
In the case of the adjective ‘red’, for example, we
would need a set of the form (car, red car), (h ,
red shirt), (���
</bodyText>
<subsubsectionHeader confidence="0.33826">
������ shoe, ������
</subsubsectionHeader>
<bodyText confidence="0.906726909090909">
red shoe) and so on, where the sec-
ond vector in each pair is the contextual vector of the
whole phrase created exactly as if it were a single
word. The goal of the learning process is to find the
parameters adj2 and ��b such that:
adj noun � adj2 x ����
������� noun + b (8)
for all nouns modified by the specific adjective. In
��
practice, the bias b is embedded in adj2, hence
the above procedure provides us with a matrix for
the adjective. One can generalize this procedure to
tensors of higher order by proceeding step-wise, as
done by Grefenstette et al. (2013). For the case
of a transitive verb, they first use exemplar pairs
of the form (���
subj, ���������
subj verb obj) to learn a matrix
verb obj2 for the verb phrase; then, they perform
a new training session with exemplars of the form
(��obj, verb obj2), the result of which is an order 3
tensor for the verb.
</bodyText>
<sectionHeader confidence="0.986336" genericHeader="method">
6 Generic context-based disambiguation
</sectionHeader>
<bodyText confidence="0.999991255813954">
In all of the models of Section 5, the training of a
relational word tensor is based on the set of contexts
where this word occurs. Hence, in these models the
problem of creating disambiguated versions of ten-
sors can be recasted to that of further breaking the
set of contexts in a way that each subset reflects a
different sense of the word in the corpus. If, for ex-
ample, S is the whole set of sentences for a word
w that occurs in the corpus under n different senses,
then the goal is to create n subsets S1,... Sn such
that S1 contains the sentences where w appears un-
der the first sense, S2 the sentences where w occurs
under the second sense, and so on. Each one of these
subsets can then be used to train a tensor for a spe-
cific sense of the target relational word.
Towards this purpose we use a variation of the ef-
fective procedure of Sch¨utze (1998): first, each con-
text for a target word wt is represented by a context
vector of the form 1n(i + ... + n), where wi�� is
the lexical vector of some other word wi =� wt in the
same context. Next, we apply a clustering method
on this set of vectors in order to discover the latent
senses of wt. The assumption is that the contexts
of wt will vary according to the specific sense this
word is used: ‘bank’ as a financial institution should
appear in quite different contexts than as land.
The above procedure will give us a number of
clusters, each consisting of context vectors; we use
the centroid of each cluster as a vectorial repre-
sentation of the corresponding sense. So in our
model each word w is initially represented by a tuple
(—W*, S), where w is the lexical vector of the word as
created by the usual distributional practice, and S
is a set of sense vectors (centroids of context vec-
tor clusters) produced by the above procedure. The
disambiguation of a new word w under a context C
can now be accomplished as follows: we create a
context vector c for C as above, and we compare it
with every sense vector of w; the word is assigned to
the sense corresponding to the closest sense vector.
Specifically, if S,,, is the set of sense vectors for w,
*�c the context vector for C, and d(v , u ) our vector
distance measure, the preferred sense s� is given by:
</bodyText>
<equation confidence="0.947859">
d(-V-,+, - ) (9)
</equation>
<bodyText confidence="0.999922071428571">
For the actual clustering step we follow the set-
ting of Kartsaklis et al. (2013), which worked well in
tasks very similar to ours. Specifically, we perform
hierarchical agglomerative clustering (HAC) using
Ward’s method as the inter-cluster distance, while
the distance between vectors is measured with Pear-
son’s correlation.3 In the above work, this configura-
tion has been found to return the highest V-measure
(Rosenberg and Hirschberg, 2007) on the noun set
of SEMEVAL 2010 Word Sense Induction &amp; Disam-
biguation Task (Manandhar et al., 2010). As con-
text for a word, we consider the sentence in which
this word occurs. The output of HAC is a dendro-
gram embedding all the possible partitionings of the
</bodyText>
<footnote confidence="0.797096">
3Informal experimentation with more robust probabilistic
techniques, such as Dirichlet process gaussian mixture models,
revealed no significant benefits for our setting.
</footnote>
<equation confidence="0.739273">
s� = arg min
�� v3ESw
</equation>
<page confidence="0.902019">
1594
</page>
<bodyText confidence="0.999982272727273">
data. In order to select the optimal partitioning, we
rely on the Cali´nski/Harabasz index (Cali´nski and
Harabasz, 1974), also known as variance ratio cri-
terion (VRC). VRC is calculated as the ratio of the
sum of the inter-cluster variances over the sum of
the intra-cluster variances, bearing the intuition that
the optimal partitioning should be the one that re-
sults in the most compact and maximally separated
clusters. We compute the VRC for a range of differ-
ent partitionings (from 2 to 10 clusters) and keep the
partitioning with the highest score.
</bodyText>
<sectionHeader confidence="0.93317" genericHeader="method">
7 Constructing unambiguous verb tensors
</sectionHeader>
<bodyText confidence="0.999777266666667">
The procedure described in Section 6 provides us
with n clusters of context vectors for a target word.
Since in our case each context vector corresponds
to a distinct sentence, the output of the clustering
scheme can also be seen as n subsets of sentences,
where the word appears under different senses. It
is now quite straightforward to use this partitioning
of the training corpus in order to learn unambiguous
versions of verb tensors, as detailed below.
Relational/Frobenius Both the Relational and the
Frobenius models use the same way of creating an
initial verb matrix (Equation 3) which then they ex-
pand to a higher order tensor. Let S1 ... Sn be the
sets of sentences returned by the clustering step for
a verb. Then, the verb tensor for the ith sense is:
</bodyText>
<equation confidence="0.847366333333333">
(����
subjs ® ���
objs) (10)
</equation>
<bodyText confidence="0.97802425">
where subjs and objs refer to the subject/object pair
that occurred with the verb in sentence s. This can
be generalized to any arity n as follows:
ar
</bodyText>
<equation confidence="0.989944">
g (11)
</equation>
<bodyText confidence="0.9996236">
where argk,s denotes the kth argument of the target
word in sentence s.
Kronecker For a given verb v in a context C, let
Z be the sense vector of v given C corresponding to
the sense i returned by Equation 9. Then we have:
</bodyText>
<equation confidence="0.952682">
verb2 i = �� vi ® �� vi (12)
</equation>
<bodyText confidence="0.651045">
The generalized version to arity n is given by:
</bodyText>
<equation confidence="0.9185285">
n n Z (13)
wordi = k=1
</equation>
<bodyText confidence="0.999851692307692">
Linear regression Creating unambiguous full ten-
sors using linear regression is also quite straightfor-
ward. Let us assume again that the clustering step
for a verb v returns n sets of sentences S1 ... Sn,
where each sentence set corresponds to a different
sense. Then, we have n different regression prob-
lems, each one of which will be trained on exemplar
pairs derived exclusively from the sentences of the
corresponding set. This will result in n verb tensors,
which will correspond to the different senses of the
verb. Generalization to higher arities is a straightfor-
ward extension of the step-wise process in Section 5
for transitive verbs.
</bodyText>
<sectionHeader confidence="0.999077" genericHeader="method">
8 Experiments
</sectionHeader>
<bodyText confidence="0.999986529411765">
In this section we will test the effect of disambigua-
tion on the models of Section 5 in a variety of tasks.
Due to the significant methodological differences
of the linear regression model from the other ap-
proaches and the variety of its set of parameters, we
decided that it would be better if this was left as the
subject of a distinct work.
Experimental setting We train our vectors using
ukWaC (Ferraresi et al., 2008), a corpus of English
text with 2 billion words (100m sentences). We use
2000 dimensions, with weights calculated as the ra-
tio of the probability of the context word given the
target word to the probability of the context word
overall. The context here is a 5-word window on
both sides of the target word. The vectors are disam-
biguated both syntactically and semantically: first,
separate vectors have been created for different syn-
tactic usages of the same word in the corpus; for ex-
ample, the word ‘book’ has two vectors, one for its
noun sense and one for its verb sense. Furthermore,
each word is semantically disambiguated according
to the method of Section 6.
Models We compare the tensor-based models of
Section 5 with the multiplicative and additive mod-
els of Mitchell and Lapata (2008), reporting results
for both ambiguous and disambiguated versions.
For all the disambiguated models, the best sense for
each word in the sentence or phrase is first selected
by applying the procedure of Section 6 and Equa-
tion 9. If the model is based on a vector mixture, the
sense vectors corresponding to these senses are mul-
tiplied or added to form the composite representa-
tion for the sentence or phrase. For the tensor-based
models, the composite meanings are calculated ac-
</bodyText>
<equation confidence="0.998875714285714">
E
2
verbi =
sESi
n
wordi n = E
sESi k=1
</equation>
<page confidence="0.947064">
1595
</page>
<bodyText confidence="0.999977625">
cording to the equations of Section 5, using verb
tensors created by the procedures of Section 7. The
semantic similarity of two phrases or sentences is
measured as the cosine distance between their com-
posite vectors. For models that return a matrix (e.g.
Relational, Kronecker), the distance is based on the
Frobenius inner product.
Implementation details Our code is mainly writ-
ten in Python and C++, and for the actual cluster-
ing step we use the Python interface of the efficient
FASTCLUSTER library (M¨ullner, 2013). In a shared
24-core Xeon machine with 72 GB of memory, and
with a fair amount of parallelism applied, the aver-
age processing time per word was about 4 minutes;
this is roughly translated to 12-13 hours of training
on average per dataset.
</bodyText>
<subsectionHeader confidence="0.943715">
8.1 Verb disambiguation task
</subsectionHeader>
<bodyText confidence="0.999951928571429">
Perhaps surprisingly, one of the most popular tasks
for testing compositionality in distributional models
is based on disambiguation. This task, originally in-
troduced by Kintsch (2001), has been adopted by
Mitchell and Lapata (2008) and others for evaluating
the quality of composition in vector spaces. Given
an ambiguous verb such as ‘file’, the goal is to find
out to what extent the presence of an appropriate
context will disambiguate its intended meaning. The
context (e.g. a subject/object pair) is composed with
two landmark verbs corresponding to the different
senses (‘smooth’ and ‘register’) to create simple sen-
tences. The assumption is that a good compositional
model should be able to reflect that ‘woman files ap-
plication’ is closer to ‘woman registers application’
than to ‘woman smooths application’.
In this paper we test our models on two different
datasets of transitive sentences, that of Grefenstette
and Sadrzadeh (2011a) and Kartsaklis et al. (2013)4.
Specific details about the creation of the datasets can
be found in the above papers; for the purposes of
the current work it is sufficient to mention that their
main difference is that in the former the verbs and
their alternative meanings have been selected auto-
matically using the JCN metric of semantic similar-
ity (Jiang and Conrath, 1997), while in the latter the
selection was based on human judgements from the
work of Pickering and Frisson (2001). So, while
</bodyText>
<footnote confidence="0.971779666666667">
4This dataset has been created by Mehrnoosh Sadrzadeh in
collaboration with Edward Grefenstette, but remained unpub-
lished until (Kartsaklis et al., 2013).
</footnote>
<bodyText confidence="0.999862021276596">
in the first dataset many verbs cannot be consid-
ered as genuinely ambiguous (e.g. ‘say’ with mean-
ings state and allege or ‘write’ with meanings pub-
lish and spell), the landmarks in the second dataset
correspond to clearly separated senses (e.g. ‘file’
with meanings register and smooth or ‘charge’ with
meanings accuse and bill). Furthermore, subjects
and objects of this latter case are modified by appro-
priate adjectives, overall creating a richer and more
linguistically balanced dataset.
In both cases the evaluation methodology is the
same: each entry of the dataset has the form
(subject, verb, object, high-sim landmark, low-sim
landmark). The context is combined with the verb
and the two landmarks, creating three simple tran-
sitive sentences. The main-verb sentence is paired
with both the landmark sentences, and these pairs
are randomly presented to human evaluators, the
duty of which is to evaluate the similarity of the sen-
tences within a pair in a scale from 1 to 7. The scores
of the compositional models are the cosine distances
(or the Frobenius inner products, in the case of ma-
trices) between the composite representations of the
sentences of each pair. As an overall score for each
model, we report its Spearman’s p correlation with
the human judgements. Both datasets consist of 200
pairs of sentences (10 main verbs x 2 landmarks x
10 contexts).
Results The results for the G&amp;S dataset are shown
in Table 1.5 The verbs-only model (BL) refers to a
non-compositional evaluation, where the similarity
between two sentences is solely based on the dis-
tance between the two verbs, without applying any
compositional step with subject and object.
The tensor-based models present much better per-
formance than the vector mixture ones, with the dis-
ambiguated version of the copy-object model sig-
nificantly higher than the relational model. By de-
sign, the copy-object model retains more informa-
tion about the objects; so this result confirms pre-
vious findings, that in this certain dataset the role
of objects is more important than that of subjects
(Kartsaklis et al., 2012). In general, the disambigua-
tion step improves the results of all the tensor-based
models except Kronecker; the effect is reversed for
the vector mixture models, where the disambiguated
versions present much worse performance (these
</bodyText>
<footnote confidence="0.966124">
5For all tables in this section, « and » denote highly sta-
tistically significant differences with P &lt; 0.001.
</footnote>
<page confidence="0.937874">
1596
</page>
<table confidence="0.999615666666667">
Model Ambig. Disamb.
BL Verbs only 0.198 » 0.132
M1 Multiplicative 0.137 » 0.044
M2 Additive 0.127 » 0.047
T1 Relational 0.219 &lt; 0.223
T2 Kronecker 0.207 » 0.061
T3 Copy-subject 0.070 « 0.122
T4 Copy-object 0.241 « 0.262
Human agreement 0.599
</table>
<tableCaption confidence="0.992296">
Difference between T4 and T1 is s.s. with p &lt; 0.001
Table 1: Results for the G&amp;S dataset.
</tableCaption>
<table confidence="0.999934222222222">
Model Ambig. Disamb.
BL Verbs only 0.151 « 0.217
M1 Multiplicative 0.131 &lt; 0.137
M2 Additive 0.085 « 0.193
T1 Relational 0.036 « 0.121
T2 Kronecker 0.159 &lt; 0.166
T3 Copy-subject 0.035 « 0.117
T4 Copy-object 0.033 « 0.095
Human agreement 0.383
</table>
<tableCaption confidence="0.993419">
Difference between BL and M2 is s.s. with p &lt; 0.001
Table 2: Results for the Kartsaklis et al. dataset.
</tableCaption>
<bodyText confidence="0.999157742857143">
findings are further discussed in Section 9).
The result of disambiguation is clearer for the
dataset of Kartsaklis et al. (Table 2). The longer
context in combination with genuinely ambiguous
verbs produces two effects: first, disambiguation is
now helpful for all models, either vector mixtures
or tensor-based; second, the disambiguation of just
the verb (verbs-only model), without any interac-
tion with the context, is sufficient to provide the best
score (0.22) with a difference statistically significant
from the second model (0.19 for disambiguated ad-
ditive). In fact, further composition of the verb with
the context decreases performance, confirming the
results reported by Kartsaklis et al. (2013) for vec-
tors trained using BNC. Given the nature of the spe-
cific task, which is designed around the ambiguity of
the verb, this result is not surprising: a direct disam-
biguation of the verb based on the rest of the con-
text should naturally constitute the best method to
achieve top performance—no composition is neces-
sary for this task to be successful.
However, when one does use a task like this in
order to evaluate compositional models (as we do
here and as is commonly the case), they implic-
itly correlate the strength of the disambiguation ef-
fect that takes place during the composition with the
quality of composition, essentially assuming that the
stronger the disambiguation, the better the composi-
tional model that produced this side-effect. Unfor-
tunately, the extent to which this assumption is valid
or not is still not quite clear; the subject is addressed
in more detail in (Kartsaklis et al., 2013). Keeping a
note of this observation, we now proceed to examine
the performance of our models in a task that does not
use disambiguation as a criterion of composition.
</bodyText>
<subsectionHeader confidence="0.999089">
8.2 Phrase/sentence similarity task
</subsectionHeader>
<bodyText confidence="0.999994516129032">
Our second set of experiments is based on the phrase
similarity task of Mitchell and Lapata (2010). On
the contrary with the task of Section 8.1, this one
does not involve any assumptions about disambigua-
tion, and thus it seems like a more genuine test of
models aiming to provide appropriate phrasal or sen-
tential semantic representations; the only criterion is
the degree to which these models correctly evaluate
the similarity between pairs of sentences or phrases.
We work on the verb-phrase part of the dataset, con-
sisting of 72 short verb phrases (verb-object struc-
tures). These 72 phrases have been paired in three
different ways to form groups exhibiting various
degrees of similarity: the first group contains 36
pairs of highly similar phrases (e.g. produce effect-
achieve result), the pairs of the second group are
of medium similarity (e.g. write book-hear word),
while a last group contains low-similarity pairs (use
knowledge-provide system). The task is again to
compare the similarity scores given by the various
models for each phrase pair with those of human an-
notators. Additionally to the verb phrases task, we
also perform a richer version of the experiment us-
ing transitive sentences.
Verb phrases It can be shown that for simple verb
phrases the relational model reduces itself to the
copy-subject model; for both of these methods, the
meaning of the verb phrase is calculated according
to Equation 6. Furthermore, according to the copy-
object model the meaning of a verb phrase computed
by a verb matrix Eij vij(i ®&apos;nj) and an object vec-
</bodyText>
<equation confidence="0.832066333333333">
tor Ej ojnj becomes:
�verb object2 � vijoj(i ® ��nj) (14)
ij
</equation>
<bodyText confidence="0.90322">
Finally, the Kronecker model has no meaning for
verb phrases, since the vector of a verb phrase
��
will become (�� vs ® �� vs) x obj, which is equal to
(s ��obj)s, where (3 ��obj) denotes the inner prod-
uct between vectors of verb and object. Hence, the
</bodyText>
<page confidence="0.975601">
1597
</page>
<table confidence="0.947072555555556">
Model Ambig. Disamb.
BL Verbs only 0.310 « 0.420
M1 Multiplicative 0.315 « 0.448
M2 Additive 0.291 « 0.436
T1 Rel./Copy-sbj 0.340 « 0.367
T2 Copy-object 0.290 « 0.393
Human agreement 0.550
Difference between M1 and M2 is not s.s.
Difference between M1 and BL is s.s. with p &lt; 0.001
</table>
<tableCaption confidence="0.999375">
Table 3: Results for the original M&amp;L task.
</tableCaption>
<bodyText confidence="0.99978425">
meaning of a verb phrase becomes a scalar multipli-
cation of the meaning of its verb. As a result, the
cosine distance (used for measuring similarity) be-
tween the meanings of two verb phrases is reduced
to the distance between the vectors of their verbs,
completely dismissing the role of their objects.
Hence our models are limited to those of Table
3. The effects of disambiguation for this task are
quite impressive: the differences between the scores
of all disambiguated models and those of the am-
biguous versions are highly statistically significant
(with p &lt; 0.001), while 4 of the 5 models present
an improvement greater than 10 units of correla-
tion. The models that benefit the most from disam-
biguation are the vector mixtures; both of these ap-
proaches perform significantly better than the best
tensor-based model (copy-object). In fact, the score
of M1 (0.45) is quite high, given that the inter-
annotator agreement is 0.55 (best score reported by
Mitchell and Lapata was 0.41 for their LDA-dilation
model).
Transitive sentences The second part of this ex-
periment aims to examine the extent to which the
above picture can change for the case of text struc-
tures longer than verb phrases. In order to achieve
this, we extend each one of the 72 verb phrases to
a full transitive sentence by adding an appropriate
subject such that the similarity relationships of the
original dataset are retained as much as possible,
so the human judgements for the verb phrase pairs
could as well be used for the transitive cases. We
worked pair-wise: for each pair of verb phrases, we
first selected one of the 5 most frequent subjects for
the first phrase; then, the subject of the other phrase
was selected by a list of synonyms of the first sub-
ject in a way that the new pair of transitive sen-
tences constitutes the least more specific version of
the given verb-phrase pair. So, for example, the pair
produce effect/achieve result became drug produce
effect/medication achieve result, while the pair pose
problem/address question became study pose prob-
lem/paper address question.6
The restrictions of the verb-phrase version do not
hold here, so we evaluate on the full set of models
(Table 4). Once more disambiguation produces bet-
ter results in all cases, with highly statistically sig-
nificant differences for all but one model. Further-
more, now the best score is delivered by one of the
tensor-based models (Kronecker), with a difference
not statistically significant from disambiguated ad-
ditive. In any case, the result suggests that as the
length of the text segments increases, the perfor-
mance of vector mixtures and tensor-based models
converges. Indeed, note how the performance of the
vector mixture models are significantly decreased
compared to the verb phrase task.
</bodyText>
<sectionHeader confidence="0.999022" genericHeader="method">
9 Discussion
</sectionHeader>
<bodyText confidence="0.999975117647059">
The purpose of this work was twofold: our main ob-
jective was to investigate how disambiguation can
affect the compositional models which are based on
higher order vector spaces; a second, but not less
important goal, was to compare this more linguisti-
cally motivated approach to the simpler vector mix-
ture methods. Based on the experimental work pre-
sented here, we can say with enough confidence that
disambiguation as an additional step prior to com-
position is indeed very beneficial for tensor-based
models. Furthermore, our experiments confirm and
strengthen previous work (Reddy et al., 2011; Kart-
saklis et al., 2013) that showed better performance of
disambiguated vector mixture models compared to
their ambiguous versions. The positive effect of dis-
ambiguation is more evident for the vector mixture
models (especially for the additive model) than for
</bodyText>
<footnote confidence="0.7082545">
6The dataset will be available at http://www.cs.ox.
ac.uk/activities/compdistmeaning/.
</footnote>
<table confidence="0.999681111111111">
Model Ambig. Disamb.
BL Verbs only 0.310 « 0.341
M1 Multiplicative 0.325 « 0.404
M2 Additive 0.368 « 0.410
T1 Relational 0.368 « 0.397
T2 Kronecker 0.404 &lt; 0.412
T3 Copy-subject 0.310 « 0.337
T4 Copy-object 0.321 « 0.368
Human agreement 0.550
</table>
<tableCaption confidence="0.783523">
Difference between T2 and M2 is not s.s.
Table 4: Transitive version of M&amp;L task.
</tableCaption>
<page confidence="0.990789">
1598
</page>
<bodyText confidence="0.9999905">
the tensor-based ones. This is expected: composite
representations created by element-wise operations
are averages, and a prior step of disambiguation can
make a great difference.
From a task perspective, the effect of disambigua-
tion was much more definite in the phrase/sentence
similarity task. This observation is really interest-
ing, since the words of that dataset were not se-
lected in order to be ambiguous in any way. The
superior performance of the disambiguated models,
therefore, implies that the proposed methodology
can improve tasks based on phrase or sentence sim-
ilarity regardless of the level of ambiguity in the
vocabulary. For these cases, the proposed disam-
biguation algorithm acts as a fine-tuning process, the
outcome of which seems to be always positive; it
can only produce better composite representations,
not worse. In general, the positive effect of dis-
ambiguation in the phrase/sentence similarity task is
quite encouraging, especially given the fact that this
task constitutes a more appropriate test for evaluat-
ing compositional models, avoiding the pitfalls of
disambiguation-based experiments (as shortly dis-
cussed in Section 8.1).
For disambiguation-based tasks similar to those
of Section 8.1, the form of dataset is very important;
hence the inferior performance of disambiguated
models in the G&amp;S dataset, compared to the dataset
of Kartsaklis et al.. In fact, the G&amp;S dataset was
the only one where disambiguation was not helpful
for some cases (specifically, for vector mixtures and
the Kronecker model). We believe the reason behind
this lies in the fact that the automatic selection of
landmark verbs using the JCN metric (as done with
the G&amp;S dataset) was not very efficient for certain
cases. Note, for example, that the bare baseline of
comparing just ambiguous versions of verbs (with-
out any composition) in that dataset already achieves
a very high correlation of 0.198 with human judge-
ments (Table 1).7. This number is only 0.15 for the
Kartsaklis et al. dataset, due to the more efficient
verb selection procedure. In general, we consider
the results gained by this latter experiment more re-
liable for the specific task, the successful evaluation
of which requires genuinely ambiguous verbs.
The results are less conclusive for the second
question we posed in the beginning of this section,
regarding the comparison of the two classes of mod-
</bodyText>
<footnote confidence="0.6374695">
7The reported number for this baseline by Grefenstette and
Sadrzadeh (2011a) was 0.16 using vectors trained from BNC.
</footnote>
<bodyText confidence="0.999919035714286">
els. Despite the obvious benefits of the tensor-based
approaches, this work suggests for one more time
that vector mixture models might constitute a hard-
to-beat baseline; similar observations have been
made, for example, in the comparative study of Bla-
coe and Lapata (2012). However, when trying to in-
terpret the mixing results regarding the effectiveness
of the tensor-based models compared to vector mix-
tures, we need to take into account that the tensor-
based models tested in this work were all “hybrid”,
in the sense that they all involved some element
of point-wise operation; in other words, they con-
stituted a trade-off between transformational power
and complexity.
Even with this compromise, though, the study
presented in Section 8.2 implies that the effective-
ness of each method depends to some extent on the
length of the text segment: when more words are
involved, vector mixture models tend to be less ef-
fective; on the contrary, the performance of tensor-
based models seems to be proportional to the length
of the phrase or sentence—the more, the better.
These observations comply with the nature of the
approaches: “averaging” larger numbers of points
results in more general (hence less accurate) repre-
sentations; on the other hand, a larger number of
arguments makes a function (such as a verb) more
accurate.
</bodyText>
<sectionHeader confidence="0.952199" genericHeader="conclusions">
10 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999994615384616">
In the present paper we showed how to improve
a number of tensor-based compositional distribu-
tional models of meaning by introducing a step
of disambiguation prior to composition. Our sim-
ple algorithm (based on the procedure of Sch¨utze
(1998)) creates unambiguous versions of tensors be-
fore these are composed with vectors of nouns in
order to construct vectors for sentences and phrases.
This algorithm is quite generic, and can be applied
to any model that follows the tensor contraction pro-
cess described in Section 4. As for future work, we
aim to investigate the application of this procedure
to the regression model of Grefenstette et al. (2013).
</bodyText>
<sectionHeader confidence="0.997641" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999956">
We would like to thank Edward Grefenstette for his
comments on the first draft of this paper, as well as
the three anonymous reviewers for their fruitful sug-
gestions. Support by EPSRC grant EP/F042728/1 is
gratefully acknowledged by the authors.
</bodyText>
<page confidence="0.99705">
1599
</page>
<sectionHeader confidence="0.993881" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999900819148936">
Baroni, M. and Zamparelli, R. (2010). Nouns are
Vectors, Adjectives are Matrices. In Proceedings
of Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Blacoe, W. and Lapata, M. (2012). A comparison of
vector-based representations for semantic compo-
sition. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 546–556, Jeju Island, Korea. As-
sociation for Computational Linguistics.
Bourbaki, N. (1989). Commutative Algebra: Chap-
ters 1-7. Srpinger Verlag, Berlin/New York.
Cali´nski, T. and Harabasz, J. (1974). A Dendrite
Method for Cluster Analysis. Communications in
Statistics-Theory and Methods, 3(1):1–27.
Coecke, B., Sadrzadeh, M., and Clark, S.
(2010). Mathematical Foundations for Dis-
tributed Compositional Model of Meaning. Lam-
bek Festschrift. Linguistic Analysis, 36:345–384.
Curran, J. (2004). From Distributional to Seman-
tic Similarity. PhD thesis, School of Informatics,
University of Edinburgh.
Ferraresi, A., Zanchetta, E., Baroni, M., and Bernar-
dini, S. (2008). Introducing and evaluating
ukWaC, a very large web-derived corpus of En-
glish. In Proceedings of the 4th Web as Corpus
Workshop (WAC-4) Can we beat Google, pages
47–54.
Grefenstette, E., Dinu, G., Zhang, Y.-Z., Sadrzadeh,
M., and Baroni, M. (2013). Multi-step regres-
sion learning for compositional distributional se-
mantics. In Proceedings of the 10th International
Conference on Computational Semantics (IWCS
2013).
Grefenstette, E. and Sadrzadeh, M. (2011a). Exper-
imental Support for a Categorical Compositional
Distributional Model of Meaning. In Proceedings
of Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Grefenstette, E. and Sadrzadeh, M. (2011b). Exper-
imenting with Transitive Verbs in a DisCoCat. In
Proceedings of Workshop on Geometrical Models
of Natural Language Semantics (GEMS).
Harris, Z. (1968). Mathematical Structures of Lan-
guage. Wiley.
Jiang, J. and Conrath, D. (1997). Semantic similar-
ity based on corpus statistics and lexical taxon-
omy. In Proceedings on International Conference
on Research in Computational Linguistics, pages
19–33, Taiwan.
Kartsaklis, D., Sadrzadeh, M., and Pulman, S.
(2012). A unified sentence space for categori-
cal distributional-compositional semantics: The-
ory and experiments. In Proceedings of 24th
International Conference on Computational Lin-
guistics (COLING 2012): Posters, pages 549–
558, Mumbai, India. The COLING 2012 Orga-
nizing Committee.
Kartsaklis, D., Sadrzadeh, M., and Pulman, S.
(2013). Separating Disambiguation from Com-
position in Distributional Semantics. In Proceed-
ings of 17th Conference on Computational Nat-
ural Language Learning (CoNLL-2013), Sofia,
Bulgaria.
Kintsch, W. (2001). Predication. Cognitive Science,
25(2):173–202.
Landauer, T. and Dumais, S. (1997). A Solution to
Plato’s Problem: The Latent Semantic Analysis
Theory of Acquision, Induction, and Representa-
tion of Knowledge. Psychological Review.
Manandhar, S., Klapaftis, I., Dligach, D., and Prad-
han, S. (2010). Semeval-2010 task 14: Word
sense induction &amp; disambiguation. In Proceed-
ings of the 5th International Workshop on Se-
mantic Evaluation, pages 63–68. Association for
Computational Linguistics.
Manning, C., Raghavan, P., and Sch¨utze, H. (2008).
Introduction to Information Retrieval. Cambridge
University Press.
Mitchell, J. and Lapata, M. (2008). Vector-based
Models of Semantic Composition. In Proceedings
of the 46th Annual Meeting of the Association for
Computational Linguistics, pages 236–244.
Mitchell, J. and Lapata, M. (2010). Composition
in distributional models of semantics. Cognitive
Science, 34(8):1388–1439.
M¨ullner, D. (2013). fastcluster: Fast Hierarchical
Clustering Routines for R and Python. Journal of
Statistical Software, 9(53):1–18.
Pickering, M. and Frisson, S. (2001). Processing
ambiguous verbs: Evidence from eye movements.
Journal of Experimental Psychology: Learning,
Memory, and Cognition, 27(2):556.
</reference>
<page confidence="0.709958">
1600
</page>
<reference confidence="0.99986285">
Pulman, S. (2013). Combining Compositional and
Distributional Models of Semantics. In Heunen,
C., Sadrzadeh, M., and Grefenstette, E., editors,
Quantum Physics and Linguistics: A Composi-
tional, Diagrammatic Discourse. Oxford Univer-
sity Press.
Reddy, S., Klapaftis, I., McCarthy, D., and Man-
andhar, S. (2011). Dynamic and static prototype
vectors for semantic composition. In Proceedings
of 5th International Joint Conference on Natural
Language Processing, pages 705–713.
Rosenberg, A. and Hirschberg, J. (2007). V-
measure: A conditional entropy-based external
cluster evaluation measure. In Proceedings of the
2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 410–420.
Sch¨utze, H. (1998). Automatic Word Sense Dis-
crimination. Computational Linguistics, 24:97–
123.
</reference>
<page confidence="0.99167">
1601
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.084043">
<title confidence="0.998305">Prior Disambiguation of Word for Constructing Sentence Vectors</title>
<author confidence="0.998435">Dimitri</author>
<affiliation confidence="0.995411333333333">University of Department Computer</affiliation>
<address confidence="0.901524">Wolfson Building, Parks Oxford, OX1 3QD,</address>
<email confidence="0.997278">dimitri.kartsaklis@cs.ox.ac.uk</email>
<title confidence="0.42951125">Mehrnoosh Queen Mary University of School of Electronic and Computer</title>
<author confidence="0.91374">Mile End</author>
<affiliation confidence="0.556773">London, E1 4NS,</affiliation>
<email confidence="0.997608">mehrs@eecs.qmul.ac.uk</email>
<abstract confidence="0.998210666666667">Recent work has shown that compositionaldistributional models using element-wise op-</abstract>
<intro confidence="0.58676">erations on contextual word vectors benefit</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>R Zamparelli</author>
</authors>
<title>Nouns are Vectors, Adjectives are Matrices.</title>
<date>2010</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="4145" citStr="Baroni and Zamparelli (2010)" startWordPosition="644" endWordPosition="647">ifying, a number of arguments rather than lexical units of the same order as them; an adjective, for example, is a function that returns a modified version of its input noun. Inspired from 1590 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1590–1601, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics this more-aligned-to-formal-semantics view, a second research direction aims to represent relational words as linear maps (tensors of various orders) that can be applied to one or more arguments (vectors). Baroni and Zamparelli (2010), for example, model adjectives as matrices which, when matrixmultiplied with a noun vector, will produce a vectorial representation of the specific adjective-noun compound. The notion of a framework where relational words are entities living in vector spaces of higher order than nouns, which are simple vectors, has been formalized by Coecke et al. (2010) in the context of the abstract mathematical framework of compact closed categories. We refer to this class of models as tensor-based. Regardless of the way they approach the representation of relational words and their composition operation, </context>
<context position="14233" citStr="Baroni and Zamparelli (2010)" startWordPosition="2377" endWordPosition="2380">rther interaction of this result with the subject will return a vector for the whole transitive sentence living in RJ. We should note that the order in which the verb is applied to its arguments is not important; so in general the meaning of a transitive sentence is given by: (V x O)T x S = (VT x S) x O (2) where T denotes a transpose and makes indices match, since subject precedes the verb. 5 Creating verb tensors In this section we review a number of proposals regarding concrete methods of constructing tensors for relational words in the context of the frameworks of Coecke et al. (2010) and Baroni and Zamparelli (2010), which both comply to the setting of Section 4.2 Relational Following ideas from the set-theoretic view of formal semantics, Grefenstette and Sadrzadeh (2011a) suggest that the meaning of a relational word should be represented as the sum of its arguments. The meaning of adjective ‘red’, for example, becomes the sum of the vectors of all the nouns that ‘red’ modifies in the corpus; so red= � −� i −−−� nouni, where i iterates through all the occurrences of ‘red’. This can be generalised to relational words of any arity, by summing the tensor product of their arguments. So for a transitive verb</context>
<context position="17649" citStr="Baroni and Zamparelli (2010)" startWordPosition="2988" endWordPosition="2991"> the matrix of Equation 3 to a tensor of order 3 (as required by the type-logical identities) by copying one of the existing dimensions. When the dimension of rows (corresponding to subjects) is copied, the calculation of a vector for a transitive sentence becomes: −−−−−−−−−� 2 subj verb obj = subj O (verb x −� −−� obj) (6) 1593 Copying the column dimension (objects) gives: � ) � 2)T � subj verb obj = obj O (verb) x subj (7) Linear regression None of the above models create tensors that are fully populated: one or more dimensions will always remain empty. Following an idea first introduced by Baroni and Zamparelli (2010) for the creation of adjective matrices, Grefenstette et al. (2013) use linear regression in order to learn full tensors of order 3 for transitive verbs. Linear regression is a supervised method of learning, so it needs a number of exemplar data points. In the case of the adjective ‘red’, for example, we would need a set of the form (car, red car), (h , red shirt), (��� ������ shoe, ������ red shoe) and so on, where the second vector in each pair is the contextual vector of the whole phrase created exactly as if it were a single word. The goal of the learning process is to find the parameters </context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Baroni, M. and Zamparelli, R. (2010). Nouns are Vectors, Adjectives are Matrices. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Blacoe</author>
<author>M Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>546--556</pages>
<institution>Jeju Island, Korea. Association for Computational Linguistics.</institution>
<contexts>
<context position="42734" citStr="Blacoe and Lapata (2012)" startWordPosition="7227" endWordPosition="7231">ecific task, the successful evaluation of which requires genuinely ambiguous verbs. The results are less conclusive for the second question we posed in the beginning of this section, regarding the comparison of the two classes of mod7The reported number for this baseline by Grefenstette and Sadrzadeh (2011a) was 0.16 using vectors trained from BNC. els. Despite the obvious benefits of the tensor-based approaches, this work suggests for one more time that vector mixture models might constitute a hardto-beat baseline; similar observations have been made, for example, in the comparative study of Blacoe and Lapata (2012). However, when trying to interpret the mixing results regarding the effectiveness of the tensor-based models compared to vector mixtures, we need to take into account that the tensorbased models tested in this work were all “hybrid”, in the sense that they all involved some element of point-wise operation; in other words, they constituted a trade-off between transformational power and complexity. Even with this compromise, though, the study presented in Section 8.2 implies that the effectiveness of each method depends to some extent on the length of the text segment: when more words are invol</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>Blacoe, W. and Lapata, M. (2012). A comparison of vector-based representations for semantic composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546–556, Jeju Island, Korea. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bourbaki</author>
</authors>
<title>Commutative Algebra: Chapters 1-7. Srpinger Verlag,</title>
<date>1989</date>
<location>Berlin/New York.</location>
<contexts>
<context position="11927" citStr="Bourbaki, 1989" startWordPosition="1921" endWordPosition="1922">ased models, the principles of which are shortly discussed in the next section. 4 Tensors as multilinear maps A tensor is a geometric object that can be seen as the generalization of the familiar notion of a vector in higher dimensions. The order of a tensor is the number of its dimensions; in other words, the number of indices we need to fully describe a random element of the tensor. Hence, a vector is a tensor of order 1, a matrix is a tensor of order 2, and so on. Tensors and multilinear maps stand in one-to-one correspondence, as stated by the following well-known “map-state” isomorphism (Bourbaki, 1989): f : V1 → ... → V3 → Vk ∼� Vk⊗V3⊗...⊗V1 (1) This offers an elegant way to adopt a formal semantics view of natural language in vector spaces. Let nouns live in a basic vector space N ∈ RD; returning to our previous example, an adjective then can be seen as a map f : N → N which is isomorphic to N ⊗ N (that is, to a matrix). In general, the order of the tensor is equal to the number of arguments plus one dimension that carries the result; so a unary function (e.g. adjectives, intransitive verbs) is represented by a tensor of order 2 (a matrix), a binary function (e.g. a transitive verb) as an </context>
</contexts>
<marker>Bourbaki, 1989</marker>
<rawString>Bourbaki, N. (1989). Commutative Algebra: Chapters 1-7. Srpinger Verlag, Berlin/New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cali´nski</author>
<author>J Harabasz</author>
</authors>
<title>A Dendrite Method for Cluster Analysis.</title>
<date>1974</date>
<booktitle>Communications in Statistics-Theory and Methods,</booktitle>
<pages>3--1</pages>
<marker>Cali´nski, Harabasz, 1974</marker>
<rawString>Cali´nski, T. and Harabasz, J. (1974). A Dendrite Method for Cluster Analysis. Communications in Statistics-Theory and Methods, 3(1):1–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Coecke</author>
<author>M Sadrzadeh</author>
<author>S Clark</author>
</authors>
<title>Mathematical Foundations for Distributed Compositional Model of Meaning. Lambek Festschrift. Linguistic Analysis,</title>
<date>2010</date>
<pages>36--345</pages>
<contexts>
<context position="4502" citStr="Coecke et al. (2010)" startWordPosition="701" endWordPosition="704">or Computational Linguistics this more-aligned-to-formal-semantics view, a second research direction aims to represent relational words as linear maps (tensors of various orders) that can be applied to one or more arguments (vectors). Baroni and Zamparelli (2010), for example, model adjectives as matrices which, when matrixmultiplied with a noun vector, will produce a vectorial representation of the specific adjective-noun compound. The notion of a framework where relational words are entities living in vector spaces of higher order than nouns, which are simple vectors, has been formalized by Coecke et al. (2010) in the context of the abstract mathematical framework of compact closed categories. We refer to this class of models as tensor-based. Regardless of the way they approach the representation of relational words and their composition operation, however, most current compositionaldistributional models do share a common feature: they all rely on ambiguous vector representations, where all the senses of a polysemous word, such as the verb ‘file’ (which can mean register or smooth), are merged into the same vector or tensor. At least for the vector mixture approach, this practice has been proved sub</context>
<context position="14200" citStr="Coecke et al. (2010)" startWordPosition="2372" endWordPosition="2375">xJ (i.e. a matrix)1; a further interaction of this result with the subject will return a vector for the whole transitive sentence living in RJ. We should note that the order in which the verb is applied to its arguments is not important; so in general the meaning of a transitive sentence is given by: (V x O)T x S = (VT x S) x O (2) where T denotes a transpose and makes indices match, since subject precedes the verb. 5 Creating verb tensors In this section we review a number of proposals regarding concrete methods of constructing tensors for relational words in the context of the frameworks of Coecke et al. (2010) and Baroni and Zamparelli (2010), which both comply to the setting of Section 4.2 Relational Following ideas from the set-theoretic view of formal semantics, Grefenstette and Sadrzadeh (2011a) suggest that the meaning of a relational word should be represented as the sum of its arguments. The meaning of adjective ‘red’, for example, becomes the sum of the vectors of all the nouns that ‘red’ modifies in the corpus; so red= � −� i −−−� nouni, where i iterates through all the occurrences of ‘red’. This can be generalised to relational words of any arity, by summing the tensor product of their ar</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Coecke, B., Sadrzadeh, M., and Clark, S. (2010). Mathematical Foundations for Distributed Compositional Model of Meaning. Lambek Festschrift. Linguistic Analysis, 36:345–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>PhD thesis,</tech>
<institution>School of Informatics, University of Edinburgh.</institution>
<contexts>
<context position="1397" citStr="Curran, 2004" startWordPosition="202" endWordPosition="203">ments (vectors). We propose disambiguation algorithms for a number of tensor-based models, which we then test on a variety of tasks. The results show that disambiguation can provide better compositional representation even for the case of tensor-based models. Furthermore, we confirm previous findings regarding the positive effect of disambiguation on vector mixture models, and we compare the effectiveness of the two approaches. 1 Introduction Distributional models of meaning have been proved extremely useful for a number of natural language processing tasks, ranging from thesaurus extraction (Curran, 2004) to topic modelling (Landauer and Dumais, 1997) and information retrieval (Manning et al., 2008), to name just a few. These models are based on the distributional hypothesis of Harris (1968), which states that the meaning of a word depends on its context. This idea allows the words to be represented by vectors of statistics collected from a sufficiently large corpus of text; each element of the vector reflects how many times a word co-occurs in the same context with another word of the vocabulary. However, due to the generative power of natural language, which is able to produce infinite new s</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>Curran, J. (2004). From Distributional to Semantic Similarity. PhD thesis, School of Informatics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ferraresi</author>
<author>E Zanchetta</author>
<author>M Baroni</author>
<author>S Bernardini</author>
</authors>
<title>Introducing and evaluating ukWaC, a very large web-derived corpus of English.</title>
<date>2008</date>
<booktitle>In Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google,</booktitle>
<pages>47--54</pages>
<contexts>
<context position="25049" citStr="Ferraresi et al., 2008" startWordPosition="4321" endWordPosition="4324">rs, which will correspond to the different senses of the verb. Generalization to higher arities is a straightforward extension of the step-wise process in Section 5 for transitive verbs. 8 Experiments In this section we will test the effect of disambiguation on the models of Section 5 in a variety of tasks. Due to the significant methodological differences of the linear regression model from the other approaches and the variety of its set of parameters, we decided that it would be better if this was left as the subject of a distinct work. Experimental setting We train our vectors using ukWaC (Ferraresi et al., 2008), a corpus of English text with 2 billion words (100m sentences). We use 2000 dimensions, with weights calculated as the ratio of the probability of the context word given the target word to the probability of the context word overall. The context here is a 5-word window on both sides of the target word. The vectors are disambiguated both syntactically and semantically: first, separate vectors have been created for different syntactic usages of the same word in the corpus; for example, the word ‘book’ has two vectors, one for its noun sense and one for its verb sense. Furthermore, each word is</context>
</contexts>
<marker>Ferraresi, Zanchetta, Baroni, Bernardini, 2008</marker>
<rawString>Ferraresi, A., Zanchetta, E., Baroni, M., and Bernardini, S. (2008). Introducing and evaluating ukWaC, a very large web-derived corpus of English. In Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google, pages 47–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Grefenstette</author>
<author>G Dinu</author>
<author>Y-Z Zhang</author>
<author>M Sadrzadeh</author>
<author>M Baroni</author>
</authors>
<title>Multi-step regression learning for compositional distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics (IWCS</booktitle>
<contexts>
<context position="17716" citStr="Grefenstette et al. (2013)" startWordPosition="2998" endWordPosition="3002">type-logical identities) by copying one of the existing dimensions. When the dimension of rows (corresponding to subjects) is copied, the calculation of a vector for a transitive sentence becomes: −−−−−−−−−� 2 subj verb obj = subj O (verb x −� −−� obj) (6) 1593 Copying the column dimension (objects) gives: � ) � 2)T � subj verb obj = obj O (verb) x subj (7) Linear regression None of the above models create tensors that are fully populated: one or more dimensions will always remain empty. Following an idea first introduced by Baroni and Zamparelli (2010) for the creation of adjective matrices, Grefenstette et al. (2013) use linear regression in order to learn full tensors of order 3 for transitive verbs. Linear regression is a supervised method of learning, so it needs a number of exemplar data points. In the case of the adjective ‘red’, for example, we would need a set of the form (car, red car), (h , red shirt), (��� ������ shoe, ������ red shoe) and so on, where the second vector in each pair is the contextual vector of the whole phrase created exactly as if it were a single word. The goal of the learning process is to find the parameters adj2 and ��b such that: adj noun � adj2 x ���� ������� noun + b (8)</context>
</contexts>
<marker>Grefenstette, Dinu, Zhang, Sadrzadeh, Baroni, 2013</marker>
<rawString>Grefenstette, E., Dinu, G., Zhang, Y.-Z., Sadrzadeh, M., and Baroni, M. (2013). Multi-step regression learning for compositional distributional semantics. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Grefenstette</author>
<author>M Sadrzadeh</author>
</authors>
<title>Experimental Support for a Categorical Compositional Distributional Model of Meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="14391" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="2400" endWordPosition="2403">ich the verb is applied to its arguments is not important; so in general the meaning of a transitive sentence is given by: (V x O)T x S = (VT x S) x O (2) where T denotes a transpose and makes indices match, since subject precedes the verb. 5 Creating verb tensors In this section we review a number of proposals regarding concrete methods of constructing tensors for relational words in the context of the frameworks of Coecke et al. (2010) and Baroni and Zamparelli (2010), which both comply to the setting of Section 4.2 Relational Following ideas from the set-theoretic view of formal semantics, Grefenstette and Sadrzadeh (2011a) suggest that the meaning of a relational word should be represented as the sum of its arguments. The meaning of adjective ‘red’, for example, becomes the sum of the vectors of all the nouns that ‘red’ modifies in the corpus; so red= � −� i −−−� nouni, where i iterates through all the occurrences of ‘red’. This can be generalised to relational words of any arity, by summing the tensor product of their arguments. So for a transitive verb we have: �verb2 = (subji ® obji) (3) i 1The symbol x denotes tensor contraction. 2In what follows we use the case of a transitive verb as an example; however</context>
<context position="16265" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="2741" endWordPosition="2744">he transitive case this increases the order of the verb tensor to 4 (2 dimensions for the arguments plus another 2 for the result). In spite of this, however, note that the method of Equation 3 produces a matrix. The other two dimensions of the tensor remain empty (filled with zeros), a fact that simplifies the calculations but also considerably weakens the expressive power of the model. This simplification transforms Equation 2 to the following: 2 � � 2 subj verb obj = (subj ® obj) O verb (4) where ® denotes the tensor product and O elementwise multiplication. Kronecker In a subsequent work (Grefenstette and Sadrzadeh, 2011b), the same team proposes the creation of a verb matrix as the Kronecker product of the verb’s contextual vector with itself: 2 verb = −−� verb ® −−� verb (5) Again in this model the sentence space is of order 2, and the meaning of a transitive sentence is calculated using Equation 4. Frobenius The previous models bring the important limitation that only sentences of the same structure can be meaningfully compared; it is not possible, for example, to compare an intransitive sentence (e.g. ‘kids play’) with a transitive one (‘children play football’), since the former is a vector and the latte</context>
<context position="28096" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="4828" endWordPosition="4831">an ambiguous verb such as ‘file’, the goal is to find out to what extent the presence of an appropriate context will disambiguate its intended meaning. The context (e.g. a subject/object pair) is composed with two landmark verbs corresponding to the different senses (‘smooth’ and ‘register’) to create simple sentences. The assumption is that a good compositional model should be able to reflect that ‘woman files application’ is closer to ‘woman registers application’ than to ‘woman smooths application’. In this paper we test our models on two different datasets of transitive sentences, that of Grefenstette and Sadrzadeh (2011a) and Kartsaklis et al. (2013)4. Specific details about the creation of the datasets can be found in the above papers; for the purposes of the current work it is sufficient to mention that their main difference is that in the former the verbs and their alternative meanings have been selected automatically using the JCN metric of semantic similarity (Jiang and Conrath, 1997), while in the latter the selection was based on human judgements from the work of Pickering and Frisson (2001). So, while 4This dataset has been created by Mehrnoosh Sadrzadeh in collaboration with Edward Grefenstette, but</context>
<context position="42417" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="7178" endWordPosition="7181">t any composition) in that dataset already achieves a very high correlation of 0.198 with human judgements (Table 1).7. This number is only 0.15 for the Kartsaklis et al. dataset, due to the more efficient verb selection procedure. In general, we consider the results gained by this latter experiment more reliable for the specific task, the successful evaluation of which requires genuinely ambiguous verbs. The results are less conclusive for the second question we posed in the beginning of this section, regarding the comparison of the two classes of mod7The reported number for this baseline by Grefenstette and Sadrzadeh (2011a) was 0.16 using vectors trained from BNC. els. Despite the obvious benefits of the tensor-based approaches, this work suggests for one more time that vector mixture models might constitute a hardto-beat baseline; similar observations have been made, for example, in the comparative study of Blacoe and Lapata (2012). However, when trying to interpret the mixing results regarding the effectiveness of the tensor-based models compared to vector mixtures, we need to take into account that the tensorbased models tested in this work were all “hybrid”, in the sense that they all involved some element</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Grefenstette, E. and Sadrzadeh, M. (2011a). Experimental Support for a Categorical Compositional Distributional Model of Meaning. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Grefenstette</author>
<author>M Sadrzadeh</author>
</authors>
<title>Experimenting with Transitive Verbs in a DisCoCat.</title>
<date>2011</date>
<booktitle>In Proceedings of Workshop on Geometrical Models of Natural Language Semantics (GEMS).</booktitle>
<contexts>
<context position="14391" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="2400" endWordPosition="2403">ich the verb is applied to its arguments is not important; so in general the meaning of a transitive sentence is given by: (V x O)T x S = (VT x S) x O (2) where T denotes a transpose and makes indices match, since subject precedes the verb. 5 Creating verb tensors In this section we review a number of proposals regarding concrete methods of constructing tensors for relational words in the context of the frameworks of Coecke et al. (2010) and Baroni and Zamparelli (2010), which both comply to the setting of Section 4.2 Relational Following ideas from the set-theoretic view of formal semantics, Grefenstette and Sadrzadeh (2011a) suggest that the meaning of a relational word should be represented as the sum of its arguments. The meaning of adjective ‘red’, for example, becomes the sum of the vectors of all the nouns that ‘red’ modifies in the corpus; so red= � −� i −−−� nouni, where i iterates through all the occurrences of ‘red’. This can be generalised to relational words of any arity, by summing the tensor product of their arguments. So for a transitive verb we have: �verb2 = (subji ® obji) (3) i 1The symbol x denotes tensor contraction. 2In what follows we use the case of a transitive verb as an example; however</context>
<context position="16265" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="2741" endWordPosition="2744">he transitive case this increases the order of the verb tensor to 4 (2 dimensions for the arguments plus another 2 for the result). In spite of this, however, note that the method of Equation 3 produces a matrix. The other two dimensions of the tensor remain empty (filled with zeros), a fact that simplifies the calculations but also considerably weakens the expressive power of the model. This simplification transforms Equation 2 to the following: 2 � � 2 subj verb obj = (subj ® obj) O verb (4) where ® denotes the tensor product and O elementwise multiplication. Kronecker In a subsequent work (Grefenstette and Sadrzadeh, 2011b), the same team proposes the creation of a verb matrix as the Kronecker product of the verb’s contextual vector with itself: 2 verb = −−� verb ® −−� verb (5) Again in this model the sentence space is of order 2, and the meaning of a transitive sentence is calculated using Equation 4. Frobenius The previous models bring the important limitation that only sentences of the same structure can be meaningfully compared; it is not possible, for example, to compare an intransitive sentence (e.g. ‘kids play’) with a transitive one (‘children play football’), since the former is a vector and the latte</context>
<context position="28096" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="4828" endWordPosition="4831">an ambiguous verb such as ‘file’, the goal is to find out to what extent the presence of an appropriate context will disambiguate its intended meaning. The context (e.g. a subject/object pair) is composed with two landmark verbs corresponding to the different senses (‘smooth’ and ‘register’) to create simple sentences. The assumption is that a good compositional model should be able to reflect that ‘woman files application’ is closer to ‘woman registers application’ than to ‘woman smooths application’. In this paper we test our models on two different datasets of transitive sentences, that of Grefenstette and Sadrzadeh (2011a) and Kartsaklis et al. (2013)4. Specific details about the creation of the datasets can be found in the above papers; for the purposes of the current work it is sufficient to mention that their main difference is that in the former the verbs and their alternative meanings have been selected automatically using the JCN metric of semantic similarity (Jiang and Conrath, 1997), while in the latter the selection was based on human judgements from the work of Pickering and Frisson (2001). So, while 4This dataset has been created by Mehrnoosh Sadrzadeh in collaboration with Edward Grefenstette, but</context>
<context position="42417" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="7178" endWordPosition="7181">t any composition) in that dataset already achieves a very high correlation of 0.198 with human judgements (Table 1).7. This number is only 0.15 for the Kartsaklis et al. dataset, due to the more efficient verb selection procedure. In general, we consider the results gained by this latter experiment more reliable for the specific task, the successful evaluation of which requires genuinely ambiguous verbs. The results are less conclusive for the second question we posed in the beginning of this section, regarding the comparison of the two classes of mod7The reported number for this baseline by Grefenstette and Sadrzadeh (2011a) was 0.16 using vectors trained from BNC. els. Despite the obvious benefits of the tensor-based approaches, this work suggests for one more time that vector mixture models might constitute a hardto-beat baseline; similar observations have been made, for example, in the comparative study of Blacoe and Lapata (2012). However, when trying to interpret the mixing results regarding the effectiveness of the tensor-based models compared to vector mixtures, we need to take into account that the tensorbased models tested in this work were all “hybrid”, in the sense that they all involved some element</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Grefenstette, E. and Sadrzadeh, M. (2011b). Experimenting with Transitive Verbs in a DisCoCat. In Proceedings of Workshop on Geometrical Models of Natural Language Semantics (GEMS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structures of Language.</booktitle>
<publisher>Wiley.</publisher>
<contexts>
<context position="1587" citStr="Harris (1968)" startWordPosition="232" endWordPosition="234">ompositional representation even for the case of tensor-based models. Furthermore, we confirm previous findings regarding the positive effect of disambiguation on vector mixture models, and we compare the effectiveness of the two approaches. 1 Introduction Distributional models of meaning have been proved extremely useful for a number of natural language processing tasks, ranging from thesaurus extraction (Curran, 2004) to topic modelling (Landauer and Dumais, 1997) and information retrieval (Manning et al., 2008), to name just a few. These models are based on the distributional hypothesis of Harris (1968), which states that the meaning of a word depends on its context. This idea allows the words to be represented by vectors of statistics collected from a sufficiently large corpus of text; each element of the vector reflects how many times a word co-occurs in the same context with another word of the vocabulary. However, due to the generative power of natural language, which is able to produce infinite new structures from a finite set of resources (words), no text corpus, regardless of its size, can provide reliable distributional representations for anything longer than single words or perhaps</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Harris, Z. (1968). Mathematical Structures of Language. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jiang</author>
<author>D Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings on International Conference on Research in Computational Linguistics,</booktitle>
<pages>pages</pages>
<contexts>
<context position="28473" citStr="Jiang and Conrath, 1997" startWordPosition="4892" endWordPosition="4895"> to reflect that ‘woman files application’ is closer to ‘woman registers application’ than to ‘woman smooths application’. In this paper we test our models on two different datasets of transitive sentences, that of Grefenstette and Sadrzadeh (2011a) and Kartsaklis et al. (2013)4. Specific details about the creation of the datasets can be found in the above papers; for the purposes of the current work it is sufficient to mention that their main difference is that in the former the verbs and their alternative meanings have been selected automatically using the JCN metric of semantic similarity (Jiang and Conrath, 1997), while in the latter the selection was based on human judgements from the work of Pickering and Frisson (2001). So, while 4This dataset has been created by Mehrnoosh Sadrzadeh in collaboration with Edward Grefenstette, but remained unpublished until (Kartsaklis et al., 2013). in the first dataset many verbs cannot be considered as genuinely ambiguous (e.g. ‘say’ with meanings state and allege or ‘write’ with meanings publish and spell), the landmarks in the second dataset correspond to clearly separated senses (e.g. ‘file’ with meanings register and smooth or ‘charge’ with meanings accuse and</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jiang, J. and Conrath, D. (1997). Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings on International Conference on Research in Computational Linguistics, pages 19–33, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kartsaklis</author>
<author>M Sadrzadeh</author>
<author>S Pulman</author>
</authors>
<title>A unified sentence space for categorical distributional-compositional semantics: Theory and experiments.</title>
<date>2012</date>
<booktitle>In Proceedings of 24th International Conference on Computational Linguistics (COLING 2012): Posters,</booktitle>
<pages>549--558</pages>
<location>Mumbai,</location>
<contexts>
<context position="16927" citStr="Kartsaklis et al. (2012)" startWordPosition="2858" endWordPosition="2862">n of a verb matrix as the Kronecker product of the verb’s contextual vector with itself: 2 verb = −−� verb ® −−� verb (5) Again in this model the sentence space is of order 2, and the meaning of a transitive sentence is calculated using Equation 4. Frobenius The previous models bring the important limitation that only sentences of the same structure can be meaningfully compared; it is not possible, for example, to compare an intransitive sentence (e.g. ‘kids play’) with a transitive one (‘children play football’), since the former is a vector and the latter a matrix. Using Frobenius algebras, Kartsaklis et al. (2012) provide a unified sentence space for every sentence regardless of its type. These models turn the matrix of Equation 3 to a tensor of order 3 (as required by the type-logical identities) by copying one of the existing dimensions. When the dimension of rows (corresponding to subjects) is copied, the calculation of a vector for a transitive sentence becomes: −−−−−−−−−� 2 subj verb obj = subj O (verb x −� −−� obj) (6) 1593 Copying the column dimension (objects) gives: � ) � 2)T � subj verb obj = obj O (verb) x subj (7) Linear regression None of the above models create tensors that are fully popu</context>
<context position="30838" citStr="Kartsaklis et al., 2012" startWordPosition="5276" endWordPosition="5279">s to a non-compositional evaluation, where the similarity between two sentences is solely based on the distance between the two verbs, without applying any compositional step with subject and object. The tensor-based models present much better performance than the vector mixture ones, with the disambiguated version of the copy-object model significantly higher than the relational model. By design, the copy-object model retains more information about the objects; so this result confirms previous findings, that in this certain dataset the role of objects is more important than that of subjects (Kartsaklis et al., 2012). In general, the disambiguation step improves the results of all the tensor-based models except Kronecker; the effect is reversed for the vector mixture models, where the disambiguated versions present much worse performance (these 5For all tables in this section, « and » denote highly statistically significant differences with P &lt; 0.001. 1596 Model Ambig. Disamb. BL Verbs only 0.198 » 0.132 M1 Multiplicative 0.137 » 0.044 M2 Additive 0.127 » 0.047 T1 Relational 0.219 &lt; 0.223 T2 Kronecker 0.207 » 0.061 T3 Copy-subject 0.070 « 0.122 T4 Copy-object 0.241 « 0.262 Human agreement 0.599 Difference</context>
</contexts>
<marker>Kartsaklis, Sadrzadeh, Pulman, 2012</marker>
<rawString>Kartsaklis, D., Sadrzadeh, M., and Pulman, S. (2012). A unified sentence space for categorical distributional-compositional semantics: Theory and experiments. In Proceedings of 24th International Conference on Computational Linguistics (COLING 2012): Posters, pages 549– 558, Mumbai, India. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kartsaklis</author>
<author>M Sadrzadeh</author>
<author>S Pulman</author>
</authors>
<title>Separating Disambiguation from Composition in Distributional Semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of 17th Conference on Computational Natural Language Learning (CoNLL-2013),</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="5159" citStr="Kartsaklis et al. (2013)" startWordPosition="805" endWordPosition="808">athematical framework of compact closed categories. We refer to this class of models as tensor-based. Regardless of the way they approach the representation of relational words and their composition operation, however, most current compositionaldistributional models do share a common feature: they all rely on ambiguous vector representations, where all the senses of a polysemous word, such as the verb ‘file’ (which can mean register or smooth), are merged into the same vector or tensor. At least for the vector mixture approach, this practice has been proved suboptimal: Reddy et al. (2011) and Kartsaklis et al. (2013) test a number of simple multiplicative and additive models using disambiguated vector representations on various tasks, showing that the introduction of a disambiguation step prior to actual composition can indeed increase the quality of the composite vectors. However, the fact that disambiguation can be beneficial for models based on vector mixtures is not very surprising. Both additive and multiplicative compositions are but a kind of average of the vectors of the words in the sentence, hence can directly benefit from the provision of more accurate starting points. Perhaps a more interestin</context>
<context position="9229" citStr="Kartsaklis et al. (2013)" startWordPosition="1464" endWordPosition="1467">eatures are also taken into account, offering (in principle) more power. Furthermore, by design, the bag-of-words problem is not present here. Overall, tensor-based models offer a more complete and linguistically motivated solution to the problem of composition. For example, one can consider building linear maps for prepositions and logical words, rather than treating them as noise and discard them, as commonly done in the vector mixture models. 3 Disambiguation in vector mixtures For a compositional model based on vector mixtures, polysemy of words can be a critical factor. Pulman (2013) and Kartsaklis et al. (2013) point out that the element-wise combination of “ambiguous” vectors produces results that are hard to interpret; the composed vector is not a purely compositional representation but a product of two tasks that take place in parallel: composition and some amount of disambiguation that emerges as a side-effect of the compositional process, leaving the resulting vector in an intermediate state. This effect is demonstrated in Figure 2, which shows the composition of the ambiguous verb ‘run’ (with meanings moving fast and dissolving) with the subject ‘horse’. The first three components of our toy v</context>
<context position="11119" citStr="Kartsaklis et al. (2013)" startWordPosition="1777" endWordPosition="1780">ple. The left part of Figure 2 shows what happens when the ambiguous ‘run’ vector is used; the multiplication with the ‘horse’ vector will produce an impure result, half affected by composition and half by disambiguation. However, what we really want is a vector where all the dissolving-related components will be eliminated, since they are irrelevant to the way the word ‘run’ is used in the sentence. In order to achieve this, we have to introduce a disambiguation step prior to composition (right part of Figure 2). These ideas are experimentally verified in the works of Reddy et al. (2011) and Kartsaklis et al. (2013); Pulman (2013) also presents a comprehensive analysis of the problem. What remains to be seen is if disambiguation can also provide benefits for the linguistically motivated setting of tensorbased models, the principles of which are shortly discussed in the next section. 4 Tensors as multilinear maps A tensor is a geometric object that can be seen as the generalization of the familiar notion of a vector in higher dimensions. The order of a tensor is the number of its dimensions; in other words, the number of indices we need to fully describe a random element of the tensor. Hence, a vector is </context>
<context position="21276" citStr="Kartsaklis et al. (2013)" startWordPosition="3671" endWordPosition="3674">d S is a set of sense vectors (centroids of context vector clusters) produced by the above procedure. The disambiguation of a new word w under a context C can now be accomplished as follows: we create a context vector c for C as above, and we compare it with every sense vector of w; the word is assigned to the sense corresponding to the closest sense vector. Specifically, if S,,, is the set of sense vectors for w, *�c the context vector for C, and d(v , u ) our vector distance measure, the preferred sense s� is given by: d(-V-,+, - ) (9) For the actual clustering step we follow the setting of Kartsaklis et al. (2013), which worked well in tasks very similar to ours. Specifically, we perform hierarchical agglomerative clustering (HAC) using Ward’s method as the inter-cluster distance, while the distance between vectors is measured with Pearson’s correlation.3 In the above work, this configuration has been found to return the highest V-measure (Rosenberg and Hirschberg, 2007) on the noun set of SEMEVAL 2010 Word Sense Induction &amp; Disambiguation Task (Manandhar et al., 2010). As context for a word, we consider the sentence in which this word occurs. The output of HAC is a dendrogram embedding all the possibl</context>
<context position="28127" citStr="Kartsaklis et al. (2013)" startWordPosition="4833" endWordPosition="4836">oal is to find out to what extent the presence of an appropriate context will disambiguate its intended meaning. The context (e.g. a subject/object pair) is composed with two landmark verbs corresponding to the different senses (‘smooth’ and ‘register’) to create simple sentences. The assumption is that a good compositional model should be able to reflect that ‘woman files application’ is closer to ‘woman registers application’ than to ‘woman smooths application’. In this paper we test our models on two different datasets of transitive sentences, that of Grefenstette and Sadrzadeh (2011a) and Kartsaklis et al. (2013)4. Specific details about the creation of the datasets can be found in the above papers; for the purposes of the current work it is sufficient to mention that their main difference is that in the former the verbs and their alternative meanings have been selected automatically using the JCN metric of semantic similarity (Jiang and Conrath, 1997), while in the latter the selection was based on human judgements from the work of Pickering and Frisson (2001). So, while 4This dataset has been created by Mehrnoosh Sadrzadeh in collaboration with Edward Grefenstette, but remained unpublished until (Ka</context>
<context position="32573" citStr="Kartsaklis et al. (2013)" startWordPosition="5558" endWordPosition="5561">r for the dataset of Kartsaklis et al. (Table 2). The longer context in combination with genuinely ambiguous verbs produces two effects: first, disambiguation is now helpful for all models, either vector mixtures or tensor-based; second, the disambiguation of just the verb (verbs-only model), without any interaction with the context, is sufficient to provide the best score (0.22) with a difference statistically significant from the second model (0.19 for disambiguated additive). In fact, further composition of the verb with the context decreases performance, confirming the results reported by Kartsaklis et al. (2013) for vectors trained using BNC. Given the nature of the specific task, which is designed around the ambiguity of the verb, this result is not surprising: a direct disambiguation of the verb based on the rest of the context should naturally constitute the best method to achieve top performance—no composition is necessary for this task to be successful. However, when one does use a task like this in order to evaluate compositional models (as we do here and as is commonly the case), they implicitly correlate the strength of the disambiguation effect that takes place during the composition with th</context>
<context position="39301" citStr="Kartsaklis et al., 2013" startWordPosition="6688" endWordPosition="6692">on The purpose of this work was twofold: our main objective was to investigate how disambiguation can affect the compositional models which are based on higher order vector spaces; a second, but not less important goal, was to compare this more linguistically motivated approach to the simpler vector mixture methods. Based on the experimental work presented here, we can say with enough confidence that disambiguation as an additional step prior to composition is indeed very beneficial for tensor-based models. Furthermore, our experiments confirm and strengthen previous work (Reddy et al., 2011; Kartsaklis et al., 2013) that showed better performance of disambiguated vector mixture models compared to their ambiguous versions. The positive effect of disambiguation is more evident for the vector mixture models (especially for the additive model) than for 6The dataset will be available at http://www.cs.ox. ac.uk/activities/compdistmeaning/. Model Ambig. Disamb. BL Verbs only 0.310 « 0.341 M1 Multiplicative 0.325 « 0.404 M2 Additive 0.368 « 0.410 T1 Relational 0.368 « 0.397 T2 Kronecker 0.404 &lt; 0.412 T3 Copy-subject 0.310 « 0.337 T4 Copy-object 0.321 « 0.368 Human agreement 0.550 Difference between T2 and M2 is </context>
</contexts>
<marker>Kartsaklis, Sadrzadeh, Pulman, 2013</marker>
<rawString>Kartsaklis, D., Sadrzadeh, M., and Pulman, S. (2013). Separating Disambiguation from Composition in Distributional Semantics. In Proceedings of 17th Conference on Computational Natural Language Learning (CoNLL-2013), Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kintsch</author>
</authors>
<date>2001</date>
<journal>Predication. Cognitive Science,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="6655" citStr="Kintsch (2001)" startWordPosition="1042" endWordPosition="1043"> (a) propose disambiguation algorithms for a number of tensorbased distributional models; (b) examine the effect of disambiguation on tensors for relational words; and (c) meaningfully compare the effectiveness of tensor-based against vector mixture models in a number of tasks. Based on the generic procedure of Sch¨utze (1998), we propose algorithms for a number of tensor-based models, where the composition is modelled as the application of linear maps (tensor contractions). Following Mitchell and Lapata (2008) and many others, we test our models on two disambiguation tasks similar to that of Kintsch (2001), and on the phrase similarity task introduced in (Mitchell and Lapata, 2010). In almost every case, the results show that disambiguation can make a great difference in the case of tensor-based models; they also reconfirm previous findings regarding the effectiveness of the method for simple vector mixture models. 2 Vectors vs tensors The simple models of Mitchell and Lapata (2008) constitute the easiest and perhaps the most intuitive way of composing two or more vectors: each element of the resulting vector is computed as the sum or the product of the corresponding elements in the input vecto</context>
<context position="27338" citStr="Kintsch (2001)" startWordPosition="4710" endWordPosition="4711">ur code is mainly written in Python and C++, and for the actual clustering step we use the Python interface of the efficient FASTCLUSTER library (M¨ullner, 2013). In a shared 24-core Xeon machine with 72 GB of memory, and with a fair amount of parallelism applied, the average processing time per word was about 4 minutes; this is roughly translated to 12-13 hours of training on average per dataset. 8.1 Verb disambiguation task Perhaps surprisingly, one of the most popular tasks for testing compositionality in distributional models is based on disambiguation. This task, originally introduced by Kintsch (2001), has been adopted by Mitchell and Lapata (2008) and others for evaluating the quality of composition in vector spaces. Given an ambiguous verb such as ‘file’, the goal is to find out to what extent the presence of an appropriate context will disambiguate its intended meaning. The context (e.g. a subject/object pair) is composed with two landmark verbs corresponding to the different senses (‘smooth’ and ‘register’) to create simple sentences. The assumption is that a good compositional model should be able to reflect that ‘woman files application’ is closer to ‘woman registers application’ tha</context>
</contexts>
<marker>Kintsch, 2001</marker>
<rawString>Kintsch, W. (2001). Predication. Cognitive Science, 25(2):173–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Landauer</author>
<author>S Dumais</author>
</authors>
<title>A Solution to Plato’s Problem: The Latent Semantic Analysis Theory of Acquision, Induction, and Representation of Knowledge. Psychological Review.</title>
<date>1997</date>
<contexts>
<context position="1444" citStr="Landauer and Dumais, 1997" startWordPosition="207" endWordPosition="210">iguation algorithms for a number of tensor-based models, which we then test on a variety of tasks. The results show that disambiguation can provide better compositional representation even for the case of tensor-based models. Furthermore, we confirm previous findings regarding the positive effect of disambiguation on vector mixture models, and we compare the effectiveness of the two approaches. 1 Introduction Distributional models of meaning have been proved extremely useful for a number of natural language processing tasks, ranging from thesaurus extraction (Curran, 2004) to topic modelling (Landauer and Dumais, 1997) and information retrieval (Manning et al., 2008), to name just a few. These models are based on the distributional hypothesis of Harris (1968), which states that the meaning of a word depends on its context. This idea allows the words to be represented by vectors of statistics collected from a sufficiently large corpus of text; each element of the vector reflects how many times a word co-occurs in the same context with another word of the vocabulary. However, due to the generative power of natural language, which is able to produce infinite new structures from a finite set of resources (words</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Landauer, T. and Dumais, S. (1997). A Solution to Plato’s Problem: The Latent Semantic Analysis Theory of Acquision, Induction, and Representation of Knowledge. Psychological Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Manandhar</author>
<author>I Klapaftis</author>
<author>D Dligach</author>
<author>S Pradhan</author>
</authors>
<title>Semeval-2010 task 14: Word sense induction &amp; disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>63--68</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21740" citStr="Manandhar et al., 2010" startWordPosition="3742" endWordPosition="3745"> vector distance measure, the preferred sense s� is given by: d(-V-,+, - ) (9) For the actual clustering step we follow the setting of Kartsaklis et al. (2013), which worked well in tasks very similar to ours. Specifically, we perform hierarchical agglomerative clustering (HAC) using Ward’s method as the inter-cluster distance, while the distance between vectors is measured with Pearson’s correlation.3 In the above work, this configuration has been found to return the highest V-measure (Rosenberg and Hirschberg, 2007) on the noun set of SEMEVAL 2010 Word Sense Induction &amp; Disambiguation Task (Manandhar et al., 2010). As context for a word, we consider the sentence in which this word occurs. The output of HAC is a dendrogram embedding all the possible partitionings of the 3Informal experimentation with more robust probabilistic techniques, such as Dirichlet process gaussian mixture models, revealed no significant benefits for our setting. s� = arg min �� v3ESw 1594 data. In order to select the optimal partitioning, we rely on the Cali´nski/Harabasz index (Cali´nski and Harabasz, 1974), also known as variance ratio criterion (VRC). VRC is calculated as the ratio of the sum of the inter-cluster variances ov</context>
</contexts>
<marker>Manandhar, Klapaftis, Dligach, Pradhan, 2010</marker>
<rawString>Manandhar, S., Klapaftis, I., Dligach, D., and Pradhan, S. (2010). Semeval-2010 task 14: Word sense induction &amp; disambiguation. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 63–68. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>P Raghavan</author>
<author>H Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Manning, C., Raghavan, P., and Sch¨utze, H. (2008). Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mitchell</author>
<author>M Lapata</author>
</authors>
<title>Vector-based Models of Semantic Composition.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>236--244</pages>
<contexts>
<context position="2799" citStr="Mitchell and Lapata, 2008" startWordPosition="433" endWordPosition="436">rds or perhaps very short phrases consisting of two words; in other words, this technique cannot scale up to the phrase or sentence level. Much research activity has been recently dedicated to provide a solution to this problem: although the direct construction of a sentence vector is not possible, we might still be able to synthetically create such a vectorial representation by somehow composing the vectors of the words that comprise the sentence. Towards this goal, researchers have employed a variety of approaches that roughly fall into two general categories. Following an influential work (Mitchell and Lapata, 2008), the models in the first category compute a sentence vector as a mixture of the original word vectors, using simple operations such as element-wise multiplication and addition; we refer to these models as vector mixtures. The main characteristic of these models is that they do not distinguish between the type-logical identities of the different words: an intransitive verb, for example, is of the same order as its subject (a noun), and both will contribute equally to the composite sentence vector. However, this symmetric treatment of composition seems unjustified from a formal semantics point </context>
<context position="6557" citStr="Mitchell and Lapata (2008)" startWordPosition="1023" endWordPosition="1026">ute more powerful models for natural language (see discussion in Section 2). Specifically, this paper aims to: (a) propose disambiguation algorithms for a number of tensorbased distributional models; (b) examine the effect of disambiguation on tensors for relational words; and (c) meaningfully compare the effectiveness of tensor-based against vector mixture models in a number of tasks. Based on the generic procedure of Sch¨utze (1998), we propose algorithms for a number of tensor-based models, where the composition is modelled as the application of linear maps (tensor contractions). Following Mitchell and Lapata (2008) and many others, we test our models on two disambiguation tasks similar to that of Kintsch (2001), and on the phrase similarity task introduced in (Mitchell and Lapata, 2010). In almost every case, the results show that disambiguation can make a great difference in the case of tensor-based models; they also reconfirm previous findings regarding the effectiveness of the method for simple vector mixture models. 2 Vectors vs tensors The simple models of Mitchell and Lapata (2008) constitute the easiest and perhaps the most intuitive way of composing two or more vectors: each element of the resul</context>
<context position="25843" citStr="Mitchell and Lapata (2008)" startWordPosition="4457" endWordPosition="4460">ven the target word to the probability of the context word overall. The context here is a 5-word window on both sides of the target word. The vectors are disambiguated both syntactically and semantically: first, separate vectors have been created for different syntactic usages of the same word in the corpus; for example, the word ‘book’ has two vectors, one for its noun sense and one for its verb sense. Furthermore, each word is semantically disambiguated according to the method of Section 6. Models We compare the tensor-based models of Section 5 with the multiplicative and additive models of Mitchell and Lapata (2008), reporting results for both ambiguous and disambiguated versions. For all the disambiguated models, the best sense for each word in the sentence or phrase is first selected by applying the procedure of Section 6 and Equation 9. If the model is based on a vector mixture, the sense vectors corresponding to these senses are multiplied or added to form the composite representation for the sentence or phrase. For the tensor-based models, the composite meanings are calculated acE 2 verbi = sESi n wordi n = E sESi k=1 1595 cording to the equations of Section 5, using verb tensors created by the proc</context>
<context position="27386" citStr="Mitchell and Lapata (2008)" startWordPosition="4716" endWordPosition="4719">and C++, and for the actual clustering step we use the Python interface of the efficient FASTCLUSTER library (M¨ullner, 2013). In a shared 24-core Xeon machine with 72 GB of memory, and with a fair amount of parallelism applied, the average processing time per word was about 4 minutes; this is roughly translated to 12-13 hours of training on average per dataset. 8.1 Verb disambiguation task Perhaps surprisingly, one of the most popular tasks for testing compositionality in distributional models is based on disambiguation. This task, originally introduced by Kintsch (2001), has been adopted by Mitchell and Lapata (2008) and others for evaluating the quality of composition in vector spaces. Given an ambiguous verb such as ‘file’, the goal is to find out to what extent the presence of an appropriate context will disambiguate its intended meaning. The context (e.g. a subject/object pair) is composed with two landmark verbs corresponding to the different senses (‘smooth’ and ‘register’) to create simple sentences. The assumption is that a good compositional model should be able to reflect that ‘woman files application’ is closer to ‘woman registers application’ than to ‘woman smooths application’. In this paper </context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Mitchell, J. and Lapata, M. (2008). Vector-based Models of Semantic Composition. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mitchell</author>
<author>M Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="6732" citStr="Mitchell and Lapata, 2010" startWordPosition="1052" endWordPosition="1055">d distributional models; (b) examine the effect of disambiguation on tensors for relational words; and (c) meaningfully compare the effectiveness of tensor-based against vector mixture models in a number of tasks. Based on the generic procedure of Sch¨utze (1998), we propose algorithms for a number of tensor-based models, where the composition is modelled as the application of linear maps (tensor contractions). Following Mitchell and Lapata (2008) and many others, we test our models on two disambiguation tasks similar to that of Kintsch (2001), and on the phrase similarity task introduced in (Mitchell and Lapata, 2010). In almost every case, the results show that disambiguation can make a great difference in the case of tensor-based models; they also reconfirm previous findings regarding the effectiveness of the method for simple vector mixture models. 2 Vectors vs tensors The simple models of Mitchell and Lapata (2008) constitute the easiest and perhaps the most intuitive way of composing two or more vectors: each element of the resulting vector is computed as the sum or the product of the corresponding elements in the input vectors (left part in Figure 1). In the case of addition, the components of the ou</context>
<context position="33788" citStr="Mitchell and Lapata (2010)" startWordPosition="5764" endWordPosition="5767">ion with the quality of composition, essentially assuming that the stronger the disambiguation, the better the compositional model that produced this side-effect. Unfortunately, the extent to which this assumption is valid or not is still not quite clear; the subject is addressed in more detail in (Kartsaklis et al., 2013). Keeping a note of this observation, we now proceed to examine the performance of our models in a task that does not use disambiguation as a criterion of composition. 8.2 Phrase/sentence similarity task Our second set of experiments is based on the phrase similarity task of Mitchell and Lapata (2010). On the contrary with the task of Section 8.1, this one does not involve any assumptions about disambiguation, and thus it seems like a more genuine test of models aiming to provide appropriate phrasal or sentential semantic representations; the only criterion is the degree to which these models correctly evaluate the similarity between pairs of sentences or phrases. We work on the verb-phrase part of the dataset, consisting of 72 short verb phrases (verb-object structures). These 72 phrases have been paired in three different ways to form groups exhibiting various degrees of similarity: the </context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Mitchell, J. and Lapata, M. (2010). Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M¨ullner</author>
</authors>
<title>fastcluster: Fast Hierarchical Clustering Routines for R and Python.</title>
<date>2013</date>
<journal>Journal of Statistical Software,</journal>
<volume>9</volume>
<issue>53</issue>
<marker>M¨ullner, 2013</marker>
<rawString>M¨ullner, D. (2013). fastcluster: Fast Hierarchical Clustering Routines for R and Python. Journal of Statistical Software, 9(53):1–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pickering</author>
<author>S Frisson</author>
</authors>
<title>Processing ambiguous verbs: Evidence from eye movements.</title>
<date>2001</date>
<journal>Journal of Experimental Psychology: Learning, Memory, and Cognition,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="28584" citStr="Pickering and Frisson (2001)" startWordPosition="4911" endWordPosition="4914">s application’. In this paper we test our models on two different datasets of transitive sentences, that of Grefenstette and Sadrzadeh (2011a) and Kartsaklis et al. (2013)4. Specific details about the creation of the datasets can be found in the above papers; for the purposes of the current work it is sufficient to mention that their main difference is that in the former the verbs and their alternative meanings have been selected automatically using the JCN metric of semantic similarity (Jiang and Conrath, 1997), while in the latter the selection was based on human judgements from the work of Pickering and Frisson (2001). So, while 4This dataset has been created by Mehrnoosh Sadrzadeh in collaboration with Edward Grefenstette, but remained unpublished until (Kartsaklis et al., 2013). in the first dataset many verbs cannot be considered as genuinely ambiguous (e.g. ‘say’ with meanings state and allege or ‘write’ with meanings publish and spell), the landmarks in the second dataset correspond to clearly separated senses (e.g. ‘file’ with meanings register and smooth or ‘charge’ with meanings accuse and bill). Furthermore, subjects and objects of this latter case are modified by appropriate adjectives, overall c</context>
</contexts>
<marker>Pickering, Frisson, 2001</marker>
<rawString>Pickering, M. and Frisson, S. (2001). Processing ambiguous verbs: Evidence from eye movements. Journal of Experimental Psychology: Learning, Memory, and Cognition, 27(2):556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pulman</author>
</authors>
<title>Combining Compositional and Distributional Models of Semantics.</title>
<date>2013</date>
<booktitle>Quantum Physics and Linguistics: A Compositional, Diagrammatic Discourse.</booktitle>
<editor>In Heunen, C., Sadrzadeh, M., and Grefenstette, E., editors,</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="9200" citStr="Pulman (2013)" startWordPosition="1461" endWordPosition="1462">etween different features are also taken into account, offering (in principle) more power. Furthermore, by design, the bag-of-words problem is not present here. Overall, tensor-based models offer a more complete and linguistically motivated solution to the problem of composition. For example, one can consider building linear maps for prepositions and logical words, rather than treating them as noise and discard them, as commonly done in the vector mixture models. 3 Disambiguation in vector mixtures For a compositional model based on vector mixtures, polysemy of words can be a critical factor. Pulman (2013) and Kartsaklis et al. (2013) point out that the element-wise combination of “ambiguous” vectors produces results that are hard to interpret; the composed vector is not a purely compositional representation but a product of two tasks that take place in parallel: composition and some amount of disambiguation that emerges as a side-effect of the compositional process, leaving the resulting vector in an intermediate state. This effect is demonstrated in Figure 2, which shows the composition of the ambiguous verb ‘run’ (with meanings moving fast and dissolving) with the subject ‘horse’. The first </context>
<context position="11134" citStr="Pulman (2013)" startWordPosition="1781" endWordPosition="1782">re 2 shows what happens when the ambiguous ‘run’ vector is used; the multiplication with the ‘horse’ vector will produce an impure result, half affected by composition and half by disambiguation. However, what we really want is a vector where all the dissolving-related components will be eliminated, since they are irrelevant to the way the word ‘run’ is used in the sentence. In order to achieve this, we have to introduce a disambiguation step prior to composition (right part of Figure 2). These ideas are experimentally verified in the works of Reddy et al. (2011) and Kartsaklis et al. (2013); Pulman (2013) also presents a comprehensive analysis of the problem. What remains to be seen is if disambiguation can also provide benefits for the linguistically motivated setting of tensorbased models, the principles of which are shortly discussed in the next section. 4 Tensors as multilinear maps A tensor is a geometric object that can be seen as the generalization of the familiar notion of a vector in higher dimensions. The order of a tensor is the number of its dimensions; in other words, the number of indices we need to fully describe a random element of the tensor. Hence, a vector is a tensor of ord</context>
</contexts>
<marker>Pulman, 2013</marker>
<rawString>Pulman, S. (2013). Combining Compositional and Distributional Models of Semantics. In Heunen, C., Sadrzadeh, M., and Grefenstette, E., editors, Quantum Physics and Linguistics: A Compositional, Diagrammatic Discourse. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Reddy</author>
<author>I Klapaftis</author>
<author>D McCarthy</author>
<author>S Manandhar</author>
</authors>
<title>Dynamic and static prototype vectors for semantic composition.</title>
<date>2011</date>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>705--713</pages>
<contexts>
<context position="5130" citStr="Reddy et al. (2011)" startWordPosition="800" endWordPosition="803">ontext of the abstract mathematical framework of compact closed categories. We refer to this class of models as tensor-based. Regardless of the way they approach the representation of relational words and their composition operation, however, most current compositionaldistributional models do share a common feature: they all rely on ambiguous vector representations, where all the senses of a polysemous word, such as the verb ‘file’ (which can mean register or smooth), are merged into the same vector or tensor. At least for the vector mixture approach, this practice has been proved suboptimal: Reddy et al. (2011) and Kartsaklis et al. (2013) test a number of simple multiplicative and additive models using disambiguated vector representations on various tasks, showing that the introduction of a disambiguation step prior to actual composition can indeed increase the quality of the composite vectors. However, the fact that disambiguation can be beneficial for models based on vector mixtures is not very surprising. Both additive and multiplicative compositions are but a kind of average of the vectors of the words in the sentence, hence can directly benefit from the provision of more accurate starting poin</context>
<context position="11090" citStr="Reddy et al. (2011)" startWordPosition="1772" endWordPosition="1775">ord ‘painting’, for example. The left part of Figure 2 shows what happens when the ambiguous ‘run’ vector is used; the multiplication with the ‘horse’ vector will produce an impure result, half affected by composition and half by disambiguation. However, what we really want is a vector where all the dissolving-related components will be eliminated, since they are irrelevant to the way the word ‘run’ is used in the sentence. In order to achieve this, we have to introduce a disambiguation step prior to composition (right part of Figure 2). These ideas are experimentally verified in the works of Reddy et al. (2011) and Kartsaklis et al. (2013); Pulman (2013) also presents a comprehensive analysis of the problem. What remains to be seen is if disambiguation can also provide benefits for the linguistically motivated setting of tensorbased models, the principles of which are shortly discussed in the next section. 4 Tensors as multilinear maps A tensor is a geometric object that can be seen as the generalization of the familiar notion of a vector in higher dimensions. The order of a tensor is the number of its dimensions; in other words, the number of indices we need to fully describe a random element of th</context>
<context position="39275" citStr="Reddy et al., 2011" startWordPosition="6684" endWordPosition="6687">ase task. 9 Discussion The purpose of this work was twofold: our main objective was to investigate how disambiguation can affect the compositional models which are based on higher order vector spaces; a second, but not less important goal, was to compare this more linguistically motivated approach to the simpler vector mixture methods. Based on the experimental work presented here, we can say with enough confidence that disambiguation as an additional step prior to composition is indeed very beneficial for tensor-based models. Furthermore, our experiments confirm and strengthen previous work (Reddy et al., 2011; Kartsaklis et al., 2013) that showed better performance of disambiguated vector mixture models compared to their ambiguous versions. The positive effect of disambiguation is more evident for the vector mixture models (especially for the additive model) than for 6The dataset will be available at http://www.cs.ox. ac.uk/activities/compdistmeaning/. Model Ambig. Disamb. BL Verbs only 0.310 « 0.341 M1 Multiplicative 0.325 « 0.404 M2 Additive 0.368 « 0.410 T1 Relational 0.368 « 0.397 T2 Kronecker 0.404 &lt; 0.412 T3 Copy-subject 0.310 « 0.337 T4 Copy-object 0.321 « 0.368 Human agreement 0.550 Differ</context>
</contexts>
<marker>Reddy, Klapaftis, McCarthy, Manandhar, 2011</marker>
<rawString>Reddy, S., Klapaftis, I., McCarthy, D., and Manandhar, S. (2011). Dynamic and static prototype vectors for semantic composition. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 705–713.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rosenberg</author>
<author>J Hirschberg</author>
</authors>
<title>Vmeasure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>410--420</pages>
<contexts>
<context position="21640" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="3724" endWordPosition="3727">or. Specifically, if S,,, is the set of sense vectors for w, *�c the context vector for C, and d(v , u ) our vector distance measure, the preferred sense s� is given by: d(-V-,+, - ) (9) For the actual clustering step we follow the setting of Kartsaklis et al. (2013), which worked well in tasks very similar to ours. Specifically, we perform hierarchical agglomerative clustering (HAC) using Ward’s method as the inter-cluster distance, while the distance between vectors is measured with Pearson’s correlation.3 In the above work, this configuration has been found to return the highest V-measure (Rosenberg and Hirschberg, 2007) on the noun set of SEMEVAL 2010 Word Sense Induction &amp; Disambiguation Task (Manandhar et al., 2010). As context for a word, we consider the sentence in which this word occurs. The output of HAC is a dendrogram embedding all the possible partitionings of the 3Informal experimentation with more robust probabilistic techniques, such as Dirichlet process gaussian mixture models, revealed no significant benefits for our setting. s� = arg min �� v3ESw 1594 data. In order to select the optimal partitioning, we rely on the Cali´nski/Harabasz index (Cali´nski and Harabasz, 1974), also known as varianc</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Rosenberg, A. and Hirschberg, J. (2007). Vmeasure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 410–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Automatic Word Sense Discrimination. Computational Linguistics,</title>
<date>1998</date>
<pages>24--97</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Sch¨utze, H. (1998). Automatic Word Sense Discrimination. Computational Linguistics, 24:97– 123.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
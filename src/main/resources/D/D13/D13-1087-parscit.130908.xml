<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014040">
<title confidence="0.973973">
Decipherment with a Million Random Restarts
</title>
<author confidence="0.999052">
Taylor Berg-Kirkpatrick Dan Klein
</author>
<affiliation confidence="0.998816">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.99939">
{tberg,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.998603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991405882353">
This paper investigates the utility and effect of
running numerous random restarts when us-
ing EM to attack decipherment problems. We
find that simple decipherment models are able
to crack homophonic substitution ciphers with
high accuracy if a large number of random
restarts are used but almost completely fail
with only a few random restarts. For partic-
ularly difficult homophonic ciphers, we find
that big gains in accuracy are to be had by run-
ning upwards of 100K random restarts, which
we accomplish efficiently using a GPU-based
parallel implementation. We run a series of
experiments using millions of random restarts
in order to investigate other empirical proper-
ties of decipherment problems, including the
famously uncracked Zodiac 340.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999609433333333">
What can a million restarts do for decipherment?
EM frequently gets stuck in local optima, so running
between ten and a hundred random restarts is com-
mon practice (Knight et al., 2006; Ravi and Knight,
2011; Berg-Kirkpatrick and Klein, 2011). But, how
important are random restarts and how many random
restarts does it take to saturate gains in accuracy?
We find that the answer depends on the cipher. We
look at both Zodiac 408, a famous homophonic sub-
stitution cipher, and a more difficult homophonic ci-
pher constructed to match properties of the famously
unsolved Zodiac 340. Gains in accuracy saturate af-
ter only a hundred random restarts for Zodiac 408,
but for the constructed cipher we see large gains
in accuracy even as we scale the number of ran-
dom restarts up into the hundred thousands. In both
cases the difference between few and many random
restarts is the difference between almost complete
failure and successful decipherment.
We also find that millions of random restarts can
be helpful for performing exploratory analysis. We
look at some empirical properties of decipherment
problems, visualizing the distribution of local op-
tima encountered by EM both in a successful deci-
pherment of a homophonic cipher and in an unsuc-
cessful attempt to decipher Zodiac 340. Finally, we
attack a series of ciphers generated to match proper-
ties of Zodiac 340 and use the results to argue that
Zodiac 340 is likely not a homophonic cipher under
the commonly assumed linearization order.
</bodyText>
<sectionHeader confidence="0.999341" genericHeader="method">
2 Decipherment Model
</sectionHeader>
<bodyText confidence="0.99993875">
Various types of ciphers have been tackled by the
NLP community with great success (Knight et al.,
2006; Snyder et al., 2010; Ravi and Knight, 2011).
Many of these approaches learn an encryption key
by maximizing the score of the decrypted message
under a language model. We focus on homophonic
substitution ciphers, where the encryption key is a
1-to-many mapping from a plaintext alphabet to a
cipher alphabet. We use a simple method introduced
by Knight et al. (2006): the EM algorithm (Demp-
ster et al., 1977) is used to learn the emission pa-
rameters of an HMM that has a character trigram
language model as a backbone and the ciphertext
as the observed sequence of emissions. This means
that we learn a multinomial over cipher symbols for
each plaintext character, but do not learn transition
</bodyText>
<page confidence="0.975606">
874
</page>
<bodyText confidence="0.8226016">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 874–878,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
parameters, which are fixed by the language model.
We predict the deciphered text using posterior de-
coding in the learned HMM.
</bodyText>
<subsectionHeader confidence="0.976471">
2.1 Implementation
</subsectionHeader>
<bodyText confidence="0.999366166666667">
Running multiple random restarts means running
EM to convergence multiple times, which can be
computationally intensive; luckily, restarts can be
run in parallel. This kind of parallelism is a good
fit for the Same Instruction Multiple Thread (SIMT)
hardware paradigm implemented by modern GPUs.
We implemented EM with parallel random restarts
using the CUDA API (Nickolls et al., 2008). With a
GPU workstation,1 we can complete a million ran-
dom restarts roughly a thousand times more quickly
than we can complete the same computation with a
serial implementation on a CPU.
</bodyText>
<figure confidence="0.998934055555555">
0.8
0.7
0.4
0.3
0.2
0.1 -1530
1 10 100 1000 10000 100000 1e+06
Log likelihood
Accuracy
-1460
-1470
-1480
-1490
-1500
-1510
-1520
Log likelihood
Number of random restarts
</figure>
<figureCaption confidence="0.999586333333333">
Figure 1: Zodiac 408 cipher. Accuracy by best model score and
best model score vs. number of random restarts. Bootstrapped
from 1M random restarts.
</figureCaption>
<figure confidence="0.9948446">
Accuracy
0.6
0.5
1
0.9
</figure>
<sectionHeader confidence="0.976151" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999987">
We ran experiments on several homophonic sub-
stitution ciphers: some produced by the infamous
Zodiac killer and others that were automatically
generated to be similar to the Zodiac ciphers. In
each of these experiments, we ran numerous random
restarts; and in all cases we chose the random restart
that attained the highest model score in order to pro-
duce the final decode.
</bodyText>
<subsectionHeader confidence="0.995485">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9908826">
The specifics of how random restarts are produced
is usually considered a detail; however, in this work
it is important to describe the process precisely. In
order to generate random restarts, we sampled emis-
sion parameters by drawing uniformly at random
from the interval [0, 1] and then normalizing. The
corresponding distribution on the multinomial emis-
sion parameters is mildly concentrated at the center
of the simplex.2
For each random restart, we ran EM for 200 itera-
1We used a single workstation with three NVIDIA GTX 580
GPUs. These are consumer graphics cards introduced in 2011.
2We also ran experiments where emission parameters were
drawn from Dirichlet distributions with various concentration
parameter settings. We noticed little effect so long as the distri-
bution did not favor the corners of the simplex. If the distribu-
tion did favor the corners of the simplex, decipherment results
deteriorated sharply.
tions.3 We found that smoothing EM was important
for good performance. We added a smoothing con-
stant of 0.1 to the expected emission counts before
each M-step. We tuned this value on a small held
out set of automatically generated ciphers.
In all experiments we used a trigram character
language model that was linearly interpolated from
character unigram, bigram, and trigram counts ex-
tracted from both the Google N-gram dataset (Brants
and Franz, 2006) and a small corpus (about 2K
words) of plaintext messages authored by the Zodiac
killer.4
</bodyText>
<subsectionHeader confidence="0.999787">
3.2 An Easy Cipher: Zodiac 408
</subsectionHeader>
<bodyText confidence="0.999924">
Zodiac 408 is a homophonic cipher that is 408 char-
acters long and contains 54 different cipher sym-
bols. Produced by the Zodiac killer, this cipher was
solved, manually, by two amateur code-breakers a
week after its release to the public in 1969. Ravi and
Knight (2011) were the first to crack Zodiac 408 us-
ing completely automatic methods.
In our first experiment, we compare a decode of
Zodiac 408 using one random restart to a decode us-
ing 100 random restarts. Random restarts have high
</bodyText>
<footnote confidence="0.965434">
3While this does not guarantee convergence, in practice 200
iterations seems to be sufficient for the problems we looked at.
4The interpolation between n-gram orders is uniform, and
the interpolation between corpora favors the Zodiac corpus with
weight 0.9.
</footnote>
<page confidence="0.994617">
875
</page>
<bodyText confidence="0.989176285714286">
Accuracy
variance, so when we present the accuracy corre-
sponding to a given number of restarts we present an
average over many bootstrap samples, drawn from
a set of one million random restarts. If we attack
Zodiac 408 with a single random restart, on aver-
age we achieve an accuracy of 18%. If we instead
use 100 random restarts we achieve a much better
average accuracy of 90%. The accuracies for vari-
ous numbers of random restarts are plotted in Fig-
ure 1. Based on these results, we expect accuracy
to increase by about 72% when using 100 random
restarts instead of a single random restart; however,
using more than 100 random restarts for this partic-
ular cipher does not appear to be useful.
Also in Figure 1, we plot a related graph, this time
showing the effect that random restarts have on the
achieved model score. By construction, the (maxi-
mum) model score must increase as we increase the
number of random restarts. We see that it quickly
saturates in the same way that accuracy did.
This raises the question: have we actually
achieved the globally optimal model score or have
we only saturated the usefulness of random restarts?
We can’t prove that we have achieved the global op-
timum,5 but we can at least check that we have sur-
passed the model score achieved by EM when it is
initialized with the gold encryption key. On Zodiac
408, if we initialize with the gold key, EM finds
a local optimum with a model score of −1467.4.
The best model score over 1M random restarts is
−1466.5, which means we have surpassed the gold
initialization.
The accuracy after gold initialization was 92%,
while the accuracy of the best local optimum was
only 89%. This suggests that the global optimum
may not be worth finding if we haven’t already
found it. From Figure 1, it appears that large in-
creases in likelihood are correlated with increases
in accuracy, but small improvements to high like-
lihoods (e.g. the best local optimum versus the gold
initialization) may not to be.
</bodyText>
<footnote confidence="0.539619428571429">
5ILP solvers can be used to globally optimize objectives
corresponding to short 1-to-1 substitution ciphers (Ravi and
Knight, 2008) (though these objectives are slightly different
from the likelihood objectives faced by EM), but we find that
ILP encodings for even the shortest homophonic ciphers cannot
be optimized in any reasonable amount of time.
Number of random restarts
</footnote>
<figureCaption confidence="0.999727333333333">
Figure 2: Synth 340 cipher. Accuracy by best model score and
best model score vs. number of random restarts. Bootstrapped
from 1M random restarts.
</figureCaption>
<subsectionHeader confidence="0.991156">
3.3 A Hard Cipher: Synth 340
</subsectionHeader>
<bodyText confidence="0.999956178571429">
What do these graphs look like for a harder cipher?
Zodiac 340 is the second cipher released by the Zo-
diac killer, and it remains unsolved to this day. How-
ever, it is unknown whether Zodiac 340 is actually a
homophonic cipher. If it were a homophonic cipher
we would certainly expect it to be harder than Zo-
diac 408 because Zodiac 340 is shorter (only 340
characters long) and at the same time has more ci-
pher symbols: 63. For our next experiment we gen-
erate a cipher, which we call Synth 340, to match
properties of Zodiac 340; later we will generate mul-
tiple such ciphers.
We sample a random consecutive sequence of 340
characters from our small Zodiac corpus and use
this as our message (and, of course, remove this se-
quence from our language model training data). We
then generate an encryption key by assigning each
of 63 cipher symbols to a single plain text charac-
ter so that the number of cipher symbols mapped to
each plaintext character is proportional to the fre-
quency of that character in the message (this bal-
ancing makes the cipher more difficult). Finally, we
generate the actual ciphertext by randomly sampling
a cipher token for each plain text token uniformly at
random from the cipher symbols allowed for that to-
ken under our generated key.
In Figure 2, we display the same type of plot, this
time for Synth 340. For this cipher, there is an abso-
</bodyText>
<figure confidence="0.990843553191489">
0.75
0.65
0.55
0.45
0.35
0.8
0.7
0.6
0.5
0.4
0.3
-1310
1 10 100 1000 10000 100000 1e+06
Log likelihood
Accuracy
-1280
-1285
-1290
-1295
-1300
-1305
Log likelihood
876
Frequency
Frequency
14000
12000
10000
4000
8000
6000
2000
0
40000
60000
50000
30000
20000
10000
0
42%
ntyouldli 59%
veautital
74%
veautiful
-1340 -1330 -1320 -1310 -1300 -1290 -1280 -1270
Log likelihood
</figure>
<figureCaption confidence="0.9846695">
Figure 3: Synth 340 cipher. Histogram of the likelihoods of the
local optima encountered by EM across 1M random restarts.
Several peaks are labeled with their average accuracy and a
snippet of a decode. The gold snippet is “beautiful.”
</figureCaption>
<bodyText confidence="0.999958518518518">
lute gain in accuracy of about 9% between 100 ran-
dom restarts and 100K random restarts. A similarly
large gain is seen for model score as we scale up the
number of restarts. This means that, even after tens
of thousands of random restarts, EM is still finding
new local optima with better likelihoods. It also ap-
pears that, even for a short cipher like Synth 340,
likelihood and accuracy are reasonably coupled.
We can visualize the distribution of local optima
encountered by EM across 1M random restarts by
plotting a histogram. Figure 3 shows, for each range
of likelihood, the number of random restarts that
led to a local optimum with a model score in that
range. It is quickly visible that a few model scores
are substantially more likely than all the rest. This
kind of sparsity might be expected if there were
a small number of local optima that EM was ex-
tremely likely to find. We can check whether the
peaks of this histogram each correspond to a single
local optimum or whether each is composed of mul-
tiple local optima that happen to have the same like-
lihood. For the histogram bucket corresponding to a
particular peak, we compute the average relative dif-
ference between each multinomial parameter and its
mean. The average relative difference for the highest
peak in Figure 3 is 0.8%, and for the second highest
peak is 0.3%. These values are much smaller than
</bodyText>
<figure confidence="0.5730135">
-1330 -1320 -1310 -1300 -1290 -1280
Log likelihood
</figure>
<figureCaption confidence="0.9962975">
Figure 4: Zodiac 340 cipher. Histogram of the likelihoods of the
local optima encountered by EM across 1M random restarts.
</figureCaption>
<bodyText confidence="0.9994616">
the average relative difference between the means of
these two peaks, 40%, indicating that the peaks do
correspond to single local optima or collections of
extremely similar local optima.
There are several very small peaks that have the
highest model scores (the peak with the highest
model score has a frequency of 90 which is too
small to be visible in Figure 3). The fact that these
model scores are both high and rare is the reason we
continue to see improvements to both accuracy and
model score as we run numerous random restarts.
The two tallest peaks and the peak with highest
model score are labeled with their average accuracy
and a small snippet of a decode in Figure 3. The
gold snippet is the word “beautiful.”
</bodyText>
<subsectionHeader confidence="0.98667">
3.4 An Unsolved Cipher: Zodiac 340
</subsectionHeader>
<bodyText confidence="0.999721583333333">
In a final experiment, we look at the Zodiac 340
cipher. As mentioned, this cipher has never been
cracked and may not be a homphonic cipher or even
a valid cipher of any kind. The reading order of
the cipher, which consists of a grid of symbols, is
unknown. We make two arguments supporting the
claim that Zodiac 340 is not a homophonic cipher
with row-major reading order: the first is statistical,
based on the success rate of attempts to crack similar
synthetic ciphers; the second is qualitative, compar-
ing distributions of local optimum likelihoods.
If Zodiac 340 is a homophonic cipher should we
</bodyText>
<page confidence="0.990492">
877
</page>
<bodyText confidence="0.999982357142857">
expect to crack it? In order to answer this question
we generate 100 more ciphers in the same way we
generated Synth 340. We use 10K random restarts to
attack each cipher, and compute accuracies by best
model score. The average accuracy across these 100
ciphers was 75% and the minimum accuracy was
36%. All but two of the ciphers were deciphered
with more than 51% accuracy, which is usually suf-
ficient for a human to identify a decode as partially
correct.
We attempted to crack Zodiac 340 using a row-
major reading order and 1M random restarts, but the
decode with best model score was nonsensical. This
outcome would be unlikely if Zodiac 340 were like
our synthetic ciphers, so Zodiac 340 is probably not
a homophonic cipher with a row-major order. Of
course, it could be a homophonic cipher with a dif-
ferent reading order. It could also be the case that
a large number of salt tokens were inserted, or that
some other assumption is incorrect.
In Figure 4, we show the histogram of model
scores for the attempt to crack Zodiac 340. We note
that this histogram is strikingly different from the
histogram for Synth 340. Zodiac 340’s histogram is
not as sparse, and the range of model scores is much
smaller. The sparsity of Synth 340’s histogram (but
not Zodiac 340’s histogram) is typical of histograms
corresponding to our set of 100 generated ciphers.
</bodyText>
<sectionHeader confidence="0.999771" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999859588235294">
Random restarts, often considered a footnote of ex-
perimental design, can indeed be useful on scales
beyond that generally used in past work. In particu-
lar, we found that the initializations that lead to the
local optima with highest likelihoods are sometimes
very rare, but finding them can be worthwhile; for
the problems we looked at, local optima with high
likelihoods also achieved high accuracies. While the
present experiments are on a very specific unsuper-
vised learning problem, it is certainly reasonable to
think that large-scale random restarts have potential
more broadly.
In addition to improving search, large-scale
restarts can also provide a novel perspective when
performing exploratory analysis, here letting us ar-
gue in support for the hypothesis that Zodiac 340 is
not a row-major homophonic cipher.
</bodyText>
<sectionHeader confidence="0.998028" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9977835">
Taylor Berg-Kirkpatrick and Dan Klein. 2011. Sim-
ple effective decipherment via combinatorial optimiza-
tion. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. Linguistic Data Consortium, Catalog Num-
ber LDC2009T25.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipherment
problems. In Proceedings of the 2006 Annual Meeting
of the Association for Computational Linguistics.
John Nickolls, Ian Buck, Michael Garland, and Kevin
Skadron. 2008. Scalable parallel programming with
CUDA. Queue.
Sujith Ravi and Kevin Knight. 2008. Attacking deci-
pherment problems optimally with low-order n-gram
models. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing.
Sujith Ravi and Kevin Knight. 2011. Bayesian inference
for Zodiac and other homophonic ciphers. In Proceed-
ings of the 2011 Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language decipher-
ment. In Proceedings of the 2010 Annual Meeting of
the Association for Computational Linguistics.
</reference>
<page confidence="0.99768">
878
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.850122">
<title confidence="0.998857">Decipherment with a Million Random Restarts</title>
<author confidence="0.999345">Taylor Berg-Kirkpatrick Dan</author>
<affiliation confidence="0.9999195">Computer Science University of California,</affiliation>
<abstract confidence="0.991602555555555">This paper investigates the utility and effect of running numerous random restarts when using EM to attack decipherment problems. We find that simple decipherment models are able to crack homophonic substitution ciphers with high accuracy if a large number of random restarts are used but almost completely fail with only a few random restarts. For particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Simple effective decipherment via combinatorial optimization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1181" citStr="Berg-Kirkpatrick and Klein, 2011" startWordPosition="174" endWordPosition="177">t homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340. 1 Introduction What can a million restarts do for decipherment? EM frequently gets stuck in local optima, so running between ten and a hundred random restarts is common practice (Knight et al., 2006; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). But, how important are random restarts and how many random restarts does it take to saturate gains in accuracy? We find that the answer depends on the cipher. We look at both Zodiac 408, a famous homophonic substitution cipher, and a more difficult homophonic cipher constructed to match properties of the famously unsolved Zodiac 340. Gains in accuracy saturate after only a hundred random restarts for Zodiac 408, but for the constructed cipher we see large gains in accuracy even as we scale the number of random restarts up into the hundred thousands. In both cases the difference between few a</context>
</contexts>
<marker>Berg-Kirkpatrick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple effective decipherment via combinatorial optimization. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1t 5-gram version 1. Linguistic Data Consortium, Catalog Number LDC2009T25.</title>
<date>2006</date>
<contexts>
<context position="6299" citStr="Brants and Franz, 2006" startWordPosition="1007" endWordPosition="1010"> long as the distribution did not favor the corners of the simplex. If the distribution did favor the corners of the simplex, decipherment results deteriorated sharply. tions.3 We found that smoothing EM was important for good performance. We added a smoothing constant of 0.1 to the expected emission counts before each M-step. We tuned this value on a small held out set of automatically generated ciphers. In all experiments we used a trigram character language model that was linearly interpolated from character unigram, bigram, and trigram counts extracted from both the Google N-gram dataset (Brants and Franz, 2006) and a small corpus (about 2K words) of plaintext messages authored by the Zodiac killer.4 3.2 An Easy Cipher: Zodiac 408 Zodiac 408 is a homophonic cipher that is 408 characters long and contains 54 different cipher symbols. Produced by the Zodiac killer, this cipher was solved, manually, by two amateur code-breakers a week after its release to the public in 1969. Ravi and Knight (2011) were the first to crack Zodiac 408 using completely automatic methods. In our first experiment, we compare a decode of Zodiac 408 using one random restart to a decode using 100 random restarts. Random restarts</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram version 1. Linguistic Data Consortium, Catalog Number LDC2009T25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Dempster</author>
<author>Nan Laird</author>
<author>Donald Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society.</journal>
<contexts>
<context position="2963" citStr="Dempster et al., 1977" startWordPosition="471" endWordPosition="475">ac 340 is likely not a homophonic cipher under the commonly assumed linearization order. 2 Decipherment Model Various types of ciphers have been tackled by the NLP community with great success (Knight et al., 2006; Snyder et al., 2010; Ravi and Knight, 2011). Many of these approaches learn an encryption key by maximizing the score of the decrypted message under a language model. We focus on homophonic substitution ciphers, where the encryption key is a 1-to-many mapping from a plaintext alphabet to a cipher alphabet. We use a simple method introduced by Knight et al. (2006): the EM algorithm (Dempster et al., 1977) is used to learn the emission parameters of an HMM that has a character trigram language model as a backbone and the ciphertext as the observed sequence of emissions. This means that we learn a multinomial over cipher symbols for each plaintext character, but do not learn transition 874 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 874–878, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics parameters, which are fixed by the language model. We predict the deciphered text using posterior decoding in the</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur Dempster, Nan Laird, and Donald Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Anish Nair</author>
<author>Nishit Rathod</author>
<author>Kenji Yamada</author>
</authors>
<title>Unsupervised analysis for decipherment problems.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1123" citStr="Knight et al., 2006" startWordPosition="166" endWordPosition="169">w random restarts. For particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340. 1 Introduction What can a million restarts do for decipherment? EM frequently gets stuck in local optima, so running between ten and a hundred random restarts is common practice (Knight et al., 2006; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). But, how important are random restarts and how many random restarts does it take to saturate gains in accuracy? We find that the answer depends on the cipher. We look at both Zodiac 408, a famous homophonic substitution cipher, and a more difficult homophonic cipher constructed to match properties of the famously unsolved Zodiac 340. Gains in accuracy saturate after only a hundred random restarts for Zodiac 408, but for the constructed cipher we see large gains in accuracy even as we scale the number of random restarts up into the hun</context>
<context position="2554" citStr="Knight et al., 2006" startWordPosition="403" endWordPosition="406">e helpful for performing exploratory analysis. We look at some empirical properties of decipherment problems, visualizing the distribution of local optima encountered by EM both in a successful decipherment of a homophonic cipher and in an unsuccessful attempt to decipher Zodiac 340. Finally, we attack a series of ciphers generated to match properties of Zodiac 340 and use the results to argue that Zodiac 340 is likely not a homophonic cipher under the commonly assumed linearization order. 2 Decipherment Model Various types of ciphers have been tackled by the NLP community with great success (Knight et al., 2006; Snyder et al., 2010; Ravi and Knight, 2011). Many of these approaches learn an encryption key by maximizing the score of the decrypted message under a language model. We focus on homophonic substitution ciphers, where the encryption key is a 1-to-many mapping from a plaintext alphabet to a cipher alphabet. We use a simple method introduced by Knight et al. (2006): the EM algorithm (Dempster et al., 1977) is used to learn the emission parameters of an HMM that has a character trigram language model as a backbone and the ciphertext as the observed sequence of emissions. This means that we lear</context>
</contexts>
<marker>Knight, Nair, Rathod, Yamada, 2006</marker>
<rawString>Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Yamada. 2006. Unsupervised analysis for decipherment problems. In Proceedings of the 2006 Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Nickolls</author>
<author>Ian Buck</author>
<author>Michael Garland</author>
<author>Kevin Skadron</author>
</authors>
<title>Scalable parallel programming with CUDA.</title>
<date>2008</date>
<publisher>Queue.</publisher>
<contexts>
<context position="3982" citStr="Nickolls et al., 2008" startWordPosition="629" endWordPosition="632">le, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics parameters, which are fixed by the language model. We predict the deciphered text using posterior decoding in the learned HMM. 2.1 Implementation Running multiple random restarts means running EM to convergence multiple times, which can be computationally intensive; luckily, restarts can be run in parallel. This kind of parallelism is a good fit for the Same Instruction Multiple Thread (SIMT) hardware paradigm implemented by modern GPUs. We implemented EM with parallel random restarts using the CUDA API (Nickolls et al., 2008). With a GPU workstation,1 we can complete a million random restarts roughly a thousand times more quickly than we can complete the same computation with a serial implementation on a CPU. 0.8 0.7 0.4 0.3 0.2 0.1 -1530 1 10 100 1000 10000 100000 1e+06 Log likelihood Accuracy -1460 -1470 -1480 -1490 -1500 -1510 -1520 Log likelihood Number of random restarts Figure 1: Zodiac 408 cipher. Accuracy by best model score and best model score vs. number of random restarts. Bootstrapped from 1M random restarts. Accuracy 0.6 0.5 1 0.9 3 Experiments We ran experiments on several homophonic substitution cip</context>
</contexts>
<marker>Nickolls, Buck, Garland, Skadron, 2008</marker>
<rawString>John Nickolls, Ian Buck, Michael Garland, and Kevin Skadron. 2008. Scalable parallel programming with CUDA. Queue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Attacking decipherment problems optimally with low-order n-gram models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="9277" citStr="Ravi and Knight, 2008" startWordPosition="1521" endWordPosition="1524">s −1466.5, which means we have surpassed the gold initialization. The accuracy after gold initialization was 92%, while the accuracy of the best local optimum was only 89%. This suggests that the global optimum may not be worth finding if we haven’t already found it. From Figure 1, it appears that large increases in likelihood are correlated with increases in accuracy, but small improvements to high likelihoods (e.g. the best local optimum versus the gold initialization) may not to be. 5ILP solvers can be used to globally optimize objectives corresponding to short 1-to-1 substitution ciphers (Ravi and Knight, 2008) (though these objectives are slightly different from the likelihood objectives faced by EM), but we find that ILP encodings for even the shortest homophonic ciphers cannot be optimized in any reasonable amount of time. Number of random restarts Figure 2: Synth 340 cipher. Accuracy by best model score and best model score vs. number of random restarts. Bootstrapped from 1M random restarts. 3.3 A Hard Cipher: Synth 340 What do these graphs look like for a harder cipher? Zodiac 340 is the second cipher released by the Zodiac killer, and it remains unsolved to this day. However, it is unknown whe</context>
</contexts>
<marker>Ravi, Knight, 2008</marker>
<rawString>Sujith Ravi and Kevin Knight. 2008. Attacking decipherment problems optimally with low-order n-gram models. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Bayesian inference for Zodiac and other homophonic ciphers.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Annual Meeting of the Association</booktitle>
<contexts>
<context position="1146" citStr="Ravi and Knight, 2011" startWordPosition="170" endWordPosition="173">r particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340. 1 Introduction What can a million restarts do for decipherment? EM frequently gets stuck in local optima, so running between ten and a hundred random restarts is common practice (Knight et al., 2006; Ravi and Knight, 2011; Berg-Kirkpatrick and Klein, 2011). But, how important are random restarts and how many random restarts does it take to saturate gains in accuracy? We find that the answer depends on the cipher. We look at both Zodiac 408, a famous homophonic substitution cipher, and a more difficult homophonic cipher constructed to match properties of the famously unsolved Zodiac 340. Gains in accuracy saturate after only a hundred random restarts for Zodiac 408, but for the constructed cipher we see large gains in accuracy even as we scale the number of random restarts up into the hundred thousands. In both</context>
<context position="2599" citStr="Ravi and Knight, 2011" startWordPosition="411" endWordPosition="414">sis. We look at some empirical properties of decipherment problems, visualizing the distribution of local optima encountered by EM both in a successful decipherment of a homophonic cipher and in an unsuccessful attempt to decipher Zodiac 340. Finally, we attack a series of ciphers generated to match properties of Zodiac 340 and use the results to argue that Zodiac 340 is likely not a homophonic cipher under the commonly assumed linearization order. 2 Decipherment Model Various types of ciphers have been tackled by the NLP community with great success (Knight et al., 2006; Snyder et al., 2010; Ravi and Knight, 2011). Many of these approaches learn an encryption key by maximizing the score of the decrypted message under a language model. We focus on homophonic substitution ciphers, where the encryption key is a 1-to-many mapping from a plaintext alphabet to a cipher alphabet. We use a simple method introduced by Knight et al. (2006): the EM algorithm (Dempster et al., 1977) is used to learn the emission parameters of an HMM that has a character trigram language model as a backbone and the ciphertext as the observed sequence of emissions. This means that we learn a multinomial over cipher symbols for each </context>
<context position="6689" citStr="Ravi and Knight (2011)" startWordPosition="1075" endWordPosition="1078">enerated ciphers. In all experiments we used a trigram character language model that was linearly interpolated from character unigram, bigram, and trigram counts extracted from both the Google N-gram dataset (Brants and Franz, 2006) and a small corpus (about 2K words) of plaintext messages authored by the Zodiac killer.4 3.2 An Easy Cipher: Zodiac 408 Zodiac 408 is a homophonic cipher that is 408 characters long and contains 54 different cipher symbols. Produced by the Zodiac killer, this cipher was solved, manually, by two amateur code-breakers a week after its release to the public in 1969. Ravi and Knight (2011) were the first to crack Zodiac 408 using completely automatic methods. In our first experiment, we compare a decode of Zodiac 408 using one random restart to a decode using 100 random restarts. Random restarts have high 3While this does not guarantee convergence, in practice 200 iterations seems to be sufficient for the problems we looked at. 4The interpolation between n-gram orders is uniform, and the interpolation between corpora favors the Zodiac corpus with weight 0.9. 875 Accuracy variance, so when we present the accuracy corresponding to a given number of restarts we present an average </context>
</contexts>
<marker>Ravi, Knight, 2011</marker>
<rawString>Sujith Ravi and Kevin Knight. 2011. Bayesian inference for Zodiac and other homophonic ciphers. In Proceedings of the 2011 Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
<author>Kevin Knight</author>
</authors>
<title>A statistical model for lost language decipherment.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2575" citStr="Snyder et al., 2010" startWordPosition="407" endWordPosition="410">ing exploratory analysis. We look at some empirical properties of decipherment problems, visualizing the distribution of local optima encountered by EM both in a successful decipherment of a homophonic cipher and in an unsuccessful attempt to decipher Zodiac 340. Finally, we attack a series of ciphers generated to match properties of Zodiac 340 and use the results to argue that Zodiac 340 is likely not a homophonic cipher under the commonly assumed linearization order. 2 Decipherment Model Various types of ciphers have been tackled by the NLP community with great success (Knight et al., 2006; Snyder et al., 2010; Ravi and Knight, 2011). Many of these approaches learn an encryption key by maximizing the score of the decrypted message under a language model. We focus on homophonic substitution ciphers, where the encryption key is a 1-to-many mapping from a plaintext alphabet to a cipher alphabet. We use a simple method introduced by Knight et al. (2006): the EM algorithm (Dempster et al., 1977) is used to learn the emission parameters of an HMM that has a character trigram language model as a backbone and the ciphertext as the observed sequence of emissions. This means that we learn a multinomial over </context>
</contexts>
<marker>Snyder, Barzilay, Knight, 2010</marker>
<rawString>Benjamin Snyder, Regina Barzilay, and Kevin Knight. 2010. A statistical model for lost language decipherment. In Proceedings of the 2010 Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
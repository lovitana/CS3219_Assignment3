<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.757684333333333">
Summarizing Complex Events: a Cross-modal Solution of Storylines
Extraction and Reconstruction
Shize Xu Shanshan Wang Yan Zhang∗
</title>
<author confidence="0.384498">
xsz@pku.edu.cn cheers echo mch@163.com zhy@cis.pku.edu.cn
</author>
<affiliation confidence="0.6012975">
Department of Machine Intelligence, Peking University, Beijing, China
Key Laboratory on Machine Perception, Ministry of Education, Beijing, China
</affiliation>
<sectionHeader confidence="0.984777" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998555">
The rapid development of Web2.0 leads to
significant information redundancy. Espe-
cially for a complex news event, it is diffi-
cult to understand its general idea within a
single coherent picture. A complex event of-
ten contains branches, intertwining narratives
and side news which are all called storylines.
In this paper, we propose a novel solution to
tackle the challenging problem of storylines
extraction and reconstruction. Specifically, we
first investigate two requisite properties of an
ideal storyline. Then a unified algorithm is
devised to extract all effective storylines by
optimizing these properties at the same time.
Finally, we reconstruct all extracted lines and
generate the high-quality story map. Exper-
iments on real-world datasets show that our
method is quite efficient and highly compet-
itive, which can bring about quicker, clearer
and deeper comprehension to readers.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988678066666667">
News reports usually consist of various modalities
of tremendous information, especially all kinds of
textual information and visual information, which
make web users dazzled and lost. The situation gets
worse on complex news events. To help readers
quickly grasp the general information of the news,
a more concise and convenient system over multi-
modality information should be provided. For ex-
ample, given a large collection of texts and images
related to a specified news event (e.g., East Japan
∗Corresponding author
Earthquake), such a system should present a terse
and brief summarization about the event by showing
different clues of its development, and thus helping
readers to effectively find out “when, where, what,
how and why” at a glance.
The researches (Goldstein et al., 2000) on auto-
matic multi-document summarization (MDS) have
helped a lot when we generate a description for a
specific event. However, it traditionally exhibits in a
very simple style like a “0-dimensional” point. The
appearance of Timeline (Allan et al., 2001) brings
about a visual progress for massive documents anal-
yses. Readers can not only get the most important
ideas, but also browse the story evolution in chrono-
logical order. Previous news summarization systems
with structured output (Yan et al., 2011) have fo-
cused on timeline generation. Timeline becomes a
“1-dimensional” line. This style of summarization
only works for simple stories, which are linear in na-
ture. However, the structure of complex stories usu-
ally turns out to be non-linear. These stories branch
into storylines, dead ends, intertwining narratives
and side news. To explore these lines, we need a
map to reorganize all the information. Therefore,
a “2-dimensional” story map is in bad need. Fig-
ure 1 shows a part of the story map generated by our
system for representing East Japan Earthquake. We
notice that the whole event evolves into 4 branches.
Each of them focuses on a specific sub-topic and is
distinct from other lines. Figure 2 takes a close look
at the 4 nodes from different lines, and they differ a
lot from each other as expected.
Text information is more precise and exquisite
when compared with images. Nevertheless, as the
</bodyText>
<page confidence="0.92252">
1281
</page>
<note confidence="0.94368825">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1281–1291,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
Blue Nuclear Crisis Black Post- Green Tsunamis
disaster Red Rescue
</note>
<bodyText confidence="0.953350777777778">
Japan&apos;s nuclear safety agency says the
cooling system of a third nuclear reactor at
Fukushima has failed.
A government spokesman says the blast
destroyed a building which housed a nuclear
actor, but the reactor escaped unscathed.
reAround 170,000 people have been
evacuated from a 12-mile radius around the
Fukushima number one nuclear plant.
A 9.0 magnitude quake triggers a devastating tsunami off
northeast Japan, leaving some 19,000 people dead or missing.
A massive earthquake, 8.9 on the Richter scale, unleashes a
huge tsunami which crashes through Japan&apos;s eastern
coastline, sweeping buildings, boats, cars and people miles
inland.
Japan&apos;s most powerful earthquake since records began has
struck the north-east coast, triggering a massive tsunami.
........
</bodyText>
<figure confidence="0.424182">
····
A Japanese rescue team member walks through the completely leveled village of Saito, in northeastern Japan.
Half a million people have been made homeless by the devastating quake.
Fire crews from Greater Manchester and Lancashire have flown out to Japan as part of the UK&apos;s International
Search and Rescue team.
····
• Storyline
• Story node
• Candidate
</figure>
<bodyText confidence="0.9426861">
Japan&apos;s cabinet on Friday approved a
$49 billion budget to help in the
reconstruction of areas decimated by last
onth&apos;s earthquake and tsunami.
mJapan faces a gnstruction bill of at
least $180 billion, cor 3 percent of its
annual economic output.
In an televised statement after the blast,
prime minister Kan urges those within 19
miles of the area to stay indoors.
</bodyText>
<note confidence="0.5329905">
····
Mar 11 Mar 12 Mar 13 Mar 14 Mar 15 Mar 16 Mar 17, 2011
</note>
<figureCaption confidence="0.99563">
Figure 1: Four storylines are obtained in the story map of “East Japan Earthquake”. They focus on Tsunamis, Nuclear
Crisis, Rescue and Post-disaster respectively.
</figureCaption>
<bodyText confidence="0.99984188">
saying goes, “a picture paints a thousand words”,
an image could provide far more information than
words do. In fact, a summarization including both
texts and images will absolutely yield a more pow-
erful and intuitive description about the news event.
Under this motivation, we study on extracting and
reconstructing tracks with different sub-topics for a
complex event. To the best of our knowledge, the ex-
ploration and analysis of 2-dimensional cross-modal
summarization is academically novel.
We are faced with two main problems. The first
is how to select the most important sentences and
images to make up the final story map. The previ-
ous work by Shahaf et al. presents a 2-D story map
called “metro map”, which summarizes the com-
plex topics (Shahaf et al., 2012). They study on the
document-level, and use the entire news document
as one story node. But on real web, this may con-
front some difficulties. On one hand, news articles
may report the event from different perspectives, es-
pecially those reviews or retrospective reports. This
kind of documents contains many useful saliency in-
formation of different sub-topics, but they cannot
be further subdivided to help understand each bet-
ter. On the other hand, some documents, such as
</bodyText>
<subsectionHeader confidence="0.669032">
Line of “Rescue” on Mar 13 Line “Post-disaster” on Mar 17
</subsectionHeader>
<figureCaption confidence="0.9401435">
Figure 2: Word distributions vary a lot among nodes in
different storylines
</figureCaption>
<bodyText confidence="0.977814625">
cover news and interviewing reports, contain one or
two famous remarks. Since these documents also in-
clude too much useless information, it’s inappropri-
ate to use the whole document as a story node. So
our work, the sentence-level story map extraction,
is just aimed at this brand new problem. The sec-
ond problem is the way to utilize the cross-modal
Line of “Tsunamis” on Mar 11 Line of “Nuclear Explosion” on Mar 12
</bodyText>
<page confidence="0.931957">
1282
</page>
<bodyText confidence="0.99997588">
information suitably. Our goal is not only to fuse
sentences and images together, but also to provide a
unified framework to improve them mutually.
In this paper, we introduce a novel solution for
the story map summarization problem. All the sen-
tences and images are the candidates for making up
the final map. The analysis of complicated infor-
mation usually requires a semantic-level knowledge
study. We address this in the pre-processing of data
in Section 3.1. The key task of our research is the
extraction of storylines. We reveal two fundamental
properties of an ideal storyline, and propose an op-
timization algorithm in Section 3.2. A highly com-
patible MDS sub-algorithm is also fused in and of-
fers help to the sentences/images selection. In Sec-
tion 3.3, the extracted storylines are reconstructed
as a final story map. The experimental results con-
ducted on four real datasets show that our approach
can perform effectively.
The rest of this paper is organized as follows.
Some related researches are demonstrated in Sec-
tion 2. We introduce our methodology in Section 3.
The experimental results in Section 4 prove the ef-
fectiveness of our approach. Finally, we conclude
this paper and present our future work in Section 5.
</bodyText>
<sectionHeader confidence="0.999731" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999894416666667">
Generally speaking, multi-document summarization
can be either extractive or abstractive. Researchers
mainly focus on the former which extracts the in-
formation deemed most important to the summary.
Various techniques have been used for this type of
MDS (Haghighi and Vanderwende, 2009; Contrac-
tor et al., 2012). Graph-based text summarization
techniques have been widely used for years. The
algorithms, used in TextRank (Mihalcea and Tarau,
2005) and LexPageRank (Radev et al., 2004), which
are meant to compute sentence importance, are sim-
ilar to those in PageRank and HITS.
Recently, timeline becomes a popular style to
present a schedule of events and attracts many re-
searchers consequently. For example, Yan et al.
make use of timestamps to generate an evolutionary
timeline (Yan et al., 2011). Shahaf et al. present a
2-D story map called “metro map” to summarize the
complex topic (Shahaf et al., 2012). But previous
work only studies on the document level, which in-
evitably brings about much information redundancy.
Previous studies show that the use of visual ma-
terials not only leads to the conservation of infor-
mation but also promotes comprehension (Panjwani
et al., 2009). Thus the cross-modal fusion is nec-
essary. Wu et al. propose a framework of multi-
modal information fusion for multimedia data anal-
ysis by learning the optimal combination of multi-
modal information with the superkernel fusion (Wu
et al., 2004). Borrowing the idea of recommendation
in heterogeneous network into the cross-modal news
summarization is also a convincing research. Xu et
al. tackle this task and bring out an 1-D cross-media
timeline generation framework (Xu et al., 2013).
Summarization of multimedia involves researches
on information retrieval of multimedia. Since tex-
tual and visual information are quite different from
each other, how to make a good transformation to
mine latent knowledge from unannotated images is
of great concern. Feng and Lapata (2010) use visual
words to describe visual features and then propose
a probabilistic model based on the assumption that
images and their co-occurring textual data are gen-
erated by mixtures of latent topics.
However, to the best of our knowledge, no exist-
ing research manages to generate a 2-dimensional
story map automatically and integrate images and
texts into a unified framework at the same time.
</bodyText>
<sectionHeader confidence="0.997939" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999990705882353">
The original data of one event is a collection of news
documents on different days, or in a finer granu-
larity, a set of sentences and images with different
timestamps. Each item in the data collection is a
candidate for selection to form the final map. We
denote the data collection as C, and C = C3 U C,,,
where C3 is the subset containing all sentences and
C„ contains all images. In the following elaboration
of our method, dateset C is the important knowl-
edge base. As the ultimate goal for a specific event,
we would like to generate the “2-D” story map M,
whose main component is a set of “1-D” storylines
L = {L1, L2,...}. Each storyline L E L is made
up of a set of “0-D” story nodes, L = {I1, I2, ...},
each of which is composed of a set of candidates
sharing the same timestamp I = {C1, C2, ...}. For
more concise, our method can be scheduled with the
</bodyText>
<page confidence="0.908009">
1283
</page>
<bodyText confidence="0.851416">
following three steps:
</bodyText>
<listItem confidence="0.998306666666667">
1. Prepare semantic knowledge for each candi-
date, and then purify data collection C by eliminat-
ing the noisy candidates;
2. Conduct OPT-LSH algorithm to extract L that
contains all qualified storylines;
3. Reconstruct the storylines as the final map M.
</listItem>
<subsectionHeader confidence="0.995397">
3.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.997069397058824">
Before we extract the storylines, we have to solve
two problems first. Since our work is cross-modal,
the semantic knowledge under the literal and visual
surface is basically required. Recall from Section 2,
there exist many effective ways to dig into the se-
mantic level of image and text. In this paper, we em-
ploy the approach proposed by Jiang and Tan (2006).
They present a convincing approach which employs
a multilingual retrieval model to apply knowledge
mining on semantic level. The first is the sentence
feature vector generation. With the preprocessing
such as stemming and stop words removal, they ex-
tract the textual TF-IDF feature V ecs of each sen-
tence. The second, also the challenging part, is
the image feature vector generation. In this step,
for each region they extract visual features that are
consisting of 6 color features and 60 gabor features
which have been proven to be useful in many ap-
plications. Color features are the means and vari-
ances of the RGB color spaces. Gabor features are
extracted by calculating the means and variations of
the filtered image regions on 6 orientations. After
the visual feature vectors of the image regions are
extracted, all image regions are clustered using the
k-means algorithm. The generated clusters, called
“visterms” or “visual words”, are treated as a vocab-
ulary for the images. Besides visual features, they
also utilize the context textual feature of each im-
age as the semantic supplement to generate to final
feature vector V ecv.
Based on these feature vector of each canti-
date, they further calculating the intra-modal sim-
ilarity with classical IR methods, they obtain the
inter-modal similarity through the vague transfor-
mation (Mandl T, 1998). We also note that trans-
lation tools such as VIPS (Cai et al., 2003) and We-
bKit&apos; can help us to segment web documents and
lhttp://www.webkit.org
pick those text blocks whose coordinates are neigh-
boring to each specified image. In this way we can
successfully obtain images, text contexts and con-
tent sentences. Three kinds of semantic similar-
ity are now ready. They are uniformly denoted as
sim(cz, cj), representing the similarity between two
candidates. cz and cj can be any type of modalities.
Another problem is the noise from irregular data.
We would like to utilize the sentences and images of
high quality. The intuitive assumption is that a good
candidate should have substance in speech, and be
coherent with other good candidates. Fortunately,
many useful measures are now available. They can
be used to choose better candidates. Inspired by the
analysis analogy to information retrieval, we extend
the idea of the classical PageRank algorithm to esti-
mate the authority for each candidate. The similarity
between two candidates is regarded as the weighted
“link” between them.
Inspired by the idea of classic “update summa-
rization” task, we try to avoid those chronologically
ordered documents sets focusing on a constant topic.
Therefore, given the particularity of our task, we
also have to develop the weighting function Γ with
the temporal factor before starting the ranking algo-
rithm. Our fundamental assumption is that the inter-
date and inter-modal “links” have different influence
comparing with the intra ones. Therefore the core
formula calculating the authority of cz is adapted as
follows to make it become weight-compatible:
</bodyText>
<equation confidence="0.9972492">
1 − q �
Auth(cz) = |C |+ q · [ α
� Auth(ck)
+ (1 − α) Γ(cz, ck) ·
ckEC″ O(ck) ]
</equation>
<bodyText confidence="0.999888888888889">
C′ is the subset of C which only includes the can-
didates of the same modality with cz, and C″is the
cross-modal candidates subset. O(cj) denotes the
out-degree of cj, and smoothing parameter q is set
as the common value 0.85. Parameter α is used to
balance the biases of intra- and inter-modal impact.
If α is set to 1, it means the cross-modal information
is abandoned, and vice versa. Γ(cz, cj) contains two
terms as follows:
</bodyText>
<equation confidence="0.9181325">
Γ(cz,cj) = sim(cz,cj) · e
cjEC′
Γ c�
( i, �) c� Auth(cj
O(cj)
−ll.0−-j All
</equation>
<page confidence="0.747623">
2σ2
1284
</page>
<bodyText confidence="0.999990875">
The second term of Γ’s formula is Gaussian Ker-
nel (Aliguliyev R, 2009), which is used to measure
the temporal gap between two candidates (*.t de-
notes the date-based timestamp). Note that the simi-
larity metric is content-based and time-independent,
since the time decay function is only used to adjust
the ranking impact strength. In this way, we can
give higher authority to the more informative and
coherent candidates. The optimal value of σ, which
controls the spread of kernel curves, is sensitive to
datasets and will be discussed later.
We eliminate the candidates whose authority goes
under threshold. The data collection C is then sig-
nificantly downsized and purified for later work.
Authority is also used to determine the presentation
sequence inside each story node of final map.
</bodyText>
<subsectionHeader confidence="0.998612">
3.2 Extraction: LSH-OPT Algorithm
</subsectionHeader>
<bodyText confidence="0.999976444444445">
Other traditional relative methods need to pre-decide
how many sub-topics are going to be obtained, like
clustering or other supervised models. They fail
in the unsupervised automatic storylines extraction
problem. In this Section, we propose a concrete al-
gorithm to extract storyline set L from C. Our pro-
posed LSH-based algorithm can automatically op-
timize the number of storylines according to their
self-evaluation.
</bodyText>
<subsubsectionHeader confidence="0.507011">
3.2.1 Task Formalization
</subsubsectionHeader>
<bodyText confidence="0.991243923076923">
Let’s investigate the storyline first. We may think
of some basic attributes as well as many exten-
sion properties. The number of nodes in the sto-
ryline L (denoted as |L|) is intuitively one of its
basic attributes. We would like to define another
basic attribute called the SUPPORT of L. In de-
tail, Support(L) = minIkEL |Ik|, which denotes
the smallest size among all the story nodes it has.
The most challenging part is to properly model
extension properties. We observe that an effective
storyline should meet three key requirements: (1)
Coherence. Within one storyline, news changes
gradually as time goes and the evolution indicates
consistency among component story nodes. We rely
on the notion of coherence developed in Connect-
the-Dots (Shahaf and Guestrin, 2010) and transform
it to what we exactly need in this research; (2) Di-
versity. According to MMR principle (Goldstein et
al., 1999), though the work is about summary, we
still can draw an analogy and derive that a good sto-
ryline should be concise and contain redundant in-
formation as few as possible, i.e., two sentences pro-
viding information of similar content should not be
presented in different storylines; (3) Coverage. The
extracted storyline set L should keep alignment with
the source collection C, which is intuitive and even
proved to be significant as proposed in (Li et al.,
2009). However, Coverage in some ways is tech-
nically redundant in front of Diversity. We decide
to use the first two criteria in extraction process and
use the last one to verify the effectiveness.
Since a storyline is composed of several nodes,
we can select or abandon nodes mainly according
to these two requirements. In fact, both of them in-
volve a measurement of similarity between two story
nodes, denoted by two word distributions (see Fig-
ure 2). Specifically, for story node Ii, its distribu-
tion probability of word w is estimated as p(w|Ii) =
∑
</bodyText>
<equation confidence="0.680493">
c∈Ii TF(w)
</equation>
<bodyText confidence="0.9986681">
In addition, we introduce the decreasing and
increasing variants based on logistic functions,
D♭KL = 1/(1 + eDKL) and D♯KL = eDKL/(1 +
eDKL), to map the distance into [0, 1]. Given the
measurement, we can formulate the two properties.
For Coherence, a storyline Li consists of a series
of individual but correlated nodes, which do not nec-
essarily have the serial timestamps. We would like
to choose such a set of nodes {I1, I2, ...}, and at the
same time guarantee this criterion:
</bodyText>
<equation confidence="0.922339">
1 Cor(Li) = |Li |�1&lt;k&lt;|Li |D♭KL(Ik,Ik+1)
</equation>
<bodyText confidence="0.999760166666667">
For Diversity, each storyline Li E L should
demonstrate quite different subtopics with other sto-
rylines. This is the most essential motivation for us
to step into 2-dimensional field. This criterion can
be used to maximize the minimum diversity value
among all storylines:
</bodyText>
<equation confidence="0.749096333333333">
E EIk′ELj D♯ KL(Ik,Ik′)
IkELi
|Li |- |Lj|
</equation>
<bodyText confidence="0.5998046">
∑ ∑c∈Ii TF(w) where the denominator is used for
w
normalization. Then Kullback-Leibler divergence is
employed to denote the distance between two nodes
Ii and Ij:
</bodyText>
<equation confidence="0.492801833333333">
� p(w|Ii) log p(w|Ii)
DKL(Ii, Ij) = p(w|Ij)
w
Div(L) = min {
Li,LjEL
}
</equation>
<page confidence="0.932361">
1285
</page>
<bodyText confidence="0.999795333333333">
Then the problem can be transformed into the fol-
lowing optimization problem. Parameters θ1, θ2 and
θ3 denote the minimum number of nodes in each
line, the smallest size of candidates in each node and
the coherence lower bound respectively. The task is
to extract an optimal L out of C, such that:
</bodyText>
<equation confidence="0.972872">
VL E L, |L |&gt; θ1 &amp; Support(L) &gt; θ2;
VL E L, Cor(L) &gt; θ3;
Div(L) is maximized.
</equation>
<subsubsectionHeader confidence="0.53336">
3.2.2 Optimization Algorithm
</subsubsectionHeader>
<bodyText confidence="0.999982818181818">
It can be proved that finding the optimal set L is
an NP-Complete problem (not presented due to the
limited space). Thus the brute-force exhaustive ap-
proach is crashed. We develop a near-optimal al-
gorithm based on locality sensitive hashing (OPT-
LSH). The original LSH solution is a popular tech-
nique used to solve the nearest neighbor search
problems in high dimensions. Its basic idea is to
hash similar input items into the same bucket (i.e.,
uniquely definable hash signature) with high proba-
bility. All potential storylines can be targeted fast if
we make good use of this idea.
LSH performs probabilistic dimension reduction
of high dimensional data by projecting a higher d-
dimensional vector V ec, (recall from Section 3.1)
to a lower d′-dimensional vector (d′&lt;&lt;d), such that
the candidates which are in close proximity in the
higher dimension get mapped into the same item in
the lower dimensional space with high probability.
It guarantees a lower bound on the probability that
two similar input items fall into the same bucket in
the projected space and also the upper bound on the
probability that two dissimilar vectors fall into the
same bucket (Indyk and Motwani, 1998).
One of the key requirements for good perfor-
mance of LSH is the careful selection of the fam-
ily of hashing functions. In OPT-LSH, we use the
hashing scheme proposed by Charikar (Charikar M,
2002). In detail, d′ random unit d-dimensional vec-
tors ⃗r1, ⃗r2,... , ⃗rd′ are generated first. Each of the
d entries of ⃗r� is drawn from a standardized normal
distribution N(0,1). Then the d′ hashing functions
are defined as:
</bodyText>
<equation confidence="0.949142333333333">
�
1, if ⃗r� · V ec(c) &gt; 0
h�(c) =0, if ⃗r� · V ec(c) &lt; 0 1 � i � d′
</equation>
<bodyText confidence="0.999993583333333">
We represent the d′-dimensional bucket feature h
for c, h(c) := [h1(c), ... , hd′(c)]. There are 2d′ dif-
ferent buckets at most. Each denotes a potential sto-
ryline, so we have to verify the probability of similar
candidates falling into the same bucket, whose lower
bound is given by Charikar (Charikar M, 2002).
Simply filtering and searching among all poten-
tial lines in single pass may lead to empty result
set if in post-processing no bucket satisfies all con-
straints. This could probably happen because the
input parameter d′ is set so large that the optimal
set of candidates is separated into different buck-
ets. However, we will get a suboptimal result in
turn when d′ is too small. We are then motivated
to tune LSH by iterative relaxation that varies d′ in
each iteration. Changing the value of d′ balances
the leverage between expected number of potential
storylines and their properties. We perform a binary
search between 1 and d′ to identify the ideal number
of hash functions to employ. Algorithm 1 shows the
pseudo code of our OPT-LSH algorithm. The LSH
time is bounded by O(d′|C |log |C|) since the binary
search relaxation iteration runs for log |C |times in
the worst case and the hashing time is O(d′|C|).
</bodyText>
<subsectionHeader confidence="0.999391">
3.3 Storylines Reconstruction
</subsectionHeader>
<bodyText confidence="0.999988214285714">
At last, we manage to reconstruct all the storylines
in LO. A real-world storyline may sometimes in-
tertwine with another, educe other branches, and
end its own evolvement. The way to reconstruct a
more effective layout of the story map requires fur-
ther study and provides a good research direction in
the future. However, in this paper we order the sen-
tences/images in each story node according to their
authority scores. Next, all storylines are arranged to
proceed along the timestamps, thus a storyline never
turns back in the map. Then we adjust the structure
to make story nodes sharing the same timestamp stay
close, though they belong to different lines. Figure 1
shows the sample output of our system.
</bodyText>
<sectionHeader confidence="0.999544" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.921671">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.99975975">
There is no existing standard evaluation data set for
2-dimensional cross-modal summarization methods.
We randomly choose 4 news topics from 4 selected
news websites: New York Times, BBC, CNN and
</bodyText>
<page confidence="0.979526">
1286
</page>
<figure confidence="0.4071716">
Algorithm 1 OPT-LSH Algorithm
Input: Candidate set C, similarity function sim,
bucket dimensions d′
Output: A near-optimal storylines set L°pt
// Main Algorithm
</figure>
<listItem confidence="0.970623724137931">
1: Initialize left = 1, right = d′, max = −1
2: repeat
3: d′ = (left + right)/2
4: L L5H(C, d′)
5: Revise all word distributions p(w|I) in L
6: if VLEL, |L|&gt;01 and 5upport(L)&gt;02 and
Cor(L)&gt;δ3 then
7: if Div(L) &gt; max then
8: L°pt L, max Div(L)
9: end if
10: left = d′ + 1
11: else
12: right = d′ − 1
13: end if
14: until left &gt; right
15: return L°pt
// L5H(C, d′) : Buckets
16: Generate d′ unit vectors randomly
17: for j = 1 to |C |do
18: for i = 1 to d′ do
19: if ⃗ri · V ec(cj) &gt; 0 then
20: hi(cj) 1
21: else
22: hi(cj) 0
23: end if
24: end for
25: h(cj) = [h1(cj), ... , hd′(cj)]
26: end for
27: return Buckets {h(cj)|cj E C}
</listItem>
<bodyText confidence="0.999893666666667">
Reuters. We query each event confined to these sites
and crawl webpages’ html docs. Referring to Sec-
tion 3.1, timestamps, text contents, images and their
text contexts are extracted. Table 1 shows the de-
tails. These 4 datasets all contain massive informa-
tion and complex evolutions.
</bodyText>
<subsectionHeader confidence="0.999264">
4.2 Analysis of Our System
</subsectionHeader>
<bodyText confidence="0.998023666666667">
Since there is no standard for us to verify the ef-
fectiveness of our solution, we have to utilize con-
vincing criteria based on manual evaluation of ex-
</bodyText>
<tableCaption confidence="0.998505">
Table 1: Statistics of Datasets.
</tableCaption>
<table confidence="0.8420755">
Event (Query) Document Time Span σ α
EJE 504 Mar 11-Apr 8, 2011 3 0.9
OWS 638 Sept 17-Dec 10, 2011 12 0.7
NBA 489 July 1-Dec 28, 2011 17 0.6
ME 437 June 21-Aug 31, 2011 9 0.7
* Abbreviations EJE, OWS, NBA, ME denote East Japan
Earthquake, Occupy Wall Street, NBA Lockout and Murdock’s
Eavesdropping respectively.
</table>
<bodyText confidence="0.9998885">
perts, and then we can compare them with other ap-
proaches. In order to setup the system, based on the
optimization problem shown in Section 3.2, we as-
sign the value of 1 to both 01 and 02, and empiri-
cally set 03 as 0.6 which can balance the number of
potential lines and the quality of story map as well.
Then the problem can be re-interpreted in natural
language as follows. Given the data collection, we
manage to find out a set of storylines such that every
line in it contains at least one non-null story node
and keeps self-coherence not less than 0.6. What’s
more, the diversity of the whole set is maximized.
Before comparing OPT-LSH with other systems, we
do some further analysis of inherent properties first.
</bodyText>
<subsectionHeader confidence="0.621647">
4.2.1 Compactness
</subsectionHeader>
<bodyText confidence="0.999948117647059">
The essential idea of summarization is to reduce
the data size, so that a more concise representation
will be generated and help users to fast grasp the
main points. Therefore the Compactness of a story
map needs to be guaranteed. In the pre-processing
module, we have already excluded significant num-
ber of inferior candidates with the extended PageR-
ank. Nevertheless what we really care is the com-
pactness that OPT-LSH brings about. In the candi-
dates and story nodes selection processes, only the
most saliency and coherent candidates can appear
in the final representation. We count the number of
sentences and images in L°pt, denoted as ||L°pt||,
and then we compare it with the collection size |C|
(both before and after pre-processing) to test the
compactness. Table 2 shows that OPT-LSH further
reduces the representation scale significantly.
</bodyText>
<subsectionHeader confidence="0.803554">
4.2.2 Coverage
</subsectionHeader>
<bodyText confidence="0.9999926">
Obviously, only the verification of compactness
is far from enough. As mentioned in Section 3.2,
the storyline set L we extract should keep alignment
with the source collection, and contain informative
as well as comprehensive information in C. Thus we
</bodyText>
<page confidence="0.997818">
1287
</page>
<tableCaption confidence="0.998712">
Table 2: The compactness of OPT-LSH
</tableCaption>
<table confidence="0.9998118">
Dataset EJE OWS NBA ME
|C|-before 12049 22890 20403 10237
|C|-after 1454 2357 1187 1042
||G ||87 168 113 92
Downsizing 94.0% 92.9% 90.5% 91.2%
</table>
<bodyText confidence="0.925508">
need to verify another property of our summariza-
tion, the Coverage. Inspired by (Shannon C, 2001),
we employ the Information Entropy to represent the
information quantity based on solid mathematical
theory. The less information quantity decreases af-
ter summarizing, the more story comprehensiveness
is maintained. In this way we verify the property of
coverage. Particularly, Shannon denotes the entropy
H as follows of a discrete random variable X, which
in fact is a word distribution. The base knowledge
in our work is the global probability mass function
P(WC) based on the entire vocabulary of C, with
possible values {p(wi), ... , p(w|WC|)}.
</bodyText>
<equation confidence="0.998173">
H(X) = E{− log (P(WC))∥X}
�= − p(wi) log (p(wi))
wi∈X
</equation>
<bodyText confidence="0.999971347826087">
Although different word distributions may have
the same H, we do not focus on the similarity of two
corpora, but the difference of information quantities
they are carrying. So H is an ideal criterion.
Besides the comparison of the entropies of C and
G, it gives us a chance to study different modules’
contributions in our solution. There are two places
that we may simplify the solution. One is the fea-
ture of temporal gap. If we set the parameter σ to
infinity, then we can remove the second term of
the calculation of F (i.e. σ←∞) and then bring out
a time-insensitive system. The other is the cross-
modal feature. We set parameter α as 1 to make
the system work in one single modality, and ignore
all images (only the textual sentences are available)
to make up story map. The work then becomes the
study with text-bias. We also implement the sim-
plest system that blocks images as well as tempo-
ral feature. Figure 3 highlights the good perfor-
mance of our system. We are maintaining infor-
mation by a larger proportion of the original data
collection. Considering the property of high com-
pactness, our solution tackles the information re-
</bodyText>
<figure confidence="0.959506333333333">
40
30
20
10
0
EJE OWS NBA ME
</figure>
<figureCaption confidence="0.9966175">
Figure 3: The y-axis denotes the entropy. And the larger
H is, the richer information it brings.
</figureCaption>
<bodyText confidence="0.995046882352941">
dundancy quite well, and promisingly delivers entire
knowledge with compact structure.
During our experiments on Coverage, we have
some interesting findings. Datasets perform differ-
ently when we take different values of σ and α,
which controls the temporal decaying rate and cross-
modal learning respectively. The events with short
life-cycles prefer a smaller value of σ to dominate
the influence from neighbors, as well as the intra-
modal bias. On the contrary, long-living events pre-
fer lager σ and more inter-modal bias to get informa-
tion replenishment from different dates and modal-
ity. Due to the limited space we don’t present the
tuning details, but the optimal values are shown in
Table 1. In fact, using the cross-modal mutual influ-
ence can, more or less, help to improve the effective-
ness of information extraction and summarization.
</bodyText>
<subsectionHeader confidence="0.999805">
4.3 User Study
</subsectionHeader>
<bodyText confidence="0.99993525">
Before we introduce other existing methods that can
also tackle the cross-modal 2-dimensional summa-
rization problem, we have to setup the appropriate
standards to quantify users’ evaluation.
</bodyText>
<subsectionHeader confidence="0.944485">
4.3.1 Metrics
</subsectionHeader>
<bodyText confidence="0.995889">
In the user study, we evaluate the effectiveness of
our story maps in aiding users to integrate different
aspects of multi-faceted information. (Shahaf et al.,
2012) also focuses on story map generation and puts
forward two convincing metrics to answer the fol-
lowing questions:
</bodyText>
<listItem confidence="0.99692875">
• Micro-Knowledge: Can the maps help users re-
trieve information faster than other methods?
• Macro-Knowledge: Can the maps help users un-
derstand the big picture better than other methods?
</listItem>
<figure confidence="0.992783833333333">
H(X)
Original C
Our system
Time-insensitive
Text-bias modal
Simplest
</figure>
<page confidence="0.984079">
1288
</page>
<bodyText confidence="0.999995536585366">
For micro-knowledge, we wish to see how maps
help users answer specific questions. We compare
the level of knowledge attained by users using our
method with two other systems: Google News and
TDT. Google News is a computer-generated site that
aggregates headlines from news sources worldwide.
News-viewing tools are dominated by portal and
search approaches, and Google News is a typical
representative of those tools. TDT (Nallapati et al.,
2004) is a successful system which captures the rich
structure and dependencies of news events.
We have noticed that making comparisons be-
tween different systems is not convincing, since the
output of Google News and TDT is different both in
content and in presentation (and in particular, can-
not be double-blind). In order to isolate the effects
of sentence selection vs. map organization, we in-
troduce a hybrid system into the study: the system
with structureless story map displays the same sen-
tences and images as our system but with none of the
structure. Its output is basically the same with our
full system but with a single storyline and merges
node content for each date. And each story nodes
are sorted chronologically and displayed similarly to
Google News. We implement TDT based on (Nalla-
pati et al., 2004) (cos + TD + SimpleThresholding),
and pick a representative article from each cluster.
The purpose of the study is to test a single query.
We also obtain the results from Google News using
the same queries.
We recruit 64 volunteers to browse all the four
events, and one of the four systems is assigned to
each person randomly. After browsing, users are
asked to answer a short questionnaire (8 questions),
composed by domain experts. Users answer as many
questions as possible in limited time (8 minutes).
The statistics of their answers are promised to eval-
uate the micro-knowledge on different systems. In
order to aid in comprehension, we give some exam-
ples about those asked questions. For the event of
EJE, we ask that
</bodyText>
<listItem confidence="0.9216498">
1. How many magnitude was initially reported by
the USGS, and what about the finally report?
2. List at least six countries that had dispatched
their rescue teams.
3....
</listItem>
<bodyText confidence="0.286127">
And for OW S, we ask that
</bodyText>
<tableCaption confidence="0.99956">
Table 3: Macro-knowledge performance on four datasets
</tableCaption>
<table confidence="0.9997034">
Dataset Our System Google News TDT
EJE 56.3% 23.2% 20.5%
OWS 62.2% 22.1% 15.7%
NBA 58.3% 18.9% 22.8%
ME 47.2% 26.3% 26.5%
</table>
<listItem confidence="0.757179">
1. What was the attitude of President Obama about
the protesters on October?
2. When did the protesters begin dressing “corpo-
rate zombies” in New York?
3....
</listItem>
<bodyText confidence="0.999584611111111">
These questions can effectively help us to investi-
gate users’ micro-knowledge about the events.
As for macro-knowledge, unlike the retrieval
study that evaluates users’ ability to answer ques-
tions, we are interested in the use of story maps as
high-level overviews, allowing users to understand
the big picture. We believe that the ability to ex-
plain a certain issue is the only proof of understand-
ing. Therefore, the 64 volunteers are then asked to
write four paragraphs to summarize the four events
respectively. This time, all three systems’ (the struc-
tureless system presents the same content as our sys-
tem) results are provided and we let users choose the
sentences with complete freedom. Then we count
the number of sentences they employed from each
system and derive the average proportions. Accord-
ing to the results we can research on the macro-
knowledge that different systems deliver.
</bodyText>
<sectionHeader confidence="0.869753" genericHeader="evaluation">
4.3.2 Results
</sectionHeader>
<bodyText confidence="0.999949642857143">
We take the time cost and the average numbers of
correct answers of different systems to evaluate on
the micro-knowledge. Figure 4 shows the results.
We can find out that our system outperforms the
others significantly when users taking less time to
learn the knowledge. The failure of structureless
system proves that our work of storyline reconstruc-
tion makes lots of advantages to help reading.
On the other hand, Table 3 analyzes the statistics
of macro-knowledge. It’s also obvious that users
would like to refer to the sentences that our sys-
tem provides. A reasonable explanation is that the
story maps we generate can clarify users’ thoughts
and views on the complicated events.
</bodyText>
<page confidence="0.985643">
1289
</page>
<figure confidence="0.6245055">
minutes
Murdock&apos;s Eavesdropping
</figure>
<figureCaption confidence="0.994581">
Figure 4: Micro-knowledge performance on four datasets
</figureCaption>
<tableCaption confidence="0.995078">
Table 4: Average runtime of different datasets
</tableCaption>
<table confidence="0.99918825">
Dataset EJE OWS NBA ME
ICI 1454 2357 1187 1042
Iterations 3.5 4.7 5.4 3.7
Runtime (ms) 1582 3214 3089 1314
</table>
<subsectionHeader confidence="0.99425">
4.4 Runtime Analysis
</subsectionHeader>
<bodyText confidence="0.9999808">
At last, we analyze the time performance of our
OPT-LSH algorithm on a PC server (16G RAM,
2.67GHz 4-processors CPU). The average iterations
for different initial value of d′ and the runtime are
shown in Table 4. The results are acceptable.
</bodyText>
<sectionHeader confidence="0.999623" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999987043478261">
In this paper, we study the feasibility of automati-
cally generating cross-modal story maps and present
a novel solution to this challenging problem. Our
works mainly tackle the problems of storylines ex-
traction and reconstruction. Specifically, we inves-
tigate two requisite properties of an ideal storyline,
Coherence and Diversity. Then the convincing cri-
teria are devised to model both. We formalize the
task as an optimization problem and design an al-
gorithm to solve it. Classical IR and text analyzing
techniques like PageRank are fused into the unified
framework, and a near-optimal solution is employed
to deal with the NP-complete problem. Experiments
on web datasets show that our method is quite effi-
cient and competitive. We also verify that it brings
quicker, clearer and deeper comprehension to users.
As a future work, we plan to adapt parame-
ters automatically on the basis of different types of
datasets. Improving the layout quality of story map
by concerning the interactivity of different media
(e.g. images order) is also significant. Furthermore,
our framework is universal, so that the media other
than text and image can be adopted as well.
</bodyText>
<sectionHeader confidence="0.998027" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997225166666667">
We sincerely thank all the anonymous reviewers for
their valuable comments, which have helped to im-
prove this paper greatly.
This work is supported by NSFC with Grant No.
61073081 and 61370054, and 973 Program with
Grant No. 2014CB340405.
</bodyText>
<figure confidence="0.9981041">
minutes
East Japan Earthquake
minutes
Occupy Wall Street
minutes
NBA Lockout
Correct Number
Correct Number
Correct Number
Correct Number
</figure>
<page confidence="0.938362">
1290
</page>
<sectionHeader confidence="0.986997" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999907481132076">
Agrawal R, Gollapudi S, Kannan A, et al. 2011 En-
riching textbooks with images. Proceedings of the 20th
ACM International Conference on Information and
Knowledge Management, ACM, pages 1847-1856.
Aliguliyev R M. 2009 A new sentence similarity mea-
sure and sentence based extractive technique for auto-
matic text summarization. Expert Systems with Appli-
cations, 2009, 36(4): 7764-7772.
Allan J, Gupta R, Khandelwal V. 2001 Temporal sum-
maries of new topics. Proceedings of the 24th Annual
International ACM Conference on SIGIR, pages 10-
18.
Cai D, Yu S, Wen J R, et al. 2003 VIPS: a visionbased
page segmentation algorithm. Microsoft Technical Re-
port, MSR-TR-2003-79.
Charikar M S. 2002 Similarity estimation techniques
from rounding algorithms. Proceedings of the 34th
Annual ACM Symposium on Theory of Computing,
ACM, pages 380-388.
Chen Y, Jin O, Xue G R, et al. 2010 Visual contex-
tual advertising: Bringing textual advertisements to
images. Proceedings of the 24th AAAI Conference,
AAAI, pages 1314-1320.
Contractor D, Guo Y, Korhonen A. 2012. Using Argu-
mentative Zones for Extractive Summarization of Sci-
entific Articles. COLING, pages 663-678.
Evans D K, McKeown K, Klavans J L. 2005 Similarity-
based multilingual multi-document summarization.
IEEE Transactions on Information Theory, pages
1858C1860.
Feng Y, Lapata M. 2010 Topic models for image annota-
tion and text illustration. Human Language Technolo-
gies: The 2010 Annual Conference of NAACL, pages
831-839.
Goldstein J, Mittal V, Carbonell J, Kantrowitz M.
2000 Multi-document summarization by sentence
extraction. Proceedings of the 2000 NAACL-
ANLPWorkshop on Automatic summarization-
Volume 4. Association for Computational Linguistics,
pages 40-48.
Goldstein J, Kantrowitz M, Mittal V, et al. 1999 Summa-
rizing text documents: sentence selection and evalua-
tion metrics. Proceedings of the 22nd Annual Interna-
tional ACM Conference on SIGIR, ACM, pages 121-
128.
Haghighi A, Vanderwende L. 2009. Exploring content
models for multi-document summarization. Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of NAACL, Association for Compu-
tational Linguistics, pages 362-370.
Indyk P, Motwani R. 1998 Approximate nearest neigh-
bors: towards removing the curse of dimensionality.
Proceedings of the 30th Annual ACM Symposium on
Theory of Computing, ACM, pages 604-613.
Jiang T, Tan A H. 2006 Discovering image-text associa-
tions for cross-media web information fusion. Knowl-
edge Discovery in Databases: PKDD 2006, pages 561-
568.
Li L, Zhou K, Xue G R, et al. 2009 Enhancing diver-
sity, coverage and balance for summarization through
structure learning. Proceedings of the 18th Interna-
tional Conference on World Wide Web, ACM, pages
71-80.
Mandl T. 1998 Vague transformations in information
retrieval. ISI 1998, pages 312-325.
Mihalcea R, Tarau P. 2005 A language independent al-
gorithm for single and multiple document summariza-
tion. Proceedings of IJCNLP 2005.
Nallapati R, Feng A, Peng F, et al. 2004 Event threading
within news topics. Proceedings of the 13rd ACM In-
ternational Conference on Information and Knowledge
Management, ACM, pages 446-453.
Panjwani S, Micallef L, Fenech K, et al. 2009 Effects of
integrating digital visual materials with textbook scans
in the classroom. International Journal of Education
and Development using ICT, 2009, 5(3).
Radev D R, Jing H, et al. 2004 Centroid-based summa-
rization of multiple documents. Information Process-
ing &amp; Management, 2004, 40(6): 919-938.
Radev D, Winkel A, Topper M. 2002 Multi document
centroid-based text summarization. ACL Demo Ses-
sion, 2002.
Shahaf D, Guestrin C. 2010 Connecting the dots be-
tween news articles. Proceedings of the 16th ACM
Conference on SIGKDD, ACM, pages 623-632.
Shahaf D, Guestrin C, Horvitz E. 2012 Trains of
thought: Generating information maps. Proceedings
of the 21st International Conference on World Wide
Web, ACM, pages 899-908.
Shannon C E. 2001 A mathematical theory of commu-
nication. ACM SIGMOBILE Mobile Computing and
Communications Review, 2001, 5(1): 3-55.
Wu Y, Chang E Y, Chang K C C, et al. 2004 Optimal
multimodal fusion for multimedia data analysis. Pro-
ceedings of the 12th annual ACM International Con-
ference on Multimedia, ACM, 2004: 572-579.
Xu S, Kong L, Zhang Y. 2013 A cross-media evolution-
ary timeline generation framework based on iterative
recommendation. Proceedings of the 3rd ACM confer-
ence on International Conference on Multimedia Re-
trieval, ACM, pages 73-80.
Yan R, Wan X, Otterbacher J, et al. 2011 Evolution-
ary timeline summarization: a balanced optimization
framework via iterative substitution. Proceedings of
the 34th International ACM Conference on SIGIR,
ACM, pages 745-754.
</reference>
<page confidence="0.991063">
1291
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.496193">
<title confidence="0.999244">Summarizing Complex Events: a Cross-modal Solution of Extraction and Reconstruction</title>
<author confidence="0.77862">Xu Shanshan Wang Yan xszpku edu cn cheers echo mch com zhycis pku edu cn</author>
<affiliation confidence="0.99975">Department of Machine Intelligence, Peking University, Beijing,</affiliation>
<address confidence="0.89966">Key Laboratory on Machine Perception, Ministry of Education, Beijing, China</address>
<abstract confidence="0.999455428571428">The rapid development of Web2.0 leads to significant information redundancy. Especially for a complex news event, it is difficult to understand its general idea within a single coherent picture. A complex event often contains branches, intertwining narratives and side news which are all called storylines. In this paper, we propose a novel solution to tackle the challenging problem of storylines extraction and reconstruction. Specifically, we first investigate two requisite properties of an ideal storyline. Then a unified algorithm is devised to extract all effective storylines by optimizing these properties at the same time. Finally, we reconstruct all extracted lines and generate the high-quality story map. Experiments on real-world datasets show that our method is quite efficient and highly competitive, which can bring about quicker, clearer and deeper comprehension to readers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Agrawal</author>
<author>S Gollapudi</author>
<author>A Kannan</author>
</authors>
<title>Enriching textbooks with images.</title>
<date>2011</date>
<booktitle>Proceedings of the 20th ACM International Conference on Information and Knowledge Management, ACM,</booktitle>
<pages>1847--1856</pages>
<marker>Agrawal, Gollapudi, Kannan, 2011</marker>
<rawString>Agrawal R, Gollapudi S, Kannan A, et al. 2011 Enriching textbooks with images. Proceedings of the 20th ACM International Conference on Information and Knowledge Management, ACM, pages 1847-1856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Aliguliyev</author>
</authors>
<title>A new sentence similarity measure and sentence based extractive technique for automatic text summarization. Expert Systems with Applications,</title>
<date>2009</date>
<volume>36</volume>
<issue>4</issue>
<pages>7764--7772</pages>
<marker>Aliguliyev, 2009</marker>
<rawString>Aliguliyev R M. 2009 A new sentence similarity measure and sentence based extractive technique for automatic text summarization. Expert Systems with Applications, 2009, 36(4): 7764-7772.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>R Gupta</author>
<author>V Khandelwal</author>
</authors>
<title>Temporal summaries of new topics.</title>
<date>2001</date>
<booktitle>Proceedings of the 24th Annual International ACM Conference on SIGIR,</booktitle>
<pages>10--18</pages>
<contexts>
<context position="2297" citStr="Allan et al., 2001" startWordPosition="338" endWordPosition="341">f texts and images related to a specified news event (e.g., East Japan ∗Corresponding author Earthquake), such a system should present a terse and brief summarization about the event by showing different clues of its development, and thus helping readers to effectively find out “when, where, what, how and why” at a glance. The researches (Goldstein et al., 2000) on automatic multi-document summarization (MDS) have helped a lot when we generate a description for a specific event. However, it traditionally exhibits in a very simple style like a “0-dimensional” point. The appearance of Timeline (Allan et al., 2001) brings about a visual progress for massive documents analyses. Readers can not only get the most important ideas, but also browse the story evolution in chronological order. Previous news summarization systems with structured output (Yan et al., 2011) have focused on timeline generation. Timeline becomes a “1-dimensional” line. This style of summarization only works for simple stories, which are linear in nature. However, the structure of complex stories usually turns out to be non-linear. These stories branch into storylines, dead ends, intertwining narratives and side news. To explore these</context>
</contexts>
<marker>Allan, Gupta, Khandelwal, 2001</marker>
<rawString>Allan J, Gupta R, Khandelwal V. 2001 Temporal summaries of new topics. Proceedings of the 24th Annual International ACM Conference on SIGIR, pages 10-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cai</author>
<author>S Yu</author>
<author>J R Wen</author>
</authors>
<title>VIPS: a visionbased page segmentation algorithm.</title>
<date>2003</date>
<tech>Microsoft Technical Report,</tech>
<pages>2003--79</pages>
<contexts>
<context position="13803" citStr="Cai et al., 2003" startWordPosition="2235" endWordPosition="2238"> extracted, all image regions are clustered using the k-means algorithm. The generated clusters, called “visterms” or “visual words”, are treated as a vocabulary for the images. Besides visual features, they also utilize the context textual feature of each image as the semantic supplement to generate to final feature vector V ecv. Based on these feature vector of each cantidate, they further calculating the intra-modal similarity with classical IR methods, they obtain the inter-modal similarity through the vague transformation (Mandl T, 1998). We also note that translation tools such as VIPS (Cai et al., 2003) and WebKit&apos; can help us to segment web documents and lhttp://www.webkit.org pick those text blocks whose coordinates are neighboring to each specified image. In this way we can successfully obtain images, text contexts and content sentences. Three kinds of semantic similarity are now ready. They are uniformly denoted as sim(cz, cj), representing the similarity between two candidates. cz and cj can be any type of modalities. Another problem is the noise from irregular data. We would like to utilize the sentences and images of high quality. The intuitive assumption is that a good candidate shou</context>
</contexts>
<marker>Cai, Yu, Wen, 2003</marker>
<rawString>Cai D, Yu S, Wen J R, et al. 2003 VIPS: a visionbased page segmentation algorithm. Microsoft Technical Report, MSR-TR-2003-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Charikar</author>
</authors>
<title>Similarity estimation techniques from rounding algorithms.</title>
<date>2002</date>
<booktitle>Proceedings of the 34th Annual ACM Symposium on Theory of Computing, ACM,</booktitle>
<pages>380--388</pages>
<marker>Charikar, 2002</marker>
<rawString>Charikar M S. 2002 Similarity estimation techniques from rounding algorithms. Proceedings of the 34th Annual ACM Symposium on Theory of Computing, ACM, pages 380-388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chen</author>
<author>O Jin</author>
<author>G R Xue</author>
</authors>
<title>Visual contextual advertising: Bringing textual advertisements to images.</title>
<date>2010</date>
<booktitle>Proceedings of the 24th AAAI Conference, AAAI,</booktitle>
<pages>1314--1320</pages>
<marker>Chen, Jin, Xue, 2010</marker>
<rawString>Chen Y, Jin O, Xue G R, et al. 2010 Visual contextual advertising: Bringing textual advertisements to images. Proceedings of the 24th AAAI Conference, AAAI, pages 1314-1320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Contractor</author>
<author>Y Guo</author>
<author>A Korhonen</author>
</authors>
<title>Using Argumentative Zones for Extractive Summarization of Scientific Articles.</title>
<date>2012</date>
<pages>663--678</pages>
<publisher>COLING,</publisher>
<contexts>
<context position="8798" citStr="Contractor et al., 2012" startWordPosition="1396" endWordPosition="1400">The rest of this paper is organized as follows. Some related researches are demonstrated in Section 2. We introduce our methodology in Section 3. The experimental results in Section 4 prove the effectiveness of our approach. Finally, we conclude this paper and present our future work in Section 5. 2 Related Work Generally speaking, multi-document summarization can be either extractive or abstractive. Researchers mainly focus on the former which extracts the information deemed most important to the summary. Various techniques have been used for this type of MDS (Haghighi and Vanderwende, 2009; Contractor et al., 2012). Graph-based text summarization techniques have been widely used for years. The algorithms, used in TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Radev et al., 2004), which are meant to compute sentence importance, are similar to those in PageRank and HITS. Recently, timeline becomes a popular style to present a schedule of events and attracts many researchers consequently. For example, Yan et al. make use of timestamps to generate an evolutionary timeline (Yan et al., 2011). Shahaf et al. present a 2-D story map called “metro map” to summarize the complex topic (Shahaf et al., 2012). </context>
</contexts>
<marker>Contractor, Guo, Korhonen, 2012</marker>
<rawString>Contractor D, Guo Y, Korhonen A. 2012. Using Argumentative Zones for Extractive Summarization of Scientific Articles. COLING, pages 663-678.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Evans</author>
<author>K McKeown</author>
<author>J L Klavans</author>
</authors>
<title>Similaritybased multilingual multi-document summarization.</title>
<date>2005</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>1858--1860</pages>
<marker>Evans, McKeown, Klavans, 2005</marker>
<rawString>Evans D K, McKeown K, Klavans J L. 2005 Similaritybased multilingual multi-document summarization. IEEE Transactions on Information Theory, pages 1858C1860.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Feng</author>
<author>M Lapata</author>
</authors>
<title>Topic models for image annotation and text illustration. Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of NAACL,</booktitle>
<pages>831--839</pages>
<contexts>
<context position="10446" citStr="Feng and Lapata (2010)" startWordPosition="1658" endWordPosition="1661">ptimal combination of multimodal information with the superkernel fusion (Wu et al., 2004). Borrowing the idea of recommendation in heterogeneous network into the cross-modal news summarization is also a convincing research. Xu et al. tackle this task and bring out an 1-D cross-media timeline generation framework (Xu et al., 2013). Summarization of multimedia involves researches on information retrieval of multimedia. Since textual and visual information are quite different from each other, how to make a good transformation to mine latent knowledge from unannotated images is of great concern. Feng and Lapata (2010) use visual words to describe visual features and then propose a probabilistic model based on the assumption that images and their co-occurring textual data are generated by mixtures of latent topics. However, to the best of our knowledge, no existing research manages to generate a 2-dimensional story map automatically and integrate images and texts into a unified framework at the same time. 3 Methodology The original data of one event is a collection of news documents on different days, or in a finer granularity, a set of sentences and images with different timestamps. Each item in the data c</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Feng Y, Lapata M. 2010 Topic models for image annotation and text illustration. Human Language Technologies: The 2010 Annual Conference of NAACL, pages 831-839.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldstein</author>
<author>V Mittal</author>
<author>J Carbonell</author>
<author>M Kantrowitz</author>
</authors>
<title>Multi-document summarization by sentence extraction.</title>
<date>2000</date>
<booktitle>Proceedings of the 2000 NAACLANLPWorkshop on Automatic summarizationVolume 4. Association for Computational Linguistics,</booktitle>
<pages>40--48</pages>
<contexts>
<context position="2042" citStr="Goldstein et al., 2000" startWordPosition="298" endWordPosition="301">zzled and lost. The situation gets worse on complex news events. To help readers quickly grasp the general information of the news, a more concise and convenient system over multimodality information should be provided. For example, given a large collection of texts and images related to a specified news event (e.g., East Japan ∗Corresponding author Earthquake), such a system should present a terse and brief summarization about the event by showing different clues of its development, and thus helping readers to effectively find out “when, where, what, how and why” at a glance. The researches (Goldstein et al., 2000) on automatic multi-document summarization (MDS) have helped a lot when we generate a description for a specific event. However, it traditionally exhibits in a very simple style like a “0-dimensional” point. The appearance of Timeline (Allan et al., 2001) brings about a visual progress for massive documents analyses. Readers can not only get the most important ideas, but also browse the story evolution in chronological order. Previous news summarization systems with structured output (Yan et al., 2011) have focused on timeline generation. Timeline becomes a “1-dimensional” line. This style of </context>
</contexts>
<marker>Goldstein, Mittal, Carbonell, Kantrowitz, 2000</marker>
<rawString>Goldstein J, Mittal V, Carbonell J, Kantrowitz M. 2000 Multi-document summarization by sentence extraction. Proceedings of the 2000 NAACLANLPWorkshop on Automatic summarizationVolume 4. Association for Computational Linguistics, pages 40-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldstein</author>
<author>M Kantrowitz</author>
<author>V Mittal</author>
</authors>
<title>Summarizing text documents: sentence selection and evaluation metrics.</title>
<date>1999</date>
<booktitle>Proceedings of the 22nd Annual International ACM Conference on SIGIR, ACM,</booktitle>
<pages>121--128</pages>
<contexts>
<context position="18139" citStr="Goldstein et al., 1999" startWordPosition="2951" endWordPosition="2954">e SUPPORT of L. In detail, Support(L) = minIkEL |Ik|, which denotes the smallest size among all the story nodes it has. The most challenging part is to properly model extension properties. We observe that an effective storyline should meet three key requirements: (1) Coherence. Within one storyline, news changes gradually as time goes and the evolution indicates consistency among component story nodes. We rely on the notion of coherence developed in Connectthe-Dots (Shahaf and Guestrin, 2010) and transform it to what we exactly need in this research; (2) Diversity. According to MMR principle (Goldstein et al., 1999), though the work is about summary, we still can draw an analogy and derive that a good storyline should be concise and contain redundant information as few as possible, i.e., two sentences providing information of similar content should not be presented in different storylines; (3) Coverage. The extracted storyline set L should keep alignment with the source collection C, which is intuitive and even proved to be significant as proposed in (Li et al., 2009). However, Coverage in some ways is technically redundant in front of Diversity. We decide to use the first two criteria in extraction proc</context>
</contexts>
<marker>Goldstein, Kantrowitz, Mittal, 1999</marker>
<rawString>Goldstein J, Kantrowitz M, Mittal V, et al. 1999 Summarizing text documents: sentence selection and evaluation metrics. Proceedings of the 22nd Annual International ACM Conference on SIGIR, ACM, pages 121-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>L Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>Proceedings of Human Language Technologies: The 2009 Annual Conference of NAACL, Association for Computational Linguistics,</booktitle>
<pages>362--370</pages>
<contexts>
<context position="8772" citStr="Haghighi and Vanderwende, 2009" startWordPosition="1392" endWordPosition="1395">proach can perform effectively. The rest of this paper is organized as follows. Some related researches are demonstrated in Section 2. We introduce our methodology in Section 3. The experimental results in Section 4 prove the effectiveness of our approach. Finally, we conclude this paper and present our future work in Section 5. 2 Related Work Generally speaking, multi-document summarization can be either extractive or abstractive. Researchers mainly focus on the former which extracts the information deemed most important to the summary. Various techniques have been used for this type of MDS (Haghighi and Vanderwende, 2009; Contractor et al., 2012). Graph-based text summarization techniques have been widely used for years. The algorithms, used in TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Radev et al., 2004), which are meant to compute sentence importance, are similar to those in PageRank and HITS. Recently, timeline becomes a popular style to present a schedule of events and attracts many researchers consequently. For example, Yan et al. make use of timestamps to generate an evolutionary timeline (Yan et al., 2011). Shahaf et al. present a 2-D story map called “metro map” to summarize the complex top</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Haghighi A, Vanderwende L. 2009. Exploring content models for multi-document summarization. Proceedings of Human Language Technologies: The 2009 Annual Conference of NAACL, Association for Computational Linguistics, pages 362-370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Indyk</author>
<author>R Motwani</author>
</authors>
<title>Approximate nearest neighbors: towards removing the curse of dimensionality.</title>
<date>1998</date>
<booktitle>Proceedings of the 30th Annual ACM Symposium on Theory of Computing, ACM,</booktitle>
<pages>604--613</pages>
<contexts>
<context position="21821" citStr="Indyk and Motwani, 1998" startWordPosition="3581" endWordPosition="3584">e good use of this idea. LSH performs probabilistic dimension reduction of high dimensional data by projecting a higher ddimensional vector V ec, (recall from Section 3.1) to a lower d′-dimensional vector (d′&lt;&lt;d), such that the candidates which are in close proximity in the higher dimension get mapped into the same item in the lower dimensional space with high probability. It guarantees a lower bound on the probability that two similar input items fall into the same bucket in the projected space and also the upper bound on the probability that two dissimilar vectors fall into the same bucket (Indyk and Motwani, 1998). One of the key requirements for good performance of LSH is the careful selection of the family of hashing functions. In OPT-LSH, we use the hashing scheme proposed by Charikar (Charikar M, 2002). In detail, d′ random unit d-dimensional vectors ⃗r1, ⃗r2,... , ⃗rd′ are generated first. Each of the d entries of ⃗r� is drawn from a standardized normal distribution N(0,1). Then the d′ hashing functions are defined as: � 1, if ⃗r� · V ec(c) &gt; 0 h�(c) =0, if ⃗r� · V ec(c) &lt; 0 1 � i � d′ We represent the d′-dimensional bucket feature h for c, h(c) := [h1(c), ... , hd′(c)]. There are 2d′ different bu</context>
</contexts>
<marker>Indyk, Motwani, 1998</marker>
<rawString>Indyk P, Motwani R. 1998 Approximate nearest neighbors: towards removing the curse of dimensionality. Proceedings of the 30th Annual ACM Symposium on Theory of Computing, ACM, pages 604-613.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Jiang</author>
<author>A H Tan</author>
</authors>
<title>Discovering image-text associations for cross-media web information fusion. Knowledge Discovery in Databases: PKDD</title>
<date>2006</date>
<pages>561--568</pages>
<contexts>
<context position="12377" citStr="Jiang and Tan (2006)" startWordPosition="2001" endWordPosition="2004">emantic knowledge for each candidate, and then purify data collection C by eliminating the noisy candidates; 2. Conduct OPT-LSH algorithm to extract L that contains all qualified storylines; 3. Reconstruct the storylines as the final map M. 3.1 Pre-processing Before we extract the storylines, we have to solve two problems first. Since our work is cross-modal, the semantic knowledge under the literal and visual surface is basically required. Recall from Section 2, there exist many effective ways to dig into the semantic level of image and text. In this paper, we employ the approach proposed by Jiang and Tan (2006). They present a convincing approach which employs a multilingual retrieval model to apply knowledge mining on semantic level. The first is the sentence feature vector generation. With the preprocessing such as stemming and stop words removal, they extract the textual TF-IDF feature V ecs of each sentence. The second, also the challenging part, is the image feature vector generation. In this step, for each region they extract visual features that are consisting of 6 color features and 60 gabor features which have been proven to be useful in many applications. Color features are the means and v</context>
</contexts>
<marker>Jiang, Tan, 2006</marker>
<rawString>Jiang T, Tan A H. 2006 Discovering image-text associations for cross-media web information fusion. Knowledge Discovery in Databases: PKDD 2006, pages 561-568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Li</author>
<author>K Zhou</author>
<author>G R Xue</author>
</authors>
<title>Enhancing diversity, coverage and balance for summarization through structure learning.</title>
<date>2009</date>
<booktitle>Proceedings of the 18th International Conference on World Wide Web, ACM,</booktitle>
<pages>71--80</pages>
<contexts>
<context position="18600" citStr="Li et al., 2009" startWordPosition="3029" endWordPosition="3032">s (Shahaf and Guestrin, 2010) and transform it to what we exactly need in this research; (2) Diversity. According to MMR principle (Goldstein et al., 1999), though the work is about summary, we still can draw an analogy and derive that a good storyline should be concise and contain redundant information as few as possible, i.e., two sentences providing information of similar content should not be presented in different storylines; (3) Coverage. The extracted storyline set L should keep alignment with the source collection C, which is intuitive and even proved to be significant as proposed in (Li et al., 2009). However, Coverage in some ways is technically redundant in front of Diversity. We decide to use the first two criteria in extraction process and use the last one to verify the effectiveness. Since a storyline is composed of several nodes, we can select or abandon nodes mainly according to these two requirements. In fact, both of them involve a measurement of similarity between two story nodes, denoted by two word distributions (see Figure 2). Specifically, for story node Ii, its distribution probability of word w is estimated as p(w|Ii) = ∑ c∈Ii TF(w) In addition, we introduce the decreasing</context>
</contexts>
<marker>Li, Zhou, Xue, 2009</marker>
<rawString>Li L, Zhou K, Xue G R, et al. 2009 Enhancing diversity, coverage and balance for summarization through structure learning. Proceedings of the 18th International Conference on World Wide Web, ACM, pages 71-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mandl</author>
</authors>
<title>Vague transformations in information retrieval. ISI</title>
<date>1998</date>
<pages>312--325</pages>
<marker>Mandl, 1998</marker>
<rawString>Mandl T. 1998 Vague transformations in information retrieval. ISI 1998, pages 312-325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>A language independent algorithm for single and multiple document summarization.</title>
<date>2005</date>
<booktitle>Proceedings of IJCNLP</booktitle>
<contexts>
<context position="8934" citStr="Mihalcea and Tarau, 2005" startWordPosition="1416" endWordPosition="1419">Section 3. The experimental results in Section 4 prove the effectiveness of our approach. Finally, we conclude this paper and present our future work in Section 5. 2 Related Work Generally speaking, multi-document summarization can be either extractive or abstractive. Researchers mainly focus on the former which extracts the information deemed most important to the summary. Various techniques have been used for this type of MDS (Haghighi and Vanderwende, 2009; Contractor et al., 2012). Graph-based text summarization techniques have been widely used for years. The algorithms, used in TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Radev et al., 2004), which are meant to compute sentence importance, are similar to those in PageRank and HITS. Recently, timeline becomes a popular style to present a schedule of events and attracts many researchers consequently. For example, Yan et al. make use of timestamps to generate an evolutionary timeline (Yan et al., 2011). Shahaf et al. present a 2-D story map called “metro map” to summarize the complex topic (Shahaf et al., 2012). But previous work only studies on the document level, which inevitably brings about much information redundancy. Previous studies show t</context>
</contexts>
<marker>Mihalcea, Tarau, 2005</marker>
<rawString>Mihalcea R, Tarau P. 2005 A language independent algorithm for single and multiple document summarization. Proceedings of IJCNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nallapati</author>
<author>A Feng</author>
<author>F Peng</author>
</authors>
<title>Event threading within news topics.</title>
<date>2004</date>
<booktitle>Proceedings of the 13rd ACM International Conference on Information and Knowledge Management, ACM,</booktitle>
<pages>446--453</pages>
<contexts>
<context position="32150" citStr="Nallapati et al., 2004" startWordPosition="5363" endWordPosition="5366">ods? • Macro-Knowledge: Can the maps help users understand the big picture better than other methods? H(X) Original C Our system Time-insensitive Text-bias modal Simplest 1288 For micro-knowledge, we wish to see how maps help users answer specific questions. We compare the level of knowledge attained by users using our method with two other systems: Google News and TDT. Google News is a computer-generated site that aggregates headlines from news sources worldwide. News-viewing tools are dominated by portal and search approaches, and Google News is a typical representative of those tools. TDT (Nallapati et al., 2004) is a successful system which captures the rich structure and dependencies of news events. We have noticed that making comparisons between different systems is not convincing, since the output of Google News and TDT is different both in content and in presentation (and in particular, cannot be double-blind). In order to isolate the effects of sentence selection vs. map organization, we introduce a hybrid system into the study: the system with structureless story map displays the same sentences and images as our system but with none of the structure. Its output is basically the same with our fu</context>
</contexts>
<marker>Nallapati, Feng, Peng, 2004</marker>
<rawString>Nallapati R, Feng A, Peng F, et al. 2004 Event threading within news topics. Proceedings of the 13rd ACM International Conference on Information and Knowledge Management, ACM, pages 446-453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Panjwani</author>
<author>L Micallef</author>
<author>K Fenech</author>
</authors>
<title>Effects of integrating digital visual materials with textbook scans in the classroom.</title>
<date>2009</date>
<booktitle>International Journal of Education and Development using ICT,</booktitle>
<pages>5--3</pages>
<contexts>
<context position="9671" citStr="Panjwani et al., 2009" startWordPosition="1538" endWordPosition="1541">Rank and HITS. Recently, timeline becomes a popular style to present a schedule of events and attracts many researchers consequently. For example, Yan et al. make use of timestamps to generate an evolutionary timeline (Yan et al., 2011). Shahaf et al. present a 2-D story map called “metro map” to summarize the complex topic (Shahaf et al., 2012). But previous work only studies on the document level, which inevitably brings about much information redundancy. Previous studies show that the use of visual materials not only leads to the conservation of information but also promotes comprehension (Panjwani et al., 2009). Thus the cross-modal fusion is necessary. Wu et al. propose a framework of multimodal information fusion for multimedia data analysis by learning the optimal combination of multimodal information with the superkernel fusion (Wu et al., 2004). Borrowing the idea of recommendation in heterogeneous network into the cross-modal news summarization is also a convincing research. Xu et al. tackle this task and bring out an 1-D cross-media timeline generation framework (Xu et al., 2013). Summarization of multimedia involves researches on information retrieval of multimedia. Since textual and visual </context>
</contexts>
<marker>Panjwani, Micallef, Fenech, 2009</marker>
<rawString>Panjwani S, Micallef L, Fenech K, et al. 2009 Effects of integrating digital visual materials with textbook scans in the classroom. International Journal of Education and Development using ICT, 2009, 5(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>H Jing</author>
</authors>
<title>Centroid-based summarization of multiple documents.</title>
<date>2004</date>
<booktitle>Information Processing &amp; Management,</booktitle>
<volume>40</volume>
<issue>6</issue>
<pages>919--938</pages>
<marker>Radev, Jing, 2004</marker>
<rawString>Radev D R, Jing H, et al. 2004 Centroid-based summarization of multiple documents. Information Processing &amp; Management, 2004, 40(6): 919-938.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>A Winkel</author>
<author>M Topper</author>
</authors>
<title>Multi document centroid-based text summarization. ACL Demo Session,</title>
<date>2002</date>
<marker>Radev, Winkel, Topper, 2002</marker>
<rawString>Radev D, Winkel A, Topper M. 2002 Multi document centroid-based text summarization. ACL Demo Session, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shahaf</author>
<author>C Guestrin</author>
</authors>
<title>Connecting the dots between news articles.</title>
<date>2010</date>
<booktitle>Proceedings of the 16th ACM Conference on SIGKDD, ACM,</booktitle>
<pages>623--632</pages>
<contexts>
<context position="18013" citStr="Shahaf and Guestrin, 2010" startWordPosition="2929" endWordPosition="2932">toryline L (denoted as |L|) is intuitively one of its basic attributes. We would like to define another basic attribute called the SUPPORT of L. In detail, Support(L) = minIkEL |Ik|, which denotes the smallest size among all the story nodes it has. The most challenging part is to properly model extension properties. We observe that an effective storyline should meet three key requirements: (1) Coherence. Within one storyline, news changes gradually as time goes and the evolution indicates consistency among component story nodes. We rely on the notion of coherence developed in Connectthe-Dots (Shahaf and Guestrin, 2010) and transform it to what we exactly need in this research; (2) Diversity. According to MMR principle (Goldstein et al., 1999), though the work is about summary, we still can draw an analogy and derive that a good storyline should be concise and contain redundant information as few as possible, i.e., two sentences providing information of similar content should not be presented in different storylines; (3) Coverage. The extracted storyline set L should keep alignment with the source collection C, which is intuitive and even proved to be significant as proposed in (Li et al., 2009). However, Co</context>
</contexts>
<marker>Shahaf, Guestrin, 2010</marker>
<rawString>Shahaf D, Guestrin C. 2010 Connecting the dots between news articles. Proceedings of the 16th ACM Conference on SIGKDD, ACM, pages 623-632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shahaf</author>
<author>C Guestrin</author>
<author>E Horvitz</author>
</authors>
<title>Trains of thought: Generating information maps.</title>
<date>2012</date>
<booktitle>Proceedings of the 21st International Conference on World Wide Web, ACM,</booktitle>
<pages>899--908</pages>
<contexts>
<context position="6212" citStr="Shahaf et al., 2012" startWordPosition="970" endWordPosition="973">texts and images will absolutely yield a more powerful and intuitive description about the news event. Under this motivation, we study on extracting and reconstructing tracks with different sub-topics for a complex event. To the best of our knowledge, the exploration and analysis of 2-dimensional cross-modal summarization is academically novel. We are faced with two main problems. The first is how to select the most important sentences and images to make up the final story map. The previous work by Shahaf et al. presents a 2-D story map called “metro map”, which summarizes the complex topics (Shahaf et al., 2012). They study on the document-level, and use the entire news document as one story node. But on real web, this may confront some difficulties. On one hand, news articles may report the event from different perspectives, especially those reviews or retrospective reports. This kind of documents contains many useful saliency information of different sub-topics, but they cannot be further subdivided to help understand each better. On the other hand, some documents, such as Line of “Rescue” on Mar 13 Line “Post-disaster” on Mar 17 Figure 2: Word distributions vary a lot among nodes in different stor</context>
<context position="9396" citStr="Shahaf et al., 2012" startWordPosition="1494" endWordPosition="1497">ntractor et al., 2012). Graph-based text summarization techniques have been widely used for years. The algorithms, used in TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Radev et al., 2004), which are meant to compute sentence importance, are similar to those in PageRank and HITS. Recently, timeline becomes a popular style to present a schedule of events and attracts many researchers consequently. For example, Yan et al. make use of timestamps to generate an evolutionary timeline (Yan et al., 2011). Shahaf et al. present a 2-D story map called “metro map” to summarize the complex topic (Shahaf et al., 2012). But previous work only studies on the document level, which inevitably brings about much information redundancy. Previous studies show that the use of visual materials not only leads to the conservation of information but also promotes comprehension (Panjwani et al., 2009). Thus the cross-modal fusion is necessary. Wu et al. propose a framework of multimodal information fusion for multimedia data analysis by learning the optimal combination of multimodal information with the superkernel fusion (Wu et al., 2004). Borrowing the idea of recommendation in heterogeneous network into the cross-mod</context>
<context position="31328" citStr="Shahaf et al., 2012" startWordPosition="5235" endWordPosition="5238"> space we don’t present the tuning details, but the optimal values are shown in Table 1. In fact, using the cross-modal mutual influence can, more or less, help to improve the effectiveness of information extraction and summarization. 4.3 User Study Before we introduce other existing methods that can also tackle the cross-modal 2-dimensional summarization problem, we have to setup the appropriate standards to quantify users’ evaluation. 4.3.1 Metrics In the user study, we evaluate the effectiveness of our story maps in aiding users to integrate different aspects of multi-faceted information. (Shahaf et al., 2012) also focuses on story map generation and puts forward two convincing metrics to answer the following questions: • Micro-Knowledge: Can the maps help users retrieve information faster than other methods? • Macro-Knowledge: Can the maps help users understand the big picture better than other methods? H(X) Original C Our system Time-insensitive Text-bias modal Simplest 1288 For micro-knowledge, we wish to see how maps help users answer specific questions. We compare the level of knowledge attained by users using our method with two other systems: Google News and TDT. Google News is a computer-ge</context>
</contexts>
<marker>Shahaf, Guestrin, Horvitz, 2012</marker>
<rawString>Shahaf D, Guestrin C, Horvitz E. 2012 Trains of thought: Generating information maps. Proceedings of the 21st International Conference on World Wide Web, ACM, pages 899-908.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>2001</date>
<journal>ACM SIGMOBILE Mobile Computing and Communications Review,</journal>
<volume>5</volume>
<issue>1</issue>
<pages>3--55</pages>
<marker>Shannon, 2001</marker>
<rawString>Shannon C E. 2001 A mathematical theory of communication. ACM SIGMOBILE Mobile Computing and Communications Review, 2001, 5(1): 3-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wu</author>
<author>E Y Chang</author>
<author>K C C Chang</author>
</authors>
<title>Optimal multimodal fusion for multimedia data analysis.</title>
<date>2004</date>
<booktitle>Proceedings of the 12th annual ACM International Conference on Multimedia, ACM,</booktitle>
<pages>572--579</pages>
<contexts>
<context position="9914" citStr="Wu et al., 2004" startWordPosition="1578" endWordPosition="1581">l. present a 2-D story map called “metro map” to summarize the complex topic (Shahaf et al., 2012). But previous work only studies on the document level, which inevitably brings about much information redundancy. Previous studies show that the use of visual materials not only leads to the conservation of information but also promotes comprehension (Panjwani et al., 2009). Thus the cross-modal fusion is necessary. Wu et al. propose a framework of multimodal information fusion for multimedia data analysis by learning the optimal combination of multimodal information with the superkernel fusion (Wu et al., 2004). Borrowing the idea of recommendation in heterogeneous network into the cross-modal news summarization is also a convincing research. Xu et al. tackle this task and bring out an 1-D cross-media timeline generation framework (Xu et al., 2013). Summarization of multimedia involves researches on information retrieval of multimedia. Since textual and visual information are quite different from each other, how to make a good transformation to mine latent knowledge from unannotated images is of great concern. Feng and Lapata (2010) use visual words to describe visual features and then propose a pro</context>
</contexts>
<marker>Wu, Chang, Chang, 2004</marker>
<rawString>Wu Y, Chang E Y, Chang K C C, et al. 2004 Optimal multimodal fusion for multimedia data analysis. Proceedings of the 12th annual ACM International Conference on Multimedia, ACM, 2004: 572-579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Xu</author>
<author>L Kong</author>
<author>Y Zhang</author>
</authors>
<title>A cross-media evolutionary timeline generation framework based on iterative recommendation.</title>
<date>2013</date>
<booktitle>Proceedings of the 3rd ACM conference on International Conference on Multimedia Retrieval, ACM,</booktitle>
<pages>73--80</pages>
<contexts>
<context position="10156" citStr="Xu et al., 2013" startWordPosition="1615" endWordPosition="1618">e use of visual materials not only leads to the conservation of information but also promotes comprehension (Panjwani et al., 2009). Thus the cross-modal fusion is necessary. Wu et al. propose a framework of multimodal information fusion for multimedia data analysis by learning the optimal combination of multimodal information with the superkernel fusion (Wu et al., 2004). Borrowing the idea of recommendation in heterogeneous network into the cross-modal news summarization is also a convincing research. Xu et al. tackle this task and bring out an 1-D cross-media timeline generation framework (Xu et al., 2013). Summarization of multimedia involves researches on information retrieval of multimedia. Since textual and visual information are quite different from each other, how to make a good transformation to mine latent knowledge from unannotated images is of great concern. Feng and Lapata (2010) use visual words to describe visual features and then propose a probabilistic model based on the assumption that images and their co-occurring textual data are generated by mixtures of latent topics. However, to the best of our knowledge, no existing research manages to generate a 2-dimensional story map aut</context>
</contexts>
<marker>Xu, Kong, Zhang, 2013</marker>
<rawString>Xu S, Kong L, Zhang Y. 2013 A cross-media evolutionary timeline generation framework based on iterative recommendation. Proceedings of the 3rd ACM conference on International Conference on Multimedia Retrieval, ACM, pages 73-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yan</author>
<author>X Wan</author>
<author>J Otterbacher</author>
</authors>
<title>Evolutionary timeline summarization: a balanced optimization framework via iterative substitution.</title>
<date>2011</date>
<booktitle>Proceedings of the 34th International ACM Conference on SIGIR, ACM,</booktitle>
<pages>745--754</pages>
<contexts>
<context position="2549" citStr="Yan et al., 2011" startWordPosition="378" endWordPosition="381"> to effectively find out “when, where, what, how and why” at a glance. The researches (Goldstein et al., 2000) on automatic multi-document summarization (MDS) have helped a lot when we generate a description for a specific event. However, it traditionally exhibits in a very simple style like a “0-dimensional” point. The appearance of Timeline (Allan et al., 2001) brings about a visual progress for massive documents analyses. Readers can not only get the most important ideas, but also browse the story evolution in chronological order. Previous news summarization systems with structured output (Yan et al., 2011) have focused on timeline generation. Timeline becomes a “1-dimensional” line. This style of summarization only works for simple stories, which are linear in nature. However, the structure of complex stories usually turns out to be non-linear. These stories branch into storylines, dead ends, intertwining narratives and side news. To explore these lines, we need a map to reorganize all the information. Therefore, a “2-dimensional” story map is in bad need. Figure 1 shows a part of the story map generated by our system for representing East Japan Earthquake. We notice that the whole event evolve</context>
<context position="9285" citStr="Yan et al., 2011" startWordPosition="1474" endWordPosition="1477">t to the summary. Various techniques have been used for this type of MDS (Haghighi and Vanderwende, 2009; Contractor et al., 2012). Graph-based text summarization techniques have been widely used for years. The algorithms, used in TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Radev et al., 2004), which are meant to compute sentence importance, are similar to those in PageRank and HITS. Recently, timeline becomes a popular style to present a schedule of events and attracts many researchers consequently. For example, Yan et al. make use of timestamps to generate an evolutionary timeline (Yan et al., 2011). Shahaf et al. present a 2-D story map called “metro map” to summarize the complex topic (Shahaf et al., 2012). But previous work only studies on the document level, which inevitably brings about much information redundancy. Previous studies show that the use of visual materials not only leads to the conservation of information but also promotes comprehension (Panjwani et al., 2009). Thus the cross-modal fusion is necessary. Wu et al. propose a framework of multimodal information fusion for multimedia data analysis by learning the optimal combination of multimodal information with the superke</context>
</contexts>
<marker>Yan, Wan, Otterbacher, 2011</marker>
<rawString>Yan R, Wan X, Otterbacher J, et al. 2011 Evolutionary timeline summarization: a balanced optimization framework via iterative substitution. Proceedings of the 34th International ACM Conference on SIGIR, ACM, pages 745-754.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
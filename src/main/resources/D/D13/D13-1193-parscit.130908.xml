<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.983343">
Authorship Attribution of Micro-Messages
</title>
<author confidence="0.998556">
Roy Schwartz Oren Tsur Ari Rappoport Moshe Koppel
</author>
<affiliation confidence="0.999301">
Institute of Computer Science Department of Computer Science
Hebrew University of Jerusalem Bar Ilan University
</affiliation>
<email confidence="0.997655">
{roys02|oren|arir}@cs.huji.ac.il koppel@macs.biu.ac.il
</email>
<sectionHeader confidence="0.993765" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9972004">
Work on authorship attribution has tradition-
ally focused on long texts. In this work, we
tackle the question of whether the author of
a very short text can be successfully iden-
tified. We use Twitter as an experimental
testbed. We introduce the concept of an au-
thor’s unique “signature”, and show that such
signatures are typical of many authors when
writing very short texts. We also present a new
authorship attribution feature (“flexible pat-
terns”) and demonstrate a significant improve-
ment over our baselines. Our results show that
the author of a single tweet can be identified
with good accuracy in an array of flavors of
the authorship attribution task.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999875962264151">
Research in authorship attribution has developed
substantially over the last decade (Stamatatos,
2009). The vast majority of such research has been
dedicated towards finding the author of long texts,
ranging from single passages to book chapters. In
recent years, the growing popularity of social me-
dia has created special interest, both theoretical and
computational, in short texts. This has led to many
recent authorship attribution projects that experi-
mented with web data such as emails (Abbasi and
Chen, 2008), web forum messages (Solorio et al.,
2011) and blogs (Koppel et al., 2011b). This paper
addresses the question to what extent the authors of
very short texts can be identified. To answer this
question, we experiment with Twitter tweets.
Twitter messages (tweets) are limited to 140 char-
acters. This restriction imposes major difficulties on
authorship attribution systems, since authorship at-
tribution methods that work well on long texts are
often not as useful when applied to short texts (Bur-
rows, 2002; Sanderson and Guenter, 2006).
Nonetheless, tweets are relatively self-contained
and have smaller sentence length variance com-
pared to excerpts from longer texts (see Section 3).
These characteristics make Twitter data appealing as
a testbed when focusing on short texts. Moreover,
an authorship attribution system of tweets may have
various applications. Specifically, a range of cyber-
crimes can be addressed using such a system, includ-
ing identity fraud and phishing.
In this paper, we introduce the concept of k-
signatures. We denote the k-signatures of an author
a as the features that appear in at least k% of a’s
training samples, while not appearing in the training
set of any other author. When k is large, such signa-
tures capture a unique style used by a. An analysis
of our training set reveals that unique k-signatures
are typical of many authors. Moreover, a substantial
portion of the tweets in our training set contain at
least one such signature. These findings suggest that
a single tweet, although short and sparse, often con-
tains sufficient information for identifying its author.
Our results show that this is indeed the case.
We train an SVM classifier with a set of features
that include character n-grams and word n-grams.
We use a rigorous experimental setup, with varying
number of authors (values between 50-1,000) and
various sizes of the training set, ranging from 50 to
1,000 tweets per author. In all our experiments, a
single tweet is used as test document. We also use
a setting in which the system is allowed to respond
don’t know in cases of uncertainty. Applying this
option results in higher precision, at the expense of
</bodyText>
<page confidence="0.923709">
1880
</page>
<note confidence="0.734517">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1880–1891,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.997405137931034">
lower recall.
Our results show that the author of a tweet can be
successfully identified. For example, when using a
dataset of as many as 1,000 authors with 200 train-
ing tweets per author, we are able to obtain 30.3%
accuracy (as opposed to a random baseline of only
0.1%). Using a dataset of 50 authors with as few
as 50 training tweets per author, we obtain 50.7%
accuracy. Using a dataset of 50 authors with 1,000
training tweets per author, our results reach as high
as 71.2% in the standard classification setting, and
exceed 91% accuracy with 60% recall in the don’t
know setting.
We also apply a new set of features, never previ-
ously used for this task – flexible patterns. Flexi-
ble patterns essentially capture the context in which
function words are used. The effectiveness of func-
tion words as authorship attribution features (Koppel
et al., 2009) suggests using flexible pattern features.
The fact that flexible patterns are learned from plain
text in a fully unsupervised manner makes them
domain and language independent. We demon-
strate that using flexible patterns gives significant
improvement over our baseline system. Further-
more, using flexible patterns, our system obtains a
6.1% improvement over current state-of-the-art re-
sults in authorship attribution on Twitter.
To summarize, the contribution of this paper is
threefold.
</bodyText>
<listItem confidence="0.94604">
• We provide the most extensive research to date
on authorship attribution of micro-messages,
and show that authors of very short texts can
be successfully identified.
• We introduce the concept of an author’s unique
k-signature, and demonstrate that such signa-
tures are used by many authors in their writing
of micro-messages.
• We present a new feature for authorship attri-
</listItem>
<bodyText confidence="0.917386818181818">
bution – flexible patterns – and show its sig-
nificant added value over other methods. Us-
ing this feature, our system obtains a 6.1% im-
provement over the current state-of-the-art.
The rest of the paper is organized as follows. Sec-
tions 2 and 3 describe our methods and our experi-
mental testbed (Twitter). Section 4 presents the con-
cept of k-signatures. Sections 5 and 6 present our
experiments and results. Flexible patterns are pre-
sented in Section 7 and related work is presented in
Section 8.
</bodyText>
<sectionHeader confidence="0.993593" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.999795538461539">
In the following we briefly describe the main fea-
tures employed by our system. The features below
are binary features.
Character n-grams. Character n-gram features
are especially useful for authorship attribution on
micro-messages since they are relatively tolerant
to typos and non-standard use of punctuation (Sta-
matatos, 2009). These are common in the non-
formal style generally applied in social media ser-
vices. Consider the example of misspelling “Brit-
ney” as “Brittney”. The misspelled name shares the
4-grams “Brit” and “tney” with the correct name. As
a result, these features provide information about the
author’s style (or at least her topic of interest), which
is not available through lexical features.
Following standard practice, we use 4-grams
(Sanderson and Guenter, 2006; Layton et al., 2010;
Koppel et al., 2011b). White spaces are considered
characters (i.e., a character n-gram may be com-
posed of letters from two different words). A sin-
gle white-space is appended to the beginning and
the end of each tweet. For efficiency, we consider
only character n-gram features that appear at least
ting times in the training set of at least one author
(see Section 5).
Word n-grams. We hypothesize that word n-gram
features would be useful for authorship attribution
on micro-messages. We assume that under a strict
length restriction, many authors would prefer using
short, repeating phrases (word n-grams).
In our experiments, we consider 2 &lt; n &lt; 5.1
We regard sequences of punctuation marks as words.
Two special words are added to each tweet to indi-
cate the beginning and the end of the tweet. For effi-
ciency, we consider only word n-gram features that
appear at least twng times in the training set of at
least one author (see Section 5).
Model. We use libsvm’s Matlab implementation
of a multi-class SVM classifier with a linear kernel
</bodyText>
<footnote confidence="0.9968425">
1We skip unigrams as they are generally captured by the
character n-gram features.
</footnote>
<page confidence="0.977331">
1881
</page>
<subsectionHeader confidence="0.556567">
Number of Users
</subsectionHeader>
<bodyText confidence="0.9995">
(Chang and Lin, 2011). We use ten-fold cross vali-
dation on the training set to select the best regular-
ization factor between 0.5 and 0.005.2
</bodyText>
<sectionHeader confidence="0.989076" genericHeader="method">
3 Experimental Testbed
</sectionHeader>
<bodyText confidence="0.999990526315789">
Our main research question in this paper is to deter-
mine the extent to which authors of very short texts
can be identified. A major issue in working with
short texts is selecting the right dataset. One ap-
proach is breaking longer texts into shorter chunks
(Sanderson and Guenter, 2006). We take a differ-
ent approach and experiment with micro-messages
(specifically, tweets).
Tweets have several properties making them an
ideal testbed for authorship attribution of short texts.
First, tweets are posted as single units and do not
necessarily refer to each other. As a result, they tend
to be self contained. Second, tweets have more stan-
dardized length distribution compared to other types
of web data. We compared the mean and standard
deviation of sentence length in our Twitter dataset
and in a corpus of English web data (Ferraresi et al.,
2008).3 We found that (a) tweets are shorter than
standard web data (14.2 words compared to 20.9),
and (b) the standard deviation of the length of tweets
is much smaller (6.4 vs. 21.4).
Pre-Processing. We use a Twitter corpus that in-
cludes approximately 5 x 108 tweets.4 All non-
English tweets and tweets that contain fewer than
3 words are removed from the dataset. We also re-
move tweets marked as retweets (using the RT sign,
a standard Twitter symbol to indicate that this tweet
was written by a different user). As some users
retweet without using the RT sign, we also remove
tweets that are an exact copy of an existing tweet
posted in the previous seven days.
Apart from plain text, some tweets contain ref-
erences to other Twitter users (in the format of
@&lt;user&gt;). Since using reference information
makes this task substantially easier (Layton et al.,
2010), we replace each user reference with the spe-
cial meta tag REF. For sparsity reasons, we also re-
place web addresses with the meta tag URL, num-
</bodyText>
<footnote confidence="0.9014326">
2In practice, 0.05 or 0.1 are selected in almost all cases.
3http://wacky.sslmit.unibo.it
4These comprise —15% of all public tweets created from
May 2009 to March 2010.
Number of k−signatures per user
</footnote>
<figureCaption confidence="0.997917">
Figure 1: Number of users with at least x k-signatures
(100 authors, 180 training tweets per author).
</figureCaption>
<bodyText confidence="0.829626">
bers with the meta tag NUM, time of day with the
meta tag TIME and dates with the meta tag DATE.
</bodyText>
<sectionHeader confidence="0.859542" genericHeader="method">
4 k-Signatures
</sectionHeader>
<bodyText confidence="0.999604217391304">
In this section, we show that many authors adopt
a unique style when writing micro-messages. This
style can be detected by a strong classification algo-
rithm (such as SVM), and be sufficient to correctly
identify the author of a single tweet.
We define the concept of the k-signature of an au-
thor a to be a feature that appears in at least k% of
a’s training set, while not appearing in the training
set of any other user. Such signatures can be useful
for identifying future (unlabeled) tweets written by
a.
To validate our hypothesis, we use a dataset of
100 authors with 180 tweets per author. We com-
pute the number of k-signatures used by each of
the authors in our dataset. Figure 1 shows our re-
sults for a range of k values (2%, 5%, 10%, 20%
and 50%). Results demonstrate that 81 users use
at least one 2%-signature, 43 users use at least one
5%-signature, and 17 users use at least one 10%-
signature. These results indicate that a large portion
of the users adopt a unique signature (or set of sig-
natures) when writing short texts. Table 1 provides
examples of 10%-signatures.
</bodyText>
<figure confidence="0.967347">
90
80
70
60
50
40
30
20
10
0
0 5 10 15 20 25 30 35 40 45 &gt;50
</figure>
<equation confidence="0.9984338">
k = 2%
k = 5%
k = 10%
k = 20%
k = 50%
</equation>
<page confidence="0.974601">
1882
</page>
<table confidence="0.999431529411765">
Signature Type 10%-signature Examples
Character n-grams ‘ &amp;quot; &amp;quot;’ REF oh ok &amp;quot; &amp;quot; Glad you found it!
Hope everyone is having a good afternoon &amp;quot; &amp;quot;
REF Smirnoff lol keeping the goose in the freezer &amp;quot; &amp;quot;
‘yew ’ gurl yew serving me tea nooch
REF about wen yew and ronnie see each other
REF lol so yew goin to check out tini’s tonight huh???
Word n-grams .. lal REF aww those are cool where u get those.. how do ppl react.. lal
Ludas album is gone be hott.. lal
Dayum refs don’t get injury timeouts.. lal.. get him off the field..
I’m just back after takin’ a very long, icy cold
shower Shivering smoochies,E3 http://bit.ly/4CzzP9
smoochies , e3
A blue stout or two would be nice as well, Purr!Blue smooth
smoochies,E3 http://bit.ly/75D4fO
That is sooooooooooooooooooo unfair!Double smoochies,E3
http://bit.ly/07sXRGX
</table>
<tableCaption confidence="0.999756">
Table 1: Examples of 10%-signatures.
</tableCaption>
<bodyText confidence="0.999799884615385">
Results also show that seven users use one or
more 20%-signatures, and five users even use one
or more 50%-signatures. Looking carefully at these
users, we find that they write very structured mes-
sages, and are probably bots, such as news feeds,
bidding systems, etc. Table 2 provides examples of
tweets posted by such users.5
Another interesting question is how many tweets
contain at least one k-signature. Figure 2 shows
for each user the number of tweets in her training
set for which at least one k-signature is found. Re-
sults demonstrate that a total of 18.6% of the train-
ing tweets contain at least one 2%-signature, 10.3%
the training tweets contain at least one 5%-signature
and 6.5% of the training tweets contain at least one
10%-signature. These findings validate our assump-
tion that many users use k-signatures in short texts.
These findings also have direct implications on
authorship attribution of micro-messages, since k-
signatures are reliable classification features. As
a result, texts written by authors that tend to use
k-signatures are likely to be easily identified by a
reasonable classification algorithm. Consequently,
k-signatures provide a possible explanation for the
high quality results presented in this paper.
In the broader context, the presence (and contri-
</bodyText>
<footnote confidence="0.795844">
5Our k-signature method can actually be useful for automat-
ically identifying such users. We defer this to future work.
</footnote>
<figure confidence="0.462693">
Number of Tweets with at least one k−Signature
</figure>
<figureCaption confidence="0.990297">
Figure 2: Number of users with at least x training tweets
that contain at least one k-signature (100 authors, 180
training tweets per author).
</figureCaption>
<bodyText confidence="0.999559">
bution) of k-signatures is in line with the hypothesis
proposed by (Davidov et al., 2010a): while still us-
ing an informal and unstructured (grammatical) lan-
guage, authors tend to use typical and unique struc-
tures in order to allow a short message to stand alone
without a clear conversational context.
</bodyText>
<figure confidence="0.9826025">
0 20 40 60 80 100 120 140 160 180
Number of Users
60
40
20
90
80
70
50
30
10
0
</figure>
<equation confidence="0.9976674">
k = 2%
k = 5%
k = 10%
k = 20%
k = 50%
</equation>
<page confidence="0.93072">
1883
</page>
<table confidence="0.995982047619048">
User 20%-signature Examples
1 I’m listening to : I’m listening to: Sigur R?s ? Intro:
http://www.last.fm/music/Sigur+R%C3%B3s http://bit.ly/3XJHyb
I’m listening to: Tina Arena ? In Command:
http://www.last.fm/music/Tina+Arena http://bit.ly/7q9E25
I’m listening to: Midnight Oil ? Under the Overpass:
http://www.last.fm/music/Midnight+Oil http://bit.ly/7IH4cg
2 news now ( str ) #Hotel News Now(STR) 5 things to know: 27 May 2009: From the desks of
the HotelNewsNow.com editor... http://bit.ly/aZTZOq #Tourism #Lodging
#Hotel News Now(STR) Five sales renegotiating tactics: As bookings rep-
resentatives press to reneg... http://bit.ly/bHPn2L
#Hotel News Now(STR) Risk of hotel recession retreats: The Hotel Indus-
try’s Pulse Index increases... http://bit.ly/a8EKrm #Tourism #Lodging
3 ( NUM bids ) NEW PINK NINTENDO DS LITE CONSOLE WITH 21 GIFTS +
end date : CASE: £66.50 (13 Bids) End Date: Tuesday Dec-08-2009 17:..
http://bit.ly/7uPt6V
Microsoft Xbox 360 Game System - Console Only - Working: US $51.99
(25 Bids) End Date: Saturday Dec-12-2009 13:.. http://bit.ly/8VgdTv
Microsoft Sony Playstation 3 (80 GB) Console 6 Months Old:
£190.00 (25 Bids) End Date: Sunday Dec-13-2009 21:21:39 G..
http://bit.ly/7kwtDS
</table>
<tableCaption confidence="0.9670435">
Table 2: Examples of tweets published by very structured users, suspected to be bots, along with one of their 20%-
signatures.
</tableCaption>
<sectionHeader confidence="0.997028" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.987162238095238">
We report of three different experimental configu-
rations. In the experiments described below, each
dataset is divided into training and test sets using
ten-fold cross validation. On the test phase, each
document contains a single tweet.
Experimenting with varying Training Set Sizes.
In order to test the affect of the training set size,
we experiment with an increasingly larger number
of tweets per author. Experimenting with a range of
training set sizes serves two purposes: (a) to check
whether the author of a tweet can be identified us-
ing a very small number of (short) training samples,
and (b) check how much our system can benefit from
training on a larger corpus.
In our experiments we only consider users who
posted between 1,000–2,000 tweets6 (a total of
6This range is selected since on one hand we want at least
1,000 tweets per author for our experiments, and on the other
hand we noticed that users with a larger number of tweets in
corpus tend to be spammers or bots that are very easy to identify,
so we limit this number to 2,000.
10,183 users), and randomly select 1,000 tweets per
user. From these users, we select 10 groups of 50
users each.7 We perform a set of classification ex-
periments, selecting for each author an increasingly
larger subset of her 1,000 tweets as training set. Sub-
set sizes are (50, 100, 200, 500, 1,000). Thresh-
old values for our features in each setting (see Sec-
tion 2) are (2, 2, 4, 10, 20) for t,ng and (2, 2, 2, 3, 5)
for t,,,ng, respectively.
Experimenting with varying Numbers of Au-
thors. In a second set of experiments, we use an
increasingly larger number of authors (values be-
tween 100-1,000), in order to check whether the au-
thor of a very short text can be identified in a “needle
in a haystack” type of setting.
Due to complexity issues, we only experiment
with 200 tweets per author as training set. We se-
lect groups of size 100, 200, 500 and 1,000 users
(one group per size). We use the same threshold val-
ues as the 200 tweets per author setting previously
described (t,ng = 4, t,,,ng = 2).
</bodyText>
<footnote confidence="0.840884">
7An eleventh group is selected as development set.
</footnote>
<page confidence="0.977806">
1884
</page>
<figure confidence="0.998408454545454">
Accuracy (%)
65
60
45
70
55
50
Char. N−grams + Word N−grams
Char. N−grams
Accuracy (%)
60
45
40
25
55
50
35
30
Char. N−grams + Word N−grams
Char. N−grams
0 100 200 300 400 500 600 700 800 900 1000
Training Set Size
</figure>
<figureCaption confidence="0.998627">
Figure 3: Authorship attribution accuracy for 50 authors
with various training set sizes. The values are averaged
over 10 groups. The random baseline is 2%.
</figureCaption>
<bodyText confidence="0.999327291666667">
Recall-Precision Tradeoff. Another aspect of our
research question is the level of certainty our system
has when suggesting an author for a given tweet.
In cases of uncertainty, many real life applications
would prefer not to get any response instead of get-
ting a response with low certainty. Moreover, in real
life applications we are often not even sure that the
real author is part of our training set. Consequently,
we allow our system to respond “don’t know” in
cases of low confidence (Koppel et al., 2006; Kop-
pel et al., 2011b). This allows our system to obtain
higher precision, at the expense of lower recall.
To implement this feature, we use SVM’s proba-
bility estimates, as implemented in libsvm. These
estimates give a score to each potential author.
These scores reflect the probability that this author
is the correct author, as decided by the prediction
model. The selected author is always the one with
the highest probability estimate.
As selection criterion, we use a set of increasingly
larger thresholds (0.05-0.9) for the probability of the
selected author. This means that we do not select test
samples for which the selected author has a proba-
bility estimate value lower than the threshold.
</bodyText>
<figure confidence="0.986111">
0 100 200 300 400 500 600 700 800 900 1000
Number of Candidate Authors
</figure>
<figureCaption confidence="0.9673642">
Figure 4: Authorship attribution accuracy with varying
number of candidate authors, using 200 training tweets
per author. The random baselines for 509, 100, 200, 500
and 1,000 authors are 2%, 1%, 0.5%, 0.2% and 0.1%,
respectively.
</figureCaption>
<sectionHeader confidence="0.979913" genericHeader="method">
6 Basic Results
</sectionHeader>
<bodyText confidence="0.9955458">
Experimenting with varying Training Set Sizes.
Figure 3 shows results for our experiments with
50 authors and various training set sizes. Results
demonstrate that authors of very short texts can be
successfully identified, even with as few as 50 tweets
per author (49.5%). When given more training sam-
ples, authors are identified much more accurately
(up to 69.7%). Results also show that, according to
our hypothesis, word n-gram features substantially
improve over character n-grams features only (3%
averaged improvement over all settings).
Experimenting with varying Numbers of Au-
thors. Figure 4 shows our results for various num-
bers of authors, using 200 tweets per author as train-
ing set. Results demonstrate that authors of an
unknown tweet can be identified to a large extent
even when there are as many as 1,000 candidate au-
thors (30.3%, as opposed to a random baseline of
only 0.1%). Results further validate that word n-
gram features substantially improve over character
</bodyText>
<footnote confidence="0.9065305">
9Results for 50 authors with 200 tweets per author are taken
from Figure 3.
</footnote>
<page confidence="0.964684">
1885
</page>
<figure confidence="0.999368516129032">
100
100
90
90
80
80
Precision (%)
60
70
60
50
50
1,000 tweets/author
500 tweets/author
200 tweets/author
100 tweets/author
50 tweets/author
40
50 authors
100 authors
200 authors
500 authors
1,000 authors
40
0 10 20 30 40 50 60 70 80 90 100
Recall (%)
30
0 10 20 30 40 50 60 70 80 90 100
Recall (%)
70
Precision (%)
</figure>
<figureCaption confidence="0.999002">
Figure 5: Recall-precision curves for 50 authors with
varying training set sizes.
</figureCaption>
<bodyText confidence="0.996869571428571">
n-grams features (2.6% averaged improvement).
Recall-Precision Tradeoff. Figure 5 shows the
recall-precision curves for our experiments with 50
authors and varying training set sizes. Results
demonstrate that we are able to obtain very high pre-
cision (over 90%) while still maintaining a relatively
high recall (from —35% recall for 50 tweets per au-
thor up to &gt; 60% recall for 1,000 tweets per author).
Figure 6 shows the recall-precision curves for our
experiments with varying number of authors. Re-
sults demonstrate that even in the 1,000 authors set-
ting, we are able to obtain high precision values
(90% and 70%) with reasonable recall values (18%
and —30%, respectively).
</bodyText>
<sectionHeader confidence="0.991639" genericHeader="method">
7 Flexible Patterns
</sectionHeader>
<bodyText confidence="0.999244090909091">
In previous sections we provided strong evidence
that authors of micro-messages can be successfully
identified using standard methods. In this section we
present a new feature, never previously used for this
task – flexible patterns. We show that flexible pat-
terns can be used to improve classification results.
Flexible patterns are a generalization of word n-
grams, in the sense that they capture potentially un-
seen word n-grams. As a result, flexible patterns
can pick up fine-grained differences between au-
thors’ styles. Unlike other types of pattern features,
</bodyText>
<figureCaption confidence="0.997851">
Figure 6: Recall-precision curves for varying number of
authors.
</figureCaption>
<bodyText confidence="0.973721545454546">
flexible patterns are computed automatically from
plain text. As such, they can be applied to various
tasks, independently of domain and language. We
describe them in detail.
Word Frequency. Flexible patterns are composed
of high frequency words (HFW) and content words
(CW). Every word in the corpus is defined as either
HFW or CW. This clustering is performed by count-
ing the number of times each word appears in the
corpus of size s. A word that appears more than
10−4xs times in a corpus is considered HFW. A
word that appears less than 10−3xs times in a cor-
pus is considered CW. Some words may serve both
as HFWs and CWs (see Davidov and Rappoport
(2008b) for discussion).
Structure of a Flexible Pattern. Flexible patterns
start and end with an HFW. A sequence of zero or
more CWs separates consecutive HFWs. At least
one CW must appear in every pattern.10 For effi-
ciency, at most six HFWs (and as a result, five CW
sequences) may appear in a flexible pattern. Exam-
ples of flexible patterns include
</bodyText>
<footnote confidence="0.8856365">
1. “theHFW CW ofHFW theHFW”
10Omitting this treats word n-grams as flexible patterns.
</footnote>
<page confidence="0.967085">
1886
</page>
<figure confidence="0.25459">
Accuracy (%)
</figure>
<listItem confidence="0.833031263157895">
Flexible Pattern Features. Flexible patterns can
serve as binary classification features; a tweet
matches a given flexible pattern if it contains the
flexible pattern sequence. For example, (1) is
matched by (2).
2. “Go to the„FW houseCW of„FW the„FW rising sun”
Partial Flexible Patterns. A flexible pattern may
appear in a given tweet with additional words not
originally found in the flexible pattern, and/or with
only a subset of the HFWs (Davidov et al., 2010a).
For example, (3) is a partial match of (1), since the
word “great” is not part of the original flexible pat-
tern. Similarly, (4) is another partial match of (1),
since (a) the word “good” is not part of the original
flexible pattern and (b) the second occurrence of the
word “the” does not appear in (4) (missing word is
marked by ).
3. “The„FW great..., kingCW of„FW the„FW ring”
4. “The„FW good..., kingCW of„FW Spain”
</listItem>
<bodyText confidence="0.929926">
We use such cases as features with lower weight,
proportional to the number of found HFWs in the
tweet (w = an X n found ). For example, (1) receives a
expected
weight of 1 (complete match) against (2). Against
(3), it receives a weight of 0.5 (= 0.5�3
</bodyText>
<sectionHeader confidence="0.75024" genericHeader="method">
3 , partial
</sectionHeader>
<bodyText confidence="0.983236666666667">
match with no missing HFWs). Against (4) it re-
ceives a weight of 1/3 (= 0.5�2
3 , partial match with
only 2/3 HFWs found).
Experimenting with Flexible Pattern Features.
We repeat our experiments with varying training set
sizes (see Section 5) with two more systems: one
that uses character n-grams and flexible pattern fea-
tures, and another that uses character n-grams, word
n-grams and flexible patterns. High frequency word
counts are computed separately for each author us-
ing her training set. We only consider flexible pat-
tern features that appear at least tfp times in the
training set of at least one author. Values of tfp for
training set sizes (50, 100, 200, 500, 1,000) are (2,
3, 7, 7, 8), respectively.
Results. Figure 7 shows our results. Results
demonstrate that flexible pattern features have an
added value over both character n-grams alone (av-
eraged 2.9% improvement) and over character n-
grams and word n-grams together (averaged 1.5%
</bodyText>
<figure confidence="0.9212845">
0 100 200 300 400 500 600 700 800 900 1000
Training Set Size
</figure>
<figureCaption confidence="0.903234875">
Figure 7: Authorship attribution accuracy for 50 authors
with various training set sizes and various feature sets.
The values are averaged over 10 groups. The random
baseline is 2%.
Comparison to previous work: SCAP – SCAP algo-
rithm results, as reported by (Layton et al., 2010), Naive
Bayes – Naive Bayes algorithm results, as reported by
(Boutwell, 2011).
</figureCaption>
<bodyText confidence="0.999923523809524">
improvement). We perform t-tests on each of our
training set sizes to check whether the latter im-
provement is significant. Results demonstrate that
it is highly significant in all settings, with p-values
smaller than values between 10−3 (for 50 tweets per
author) and 10−8 (1,000 tweets per author).
Comparison to Previous Works. Figure 7 also
shows results for the only two works that experi-
mented in some of the settings we experimented in:
Layton et al. (2010) and Boutwell (2011) (see Sec-
tion 8). Our system substantially outperforms these
two systems, by margins of 5.9% to 19%. These
margins are explained by the choice of algorithm
(SVM and not SCAP/naive Bayes) and our set of
features (character n-grams + word n-grams + flex-
ible patterns compared to character n-grams only).
In order to rule out the possibility that these mar-
gins stem from using different datasets, we tested
our system on the dataset used in (Layton et al.,
2010). Our system obtains even higher results on
this dataset than on our datasets (61.6%, a total im-
</bodyText>
<page confidence="0.563746">
65
</page>
<figure confidence="0.981329333333333">
60
45
40
75
70
55
50
35
Char. N−grams, Word N−grams &amp;
Flex. Patt. Feats.
Char. N−grams + Flex. Patt. Feats.
Char. N−grams + Word N−grams
Char. N−grams
SCAP
Naive Bayes
</figure>
<page confidence="0.985965">
1887
</page>
<bodyText confidence="0.99029475">
provement of 6.1% over (Layton et al., 2010)).
Discussion. To illustrate the additional contribu-
tion of flexible patterns over word n-grams, consider
the following tweets, written by the same author.
</bodyText>
<listItem confidence="0.99343875">
5. “... the„.W wayCW I„.W treatedCW her„.W”
6. “...half of the„.W thingsCW I„.W have seen”
7. “... the„.W friendsCW I„.W have had for years”
8. “... in the„.W neighborhoodCW I„.W grew up in”
</listItem>
<bodyText confidence="0.999333833333333">
Consider a case where (5) is part of the test set,
while (6-8) appear in the training set. As (5) shares
no sequence of words with (6-8), no word n-gram
feature is able to identify the author’s style in (5).
However, this style can be successfully identified us-
ing the flexible pattern (9), shared by (5-8).
</bodyText>
<listItem confidence="0.808426">
9. the„.W CW I„.W
</listItem>
<bodyText confidence="0.998413">
This demonstrates the added value flexible pattern
features have over word n-gram features.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999954647887324">
Authorship attribution dates back to the end of 19th
century, when (Mendenhall, 1887) applied sentence
length and word length features to plays of Shake-
speare. Ever since, many methods have been devel-
oped for this task. For recent surveys, see (Koppel
et al., 2009; Stamatatos, 2009; Juola, 2012).
Authorship attribution methods can be generally
divided into two categories (Stamatatos, 2009). In
similarity-based methods, an anonymous text is at-
tributed to some author whose writing style is most
similar (by some distance metric). In machine learn-
ing methods, which we follow in this paper, anony-
mous texts are classified, using machine learning al-
gorithms, into different categories (in this case, dif-
ferent authors).
Machine learning papers differ from each other by
the features and machine learning algorithm. Exam-
ples of features include HFWs (Mosteller and Wal-
lace, 1964; Argamon et al., 2007), character n-gram
(Kjell, 1994; Hoorn et al., 1999; Stamatatos, 2008),
word n-grams (Peng et al., 2004), part-of-speech
n-grams (Koppel and Schler, 2003; Koppel et al.,
2005) and vocabulary richness (Abbasi and Chen,
2005).
The various machine learning algorithms used in-
clude naive Bayes (Mosteller and Wallace, 1964;
Kjell, 1994), neural networks (Matthews and Mer-
riam, 1993; Kjell, 1994), K-nearest neighbors (Kjell
et al., 1995; Hoorn et al., 1999) and SVM (De Vel et
al., 2001; Diederich et al., 2003; Koppel and Schler,
2003).
Traditionally, authorship attribution systems have
mainly been evaluated against long texts such as
theater plays (Mendenhall, 1887), essays (Yule,
1939; Mosteller and Wallace, 1964), biblical books
(Mealand, 1995; Koppel et al., 2011a) and book
chapters (Argamon et al., 2007; Koppel et al., 2007).
In recent year, many works focused on web data
such as emails (De Vel et al., 2001; Koppel and
Schler, 2003; Abbasi and Chen, 2008), web forum
messages (Abbasi and Chen, 2005; Solorio et al.,
2011), blogs (Koppel et al., 2006; Koppel et al.,
2011b) and chat messages (Abbasi and Chen, 2008).
Some works focused on SMS messages (Mohan et
al., 2010; Ishihara, 2011).
Authorship Attribution on Twitter. The perfor-
mance of authorship attribution systems on short
texts is affected by several factors (Stamatatos,
2009). These factors include the number of candi-
date authors, the training set size and the size of the
test document.
Very few authorship attribution works experi-
mented with Twitter. Unlike our work, all used a
single group of authors (group sizes varied between
3-50). Layton et al. (2010) used the SCAP method-
ology (Frantzeskou et al., 2007) with character n-
gram features. They experimented with 50 authors
and compared different numbers of tweets per au-
thor (values between 20-200). Surprisingly, they
showed that their system does not improve when
given more training tweets. In our work, we no-
ticed a different trend, and showed that more data
can be extremely valuable for authorship attribution
systems on micro-messages (see Section 6). Silva
et al. (2011) trained an SVM classifier with various
features (e.g., punctuation and vocabulary features)
on a small dataset of three authors only, with vary-
ing training set size. Although their work used a
set of Twitter-specific features that we do not explic-
itly use, our features implicitly cover a large portion
of their features (such as punctuation and emoticon
</bodyText>
<page confidence="0.98553">
1888
</page>
<bodyText confidence="0.999913268292683">
features, which are largely covered by character n-
grams).
Boutwell (2011) used a naive Bayes classifier
with character n-gram features. She experimented
with 50 authors and two training size values (120
and 230). She also provided a set of experiments that
studied the effect of joining several tweets into a sin-
gle document. Mikros and Perifanos (2013) trained
an SVM classifier with character n-gram and word
n-grams. They experimented with 10 authors of
Greek text, and also joined several tweets into a sin-
gle document. Joining several tweets into a longer
document is appealing since it can lead to substantial
improvement of the classification results, as demon-
strated by the works above. However, this approach
requires the test data to contain several tweets that
are known a-priori to be written by the same author.
This assumption is not always realistic. In our paper,
we intentionally focus on a single tweet as document
size.
Flexible Patterns. Patterns were introduced by
(Hearst, 1992), who used hand crafted patterns
to discover hyponyms. Hard coded patterns
were used for many tasks, such as discovering
meronymy (Berland and Charniak, 1999), noun cat-
egories (Widdows and Dorow, 2002), verb relations
(Chklovski and Pantel, 2004) and semantic class
learning (Kozareva et al., 2008).
Patterns were first extracted in a fully unsuper-
vised manner (“flexible patterns”) by (Davidov and
Rappoport, 2006), who used flexible patterns in or-
der to establish noun categories, and (Bicic¸i and
Yuret, 2006) who used them for analogy question
answering. Ever since, flexible patterns were used
as features for various tasks such as extraction of
semantic relationships (Davidov et al., 2007; Tur-
ney, 2008b; Bollegala et al., 2009), detection of
synonyms (Turney, 2008a), disambiguation of nom-
inal compound relations (Davidov and Rappoport,
2008a), sentiment analysis (Davidov et al., 2010b)
and detection of sarcasm (Tsur et al., 2010).
</bodyText>
<sectionHeader confidence="0.9974" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999963785714286">
The main goal of this paper is to measure to what
extent authors of micro-messages can be identified.
We have shown that authors of very short texts
can be successfully identified in an array of au-
thorship attribution settings reported for long doc-
uments. This is the first work on micro-messages
to address some of these settings. We introduced
the concept of k-signature. Using this concept, we
proposed an interpretation of our results. Last, we
presented the first authorship attribution system that
uses flexible patterns, and demonstrated that using
these features significantly improves over other sys-
tems. Our system obtains 6.1% improvement over
the current state-of-the-art.
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999642142857143">
We would like to thank Elad Eban and Susan Good-
man for their helpful advice, as well as Robert Lay-
ton for providing us with his dataset. This research
was funded (in part) by the Harry and Sylvia Hoff-
man leadership and responsibility program (for the
first author) and the Intel Collaborative Research In-
stitute for Computational Intelligence (ICRI-CI).
</bodyText>
<sectionHeader confidence="0.982338" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.848336392857143">
Ahmed Abbasi and Hsinchun Chen. 2005. Applying au-
thorship analysis to extremist-group web forum mes-
sages. IEEE Intelligent Systems, 20:67–75.
Ahmed Abbasi and Hsinchun Chen. 2008. Writeprints:
A stylometric approach to identity-level identification
and similarity detection in cyberspace. ACM Transac-
tions on Information Systems, 26(2):7:1–7:29.
Shlomo Argamon, Casey Whitelaw, Paul Chase, Sob-
han Raj Hota, Navendu Garg, and Shlomo Levitan.
2007. Stylistic text classification using functional lex-
ical features: Research articles. J. Am. Soc. Inf. Sci.
Technol., 58(6):802–822.
Matthew Berland and Eugene Charniak. 1999. Finding
parts in very large corpora. In Proc. of ACL, pages
57–64, College Park, Maryland, USA.
Ergun Bicic¸i and Deniz Yuret. 2006. Clustering word
pairs to answer analogy questions. In Proc. of TAINN,
pages 1–8.
Danushka T. Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2009. Measuring the similarity between
implicit semantic relations from the web. In Proc. of
WWW, New York, New York, USA. ACM Press.
Sarah R. Boutwell. 2011. Authorship Attribution of
Short Messages Using Multimodal Features. Master’s
thesis, Naval Postgraduate School.
John Burrows. 2002. ‘Delta’: a Measure of Stylistic
Difference and a Guide to Likely Authorship. Literary
and Linguistic Computing, 17(3):267–287.
</reference>
<page confidence="0.969754">
1889
</page>
<reference confidence="0.989071438095237">
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1–
27:27. Software available at http://www.csie.
ntu.edu.tw/˜cjlin/libsvm.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Dekang Lin and Dekai Wu, editors, Proc.
of EMNLP, pages 33–40, Barcelona, Spain.
Dmitry Davidov and Ari Rappoport. 2006. Efficient un-
supervised discovery of word categories using sym-
metric patterns and high frequency words. In Proc.
ofACL-Coling, pages 297–304, Sydney, Australia.
Dmitry Davidov and Ari Rappoport. 2008a. Classifi-
cation of semantic relationships between nominals us-
ing pattern clusters. In Proceedings of ACL-08: HLT,
pages 227–235, Columbus, Ohio, June. Association
for Computational Linguistics.
Dmitry Davidov and Ari Rappoport. 2008b. Unsuper-
vised discovery of generic relationships using pattern
clusters and its evaluation by automatically generated
SAT analogy questions. In Proc. of ACL-HLT, pages
692–700, Columbus, Ohio.
Dmitry Davidov, Ari Rappoport, and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Proc. ofACL,
pages 232–239, Prague, Czech Republic.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010a.
Semi-supervised recognition of sarcastic sentences in
twitter and amazon. In Proc. of CoNLL, pages 107–
116, Uppsala, Sweden.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010b.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proc. of Coling, pages 241–249, Bei-
jing, China.
Olivier De Vel, Alison Anderson, Malcolm Corney, and
George Mohay. 2001. Mining e-mail content for au-
thor identification forensics. ACM Sigmod Record,
30(4):55–64.
Joachim Diederich, J¨org Kindermann, Edda Leopold, and
Gerhard Paass. 2003. Authorship attribution with
support vector machines. Applied intelligence, 19(1-
2):109–123.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukwac, a very large web-derived corpus of english. In
Proc. of the 4th Web as Corpus Workshop, WAC-4.
Georgia Frantzeskou, Efstathios Stamatatos, Stefanos
Gritzalis, and Carole E Chaski. 2007. Identifying au-
thorship by byte-level n-grams: The source code au-
thor profile (scap) method. Int Journal of Digital Evi-
dence, 6(1):1–18.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of Coling
– Volume 2, pages 539–545, Stroudsburg, PA, USA.
Johan F Hoorn, Stefan L Frank, Wojtek Kowalczyk, and
Floor van der Ham. 1999. Neural network identifi-
cation of poets using letter sequences. Literary and
Linguistic Computing, 14(3):311–338.
Shunichi Ishihara. 2011. A forensic authorship clas-
sification in sms messages: A likelihood ratio based
approach using n-gram. In Proc. of the Australasian
Language Technology Association Workshop 2011,
pages 47–56, Canberra, Australia.
Patrick Juola. 2012. Large-scale experiments in author-
ship attribution. English Studies, 93(3):275–283.
Bradley Kjell, W Addison Woods, and Ophir Frieder.
1995. Information retrieval using letter tuples with
neural network and nearest neighbor classifiers. In
IEEE International Conference on Systems, Man and
Cybernetics, volume 2, pages 1222–1226. IEEE.
Bradley Kjell. 1994. Authorship determination using let-
ter pair frequency features with neural network classi-
fiers. Literary and Linguistic Computing, 9(2):119–
124.
Moshe Koppel and Jonathan Schler. 2003. Exploiting
stylistic idiosyncrasies for authorship attribution. In
Proc. of IJCAI’03 Workshop on Computational Ap-
proaches to Style Analysis and Synthesis, volume 69,
page 72.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author’s native language by mining a
text for errors. In Proc. of the eleventh ACM SIGKDD
international conference on Knowledge discovery in
data mining, KDD ’05, pages 624–628, New York,
NY, USA.
Moshe Koppel, Jonathan Schler, Shlomo Argamon, and
Eran Messeri. 2006. Authorship attribution with thou-
sands of candidate authors. In SIGIR, pages 659–660.
Moshe Koppel, Jonathan Schler, and Elisheva Bonchek-
Dokow. 2007. Measuring differentiability: Unmask-
ing pseudonymous authors. JMLR, 8:1261–1276.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2009. Computational methods in authorship attribu-
tion. J. Am. Soc. Inf. Sci. Technol., 60(1):9–26.
Moshe Koppel, Navot Akiva, Idan Dershowitz, and
Nachum Dershowitz. 2011a. Unsupervised decom-
position of a document into authorial components. In
Proc. of ACL-HLT, pages 1356–1364, Portland, Ore-
gon, USA.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2011b. Authorship attribution in the wild. Language
Resources and Evaluation, 45(1):83–94.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
</reference>
<page confidence="0.772372">
1890
</page>
<reference confidence="0.999900605633803">
pattern linkage graphs. In Proc. of ACL-HLT, pages
1048–1056, Columbus, Ohio.
Robert Layton, Paul Watters, and Richard Dazeley. 2010.
Authorship attribution for twitter in 140 characters or
less. In Proc. of the 2010 Second Cybercrime and
Trustworthy Computing Workshop, CTC ’10, pages 1–
8, Washington, DC, USA. IEEE Computer Society.
Robert AJ Matthews and Thomas VN Merriam. 1993.
Neural computation in stylometry i: An application to
the works of shakespeare and fletcher. Literary and
Linguistic Computing, 8(4):203–209.
DL Mealand. 1995. Correspondence analysis of luke.
Literary and linguistic computing, 10(3):171–182.
Thomas Corwin Mendenhall. 1887. The characteristic
curves of composition. Science, ns-9(214S):237–246.
George K Mikros and Kostas Perifanos. 2013. Author-
ship attribution in greek tweets using authors multi-
level n-gram profiles. In 2013 AAAI Spring Sympo-
sium Series.
Ashwin Mohan, Ibrahim M Baggili, and Marcus K
Rogers. 2010. Authorship attribution of sms mes-
sages using an n-grams approach. Technical report,
CERIAS Tech Report 2011.
Frederick Mosteller and David Lee Wallace. 1964.
Inference and disputed authorship: The Federalist.
Addison-Wesley.
Fuchun Peng, Dale Schuurmans, and Shaojun Wang.
2004. Augmenting naive bayes classifiers with sta-
tistical language models. Information Retrieval, 7(3-
4):317–345.
Conrad Sanderson and Simon Guenter. 2006. Short text
authorship attribution via sequence kernels, markov
chains and author unmasking: An investigation. In
Proc. of EMNLP, pages 482–491, Sydney, Australia.
Rui Sousa Silva, Gustavo Laboreiro, Lu´ıs Sarmento, Tim
Grant, Eug´enio Oliveira, and Belinda Maia. 2011.
‘twazn me!!! ;(’ automatic authorship analysis of
micro-blogging messages. In Proc. of the 16th inter-
national conference on Natural language processing
and information systems, NLDB’11, pages 161–168,
Berlin, Heidelberg. Springer-Verlag.
Thamar Solorio, Sangita Pillay, Sindhu Raghavan, and
Manuel Montes-Gomez. 2011. Modality specific
meta features for authorship attribution in web forum
posts. In Proc. of IJCNLP, pages 156–164, Chiang
Mai, Thailand, November.
Efstathios Stamatatos. 2008. Author identification: Us-
ing text sampling to handle the class imbalance prob-
lem. Inf. Process. Manage., 44(2):790–799.
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for Information Science and Technology,
60(3):538–556.
Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.
Icwsm–a great catchy name: Semi-supervised recog-
nition of sarcastic sentences in online product reviews.
In Proc. of ICWSM.
Peter Turney. 2008a. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proc. of
Coling, pages 905–912, Manchester, UK, August. Col-
ing 2008 Organizing Committee.
Peter D. Turney. 2008b. The latent relation mapping en-
gine: Algorithm and experiments. Journal ofArtificial
Intelligence Research, 33:615–655.
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. In Proc.
of Coling, pages 1–7, Stroudsburg, PA, USA.
George Udny Yule. 1939. On sentence-length as a statis-
tical characteristic of style in prose: with application
to two cases of disputed authorship. Biometrika, 30(3-
4):363–390.
</reference>
<page confidence="0.993406">
1891
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.958299">
<title confidence="0.999756">Authorship Attribution of Micro-Messages</title>
<author confidence="0.999925">Roy Schwartz Oren Tsur Ari Rappoport Moshe Koppel</author>
<affiliation confidence="0.9999635">Institute of Computer Science Department of Computer Science Hebrew University of Jerusalem Bar Ilan University</affiliation>
<email confidence="0.989536">koppel@macs.biu.ac.il</email>
<abstract confidence="0.9967590625">Work on authorship attribution has traditionally focused on long texts. In this work, we tackle the question of whether the author of a very short text can be successfully identified. We use Twitter as an experimental testbed. We introduce the concept of an author’s unique “signature”, and show that such signatures are typical of many authors when writing very short texts. We also present a new authorship attribution feature (“flexible patterns”) and demonstrate a significant improvement over our baselines. Our results show that the author of a single tweet can be identified with good accuracy in an array of flavors of the authorship attribution task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ahmed Abbasi</author>
<author>Hsinchun Chen</author>
</authors>
<title>Applying authorship analysis to extremist-group web forum messages.</title>
<date>2005</date>
<journal>IEEE Intelligent Systems,</journal>
<pages>20--67</pages>
<contexts>
<context position="29436" citStr="Abbasi and Chen, 2005" startWordPosition="4923" endWordPosition="4926">lar (by some distance metric). In machine learning methods, which we follow in this paper, anonymous texts are classified, using machine learning algorithms, into different categories (in this case, different authors). Machine learning papers differ from each other by the features and machine learning algorithm. Examples of features include HFWs (Mosteller and Wallace, 1964; Argamon et al., 2007), character n-gram (Kjell, 1994; Hoorn et al., 1999; Stamatatos, 2008), word n-grams (Peng et al., 2004), part-of-speech n-grams (Koppel and Schler, 2003; Koppel et al., 2005) and vocabulary richness (Abbasi and Chen, 2005). The various machine learning algorithms used include naive Bayes (Mosteller and Wallace, 1964; Kjell, 1994), neural networks (Matthews and Merriam, 1993; Kjell, 1994), K-nearest neighbors (Kjell et al., 1995; Hoorn et al., 1999) and SVM (De Vel et al., 2001; Diederich et al., 2003; Koppel and Schler, 2003). Traditionally, authorship attribution systems have mainly been evaluated against long texts such as theater plays (Mendenhall, 1887), essays (Yule, 1939; Mosteller and Wallace, 1964), biblical books (Mealand, 1995; Koppel et al., 2011a) and book chapters (Argamon et al., 2007; Koppel et a</context>
</contexts>
<marker>Abbasi, Chen, 2005</marker>
<rawString>Ahmed Abbasi and Hsinchun Chen. 2005. Applying authorship analysis to extremist-group web forum messages. IEEE Intelligent Systems, 20:67–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Abbasi</author>
<author>Hsinchun Chen</author>
</authors>
<title>Writeprints: A stylometric approach to identity-level identification and similarity detection in cyberspace.</title>
<date>2008</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="1457" citStr="Abbasi and Chen, 2008" startWordPosition="217" endWordPosition="220">be identified with good accuracy in an array of flavors of the authorship attribution task. 1 Introduction Research in authorship attribution has developed substantially over the last decade (Stamatatos, 2009). The vast majority of such research has been dedicated towards finding the author of long texts, ranging from single passages to book chapters. In recent years, the growing popularity of social media has created special interest, both theoretical and computational, in short texts. This has led to many recent authorship attribution projects that experimented with web data such as emails (Abbasi and Chen, 2008), web forum messages (Solorio et al., 2011) and blogs (Koppel et al., 2011b). This paper addresses the question to what extent the authors of very short texts can be identified. To answer this question, we experiment with Twitter tweets. Twitter messages (tweets) are limited to 140 characters. This restriction imposes major difficulties on authorship attribution systems, since authorship attribution methods that work well on long texts are often not as useful when applied to short texts (Burrows, 2002; Sanderson and Guenter, 2006). Nonetheless, tweets are relatively self-contained and have sma</context>
<context position="30178" citStr="Abbasi and Chen, 2008" startWordPosition="5041" endWordPosition="5044">tworks (Matthews and Merriam, 1993; Kjell, 1994), K-nearest neighbors (Kjell et al., 1995; Hoorn et al., 1999) and SVM (De Vel et al., 2001; Diederich et al., 2003; Koppel and Schler, 2003). Traditionally, authorship attribution systems have mainly been evaluated against long texts such as theater plays (Mendenhall, 1887), essays (Yule, 1939; Mosteller and Wallace, 1964), biblical books (Mealand, 1995; Koppel et al., 2011a) and book chapters (Argamon et al., 2007; Koppel et al., 2007). In recent year, many works focused on web data such as emails (De Vel et al., 2001; Koppel and Schler, 2003; Abbasi and Chen, 2008), web forum messages (Abbasi and Chen, 2005; Solorio et al., 2011), blogs (Koppel et al., 2006; Koppel et al., 2011b) and chat messages (Abbasi and Chen, 2008). Some works focused on SMS messages (Mohan et al., 2010; Ishihara, 2011). Authorship Attribution on Twitter. The performance of authorship attribution systems on short texts is affected by several factors (Stamatatos, 2009). These factors include the number of candidate authors, the training set size and the size of the test document. Very few authorship attribution works experimented with Twitter. Unlike our work, all used a single gro</context>
</contexts>
<marker>Abbasi, Chen, 2008</marker>
<rawString>Ahmed Abbasi and Hsinchun Chen. 2008. Writeprints: A stylometric approach to identity-level identification and similarity detection in cyberspace. ACM Transactions on Information Systems, 26(2):7:1–7:29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shlomo Argamon</author>
<author>Casey Whitelaw</author>
<author>Paul Chase</author>
<author>Sobhan Raj Hota</author>
<author>Navendu Garg</author>
<author>Shlomo Levitan</author>
</authors>
<title>Stylistic text classification using functional lexical features: Research articles.</title>
<date>2007</date>
<journal>J. Am. Soc. Inf. Sci. Technol.,</journal>
<volume>58</volume>
<issue>6</issue>
<contexts>
<context position="29213" citStr="Argamon et al., 2007" startWordPosition="4890" endWordPosition="4893">09; Juola, 2012). Authorship attribution methods can be generally divided into two categories (Stamatatos, 2009). In similarity-based methods, an anonymous text is attributed to some author whose writing style is most similar (by some distance metric). In machine learning methods, which we follow in this paper, anonymous texts are classified, using machine learning algorithms, into different categories (in this case, different authors). Machine learning papers differ from each other by the features and machine learning algorithm. Examples of features include HFWs (Mosteller and Wallace, 1964; Argamon et al., 2007), character n-gram (Kjell, 1994; Hoorn et al., 1999; Stamatatos, 2008), word n-grams (Peng et al., 2004), part-of-speech n-grams (Koppel and Schler, 2003; Koppel et al., 2005) and vocabulary richness (Abbasi and Chen, 2005). The various machine learning algorithms used include naive Bayes (Mosteller and Wallace, 1964; Kjell, 1994), neural networks (Matthews and Merriam, 1993; Kjell, 1994), K-nearest neighbors (Kjell et al., 1995; Hoorn et al., 1999) and SVM (De Vel et al., 2001; Diederich et al., 2003; Koppel and Schler, 2003). Traditionally, authorship attribution systems have mainly been eva</context>
</contexts>
<marker>Argamon, Whitelaw, Chase, Hota, Garg, Levitan, 2007</marker>
<rawString>Shlomo Argamon, Casey Whitelaw, Paul Chase, Sobhan Raj Hota, Navendu Garg, and Shlomo Levitan. 2007. Stylistic text classification using functional lexical features: Research articles. J. Am. Soc. Inf. Sci. Technol., 58(6):802–822.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Berland</author>
<author>Eugene Charniak</author>
</authors>
<title>Finding parts in very large corpora.</title>
<date>1999</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>57--64</pages>
<location>College Park, Maryland, USA.</location>
<contexts>
<context position="32840" citStr="Berland and Charniak, 1999" startWordPosition="5468" endWordPosition="5471"> several tweets into a longer document is appealing since it can lead to substantial improvement of the classification results, as demonstrated by the works above. However, this approach requires the test data to contain several tweets that are known a-priori to be written by the same author. This assumption is not always realistic. In our paper, we intentionally focus on a single tweet as document size. Flexible Patterns. Patterns were introduced by (Hearst, 1992), who used hand crafted patterns to discover hyponyms. Hard coded patterns were used for many tasks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms</context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>Matthew Berland and Eugene Charniak. 1999. Finding parts in very large corpora. In Proc. of ACL, pages 57–64, College Park, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bicic¸i</author>
<author>Deniz Yuret</author>
</authors>
<title>Clustering word pairs to answer analogy questions.</title>
<date>2006</date>
<booktitle>In Proc. of TAINN,</booktitle>
<pages>1--8</pages>
<marker>Bicic¸i, Yuret, 2006</marker>
<rawString>Ergun Bicic¸i and Deniz Yuret. 2006. Clustering word pairs to answer analogy questions. In Proc. of TAINN, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danushka T Bollegala</author>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Measuring the similarity between implicit semantic relations from the web. In</title>
<date>2009</date>
<booktitle>Proc. of WWW,</booktitle>
<publisher>ACM Press.</publisher>
<location>New York, New York, USA.</location>
<contexts>
<context position="33417" citStr="Bollegala et al., 2009" startWordPosition="5557" endWordPosition="5560">covering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2008a), sentiment analysis (Davidov et al., 2010b) and detection of sarcasm (Tsur et al., 2010). 9 Conclusion The main goal of this paper is to measure to what extent authors of micro-messages can be identified. We have shown that authors of very short texts can be successfully identified in an array of authorship attribution settings reported for long documents. This is the first work on micro-messages to address some of these settings. We introduced the concept of k-signature. Using </context>
</contexts>
<marker>Bollegala, Matsuo, Ishizuka, 2009</marker>
<rawString>Danushka T. Bollegala, Yutaka Matsuo, and Mitsuru Ishizuka. 2009. Measuring the similarity between implicit semantic relations from the web. In Proc. of WWW, New York, New York, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah R Boutwell</author>
</authors>
<title>Authorship Attribution of Short Messages Using Multimodal Features. Master’s thesis,</title>
<date>2011</date>
<institution>Naval Postgraduate School.</institution>
<contexts>
<context position="26272" citStr="Boutwell, 2011" startWordPosition="4402" endWordPosition="4403">sults demonstrate that flexible pattern features have an added value over both character n-grams alone (averaged 2.9% improvement) and over character ngrams and word n-grams together (averaged 1.5% 0 100 200 300 400 500 600 700 800 900 1000 Training Set Size Figure 7: Authorship attribution accuracy for 50 authors with various training set sizes and various feature sets. The values are averaged over 10 groups. The random baseline is 2%. Comparison to previous work: SCAP – SCAP algorithm results, as reported by (Layton et al., 2010), Naive Bayes – Naive Bayes algorithm results, as reported by (Boutwell, 2011). improvement). We perform t-tests on each of our training set sizes to check whether the latter improvement is significant. Results demonstrate that it is highly significant in all settings, with p-values smaller than values between 10−3 (for 50 tweets per author) and 10−8 (1,000 tweets per author). Comparison to Previous Works. Figure 7 also shows results for the only two works that experimented in some of the settings we experimented in: Layton et al. (2010) and Boutwell (2011) (see Section 8). Our system substantially outperforms these two systems, by margins of 5.9% to 19%. These margins </context>
<context position="31755" citStr="Boutwell (2011)" startWordPosition="5296" endWordPosition="5297">our work, we noticed a different trend, and showed that more data can be extremely valuable for authorship attribution systems on micro-messages (see Section 6). Silva et al. (2011) trained an SVM classifier with various features (e.g., punctuation and vocabulary features) on a small dataset of three authors only, with varying training set size. Although their work used a set of Twitter-specific features that we do not explicitly use, our features implicitly cover a large portion of their features (such as punctuation and emoticon 1888 features, which are largely covered by character ngrams). Boutwell (2011) used a naive Bayes classifier with character n-gram features. She experimented with 50 authors and two training size values (120 and 230). She also provided a set of experiments that studied the effect of joining several tweets into a single document. Mikros and Perifanos (2013) trained an SVM classifier with character n-gram and word n-grams. They experimented with 10 authors of Greek text, and also joined several tweets into a single document. Joining several tweets into a longer document is appealing since it can lead to substantial improvement of the classification results, as demonstrate</context>
</contexts>
<marker>Boutwell, 2011</marker>
<rawString>Sarah R. Boutwell. 2011. Authorship Attribution of Short Messages Using Multimodal Features. Master’s thesis, Naval Postgraduate School.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Burrows</author>
</authors>
<title>Delta’: a Measure of Stylistic Difference and a Guide to Likely Authorship. Literary and Linguistic Computing,</title>
<date>2002</date>
<contexts>
<context position="1963" citStr="Burrows, 2002" startWordPosition="299" endWordPosition="301">cent authorship attribution projects that experimented with web data such as emails (Abbasi and Chen, 2008), web forum messages (Solorio et al., 2011) and blogs (Koppel et al., 2011b). This paper addresses the question to what extent the authors of very short texts can be identified. To answer this question, we experiment with Twitter tweets. Twitter messages (tweets) are limited to 140 characters. This restriction imposes major difficulties on authorship attribution systems, since authorship attribution methods that work well on long texts are often not as useful when applied to short texts (Burrows, 2002; Sanderson and Guenter, 2006). Nonetheless, tweets are relatively self-contained and have smaller sentence length variance compared to excerpts from longer texts (see Section 3). These characteristics make Twitter data appealing as a testbed when focusing on short texts. Moreover, an authorship attribution system of tweets may have various applications. Specifically, a range of cybercrimes can be addressed using such a system, including identity fraud and phishing. In this paper, we introduce the concept of ksignatures. We denote the k-signatures of an author a as the features that appear in </context>
</contexts>
<marker>Burrows, 2002</marker>
<rawString>John Burrows. 2002. ‘Delta’: a Measure of Stylistic Difference and a Guide to Likely Authorship. Literary and Linguistic Computing, 17(3):267–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<pages>27--27</pages>
<note>Software available at http://www.csie. ntu.edu.tw/˜cjlin/libsvm.</note>
<contexts>
<context position="8032" citStr="Chang and Lin, 2011" startWordPosition="1289" endWordPosition="1292">s would prefer using short, repeating phrases (word n-grams). In our experiments, we consider 2 &lt; n &lt; 5.1 We regard sequences of punctuation marks as words. Two special words are added to each tweet to indicate the beginning and the end of the tweet. For efficiency, we consider only word n-gram features that appear at least twng times in the training set of at least one author (see Section 5). Model. We use libsvm’s Matlab implementation of a multi-class SVM classifier with a linear kernel 1We skip unigrams as they are generally captured by the character n-gram features. 1881 Number of Users (Chang and Lin, 2011). We use ten-fold cross validation on the training set to select the best regularization factor between 0.5 and 0.005.2 3 Experimental Testbed Our main research question in this paper is to determine the extent to which authors of very short texts can be identified. A major issue in working with short texts is selecting the right dataset. One approach is breaking longer texts into shorter chunks (Sanderson and Guenter, 2006). We take a different approach and experiment with micro-messages (specifically, tweets). Tweets have several properties making them an ideal testbed for authorship attribu</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1– 27:27. Software available at http://www.csie. ntu.edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>Verbocean: Mining the web for fine-grained semantic verb relations.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proc. of EMNLP,</booktitle>
<pages>33--40</pages>
<location>Barcelona, Spain.</location>
<contexts>
<context position="32928" citStr="Chklovski and Pantel, 2004" startWordPosition="5481" endWordPosition="5484">rovement of the classification results, as demonstrated by the works above. However, this approach requires the test data to contain several tweets that are known a-priori to be written by the same author. This assumption is not always realistic. In our paper, we intentionally focus on a single tweet as document size. Flexible Patterns. Patterns were introduced by (Hearst, 1992), who used hand crafted patterns to discover hyponyms. Hard coded patterns were used for many tasks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. Verbocean: Mining the web for fine-grained semantic verb relations. In Dekang Lin and Dekai Wu, editors, Proc. of EMNLP, pages 33–40, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Efficient unsupervised discovery of word categories using symmetric patterns and high frequency words. In</title>
<date>2006</date>
<booktitle>Proc. ofACL-Coling,</booktitle>
<pages>297--304</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="33097" citStr="Davidov and Rappoport, 2006" startWordPosition="5506" endWordPosition="5509">ori to be written by the same author. This assumption is not always realistic. In our paper, we intentionally focus on a single tweet as document size. Flexible Patterns. Patterns were introduced by (Hearst, 1992), who used hand crafted patterns to discover hyponyms. Hard coded patterns were used for many tasks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2008a), sentiment analysis (Davidov et al., 2010b) and detection of sarcasm (Tsur et al., 2010). 9 Conclusion The main goal of this paper is to measure to what extent aut</context>
</contexts>
<marker>Davidov, Rappoport, 2006</marker>
<rawString>Dmitry Davidov and Ari Rappoport. 2006. Efficient unsupervised discovery of word categories using symmetric patterns and high frequency words. In Proc. ofACL-Coling, pages 297–304, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Classification of semantic relationships between nominals using pattern clusters.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>227--235</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="23300" citStr="Davidov and Rappoport (2008" startWordPosition="3890" endWordPosition="3893">automatically from plain text. As such, they can be applied to various tasks, independently of domain and language. We describe them in detail. Word Frequency. Flexible patterns are composed of high frequency words (HFW) and content words (CW). Every word in the corpus is defined as either HFW or CW. This clustering is performed by counting the number of times each word appears in the corpus of size s. A word that appears more than 10−4xs times in a corpus is considered HFW. A word that appears less than 10−3xs times in a corpus is considered CW. Some words may serve both as HFWs and CWs (see Davidov and Rappoport (2008b) for discussion). Structure of a Flexible Pattern. Flexible patterns start and end with an HFW. A sequence of zero or more CWs separates consecutive HFWs. At least one CW must appear in every pattern.10 For efficiency, at most six HFWs (and as a result, five CW sequences) may appear in a flexible pattern. Examples of flexible patterns include 1. “theHFW CW ofHFW theHFW” 10Omitting this treats word n-grams as flexible patterns. 1886 Accuracy (%) Flexible Pattern Features. Flexible patterns can serve as binary classification features; a tweet matches a given flexible pattern if it contains the</context>
<context position="33531" citStr="Davidov and Rappoport, 2008" startWordPosition="5572" endWordPosition="5575">lovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2008a), sentiment analysis (Davidov et al., 2010b) and detection of sarcasm (Tsur et al., 2010). 9 Conclusion The main goal of this paper is to measure to what extent authors of micro-messages can be identified. We have shown that authors of very short texts can be successfully identified in an array of authorship attribution settings reported for long documents. This is the first work on micro-messages to address some of these settings. We introduced the concept of k-signature. Using this concept, we proposed an interpretation of our results. Last, we presented the first authorship attribution sy</context>
</contexts>
<marker>Davidov, Rappoport, 2008</marker>
<rawString>Dmitry Davidov and Ari Rappoport. 2008a. Classification of semantic relationships between nominals using pattern clusters. In Proceedings of ACL-08: HLT, pages 227–235, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Unsupervised discovery of generic relationships using pattern clusters and its evaluation by automatically generated SAT analogy questions.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-HLT,</booktitle>
<pages>692--700</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="23300" citStr="Davidov and Rappoport (2008" startWordPosition="3890" endWordPosition="3893">automatically from plain text. As such, they can be applied to various tasks, independently of domain and language. We describe them in detail. Word Frequency. Flexible patterns are composed of high frequency words (HFW) and content words (CW). Every word in the corpus is defined as either HFW or CW. This clustering is performed by counting the number of times each word appears in the corpus of size s. A word that appears more than 10−4xs times in a corpus is considered HFW. A word that appears less than 10−3xs times in a corpus is considered CW. Some words may serve both as HFWs and CWs (see Davidov and Rappoport (2008b) for discussion). Structure of a Flexible Pattern. Flexible patterns start and end with an HFW. A sequence of zero or more CWs separates consecutive HFWs. At least one CW must appear in every pattern.10 For efficiency, at most six HFWs (and as a result, five CW sequences) may appear in a flexible pattern. Examples of flexible patterns include 1. “theHFW CW ofHFW theHFW” 10Omitting this treats word n-grams as flexible patterns. 1886 Accuracy (%) Flexible Pattern Features. Flexible patterns can serve as binary classification features; a tweet matches a given flexible pattern if it contains the</context>
<context position="33531" citStr="Davidov and Rappoport, 2008" startWordPosition="5572" endWordPosition="5575">lovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2008a), sentiment analysis (Davidov et al., 2010b) and detection of sarcasm (Tsur et al., 2010). 9 Conclusion The main goal of this paper is to measure to what extent authors of micro-messages can be identified. We have shown that authors of very short texts can be successfully identified in an array of authorship attribution settings reported for long documents. This is the first work on micro-messages to address some of these settings. We introduced the concept of k-signature. Using this concept, we proposed an interpretation of our results. Last, we presented the first authorship attribution sy</context>
</contexts>
<marker>Davidov, Rappoport, 2008</marker>
<rawString>Dmitry Davidov and Ari Rappoport. 2008b. Unsupervised discovery of generic relationships using pattern clusters and its evaluation by automatically generated SAT analogy questions. In Proc. of ACL-HLT, pages 692–700, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
<author>Moshe Koppel</author>
</authors>
<title>Fully unsupervised discovery of conceptspecific relationships by web mining.</title>
<date>2007</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>232--239</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="33377" citStr="Davidov et al., 2007" startWordPosition="5550" endWordPosition="5553">were used for many tasks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2008a), sentiment analysis (Davidov et al., 2010b) and detection of sarcasm (Tsur et al., 2010). 9 Conclusion The main goal of this paper is to measure to what extent authors of micro-messages can be identified. We have shown that authors of very short texts can be successfully identified in an array of authorship attribution settings reported for long documents. This is the first work on micro-messages to address some of these settings. We intro</context>
</contexts>
<marker>Davidov, Rappoport, Koppel, 2007</marker>
<rawString>Dmitry Davidov, Ari Rappoport, and Moshe Koppel. 2007. Fully unsupervised discovery of conceptspecific relationships by web mining. In Proc. ofACL, pages 232–239, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Semi-supervised recognition of sarcastic sentences in twitter and amazon.</title>
<date>2010</date>
<booktitle>In Proc. of CoNLL,</booktitle>
<pages>107--116</pages>
<location>Uppsala,</location>
<contexts>
<context position="14158" citStr="Davidov et al., 2010" startWordPosition="2351" endWordPosition="2354">be easily identified by a reasonable classification algorithm. Consequently, k-signatures provide a possible explanation for the high quality results presented in this paper. In the broader context, the presence (and contri5Our k-signature method can actually be useful for automatically identifying such users. We defer this to future work. Number of Tweets with at least one k−Signature Figure 2: Number of users with at least x training tweets that contain at least one k-signature (100 authors, 180 training tweets per author). bution) of k-signatures is in line with the hypothesis proposed by (Davidov et al., 2010a): while still using an informal and unstructured (grammatical) language, authors tend to use typical and unique structures in order to allow a short message to stand alone without a clear conversational context. 0 20 40 60 80 100 120 140 160 180 Number of Users 60 40 20 90 80 70 50 30 10 0 k = 2% k = 5% k = 10% k = 20% k = 50% 1883 User 20%-signature Examples 1 I’m listening to : I’m listening to: Sigur R?s ? Intro: http://www.last.fm/music/Sigur+R%C3%B3s http://bit.ly/3XJHyb I’m listening to: Tina Arena ? In Command: http://www.last.fm/music/Tina+Arena http://bit.ly/7q9E25 I’m listening to:</context>
<context position="24215" citStr="Davidov et al., 2010" startWordPosition="4041" endWordPosition="4044">ible pattern. Examples of flexible patterns include 1. “theHFW CW ofHFW theHFW” 10Omitting this treats word n-grams as flexible patterns. 1886 Accuracy (%) Flexible Pattern Features. Flexible patterns can serve as binary classification features; a tweet matches a given flexible pattern if it contains the flexible pattern sequence. For example, (1) is matched by (2). 2. “Go to the„FW houseCW of„FW the„FW rising sun” Partial Flexible Patterns. A flexible pattern may appear in a given tweet with additional words not originally found in the flexible pattern, and/or with only a subset of the HFWs (Davidov et al., 2010a). For example, (3) is a partial match of (1), since the word “great” is not part of the original flexible pattern. Similarly, (4) is another partial match of (1), since (a) the word “good” is not part of the original flexible pattern and (b) the second occurrence of the word “the” does not appear in (4) (missing word is marked by ). 3. “The„FW great..., kingCW of„FW the„FW ring” 4. “The„FW good..., kingCW of„FW Spain” We use such cases as features with lower weight, proportional to the number of found HFWs in the tweet (w = an X n found ). For example, (1) receives a expected weight of 1 (co</context>
<context position="33575" citStr="Davidov et al., 2010" startWordPosition="5578" endWordPosition="5581">g (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2008a), sentiment analysis (Davidov et al., 2010b) and detection of sarcasm (Tsur et al., 2010). 9 Conclusion The main goal of this paper is to measure to what extent authors of micro-messages can be identified. We have shown that authors of very short texts can be successfully identified in an array of authorship attribution settings reported for long documents. This is the first work on micro-messages to address some of these settings. We introduced the concept of k-signature. Using this concept, we proposed an interpretation of our results. Last, we presented the first authorship attribution system that uses flexible patterns, and demons</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010a. Semi-supervised recognition of sarcastic sentences in twitter and amazon. In Proc. of CoNLL, pages 107– 116, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proc. of Coling,</booktitle>
<pages>241--249</pages>
<location>Beijing, China.</location>
<contexts>
<context position="14158" citStr="Davidov et al., 2010" startWordPosition="2351" endWordPosition="2354">be easily identified by a reasonable classification algorithm. Consequently, k-signatures provide a possible explanation for the high quality results presented in this paper. In the broader context, the presence (and contri5Our k-signature method can actually be useful for automatically identifying such users. We defer this to future work. Number of Tweets with at least one k−Signature Figure 2: Number of users with at least x training tweets that contain at least one k-signature (100 authors, 180 training tweets per author). bution) of k-signatures is in line with the hypothesis proposed by (Davidov et al., 2010a): while still using an informal and unstructured (grammatical) language, authors tend to use typical and unique structures in order to allow a short message to stand alone without a clear conversational context. 0 20 40 60 80 100 120 140 160 180 Number of Users 60 40 20 90 80 70 50 30 10 0 k = 2% k = 5% k = 10% k = 20% k = 50% 1883 User 20%-signature Examples 1 I’m listening to : I’m listening to: Sigur R?s ? Intro: http://www.last.fm/music/Sigur+R%C3%B3s http://bit.ly/3XJHyb I’m listening to: Tina Arena ? In Command: http://www.last.fm/music/Tina+Arena http://bit.ly/7q9E25 I’m listening to:</context>
<context position="24215" citStr="Davidov et al., 2010" startWordPosition="4041" endWordPosition="4044">ible pattern. Examples of flexible patterns include 1. “theHFW CW ofHFW theHFW” 10Omitting this treats word n-grams as flexible patterns. 1886 Accuracy (%) Flexible Pattern Features. Flexible patterns can serve as binary classification features; a tweet matches a given flexible pattern if it contains the flexible pattern sequence. For example, (1) is matched by (2). 2. “Go to the„FW houseCW of„FW the„FW rising sun” Partial Flexible Patterns. A flexible pattern may appear in a given tweet with additional words not originally found in the flexible pattern, and/or with only a subset of the HFWs (Davidov et al., 2010a). For example, (3) is a partial match of (1), since the word “great” is not part of the original flexible pattern. Similarly, (4) is another partial match of (1), since (a) the word “good” is not part of the original flexible pattern and (b) the second occurrence of the word “the” does not appear in (4) (missing word is marked by ). 3. “The„FW great..., kingCW of„FW the„FW ring” 4. “The„FW good..., kingCW of„FW Spain” We use such cases as features with lower weight, proportional to the number of found HFWs in the tweet (w = an X n found ). For example, (1) receives a expected weight of 1 (co</context>
<context position="33575" citStr="Davidov et al., 2010" startWordPosition="5578" endWordPosition="5581">g (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2008a), sentiment analysis (Davidov et al., 2010b) and detection of sarcasm (Tsur et al., 2010). 9 Conclusion The main goal of this paper is to measure to what extent authors of micro-messages can be identified. We have shown that authors of very short texts can be successfully identified in an array of authorship attribution settings reported for long documents. This is the first work on micro-messages to address some of these settings. We introduced the concept of k-signature. Using this concept, we proposed an interpretation of our results. Last, we presented the first authorship attribution system that uses flexible patterns, and demons</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010b. Enhanced sentiment learning using twitter hashtags and smileys. In Proc. of Coling, pages 241–249, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier De Vel</author>
<author>Alison Anderson</author>
<author>Malcolm Corney</author>
<author>George Mohay</author>
</authors>
<title>Mining e-mail content for author identification forensics.</title>
<date>2001</date>
<journal>ACM Sigmod Record,</journal>
<volume>30</volume>
<issue>4</issue>
<marker>De Vel, Anderson, Corney, Mohay, 2001</marker>
<rawString>Olivier De Vel, Alison Anderson, Malcolm Corney, and George Mohay. 2001. Mining e-mail content for author identification forensics. ACM Sigmod Record, 30(4):55–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Diederich</author>
<author>J¨org Kindermann</author>
<author>Edda Leopold</author>
<author>Gerhard Paass</author>
</authors>
<title>Authorship attribution with support vector machines. Applied intelligence,</title>
<date>2003</date>
<pages>19--1</pages>
<contexts>
<context position="29719" citStr="Diederich et al., 2003" startWordPosition="4969" endWordPosition="4972">nd machine learning algorithm. Examples of features include HFWs (Mosteller and Wallace, 1964; Argamon et al., 2007), character n-gram (Kjell, 1994; Hoorn et al., 1999; Stamatatos, 2008), word n-grams (Peng et al., 2004), part-of-speech n-grams (Koppel and Schler, 2003; Koppel et al., 2005) and vocabulary richness (Abbasi and Chen, 2005). The various machine learning algorithms used include naive Bayes (Mosteller and Wallace, 1964; Kjell, 1994), neural networks (Matthews and Merriam, 1993; Kjell, 1994), K-nearest neighbors (Kjell et al., 1995; Hoorn et al., 1999) and SVM (De Vel et al., 2001; Diederich et al., 2003; Koppel and Schler, 2003). Traditionally, authorship attribution systems have mainly been evaluated against long texts such as theater plays (Mendenhall, 1887), essays (Yule, 1939; Mosteller and Wallace, 1964), biblical books (Mealand, 1995; Koppel et al., 2011a) and book chapters (Argamon et al., 2007; Koppel et al., 2007). In recent year, many works focused on web data such as emails (De Vel et al., 2001; Koppel and Schler, 2003; Abbasi and Chen, 2008), web forum messages (Abbasi and Chen, 2005; Solorio et al., 2011), blogs (Koppel et al., 2006; Koppel et al., 2011b) and chat messages (Abba</context>
</contexts>
<marker>Diederich, Kindermann, Leopold, Paass, 2003</marker>
<rawString>Joachim Diederich, J¨org Kindermann, Edda Leopold, and Gerhard Paass. 2003. Authorship attribution with support vector machines. Applied intelligence, 19(1-2):109–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
</authors>
<title>Introducing and evaluating ukwac, a very large web-derived corpus of english.</title>
<date>2008</date>
<booktitle>In Proc. of the 4th Web as Corpus Workshop, WAC-4.</booktitle>
<contexts>
<context position="9024" citStr="Ferraresi et al., 2008" startWordPosition="1455" endWordPosition="1458">ger texts into shorter chunks (Sanderson and Guenter, 2006). We take a different approach and experiment with micro-messages (specifically, tweets). Tweets have several properties making them an ideal testbed for authorship attribution of short texts. First, tweets are posted as single units and do not necessarily refer to each other. As a result, they tend to be self contained. Second, tweets have more standardized length distribution compared to other types of web data. We compared the mean and standard deviation of sentence length in our Twitter dataset and in a corpus of English web data (Ferraresi et al., 2008).3 We found that (a) tweets are shorter than standard web data (14.2 words compared to 20.9), and (b) the standard deviation of the length of tweets is much smaller (6.4 vs. 21.4). Pre-Processing. We use a Twitter corpus that includes approximately 5 x 108 tweets.4 All nonEnglish tweets and tweets that contain fewer than 3 words are removed from the dataset. We also remove tweets marked as retweets (using the RT sign, a standard Twitter symbol to indicate that this tweet was written by a different user). As some users retweet without using the RT sign, we also remove tweets that are an exact c</context>
</contexts>
<marker>Ferraresi, Zanchetta, Baroni, Bernardini, 2008</marker>
<rawString>Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evaluating ukwac, a very large web-derived corpus of english. In Proc. of the 4th Web as Corpus Workshop, WAC-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgia Frantzeskou</author>
<author>Efstathios Stamatatos</author>
<author>Stefanos Gritzalis</author>
<author>Carole E Chaski</author>
</authors>
<title>Identifying authorship by byte-level n-grams: The source code author profile (scap) method.</title>
<date>2007</date>
<journal>Int Journal of Digital Evidence,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="30900" citStr="Frantzeskou et al., 2007" startWordPosition="5158" endWordPosition="5161">oppel et al., 2011b) and chat messages (Abbasi and Chen, 2008). Some works focused on SMS messages (Mohan et al., 2010; Ishihara, 2011). Authorship Attribution on Twitter. The performance of authorship attribution systems on short texts is affected by several factors (Stamatatos, 2009). These factors include the number of candidate authors, the training set size and the size of the test document. Very few authorship attribution works experimented with Twitter. Unlike our work, all used a single group of authors (group sizes varied between 3-50). Layton et al. (2010) used the SCAP methodology (Frantzeskou et al., 2007) with character ngram features. They experimented with 50 authors and compared different numbers of tweets per author (values between 20-200). Surprisingly, they showed that their system does not improve when given more training tweets. In our work, we noticed a different trend, and showed that more data can be extremely valuable for authorship attribution systems on micro-messages (see Section 6). Silva et al. (2011) trained an SVM classifier with various features (e.g., punctuation and vocabulary features) on a small dataset of three authors only, with varying training set size. Although the</context>
</contexts>
<marker>Frantzeskou, Stamatatos, Gritzalis, Chaski, 2007</marker>
<rawString>Georgia Frantzeskou, Efstathios Stamatatos, Stefanos Gritzalis, and Carole E Chaski. 2007. Identifying authorship by byte-level n-grams: The source code author profile (scap) method. Int Journal of Digital Evidence, 6(1):1–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. of Coling –</booktitle>
<volume>2</volume>
<pages>539--545</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="32682" citStr="Hearst, 1992" startWordPosition="5446" endWordPosition="5447">aracter n-gram and word n-grams. They experimented with 10 authors of Greek text, and also joined several tweets into a single document. Joining several tweets into a longer document is appealing since it can lead to substantial improvement of the classification results, as demonstrated by the works above. However, this approach requires the test data to contain several tweets that are known a-priori to be written by the same author. This assumption is not always realistic. In our paper, we intentionally focus on a single tweet as document size. Flexible Patterns. Patterns were introduced by (Hearst, 1992), who used hand crafted patterns to discover hyponyms. Hard coded patterns were used for many tasks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used a</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. of Coling – Volume 2, pages 539–545, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan F Hoorn</author>
<author>Stefan L Frank</author>
<author>Wojtek Kowalczyk</author>
<author>Floor van der Ham</author>
</authors>
<title>Neural network identification of poets using letter sequences.</title>
<date>1999</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>14--3</pages>
<marker>Hoorn, Frank, Kowalczyk, van der Ham, 1999</marker>
<rawString>Johan F Hoorn, Stefan L Frank, Wojtek Kowalczyk, and Floor van der Ham. 1999. Neural network identification of poets using letter sequences. Literary and Linguistic Computing, 14(3):311–338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shunichi Ishihara</author>
</authors>
<title>A forensic authorship classification in sms messages: A likelihood ratio based approach using n-gram.</title>
<date>2011</date>
<booktitle>In Proc. of the Australasian Language Technology Association Workshop</booktitle>
<pages>47--56</pages>
<location>Canberra, Australia.</location>
<contexts>
<context position="30410" citStr="Ishihara, 2011" startWordPosition="5082" endWordPosition="5083">ve mainly been evaluated against long texts such as theater plays (Mendenhall, 1887), essays (Yule, 1939; Mosteller and Wallace, 1964), biblical books (Mealand, 1995; Koppel et al., 2011a) and book chapters (Argamon et al., 2007; Koppel et al., 2007). In recent year, many works focused on web data such as emails (De Vel et al., 2001; Koppel and Schler, 2003; Abbasi and Chen, 2008), web forum messages (Abbasi and Chen, 2005; Solorio et al., 2011), blogs (Koppel et al., 2006; Koppel et al., 2011b) and chat messages (Abbasi and Chen, 2008). Some works focused on SMS messages (Mohan et al., 2010; Ishihara, 2011). Authorship Attribution on Twitter. The performance of authorship attribution systems on short texts is affected by several factors (Stamatatos, 2009). These factors include the number of candidate authors, the training set size and the size of the test document. Very few authorship attribution works experimented with Twitter. Unlike our work, all used a single group of authors (group sizes varied between 3-50). Layton et al. (2010) used the SCAP methodology (Frantzeskou et al., 2007) with character ngram features. They experimented with 50 authors and compared different numbers of tweets per</context>
</contexts>
<marker>Ishihara, 2011</marker>
<rawString>Shunichi Ishihara. 2011. A forensic authorship classification in sms messages: A likelihood ratio based approach using n-gram. In Proc. of the Australasian Language Technology Association Workshop 2011, pages 47–56, Canberra, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Juola</author>
</authors>
<title>Large-scale experiments in authorship attribution.</title>
<date>2012</date>
<journal>English Studies,</journal>
<volume>93</volume>
<issue>3</issue>
<contexts>
<context position="28608" citStr="Juola, 2012" startWordPosition="4799" endWordPosition="4800">of words with (6-8), no word n-gram feature is able to identify the author’s style in (5). However, this style can be successfully identified using the flexible pattern (9), shared by (5-8). 9. the„.W CW I„.W This demonstrates the added value flexible pattern features have over word n-gram features. 8 Related Work Authorship attribution dates back to the end of 19th century, when (Mendenhall, 1887) applied sentence length and word length features to plays of Shakespeare. Ever since, many methods have been developed for this task. For recent surveys, see (Koppel et al., 2009; Stamatatos, 2009; Juola, 2012). Authorship attribution methods can be generally divided into two categories (Stamatatos, 2009). In similarity-based methods, an anonymous text is attributed to some author whose writing style is most similar (by some distance metric). In machine learning methods, which we follow in this paper, anonymous texts are classified, using machine learning algorithms, into different categories (in this case, different authors). Machine learning papers differ from each other by the features and machine learning algorithm. Examples of features include HFWs (Mosteller and Wallace, 1964; Argamon et al., </context>
</contexts>
<marker>Juola, 2012</marker>
<rawString>Patrick Juola. 2012. Large-scale experiments in authorship attribution. English Studies, 93(3):275–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Kjell</author>
<author>W Addison Woods</author>
<author>Ophir Frieder</author>
</authors>
<title>Information retrieval using letter tuples with neural network and nearest neighbor classifiers.</title>
<date>1995</date>
<booktitle>In IEEE International Conference on Systems, Man and Cybernetics,</booktitle>
<volume>2</volume>
<pages>1222--1226</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="29645" citStr="Kjell et al., 1995" startWordPosition="4954" endWordPosition="4957">ors). Machine learning papers differ from each other by the features and machine learning algorithm. Examples of features include HFWs (Mosteller and Wallace, 1964; Argamon et al., 2007), character n-gram (Kjell, 1994; Hoorn et al., 1999; Stamatatos, 2008), word n-grams (Peng et al., 2004), part-of-speech n-grams (Koppel and Schler, 2003; Koppel et al., 2005) and vocabulary richness (Abbasi and Chen, 2005). The various machine learning algorithms used include naive Bayes (Mosteller and Wallace, 1964; Kjell, 1994), neural networks (Matthews and Merriam, 1993; Kjell, 1994), K-nearest neighbors (Kjell et al., 1995; Hoorn et al., 1999) and SVM (De Vel et al., 2001; Diederich et al., 2003; Koppel and Schler, 2003). Traditionally, authorship attribution systems have mainly been evaluated against long texts such as theater plays (Mendenhall, 1887), essays (Yule, 1939; Mosteller and Wallace, 1964), biblical books (Mealand, 1995; Koppel et al., 2011a) and book chapters (Argamon et al., 2007; Koppel et al., 2007). In recent year, many works focused on web data such as emails (De Vel et al., 2001; Koppel and Schler, 2003; Abbasi and Chen, 2008), web forum messages (Abbasi and Chen, 2005; Solorio et al., 2011),</context>
</contexts>
<marker>Kjell, Woods, Frieder, 1995</marker>
<rawString>Bradley Kjell, W Addison Woods, and Ophir Frieder. 1995. Information retrieval using letter tuples with neural network and nearest neighbor classifiers. In IEEE International Conference on Systems, Man and Cybernetics, volume 2, pages 1222–1226. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Kjell</author>
</authors>
<title>Authorship determination using letter pair frequency features with neural network classifiers.</title>
<date>1994</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<volume>9</volume>
<issue>2</issue>
<pages>124</pages>
<contexts>
<context position="29244" citStr="Kjell, 1994" startWordPosition="4896" endWordPosition="4897">methods can be generally divided into two categories (Stamatatos, 2009). In similarity-based methods, an anonymous text is attributed to some author whose writing style is most similar (by some distance metric). In machine learning methods, which we follow in this paper, anonymous texts are classified, using machine learning algorithms, into different categories (in this case, different authors). Machine learning papers differ from each other by the features and machine learning algorithm. Examples of features include HFWs (Mosteller and Wallace, 1964; Argamon et al., 2007), character n-gram (Kjell, 1994; Hoorn et al., 1999; Stamatatos, 2008), word n-grams (Peng et al., 2004), part-of-speech n-grams (Koppel and Schler, 2003; Koppel et al., 2005) and vocabulary richness (Abbasi and Chen, 2005). The various machine learning algorithms used include naive Bayes (Mosteller and Wallace, 1964; Kjell, 1994), neural networks (Matthews and Merriam, 1993; Kjell, 1994), K-nearest neighbors (Kjell et al., 1995; Hoorn et al., 1999) and SVM (De Vel et al., 2001; Diederich et al., 2003; Koppel and Schler, 2003). Traditionally, authorship attribution systems have mainly been evaluated against long texts such </context>
</contexts>
<marker>Kjell, 1994</marker>
<rawString>Bradley Kjell. 1994. Authorship determination using letter pair frequency features with neural network classifiers. Literary and Linguistic Computing, 9(2):119– 124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
</authors>
<title>Exploiting stylistic idiosyncrasies for authorship attribution.</title>
<date>2003</date>
<booktitle>In Proc. of IJCAI’03 Workshop on Computational Approaches to Style Analysis and Synthesis,</booktitle>
<volume>69</volume>
<pages>72</pages>
<contexts>
<context position="29366" citStr="Koppel and Schler, 2003" startWordPosition="4912" endWordPosition="4915">mous text is attributed to some author whose writing style is most similar (by some distance metric). In machine learning methods, which we follow in this paper, anonymous texts are classified, using machine learning algorithms, into different categories (in this case, different authors). Machine learning papers differ from each other by the features and machine learning algorithm. Examples of features include HFWs (Mosteller and Wallace, 1964; Argamon et al., 2007), character n-gram (Kjell, 1994; Hoorn et al., 1999; Stamatatos, 2008), word n-grams (Peng et al., 2004), part-of-speech n-grams (Koppel and Schler, 2003; Koppel et al., 2005) and vocabulary richness (Abbasi and Chen, 2005). The various machine learning algorithms used include naive Bayes (Mosteller and Wallace, 1964; Kjell, 1994), neural networks (Matthews and Merriam, 1993; Kjell, 1994), K-nearest neighbors (Kjell et al., 1995; Hoorn et al., 1999) and SVM (De Vel et al., 2001; Diederich et al., 2003; Koppel and Schler, 2003). Traditionally, authorship attribution systems have mainly been evaluated against long texts such as theater plays (Mendenhall, 1887), essays (Yule, 1939; Mosteller and Wallace, 1964), biblical books (Mealand, 1995; Kopp</context>
</contexts>
<marker>Koppel, Schler, 2003</marker>
<rawString>Moshe Koppel and Jonathan Schler. 2003. Exploiting stylistic idiosyncrasies for authorship attribution. In Proc. of IJCAI’03 Workshop on Computational Approaches to Style Analysis and Synthesis, volume 69, page 72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
<author>Kfir Zigdon</author>
</authors>
<title>Determining an author’s native language by mining a text for errors.</title>
<date>2005</date>
<booktitle>In Proc. of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, KDD ’05,</booktitle>
<pages>624--628</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="29388" citStr="Koppel et al., 2005" startWordPosition="4916" endWordPosition="4919">o some author whose writing style is most similar (by some distance metric). In machine learning methods, which we follow in this paper, anonymous texts are classified, using machine learning algorithms, into different categories (in this case, different authors). Machine learning papers differ from each other by the features and machine learning algorithm. Examples of features include HFWs (Mosteller and Wallace, 1964; Argamon et al., 2007), character n-gram (Kjell, 1994; Hoorn et al., 1999; Stamatatos, 2008), word n-grams (Peng et al., 2004), part-of-speech n-grams (Koppel and Schler, 2003; Koppel et al., 2005) and vocabulary richness (Abbasi and Chen, 2005). The various machine learning algorithms used include naive Bayes (Mosteller and Wallace, 1964; Kjell, 1994), neural networks (Matthews and Merriam, 1993; Kjell, 1994), K-nearest neighbors (Kjell et al., 1995; Hoorn et al., 1999) and SVM (De Vel et al., 2001; Diederich et al., 2003; Koppel and Schler, 2003). Traditionally, authorship attribution systems have mainly been evaluated against long texts such as theater plays (Mendenhall, 1887), essays (Yule, 1939; Mosteller and Wallace, 1964), biblical books (Mealand, 1995; Koppel et al., 2011a) and </context>
</contexts>
<marker>Koppel, Schler, Zigdon, 2005</marker>
<rawString>Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005. Determining an author’s native language by mining a text for errors. In Proc. of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, KDD ’05, pages 624–628, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
<author>Shlomo Argamon</author>
<author>Eran Messeri</author>
</authors>
<title>Authorship attribution with thousands of candidate authors.</title>
<date>2006</date>
<booktitle>In SIGIR,</booktitle>
<pages>659--660</pages>
<contexts>
<context position="18837" citStr="Koppel et al., 2006" startWordPosition="3147" endWordPosition="3150">authors with various training set sizes. The values are averaged over 10 groups. The random baseline is 2%. Recall-Precision Tradeoff. Another aspect of our research question is the level of certainty our system has when suggesting an author for a given tweet. In cases of uncertainty, many real life applications would prefer not to get any response instead of getting a response with low certainty. Moreover, in real life applications we are often not even sure that the real author is part of our training set. Consequently, we allow our system to respond “don’t know” in cases of low confidence (Koppel et al., 2006; Koppel et al., 2011b). This allows our system to obtain higher precision, at the expense of lower recall. To implement this feature, we use SVM’s probability estimates, as implemented in libsvm. These estimates give a score to each potential author. These scores reflect the probability that this author is the correct author, as decided by the prediction model. The selected author is always the one with the highest probability estimate. As selection criterion, we use a set of increasingly larger thresholds (0.05-0.9) for the probability of the selected author. This means that we do not select</context>
<context position="30272" citStr="Koppel et al., 2006" startWordPosition="5057" endWordPosition="5060">et al., 1999) and SVM (De Vel et al., 2001; Diederich et al., 2003; Koppel and Schler, 2003). Traditionally, authorship attribution systems have mainly been evaluated against long texts such as theater plays (Mendenhall, 1887), essays (Yule, 1939; Mosteller and Wallace, 1964), biblical books (Mealand, 1995; Koppel et al., 2011a) and book chapters (Argamon et al., 2007; Koppel et al., 2007). In recent year, many works focused on web data such as emails (De Vel et al., 2001; Koppel and Schler, 2003; Abbasi and Chen, 2008), web forum messages (Abbasi and Chen, 2005; Solorio et al., 2011), blogs (Koppel et al., 2006; Koppel et al., 2011b) and chat messages (Abbasi and Chen, 2008). Some works focused on SMS messages (Mohan et al., 2010; Ishihara, 2011). Authorship Attribution on Twitter. The performance of authorship attribution systems on short texts is affected by several factors (Stamatatos, 2009). These factors include the number of candidate authors, the training set size and the size of the test document. Very few authorship attribution works experimented with Twitter. Unlike our work, all used a single group of authors (group sizes varied between 3-50). Layton et al. (2010) used the SCAP methodolog</context>
</contexts>
<marker>Koppel, Schler, Argamon, Messeri, 2006</marker>
<rawString>Moshe Koppel, Jonathan Schler, Shlomo Argamon, and Eran Messeri. 2006. Authorship attribution with thousands of candidate authors. In SIGIR, pages 659–660.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
<author>Elisheva BonchekDokow</author>
</authors>
<title>Measuring differentiability: Unmasking pseudonymous authors.</title>
<date>2007</date>
<journal>JMLR,</journal>
<pages>8--1261</pages>
<contexts>
<context position="30045" citStr="Koppel et al., 2007" startWordPosition="5016" endWordPosition="5019">Chen, 2005). The various machine learning algorithms used include naive Bayes (Mosteller and Wallace, 1964; Kjell, 1994), neural networks (Matthews and Merriam, 1993; Kjell, 1994), K-nearest neighbors (Kjell et al., 1995; Hoorn et al., 1999) and SVM (De Vel et al., 2001; Diederich et al., 2003; Koppel and Schler, 2003). Traditionally, authorship attribution systems have mainly been evaluated against long texts such as theater plays (Mendenhall, 1887), essays (Yule, 1939; Mosteller and Wallace, 1964), biblical books (Mealand, 1995; Koppel et al., 2011a) and book chapters (Argamon et al., 2007; Koppel et al., 2007). In recent year, many works focused on web data such as emails (De Vel et al., 2001; Koppel and Schler, 2003; Abbasi and Chen, 2008), web forum messages (Abbasi and Chen, 2005; Solorio et al., 2011), blogs (Koppel et al., 2006; Koppel et al., 2011b) and chat messages (Abbasi and Chen, 2008). Some works focused on SMS messages (Mohan et al., 2010; Ishihara, 2011). Authorship Attribution on Twitter. The performance of authorship attribution systems on short texts is affected by several factors (Stamatatos, 2009). These factors include the number of candidate authors, the training set size and t</context>
</contexts>
<marker>Koppel, Schler, BonchekDokow, 2007</marker>
<rawString>Moshe Koppel, Jonathan Schler, and Elisheva BonchekDokow. 2007. Measuring differentiability: Unmasking pseudonymous authors. JMLR, 8:1261–1276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
<author>Shlomo Argamon</author>
</authors>
<title>Computational methods in authorship attribution.</title>
<date>2009</date>
<journal>J. Am. Soc. Inf. Sci. Technol.,</journal>
<volume>60</volume>
<issue>1</issue>
<contexts>
<context position="4681" citStr="Koppel et al., 2009" startWordPosition="748" endWordPosition="751"> to a random baseline of only 0.1%). Using a dataset of 50 authors with as few as 50 training tweets per author, we obtain 50.7% accuracy. Using a dataset of 50 authors with 1,000 training tweets per author, our results reach as high as 71.2% in the standard classification setting, and exceed 91% accuracy with 60% recall in the don’t know setting. We also apply a new set of features, never previously used for this task – flexible patterns. Flexible patterns essentially capture the context in which function words are used. The effectiveness of function words as authorship attribution features (Koppel et al., 2009) suggests using flexible pattern features. The fact that flexible patterns are learned from plain text in a fully unsupervised manner makes them domain and language independent. We demonstrate that using flexible patterns gives significant improvement over our baseline system. Furthermore, using flexible patterns, our system obtains a 6.1% improvement over current state-of-the-art results in authorship attribution on Twitter. To summarize, the contribution of this paper is threefold. • We provide the most extensive research to date on authorship attribution of micro-messages, and show that aut</context>
<context position="28576" citStr="Koppel et al., 2009" startWordPosition="4793" endWordPosition="4796">raining set. As (5) shares no sequence of words with (6-8), no word n-gram feature is able to identify the author’s style in (5). However, this style can be successfully identified using the flexible pattern (9), shared by (5-8). 9. the„.W CW I„.W This demonstrates the added value flexible pattern features have over word n-gram features. 8 Related Work Authorship attribution dates back to the end of 19th century, when (Mendenhall, 1887) applied sentence length and word length features to plays of Shakespeare. Ever since, many methods have been developed for this task. For recent surveys, see (Koppel et al., 2009; Stamatatos, 2009; Juola, 2012). Authorship attribution methods can be generally divided into two categories (Stamatatos, 2009). In similarity-based methods, an anonymous text is attributed to some author whose writing style is most similar (by some distance metric). In machine learning methods, which we follow in this paper, anonymous texts are classified, using machine learning algorithms, into different categories (in this case, different authors). Machine learning papers differ from each other by the features and machine learning algorithm. Examples of features include HFWs (Mosteller and</context>
</contexts>
<marker>Koppel, Schler, Argamon, 2009</marker>
<rawString>Moshe Koppel, Jonathan Schler, and Shlomo Argamon. 2009. Computational methods in authorship attribution. J. Am. Soc. Inf. Sci. Technol., 60(1):9–26.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Moshe Koppel</author>
</authors>
<title>Navot Akiva, Idan Dershowitz, and Nachum Dershowitz. 2011a. Unsupervised decomposition of a document into authorial components.</title>
<booktitle>In Proc. of ACL-HLT,</booktitle>
<pages>1356--1364</pages>
<location>Portland, Oregon, USA.</location>
<marker>Koppel, </marker>
<rawString>Moshe Koppel, Navot Akiva, Idan Dershowitz, and Nachum Dershowitz. 2011a. Unsupervised decomposition of a document into authorial components. In Proc. of ACL-HLT, pages 1356–1364, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
<author>Shlomo Argamon</author>
</authors>
<booktitle>2011b. Authorship attribution in the wild. Language Resources and Evaluation,</booktitle>
<pages>45--1</pages>
<marker>Koppel, Schler, Argamon, </marker>
<rawString>Moshe Koppel, Jonathan Schler, and Shlomo Argamon. 2011b. Authorship attribution in the wild. Language Resources and Evaluation, 45(1):83–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard Hovy</author>
</authors>
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-HLT,</booktitle>
<pages>1048--1056</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="32980" citStr="Kozareva et al., 2008" startWordPosition="5489" endWordPosition="5492">y the works above. However, this approach requires the test data to contain several tweets that are known a-priori to be written by the same author. This assumption is not always realistic. In our paper, we intentionally focus on a single tweet as document size. Flexible Patterns. Patterns were introduced by (Hearst, 1992), who used hand crafted patterns to discover hyponyms. Hard coded patterns were used for many tasks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2008a), sentiment analysis (Davidov et al., 2010b) an</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage graphs. In Proc. of ACL-HLT, pages 1048–1056, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Layton</author>
<author>Paul Watters</author>
<author>Richard Dazeley</author>
</authors>
<title>Authorship attribution for twitter in 140 characters or less.</title>
<date>2010</date>
<booktitle>In Proc. of the 2010 Second Cybercrime and Trustworthy Computing Workshop, CTC ’10,</booktitle>
<volume>1</volume>
<pages>pages</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="6862" citStr="Layton et al., 2010" startWordPosition="1091" endWordPosition="1094"> authorship attribution on micro-messages since they are relatively tolerant to typos and non-standard use of punctuation (Stamatatos, 2009). These are common in the nonformal style generally applied in social media services. Consider the example of misspelling “Britney” as “Brittney”. The misspelled name shares the 4-grams “Brit” and “tney” with the correct name. As a result, these features provide information about the author’s style (or at least her topic of interest), which is not available through lexical features. Following standard practice, we use 4-grams (Sanderson and Guenter, 2006; Layton et al., 2010; Koppel et al., 2011b). White spaces are considered characters (i.e., a character n-gram may be composed of letters from two different words). A single white-space is appended to the beginning and the end of each tweet. For efficiency, we consider only character n-gram features that appear at least ting times in the training set of at least one author (see Section 5). Word n-grams. We hypothesize that word n-gram features would be useful for authorship attribution on micro-messages. We assume that under a strict length restriction, many authors would prefer using short, repeating phrases (wor</context>
<context position="9881" citStr="Layton et al., 2010" startWordPosition="1604" endWordPosition="1607">proximately 5 x 108 tweets.4 All nonEnglish tweets and tweets that contain fewer than 3 words are removed from the dataset. We also remove tweets marked as retweets (using the RT sign, a standard Twitter symbol to indicate that this tweet was written by a different user). As some users retweet without using the RT sign, we also remove tweets that are an exact copy of an existing tweet posted in the previous seven days. Apart from plain text, some tweets contain references to other Twitter users (in the format of @&lt;user&gt;). Since using reference information makes this task substantially easier (Layton et al., 2010), we replace each user reference with the special meta tag REF. For sparsity reasons, we also replace web addresses with the meta tag URL, num2In practice, 0.05 or 0.1 are selected in almost all cases. 3http://wacky.sslmit.unibo.it 4These comprise —15% of all public tweets created from May 2009 to March 2010. Number of k−signatures per user Figure 1: Number of users with at least x k-signatures (100 authors, 180 training tweets per author). bers with the meta tag NUM, time of day with the meta tag TIME and dates with the meta tag DATE. 4 k-Signatures In this section, we show that many authors </context>
<context position="26194" citStr="Layton et al., 2010" startWordPosition="4388" endWordPosition="4391">, 1,000) are (2, 3, 7, 7, 8), respectively. Results. Figure 7 shows our results. Results demonstrate that flexible pattern features have an added value over both character n-grams alone (averaged 2.9% improvement) and over character ngrams and word n-grams together (averaged 1.5% 0 100 200 300 400 500 600 700 800 900 1000 Training Set Size Figure 7: Authorship attribution accuracy for 50 authors with various training set sizes and various feature sets. The values are averaged over 10 groups. The random baseline is 2%. Comparison to previous work: SCAP – SCAP algorithm results, as reported by (Layton et al., 2010), Naive Bayes – Naive Bayes algorithm results, as reported by (Boutwell, 2011). improvement). We perform t-tests on each of our training set sizes to check whether the latter improvement is significant. Results demonstrate that it is highly significant in all settings, with p-values smaller than values between 10−3 (for 50 tweets per author) and 10−8 (1,000 tweets per author). Comparison to Previous Works. Figure 7 also shows results for the only two works that experimented in some of the settings we experimented in: Layton et al. (2010) and Boutwell (2011) (see Section 8). Our system substant</context>
<context position="27532" citStr="Layton et al., 2010" startWordPosition="4618" endWordPosition="4621"> (SVM and not SCAP/naive Bayes) and our set of features (character n-grams + word n-grams + flexible patterns compared to character n-grams only). In order to rule out the possibility that these margins stem from using different datasets, we tested our system on the dataset used in (Layton et al., 2010). Our system obtains even higher results on this dataset than on our datasets (61.6%, a total im65 60 45 40 75 70 55 50 35 Char. N−grams, Word N−grams &amp; Flex. Patt. Feats. Char. N−grams + Flex. Patt. Feats. Char. N−grams + Word N−grams Char. N−grams SCAP Naive Bayes 1887 provement of 6.1% over (Layton et al., 2010)). Discussion. To illustrate the additional contribution of flexible patterns over word n-grams, consider the following tweets, written by the same author. 5. “... the„.W wayCW I„.W treatedCW her„.W” 6. “...half of the„.W thingsCW I„.W have seen” 7. “... the„.W friendsCW I„.W have had for years” 8. “... in the„.W neighborhoodCW I„.W grew up in” Consider a case where (5) is part of the test set, while (6-8) appear in the training set. As (5) shares no sequence of words with (6-8), no word n-gram feature is able to identify the author’s style in (5). However, this style can be successfully ident</context>
<context position="30847" citStr="Layton et al. (2010)" startWordPosition="5149" endWordPosition="5152">rio et al., 2011), blogs (Koppel et al., 2006; Koppel et al., 2011b) and chat messages (Abbasi and Chen, 2008). Some works focused on SMS messages (Mohan et al., 2010; Ishihara, 2011). Authorship Attribution on Twitter. The performance of authorship attribution systems on short texts is affected by several factors (Stamatatos, 2009). These factors include the number of candidate authors, the training set size and the size of the test document. Very few authorship attribution works experimented with Twitter. Unlike our work, all used a single group of authors (group sizes varied between 3-50). Layton et al. (2010) used the SCAP methodology (Frantzeskou et al., 2007) with character ngram features. They experimented with 50 authors and compared different numbers of tweets per author (values between 20-200). Surprisingly, they showed that their system does not improve when given more training tweets. In our work, we noticed a different trend, and showed that more data can be extremely valuable for authorship attribution systems on micro-messages (see Section 6). Silva et al. (2011) trained an SVM classifier with various features (e.g., punctuation and vocabulary features) on a small dataset of three autho</context>
</contexts>
<marker>Layton, Watters, Dazeley, 2010</marker>
<rawString>Robert Layton, Paul Watters, and Richard Dazeley. 2010. Authorship attribution for twitter in 140 characters or less. In Proc. of the 2010 Second Cybercrime and Trustworthy Computing Workshop, CTC ’10, pages 1– 8, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert AJ Matthews</author>
<author>Thomas VN Merriam</author>
</authors>
<title>Neural computation in stylometry i: An application to the works of shakespeare and fletcher.</title>
<date>1993</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="29590" citStr="Matthews and Merriam, 1993" startWordPosition="4945" endWordPosition="4949">rithms, into different categories (in this case, different authors). Machine learning papers differ from each other by the features and machine learning algorithm. Examples of features include HFWs (Mosteller and Wallace, 1964; Argamon et al., 2007), character n-gram (Kjell, 1994; Hoorn et al., 1999; Stamatatos, 2008), word n-grams (Peng et al., 2004), part-of-speech n-grams (Koppel and Schler, 2003; Koppel et al., 2005) and vocabulary richness (Abbasi and Chen, 2005). The various machine learning algorithms used include naive Bayes (Mosteller and Wallace, 1964; Kjell, 1994), neural networks (Matthews and Merriam, 1993; Kjell, 1994), K-nearest neighbors (Kjell et al., 1995; Hoorn et al., 1999) and SVM (De Vel et al., 2001; Diederich et al., 2003; Koppel and Schler, 2003). Traditionally, authorship attribution systems have mainly been evaluated against long texts such as theater plays (Mendenhall, 1887), essays (Yule, 1939; Mosteller and Wallace, 1964), biblical books (Mealand, 1995; Koppel et al., 2011a) and book chapters (Argamon et al., 2007; Koppel et al., 2007). In recent year, many works focused on web data such as emails (De Vel et al., 2001; Koppel and Schler, 2003; Abbasi and Chen, 2008), web forum </context>
</contexts>
<marker>Matthews, Merriam, 1993</marker>
<rawString>Robert AJ Matthews and Thomas VN Merriam. 1993. Neural computation in stylometry i: An application to the works of shakespeare and fletcher. Literary and Linguistic Computing, 8(4):203–209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DL Mealand</author>
</authors>
<title>Correspondence analysis of luke. Literary and linguistic computing,</title>
<date>1995</date>
<pages>10--3</pages>
<contexts>
<context position="29960" citStr="Mealand, 1995" startWordPosition="5003" endWordPosition="5004">el and Schler, 2003; Koppel et al., 2005) and vocabulary richness (Abbasi and Chen, 2005). The various machine learning algorithms used include naive Bayes (Mosteller and Wallace, 1964; Kjell, 1994), neural networks (Matthews and Merriam, 1993; Kjell, 1994), K-nearest neighbors (Kjell et al., 1995; Hoorn et al., 1999) and SVM (De Vel et al., 2001; Diederich et al., 2003; Koppel and Schler, 2003). Traditionally, authorship attribution systems have mainly been evaluated against long texts such as theater plays (Mendenhall, 1887), essays (Yule, 1939; Mosteller and Wallace, 1964), biblical books (Mealand, 1995; Koppel et al., 2011a) and book chapters (Argamon et al., 2007; Koppel et al., 2007). In recent year, many works focused on web data such as emails (De Vel et al., 2001; Koppel and Schler, 2003; Abbasi and Chen, 2008), web forum messages (Abbasi and Chen, 2005; Solorio et al., 2011), blogs (Koppel et al., 2006; Koppel et al., 2011b) and chat messages (Abbasi and Chen, 2008). Some works focused on SMS messages (Mohan et al., 2010; Ishihara, 2011). Authorship Attribution on Twitter. The performance of authorship attribution systems on short texts is affected by several factors (Stamatatos, 2009</context>
</contexts>
<marker>Mealand, 1995</marker>
<rawString>DL Mealand. 1995. Correspondence analysis of luke. Literary and linguistic computing, 10(3):171–182.</rawString>
</citation>
<citation valid="false">
<title>The characteristic curves of composition.</title>
<journal>Science,</journal>
<pages>9--214</pages>
<marker></marker>
<rawString>Thomas Corwin Mendenhall. 1887. The characteristic curves of composition. Science, ns-9(214S):237–246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George K Mikros</author>
<author>Kostas Perifanos</author>
</authors>
<title>Authorship attribution in greek tweets using authors multilevel n-gram profiles.</title>
<date>2013</date>
<booktitle>In 2013 AAAI Spring Symposium Series.</booktitle>
<contexts>
<context position="32035" citStr="Mikros and Perifanos (2013)" startWordPosition="5340" endWordPosition="5343">ures) on a small dataset of three authors only, with varying training set size. Although their work used a set of Twitter-specific features that we do not explicitly use, our features implicitly cover a large portion of their features (such as punctuation and emoticon 1888 features, which are largely covered by character ngrams). Boutwell (2011) used a naive Bayes classifier with character n-gram features. She experimented with 50 authors and two training size values (120 and 230). She also provided a set of experiments that studied the effect of joining several tweets into a single document. Mikros and Perifanos (2013) trained an SVM classifier with character n-gram and word n-grams. They experimented with 10 authors of Greek text, and also joined several tweets into a single document. Joining several tweets into a longer document is appealing since it can lead to substantial improvement of the classification results, as demonstrated by the works above. However, this approach requires the test data to contain several tweets that are known a-priori to be written by the same author. This assumption is not always realistic. In our paper, we intentionally focus on a single tweet as document size. Flexible Patte</context>
</contexts>
<marker>Mikros, Perifanos, 2013</marker>
<rawString>George K Mikros and Kostas Perifanos. 2013. Authorship attribution in greek tweets using authors multilevel n-gram profiles. In 2013 AAAI Spring Symposium Series.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashwin Mohan</author>
<author>Ibrahim M Baggili</author>
<author>Marcus K Rogers</author>
</authors>
<title>Authorship attribution of sms messages using an n-grams approach.</title>
<date>2010</date>
<tech>Technical report, CERIAS Tech Report</tech>
<contexts>
<context position="30393" citStr="Mohan et al., 2010" startWordPosition="5078" endWordPosition="5081">tribution systems have mainly been evaluated against long texts such as theater plays (Mendenhall, 1887), essays (Yule, 1939; Mosteller and Wallace, 1964), biblical books (Mealand, 1995; Koppel et al., 2011a) and book chapters (Argamon et al., 2007; Koppel et al., 2007). In recent year, many works focused on web data such as emails (De Vel et al., 2001; Koppel and Schler, 2003; Abbasi and Chen, 2008), web forum messages (Abbasi and Chen, 2005; Solorio et al., 2011), blogs (Koppel et al., 2006; Koppel et al., 2011b) and chat messages (Abbasi and Chen, 2008). Some works focused on SMS messages (Mohan et al., 2010; Ishihara, 2011). Authorship Attribution on Twitter. The performance of authorship attribution systems on short texts is affected by several factors (Stamatatos, 2009). These factors include the number of candidate authors, the training set size and the size of the test document. Very few authorship attribution works experimented with Twitter. Unlike our work, all used a single group of authors (group sizes varied between 3-50). Layton et al. (2010) used the SCAP methodology (Frantzeskou et al., 2007) with character ngram features. They experimented with 50 authors and compared different numb</context>
</contexts>
<marker>Mohan, Baggili, Rogers, 2010</marker>
<rawString>Ashwin Mohan, Ibrahim M Baggili, and Marcus K Rogers. 2010. Authorship attribution of sms messages using an n-grams approach. Technical report, CERIAS Tech Report 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Mosteller</author>
<author>David Lee Wallace</author>
</authors>
<title>Inference and disputed authorship: The Federalist.</title>
<date>1964</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="29190" citStr="Mosteller and Wallace, 1964" startWordPosition="4885" endWordPosition="4889"> et al., 2009; Stamatatos, 2009; Juola, 2012). Authorship attribution methods can be generally divided into two categories (Stamatatos, 2009). In similarity-based methods, an anonymous text is attributed to some author whose writing style is most similar (by some distance metric). In machine learning methods, which we follow in this paper, anonymous texts are classified, using machine learning algorithms, into different categories (in this case, different authors). Machine learning papers differ from each other by the features and machine learning algorithm. Examples of features include HFWs (Mosteller and Wallace, 1964; Argamon et al., 2007), character n-gram (Kjell, 1994; Hoorn et al., 1999; Stamatatos, 2008), word n-grams (Peng et al., 2004), part-of-speech n-grams (Koppel and Schler, 2003; Koppel et al., 2005) and vocabulary richness (Abbasi and Chen, 2005). The various machine learning algorithms used include naive Bayes (Mosteller and Wallace, 1964; Kjell, 1994), neural networks (Matthews and Merriam, 1993; Kjell, 1994), K-nearest neighbors (Kjell et al., 1995; Hoorn et al., 1999) and SVM (De Vel et al., 2001; Diederich et al., 2003; Koppel and Schler, 2003). Traditionally, authorship attribution syste</context>
</contexts>
<marker>Mosteller, Wallace, 1964</marker>
<rawString>Frederick Mosteller and David Lee Wallace. 1964. Inference and disputed authorship: The Federalist. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Dale Schuurmans</author>
<author>Shaojun Wang</author>
</authors>
<title>Augmenting naive bayes classifiers with statistical language models.</title>
<date>2004</date>
<journal>Information Retrieval,</journal>
<pages>7--3</pages>
<contexts>
<context position="29317" citStr="Peng et al., 2004" startWordPosition="4906" endWordPosition="4909">2009). In similarity-based methods, an anonymous text is attributed to some author whose writing style is most similar (by some distance metric). In machine learning methods, which we follow in this paper, anonymous texts are classified, using machine learning algorithms, into different categories (in this case, different authors). Machine learning papers differ from each other by the features and machine learning algorithm. Examples of features include HFWs (Mosteller and Wallace, 1964; Argamon et al., 2007), character n-gram (Kjell, 1994; Hoorn et al., 1999; Stamatatos, 2008), word n-grams (Peng et al., 2004), part-of-speech n-grams (Koppel and Schler, 2003; Koppel et al., 2005) and vocabulary richness (Abbasi and Chen, 2005). The various machine learning algorithms used include naive Bayes (Mosteller and Wallace, 1964; Kjell, 1994), neural networks (Matthews and Merriam, 1993; Kjell, 1994), K-nearest neighbors (Kjell et al., 1995; Hoorn et al., 1999) and SVM (De Vel et al., 2001; Diederich et al., 2003; Koppel and Schler, 2003). Traditionally, authorship attribution systems have mainly been evaluated against long texts such as theater plays (Mendenhall, 1887), essays (Yule, 1939; Mosteller and Wa</context>
</contexts>
<marker>Peng, Schuurmans, Wang, 2004</marker>
<rawString>Fuchun Peng, Dale Schuurmans, and Shaojun Wang. 2004. Augmenting naive bayes classifiers with statistical language models. Information Retrieval, 7(3-4):317–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Conrad Sanderson</author>
<author>Simon Guenter</author>
</authors>
<title>Short text authorship attribution via sequence kernels, markov chains and author unmasking: An investigation.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>482--491</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1993" citStr="Sanderson and Guenter, 2006" startWordPosition="302" endWordPosition="305"> attribution projects that experimented with web data such as emails (Abbasi and Chen, 2008), web forum messages (Solorio et al., 2011) and blogs (Koppel et al., 2011b). This paper addresses the question to what extent the authors of very short texts can be identified. To answer this question, we experiment with Twitter tweets. Twitter messages (tweets) are limited to 140 characters. This restriction imposes major difficulties on authorship attribution systems, since authorship attribution methods that work well on long texts are often not as useful when applied to short texts (Burrows, 2002; Sanderson and Guenter, 2006). Nonetheless, tweets are relatively self-contained and have smaller sentence length variance compared to excerpts from longer texts (see Section 3). These characteristics make Twitter data appealing as a testbed when focusing on short texts. Moreover, an authorship attribution system of tweets may have various applications. Specifically, a range of cybercrimes can be addressed using such a system, including identity fraud and phishing. In this paper, we introduce the concept of ksignatures. We denote the k-signatures of an author a as the features that appear in at least k% of a’s training sa</context>
<context position="6841" citStr="Sanderson and Guenter, 2006" startWordPosition="1087" endWordPosition="1090">res are especially useful for authorship attribution on micro-messages since they are relatively tolerant to typos and non-standard use of punctuation (Stamatatos, 2009). These are common in the nonformal style generally applied in social media services. Consider the example of misspelling “Britney” as “Brittney”. The misspelled name shares the 4-grams “Brit” and “tney” with the correct name. As a result, these features provide information about the author’s style (or at least her topic of interest), which is not available through lexical features. Following standard practice, we use 4-grams (Sanderson and Guenter, 2006; Layton et al., 2010; Koppel et al., 2011b). White spaces are considered characters (i.e., a character n-gram may be composed of letters from two different words). A single white-space is appended to the beginning and the end of each tweet. For efficiency, we consider only character n-gram features that appear at least ting times in the training set of at least one author (see Section 5). Word n-grams. We hypothesize that word n-gram features would be useful for authorship attribution on micro-messages. We assume that under a strict length restriction, many authors would prefer using short, r</context>
<context position="8460" citStr="Sanderson and Guenter, 2006" startWordPosition="1363" endWordPosition="1366">Matlab implementation of a multi-class SVM classifier with a linear kernel 1We skip unigrams as they are generally captured by the character n-gram features. 1881 Number of Users (Chang and Lin, 2011). We use ten-fold cross validation on the training set to select the best regularization factor between 0.5 and 0.005.2 3 Experimental Testbed Our main research question in this paper is to determine the extent to which authors of very short texts can be identified. A major issue in working with short texts is selecting the right dataset. One approach is breaking longer texts into shorter chunks (Sanderson and Guenter, 2006). We take a different approach and experiment with micro-messages (specifically, tweets). Tweets have several properties making them an ideal testbed for authorship attribution of short texts. First, tweets are posted as single units and do not necessarily refer to each other. As a result, they tend to be self contained. Second, tweets have more standardized length distribution compared to other types of web data. We compared the mean and standard deviation of sentence length in our Twitter dataset and in a corpus of English web data (Ferraresi et al., 2008).3 We found that (a) tweets are shor</context>
</contexts>
<marker>Sanderson, Guenter, 2006</marker>
<rawString>Conrad Sanderson and Simon Guenter. 2006. Short text authorship attribution via sequence kernels, markov chains and author unmasking: An investigation. In Proc. of EMNLP, pages 482–491, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Sousa Silva</author>
<author>Gustavo Laboreiro</author>
<author>Lu´ıs Sarmento</author>
<author>Tim Grant</author>
<author>Eug´enio Oliveira</author>
<author>Belinda Maia</author>
</authors>
<title>twazn me!!! ;(’ automatic authorship analysis of micro-blogging messages.</title>
<date>2011</date>
<booktitle>In Proc. of the 16th international conference on Natural language processing and information systems, NLDB’11,</booktitle>
<pages>161--168</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="31321" citStr="Silva et al. (2011)" startWordPosition="5225" endWordPosition="5228">tion works experimented with Twitter. Unlike our work, all used a single group of authors (group sizes varied between 3-50). Layton et al. (2010) used the SCAP methodology (Frantzeskou et al., 2007) with character ngram features. They experimented with 50 authors and compared different numbers of tweets per author (values between 20-200). Surprisingly, they showed that their system does not improve when given more training tweets. In our work, we noticed a different trend, and showed that more data can be extremely valuable for authorship attribution systems on micro-messages (see Section 6). Silva et al. (2011) trained an SVM classifier with various features (e.g., punctuation and vocabulary features) on a small dataset of three authors only, with varying training set size. Although their work used a set of Twitter-specific features that we do not explicitly use, our features implicitly cover a large portion of their features (such as punctuation and emoticon 1888 features, which are largely covered by character ngrams). Boutwell (2011) used a naive Bayes classifier with character n-gram features. She experimented with 50 authors and two training size values (120 and 230). She also provided a set of</context>
</contexts>
<marker>Silva, Laboreiro, Sarmento, Grant, Oliveira, Maia, 2011</marker>
<rawString>Rui Sousa Silva, Gustavo Laboreiro, Lu´ıs Sarmento, Tim Grant, Eug´enio Oliveira, and Belinda Maia. 2011. ‘twazn me!!! ;(’ automatic authorship analysis of micro-blogging messages. In Proc. of the 16th international conference on Natural language processing and information systems, NLDB’11, pages 161–168, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thamar Solorio</author>
<author>Sangita Pillay</author>
<author>Sindhu Raghavan</author>
<author>Manuel Montes-Gomez</author>
</authors>
<title>Modality specific meta features for authorship attribution in web forum posts.</title>
<date>2011</date>
<booktitle>In Proc. of IJCNLP,</booktitle>
<pages>156--164</pages>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="1500" citStr="Solorio et al., 2011" startWordPosition="224" endWordPosition="227"> of flavors of the authorship attribution task. 1 Introduction Research in authorship attribution has developed substantially over the last decade (Stamatatos, 2009). The vast majority of such research has been dedicated towards finding the author of long texts, ranging from single passages to book chapters. In recent years, the growing popularity of social media has created special interest, both theoretical and computational, in short texts. This has led to many recent authorship attribution projects that experimented with web data such as emails (Abbasi and Chen, 2008), web forum messages (Solorio et al., 2011) and blogs (Koppel et al., 2011b). This paper addresses the question to what extent the authors of very short texts can be identified. To answer this question, we experiment with Twitter tweets. Twitter messages (tweets) are limited to 140 characters. This restriction imposes major difficulties on authorship attribution systems, since authorship attribution methods that work well on long texts are often not as useful when applied to short texts (Burrows, 2002; Sanderson and Guenter, 2006). Nonetheless, tweets are relatively self-contained and have smaller sentence length variance compared to e</context>
<context position="30244" citStr="Solorio et al., 2011" startWordPosition="5052" endWordPosition="5055">rs (Kjell et al., 1995; Hoorn et al., 1999) and SVM (De Vel et al., 2001; Diederich et al., 2003; Koppel and Schler, 2003). Traditionally, authorship attribution systems have mainly been evaluated against long texts such as theater plays (Mendenhall, 1887), essays (Yule, 1939; Mosteller and Wallace, 1964), biblical books (Mealand, 1995; Koppel et al., 2011a) and book chapters (Argamon et al., 2007; Koppel et al., 2007). In recent year, many works focused on web data such as emails (De Vel et al., 2001; Koppel and Schler, 2003; Abbasi and Chen, 2008), web forum messages (Abbasi and Chen, 2005; Solorio et al., 2011), blogs (Koppel et al., 2006; Koppel et al., 2011b) and chat messages (Abbasi and Chen, 2008). Some works focused on SMS messages (Mohan et al., 2010; Ishihara, 2011). Authorship Attribution on Twitter. The performance of authorship attribution systems on short texts is affected by several factors (Stamatatos, 2009). These factors include the number of candidate authors, the training set size and the size of the test document. Very few authorship attribution works experimented with Twitter. Unlike our work, all used a single group of authors (group sizes varied between 3-50). Layton et al. (20</context>
</contexts>
<marker>Solorio, Pillay, Raghavan, Montes-Gomez, 2011</marker>
<rawString>Thamar Solorio, Sangita Pillay, Sindhu Raghavan, and Manuel Montes-Gomez. 2011. Modality specific meta features for authorship attribution in web forum posts. In Proc. of IJCNLP, pages 156–164, Chiang Mai, Thailand, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efstathios Stamatatos</author>
</authors>
<title>Author identification: Using text sampling to handle the class imbalance problem.</title>
<date>2008</date>
<journal>Inf. Process. Manage.,</journal>
<volume>44</volume>
<issue>2</issue>
<contexts>
<context position="29283" citStr="Stamatatos, 2008" startWordPosition="4902" endWordPosition="4903">into two categories (Stamatatos, 2009). In similarity-based methods, an anonymous text is attributed to some author whose writing style is most similar (by some distance metric). In machine learning methods, which we follow in this paper, anonymous texts are classified, using machine learning algorithms, into different categories (in this case, different authors). Machine learning papers differ from each other by the features and machine learning algorithm. Examples of features include HFWs (Mosteller and Wallace, 1964; Argamon et al., 2007), character n-gram (Kjell, 1994; Hoorn et al., 1999; Stamatatos, 2008), word n-grams (Peng et al., 2004), part-of-speech n-grams (Koppel and Schler, 2003; Koppel et al., 2005) and vocabulary richness (Abbasi and Chen, 2005). The various machine learning algorithms used include naive Bayes (Mosteller and Wallace, 1964; Kjell, 1994), neural networks (Matthews and Merriam, 1993; Kjell, 1994), K-nearest neighbors (Kjell et al., 1995; Hoorn et al., 1999) and SVM (De Vel et al., 2001; Diederich et al., 2003; Koppel and Schler, 2003). Traditionally, authorship attribution systems have mainly been evaluated against long texts such as theater plays (Mendenhall, 1887), es</context>
</contexts>
<marker>Stamatatos, 2008</marker>
<rawString>Efstathios Stamatatos. 2008. Author identification: Using text sampling to handle the class imbalance problem. Inf. Process. Manage., 44(2):790–799.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efstathios Stamatatos</author>
</authors>
<title>A survey of modern authorship attribution methods.</title>
<date>2009</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>60</volume>
<issue>3</issue>
<contexts>
<context position="1044" citStr="Stamatatos, 2009" startWordPosition="153" endWordPosition="154">uccessfully identified. We use Twitter as an experimental testbed. We introduce the concept of an author’s unique “signature”, and show that such signatures are typical of many authors when writing very short texts. We also present a new authorship attribution feature (“flexible patterns”) and demonstrate a significant improvement over our baselines. Our results show that the author of a single tweet can be identified with good accuracy in an array of flavors of the authorship attribution task. 1 Introduction Research in authorship attribution has developed substantially over the last decade (Stamatatos, 2009). The vast majority of such research has been dedicated towards finding the author of long texts, ranging from single passages to book chapters. In recent years, the growing popularity of social media has created special interest, both theoretical and computational, in short texts. This has led to many recent authorship attribution projects that experimented with web data such as emails (Abbasi and Chen, 2008), web forum messages (Solorio et al., 2011) and blogs (Koppel et al., 2011b). This paper addresses the question to what extent the authors of very short texts can be identified. To answer</context>
<context position="6383" citStr="Stamatatos, 2009" startWordPosition="1016" endWordPosition="1018">ows. Sections 2 and 3 describe our methods and our experimental testbed (Twitter). Section 4 presents the concept of k-signatures. Sections 5 and 6 present our experiments and results. Flexible patterns are presented in Section 7 and related work is presented in Section 8. 2 Methodology In the following we briefly describe the main features employed by our system. The features below are binary features. Character n-grams. Character n-gram features are especially useful for authorship attribution on micro-messages since they are relatively tolerant to typos and non-standard use of punctuation (Stamatatos, 2009). These are common in the nonformal style generally applied in social media services. Consider the example of misspelling “Britney” as “Brittney”. The misspelled name shares the 4-grams “Brit” and “tney” with the correct name. As a result, these features provide information about the author’s style (or at least her topic of interest), which is not available through lexical features. Following standard practice, we use 4-grams (Sanderson and Guenter, 2006; Layton et al., 2010; Koppel et al., 2011b). White spaces are considered characters (i.e., a character n-gram may be composed of letters from</context>
<context position="28594" citStr="Stamatatos, 2009" startWordPosition="4797" endWordPosition="4798">hares no sequence of words with (6-8), no word n-gram feature is able to identify the author’s style in (5). However, this style can be successfully identified using the flexible pattern (9), shared by (5-8). 9. the„.W CW I„.W This demonstrates the added value flexible pattern features have over word n-gram features. 8 Related Work Authorship attribution dates back to the end of 19th century, when (Mendenhall, 1887) applied sentence length and word length features to plays of Shakespeare. Ever since, many methods have been developed for this task. For recent surveys, see (Koppel et al., 2009; Stamatatos, 2009; Juola, 2012). Authorship attribution methods can be generally divided into two categories (Stamatatos, 2009). In similarity-based methods, an anonymous text is attributed to some author whose writing style is most similar (by some distance metric). In machine learning methods, which we follow in this paper, anonymous texts are classified, using machine learning algorithms, into different categories (in this case, different authors). Machine learning papers differ from each other by the features and machine learning algorithm. Examples of features include HFWs (Mosteller and Wallace, 1964; Ar</context>
<context position="30561" citStr="Stamatatos, 2009" startWordPosition="5103" endWordPosition="5104">s (Mealand, 1995; Koppel et al., 2011a) and book chapters (Argamon et al., 2007; Koppel et al., 2007). In recent year, many works focused on web data such as emails (De Vel et al., 2001; Koppel and Schler, 2003; Abbasi and Chen, 2008), web forum messages (Abbasi and Chen, 2005; Solorio et al., 2011), blogs (Koppel et al., 2006; Koppel et al., 2011b) and chat messages (Abbasi and Chen, 2008). Some works focused on SMS messages (Mohan et al., 2010; Ishihara, 2011). Authorship Attribution on Twitter. The performance of authorship attribution systems on short texts is affected by several factors (Stamatatos, 2009). These factors include the number of candidate authors, the training set size and the size of the test document. Very few authorship attribution works experimented with Twitter. Unlike our work, all used a single group of authors (group sizes varied between 3-50). Layton et al. (2010) used the SCAP methodology (Frantzeskou et al., 2007) with character ngram features. They experimented with 50 authors and compared different numbers of tweets per author (values between 20-200). Surprisingly, they showed that their system does not improve when given more training tweets. In our work, we noticed </context>
</contexts>
<marker>Stamatatos, 2009</marker>
<rawString>Efstathios Stamatatos. 2009. A survey of modern authorship attribution methods. Journal of the American Society for Information Science and Technology, 60(3):538–556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Tsur</author>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Icwsm–a great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews.</title>
<date>2010</date>
<booktitle>In Proc. of ICWSM.</booktitle>
<contexts>
<context position="33622" citStr="Tsur et al., 2010" startWordPosition="5586" endWordPosition="5589">tracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2008a), sentiment analysis (Davidov et al., 2010b) and detection of sarcasm (Tsur et al., 2010). 9 Conclusion The main goal of this paper is to measure to what extent authors of micro-messages can be identified. We have shown that authors of very short texts can be successfully identified in an array of authorship attribution settings reported for long documents. This is the first work on micro-messages to address some of these settings. We introduced the concept of k-signature. Using this concept, we proposed an interpretation of our results. Last, we presented the first authorship attribution system that uses flexible patterns, and demonstrated that using these features significantly </context>
</contexts>
<marker>Tsur, Davidov, Rappoport, 2010</marker>
<rawString>Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010. Icwsm–a great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews. In Proc. of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>A uniform approach to analogies, synonyms, antonyms, and associations.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proc. of Coling,</booktitle>
<pages>905--912</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="33391" citStr="Turney, 2008" startWordPosition="5554" endWordPosition="5556">ks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2008a), sentiment analysis (Davidov et al., 2010b) and detection of sarcasm (Tsur et al., 2010). 9 Conclusion The main goal of this paper is to measure to what extent authors of micro-messages can be identified. We have shown that authors of very short texts can be successfully identified in an array of authorship attribution settings reported for long documents. This is the first work on micro-messages to address some of these settings. We introduced the conc</context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter Turney. 2008a. A uniform approach to analogies, synonyms, antonyms, and associations. In Proc. of Coling, pages 905–912, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>The latent relation mapping engine: Algorithm and experiments.</title>
<date>2008</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>33--615</pages>
<contexts>
<context position="33391" citStr="Turney, 2008" startWordPosition="5554" endWordPosition="5556">ks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2008a), sentiment analysis (Davidov et al., 2010b) and detection of sarcasm (Tsur et al., 2010). 9 Conclusion The main goal of this paper is to measure to what extent authors of micro-messages can be identified. We have shown that authors of very short texts can be successfully identified in an array of authorship attribution settings reported for long documents. This is the first work on micro-messages to address some of these settings. We introduced the conc</context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter D. Turney. 2008b. The latent relation mapping engine: Algorithm and experiments. Journal ofArtificial Intelligence Research, 33:615–655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
<author>Beate Dorow</author>
</authors>
<title>A graph model for unsupervised lexical acquisition.</title>
<date>2002</date>
<booktitle>In Proc. of Coling,</booktitle>
<pages>1--7</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="32883" citStr="Widdows and Dorow, 2002" startWordPosition="5475" endWordPosition="5478">aling since it can lead to substantial improvement of the classification results, as demonstrated by the works above. However, this approach requires the test data to contain several tweets that are known a-priori to be written by the same author. This assumption is not always realistic. In our paper, we intentionally focus on a single tweet as document size. Flexible Patterns. Patterns were introduced by (Hearst, 1992), who used hand crafted patterns to discover hyponyms. Hard coded patterns were used for many tasks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal</context>
</contexts>
<marker>Widdows, Dorow, 2002</marker>
<rawString>Dominic Widdows and Beate Dorow. 2002. A graph model for unsupervised lexical acquisition. In Proc. of Coling, pages 1–7, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Udny Yule</author>
</authors>
<title>On sentence-length as a statistical characteristic of style in prose: with application to two cases of disputed authorship.</title>
<date>1939</date>
<journal>Biometrika,</journal>
<pages>30--3</pages>
<contexts>
<context position="29899" citStr="Yule, 1939" startWordPosition="4995" endWordPosition="4996"> n-grams (Peng et al., 2004), part-of-speech n-grams (Koppel and Schler, 2003; Koppel et al., 2005) and vocabulary richness (Abbasi and Chen, 2005). The various machine learning algorithms used include naive Bayes (Mosteller and Wallace, 1964; Kjell, 1994), neural networks (Matthews and Merriam, 1993; Kjell, 1994), K-nearest neighbors (Kjell et al., 1995; Hoorn et al., 1999) and SVM (De Vel et al., 2001; Diederich et al., 2003; Koppel and Schler, 2003). Traditionally, authorship attribution systems have mainly been evaluated against long texts such as theater plays (Mendenhall, 1887), essays (Yule, 1939; Mosteller and Wallace, 1964), biblical books (Mealand, 1995; Koppel et al., 2011a) and book chapters (Argamon et al., 2007; Koppel et al., 2007). In recent year, many works focused on web data such as emails (De Vel et al., 2001; Koppel and Schler, 2003; Abbasi and Chen, 2008), web forum messages (Abbasi and Chen, 2005; Solorio et al., 2011), blogs (Koppel et al., 2006; Koppel et al., 2011b) and chat messages (Abbasi and Chen, 2008). Some works focused on SMS messages (Mohan et al., 2010; Ishihara, 2011). Authorship Attribution on Twitter. The performance of authorship attribution systems on</context>
</contexts>
<marker>Yule, 1939</marker>
<rawString>George Udny Yule. 1939. On sentence-length as a statistical characteristic of style in prose: with application to two cases of disputed authorship. Biometrika, 30(3-4):363–390.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
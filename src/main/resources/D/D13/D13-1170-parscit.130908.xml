<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9991365">
Recursive Deep Models for Semantic Compositionality
Over a Sentiment Treebank
</title>
<author confidence="0.9844345">
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang,
Christopher D. Manning, Andrew Y. Ng and Christopher Potts
</author>
<affiliation confidence="0.888589">
Stanford University, Stanford, CA 94305, USA
</affiliation>
<email confidence="0.994091">
richard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu
{jeaneis,manning,cgpotts}@stanford.edu
</email>
<sectionHeader confidence="0.996906" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.93861504">
Semantic word spaces have been very use-
ful but cannot express the meaning of longer
phrases in a principled way. Further progress
towards understanding compositionality in
tasks such as sentiment detection requires
richer supervised training and evaluation re-
sources and more powerful models of com-
position. To remedy this, we introduce a
Sentiment Treebank. It includes fine grained
sentiment labels for 215,154 phrases in the
parse trees of 11,855 sentences and presents
new challenges for sentiment composition-
ality. To address them, we introduce the
Recursive Neural Tensor Network. When
trained on the new treebank, this model out-
performs all previous methods on several met-
rics. It pushes the state of the art in single
sentence positive/negative classification from
80% up to 85.4%. The accuracy of predicting
fine-grained sentiment labels for all phrases
reaches 80.7%, an improvement of 9.7% over
bag of features baselines. Lastly, it is the only
model that can accurately capture the effects
of negation and its scope at various tree levels
for both positive and negative phrases.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9989636">
Semantic vector spaces for single words have been
widely used as features (Turney and Pantel, 2010).
Because they cannot capture the meaning of longer
phrases properly, compositionality in semantic vec-
tor spaces has recently received a lot of attention
(Mitchell and Lapata, 2010; Socher et al., 2010;
Zanzotto et al., 2010; Yessenalina and Cardie, 2011;
Socher et al., 2012; Grefenstette et al., 2013). How-
ever, progress is held back by the current lack of
large and labeled compositionality resources and
</bodyText>
<figureCaption confidence="0.9728794">
Figure 1: Example of the Recursive Neural Tensor Net-
work accurately predicting 5 sentiment classes, very neg-
ative to very positive (– –, –, 0, +, + +), at every node of a
parse tree and capturing the negation and its scope in this
sentence.
</figureCaption>
<bodyText confidence="0.98444095">
models to accurately capture the underlying phe-
nomena presented in such data. To address this need,
we introduce the Stanford Sentiment Treebank and
a powerful Recursive Neural Tensor Network that
can accurately predict the compositional semantic
effects present in this new corpus.
The Stanford Sentiment Treebank is the first cor-
pus with fully labeled parse trees that allows for a
complete analysis of the compositional effects of
sentiment in language. The corpus is based on
the dataset introduced by Pang and Lee (2005) and
consists of 11,855 single sentences extracted from
movie reviews. It was parsed with the Stanford
parser (Klein and Manning, 2003) and includes a
total of 215,154 unique phrases from those parse
trees, each annotated by 3 human judges. This new
dataset allows us to analyze the intricacies of senti-
ment and to capture complex linguistic phenomena.
Fig. 1 shows one of the many examples with clear
compositional structure. The granularity and size of
</bodyText>
<figure confidence="0.999572416666667">
0
This
0
0
does
0
film
–
0
n’t
+
cleverness
–
+
care
–
0
about
+
0
+
0
–
0
wit
+
+
or
0
0
any
0
0
other
+
0
0
+
kind
+
+
intelligent
of
0
+
+
+ +
humor
</figure>
<page confidence="0.945887">
1631
</page>
<note confidence="0.732413">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999795588235294">
this dataset will enable the community to train com-
positional models that are based on supervised and
structured machine learning techniques. While there
are several datasets with document and chunk labels
available, there is a need to better capture sentiment
from short comments, such as Twitter data, which
provide less overall signal per document.
In order to capture the compositional effects with
higher accuracy, we propose a new model called the
Recursive Neural Tensor Network (RNTN). Recur-
sive Neural Tensor Networks take as input phrases
of any length. They represent a phrase through word
vectors and a parse tree and then compute vectors for
higher nodes in the tree using the same tensor-based
composition function. We compare to several super-
vised, compositional models such as standard recur-
sive neural networks (RNN) (Socher et al., 2011b),
matrix-vector RNNs (Socher et al., 2012), and base-
lines such as neural networks that ignore word order,
Naive Bayes (NB), bi-gram NB and SVM. All mod-
els get a significant boost when trained with the new
dataset but the RNTN obtains the highest perfor-
mance with 80.7% accuracy when predicting fine-
grained sentiment for all nodes. Lastly, we use a test
set of positive and negative sentences and their re-
spective negations to show that, unlike bag of words
models, the RNTN accurately captures the sentiment
change and scope of negation. RNTNs also learn
that sentiment of phrases following the contrastive
conjunction ‘but’ dominates.
The complete training and testing code, a live
demo and the Stanford Sentiment Treebank dataset
are available at http://nlp.stanford.edu/
sentiment.
</bodyText>
<sectionHeader confidence="0.999947" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.983439896551724">
This work is connected to five different areas of NLP
research, each with their own large amount of related
work to which we cannot do full justice given space
constraints.
Semantic Vector Spaces. The dominant ap-
proach in semantic vector spaces uses distributional
similarities of single words. Often, co-occurrence
statistics of a word and its context are used to de-
scribe each word (Turney and Pantel, 2010; Baroni
and Lenci, 2010), such as tf-idf. Variants of this idea
use more complex frequencies such as how often a
word appears in a certain syntactic context (Pado
and Lapata, 2007; Erk and Pad´o, 2008). However,
distributional vectors often do not properly capture
the differences in antonyms since those often have
similar contexts. One possibility to remedy this is to
use neural word vectors (Bengio et al., 2003). These
vectors can be trained in an unsupervised fashion
to capture distributional similarities (Collobert and
Weston, 2008; Huang et al., 2012) but then also be
fine-tuned and trained to specific tasks such as sen-
timent detection (Socher et al., 2011b). The models
in this paper can use purely supervised word repre-
sentations learned entirely on the new corpus.
Compositionality in Vector Spaces. Most of
the compositionality algorithms and related datasets
capture two word compositions. Mitchell and La-
pata (2010) use e.g. two-word phrases and analyze
similarities computed by vector addition, multiplica-
tion and others. Some related models such as holo-
graphic reduced representations (Plate, 1995), quan-
tum logic (Widdows, 2008), discrete-continuous
models (Clark and Pulman, 2007) and the recent
compositional matrix space model (Rudolph and
Giesbrecht, 2010) have not been experimentally val-
idated on larger corpora. Yessenalina and Cardie
(2011) compute matrix representations for longer
phrases and define composition as matrix multipli-
cation, and also evaluate on sentiment. Grefen-
stette and Sadrzadeh (2011) analyze subject-verb-
object triplets and find a matrix-based categorical
model to correlate well with human judgments. We
compare to the recent line of work on supervised
compositional models. In particular we will de-
scribe and experimentally compare our new RNTN
model to recursive neural networks (RNN) (Socher
et al., 2011b) and matrix-vector RNNs (Socher et
al., 2012) both of which have been applied to bag of
words sentiment corpora.
Logical Form. A related field that tackles com-
positionality from a very different angle is that of
trying to map sentences to logical form (Zettlemoyer
and Collins, 2005). While these models are highly
interesting and work well in closed domains and
on discrete sets, they could only capture sentiment
distributions using separate mechanisms beyond the
currently used logical forms.
Deep Learning. Apart from the above mentioned
</bodyText>
<page confidence="0.991528">
1632
</page>
<bodyText confidence="0.996600954545455">
work on RNNs, several compositionality ideas re-
lated to neural networks have been discussed by Bot-
tou (2011) and Hinton (1990) and first models such
as Recursive Auto-associative memories been exper-
imented with by Pollack (1990). The idea to relate
inputs through three way interactions, parameterized
by a tensor have been proposed for relation classifi-
cation (Sutskever et al., 2009; Jenatton et al., 2012),
extending Restricted Boltzmann machines (Ranzato
and Hinton, 2010) and as a special layer for speech
recognition (Yu et al., 2012).
Sentiment Analysis. Apart from the above-
mentioned work, most approaches in sentiment anal-
ysis use bag of words representations (Pang and Lee,
2008). Snyder and Barzilay (2007) analyzed larger
reviews in more detail by analyzing the sentiment
of multiple aspects of restaurants, such as food or
atmosphere. Several works have explored sentiment
compositionality through careful engineering of fea-
tures or polarity shifting rules on syntactic structures
(Polanyi and Zaenen, 2006; Moilanen and Pulman,
2007; Rentoumi et al., 2010; Nakagawa et al., 2010).
</bodyText>
<sectionHeader confidence="0.969934" genericHeader="method">
3 Stanford Sentiment Treebank
</sectionHeader>
<bodyText confidence="0.999909652173913">
Bag of words classifiers can work well in longer
documents by relying on a few words with strong
sentiment like ‘awesome’ or ‘exhilarating.’ How-
ever, sentiment accuracies even for binary posi-
tive/negative classification for single sentences has
not exceeded 80% for several years. For the more
difficult multiclass case including a neutral class,
accuracy is often below 60% for short messages
on Twitter (Wang et al., 2012). From a linguistic
or cognitive standpoint, ignoring word order in the
treatment of a semantic task is not plausible, and, as
we will show, it cannot accurately classify hard ex-
amples of negation. Correctly predicting these hard
cases is necessary to further improve performance.
In this section we will introduce and provide some
analyses for the new Sentiment Treebank which in-
cludes labels for every syntactically plausible phrase
in thousands of sentences, allowing us to train and
evaluate compositional models.
We consider the corpus of movie review excerpts
from the rottentomatoes.com website orig-
inally collected and published by Pang and Lee
(2005). The original dataset includes 10,662 sen-
</bodyText>
<figure confidence="0.45669">
nerdy folks
</figure>
<figureCaption confidence="0.994076">
Figure 3: The labeling interface. Random phrases were
shown and annotators had a slider for selecting the senti-
ment and its degree.
</figureCaption>
<bodyText confidence="0.999888142857143">
tences, half of which were considered positive and
the other half negative. Each label is extracted from
a longer movie review and reflects the writer’s over-
all intention for this review. The normalized, lower-
cased text is first used to recover, from the origi-
nal website, the text with capitalization. Remaining
HTML tags and sentences that are not in English
are deleted. The Stanford Parser (Klein and Man-
ning, 2003) is used to parses all 10,662 sentences.
In approximately 1,100 cases it splits the snippet
into multiple sentences. We then used Amazon Me-
chanical Turk to label the resulting 215,154 phrases.
Fig. 3 shows the interface annotators saw. The slider
has 25 different values and is initially set to neutral.
The phrases in each hit are randomly sampled from
the set of all phrases in order to prevent labels being
influenced by what follows. For more details on the
dataset collection, see supplementary material.
Fig. 2 shows the normalized label distributions at
each n-gram length. Starting at length 20, the ma-
jority are full sentences. One of the findings from
labeling sentences based on reader’s perception is
that many of them could be considered neutral. We
also notice that stronger sentiment often builds up
in longer phrases and the majority of the shorter
phrases are neutral. Another observation is that most
annotators moved the slider to one of the five po-
sitions: negative, somewhat negative, neutral, posi-
tive or somewhat positive. The extreme values were
rarely used and the slider was not often left in be-
tween the ticks. Hence, even a 5-class classification
into these categories captures the main variability
of the labels. We will name this fine-grained senti-
ment classification and our main experiment will be
to recover these five labels for phrases of all lengths.
</bodyText>
<figure confidence="0.998479147540983">
|
Very
negative
|
Negative
|
Somewhat
negative
|
Neutral
|
Somewhat
positive
|
Positive
|
Very
positive
phenomenal fantasy best sellers
|
Very
negative
|
Negative
|
Somewhat
negative
|
Neutral
|
Somewhat
positive
|
Positive
|
Very
positive
1633
(a)
(b)
(c)
(d)
% of Sentiment Values
Distributions of sentiment values for (a) unigrams,
(b) 10-grams, (c) 20-grams, and (d) full sentences.
100%
Ve ry Positive
Positive
80%
Somewhat Positive
60%
Neutral
40%
Somewhat Negative
Negative
Very N egative
(d)
20%
0%
5 10 15 20 25 30 35 40 45
N-Gram Length
</figure>
<figureCaption confidence="0.990151">
Figure 2: Normalized histogram of sentiment annotations at each n-gram length. Many shorter n-grams are neutral;
longer phrases are well distributed. Few annotators used slider positions between ticks or the extreme values. Hence
the two strongest labels and intermediate tick positions are merged into 5 classes.
</figureCaption>
<sectionHeader confidence="0.995684" genericHeader="method">
4 Recursive Neural Models
</sectionHeader>
<bodyText confidence="0.9999905">
The models in this section compute compositional
vector representations for phrases of variable length
and syntactic type. These representations will then
be used as features to classify each phrase. Fig. 4
displays this approach. When an n-gram is given to
the compositional models, it is parsed into a binary
tree and each leaf node, corresponding to a word,
is represented as a vector. Recursive neural mod-
els will then compute parent vectors in a bottom
up fashion using different types of compositional-
ity functions g. The parent vectors are again given
as features to a classifier. For ease of exposition,
we will use the tri-gram in this figure to explain all
models.
We first describe the operations that the below re-
cursive neural models have in common: word vector
representations and classification. This is followed
by descriptions of two previous RNN models and
our RNTN.
Each word is represented as a d-dimensional vec-
tor. We initialize all word vectors by randomly
sampling each value from a uniform distribution:
U(−r, r), where r = 0.0001. All the word vec-
tors are stacked in the word embedding matrix L E
Rdx|V |, where |V  |is the size of the vocabulary. Ini-
tially the word vectors will be random but the L ma-
trix is seen as a parameter that is trained jointly with
the compositionality models.
We can use the word vectors immediately as
parameters to optimize and as feature inputs to
a softmax classifier. For classification into five
classes, we compute the posterior probability over
</bodyText>
<figureCaption confidence="0.660124">
Figure 4: Approach of Recursive Neural Network mod-
</figureCaption>
<bodyText confidence="0.951241">
els for sentiment: Compute parent vectors in a bottom up
fashion using a compositionality function g and use node
vectors as features for a classifier at that node. This func-
tion varies for the different models.
labels given the word vector via:
</bodyText>
<equation confidence="0.951167">
y°&apos; = softmax(Wsa), (1)
</equation>
<bodyText confidence="0.9992016">
where Ws E R5xd is the sentiment classification
matrix. For the given tri-gram, this is repeated for
vectors b and c. The main task of and difference
between the models will be to compute the hidden
vectors pi E Rd in a bottom up fashion.
</bodyText>
<subsectionHeader confidence="0.983748">
4.1 RNN: Recursive Neural Network
</subsectionHeader>
<bodyText confidence="0.999939">
The simplest member of this family of neural net-
work models is the standard recursive neural net-
work (Goller and K¨uchler, 1996; Socher et al.,
2011a). First, it is determined which parent already
has all its children computed. In the above tree ex-
ample, p1 has its two children’s vectors since both
are words. RNNs use the following equations to
compute the parent vectors:
</bodyText>
<equation confidence="0.936051555555556">
not very good ...
a b c
0 0 +
p2 = g(a,p1)
-
p1 =g(b,c)
+
1634
p1 = f(W[c]l,p2 = f(W[a1]),
</equation>
<bodyText confidence="0.999996157894737">
where f = tanh is a standard element-wise nonlin-
earity, W E Rdx2d is the main parameter to learn
and we omit the bias for simplicity. The bias can be
added as an extra column to W if an additional 1 is
added to the concatenation of the input vectors. The
parent vectors must be of the same dimensionality to
be recursively compatible and be used as input to the
next composition. Each parent vector pi, is given to
the same softmax classifier of Eq. 1 to compute its
label probabilities.
This model uses the same compositionality func-
tion as the recursive autoencoder (Socher et al.,
2011b) and recursive auto-associate memories (Pol-
lack, 1990). The only difference to the former model
is that we fix the tree structures and ignore the re-
construction loss. In initial experiments, we found
that with the additional amount of training data, the
reconstruction loss at each node is not necessary to
obtain high performance.
</bodyText>
<subsectionHeader confidence="0.960026">
4.2 MV-RNN: Matrix-Vector RNN
</subsectionHeader>
<bodyText confidence="0.99353845">
The MV-RNN is linguistically motivated in that
most of the parameters are associated with words
and each composition function that computes vec-
tors for longer phrases depends on the actual words
being combined. The main idea of the MV-RNN
(Socher et al., 2012) is to represent every word and
longer phrase in a parse tree as both a vector and
a matrix. When two constituents are combined the
matrix of one is multiplied with the vector of the
other and vice versa. Hence, the compositional func-
tion is parameterized by the words that participate in
it.
Each word’s matrix is initialized as a dxd identity
matrix, plus a small amount of Gaussian noise. Sim-
ilar to the random word vectors, the parameters of
these matrices will be trained to minimize the clas-
sification error at each node. For this model, each n-
gram is represented as a list of (vector,matrix) pairs,
together with the parse tree. For the tree with (vec-
tor,matrix) nodes:
</bodyText>
<equation confidence="0.996003333333333">
(p2,P2)
(a,A) (p1,P1)
(b,B) (c,C)
</equation>
<bodyText confidence="0.972232888888889">
the MV-RNN computes the first parent vector and its
matrix via two equations:
~ p1 = f IBc Cb 1) ,P1 = f (WM IC B 1) ,
where WM E Rdx2d and the result is again a d x d
matrix. Similarly, the second parent node is com-
puted using the previously computed (vector,matrix)
pair (p1, P1) as well as (a, A). The vectors are used
for classifying each phrase using the same softmax
classifier as in Eq. 1.
</bodyText>
<subsectionHeader confidence="0.981907">
4.3 RNTN:Recursive Neural Tensor Network
</subsectionHeader>
<bodyText confidence="0.999989125">
One problem with the MV-RNN is that the number
of parameters becomes very large and depends on
the size of the vocabulary. It would be cognitively
more plausible if there was a single powerful com-
position function with a fixed number of parameters.
The standard RNN is a good candidate for such a
function. However, in the standard RNN, the input
vectors only implicitly interact through the nonlin-
earity (squashing) function. A more direct, possibly
multiplicative, interaction would allow the model to
have greater interactions between the input vectors.
Motivated by these ideas we ask the question: Can
a single, more powerful composition function per-
form better and compose aggregate meaning from
smaller constituents more accurately than many in-
put specific ones? In order to answer this question,
we propose a new model called the Recursive Neu-
ral Tensor Network (RNTN). The main idea is to use
the same, tensor-based composition function for all
nodes.
Fig. 5 shows a single tensor layer. We define the
output of a tensor product h E Rd via the follow-
ing vectorized notation and the equivalent but more
detailed notation for each slice V [i] E Rdxd:
</bodyText>
<equation confidence="0.981704666666667">
� b�T � b~ � b�T � b~
h = V [1:d] ;hi = V [i] .
c c c c
</equation>
<bodyText confidence="0.9976165">
where V [1:d] E R2dx2dxd is the tensor that defines
multiple bilinear forms.
</bodyText>
<page confidence="0.987933">
1635
</page>
<figureCaption confidence="0.99537275">
Figure 5: A single layer of the Recursive Neural Ten-
sor Network. Each dashed box represents one of d-many
slices and can capture a type of influence a child can have
on its parent.
</figureCaption>
<bodyText confidence="0.897797">
The RNTN uses this definition for computing p1:
</bodyText>
<equation confidence="0.910056333333333">
T V [1:d]
[b] Ib]+ W[b] ,
c c c
</equation>
<bodyText confidence="0.999955333333333">
where W is as defined in the previous models. The
next parent vector p2 in the tri-gram will be com-
puted with the same weights:
</bodyText>
<equation confidence="0.99175625">
� a ~T � a � � a �!
V [1:d]
p2 = f + W .
p1 p1 p1
</equation>
<bodyText confidence="0.999876833333333">
The main advantage over the previous RNN
model, which is a special case of the RNTN when
V is set to 0, is that the tensor can directly relate in-
put vectors. Intuitively, we can interpret each slice
of the tensor as capturing a specific type of compo-
sition.
An alternative to RNTNs would be to make the
compositional function more powerful by adding a
second neural network layer. However, initial exper-
iments showed that it is hard to optimize this model
and vector interactions are still more implicit than in
the RNTN.
</bodyText>
<subsectionHeader confidence="0.978437">
4.4 Tensor Backprop through Structure
</subsectionHeader>
<bodyText confidence="0.9997856875">
We describe in this section how to train the RNTN
model. As mentioned above, each node has a
softmax classifier trained on its vector representa-
tion to predict a given ground truth or target vector
t. We assume the target distribution vector at each
node has a 0-1 encoding. If there are C classes, then
it has length C and a 1 at the correct label. All other
entries are 0.
We want to maximize the probability of the cor-
rect prediction, or minimize the cross-entropy error
between the predicted distribution yi E RCx1 at
node i and the target distribution ti E RCx1 at that
node. This is equivalent (up to a constant) to mini-
mizing the KL-divergence between the two distribu-
tions. The error as a function of the RNTN parame-
ters θ = (V, W, Ws, L) for a sentence is:
</bodyText>
<equation confidence="0.9814895">
E(θ) = X X tij log yij + λ11θ112 (2)
i j
</equation>
<bodyText confidence="0.981923541666667">
The derivative for the weights of the softmax clas-
sifier are standard and simply sum up from each
node’s error. We define xi to be the vector at node
i (in the example trigram, the xi E Rdx1’s are
(a, b, c, p1, p2)). We skip the standard derivative for
Ws. Each node backpropagates its error through to
the recursively used weights V, W. Let δi,s E Rdx1
be the softmax error vector at node i:
δi,s = (WsT (yi − ti)) ® f�(xi),
where ® is the Hadamard product between the two
vectors and f� is the element-wise derivative of f
which in the standard case of using f = tanh can
be computed using only f(xi).
The remaining derivatives can only be computed
in a top-down fashion from the top node through the
tree and into the leaf nodes. The full derivative for
V and W is the sum of the derivatives at each of
the nodes. We define the complete incoming error
messages for a node i as δi,com. The top node, in
our case p2, only received errors from the top node’s
softmax. Hence, δp2,com = δp2,s which we can
use to obtain the standard backprop derivative for
W (Goller and K¨uchler, 1996; Socher et al., 2010).
For the derivative of each slice k = 1, ... , d, we get:
</bodyText>
<equation confidence="0.913114">
= δp2,com ,
k p1 p1
� a � � a � T
</equation>
<bodyText confidence="0.680725">
where δp2,com is just the k’th element of this vector.
k
Now, we can compute the error message for the two
</bodyText>
<figure confidence="0.984319722222222">
Neural Tensor Layer
p = f
p = f
Slices of Standard
Tensor Layer Layer
b
c
T
V[1:2]
b
c
+ W
+
b
c
p1 = f
∂Ep2
∂V [k]
</figure>
<page confidence="0.88575">
1636
</page>
<table confidence="0.997194636363636">
Model Fine-grained Positive/Negative
All Root All Root
NB 67.2 41.0 82.6 81.8
SVM 64.3 40.7 84.6 79.4
BiNB 71.0 41.9 82.7 83.1
VecAvg 73.3 32.7 85.1 80.1
RNN 79.0 43.2 86.1 82.4
MV-RNN 78.7 44.4 86.8 82.9
RNTN 80.7 45.7 87.6 85.4
children of p2:
δp2,down =CWT δp2,com + S� ®f / \ pa1 J
</table>
<bodyText confidence="0.57137">
where we define
</bodyText>
<equation confidence="0.836265666666667">
� ~V [k]�T� � a �
δp2,com V [k] +
k p1
Xd
k=1
S =
</equation>
<bodyText confidence="0.999744666666667">
The children of p2, will then each take half of this
vector and add their own softmax error message for
the complete δ. In particular, we have
</bodyText>
<equation confidence="0.987887">
δp1,com = δp1,s + δp2,down[d + 1 : 2d],
</equation>
<bodyText confidence="0.998069333333333">
where δp2,down[d + 1 : 2d] indicates that p1 is the
right child of p2 and hence takes the 2nd half of the
error, for the final word vector derivative for a, it
will be δp2,down[1 : d].
The full derivative for slice V [k] for this trigram
tree then is the sum at each node:
</bodyText>
<equation confidence="0.99868225">
� b � � b ~T
Ep2
∂V [k] + δp1,com ,
k c c
</equation>
<bodyText confidence="0.999942333333333">
and similarly for W. For this nonconvex optimiza-
tion we use AdaGrad (Duchi et al., 2011) which con-
verges in less than 3 hours to a local optimum.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999929111111111">
We include two types of analyses. The first type in-
cludes several large quantitative evaluations on the
test set. The second type focuses on two linguistic
phenomena that are important in sentiment.
For all models, we use the dev set and cross-
validate over regularization of the weights, word
vector size as well as learning rate and minibatch
size for AdaGrad. Optimal performance for all mod-
els was achieved at word vector sizes between 25
and 35 dimensions and batch sizes between 20 and
30. Performance decreased at larger or smaller vec-
tor and batch sizes. This indicates that the RNTN
does not outperform the standard RNN due to sim-
ply having more parameters. The MV-RNN has or-
ders of magnitudes more parameters than any other
model due to the word matrices. The RNTN would
usually achieve its best performance on the dev set
after training for 3 - 5 hours. Initial experiments
</bodyText>
<tableCaption confidence="0.9592335">
Table 1: Accuracy for fine grained (5-class) and binary
predictions at the sentence level (root) and for all nodes.
</tableCaption>
<bodyText confidence="0.999978764705882">
showed that the recursive models worked signifi-
cantly worse (over 5% drop in accuracy) when no
nonlinearity was used. We use f = tanh in all ex-
periments.
We compare to commonly used methods that use
bag of words features with Naive Bayes and SVMs,
as well as Naive Bayes with bag of bigram features.
We abbreviate these with NB, SVM and biNB. We
also compare to a model that averages neural word
vectors and ignores word order (VecAvg).
The sentences in the treebank were split into a
train (8544), dev (1101) and test splits (2210) and
these splits are made available with the data release.
We also analyze performance on only positive and
negative sentences, ignoring the neutral class. This
filters about 20% of the data with the three sets hav-
ing 6920/872/1821 sentences.
</bodyText>
<subsectionHeader confidence="0.958597">
5.1 Fine-grained Sentiment For All Phrases
</subsectionHeader>
<bodyText confidence="0.9855306875">
The main novel experiment and evaluation metric
analyze the accuracy of fine-grained sentiment clas-
sification for all phrases. Fig. 2 showed that a fine
grained classification into 5 classes is a reasonable
approximation to capture most of the data variation.
Fig. 6 shows the result on this new corpus. The
RNTN gets the highest performance, followed by
the MV-RNN and RNN. The recursive models work
very well on shorter phrases, where negation and
composition are important, while bag of features
baselines perform well only with longer sentences.
The RNTN accuracy upper bounds other models at
most n-gram lengths.
Table 1 (left) shows the overall accuracy numbers
for fine grained prediction at all phrase lengths and
full sentences.
</bodyText>
<equation confidence="0.7106435">
∂E
∂V [k]
</equation>
<page confidence="0.754557">
1637
</page>
<figure confidence="0.99844465">
1.0
0.8
Accuracy
0.6
0.4
0.2
1.0
Cumulative Accuracy
0.9
0.8
0.7
0.6
Model
RNTN
MV-RNN
RNN
WNB
NB
5 10 15 20 25 5 10 15 20 25
NGram Length N-�ram Length
</figure>
<figureCaption confidence="0.99813">
Figure 6: Accuracy curves for fine grained sentiment classification at each n-gram lengths. Left: Accuracy separately
for each set of n-grams. Right: Cumulative accuracy of all &lt; n-grams.
</figureCaption>
<subsectionHeader confidence="0.997094">
5.2 Full Sentence Binary Sentiment
</subsectionHeader>
<bodyText confidence="0.999989428571429">
This setup is comparable to previous work on the
original rotten tomatoes dataset which only used
full sentence labels and binary classification of pos-
itive/negative. Hence, these experiments show the
improvement even baseline methods can achieve
with the sentiment treebank. Table 1 shows results
of this binary classification for both all phrases and
for only full sentences. The previous state of the
art was below 80% (Socher et al., 2012). With the
coarse bag of words annotation for training, many of
the more complex phenomena could not be captured,
even by more powerful models. The combination of
the new sentiment treebank and the RNTN pushes
the state of the art on short phrases up to 85.4%.
</bodyText>
<subsectionHeader confidence="0.99742">
5.3 Model Analysis: Contrastive Conjunction
</subsectionHeader>
<bodyText confidence="0.9996323125">
In this section, we use a subset of the test set which
includes only sentences with an ‘X but Y ’ structure:
A phrase X being followed by but which is followed
by a phrase Y . The conjunction is interpreted as
an argument for the second conjunct, with the first
functioning concessively (Lakoff, 1971; Blakemore,
1989; Merin, 1999). Fig. 7 contains an example. We
analyze a strict setting, where X and Y are phrases
of different sentiment (including neutral). The ex-
ample is counted as correct, if the classifications for
both phrases X and Y are correct. Furthermore,
the lowest node that dominates both of the word
but and the node that spans Y also have to have the
same correct sentiment. For the resulting 131 cases,
the RNTN obtains an accuracy of 41% compared to
MV-RNN (37), RNN (36) and biNB (27).
</bodyText>
<subsectionHeader confidence="0.993945">
5.4 Model Analysis: High Level Negation
</subsectionHeader>
<bodyText confidence="0.9973605">
We investigate two types of negation. For each type,
we use a separate dataset for evaluation.
</bodyText>
<figureCaption confidence="0.8836355">
Figure 7: Example of correct prediction for contrastive
conjunction X but Y .
</figureCaption>
<bodyText confidence="0.97521444">
Set 1: Negating Positive Sentences. The first set
contains positive sentences and their negation. In
this set, the negation changes the overall sentiment
of a sentence from positive to negative. Hence, we
compute accuracy in terms of correct sentiment re-
versal from positive to negative. Fig. 9 shows two
examples of positive negation the RNTN correctly
classified, even if negation is less obvious in the case
of ‘least’. Table 2 (left) gives the accuracies over 21
positive sentences and their negation for all models.
The RNTN has the highest reversal accuracy, show-
ing its ability to structurally learn negation of posi-
tive sentences. But what if the model simply makes
phrases very negative when negation is in the sen-
tence? The next experiments show that the model
captures more than such a simplistic negation rule.
Set 2: Negating Negative Sentences. The sec-
ond set contains negative sentences and their nega-
tion. When negative sentences are negated, the sen-
timent treebank shows that overall sentiment should
become less negative, but not necessarily positive.
For instance, ‘The movie was terrible’ is negative
but the ‘The movie was not terrible’ says only that it
was less bad than a terrible one, not that it was good
(Horn, 1989; Israel, 2001). Hence, we evaluate ac-
</bodyText>
<figure confidence="0.999647787234042">
–
0
There
slow
–
–
–
0
and
+
0
it
0
–
0
parts
+
+
0 +
it interesting
0
has
0
just
0
+
+
0
enough
0
+
+
spice
+
0
to
0
0
keep
+
–
0
are
0
0
but
repetitive
</figure>
<page confidence="0.639977">
1638
</page>
<figureCaption confidence="0.995627">
Figure 9: RNTN prediction of positive and negative (bottom right) sentences and their negation.
</figureCaption>
<figure confidence="0.997267100591718">
+ +
0
+
0
Roger
0
Dodger
+
0
.
0
is
+
0
+
one
0
of
+
+
0
0
the
+
0
most
0
on
0
this
0
0
theme
+
0
variations
+
compelling
–
0
–
0
Roger
0
Dodger
0
–
0
.
–
is
0
–
0
of
one
–
–
0
0
the
–
–
0
on
0
0
variations
+
compelling
–
least
0
this
0
theme
+
0
+
I
+
0
+
0
.
liked
0
0
0
0
0
0
every
single
0
minute
0
0
0
this
film
of
–
0
It
–
–
0
0
–
dull
0
’s
0
just
0
0
It
0
0
0
0
dull
0
not
definitely
+
0
’s
+
incredibly
0
0
a
of
0
0
this
film
single
minute
–
0
–
I
–
0
0
0
.
0
0
0
0
did
n’t
like
0
0
0
0
0
0
Model Accuracy
Negated Positive Negated Negative
biNB 19.0 27.3
RNN 33.3 45.5
MV-RNN 52.4 54.6
RNTN 71.4 81.8
</figure>
<figureCaption confidence="0.920178666666667">
Table 2: Accuracy of negation detection. Negated posi-
tive is measured as correct sentiment inversions. Negated
negative is measured as increases in positive activations.
</figureCaption>
<bodyText confidence="0.930689454545455">
curacy in terms of how often each model was able
to increase non-negative activation in the sentiment
of the sentence. Table 2 (right) shows the accuracy.
In over 81% of cases, the RNTN correctly increases
the positive activations. Fig. 9 (bottom right) shows
a typical case in which sentiment was made more
positive by switching the main class from negative
to neutral even though both not and dull were nega-
tive. Fig. 8 shows the changes in activation for both
sets. Negative values indicate a decrease in aver-
Negated Positive Sentences: Change in Activation
</bodyText>
<figureCaption confidence="0.9738064">
Figure 8: Change in activations for negations. Only the
RNTN correctly captures both types. It decreases positive
sentiment more when it is negated and learns that negat-
ing negative phrases (such as not terrible) should increase
neutral and positive activations.
</figureCaption>
<bodyText confidence="0.999927833333333">
age positive activation (for set 1) and positive values
mean an increase in average positive activation (set
2). The RNTN has the largest shifts in the correct di-
rections. Therefore we can conclude that the RNTN
is best able to identify the effect of negations upon
both positive and negative sentiment sentences.
</bodyText>
<figure confidence="0.9662257">
-0.6 -0.4 -0.2 0.0 0.2 0.4
Negated Negative Sentences: Change in Activation
-0.6 -0.4 -0.2 0.0 0.2 0.4
biNB
RRN
MV-RNN
RNTN
biNB
RRN
MV-RN
N
RNTN -0.57
-0.5
-0.34
-0.16
-0.01
-0.01
+0.01
+0.35
n Most positive n-grams Most negative n-grams
</figure>
<equation confidence="0.6995925">
1 engaging; best; powerful; love; beautiful bad; dull; boring; fails; worst; stupid; painfully
2 excellent performances; A masterpiece; masterful
</equation>
<bodyText confidence="0.694702875">
film; wonderful movie; marvelous performances
3 an amazing performance; wonderful all-ages tri-
umph; a wonderful movie; most visually stunning
5 nicely acted and beautifully shot; gorgeous im-
agery, effective performances; the best of the
year; a terrific American sports movie; refresh-
ingly honest and ultimately touching
8 one of the best films of the year; A love for films
shines through each frame; created a masterful
piece of artistry right here; A masterful film from
a master filmmaker,
worst movie; very bad; shapeless mess; worst
thing; instantly forgettable; complete failure
for worst movie; A lousy movie; a complete fail-
ure; most painfully marginal; very bad sign
silliest and most incoherent movie; completely
crass and forgettable movie; just another bad
movie. A cumbersome and cliche-ridden movie;
a humorless, disjointed mess
A trashy, exploitative, thoroughly unpleasant ex-
perience ; this sloppy drama is an empty ves-
sel.; quickly drags on becoming boring and pre-
dictable.; be the worst special-effects creation of
the year
</bodyText>
<tableCaption confidence="0.998159">
Table 3: Examples of n-grams for which the RNTN predicted the most positive and most negative responses.
</tableCaption>
<figure confidence="0.997956">
0.8
0.7
1 2 3 4 5 6 7 8 9 10
N-Gram Length
</figure>
<figureCaption confidence="0.996757">
Figure 10: Average ground truth sentiment of top 10 most
positive n-grams at various n. The RNTN correctly picks
the more negative and positive examples.
</figureCaption>
<subsectionHeader confidence="0.9507165">
5.5 Model Analysis: Most Positive and
Negative Phrases
</subsectionHeader>
<bodyText confidence="0.999925692307692">
We queried the model for its predictions on what
the most positive or negative n-grams are, measured
as the highest activation of the most negative and
most positive classes. Table 3 shows some phrases
from the dev set which the RNTN selected for their
strongest sentiment.
Due to lack of space we cannot compare top
phrases of the other models but Fig. 10 shows that
the RNTN selects more strongly positive phrases at
most n-gram lengths compared to other models.
For this and the previous experiment, please find
additional examples and descriptions in the supple-
mentary material.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99997275">
We introduced Recursive Neural Tensor Networks
and the Stanford Sentiment Treebank. The combi-
nation of new model and data results in a system
for single sentence sentiment detection that pushes
state of the art by 5.4% for positive/negative sen-
tence classification. Apart from this standard set-
ting, the dataset also poses important new challenges
and allows for new evaluation metrics. For instance,
the RNTN obtains 80.7% accuracy on fine-grained
sentiment prediction across all phrases and captures
negation of different sentiments and scope more ac-
curately than previous models.
</bodyText>
<sectionHeader confidence="0.997297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999675333333334">
We thank Rukmani Ravisundaram and Tayyab
Tariq for the first version of the online demo.
Richard is partly supported by a Microsoft Re-
search PhD fellowship. The authors gratefully ac-
knowledge the support of the Defense Advanced Re-
search Projects Agency (DARPA) Deep Exploration
and Filtering of Text (DEFT) Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-13-2-0040, the DARPA Deep Learning
program under contract number FA8650-10-C-7020
and NSF IIS-1159679. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect the view of DARPA, AFRL, or the US
government.
</bodyText>
<figure confidence="0.998719142857143">
Awage Ground Truth Sentiment
1.0
0.9
Model
RNTN
MV-RNN
RNN
</figure>
<page confidence="0.951115">
1640
</page>
<sectionHeader confidence="0.99572" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999416447619047">
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673–721.
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin.
2003. A neural probabilistic language model. J.
Mach. Learn. Res., 3, March.
D. Blakemore. 1989. Denial and contrast: A relevance
theoretic analysis of ‘but’. Linguistics and Philoso-
phy, 12:15–37.
L. Bottou. 2011. From machine learning to machine
reasoning. CoRR, abs/1102.1808.
S. Clark and S. Pulman. 2007. Combining symbolic and
distributional models of meaning. In Proceedings of
the AAAI Spring Symposium on Quantum Interaction,
pages 52–55.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: deep neural networks
with multitask learning. In ICML.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic op-
timization. JMLR, 12, July.
K. Erk and S. Pad´o. 2008. A structured vector space
model for word meaning in context. In EMNLP.
C. Goller and A. K¨uchler. 1996. Learning task-
dependent distributed representations by backpropaga-
tion through structure. In Proceedings of the Interna-
tional Conference on Neural Networks (ICNN-96).
E. Grefenstette and M. Sadrzadeh. 2011. Experimental
support for a categorical compositional distributional
model of meaning. In EMNLP.
E. Grefenstette, G. Dinu, Y.-Z. Zhang, M. Sadrzadeh, and
M. Baroni. 2013. Multi-step regression learning for
compositional distributional semantics. In IWCS.
G. E. Hinton. 1990. Mapping part-whole hierarchies into
connectionist networks. Artificial Intelligence, 46(1-
2).
L. R. Horn. 1989. A natural history of negation, volume
960. University of Chicago Press Chicago.
E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng.
2012. Improving Word Representations via Global
Context and Multiple Word Prototypes. In ACL.
M. Israel. 2001. Minimizers, maximizers, and the
rhetoric of scalar reasoning. Journal of Semantics,
18(4):297–331.
R. Jenatton, N. Le Roux, A. Bordes, and G. Obozinski.
2012. A latent factor model for highly multi-relational
data. In NIPS.
D. Klein and C. D. Manning. 2003. Accurate unlexical-
ized parsing. In ACL.
R. Lakoff. 1971. If’s, and’s, and but’s about conjunction.
In Charles J. Fillmore and D. Terence Langendoen, ed-
itors, Studies in Linguistic Semantics, pages 114–149.
Holt, Rinehart, and Winston, New York.
A. Merin. 1999. Information, relevance, and social deci-
sionmaking: Some principles and results of decision-
theoretic semantics. In Lawrence S. Moss, Jonathan
Ginzburg, and Maarten de Rijke, editors, Logic, Lan-
guage, and Information, volume 2. CSLI, Stanford,
CA.
J. Mitchell and M. Lapata. 2010. Composition in dis-
tributional models of semantics. Cognitive Science,
34(8):1388–1429.
K. Moilanen and S. Pulman. 2007. Sentiment composi-
tion. In In Proceedings of Recent Advances in Natural
Language Processing.
T. Nakagawa, K. Inui, and S. Kurohashi. 2010. Depen-
dency tree-based sentiment classification using CRFs
with hidden variables. In NAACL, HLT.
S. Pado and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161–199.
B. Pang and L. Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect
to rating scales. In ACL, pages 115–124.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1–135.
T. A. Plate. 1995. Holographic reduced representations.
IEEE Transactions on Neural Networks, 6(3):623–
641.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters. In W. Bruce Croft, James Shanahan, Yan Qu,
and Janyce Wiebe, editors, Computing Attitude and Af-
fect in Text: Theory and Applications, volume 20 of
The Information Retrieval Series, chapter 1.
J. B. Pollack. 1990. Recursive distributed representa-
tions. Artificial Intelligence, 46, November.
M. Ranzato and A. Krizhevsky G. E. Hinton. 2010.
Factored 3-Way Restricted Boltzmann Machines For
Modeling Natural Images. AISTATS.
V. Rentoumi, S. Petrakis, M. Klenner, G. A. Vouros, and
V. Karkaletsis. 2010. United we stand: Improving
sentiment analysis by joining machine learning and
rule based methods. In Proceedings of the Seventh
conference on International Language Resources and
Evaluation (LREC’10), Valletta, Malta.
S. Rudolph and E. Giesbrecht. 2010. Compositional
matrix-space models of language. In ACL.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the Good Grief algorithm. In HLT-NAACL.
R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learning
continuous phrase representations and syntactic pars-
ing with recursive neural networks. In Proceedings of
the NIPS-2010 Deep Learning and Unsupervised Fea-
ture Learning Workshop.
</reference>
<page confidence="0.806183">
1641
</page>
<reference confidence="0.998674222222222">
R. Socher, C. Lin, A. Y. Ng, and C.D. Manning. 2011a.
Parsing Natural Scenes and Natural Language with
Recursive Neural Networks. In ICML.
R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and
C. D. Manning. 2011b. Semi-Supervised Recursive
Autoencoders for Predicting Sentiment Distributions.
In EMNLP.
R. Socher, B. Huval, C. D. Manning, and A. Y. Ng. 2012.
Semantic compositionality through recursive matrix-
vector spaces. In EMNLP.
I. Sutskever, R. Salakhutdinov, and J. B. Tenenbaum.
2009. Modelling relational data using Bayesian clus-
tered tensor factorization. In NIPS.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141–188.
H. Wang, D. Can, A. Kazemzadeh, F. Bar, and
S. Narayanan. 2012. A system for real-time twit-
ter sentiment analysis of 2012 u.s. presidential elec-
tion cycle. In Proceedings of the ACL 2012 System
Demonstrations.
D. Widdows. 2008. Semantic vector products: Some ini-
tial investigations. In Proceedings of the Second AAAI
Symposium on Quantum Interaction.
A. Yessenalina and C. Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
EMNLP.
D. Yu, L. Deng, and F. Seide. 2012. Large vocabulary
speech recognition using deep tensor neural networks.
In INTERSPEECH.
F.M. Zanzotto, I. Korkontzelos, F. Fallucchi, and S. Man-
andhar. 2010. Estimating linear models for composi-
tional distributional semantics. In COLING.
L. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In UAI.
</reference>
<page confidence="0.99415">
1642
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.433341">
<title confidence="0.99637">Recursive Deep Models for Semantic Over a Sentiment Treebank</title>
<author confidence="0.9048015">Richard Socher</author>
<author confidence="0.9048015">Alex Perelygin</author>
<author confidence="0.9048015">Jean Y Wu</author>
<author confidence="0.9048015">Jason Christopher D Manning</author>
<author confidence="0.9048015">Andrew Y Ng</author>
<author confidence="0.9048015">Christopher</author>
<affiliation confidence="0.467238">Stanford University, Stanford, CA 94305, USA</affiliation>
<abstract confidence="0.999529115384615">Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>A Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="5627" citStr="Baroni and Lenci, 2010" startWordPosition="895" endWordPosition="898">junction ‘but’ dominates. The complete training and testing code, a live demo and the Stanford Sentiment Treebank dataset are available at http://nlp.stanford.edu/ sentiment. 2 Related Work This work is connected to five different areas of NLP research, each with their own large amount of related work to which we cannot do full justice given space constraints. Semantic Vector Spaces. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Variants of this idea use more complex frequencies such as how often a word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>M. Baroni and A. Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673–721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>3</volume>
<contexts>
<context position="6019" citStr="Bengio et al., 2003" startWordPosition="959" endWordPosition="962">nant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Variants of this idea use more complex frequencies such as how often a word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, mu</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. 2003. A neural probabilistic language model. J. Mach. Learn. Res., 3, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blakemore</author>
</authors>
<title>Denial and contrast: A relevance theoretic analysis of ‘but’. Linguistics and Philosophy,</title>
<date>1989</date>
<pages>12--15</pages>
<contexts>
<context position="27575" citStr="Blakemore, 1989" startWordPosition="4694" endWordPosition="4695">the coarse bag of words annotation for training, many of the more complex phenomena could not be captured, even by more powerful models. The combination of the new sentiment treebank and the RNTN pushes the state of the art on short phrases up to 85.4%. 5.3 Model Analysis: Contrastive Conjunction In this section, we use a subset of the test set which includes only sentences with an ‘X but Y ’ structure: A phrase X being followed by but which is followed by a phrase Y . The conjunction is interpreted as an argument for the second conjunct, with the first functioning concessively (Lakoff, 1971; Blakemore, 1989; Merin, 1999). Fig. 7 contains an example. We analyze a strict setting, where X and Y are phrases of different sentiment (including neutral). The example is counted as correct, if the classifications for both phrases X and Y are correct. Furthermore, the lowest node that dominates both of the word but and the node that spans Y also have to have the same correct sentiment. For the resulting 131 cases, the RNTN obtains an accuracy of 41% compared to MV-RNN (37), RNN (36) and biNB (27). 5.4 Model Analysis: High Level Negation We investigate two types of negation. For each type, we use a separate</context>
</contexts>
<marker>Blakemore, 1989</marker>
<rawString>D. Blakemore. 1989. Denial and contrast: A relevance theoretic analysis of ‘but’. Linguistics and Philosophy, 12:15–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bottou</author>
</authors>
<title>From machine learning to machine reasoning.</title>
<date>2011</date>
<location>CoRR, abs/1102.1808.</location>
<contexts>
<context position="8119" citStr="Bottou (2011)" startWordPosition="1277" endWordPosition="1279">12) both of which have been applied to bag of words sentiment corpora. Logical Form. A related field that tackles compositionality from a very different angle is that of trying to map sentences to logical form (Zettlemoyer and Collins, 2005). While these models are highly interesting and work well in closed domains and on discrete sets, they could only capture sentiment distributions using separate mechanisms beyond the currently used logical forms. Deep Learning. Apart from the above mentioned 1632 work on RNNs, several compositionality ideas related to neural networks have been discussed by Bottou (2011) and Hinton (1990) and first models such as Recursive Auto-associative memories been experimented with by Pollack (1990). The idea to relate inputs through three way interactions, parameterized by a tensor have been proposed for relation classification (Sutskever et al., 2009; Jenatton et al., 2012), extending Restricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al., 2012). Sentiment Analysis. Apart from the abovementioned work, most approaches in sentiment analysis use bag of words representations (Pang and Lee, 2008). Snyder and Barzil</context>
</contexts>
<marker>Bottou, 2011</marker>
<rawString>L. Bottou. 2011. From machine learning to machine reasoning. CoRR, abs/1102.1808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>S Pulman</author>
</authors>
<title>Combining symbolic and distributional models of meaning.</title>
<date>2007</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Quantum Interaction,</booktitle>
<pages>52--55</pages>
<contexts>
<context position="6805" citStr="Clark and Pulman, 2007" startWordPosition="1075" endWordPosition="1078">fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Giesbrecht, 2010) have not been experimentally validated on larger corpora. Yessenalina and Cardie (2011) compute matrix representations for longer phrases and define composition as matrix multiplication, and also evaluate on sentiment. Grefenstette and Sadrzadeh (2011) analyze subject-verbobject triplets and find a matrix-based categorical model to correlate well with human judgments. We compare to the recent line of work on supervised compositional models. In particular we will describe and experimentally compare our new RNTN mode</context>
</contexts>
<marker>Clark, Pulman, 2007</marker>
<rawString>S. Clark and S. Pulman. 2007. Combining symbolic and distributional models of meaning. In Proceedings of the AAAI Spring Symposium on Quantum Interaction, pages 52–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="6143" citStr="Collobert and Weston, 2008" startWordPosition="976" endWordPosition="979">ics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Variants of this idea use more complex frequencies such as how often a word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdo</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>E Hazan</author>
<author>Y Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>JMLR,</journal>
<volume>12</volume>
<contexts>
<context position="23490" citStr="Duchi et al., 2011" startWordPosition="4000" endWordPosition="4003"> δp2,com V [k] + k p1 Xd k=1 S = The children of p2, will then each take half of this vector and add their own softmax error message for the complete δ. In particular, we have δp1,com = δp1,s + δp2,down[d + 1 : 2d], where δp2,down[d + 1 : 2d] indicates that p1 is the right child of p2 and hence takes the 2nd half of the error, for the final word vector derivative for a, it will be δp2,down[1 : d]. The full derivative for slice V [k] for this trigram tree then is the sum at each node: � b � � b ~T Ep2 ∂V [k] + δp1,com , k c c and similarly for W. For this nonconvex optimization we use AdaGrad (Duchi et al., 2011) which converges in less than 3 hours to a local optimum. 5 Experiments We include two types of analyses. The first type includes several large quantitative evaluations on the test set. The second type focuses on two linguistic phenomena that are important in sentiment. For all models, we use the dev set and crossvalidate over regularization of the weights, word vector size as well as learning rate and minibatch size for AdaGrad. Optimal performance for all models was achieved at word vector sizes between 25 and 35 dimensions and batch sizes between 20 and 30. Performance decreased at larger o</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
<author>S Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<marker>Erk, Pad´o, 2008</marker>
<rawString>K. Erk and S. Pad´o. 2008. A structured vector space model for word meaning in context. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Goller</author>
<author>A K¨uchler</author>
</authors>
<title>Learning taskdependent distributed representations by backpropagation through structure.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Neural Networks (ICNN-96).</booktitle>
<marker>Goller, K¨uchler, 1996</marker>
<rawString>C. Goller and A. K¨uchler. 1996. Learning taskdependent distributed representations by backpropagation through structure. In Proceedings of the International Conference on Neural Networks (ICNN-96).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Grefenstette</author>
<author>M Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="7137" citStr="Grefenstette and Sadrzadeh (2011)" startWordPosition="1121" endWordPosition="1125">ions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Giesbrecht, 2010) have not been experimentally validated on larger corpora. Yessenalina and Cardie (2011) compute matrix representations for longer phrases and define composition as matrix multiplication, and also evaluate on sentiment. Grefenstette and Sadrzadeh (2011) analyze subject-verbobject triplets and find a matrix-based categorical model to correlate well with human judgments. We compare to the recent line of work on supervised compositional models. In particular we will describe and experimentally compare our new RNTN model to recursive neural networks (RNN) (Socher et al., 2011b) and matrix-vector RNNs (Socher et al., 2012) both of which have been applied to bag of words sentiment corpora. Logical Form. A related field that tackles compositionality from a very different angle is that of trying to map sentences to logical form (Zettlemoyer and Coll</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>E. Grefenstette and M. Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Grefenstette</author>
<author>G Dinu</author>
<author>Y-Z Zhang</author>
<author>M Sadrzadeh</author>
<author>M Baroni</author>
</authors>
<title>Multi-step regression learning for compositional distributional semantics.</title>
<date>2013</date>
<booktitle>In IWCS.</booktitle>
<contexts>
<context position="1855" citStr="Grefenstette et al., 2013" startWordPosition="268" endWordPosition="271">mprovement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases. 1 Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010). Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and Figure 1: Example of the Recursive Neural Tensor Network accurately predicting 5 sentiment classes, very negative to very positive (– –, –, 0, +, + +), at every node of a parse tree and capturing the negation and its scope in this sentence. models to accurately capture the underlying phenomena presented in such data. To address this need, we introduce the Stanford Sentiment Treebank and a powerful Recursive Neural Tensor Network that can accurately predict the compositional semantic effects</context>
</contexts>
<marker>Grefenstette, Dinu, Zhang, Sadrzadeh, Baroni, 2013</marker>
<rawString>E. Grefenstette, G. Dinu, Y.-Z. Zhang, M. Sadrzadeh, and M. Baroni. 2013. Multi-step regression learning for compositional distributional semantics. In IWCS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
</authors>
<title>Mapping part-whole hierarchies into connectionist networks.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<pages>46--1</pages>
<contexts>
<context position="8137" citStr="Hinton (1990)" startWordPosition="1281" endWordPosition="1282">have been applied to bag of words sentiment corpora. Logical Form. A related field that tackles compositionality from a very different angle is that of trying to map sentences to logical form (Zettlemoyer and Collins, 2005). While these models are highly interesting and work well in closed domains and on discrete sets, they could only capture sentiment distributions using separate mechanisms beyond the currently used logical forms. Deep Learning. Apart from the above mentioned 1632 work on RNNs, several compositionality ideas related to neural networks have been discussed by Bottou (2011) and Hinton (1990) and first models such as Recursive Auto-associative memories been experimented with by Pollack (1990). The idea to relate inputs through three way interactions, parameterized by a tensor have been proposed for relation classification (Sutskever et al., 2009; Jenatton et al., 2012), extending Restricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al., 2012). Sentiment Analysis. Apart from the abovementioned work, most approaches in sentiment analysis use bag of words representations (Pang and Lee, 2008). Snyder and Barzilay (2007) analyzed</context>
</contexts>
<marker>Hinton, 1990</marker>
<rawString>G. E. Hinton. 1990. Mapping part-whole hierarchies into connectionist networks. Artificial Intelligence, 46(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Horn</author>
</authors>
<title>A natural history of negation, volume 960.</title>
<date>1989</date>
<institution>University of Chicago Press Chicago.</institution>
<contexts>
<context position="29520" citStr="Horn, 1989" startWordPosition="5021" endWordPosition="5022">ences. But what if the model simply makes phrases very negative when negation is in the sentence? The next experiments show that the model captures more than such a simplistic negation rule. Set 2: Negating Negative Sentences. The second set contains negative sentences and their negation. When negative sentences are negated, the sentiment treebank shows that overall sentiment should become less negative, but not necessarily positive. For instance, ‘The movie was terrible’ is negative but the ‘The movie was not terrible’ says only that it was less bad than a terrible one, not that it was good (Horn, 1989; Israel, 2001). Hence, we evaluate ac– 0 There slow – – – 0 and + 0 it 0 – 0 parts + + 0 + it interesting 0 has 0 just 0 + + 0 enough 0 + + spice + 0 to 0 0 keep + – 0 are 0 0 but repetitive 1638 Figure 9: RNTN prediction of positive and negative (bottom right) sentences and their negation. + + 0 + 0 Roger 0 Dodger + 0 . 0 is + 0 + one 0 of + + 0 0 the + 0 most 0 on 0 this 0 0 theme + 0 variations + compelling – 0 – 0 Roger 0 Dodger 0 – 0 . – is 0 – 0 of one – – 0 0 the – – 0 on 0 0 variations + compelling – least 0 this 0 theme + 0 + I + 0 + 0 . liked 0 0 0 0 0 0 every single 0 minute 0 0 0 </context>
</contexts>
<marker>Horn, 1989</marker>
<rawString>L. R. Horn. 1989. A natural history of negation, volume 960. University of Chicago Press Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Huang</author>
<author>R Socher</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Improving Word Representations via Global Context and Multiple Word Prototypes.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6164" citStr="Huang et al., 2012" startWordPosition="980" endWordPosition="983">t are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Variants of this idea use more complex frequencies such as how often a word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-c</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng. 2012. Improving Word Representations via Global Context and Multiple Word Prototypes. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Israel</author>
</authors>
<title>Minimizers, maximizers, and the rhetoric of scalar reasoning.</title>
<date>2001</date>
<journal>Journal of Semantics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="29535" citStr="Israel, 2001" startWordPosition="5023" endWordPosition="5024">hat if the model simply makes phrases very negative when negation is in the sentence? The next experiments show that the model captures more than such a simplistic negation rule. Set 2: Negating Negative Sentences. The second set contains negative sentences and their negation. When negative sentences are negated, the sentiment treebank shows that overall sentiment should become less negative, but not necessarily positive. For instance, ‘The movie was terrible’ is negative but the ‘The movie was not terrible’ says only that it was less bad than a terrible one, not that it was good (Horn, 1989; Israel, 2001). Hence, we evaluate ac– 0 There slow – – – 0 and + 0 it 0 – 0 parts + + 0 + it interesting 0 has 0 just 0 + + 0 enough 0 + + spice + 0 to 0 0 keep + – 0 are 0 0 but repetitive 1638 Figure 9: RNTN prediction of positive and negative (bottom right) sentences and their negation. + + 0 + 0 Roger 0 Dodger + 0 . 0 is + 0 + one 0 of + + 0 0 the + 0 most 0 on 0 this 0 0 theme + 0 variations + compelling – 0 – 0 Roger 0 Dodger 0 – 0 . – is 0 – 0 of one – – 0 0 the – – 0 on 0 0 variations + compelling – least 0 this 0 theme + 0 + I + 0 + 0 . liked 0 0 0 0 0 0 every single 0 minute 0 0 0 this film of – </context>
</contexts>
<marker>Israel, 2001</marker>
<rawString>M. Israel. 2001. Minimizers, maximizers, and the rhetoric of scalar reasoning. Journal of Semantics, 18(4):297–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jenatton</author>
<author>N Le Roux</author>
<author>A Bordes</author>
<author>G Obozinski</author>
</authors>
<title>A latent factor model for highly multi-relational data.</title>
<date>2012</date>
<booktitle>In NIPS.</booktitle>
<marker>Jenatton, Le Roux, Bordes, Obozinski, 2012</marker>
<rawString>R. Jenatton, N. Le Roux, A. Bordes, and G. Obozinski. 2012. A latent factor model for highly multi-relational data. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2861" citStr="Klein and Manning, 2003" startWordPosition="434" endWordPosition="437">erlying phenomena presented in such data. To address this need, we introduce the Stanford Sentiment Treebank and a powerful Recursive Neural Tensor Network that can accurately predict the compositional semantic effects present in this new corpus. The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges. This new dataset allows us to analyze the intricacies of sentiment and to capture complex linguistic phenomena. Fig. 1 shows one of the many examples with clear compositional structure. The granularity and size of 0 This 0 0 does 0 film – 0 n’t + cleverness – + care – 0 about + 0 + 0 – 0 wit + + or 0 0 any 0 0 other + 0 0 + kind + + intelligent of 0 + + + + humor 1631 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washingto</context>
<context position="10824" citStr="Klein and Manning, 2003" startWordPosition="1696" endWordPosition="1700">Pang and Lee (2005). The original dataset includes 10,662 sennerdy folks Figure 3: The labeling interface. Random phrases were shown and annotators had a slider for selecting the sentiment and its degree. tences, half of which were considered positive and the other half negative. Each label is extracted from a longer movie review and reflects the writer’s overall intention for this review. The normalized, lowercased text is first used to recover, from the original website, the text with capitalization. Remaining HTML tags and sentences that are not in English are deleted. The Stanford Parser (Klein and Manning, 2003) is used to parses all 10,662 sentences. In approximately 1,100 cases it splits the snippet into multiple sentences. We then used Amazon Mechanical Turk to label the resulting 215,154 phrases. Fig. 3 shows the interface annotators saw. The slider has 25 different values and is initially set to neutral. The phrases in each hit are randomly sampled from the set of all phrases in order to prevent labels being influenced by what follows. For more details on the dataset collection, see supplementary material. Fig. 2 shows the normalized label distributions at each n-gram length. Starting at length </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Accurate unlexicalized parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lakoff</author>
</authors>
<title>If’s, and’s, and but’s about conjunction.</title>
<date>1971</date>
<booktitle>Studies in Linguistic Semantics,</booktitle>
<pages>114--149</pages>
<editor>In Charles J. Fillmore and D. Terence Langendoen, editors,</editor>
<location>Holt, Rinehart, and Winston, New York.</location>
<contexts>
<context position="27558" citStr="Lakoff, 1971" startWordPosition="4692" endWordPosition="4693">, 2012). With the coarse bag of words annotation for training, many of the more complex phenomena could not be captured, even by more powerful models. The combination of the new sentiment treebank and the RNTN pushes the state of the art on short phrases up to 85.4%. 5.3 Model Analysis: Contrastive Conjunction In this section, we use a subset of the test set which includes only sentences with an ‘X but Y ’ structure: A phrase X being followed by but which is followed by a phrase Y . The conjunction is interpreted as an argument for the second conjunct, with the first functioning concessively (Lakoff, 1971; Blakemore, 1989; Merin, 1999). Fig. 7 contains an example. We analyze a strict setting, where X and Y are phrases of different sentiment (including neutral). The example is counted as correct, if the classifications for both phrases X and Y are correct. Furthermore, the lowest node that dominates both of the word but and the node that spans Y also have to have the same correct sentiment. For the resulting 131 cases, the RNTN obtains an accuracy of 41% compared to MV-RNN (37), RNN (36) and biNB (27). 5.4 Model Analysis: High Level Negation We investigate two types of negation. For each type, </context>
</contexts>
<marker>Lakoff, 1971</marker>
<rawString>R. Lakoff. 1971. If’s, and’s, and but’s about conjunction. In Charles J. Fillmore and D. Terence Langendoen, editors, Studies in Linguistic Semantics, pages 114–149. Holt, Rinehart, and Winston, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Merin</author>
</authors>
<title>Information, relevance, and social decisionmaking: Some principles and results of decisiontheoretic semantics.</title>
<date>1999</date>
<booktitle>Logic, Language, and Information,</booktitle>
<volume>2</volume>
<editor>In Lawrence S. Moss, Jonathan Ginzburg, and Maarten de Rijke, editors,</editor>
<publisher>CSLI,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="27589" citStr="Merin, 1999" startWordPosition="4696" endWordPosition="4697"> words annotation for training, many of the more complex phenomena could not be captured, even by more powerful models. The combination of the new sentiment treebank and the RNTN pushes the state of the art on short phrases up to 85.4%. 5.3 Model Analysis: Contrastive Conjunction In this section, we use a subset of the test set which includes only sentences with an ‘X but Y ’ structure: A phrase X being followed by but which is followed by a phrase Y . The conjunction is interpreted as an argument for the second conjunct, with the first functioning concessively (Lakoff, 1971; Blakemore, 1989; Merin, 1999). Fig. 7 contains an example. We analyze a strict setting, where X and Y are phrases of different sentiment (including neutral). The example is counted as correct, if the classifications for both phrases X and Y are correct. Furthermore, the lowest node that dominates both of the word but and the node that spans Y also have to have the same correct sentiment. For the resulting 131 cases, the RNTN obtains an accuracy of 41% compared to MV-RNN (37), RNN (36) and biNB (27). 5.4 Model Analysis: High Level Negation We investigate two types of negation. For each type, we use a separate dataset for e</context>
</contexts>
<marker>Merin, 1999</marker>
<rawString>A. Merin. 1999. Information, relevance, and social decisionmaking: Some principles and results of decisiontheoretic semantics. In Lawrence S. Moss, Jonathan Ginzburg, and Maarten de Rijke, editors, Logic, Language, and Information, volume 2. CSLI, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mitchell</author>
<author>M Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="1732" citStr="Mitchell and Lapata, 2010" startWordPosition="248" endWordPosition="251">ication from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases. 1 Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010). Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and Figure 1: Example of the Recursive Neural Tensor Network accurately predicting 5 sentiment classes, very negative to very positive (– –, –, 0, +, + +), at every node of a parse tree and capturing the negation and its scope in this sentence. models to accurately capture the underlying phenomena presented in such data. To address this need, we introduce the Stanford Senti</context>
<context position="6536" citStr="Mitchell and Lapata (2010)" startWordPosition="1037" endWordPosition="1041">ten have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Giesbrecht, 2010) have not been experimentally validated on larger corpora. Yessenalina and Cardie (2011) compute matrix representations for longer phrases and define composition as matrix multiplication, and also evaluate on sentiment. Grefenstette and Sadrzadeh (2011</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>J. Mitchell and M. Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Moilanen</author>
<author>S Pulman</author>
</authors>
<title>Sentiment composition. In</title>
<date>2007</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing.</booktitle>
<contexts>
<context position="9057" citStr="Moilanen and Pulman, 2007" startWordPosition="1416" endWordPosition="1419">estricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al., 2012). Sentiment Analysis. Apart from the abovementioned work, most approaches in sentiment analysis use bag of words representations (Pang and Lee, 2008). Snyder and Barzilay (2007) analyzed larger reviews in more detail by analyzing the sentiment of multiple aspects of restaurants, such as food or atmosphere. Several works have explored sentiment compositionality through careful engineering of features or polarity shifting rules on syntactic structures (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Rentoumi et al., 2010; Nakagawa et al., 2010). 3 Stanford Sentiment Treebank Bag of words classifiers can work well in longer documents by relying on a few words with strong sentiment like ‘awesome’ or ‘exhilarating.’ However, sentiment accuracies even for binary positive/negative classification for single sentences has not exceeded 80% for several years. For the more difficult multiclass case including a neutral class, accuracy is often below 60% for short messages on Twitter (Wang et al., 2012). From a linguistic or cognitive standpoint, ignoring word order in the treatment of a semantic t</context>
</contexts>
<marker>Moilanen, Pulman, 2007</marker>
<rawString>K. Moilanen and S. Pulman. 2007. Sentiment composition. In In Proceedings of Recent Advances in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nakagawa</author>
<author>K Inui</author>
<author>S Kurohashi</author>
</authors>
<title>Dependency tree-based sentiment classification using CRFs with hidden variables.</title>
<date>2010</date>
<booktitle>In NAACL, HLT.</booktitle>
<contexts>
<context position="9104" citStr="Nakagawa et al., 2010" startWordPosition="1424" endWordPosition="1427">2010) and as a special layer for speech recognition (Yu et al., 2012). Sentiment Analysis. Apart from the abovementioned work, most approaches in sentiment analysis use bag of words representations (Pang and Lee, 2008). Snyder and Barzilay (2007) analyzed larger reviews in more detail by analyzing the sentiment of multiple aspects of restaurants, such as food or atmosphere. Several works have explored sentiment compositionality through careful engineering of features or polarity shifting rules on syntactic structures (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Rentoumi et al., 2010; Nakagawa et al., 2010). 3 Stanford Sentiment Treebank Bag of words classifiers can work well in longer documents by relying on a few words with strong sentiment like ‘awesome’ or ‘exhilarating.’ However, sentiment accuracies even for binary positive/negative classification for single sentences has not exceeded 80% for several years. For the more difficult multiclass case including a neutral class, accuracy is often below 60% for short messages on Twitter (Wang et al., 2012). From a linguistic or cognitive standpoint, ignoring word order in the treatment of a semantic task is not plausible, and, as we will show, it </context>
</contexts>
<marker>Nakagawa, Inui, Kurohashi, 2010</marker>
<rawString>T. Nakagawa, K. Inui, and S. Kurohashi. 2010. Dependency tree-based sentiment classification using CRFs with hidden variables. In NAACL, HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pado</author>
<author>M Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="5782" citStr="Pado and Lapata, 2007" startWordPosition="922" endWordPosition="925">d.edu/ sentiment. 2 Related Work This work is connected to five different areas of NLP research, each with their own large amount of related work to which we cannot do full justice given space constraints. Semantic Vector Spaces. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Variants of this idea use more complex frequencies such as how often a word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus.</context>
</contexts>
<marker>Pado, Lapata, 2007</marker>
<rawString>S. Pado and M. Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="2726" citStr="Pang and Lee (2005)" startWordPosition="413" endWordPosition="416"> +), at every node of a parse tree and capturing the negation and its scope in this sentence. models to accurately capture the underlying phenomena presented in such data. To address this need, we introduce the Stanford Sentiment Treebank and a powerful Recursive Neural Tensor Network that can accurately predict the compositional semantic effects present in this new corpus. The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges. This new dataset allows us to analyze the intricacies of sentiment and to capture complex linguistic phenomena. Fig. 1 shows one of the many examples with clear compositional structure. The granularity and size of 0 This 0 0 does 0 film – 0 n’t + cleverness – + care – 0 about + 0 + 0 – 0 wit + + or 0 0 any 0 0 other + 0 0 + kind + + intelligent of 0 + + + +</context>
<context position="10219" citStr="Pang and Lee (2005)" startWordPosition="1597" endWordPosition="1600">int, ignoring word order in the treatment of a semantic task is not plausible, and, as we will show, it cannot accurately classify hard examples of negation. Correctly predicting these hard cases is necessary to further improve performance. In this section we will introduce and provide some analyses for the new Sentiment Treebank which includes labels for every syntactically plausible phrase in thousands of sentences, allowing us to train and evaluate compositional models. We consider the corpus of movie review excerpts from the rottentomatoes.com website originally collected and published by Pang and Lee (2005). The original dataset includes 10,662 sennerdy folks Figure 3: The labeling interface. Random phrases were shown and annotators had a slider for selecting the sentiment and its degree. tences, half of which were considered positive and the other half negative. Each label is extracted from a longer movie review and reflects the writer’s overall intention for this review. The normalized, lowercased text is first used to recover, from the original website, the text with capitalization. Remaining HTML tags and sentences that are not in English are deleted. The Stanford Parser (Klein and Manning, </context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>B. Pang and L. Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="8700" citStr="Pang and Lee, 2008" startWordPosition="1365" endWordPosition="1368">s have been discussed by Bottou (2011) and Hinton (1990) and first models such as Recursive Auto-associative memories been experimented with by Pollack (1990). The idea to relate inputs through three way interactions, parameterized by a tensor have been proposed for relation classification (Sutskever et al., 2009; Jenatton et al., 2012), extending Restricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al., 2012). Sentiment Analysis. Apart from the abovementioned work, most approaches in sentiment analysis use bag of words representations (Pang and Lee, 2008). Snyder and Barzilay (2007) analyzed larger reviews in more detail by analyzing the sentiment of multiple aspects of restaurants, such as food or atmosphere. Several works have explored sentiment compositionality through careful engineering of features or polarity shifting rules on syntactic structures (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Rentoumi et al., 2010; Nakagawa et al., 2010). 3 Stanford Sentiment Treebank Bag of words classifiers can work well in longer documents by relying on a few words with strong sentiment like ‘awesome’ or ‘exhilarating.’ However, sentiment accu</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>B. Pang and L. Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A Plate</author>
</authors>
<title>Holographic reduced representations.</title>
<date>1995</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<volume>6</volume>
<issue>3</issue>
<pages>641</pages>
<contexts>
<context position="6721" citStr="Plate, 1995" startWordPosition="1066" endWordPosition="1067">rities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Giesbrecht, 2010) have not been experimentally validated on larger corpora. Yessenalina and Cardie (2011) compute matrix representations for longer phrases and define composition as matrix multiplication, and also evaluate on sentiment. Grefenstette and Sadrzadeh (2011) analyze subject-verbobject triplets and find a matrix-based categorical model to correlate well with human judgments. We compare to the recent line of work on supervised compositional</context>
</contexts>
<marker>Plate, 1995</marker>
<rawString>T. A. Plate. 1995. Holographic reduced representations. IEEE Transactions on Neural Networks, 6(3):623– 641.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Polanyi</author>
<author>A Zaenen</author>
</authors>
<title>Contextual valence shifters. In</title>
<date>2006</date>
<booktitle>Computing Attitude and Affect in Text: Theory and Applications, volume 20 of The Information Retrieval Series, chapter 1.</booktitle>
<editor>W. Bruce Croft, James Shanahan, Yan Qu, and Janyce Wiebe, editors,</editor>
<contexts>
<context position="9030" citStr="Polanyi and Zaenen, 2006" startWordPosition="1412" endWordPosition="1415">et al., 2012), extending Restricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al., 2012). Sentiment Analysis. Apart from the abovementioned work, most approaches in sentiment analysis use bag of words representations (Pang and Lee, 2008). Snyder and Barzilay (2007) analyzed larger reviews in more detail by analyzing the sentiment of multiple aspects of restaurants, such as food or atmosphere. Several works have explored sentiment compositionality through careful engineering of features or polarity shifting rules on syntactic structures (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Rentoumi et al., 2010; Nakagawa et al., 2010). 3 Stanford Sentiment Treebank Bag of words classifiers can work well in longer documents by relying on a few words with strong sentiment like ‘awesome’ or ‘exhilarating.’ However, sentiment accuracies even for binary positive/negative classification for single sentences has not exceeded 80% for several years. For the more difficult multiclass case including a neutral class, accuracy is often below 60% for short messages on Twitter (Wang et al., 2012). From a linguistic or cognitive standpoint, ignoring word order in th</context>
</contexts>
<marker>Polanyi, Zaenen, 2006</marker>
<rawString>L. Polanyi and A. Zaenen. 2006. Contextual valence shifters. In W. Bruce Croft, James Shanahan, Yan Qu, and Janyce Wiebe, editors, Computing Attitude and Affect in Text: Theory and Applications, volume 20 of The Information Retrieval Series, chapter 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Pollack</author>
</authors>
<title>Recursive distributed representations.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<volume>46</volume>
<contexts>
<context position="8239" citStr="Pollack (1990)" startWordPosition="1296" endWordPosition="1297">sitionality from a very different angle is that of trying to map sentences to logical form (Zettlemoyer and Collins, 2005). While these models are highly interesting and work well in closed domains and on discrete sets, they could only capture sentiment distributions using separate mechanisms beyond the currently used logical forms. Deep Learning. Apart from the above mentioned 1632 work on RNNs, several compositionality ideas related to neural networks have been discussed by Bottou (2011) and Hinton (1990) and first models such as Recursive Auto-associative memories been experimented with by Pollack (1990). The idea to relate inputs through three way interactions, parameterized by a tensor have been proposed for relation classification (Sutskever et al., 2009; Jenatton et al., 2012), extending Restricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al., 2012). Sentiment Analysis. Apart from the abovementioned work, most approaches in sentiment analysis use bag of words representations (Pang and Lee, 2008). Snyder and Barzilay (2007) analyzed larger reviews in more detail by analyzing the sentiment of multiple aspects of restaurants, such as </context>
<context position="16307" citStr="Pollack, 1990" startWordPosition="2630" endWordPosition="2632">lement-wise nonlinearity, W E Rdx2d is the main parameter to learn and we omit the bias for simplicity. The bias can be added as an extra column to W if an additional 1 is added to the concatenation of the input vectors. The parent vectors must be of the same dimensionality to be recursively compatible and be used as input to the next composition. Each parent vector pi, is given to the same softmax classifier of Eq. 1 to compute its label probabilities. This model uses the same compositionality function as the recursive autoencoder (Socher et al., 2011b) and recursive auto-associate memories (Pollack, 1990). The only difference to the former model is that we fix the tree structures and ignore the reconstruction loss. In initial experiments, we found that with the additional amount of training data, the reconstruction loss at each node is not necessary to obtain high performance. 4.2 MV-RNN: Matrix-Vector RNN The MV-RNN is linguistically motivated in that most of the parameters are associated with words and each composition function that computes vectors for longer phrases depends on the actual words being combined. The main idea of the MV-RNN (Socher et al., 2012) is to represent every word and </context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>J. B. Pollack. 1990. Recursive distributed representations. Artificial Intelligence, 46, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ranzato</author>
<author>A Krizhevsky G E Hinton</author>
</authors>
<title>Factored 3-Way Restricted Boltzmann Machines For Modeling Natural Images.</title>
<date>2010</date>
<publisher>AISTATS.</publisher>
<contexts>
<context position="8487" citStr="Ranzato and Hinton, 2010" startWordPosition="1330" endWordPosition="1333">apture sentiment distributions using separate mechanisms beyond the currently used logical forms. Deep Learning. Apart from the above mentioned 1632 work on RNNs, several compositionality ideas related to neural networks have been discussed by Bottou (2011) and Hinton (1990) and first models such as Recursive Auto-associative memories been experimented with by Pollack (1990). The idea to relate inputs through three way interactions, parameterized by a tensor have been proposed for relation classification (Sutskever et al., 2009; Jenatton et al., 2012), extending Restricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al., 2012). Sentiment Analysis. Apart from the abovementioned work, most approaches in sentiment analysis use bag of words representations (Pang and Lee, 2008). Snyder and Barzilay (2007) analyzed larger reviews in more detail by analyzing the sentiment of multiple aspects of restaurants, such as food or atmosphere. Several works have explored sentiment compositionality through careful engineering of features or polarity shifting rules on syntactic structures (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Rentoumi et al., 2010; Nakag</context>
</contexts>
<marker>Ranzato, Hinton, 2010</marker>
<rawString>M. Ranzato and A. Krizhevsky G. E. Hinton. 2010. Factored 3-Way Restricted Boltzmann Machines For Modeling Natural Images. AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rentoumi</author>
<author>S Petrakis</author>
<author>M Klenner</author>
<author>G A Vouros</author>
<author>V Karkaletsis</author>
</authors>
<title>United we stand: Improving sentiment analysis by joining machine learning and rule based methods.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<location>Valletta,</location>
<contexts>
<context position="9080" citStr="Rentoumi et al., 2010" startWordPosition="1420" endWordPosition="1423">s (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al., 2012). Sentiment Analysis. Apart from the abovementioned work, most approaches in sentiment analysis use bag of words representations (Pang and Lee, 2008). Snyder and Barzilay (2007) analyzed larger reviews in more detail by analyzing the sentiment of multiple aspects of restaurants, such as food or atmosphere. Several works have explored sentiment compositionality through careful engineering of features or polarity shifting rules on syntactic structures (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Rentoumi et al., 2010; Nakagawa et al., 2010). 3 Stanford Sentiment Treebank Bag of words classifiers can work well in longer documents by relying on a few words with strong sentiment like ‘awesome’ or ‘exhilarating.’ However, sentiment accuracies even for binary positive/negative classification for single sentences has not exceeded 80% for several years. For the more difficult multiclass case including a neutral class, accuracy is often below 60% for short messages on Twitter (Wang et al., 2012). From a linguistic or cognitive standpoint, ignoring word order in the treatment of a semantic task is not plausible, a</context>
</contexts>
<marker>Rentoumi, Petrakis, Klenner, Vouros, Karkaletsis, 2010</marker>
<rawString>V. Rentoumi, S. Petrakis, M. Klenner, G. A. Vouros, and V. Karkaletsis. 2010. United we stand: Improving sentiment analysis by joining machine learning and rule based methods. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Rudolph</author>
<author>E Giesbrecht</author>
</authors>
<title>Compositional matrix-space models of language.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6884" citStr="Rudolph and Giesbrecht, 2010" startWordPosition="1086" endWordPosition="1089">her et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Giesbrecht, 2010) have not been experimentally validated on larger corpora. Yessenalina and Cardie (2011) compute matrix representations for longer phrases and define composition as matrix multiplication, and also evaluate on sentiment. Grefenstette and Sadrzadeh (2011) analyze subject-verbobject triplets and find a matrix-based categorical model to correlate well with human judgments. We compare to the recent line of work on supervised compositional models. In particular we will describe and experimentally compare our new RNTN model to recursive neural networks (RNN) (Socher et al., 2011b) and matrix-vector R</context>
</contexts>
<marker>Rudolph, Giesbrecht, 2010</marker>
<rawString>S. Rudolph and E. Giesbrecht. 2010. Compositional matrix-space models of language. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>R Barzilay</author>
</authors>
<title>Multiple aspect ranking using the Good Grief algorithm.</title>
<date>2007</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="8728" citStr="Snyder and Barzilay (2007)" startWordPosition="1369" endWordPosition="1372"> by Bottou (2011) and Hinton (1990) and first models such as Recursive Auto-associative memories been experimented with by Pollack (1990). The idea to relate inputs through three way interactions, parameterized by a tensor have been proposed for relation classification (Sutskever et al., 2009; Jenatton et al., 2012), extending Restricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al., 2012). Sentiment Analysis. Apart from the abovementioned work, most approaches in sentiment analysis use bag of words representations (Pang and Lee, 2008). Snyder and Barzilay (2007) analyzed larger reviews in more detail by analyzing the sentiment of multiple aspects of restaurants, such as food or atmosphere. Several works have explored sentiment compositionality through careful engineering of features or polarity shifting rules on syntactic structures (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Rentoumi et al., 2010; Nakagawa et al., 2010). 3 Stanford Sentiment Treebank Bag of words classifiers can work well in longer documents by relying on a few words with strong sentiment like ‘awesome’ or ‘exhilarating.’ However, sentiment accuracies even for binary posit</context>
</contexts>
<marker>Snyder, Barzilay, 2007</marker>
<rawString>B. Snyder and R. Barzilay. 2007. Multiple aspect ranking using the Good Grief algorithm. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</booktitle>
<contexts>
<context position="1753" citStr="Socher et al., 2010" startWordPosition="252" endWordPosition="255">%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases. 1 Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010). Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and Figure 1: Example of the Recursive Neural Tensor Network accurately predicting 5 sentiment classes, very negative to very positive (– –, –, 0, +, + +), at every node of a parse tree and capturing the negation and its scope in this sentence. models to accurately capture the underlying phenomena presented in such data. To address this need, we introduce the Stanford Sentiment Treebank and a p</context>
<context position="22231" citStr="Socher et al., 2010" startWordPosition="3722" endWordPosition="3725"> f� is the element-wise derivative of f which in the standard case of using f = tanh can be computed using only f(xi). The remaining derivatives can only be computed in a top-down fashion from the top node through the tree and into the leaf nodes. The full derivative for V and W is the sum of the derivatives at each of the nodes. We define the complete incoming error messages for a node i as δi,com. The top node, in our case p2, only received errors from the top node’s softmax. Hence, δp2,com = δp2,s which we can use to obtain the standard backprop derivative for W (Goller and K¨uchler, 1996; Socher et al., 2010). For the derivative of each slice k = 1, ... , d, we get: = δp2,com , k p1 p1 � a � � a � T where δp2,com is just the k’th element of this vector. k Now, we can compute the error message for the two Neural Tensor Layer p = f p = f Slices of Standard Tensor Layer Layer b c T V[1:2] b c + W + b c p1 = f ∂Ep2 ∂V [k] 1636 Model Fine-grained Positive/Negative All Root All Root NB 67.2 41.0 82.6 81.8 SVM 64.3 40.7 84.6 79.4 BiNB 71.0 41.9 82.7 83.1 VecAvg 73.3 32.7 85.1 80.1 RNN 79.0 43.2 86.1 82.4 MV-RNN 78.7 44.4 86.8 82.9 RNTN 80.7 45.7 87.6 85.4 children of p2: δp2,down =CWT δp2,com + S� ®f / \</context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>C Lin</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Parsing Natural Scenes and Natural Language with Recursive Neural Networks.</title>
<date>2011</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="4392" citStr="Socher et al., 2011" startWordPosition="697" endWordPosition="700"> better capture sentiment from short comments, such as Twitter data, which provide less overall signal per document. In order to capture the compositional effects with higher accuracy, we propose a new model called the Recursive Neural Tensor Network (RNTN). Recursive Neural Tensor Networks take as input phrases of any length. They represent a phrase through word vectors and a parse tree and then compute vectors for higher nodes in the tree using the same tensor-based composition function. We compare to several supervised, compositional models such as standard recursive neural networks (RNN) (Socher et al., 2011b), matrix-vector RNNs (Socher et al., 2012), and baselines such as neural networks that ignore word order, Naive Bayes (NB), bi-gram NB and SVM. All models get a significant boost when trained with the new dataset but the RNTN obtains the highest performance with 80.7% accuracy when predicting finegrained sentiment for all nodes. Lastly, we use a test set of positive and negative sentences and their respective negations to show that, unlike bag of words models, the RNTN accurately captures the sentiment change and scope of negation. RNTNs also learn that sentiment of phrases following the con</context>
<context position="6271" citStr="Socher et al., 2011" startWordPosition="999" endWordPosition="1002">s of this idea use more complex frequencies such as how often a word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Gies</context>
<context position="15345" citStr="Socher et al., 2011" startWordPosition="2454" endWordPosition="2457"> compositionality function g and use node vectors as features for a classifier at that node. This function varies for the different models. labels given the word vector via: y°&apos; = softmax(Wsa), (1) where Ws E R5xd is the sentiment classification matrix. For the given tri-gram, this is repeated for vectors b and c. The main task of and difference between the models will be to compute the hidden vectors pi E Rd in a bottom up fashion. 4.1 RNN: Recursive Neural Network The simplest member of this family of neural network models is the standard recursive neural network (Goller and K¨uchler, 1996; Socher et al., 2011a). First, it is determined which parent already has all its children computed. In the above tree example, p1 has its two children’s vectors since both are words. RNNs use the following equations to compute the parent vectors: not very good ... a b c 0 0 + p2 = g(a,p1) - p1 =g(b,c) + 1634 p1 = f(W[c]l,p2 = f(W[a1]), where f = tanh is a standard element-wise nonlinearity, W E Rdx2d is the main parameter to learn and we omit the bias for simplicity. The bias can be added as an extra column to W if an additional 1 is added to the concatenation of the input vectors. The parent vectors must be of t</context>
</contexts>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>R. Socher, C. Lin, A. Y. Ng, and C.D. Manning. 2011a. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>J Pennington</author>
<author>E H Huang</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="4392" citStr="Socher et al., 2011" startWordPosition="697" endWordPosition="700"> better capture sentiment from short comments, such as Twitter data, which provide less overall signal per document. In order to capture the compositional effects with higher accuracy, we propose a new model called the Recursive Neural Tensor Network (RNTN). Recursive Neural Tensor Networks take as input phrases of any length. They represent a phrase through word vectors and a parse tree and then compute vectors for higher nodes in the tree using the same tensor-based composition function. We compare to several supervised, compositional models such as standard recursive neural networks (RNN) (Socher et al., 2011b), matrix-vector RNNs (Socher et al., 2012), and baselines such as neural networks that ignore word order, Naive Bayes (NB), bi-gram NB and SVM. All models get a significant boost when trained with the new dataset but the RNTN obtains the highest performance with 80.7% accuracy when predicting finegrained sentiment for all nodes. Lastly, we use a test set of positive and negative sentences and their respective negations to show that, unlike bag of words models, the RNTN accurately captures the sentiment change and scope of negation. RNTNs also learn that sentiment of phrases following the con</context>
<context position="6271" citStr="Socher et al., 2011" startWordPosition="999" endWordPosition="1002">s of this idea use more complex frequencies such as how often a word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Gies</context>
<context position="15345" citStr="Socher et al., 2011" startWordPosition="2454" endWordPosition="2457"> compositionality function g and use node vectors as features for a classifier at that node. This function varies for the different models. labels given the word vector via: y°&apos; = softmax(Wsa), (1) where Ws E R5xd is the sentiment classification matrix. For the given tri-gram, this is repeated for vectors b and c. The main task of and difference between the models will be to compute the hidden vectors pi E Rd in a bottom up fashion. 4.1 RNN: Recursive Neural Network The simplest member of this family of neural network models is the standard recursive neural network (Goller and K¨uchler, 1996; Socher et al., 2011a). First, it is determined which parent already has all its children computed. In the above tree example, p1 has its two children’s vectors since both are words. RNNs use the following equations to compute the parent vectors: not very good ... a b c 0 0 + p2 = g(a,p1) - p1 =g(b,c) + 1634 p1 = f(W[c]l,p2 = f(W[a1]), where f = tanh is a standard element-wise nonlinearity, W E Rdx2d is the main parameter to learn and we omit the bias for simplicity. The bias can be added as an extra column to W if an additional 1 is added to the concatenation of the input vectors. The parent vectors must be of t</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning. 2011b. Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>B Huval</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrixvector spaces.</title>
<date>2012</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1827" citStr="Socher et al., 2012" startWordPosition="264" endWordPosition="267">s reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases. 1 Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010). Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and Figure 1: Example of the Recursive Neural Tensor Network accurately predicting 5 sentiment classes, very negative to very positive (– –, –, 0, +, + +), at every node of a parse tree and capturing the negation and its scope in this sentence. models to accurately capture the underlying phenomena presented in such data. To address this need, we introduce the Stanford Sentiment Treebank and a powerful Recursive Neural Tensor Network that can accurately predict the co</context>
<context position="4436" citStr="Socher et al., 2012" startWordPosition="703" endWordPosition="706">ts, such as Twitter data, which provide less overall signal per document. In order to capture the compositional effects with higher accuracy, we propose a new model called the Recursive Neural Tensor Network (RNTN). Recursive Neural Tensor Networks take as input phrases of any length. They represent a phrase through word vectors and a parse tree and then compute vectors for higher nodes in the tree using the same tensor-based composition function. We compare to several supervised, compositional models such as standard recursive neural networks (RNN) (Socher et al., 2011b), matrix-vector RNNs (Socher et al., 2012), and baselines such as neural networks that ignore word order, Naive Bayes (NB), bi-gram NB and SVM. All models get a significant boost when trained with the new dataset but the RNTN obtains the highest performance with 80.7% accuracy when predicting finegrained sentiment for all nodes. Lastly, we use a test set of positive and negative sentences and their respective negations to show that, unlike bag of words models, the RNTN accurately captures the sentiment change and scope of negation. RNTNs also learn that sentiment of phrases following the contrastive conjunction ‘but’ dominates. The co</context>
<context position="7509" citStr="Socher et al., 2012" startWordPosition="1179" endWordPosition="1182"> not been experimentally validated on larger corpora. Yessenalina and Cardie (2011) compute matrix representations for longer phrases and define composition as matrix multiplication, and also evaluate on sentiment. Grefenstette and Sadrzadeh (2011) analyze subject-verbobject triplets and find a matrix-based categorical model to correlate well with human judgments. We compare to the recent line of work on supervised compositional models. In particular we will describe and experimentally compare our new RNTN model to recursive neural networks (RNN) (Socher et al., 2011b) and matrix-vector RNNs (Socher et al., 2012) both of which have been applied to bag of words sentiment corpora. Logical Form. A related field that tackles compositionality from a very different angle is that of trying to map sentences to logical form (Zettlemoyer and Collins, 2005). While these models are highly interesting and work well in closed domains and on discrete sets, they could only capture sentiment distributions using separate mechanisms beyond the currently used logical forms. Deep Learning. Apart from the above mentioned 1632 work on RNNs, several compositionality ideas related to neural networks have been discussed by Bot</context>
<context position="16875" citStr="Socher et al., 2012" startWordPosition="2722" endWordPosition="2725">nd recursive auto-associate memories (Pollack, 1990). The only difference to the former model is that we fix the tree structures and ignore the reconstruction loss. In initial experiments, we found that with the additional amount of training data, the reconstruction loss at each node is not necessary to obtain high performance. 4.2 MV-RNN: Matrix-Vector RNN The MV-RNN is linguistically motivated in that most of the parameters are associated with words and each composition function that computes vectors for longer phrases depends on the actual words being combined. The main idea of the MV-RNN (Socher et al., 2012) is to represent every word and longer phrase in a parse tree as both a vector and a matrix. When two constituents are combined the matrix of one is multiplied with the vector of the other and vice versa. Hence, the compositional function is parameterized by the words that participate in it. Each word’s matrix is initialized as a dxd identity matrix, plus a small amount of Gaussian noise. Similar to the random word vectors, the parameters of these matrices will be trained to minimize the classification error at each node. For this model, each ngram is represented as a list of (vector,matrix) p</context>
<context position="26953" citStr="Socher et al., 2012" startWordPosition="4584" endWordPosition="4587">timent classification at each n-gram lengths. Left: Accuracy separately for each set of n-grams. Right: Cumulative accuracy of all &lt; n-grams. 5.2 Full Sentence Binary Sentiment This setup is comparable to previous work on the original rotten tomatoes dataset which only used full sentence labels and binary classification of positive/negative. Hence, these experiments show the improvement even baseline methods can achieve with the sentiment treebank. Table 1 shows results of this binary classification for both all phrases and for only full sentences. The previous state of the art was below 80% (Socher et al., 2012). With the coarse bag of words annotation for training, many of the more complex phenomena could not be captured, even by more powerful models. The combination of the new sentiment treebank and the RNTN pushes the state of the art on short phrases up to 85.4%. 5.3 Model Analysis: Contrastive Conjunction In this section, we use a subset of the test set which includes only sentences with an ‘X but Y ’ structure: A phrase X being followed by but which is followed by a phrase Y . The conjunction is interpreted as an argument for the second conjunct, with the first functioning concessively (Lakoff,</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>R. Socher, B. Huval, C. D. Manning, and A. Y. Ng. 2012. Semantic compositionality through recursive matrixvector spaces. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Sutskever</author>
<author>R Salakhutdinov</author>
<author>J B Tenenbaum</author>
</authors>
<title>Modelling relational data using Bayesian clustered tensor factorization.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="8395" citStr="Sutskever et al., 2009" startWordPosition="1318" endWordPosition="1321">ighly interesting and work well in closed domains and on discrete sets, they could only capture sentiment distributions using separate mechanisms beyond the currently used logical forms. Deep Learning. Apart from the above mentioned 1632 work on RNNs, several compositionality ideas related to neural networks have been discussed by Bottou (2011) and Hinton (1990) and first models such as Recursive Auto-associative memories been experimented with by Pollack (1990). The idea to relate inputs through three way interactions, parameterized by a tensor have been proposed for relation classification (Sutskever et al., 2009; Jenatton et al., 2012), extending Restricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al., 2012). Sentiment Analysis. Apart from the abovementioned work, most approaches in sentiment analysis use bag of words representations (Pang and Lee, 2008). Snyder and Barzilay (2007) analyzed larger reviews in more detail by analyzing the sentiment of multiple aspects of restaurants, such as food or atmosphere. Several works have explored sentiment compositionality through careful engineering of features or polarity shifting rules on syntactic s</context>
</contexts>
<marker>Sutskever, Salakhutdinov, Tenenbaum, 2009</marker>
<rawString>I. Sutskever, R. Salakhutdinov, and J. B. Tenenbaum. 2009. Modelling relational data using Bayesian clustered tensor factorization. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>P Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1552" citStr="Turney and Pantel, 2010" startWordPosition="221" endWordPosition="224">work. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases. 1 Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010). Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and Figure 1: Example of the Recursive Neural Tensor Network accurately predicting 5 sentiment classes, very negative to very positive (– –, –, 0, +, + +), at every node of a parse tree and captur</context>
<context position="5602" citStr="Turney and Pantel, 2010" startWordPosition="891" endWordPosition="894">owing the contrastive conjunction ‘but’ dominates. The complete training and testing code, a live demo and the Stanford Sentiment Treebank dataset are available at http://nlp.stanford.edu/ sentiment. 2 Related Work This work is connected to five different areas of NLP research, each with their own large amount of related work to which we cannot do full justice given space constraints. Semantic Vector Spaces. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Variants of this idea use more complex frequencies such as how often a word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and train</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>P. D. Turney and P. Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wang</author>
<author>D Can</author>
<author>A Kazemzadeh</author>
<author>F Bar</author>
<author>S Narayanan</author>
</authors>
<title>A system for real-time twitter sentiment analysis of 2012 u.s. presidential election cycle.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 System Demonstrations.</booktitle>
<contexts>
<context position="9560" citStr="Wang et al., 2012" startWordPosition="1495" endWordPosition="1498">of features or polarity shifting rules on syntactic structures (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Rentoumi et al., 2010; Nakagawa et al., 2010). 3 Stanford Sentiment Treebank Bag of words classifiers can work well in longer documents by relying on a few words with strong sentiment like ‘awesome’ or ‘exhilarating.’ However, sentiment accuracies even for binary positive/negative classification for single sentences has not exceeded 80% for several years. For the more difficult multiclass case including a neutral class, accuracy is often below 60% for short messages on Twitter (Wang et al., 2012). From a linguistic or cognitive standpoint, ignoring word order in the treatment of a semantic task is not plausible, and, as we will show, it cannot accurately classify hard examples of negation. Correctly predicting these hard cases is necessary to further improve performance. In this section we will introduce and provide some analyses for the new Sentiment Treebank which includes labels for every syntactically plausible phrase in thousands of sentences, allowing us to train and evaluate compositional models. We consider the corpus of movie review excerpts from the rottentomatoes.com websit</context>
</contexts>
<marker>Wang, Can, Kazemzadeh, Bar, Narayanan, 2012</marker>
<rawString>H. Wang, D. Can, A. Kazemzadeh, F. Bar, and S. Narayanan. 2012. A system for real-time twitter sentiment analysis of 2012 u.s. presidential election cycle. In Proceedings of the ACL 2012 System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
</authors>
<title>Semantic vector products: Some initial investigations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second AAAI Symposium on Quantum Interaction.</booktitle>
<contexts>
<context position="6752" citStr="Widdows, 2008" startWordPosition="1071" endWordPosition="1072"> 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Giesbrecht, 2010) have not been experimentally validated on larger corpora. Yessenalina and Cardie (2011) compute matrix representations for longer phrases and define composition as matrix multiplication, and also evaluate on sentiment. Grefenstette and Sadrzadeh (2011) analyze subject-verbobject triplets and find a matrix-based categorical model to correlate well with human judgments. We compare to the recent line of work on supervised compositional models. In particular we will </context>
</contexts>
<marker>Widdows, 2008</marker>
<rawString>D. Widdows. 2008. Semantic vector products: Some initial investigations. In Proceedings of the Second AAAI Symposium on Quantum Interaction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yessenalina</author>
<author>C Cardie</author>
</authors>
<title>Compositional matrix-space models for sentiment analysis.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1806" citStr="Yessenalina and Cardie, 2011" startWordPosition="260" endWordPosition="263">entiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases. 1 Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010). Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and Figure 1: Example of the Recursive Neural Tensor Network accurately predicting 5 sentiment classes, very negative to very positive (– –, –, 0, +, + +), at every node of a parse tree and capturing the negation and its scope in this sentence. models to accurately capture the underlying phenomena presented in such data. To address this need, we introduce the Stanford Sentiment Treebank and a powerful Recursive Neural Tensor Network that can accu</context>
<context position="6972" citStr="Yessenalina and Cardie (2011)" startWordPosition="1099" endWordPosition="1102">ons learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Giesbrecht, 2010) have not been experimentally validated on larger corpora. Yessenalina and Cardie (2011) compute matrix representations for longer phrases and define composition as matrix multiplication, and also evaluate on sentiment. Grefenstette and Sadrzadeh (2011) analyze subject-verbobject triplets and find a matrix-based categorical model to correlate well with human judgments. We compare to the recent line of work on supervised compositional models. In particular we will describe and experimentally compare our new RNTN model to recursive neural networks (RNN) (Socher et al., 2011b) and matrix-vector RNNs (Socher et al., 2012) both of which have been applied to bag of words sentiment corp</context>
</contexts>
<marker>Yessenalina, Cardie, 2011</marker>
<rawString>A. Yessenalina and C. Cardie. 2011. Compositional matrix-space models for sentiment analysis. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yu</author>
<author>L Deng</author>
<author>F Seide</author>
</authors>
<title>Large vocabulary speech recognition using deep tensor neural networks.</title>
<date>2012</date>
<booktitle>In INTERSPEECH.</booktitle>
<contexts>
<context position="8551" citStr="Yu et al., 2012" startWordPosition="1342" endWordPosition="1345">ntly used logical forms. Deep Learning. Apart from the above mentioned 1632 work on RNNs, several compositionality ideas related to neural networks have been discussed by Bottou (2011) and Hinton (1990) and first models such as Recursive Auto-associative memories been experimented with by Pollack (1990). The idea to relate inputs through three way interactions, parameterized by a tensor have been proposed for relation classification (Sutskever et al., 2009; Jenatton et al., 2012), extending Restricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al., 2012). Sentiment Analysis. Apart from the abovementioned work, most approaches in sentiment analysis use bag of words representations (Pang and Lee, 2008). Snyder and Barzilay (2007) analyzed larger reviews in more detail by analyzing the sentiment of multiple aspects of restaurants, such as food or atmosphere. Several works have explored sentiment compositionality through careful engineering of features or polarity shifting rules on syntactic structures (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Rentoumi et al., 2010; Nakagawa et al., 2010). 3 Stanford Sentiment Treebank Bag of words cl</context>
</contexts>
<marker>Yu, Deng, Seide, 2012</marker>
<rawString>D. Yu, L. Deng, and F. Seide. 2012. Large vocabulary speech recognition using deep tensor neural networks. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Zanzotto</author>
<author>I Korkontzelos</author>
<author>F Fallucchi</author>
<author>S Manandhar</author>
</authors>
<title>Estimating linear models for compositional distributional semantics.</title>
<date>2010</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="1776" citStr="Zanzotto et al., 2010" startWordPosition="256" endWordPosition="259">edicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases. 1 Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010). Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and Figure 1: Example of the Recursive Neural Tensor Network accurately predicting 5 sentiment classes, very negative to very positive (– –, –, 0, +, + +), at every node of a parse tree and capturing the negation and its scope in this sentence. models to accurately capture the underlying phenomena presented in such data. To address this need, we introduce the Stanford Sentiment Treebank and a powerful Recursive Neura</context>
</contexts>
<marker>Zanzotto, Korkontzelos, Fallucchi, Manandhar, 2010</marker>
<rawString>F.M. Zanzotto, I. Korkontzelos, F. Fallucchi, and S. Manandhar. 2010. Estimating linear models for compositional distributional semantics. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In UAI.</booktitle>
<contexts>
<context position="7747" citStr="Zettlemoyer and Collins, 2005" startWordPosition="1219" endWordPosition="1222">and Sadrzadeh (2011) analyze subject-verbobject triplets and find a matrix-based categorical model to correlate well with human judgments. We compare to the recent line of work on supervised compositional models. In particular we will describe and experimentally compare our new RNTN model to recursive neural networks (RNN) (Socher et al., 2011b) and matrix-vector RNNs (Socher et al., 2012) both of which have been applied to bag of words sentiment corpora. Logical Form. A related field that tackles compositionality from a very different angle is that of trying to map sentences to logical form (Zettlemoyer and Collins, 2005). While these models are highly interesting and work well in closed domains and on discrete sets, they could only capture sentiment distributions using separate mechanisms beyond the currently used logical forms. Deep Learning. Apart from the above mentioned 1632 work on RNNs, several compositionality ideas related to neural networks have been discussed by Bottou (2011) and Hinton (1990) and first models such as Recursive Auto-associative memories been experimented with by Pollack (1990). The idea to relate inputs through three way interactions, parameterized by a tensor have been proposed for</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>L. Zettlemoyer and M. Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In UAI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
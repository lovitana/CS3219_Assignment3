<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002076">
<title confidence="0.990517">
Chinese Zero Pronoun Resolution: Some Recent Advances
</title>
<author confidence="0.99274">
Chen Chen and Vincent Ng
</author>
<affiliation confidence="0.991909">
Human Language Technology Research Institute
University of Texas at Dallas
</affiliation>
<address confidence="0.924178">
Richardson, TX 75083-0688
</address>
<email confidence="0.999576">
{yzcchen,vince}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.993916" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9985713">
We extend Zhao and Ng&apos;s (2007) Chinese
anaphoric zero pronoun resolver by (1) using
a richer set of features and (2) exploiting the
coreference links between zero pronouns dur-
ing resolution. Results on OntoNotes show
that our approach significantly outperforms
two state-of-the-art anaphoric zero pronoun re-
solvers. To our knowledge, this is the first
work to report results obtained by an end-to-
end Chinese zero pronoun resolver.
</bodyText>
<sectionHeader confidence="0.99879" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997552631579">
A zero pronoun (ZP) is a gap in a sentence that is
found when a phonetically null form is used to refer
to a real-world entity. An anaphoric zero pronoun
(AZP) is a ZP that corefers with one or more preced-
ing noun phrases (NPs) in the associated text. Un-
like overt pronouns, ZPs lack grammatical attributes
that are useful for overt pronoun resolution such as
number and gender. This makes ZP resolution more
challenging than overt pronoun resolution.
We aim to improve the state of the art in Chinese
AZP resolution by proposing two extensions. First,
while previous approaches to this task have primarily
focused on employing positional and syntactic fea-
tures (e.g., Zhao and Ng (2007) [Z&amp;N], Kong and
Zhou (2010) [K&amp;Z]), we exploit a richer set of fea-
tures for capturing the context of an AZP and its
candidate antecedents. Second, to alleviate the diffi-
culty of resolving an AZP to an antecedent far away
from it, we break down the process into smaller, in-
termediate steps, where we allow coreference links
between AZPs to be established.
We apply our two extensions to a state-of-the-art
Chinese AZP resolver proposed by Z&amp;N and eval-
uate the resulting resolver on the OntoNotes cor-
pus. Experimental results show that this resolver sig-
nificantly outperforms both Z&amp;N&apos;s resolver and an-
other state-of-the-art resolver proposed by K&amp;Z. It
is worth noting that while previous work on Chinese
ZP resolution has reported results obtained via gold
information (e.g., using gold AZPs and extracting
candidate antecedents and other features from gold
syntactic parse trees), this is the first work to report
the results of an end-to-end Chinese ZP resolver.
The rest of this paper is organized as follows. Sec-
tion 2 describes the two baseline AZP resolvers. Sec-
tions 3 and 4 discuss our two extensions. We present
our evaluation results in Section 5 and our conclu-
sions in Section 6.
</bodyText>
<sectionHeader confidence="0.991042" genericHeader="method">
2 Baseline AZP Resolution Systems
</sectionHeader>
<bodyText confidence="0.9998675">
An AZP resolution algorithm takes as input a set
of AZPs produced by an AZP identification system.
Below we first describe the AZP identifier we em-
ploy, followed by our two baseline AZP resolvers.
</bodyText>
<subsectionHeader confidence="0.997865">
2.1 Anaphoric Zero Pronoun Identification
</subsectionHeader>
<bodyText confidence="0.999111416666667">
We employ two steps to identify AZPs. In the extrac-
tion step, we heuristically extract candidate ZPs. In
the classification step, we train a classifier to distin-
guish AZPs from non-AZPs.
To implement the extraction step, we use Z&amp;N&apos;s
and K&amp;Z&apos;s observation: ZPs can only occur before a
VP node in a syntactic parse tree. However, accord-
ing to K&amp;Z, ZPs do not need to be extracted from
every VP: if a VP node occurs in a coordinate struc-
ture or is modified by an adverbial node, then only its
parent VP node needs to be considered. We extract
ZPs from all VPs that satisfy the above constraints.
</bodyText>
<page confidence="0.876896">
1360
</page>
<bodyText confidence="0.468185923076923">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1360–1365,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
Syntactic whether z is the first gap in an IP clause; whether z is the first gap in a subject-less IP clause, and if
features so, POS(w1); whether POS(w1) is NT; whether t1 is a verb that appears in a NP or VP; whether Pl is
(13) a NP node; whether PT is a VP node; the phrasal label of the parent of the node containing POS(t1);
whether V has a NP, VP or CP ancestor; whether C is a VP node; whether there is a VP node whose
parent is an IP node in the path from t1 to C.
Lexical the words surrounding z and/or their POS tags, including w1, w_1, POS(w1), POS(w_1)+POS(w1),
features POS(w1)+POS(w2), POS(w_2)+POS(w_1), POS(w1)+POS(w2)+POS(w3), POS(w_1)+w1, and
(13) w_1+POS(w1); whether w1 is a transitive verb, an intransitive verb or a preposition; whether w_1 is
a transitive verb without an object.
Other fea- whether z is the first gap in a sentence; whether z is in the headline of the text; the type of the clause in
tures (6) which z appears; the grammatical role of z; whether w_1 is a punctuation; whether w_1 is a comma.
</bodyText>
<tableCaption confidence="0.988015">
Table 1: Features for AZP identification. z is a zero pronoun. V is the VP node following z. wi is the ith word to the
right of z (if i is positive) or the ith word to the left of z (if i is negative). C is lowest common ancestor of w_1 and
w1. Pl and PT are the child nodes of C that are the ancestors of w_1 and w1 respectively.
</tableCaption>
<bodyText confidence="0.997169416666667">
Features the sentence distance between a and z; the segment distance between a and z, where segments are
between a separated by punctuations; whether a is the closest NP to z; whether a and z are siblings in the
and z (4) associated parse tree.
Features whether a has an ancestor NP, and if so, whether this NP is a descendent of a&apos;s lowest ancestor IP;
on a (12) whether a has an ancestor VP, and if so, whether this VP is a descendent of a&apos;s lowest ancestor IP;
whether a has an ancestor CP; the grammatical role of a; the clause type in which a appears; whether
a is an adverbial NP, a temporal NP, a pronoun or a named entity; whether a is in the headline of the
text.
Features whether V has an ancestor NP, and if so, whether this NP node is a descendent of V&apos;s lowest ancestor
on z (10) IP; whether V has an ancestor VP, and if so, whether this VP is a descendent of V&apos;s lowest ancestor IP;
whether V has an ancestor CP; the grammatical role of z; the type of the clause in which V appears;
whether z is the first or last ZP of the sentence; whether z is in the headline of the text.
</bodyText>
<tableCaption confidence="0.7715485">
Table 2: Features for AZP resolution in the Zhao and Ng (2007) baseline system. z is a zero pronoun. a is a candidate
antecedent of z. V is the VP node following z in the parse tree.
</tableCaption>
<bodyText confidence="0.999962">
To implement the classification step, we train a
classifier using SVMlight (Joachims,1999) to distin-
guish AZPs from non-AZPs. We employ 32 features,
13 of which were proposed by Z&amp;N and 19 of which
were proposed by Yang and Xue (2010). A brief de-
scription of these features can be found in Table 1.
</bodyText>
<subsectionHeader confidence="0.999658">
2.2 Two Baseline AZP Resolvers
</subsectionHeader>
<bodyText confidence="0.999369423076923">
The Zhao and Ng (2007) [Z&amp;N] baseline. In
our implementation of the Z&amp;N baseline, we use
SVMlight to train a mention-pair model for determin-
ing whether an AZP z and a candidate antecedent
of z are coreferent. We consider all NPs preced-
ing z that do not have the same head as its parent
NP in the parse tree to be z&apos;s candidate antecedents.
We use Soon et al.&apos;s (2001) method to create train-
ing instances: we create a positive instance between
an AZP, z, and its closest overt antecedent, and we
create a negative instance between z and each of the
intervening candidates. Each instance is represented
by the 26 features employed by Z&amp;N. A brief de-
scription of these features can be found in Table 2.
During testing, we adopt the closest-first resolution
strategy, resolving an AZP to the closest candidate
antecedent that is classified as coreferent with it.1
The Kong and Zhou (2010) [K&amp;Z] baseline.
K&amp;Z employ a tree kernel-based approach to AZP
resolution. Like Z&amp;N, K&amp;Z (1) train a mention-
pair model for determining whether an AZP z and
a candidate antecedent of z are coreferent, (2) use
Soon et al.&apos;s method to create training instances, and
(3) resolve an AZP to its closest coreferent can-
didate antecedent. Unlike Z&amp;N, however, K&amp;Z
use the SVMlight−Tx learning algorithm (Moschitti,
</bodyText>
<footnote confidence="0.9366234">
1When resolving a gold AZP z, if none of the preceding can-
didate antecedents is classified as coreferent with it, we resolve
it to the candidate that has the highest coreference likelihood
with it. Here, we employ the signed distance from the SVM
hyperplane to measure the coreference likelihood.
</footnote>
<page confidence="0.990079">
1361
</page>
<bodyText confidence="0.997179">
2006) to train their model, employing a parse sub-
tree known as a dynamic expansion tree (Zhou et al.,
2008) as a structured feature to represent an instance.
</bodyText>
<sectionHeader confidence="0.969441" genericHeader="method">
3 Extension 1: Novel Features
</sectionHeader>
<bodyText confidence="0.998687916666667">
We propose three kinds of features to better capture
the context of an AZP, as described below.
Antecedent compatibility. AZPs are omitted sub-
jects that precede VP nodes in a sentence&apos;s parse
tree. From the VP node, we can extract its head verb
(Predz) and the head of its object NP (Obj), if any.
Note that Predz and Obj contain important contex-
tual information for an AZP.
Next, observe that if a NP is coreferent with an
AZP, it should be able to fill the AZP&apos;s gap and be
compatible with the gap&apos;s context. Consider the fol-
lowing example:
</bodyText>
<equation confidence="0.895515">
E1: ∗pro∗ Ꮰᳯ
ࠄб᳜៥们ⱘᮙᅶᴹⱘᯊ׭DŽ
</equation>
<bodyText confidence="0.998587541666667">
(They are trying that service. That means ∗pro∗
hope that our visitors can try it when they come in
September.)
The head of the VP following ∗pro∗ is Ꮰ ᳯ
(hope). There are two candidate antecedents, 他们
(They) and 那个服务 (that service). If we try us-
ing them to fill this AZP&apos;s gap, we know based on
selectional preferences that 他们Ꮰᳯ (They hope)
makes more sense than 那个服务Ꮰᳯ (that service
hope). We supply the AZP resolver with the fol-
lowing information to help it make these decisions.
First, we find the head word of each candidate an-
tecedent, Headc. Then we form two strings, Headc
+ Predz and Headc + Predz + Obj (if the object
of the VP is present). Finally, we employ them as bi-
nary lexical features, setting their feature values to 1
if and only if they can be extracted from the instance
under consideration. The training data can be used
to determine which of these features are useful.2
Narrative event chains. A narrative event chain is
a partially ordered set of events related by a common
protagonist (Chambers and Jurafsky, 2008). For ex-
ample, we can infer from the chain &amp;quot;borrow-s invest-
s spend-s lend-s&amp;quot; that a person who borrows (pre-
</bodyText>
<footnote confidence="0.882241">
2We tried to apply Kehler et al.&apos;s (2004) and Yang et
al.&apos;s (2005) methods to learn Chinese selectional preferences
from unlabeled data, but without success.
</footnote>
<equation confidence="0.508598">
sumably money) can invest it, spend it, or lend it to
other people.3 Consider the following example:
E2: R*㒭䪅њˈ∗pro∗ ᦤկ䖭䚼ߚⱘ䪅䛑ᰃ㞾
Ꮕ᠔䞠᣷ⱘDŽ
</equation>
<bodyText confidence="0.976708911111111">
(The country gives our department money, but all
∗pro∗ provides is exactly what we worked for.)
In E2, ∗pro∗ is coreferent with 国家 (The coun-
try), and the presence of the narrative event chain 㒭
− ᦤկ (gives−provides) suggests that the subjects
of the two events are likely to be coreferent.
However, given the unavailability of induced or
hand-crafted narrative chains in Chinese4, we make
the simplifying assumption that two verbs form a
lexical chain if they are lexically identical.5 We
create two features to exploit narrative event chains
for a candidate NP, c, if it serves as a subject or
object. Specifically, let the verb governing c be
Predc. The first feature, which encodes whether
narrative chains are present, has three possible val-
ues: 0 if Predc and Predz are not the same; 1 if
Predc and Predz are the same and c is a subject;
and 2 if Predc and Predz are the same and c is an
object. The second feature is a binary lexical fea-
ture, Predc+Predz+Subject/Object; its value is
1 if and only if Predc, Predz, and Subject/Object
can be found in the associated instance, where
Subject/Object denotes the grammatical role of c.
Final punctuation hint. We observe that the punc-
tuation (Punc) at the end of a sentence where an
AZP occurs also provides contextual information,
especially in conversation documents. In conversa-
tions, if a sentence containing an AZP ends with a
3&amp;quot;-s&amp;quot; denotes the fact that the protagonist serves as the gram-
matical subject in these events.
4We tried to construct narrative chains for Chinese using
both learning-based and dictionary-based methods. Specifi-
cally, we induced narrative chains using Chambers and Juraf-
sky&apos;s (2008) method, but were not successful owing to the lack
of an accurate Chinese coreference resolver. In addition, we
constructed narrative chains using both lexically identical verbs
and the synonyms obtained from a WordNet-like Chinese re-
source called Tongyicicilin, but they did not help improve reso-
lution performance.
5Experiments on the training data show that if an AZP and
a candidate antecedent are subjects of (different occurrences of)
the same verb, then the probability that the candidate antecedent
is coreferent with the AZP is 0.703. This result suggests that our
assumption, though somewhat simplistic, is useful as far as AZP
resolution is concerned.
</bodyText>
<page confidence="0.990144">
1362
</page>
<figure confidence="0.9799827">
A: 她⦄೼⫳⌏ᗢМḋ˛
(A: How is her life now? )
B: *pro,* ᇍ⫳⌏ˈህᰃᕜᴈ㋴ˈᕜㅔऩDŽ
(B: *pro,* attitude toward life is plain and simple.)
A: ௃DŽ
(A: Yes.)
A: *pro2* ᰃ೼࣫Ҁ䖬ᰃ೼㕢国˛
(A: *pro2* is living in Beijing or the USA?)
B: *pro3* ೼㕢国DŽ
(B: *pro3* is living in the USA.)
</figure>
<figureCaption confidence="0.999919">
Figure 1: An illustrative example.
</figureCaption>
<bodyText confidence="0.9384893">
question mark, the mention this AZP refers to is less
likely to be the speaker himself6, as illustrated in the
following example:
E3: ހ໽ *pro* ދ৫˛
(Are *pro* cold in the winter?)
Here, *pro* refers to the person the speaker talks
with. To capture this information, we create a binary
lexical feature, Head,+Punc, whose value is 1 if
and only if Head, and Punc appear in the instance
under consideration.
</bodyText>
<sectionHeader confidence="0.99094" genericHeader="method">
4 Extension 2: Zero Pronoun Links
</sectionHeader>
<subsectionHeader confidence="0.971863">
4.1 Motivation
</subsectionHeader>
<bodyText confidence="0.99996205882353">
Like an overt pronoun, a ZP whose closest overt
antecedent is far away from it is harder to resolve
than one that has a nearby overt antecedent. How-
ever, a corpus study of our training data reveals that
only 55.2% of the AZPs appear in the same sentence
as their closest overt antecedent, and 22.7% of the
AZPs appear two or more sentences away from their
closest overt antecedent.
Fortunately, we found that some of the difficult-
to-resolve AZPs (i.e., AZPs whose closest overt an-
tecedents are far away from them) are coreferential
with nearby ZPs. Figure 1, which consists of a set of
sentences from a conversation, illustrates this phe-
nomenon. There are three AZPs (denoted by *prof*,
where 1 G i G 3), all of which refer to the overt
pronoun 她 (She) in the first sentence. In this ex-
ample, it is fairly easy to resolve *pro,* correctly,
</bodyText>
<footnote confidence="0.996225">
6One may wonder whether we can similarly identify con-
straints on the antecedents of a ZP from clause conjunctions.
Our preliminary analysis suggests that the answer is no.
</footnote>
<table confidence="0.9996275">
Training Test
Documents 1,391 172
Sentences 36,487 6,083
Words 756,063 110,034
ZPs 23,065 3,658
AZPs 12,111 1,713
</table>
<tableCaption confidence="0.999905">
Table 3: Statistics on the training and test sets.
</tableCaption>
<bodyText confidence="0.999746666666667">
since its antecedent is the subject of previous sen-
tence. However, *pro3* and its closest overt an-
tecedent 她 (She) are four sentences apart. Together
with the fact that there are many intervening candi-
date antecedents, it is not easy for a resolver to cor-
rectly resolve *pro3*.
To facilitate the resolution of *pro3* and difficult-
to-resolve AZPs in general, we propose the follow-
ing idea. We allow an AZP resolver to (1) establish
coreferent links between two consecutive ZPs (i.e.,
*pro,*−*pro2* and *pro2*−*pro3* in our exam-
ple), which are presumably easy to establish because
the two AZPs involved are close to each other; and
then (2) treat them as bridges and infer that *pro3*&apos;s
overt antecedent is 她 (She).
</bodyText>
<subsectionHeader confidence="0.998479">
4.2 Modified Resolution Algorithm
</subsectionHeader>
<bodyText confidence="0.999868818181818">
We implement the aforementioned idea by modify-
ing the AZP resolver as follows. When we resolve an
AZP z during testing, we augment the set of candi-
date antecedents for z with the set of AZPs preceding
z. Since we have only specified how to compute fea-
tures for instances composed of an AZP and an overt
candidate antecedent thus far (see Section 2.2), the
question, then, is: how can we compute features for
instances composed of two AZPs?
To answer this question, we first note that the
AZPs in a test text are resolved in a left-to-right man-
ner. Hence, by the time we resolve an AZP z, all the
AZPs preceding z have been resolved. Hence, when
we create a test instance i between z and one of the
preceding AZPs (say y), we create i as if the gap y
was filled with the smallest tree embedding the NP
to which y was resolved.
By allowing coreference links between (presum-
ably nearby) ZPs to be established, we can reason
over the resulting coreference links, treating them as
bridges that can help us find an overt antecedent that
is far away from an AZP.
</bodyText>
<page confidence="0.834532">
1363
</page>
<table confidence="0.997804">
Gold AZP System AZP System AZP
Gold Parse Tree Gold Parse Tree System Parse Tree
System Variation R P F R P F R P F
K&amp;Z Baseline System 38.0 38.0 38.0 17.7 22.4 19.8 10.6 13.6 11.9
Z&amp;N Baseline System 41.5 41.5 41.5 22.4 24.4 23.3 12.7 14.2 13.4
Z&amp;N Baseline + Contextual Features 46.2 46.2 46.2 25.2 27.5 26.3 14.4 16.1 15.2
Z&amp;N Baseline + Zero Pronoun Links 42.7 42.7 42.7 22.5 24.6 23.5 13.2 14.8 13.9
Full System 47.7 47.7 47.7 25.3 27.6 26.4 14.9 16.7 15.7
</table>
<tableCaption confidence="0.999576">
Table 4: Resolution results on the test set.
</tableCaption>
<sectionHeader confidence="0.996719" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.998619">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999950071428571">
Dataset. For evaluation, we employ the portion of
the OntoNotes 4.0 corpus that was used in the official
CoNLL-2012 shared task. The shared task dataset is
composed of a training set, a development set, and
a test set. Since only the training set and the de-
velopment set are annotated with ZPs, we use the
training set for classifier training and reserve the de-
velopment set for testing purposes. Statistics on the
datasets are shown in Table 3. In these datasets, a ZP
is marked as *pro*. We consider a ZP anaphoric if
it is coreferential with a preceding ZP or overt NP.
Evaluation measures. We express the results of
both AZP identification and AZP resolution in terms
of recall (R), precision (P) and F-score (F).
</bodyText>
<subsectionHeader confidence="0.832162">
5.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999982166666667">
The three major columns of Table 4 show the re-
sults obtained in three settings, which differ in
terms of whether gold/system AZPs and manu-
ally/automatically constructed parse trees are used to
extract candidate antecedents and features.
In the first setting, the resolvers are provided with
gold AZPs and gold parse trees. Results are shown in
column 1. As we can see, the Z&amp;N baseline signifi-
cantly outperforms the K&amp;Z baseline by 3.5% in F-
score.7 Adding the contextual features, the ZP links,
and both extensions to Z&amp;N increase its F-score sig-
nificantly by 4.7%, 1.2% and 6.2%, respectively.
In the next two settings, the resolvers operate on
the system AZPs provided by the AZP identification
component. When gold parse trees are employed,
the recall, precision and F-score of AZP identifica-
tion are 50.6%, 55.1% and 52.8% respectively. Col-
umn 2 shows the results of the resolvers obtained
</bodyText>
<footnote confidence="0.448199">
7All significance tests are paired t-tests, with p &lt; 0.05.
</footnote>
<bodyText confidence="0.999988517241379">
when these automatically identified AZPs are used.
As we can see, Z&amp;N again significantly outperforms
K&amp;Z by 3.5% in F-score. Adding the contextual fea-
tures, the ZP links, and both extensions to Z&amp;N in-
crease its F-score by 3.0%, 0.2% and 3.1%, respec-
tively. The system with contextual features and the
full system both yield results that are significantly
better than those of the Z&amp;N baseline. A closer ex-
amination of the results reveals why the ZP links are
not effective in improving performance: when em-
ploying system AZPs, many erroneous ZP links were
introduced to the system.
Column 3 shows the results of the resolvers when
we employ system AZPs and the automatically gen-
erated parse trees provided by the CoNLL-2012
shared task organizers to compute candidate an-
tecedents and features. Hence, these are end-to-end
ZP resolution results. To our knowledge, these are
the first reported results on end-to-end Chinese ZP
resolution. Using automatic parse trees, the perfor-
mance on AZP identification drops to 30.8% (R),
34.4% (P) and 32.5% (F). In this setting, Z&amp;N still
outperforms K&amp;Z significantly, though by a smaller
margin when compared to the previous settings. In-
corporating the contextual features, the ZP links, and
both extensions increase the F-score by 1.8%, 0.5%
and 2.3%, respectively. The system with contextual
features and the full system both yield results that are
significantly better than those of the Z&amp;N baseline.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999774285714286">
We proposed two extensions to a state-of-the-
art Chinese AZP resolver proposed by Zhao and
Ng (2007). Experimental results on the OntoNotes
dataset showed that the resulting resolver signifi-
cantly improved both Zhao and Ng&apos;s and Kong and
Zhou&apos;s (2010) resolvers, regardless of whether gold
or system AZPs and syntactic parse trees are used.
</bodyText>
<page confidence="0.994365">
1364
</page>
<sectionHeader confidence="0.998318" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999955875">
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier draft
of the paper. This work was supported in part by
NSF Grants IIS-1147644 and IIS-1219142. Any
opinions, findings, conclusions or recommendations
expressed in this paper are those of the authors and
do not necessarily reflect the views or official poli-
cies, either expressed or implied, of NSF.
</bodyText>
<sectionHeader confidence="0.99918" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999791291666667">
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of the 46th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 787--797.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scholkopf and Alexan-
der Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning, pages 44--56. MIT Press.
Andrew Kehler, Douglas Appelt, Lara Taylor, and Alek-
sandr Simma. 2004. Competitive self-trained pronoun
interpretation. In Proceedings of HLT-NAACL 2004:
Short Papers, pages 33--36.
Fang Kong and Guodong Zhou. 2010. A tree kernel-
based unified framework for Chinese zero anaphora
resolution. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 882--891.
Alessandro Moschitti. 2006. Making tree kernels prac-
tical for natural language processing. In Proceedings
of the 11th Conference of the European Chapter of the
Association for Computational Linguistics, pages 113-
-120.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521--544.
Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost:
recovering empty categories in the Chinese Treebank.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, pages 1382--
1390.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Im-
proving pronoun resolution using statistics-based se-
mantic compatibility information. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 165--172.
Shanheng Zhao and Hwee Tou Ng. 2007. Identification
and resolution of Chinese zero pronouns: A machine
learning approach. In Proceedings of the 2007 Joint
Conference on Empirical Methods on Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 541--550.
GuoDong Zhou, Fang Kong, and Qiaoming Zhu. 2008.
Context-sensitive convolution tree kernel for pronoun
resolution. In Proceedings of the 3rd International
Joint Conference on Natural Language Processing,
pages 25--31.
</reference>
<page confidence="0.992771">
1365
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.842399">
<title confidence="0.999888">Chinese Zero Pronoun Resolution: Some Recent Advances</title>
<author confidence="0.99989">Chen Ng</author>
<affiliation confidence="0.9934095">Human Language Technology Research University of Texas at</affiliation>
<address confidence="0.873715">Richardson, TX 75083-0688</address>
<email confidence="0.999774">yzcchen@hlt.utdallas.edu</email>
<email confidence="0.999774">vince@hlt.utdallas.edu</email>
<abstract confidence="0.997939909090909">We extend Zhao and Ng&apos;s (2007) Chinese anaphoric zero pronoun resolver by (1) using a richer set of features and (2) exploiting the coreference links between zero pronouns during resolution. Results on OntoNotes show that our approach significantly outperforms two state-of-the-art anaphoric zero pronoun resolvers. To our knowledge, this is the first to report results obtained by an end-tozero pronoun resolver.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative event chains.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>787--797</pages>
<contexts>
<context position="9976" citStr="Chambers and Jurafsky, 2008" startWordPosition="1754" endWordPosition="1757">esolver with the following information to help it make these decisions. First, we find the head word of each candidate antecedent, Headc. Then we form two strings, Headc + Predz and Headc + Predz + Obj (if the object of the VP is present). Finally, we employ them as binary lexical features, setting their feature values to 1 if and only if they can be extracted from the instance under consideration. The training data can be used to determine which of these features are useful.2 Narrative event chains. A narrative event chain is a partially ordered set of events related by a common protagonist (Chambers and Jurafsky, 2008). For example, we can infer from the chain &amp;quot;borrow-s invests spend-s lend-s&amp;quot; that a person who borrows (pre2We tried to apply Kehler et al.&apos;s (2004) and Yang et al.&apos;s (2005) methods to learn Chinese selectional preferences from unlabeled data, but without success. sumably money) can invest it, spend it, or lend it to other people.3 Consider the following example: E2: R*㒭䪅њˈ∗pro∗ ᦤկ䖭䚼ߚⱘ䪅䛑㞾 Ꮕ᠔䞠ⱘDŽ (The country gives our department money, but all ∗pro∗ provides is exactly what we worked for.) In E2, ∗pro∗ is coreferent with 国家 (The country), and the presence of the narrative event chain 㒭 − ᦤկ </context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised learning of narrative event chains. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 787--797.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>In Bernhard Scholkopf and Alexander Smola, editors, Advances in Kernel Methods - Support Vector Learning,</booktitle>
<pages>44--56</pages>
<publisher>MIT Press.</publisher>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. In Bernhard Scholkopf and Alexander Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 44--56. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kehler</author>
<author>Douglas Appelt</author>
<author>Lara Taylor</author>
<author>Aleksandr Simma</author>
</authors>
<title>Competitive self-trained pronoun interpretation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL 2004: Short Papers,</booktitle>
<pages>33--36</pages>
<marker>Kehler, Appelt, Taylor, Simma, 2004</marker>
<rawString>Andrew Kehler, Douglas Appelt, Lara Taylor, and Aleksandr Simma. 2004. Competitive self-trained pronoun interpretation. In Proceedings of HLT-NAACL 2004: Short Papers, pages 33--36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fang Kong</author>
<author>Guodong Zhou</author>
</authors>
<title>A tree kernelbased unified framework for Chinese zero anaphora resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>882--891</pages>
<contexts>
<context position="1383" citStr="Kong and Zhou (2010)" startWordPosition="215" endWordPosition="218">to refer to a real-world entity. An anaphoric zero pronoun (AZP) is a ZP that corefers with one or more preceding noun phrases (NPs) in the associated text. Unlike overt pronouns, ZPs lack grammatical attributes that are useful for overt pronoun resolution such as number and gender. This makes ZP resolution more challenging than overt pronoun resolution. We aim to improve the state of the art in Chinese AZP resolution by proposing two extensions. First, while previous approaches to this task have primarily focused on employing positional and syntactic features (e.g., Zhao and Ng (2007) [Z&amp;N], Kong and Zhou (2010) [K&amp;Z]), we exploit a richer set of features for capturing the context of an AZP and its candidate antecedents. Second, to alleviate the difficulty of resolving an AZP to an antecedent far away from it, we break down the process into smaller, intermediate steps, where we allow coreference links between AZPs to be established. We apply our two extensions to a state-of-the-art Chinese AZP resolver proposed by Z&amp;N and evaluate the resulting resolver on the OntoNotes corpus. Experimental results show that this resolver significantly outperforms both Z&amp;N&apos;s resolver and another state-of-the-art reso</context>
<context position="7469" citStr="Kong and Zhou (2010)" startWordPosition="1316" endWordPosition="1319">me head as its parent NP in the parse tree to be z&apos;s candidate antecedents. We use Soon et al.&apos;s (2001) method to create training instances: we create a positive instance between an AZP, z, and its closest overt antecedent, and we create a negative instance between z and each of the intervening candidates. Each instance is represented by the 26 features employed by Z&amp;N. A brief description of these features can be found in Table 2. During testing, we adopt the closest-first resolution strategy, resolving an AZP to the closest candidate antecedent that is classified as coreferent with it.1 The Kong and Zhou (2010) [K&amp;Z] baseline. K&amp;Z employ a tree kernel-based approach to AZP resolution. Like Z&amp;N, K&amp;Z (1) train a mentionpair model for determining whether an AZP z and a candidate antecedent of z are coreferent, (2) use Soon et al.&apos;s method to create training instances, and (3) resolve an AZP to its closest coreferent candidate antecedent. Unlike Z&amp;N, however, K&amp;Z use the SVMlight−Tx learning algorithm (Moschitti, 1When resolving a gold AZP z, if none of the preceding candidate antecedents is classified as coreferent with it, we resolve it to the candidate that has the highest coreference likelihood with</context>
</contexts>
<marker>Kong, Zhou, 2010</marker>
<rawString>Fang Kong and Guodong Zhou. 2010. A tree kernelbased unified framework for Chinese zero anaphora resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 882--891.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Making tree kernels practical for natural language processing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>113--120</pages>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Making tree kernels practical for natural language processing. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 113--120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<pages>27--4</pages>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521--544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaqin Yang</author>
<author>Nianwen Xue</author>
</authors>
<title>Chasing the ghost: recovering empty categories in the Chinese Treebank.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>1382--1390</pages>
<contexts>
<context position="6489" citStr="Yang and Xue (2010)" startWordPosition="1142" endWordPosition="1145">stor IP; whether V has an ancestor CP; the grammatical role of z; the type of the clause in which V appears; whether z is the first or last ZP of the sentence; whether z is in the headline of the text. Table 2: Features for AZP resolution in the Zhao and Ng (2007) baseline system. z is a zero pronoun. a is a candidate antecedent of z. V is the VP node following z in the parse tree. To implement the classification step, we train a classifier using SVMlight (Joachims,1999) to distinguish AZPs from non-AZPs. We employ 32 features, 13 of which were proposed by Z&amp;N and 19 of which were proposed by Yang and Xue (2010). A brief description of these features can be found in Table 1. 2.2 Two Baseline AZP Resolvers The Zhao and Ng (2007) [Z&amp;N] baseline. In our implementation of the Z&amp;N baseline, we use SVMlight to train a mention-pair model for determining whether an AZP z and a candidate antecedent of z are coreferent. We consider all NPs preceding z that do not have the same head as its parent NP in the parse tree to be z&apos;s candidate antecedents. We use Soon et al.&apos;s (2001) method to create training instances: we create a positive instance between an AZP, z, and its closest overt antecedent, and we create a </context>
</contexts>
<marker>Yang, Xue, 2010</marker>
<rawString>Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost: recovering empty categories in the Chinese Treebank. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 1382--1390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Improving pronoun resolution using statistics-based semantic compatibility information.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>165--172</pages>
<marker>Yang, Su, Tan, 2005</marker>
<rawString>Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Improving pronoun resolution using statistics-based semantic compatibility information. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 165--172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shanheng Zhao</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Identification and resolution of Chinese zero pronouns: A machine learning approach.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods on Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>541--550</pages>
<contexts>
<context position="1355" citStr="Zhao and Ng (2007)" startWordPosition="210" endWordPosition="213">tically null form is used to refer to a real-world entity. An anaphoric zero pronoun (AZP) is a ZP that corefers with one or more preceding noun phrases (NPs) in the associated text. Unlike overt pronouns, ZPs lack grammatical attributes that are useful for overt pronoun resolution such as number and gender. This makes ZP resolution more challenging than overt pronoun resolution. We aim to improve the state of the art in Chinese AZP resolution by proposing two extensions. First, while previous approaches to this task have primarily focused on employing positional and syntactic features (e.g., Zhao and Ng (2007) [Z&amp;N], Kong and Zhou (2010) [K&amp;Z]), we exploit a richer set of features for capturing the context of an AZP and its candidate antecedents. Second, to alleviate the difficulty of resolving an AZP to an antecedent far away from it, we break down the process into smaller, intermediate steps, where we allow coreference links between AZPs to be established. We apply our two extensions to a state-of-the-art Chinese AZP resolver proposed by Z&amp;N and evaluate the resulting resolver on the OntoNotes corpus. Experimental results show that this resolver significantly outperforms both Z&amp;N&apos;s resolver and a</context>
<context position="6134" citStr="Zhao and Ng (2007)" startWordPosition="1077" endWordPosition="1080">pe in which a appears; whether a is an adverbial NP, a temporal NP, a pronoun or a named entity; whether a is in the headline of the text. Features whether V has an ancestor NP, and if so, whether this NP node is a descendent of V&apos;s lowest ancestor on z (10) IP; whether V has an ancestor VP, and if so, whether this VP is a descendent of V&apos;s lowest ancestor IP; whether V has an ancestor CP; the grammatical role of z; the type of the clause in which V appears; whether z is the first or last ZP of the sentence; whether z is in the headline of the text. Table 2: Features for AZP resolution in the Zhao and Ng (2007) baseline system. z is a zero pronoun. a is a candidate antecedent of z. V is the VP node following z in the parse tree. To implement the classification step, we train a classifier using SVMlight (Joachims,1999) to distinguish AZPs from non-AZPs. We employ 32 features, 13 of which were proposed by Z&amp;N and 19 of which were proposed by Yang and Xue (2010). A brief description of these features can be found in Table 1. 2.2 Two Baseline AZP Resolvers The Zhao and Ng (2007) [Z&amp;N] baseline. In our implementation of the Z&amp;N baseline, we use SVMlight to train a mention-pair model for determining wheth</context>
<context position="20234" citStr="Zhao and Ng (2007)" startWordPosition="3502" endWordPosition="3505">sing automatic parse trees, the performance on AZP identification drops to 30.8% (R), 34.4% (P) and 32.5% (F). In this setting, Z&amp;N still outperforms K&amp;Z significantly, though by a smaller margin when compared to the previous settings. Incorporating the contextual features, the ZP links, and both extensions increase the F-score by 1.8%, 0.5% and 2.3%, respectively. The system with contextual features and the full system both yield results that are significantly better than those of the Z&amp;N baseline. 6 Conclusions We proposed two extensions to a state-of-theart Chinese AZP resolver proposed by Zhao and Ng (2007). Experimental results on the OntoNotes dataset showed that the resulting resolver significantly improved both Zhao and Ng&apos;s and Kong and Zhou&apos;s (2010) resolvers, regardless of whether gold or system AZPs and syntactic parse trees are used. 1364 Acknowledgments We thank the three anonymous reviewers for their detailed and insightful comments on an earlier draft of the paper. This work was supported in part by NSF Grants IIS-1147644 and IIS-1219142. Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views or</context>
</contexts>
<marker>Zhao, Ng, 2007</marker>
<rawString>Shanheng Zhao and Hwee Tou Ng. 2007. Identification and resolution of Chinese zero pronouns: A machine learning approach. In Proceedings of the 2007 Joint Conference on Empirical Methods on Natural Language Processing and Computational Natural Language Learning, pages 541--550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GuoDong Zhou</author>
<author>Fang Kong</author>
<author>Qiaoming Zhu</author>
</authors>
<title>Context-sensitive convolution tree kernel for pronoun resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd International Joint Conference on Natural Language Processing,</booktitle>
<pages>25--31</pages>
<contexts>
<context position="8285" citStr="Zhou et al., 2008" startWordPosition="1452" endWordPosition="1455">t, (2) use Soon et al.&apos;s method to create training instances, and (3) resolve an AZP to its closest coreferent candidate antecedent. Unlike Z&amp;N, however, K&amp;Z use the SVMlight−Tx learning algorithm (Moschitti, 1When resolving a gold AZP z, if none of the preceding candidate antecedents is classified as coreferent with it, we resolve it to the candidate that has the highest coreference likelihood with it. Here, we employ the signed distance from the SVM hyperplane to measure the coreference likelihood. 1361 2006) to train their model, employing a parse subtree known as a dynamic expansion tree (Zhou et al., 2008) as a structured feature to represent an instance. 3 Extension 1: Novel Features We propose three kinds of features to better capture the context of an AZP, as described below. Antecedent compatibility. AZPs are omitted subjects that precede VP nodes in a sentence&apos;s parse tree. From the VP node, we can extract its head verb (Predz) and the head of its object NP (Obj), if any. Note that Predz and Obj contain important contextual information for an AZP. Next, observe that if a NP is coreferent with an AZP, it should be able to fill the AZP&apos;s gap and be compatible with the gap&apos;s context. Consider</context>
</contexts>
<marker>Zhou, Kong, Zhu, 2008</marker>
<rawString>GuoDong Zhou, Fang Kong, and Qiaoming Zhu. 2008. Context-sensitive convolution tree kernel for pronoun resolution. In Proceedings of the 3rd International Joint Conference on Natural Language Processing, pages 25--31.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
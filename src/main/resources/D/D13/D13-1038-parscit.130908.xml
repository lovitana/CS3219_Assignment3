<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.027583">
<title confidence="0.988045">
Towards Situated Dialogue: Revisiting Referring Expression Generation
</title>
<author confidence="0.998712">
Rui Fang, Changsong Liu, Lanbo She, Joyce Y. Chai
</author>
<affiliation confidence="0.991395">
Department of Computer Science and Engineering
Michigan State University, East Lansing, MI, 48824, USA
</affiliation>
<email confidence="0.994198">
Ifangrui,cliu,shelanbo,jchail@cse.msu.edu
</email>
<sectionHeader confidence="0.993723" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993843428571429">
In situated dialogue, humans and agents have
mismatched capabilities of perceiving the
shared environment. Their representations
of the shared world are misaligned. Thus
referring expression generation (REG) will
need to take this discrepancy into consider-
ation. To address this issue, we developed
a hypergraph-based approach to account for
group-based spatial relations and uncertain-
ties in perceiving the environment. Our em-
pirical results have shown that this approach
outperforms a previous graph-based approach
with an absolute gain of 9%. However, while
these graph-based approaches perform effec-
tively when the agent has perfect knowledge
or perception of the environment (e.g., 84%),
they perform rather poorly when the agent has
imperfect perception of the environment (e.g.,
45%). This big performance gap calls for new
solutions to REG that can mediate a shared
perceptual basis in situated dialogue.
</bodyText>
<sectionHeader confidence="0.999125" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954645833333">
Situated human robot dialogue has received increas-
ing attention in recent years. In situated dialogue,
robots/artificial agents and their human partners are
co-present in a shared physical world. Robots need
to automatically perceive and make inference of the
shared environment. Due to its limited perceptual
and reasoning capabilities, the robot’s representation
of the shared world is often incomplete, error-prone,
and significantly mismatched from that of its human
partner’s. Although physically co-present, a joint
perceptual basis between the human and the robot
cannot be established (Clark and Brennan, 1991).
Thus, referential communication between the hu-
man and the robot becomes difficult.
How this mismatched perceptual basis affects ref-
erential communication in situated dialogue was in-
vestigated in our previous work (Liu et al., 2012).
In that work, the main focus is on reference resolu-
tion: given referential descriptions from human part-
ners, how to identify referents in the environment
even though the robot only has imperfect percep-
tion of the environment. Since robots need to col-
laborate with human partners to establish a joint per-
ceptual basis, referring expression generation (REG)
becomes an equally important problem in situated
dialogue. Robots have much lower perceptual capa-
bilities of the environment than humans. How can
a robot effectively generate referential descriptions
about the environment so that its human partner can
understand which objects are being referred to?
There has been a tremendous amount of work
on referring expression generation in the last two
decades (Dale, 1995; Krahmer and Deemter, 2012).
However, most existing REG algorithms were devel-
oped and evaluated under the assumption that agents
and humans have access to the same kind of domain
information. For example, many experimental se-
tups (Gatt et al., 2007; Viethen and Dale, 2008;
Golland et al., 2010; Striegnitz et al., 2012) were
developed based on a visual world for which the in-
ternal representation is assumed to be known and
can be represented symbolically. However, this as-
sumption no longer holds in situated dialogue with
robots. There are two important distinctions in situ-
ated dialogue. First, the perfect knowledge of the en-
vironment is not available to the agent ahead of time.
The agent needs to automatically make inferences to
connect recognized lower-level visual features with
</bodyText>
<page confidence="0.964389">
392
</page>
<note confidence="0.734258">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 392–402,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999860263157894">
symbolic labels or descriptors. Both recognition and
inference are error-prone and full of uncertainties.
Second, in situated dialogue the agent and the hu-
man have mismatched representations of the envi-
ronment. The agent needs to take this difference into
consideration to identify the most reliable features
for REG. Given these two distinctions, it is not clear
whether state-of-the-art REG approaches are appli-
cable under mismatched perceptual basis in situated
dialogue.
To address this issue, this paper revisits the prob-
lem of REG in the context of mismatched percep-
tual basis. We extended a well known graph-based
approach (Krahmer et al., 2003) that has shown
to be effective in previous work (Gatt and Belz,
2008; Gatt et al., 2009). We incorporated uncer-
tainties in perception into cost functions. We fur-
ther extended regular graph representation into hy-
pergraph representation to account for group-based
spatial relations that are important for visual descrip-
tions (Dhande, 2003; Tenbrink and Moratz, 2003;
Funakoshi et al., 2006; Liu et al., 2012). Our em-
pirical results demonstrate that both enhancements
lead to about a 9% absolute performance gain com-
pared to the original approach. However, while
our approache performs effectively when the agent
has perfect knowledge or perception of the environ-
ment (e.g., 84%), it performs poorly under the mis-
matched perceptual basis (e.g., 45%). This perfor-
mance gap calls for new solutions for REG that are
capable of mediating mismatched perceptual basis.
In the following sections, we first describe our
hypergraph-based representations and illustrate how
uncertainties from automated perception can be in-
corporated. We then describe an empirical study us-
ing Amazon Mechanical Turks for evaluating gener-
ated referring expressions. Finally we present evalu-
ation results and discuss potential future directions.
</bodyText>
<sectionHeader confidence="0.999852" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999922173076923">
Since the Full Brevity algorithm (Dale, 1989), many
approaches have been developed and evaluated for
REG (Dale, 1995; Krahmer and Deemter, 2012),
such as the incremental algorithm (Dale, 1995),
the locative algorithm (Kelleher and Kruijff, 2006),
and graph-based approaches (Krahmer et al., 2003;
Croitoru and Van Deemter, 2007). Most of these ap-
proaches assume the agent has access to a complete
symbolic representation of the domain. While these
approaches work well for many applications involv-
ing user interfaces, the question is whether they can
be extended to the situation where the agent has in-
complete or incorrect knowledge and needs to make
inference about the domain or the world.
Recently, there has been increasing interest in
REG for visual objects (Roy, 2002; Golland et al.,
2010; Mitchell et al., 2013). Some work (Golland
et al., 2010) uses visual scenes that are generated by
computer graphics and thus the internal representa-
tion of the scene is known. Some other work focuses
on the connection between lower-level visual fea-
tures and symbolic descriptors for REG (Roy, 2002;
Mitchell et al., 2013). However, most work assumes
no vision recognition errors. It is well established
that automated recognition of visual scenes is ex-
tremely challenging. This process is error-prone
and full of uncertainties. It is not clear whether
the existing approaches can be extended to the sit-
uation where the agent has imperfect perception of
the shared environment.
An earlier work by Horacek (Horacek, 2005)
has looked into the problem of mismatched knowl-
edge between conversation partners for REG. The
approach is a direct extension of the incremental al-
gorithm (Dale, 1995). However, this work only pro-
vides a proof of concept example to illustrate the
idea. No empirical evaluation was given.
All these previous works have motivated our
present investigation. We are interested in REG un-
der mismatched perceptual basis between conversa-
tion partners, where the agent has imperfect percep-
tion and knowledge of the shared environment. In
particular, we took a well-studied graph-based ap-
proach (Krahmer et al., 2003) and extended it to in-
corporate group spatial relations and uncertainties
associated with automated perception of the envi-
ronment. The reason we chose a graph-based ap-
proach is that graph representations are widely used
in the fields of computer vision (CV) and pattern
recognition to represent spatially rich scenes. Never-
theless, the findings from this investigation provide
insight to other approaches.
</bodyText>
<page confidence="0.998262">
393
</page>
<figure confidence="0.9977475">
(a) an original scene (b) the corresponding impoverished
scene
</figure>
<figureCaption confidence="0.999485">
Figure 1: An original scene and its impoverished scene processed by CV algorithm
</figureCaption>
<sectionHeader confidence="0.998383" genericHeader="method">
3 Hypergraph-based REG
</sectionHeader>
<bodyText confidence="0.999970952380953">
Towards mediating a shared perceptual basis in sit-
uated dialogue, our previous work (Liu et al., 2012)
has conducted experiments to study referential com-
munication between partners with mismatched per-
ceptual capabilities. We simulated mismatched ca-
pabilities by making an original scene (Figure 1(a))
available to a director (simulating higher perceptual
calibre) and a corresponding impoverished scene
(Figure 1(b)) available to a matcher (simulating low-
ered perceptual calibre). The impoverished scene
is created by re-rendering automated recognition re-
sults of the original scene by a CV algorithm. An
example of the original scene and an impoverished
scene is shown in Figure 1. Using this setup, the di-
rector and the matcher were instructed to collaborate
with each other on some naming games. Through
these games, they collected data on how partners
with mismatched perceptual capabilities collaborate
to ground their referential communication.
The setup in (Liu et al., 2012) is intended to sim-
ulate situated dialogue between a human (like the
director) and a robot (like the matcher). The robot
has a significantly lowered ability in perception and
reasoning. The robot’s internal representation of the
shared world will be much like the impoverished
scene which contains many recognition errors. The
data from (Liu et al., 2012; Liu et al., 2013) shows
that different strategies were used by conversation
partners to produce referential descriptions. Besides
directly describing attributes or binary relations with
a relatum, they often use group-based descriptions
(e.g., a cluster offour objects on the right). This is
mainly due to the fact that some objects are simply
not recognizable to the matcher. Binary spatial rela-
tionships sometimes are difficult to describe the tar-
get object, so the matcher must resort to group infor-
mation to distinguish the target object from the rest
of the objects. For example, suppose the matcher
needs to describe the target object 5 in Figure 1(b),
he/she may have to start by indicating the group of
three objects at the bottom and then specify the re-
lationship (i.e., top) of the target object within this
group.
The importance of group descriptions has been
shown not only here, but also in previous works
on REG (Funakoshi et al., 2004; Funakoshi et al.,
2006; Weijers, 2011). While the original graph-
based approach can effectively represent attributes
and binary relations between objects (Krahmer et al.,
2003), it is insufficient to capture within-group or
between-group relations. Therefore, to address the
low perceptual capabilities of artificial agents, we in-
troduce hypergraphs to represent the shared environ-
ment. Our approach has two unique characteristics
compared to previous graph-based approaches: (1)
A hypergraph representation is more general than
a regular graph. Besides attributes and binary re-
lations, it can also represent group-based relations.
(2) Unlike previous work, here the generation of hy-
pergraphs are completely driven by automated per-
ception of the environment. This is done by incor-
porating uncertainties in perception and reasoning
into cost functions associated with graphs. Next we
</bodyText>
<page confidence="0.99819">
394
</page>
<bodyText confidence="0.998824">
These numerical features will be further converted to
symbolic labels with certain confidence scores (de-
scribed later in Section 3.3.2).
give a detailed account on hypergraph representa-
tion, cost functions incorporating uncertainties, and
the search algorithm for REG.
</bodyText>
<subsectionHeader confidence="0.999528">
3.1 Hypergraph Representation
</subsectionHeader>
<bodyText confidence="0.999982">
A directed hypergraph G (Gallo et al., 1993) is a
tuple of the form: G = (X, A), in which
</bodyText>
<equation confidence="0.988558">
X = {xm}
A = {ai = (ti, hi)  |ti C X, hi C X}
</equation>
<bodyText confidence="0.999976105263158">
Similar to regular graphs, a hypergraph consists
of a set of nodes X and a set of arcs A. However,
different from regular graphs, each arc in A is con-
sidered as a hyperarc in the sense that it can capture
relations between any two subsets of nodes: a tail
(ti) and a head (hi). Therefore, a hypergraph is a
generalization of a regular graph. It becomes a reg-
ular graph if the cardinalities of both the tail and the
head are restricted to one for all hyperarcs. While
regular graphs are commonly used to represent bi-
nary relations between two nodes, hypergraphs pro-
vide a more general representation for n-ary rela-
tions among multiple nodes.
We use hypergraphs to represent the agent’s per-
ceived physical environment (also called scene hy-
pergraphs). More specifically, each perceived ob-
ject is represented by a node in the graph. Each per-
ceived visual attribute of an object (e.g., color, size,
type information) or a group of objects (e.g., num-
ber of objects in the group, location) is captured by
a self-looping hyperarc. Hyperarcs are also used to
capture the spatial relations between any two subsets
of nodes, whether it is a relation between two ob-
jects, or between two groups of objects, or between
one or more objects within a group of objects.
For example, Figure 2 shows a hypergraph cre-
ated for part of the impoverished scene shown in
Figure 1(b) (i.e., the upper right corner including
objects 7, 8, 9, 11, and 13). One important char-
acteristic is that, because the graph is created based
on an automated vision recognition system, the val-
ues of an attribute or a relation in the hypergraph
are numeric (except for the type attribute). For ex-
ample, the value of the color attribute is the RGB
distribution extracted from the corresponding visual
object, the value of the size attribute is the width and
height of the bounding box and the value of the lo-
cation attribute is a function of spatial coordinates.
</bodyText>
<subsectionHeader confidence="0.999666">
3.2 Hypergraph Pruning
</subsectionHeader>
<bodyText confidence="0.99988745">
The perceived visual scene can be represented as a
complete hypergraph, in which any pair of two sub-
sets of nodes are connected by a hyperarc. However,
such a complete hypergraph is not only inefficient
but also unnecessary. Instead of keeping all possible
n-ary relations (i.e., hyperarcs), we only retain those
relations that are likely used by humans to produce
referring expressions, based on two heuristics.
The first heuristic is based on perceptual prin-
ciples, also called the Gestalt Laws of perception
(Sternberg, 2003), which describe how people group
visually similar objects into entities or groups. Two
well known principles of perceptual grouping are
proximity and similarity (Wertheimer, 1938): ob-
jects that lie close together are often perceived as
groups; objects of similar shape, size or color are
more likely to form groups than objects differing
along these dimensions. Based on these two prin-
ciples, previous works have developed different al-
gorithms for perceptual grouping (Thrisson, 1994;
Gatt, 2006). In our investigation, we adopted Gatt’s
algorithm (Gatt, 2006), which has shown to be more
accurate for spatial grouping. Given the results
from spatial grouping, we only retain hyperarcs that
represent spatial relations between two objects, be-
tween two perceived groups, between one object and
a perceived group, or between one object and the
group it belongs to.
The second heuristic is based on the observation
that, given a certain orientation, people tend to use a
relatum that is closer to the referent than more dis-
tant relata. In other words, it is less likely to refer to
an object relative to a distant relatum when there is
a closer relatum. For example, when referring to the
stapler (object 9 in Figure 1(a) ), it is more likely to
use “the stapler above the battery” than “the stapler
above the cellphone”. Based on this observation, we
prune the hypergraphs by only retaining hyperarcs
between an object and their closest relata for each
possible orientation.
</bodyText>
<figureCaption confidence="0.934728666666667">
Figure 2 shows the resulting hypergraph for rep-
resenting a subset of objects (7, 8, 9, 11, and 13) in
Figure 1(a).
</figureCaption>
<page confidence="0.989579">
395
</page>
<figureCaption confidence="0.992185">
Figure 2: An example of hypergraph representing the per-
ceived scene (a partial scene only including object 7, 8,
9, 11, 13 for Figure 1(a)).
</figureCaption>
<subsectionHeader confidence="0.999273">
3.3 Symbolic Descriptors for Attributes
</subsectionHeader>
<bodyText confidence="0.999992">
As mentioned earlier, the values of attributes of ob-
jects and their relations are numerical in nature. In
order for the agent to generate natural language de-
scriptions, the first step is to assign symbolic labels
or descriptors to those attributes and relations. Next
we describe how we use a lexicon with grounded se-
mantics in this process.
</bodyText>
<subsectionHeader confidence="0.989926">
3.3.1 Lexicon with Grounded Semantics
</subsectionHeader>
<bodyText confidence="0.999954631578947">
Grounded semantics provides a bridge to connect
symbolic labels or words with lower level visual fea-
tures (Harnad, 1990). Previous work has developed
various approaches for grounded semantics mainly
for the reference resolution task, i.e., identifying vi-
sual objects in the environment given language de-
scriptions (Dhande, 2003; Gorniak and Roy, 2004;
Tenbrink and Moratz, 2003; Siebert and Schlangen,
2008; Liu et al., 2012). For the referring expression
generation task here, we also need a lexicon with
grounded semantics.
In our lexicon, the semantics of each category
of words is defined by a set of semantic grounding
functions that are parameterized on visual features.
For example, for the color category it is defined as a
multivariate Gaussian distribution based on the RGB
distribution. Specific words such as green, red, or
blue have different means and co-variances as the
following:
</bodyText>
<equation confidence="0.999844666666667">
color : red = fr(vcolor) = N(vcolor  |A1, E1)
color : green = fg(vcolor) = N(vcolor  |A2, E2)
color : blue = fb(vcolor) = N(vcolor  |A3, E3)
</equation>
<bodyText confidence="0.999927375">
The above functions define how likely a set of rec-
ognized visual features (i.e., vcolor) describing the
color dimensions (i.e., RGB distribution) is to match
the color terms red, green, and blue.
For the spatial relation terms such as above, be-
low, left, right, the semantic grounding functions
take both vertical and horizontal coordinates of two
objects, as follows 1:
</bodyText>
<equation confidence="0.9945648">
spatial(Rel : above(a, b) = fabgo�ve(�valoc, vbloc)
I 1 — |xa−xb |if ya &lt; Yb;
&lt;Il 400
=
0 otherwise.
</equation>
<bodyText confidence="0.999944428571429">
Using the above convention, we have defined se-
mantic grounding functions for size category words
(e.g., small and big) and absolute position words
(e.g., top, below, left, and right). In addition, we use
object recognition models (Zhang and Lu, 2002) to
define class type category words such as apple and
orange used in our domain.
</bodyText>
<subsectionHeader confidence="0.986329">
3.3.2 Attribute Descriptors and Cost Functions
</subsectionHeader>
<bodyText confidence="0.999956363636364">
Given the lexicon with grounded semantics as de-
scribed above, the numerical attributes captured in
the scene hypergraph can be converted to symbolic
descriptors. For each attribute (e.g., color) or re-
lation, the corresponding visual feature vector (i.e.,
vcolor) is plugged into the semantic grounding func-
tions for the corresponding category of words. The
word that best describes the attribute is chosen as the
descriptor for that attribute. For example, given an
RGB color distribution vcolor, we can find the color
descriptor as follows:
</bodyText>
<construct confidence="0.476621">
color: w* = arg max fw(�vcolor),
red,green,blue
</construct>
<bodyText confidence="0.988933666666667">
For each attribute or relation, we can find a best
descriptor in this manner. In addition, we also ob-
tain a numerical value (returned from the semantic
</bodyText>
<footnote confidence="0.98509">
1The size of the overall scene is 800x800.
</footnote>
<page confidence="0.997173">
396
</page>
<bodyText confidence="0.999362666666667">
grounding functions) that measures how well this
descriptor describes the corresponding visual fea-
tures. Intuitively, one would choose a descriptor that
closely matches the visual features. Based on this
intuition, we define the cost for each attribute A as
the following:
</bodyText>
<equation confidence="0.981775">
cost(A) = 1 − fw*( vA)
</equation>
<bodyText confidence="0.999923">
where w* is the best descriptor for the attribute.
Given an attribute, the better the descriptor
matches the extracted visual features, the lower the
cost of the corresponding hyperarc.
</bodyText>
<subsectionHeader confidence="0.954391">
3.4 Graph Matching for REG
</subsectionHeader>
<bodyText confidence="0.999944181818182">
Now the hypergraph representing the perceived en-
vironment has symbolic descriptors for its attributes
and relations together with corresponding costs.
Given this representation, REG can be formulated as
a graph matching algorithm similar to that described
in (Krahmer et al., 2003). We use the same Branch
and Bound algorithm described in (Krahmer et al.,
2003). In this approach, a hypothesis hypergraph
(starting with one node representing the target ob-
ject) is gradually expanded by adding in a least cost
hyperarc from the scene hypergraph. At each ex-
pansion, the hypothesis graph is matched against the
scene hypergraph to decide whether it matches any
nodes other than the target node in the scene hyper-
graph. The expansion stops if the hypothesis graph
does not cover any other nodes except for the target
node. At this point, the hypothesis graph captures all
the content (e.g., attributes and relations) required to
uniquely describe the target object. We then apply
a set of simple generation templates to generate the
surface form of referring expressions based on the
hypothesis graph.
</bodyText>
<sectionHeader confidence="0.996731" genericHeader="method">
4 Empirical Evaluations
</sectionHeader>
<subsectionHeader confidence="0.969562">
4.1 Evaluation Setup
</subsectionHeader>
<bodyText confidence="0.99990512">
To evaluate the performance of this hypergraph-
based approach to REG, we conducted a compara-
tive study using crowd-sourcing. More specifically,
we created 48 different scenes similar to that in Fig-
ure 1(a). Each scene has 13 objects on average and
there are 621 objects in total. For each of these
scenes, we applied a CV algorithm (Zhang and Lu,
2002) and generated scene hypergraphs as described
in Section 3.1. We then use different generation
strategies (varied in terms of graph representations
and cost functions, to be explained in Section 4.2) to
automatically generate referring expressions to refer
to each object.
To evaluate the quality of these generated refer-
ring expressions, we applied Amazon Mechanical
Turk to solicit feedback from the crowd 2. Through
an interface, we displayed an original scene and gen-
erated referring expressions (from different genera-
tion strategies) in a random order. We asked each
turk to select the object in the scene that he/she be-
lieved was the one referred to by the shown refer-
ring expression (i.e., reference identification task).
Each referring expression received three votes from
the crowd. In total, 217 turks participated in our ex-
periment.
</bodyText>
<subsectionHeader confidence="0.976859">
4.2 Generation Strategies
</subsectionHeader>
<bodyText confidence="0.99970225">
We applied a set of different strategies to generate
referring expressions for each object. The variations
lie in two dimensions: (1) different graph repre-
sentations: using a hypergraph to represent the per-
ceived scene as described in Section 3.1 versus us-
ing a regular graph as introduced in (Krahmer et al.,
2003); and (2) different cost functions for attributes
and relations: cost functions that have been used in
previous works (Theune et al., 2007; Krahmer et al.,
2008) and cost functions that incorporate uncertain-
ties of perception as described in Section 3.3.2.
Cost functions play an important role in graph-
based approaches (Krahmer et al., 2003). Previous
works have examined different types of cost func-
tions (Theune et al., 2007; Krahmer et al., 2008;
Theune et al., 2011). We adopted some commonly
used cost functions from previous work together
with the cost functions defined here. In particular,
we experimented with the following different cost
functions:
</bodyText>
<listItem confidence="0.985840666666667">
Simple Cost: The costs for all hyperarcs are set to
1. With this cost function, the graph-based algorithm
resembles the Full Brevity algorithm of Dale (Dale,
</listItem>
<bodyText confidence="0.947170333333333">
2To control the quality of crowdsourcing, we recruited par-
ticipants based on the following criteria: Participants’ locations
are limited to the United States. Approval rate for each partic-
ipant’s previous work is greater than or equal to 95%, and the
number of each participant’s previous approved work is greater
than or equal to 1000.
</bodyText>
<page confidence="0.9962">
397
</page>
<bodyText confidence="0.999273268292683">
1992) in that a shortest distinguishing description is
preferred.
Absolute Preferred: The costs for hyperarcs rep-
resenting absolute attributes (e.g., type, color, and
position) are set to 1. The costs for relative at-
tributes (e.g., size) and relations are set to 2. This
cost function mimics human’s preference for abso-
lute attributes over relative ones (Dale, 1995).
Relative Preferred: The costs for hyperarcs repre-
senting absolute attributes are set to 2 and for rela-
tive attributes and relations are set to 1. This cost
function has been applied previously to emphasize
the importance of spatial relations in REG (Viethen
and Dale, 2008).
Uncertainty Based: The costs for all hyperarcs are
defined by incorporating uncertainties from percep-
tion as described in Section 3.3.2.
Uncertainty Relative Preferred: To emphasize the
importance of spatial relations as demonstrated in
situated interaction (Tenbrink and Moratz, 2003;
Kelleher and Kruijff, 2006), the costs for hyperarcs
representing relative attributes and relations are di-
vided by 3. This cost function will allow the algo-
rithm to prefer spatial relations through the reduced
cost.
Note that we only tested a few (not all) com-
monly used cost functions proposed by previous
work (Krahmer et al., 2003; Theune et al., 2007;
Krahmer et al., 2008; Theune et al., 2011). For ex-
ample, we did not include the stochastic cost func-
tion which is defined based on the frequencies of at-
tribute selection from the training data (Krahmer et
al., 2003). On the one hand, we did not have a large
set of human descriptions of the impoverished scene
to learn the stochastic cost. On the other hand, it
is not clear whether human strategies of describing
the impoverished scene should be used to represent
optimal strategies for the robot. Nevertheless, the
above different cost functions will allow us to eval-
uate whether incorporating perceptual uncertainties
will make a difference in the REG performance.
</bodyText>
<subsectionHeader confidence="0.998514">
4.3 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.991871">
As mentioned earlier, each generated referring ex-
pression received three independent votes regarding
its referent from the crowd. The referent with the
most votes is taken as the predicted referent and is
used for evaluation. If all three votes are differ-
</bodyText>
<table confidence="0.998311333333333">
Cost Function Regular Graph Hypergraph
Simple Costs 33.2% 33.3%
Absolute Preferred 30.1% 30.3%
Relative Preferred 31.1% 35.4%
Uncertainty Based 35.7% 37.5%
Uncertainty Rel. Prefer. 36.7% 45.2%
</table>
<tableCaption confidence="0.999991">
Table 1: Results with different cost functions
</tableCaption>
<bodyText confidence="0.999809142857143">
ent, then by default, it is deemed that the referent
is not correctly identified for that expression. We
use the accuracy of the referential identification task
(i.e., the percentage of generated referring expres-
sions where the referents are correctly identified) as
the metric to evaluate different generation strategies
illustrated in Section 4.2.
</bodyText>
<subsubsectionHeader confidence="0.657946">
4.3.1 The Role of Cost Functions
</subsubsectionHeader>
<bodyText confidence="0.9994166">
Table 1 shows the results based on different cost
functions and different graph representations. There
are several observations.
First, when the agent does not have perfect knowl-
edge of the environment and has to automatically
infer the environment as in our setting here, cost
functions based on uncertainties of perception lead
to better results. This occurs for both regular graphs
and hypergraphs. This result is not surprising and
indicates that cost functions should be tied to the
agent’s ability to perceive and infer the environment.
The uncertainty based cost functions allow the agent
to prefer reliable attributes or relations.
Second, consistent with previous work (Viethen
and Dale, 2008), we observed the importance of spa-
tial relations. Especially when the perceived world
is full of uncertainties, spatial relations tend to be
more reliable. In particular, as shown in Table 1,
using hypergraphs enables generating group-based
relations and results in significantly better perfor-
mance (45.2%) compared to regular graphs (36.7%)
(p = 0.002).
Note that our current cost function only includes
uncertainties of the agent’s own perception in a sim-
plistic form. When humans and agents have mis-
matched perceptual basis, the human’s model of
comprehension and tolerance of inaccurate descrip-
tion could play a role in REG. Incorporating human
models in the cost function will require in-depth em-
pirical studies and we will leave that to our future
</bodyText>
<page confidence="0.996566">
398
</page>
<bodyText confidence="0.761563">
work.
</bodyText>
<subsectionHeader confidence="0.761859">
4.3.2 The Role of Imperfect Perception
</subsectionHeader>
<bodyText confidence="0.999892822222222">
To further understand the role of hypergraphs in
mediating mismatched perceptions between humans
and agents, we created a perfect scene regular graph
and a perfect scene hypergraph (representing the
agent’s perfect knowledge of the environment) for
each of the 48 scenes used in the experiments. In
each of these scene graphs, the attribute and rela-
tion descriptors are manually provided. We fur-
ther applied the Absolute Preferred cost function
(which has shown competitive performance in previ-
ous work) to generate referring expressions for each
object. Again, each referring expression received
three votes from the crowd.
Table 2 shows the results comparing two con-
ditions: (1) REs generated (by the Absolute Pre-
ferred cost function) based on the perfect graphs
which represent the agent’s perfect knowledge and
perception of the environment; and (2) REs gener-
ated based on automatically created graphs (by the
Uncertainty Relative Preferred cost function) which
represent the agent’s imperfect knowledge of the
environment as a result of automated recognition
and inference. The result shows that given perfect
knowledge of the environment, hypergraphs only
perform marginally better than the regular graphs
(p = 0.07). Given imperfect knowledge of the envi-
ronment, hypergraphs significantly outperforms the
regular graphs by taking advantage of spatial group-
ing information (p = 0.002). It is worthwhile to
mention that currently we use spatial proximity to
identify groups. However, the hypergraph based ap-
proach is not restricted to spatial grouping. In the-
ory, it can represent any type of group based on dif-
ferent similarity criteria.
Furthermore, our result shows that the graph-
based approaches perform quite competitively under
the condition of perfect knowledge and perception.
Although evaluated on different data sets, this result
is consistent with results from previous work (Gatt
and Belz, 2008; Gatt et al., 2009). However, what is
more interesting here is that while graph-based ap-
proaches perform well when the agent has perfect
knowledge of the environment, as its human part-
ner, these approaches literally fall apart with close
to 40% performance degradation when applied to
</bodyText>
<table confidence="0.997076">
Environment Regular Graph Hypergraph
Pefect Perception 80.4% 84.2%
Imperfect Perception 36.7% 45.2%
</table>
<tableCaption confidence="0.9822565">
Table 2: Results of comparing perfect perception and im-
perfect perception of the shared world.
</tableCaption>
<bodyText confidence="0.999798736842105">
the situation where the agent’s representation of the
shared world is problematic and full of mistakes.
These results indicate that REG for automati-
cally perceived scenes can be extremely challeng-
ing. Many errors result from automated perception
and reasoning that will affect the internal representa-
tion of the world and thus the generated REs. In our
experiments here, we applied a very basic CV algo-
rithm which resulted in rather poor performance in
our data: overall, 60.3% of objects in the original
scene are mis-recognized, and 10.5% of objects are
mis-segmented. We think this poor CV performance
represents a more challenging problem.
Some errors such as recognition errors can be by-
passed using our current approach based on hyper-
graphs. For example, in Figure 1 target object 9 (a
stapler) and 13 (a key) are mis-recognized as a cup
and a pen. Using our hypergraph-based approach,
for the target object 9, instead of generating “a small
cup” (as in the case of using regular graphs), “a gray
object on the top within a cluster of four objects”
is generated. For the target object 13, instead of “a
pen” as generated by regular graphs, “a small object
on the right within a cluster of 4” is generated. Even
with recognition errors, these group-based descrip-
tions will allow the listener to identify target objects
in their representation correctly. Nevertheless, many
processing errors cannot be handled by our current
approach. For example, an object can be mistak-
enly segmented into multiple parts or several objects
can be mistakenly grouped into one object. In addi-
tion, our current semantic grounding functions are
simple. Sometimes they do not provide correct de-
scriptors for the extracted visual features. More so-
phisticated functions that better reflect human’s vi-
sual perception (Regier, 1996; Mojsilovic, 2005;
Mitchell et al., 2011) should be pursued in the fu-
ture.
</bodyText>
<page confidence="0.996591">
399
</page>
<table confidence="0.999520666666667">
Minimum Effort Extra Effort
Pefect Perception 84.2% 88.1%
Imperfect Perception 45.2% 51.5%
</table>
<tableCaption confidence="0.975419">
Table 3: Results of comparing minimum effort and extra
effort using hypergraphs
</tableCaption>
<subsectionHeader confidence="0.711648">
4.3.3 The Role of Extra Effort
</subsectionHeader>
<bodyText confidence="0.999985482758621">
While REG systems have a tendency to produce
minimal descriptions, recent psycholinguistic stud-
ies have shown that speakers do not necessarily fol-
low the Grice’s maxim of quantity, and they tend
to provide redundant properties in their descrip-
tions (Jordan and Walker, 2000; Belke and Meyer,
2002; Arts et al., 2011). With this in mind, we
conducted a very simple evaluation on the role of
extra effort. Once a set of descriptors are selected
based on the minimum cost, one additional descrip-
tor (with the least cost among the remaining at-
tributes or relations) is added to the referential de-
scription. We once again solicited the crowd feed-
back to this set of expressions generated by extra
effort. Each expression again received three votes
from the crowd.
Table 3 shows the results by comparing minimum
effort with extra effort when using hypergraphs to
generate REs. As indicated here, extra effort (by
adding one additional descriptor) leads to more com-
prehensible REs with 3.9% improvement under per-
fect perception and 6.3% improvement under imper-
fect perception (both are significant, p &lt; 0.05). The
improvement is larger under imperfect perception.
This seems to indicate that exploring extra effort in
REG could help mediate mismatched perceptions in
situated dialogue. However, more understanding on
how to engage in such extra effort will be required
in the future.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999986138888889">
In situated dialogue, humans and agents have mis-
matched perceptions of the shared environment. To
facilitate successful referential communication be-
tween a human and an agent, the agent needs to take
such discrepancies into consideration and generate
referential descriptions that can be understood by
its human partner. With this in mind, we re-visited
the problem of referring expression generation in the
context of mismatched perceptions between humans
and agents. In particular, we applied and extended
the state of the art graph-based approach (Krahmer
et al., 2003) in this new setting. Our empirical re-
sults have shown that, to address the agent’s limited
perceptual capability, REG algorithms will need to
take into account the uncertainties in perception and
reasoning. Group-based information appears more
reliable and thus should be modeled by an approach
that deals with automated perception of spatially
rich scenes.
While graph-based approaches have shown effec-
tive for the situation where the agent has complete
knowledge of the environment, as its human part-
ner, these approaches are often inadequate when hu-
mans and agents have mismatched representations
of the shared world. Our empirical results here call
for new solutions to address the mismatched per-
ceptual basis. Previous work indicated that referen-
tial communication is a collaborative process (Clark
and Wilkes-Gibbs, 1986; Heeman and Hirst, 1995).
Conversation partners make extra effort to collab-
orate with each other. For the situation with mis-
matched perceptual basis, a potential solution thus
should go beyond the objective of generating a mini-
mum description, and towards a collaborative model
which incorporates immediate feedback from the
conversation partner (Edmonds, 1994).
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999226">
This work was supported by N00014-11-1-0410
from the Office of Naval Research and IIS-1208390
from the National Science Foundation.
</bodyText>
<sectionHeader confidence="0.99861" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997699307692308">
Anja Arts, Alfons Maes, Leo Noordman, and Carel
Jansen. 2011. Overspecification facilitates object
identification. Journal of Pragmatics, 43(1):361–374.
E. Belke and A. S. Meyer. 2002. Tracking the time
course of multidimensional stimulus discrimination:
Analyses of viewing patterns and processing times
during ”same”-”different” decisions. European Jour-
nal of Cognitive Psychology, 14(2):237–266.
H.H. Clark and S.E. Brennan. 1991. Grounding in com-
munication. Perspectives on socially shared cognition,
13:127–149.
H. H Clark and D Wilkes-Gibbs. 1986. Referring as a
collaborative process. Cognition, 22:1–39.
</reference>
<page confidence="0.985601">
400
</page>
<reference confidence="0.997984952380953">
Madalina Croitoru and Kees Van Deemter. 2007. A con-
ceptual graph approach to the generation of referring
expressions. In Proceedings of the 20th international
joint conference on Artifical intelligence, IJCAI’07,
pages 2456–2461.
Robert Dale. 1989. Cooking up referring expressions.
In Proceedings of the 27th annual meeting on Associ-
ation for Computational Linguistics, ACL ’89, pages
68–75, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Robert Dale. 1992. Generating Referring Expressions:
Constructing Descriptions in a Domain of Objects and
Processes. The MIT Press,Cambridge, Massachusetts.
Robert Dale. 1995. Computational interpretations of the
gricean maxims in the generation of referring expres-
sions. Cognitive Science, 19:233–263.
Sheel Sanjay Dhande. 2003. A computational model to
connect gestalt perception and natural language. In
Masters thesis, Massachusetts Institure of Technology.
Philip G. Edmonds. 1994. Collaboration on reference to
objects that are not mutually known. In Proceedings
of the 15th conference on Computational linguistics -
Volume 2, COLING ’94, pages 1118–1122, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Kotaro Funakoshi, Satoru Watanabe, Naoko Kuriyama,
and Takenobu Tokunaga. 2004. Generation of relative
referring expressions based on perceptual grouping. In
COLING.
Kotaro Funakoshi, Satoru Watanabe, and Takenobu
Tokunaga. 2006. Group-based generation of referring
expressions. In INLG, pages 73–80.
Giorgio Gallo, Giustino Longo, Stefano Pallottino, and
Sang Nguyen. 1993. Directed hypergraphs and ap-
plications. Discrete applied mathematics, 42(2):177–
201.
Albert Gatt and Anja Belz. 2008. Attribute selection for
referring expression generation: new algorithms and
evaluation methods. In Proceedings of the Fifth In-
ternational Natural Language Generation Conference,
INLG ’08, pages 50–58, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Albert Gatt, Ielka van der Sluis, and Kees van Deemter.
2007. Evaluating algorithms for the generation of re-
ferring expressions using a balanced corpus. In Pro-
ceedings of the Eleventh European Workshop on Nat-
ural Language Generation, ENLG ’07, pages 49–56,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The tuna-
reg challenge 2009: overview and evaluation results.
In Proceedings of the 12th European Workshop on
Natural Language Generation, ENLG ’09, pages 174–
182, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Albert Gatt. 2006. Structuring knowledge for reference
generation: A clustering algorithm. In Proceedings of
the 11th Conference of the European Chapter of the
Association for Computational Linguistics, Associa-
tion for Computational Linguistics, pages 321–328.
Dave Golland, Percy Liang, and Dan Klein. 2010. A
game-theoretic approach to generating spatial descrip-
tions. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ’10, pages 410–419, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Peter Gorniak and Deb Roy. 2004. Grounded seman-
tic composition for visual scenes. Journal of Artificial
Intelligence Research, 21:429–470.
Stevan Harnad. 1990. The symbol grounding problem.
Physica D, 42:335–346.
Peter A. Heeman and Graeme Hirst. 1995. Collaborating
on referring expressions. Computational Linguistics,
21:351–382.
Helmut Horacek. 2005. Generating referential descrip-
tions under conditions of uncertainty. In Proceedings
of the 10th European Workshop on Natural Language
Generation (ENLG) pages 58-67, Aberdeen, UK.
Pamela W Jordan and Marilyn Walker. 2000. Learning
attribute selections for non-pronominal expressions.
In Proceedings of the 38th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 181-190.
John D. Kelleher and Geert-Jan M. Kruijff. 2006. In-
cremental generation of spatial referring expressions
in situated dialog. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Com-
putational Linguistics, ACL-44, pages 1041–1048,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Emiel Krahmer and Kees Van Deemter. 2012. Compu-
tational generation of referring expressions: A survey.
computational linguistics, 38(1):173–218.
Emiel Krahmer Krahmer, Sebastiaan van Erk, and Andr´e
Verleg. 2003. Graph-based generation of referring
expressions. Computational Linguistics, 29(1):53–72,
March.
Emiel Krahmer, Mariet Theune, Jette Viethen, and Iris
Hendrickx. 2008. Graph: The costs of redundancy
in referring expressions. In In Proceedings of the 5th
International Conference on Natural Language Gen-
eration, Salt Fork OH, USA.
Changsong Liu, Rui Fang, and Joyce Y. Chai. 2012. To-
wards mediating shared perceptual basis in situated di-
alogue. In Proceedings of the 13th Annual Meeting of
</reference>
<page confidence="0.980731">
401
</page>
<reference confidence="0.999754539473684">
the Special Interest Group on Discourse and Dialogue,
SIGDIAL ’12, pages 140–149, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Changsong Liu, Rui Fang, Lanbo She, and Joyce Y. Chai.
2013. Modeling collaborative referring for situated
referential grounding. In The 14th Annual SIGdial
Meeting on Discourse and Dialogue.
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2011. Two approaches for generating size modifiers.
In Proceedings of the 13th European Workshop on
Natural Language Generation, ENLG ’11, pages 63–
70, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2013. Generating expressions that refer to visible ob-
jects. In Proceedings of NAACL-HLT 2013, pages
1174-1184.
Aleksandra Mojsilovic. 2005. A computational model
for color naming and describing color composition
of images. IEEE Transactions on Image Processing,
14:690 – 699.
Terry Regier. 1996. The human semantic potential. The
MIT Press,Cambridge, Massachusetts.
Deb Roy. 2002. Learning visually grounded words and
syntax of natural spoken language. Evolution of Com-
munication, 4.
Alexander Siebert and David Schlangen. 2008. A sim-
ple method for resolution of definite reference in a
shared visual context. In Proceedings of the 9th SIG-
dial Workshop on Discourse and Dialogue, SIGdial
’08, pages 84–87, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Robert Sternberg. 2003. Cognitive Psychology,Third
Edition. Thomson Wadsworth.
Kristina Striegnitz, Hendrik Buschmeier, and Stefan
Kopp. 2012. Referring in installments: a corpus
study of spoken object references in an interactive vir-
tual environment. In Proceedings of the Seventh In-
ternational Natural Language Generation Conference,
INLG ’12, pages 12–16, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Thora Tenbrink and Reinhard Moratz. 2003. Group-
based spatial reference in linguistic human-robot in-
teraction. Spatial Cognition and Computation, 6:63–
106.
Mari¨et Theune, Pascal Touset, Jette Viethen, and Emiel
Krahmer. 2007. Cost-based attribute selection for gre
(graph-sc/graph-fp). In Proceedings of the MT Summit
XI Workshop on Using Corpora for NLG: Language
Generation and Machine Translation (UCNLG+MT).
Mari¨et Theune, Ruud Koolen, Emiel Krahmer, and
Sander Wubben. 2011. Does size matter – how much
data is required to train a reg algorithm? In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 660–664, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Kristinn R. Thrisson. 1994. Simulated perceptual group-
ing: An application to human-computer interaction. In
Proceedings of the Sixteenth Annual Conference of the
Cognitive Science Society, pages 876–881.
Jette Viethen and Robert Dale. 2008. The use of spa-
tial relations in referring expression generation. In
Proceedings of the Fifth International Natural Lan-
guage Generation Conference, INLG ’08, pages 59–
67, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
S. Weijers. 2011. Referring expressions with groups as
landmarks. volume 15. University of Twente.
Max Wertheimer. 1938. Laws of organization in per-
ceptual forms. A Source Book of Gestalt Psychology.
Routledge and Kegan Paul, London.
Dengsheng Zhang and Guojun Lu. 2002. An integrated
approach to shape based image retrieval. In Proc.
of 5th Asian Conference on Computer Vision (ACCV,
pages 652–657.
</reference>
<page confidence="0.998599">
402
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.711757">
<title confidence="0.999882">Towards Situated Dialogue: Revisiting Referring Expression Generation</title>
<author confidence="0.998156">Rui Fang</author>
<author confidence="0.998156">Changsong Liu</author>
<author confidence="0.998156">Lanbo She</author>
<author confidence="0.998156">Y Joyce</author>
<affiliation confidence="0.9997">Department of Computer Science and</affiliation>
<address confidence="0.732335">Michigan State University, East Lansing, MI, 48824,</address>
<email confidence="0.989669">Ifangrui,cliu,shelanbo,jchail@cse.msu.edu</email>
<abstract confidence="0.998924818181818">In situated dialogue, humans and agents have mismatched capabilities of perceiving the shared environment. Their representations of the shared world are misaligned. Thus referring expression generation (REG) will need to take this discrepancy into consideration. To address this issue, we developed a hypergraph-based approach to account for group-based spatial relations and uncertainties in perceiving the environment. Our empirical results have shown that this approach outperforms a previous graph-based approach with an absolute gain of 9%. However, while these graph-based approaches perform effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), they perform rather poorly when the agent has imperfect perception of the environment (e.g., 45%). This big performance gap calls for new solutions to REG that can mediate a shared perceptual basis in situated dialogue.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anja Arts</author>
<author>Alfons Maes</author>
<author>Leo Noordman</author>
<author>Carel Jansen</author>
</authors>
<title>Overspecification facilitates object identification.</title>
<date>2011</date>
<journal>Journal of Pragmatics,</journal>
<volume>43</volume>
<issue>1</issue>
<contexts>
<context position="32818" citStr="Arts et al., 2011" startWordPosition="5182" endWordPosition="5185">eption (Regier, 1996; Mojsilovic, 2005; Mitchell et al., 2011) should be pursued in the future. 399 Minimum Effort Extra Effort Pefect Perception 84.2% 88.1% Imperfect Perception 45.2% 51.5% Table 3: Results of comparing minimum effort and extra effort using hypergraphs 4.3.3 The Role of Extra Effort While REG systems have a tendency to produce minimal descriptions, recent psycholinguistic studies have shown that speakers do not necessarily follow the Grice’s maxim of quantity, and they tend to provide redundant properties in their descriptions (Jordan and Walker, 2000; Belke and Meyer, 2002; Arts et al., 2011). With this in mind, we conducted a very simple evaluation on the role of extra effort. Once a set of descriptors are selected based on the minimum cost, one additional descriptor (with the least cost among the remaining attributes or relations) is added to the referential description. We once again solicited the crowd feedback to this set of expressions generated by extra effort. Each expression again received three votes from the crowd. Table 3 shows the results by comparing minimum effort with extra effort when using hypergraphs to generate REs. As indicated here, extra effort (by adding on</context>
</contexts>
<marker>Arts, Maes, Noordman, Jansen, 2011</marker>
<rawString>Anja Arts, Alfons Maes, Leo Noordman, and Carel Jansen. 2011. Overspecification facilitates object identification. Journal of Pragmatics, 43(1):361–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Belke</author>
<author>A S Meyer</author>
</authors>
<title>Tracking the time course of multidimensional stimulus discrimination: Analyses of viewing patterns and processing times during ”same”-”different” decisions.</title>
<date>2002</date>
<journal>European Journal of Cognitive Psychology,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="32798" citStr="Belke and Meyer, 2002" startWordPosition="5178" endWordPosition="5181">ect human’s visual perception (Regier, 1996; Mojsilovic, 2005; Mitchell et al., 2011) should be pursued in the future. 399 Minimum Effort Extra Effort Pefect Perception 84.2% 88.1% Imperfect Perception 45.2% 51.5% Table 3: Results of comparing minimum effort and extra effort using hypergraphs 4.3.3 The Role of Extra Effort While REG systems have a tendency to produce minimal descriptions, recent psycholinguistic studies have shown that speakers do not necessarily follow the Grice’s maxim of quantity, and they tend to provide redundant properties in their descriptions (Jordan and Walker, 2000; Belke and Meyer, 2002; Arts et al., 2011). With this in mind, we conducted a very simple evaluation on the role of extra effort. Once a set of descriptors are selected based on the minimum cost, one additional descriptor (with the least cost among the remaining attributes or relations) is added to the referential description. We once again solicited the crowd feedback to this set of expressions generated by extra effort. Each expression again received three votes from the crowd. Table 3 shows the results by comparing minimum effort with extra effort when using hypergraphs to generate REs. As indicated here, extra </context>
</contexts>
<marker>Belke, Meyer, 2002</marker>
<rawString>E. Belke and A. S. Meyer. 2002. Tracking the time course of multidimensional stimulus discrimination: Analyses of viewing patterns and processing times during ”same”-”different” decisions. European Journal of Cognitive Psychology, 14(2):237–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>S E Brennan</author>
</authors>
<title>Grounding in communication. Perspectives on socially shared cognition,</title>
<date>1991</date>
<pages>13--127</pages>
<contexts>
<context position="1820" citStr="Clark and Brennan, 1991" startWordPosition="252" endWordPosition="255">ction Situated human robot dialogue has received increasing attention in recent years. In situated dialogue, robots/artificial agents and their human partners are co-present in a shared physical world. Robots need to automatically perceive and make inference of the shared environment. Due to its limited perceptual and reasoning capabilities, the robot’s representation of the shared world is often incomplete, error-prone, and significantly mismatched from that of its human partner’s. Although physically co-present, a joint perceptual basis between the human and the robot cannot be established (Clark and Brennan, 1991). Thus, referential communication between the human and the robot becomes difficult. How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work (Liu et al., 2012). In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment. Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes a</context>
</contexts>
<marker>Clark, Brennan, 1991</marker>
<rawString>H.H. Clark and S.E. Brennan. 1991. Grounding in communication. Perspectives on socially shared cognition, 13:127–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>D Wilkes-Gibbs</author>
</authors>
<title>Referring as a collaborative process.</title>
<date>1986</date>
<journal>Cognition,</journal>
<pages>22--1</pages>
<marker>Clark, Wilkes-Gibbs, 1986</marker>
<rawString>H. H Clark and D Wilkes-Gibbs. 1986. Referring as a collaborative process. Cognition, 22:1–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Madalina Croitoru</author>
<author>Kees Van Deemter</author>
</authors>
<title>A conceptual graph approach to the generation of referring expressions.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI’07,</booktitle>
<pages>2456--2461</pages>
<marker>Croitoru, Van Deemter, 2007</marker>
<rawString>Madalina Croitoru and Kees Van Deemter. 2007. A conceptual graph approach to the generation of referring expressions. In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI’07, pages 2456–2461.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Cooking up referring expressions.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th annual meeting on Association for Computational Linguistics, ACL ’89,</booktitle>
<pages>68--75</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5744" citStr="Dale, 1989" startWordPosition="857" endWordPosition="858"> (e.g., 84%), it performs poorly under the mismatched perceptual basis (e.g., 45%). This performance gap calls for new solutions for REG that are capable of mediating mismatched perceptual basis. In the following sections, we first describe our hypergraph-based representations and illustrate how uncertainties from automated perception can be incorporated. We then describe an empirical study using Amazon Mechanical Turks for evaluating generated referring expressions. Finally we present evaluation results and discuss potential future directions. 2 Related Work Since the Full Brevity algorithm (Dale, 1989), many approaches have been developed and evaluated for REG (Dale, 1995; Krahmer and Deemter, 2012), such as the incremental algorithm (Dale, 1995), the locative algorithm (Kelleher and Kruijff, 2006), and graph-based approaches (Krahmer et al., 2003; Croitoru and Van Deemter, 2007). Most of these approaches assume the agent has access to a complete symbolic representation of the domain. While these approaches work well for many applications involving user interfaces, the question is whether they can be extended to the situation where the agent has incomplete or incorrect knowledge and needs t</context>
</contexts>
<marker>Dale, 1989</marker>
<rawString>Robert Dale. 1989. Cooking up referring expressions. In Proceedings of the 27th annual meeting on Association for Computational Linguistics, ACL ’89, pages 68–75, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Generating Referring Expressions: Constructing Descriptions in a Domain of Objects and Processes. The MIT Press,Cambridge,</title>
<date>1992</date>
<location>Massachusetts.</location>
<marker>Dale, 1992</marker>
<rawString>Robert Dale. 1992. Generating Referring Expressions: Constructing Descriptions in a Domain of Objects and Processes. The MIT Press,Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Computational interpretations of the gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<pages>19--233</pages>
<contexts>
<context position="2824" citStr="Dale, 1995" startWordPosition="409" endWordPosition="410">though the robot only has imperfect perception of the environment. Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue. Robots have much lower perceptual capabilities of the environment than humans. How can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to? There has been a tremendous amount of work on referring expression generation in the last two decades (Dale, 1995; Krahmer and Deemter, 2012). However, most existing REG algorithms were developed and evaluated under the assumption that agents and humans have access to the same kind of domain information. For example, many experimental setups (Gatt et al., 2007; Viethen and Dale, 2008; Golland et al., 2010; Striegnitz et al., 2012) were developed based on a visual world for which the internal representation is assumed to be known and can be represented symbolically. However, this assumption no longer holds in situated dialogue with robots. There are two important distinctions in situated dialogue. First, </context>
<context position="5815" citStr="Dale, 1995" startWordPosition="868" endWordPosition="869">e.g., 45%). This performance gap calls for new solutions for REG that are capable of mediating mismatched perceptual basis. In the following sections, we first describe our hypergraph-based representations and illustrate how uncertainties from automated perception can be incorporated. We then describe an empirical study using Amazon Mechanical Turks for evaluating generated referring expressions. Finally we present evaluation results and discuss potential future directions. 2 Related Work Since the Full Brevity algorithm (Dale, 1989), many approaches have been developed and evaluated for REG (Dale, 1995; Krahmer and Deemter, 2012), such as the incremental algorithm (Dale, 1995), the locative algorithm (Kelleher and Kruijff, 2006), and graph-based approaches (Krahmer et al., 2003; Croitoru and Van Deemter, 2007). Most of these approaches assume the agent has access to a complete symbolic representation of the domain. While these approaches work well for many applications involving user interfaces, the question is whether they can be extended to the situation where the agent has incomplete or incorrect knowledge and needs to make inference about the domain or the world. Recently, there has bee</context>
<context position="7384" citStr="Dale, 1995" startWordPosition="1118" endWordPosition="1119">rs for REG (Roy, 2002; Mitchell et al., 2013). However, most work assumes no vision recognition errors. It is well established that automated recognition of visual scenes is extremely challenging. This process is error-prone and full of uncertainties. It is not clear whether the existing approaches can be extended to the situation where the agent has imperfect perception of the shared environment. An earlier work by Horacek (Horacek, 2005) has looked into the problem of mismatched knowledge between conversation partners for REG. The approach is a direct extension of the incremental algorithm (Dale, 1995). However, this work only provides a proof of concept example to illustrate the idea. No empirical evaluation was given. All these previous works have motivated our present investigation. We are interested in REG under mismatched perceptual basis between conversation partners, where the agent has imperfect perception and knowledge of the shared environment. In particular, we took a well-studied graph-based approach (Krahmer et al., 2003) and extended it to incorporate group spatial relations and uncertainties associated with automated perception of the environment. The reason we chose a graph-</context>
<context position="24023" citStr="Dale, 1995" startWordPosition="3804" endWordPosition="3805">ria: Participants’ locations are limited to the United States. Approval rate for each participant’s previous work is greater than or equal to 95%, and the number of each participant’s previous approved work is greater than or equal to 1000. 397 1992) in that a shortest distinguishing description is preferred. Absolute Preferred: The costs for hyperarcs representing absolute attributes (e.g., type, color, and position) are set to 1. The costs for relative attributes (e.g., size) and relations are set to 2. This cost function mimics human’s preference for absolute attributes over relative ones (Dale, 1995). Relative Preferred: The costs for hyperarcs representing absolute attributes are set to 2 and for relative attributes and relations are set to 1. This cost function has been applied previously to emphasize the importance of spatial relations in REG (Viethen and Dale, 2008). Uncertainty Based: The costs for all hyperarcs are defined by incorporating uncertainties from perception as described in Section 3.3.2. Uncertainty Relative Preferred: To emphasize the importance of spatial relations as demonstrated in situated interaction (Tenbrink and Moratz, 2003; Kelleher and Kruijff, 2006), the cost</context>
</contexts>
<marker>Dale, 1995</marker>
<rawString>Robert Dale. 1995. Computational interpretations of the gricean maxims in the generation of referring expressions. Cognitive Science, 19:233–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sheel Sanjay Dhande</author>
</authors>
<title>A computational model to connect gestalt perception and natural language.</title>
<date>2003</date>
<booktitle>In Masters thesis, Massachusetts Institure of Technology.</booktitle>
<contexts>
<context position="4805" citStr="Dhande, 2003" startWordPosition="715" endWordPosition="716">-of-the-art REG approaches are applicable under mismatched perceptual basis in situated dialogue. To address this issue, this paper revisits the problem of REG in the context of mismatched perceptual basis. We extended a well known graph-based approach (Krahmer et al., 2003) that has shown to be effective in previous work (Gatt and Belz, 2008; Gatt et al., 2009). We incorporated uncertainties in perception into cost functions. We further extended regular graph representation into hypergraph representation to account for group-based spatial relations that are important for visual descriptions (Dhande, 2003; Tenbrink and Moratz, 2003; Funakoshi et al., 2006; Liu et al., 2012). Our empirical results demonstrate that both enhancements lead to about a 9% absolute performance gain compared to the original approach. However, while our approache performs effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), it performs poorly under the mismatched perceptual basis (e.g., 45%). This performance gap calls for new solutions for REG that are capable of mediating mismatched perceptual basis. In the following sections, we first describe our hypergraph-based representa</context>
<context position="16962" citStr="Dhande, 2003" startWordPosition="2667" endWordPosition="2668">nature. In order for the agent to generate natural language descriptions, the first step is to assign symbolic labels or descriptors to those attributes and relations. Next we describe how we use a lexicon with grounded semantics in this process. 3.3.1 Lexicon with Grounded Semantics Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990). Previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i.e., identifying visual objects in the environment given language descriptions (Dhande, 2003; Gorniak and Roy, 2004; Tenbrink and Moratz, 2003; Siebert and Schlangen, 2008; Liu et al., 2012). For the referring expression generation task here, we also need a lexicon with grounded semantics. In our lexicon, the semantics of each category of words is defined by a set of semantic grounding functions that are parameterized on visual features. For example, for the color category it is defined as a multivariate Gaussian distribution based on the RGB distribution. Specific words such as green, red, or blue have different means and co-variances as the following: color : red = fr(vcolor) = N(v</context>
</contexts>
<marker>Dhande, 2003</marker>
<rawString>Sheel Sanjay Dhande. 2003. A computational model to connect gestalt perception and natural language. In Masters thesis, Massachusetts Institure of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip G Edmonds</author>
</authors>
<title>Collaboration on reference to objects that are not mutually known.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th conference on Computational linguistics -Volume 2, COLING ’94,</booktitle>
<pages>1118--1122</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Edmonds, 1994</marker>
<rawString>Philip G. Edmonds. 1994. Collaboration on reference to objects that are not mutually known. In Proceedings of the 15th conference on Computational linguistics -Volume 2, COLING ’94, pages 1118–1122, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kotaro Funakoshi</author>
<author>Satoru Watanabe</author>
<author>Naoko Kuriyama</author>
<author>Takenobu Tokunaga</author>
</authors>
<title>Generation of relative referring expressions based on perceptual grouping.</title>
<date>2004</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="10689" citStr="Funakoshi et al., 2004" startWordPosition="1634" endWordPosition="1637">jects are simply not recognizable to the matcher. Binary spatial relationships sometimes are difficult to describe the target object, so the matcher must resort to group information to distinguish the target object from the rest of the objects. For example, suppose the matcher needs to describe the target object 5 in Figure 1(b), he/she may have to start by indicating the group of three objects at the bottom and then specify the relationship (i.e., top) of the target object within this group. The importance of group descriptions has been shown not only here, but also in previous works on REG (Funakoshi et al., 2004; Funakoshi et al., 2006; Weijers, 2011). While the original graphbased approach can effectively represent attributes and binary relations between objects (Krahmer et al., 2003), it is insufficient to capture within-group or between-group relations. Therefore, to address the low perceptual capabilities of artificial agents, we introduce hypergraphs to represent the shared environment. Our approach has two unique characteristics compared to previous graph-based approaches: (1) A hypergraph representation is more general than a regular graph. Besides attributes and binary relations, it can also </context>
</contexts>
<marker>Funakoshi, Watanabe, Kuriyama, Tokunaga, 2004</marker>
<rawString>Kotaro Funakoshi, Satoru Watanabe, Naoko Kuriyama, and Takenobu Tokunaga. 2004. Generation of relative referring expressions based on perceptual grouping. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kotaro Funakoshi</author>
<author>Satoru Watanabe</author>
<author>Takenobu Tokunaga</author>
</authors>
<title>Group-based generation of referring expressions.</title>
<date>2006</date>
<booktitle>In INLG,</booktitle>
<pages>73--80</pages>
<contexts>
<context position="4856" citStr="Funakoshi et al., 2006" startWordPosition="721" endWordPosition="724"> under mismatched perceptual basis in situated dialogue. To address this issue, this paper revisits the problem of REG in the context of mismatched perceptual basis. We extended a well known graph-based approach (Krahmer et al., 2003) that has shown to be effective in previous work (Gatt and Belz, 2008; Gatt et al., 2009). We incorporated uncertainties in perception into cost functions. We further extended regular graph representation into hypergraph representation to account for group-based spatial relations that are important for visual descriptions (Dhande, 2003; Tenbrink and Moratz, 2003; Funakoshi et al., 2006; Liu et al., 2012). Our empirical results demonstrate that both enhancements lead to about a 9% absolute performance gain compared to the original approach. However, while our approache performs effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), it performs poorly under the mismatched perceptual basis (e.g., 45%). This performance gap calls for new solutions for REG that are capable of mediating mismatched perceptual basis. In the following sections, we first describe our hypergraph-based representations and illustrate how uncertainties from automat</context>
<context position="10713" citStr="Funakoshi et al., 2006" startWordPosition="1638" endWordPosition="1641">ognizable to the matcher. Binary spatial relationships sometimes are difficult to describe the target object, so the matcher must resort to group information to distinguish the target object from the rest of the objects. For example, suppose the matcher needs to describe the target object 5 in Figure 1(b), he/she may have to start by indicating the group of three objects at the bottom and then specify the relationship (i.e., top) of the target object within this group. The importance of group descriptions has been shown not only here, but also in previous works on REG (Funakoshi et al., 2004; Funakoshi et al., 2006; Weijers, 2011). While the original graphbased approach can effectively represent attributes and binary relations between objects (Krahmer et al., 2003), it is insufficient to capture within-group or between-group relations. Therefore, to address the low perceptual capabilities of artificial agents, we introduce hypergraphs to represent the shared environment. Our approach has two unique characteristics compared to previous graph-based approaches: (1) A hypergraph representation is more general than a regular graph. Besides attributes and binary relations, it can also represent group-based re</context>
</contexts>
<marker>Funakoshi, Watanabe, Tokunaga, 2006</marker>
<rawString>Kotaro Funakoshi, Satoru Watanabe, and Takenobu Tokunaga. 2006. Group-based generation of referring expressions. In INLG, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giorgio Gallo</author>
<author>Giustino Longo</author>
<author>Stefano Pallottino</author>
<author>Sang Nguyen</author>
</authors>
<title>Directed hypergraphs and applications. Discrete applied mathematics,</title>
<date>1993</date>
<volume>42</volume>
<issue>2</issue>
<pages>201</pages>
<contexts>
<context position="11920" citStr="Gallo et al., 1993" startWordPosition="1811" endWordPosition="1814">oup-based relations. (2) Unlike previous work, here the generation of hypergraphs are completely driven by automated perception of the environment. This is done by incorporating uncertainties in perception and reasoning into cost functions associated with graphs. Next we 394 These numerical features will be further converted to symbolic labels with certain confidence scores (described later in Section 3.3.2). give a detailed account on hypergraph representation, cost functions incorporating uncertainties, and the search algorithm for REG. 3.1 Hypergraph Representation A directed hypergraph G (Gallo et al., 1993) is a tuple of the form: G = (X, A), in which X = {xm} A = {ai = (ti, hi) |ti C X, hi C X} Similar to regular graphs, a hypergraph consists of a set of nodes X and a set of arcs A. However, different from regular graphs, each arc in A is considered as a hyperarc in the sense that it can capture relations between any two subsets of nodes: a tail (ti) and a head (hi). Therefore, a hypergraph is a generalization of a regular graph. It becomes a regular graph if the cardinalities of both the tail and the head are restricted to one for all hyperarcs. While regular graphs are commonly used to repres</context>
</contexts>
<marker>Gallo, Longo, Pallottino, Nguyen, 1993</marker>
<rawString>Giorgio Gallo, Giustino Longo, Stefano Pallottino, and Sang Nguyen. 1993. Directed hypergraphs and applications. Discrete applied mathematics, 42(2):177– 201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>Anja Belz</author>
</authors>
<title>Attribute selection for referring expression generation: new algorithms and evaluation methods.</title>
<date>2008</date>
<booktitle>In Proceedings of the Fifth International Natural Language Generation Conference, INLG ’08,</booktitle>
<pages>50--58</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4537" citStr="Gatt and Belz, 2008" startWordPosition="674" endWordPosition="677">es. Second, in situated dialogue the agent and the human have mismatched representations of the environment. The agent needs to take this difference into consideration to identify the most reliable features for REG. Given these two distinctions, it is not clear whether state-of-the-art REG approaches are applicable under mismatched perceptual basis in situated dialogue. To address this issue, this paper revisits the problem of REG in the context of mismatched perceptual basis. We extended a well known graph-based approach (Krahmer et al., 2003) that has shown to be effective in previous work (Gatt and Belz, 2008; Gatt et al., 2009). We incorporated uncertainties in perception into cost functions. We further extended regular graph representation into hypergraph representation to account for group-based spatial relations that are important for visual descriptions (Dhande, 2003; Tenbrink and Moratz, 2003; Funakoshi et al., 2006; Liu et al., 2012). Our empirical results demonstrate that both enhancements lead to about a 9% absolute performance gain compared to the original approach. However, while our approache performs effectively when the agent has perfect knowledge or perception of the environment (e.</context>
<context position="29928" citStr="Gatt and Belz, 2008" startWordPosition="4719" endWordPosition="4722">tly outperforms the regular graphs by taking advantage of spatial grouping information (p = 0.002). It is worthwhile to mention that currently we use spatial proximity to identify groups. However, the hypergraph based approach is not restricted to spatial grouping. In theory, it can represent any type of group based on different similarity criteria. Furthermore, our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception. Although evaluated on different data sets, this result is consistent with results from previous work (Gatt and Belz, 2008; Gatt et al., 2009). However, what is more interesting here is that while graph-based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40% performance degradation when applied to Environment Regular Graph Hypergraph Pefect Perception 80.4% 84.2% Imperfect Perception 36.7% 45.2% Table 2: Results of comparing perfect perception and imperfect perception of the shared world. the situation where the agent’s representation of the shared world is problematic and full of mistakes. These results i</context>
</contexts>
<marker>Gatt, Belz, 2008</marker>
<rawString>Albert Gatt and Anja Belz. 2008. Attribute selection for referring expression generation: new algorithms and evaluation methods. In Proceedings of the Fifth International Natural Language Generation Conference, INLG ’08, pages 50–58, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>Ielka van der Sluis</author>
<author>Kees van Deemter</author>
</authors>
<title>Evaluating algorithms for the generation of referring expressions using a balanced corpus.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eleventh European Workshop on Natural Language Generation, ENLG ’07,</booktitle>
<pages>49--56</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Gatt, van der Sluis, van Deemter, 2007</marker>
<rawString>Albert Gatt, Ielka van der Sluis, and Kees van Deemter. 2007. Evaluating algorithms for the generation of referring expressions using a balanced corpus. In Proceedings of the Eleventh European Workshop on Natural Language Generation, ENLG ’07, pages 49–56, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>Anja Belz</author>
<author>Eric Kow</author>
</authors>
<title>The tunareg challenge 2009: overview and evaluation results.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th European Workshop on Natural Language Generation, ENLG ’09,</booktitle>
<pages>174--182</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4557" citStr="Gatt et al., 2009" startWordPosition="678" endWordPosition="681">ed dialogue the agent and the human have mismatched representations of the environment. The agent needs to take this difference into consideration to identify the most reliable features for REG. Given these two distinctions, it is not clear whether state-of-the-art REG approaches are applicable under mismatched perceptual basis in situated dialogue. To address this issue, this paper revisits the problem of REG in the context of mismatched perceptual basis. We extended a well known graph-based approach (Krahmer et al., 2003) that has shown to be effective in previous work (Gatt and Belz, 2008; Gatt et al., 2009). We incorporated uncertainties in perception into cost functions. We further extended regular graph representation into hypergraph representation to account for group-based spatial relations that are important for visual descriptions (Dhande, 2003; Tenbrink and Moratz, 2003; Funakoshi et al., 2006; Liu et al., 2012). Our empirical results demonstrate that both enhancements lead to about a 9% absolute performance gain compared to the original approach. However, while our approache performs effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), it perform</context>
<context position="29948" citStr="Gatt et al., 2009" startWordPosition="4723" endWordPosition="4726">egular graphs by taking advantage of spatial grouping information (p = 0.002). It is worthwhile to mention that currently we use spatial proximity to identify groups. However, the hypergraph based approach is not restricted to spatial grouping. In theory, it can represent any type of group based on different similarity criteria. Furthermore, our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception. Although evaluated on different data sets, this result is consistent with results from previous work (Gatt and Belz, 2008; Gatt et al., 2009). However, what is more interesting here is that while graph-based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40% performance degradation when applied to Environment Regular Graph Hypergraph Pefect Perception 80.4% 84.2% Imperfect Perception 36.7% 45.2% Table 2: Results of comparing perfect perception and imperfect perception of the shared world. the situation where the agent’s representation of the shared world is problematic and full of mistakes. These results indicate that REG for</context>
</contexts>
<marker>Gatt, Belz, Kow, 2009</marker>
<rawString>Albert Gatt, Anja Belz, and Eric Kow. 2009. The tunareg challenge 2009: overview and evaluation results. In Proceedings of the 12th European Workshop on Natural Language Generation, ENLG ’09, pages 174– 182, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
</authors>
<title>Structuring knowledge for reference generation: A clustering algorithm.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics,</booktitle>
<pages>321--328</pages>
<contexts>
<context position="14978" citStr="Gatt, 2006" startWordPosition="2339" endWordPosition="2340">he first heuristic is based on perceptual principles, also called the Gestalt Laws of perception (Sternberg, 2003), which describe how people group visually similar objects into entities or groups. Two well known principles of perceptual grouping are proximity and similarity (Wertheimer, 1938): objects that lie close together are often perceived as groups; objects of similar shape, size or color are more likely to form groups than objects differing along these dimensions. Based on these two principles, previous works have developed different algorithms for perceptual grouping (Thrisson, 1994; Gatt, 2006). In our investigation, we adopted Gatt’s algorithm (Gatt, 2006), which has shown to be more accurate for spatial grouping. Given the results from spatial grouping, we only retain hyperarcs that represent spatial relations between two objects, between two perceived groups, between one object and a perceived group, or between one object and the group it belongs to. The second heuristic is based on the observation that, given a certain orientation, people tend to use a relatum that is closer to the referent than more distant relata. In other words, it is less likely to refer to an object relativ</context>
</contexts>
<marker>Gatt, 2006</marker>
<rawString>Albert Gatt. 2006. Structuring knowledge for reference generation: A clustering algorithm. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, Association for Computational Linguistics, pages 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dave Golland</author>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>A game-theoretic approach to generating spatial descriptions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>410--419</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3119" citStr="Golland et al., 2010" startWordPosition="455" endWordPosition="458">tual capabilities of the environment than humans. How can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to? There has been a tremendous amount of work on referring expression generation in the last two decades (Dale, 1995; Krahmer and Deemter, 2012). However, most existing REG algorithms were developed and evaluated under the assumption that agents and humans have access to the same kind of domain information. For example, many experimental setups (Gatt et al., 2007; Viethen and Dale, 2008; Golland et al., 2010; Striegnitz et al., 2012) were developed based on a visual world for which the internal representation is assumed to be known and can be represented symbolically. However, this assumption no longer holds in situated dialogue with robots. There are two important distinctions in situated dialogue. First, the perfect knowledge of the environment is not available to the agent ahead of time. The agent needs to automatically make inferences to connect recognized lower-level visual features with 392 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 392–402</context>
<context position="6495" citStr="Golland et al., 2010" startWordPosition="974" endWordPosition="977">thm (Dale, 1995), the locative algorithm (Kelleher and Kruijff, 2006), and graph-based approaches (Krahmer et al., 2003; Croitoru and Van Deemter, 2007). Most of these approaches assume the agent has access to a complete symbolic representation of the domain. While these approaches work well for many applications involving user interfaces, the question is whether they can be extended to the situation where the agent has incomplete or incorrect knowledge and needs to make inference about the domain or the world. Recently, there has been increasing interest in REG for visual objects (Roy, 2002; Golland et al., 2010; Mitchell et al., 2013). Some work (Golland et al., 2010) uses visual scenes that are generated by computer graphics and thus the internal representation of the scene is known. Some other work focuses on the connection between lower-level visual features and symbolic descriptors for REG (Roy, 2002; Mitchell et al., 2013). However, most work assumes no vision recognition errors. It is well established that automated recognition of visual scenes is extremely challenging. This process is error-prone and full of uncertainties. It is not clear whether the existing approaches can be extended to the</context>
</contexts>
<marker>Golland, Liang, Klein, 2010</marker>
<rawString>Dave Golland, Percy Liang, and Dan Klein. 2010. A game-theoretic approach to generating spatial descriptions. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 410–419, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Gorniak</author>
<author>Deb Roy</author>
</authors>
<title>Grounded semantic composition for visual scenes.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>21--429</pages>
<contexts>
<context position="16985" citStr="Gorniak and Roy, 2004" startWordPosition="2669" endWordPosition="2672">er for the agent to generate natural language descriptions, the first step is to assign symbolic labels or descriptors to those attributes and relations. Next we describe how we use a lexicon with grounded semantics in this process. 3.3.1 Lexicon with Grounded Semantics Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990). Previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i.e., identifying visual objects in the environment given language descriptions (Dhande, 2003; Gorniak and Roy, 2004; Tenbrink and Moratz, 2003; Siebert and Schlangen, 2008; Liu et al., 2012). For the referring expression generation task here, we also need a lexicon with grounded semantics. In our lexicon, the semantics of each category of words is defined by a set of semantic grounding functions that are parameterized on visual features. For example, for the color category it is defined as a multivariate Gaussian distribution based on the RGB distribution. Specific words such as green, red, or blue have different means and co-variances as the following: color : red = fr(vcolor) = N(vcolor |A1, E1) color : </context>
</contexts>
<marker>Gorniak, Roy, 2004</marker>
<rawString>Peter Gorniak and Deb Roy. 2004. Grounded semantic composition for visual scenes. Journal of Artificial Intelligence Research, 21:429–470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stevan Harnad</author>
</authors>
<title>The symbol grounding problem.</title>
<date>1990</date>
<journal>Physica D,</journal>
<pages>42--335</pages>
<contexts>
<context position="16755" citStr="Harnad, 1990" startWordPosition="2638" endWordPosition="2639">(a partial scene only including object 7, 8, 9, 11, 13 for Figure 1(a)). 3.3 Symbolic Descriptors for Attributes As mentioned earlier, the values of attributes of objects and their relations are numerical in nature. In order for the agent to generate natural language descriptions, the first step is to assign symbolic labels or descriptors to those attributes and relations. Next we describe how we use a lexicon with grounded semantics in this process. 3.3.1 Lexicon with Grounded Semantics Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990). Previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i.e., identifying visual objects in the environment given language descriptions (Dhande, 2003; Gorniak and Roy, 2004; Tenbrink and Moratz, 2003; Siebert and Schlangen, 2008; Liu et al., 2012). For the referring expression generation task here, we also need a lexicon with grounded semantics. In our lexicon, the semantics of each category of words is defined by a set of semantic grounding functions that are parameterized on visual features. For example, for the color category it is d</context>
</contexts>
<marker>Harnad, 1990</marker>
<rawString>Stevan Harnad. 1990. The symbol grounding problem. Physica D, 42:335–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Heeman</author>
<author>Graeme Hirst</author>
</authors>
<title>Collaborating on referring expressions.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--351</pages>
<marker>Heeman, Hirst, 1995</marker>
<rawString>Peter A. Heeman and Graeme Hirst. 1995. Collaborating on referring expressions. Computational Linguistics, 21:351–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Horacek</author>
</authors>
<title>Generating referential descriptions under conditions of uncertainty.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th European Workshop on Natural Language Generation (ENLG)</booktitle>
<pages>58--67</pages>
<location>Aberdeen, UK.</location>
<contexts>
<context position="7216" citStr="Horacek, 2005" startWordPosition="1091" endWordPosition="1092">r graphics and thus the internal representation of the scene is known. Some other work focuses on the connection between lower-level visual features and symbolic descriptors for REG (Roy, 2002; Mitchell et al., 2013). However, most work assumes no vision recognition errors. It is well established that automated recognition of visual scenes is extremely challenging. This process is error-prone and full of uncertainties. It is not clear whether the existing approaches can be extended to the situation where the agent has imperfect perception of the shared environment. An earlier work by Horacek (Horacek, 2005) has looked into the problem of mismatched knowledge between conversation partners for REG. The approach is a direct extension of the incremental algorithm (Dale, 1995). However, this work only provides a proof of concept example to illustrate the idea. No empirical evaluation was given. All these previous works have motivated our present investigation. We are interested in REG under mismatched perceptual basis between conversation partners, where the agent has imperfect perception and knowledge of the shared environment. In particular, we took a well-studied graph-based approach (Krahmer et a</context>
</contexts>
<marker>Horacek, 2005</marker>
<rawString>Helmut Horacek. 2005. Generating referential descriptions under conditions of uncertainty. In Proceedings of the 10th European Workshop on Natural Language Generation (ENLG) pages 58-67, Aberdeen, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela W Jordan</author>
<author>Marilyn Walker</author>
</authors>
<title>Learning attribute selections for non-pronominal expressions.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>181--190</pages>
<contexts>
<context position="32775" citStr="Jordan and Walker, 2000" startWordPosition="5174" endWordPosition="5177">unctions that better reflect human’s visual perception (Regier, 1996; Mojsilovic, 2005; Mitchell et al., 2011) should be pursued in the future. 399 Minimum Effort Extra Effort Pefect Perception 84.2% 88.1% Imperfect Perception 45.2% 51.5% Table 3: Results of comparing minimum effort and extra effort using hypergraphs 4.3.3 The Role of Extra Effort While REG systems have a tendency to produce minimal descriptions, recent psycholinguistic studies have shown that speakers do not necessarily follow the Grice’s maxim of quantity, and they tend to provide redundant properties in their descriptions (Jordan and Walker, 2000; Belke and Meyer, 2002; Arts et al., 2011). With this in mind, we conducted a very simple evaluation on the role of extra effort. Once a set of descriptors are selected based on the minimum cost, one additional descriptor (with the least cost among the remaining attributes or relations) is added to the referential description. We once again solicited the crowd feedback to this set of expressions generated by extra effort. Each expression again received three votes from the crowd. Table 3 shows the results by comparing minimum effort with extra effort when using hypergraphs to generate REs. As</context>
</contexts>
<marker>Jordan, Walker, 2000</marker>
<rawString>Pamela W Jordan and Marilyn Walker. 2000. Learning attribute selections for non-pronominal expressions. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 181-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Kelleher</author>
<author>Geert-Jan M Kruijff</author>
</authors>
<title>Incremental generation of spatial referring expressions in situated dialog.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>1041--1048</pages>
<contexts>
<context position="5944" citStr="Kelleher and Kruijff, 2006" startWordPosition="884" endWordPosition="887">tual basis. In the following sections, we first describe our hypergraph-based representations and illustrate how uncertainties from automated perception can be incorporated. We then describe an empirical study using Amazon Mechanical Turks for evaluating generated referring expressions. Finally we present evaluation results and discuss potential future directions. 2 Related Work Since the Full Brevity algorithm (Dale, 1989), many approaches have been developed and evaluated for REG (Dale, 1995; Krahmer and Deemter, 2012), such as the incremental algorithm (Dale, 1995), the locative algorithm (Kelleher and Kruijff, 2006), and graph-based approaches (Krahmer et al., 2003; Croitoru and Van Deemter, 2007). Most of these approaches assume the agent has access to a complete symbolic representation of the domain. While these approaches work well for many applications involving user interfaces, the question is whether they can be extended to the situation where the agent has incomplete or incorrect knowledge and needs to make inference about the domain or the world. Recently, there has been increasing interest in REG for visual objects (Roy, 2002; Golland et al., 2010; Mitchell et al., 2013). Some work (Golland et a</context>
<context position="24613" citStr="Kelleher and Kruijff, 2006" startWordPosition="3890" endWordPosition="3893">butes over relative ones (Dale, 1995). Relative Preferred: The costs for hyperarcs representing absolute attributes are set to 2 and for relative attributes and relations are set to 1. This cost function has been applied previously to emphasize the importance of spatial relations in REG (Viethen and Dale, 2008). Uncertainty Based: The costs for all hyperarcs are defined by incorporating uncertainties from perception as described in Section 3.3.2. Uncertainty Relative Preferred: To emphasize the importance of spatial relations as demonstrated in situated interaction (Tenbrink and Moratz, 2003; Kelleher and Kruijff, 2006), the costs for hyperarcs representing relative attributes and relations are divided by 3. This cost function will allow the algorithm to prefer spatial relations through the reduced cost. Note that we only tested a few (not all) commonly used cost functions proposed by previous work (Krahmer et al., 2003; Theune et al., 2007; Krahmer et al., 2008; Theune et al., 2011). For example, we did not include the stochastic cost function which is defined based on the frequencies of attribute selection from the training data (Krahmer et al., 2003). On the one hand, we did not have a large set of human </context>
</contexts>
<marker>Kelleher, Kruijff, 2006</marker>
<rawString>John D. Kelleher and Geert-Jan M. Kruijff. 2006. Incremental generation of spatial referring expressions in situated dialog. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 1041–1048,</rawString>
</citation>
<citation valid="false">
<authors>
<author>PA Stroudsburg</author>
</authors>
<institution>USA. Association for Computational Linguistics.</institution>
<marker>Stroudsburg, </marker>
<rawString>Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Kees Van Deemter</author>
</authors>
<title>Computational generation of referring expressions: A survey. computational linguistics,</title>
<date>2012</date>
<pages>38--1</pages>
<marker>Krahmer, Van Deemter, 2012</marker>
<rawString>Emiel Krahmer and Kees Van Deemter. 2012. Computational generation of referring expressions: A survey. computational linguistics, 38(1):173–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer Krahmer</author>
<author>Sebastiaan van Erk</author>
<author>Andr´e Verleg</author>
</authors>
<title>Graph-based generation of referring expressions.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Krahmer, van Erk, Verleg, 2003</marker>
<rawString>Emiel Krahmer Krahmer, Sebastiaan van Erk, and Andr´e Verleg. 2003. Graph-based generation of referring expressions. Computational Linguistics, 29(1):53–72, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Mariet Theune</author>
<author>Jette Viethen</author>
<author>Iris Hendrickx</author>
</authors>
<title>Graph: The costs of redundancy in referring expressions. In</title>
<date>2008</date>
<booktitle>In Proceedings of the 5th International Conference on Natural Language Generation,</booktitle>
<location>Salt Fork OH, USA.</location>
<contexts>
<context position="22659" citStr="Krahmer et al., 2008" startWordPosition="3587" endWordPosition="3590">k). Each referring expression received three votes from the crowd. In total, 217 turks participated in our experiment. 4.2 Generation Strategies We applied a set of different strategies to generate referring expressions for each object. The variations lie in two dimensions: (1) different graph representations: using a hypergraph to represent the perceived scene as described in Section 3.1 versus using a regular graph as introduced in (Krahmer et al., 2003); and (2) different cost functions for attributes and relations: cost functions that have been used in previous works (Theune et al., 2007; Krahmer et al., 2008) and cost functions that incorporate uncertainties of perception as described in Section 3.3.2. Cost functions play an important role in graphbased approaches (Krahmer et al., 2003). Previous works have examined different types of cost functions (Theune et al., 2007; Krahmer et al., 2008; Theune et al., 2011). We adopted some commonly used cost functions from previous work together with the cost functions defined here. In particular, we experimented with the following different cost functions: Simple Cost: The costs for all hyperarcs are set to 1. With this cost function, the graph-based algor</context>
<context position="24962" citStr="Krahmer et al., 2008" startWordPosition="3950" endWordPosition="3953">arcs are defined by incorporating uncertainties from perception as described in Section 3.3.2. Uncertainty Relative Preferred: To emphasize the importance of spatial relations as demonstrated in situated interaction (Tenbrink and Moratz, 2003; Kelleher and Kruijff, 2006), the costs for hyperarcs representing relative attributes and relations are divided by 3. This cost function will allow the algorithm to prefer spatial relations through the reduced cost. Note that we only tested a few (not all) commonly used cost functions proposed by previous work (Krahmer et al., 2003; Theune et al., 2007; Krahmer et al., 2008; Theune et al., 2011). For example, we did not include the stochastic cost function which is defined based on the frequencies of attribute selection from the training data (Krahmer et al., 2003). On the one hand, we did not have a large set of human descriptions of the impoverished scene to learn the stochastic cost. On the other hand, it is not clear whether human strategies of describing the impoverished scene should be used to represent optimal strategies for the robot. Nevertheless, the above different cost functions will allow us to evaluate whether incorporating perceptual uncertainties</context>
</contexts>
<marker>Krahmer, Theune, Viethen, Hendrickx, 2008</marker>
<rawString>Emiel Krahmer, Mariet Theune, Jette Viethen, and Iris Hendrickx. 2008. Graph: The costs of redundancy in referring expressions. In In Proceedings of the 5th International Conference on Natural Language Generation, Salt Fork OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Changsong Liu</author>
<author>Rui Fang</author>
<author>Joyce Y Chai</author>
</authors>
<title>Towards mediating shared perceptual basis in situated dialogue.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’12,</booktitle>
<pages>140--149</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2053" citStr="Liu et al., 2012" startWordPosition="287" endWordPosition="290">and make inference of the shared environment. Due to its limited perceptual and reasoning capabilities, the robot’s representation of the shared world is often incomplete, error-prone, and significantly mismatched from that of its human partner’s. Although physically co-present, a joint perceptual basis between the human and the robot cannot be established (Clark and Brennan, 1991). Thus, referential communication between the human and the robot becomes difficult. How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work (Liu et al., 2012). In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment. Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue. Robots have much lower perceptual capabilities of the environment than humans. How can a robot effectively generate referential descriptions about the environment so that its human pa</context>
<context position="4875" citStr="Liu et al., 2012" startWordPosition="725" endWordPosition="728">tual basis in situated dialogue. To address this issue, this paper revisits the problem of REG in the context of mismatched perceptual basis. We extended a well known graph-based approach (Krahmer et al., 2003) that has shown to be effective in previous work (Gatt and Belz, 2008; Gatt et al., 2009). We incorporated uncertainties in perception into cost functions. We further extended regular graph representation into hypergraph representation to account for group-based spatial relations that are important for visual descriptions (Dhande, 2003; Tenbrink and Moratz, 2003; Funakoshi et al., 2006; Liu et al., 2012). Our empirical results demonstrate that both enhancements lead to about a 9% absolute performance gain compared to the original approach. However, while our approache performs effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), it performs poorly under the mismatched perceptual basis (e.g., 45%). This performance gap calls for new solutions for REG that are capable of mediating mismatched perceptual basis. In the following sections, we first describe our hypergraph-based representations and illustrate how uncertainties from automated perception can b</context>
<context position="8504" citStr="Liu et al., 2012" startWordPosition="1289" endWordPosition="1292">certainties associated with automated perception of the environment. The reason we chose a graph-based approach is that graph representations are widely used in the fields of computer vision (CV) and pattern recognition to represent spatially rich scenes. Nevertheless, the findings from this investigation provide insight to other approaches. 393 (a) an original scene (b) the corresponding impoverished scene Figure 1: An original scene and its impoverished scene processed by CV algorithm 3 Hypergraph-based REG Towards mediating a shared perceptual basis in situated dialogue, our previous work (Liu et al., 2012) has conducted experiments to study referential communication between partners with mismatched perceptual capabilities. We simulated mismatched capabilities by making an original scene (Figure 1(a)) available to a director (simulating higher perceptual calibre) and a corresponding impoverished scene (Figure 1(b)) available to a matcher (simulating lowered perceptual calibre). The impoverished scene is created by re-rendering automated recognition results of the original scene by a CV algorithm. An example of the original scene and an impoverished scene is shown in Figure 1. Using this setup, t</context>
<context position="9738" citStr="Liu et al., 2012" startWordPosition="1477" endWordPosition="1480">e matcher were instructed to collaborate with each other on some naming games. Through these games, they collected data on how partners with mismatched perceptual capabilities collaborate to ground their referential communication. The setup in (Liu et al., 2012) is intended to simulate situated dialogue between a human (like the director) and a robot (like the matcher). The robot has a significantly lowered ability in perception and reasoning. The robot’s internal representation of the shared world will be much like the impoverished scene which contains many recognition errors. The data from (Liu et al., 2012; Liu et al., 2013) shows that different strategies were used by conversation partners to produce referential descriptions. Besides directly describing attributes or binary relations with a relatum, they often use group-based descriptions (e.g., a cluster offour objects on the right). This is mainly due to the fact that some objects are simply not recognizable to the matcher. Binary spatial relationships sometimes are difficult to describe the target object, so the matcher must resort to group information to distinguish the target object from the rest of the objects. For example, suppose the m</context>
<context position="17060" citStr="Liu et al., 2012" startWordPosition="2681" endWordPosition="2684">o assign symbolic labels or descriptors to those attributes and relations. Next we describe how we use a lexicon with grounded semantics in this process. 3.3.1 Lexicon with Grounded Semantics Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990). Previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i.e., identifying visual objects in the environment given language descriptions (Dhande, 2003; Gorniak and Roy, 2004; Tenbrink and Moratz, 2003; Siebert and Schlangen, 2008; Liu et al., 2012). For the referring expression generation task here, we also need a lexicon with grounded semantics. In our lexicon, the semantics of each category of words is defined by a set of semantic grounding functions that are parameterized on visual features. For example, for the color category it is defined as a multivariate Gaussian distribution based on the RGB distribution. Specific words such as green, red, or blue have different means and co-variances as the following: color : red = fr(vcolor) = N(vcolor |A1, E1) color : green = fg(vcolor) = N(vcolor |A2, E2) color : blue = fb(vcolor) = N(vcolor</context>
</contexts>
<marker>Liu, Fang, Chai, 2012</marker>
<rawString>Changsong Liu, Rui Fang, and Joyce Y. Chai. 2012. Towards mediating shared perceptual basis in situated dialogue. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’12, pages 140–149, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Changsong Liu</author>
<author>Rui Fang</author>
<author>Lanbo She</author>
<author>Joyce Y Chai</author>
</authors>
<title>Modeling collaborative referring for situated referential grounding.</title>
<date>2013</date>
<booktitle>In The 14th Annual SIGdial Meeting on Discourse and Dialogue.</booktitle>
<contexts>
<context position="9757" citStr="Liu et al., 2013" startWordPosition="1481" endWordPosition="1484">tructed to collaborate with each other on some naming games. Through these games, they collected data on how partners with mismatched perceptual capabilities collaborate to ground their referential communication. The setup in (Liu et al., 2012) is intended to simulate situated dialogue between a human (like the director) and a robot (like the matcher). The robot has a significantly lowered ability in perception and reasoning. The robot’s internal representation of the shared world will be much like the impoverished scene which contains many recognition errors. The data from (Liu et al., 2012; Liu et al., 2013) shows that different strategies were used by conversation partners to produce referential descriptions. Besides directly describing attributes or binary relations with a relatum, they often use group-based descriptions (e.g., a cluster offour objects on the right). This is mainly due to the fact that some objects are simply not recognizable to the matcher. Binary spatial relationships sometimes are difficult to describe the target object, so the matcher must resort to group information to distinguish the target object from the rest of the objects. For example, suppose the matcher needs to des</context>
</contexts>
<marker>Liu, Fang, She, Chai, 2013</marker>
<rawString>Changsong Liu, Rui Fang, Lanbo She, and Joyce Y. Chai. 2013. Modeling collaborative referring for situated referential grounding. In The 14th Annual SIGdial Meeting on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Kees van Deemter</author>
<author>Ehud Reiter</author>
</authors>
<title>Two approaches for generating size modifiers.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation, ENLG ’11,</booktitle>
<pages>63--70</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Mitchell, van Deemter, Reiter, 2011</marker>
<rawString>Margaret Mitchell, Kees van Deemter, and Ehud Reiter. 2011. Two approaches for generating size modifiers. In Proceedings of the 13th European Workshop on Natural Language Generation, ENLG ’11, pages 63– 70, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Kees van Deemter</author>
<author>Ehud Reiter</author>
</authors>
<title>Generating expressions that refer to visible objects.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT 2013,</booktitle>
<pages>1174--1184</pages>
<marker>Mitchell, van Deemter, Reiter, 2013</marker>
<rawString>Margaret Mitchell, Kees van Deemter, and Ehud Reiter. 2013. Generating expressions that refer to visible objects. In Proceedings of NAACL-HLT 2013, pages 1174-1184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aleksandra Mojsilovic</author>
</authors>
<title>A computational model for color naming and describing color composition of images.</title>
<date>2005</date>
<journal>IEEE Transactions on Image Processing, 14:690 –</journal>
<pages>699</pages>
<contexts>
<context position="32238" citStr="Mojsilovic, 2005" startWordPosition="5091" endWordPosition="5092">ven with recognition errors, these group-based descriptions will allow the listener to identify target objects in their representation correctly. Nevertheless, many processing errors cannot be handled by our current approach. For example, an object can be mistakenly segmented into multiple parts or several objects can be mistakenly grouped into one object. In addition, our current semantic grounding functions are simple. Sometimes they do not provide correct descriptors for the extracted visual features. More sophisticated functions that better reflect human’s visual perception (Regier, 1996; Mojsilovic, 2005; Mitchell et al., 2011) should be pursued in the future. 399 Minimum Effort Extra Effort Pefect Perception 84.2% 88.1% Imperfect Perception 45.2% 51.5% Table 3: Results of comparing minimum effort and extra effort using hypergraphs 4.3.3 The Role of Extra Effort While REG systems have a tendency to produce minimal descriptions, recent psycholinguistic studies have shown that speakers do not necessarily follow the Grice’s maxim of quantity, and they tend to provide redundant properties in their descriptions (Jordan and Walker, 2000; Belke and Meyer, 2002; Arts et al., 2011). With this in mind,</context>
</contexts>
<marker>Mojsilovic, 2005</marker>
<rawString>Aleksandra Mojsilovic. 2005. A computational model for color naming and describing color composition of images. IEEE Transactions on Image Processing, 14:690 – 699.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Regier</author>
</authors>
<title>The human semantic potential. The MIT Press,Cambridge,</title>
<date>1996</date>
<location>Massachusetts.</location>
<contexts>
<context position="32220" citStr="Regier, 1996" startWordPosition="5089" endWordPosition="5090">s generated. Even with recognition errors, these group-based descriptions will allow the listener to identify target objects in their representation correctly. Nevertheless, many processing errors cannot be handled by our current approach. For example, an object can be mistakenly segmented into multiple parts or several objects can be mistakenly grouped into one object. In addition, our current semantic grounding functions are simple. Sometimes they do not provide correct descriptors for the extracted visual features. More sophisticated functions that better reflect human’s visual perception (Regier, 1996; Mojsilovic, 2005; Mitchell et al., 2011) should be pursued in the future. 399 Minimum Effort Extra Effort Pefect Perception 84.2% 88.1% Imperfect Perception 45.2% 51.5% Table 3: Results of comparing minimum effort and extra effort using hypergraphs 4.3.3 The Role of Extra Effort While REG systems have a tendency to produce minimal descriptions, recent psycholinguistic studies have shown that speakers do not necessarily follow the Grice’s maxim of quantity, and they tend to provide redundant properties in their descriptions (Jordan and Walker, 2000; Belke and Meyer, 2002; Arts et al., 2011). </context>
</contexts>
<marker>Regier, 1996</marker>
<rawString>Terry Regier. 1996. The human semantic potential. The MIT Press,Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deb Roy</author>
</authors>
<title>Learning visually grounded words and syntax of natural spoken language.</title>
<date>2002</date>
<journal>Evolution of Communication,</journal>
<volume>4</volume>
<contexts>
<context position="6473" citStr="Roy, 2002" startWordPosition="972" endWordPosition="973">ntal algorithm (Dale, 1995), the locative algorithm (Kelleher and Kruijff, 2006), and graph-based approaches (Krahmer et al., 2003; Croitoru and Van Deemter, 2007). Most of these approaches assume the agent has access to a complete symbolic representation of the domain. While these approaches work well for many applications involving user interfaces, the question is whether they can be extended to the situation where the agent has incomplete or incorrect knowledge and needs to make inference about the domain or the world. Recently, there has been increasing interest in REG for visual objects (Roy, 2002; Golland et al., 2010; Mitchell et al., 2013). Some work (Golland et al., 2010) uses visual scenes that are generated by computer graphics and thus the internal representation of the scene is known. Some other work focuses on the connection between lower-level visual features and symbolic descriptors for REG (Roy, 2002; Mitchell et al., 2013). However, most work assumes no vision recognition errors. It is well established that automated recognition of visual scenes is extremely challenging. This process is error-prone and full of uncertainties. It is not clear whether the existing approaches </context>
</contexts>
<marker>Roy, 2002</marker>
<rawString>Deb Roy. 2002. Learning visually grounded words and syntax of natural spoken language. Evolution of Communication, 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Siebert</author>
<author>David Schlangen</author>
</authors>
<title>A simple method for resolution of definite reference in a shared visual context.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, SIGdial ’08,</booktitle>
<pages>84--87</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17041" citStr="Siebert and Schlangen, 2008" startWordPosition="2677" endWordPosition="2680">riptions, the first step is to assign symbolic labels or descriptors to those attributes and relations. Next we describe how we use a lexicon with grounded semantics in this process. 3.3.1 Lexicon with Grounded Semantics Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990). Previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i.e., identifying visual objects in the environment given language descriptions (Dhande, 2003; Gorniak and Roy, 2004; Tenbrink and Moratz, 2003; Siebert and Schlangen, 2008; Liu et al., 2012). For the referring expression generation task here, we also need a lexicon with grounded semantics. In our lexicon, the semantics of each category of words is defined by a set of semantic grounding functions that are parameterized on visual features. For example, for the color category it is defined as a multivariate Gaussian distribution based on the RGB distribution. Specific words such as green, red, or blue have different means and co-variances as the following: color : red = fr(vcolor) = N(vcolor |A1, E1) color : green = fg(vcolor) = N(vcolor |A2, E2) color : blue = fb</context>
</contexts>
<marker>Siebert, Schlangen, 2008</marker>
<rawString>Alexander Siebert and David Schlangen. 2008. A simple method for resolution of definite reference in a shared visual context. In Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, SIGdial ’08, pages 84–87, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Sternberg</author>
</authors>
<title>Cognitive Psychology,Third Edition.</title>
<date>2003</date>
<publisher>Thomson Wadsworth.</publisher>
<contexts>
<context position="14481" citStr="Sternberg, 2003" startWordPosition="2264" endWordPosition="2265"> of the location attribute is a function of spatial coordinates. 3.2 Hypergraph Pruning The perceived visual scene can be represented as a complete hypergraph, in which any pair of two subsets of nodes are connected by a hyperarc. However, such a complete hypergraph is not only inefficient but also unnecessary. Instead of keeping all possible n-ary relations (i.e., hyperarcs), we only retain those relations that are likely used by humans to produce referring expressions, based on two heuristics. The first heuristic is based on perceptual principles, also called the Gestalt Laws of perception (Sternberg, 2003), which describe how people group visually similar objects into entities or groups. Two well known principles of perceptual grouping are proximity and similarity (Wertheimer, 1938): objects that lie close together are often perceived as groups; objects of similar shape, size or color are more likely to form groups than objects differing along these dimensions. Based on these two principles, previous works have developed different algorithms for perceptual grouping (Thrisson, 1994; Gatt, 2006). In our investigation, we adopted Gatt’s algorithm (Gatt, 2006), which has shown to be more accurate f</context>
</contexts>
<marker>Sternberg, 2003</marker>
<rawString>Robert Sternberg. 2003. Cognitive Psychology,Third Edition. Thomson Wadsworth.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Striegnitz</author>
<author>Hendrik Buschmeier</author>
<author>Stefan Kopp</author>
</authors>
<title>Referring in installments: a corpus study of spoken object references in an interactive virtual environment.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh International Natural Language Generation Conference, INLG ’12,</booktitle>
<pages>12--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3145" citStr="Striegnitz et al., 2012" startWordPosition="459" endWordPosition="462">he environment than humans. How can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to? There has been a tremendous amount of work on referring expression generation in the last two decades (Dale, 1995; Krahmer and Deemter, 2012). However, most existing REG algorithms were developed and evaluated under the assumption that agents and humans have access to the same kind of domain information. For example, many experimental setups (Gatt et al., 2007; Viethen and Dale, 2008; Golland et al., 2010; Striegnitz et al., 2012) were developed based on a visual world for which the internal representation is assumed to be known and can be represented symbolically. However, this assumption no longer holds in situated dialogue with robots. There are two important distinctions in situated dialogue. First, the perfect knowledge of the environment is not available to the agent ahead of time. The agent needs to automatically make inferences to connect recognized lower-level visual features with 392 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 392–402, Seattle, Washington, USA</context>
</contexts>
<marker>Striegnitz, Buschmeier, Kopp, 2012</marker>
<rawString>Kristina Striegnitz, Hendrik Buschmeier, and Stefan Kopp. 2012. Referring in installments: a corpus study of spoken object references in an interactive virtual environment. In Proceedings of the Seventh International Natural Language Generation Conference, INLG ’12, pages 12–16, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thora Tenbrink</author>
<author>Reinhard Moratz</author>
</authors>
<title>Groupbased spatial reference in linguistic human-robot interaction.</title>
<date>2003</date>
<journal>Spatial Cognition and Computation,</journal>
<volume>6</volume>
<pages>106</pages>
<contexts>
<context position="4832" citStr="Tenbrink and Moratz, 2003" startWordPosition="717" endWordPosition="720">G approaches are applicable under mismatched perceptual basis in situated dialogue. To address this issue, this paper revisits the problem of REG in the context of mismatched perceptual basis. We extended a well known graph-based approach (Krahmer et al., 2003) that has shown to be effective in previous work (Gatt and Belz, 2008; Gatt et al., 2009). We incorporated uncertainties in perception into cost functions. We further extended regular graph representation into hypergraph representation to account for group-based spatial relations that are important for visual descriptions (Dhande, 2003; Tenbrink and Moratz, 2003; Funakoshi et al., 2006; Liu et al., 2012). Our empirical results demonstrate that both enhancements lead to about a 9% absolute performance gain compared to the original approach. However, while our approache performs effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), it performs poorly under the mismatched perceptual basis (e.g., 45%). This performance gap calls for new solutions for REG that are capable of mediating mismatched perceptual basis. In the following sections, we first describe our hypergraph-based representations and illustrate how un</context>
<context position="17012" citStr="Tenbrink and Moratz, 2003" startWordPosition="2673" endWordPosition="2676">erate natural language descriptions, the first step is to assign symbolic labels or descriptors to those attributes and relations. Next we describe how we use a lexicon with grounded semantics in this process. 3.3.1 Lexicon with Grounded Semantics Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990). Previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i.e., identifying visual objects in the environment given language descriptions (Dhande, 2003; Gorniak and Roy, 2004; Tenbrink and Moratz, 2003; Siebert and Schlangen, 2008; Liu et al., 2012). For the referring expression generation task here, we also need a lexicon with grounded semantics. In our lexicon, the semantics of each category of words is defined by a set of semantic grounding functions that are parameterized on visual features. For example, for the color category it is defined as a multivariate Gaussian distribution based on the RGB distribution. Specific words such as green, red, or blue have different means and co-variances as the following: color : red = fr(vcolor) = N(vcolor |A1, E1) color : green = fg(vcolor) = N(vcol</context>
<context position="24584" citStr="Tenbrink and Moratz, 2003" startWordPosition="3886" endWordPosition="3889">eference for absolute attributes over relative ones (Dale, 1995). Relative Preferred: The costs for hyperarcs representing absolute attributes are set to 2 and for relative attributes and relations are set to 1. This cost function has been applied previously to emphasize the importance of spatial relations in REG (Viethen and Dale, 2008). Uncertainty Based: The costs for all hyperarcs are defined by incorporating uncertainties from perception as described in Section 3.3.2. Uncertainty Relative Preferred: To emphasize the importance of spatial relations as demonstrated in situated interaction (Tenbrink and Moratz, 2003; Kelleher and Kruijff, 2006), the costs for hyperarcs representing relative attributes and relations are divided by 3. This cost function will allow the algorithm to prefer spatial relations through the reduced cost. Note that we only tested a few (not all) commonly used cost functions proposed by previous work (Krahmer et al., 2003; Theune et al., 2007; Krahmer et al., 2008; Theune et al., 2011). For example, we did not include the stochastic cost function which is defined based on the frequencies of attribute selection from the training data (Krahmer et al., 2003). On the one hand, we did n</context>
</contexts>
<marker>Tenbrink, Moratz, 2003</marker>
<rawString>Thora Tenbrink and Reinhard Moratz. 2003. Groupbased spatial reference in linguistic human-robot interaction. Spatial Cognition and Computation, 6:63– 106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari¨et Theune</author>
<author>Pascal Touset</author>
<author>Jette Viethen</author>
<author>Emiel Krahmer</author>
</authors>
<title>Cost-based attribute selection for gre (graph-sc/graph-fp).</title>
<date>2007</date>
<booktitle>In Proceedings of the MT Summit XI Workshop on Using Corpora for NLG: Language Generation and Machine Translation (UCNLG+MT).</booktitle>
<contexts>
<context position="22636" citStr="Theune et al., 2007" startWordPosition="3583" endWordPosition="3586">ce identification task). Each referring expression received three votes from the crowd. In total, 217 turks participated in our experiment. 4.2 Generation Strategies We applied a set of different strategies to generate referring expressions for each object. The variations lie in two dimensions: (1) different graph representations: using a hypergraph to represent the perceived scene as described in Section 3.1 versus using a regular graph as introduced in (Krahmer et al., 2003); and (2) different cost functions for attributes and relations: cost functions that have been used in previous works (Theune et al., 2007; Krahmer et al., 2008) and cost functions that incorporate uncertainties of perception as described in Section 3.3.2. Cost functions play an important role in graphbased approaches (Krahmer et al., 2003). Previous works have examined different types of cost functions (Theune et al., 2007; Krahmer et al., 2008; Theune et al., 2011). We adopted some commonly used cost functions from previous work together with the cost functions defined here. In particular, we experimented with the following different cost functions: Simple Cost: The costs for all hyperarcs are set to 1. With this cost function</context>
<context position="24940" citStr="Theune et al., 2007" startWordPosition="3946" endWordPosition="3949">e costs for all hyperarcs are defined by incorporating uncertainties from perception as described in Section 3.3.2. Uncertainty Relative Preferred: To emphasize the importance of spatial relations as demonstrated in situated interaction (Tenbrink and Moratz, 2003; Kelleher and Kruijff, 2006), the costs for hyperarcs representing relative attributes and relations are divided by 3. This cost function will allow the algorithm to prefer spatial relations through the reduced cost. Note that we only tested a few (not all) commonly used cost functions proposed by previous work (Krahmer et al., 2003; Theune et al., 2007; Krahmer et al., 2008; Theune et al., 2011). For example, we did not include the stochastic cost function which is defined based on the frequencies of attribute selection from the training data (Krahmer et al., 2003). On the one hand, we did not have a large set of human descriptions of the impoverished scene to learn the stochastic cost. On the other hand, it is not clear whether human strategies of describing the impoverished scene should be used to represent optimal strategies for the robot. Nevertheless, the above different cost functions will allow us to evaluate whether incorporating pe</context>
</contexts>
<marker>Theune, Touset, Viethen, Krahmer, 2007</marker>
<rawString>Mari¨et Theune, Pascal Touset, Jette Viethen, and Emiel Krahmer. 2007. Cost-based attribute selection for gre (graph-sc/graph-fp). In Proceedings of the MT Summit XI Workshop on Using Corpora for NLG: Language Generation and Machine Translation (UCNLG+MT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari¨et Theune</author>
<author>Ruud Koolen</author>
<author>Emiel Krahmer</author>
<author>Sander Wubben</author>
</authors>
<title>Does size matter – how much data is required to train a reg algorithm?</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>660--664</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="22969" citStr="Theune et al., 2011" startWordPosition="3637" endWordPosition="3640"> using a hypergraph to represent the perceived scene as described in Section 3.1 versus using a regular graph as introduced in (Krahmer et al., 2003); and (2) different cost functions for attributes and relations: cost functions that have been used in previous works (Theune et al., 2007; Krahmer et al., 2008) and cost functions that incorporate uncertainties of perception as described in Section 3.3.2. Cost functions play an important role in graphbased approaches (Krahmer et al., 2003). Previous works have examined different types of cost functions (Theune et al., 2007; Krahmer et al., 2008; Theune et al., 2011). We adopted some commonly used cost functions from previous work together with the cost functions defined here. In particular, we experimented with the following different cost functions: Simple Cost: The costs for all hyperarcs are set to 1. With this cost function, the graph-based algorithm resembles the Full Brevity algorithm of Dale (Dale, 2To control the quality of crowdsourcing, we recruited participants based on the following criteria: Participants’ locations are limited to the United States. Approval rate for each participant’s previous work is greater than or equal to 95%, and the nu</context>
<context position="24984" citStr="Theune et al., 2011" startWordPosition="3954" endWordPosition="3957">corporating uncertainties from perception as described in Section 3.3.2. Uncertainty Relative Preferred: To emphasize the importance of spatial relations as demonstrated in situated interaction (Tenbrink and Moratz, 2003; Kelleher and Kruijff, 2006), the costs for hyperarcs representing relative attributes and relations are divided by 3. This cost function will allow the algorithm to prefer spatial relations through the reduced cost. Note that we only tested a few (not all) commonly used cost functions proposed by previous work (Krahmer et al., 2003; Theune et al., 2007; Krahmer et al., 2008; Theune et al., 2011). For example, we did not include the stochastic cost function which is defined based on the frequencies of attribute selection from the training data (Krahmer et al., 2003). On the one hand, we did not have a large set of human descriptions of the impoverished scene to learn the stochastic cost. On the other hand, it is not clear whether human strategies of describing the impoverished scene should be used to represent optimal strategies for the robot. Nevertheless, the above different cost functions will allow us to evaluate whether incorporating perceptual uncertainties will make a differenc</context>
</contexts>
<marker>Theune, Koolen, Krahmer, Wubben, 2011</marker>
<rawString>Mari¨et Theune, Ruud Koolen, Emiel Krahmer, and Sander Wubben. 2011. Does size matter – how much data is required to train a reg algorithm? In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 660–664, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristinn R Thrisson</author>
</authors>
<title>Simulated perceptual grouping: An application to human-computer interaction.</title>
<date>1994</date>
<booktitle>In Proceedings of the Sixteenth Annual Conference of the Cognitive Science Society,</booktitle>
<pages>876--881</pages>
<contexts>
<context position="14965" citStr="Thrisson, 1994" startWordPosition="2337" endWordPosition="2338">wo heuristics. The first heuristic is based on perceptual principles, also called the Gestalt Laws of perception (Sternberg, 2003), which describe how people group visually similar objects into entities or groups. Two well known principles of perceptual grouping are proximity and similarity (Wertheimer, 1938): objects that lie close together are often perceived as groups; objects of similar shape, size or color are more likely to form groups than objects differing along these dimensions. Based on these two principles, previous works have developed different algorithms for perceptual grouping (Thrisson, 1994; Gatt, 2006). In our investigation, we adopted Gatt’s algorithm (Gatt, 2006), which has shown to be more accurate for spatial grouping. Given the results from spatial grouping, we only retain hyperarcs that represent spatial relations between two objects, between two perceived groups, between one object and a perceived group, or between one object and the group it belongs to. The second heuristic is based on the observation that, given a certain orientation, people tend to use a relatum that is closer to the referent than more distant relata. In other words, it is less likely to refer to an o</context>
</contexts>
<marker>Thrisson, 1994</marker>
<rawString>Kristinn R. Thrisson. 1994. Simulated perceptual grouping: An application to human-computer interaction. In Proceedings of the Sixteenth Annual Conference of the Cognitive Science Society, pages 876–881.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jette Viethen</author>
<author>Robert Dale</author>
</authors>
<title>The use of spatial relations in referring expression generation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Fifth International Natural Language Generation Conference, INLG ’08,</booktitle>
<pages>59--67</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3097" citStr="Viethen and Dale, 2008" startWordPosition="451" endWordPosition="454">s have much lower perceptual capabilities of the environment than humans. How can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to? There has been a tremendous amount of work on referring expression generation in the last two decades (Dale, 1995; Krahmer and Deemter, 2012). However, most existing REG algorithms were developed and evaluated under the assumption that agents and humans have access to the same kind of domain information. For example, many experimental setups (Gatt et al., 2007; Viethen and Dale, 2008; Golland et al., 2010; Striegnitz et al., 2012) were developed based on a visual world for which the internal representation is assumed to be known and can be represented symbolically. However, this assumption no longer holds in situated dialogue with robots. There are two important distinctions in situated dialogue. First, the perfect knowledge of the environment is not available to the agent ahead of time. The agent needs to automatically make inferences to connect recognized lower-level visual features with 392 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Pro</context>
<context position="24298" citStr="Viethen and Dale, 2008" startWordPosition="3847" endWordPosition="3850">ortest distinguishing description is preferred. Absolute Preferred: The costs for hyperarcs representing absolute attributes (e.g., type, color, and position) are set to 1. The costs for relative attributes (e.g., size) and relations are set to 2. This cost function mimics human’s preference for absolute attributes over relative ones (Dale, 1995). Relative Preferred: The costs for hyperarcs representing absolute attributes are set to 2 and for relative attributes and relations are set to 1. This cost function has been applied previously to emphasize the importance of spatial relations in REG (Viethen and Dale, 2008). Uncertainty Based: The costs for all hyperarcs are defined by incorporating uncertainties from perception as described in Section 3.3.2. Uncertainty Relative Preferred: To emphasize the importance of spatial relations as demonstrated in situated interaction (Tenbrink and Moratz, 2003; Kelleher and Kruijff, 2006), the costs for hyperarcs representing relative attributes and relations are divided by 3. This cost function will allow the algorithm to prefer spatial relations through the reduced cost. Note that we only tested a few (not all) commonly used cost functions proposed by previous work </context>
<context position="27213" citStr="Viethen and Dale, 2008" startWordPosition="4301" endWordPosition="4304"> representations. There are several observations. First, when the agent does not have perfect knowledge of the environment and has to automatically infer the environment as in our setting here, cost functions based on uncertainties of perception lead to better results. This occurs for both regular graphs and hypergraphs. This result is not surprising and indicates that cost functions should be tied to the agent’s ability to perceive and infer the environment. The uncertainty based cost functions allow the agent to prefer reliable attributes or relations. Second, consistent with previous work (Viethen and Dale, 2008), we observed the importance of spatial relations. Especially when the perceived world is full of uncertainties, spatial relations tend to be more reliable. In particular, as shown in Table 1, using hypergraphs enables generating group-based relations and results in significantly better performance (45.2%) compared to regular graphs (36.7%) (p = 0.002). Note that our current cost function only includes uncertainties of the agent’s own perception in a simplistic form. When humans and agents have mismatched perceptual basis, the human’s model of comprehension and tolerance of inaccurate descript</context>
</contexts>
<marker>Viethen, Dale, 2008</marker>
<rawString>Jette Viethen and Robert Dale. 2008. The use of spatial relations in referring expression generation. In Proceedings of the Fifth International Natural Language Generation Conference, INLG ’08, pages 59– 67, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Weijers</author>
</authors>
<title>Referring expressions with groups as landmarks. volume 15.</title>
<date>2011</date>
<institution>University of Twente.</institution>
<contexts>
<context position="10729" citStr="Weijers, 2011" startWordPosition="1642" endWordPosition="1643">. Binary spatial relationships sometimes are difficult to describe the target object, so the matcher must resort to group information to distinguish the target object from the rest of the objects. For example, suppose the matcher needs to describe the target object 5 in Figure 1(b), he/she may have to start by indicating the group of three objects at the bottom and then specify the relationship (i.e., top) of the target object within this group. The importance of group descriptions has been shown not only here, but also in previous works on REG (Funakoshi et al., 2004; Funakoshi et al., 2006; Weijers, 2011). While the original graphbased approach can effectively represent attributes and binary relations between objects (Krahmer et al., 2003), it is insufficient to capture within-group or between-group relations. Therefore, to address the low perceptual capabilities of artificial agents, we introduce hypergraphs to represent the shared environment. Our approach has two unique characteristics compared to previous graph-based approaches: (1) A hypergraph representation is more general than a regular graph. Besides attributes and binary relations, it can also represent group-based relations. (2) Unl</context>
</contexts>
<marker>Weijers, 2011</marker>
<rawString>S. Weijers. 2011. Referring expressions with groups as landmarks. volume 15. University of Twente.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Wertheimer</author>
</authors>
<title>Laws of organization in perceptual forms. A Source Book of Gestalt Psychology. Routledge and Kegan Paul,</title>
<date>1938</date>
<location>London.</location>
<contexts>
<context position="14661" citStr="Wertheimer, 1938" startWordPosition="2289" endWordPosition="2290">f two subsets of nodes are connected by a hyperarc. However, such a complete hypergraph is not only inefficient but also unnecessary. Instead of keeping all possible n-ary relations (i.e., hyperarcs), we only retain those relations that are likely used by humans to produce referring expressions, based on two heuristics. The first heuristic is based on perceptual principles, also called the Gestalt Laws of perception (Sternberg, 2003), which describe how people group visually similar objects into entities or groups. Two well known principles of perceptual grouping are proximity and similarity (Wertheimer, 1938): objects that lie close together are often perceived as groups; objects of similar shape, size or color are more likely to form groups than objects differing along these dimensions. Based on these two principles, previous works have developed different algorithms for perceptual grouping (Thrisson, 1994; Gatt, 2006). In our investigation, we adopted Gatt’s algorithm (Gatt, 2006), which has shown to be more accurate for spatial grouping. Given the results from spatial grouping, we only retain hyperarcs that represent spatial relations between two objects, between two perceived groups, between o</context>
</contexts>
<marker>Wertheimer, 1938</marker>
<rawString>Max Wertheimer. 1938. Laws of organization in perceptual forms. A Source Book of Gestalt Psychology. Routledge and Kegan Paul, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dengsheng Zhang</author>
<author>Guojun Lu</author>
</authors>
<title>An integrated approach to shape based image retrieval.</title>
<date>2002</date>
<booktitle>In Proc. of 5th Asian Conference on Computer Vision (ACCV,</booktitle>
<pages>652--657</pages>
<contexts>
<context position="18392" citStr="Zhang and Lu, 2002" startWordPosition="2903" endWordPosition="2906"> color dimensions (i.e., RGB distribution) is to match the color terms red, green, and blue. For the spatial relation terms such as above, below, left, right, the semantic grounding functions take both vertical and horizontal coordinates of two objects, as follows 1: spatial(Rel : above(a, b) = fabgo�ve(�valoc, vbloc) I 1 — |xa−xb |if ya &lt; Yb; &lt;Il 400 = 0 otherwise. Using the above convention, we have defined semantic grounding functions for size category words (e.g., small and big) and absolute position words (e.g., top, below, left, and right). In addition, we use object recognition models (Zhang and Lu, 2002) to define class type category words such as apple and orange used in our domain. 3.3.2 Attribute Descriptors and Cost Functions Given the lexicon with grounded semantics as described above, the numerical attributes captured in the scene hypergraph can be converted to symbolic descriptors. For each attribute (e.g., color) or relation, the corresponding visual feature vector (i.e., vcolor) is plugged into the semantic grounding functions for the corresponding category of words. The word that best describes the attribute is chosen as the descriptor for that attribute. For example, given an RGB c</context>
<context position="21313" citStr="Zhang and Lu, 2002" startWordPosition="3374" endWordPosition="3377">tent (e.g., attributes and relations) required to uniquely describe the target object. We then apply a set of simple generation templates to generate the surface form of referring expressions based on the hypothesis graph. 4 Empirical Evaluations 4.1 Evaluation Setup To evaluate the performance of this hypergraphbased approach to REG, we conducted a comparative study using crowd-sourcing. More specifically, we created 48 different scenes similar to that in Figure 1(a). Each scene has 13 objects on average and there are 621 objects in total. For each of these scenes, we applied a CV algorithm (Zhang and Lu, 2002) and generated scene hypergraphs as described in Section 3.1. We then use different generation strategies (varied in terms of graph representations and cost functions, to be explained in Section 4.2) to automatically generate referring expressions to refer to each object. To evaluate the quality of these generated referring expressions, we applied Amazon Mechanical Turk to solicit feedback from the crowd 2. Through an interface, we displayed an original scene and generated referring expressions (from different generation strategies) in a random order. We asked each turk to select the object in</context>
</contexts>
<marker>Zhang, Lu, 2002</marker>
<rawString>Dengsheng Zhang and Guojun Lu. 2002. An integrated approach to shape based image retrieval. In Proc. of 5th Asian Conference on Computer Vision (ACCV, pages 652–657.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.098939">
<title confidence="0.990684">
Is Twitter A Better Corpus for Measuring Sentiment Similarity?
</title>
<author confidence="0.999141">
Shi Feng1, Le Zhang1, Binyang Li2,3, Daling Wang1, Ge Yu1, Kam-Fai Wong3
</author>
<affiliation confidence="0.998235333333333">
1Northeastern University, Shenyang, China
2University of International Relations, Beijing, China
3The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
</affiliation>
<email confidence="0.9802825">
{fengshi,wangdaling,yuge}@ise.neu.edu.cn, zhang777le@gmail.com
{byli,kfwong}@se.cuhk.edu.hk
</email>
<sectionHeader confidence="0.998579" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999409466666667">
Extensive experiments have validated the ef-
fectiveness of the corpus-based method for
classifying the word’s sentiment polarity.
However, no work is done for comparing d-
ifferent corpora in the polarity classification
task. Nowadays, Twitter has aggregated huge
amount of data that are full of people’s senti-
ments. In this paper, we empirically evaluate
the performance of different corpora in sen-
timent similarity measurement, which is the
fundamental task for word polarity classifica-
tion. Experiment results show that the Twitter
data can achieve a much better performance
than the Google, Web1T and Wikipedia based
methods.
</bodyText>
<sectionHeader confidence="0.99952" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9991442">
Measuring semantic similarity for words and short
texts has long been a fundamental problem for many
applications such as word sense disambiguation,
query expansion, search advertising and so on.
Determining the word’s polarity plays a critical
role in opinion mining and sentiment analysis task.
Usually we can detect the word’s polarity by mea-
suring it’s semantic similarity with a positive seed
word seg, and a negative seed word sen respectively,
as shown in Formula (1):
</bodyText>
<equation confidence="0.989562">
SO(w) = sim(w, seg,) − sim(w, sen) (1)
</equation>
<bodyText confidence="0.999977842105263">
where sim(wi, wj) is the semantic similarity mea-
surement method for the given word wi and wj. A
lot of papers have been published for designing ap-
propriate similarity measurements. One direction is
to learn similarity from the knowledge base or con-
cept taxonomy (Lin, 1998; Resnik, 1999). Anoth-
er direction is to learn semantic similarity with the
help of large corpus such as Web or Wikipedia da-
ta (Sahami and Heilman, 2006; Yih and Meek, 2007;
Bollegala et al., 2011; Gabrilovich and Markovitch,
2007). The basic assumption of this kind of methods
is that the word with similar semantic meanings of-
ten co-occur in the given corpus. Extensive experi-
ments have validated the effectiveness of the corpus-
based method in polarity classification task (Turney,
2002; Kaji and Kitsuregawa, 2007; Velikovich et al.,
2010). For example, PMI is a well-known similari-
ty measurement (Turney, 2002), which makes use of
the whole Web as the corpus, and utilizes the search
engine hits number to estimate the co-occurrence
probability of the give word pairs. The PMI based
method has achieved promising results. However,
according to Kanayama’s investigation, only 60%
co-occurrences in the same window in Web pages
reflect the same sentiment orientation (Kanayama
and Nasukawa, 2006). Therefore, we may ask the
question whether the choosing of corpus can change
the performance of sim and is there any better cor-
pus than the Web page data for measuring the senti-
ment similarity?
Everyday, enormous numbers of tweets that con-
tain people’s rich sentiments are published in Twit-
ter. The Twitter may be a good source for measuring
the sentiment similarity. Compared with the Web
page data, the tweets have a higher rate of subjective
text posts. The length limitation can guarantee the
polarity consistency of each tweet. Moreover, the
tweets contain graphical emoticons, which can be
</bodyText>
<page confidence="0.968394">
897
</page>
<bodyText confidence="0.872370571428571">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 897–902,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
considered as natural sentiment labels for the corre-
sponding tweets in Twitter. In this paper, we attempt
to empirically evaluate the performance of differen-
t corpora in sentiment similarity measurement task.
As far as we know, no work is done on this topic.
</bodyText>
<sectionHeader confidence="0.91276" genericHeader="method">
2 The Characteristics of Twitter Data
</sectionHeader>
<bodyText confidence="0.991004285714286">
As the world’s second largest SNS website, at the
end of 2012 Twitter had aggregated more than 500
million registered users, among which 200 million
were active users . More than 400 million tweets are
posted every day.
Several examples of typical posts from Twitter are
shown below.
</bodyText>
<listItem confidence="0.990955">
(1) She had a headache and feeling light headed
with no energy.:(
(2) @username Nice work! Looks like you had a
fun day. I’m headed there Sat or Sun.:)
(3) I seen the movie on Direc Tv. I ordered it and
I really liked it. I can’t wait to get it for blu ray!
Excellent work Rob!
</listItem>
<bodyText confidence="0.99977975">
We observe that comparing with the other corpus,
the Twitter data has several advantages in measuring
the sentiment similarity.
Large. Users like to record their personal feelings
and talk about the trend topics in Twitter (Java et al.,
2007; Kwak et al., 2010). So there are huge amount
of subjective texts with various topics generated in
the millions of tweets everyday. Further more, the
flexible Twitter API makes these data easy to access
and collect.
Length Limitation. Twitter has a length limita-
tion of 140 characters. Users have limited space to
express their feelings. So the sentiments in tweet-
s are usually concise, straightforward and polarity
consistent.
Emoticons. Users tend to utilize emoticons to
emphasize their sentiment feelings. According to
the statistics, about 8.1% tweets contain at least one
emoticon (Yang and Leskovec, 2011). Since the
tweets have the length limitation, the sentiments ex-
pressed in these short texts are usually consistent
with the embedded emoticons, such as the word fun
and headache in above examples.
In addition to the above advantages, there are al-
so some disadvantages for measuring sentiment sim-
ilarity using Twitter data. The spam tweets that
caused by advertisements may add noise and bias
during the similarity measurement. The short length
may also bring in lower co-occurrence probability
of words. Some words may not co-occur with each
other when the corpus is small. These disadvantages
set obstacles for measuring sentiment similarity by
using Twitter data as corpus. In the experiment sec-
tion, we will see if we can overcome these draw-
backs and get benefit from the advantages of Twitter
data.
</bodyText>
<sectionHeader confidence="0.982891" genericHeader="method">
3 The Corpus-based Sentiment Similarity
Measurements
</sectionHeader>
<bodyText confidence="0.998310222222222">
The intuition behind the corpus-based semantic sim-
ilarity measuring method is that the words with sim-
ilar meanings tend to co-occur in the corpus. Given
the word wi, wj, we use the notation P(wi) to de-
note the occurrence counts of word wi in the corpus
C. P(wi, wj) denotes the co-occurrence counts of
word wi and wj in C. In this paper we employ the
corpus-based version of the three well-known simi-
larity measurements: Jaccard, Dice and PMI.
</bodyText>
<equation confidence="0.9985416">
CorpusDice(wi, wj) = P(wi) + P(wj) (3)
P(wi,wj)
N
P(wj)
N
</equation>
<bodyText confidence="0.99998475">
In Formula (4), N is the number of documents in
the corpus C. The above similarity measurements
may have their own strengths and weaknesses. In
this paper, we utilize these classical measurements
to evaluate the quality of the corpus in polarity clas-
sification task.
Google is the world’s largest search engine, which
has indexed a huge number of Web pages. Us-
ing the extreme large indexed Web pages as cor-
pus, Cilibrasi and Vitanyi (2007) presented a method
for measuring similarity between words and phras-
es based on information distance and Kolmogorov
complexity. The search result page counts of Google
were utilized to estimate the occurrence frequencies
of the words in the corpus. Suppose wi, wj rep-
resent the candidate words, the Normalized Google
</bodyText>
<equation confidence="0.942978733333333">
CorpusJaccard(wi, wj)
(2)
P(wi,wj)
P(wi)+P(wj)−P(wi,wj)
=
2 x P(wi, wj)
CorpusPMI(wi, wj) = lo92(
P(wi)
N
) (4)
898
Distance is defined as:
NGD(wi, wj) _
max{log P(wi),log P(wj)}−log P(wi,wj) (5)
log N−min{log P(wi),log P(wj)}
</equation>
<bodyText confidence="0.999942615384615">
where P(wi) denotes page counts returned by
Google using wi as keyword; P(wi, wj) denotes the
page counts by using wi and wj as joint keywords;
N is the number of Web pages indexed by Google.
Cilibrasi and Vitanyi have validated the effective-
ness of Google distance in measuring the semantic
similarity between concept words.
Based on the above formulas, we compare the
Twitter data with the Web and Wikipedia data as the
similarity measurement corpus. Given a candidate
word w, we firstly measure its sentiment similar-
ity with a positive seed word and a negative seed
word respectively in Formula (1), and the difference
of sim is used to further detect the polarity of w.
The above four similarity measurements serve as
sim with Web, Wikipedia and Twitter data as cor-
pus. Turney (2002) chose excellent and poor as
seed words. However, using isolated seed word-
s may cause the bias problem. Therefore, we fur-
ther select two groups of seed words that are lack
of sensitivity to context and form a positive seed set
PS and a negative seed set NS (Turney, 2003). The
Formula (1) can be rewritten as:
are also removed. Finally, we construct the Twitter
corpus that consists of 266.8 million English tweets.
For calculating page counts in Web data, the candi-
date words were launched to Google from February
2013 to April 2013. We also conduct the experi-
ments on the Google Web 1T data that consists of
Google n-gram counts (frequency of occurrence of
each n-gram) for 1 &lt; n &lt; 5 (Brants and Franz,
2006). The Web 1T data provides a nice approxi-
mation to the word co-occurrence statistics in Web
pages in a predefined window size (1 &lt; n &lt; 5).
For example, the 5 gram Web1T data means the co-
occurrence window size is 5. The English Wikipedia
dump 1 we used was extracted at the end of March
2013, which contained more than 13 million articles.
We extracted the plain texts of the Wikipedia data as
the training corpus for the Formula (6).
Evaluation Method. Two well-know sentiment lex-
icons are utilized as gold standard for polarity clas-
sification task. The statistics of Liu’s sentiment lex-
icon (Liu et al., 2005) and MPQA subjectivity lexi-
con (Wilson et al., 2005) are shown in Table 1. For
each word w in the lexicons, we employ the Formu-
la (6) to calculate the word’s polarity using different
corpora. If SO(w) &gt; 0, the word w is classified in-
to the positive category. Otherwise if SO(w) &lt; 0, it
is classified into the negative category. The accura-
cy of the classification result is used to measure the
quality of the corpus.
</bodyText>
<figure confidence="0.959313666666667">
sim(w, sen) (6)
11
SO(w) _
sepEPS
Positive# Negative#
Liu 2,006 4,783
MPQA 2,304 4,153
11 sim(w, sep) −
senENS
</figure>
<bodyText confidence="0.996431666666667">
Based on the Formula(6) and the sentiment seed
words, we can measure the sentiment polarity of the
given candidate words.
</bodyText>
<sectionHeader confidence="0.999907" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.99912">
4.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.999161625">
Corpus Preparing. The Twitter corpus corre-
sponds to the 476 million Twitter tweets (Yang and
Leskovec, 2011), which includes over 476 million
Twitter posts from 20 million users, covering a 7
month period from June 1, 2009 to December 31,
2009. We filter out the non-English tweets and the
spam tweets that have only few words with URLs.
The tweets that contain three or more trending topics
</bodyText>
<tableCaption confidence="0.998578">
Table 1: Lexicon size
</tableCaption>
<subsectionHeader confidence="0.995116">
4.2 Experiment Results
</subsectionHeader>
<bodyText confidence="0.999787888888889">
Firstly, we chose the seed words excellent and poor
as Turney’s (2002) settings. The polarity classifica-
tion accuracies are shown in Table 2.
In Table 2, Google, Web1T, Wikipedia, Twitter
represent the corpora that used in the experiment;
CJ, CD, CP, GD represent the Formula (2) to For-
mula (5) respectively. We can see from the Table 2
that the Twitter based method can achieve the best
performance. The rich sentiment information and
</bodyText>
<footnote confidence="0.992204">
1http://en.wikipedia.org/
</footnote>
<page confidence="0.981289">
899
</page>
<table confidence="0.999887727272727">
Lexicon Corpus CJ CD CP GD
Liu Google 0.5116 0.5117 0.5064 0.5076
Web1T-5gram 0.3903 0.3903 0.3897 0.3864
Web1T-4gram 0.3771 0.3771 0.3772 0.3227
Wikipedia 0.5280 0.5280 0.5350 0.5412
Twitter 0.5567 0.5567 0.5635 0.5635
MPQA Google 0.4897 0.4890 0.4891 0.4864
Web1T-5gram 0.3843 0.3843 0.3837 0.3783
Web1T-4gram 0.3729 0.3729 0.3714 0.3225
Wikipedia 0.5181 0.5181 0.5380 0.5344
Twitter 0.5421 0.5421 0.5493 0.5494
</table>
<tableCaption confidence="0.997031">
Table 2: Polarity classification accuracies using excellent
and poor as seed words
</tableCaption>
<bodyText confidence="0.999664685714286">
natural window size (140 characters) have a posi-
tive impact on determining the word’s polarity. The
Google based method gets a lower accuracy, this
may be due to the length of Web documents which
can not usually guarantee the semantic consistency
in the returned data. Even though two words appear
in one page (returned by Google), they might not be
semantically related. Furthermore, the Google based
method is time-consuming, because we have to peri-
odically send queries in order to avoid being blocked
by Google. The Web1T based method gets a much
worse accuracy. After detailed analysis, we find that
although the small window size (4 or 5) can guar-
antee the semantic consistency, the short length also
brings in lower co-occurrence probability. Statistics
show that about 38% SO values are zero when using
Web1T corpus. Due to the short length, the Twitter
data also suffers from the low co-occurrence prob-
lem.
To tackle the low co-occurrence problem, the seed
word sets are selected as Turney’s (2003) settings.
The positive word set PS={good, nice, excellent,
positive, fortunate, correct, superior} and negative
word set NS = {bad, nasty, poor, negative, unfortu-
nate, wrong, inferior} for the Formula (6). These
seed words have been verified to be effective in Tur-
ney’s paper for polarity classification. The experi-
ment results are shown in Table 3.
Table 3 shows that the performance of Twitter cor-
pus is much improved since the multiple seed words
alleviate the problem of low co-occurrence probabil-
ity in tweets. Generally, when using the seed word
groups the Twitter can achieve a much better per-
formance than all the other corpora. The improve-
ments are statistically significant (p-value &lt; 0.05).
</bodyText>
<table confidence="0.999694615384615">
Lexicon Corpus CJ CD CP GD
Liu Google 0.4859 0.4936 0.4884 0.5060
Web1T-5gram 0.5785 0.5785 0.3963 0.5782
Web1T-4gram 0.5766 0.5766 0.3872 0.5775
Wikipedia 0.6226 0.6225 0.5957 0.6145
Twitter 0.6678 0.6678 0.6917 0.6457
Twitter+ 0.6921 0.6921 0.7273 0.6599
MPQA Google 0.5108 0.5225 0.5735 0.5763
Web1T-5gram 0.5737 0.5737 0.4225 0.5718
Web1T-4gram 0.5749 0.5749 0.3329 0.4797
Wikipedia 0.6086 0.6085 0.5773 0.5985
Twitter 0.6431 0.6431 0.6671 0.6253
Twitter+ 0.6665 0.6665 0.7001 0.6383
</table>
<tableCaption confidence="0.995042">
Table 3: Polarity classification accuracies using the seed
word groups
</tableCaption>
<bodyText confidence="0.9999467">
We further add the emoticons ‘:)’ and ‘:(’ into the
seed word groups, denoted by Twitter+ in Table 3.
The emoticons are natural sentiment labels. We can
see that the performances are further improved by
considering emoticons as seed words. The above
experiment results have validated the effectiveness
of Twitter data as a better corpus for measuring the
sentiment similarity. The results also reveal the po-
tential usefulness of Twitter corpus in semantic sim-
ilarity measurement.
</bodyText>
<sectionHeader confidence="0.999913" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999946636363636">
Detecting the polarity of words is the fundamental
problem for most of sentiment analysis tasks (Hatzi-
vassiloglou and McKeown, 1997; Pang and Lee,
2007; Feldman, 2013).
Many methods have been proposed to measure
the words’ or short texts similarity based on large
corpus (Sahami and Heilman, 2006; Yih and Meek,
2007; Gabrilovich and Markovitch, 2007). Bolle-
gala et al. (2011) submitted the word to the search
engine, and the related result pages were employed
to represent the meaning of the original word. Mi-
halcea et al. (2006) proposed a method to measure
the semantic similarity of words or short texts, con-
sidering both corpus-based and knowledge-based in-
formation. Although the previous algorithms have
achieved promising results, there are no work done
on evaluating the quality of different corpora.
Mohtarami et al. (2012; 2013a; 2013b) intro-
duced the concept of sentiment similarity, which
was considered as different from the traditional se-
mantic similarity, and more focused on revealing the
underlying sentiment relations between words. Mo-
</bodyText>
<page confidence="0.988397">
900
</page>
<bodyText confidence="0.999545333333333">
htarami et al. (2013b) proposed a hidden emotion-
al model to calculating the sentiment similarity of
word pairs. However, the impact of the different cor-
pora is not considered for this task.
Mohammad et al. (2013) generated word-
sentiment association lexicons from Tweets with the
help of hashtags and emoticons. Pak and Paroubek
(2010) collected tweets with happy and sad emoti-
cons as training corpus, and built sentiment classi-
fier based on traditional machine learning methods.
Brody and Diakopoulos (2011) showed that length-
ening was strongly associated with subjectivity and
sentiment in tweets. Davidov et al. (2010) treated 50
Twitter tags and 15 smileys as sentiment labels and
a supervised sentiment classification framework was
proposed to classify the tweets. The previous litera-
tures have showed that the emoticons can be treated
as natural sentiment labels of the tweets.
</bodyText>
<sectionHeader confidence="0.993766" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999921">
The quality of corpus may affect the performance
of sentiment similarity measurement. In this pa-
per, we compare the Twitter data with the Google,
Web1T and Wikipedia data in polarity classification
task. The experiment results validate that when us-
ing the seed word groups the Twitter can achieve a
much better performance than the other corpora and
adding emoticons as seed words can further improve
the performance. It is observed that the twitter cor-
pus is a potential good source for measuring senti-
ment similarity between words. In future work, we
intend to design new similarity measurements that
can make best of the advantages of Twitter data.
</bodyText>
<sectionHeader confidence="0.986917" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999486166666667">
This research is partially supported by Gener-
al Research Fund of Hong Kong (No. 417112)
and Shenzhen Fundamental Research Program (J-
CYJ20130401172046450). This research is al-
so supported by the State Key Development Pro-
gram for Basic Research of China (Grant No.
2011CB302200-G), State Key Program of Nation-
al Natural Science of China (Grant No. 61033007),
National Natural Science Foundation of China
(Grant No. 61370074, 61100026), and the Funda-
mental Research Funds for the Central Universities
(N120404007).
</bodyText>
<sectionHeader confidence="0.98148" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998083647058823">
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. Linguistic Data Consortium, ISBN: 1-
58563-397-6, Philadelphia.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll! !!!!!!!!!!!!! Using
Word Lengthening to Detect Sentiment in Microblogs.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
562–570, Edinburgh, UK, ACL.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru Ishizu-
ka. 2011. A Web Search Engine-Based Approach to
Measure Semantic Similarity between Words. IEEE
Transactions on Knowledge and Data Engineering,
23(7): 977–990.
Rudi Cilibrasi and Paul Vitanyi. 2007. The Google Simi-
larity Distance. IEEE Transactions on Knowledge and
Data Engineering, 19(3): 370–383.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced Sentiment Learning Using Twitter Hashtags
and Smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics, pages 241–
249, Beijing, China, ACL.
Ronen Feldman. 2013. Techniques and Applications for
Sentiment Analysis. Communications of the ACM,
56(4):82–89.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness Using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence, pages 1606–1611, Hyderabad, India.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics, pages
174–181, Madrid, Spain, ACL.
Akshay Java, Xiaodan Song, Tim Finin, and Belle L. T-
seng. 2007. Why We Twitter: An Analysis of a Mi-
croblogging Community. In Proceedings of the 9th
International Workshop on Knowledge Discovery on
the Web and 1st International Workshop on Social Net-
works Analysis, pages 118–138, San Jose, CA, USA,
Springer.
Nobuhiro Kaji and Masaru Kitsuregawa. Building Lex-
icon for Sentiment Analysis from Massive Collec-
tion of HTML Documents. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pp. 1075–1083, Prague, Czech
Republic, ACL.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Ful-
ly Automatic Lexicon Expansion for Domain-oriented
</reference>
<page confidence="0.991064">
901
</page>
<reference confidence="0.9981642">
Sentiment Analysis. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 355–363, Sydney, Australia, ACL.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
B. Moon. 2010. What is Twitter, a Social Network or a
News Media? In Proceedings of the 19th Internation-
al Conference on World Wide Web, pages 591–600,
Raleigh, North Carolina, USA, ACM.
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proceedings of the 17th In-
ternational Conference on Computational Linguistics,
pages 768–774, Montreal, Quebec, Canada, ACL.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion Observer: Analyzing and Comparing Opin-
ions on the Web. In Proceedings of the 14th interna-
tional conference on World Wide Web, pages 342–351,
Chiba, Japan, ACM.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and Knowledge-based Measures
of Text Semantic Similarity. In Proceedings of the 21st
National Conference on Artificial Intelligence and the
18th Innovative Applications of Artificial Intelligence
Conference, pages 775–780, Boston, Massachusetts,
USA, AAAI Press.
Mitra Mohtarami, Hadi Amiri, Man Lan, Thanh Phu
Tran, and Chew Lim Tan. 2012. Sense Sentimen-
t Similarity: An Analysis. In Proceedings of the
Twenty-Sixth AAAI Conference on Artificial Intelli-
gence, pages 1706–1712, Toronto, Ontario, Canada,
AAAI Press.
Mitra Mohtarami, Man Lan, and Chew Lim Tan. 2013a.
From Semantic to Emotional Space in Probabilistic
Sense Sentiment Analysis. In Proceedings of the
Twenty-Seventh AAAI Conference on Artificial Intel-
ligence, pages 711–717, Bellevue, Washington, USA,
AAAI Press.
Mitra Mohtarami, Man Lan, and Chew Lim Tan. 2013b.
Probabilistic Sense Sentiment Similarity through Hid-
den Emotions. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics, pages 983–992, Sofia, Bulgaria, ACL.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-of-the-
Art in Sentiment Analysis of Tweets. In Proceedings
of the seventh international workshop on Semantic E-
valuation Exercises, Atlanta, Georgia, USA, ACL.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
Corpus for Sentiment Analysis and Opinion Mining.
In Proceedings of the 2010 International Conference
on Language Resources and Evaluation, pages 1320–
1326, Valletta, Malta, ELRA.
Philip Resnik. 1999. Semantic Similarity in a Taxonomy:
An Information based Measure and Its Application to
Problems of Ambiguity in Natural Language. Journal
of Artificial Intelligence Research, 11:95–130.
Mehran Sahami and Timothy D. Heilman. 2006. A Web-
based Kernel Function for Measuring the Similarity of
Short Text Snippets. In Proceedings of the 15th inter-
national conference on World Wide Web, pages 377–
386, Edinburgh, Scotland, UK, ACM.
Bo Pang and Lillian Lee. 2007. Opinion Mining and Sen-
timent Analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1–135.
Peter D. Turney. 2002. Thumbs Up or Thumbs Down?
Semantic Orientation Applied to Unsupervised Classi-
fication of Reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, pages 417–424, Philadelphia, PA, USA, ACL.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transaction Information Sys-
tem, 21(4): 315–346.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan T. McDonald. The Viability of Web-
derived Polarity Lexicons. In Proceedings of the 2010
North American Chapter of the Association of Compu-
tational Linguistics, pp. 777–785, Los Angeles, Cali-
fornia, USA, ACL.
Theresa Wilson, Janyce Wiebe, and Paul Hoffman-
n. 2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of the 2005
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 347–354, Vancouver, British Columbi-
a, Canada, ACL.
Jaewon Yang and Jure Leskovec. 2011. Patterns of Tem-
poral Variation in Online Media. In Proceedings of
the Forth International Conference on Web Search and
Web Data Mining, pages 177–186, Hong Kong, Chi-
na, ACM.
Wen-tau Yih and Christopher Meek. 2007. Improving
Similarity Measures for Short Segments of Text. In
Proceedings of the 22nd AAAI Conference on Artificial
Intelligence, pages 1489–1494, Vancouver, British
Columbia, Canada, AAAI Press.
</reference>
<page confidence="0.997653">
902
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.819648">
<title confidence="0.984618">Is Twitter A Better Corpus for Measuring Sentiment Similarity?</title>
<author confidence="0.986272">Le_Binyang Daling Ge Kam-Fai</author>
<affiliation confidence="0.961927">University, Shenyang, of International Relations, Beijing,</affiliation>
<address confidence="0.954096">Chinese University of Hong Kong, Shatin, N.T., Hong</address>
<abstract confidence="0.996849875">Extensive experiments have validated the effectiveness of the corpus-based method for classifying the word’s sentiment polarity. However, no work is done for comparing different corpora in the polarity classification task. Nowadays, Twitter has aggregated huge amount of data that are full of people’s sentiments. In this paper, we empirically evaluate the performance of different corpora in sentiment similarity measurement, which is the fundamental task for word polarity classification. Experiment results show that the Twitter data can achieve a much better performance than the Google, Web1T and Wikipedia based methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1t 5-gram version 1.</title>
<date>2006</date>
<booktitle>Linguistic Data Consortium, ISBN:</booktitle>
<pages>1--58563</pages>
<location>Philadelphia.</location>
<contexts>
<context position="9175" citStr="Brants and Franz, 2006" startWordPosition="1486" endWordPosition="1489">he bias problem. Therefore, we further select two groups of seed words that are lack of sensitivity to context and form a positive seed set PS and a negative seed set NS (Turney, 2003). The Formula (1) can be rewritten as: are also removed. Finally, we construct the Twitter corpus that consists of 266.8 million English tweets. For calculating page counts in Web data, the candidate words were launched to Google from February 2013 to April 2013. We also conduct the experiments on the Google Web 1T data that consists of Google n-gram counts (frequency of occurrence of each n-gram) for 1 &lt; n &lt; 5 (Brants and Franz, 2006). The Web 1T data provides a nice approximation to the word co-occurrence statistics in Web pages in a predefined window size (1 &lt; n &lt; 5). For example, the 5 gram Web1T data means the cooccurrence window size is 5. The English Wikipedia dump 1 we used was extracted at the end of March 2013, which contained more than 13 million articles. We extracted the plain texts of the Wikipedia data as the training corpus for the Formula (6). Evaluation Method. Two well-know sentiment lexicons are utilized as gold standard for polarity classification task. The statistics of Liu’s sentiment lexicon (Liu et </context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram version 1. Linguistic Data Consortium, ISBN: 1-58563-397-6, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Nicholas Diakopoulos</author>
</authors>
<title>Cooooooooooooooollllllllllllll! !!!!!!!!!!!!! Using Word Lengthening to Detect Sentiment in Microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>562--570</pages>
<location>Edinburgh, UK, ACL.</location>
<contexts>
<context position="16196" citStr="Brody and Diakopoulos (2011)" startWordPosition="2618" endWordPosition="2621">e traditional semantic similarity, and more focused on revealing the underlying sentiment relations between words. Mo900 htarami et al. (2013b) proposed a hidden emotional model to calculating the sentiment similarity of word pairs. However, the impact of the different corpora is not considered for this task. Mohammad et al. (2013) generated wordsentiment association lexicons from Tweets with the help of hashtags and emoticons. Pak and Paroubek (2010) collected tweets with happy and sad emoticons as training corpus, and built sentiment classifier based on traditional machine learning methods. Brody and Diakopoulos (2011) showed that lengthening was strongly associated with subjectivity and sentiment in tweets. Davidov et al. (2010) treated 50 Twitter tags and 15 smileys as sentiment labels and a supervised sentiment classification framework was proposed to classify the tweets. The previous literatures have showed that the emoticons can be treated as natural sentiment labels of the tweets. 6 Conclusion and Future Work The quality of corpus may affect the performance of sentiment similarity measurement. In this paper, we compare the Twitter data with the Google, Web1T and Wikipedia data in polarity classificati</context>
</contexts>
<marker>Brody, Diakopoulos, 2011</marker>
<rawString>Samuel Brody and Nicholas Diakopoulos. 2011. Cooooooooooooooollllllllllllll! !!!!!!!!!!!!! Using Word Lengthening to Detect Sentiment in Microblogs. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 562–570, Edinburgh, UK, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danushka Bollegala</author>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>A Web Search Engine-Based Approach to Measure Semantic Similarity between Words.</title>
<date>2011</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>23</volume>
<issue>7</issue>
<pages>977--990</pages>
<contexts>
<context position="2021" citStr="Bollegala et al., 2011" startWordPosition="301" endWordPosition="304">ntic similarity with a positive seed word seg, and a negative seed word sen respectively, as shown in Formula (1): SO(w) = sim(w, seg,) − sim(w, sen) (1) where sim(wi, wj) is the semantic similarity measurement method for the given word wi and wj. A lot of papers have been published for designing appropriate similarity measurements. One direction is to learn similarity from the knowledge base or concept taxonomy (Lin, 1998; Resnik, 1999). Another direction is to learn semantic similarity with the help of large corpus such as Web or Wikipedia data (Sahami and Heilman, 2006; Yih and Meek, 2007; Bollegala et al., 2011; Gabrilovich and Markovitch, 2007). The basic assumption of this kind of methods is that the word with similar semantic meanings often co-occur in the given corpus. Extensive experiments have validated the effectiveness of the corpusbased method in polarity classification task (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). For example, PMI is a well-known similarity measurement (Turney, 2002), which makes use of the whole Web as the corpus, and utilizes the search engine hits number to estimate the co-occurrence probability of the give word pairs. The PMI based method ha</context>
<context position="15008" citStr="Bollegala et al. (2011)" startWordPosition="2432" endWordPosition="2436">e experiment results have validated the effectiveness of Twitter data as a better corpus for measuring the sentiment similarity. The results also reveal the potential usefulness of Twitter corpus in semantic similarity measurement. 5 Related Work Detecting the polarity of words is the fundamental problem for most of sentiment analysis tasks (Hatzivassiloglou and McKeown, 1997; Pang and Lee, 2007; Feldman, 2013). Many methods have been proposed to measure the words’ or short texts similarity based on large corpus (Sahami and Heilman, 2006; Yih and Meek, 2007; Gabrilovich and Markovitch, 2007). Bollegala et al. (2011) submitted the word to the search engine, and the related result pages were employed to represent the meaning of the original word. Mihalcea et al. (2006) proposed a method to measure the semantic similarity of words or short texts, considering both corpus-based and knowledge-based information. Although the previous algorithms have achieved promising results, there are no work done on evaluating the quality of different corpora. Mohtarami et al. (2012; 2013a; 2013b) introduced the concept of sentiment similarity, which was considered as different from the traditional semantic similarity, and m</context>
</contexts>
<marker>Bollegala, Matsuo, Ishizuka, 2011</marker>
<rawString>Danushka Bollegala, Yutaka Matsuo, and Mitsuru Ishizuka. 2011. A Web Search Engine-Based Approach to Measure Semantic Similarity between Words. IEEE Transactions on Knowledge and Data Engineering, 23(7): 977–990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudi Cilibrasi</author>
<author>Paul Vitanyi</author>
</authors>
<title>The Google Similarity Distance.</title>
<date>2007</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>19</volume>
<issue>3</issue>
<pages>370--383</pages>
<contexts>
<context position="7134" citStr="Cilibrasi and Vitanyi (2007)" startWordPosition="1139" endWordPosition="1142"> wj in C. In this paper we employ the corpus-based version of the three well-known similarity measurements: Jaccard, Dice and PMI. CorpusDice(wi, wj) = P(wi) + P(wj) (3) P(wi,wj) N P(wj) N In Formula (4), N is the number of documents in the corpus C. The above similarity measurements may have their own strengths and weaknesses. In this paper, we utilize these classical measurements to evaluate the quality of the corpus in polarity classification task. Google is the world’s largest search engine, which has indexed a huge number of Web pages. Using the extreme large indexed Web pages as corpus, Cilibrasi and Vitanyi (2007) presented a method for measuring similarity between words and phrases based on information distance and Kolmogorov complexity. The search result page counts of Google were utilized to estimate the occurrence frequencies of the words in the corpus. Suppose wi, wj represent the candidate words, the Normalized Google CorpusJaccard(wi, wj) (2) P(wi,wj) P(wi)+P(wj)−P(wi,wj) = 2 x P(wi, wj) CorpusPMI(wi, wj) = lo92( P(wi) N ) (4) 898 Distance is defined as: NGD(wi, wj) _ max{log P(wi),log P(wj)}−log P(wi,wj) (5) log N−min{log P(wi),log P(wj)} where P(wi) denotes page counts returned by Google using</context>
</contexts>
<marker>Cilibrasi, Vitanyi, 2007</marker>
<rawString>Rudi Cilibrasi and Paul Vitanyi. 2007. The Google Similarity Distance. IEEE Transactions on Knowledge and Data Engineering, 19(3): 370–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced Sentiment Learning Using Twitter Hashtags and Smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>241--249</pages>
<location>Beijing, China, ACL.</location>
<contexts>
<context position="16309" citStr="Davidov et al. (2010)" startWordPosition="2635" endWordPosition="2638"> htarami et al. (2013b) proposed a hidden emotional model to calculating the sentiment similarity of word pairs. However, the impact of the different corpora is not considered for this task. Mohammad et al. (2013) generated wordsentiment association lexicons from Tweets with the help of hashtags and emoticons. Pak and Paroubek (2010) collected tweets with happy and sad emoticons as training corpus, and built sentiment classifier based on traditional machine learning methods. Brody and Diakopoulos (2011) showed that lengthening was strongly associated with subjectivity and sentiment in tweets. Davidov et al. (2010) treated 50 Twitter tags and 15 smileys as sentiment labels and a supervised sentiment classification framework was proposed to classify the tweets. The previous literatures have showed that the emoticons can be treated as natural sentiment labels of the tweets. 6 Conclusion and Future Work The quality of corpus may affect the performance of sentiment similarity measurement. In this paper, we compare the Twitter data with the Google, Web1T and Wikipedia data in polarity classification task. The experiment results validate that when using the seed word groups the Twitter can achieve a much bett</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced Sentiment Learning Using Twitter Hashtags and Smileys. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 241– 249, Beijing, China, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronen Feldman</author>
</authors>
<title>Techniques and Applications for Sentiment Analysis.</title>
<date>2013</date>
<journal>Communications of the ACM,</journal>
<volume>56</volume>
<issue>4</issue>
<contexts>
<context position="14799" citStr="Feldman, 2013" startWordPosition="2401" endWordPosition="2402">the seed word groups, denoted by Twitter+ in Table 3. The emoticons are natural sentiment labels. We can see that the performances are further improved by considering emoticons as seed words. The above experiment results have validated the effectiveness of Twitter data as a better corpus for measuring the sentiment similarity. The results also reveal the potential usefulness of Twitter corpus in semantic similarity measurement. 5 Related Work Detecting the polarity of words is the fundamental problem for most of sentiment analysis tasks (Hatzivassiloglou and McKeown, 1997; Pang and Lee, 2007; Feldman, 2013). Many methods have been proposed to measure the words’ or short texts similarity based on large corpus (Sahami and Heilman, 2006; Yih and Meek, 2007; Gabrilovich and Markovitch, 2007). Bollegala et al. (2011) submitted the word to the search engine, and the related result pages were employed to represent the meaning of the original word. Mihalcea et al. (2006) proposed a method to measure the semantic similarity of words or short texts, considering both corpus-based and knowledge-based information. Although the previous algorithms have achieved promising results, there are no work done on eva</context>
</contexts>
<marker>Feldman, 2013</marker>
<rawString>Ronen Feldman. 2013. Techniques and Applications for Sentiment Analysis. Communications of the ACM, 56(4):82–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1606--1611</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="2056" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="305" endWordPosition="308">ositive seed word seg, and a negative seed word sen respectively, as shown in Formula (1): SO(w) = sim(w, seg,) − sim(w, sen) (1) where sim(wi, wj) is the semantic similarity measurement method for the given word wi and wj. A lot of papers have been published for designing appropriate similarity measurements. One direction is to learn similarity from the knowledge base or concept taxonomy (Lin, 1998; Resnik, 1999). Another direction is to learn semantic similarity with the help of large corpus such as Web or Wikipedia data (Sahami and Heilman, 2006; Yih and Meek, 2007; Bollegala et al., 2011; Gabrilovich and Markovitch, 2007). The basic assumption of this kind of methods is that the word with similar semantic meanings often co-occur in the given corpus. Extensive experiments have validated the effectiveness of the corpusbased method in polarity classification task (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). For example, PMI is a well-known similarity measurement (Turney, 2002), which makes use of the whole Web as the corpus, and utilizes the search engine hits number to estimate the co-occurrence probability of the give word pairs. The PMI based method has achieved promising results. Howev</context>
<context position="14983" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="2428" endWordPosition="2431">g emoticons as seed words. The above experiment results have validated the effectiveness of Twitter data as a better corpus for measuring the sentiment similarity. The results also reveal the potential usefulness of Twitter corpus in semantic similarity measurement. 5 Related Work Detecting the polarity of words is the fundamental problem for most of sentiment analysis tasks (Hatzivassiloglou and McKeown, 1997; Pang and Lee, 2007; Feldman, 2013). Many methods have been proposed to measure the words’ or short texts similarity based on large corpus (Sahami and Heilman, 2006; Yih and Meek, 2007; Gabrilovich and Markovitch, 2007). Bollegala et al. (2011) submitted the word to the search engine, and the related result pages were employed to represent the meaning of the original word. Mihalcea et al. (2006) proposed a method to measure the semantic similarity of words or short texts, considering both corpus-based and knowledge-based information. Although the previous algorithms have achieved promising results, there are no work done on evaluating the quality of different corpora. Mohtarami et al. (2012; 2013a; 2013b) introduced the concept of sentiment similarity, which was considered as different from the traditional s</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 1606–1611, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen McKeown</author>
</authors>
<title>Predicting the Semantic Orientation of Adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>174--181</pages>
<location>Madrid, Spain, ACL.</location>
<contexts>
<context position="14763" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="2392" endWordPosition="2396"> groups We further add the emoticons ‘:)’ and ‘:(’ into the seed word groups, denoted by Twitter+ in Table 3. The emoticons are natural sentiment labels. We can see that the performances are further improved by considering emoticons as seed words. The above experiment results have validated the effectiveness of Twitter data as a better corpus for measuring the sentiment similarity. The results also reveal the potential usefulness of Twitter corpus in semantic similarity measurement. 5 Related Work Detecting the polarity of words is the fundamental problem for most of sentiment analysis tasks (Hatzivassiloglou and McKeown, 1997; Pang and Lee, 2007; Feldman, 2013). Many methods have been proposed to measure the words’ or short texts similarity based on large corpus (Sahami and Heilman, 2006; Yih and Meek, 2007; Gabrilovich and Markovitch, 2007). Bollegala et al. (2011) submitted the word to the search engine, and the related result pages were employed to represent the meaning of the original word. Mihalcea et al. (2006) proposed a method to measure the semantic similarity of words or short texts, considering both corpus-based and knowledge-based information. Although the previous algorithms have achieved promising re</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen McKeown. 1997. Predicting the Semantic Orientation of Adjectives. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 174–181, Madrid, Spain, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshay Java</author>
<author>Xiaodan Song</author>
<author>Tim Finin</author>
<author>Belle L Tseng</author>
</authors>
<title>Why We Twitter: An Analysis of a Microblogging Community.</title>
<date>2007</date>
<booktitle>In Proceedings of the 9th International Workshop on Knowledge Discovery on the Web and 1st International Workshop on Social Networks Analysis,</booktitle>
<pages>118--138</pages>
<publisher>USA, Springer.</publisher>
<location>San Jose, CA,</location>
<contexts>
<context position="4723" citStr="Java et al., 2007" startWordPosition="742" endWordPosition="745">lion tweets are posted every day. Several examples of typical posts from Twitter are shown below. (1) She had a headache and feeling light headed with no energy.:( (2) @username Nice work! Looks like you had a fun day. I’m headed there Sat or Sun.:) (3) I seen the movie on Direc Tv. I ordered it and I really liked it. I can’t wait to get it for blu ray! Excellent work Rob! We observe that comparing with the other corpus, the Twitter data has several advantages in measuring the sentiment similarity. Large. Users like to record their personal feelings and talk about the trend topics in Twitter (Java et al., 2007; Kwak et al., 2010). So there are huge amount of subjective texts with various topics generated in the millions of tweets everyday. Further more, the flexible Twitter API makes these data easy to access and collect. Length Limitation. Twitter has a length limitation of 140 characters. Users have limited space to express their feelings. So the sentiments in tweets are usually concise, straightforward and polarity consistent. Emoticons. Users tend to utilize emoticons to emphasize their sentiment feelings. According to the statistics, about 8.1% tweets contain at least one emoticon (Yang and Le</context>
</contexts>
<marker>Java, Song, Finin, Tseng, 2007</marker>
<rawString>Akshay Java, Xiaodan Song, Tim Finin, and Belle L. Tseng. 2007. Why We Twitter: An Analysis of a Microblogging Community. In Proceedings of the 9th International Workshop on Knowledge Discovery on the Web and 1st International Workshop on Social Networks Analysis, pages 118–138, San Jose, CA, USA, Springer.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Nobuhiro Kaji</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Building Lexicon for Sentiment Analysis from Massive Collection of HTML Documents.</title>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1075--1083</pages>
<location>Prague, Czech Republic, ACL.</location>
<marker>Kaji, Kitsuregawa, </marker>
<rawString>Nobuhiro Kaji and Masaru Kitsuregawa. Building Lexicon for Sentiment Analysis from Massive Collection of HTML Documents. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 1075–1083, Prague, Czech Republic, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Kanayama</author>
<author>Tetsuya Nasukawa</author>
</authors>
<title>Fully Automatic Lexicon Expansion for Domain-oriented Sentiment Analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>355--363</pages>
<location>Sydney, Australia, ACL.</location>
<contexts>
<context position="2823" citStr="Kanayama and Nasukawa, 2006" startWordPosition="424" endWordPosition="427">e experiments have validated the effectiveness of the corpusbased method in polarity classification task (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). For example, PMI is a well-known similarity measurement (Turney, 2002), which makes use of the whole Web as the corpus, and utilizes the search engine hits number to estimate the co-occurrence probability of the give word pairs. The PMI based method has achieved promising results. However, according to Kanayama’s investigation, only 60% co-occurrences in the same window in Web pages reflect the same sentiment orientation (Kanayama and Nasukawa, 2006). Therefore, we may ask the question whether the choosing of corpus can change the performance of sim and is there any better corpus than the Web page data for measuring the sentiment similarity? Everyday, enormous numbers of tweets that contain people’s rich sentiments are published in Twitter. The Twitter may be a good source for measuring the sentiment similarity. Compared with the Web page data, the tweets have a higher rate of subjective text posts. The length limitation can guarantee the polarity consistency of each tweet. Moreover, the tweets contain graphical emoticons, which can be 89</context>
</contexts>
<marker>Kanayama, Nasukawa, 2006</marker>
<rawString>Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully Automatic Lexicon Expansion for Domain-oriented Sentiment Analysis. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 355–363, Sydney, Australia, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haewoon Kwak</author>
<author>Changhyun Lee</author>
<author>Hosung Park</author>
<author>Sue B Moon</author>
</authors>
<title>What is Twitter, a Social Network or a News Media?</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th International Conference on World Wide Web,</booktitle>
<pages>591--600</pages>
<publisher>ACM.</publisher>
<location>Raleigh, North Carolina, USA,</location>
<contexts>
<context position="4743" citStr="Kwak et al., 2010" startWordPosition="746" endWordPosition="749">ted every day. Several examples of typical posts from Twitter are shown below. (1) She had a headache and feeling light headed with no energy.:( (2) @username Nice work! Looks like you had a fun day. I’m headed there Sat or Sun.:) (3) I seen the movie on Direc Tv. I ordered it and I really liked it. I can’t wait to get it for blu ray! Excellent work Rob! We observe that comparing with the other corpus, the Twitter data has several advantages in measuring the sentiment similarity. Large. Users like to record their personal feelings and talk about the trend topics in Twitter (Java et al., 2007; Kwak et al., 2010). So there are huge amount of subjective texts with various topics generated in the millions of tweets everyday. Further more, the flexible Twitter API makes these data easy to access and collect. Length Limitation. Twitter has a length limitation of 140 characters. Users have limited space to express their feelings. So the sentiments in tweets are usually concise, straightforward and polarity consistent. Emoticons. Users tend to utilize emoticons to emphasize their sentiment feelings. According to the statistics, about 8.1% tweets contain at least one emoticon (Yang and Leskovec, 2011). Since</context>
</contexts>
<marker>Kwak, Lee, Park, Moon, 2010</marker>
<rawString>Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue B. Moon. 2010. What is Twitter, a Social Network or a News Media? In Proceedings of the 19th International Conference on World Wide Web, pages 591–600, Raleigh, North Carolina, USA, ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics,</booktitle>
<pages>768--774</pages>
<location>Montreal, Quebec, Canada, ACL.</location>
<contexts>
<context position="1825" citStr="Lin, 1998" startWordPosition="268" endWordPosition="269">tising and so on. Determining the word’s polarity plays a critical role in opinion mining and sentiment analysis task. Usually we can detect the word’s polarity by measuring it’s semantic similarity with a positive seed word seg, and a negative seed word sen respectively, as shown in Formula (1): SO(w) = sim(w, seg,) − sim(w, sen) (1) where sim(wi, wj) is the semantic similarity measurement method for the given word wi and wj. A lot of papers have been published for designing appropriate similarity measurements. One direction is to learn similarity from the knowledge base or concept taxonomy (Lin, 1998; Resnik, 1999). Another direction is to learn semantic similarity with the help of large corpus such as Web or Wikipedia data (Sahami and Heilman, 2006; Yih and Meek, 2007; Bollegala et al., 2011; Gabrilovich and Markovitch, 2007). The basic assumption of this kind of methods is that the word with similar semantic meanings often co-occur in the given corpus. Extensive experiments have validated the effectiveness of the corpusbased method in polarity classification task (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). For example, PMI is a well-known similarity measurement </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic Retrieval and Clustering of Similar Words. In Proceedings of the 17th International Conference on Computational Linguistics, pages 768–774, Montreal, Quebec, Canada, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Minqing Hu</author>
<author>Junsheng Cheng</author>
</authors>
<title>Opinion Observer: Analyzing and Comparing Opinions on the Web.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th international conference on World Wide Web,</booktitle>
<pages>342--351</pages>
<publisher>ACM.</publisher>
<location>Chiba, Japan,</location>
<contexts>
<context position="9785" citStr="Liu et al., 2005" startWordPosition="1595" endWordPosition="1598">, 2006). The Web 1T data provides a nice approximation to the word co-occurrence statistics in Web pages in a predefined window size (1 &lt; n &lt; 5). For example, the 5 gram Web1T data means the cooccurrence window size is 5. The English Wikipedia dump 1 we used was extracted at the end of March 2013, which contained more than 13 million articles. We extracted the plain texts of the Wikipedia data as the training corpus for the Formula (6). Evaluation Method. Two well-know sentiment lexicons are utilized as gold standard for polarity classification task. The statistics of Liu’s sentiment lexicon (Liu et al., 2005) and MPQA subjectivity lexicon (Wilson et al., 2005) are shown in Table 1. For each word w in the lexicons, we employ the Formula (6) to calculate the word’s polarity using different corpora. If SO(w) &gt; 0, the word w is classified into the positive category. Otherwise if SO(w) &lt; 0, it is classified into the negative category. The accuracy of the classification result is used to measure the quality of the corpus. sim(w, sen) (6) 11 SO(w) _ sepEPS Positive# Negative# Liu 2,006 4,783 MPQA 2,304 4,153 11 sim(w, sep) − senENS Based on the Formula(6) and the sentiment seed words, we can measure the </context>
</contexts>
<marker>Liu, Hu, Cheng, 2005</marker>
<rawString>Bing Liu, Minqing Hu, and Junsheng Cheng. 2005. Opinion Observer: Analyzing and Comparing Opinions on the Web. In Proceedings of the 14th international conference on World Wide Web, pages 342–351, Chiba, Japan, ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and Knowledge-based Measures of Text Semantic Similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence and the 18th Innovative Applications of Artificial Intelligence Conference,</booktitle>
<pages>775--780</pages>
<publisher>AAAI Press.</publisher>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="15162" citStr="Mihalcea et al. (2006)" startWordPosition="2459" endWordPosition="2463">e potential usefulness of Twitter corpus in semantic similarity measurement. 5 Related Work Detecting the polarity of words is the fundamental problem for most of sentiment analysis tasks (Hatzivassiloglou and McKeown, 1997; Pang and Lee, 2007; Feldman, 2013). Many methods have been proposed to measure the words’ or short texts similarity based on large corpus (Sahami and Heilman, 2006; Yih and Meek, 2007; Gabrilovich and Markovitch, 2007). Bollegala et al. (2011) submitted the word to the search engine, and the related result pages were employed to represent the meaning of the original word. Mihalcea et al. (2006) proposed a method to measure the semantic similarity of words or short texts, considering both corpus-based and knowledge-based information. Although the previous algorithms have achieved promising results, there are no work done on evaluating the quality of different corpora. Mohtarami et al. (2012; 2013a; 2013b) introduced the concept of sentiment similarity, which was considered as different from the traditional semantic similarity, and more focused on revealing the underlying sentiment relations between words. Mo900 htarami et al. (2013b) proposed a hidden emotional model to calculating t</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and Knowledge-based Measures of Text Semantic Similarity. In Proceedings of the 21st National Conference on Artificial Intelligence and the 18th Innovative Applications of Artificial Intelligence Conference, pages 775–780, Boston, Massachusetts, USA, AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitra Mohtarami</author>
<author>Hadi Amiri</author>
<author>Man Lan</author>
<author>Thanh Phu Tran</author>
<author>Chew Lim Tan</author>
</authors>
<title>Sense Sentiment Similarity: An Analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence,</booktitle>
<pages>1706--1712</pages>
<publisher>AAAI Press.</publisher>
<location>Toronto, Ontario, Canada,</location>
<contexts>
<context position="15463" citStr="Mohtarami et al. (2012" startWordPosition="2505" endWordPosition="2508">re the words’ or short texts similarity based on large corpus (Sahami and Heilman, 2006; Yih and Meek, 2007; Gabrilovich and Markovitch, 2007). Bollegala et al. (2011) submitted the word to the search engine, and the related result pages were employed to represent the meaning of the original word. Mihalcea et al. (2006) proposed a method to measure the semantic similarity of words or short texts, considering both corpus-based and knowledge-based information. Although the previous algorithms have achieved promising results, there are no work done on evaluating the quality of different corpora. Mohtarami et al. (2012; 2013a; 2013b) introduced the concept of sentiment similarity, which was considered as different from the traditional semantic similarity, and more focused on revealing the underlying sentiment relations between words. Mo900 htarami et al. (2013b) proposed a hidden emotional model to calculating the sentiment similarity of word pairs. However, the impact of the different corpora is not considered for this task. Mohammad et al. (2013) generated wordsentiment association lexicons from Tweets with the help of hashtags and emoticons. Pak and Paroubek (2010) collected tweets with happy and sad emo</context>
</contexts>
<marker>Mohtarami, Amiri, Lan, Tran, Tan, 2012</marker>
<rawString>Mitra Mohtarami, Hadi Amiri, Man Lan, Thanh Phu Tran, and Chew Lim Tan. 2012. Sense Sentiment Similarity: An Analysis. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, pages 1706–1712, Toronto, Ontario, Canada, AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitra Mohtarami</author>
<author>Man Lan</author>
<author>Chew Lim Tan</author>
</authors>
<title>From Semantic to Emotional Space in Probabilistic Sense Sentiment Analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence,</booktitle>
<pages>711--717</pages>
<publisher>AAAI Press.</publisher>
<location>Bellevue, Washington, USA,</location>
<marker>Mohtarami, Lan, Tan, 2013</marker>
<rawString>Mitra Mohtarami, Man Lan, and Chew Lim Tan. 2013a. From Semantic to Emotional Space in Probabilistic Sense Sentiment Analysis. In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence, pages 711–717, Bellevue, Washington, USA, AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitra Mohtarami</author>
<author>Man Lan</author>
<author>Chew Lim Tan</author>
</authors>
<title>Probabilistic Sense Sentiment Similarity through Hidden Emotions.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>983--992</pages>
<location>Sofia, Bulgaria, ACL.</location>
<marker>Mohtarami, Lan, Tan, 2013</marker>
<rawString>Mitra Mohtarami, Man Lan, and Chew Lim Tan. 2013b. Probabilistic Sense Sentiment Similarity through Hidden Emotions. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 983–992, Sofia, Bulgaria, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the State-of-theArt in Sentiment Analysis of Tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the seventh international workshop on Semantic Evaluation Exercises,</booktitle>
<location>Atlanta, Georgia, USA, ACL.</location>
<contexts>
<context position="15901" citStr="Mohammad et al. (2013)" startWordPosition="2574" endWordPosition="2577">ledge-based information. Although the previous algorithms have achieved promising results, there are no work done on evaluating the quality of different corpora. Mohtarami et al. (2012; 2013a; 2013b) introduced the concept of sentiment similarity, which was considered as different from the traditional semantic similarity, and more focused on revealing the underlying sentiment relations between words. Mo900 htarami et al. (2013b) proposed a hidden emotional model to calculating the sentiment similarity of word pairs. However, the impact of the different corpora is not considered for this task. Mohammad et al. (2013) generated wordsentiment association lexicons from Tweets with the help of hashtags and emoticons. Pak and Paroubek (2010) collected tweets with happy and sad emoticons as training corpus, and built sentiment classifier based on traditional machine learning methods. Brody and Diakopoulos (2011) showed that lengthening was strongly associated with subjectivity and sentiment in tweets. Davidov et al. (2010) treated 50 Twitter tags and 15 smileys as sentiment labels and a supervised sentiment classification framework was proposed to classify the tweets. The previous literatures have showed that t</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the State-of-theArt in Sentiment Analysis of Tweets. In Proceedings of the seventh international workshop on Semantic Evaluation Exercises, Atlanta, Georgia, USA, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a Corpus for Sentiment Analysis and Opinion Mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 International Conference on Language Resources and Evaluation,</booktitle>
<pages>1320--1326</pages>
<location>Valletta, Malta, ELRA.</location>
<contexts>
<context position="16023" citStr="Pak and Paroubek (2010)" startWordPosition="2592" endWordPosition="2595">uating the quality of different corpora. Mohtarami et al. (2012; 2013a; 2013b) introduced the concept of sentiment similarity, which was considered as different from the traditional semantic similarity, and more focused on revealing the underlying sentiment relations between words. Mo900 htarami et al. (2013b) proposed a hidden emotional model to calculating the sentiment similarity of word pairs. However, the impact of the different corpora is not considered for this task. Mohammad et al. (2013) generated wordsentiment association lexicons from Tweets with the help of hashtags and emoticons. Pak and Paroubek (2010) collected tweets with happy and sad emoticons as training corpus, and built sentiment classifier based on traditional machine learning methods. Brody and Diakopoulos (2011) showed that lengthening was strongly associated with subjectivity and sentiment in tweets. Davidov et al. (2010) treated 50 Twitter tags and 15 smileys as sentiment labels and a supervised sentiment classification framework was proposed to classify the tweets. The previous literatures have showed that the emoticons can be treated as natural sentiment labels of the tweets. 6 Conclusion and Future Work The quality of corpus </context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a Corpus for Sentiment Analysis and Opinion Mining. In Proceedings of the 2010 International Conference on Language Resources and Evaluation, pages 1320– 1326, Valletta, Malta, ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Semantic Similarity in a Taxonomy: An Information based Measure and Its Application to Problems of Ambiguity in Natural Language.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>11--95</pages>
<contexts>
<context position="1840" citStr="Resnik, 1999" startWordPosition="270" endWordPosition="271">so on. Determining the word’s polarity plays a critical role in opinion mining and sentiment analysis task. Usually we can detect the word’s polarity by measuring it’s semantic similarity with a positive seed word seg, and a negative seed word sen respectively, as shown in Formula (1): SO(w) = sim(w, seg,) − sim(w, sen) (1) where sim(wi, wj) is the semantic similarity measurement method for the given word wi and wj. A lot of papers have been published for designing appropriate similarity measurements. One direction is to learn similarity from the knowledge base or concept taxonomy (Lin, 1998; Resnik, 1999). Another direction is to learn semantic similarity with the help of large corpus such as Web or Wikipedia data (Sahami and Heilman, 2006; Yih and Meek, 2007; Bollegala et al., 2011; Gabrilovich and Markovitch, 2007). The basic assumption of this kind of methods is that the word with similar semantic meanings often co-occur in the given corpus. Extensive experiments have validated the effectiveness of the corpusbased method in polarity classification task (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). For example, PMI is a well-known similarity measurement (Turney, 2002),</context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik. 1999. Semantic Similarity in a Taxonomy: An Information based Measure and Its Application to Problems of Ambiguity in Natural Language. Journal of Artificial Intelligence Research, 11:95–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehran Sahami</author>
<author>Timothy D Heilman</author>
</authors>
<title>A Webbased Kernel Function for Measuring the Similarity of Short Text Snippets.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th international conference on World Wide Web,</booktitle>
<pages>377--386</pages>
<publisher>ACM.</publisher>
<location>Edinburgh, Scotland, UK,</location>
<contexts>
<context position="1977" citStr="Sahami and Heilman, 2006" startWordPosition="293" endWordPosition="296">ect the word’s polarity by measuring it’s semantic similarity with a positive seed word seg, and a negative seed word sen respectively, as shown in Formula (1): SO(w) = sim(w, seg,) − sim(w, sen) (1) where sim(wi, wj) is the semantic similarity measurement method for the given word wi and wj. A lot of papers have been published for designing appropriate similarity measurements. One direction is to learn similarity from the knowledge base or concept taxonomy (Lin, 1998; Resnik, 1999). Another direction is to learn semantic similarity with the help of large corpus such as Web or Wikipedia data (Sahami and Heilman, 2006; Yih and Meek, 2007; Bollegala et al., 2011; Gabrilovich and Markovitch, 2007). The basic assumption of this kind of methods is that the word with similar semantic meanings often co-occur in the given corpus. Extensive experiments have validated the effectiveness of the corpusbased method in polarity classification task (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). For example, PMI is a well-known similarity measurement (Turney, 2002), which makes use of the whole Web as the corpus, and utilizes the search engine hits number to estimate the co-occurrence probability of </context>
<context position="14928" citStr="Sahami and Heilman, 2006" startWordPosition="2420" endWordPosition="2423">erformances are further improved by considering emoticons as seed words. The above experiment results have validated the effectiveness of Twitter data as a better corpus for measuring the sentiment similarity. The results also reveal the potential usefulness of Twitter corpus in semantic similarity measurement. 5 Related Work Detecting the polarity of words is the fundamental problem for most of sentiment analysis tasks (Hatzivassiloglou and McKeown, 1997; Pang and Lee, 2007; Feldman, 2013). Many methods have been proposed to measure the words’ or short texts similarity based on large corpus (Sahami and Heilman, 2006; Yih and Meek, 2007; Gabrilovich and Markovitch, 2007). Bollegala et al. (2011) submitted the word to the search engine, and the related result pages were employed to represent the meaning of the original word. Mihalcea et al. (2006) proposed a method to measure the semantic similarity of words or short texts, considering both corpus-based and knowledge-based information. Although the previous algorithms have achieved promising results, there are no work done on evaluating the quality of different corpora. Mohtarami et al. (2012; 2013a; 2013b) introduced the concept of sentiment similarity, w</context>
</contexts>
<marker>Sahami, Heilman, 2006</marker>
<rawString>Mehran Sahami and Timothy D. Heilman. 2006. A Webbased Kernel Function for Measuring the Similarity of Short Text Snippets. In Proceedings of the 15th international conference on World Wide Web, pages 377– 386, Edinburgh, Scotland, UK, ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval,</title>
<date>2007</date>
<pages>2--1</pages>
<contexts>
<context position="14783" citStr="Pang and Lee, 2007" startWordPosition="2397" endWordPosition="2400"> ‘:)’ and ‘:(’ into the seed word groups, denoted by Twitter+ in Table 3. The emoticons are natural sentiment labels. We can see that the performances are further improved by considering emoticons as seed words. The above experiment results have validated the effectiveness of Twitter data as a better corpus for measuring the sentiment similarity. The results also reveal the potential usefulness of Twitter corpus in semantic similarity measurement. 5 Related Work Detecting the polarity of words is the fundamental problem for most of sentiment analysis tasks (Hatzivassiloglou and McKeown, 1997; Pang and Lee, 2007; Feldman, 2013). Many methods have been proposed to measure the words’ or short texts similarity based on large corpus (Sahami and Heilman, 2006; Yih and Meek, 2007; Gabrilovich and Markovitch, 2007). Bollegala et al. (2011) submitted the word to the search engine, and the related result pages were employed to represent the meaning of the original word. Mihalcea et al. (2006) proposed a method to measure the semantic similarity of words or short texts, considering both corpus-based and knowledge-based information. Although the previous algorithms have achieved promising results, there are no </context>
</contexts>
<marker>Pang, Lee, 2007</marker>
<rawString>Bo Pang and Lillian Lee. 2007. Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>417--424</pages>
<location>Philadelphia, PA, USA, ACL.</location>
<contexts>
<context position="2313" citStr="Turney, 2002" startWordPosition="348" endWordPosition="349">opriate similarity measurements. One direction is to learn similarity from the knowledge base or concept taxonomy (Lin, 1998; Resnik, 1999). Another direction is to learn semantic similarity with the help of large corpus such as Web or Wikipedia data (Sahami and Heilman, 2006; Yih and Meek, 2007; Bollegala et al., 2011; Gabrilovich and Markovitch, 2007). The basic assumption of this kind of methods is that the word with similar semantic meanings often co-occur in the given corpus. Extensive experiments have validated the effectiveness of the corpusbased method in polarity classification task (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). For example, PMI is a well-known similarity measurement (Turney, 2002), which makes use of the whole Web as the corpus, and utilizes the search engine hits number to estimate the co-occurrence probability of the give word pairs. The PMI based method has achieved promising results. However, according to Kanayama’s investigation, only 60% co-occurrences in the same window in Web pages reflect the same sentiment orientation (Kanayama and Nasukawa, 2006). Therefore, we may ask the question whether the choosing of corpus can change the perform</context>
<context position="8465" citStr="Turney (2002)" startWordPosition="1360" endWordPosition="1361">ndexed by Google. Cilibrasi and Vitanyi have validated the effectiveness of Google distance in measuring the semantic similarity between concept words. Based on the above formulas, we compare the Twitter data with the Web and Wikipedia data as the similarity measurement corpus. Given a candidate word w, we firstly measure its sentiment similarity with a positive seed word and a negative seed word respectively in Formula (1), and the difference of sim is used to further detect the polarity of w. The above four similarity measurements serve as sim with Web, Wikipedia and Twitter data as corpus. Turney (2002) chose excellent and poor as seed words. However, using isolated seed words may cause the bias problem. Therefore, we further select two groups of seed words that are lack of sensitivity to context and form a positive seed set PS and a negative seed set NS (Turney, 2003). The Formula (1) can be rewritten as: are also removed. Finally, we construct the Twitter corpus that consists of 266.8 million English tweets. For calculating page counts in Web data, the candidate words were launched to Google from February 2013 to April 2013. We also conduct the experiments on the Google Web 1T data that co</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 417–424, Philadelphia, PA, USA, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Michael Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transaction Information System,</journal>
<volume>21</volume>
<issue>4</issue>
<pages>315--346</pages>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter Turney and Michael Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transaction Information System, 21(4): 315–346.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Leonid Velikovich</author>
<author>Sasha Blair-Goldensohn</author>
<author>Kerry Hannan</author>
<author>Ryan T McDonald</author>
</authors>
<title>The Viability of Webderived Polarity Lexicons.</title>
<booktitle>In Proceedings of the 2010 North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>777--785</pages>
<location>Los Angeles, California, USA, ACL.</location>
<marker>Velikovich, Blair-Goldensohn, Hannan, McDonald, </marker>
<rawString>Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan T. McDonald. The Viability of Webderived Polarity Lexicons. In Proceedings of the 2010 North American Chapter of the Association of Computational Linguistics, pp. 777–785, Los Angeles, California, USA, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity in PhraseLevel Sentiment Analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<location>Vancouver, British Columbia, Canada, ACL.</location>
<contexts>
<context position="9837" citStr="Wilson et al., 2005" startWordPosition="1604" endWordPosition="1607">ation to the word co-occurrence statistics in Web pages in a predefined window size (1 &lt; n &lt; 5). For example, the 5 gram Web1T data means the cooccurrence window size is 5. The English Wikipedia dump 1 we used was extracted at the end of March 2013, which contained more than 13 million articles. We extracted the plain texts of the Wikipedia data as the training corpus for the Formula (6). Evaluation Method. Two well-know sentiment lexicons are utilized as gold standard for polarity classification task. The statistics of Liu’s sentiment lexicon (Liu et al., 2005) and MPQA subjectivity lexicon (Wilson et al., 2005) are shown in Table 1. For each word w in the lexicons, we employ the Formula (6) to calculate the word’s polarity using different corpora. If SO(w) &gt; 0, the word w is classified into the positive category. Otherwise if SO(w) &lt; 0, it is classified into the negative category. The accuracy of the classification result is used to measure the quality of the corpus. sim(w, sen) (6) 11 SO(w) _ sepEPS Positive# Negative# Liu 2,006 4,783 MPQA 2,304 4,153 11 sim(w, sep) − senENS Based on the Formula(6) and the sentiment seed words, we can measure the sentiment polarity of the given candidate words. 4 E</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing Contextual Polarity in PhraseLevel Sentiment Analysis. In Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 347–354, Vancouver, British Columbia, Canada, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaewon Yang</author>
<author>Jure Leskovec</author>
</authors>
<title>Patterns of Temporal Variation in Online Media.</title>
<date>2011</date>
<booktitle>In Proceedings of the Forth International Conference on Web Search and Web Data Mining,</booktitle>
<pages>177--186</pages>
<publisher>ACM.</publisher>
<location>Hong Kong, China,</location>
<contexts>
<context position="5336" citStr="Yang and Leskovec, 2011" startWordPosition="837" endWordPosition="840">t al., 2007; Kwak et al., 2010). So there are huge amount of subjective texts with various topics generated in the millions of tweets everyday. Further more, the flexible Twitter API makes these data easy to access and collect. Length Limitation. Twitter has a length limitation of 140 characters. Users have limited space to express their feelings. So the sentiments in tweets are usually concise, straightforward and polarity consistent. Emoticons. Users tend to utilize emoticons to emphasize their sentiment feelings. According to the statistics, about 8.1% tweets contain at least one emoticon (Yang and Leskovec, 2011). Since the tweets have the length limitation, the sentiments expressed in these short texts are usually consistent with the embedded emoticons, such as the word fun and headache in above examples. In addition to the above advantages, there are also some disadvantages for measuring sentiment similarity using Twitter data. The spam tweets that caused by advertisements may add noise and bias during the similarity measurement. The short length may also bring in lower co-occurrence probability of words. Some words may not co-occur with each other when the corpus is small. These disadvantages set o</context>
<context position="10576" citStr="Yang and Leskovec, 2011" startWordPosition="1734" endWordPosition="1737">using different corpora. If SO(w) &gt; 0, the word w is classified into the positive category. Otherwise if SO(w) &lt; 0, it is classified into the negative category. The accuracy of the classification result is used to measure the quality of the corpus. sim(w, sen) (6) 11 SO(w) _ sepEPS Positive# Negative# Liu 2,006 4,783 MPQA 2,304 4,153 11 sim(w, sep) − senENS Based on the Formula(6) and the sentiment seed words, we can measure the sentiment polarity of the given candidate words. 4 Experiment 4.1 Experiment Setup Corpus Preparing. The Twitter corpus corresponds to the 476 million Twitter tweets (Yang and Leskovec, 2011), which includes over 476 million Twitter posts from 20 million users, covering a 7 month period from June 1, 2009 to December 31, 2009. We filter out the non-English tweets and the spam tweets that have only few words with URLs. The tweets that contain three or more trending topics Table 1: Lexicon size 4.2 Experiment Results Firstly, we chose the seed words excellent and poor as Turney’s (2002) settings. The polarity classification accuracies are shown in Table 2. In Table 2, Google, Web1T, Wikipedia, Twitter represent the corpora that used in the experiment; CJ, CD, CP, GD represent the For</context>
</contexts>
<marker>Yang, Leskovec, 2011</marker>
<rawString>Jaewon Yang and Jure Leskovec. 2011. Patterns of Temporal Variation in Online Media. In Proceedings of the Forth International Conference on Web Search and Web Data Mining, pages 177–186, Hong Kong, China, ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Christopher Meek</author>
</authors>
<title>Improving Similarity Measures for Short Segments of Text.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd AAAI Conference on Artificial Intelligence,</booktitle>
<pages>1489--1494</pages>
<publisher>AAAI Press.</publisher>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="1997" citStr="Yih and Meek, 2007" startWordPosition="297" endWordPosition="300"> measuring it’s semantic similarity with a positive seed word seg, and a negative seed word sen respectively, as shown in Formula (1): SO(w) = sim(w, seg,) − sim(w, sen) (1) where sim(wi, wj) is the semantic similarity measurement method for the given word wi and wj. A lot of papers have been published for designing appropriate similarity measurements. One direction is to learn similarity from the knowledge base or concept taxonomy (Lin, 1998; Resnik, 1999). Another direction is to learn semantic similarity with the help of large corpus such as Web or Wikipedia data (Sahami and Heilman, 2006; Yih and Meek, 2007; Bollegala et al., 2011; Gabrilovich and Markovitch, 2007). The basic assumption of this kind of methods is that the word with similar semantic meanings often co-occur in the given corpus. Extensive experiments have validated the effectiveness of the corpusbased method in polarity classification task (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). For example, PMI is a well-known similarity measurement (Turney, 2002), which makes use of the whole Web as the corpus, and utilizes the search engine hits number to estimate the co-occurrence probability of the give word pairs.</context>
<context position="14948" citStr="Yih and Meek, 2007" startWordPosition="2424" endWordPosition="2427">proved by considering emoticons as seed words. The above experiment results have validated the effectiveness of Twitter data as a better corpus for measuring the sentiment similarity. The results also reveal the potential usefulness of Twitter corpus in semantic similarity measurement. 5 Related Work Detecting the polarity of words is the fundamental problem for most of sentiment analysis tasks (Hatzivassiloglou and McKeown, 1997; Pang and Lee, 2007; Feldman, 2013). Many methods have been proposed to measure the words’ or short texts similarity based on large corpus (Sahami and Heilman, 2006; Yih and Meek, 2007; Gabrilovich and Markovitch, 2007). Bollegala et al. (2011) submitted the word to the search engine, and the related result pages were employed to represent the meaning of the original word. Mihalcea et al. (2006) proposed a method to measure the semantic similarity of words or short texts, considering both corpus-based and knowledge-based information. Although the previous algorithms have achieved promising results, there are no work done on evaluating the quality of different corpora. Mohtarami et al. (2012; 2013a; 2013b) introduced the concept of sentiment similarity, which was considered </context>
</contexts>
<marker>Yih, Meek, 2007</marker>
<rawString>Wen-tau Yih and Christopher Meek. 2007. Improving Similarity Measures for Short Segments of Text. In Proceedings of the 22nd AAAI Conference on Artificial Intelligence, pages 1489–1494, Vancouver, British Columbia, Canada, AAAI Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
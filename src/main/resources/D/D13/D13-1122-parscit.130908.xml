<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001110">
<title confidence="0.977127">
Exploiting Multiple Sources for Open-domain Hypernym Discovery
</title>
<author confidence="0.996889">
Ruiji Fu, Bing Qin, Ting Liu*
</author>
<affiliation confidence="0.996268666666667">
Research Center for Social Computing and Information Retrieval
School of Computer Science and Technology
Harbin Institute of Technology, China
</affiliation>
<email confidence="0.969857">
{rjfu, bqin, tliu}@ir.hit.edu.cn
</email>
<sectionHeader confidence="0.994771" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999447739130435">
Hypernym discovery aims to extract such
noun pairs that one noun is a hypernym of
the other. Most previous methods are based
on lexical patterns but perform badly on open-
domain data. Other work extracts hypernym
relations from encyclopedias but has limited
coverage. This paper proposes a simple yet ef-
fective distant supervision framework for Chi-
nese open-domain hypernym discovery. Giv-
en an entity name, we try to discover its hy-
pernyms by leveraging knowledge from mul-
tiple sources, i.e., search engine results, ency-
clopedias, and morphology of the entity name.
First, we extract candidate hypernyms from
the above sources. Then, we apply a statistical
ranking model to select correct hypernyms. A
set of novel features is proposed for the rank-
ing model. We also present a heuristic strate-
gy to build a large-scale noisy training data for
the model without human annotation. Exper-
imental results demonstrate that our approach
outperforms the state-of-the-art methods on a
manually labeled test dataset.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.992555391304348">
Hypernym discovery is a task to extract such noun
pairs that one noun is a hypernym of the other (S-
now et al., 2005). A noun H is a hypernym of an-
other noun E if E is an instance or subclass of H. In
other word, H is a semantic class of E. For instance,
“actor” is a hypernym of “Mel Gibson”; “dog” is a
hypernym of “Caucasian sheepdog”; “medicine” is
a hypernym of “Aspirin”. Hypernym discovery is
an important subtask of semantic relation extraction
*Email correspondence.
and has many applications in ontology construction
(Suchanek et al., 2008), machine reading (Etzion-
i et al., 2006), question answering (McNamee et al.,
2008), and so on.
Some manually constructed thesauri such as
WordNet can also provide some semantic relations
such as hypernyms. However, these thesauri are lim-
ited in its scope and domain, and manual construc-
tion is knowledge-intensive and time-consuming.
Therefore, many researchers try to automatically ex-
tract semantic relations or to construct taxonomies.
Most previous methods on automatic hypernym
discovery are based on lexical patterns and suffer
from the problem that such patterns can only cov-
er a small part of complex linguistic circumstances
(Hearst, 1992; Turney et al., 2003; Zhang et al.,
2011). Other work tries to extract hypernym rela-
tions from large-scale encyclopedias like Wikipedia
and achieves high precision (Suchanek et al., 2008;
Hoffart et al., 2012). However, the coverage is limit-
ed since there exist many infrequent and new entities
that are missing in encyclopedias (Lin et al., 2012).
We made similar observation that more than a half
of entities in our data set have no entries in the en-
cyclopedias.
This paper proposes a simple yet effective distan-
t supervision framework for Chinese open-domain
hypernym discovery. Given an entity name, our goal
is to discover its hypernyms by leveraging knowl-
edge from multiple sources. Considering the case
where a person wants to know the meaning of an un-
known entity, he/she may search it in a search engine
and then finds out the answer after going through the
search results. Furthermore, if he/she finds an en-
try about the entity in an authentic web site, such as
Wikipedia, the information will help him/her under-
</bodyText>
<page confidence="0.942423">
1224
</page>
<note confidence="0.7316095">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1224–1234,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.996889545454546">
stand the entity. Also, the morphology of the enti-
ty name can provide supplementary information. In
this paper, we imitate the process. The evidences
from the above sources are integrated in our hyper-
nym discovery model.
Our approach is composed of two major steps:
hypernym candidate extraction and ranking. In the
first step, we collect hypernym candidates from mul-
tiple sources. Given an entity name, we search it in
a search engine and extract high-frequency nouns as
its main candidate hypernyms from the search re-
sults. We also collect the category tags for the entity
from two Chinese encyclopedias and the head word
of the entity as the candidates.
In the second step, we identify correct hypernyms
from the candidates. We view this task as a rank-
ing problem and propose a set of effective features
to build a statistical ranking model. For the param-
eter learning of the model, we also present a heuris-
tic strategy to build a large-scale noisy training data
without human annotation.
Our contributions are as follows:
</bodyText>
<listItem confidence="0.991043">
• We are the first to discover hypernym for Chi-
nese open-domain entities by exploiting mul-
tiple sources. The evidences from different
sources can authenticate and complement each
other to improve both precision and recall.
• We manually annotate a dataset containing
1,879 Chinese entities and their hypernyms,
which will be made publicly available. To the
best of our knowledge, this is the first dataset
for Chinese hypernyms.
• We propose a set of novel and effective fea-
tures for hypernym ranking. Experimental re-
sults show that our method achieves the best
performance.
</listItem>
<bodyText confidence="0.9953214">
Furthermore, our approach can be easily ported
from Chinese to English and other languages, except
that a few language dependent features need to be
changed.
The remainder of the paper is organized as fol-
lows: Section 2 discusses the related work. Section
3 introduces our method in detail. Section 4 de-
scribes the experimental setup. Section 5 shows the
experimental results. Conclusion and future work
are presented in Section 6.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999502195652174">
Previous methods for hypernym discovery can be
summarized into two major categories, i.e., pattern-
based methods and encyclopedia-based methods.
Pattern-based methods make use of manually
or automatically constructed patterns to mine hyper-
nym relations from text corpora. The pioneer work
by Hearst (1992) finds that linking two noun phras-
es (NPs) via certain lexical constructions often im-
plies hypernym relations. For example, NP1 is a hy-
pernym of NP2 in the lexical pattern “such NP1 as
NP2”. Similarly, succeeding researchers follow her
work and use handcrafted patterns to extract hyper-
nym pairs from corpora (Caraballo, 1999; Scott and
Dominic, 2003; Ciaramita and Johnson, 2003; Tur-
ney et al., 2003; Pasca, 2004; Etzioni et al., 2005;
Ritter et al., 2009; Zhang et al., 2011).
Evans (2004) considers the web data as a large
corpus and uses search engines to identify hyper-
nyms based on lexical patterns. Given an arbitrary
document, he takes each capitalized word sequence
as an entity and aims to find its potential hypernyms
through pattern-based web searching. Suppose X is
a capitalized word sequence. Some pattern queries
like “such as X” are threw into the search engine.
Then, in the retrieved documents, the nouns that im-
mediately precede the pattern are recognized as the
hypernyms of X. This work is most related to ours.
However, the patterns used in his work are too strict
to cover many low-frequency entities, and our ex-
periments show the weakness of the method.
Snow et al. (2005) for the first time propose to au-
tomatically extract large numbers of lexico-syntactic
patterns and then detect hypernym relations from
a large newswire corpus. First, they use some
known hypernym-hyponym pairs from WordNet as
seeds and collect many patterns from a syntactical-
ly parsed corpus in a bootstrapping way. Then, they
consider all noun pairs in the same sentence as po-
tential hypernym-hyponym pairs and use a statistical
classifier to recognize the correct ones. All patterns
corresponding to the noun pairs in the corpus are
fed into the classifier as features. Their method re-
lies on accurate syntactic parsers and it is difficult to
guarantee the quality of the automatically extracted
patterns. Our experiments show that their method is
inferior to ours.
</bodyText>
<page confidence="0.981803">
1225
</page>
<bodyText confidence="0.99992609375">
Encyclopedia-based methods extract hyper-
nym relations from encyclopedias like Wikipedia
(Suchanek et al., 2008; Hoffart et al., 2012). The
user-labeled information in encyclopedias, such as
category tags in Wikipedia, is often used to derive
hypernym relations.
In the construction of the famous ontology YA-
GO, Suchanek et al. (2008) consider the title of each
Wikipedia page as an entity and the corresponding
category tags as its potential hypernyms. They ap-
ply a shallow semantic parser and some rules to dis-
tinguish the correct hypernyms. Heuristically, they
find that if the head of the category tag is a plural
word, the tag is most likely to be a correct hyper-
nym. However, this method cannot be used in Chi-
nese because of the lack of plurality information.
The method of Suchanek et al. (2008) cannot han-
dle the case when the entity is absent in Wikipedia.
To solve this problem, Lin et al. (2012) connect the
absent entities with the entities present in Wikipedia
sharing common contexts. They utilize the Freebase
semantic types to label the present entities and then
propagate the types to the absent entities. The Free-
base contains most of entities in Wikipedia and as-
signs them semantic types defined in advance. But
there are no such resources in Chinese.
Compared with previous work, our approach tries
to identify hypernyms from multiple sources. The
evidences from different sources can authenticate
and complement each other to improve both preci-
sion and recall. Our experimental results show the
effectiveness of our method.
</bodyText>
<sectionHeader confidence="0.971313" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999949">
Our method is composed of two steps. First, we col-
lect candidate hypernyms from multiple sources for
a given entity. Then, a statistical model is built for
hypernym ranking based on a set of effective fea-
tures. Besides, we also present a heuristic strategy
to build a large-scale training data.
</bodyText>
<subsectionHeader confidence="0.999531">
3.1 Candidate Hypernym Collection from
Multiple Sources
</subsectionHeader>
<bodyText confidence="0.999760625">
In this work, we collect potential hypernyms from
four sources, i.e., search engine results, two ency-
clopedias, and morphology of the entity name.
We count the co-occurrence frequency between
the target entities and other words in the returned
snippets and titles, and select top N nouns (or noun
phrases) as the main candidates. As the experiments
show, this method can find at least one hypernym
for 86.91% entities when N equals 10 (see Section
5.1). This roughly explains why people often can in-
fer semantic meaning of unknown entities after go-
ing through several search results.
Furthermore, the user-generated encyclopedia
category tags are important clues if the entity exist-
s in a encyclopedia. Thus we add these tags into
the candidates. In this work, we consider two Chi-
nese encyclopedias, Baidubaike and Hudongbaike1,
as hypernym sources.
In addition, the head words of entities are also
their hypernyms sometimes. For example, the head
word of “ (Emperor Penguin)” indicates
that it’s a kind of “�&amp; (penguins)”. Thus we put
head words into the hypernym candidates. In Chi-
nese, head words are often laid after their modifiers.
Therefore, we try to segment a given entity. If it can
be segmented and the last word is a noun, we take
the last word as the head word. In our data set, the
head words of 41.35% entities are real hypernyms
(see Section 5.1).
We combine all of these hypernym candidates to-
gether as the input of the second stage. The final
coverage rate reaches 93.24%.
</bodyText>
<subsectionHeader confidence="0.999748">
3.2 Hypernym Ranking
</subsectionHeader>
<bodyText confidence="0.999841923076923">
After getting the candidate hypernyms, we then
adopt a ranking model to determine the correct hy-
pernym. In this section, we propose several effective
features for the model. The model needs training da-
ta for learning how to rank the data in addition to
parameter setting. Considering that manually anno-
tating a large-scale hypernym dataset is costly and
time-consuming, we present a heuristic strategy to
collect training data. We compare three hypernym
ranking models on this data set, including Support
Vector Machine (SVM) with a linear kernel, SVM
with a radial basis function (RBF) kernel and Logis-
tic Regression (LR).
</bodyText>
<footnote confidence="0.842564">
1Baidubaike (http://baike.baidu.com) and
Hudongbaike (http://www.baike.com) are two largest
Chinese encyclopedias containing more than 6.26 million and
7.87 million entries respectively, while Chinese Wikipedia
contains about 0.72 million entries until September, 2013.
</footnote>
<page confidence="0.932939">
1226
</page>
<table confidence="0.889048">
Feature Comment Value Range
Prior In Titles [0, 1]
Is Tag Radicals 0 or 1
</table>
<bodyText confidence="0.978932538461538">
Is Head the prior probability of a candidate being a potential hypernym 0 or 1
Synonyms whether a candidate is a category tag in the encyclopedia [0, 1]
Source Num page of the entity if it exists [0, 1]
whether a candidate is the head word of the entity 1, 2, 3, or 4
some binary features based on the frequency of occurrence of
a candidate in the document titles in the search results
the ratio of the synonyms of the candidate in the candidate
list of the entity
the ratio of the radicals of characters in a candidate matched
with the last character of the entity
the number of sources where the candidate is extracted
0 or 1
Lexicon the hypernym candidate itself and its head word 0 or 1
</bodyText>
<tableCaption confidence="0.82508475">
Table 1: The features for ranking
3.2.1 Features for Ranking
The features for hypernym ranking are shown in
Table 1. We illustrate them in detail in the following.
</tableCaption>
<bodyText confidence="0.993505214285714">
Hypernym Prior: Intuitively, different words
have different probabilities as hypernyms of some
other words. Some are more probable as hypernyms,
such as animal, plant and fruit. Some other words
such as sun, nature and alias, are not usually used
as hypernyms. Thus we use a prior probability to
express this phenomenon. The assumption is that if
the more frequent that a noun appears as category
tags, the more likely it is a hypernym. We extract
category tags from 2.4 million pages in Baidubaike,
and compute the prior probabilities prior(w) for a
word w being a potential hypernym using Equation
1. countCT(w) denotes the times a word appeared
as a category tag in the encyclopedia pages.
</bodyText>
<equation confidence="0.96650275">
(1)
countCT(w)
prior(w) =
Ew, countCTW)
</equation>
<bodyText confidence="0.999759181818182">
In Titles: When we enter a query into a search
engine, the engine returns a search result list, which
contains document titles and their snippet text. The
distributions of hypernyms and non-hypernyms in ti-
tles are compared with that in snippets respectively
in our training data. We discover that the average
frequency of occurrence of hypernyms in titles is
15.60 while this number of non-hypernyms is only
5.18, while the difference in snippets is very small
(Table 2). Thus the frequency of candidates in titles
can be used as features. In this work the frequency
</bodyText>
<table confidence="0.9975">
Avg. Frequency in
titles snippets
Hypernym 15.60 33.69
Non-Hypernym 5.18 30.61
</table>
<tableCaption confidence="0.974148">
Table 2: Distributions of candidate hypernyms in titles
and snippets
</tableCaption>
<bodyText confidence="0.9999095">
is divided into three cases: greater than 15.60, less
than 5.18, and between 5.18 and 15.60. Three binary
features are used to represent these cases.
Synonyms: If there exist synonyms of a candi-
date hypernym in the candidate list, the candidate is
probably correct answer. For example, when “gas
(medicine)” and “9_% (medicine)” both appear in
the candidate list of an entity, the entity is probably
a kind of medicine. We get synonyms of a candidate
from a Chinese semantic thesaurus – Tongyi Cilin
(Extended) (CilinE for short)2 and compute the s-
core as a feature using Equation 2.
</bodyText>
<equation confidence="0.868382">
ratiosyn(h, le) = counlen(l() , le) (2)
</equation>
<bodyText confidence="0.928375">
Given a hypernym candidate h of an entity e and
the list of all candidates le, we compute the ratio of
the synonyms of h in le. countsyn(h, le) denotes the
count of the synonyms of h in le. len(le) is the total
count of candidates.
</bodyText>
<footnote confidence="0.923943333333333">
2CilinE contains synonym and hypernym relations among
77 thousand words, which is manually organized as a hierarchy
of five levels.
</footnote>
<page confidence="0.988406">
1227
</page>
<bodyText confidence="0.99923075">
Radicals: Chinese characters are a form of
ideogram. By far, the bulk of Chinese characters
were created by linking together a character with a
related meaning and another character to indicate its
pronunciation. The character with a related meaning
is called radical. Sometimes, it is a important clue to
indicate the semantic class of the whole character.
For example, the radical “I” means insects, so it
hints “�* (dragonfly)” is a kind of insects. Simi-
larly “r” hints “*EfA (lymphoma)” is a kind of
diseases. Thus we use radicals as a feature the value
of which is computed by using Equation 3.
</bodyText>
<equation confidence="0.994829">
countRM(e, h)
radical(e, h) = (3)
len(h)
</equation>
<bodyText confidence="0.9993298">
Here radical(e, h) denotes the ratio of characters
radical-matched with the last character of the entity
e in the hypernym h. countRM(e, h) denotes the
count of the radical-matched characters in h. len(h)
denotes the total count of the characters in h.
</bodyText>
<subsectionHeader confidence="0.883226">
3.2.2 Training Data Collection
</subsectionHeader>
<bodyText confidence="0.999975555555556">
Now training data is imperative to learn the
weights of the features in Section 3.2.1. Hence, we
propose a heuristic strategy to collect training data
from encyclopedias.
Firstly, we extract a number of open-domain enti-
ties from encyclopedias randomly. Then their hyper-
nym candidates are collected by using the method
proposed in Section 3.1. We select positive training
instances following two principles:
</bodyText>
<listItem confidence="0.986099571428572">
• Principle 1: Among the four sources used for
candidate collection, the more sources from
which the hypernym candidate is extracted, the
more likely it is a correct one.
• Principle 2: The higher the prior of the candi-
date being a hypernym is, the more likely it is a
correct one.
</listItem>
<bodyText confidence="0.980132625">
We select the best candidates following Principle
1 and then select the best one in them as a positive
instance following Principle 2. And we select a can-
didate as a negative training instance when it is from
only one source and its prior is the lowest. If there
are synonyms of training instances in the candidates
list, the synonyms are also extended into the training
set.
</bodyText>
<table confidence="0.999946222222222">
Domain Dev. # of entities
Test
Biology 72 351
Health Care 61 291
Food 75 303
Movie 51 204
Industry 56 224
Others 35 136
Total 350 1529
</table>
<tableCaption confidence="0.999905">
Table 3: The evaluation data
</tableCaption>
<bodyText confidence="0.997194666666667">
In this way, we collect training data automatically,
which are used to learn the feature weights of the
ranking models.
</bodyText>
<sectionHeader confidence="0.999264" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999941166666667">
In this work, we use Baidu3 search engine, the most
popular search engine for Chinese, and get the top
100 search results for each entity. The Chinese seg-
mentation, POS tagging and dependency parsing is
provided by an open-source Chinese language pro-
cessing platform LTP4 (Che et al., 2010).
</bodyText>
<subsectionHeader confidence="0.990512">
4.1 Experimental Data
</subsectionHeader>
<bodyText confidence="0.999981789473684">
In our experiments, we prepare open-domain enti-
ties from dictionaries in wide domains, which are
published by a Chinese input method editor soft-
ware Sogou Pinyin5. The domains include biology,
health care, food, movie, industry, and so on. We
sample 1,879 entities from these domain dictionaries
and randomly split them into 1/5 for developmen-
t and 4/5 for test (Table 3). We find that only 865
(46.04%) entities exist in Baidubaike or Hudong-
baike. Then we extract candidate hypernyms for the
entities and ask two annotators to judge each hyper-
nym relation pair true or false manually. A pair (E,
H) is annotated as true if the annotators judge “E is a
(or a kind of) H” is true. Finally, we get 12.53 candi-
date hypernyms for each entity on average in which
about 2.09 hypernyms are correct. 4,330 hypernym
relation pairs are judged by both the annotators. We
measure the agreement of the judges using the Kap-
pa coefficient (Siegel and Castellan Jr, 1988). The
</bodyText>
<footnote confidence="0.999922333333333">
3http://www.baidu.com
4http://ir.hit.edu.cn/demo/ltp/
5http://pinyin.sogou.com/dict/
</footnote>
<page confidence="0.978232">
1228
</page>
<figure confidence="0.994183">
Coverage Rate
Top N
</figure>
<figureCaption confidence="0.99688">
Figure 1: Effect of candidate hypernym coverage rate
while varying N
</figureCaption>
<bodyText confidence="0.9425602">
Kappa value is 0.79.
Our training data, containing 11,481 positive in-
stances and 18,378 negative ones, is extracted from
Baidubaike and Hudongbaike using the heuristic s-
trategy proposed in Section 3.2.2.
</bodyText>
<subsectionHeader confidence="0.931008">
4.2 Experimental Metrics
</subsectionHeader>
<bodyText confidence="0.999697764705882">
The evaluation metrics for our task include:
Coverage Rate: We evaluate coverage rate of the
candidate hypernyms. Coverage rate is the number
of entities for which at least one correct hypernym is
found divided by the total number of all entities.
Precision@1: Our method returns a ranked list
of hypernyms for each entity. We evaluate precision
of top-1 hypernyms (the most probable ones) in the
ranked lists, which is the number of correct top-1
hypernyms divided by the number of all entities.
R-precision: It is equivalent to Precision@R
where R is the total number of candidates labeled
as true hypernyms of an entity.
Precision, Recall, and F-score: Besides, we can
convert our ranking models to classification models
by setting thresholds. Varying the thresholds, we can
get different precisions, recalls, and F-scores.
</bodyText>
<sectionHeader confidence="0.999835" genericHeader="evaluation">
5 Results and Analysis
</sectionHeader>
<subsectionHeader confidence="0.998004">
5.1 The Coverage of Candidate Hypernyms
</subsectionHeader>
<bodyText confidence="0.9967928">
In this section, we evaluate the coverage rate of the
candidate hypernyms. We check the candidate hy-
pernyms of the whole 1,879 entities in the develop-
ment and test sets and see how many entities we can
collect at least one correct hypernym for.
</bodyText>
<table confidence="0.997819444444444">
Source Coverage Avg. #
Rate
SR10 0.8691 9.44*
ET 0.3938 3.07
HW 0.4135 0.871
SR10 + ET 0.8909 12.02
SR10 + HW 0.9117 9.75
ET + HW 0.7073 3.92
SR10 + ET + HW 0.9324 12.53
</table>
<tableCaption confidence="0.9608705">
Table 4: Coverage evaluation of the candidate hypernym
extraction
</tableCaption>
<bodyText confidence="0.959314628571429">
There are four different sources to collect candi-
dates as described in Section 3.1, which can be di-
vided into three kinds: search results (SR for short),
encyclopedia tags (ET) and head words (HW). For
SR, we select top N frequent nouns (SRN) in the
search results of an entity as its hypernym candi-
dates. The effect of coverage rate while varying N
is shown in Figure 1. As we can see from the fig-
ure, the coverage rate is improved significantly by
increasing N until N reaches 10. After that, the
improvement becomes slight. When the candidates
from all sources are merged, the coverage rate is fur-
ther improved.
Thus we set N as 10 in the remaining experi-
ments. The detail evaluation is shown in Table 4.
We can see that top 10 frequent nouns in the search
results contain at least one correct hypernym for
86.91% entities in our data set. This coincides with
the intuition that people usually can infer the seman-
tic classes of unknown entities by searching them in
web search engines.
The coverage rate of ET merely reaches 39.38%.
We find the reason is that more than half of the enti-
ties have no encyclopedia pages. The average num-
ber of candidate hypernyms from ET is 3.07. Note
that the number is calculated among all the enti-
ties. We also calculate the average number only for
the present entities in encyclopedias. The number
reaches 6.68. The reason is that for many present en-
tities, the category tags include not only hypernyms
*For some of entities are rare, there may be less than 10
nouns in the search results. So the average count of candidates
is less than 10.
†Not all of the entities can be segmented. We cannot get the
head words of the ones that cannot be segmented.
</bodyText>
<figure confidence="0.96394225">
0 5 10 15 20
0.2 0.4 0.6 0.8 1.0
SRN
SRN+ET+HW
</figure>
<page confidence="0.948241">
1229
</page>
<table confidence="0.9995245">
Method Present Entities Absent Entities All Entities R-Prec
P@1 R-Prec P@1 R-Prec P@1
MPattern 0.5542 0.4937 0.4306 0.3638 0.5229 0.4608
MSnow 0.3199 0.2592 0.2827 0.2610 0.3092 0.2597
MPrior 0.7339 0.5483 0.3940 0.3531 0.5494 0.4423
MSV M−linear 0.8569 0.6899 0.6157 0.5837 0.7260 0.6322
MSV M−rbf 0.8484 0.6940 0.6241 0.5901 0.7266 0.6376
MLR 0.8612 0.7052 0.6807 0.6258 0.7632 0.6621
</table>
<tableCaption confidence="0.9977085">
Table 5: Precision@1 and R-Precision results on the test set. Here the present entities mean the entities existing in the
encyclopedias. The absent entities mean the ones not existing in the encyclopedias.
</tableCaption>
<bodyText confidence="0.998374136363637">
but also related words. For example, “�4A0531+
,b (Bradley Center)” in Baidubaike have 5 tags, i.e.,
“NBA”, “#-ft (sports)”, “#-ftL� (sports)”, “„
f* (basketball)”, and “*M (arena)”. Among them,
only “*M (arena)” is a proper hypernym whereas
the others are some related words indicating mere-
ly thematic vicinity. Comparing the results of SR10
and SR10 + ET, we can see that collecting candidates
from ET can improve coverage, although many in-
correct candidates are added in at the same time.
The HW source provides 0.87 candidates on av-
erage with 41.35% coverage rate. That is to say, for
these entities, people can infer the semantic classes
when they see the surface lexicon.
At last, we combine the candidates from all of the
three sources as the input of the ranking methods.
The coverage rate reaches 93.24%.
We also compare with the manually construct-
ed semantic thesaurus CilinE mentioned in Section
3.2.1. Only 29 entities exist in CilinE (coverage rate
is only 1.54%). That is why we try to automatically
extract hypernym relations.
</bodyText>
<subsectionHeader confidence="0.979016">
5.2 Evaluation of the Ranking
5.2.1 Overall Performance Comparison
</subsectionHeader>
<bodyText confidence="0.9934419">
In this section, we compare our proposed methods
with other methods. Table 5 lists the performance
measured by precision at rank 1 and R-precision of
some key methods. The precision-recall curves of
all the methods are shown in Figure 2. Table 7 lists
the maximum F-scores.
MPattern refers to the pattern-based method of
Hearst (1992). We craft Chinese Hearst-style
patterns (Table 6), in which E represents an entity
and H represents one of its hypernyms. Following
</bodyText>
<table confidence="0.997737666666667">
Pattern Translation
E H E is a (a kind of) H
E (.)V H E(,) and other H
H (,)P4(fiW) E H(,) called E
H (,)(*)�P E H(,) such as E
H (,)���fk E H(,) especially E
</table>
<tableCaption confidence="0.997673">
Table 6: Chinese Hearst-style lexical patterns
</tableCaption>
<bodyText confidence="0.999961576923077">
Evans (2004), we combine each pattern and each en-
tity and submit them into the Baidu search engine.
For example, for an entity E, we search “E A
-Al` (E is a)”, “E V (E and other)”, and so on. We
select top 100 search results of each query and get
1,285,209 results in all for the entities in the test set.
Then we use the patterns to extract hypernyms from
the search results. The result shows that 508 cor-
rect hypernyms are extracted for 568 entities (1,529
entities in total). Only a small part of the entities
can be extracted hypernyms for. This is mainly be-
cause only a few hypernym relations are expressed
in these fixed patterns in the web, and many ones are
expressed in more flexible manners. The hypernyms
are ranked based on the count of evidences where
the hypernyms are extracted.
MSnow is the method originally proposed by S-
now et al. (2005) for English but we adapt it for Chi-
nese. We consider the top 100 search results for each
known hypernym-hyponym pairs as a corpus to ex-
tract lexico-syntactic patterns. Then, an LR classi-
fier is built based on this patterns to recognize hy-
pernym relations. This method considers all noun-
s co-occurred with the focused entity in the same
sentences as candidate hypernyms. So the number
of candidates is huge, which causes inefficiency. In
</bodyText>
<page confidence="0.957685">
1230
</page>
<figure confidence="0.961733333333333">
Precision−Recall Curves on the Test Set
0.0 0.2 0.4 0.6 0.8 1.0
Recall
</figure>
<figureCaption confidence="0.999742">
Figure 2: Precision-Recall curves on the test set
</figureCaption>
<bodyText confidence="0.999951448275862">
our corpus, there are 652,181 candidates for 1,529
entities (426.54 for each entity on average), most of
which are not hypernyms. One possible reason is
that this method relies on an accurate syntactic pars-
er and it is difficult to guarantee the quality of the
automatically extracted patterns. Even worse, the
low quality of the language in the search results may
make this problem more serious.
MPrior refers to the ranking method based on on-
ly the prior of a candidate being a hypernym. As
Table 5 shows, it outperforms MSnow and achieves
comparable results with MPattern on Precision@1
and R-Precision.
Based on the features proposed in Section 3.2.1,
we train several statistical models based on SVM
and LR on the training data. MSV M−linear and
MSV M−rbf refer to the SVM models based on linear
kernels and RBF kernels respectively. MLR refers
to the LR model. The probabilities6 output by the
models are used to rank the candidate hypernyms.
All of the parameters which need to be set in the
models are selected on the development set. Table
5 shows the best models based on each algorithm.
These supervised models outperform the previous
methods. MLR achieves the best performance.
The precision-recall plot of the methods on the
test set is presented in Figure 2. MHeuristic refers
to the heuristic approach, proposed in Section 3.2.2,
to collect training data. Because this method cannot
</bodyText>
<footnote confidence="0.9932265">
6The output of an SVM is the distance from the decision
hyper-plane. Sigmoid functions can be used to convert this un-
calibrated distance into a calibrated posterior probability (Platt,
1999).
</footnote>
<table confidence="0.993251125">
Method Max. F-score
MPattern 0.2061
MSnow 0.1514
MHeuristic 0.2803
MPrior 0.5591
MSV M−linear 0.5868
MSV M−rbf 0.6014
MLR 0.5998
</table>
<tableCaption confidence="0.990876">
Table 7: Summary of maximum F-score on the test set
</tableCaption>
<table confidence="0.999955416666667">
Feature P@1 R-Prec Max.
F-score
All 0.7632 0.6621 0.5998
− Prior 0.7534 0.6546 0.5837
− Is Tag 0.6965 0.6039 0.5605
− Is Head 0.7018 0.6036 0.5694
− In Titles 0.7436 0.6513 0.5868
− Synonyms 0.7495 0.6493 0.5831
− Radicals 0.7593 0.6584 0.5890
− Source Num 0.7364 0.6556 0.5984
− Lexicon 0.7377 0.6422 0.5851
− Source Info 0.6128 0.5221 0.5459
</table>
<tableCaption confidence="0.9985265">
Table 8: Performance of LR models with different fea-
tures on the test set
</tableCaption>
<bodyText confidence="0.99993175">
provide ranking information, it is not listed in Ta-
ble 5. For fair comparison of R-precision and recall,
we add the extra correct hypernyms from MPattern
and MSnow to the test data set. The models based
on SVM and LR still perform better than the other
methods. MPattern and MSnow suffer from low re-
call and precision. MHeuristic get a high precision
but a low recall, because it can only deal with a part
of entities appearing in encyclopedias. The preci-
sion of MHeuristic reflects the quality of our training
data. We summarize the maximum F-score of dif-
ferent methods in Table 7.
</bodyText>
<subsectionHeader confidence="0.527802">
5.2.2 Feature Effect
</subsectionHeader>
<bodyText confidence="0.999614714285714">
Table 8 shows the impact of each feature on the
performance of LR models. When we remove any
one of the features, the performance is degraded
more or less. The most effective features are Is Tag
and Is Head. The last line in Table 8 shows the
performance when we remove all features about
the source information, i.e., Is Tag, Is Head, and
</bodyText>
<table confidence="0.98522635">
Precision
0.0 0.2 0.4 0.6 0.8 1.0
● Ms-
● ● ● ●●●● ● ● ●●● ●● ● ● ● ● ● Ma.�..
● ● ● MSVM−�mea.
MsVM−.at
Mi,
Maette..
MHeu.�st�c
●
●
1231
Entity Top-1 Entity Top-1
Hypernym Hypernym
��RJR-��M(cefoperazone sodium) 9aa(drug) Xr% (bullet tuna) X (fish)
40-f-*(finger citron rolls) /J&apos;%¯(snack) VK&apos;ATf(zirconite) RTf(ore)
#1 $ (The Avengers) &gt;K(movie) �3f4A�K(Felixstowe) AF1(port)
�9A�(mastigium) ÄO(datum) AVN(coxal cavity) %(plant)
Z0jR 414� t% -AR 0V
(Ethanolamine phosphotransferase) (organism) (coma) (knowledge)
</table>
<tableCaption confidence="0.990589">
Table 10: Examples of entity-hypernym pairs extracted by MLR
</tableCaption>
<table confidence="0.999936375">
Domain P@1 R-Prec Max.
F-score
Biology 0.8165 0.7203 0.6424
Health Care 0.7354 0.5962 0.6061
Food 0.7450 0.6634 0.6938
Movie 0.9310 0.8069 0.7031
Industry 0.6286 0.5841 0.4624
Others 0.6324 0.4936 0.4318
</table>
<tableCaption confidence="0.999723">
Table 9: Performance of MLR in various domains
</tableCaption>
<bodyText confidence="0.950195">
Source Num. The performance is degraded sharply.
This indicates the importance of the source informa-
tion for hypernym ranking.
</bodyText>
<subsubsectionHeader confidence="0.613939">
5.2.3 The Performance in Each Domain
</subsubsectionHeader>
<bodyText confidence="0.999894363636364">
In this section, we evaluate the performance of
MLR method in various domains. We can see from
Table 9 that the performance in movie domain is best
while the performance in industry domain is worst.
That is because the information about movies is
abundant on the web. Furthermore, most of movies
have encyclopedia pages. It is easy to get the hy-
pernyms. In contrast, the entities in industry domain
are more uncommon. On the whole, our method is
robust for different domains. In Table 10, some in-
stances in various domains are presented.
</bodyText>
<subsectionHeader confidence="0.893606">
5.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999799">
The uncovered entities7 and the false positives8 are
analyzed after the experiments. Some error exam-
ples are shown in Table 10 (in red font).
</bodyText>
<footnote confidence="0.90926625">
7Uncovered entities are entities which we do not collect any
correct hypernyms for in the first step.
8False positives are hypernyms ranked at the first places, but
actually are not correct hypernyms.
</footnote>
<bodyText confidence="0.996449416666667">
Uncovered entities: About 34% of the errors are
caused by uncovered entities. It is found that many
of the uncovered entities are rare entities. Nearly
36% of them are very rare and have only less than
100 search results in all. When we can’t get enough
information of an unknown entity from the search
engine, it’s difficult to know its semantic meaning,
such as “*AIR (mastigium)”, “IVN (coxal cav-
ity)”, “AR (coma)”. The identification of their hy-
pernyms requires more human-crafted knowledge.
The ranking models we used are unable to select
them, as the true synonyms are often below rank 10.
False positives: The remained 66% errors are
false positives. They are mainly owing to the
fact that some other related words in the candi-
date lists are more likely hypernyms. For exam-
ple, “t% (organism)” is wrongly recognized as
the most probable hypernym of “Z0JR ff
4 (Ethanolamine phosphotransferase)”, because
the entity often co-occurs with word “t% (organ-
ism)” and the latter is often used as a hypernym of
some other entities. The correct hypernyms actu-
ally are “ (enzyme)”, “��%� (chemical sub-
stance)”, and so on.
</bodyText>
<sectionHeader confidence="0.999004" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999751">
This paper proposes a novel method for finding
hypernyms of Chinese open-domain entities from
multiple sources. We collect candidate hypernyms
with wide coverage from search results, encyclope-
dia category tags and the head word of the entity.
Then, we propose a set of features to build statisti-
cal models to rank the candidate hypernyms on the
training data collected automatically. In our exper-
iments, we show that our method outperforms the
state-of-the-art methods and achieves the best preci-
</bodyText>
<page confidence="0.980199">
1232
</page>
<bodyText confidence="0.999994166666667">
sion of 76.32% on a manually labeled test dataset.
All of the features which we propose are effective,
especially the features of source information. More-
over, our method works well in various domains, e-
specially in the movie and biology domains. We al-
so conduct detailed analysis to give more insights
on the error distribution. Except some language de-
pendent features, our approach can be easily trans-
fered from Chinese to other languages. For future
work, we would like to explore knowledge from
more sources to enhance our model, such as seman-
tic thesauri and infoboxes in encyclopedias.
</bodyText>
<sectionHeader confidence="0.998179" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995366111111111">
This work was supported by National Natu-
ral Science Foundation of China (NSFC) via
grant 61133012, 61073126 and the National 863
Leading Technology Research Project via grant
2012AA011102. Special thanks to Zhenghua Li,
Wanxiang Che, Wei Song, Yanyan Zhao, Yuhang
Guo and the anonymous reviewers for insightful
comments and suggestions. Thanks are also due to
our annotators Ni Han and Zhenghua Li.
</bodyText>
<sectionHeader confidence="0.998604" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999523623376623">
Sharon A. Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In Pro-
ceedings of the 37th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 120–126,
College Park, Maryland, USA, June.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
A chinese language technology platform. In Coling
2010: Demonstrations, pages 13–16, Beijing, China,
August.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense tagging of unknown nouns in wordnet. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, pages 168–
175.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: An exper-
imental study. Artificial Intelligence, 165(1):91–134.
Oren Etzioni, Michele Banko, and Michael J Cafarella.
2006. Machine reading. In AAAI, volume 6, pages
1517–1519.
Richard Evans. 2004. A framework for named enti-
ty recognition in the open domain. Recent Advances
in Natural Language Processing III: Selected Papers
from RANLP 2003, 260:267–274.
Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th conference on Computational linguistics-Volume
2, pages 539–545.
Johannes Hoffart, Fabian M Suchanek, Klaus Berberich,
and Gerhard Weikum. 2012. Yago2: a spatially and
temporally enhanced knowledge base from wikipedia.
Artificial Intelligence, pages 1–63.
Thomas Lin, Mausam, and Oren Etzioni. 2012. No noun
phrase left behind: Detecting and typing unlinkable
entities. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 893–903, Jeju Island, Korea, July.
Paul McNamee, Rion Snow, Patrick Schone, and James
Mayfield. 2008. Learning named entity hyponyms
for question answering. In Proceedings of the Third
International Joint Conference on Natural Language
Processing, pages 799–804.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of the thir-
teenth ACM international conference on Information
and knowledge management, pages 137–145.
John Platt. 1999. Probabilistic outputs for support vec-
tor machines and comparisons to regularized likeli-
hood methods. Advances in large margin classifiers,
10(3):61–74.
Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009.
What is this, anyway: Automatic hypernym discovery.
In Proceedings of the 2009 AAAI Spring Symposium
on Learning by Reading and Learning to Read, pages
88–93.
Cederberg Scott and Widdows Dominic. 2003. Using lsa
and noun coordination information to improve the pre-
cision and recall of automatic hyponymy extraction. In
Proceedings of the seventh conference on Natural lan-
guage learning at HLT-NAACL 2003-Volume 4, pages
111–118.
Sidney Siegel and N John Castellan Jr. 1988. Nonpara-
metric statistics for the behavioral sciences. McGraw-
Hill, New York.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and L´eon
Bottou, editors, Advances in Neural Information Pro-
cessing Systems 17, pages 1297–1304.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
wikipedia and wordnet. Web Semantics: Science, Ser-
vices and Agents on the World Wide Web, 6(3):203–
217.
</reference>
<page confidence="0.531812">
1233
</page>
<reference confidence="0.99939125">
Peter Turney, Michael L Littman, Jeffrey Bigham, and
Victor Shnayder. 2003. Combining independent mod-
ules to solve multiple-choice synonym and analogy
problems. In Proceedings of the International Con-
ference RANLP-2003, pages 482–489.
Fan Zhang, Shuming Shi, Jing Liu, Shuqi Sun, and Chin-
Yew Lin. 2011. Nonlinear evidence fusion and prop-
agation for hyponymy relation mining. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1159–1168, Portland, Oregon, USA,
June.
</reference>
<page confidence="0.993818">
1234
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.888272">
<title confidence="0.999581">Exploiting Multiple Sources for Open-domain Hypernym Discovery</title>
<author confidence="0.985004">Bing Qin Fu</author>
<author confidence="0.985004">Ting</author>
<affiliation confidence="0.994341">Research Center for Social Computing and Information School of Computer Science and Harbin Institute of Technology,</affiliation>
<email confidence="0.920155">bqin,</email>
<abstract confidence="0.999597666666666">Hypernym discovery aims to extract such noun pairs that one noun is a hypernym of the other. Most previous methods are based on lexical patterns but perform badly on opendomain data. Other work extracts hypernym relations from encyclopedias but has limited coverage. This paper proposes a simple yet effective distant supervision framework for Chinese open-domain hypernym discovery. Given an entity name, we try to discover its hypernyms by leveraging knowledge from multiple sources, i.e., search engine results, encyclopedias, and morphology of the entity name. First, we extract candidate hypernyms from the above sources. Then, we apply a statistical ranking model to select correct hypernyms. A set of novel features is proposed for the ranking model. We also present a heuristic strategy to build a large-scale noisy training data for the model without human annotation. Experimental results demonstrate that our approach outperforms the state-of-the-art methods on a manually labeled test dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sharon A Caraballo</author>
</authors>
<title>Automatic construction of a hypernym-labeled noun hierarchy from text.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>120--126</pages>
<location>College Park, Maryland, USA,</location>
<contexts>
<context position="6400" citStr="Caraballo, 1999" startWordPosition="1025" endWordPosition="1026">ds for hypernym discovery can be summarized into two major categories, i.e., patternbased methods and encyclopedia-based methods. Pattern-based methods make use of manually or automatically constructed patterns to mine hypernym relations from text corpora. The pioneer work by Hearst (1992) finds that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations. For example, NP1 is a hypernym of NP2 in the lexical pattern “such NP1 as NP2”. Similarly, succeeding researchers follow her work and use handcrafted patterns to extract hypernym pairs from corpora (Caraballo, 1999; Scott and Dominic, 2003; Ciaramita and Johnson, 2003; Turney et al., 2003; Pasca, 2004; Etzioni et al., 2005; Ritter et al., 2009; Zhang et al., 2011). Evans (2004) considers the web data as a large corpus and uses search engines to identify hypernyms based on lexical patterns. Given an arbitrary document, he takes each capitalized word sequence as an entity and aims to find its potential hypernyms through pattern-based web searching. Suppose X is a capitalized word sequence. Some pattern queries like “such as X” are threw into the search engine. Then, in the retrieved documents, the nouns t</context>
</contexts>
<marker>Caraballo, 1999</marker>
<rawString>Sharon A. Caraballo. 1999. Automatic construction of a hypernym-labeled noun hierarchy from text. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 120–126, College Park, Maryland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Zhenghua Li</author>
<author>Ting Liu</author>
</authors>
<title>Ltp: A chinese language technology platform.</title>
<date>2010</date>
<booktitle>In Coling 2010: Demonstrations,</booktitle>
<pages>13--16</pages>
<location>Beijing, China,</location>
<contexts>
<context position="18317" citStr="Che et al., 2010" startWordPosition="3018" endWordPosition="3021">nto the training set. Domain Dev. # of entities Test Biology 72 351 Health Care 61 291 Food 75 303 Movie 51 204 Industry 56 224 Others 35 136 Total 350 1529 Table 3: The evaluation data In this way, we collect training data automatically, which are used to learn the feature weights of the ranking models. 4 Experimental Setup In this work, we use Baidu3 search engine, the most popular search engine for Chinese, and get the top 100 search results for each entity. The Chinese segmentation, POS tagging and dependency parsing is provided by an open-source Chinese language processing platform LTP4 (Che et al., 2010). 4.1 Experimental Data In our experiments, we prepare open-domain entities from dictionaries in wide domains, which are published by a Chinese input method editor software Sogou Pinyin5. The domains include biology, health care, food, movie, industry, and so on. We sample 1,879 entities from these domain dictionaries and randomly split them into 1/5 for development and 4/5 for test (Table 3). We find that only 865 (46.04%) entities exist in Baidubaike or Hudongbaike. Then we extract candidate hypernyms for the entities and ask two annotators to judge each hypernym relation pair true or false </context>
</contexts>
<marker>Che, Li, Liu, 2010</marker>
<rawString>Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp: A chinese language technology platform. In Coling 2010: Demonstrations, pages 13–16, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Mark Johnson</author>
</authors>
<title>Supersense tagging of unknown nouns in wordnet.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 conference on Empirical methods in natural language processing,</booktitle>
<pages>168--175</pages>
<contexts>
<context position="6454" citStr="Ciaramita and Johnson, 2003" startWordPosition="1031" endWordPosition="1034">d into two major categories, i.e., patternbased methods and encyclopedia-based methods. Pattern-based methods make use of manually or automatically constructed patterns to mine hypernym relations from text corpora. The pioneer work by Hearst (1992) finds that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations. For example, NP1 is a hypernym of NP2 in the lexical pattern “such NP1 as NP2”. Similarly, succeeding researchers follow her work and use handcrafted patterns to extract hypernym pairs from corpora (Caraballo, 1999; Scott and Dominic, 2003; Ciaramita and Johnson, 2003; Turney et al., 2003; Pasca, 2004; Etzioni et al., 2005; Ritter et al., 2009; Zhang et al., 2011). Evans (2004) considers the web data as a large corpus and uses search engines to identify hypernyms based on lexical patterns. Given an arbitrary document, he takes each capitalized word sequence as an entity and aims to find its potential hypernyms through pattern-based web searching. Suppose X is a capitalized word sequence. Some pattern queries like “such as X” are threw into the search engine. Then, in the retrieved documents, the nouns that immediately precede the pattern are recognized as </context>
</contexts>
<marker>Ciaramita, Johnson, 2003</marker>
<rawString>Massimiliano Ciaramita and Mark Johnson. 2003. Supersense tagging of unknown nouns in wordnet. In Proceedings of the 2003 conference on Empirical methods in natural language processing, pages 168– 175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: An experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="6510" citStr="Etzioni et al., 2005" startWordPosition="1042" endWordPosition="1045">yclopedia-based methods. Pattern-based methods make use of manually or automatically constructed patterns to mine hypernym relations from text corpora. The pioneer work by Hearst (1992) finds that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations. For example, NP1 is a hypernym of NP2 in the lexical pattern “such NP1 as NP2”. Similarly, succeeding researchers follow her work and use handcrafted patterns to extract hypernym pairs from corpora (Caraballo, 1999; Scott and Dominic, 2003; Ciaramita and Johnson, 2003; Turney et al., 2003; Pasca, 2004; Etzioni et al., 2005; Ritter et al., 2009; Zhang et al., 2011). Evans (2004) considers the web data as a large corpus and uses search engines to identify hypernyms based on lexical patterns. Given an arbitrary document, he takes each capitalized word sequence as an entity and aims to find its potential hypernyms through pattern-based web searching. Suppose X is a capitalized word sequence. Some pattern queries like “such as X” are threw into the search engine. Then, in the retrieved documents, the nouns that immediately precede the pattern are recognized as the hypernyms of X. This work is most related to ours. H</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: An experimental study. Artificial Intelligence, 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
</authors>
<date>2006</date>
<booktitle>Machine reading. In AAAI,</booktitle>
<volume>6</volume>
<pages>1517--1519</pages>
<contexts>
<context position="1888" citStr="Etzioni et al., 2006" startWordPosition="299" endWordPosition="303">ed test dataset. 1 Introduction Hypernym discovery is a task to extract such noun pairs that one noun is a hypernym of the other (Snow et al., 2005). A noun H is a hypernym of another noun E if E is an instance or subclass of H. In other word, H is a semantic class of E. For instance, “actor” is a hypernym of “Mel Gibson”; “dog” is a hypernym of “Caucasian sheepdog”; “medicine” is a hypernym of “Aspirin”. Hypernym discovery is an important subtask of semantic relation extraction *Email correspondence. and has many applications in ontology construction (Suchanek et al., 2008), machine reading (Etzioni et al., 2006), question answering (McNamee et al., 2008), and so on. Some manually constructed thesauri such as WordNet can also provide some semantic relations such as hypernyms. However, these thesauri are limited in its scope and domain, and manual construction is knowledge-intensive and time-consuming. Therefore, many researchers try to automatically extract semantic relations or to construct taxonomies. Most previous methods on automatic hypernym discovery are based on lexical patterns and suffer from the problem that such patterns can only cover a small part of complex linguistic circumstances (Hears</context>
</contexts>
<marker>Etzioni, Banko, Cafarella, 2006</marker>
<rawString>Oren Etzioni, Michele Banko, and Michael J Cafarella. 2006. Machine reading. In AAAI, volume 6, pages 1517–1519.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Evans</author>
</authors>
<title>A framework for named entity recognition in the open domain.</title>
<date>2004</date>
<booktitle>Recent Advances in Natural Language Processing III: Selected Papers from RANLP 2003,</booktitle>
<pages>260--267</pages>
<contexts>
<context position="6566" citStr="Evans (2004)" startWordPosition="1054" endWordPosition="1055">lly or automatically constructed patterns to mine hypernym relations from text corpora. The pioneer work by Hearst (1992) finds that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations. For example, NP1 is a hypernym of NP2 in the lexical pattern “such NP1 as NP2”. Similarly, succeeding researchers follow her work and use handcrafted patterns to extract hypernym pairs from corpora (Caraballo, 1999; Scott and Dominic, 2003; Ciaramita and Johnson, 2003; Turney et al., 2003; Pasca, 2004; Etzioni et al., 2005; Ritter et al., 2009; Zhang et al., 2011). Evans (2004) considers the web data as a large corpus and uses search engines to identify hypernyms based on lexical patterns. Given an arbitrary document, he takes each capitalized word sequence as an entity and aims to find its potential hypernyms through pattern-based web searching. Suppose X is a capitalized word sequence. Some pattern queries like “such as X” are threw into the search engine. Then, in the retrieved documents, the nouns that immediately precede the pattern are recognized as the hypernyms of X. This work is most related to ours. However, the patterns used in his work are too strict to </context>
<context position="25210" citStr="Evans (2004)" startWordPosition="4182" endWordPosition="4183">e 5 lists the performance measured by precision at rank 1 and R-precision of some key methods. The precision-recall curves of all the methods are shown in Figure 2. Table 7 lists the maximum F-scores. MPattern refers to the pattern-based method of Hearst (1992). We craft Chinese Hearst-style patterns (Table 6), in which E represents an entity and H represents one of its hypernyms. Following Pattern Translation E H E is a (a kind of) H E (.)V H E(,) and other H H (,)P4(fiW) E H(,) called E H (,)(*)�P E H(,) such as E H (,)���fk E H(,) especially E Table 6: Chinese Hearst-style lexical patterns Evans (2004), we combine each pattern and each entity and submit them into the Baidu search engine. For example, for an entity E, we search “E A -Al` (E is a)”, “E V (E and other)”, and so on. We select top 100 search results of each query and get 1,285,209 results in all for the entities in the test set. Then we use the patterns to extract hypernyms from the search results. The result shows that 508 correct hypernyms are extracted for 568 entities (1,529 entities in total). Only a small part of the entities can be extracted hypernyms for. This is mainly because only a few hypernym relations are expressed</context>
</contexts>
<marker>Evans, 2004</marker>
<rawString>Richard Evans. 2004. A framework for named entity recognition in the open domain. Recent Advances in Natural Language Processing III: Selected Papers from RANLP 2003, 260:267–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th conference on Computational linguistics-Volume 2,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="2495" citStr="Hearst, 1992" startWordPosition="393" endWordPosition="394">2006), question answering (McNamee et al., 2008), and so on. Some manually constructed thesauri such as WordNet can also provide some semantic relations such as hypernyms. However, these thesauri are limited in its scope and domain, and manual construction is knowledge-intensive and time-consuming. Therefore, many researchers try to automatically extract semantic relations or to construct taxonomies. Most previous methods on automatic hypernym discovery are based on lexical patterns and suffer from the problem that such patterns can only cover a small part of complex linguistic circumstances (Hearst, 1992; Turney et al., 2003; Zhang et al., 2011). Other work tries to extract hypernym relations from large-scale encyclopedias like Wikipedia and achieves high precision (Suchanek et al., 2008; Hoffart et al., 2012). However, the coverage is limited since there exist many infrequent and new entities that are missing in encyclopedias (Lin et al., 2012). We made similar observation that more than a half of entities in our data set have no entries in the encyclopedias. This paper proposes a simple yet effective distant supervision framework for Chinese open-domain hypernym discovery. Given an entity n</context>
<context position="6075" citStr="Hearst (1992)" startWordPosition="972" endWordPosition="973">need to be changed. The remainder of the paper is organized as follows: Section 2 discusses the related work. Section 3 introduces our method in detail. Section 4 describes the experimental setup. Section 5 shows the experimental results. Conclusion and future work are presented in Section 6. 2 Related Work Previous methods for hypernym discovery can be summarized into two major categories, i.e., patternbased methods and encyclopedia-based methods. Pattern-based methods make use of manually or automatically constructed patterns to mine hypernym relations from text corpora. The pioneer work by Hearst (1992) finds that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations. For example, NP1 is a hypernym of NP2 in the lexical pattern “such NP1 as NP2”. Similarly, succeeding researchers follow her work and use handcrafted patterns to extract hypernym pairs from corpora (Caraballo, 1999; Scott and Dominic, 2003; Ciaramita and Johnson, 2003; Turney et al., 2003; Pasca, 2004; Etzioni et al., 2005; Ritter et al., 2009; Zhang et al., 2011). Evans (2004) considers the web data as a large corpus and uses search engines to identify hypernyms based on lexical patt</context>
<context position="24859" citStr="Hearst (1992)" startWordPosition="4116" endWordPosition="4117">with the manually constructed semantic thesaurus CilinE mentioned in Section 3.2.1. Only 29 entities exist in CilinE (coverage rate is only 1.54%). That is why we try to automatically extract hypernym relations. 5.2 Evaluation of the Ranking 5.2.1 Overall Performance Comparison In this section, we compare our proposed methods with other methods. Table 5 lists the performance measured by precision at rank 1 and R-precision of some key methods. The precision-recall curves of all the methods are shown in Figure 2. Table 7 lists the maximum F-scores. MPattern refers to the pattern-based method of Hearst (1992). We craft Chinese Hearst-style patterns (Table 6), in which E represents an entity and H represents one of its hypernyms. Following Pattern Translation E H E is a (a kind of) H E (.)V H E(,) and other H H (,)P4(fiW) E H(,) called E H (,)(*)�P E H(,) such as E H (,)���fk E H(,) especially E Table 6: Chinese Hearst-style lexical patterns Evans (2004), we combine each pattern and each entity and submit them into the Baidu search engine. For example, for an entity E, we search “E A -Al` (E is a)”, “E V (E and other)”, and so on. We select top 100 search results of each query and get 1,285,209 res</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th conference on Computational linguistics-Volume 2, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Fabian M Suchanek</author>
<author>Klaus Berberich</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago2: a spatially and temporally enhanced knowledge base from wikipedia.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<pages>1--63</pages>
<contexts>
<context position="2705" citStr="Hoffart et al., 2012" startWordPosition="424" endWordPosition="427">imited in its scope and domain, and manual construction is knowledge-intensive and time-consuming. Therefore, many researchers try to automatically extract semantic relations or to construct taxonomies. Most previous methods on automatic hypernym discovery are based on lexical patterns and suffer from the problem that such patterns can only cover a small part of complex linguistic circumstances (Hearst, 1992; Turney et al., 2003; Zhang et al., 2011). Other work tries to extract hypernym relations from large-scale encyclopedias like Wikipedia and achieves high precision (Suchanek et al., 2008; Hoffart et al., 2012). However, the coverage is limited since there exist many infrequent and new entities that are missing in encyclopedias (Lin et al., 2012). We made similar observation that more than a half of entities in our data set have no entries in the encyclopedias. This paper proposes a simple yet effective distant supervision framework for Chinese open-domain hypernym discovery. Given an entity name, our goal is to discover its hypernyms by leveraging knowledge from multiple sources. Considering the case where a person wants to know the meaning of an unknown entity, he/she may search it in a search eng</context>
<context position="8180" citStr="Hoffart et al., 2012" startWordPosition="1313" endWordPosition="1316">corpus in a bootstrapping way. Then, they consider all noun pairs in the same sentence as potential hypernym-hyponym pairs and use a statistical classifier to recognize the correct ones. All patterns corresponding to the noun pairs in the corpus are fed into the classifier as features. Their method relies on accurate syntactic parsers and it is difficult to guarantee the quality of the automatically extracted patterns. Our experiments show that their method is inferior to ours. 1225 Encyclopedia-based methods extract hypernym relations from encyclopedias like Wikipedia (Suchanek et al., 2008; Hoffart et al., 2012). The user-labeled information in encyclopedias, such as category tags in Wikipedia, is often used to derive hypernym relations. In the construction of the famous ontology YAGO, Suchanek et al. (2008) consider the title of each Wikipedia page as an entity and the corresponding category tags as its potential hypernyms. They apply a shallow semantic parser and some rules to distinguish the correct hypernyms. Heuristically, they find that if the head of the category tag is a plural word, the tag is most likely to be a correct hypernym. However, this method cannot be used in Chinese because of the</context>
</contexts>
<marker>Hoffart, Suchanek, Berberich, Weikum, 2012</marker>
<rawString>Johannes Hoffart, Fabian M Suchanek, Klaus Berberich, and Gerhard Weikum. 2012. Yago2: a spatially and temporally enhanced knowledge base from wikipedia. Artificial Intelligence, pages 1–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lin</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>No noun phrase left behind: Detecting and typing unlinkable entities.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>893--903</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="2843" citStr="Lin et al., 2012" startWordPosition="447" endWordPosition="450">cally extract semantic relations or to construct taxonomies. Most previous methods on automatic hypernym discovery are based on lexical patterns and suffer from the problem that such patterns can only cover a small part of complex linguistic circumstances (Hearst, 1992; Turney et al., 2003; Zhang et al., 2011). Other work tries to extract hypernym relations from large-scale encyclopedias like Wikipedia and achieves high precision (Suchanek et al., 2008; Hoffart et al., 2012). However, the coverage is limited since there exist many infrequent and new entities that are missing in encyclopedias (Lin et al., 2012). We made similar observation that more than a half of entities in our data set have no entries in the encyclopedias. This paper proposes a simple yet effective distant supervision framework for Chinese open-domain hypernym discovery. Given an entity name, our goal is to discover its hypernyms by leveraging knowledge from multiple sources. Considering the case where a person wants to know the meaning of an unknown entity, he/she may search it in a search engine and then finds out the answer after going through the search results. Furthermore, if he/she finds an entry about the entity in an aut</context>
<context position="8952" citStr="Lin et al. (2012)" startWordPosition="1448" endWordPosition="1451"> famous ontology YAGO, Suchanek et al. (2008) consider the title of each Wikipedia page as an entity and the corresponding category tags as its potential hypernyms. They apply a shallow semantic parser and some rules to distinguish the correct hypernyms. Heuristically, they find that if the head of the category tag is a plural word, the tag is most likely to be a correct hypernym. However, this method cannot be used in Chinese because of the lack of plurality information. The method of Suchanek et al. (2008) cannot handle the case when the entity is absent in Wikipedia. To solve this problem, Lin et al. (2012) connect the absent entities with the entities present in Wikipedia sharing common contexts. They utilize the Freebase semantic types to label the present entities and then propagate the types to the absent entities. The Freebase contains most of entities in Wikipedia and assigns them semantic types defined in advance. But there are no such resources in Chinese. Compared with previous work, our approach tries to identify hypernyms from multiple sources. The evidences from different sources can authenticate and complement each other to improve both precision and recall. Our experimental results</context>
</contexts>
<marker>Lin, Mausam, Etzioni, 2012</marker>
<rawString>Thomas Lin, Mausam, and Oren Etzioni. 2012. No noun phrase left behind: Detecting and typing unlinkable entities. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 893–903, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>Rion Snow</author>
<author>Patrick Schone</author>
<author>James Mayfield</author>
</authors>
<title>Learning named entity hyponyms for question answering.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing,</booktitle>
<pages>799--804</pages>
<contexts>
<context position="1931" citStr="McNamee et al., 2008" startWordPosition="306" endWordPosition="309">scovery is a task to extract such noun pairs that one noun is a hypernym of the other (Snow et al., 2005). A noun H is a hypernym of another noun E if E is an instance or subclass of H. In other word, H is a semantic class of E. For instance, “actor” is a hypernym of “Mel Gibson”; “dog” is a hypernym of “Caucasian sheepdog”; “medicine” is a hypernym of “Aspirin”. Hypernym discovery is an important subtask of semantic relation extraction *Email correspondence. and has many applications in ontology construction (Suchanek et al., 2008), machine reading (Etzioni et al., 2006), question answering (McNamee et al., 2008), and so on. Some manually constructed thesauri such as WordNet can also provide some semantic relations such as hypernyms. However, these thesauri are limited in its scope and domain, and manual construction is knowledge-intensive and time-consuming. Therefore, many researchers try to automatically extract semantic relations or to construct taxonomies. Most previous methods on automatic hypernym discovery are based on lexical patterns and suffer from the problem that such patterns can only cover a small part of complex linguistic circumstances (Hearst, 1992; Turney et al., 2003; Zhang et al.,</context>
</contexts>
<marker>McNamee, Snow, Schone, Mayfield, 2008</marker>
<rawString>Paul McNamee, Rion Snow, Patrick Schone, and James Mayfield. 2008. Learning named entity hyponyms for question answering. In Proceedings of the Third International Joint Conference on Natural Language Processing, pages 799–804.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
</authors>
<title>Acquisition of categorized named entities for web search.</title>
<date>2004</date>
<booktitle>In Proceedings of the thirteenth ACM international conference on Information and knowledge management,</booktitle>
<pages>137--145</pages>
<contexts>
<context position="6488" citStr="Pasca, 2004" startWordPosition="1040" endWordPosition="1041">thods and encyclopedia-based methods. Pattern-based methods make use of manually or automatically constructed patterns to mine hypernym relations from text corpora. The pioneer work by Hearst (1992) finds that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations. For example, NP1 is a hypernym of NP2 in the lexical pattern “such NP1 as NP2”. Similarly, succeeding researchers follow her work and use handcrafted patterns to extract hypernym pairs from corpora (Caraballo, 1999; Scott and Dominic, 2003; Ciaramita and Johnson, 2003; Turney et al., 2003; Pasca, 2004; Etzioni et al., 2005; Ritter et al., 2009; Zhang et al., 2011). Evans (2004) considers the web data as a large corpus and uses search engines to identify hypernyms based on lexical patterns. Given an arbitrary document, he takes each capitalized word sequence as an entity and aims to find its potential hypernyms through pattern-based web searching. Suppose X is a capitalized word sequence. Some pattern queries like “such as X” are threw into the search engine. Then, in the retrieved documents, the nouns that immediately precede the pattern are recognized as the hypernyms of X. This work is m</context>
</contexts>
<marker>Pasca, 2004</marker>
<rawString>Marius Pasca. 2004. Acquisition of categorized named entities for web search. In Proceedings of the thirteenth ACM international conference on Information and knowledge management, pages 137–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.</title>
<date>1999</date>
<booktitle>Advances in large margin classifiers,</booktitle>
<pages>10--3</pages>
<contexts>
<context position="28206" citStr="Platt, 1999" startWordPosition="4697" endWordPosition="4698"> need to be set in the models are selected on the development set. Table 5 shows the best models based on each algorithm. These supervised models outperform the previous methods. MLR achieves the best performance. The precision-recall plot of the methods on the test set is presented in Figure 2. MHeuristic refers to the heuristic approach, proposed in Section 3.2.2, to collect training data. Because this method cannot 6The output of an SVM is the distance from the decision hyper-plane. Sigmoid functions can be used to convert this uncalibrated distance into a calibrated posterior probability (Platt, 1999). Method Max. F-score MPattern 0.2061 MSnow 0.1514 MHeuristic 0.2803 MPrior 0.5591 MSV M−linear 0.5868 MSV M−rbf 0.6014 MLR 0.5998 Table 7: Summary of maximum F-score on the test set Feature P@1 R-Prec Max. F-score All 0.7632 0.6621 0.5998 − Prior 0.7534 0.6546 0.5837 − Is Tag 0.6965 0.6039 0.5605 − Is Head 0.7018 0.6036 0.5694 − In Titles 0.7436 0.6513 0.5868 − Synonyms 0.7495 0.6493 0.5831 − Radicals 0.7593 0.6584 0.5890 − Source Num 0.7364 0.6556 0.5984 − Lexicon 0.7377 0.6422 0.5851 − Source Info 0.6128 0.5221 0.5459 Table 8: Performance of LR models with different features on the test set</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John Platt. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers, 10(3):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>What is this, anyway: Automatic hypernym discovery.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 AAAI Spring Symposium on Learning by Reading and Learning to Read,</booktitle>
<pages>88--93</pages>
<contexts>
<context position="6531" citStr="Ritter et al., 2009" startWordPosition="1046" endWordPosition="1049">s. Pattern-based methods make use of manually or automatically constructed patterns to mine hypernym relations from text corpora. The pioneer work by Hearst (1992) finds that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations. For example, NP1 is a hypernym of NP2 in the lexical pattern “such NP1 as NP2”. Similarly, succeeding researchers follow her work and use handcrafted patterns to extract hypernym pairs from corpora (Caraballo, 1999; Scott and Dominic, 2003; Ciaramita and Johnson, 2003; Turney et al., 2003; Pasca, 2004; Etzioni et al., 2005; Ritter et al., 2009; Zhang et al., 2011). Evans (2004) considers the web data as a large corpus and uses search engines to identify hypernyms based on lexical patterns. Given an arbitrary document, he takes each capitalized word sequence as an entity and aims to find its potential hypernyms through pattern-based web searching. Suppose X is a capitalized word sequence. Some pattern queries like “such as X” are threw into the search engine. Then, in the retrieved documents, the nouns that immediately precede the pattern are recognized as the hypernyms of X. This work is most related to ours. However, the patterns </context>
</contexts>
<marker>Ritter, Soderland, Etzioni, 2009</marker>
<rawString>Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009. What is this, anyway: Automatic hypernym discovery. In Proceedings of the 2009 AAAI Spring Symposium on Learning by Reading and Learning to Read, pages 88–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cederberg Scott</author>
<author>Widdows Dominic</author>
</authors>
<title>Using lsa and noun coordination information to improve the precision and recall of automatic hyponymy extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the seventh conference on Natural language learning at HLT-NAACL</booktitle>
<volume>2003</volume>
<pages>111--118</pages>
<contexts>
<context position="6425" citStr="Scott and Dominic, 2003" startWordPosition="1027" endWordPosition="1030">iscovery can be summarized into two major categories, i.e., patternbased methods and encyclopedia-based methods. Pattern-based methods make use of manually or automatically constructed patterns to mine hypernym relations from text corpora. The pioneer work by Hearst (1992) finds that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations. For example, NP1 is a hypernym of NP2 in the lexical pattern “such NP1 as NP2”. Similarly, succeeding researchers follow her work and use handcrafted patterns to extract hypernym pairs from corpora (Caraballo, 1999; Scott and Dominic, 2003; Ciaramita and Johnson, 2003; Turney et al., 2003; Pasca, 2004; Etzioni et al., 2005; Ritter et al., 2009; Zhang et al., 2011). Evans (2004) considers the web data as a large corpus and uses search engines to identify hypernyms based on lexical patterns. Given an arbitrary document, he takes each capitalized word sequence as an entity and aims to find its potential hypernyms through pattern-based web searching. Suppose X is a capitalized word sequence. Some pattern queries like “such as X” are threw into the search engine. Then, in the retrieved documents, the nouns that immediately precede t</context>
</contexts>
<marker>Scott, Dominic, 2003</marker>
<rawString>Cederberg Scott and Widdows Dominic. 2003. Using lsa and noun coordination information to improve the precision and recall of automatic hyponymy extraction. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, pages 111–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Siegel</author>
<author>N John Castellan Jr</author>
</authors>
<title>Nonparametric statistics for the behavioral sciences.</title>
<date>1988</date>
<publisher>McGrawHill,</publisher>
<location>New York.</location>
<marker>Siegel, Jr, 1988</marker>
<rawString>Sidney Siegel and N John Castellan Jr. 1988. Nonparametric statistics for the behavioral sciences. McGrawHill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery. In</title>
<date>2005</date>
<booktitle>Advances in Neural Information Processing Systems 17,</booktitle>
<pages>1297--1304</pages>
<editor>Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors,</editor>
<contexts>
<context position="1415" citStr="Snow et al., 2005" startWordPosition="217" endWordPosition="221">clopedias, and morphology of the entity name. First, we extract candidate hypernyms from the above sources. Then, we apply a statistical ranking model to select correct hypernyms. A set of novel features is proposed for the ranking model. We also present a heuristic strategy to build a large-scale noisy training data for the model without human annotation. Experimental results demonstrate that our approach outperforms the state-of-the-art methods on a manually labeled test dataset. 1 Introduction Hypernym discovery is a task to extract such noun pairs that one noun is a hypernym of the other (Snow et al., 2005). A noun H is a hypernym of another noun E if E is an instance or subclass of H. In other word, H is a semantic class of E. For instance, “actor” is a hypernym of “Mel Gibson”; “dog” is a hypernym of “Caucasian sheepdog”; “medicine” is a hypernym of “Aspirin”. Hypernym discovery is an important subtask of semantic relation extraction *Email correspondence. and has many applications in ontology construction (Suchanek et al., 2008), machine reading (Etzioni et al., 2006), question answering (McNamee et al., 2008), and so on. Some manually constructed thesauri such as WordNet can also provide som</context>
<context position="7272" citStr="Snow et al. (2005)" startWordPosition="1172" endWordPosition="1175">based on lexical patterns. Given an arbitrary document, he takes each capitalized word sequence as an entity and aims to find its potential hypernyms through pattern-based web searching. Suppose X is a capitalized word sequence. Some pattern queries like “such as X” are threw into the search engine. Then, in the retrieved documents, the nouns that immediately precede the pattern are recognized as the hypernyms of X. This work is most related to ours. However, the patterns used in his work are too strict to cover many low-frequency entities, and our experiments show the weakness of the method. Snow et al. (2005) for the first time propose to automatically extract large numbers of lexico-syntactic patterns and then detect hypernym relations from a large newswire corpus. First, they use some known hypernym-hyponym pairs from WordNet as seeds and collect many patterns from a syntactically parsed corpus in a bootstrapping way. Then, they consider all noun pairs in the same sentence as potential hypernym-hyponym pairs and use a statistical classifier to recognize the correct ones. All patterns corresponding to the noun pairs in the corpus are fed into the classifier as features. Their method relies on acc</context>
<context position="26054" citStr="Snow et al. (2005)" startWordPosition="4335" endWordPosition="4339">y and get 1,285,209 results in all for the entities in the test set. Then we use the patterns to extract hypernyms from the search results. The result shows that 508 correct hypernyms are extracted for 568 entities (1,529 entities in total). Only a small part of the entities can be extracted hypernyms for. This is mainly because only a few hypernym relations are expressed in these fixed patterns in the web, and many ones are expressed in more flexible manners. The hypernyms are ranked based on the count of evidences where the hypernyms are extracted. MSnow is the method originally proposed by Snow et al. (2005) for English but we adapt it for Chinese. We consider the top 100 search results for each known hypernym-hyponym pairs as a corpus to extract lexico-syntactic patterns. Then, an LR classifier is built based on this patterns to recognize hypernym relations. This method considers all nouns co-occurred with the focused entity in the same sentences as candidate hypernyms. So the number of candidates is huge, which causes inefficiency. In 1230 Precision−Recall Curves on the Test Set 0.0 0.2 0.4 0.6 0.8 1.0 Recall Figure 2: Precision-Recall curves on the test set our corpus, there are 652,181 candid</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. In Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors, Advances in Neural Information Processing Systems 17, pages 1297–1304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: A large ontology from wikipedia and wordnet. Web Semantics: Science, Services and Agents on the World Wide Web,</title>
<date>2008</date>
<volume>6</volume>
<issue>3</issue>
<pages>217</pages>
<contexts>
<context position="1848" citStr="Suchanek et al., 2008" startWordPosition="293" endWordPosition="296">te-of-the-art methods on a manually labeled test dataset. 1 Introduction Hypernym discovery is a task to extract such noun pairs that one noun is a hypernym of the other (Snow et al., 2005). A noun H is a hypernym of another noun E if E is an instance or subclass of H. In other word, H is a semantic class of E. For instance, “actor” is a hypernym of “Mel Gibson”; “dog” is a hypernym of “Caucasian sheepdog”; “medicine” is a hypernym of “Aspirin”. Hypernym discovery is an important subtask of semantic relation extraction *Email correspondence. and has many applications in ontology construction (Suchanek et al., 2008), machine reading (Etzioni et al., 2006), question answering (McNamee et al., 2008), and so on. Some manually constructed thesauri such as WordNet can also provide some semantic relations such as hypernyms. However, these thesauri are limited in its scope and domain, and manual construction is knowledge-intensive and time-consuming. Therefore, many researchers try to automatically extract semantic relations or to construct taxonomies. Most previous methods on automatic hypernym discovery are based on lexical patterns and suffer from the problem that such patterns can only cover a small part of</context>
<context position="8157" citStr="Suchanek et al., 2008" startWordPosition="1309" endWordPosition="1312">a syntactically parsed corpus in a bootstrapping way. Then, they consider all noun pairs in the same sentence as potential hypernym-hyponym pairs and use a statistical classifier to recognize the correct ones. All patterns corresponding to the noun pairs in the corpus are fed into the classifier as features. Their method relies on accurate syntactic parsers and it is difficult to guarantee the quality of the automatically extracted patterns. Our experiments show that their method is inferior to ours. 1225 Encyclopedia-based methods extract hypernym relations from encyclopedias like Wikipedia (Suchanek et al., 2008; Hoffart et al., 2012). The user-labeled information in encyclopedias, such as category tags in Wikipedia, is often used to derive hypernym relations. In the construction of the famous ontology YAGO, Suchanek et al. (2008) consider the title of each Wikipedia page as an entity and the corresponding category tags as its potential hypernyms. They apply a shallow semantic parser and some rules to distinguish the correct hypernyms. Heuristically, they find that if the head of the category tag is a plural word, the tag is most likely to be a correct hypernym. However, this method cannot be used in</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2008</marker>
<rawString>Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. Yago: A large ontology from wikipedia and wordnet. Web Semantics: Science, Services and Agents on the World Wide Web, 6(3):203– 217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Michael L Littman</author>
<author>Jeffrey Bigham</author>
<author>Victor Shnayder</author>
</authors>
<title>Combining independent modules to solve multiple-choice synonym and analogy problems.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference RANLP-2003,</booktitle>
<pages>482--489</pages>
<contexts>
<context position="2516" citStr="Turney et al., 2003" startWordPosition="395" endWordPosition="398">n answering (McNamee et al., 2008), and so on. Some manually constructed thesauri such as WordNet can also provide some semantic relations such as hypernyms. However, these thesauri are limited in its scope and domain, and manual construction is knowledge-intensive and time-consuming. Therefore, many researchers try to automatically extract semantic relations or to construct taxonomies. Most previous methods on automatic hypernym discovery are based on lexical patterns and suffer from the problem that such patterns can only cover a small part of complex linguistic circumstances (Hearst, 1992; Turney et al., 2003; Zhang et al., 2011). Other work tries to extract hypernym relations from large-scale encyclopedias like Wikipedia and achieves high precision (Suchanek et al., 2008; Hoffart et al., 2012). However, the coverage is limited since there exist many infrequent and new entities that are missing in encyclopedias (Lin et al., 2012). We made similar observation that more than a half of entities in our data set have no entries in the encyclopedias. This paper proposes a simple yet effective distant supervision framework for Chinese open-domain hypernym discovery. Given an entity name, our goal is to d</context>
<context position="6475" citStr="Turney et al., 2003" startWordPosition="1035" endWordPosition="1039">i.e., patternbased methods and encyclopedia-based methods. Pattern-based methods make use of manually or automatically constructed patterns to mine hypernym relations from text corpora. The pioneer work by Hearst (1992) finds that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations. For example, NP1 is a hypernym of NP2 in the lexical pattern “such NP1 as NP2”. Similarly, succeeding researchers follow her work and use handcrafted patterns to extract hypernym pairs from corpora (Caraballo, 1999; Scott and Dominic, 2003; Ciaramita and Johnson, 2003; Turney et al., 2003; Pasca, 2004; Etzioni et al., 2005; Ritter et al., 2009; Zhang et al., 2011). Evans (2004) considers the web data as a large corpus and uses search engines to identify hypernyms based on lexical patterns. Given an arbitrary document, he takes each capitalized word sequence as an entity and aims to find its potential hypernyms through pattern-based web searching. Suppose X is a capitalized word sequence. Some pattern queries like “such as X” are threw into the search engine. Then, in the retrieved documents, the nouns that immediately precede the pattern are recognized as the hypernyms of X. T</context>
</contexts>
<marker>Turney, Littman, Bigham, Shnayder, 2003</marker>
<rawString>Peter Turney, Michael L Littman, Jeffrey Bigham, and Victor Shnayder. 2003. Combining independent modules to solve multiple-choice synonym and analogy problems. In Proceedings of the International Conference RANLP-2003, pages 482–489.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fan Zhang</author>
<author>Shuming Shi</author>
<author>Jing Liu</author>
<author>Shuqi Sun</author>
<author>ChinYew Lin</author>
</authors>
<title>Nonlinear evidence fusion and propagation for hyponymy relation mining.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1159--1168</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="2537" citStr="Zhang et al., 2011" startWordPosition="399" endWordPosition="402">et al., 2008), and so on. Some manually constructed thesauri such as WordNet can also provide some semantic relations such as hypernyms. However, these thesauri are limited in its scope and domain, and manual construction is knowledge-intensive and time-consuming. Therefore, many researchers try to automatically extract semantic relations or to construct taxonomies. Most previous methods on automatic hypernym discovery are based on lexical patterns and suffer from the problem that such patterns can only cover a small part of complex linguistic circumstances (Hearst, 1992; Turney et al., 2003; Zhang et al., 2011). Other work tries to extract hypernym relations from large-scale encyclopedias like Wikipedia and achieves high precision (Suchanek et al., 2008; Hoffart et al., 2012). However, the coverage is limited since there exist many infrequent and new entities that are missing in encyclopedias (Lin et al., 2012). We made similar observation that more than a half of entities in our data set have no entries in the encyclopedias. This paper proposes a simple yet effective distant supervision framework for Chinese open-domain hypernym discovery. Given an entity name, our goal is to discover its hypernyms</context>
<context position="6552" citStr="Zhang et al., 2011" startWordPosition="1050" endWordPosition="1053">ods make use of manually or automatically constructed patterns to mine hypernym relations from text corpora. The pioneer work by Hearst (1992) finds that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations. For example, NP1 is a hypernym of NP2 in the lexical pattern “such NP1 as NP2”. Similarly, succeeding researchers follow her work and use handcrafted patterns to extract hypernym pairs from corpora (Caraballo, 1999; Scott and Dominic, 2003; Ciaramita and Johnson, 2003; Turney et al., 2003; Pasca, 2004; Etzioni et al., 2005; Ritter et al., 2009; Zhang et al., 2011). Evans (2004) considers the web data as a large corpus and uses search engines to identify hypernyms based on lexical patterns. Given an arbitrary document, he takes each capitalized word sequence as an entity and aims to find its potential hypernyms through pattern-based web searching. Suppose X is a capitalized word sequence. Some pattern queries like “such as X” are threw into the search engine. Then, in the retrieved documents, the nouns that immediately precede the pattern are recognized as the hypernyms of X. This work is most related to ours. However, the patterns used in his work are </context>
</contexts>
<marker>Zhang, Shi, Liu, Sun, Lin, 2011</marker>
<rawString>Fan Zhang, Shuming Shi, Jing Liu, Shuqi Sun, and ChinYew Lin. 2011. Nonlinear evidence fusion and propagation for hyponymy relation mining. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1159–1168, Portland, Oregon, USA, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
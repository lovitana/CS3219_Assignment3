<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000022">
<title confidence="0.99188">
Modeling Scientific Impact with Topical Influence Regression
</title>
<author confidence="0.996708">
James Foulds Padhraic Smyth
</author>
<affiliation confidence="0.999421">
Department of Computer Science
University of California, Irvine
</affiliation>
<email confidence="0.997632">
{jfoulds, smyth}@ics.uci.edu
</email>
<sectionHeader confidence="0.994757" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999761">
When reviewing scientific literature, it would
be useful to have automatic tools that iden-
tify the most influential scientific articles as
well as how ideas propagate between articles.
In this context, this paper introduces topical
influence, a quantitative measure of the ex-
tent to which an article tends to spread its
topics to the articles that cite it. Given the
text of the articles and their citation graph, we
show how to learn a probabilistic model to re-
cover both the degree of topical influence of
each article and the influence relationships be-
tween articles. Experimental results on cor-
pora from two well-known computer science
conferences are used to illustrate and validate
the proposed approach.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999950266666667">
Scientific articles are not created equal. Some ar-
ticles generate entire disciplines or sub-disciplines
of research, or revolutionize how we think about
a problem, while others contribute relatively little.
When we are first introduced to a new area of scien-
tific study, it would be useful to automatically find
the most important articles, and the relationships of
influence between articles. Understanding the im-
pact of scientific work is also crucial for hiring deci-
sions, allocation of funding, university rankings and
other tasks that involve the assessment of scientific
merit. If scientific works stand on the shoulders of
giants, we would like to be able to find the giants.
The importance of a scientific work has previ-
ously been measured chiefly through metrics derived
from citation counts, such as impact factors. How-
ever, citation counts are not the whole story. Many
citations are made in passing, are relevant to only
one section of an article, or make no impact on a
work but are referenced out of “politeness, policy
or piety” (Ziman, 1968). In reality, scientific impact
has many dimensions. Some articles are important
because they describe scientific discoveries that alter
our understanding of the world, while some develop
essential tools and techniques which facilitate future
research. Other articles are influential because they
introduce the seeds of new ideas, which in turn in-
spire many other articles.
In this work we introduce topical influence, a
quantitative metric for measuring the latter type of
scientific influence, defined in the context of an un-
supervised generative model for scientific corpora.
The model posits that articles “coerce” the articles
that cite them into having similar topical content to
them. Thus, articles with higher topical influence
have a larger effect on the topics of the articles that
cite them. We model this influence mechanism via
a regression on the parameters of the Dirichlet prior
over topics in an LDA-style topic model. We show
how the models can be used to recover meaningful
influence scores, both for articles and for specific ci-
tations. By looking not just at the citation graph but
also taking into account the content of the articles,
topical influence can provide a better picture of sci-
entific impact than simple citation counts.
</bodyText>
<sectionHeader confidence="0.992632" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9998468">
Bibliometrics, the quantitative study of scientific lit-
erature, has a long history. One example of a widely-
used bibliometric measure of interest is the impact
factor of a publication venue for a given year, de-
fined to be the average number of times articles from
</bodyText>
<page confidence="0.989648">
113
</page>
<note confidence="0.736223">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 113–123,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9993564">
that venue, published in the previous two years, were
cited in that year. However, the quality of articles
in a given publication venue can vary wildly, and it
is difficult to compare impact factors between dif-
ferent disciplines of study. The number of cita-
tions an article receives is an indication of impor-
tance, but this is confounded by the unknown func-
tion of each citation. Measures of importance such
as PageRank (Brin and Page, 1998) can be derived
recursively from the citation graph. Such graph-
based measures do not in general make use of the
textual content of the articles, although it is possible
to apply them to graphs where the edges between ar-
ticles are determined based on the similarity of their
content instead of the citation graph (Lin, 2008).
A variety of methods have previously been pro-
posed for analyzing text and citation links together,
such as modeling connections between words and
citations Cohn and Hofmann (2001), classifying ci-
tation function (Teufel et al., 2006), and jointly
modeling citation links and document content
(Chang and Blei, 2009). However, these methods do
not directly measure article importance or influence
relationships between articles given their citations.
More closely related to the present work,
Dietz et al. (2007) proposed the citation influence
model (CIM). Building on the latent Dirichlet al-
location (LDA) framework, CIM assumes that each
word is drawn by first selecting either (a) the distri-
bution over topics of a cited article (with probability
proportional to the influence weight of that article
on the present article) or (b) a novel topic distribu-
tion, and drawing a topic from the selected distribu-
tion, then finally drawing the word from the chosen
topic.1 In their approach, every word is assigned an
extra latent variable, namely the cited article whose
topic distribution the topic was drawn from. For the
model proposed in this paper, we do not need to in-
troduce these additional latent variables, which leads
to a simpler latent representation and fewer variables
to sample during inference. Dietz et al. (2007) also
assume that the citation graph is bipartite, consist-
ing of one set of citing articles and one set of cited
articles—in contrast, our proposed models can han-
dle arbitrary citation graphs in the form of directed
</bodyText>
<footnote confidence="0.783591">
1A somewhat similar model was also proposed by
He et al. (2009)
</footnote>
<bodyText confidence="0.999854125">
acyclic graphs (DAGs). While both the CIM and our
approach can identify the influence of specific cita-
tions between articles, our model can also infer how
influential each article is overall, and provides a flex-
ible modeling framework which can handle different
assumptions about influence.
Another related method is due to
Shaparenko and Joachims (2009), who propose
a mixture modeling approach for the detection of
novel text content. Nallapati et al. (2011) intro-
duced TopicFlow, a PLSA-based model for the flow
of topics in a document network. In their model,
citing articles “vote” on each cited article’s topic
distribution in retrospect, via a network flow model.
Since this voting occurs in time-reversed order, it
does not describe an influence mechanism and is
not a generative model that can simulate or predict
new documents.
Finally, the document influence model of
Gerrish and Blei (2010) can be viewed as orthogo-
nal to this work, in that it models the impact of doc-
uments on topics over time (specifically, how topics
change over time) rather than how articles influence
the specific articles that cite them.
</bodyText>
<sectionHeader confidence="0.989172" genericHeader="method">
3 Topical Influence Regression
</sectionHeader>
<bodyText confidence="0.999148333333333">
Scientific research is seldom performed in a vacuum.
New research builds on the research that came be-
fore it. Although there are many aspects by which
the importance of a scientific article can be judged,
in this work we are interested in the extent to which a
given article has or will have subsequent articles that
build upon it or are otherwise inspired by its ideas.
We begin by defining topical influence, a quantita-
tive measure for this type of influence.
</bodyText>
<subsectionHeader confidence="0.998913">
3.1 Topical Influence
</subsectionHeader>
<bodyText confidence="0.9999822">
It is not immediately obvious how one might quan-
tify such a notion of “idea-based” influence. How-
ever, the mechanism used in the scientific commu-
nity for giving credit to prior work is citation. The
presence of a citation from article b to article a there-
fore indicates that article b may have been influenced
by the ideas in article a, to some unknown extent.
We hypothesize that the extent of this influence man-
ifests itself in the language of b. Using latent Dirich-
let allocation (LDA) topics as a concrete proxy for
</bodyText>
<page confidence="0.997955">
114
</page>
<bodyText confidence="0.989101852941176">
the vague notion of “ideas”, we define the topical
influence of a to be the extent to which article a
coerces the documents which cite it to have simi-
lar topic distributions to it. Topical influence will be
made precise in the context of a generative model for
scientific corpora, conditioned on the citation graph,
called topical influence regression (TIR).
The proposed model extends the LDA framework
of Blei et al. (2003). In LDA, each word w(d)
i of
each document d is assigned to one of K latent top-
ics, z(d)
i . Each topic q,(k) is a discrete distribution
over words. Document d has a distribution over top-
ics B(d), which can be viewed as a “location in topic
space” summarizing its thematic content. The B(d)’s
have a Dirichlet prior distribution with parameters
α = [α1, α2, ... , αK]⊺. Although the αk’s are often
set to be equal, representing a relatively uninforma-
tive prior over the B’s, a unique α(d) for each doc-
ument can also be used to encode prior information
such as the effect of other variables on the topics
of that document (Mimno and McCallum, 2008). In
our case, we want to model the influence that a docu-
ment has on the topic distributions of the documents
that cite it. A natural way to encode such influence,
then, is to allow documents to affect the value of α(d)
for each document d that cites them.
Accordingly, we model each article d as having
a latent, non-negative “topical influence” value l(d).
Let n(d) be number of words in article d, n(d)
k be the
number of words assigned to topic k, and let C(d) be
the set of articles that d cites. We model α(d) as
</bodyText>
<equation confidence="0.99756425">
Xα(d) = l(c)¯z(c) + α , (1)
cEC(d)
where 2(c) = 1
n(�) [n(c)
</equation>
<bodyText confidence="0.969793923076923">
1 , ... , n(c)K ]⊺ is the normalized
histogram of topic counts for document c, and α is
a constant for smoothing. Since the z(c)’s sum to
one, the topical influence l(c) of article c can be in-
terpreted as the number of words of precision that
it adds to the prior of the topic distributions of each
document that cites it. As we increase l(c), the arti-
cles that cite c become more likely to have similar
topic proportions to it. Thus, l(c) encodes the degree
to which article c influences the topics of each of the
articles that cite it.
From another perspective, marginalizing out B(d),
we can view the topic counts (in the standard LDA
</bodyText>
<note confidence="0.510666">
Articles that d cites
</note>
<figureCaption confidence="0.980593666666667">
Figure 1: The graphical model for the portion of the TIR
model connected to article a (the links from the z’s and
l’s to the α(d)’s are deterministic).
</figureCaption>
<bodyText confidence="0.999862928571429">
model) for document d as being drawn from a Polya
urn scheme with α(d)
k (possibly fractional) balls of
each color k E {1, ... , K} initially in the urn. For
each word, a ball is drawn randomly from the urn
and the topic assignment is determined according to
its color k. The ball is replaced in the urn, along
with a new ball of color k. In our model, for each
article c cited by article d we place l(c) balls, with
colors distributed according to 2(c), into article d’s
urn initially. Thus, article d’s topic assignments are
more likely to be similar to those of the more influ-
ential articles that it cites. The total number of balls
that d added to other articles’ urns,
</bodyText>
<equation confidence="0.998925">
T(d) g X l(d) = l(d) {b : d E C(b)} (2)
b:dEC(b)
</equation>
<bodyText confidence="0.9979655">
measures the total impact (in a topical sense) of the
article. We refer to this as total topical influence.
</bodyText>
<sectionHeader confidence="0.9875405" genericHeader="method">
3.2 Generative Model for Topical Influence
Regression
</sectionHeader>
<bodyText confidence="0.995598666666667">
The full assumed generative process for articles in
this model begins with a directed acyclic citation
graph G = {V, E}. Intuitively, citation graphs are
typically DAGs because articles can normally only
cite articles that precede them in time. We assume
that G is a DAG so that influence relationships are
</bodyText>
<equation confidence="0.721813333333333">
w(d)
i z(d)
i
8(d) α(d)
n(d)
l(d)
</equation>
<figure confidence="0.97280052631579">
Articles that a cites
O(k)
w(a)
i
z(a)
i
8(a) α(a)
K
n(d)
Article a
l(a)
A
0
wi(d) zi(d)
8(d) α(d)
n(d)
l(d)
Articles that cite a
Articles that cite d
</figure>
<page confidence="0.992953">
115
</page>
<bodyText confidence="0.989264272727273">
consistent with some temporal ordering of the arti-
cles, and so that the resulting model is a Bayesian
network. Here, each vertex vi corresponds to an ar-
ticle di, edge e = (v1, v2) E E IFF d1 is cited by d2,
and vertices (articles) are numbered in a topological
ordering with respect to G. Such an ordering ex-
ists because G is a DAG. We model each article d’s
word vector w(d) as being generated in topological
sequence, similarly to LDA but with its prior over
topic distribution being Dirichlet(α(d)), as given by
Equation 1. Note that each α(d) is a function of the
topics of the documents that it cites, parameterized
by their topical influence values. We therefore call
this model topical influence regression (TIR).
The TIR model provides us with topical influ-
ence scores for each article, but it does not tell us
about topical influence relationships between spe-
cific pairs of cited and citing articles. To model such
relationships, we can consider a hierarchical exten-
sion to TIR, with edge-wise topical influences l(c,d)
for each edge (c, d) of the citation graph, l(c,d) —
TruncGaussian(l(c), a, l(c,d) &gt; 0). In this case,
</bodyText>
<equation confidence="0.9938355">
�α(d) = l(c,d)z(c) + α . (3)
cEC(d)
</equation>
<bodyText confidence="0.99991">
This hierarchical setup allows us to continue to infer
article-level topical influences, and provides a mech-
anism for sharing statistical strength between influ-
ences associated with one cited article. We shall re-
fer to the model with influences on just the nodes (ar-
ticles) as TIR, and the hierarchical extension with in-
fluences on the edges as TIRE. The graphical model
for TIR is given in Figure 1, and the generative pro-
cess is detailed in the following pseudocode:
</bodyText>
<listItem confidence="0.99681325">
• For each topic k
• Sample the topic 4b(k) — Dirichlet(β)
• For each document d, in topological order
• Sample an influence weight,
l(d) — Exponential(A)
• If using the TIRE model
• For each cited document c E C(d)
• Draw edge influence weight,
</listItem>
<equation confidence="0.9416">
l(c,d) —
TruncGauss(l(c), a, l(c,d) &gt; 0)
</equation>
<listItem confidence="0.9965149">
• Assign a prior over topics via
α(d) = EcEC(d) l(c)¯z(c) + α (TIR), or
α(d) = EcEC(d) l(c,d)z(c) + α (TIRE)
• Sample a distribution over topics,
B(d) — Dirichlet(α(d))
• For each word i in document d
• Sample a topic
z�d) — Discrete
(B(d))
• Sample a word
</listItem>
<equation confidence="0.890863">
wi — Discrete(�(z(d)
(d) � ))
</equation>
<sectionHeader confidence="0.675622" genericHeader="method">
3.3 Relationship to Dirichlet-Multinomial
Regression
</sectionHeader>
<bodyText confidence="0.9998759375">
The TIR model can be viewed as an adaption of the
Dirichlet-multinomial regression (DMR) framework
of Mimno and McCallum (2008) to model topical
influence. DMR also endows each document with its
own unique α(d), but with α(d)
k = exp(x(d)⊺Ak) be-
ing a function of the observed feature vector x(d) pa-
rameterized by regression coefficients A. The DMR
model can also be applied to text corpora with ci-
tation information, by setting the feature vectors to
be binary indicators of the presence of a citation to
each article. TIR differs in that the functional form
of the regression is parameterized in a way that di-
rectly models influence, and also differs in that the
regression takes advantage of the content of the cited
articles via their topic assignments.
Because an article’s prior over topic distributions
depends on the topic assignments of the articles
that it cites, TIR induces a network of dependencies
between the topic assignments of the documents.
Specifically, if we collapse out O, the dependencies
between the z’s of each document form a Bayesian
network whose graph is the citation graph. In con-
trast, DMR treats the documents as conditionally in-
dependent given their citations, and does not exploit
their content in the regression.
To illustrate this, Figure 2 shows an example ci-
tation graph and the resulting Bayesian network. In
the figure, an edge in (a) from c to d corresponds
to a citation of c by d. Conditioned on the topics,
the dependence relationships between z nodes in (b)
follow the same structure as the citation graph.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.9505095">
We perform inference using a Markov chain
Monte Carlo technique. We use a col-
lapsed Gibbs sampling approach analogous to
Griffiths and Steyvers (2004), integrating out O and
</bodyText>
<page confidence="0.997206">
116
</page>
<figureCaption confidence="0.93187075">
Figure 2: (a) An example citation network. (b) Graphical
model for TIR on the example network, collapsing out
Θ but retaining topics Φ. Influence variables and hyper-
parameters not shown for simplicity.
</figureCaption>
<equation confidence="0.991093555555556">
Polya(z(d)|α(d)) =
Γ(Pk α(d)
k ) YΓ(n(d)
k + α(d)
k ) .
Γ(n(d) + Pk α(d)
k )Γ(α(d)
k )
k
</equation>
<bodyText confidence="0.996889875">
In the case of TIR, in the collapsed model the full
conditional posterior for the topical influence values
l is Pr(l|z, λ) ∝ Pr(z|l)Pr(l|λ). Here, Pr(z|l) =
QDd=1 Polya(z(d)|lC(d), zC(d)). The topical influence
values l can be sampled using Metropolis-Hastings
updates, or slice sampling. An alternative is to per-
form stochastic EM, optimizing the likelihood or
the posterior probability of l, interleaved within the
Gibbs sampler, as in Mimno and McCallum (2008)
and Wallach (2006). In experiments on synthetic
data we found that maximum likelihood updates on
l, obtained via gradient ascent, resulted in the lowest
L1 error from the true l, so we use this strategy for
the experimental results in this paper. The deriva-
tive of the log-likelihood with respect to the topical
influence l(a) of article a is
</bodyText>
<equation confidence="0.840634152173913">
l(c)¯z(c)
k + Kα)
4
2
6
5
1
3
z(4)
z(2)
w(4)
w(6)
z(3)
w(1)
w(3)
z(6)
w(2)
z(5)
z(1)
w5)
�
K
dPr(z|l) E� (T(E E
dl(a) d:a∈C(d) k c∈C(d)
Φ. The update equation for the topic assignments is E E l(c) ¯zk(c) + Kα + n(d) ))
−T(
k c∈C(d)
P r(z(d) E K ( E l(c)¯z(c)
i = k|z−(d,i), . . .) + E �z(a) T( k + α + n(d)
d:a∈C(d) k�� k k )
c∈C(d)
(w(d)
i )−(d,i)
n+ β(d)
k wi
nk + P
−(d,i) w βw
∝ (n(d)−(d,i) + α(d)
k )
k
×
Y Polya(z(d′)|α(d′) : z(d)
d′:d∈C(d′) i = k, z−(d,i), l)
(4)
−T( E l(c)¯zk) + α)) ,
c∈C(d)
</equation>
<bodyText confidence="0.9992075">
where Ψ(.) is the digamma function. For TIRE,
the likelihood decomposes across documents and we
can optimize the incoming edge weights for each
document separately. We have
</bodyText>
<equation confidence="0.999300965517242">
dPr(z(d)|l)
dl(a,d)
XK
+
k=1
� X
¯z(a) Ψ(
k
c∈C(d)
l(c,d)¯z(c)
k + α + n(d)
k )
X �
−Ψ( l(c,d)¯z(c)
c∈C(d) k + α) .
X
c∈C(d)
X
=Ψ(
k
l(c,d)¯z(c)
k + Kα)
X
c∈C(d)
X
−Ψ(
k
l(c,d)¯z(c)
k + Kα + n(d))
</equation>
<bodyText confidence="0.999560545454546">
where the nk’s are the counts of the occurrences
of topic k over all of the entries determined by the
superscript. The −(d, i) superscript indicates ex-
cluding the current assignment for z(d)
i . The up-
date equation is similar to the update equations of
Griffiths and Steyvers, but with a different α for
each document d, and with multiplicative weights
for each document that cites it. These weights
Polya(z(d)|α(d)) are the likelihood for a multivariate
Polya (a.k.a. Dirichlet-multinomial) distribution,
</bodyText>
<page confidence="0.995605">
117
</page>
<bodyText confidence="0.9998725">
We optimize the node-level l’s in TIRE
via the least squares estimate (LSE),
</bodyText>
<equation confidence="0.972519">
l(a) = {d:aEC(d)JJ Ed:aEC(d) l(a,d). Although
</equation>
<bodyText confidence="0.911367666666667">
the LSE for the mean of a truncated Gaussian is
biased, it is widely used as it is more robust than the
MLE (A’Hearn, 2004).
</bodyText>
<sectionHeader confidence="0.993324" genericHeader="evaluation">
5 Experimental Analysis
</sectionHeader>
<bodyText confidence="0.999874571428571">
In this section we experimentally investigate the
properties of TIR and TIRE. We consider two sci-
entific corpora: a collection of 3286 of articles
from the Association for Computational Linguis-
tics (ACL) conference2 (Radev et al., 2009) pub-
lished between 1987 and 2011, and a corpus of ar-
ticles from the Neural Information Processing Sys-
tems (NIPS) conference3 containing 1740 articles
from 1987 to 1999. The corpora both contained
a small number (53, and 14, respectively) of cita-
tion graph loops due to insider knowledge of simul-
taneous publications. Some loops were removed
by manual deletion of “insider knowledge” edges,
and others were removed by deleting edges in the
loop uniformly at random. For computational ef-
ficiency, we performed approximate Gibbs updates
where we drop the multiplicative Polya likelihood
terms in Equation 4. This corresponds to only trans-
mitting influence information downward in the cita-
tion DAG, but not transmitting “reverse influence”
information upwards. Preliminary experiments on
synthetic data indicated that this did not significantly
impact the ability of the model to recover the topical
influence weights. As one might expect, LDA is al-
ready capable of inferring topic distributions which
are good enough to perform the regression on, with-
out fully exploiting the additional feedback from the
regression. This algorithm has a similar running
time to the standard collapsed Gibbs sampler for
LDA, as the regression step is not a bottleneck.
In all experiments, we set the hyper-parameters
to a = 0.1, Q = 0.1 and the a parameter for the
truncated Gaussian in TIRE to be 1. We interleaved
regression steps every 10 Gibbs iterations. For ex-
ploratory data analysis experiments the models were
</bodyText>
<footnote confidence="0.977154">
2http://clair.eecs.umich.edu/aan/
3http://www.arbylon.net/resources.html,
published by Gregor Heinrich and based on an earlier collection
due to Sam Roweis.
</footnote>
<figure confidence="0.978662">
Topical Influence per Citation Edge (words) 14
12
10
8
6
4
2
0
</figure>
<figureCaption confidence="0.968302666666667">
Figure 3: Topical influence per edge versus number of
times cited by the citing article (NIPS). Several articles
had zero in-text citations due to author or dataset errors.
</figureCaption>
<bodyText confidence="0.999008">
trained for 500 burn-in iterations, and the samples
from the final iterations were used for the analysis.
</bodyText>
<subsectionHeader confidence="0.993505">
5.1 Model Validation using Metadata
</subsectionHeader>
<bodyText confidence="0.99998612">
It is not immediately obvious how to best validate an
unsupervised model of citation influence. Ground
truth is not well-defined and human evaluation re-
quires extensive knowledge of the individual papers
in the corpora. With this in mind, we explore how
topical influence scores relate to document meta-
data, which serves as a proxy for ground truth.
In many cases, if article c is repeatedly cited in the
text of article d it may indicate that d builds heavily
on c. We would therefore expect to see an associa-
tion between repeated citations and edge-wise topi-
cal influence l(c,d). For each of the 106 papers in the
NIPS corpus with at least three distinct references,
we counted the number of repeated citations for the
most influential and least influential references ac-
cording to the TIRE model (Figure 3). Overall, the
“most influential” references were cited 171 times in
the text of their citing articles, while the “least influ-
ential” references were cited 128 times. Of the 45
articles where the counts were not tied, the most in-
fluential references had the higher citation counts 33
times. A sign test rejects the null hypothesis that the
median difference in citation counts between least
and most influential references is zero at a = 0.05,
with p-value Pz� 5 x 10−4.
</bodyText>
<figure confidence="0.896559">
0 1 2 3 4 5
Times Cited by Citing Article
</figure>
<page confidence="0.993563">
118
</page>
<bodyText confidence="0.9999778">
Self-citations, where at least one author is in com-
mon between cited and citing articles, are also infor-
mative (Figure 4). Authors often build upon their
own work, so we would expect self-citations to have
higher edge-wise topical influence on average. For
ACL the mean topical influence for a self citation
edge is 2.80 and for a non-self citation is 1.40. For
NIPS the means are 5.05 (self) and 3.15 (non-self).
A two-sample t-test finds these differences are both
significant at α = 0.05.
</bodyText>
<subsectionHeader confidence="0.99887">
5.2 Prediction Experiments
</subsectionHeader>
<bodyText confidence="0.999979888888889">
We also used a document prediction task to explore
whether the posited latent structure is predictively
useful. We selected roughly 10% of the articles in
each corpus (170 and 330 documents for NIPS and
ACL, respectively) for testing, chosen among the ar-
ticles that made at least one citation. We held out
a randomly selected set of 50% of their words and
evaluated the log probability of the held out partial
documents under each model. This is equivalent
to evaluating on a set of new documents with the
same set of references as the held out set. Evaluation
was performed using annealed importance sampling
(Neal, 2001), as in Wallach et al. (2009) except we
used multiple samples per likelihood computation.
The TIR models were compared to LDA and
an “additive” version of DMR with link function
α(kd) = x(d)⊺ Ak + α, where the As were con-
strained to be positive and given an exponential
prior with mean one. For DMR, binary feature vec-
tors encoded the presence or absence of each pos-
sible citation. For each algorithm, we burned in
for 250 iterations, then executed 1000 iterations,
optimizing topical influence weights/DMR param-
eters every 10th iteration. Held-out log proba-
bility scores were computed by performing AIS
with every 100th sample, and averaging the re-
sults to estimate the posterior predictive probability
Pr(held out article|training set, citations, model).
It was found that all of the regression methods had
superior predictive performance to LDA on these
corpora, demonstrating that topical influence has
predictive value (Table 1). Although DMR per-
formed slightly better than TIR predictively, TIR
was competitive despite the fact that it has a factor of
K less regression parameters. Note that DMR does
not provide an interpretable notion of influence.
</bodyText>
<subsectionHeader confidence="0.999576">
5.3 Exploring Topical Influence
</subsectionHeader>
<bodyText confidence="0.999527113636364">
In this section we explore the inferred topical influ-
ence scores l(d), total topical influence scores T (d)
and edgewise topical influence scores l(,,d) (recall
their definitions in Equations 1, 2 and 3, respec-
tively). Table 2 shows the most influential articles in
the ACL corpus, according to citation counts, top-
ical influence and total topical influence (the latter
two inferred with the TIR model). The most fre-
quently cited paper within the ACL corpus, written
by Papineni et al., introduces BLEU, a technique for
evaluating machine translation (MT) systems.4 This
paper is of great importance to the computational
linguistics community because the method that it
introduces is widely used to validate MT systems.
However, the BLEU article has a relatively low top-
ical influence value of 0.58, consistent with the fact
that most of the papers that cite it use the technique
as part of their methodology but do not build upon
its ideas. We emphasize that topical influence mea-
sures a specific dimension of scientific importance,
namely the tendency of an article to influence the
ideas (as mediated by the topics) of citing articles;
papers with low topical influence such as the BLEU
article may be important for other reasons.
Ranking papers by their influence weights l(d)
(Table 2, middle) has the opposite difficulty to rank-
ing by citation counts — the papers with the highest
topical influence were typically cited only once, by
the same authors. This makes sense, given what the
model is designed to do. The lone citing papers were
certainly topically influenced by these articles.
A more useful metric, however, is the total top-
ical influence T (d) (the bottom sub-table in Table
2). This is the total number of words of prior con-
centration, summed over all of its citers, that the
article has contributed, and is a measure of the to-
tal corpus-wide topical influence of the paper. This
metric ranks the BLEU paper at 5th place, down
from 1st place by citation count. The ACL paper
with the highest total topical influence, by David
Chiang, won the ACL best paper award in 2005.
The behavior of the different metrics is echoed
in the NIPS corpus (Table 3). The most
cited paper, “Handwritten Digit Recognition,” by
</bodyText>
<footnote confidence="0.5420765">
4Citations within the corpora are of course only a small frac-
tion of the total set of citations for many of these papers.
</footnote>
<page confidence="0.99515">
119
</page>
<figure confidence="0.982678428571429">
25
20
15
10
5
0
Topical Influence Per Citation Edge (words)
15
10
5
0
Topical Influence Per Citation Edge (words)
Non−Self Citations Self Citations
Non−Self Citations Self Citations
</figure>
<figureCaption confidence="0.987405">
Figure 4: Topical influence for self and non-self citation edges. Left: ACL. Right: NIPS.
</figureCaption>
<table confidence="0.999220333333333">
ACL NIPS
Wins Losses Average Wins Losses Average
Improvement Improvement
TIR 297 33 65.7 150 20 38.2
TIRE 276 54 63.0 148 22 38.7
DMR 302 28 79.1 157 13 48.4
</table>
<tableCaption confidence="0.996250666666667">
Table 1: Wins, losses and average improvement for log probabilities of held-out articles, versus LDA. Each “Win”
corresponds to the model assigning a higher log probability score for the test portion of a held-out document than LDA
assigned to that document.
</tableCaption>
<bodyText confidence="0.998653452380952">
Le Cun et al. (1990), is an early successful applica-
tion of neural networks. The paper does not in-
troduce novel models or algorithms, but rather, in
the authors’ words, “show[s] that large back propa-
gation (BP) networks can be applied to real image
recognition problems.” Thus, although it is has an
important role as a landmark neural network success
story, it does not score highly in terms of topical in-
fluence. This paper is ranked 13th according to total
topical influence, with a score of 1.6. The top two-
ranked papers according to total topical influence,
on Gaussian Process Regression and POMDPs re-
spectively, were both seminal papers that spawned
large bodies of related work. An interesting case is
the third-ranked paper in the NIPS corpus, by Wang
et al., on the theory of early stopping. It is only ref-
erenced three times, but has a very high topical in-
fluence of 19.3 words. All three citing papers are
also on the theory of early stopping, and one of the
papers, by Wang and Venkatesh, directly extends a
theoretical result of this paper. Although it is easy
to see why this paper scores highly on topical in-
fluence, in this case the metric has perhaps over-
stated its importance. A limitation of topical influ-
ence is that it can potentially give more credit than
is due when an article is cited by a small number of
topically similar papers, due to overfitting. This is
likely to be an issue for any topic-based approach
for modeling scientific influence. However, topics
help to absorb lexical ambiguity and author-specific
idiosyncracies, mitigating the problem relative to
word-based approaches.
Using the TIRE model, we can also look at in-
fluence relationships between pairs of articles. Ta-
bles 4 and 5 show the most and least topically influ-
ential references, and the most and least influenced
citing papers, for three example articles from ACL
and NIPS, respectively. The model correctly assigns
higher influence scores along the edges to and from
relevant documents. For the ACL papers, the BLEU
algorithm’s article is inferred to have zero topical in-
fluence on Chiang’s paper, consistent with its role
</bodyText>
<page confidence="0.987718">
120
</page>
<table confidence="0.970321611111111">
Top 5 Articles by Citation Count
140 BLEU: a Method for Automatic Evaluation of Machine Translation. K. Papineni, S. Roukos, T. Ward, W. Zhu.
105 Minimum Error Rate Training in Statistical Machine Translation. F. Och.
64 A Hierarchical Phrase-Based Model for Statistical Machine Translation. D. Chiang.
64 Accurate Unlexicalized Parsing. D. Klein, C. Manning.
59 Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. D. Yarowsky.
Top 5 articles by Topical Influence
11.38 Refining Event Extraction through Cross-document Inference. H. Ji, R. Grishman.
11.37 Bayesian Learning of Non-compositional Phrases with Synchronous Parsing. H. Zhang, C. Quirk, R. Moore, D. Gildea.
10.48 A Plan Recognition Model for Clarification Subdialogues. D. Litman, J. Allen.
10.38 PCFGs with Syntactic and Prosodic Indicators of Speech Repairs. J. Hale et al.
10.30 Referring as Requesting, P. Cohen
Top 5 Articles by Total Topical Influence
111.46 (1.74 x 64) A Hierarchical Phrase-Based Model for Statistical Machine Translation. D. Chiang.
101.12 (6.74 x 15) Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation. D. Xiong, Q. Liu, S. Lin.
98.56 (5.80 x 17) A Logical Semantics for Feature Structures. R. Kasper, W. Rounds.
85.15 (2.18 x 39) Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. F. Och, H. Ney
81.82 (0.58 x 140) BLEU: a Method for Automatic Evaluation of Machine Translation, K. Papineni, S. Roukos, T. Ward, and W. Zhu.
</table>
<tableCaption confidence="0.991852666666667">
Table 2: Most influential articles in the ACL Conference corpus, according to citation counts (top), topical influence
l(d) inferred by TIR (middle), and total topical influence T (d) inferred by TIR (bottom). For total topical influence,
the breakdown of T (d) = l(d)x citation count is shown in parentheses.
</tableCaption>
<figure confidence="0.608709736842105">
Top 5 Articles by Citation Count
26 Handwritten Digit Recognition with a Back-Propagation Network. Y. Le Cun, et al.
19 Optimal Brain Damage. Y. Le Cun, J. Denker, S. Solla.
17 A New Learning Algorithm for Blind Signal Separation. S. Amari, A. Cichocki, H. Yang.
17 Efficient Pattern Recognition Using a New Transformation Distance. P. Simard, Y. Le Cun, J. Denker.
14 The Cascade-Correlation Learning Architecture. S. Fahlman, C. Lebiere.
Top 5 articles by Topical Influence
29.7 Synchronization and Grammatical Inference in an Oscillating Elman Net. B. Baird, T. Troyer, F. Eeckman.
26.3 Learning the Solution to the Aperture Problem for Pattern Motion with a Hebb Rule. M. Sereno.
25.9 ALVINN: An Autonomous Land Vehicle in a Neural Network. D. Pomerleau.
25.1 Some Estimates of Necessary Number of Connections and Hidden Units for Feed-Forward Networks. A. Kowalczyk.
24.7 Complex- Cell Responses Derived from Center-Surround Inputs: The Surprising Power of Intradendritic Computation.
B. Mel, D. Ruderman, K. Archie.
Top 5 Articles by Total Topical Influence
84.7 (10.6 x 8) Gaussian Processes for Regression. C. Williams, C. Rasmussen.
63.9 (7.1 x 9) Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems. T. Jaakkola, S. Singh, M. Jordan.
57.9 (19.3 x 3) Optimal Stopping and Effective Machine Complexity in Learning. C. Wang, S. Venkatesh, J. Judd.
54.7 (10.9 x 5) Links Between Markov Models and Multilayer Perceptrons. H. Bourlard, C. Wellekens.
51.2 (3.7 x 14) The Cascade-Correlation Learning Architecture. S. Fahlman, C. Lebiere.
</figure>
<tableCaption confidence="0.954512">
Table 3: Most influential articles in the NIPS corpus, according to citation counts (top), topical influence l(d) inferred
by TIR (middle), and total topical influence T(d) inferred by TIR (bottom).
</tableCaption>
<table confidence="0.999810470588235">
A Hierarchical Phrase-Based Model for Statistical Machine Translation. D. Chiang.
Most influential reference 1.48 Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. F. Och and H. Ney.
Least influential reference 0.00 BLEU: a Method for Automatic Evaluation of Machine Translation. K. Papineni, S. Roukos, T. Ward, W. Zhu.
Most influenced citer 2.54 Toward Smaller, Faster, and Better Hierarchical Phrase-based SMT. M. Yang, J. Zheng.
Least influenced citer 0.60 An Optimal-time Binarization Algorithm for Linear Context-Free Rewriting Systems with Fan-out Two.
C. Gmez-Rodrguez, G. Satta.
Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. D. Yarowsky.
Most influential reference 2.52 Subject-dependent Co-occurrence and Word Sense Disambiguation. J. Guthrie, L. Guthrie, Y. Wilks, H. Aidinejad.
Least influential reference 0.53 Word-sense Disambiguation using Statistical Methods. P. Brown, S. Della Pietra, V. Della Pietra, R. Mercer.
Most influenced citer 1.81 Discriminating Image Senses by Clustering with Multimodal Features. N. Loeff, C. Alm, D. Forsyth.
Least influenced citer 0.00 Semi-supervised Convex Training for Dependency Parsing. Q. Wang, D. Schuurmans, D. Lin.
Accurate Unlexicalized Parsing. D. Klein, C. Manning.
Most influential reference 3.87 Parsing with Treebank Grammars: Empirical Bounds, Theoretical Models, and the Structure of the Penn Treebank.
Least influential reference 0.81 D. Klein and C. Manning.
Efficient Parsing for Bilexical Context-Free Grammars and Head Automaton Grammars. J. Eisner, G. Satta.
Most influenced citer 1.67 Evaluating the Accuracy of an Unlexicalized Statistical Parser on the PARC DepBank. T. Briscoe, J. Carroll.
Least influenced citer 0.00 Finding Contradictions in Text. M. de Marneffe, A. Rafferty, C. Manning.
</table>
<tableCaption confidence="0.9980975">
Table 4: Least and most influential references and citers, and the influence weights along these edges, inferred by the
TIRE model for three example ACL articles.
</tableCaption>
<page confidence="0.895678">
121
</page>
<table confidence="0.999778133333333">
Feudal Reinforcement Learning. P. Dayan, G. Hinton
Most influential reference 5.47 Memory-based Reinforcement Learning: Efficient Computation with Prioritized Sweeping. A. Moore, C. Atkeson.
Least influential reference 0.00 A Delay-Line Based Motion Detection Chip. T. Horiuchi, J. Lazzaro, A. Moore, C. Koch.
Most influenced citer 3.36 The Parti-Game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-Spaces. A. Moore.
Least influenced citer 1.71 Multi-time Models for Temporally Abstract Planning. D. Precup, R. Sutton.
Optimal Brain Damage. Y. Le Cun, J. Denker , S. Solla.
Most influential reference 2.82 Comparing Biases for Minimal Network Construction with Back-Propagation. S. Hanson, L. Pratt.
Least influential reference 0.15 Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment. M. Mozer, P. Smolensky.
Most influenced citer 3.08 Structural Risk Minimization for Character Recognition. I. Guyon, V. Vapnik, B. Boser, L. Bottou, S. Solla.
Least influenced citer 0.64 Structural and Behavioral Evolution of Recurrent Networks. G. Saunders, P. Angeline, J. Pollack.
An Input Output HMM Architecture. Y. Bengio, P. Frasconi.
Most influential reference 5.29 Credit Assignment through Time: Alternatives to Backpropagation. Y. Bengio, P. Frasconi.
Least influential reference 0.00 Induction of Multiscale Temporal Structure. M. Mozer
Most influenced citer 2.66 Learning Fine Motion by Markov Mixtures of Experts. M. Meila, M. Jordan.
Least influenced citer 1.47 Recursive Estimation of Dynamic Modular RBF Networks. V. Kadirkamanathan, M. Kadirkamanathan.
</table>
<tableCaption confidence="0.9984885">
Table 5: Least and most influential references and citers, and the influence weights along these edges, inferred by the
TIRE model for three example NIPS articles.
</tableCaption>
<bodyText confidence="0.999939933333333">
in the paper as an evaluation technique. The paper
most topically influenced by Chiang’s paper, written
by Yang and Zheng, aims to improve upon the ideas
in that paper. In the NIPS corpus, the article by Ben-
gio and Frasconi, on recurrent neural network archi-
tectures, extends previous work by the same authors,
which is correctly assigned the highest topical influ-
ence. A particularly interesting case is the paper by
Dayan and Hinton, which is heavily influenced by
a paper by Moore, and in turn strongly influences
a later paper by Moore, thus illustrating the inter-
play of scientific influence between authors along
the citation graph. These three papers were on re-
inforcement learning, while the lowest scoring ref-
erence and citer were on other subjects.
</bodyText>
<sectionHeader confidence="0.998552" genericHeader="conclusions">
6 Conclusions /Discussion
</sectionHeader>
<bodyText confidence="0.999990696969697">
This paper introduced the notion of topical influ-
ence, a quantitative measure of scientific impact
which arises from a latent variable model called top-
ical influence regression. The model builds upon the
ideas of Dirichlet-multinomial regression to encode
influence relationships between articles along the ci-
tation graph. By training TIR, we can recover topi-
cal influence scores that give us insight into the im-
pact of scientific articles. The model was applied to
two scientific corpora, demonstrating the utility of
the method both quantitatively and qualitatively.
In future work, the proposed framework could
readily be extended to model other aspects of sci-
entific influence, such as the effects of authors and
journals on topical influence, and to exploit the con-
text in which citations occur. From an exploratory
analysis perspective, it would be instructive to com-
pare topical influence trajectories over time for dif-
ferent papers. This could be further facilitated by ex-
plicitly modeling the dynamics of each article’s top-
ical influence score. The TIR framework could po-
tentially also be applicable to other application do-
mains such as modeling how interpersonal influence
affects the spread of memes via social media.
To complement TIR, it would be useful to also
have systems for identifying articles which are im-
portant for alternative reasons, such as providing
methodological tools and/or demonstrating impor-
tant facts. Ultimately a suite of such tools could feed
into a system such as Google Scholar or Citeseer.
We envision that this line of work will also be useful
for building visualization tools to help researchers
explore scientific corpora.
</bodyText>
<sectionHeader confidence="0.998243" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99905025">
Supported by the Intelligence Advanced Research
Projects Activity (IARPA) via Department of In-
terior National Business Center contract number
D11PC20155. The U.S. government is authorized to
reproduce and distribute reprints for Governmental
purposes notwithstanding any copyright annotation
thereon. Disclaimer: The views and conclusions
contained herein are those of the authors and should
not be interpreted as necessarily representing the of-
ficial policies or endorsements, either expressed or
implied, of IARPA, DoI/NBC, or the U.S. Govern-
ment.
</bodyText>
<page confidence="0.996705">
122
</page>
<sectionHeader confidence="0.993882" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999654506024097">
[A’Hearn2004] B. A’Hearn. 2004. A restricted max-
imum likelihood estimator for truncated height sam-
ples. Economics &amp; Human Biology, 2(1):5–19.
[Blei et al.2003] D.M. Blei, A.Y. Ng, and M.I. Jordan.
2003. Latent Dirichlet allocation. The Journal of Ma-
chine Learning Research, 3:993–1022.
[Brin and Page1998] S. Brin and L. Page. 1998. The
anatomy of a large-scale hypertextual web search en-
gine. Computer networks and ISDN systems, 30(1-
7):107–117.
[Chang and Blei2009] J. Chang and D. Blei. 2009. Rela-
tional topic models for document networks. In Artifi-
cial Intelligence and Statistics, pages 81–88.
[Cohn and Hofmann2001] D. Cohn and T. Hofmann.
2001. The missing link-a probabilistic model of docu-
ment content and hypertext connectivity. In Advances
in Neural Information Processing Systems, pages 430–
436.
[Dietz et al.2007] L. Dietz, S. Bickel, and T. Scheffer.
2007. Unsupervised prediction of citation influences.
In Proceedings of the 24th International Conference
on Machine Learning, pages 233–240.
[Gerrish and Blei2010] S. Gerrish and D.M. Blei. 2010.
A language-based approach to measuring scholarly
impact. In Proceedings of the 26th International Con-
ference on Machine Learning, pages 375–382.
[Griffiths and Steyvers2004] T.L. Griffiths and
M. Steyvers. 2004. Finding scientific topics.
Proceedings of the National Academy of Sciences of
the United States ofAmerica, 101(Suppl 1):5228.
[He et al.2009] Q. He, B. Chen, J. Pei, B. Qiu, P. Mitra,
and L. Giles. 2009. Detecting topic evolution in sci-
entific literature: how can citations help? In Proceed-
ings of the 18th ACM Conference on Information and
Knowledge Management, pages 957–966. ACM.
[Le Cun et al.1990] B.B. Le Cun, JS Denker, D. Hender-
son, RE Howard, W. Hubbard, and LD Jackel. 1990.
Handwritten digit recognition with a back-propagation
network. In Advances in Neural Information Process-
ing Systems, pages 396–404.
[Lin2008] J. Lin. 2008. Pagerank without hyper-
links: Reranking with pubmed related article networks
for biomedical text retrieval. BMC bioinformatics,
9(1):270.
[Mimno and McCallum2008] D. Mimno and A. McCal-
lum. 2008. Topic models conditioned on arbitrary
features with Dirichlet-multinomial regression. In Un-
certainty in Artificial Intelligence, pages 411–418.
[Nallapati et al.2011] R. Nallapati, D. McFarland, and
C. Manning. 2011. Topicflow model: Unsupervised
learning of topic-specific influences of hyperlinked
documents. In International Conference on Artificial
Intelligence and Statistics, pages 543–551.
[Neal2001] R.M. Neal. 2001. Annealed importance sam-
pling. Statistics and Computing, 11(2):125–139.
[Radev et al.2009] D. R. Radev, P. Muthukrishnan, and
V. Qazvinian. 2009. The ACL anthology network cor-
pus. In Proceedings, ACL Workshop on Natural Lan-
guage Processing and Information Retrieval for Digi-
tal Libraries, pages 54–61, Singapore.
[Shaparenko and Joachims2009] B. Shaparenko and
T. Joachims. 2009. Identifying the original con-
tribution of a document via language modeling. In
Machine Learning and Knowledge Discovery in
Databases, pages 350–365. Springer.
[Teufel et al.2006] S. Teufel, A. Siddharthan, and D. Tid-
har. 2006. Automatic classification of citation func-
tion. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 103–110. Association for Computational Lin-
guistics.
[Wallach et al.2009] H.M. Wallach, I. Murray,
R. Salakhutdinov, and D. Mimno. 2009. Evalu-
ation methods for topic models. In Proceedings of
the 26th Annual International Conference on Machine
Learning, pages 1105–1112. ACM.
[Wallach2006] H.M. Wallach. 2006. Topic modeling:
beyond bag-of-words. In Proceedings of the 23rd In-
ternational Conference on Machine Learning, pages
977–984. ACM.
[Ziman1968] J.M. Ziman. 1968. Public knowledge:
an essay concerning the social dimension of science.
Cambridge University Press.
</reference>
<page confidence="0.998949">
123
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.945017">
<title confidence="0.999984">Modeling Scientific Impact with Topical Influence Regression</title>
<author confidence="0.999856">James Foulds Padhraic Smyth</author>
<affiliation confidence="0.9876255">Department of Computer University of California, Irvine</affiliation>
<abstract confidence="0.998097176470588">When reviewing scientific literature, it would be useful to have automatic tools that identify the most influential scientific articles as well as how ideas propagate between articles. this context, this paper introduces a quantitative measure of the extent to which an article tends to spread its topics to the articles that cite it. Given the text of the articles and their citation graph, we show how to learn a probabilistic model to recover both the degree of topical influence of each article and the influence relationships between articles. Experimental results on corpora from two well-known computer science conferences are used to illustrate and validate the proposed approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B A’Hearn</author>
</authors>
<title>A restricted maximum likelihood estimator for truncated height samples.</title>
<date>2004</date>
<journal>Economics &amp; Human Biology,</journal>
<volume>2</volume>
<issue>1</issue>
<marker>[A’Hearn2004]</marker>
<rawString>B. A’Hearn. 2004. A restricted maximum likelihood estimator for truncated height samples. Economics &amp; Human Biology, 2(1):5–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<marker>[Blei et al.2003]</marker>
<rawString>D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent Dirichlet allocation. The Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine.</title>
<date>1998</date>
<booktitle>Computer networks and ISDN systems,</booktitle>
<pages>30--1</pages>
<marker>[Brin and Page1998]</marker>
<rawString>S. Brin and L. Page. 1998. The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems, 30(1-7):107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chang</author>
<author>D Blei</author>
</authors>
<title>Relational topic models for document networks.</title>
<date>2009</date>
<booktitle>In Artificial Intelligence and Statistics,</booktitle>
<pages>81--88</pages>
<marker>[Chang and Blei2009]</marker>
<rawString>J. Chang and D. Blei. 2009. Relational topic models for document networks. In Artificial Intelligence and Statistics, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cohn</author>
<author>T Hofmann</author>
</authors>
<title>The missing link-a probabilistic model of document content and hypertext connectivity.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>430--436</pages>
<marker>[Cohn and Hofmann2001]</marker>
<rawString>D. Cohn and T. Hofmann. 2001. The missing link-a probabilistic model of document content and hypertext connectivity. In Advances in Neural Information Processing Systems, pages 430– 436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Dietz</author>
<author>S Bickel</author>
<author>T Scheffer</author>
</authors>
<title>Unsupervised prediction of citation influences.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine Learning,</booktitle>
<pages>233--240</pages>
<marker>[Dietz et al.2007]</marker>
<rawString>L. Dietz, S. Bickel, and T. Scheffer. 2007. Unsupervised prediction of citation influences. In Proceedings of the 24th International Conference on Machine Learning, pages 233–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gerrish</author>
<author>D M Blei</author>
</authors>
<title>A language-based approach to measuring scholarly impact.</title>
<date>2010</date>
<booktitle>In Proceedings of the 26th International Conference on Machine Learning,</booktitle>
<pages>375--382</pages>
<marker>[Gerrish and Blei2010]</marker>
<rawString>S. Gerrish and D.M. Blei. 2010. A language-based approach to measuring scholarly impact. In Proceedings of the 26th International Conference on Machine Learning, pages 375–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States ofAmerica, 101(Suppl</booktitle>
<pages>1--5228</pages>
<marker>[Griffiths and Steyvers2004]</marker>
<rawString>T.L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences of the United States ofAmerica, 101(Suppl 1):5228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q He</author>
<author>B Chen</author>
<author>J Pei</author>
<author>B Qiu</author>
<author>P Mitra</author>
<author>L Giles</author>
</authors>
<title>Detecting topic evolution in scientific literature: how can citations help?</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM Conference on Information and Knowledge Management,</booktitle>
<pages>957--966</pages>
<publisher>ACM.</publisher>
<marker>[He et al.2009]</marker>
<rawString>Q. He, B. Chen, J. Pei, B. Qiu, P. Mitra, and L. Giles. 2009. Detecting topic evolution in scientific literature: how can citations help? In Proceedings of the 18th ACM Conference on Information and Knowledge Management, pages 957–966. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B B Le Cun</author>
<author>JS Denker</author>
<author>D Henderson</author>
<author>RE Howard</author>
<author>W Hubbard</author>
<author>LD Jackel</author>
</authors>
<title>Handwritten digit recognition with a back-propagation network.</title>
<date>1990</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>396--404</pages>
<marker>[Le Cun et al.1990]</marker>
<rawString>B.B. Le Cun, JS Denker, D. Henderson, RE Howard, W. Hubbard, and LD Jackel. 1990. Handwritten digit recognition with a back-propagation network. In Advances in Neural Information Processing Systems, pages 396–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
</authors>
<title>Pagerank without hyperlinks: Reranking with pubmed related article networks for biomedical text retrieval.</title>
<date>2008</date>
<journal>BMC bioinformatics,</journal>
<pages>9--1</pages>
<marker>[Lin2008]</marker>
<rawString>J. Lin. 2008. Pagerank without hyperlinks: Reranking with pubmed related article networks for biomedical text retrieval. BMC bioinformatics, 9(1):270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mimno</author>
<author>A McCallum</author>
</authors>
<title>Topic models conditioned on arbitrary features with Dirichlet-multinomial regression.</title>
<date>2008</date>
<booktitle>In Uncertainty in Artificial Intelligence,</booktitle>
<pages>411--418</pages>
<marker>[Mimno and McCallum2008]</marker>
<rawString>D. Mimno and A. McCallum. 2008. Topic models conditioned on arbitrary features with Dirichlet-multinomial regression. In Uncertainty in Artificial Intelligence, pages 411–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nallapati</author>
<author>D McFarland</author>
<author>C Manning</author>
</authors>
<title>Topicflow model: Unsupervised learning of topic-specific influences of hyperlinked documents.</title>
<date>2011</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>543--551</pages>
<marker>[Nallapati et al.2011]</marker>
<rawString>R. Nallapati, D. McFarland, and C. Manning. 2011. Topicflow model: Unsupervised learning of topic-specific influences of hyperlinked documents. In International Conference on Artificial Intelligence and Statistics, pages 543–551.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Neal</author>
</authors>
<title>Annealed importance sampling.</title>
<date>2001</date>
<journal>Statistics and Computing,</journal>
<volume>11</volume>
<issue>2</issue>
<marker>[Neal2001]</marker>
<rawString>R.M. Neal. 2001. Annealed importance sampling. Statistics and Computing, 11(2):125–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>P Muthukrishnan</author>
<author>V Qazvinian</author>
</authors>
<title>The ACL anthology network corpus.</title>
<date>2009</date>
<booktitle>In Proceedings, ACL Workshop on Natural Language Processing and Information Retrieval for Digital Libraries,</booktitle>
<pages>54--61</pages>
<marker>[Radev et al.2009]</marker>
<rawString>D. R. Radev, P. Muthukrishnan, and V. Qazvinian. 2009. The ACL anthology network corpus. In Proceedings, ACL Workshop on Natural Language Processing and Information Retrieval for Digital Libraries, pages 54–61, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Shaparenko</author>
<author>T Joachims</author>
</authors>
<title>Identifying the original contribution of a document via language modeling.</title>
<date>2009</date>
<booktitle>In Machine Learning and Knowledge Discovery in Databases,</booktitle>
<pages>350--365</pages>
<publisher>Springer.</publisher>
<marker>[Shaparenko and Joachims2009]</marker>
<rawString>B. Shaparenko and T. Joachims. 2009. Identifying the original contribution of a document via language modeling. In Machine Learning and Knowledge Discovery in Databases, pages 350–365. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>A Siddharthan</author>
<author>D Tidhar</author>
</authors>
<title>Automatic classification of citation function.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>103--110</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>[Teufel et al.2006]</marker>
<rawString>S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Automatic classification of citation function. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 103–110. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H M Wallach</author>
<author>I Murray</author>
<author>R Salakhutdinov</author>
<author>D Mimno</author>
</authors>
<title>Evaluation methods for topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning,</booktitle>
<pages>1105--1112</pages>
<publisher>ACM.</publisher>
<marker>[Wallach et al.2009]</marker>
<rawString>H.M. Wallach, I. Murray, R. Salakhutdinov, and D. Mimno. 2009. Evaluation methods for topic models. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 1105–1112. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H M Wallach</author>
</authors>
<title>Topic modeling: beyond bag-of-words.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd International Conference on Machine Learning,</booktitle>
<pages>977--984</pages>
<publisher>ACM.</publisher>
<marker>[Wallach2006]</marker>
<rawString>H.M. Wallach. 2006. Topic modeling: beyond bag-of-words. In Proceedings of the 23rd International Conference on Machine Learning, pages 977–984. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Ziman</author>
</authors>
<title>Public knowledge: an essay concerning the social dimension of science.</title>
<date>1968</date>
<publisher>Cambridge University Press.</publisher>
<marker>[Ziman1968]</marker>
<rawString>J.M. Ziman. 1968. Public knowledge: an essay concerning the social dimension of science. Cambridge University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
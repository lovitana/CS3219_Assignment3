<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000013">
<title confidence="0.996115">
A temporal model of text periodicities using Gaussian Processes
</title>
<author confidence="0.999807">
Daniel Preot¸iuc-Pietro, Trevor Cohn
</author>
<affiliation confidence="0.998208">
Department of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.916274">
Regent Court, 211 Portobello Street
Sheffield, S1 4DP, United Kingdom
</address>
<email confidence="0.999742">
{daniel,t.cohn}@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.996835" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.942556125">
Temporal variations of text are usually ig-
nored in NLP applications. However, text use
changes with time, which can affect many
applications. In this paper we model peri-
odic distributions of words over time. Focus-
ing on hashtag frequency in Twitter, we first
automatically identify the periodic patterns.
We use this for regression in order to fore-
cast the volume of a hashtag based on past
data. We use Gaussian Processes, a state-of-
the-art bayesian non-parametric model, with
a novel periodic kernel. We demonstrate this
in a text classification setting, assigning the
tweet hashtag based on the rest of its text. This
method shows significant improvements over
competitive baselines.
</bodyText>
<sectionHeader confidence="0.999511" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999402754716981">
Temporal changes in text corpora are central to our
understanding of many linguistic and social phe-
nomena. Social Media platforms and the digital-
ization of libraries provides a vast body of times-
tamped data. This allows studying of the complex
temporal patterns exhibited by text usage includ-
ing highly non-stationary distributions and period-
icities. However, temporal effects have been mostly
ignored by previous work on text analysis or at best
dealt with by making strong assumptions such as
smoothly varying parameters with time (Yogatama
et al., 2011) or modelled using a simple uni-modal
distri bution (Wang and McCallum, 2006). This pa-
per develops a temporal model for classifying mi-
croblog posts which explicitly incorporates mul-
timodal periodic behaviours using Gaussian Pro-
cesses (GPs).
We expect text usage to follow multiple period-
icities at different scales. For example, people on
Social Media might talk about different topics dur-
ing and after work on weekdays, talk every Friday
about the weekend ahead, or comment about their
favorite weekly TV show during its air time. Given
this, text frequencies will display periodic patterns.
This applies to other text related quantities like co-
occurrence values or topic distributions over time,
as well as applications outside NLP like user be-
haviour (Preot¸iuc-Pietro and Cohn, 2013).
Modelling temporal patterns and periodicities can
be useful to tasks like text classification. For exam-
ple a tweet containing ‘music’ is normally attributed
to a general hashtag about music like #np (now play-
ing). However, knowing time, if it occurs during the
(weekly periodic) air time of ‘American Idol’ it is
more likely for it to belong to #americanidol or if
its mentioned in the days building up to the Video
Music Awards to be assigned to #VMA.
In NLP, temporal models have treated time in
overly simplistic ways and without regard to period-
icities. We propose a model that first broadly iden-
tifies several types of temporal patterns: a) periodic,
b) constant in time, c) falling out of use after enjoy-
ing a brief spell of popularity (e.g. internet memes,
news). This is performed automatically only using
training data and makes no assumptions on the exis-
tence or the length of the periods we aim to model.
We demonstrate the approach by modelling frequen-
cies of hashtag occurrences in Twitter. Hashtags are
user-generated labels included in tweets by their au-
thors in order to assign them to a conversation and
can be considered as a proxy for topics.
To this end, we make use of Gaussian Pro-
cesses (GP) (Rasmussen and Williams, 2005), a
</bodyText>
<page confidence="0.959203">
977
</page>
<note confidence="0.734366">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 977–988,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.998710625">
Bayesian non-parametric model for regression. Us-
ing the Bayesian evidence we automatically perform
model selection to classify temporal patterns. We
aim to use the most suitable model for extrapolation,
i.e. predicting future values from past observations.
The GP is fully defined by the covariance structure
assumed between the observed points, and its hy-
perparameters, which can be automatically learned
from data. We also introduce a new kernel suitable
to model the periodic behaviour we observe in text:
periods of low frequency followed by bursts at reg-
ular time intervals. We demonstrate that the GP ap-
proach is more general and gives better results than
frequentist models (e.g. autoregressive models) be-
cause it incorporates uncertainty explicitly and ele-
gantly, in addition to automatic model selection and
parameter fitting.
To demonstrate the practical importance of our
approach, we use our GP prediction as a prior in a
Naive Bayes model for text classification showing
improvements over baselines which do not account
for temporal periodicities. Our approach extends to
more general uses, e.g. to discriminative text regres-
sion and classification. More broadly, we aim to es-
tablish GPs as a state-of-the-art model for regression
and classification in NLP. To our knowledge, this is
the first paper to use GP regression for forecasting
and model selection within a NLP task.
All the hashtag time series data and the imple-
mentation of the PS kernel in the popular open-
source Gaussian Processes packages GPML1 and
GPy2 are available on the author’s website3.
</bodyText>
<sectionHeader confidence="0.999944" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9992107">
Time varying text patterns have been of particular
interest in topic modelling. Griffiths and Steyvers
(2004) analyse evolution of topics over time, but
without modelling time explicitly. Extensions that
model time make different assumptions, usually re-
garding smoothing proprieties in (Wang and McCal-
lum, 2006; Blei and Lafferty, 2006; Wang et al.,
2008; Hennig et al., 2012). Yogatama et al. (2011)
proposed a regulariser for generalised linear models
that encourages local temporal smoothness.
</bodyText>
<footnote confidence="0.9998245">
1http://www.gaussianprocess.org/gpml/
code
2https://github.com/SheffieldML/GPy
3http://www.preotiuc.ro
</footnote>
<bodyText confidence="0.999735193548387">
Modelling periodicities is one of the standard ap-
plications of Gaussian Processes (Rasmussen and
Williams, 2005). Recent work by Wilson and Adams
(2013) and Durrande et al. (2013) show how differ-
ent periods can be identified from data. In general,
methods that assume certain periodicities at daily or
weekly levels were proposed e.g. in (McInerney et
al., 2013). GPs were used with text by Polajnar et
al. (2011) and for Quality Estimation regression in
(Cohn and Specia, 2013; Shah et al., 2013).
Temporal patterns for short, distinctive lexical
items such as hashtags and memes were quanti-
tatively studied (Leskovec et al., 2009) and clus-
tered (Yang and Leskovec, 2011) in Social Media.
(Yang et al., 2012) studies the dual role of hashtags,
of bookmarks of content and symbols of commu-
nity membership, in the context of hashtag adoption.
(Romero et al., 2011) analyses the patterns of tem-
poral diffusion in Social Media finding that hashtags
have also a persistence factor.
For predicting future popularity of hashtags, Tsur
and Rappoport (2012) use linear regression with a
wide range of features. (Ma et al., 2012; Ma et
al., 2013) frame the problem as classification into
a number of fixed intervals and applies all the stan-
dard classifiers. None of these studies model period-
icities, although the former stresses their importance
for accurate predictions. For predicting the hashtag
given the tweet text, Mazzia and Juett (2011) uses
the Naive Bayes classifier with the uniform and em-
pirical prior or TF-IDF weighting.
</bodyText>
<sectionHeader confidence="0.997911" genericHeader="method">
3 Gaussian Processes
</sectionHeader>
<bodyText confidence="0.996515285714286">
In this paper we consider Gaussian Process (GP)
models of regression (Rasmussen and Williams,
2005). GP is a probabilistic machine learning
framework incorporating kernels and Bayesian non-
parametrics which is widely considered as state-of-
the-art for regression. The GP defines a prior over
functions which applied at each input point gives a
response value. Given data, we can analytically infer
the posterior distribution of these functions assum-
ing Gaussian noise. The kernel of the GP defines the
covariance in response values as a function of its in-
puts.
We can identify two different set-ups for a regres-
sion problem. If the range of values to be predicted
</bodyText>
<page confidence="0.99745">
978
</page>
<bodyText confidence="0.999934176470588">
lies within the bounds of the training set we call the
prediction task as interpolation. If the range of the
prediction is outside the bounds, then our problem
that of extrapolation. In this respect, extrapolation is
considered a more difficult task and the covariance
kernel which incorporates our prior knowledge plays
a major role in the prediction.
There is the case when multiple covariance ker-
nels can describe our data. For choosing the right
kernel and its hyperparameters only using the train-
ing data we employ Bayesian model selection which
makes a trade-off between the fit of the training
data and model complexity. We now briefly give an
overview of GP regression, kernel choice and model
selection. We refer the interested reader to (Ras-
mussen and Williams, 2005) for a detailed introduc-
tion to GPs.
</bodyText>
<subsectionHeader confidence="0.984754">
3.1 Gaussian Process Regression
</subsectionHeader>
<bodyText confidence="0.963850571428572">
Consider a time series regression task where we only
have one feature, the value xt at time t. Our training
data consists of n pairs D = {(t, xt)}. The model
will need to predict values xt for values of t greater
than those in the dataset.
GP regression assumes a latent function f that
is drawn from a GP prior f(t) — !9P(m, k(t, t&apos;))
where m is the mean and k a kernel. The predic-
tion value is obtained by the function evaluated at
the corresponding data point, xt = f(t) + c, where
E — N(0, σ2) is white-noise. The GP is defined by
the mean m, here 0, and the covariance kernel func-
tion, k(t, t&apos;).
The posterior at a test point t* is given by:
</bodyText>
<equation confidence="0.997397">
p(x*|t*,D) = if p(x* |t*, f ) -p(f|D) (1)
</equation>
<bodyText confidence="0.964841">
where x* and t* are the test value and time. The pos-
terior p(f|D) shows our belief over possible func-
tions after observing the training set D. The predic-
tive posterior can be solved analytically with solu-
tion:
where k* = [k(t*, t1)...k(t*, tn)]T are the kernel
evaluations between the test point and all the train-
ing points, K = {k(ti, tj)}i�1..n
</bodyText>
<footnote confidence="0.5963895">
j�1..n is the Gram matrix
3/1 4/1 5/1 6/1
</footnote>
<figureCaption confidence="0.98031425">
Figure 1: Interpolation for #goodmorning over 3 days
with SE and PS(p=24,s=3) kernels. Prediction variance
shown in grey for PS(24). Crosses represent training
points.
</figureCaption>
<bodyText confidence="0.999805">
over the training points and t is the vector of train-
ing points. The posterior of x* includes the mean
response as well as its variance, thus expressing the
uncertainty of the prediction. In this paper, we will
consider the forecast as the expected value. Due to
the matrix inversion in 2, inference takes O(n3) time
where n is the number of training points.
</bodyText>
<subsectionHeader confidence="0.998847">
3.2 Kernels
</subsectionHeader>
<bodyText confidence="0.999989">
The covariance kernel together with its parameters
fully define the GP (we assume 0 mean). The kernel
induces similarities in the response between pairs of
data points. Intuitively, if we want a smooth func-
tion, closer points should have high covariance com-
pared to points that are further apart. If we want a
periodic behaviour points at period length intervals
should have the highest covariance. Usually, this is
defined by an isotropic kernel, which means its in-
variant to all rigid motions.
For interpolation, a standard kernel (e.g. squared
exponential) that encourages smooth functions is
normally used. Figure 1 shows regression over 3
days for #goodmorning when only a random third
of the values of the function are observed. We see
that both the SE kernel and a periodic kernel (PS,
see below) give good results.
However, for extrapolation, the choice of the ker-
nel is paramount. The kernel encodes our prior belief
about the type of function wish to learn. To illustrate
this, in Figure 3, we show the time series for #good-
morning over 2 weeks and plot the regression for the
</bodyText>
<equation confidence="0.927722833333333">
Gold
SE
PS(24)
k(t*, t*) — kT* (K + σ2nI)-1k*)
(2)
x* — N(kT* (K + σ2nI)-1t,
</equation>
<page confidence="0.9876">
979
</page>
<bodyText confidence="0.997078733333333">
future week learned by using different kernels.
In this study we will use multiple kernels, each
most suitable for a specific category of temporal pat-
terns in our data. This includes a new kernel inspired
by observed word occurrence patterns. The kernels
we use are:
Constant (C): The constant kernel is kC(t, t0) =
c. Its mean prediction will always be the value c and
its assumption is that the signal is modeled only by
Gaussian noise centred around this value. This de-
scribes the data best when we have a noisy signal
around a stationary mean value.
Squared exponential (SE): The SE kernel or the
Radial Basis Function (RBF) is the standard kernel
used in most interpolation settings.
</bodyText>
<equation confidence="0.915471">
kSE(t, t0) = s2 · exp −(t − t0)2 (3) 2l2
</equation>
<bodyText confidence="0.976334357142857">
This gives a smooth transition between neighbour-
ing points and best describes time series with a
smooth shape e.g. a uni-modal burst with a steady
decrease. However, its uncertainty grows with for
predictions well into the future. Its two parameters
s and l are the characteristic lengthscales along the
two axes. Intuitively, they control the distance of in-
puts on a particular axis from which the function
values become uncorrelated. Using the SE kernel
corresponds to Bayesian linear regression with an
infinite number of basis functions (Rasmussen and
Williams, 2005).
Linear (Lin): The linear kernel describes a linear
relationship between outputs.
</bodyText>
<equation confidence="0.980717">
kLin(t, t0) = s2 + kt · t0k (4)
</equation>
<bodyText confidence="0.9392734">
This can be obtained from linear regression by hav-
ing N(0,1) priors on the corresponding regression
weights and a prior of N(0, s2) on the bias.
Periodic (PER): The periodic kernel represents a
SE kernel in polar coordinates.
</bodyText>
<equation confidence="0.893227333333333">
sin2(2π(t − t0)2/p) �
l2
(5)
</equation>
<bodyText confidence="0.99846375">
It has a sinusoidal shape and is good at modelling
periodically patterns that oscillate between low and
high frequency. s and l are characteristic length-
scales as in the SE kernel and p is the period.
</bodyText>
<figure confidence="0.992991125">
1
s=1
s=5
s=50
0.6
0.4
0.2
025 50
</figure>
<figureCaption confidence="0.9990505">
Figure 2: Behaviour of the PS kernel (p=50) with varying
s. Values normalized in [0,1] interval.
</figureCaption>
<bodyText confidence="0.999873857142857">
Periodic spikes (PS): For textual time series, like
word frequencies, we identify the following periodic
behaviour: abrupt rise in usage, usually with a peak,
followed by periods of low occurrence, which can
be short (e.g. during the night) or long lived (e.g. the
entire week except for a few hours). For modelling
we introduce the following kernel:
</bodyText>
<equation confidence="0.780022">
2π · (t − t0)2 ��
sin
kP S(t, t0) = cos
p
· exp s cos(2π · (t − t0)2)
p − s �
(6)
</equation>
<bodyText confidence="0.998725333333333">
The kernel is parameterised by its period p and a
shape parameter s. The period indicates the time in-
terval between the peaks of the function, while the
shape parameter controls the width of the spike. The
behaviour of the kernel is illustrated in Figure 2. We
constrains
</bodyText>
<listItem confidence="0.367341">
1.
</listItem>
<bodyText confidence="0.938468142857143">
In Figure 3 we see that the forecast is highly de-
pendent on the kernel choice. We expect that for
periodic data the PER and PS kernels will forecast
best, maybe with the PS kernel doing a better job
because it captures multiple modes of the daily in-
crease in volume. We use for both kernels a period
of 168 hours. This is because although a daily pat-
tern exists, the weekly is stronger, with the day of
the week influencing the volume of the hashtag. The
NRMSE (Normalized Root Mean
≥
Square Error) in
Table 1 on the held out data confirms this finding,
with PS showing the lowest error.
</bodyText>
<equation confidence="0.736924666666667">
kPER(t, t0) = s2 · exp −2 ·
0.8
75 100 125
</equation>
<page confidence="0.895907">
980
</page>
<figureCaption confidence="0.99135">
Figure 3: Extrapolation for #goodmorning over 3 weeks with GPs using different kernels.
</figureCaption>
<table confidence="0.996867666666667">
Const Lin SE PER PS
NLML -41 -34 -176 -180 -192
NRMSE 0.213 0.214 0.262 0.119 0.107
</table>
<tableCaption confidence="0.97572825">
Table 1: Negative Log Marginal Likelihood (NLML)
shows the best fitted model for the time series in Figure 3.
NRMSE computed on the third unobserved week. Lower
values are better in both cases.
</tableCaption>
<subsectionHeader confidence="0.998743">
3.3 Model selection and optimisation
</subsectionHeader>
<bodyText confidence="0.9991433">
We now briefly discuss the concepts of model se-
lection in the GP framework, by which we refer to
choosing the model (kernel) from a set hi and op-
timising the model hyperparameters θ. In our GP
Bayesian inference scheme, we can compute the
probability of the data given the model which in-
volves the integral over the parameter space. This
is called the marginal likelihood or evidence and is
useful for model selection using only the training
set:
</bodyText>
<equation confidence="0.962817">
p(x |D, θ, hi) = ifp(x|D, f, hi)p(f|θ,hi) (7)
</equation>
<bodyText confidence="0.99998512">
Our first goal is to fit the kernel by minimizing
the negative log marginal likelihood (NLML) with
respect to the kernel parameters θ. This approxima-
tion is also known as type II maximum likelihood
(ML-II). Conditioned on kernel parameters, the evi-
dence of a GP can be computed analytically.
Our second goal is to use the evidence for
model selection because it balances the data fit and
the model complexity by automatically incorporat-
ing Occam’s Razor (Rasmussen and Ghahramani,
2000). Because the evidence must normalise, com-
plex models which can account for many datasets
achieve low evidence. One can think of the evidence
as the probability that a random draw of the param-
eter values from the model class would generate the
dataset D. This way, complex models are penalised
because they can describe many datasets, while the
simple models can describe only a few datasets, thus
the chance of a good data fit being very low. This is
for example the case of the periodic bursts in Fig-
ure 3. Although the periodic kernel can fit the data,
it will incur a high model complexity penalty. The
PS kernel in this respect is a simpler model and can
fit the data and is thus chosen as the right model.
When the dataset is observed, the evidence can se-
lect between the models. More generally, the model
choice actually gives us an implicit classification of
the temporal patterns into classes: a steady signal
with noise (C kernel), a signal with local temporal
patterns (SE kernel), an oscilating periodic pattern
(PER kernel) or a pattern with abrupt periodic peaks
(PS kernel).
We use the NLML for optimising the hyperpa-
rameters only using training data. For optimising the
hyperparameters of the kernel defined in Equation 6,
it is important to first identify the right period. We
consider as possible periods all integer values less
than half the size of the training set, and then tune
the shape parameter using gradient descent to min-
imise NLML. We then take the argmin value of
those considered. We show the NLML for a sample
regression in Figure 4.
The likelihood shows that there are multiple
canyons in the likelihood, which can lead a convex
optimisation method to local optima. These appear
when p is equal or an integer multiple of the main
period of the data, in this case 24. The lowest val-
ues are obtained when p = 168, allowing the model
to accommodate the day of week effect. Our proce-
dure is not guaranteed to reach a global optima, but
</bodyText>
<page confidence="0.993096">
981
</page>
<figureCaption confidence="0.997852">
Figure 4: NLML for #goodmorning on the training set as
a function of the 2 kernel parameters.
</figureCaption>
<bodyText confidence="0.997658526315789">
is a relatively standard technique for fitting periodic
kernels (Duvenaud et al., 2013).
The flexibility of the GP framework allows us to
combine kernels (e.g. SE-PS or PS+Lin) in order
to identify a combination of trends (Duvenaud et al.,
2013; G¨onen and Alpaydin, 2011). Experiments on a
subset of data showed no major benefits of combin-
ing kernels, but the computational time and model
complexity increased drastically due to the extra hy-
perparameters. Because we will model a proportion
of words within a limited time frame, there are few
linear trends in the data. It might seem limiting that
we only learn a single period, although we could
combine periodic kernels with different periods to-
gether. But, as we have seen in the #goodmorning
example (with overlapping weekly and daily pat-
terns), if there is a combination of periods the model
will select a single period which is the least common
multiple.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999983296296296">
For our experiments we used data collected from
Twitter using the public Gardenhose stream (10%
representative sample of the entire Twitter stream).
The data collection interval was 1 January – 28
February 2011. For simplicity in the classification
task, we filtered the stream to include only tweets
that have exactly one hashtag. These represent ap-
proximately 7.8% of our stream.
As text processing steps, we have tokenised all the
tweets and filtered them to be written in English us-
ing the Trendminer pipeline (Preoiiuc-Pietro et al.,
2012). We also remove duplicate tweets (retweets
and tweets that had the same first 6 content tokens)
because they likely represent duplicate content, au-
tomated messages or spam which would bias the
dataset, as also stated by Tsur and Rappoport (2012).
In our experiments we use the first month of data
as training and the second month as testing. Note
the challenging nature of this testing configuration
where predictions must be made for up to 28 days
into the future. We keep a total 1176 of hashtags
which appear at least 500 times in both splits of
the data. The vocabulary consists of all the tokens
that occur more than 100 times in the dataset and
start with an alphabetic letter. After processing, our
dataset consists of 6,416,591 tweets with each hav-
ing on average 9.55 tokens.
</bodyText>
<sectionHeader confidence="0.990084" genericHeader="method">
5 Forecasting hashtag frequency
</sectionHeader>
<bodyText confidence="0.999992884615384">
We treat our task of forecasting the volume of a
Twitter hashtag as a regression problem. Because the
total number of tweets varies depending on the day
and hour of day, we chose to model the proportion of
tweets with the given tag in that hour. Given a time
series of these values as the training set for a hash-
tag, we aim to predict the values in the testing set,
extrapolating to the subsequent month.
Hashtags represent free-form text labels that au-
thors add to a tweet in order to enable other users to
search them to participate in a conversation. Some
users use hashtags as regular words that are integral
to the tweet text, some hashtags are general and re-
fer to the same thing or emotion (#news, #usa, #fail),
others are Twitter games or memes (#2010diss-
apointments, #musicmonday). Other hashtags re-
fer to events which might be short lived (#world-
cup2022), long lived (#25jan) or periodic (#raw,
#americanidol). We chose to model hashtags be-
cause they group similar tweets (like topics), reflect
real world events (some of which are periodic) and
present direct means of evaluation. Note that this
approach could be applied to many other temporal
problems in NLP or other domains. We treat each
regression problem independently, learning for each
hashtag its specific model and set of parameters.
</bodyText>
<subsectionHeader confidence="0.964859">
5.1 Methods
</subsectionHeader>
<bodyText confidence="0.9998185">
We choose multiple baselines for our prediction task
in order to compare the effectiveness of our ap-
</bodyText>
<figure confidence="0.969603222222222">
1
Shape s
40 80 120 160 200
Period p
0
NLML
−500
−1000
100
75
50
25
982
#fyi
#confessionhour
#fail
#breakfast
#raw
</figure>
<figureCaption confidence="0.985772">
Figure 5: Sample regressions and their fit using different methods.
</figureCaption>
<figure confidence="0.98748975">
1
0.5
0
03/1
</figure>
<figureCaption confidence="0.49883">
proach. These are:
</figureCaption>
<bodyText confidence="0.99811744">
Mean value (M): We use as prediction the mean
of the values in the training set. Note that this is the
same as using a GP model with a constant kernel (+
noise) with a mean equal to the training set mean.
Lag model with GP determined period (Lag+):
The prediction is the mean value in the training set of
the values at lag A where A is the period rounded to
the closest integer as determined by our GP model.
This is somewhat similar to an autoregressive (AR)
model with all the coefficients except A set to 0.
We highlight that given the period A this is a very
strong model as it gives a mean estimate at each
point. Comparing to this model we can see if the GP
model can recover the underlying function that de-
scribed the periodic variation and filter out the noise
in the observations. Correctly identifying the period
is very challenging as we discuss below.
GP regression: Gaussian Process regression us-
ing only the SE kernel (GP-SE), the periodic ker-
nel (GP-PER), the PS kernel (GP-PS). The method
that chooses between kernels using model selection
as described in Section 3.3 is denoted as GP+. We
will also compare to GP regression the linear kernel
(GP-Lin), but we will not use this as a candidate for
model selection due the poor results shown below.
</bodyText>
<page confidence="0.997782">
983
</page>
<table confidence="0.999749">
Hashtag Lag(p) Const NLML SE PER NLML PS
NRMSE NLML NRMSE NRMSE NLML NRMSE NRMSE
#fyi 0.1578 -322 0.1404 -320 0.1898 -321 0.1405 -293 0.1456
#confessionhour 0.0404 -85 0.0107 -186 0.0012 -90 0.0327 -88 0.0440
#fail 0.1431 -376 0.1473 -395 0.4695 -444 0.1387 -424 0.1390
#breakfast 0.1363 -293 0.1508 -333 0.1773 -293 0.1514 -367 0.1276
#raw 0.0464 -1208 0.0863 -1208 0.0863 -1323 0.0668 -1412 0.0454
</table>
<tableCaption confidence="0.9354455">
Table 2: NRMSE shows the best performance for forecasting and NLML shows the best model for all the regressions
in Figure 5. Lower is better.
</tableCaption>
<subsectionHeader confidence="0.88429">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.99996188">
We start by qualitatively analysing a few sample re-
gressions that are representative of each category of
time series under study. These are shown in Figure 5.
For clarity, we only plotted a few kernels on each fig-
ure. The full evaluation statistics in NRMSE and the
Bayesian evidence are show in Table 2.
For the hashtag #fyi there is no clear pattern. For
this reason the model that uses the constant kernel
performs best, being the simplest one that can de-
scribe the data, although the others give similar re-
sults in terms of NRMSE on the held-out testing
set. While functions learned using this kernel never
clearly outperform others on NRMSE on held-out
data, this is very useful for interpretation of the time
series, separating noisy time series from those that
have an underlying periodic behaviour.
The #confessionhour example illustrates a be-
haviour best suited for modelling using the SE ker-
nel. We notice a sudden burst in volume which
decays over the next 2 days. This is actually
the behaviour typical of ‘internet memes’ (this
hashtag tags tweets of people posting things they
would never tell anyone) as presented in Yang and
Leskovec (2011). These cannot be modelled with a
constant kernel or a periodic one as shown by the re-
sults on held-out data and the time series plot. The
periodic kernels will fail in trying to match the large
burst with others in the training data and will at-
tribute to noise the lack of a similar peak, thus dis-
covering wrong periods and making bad predictions.
In this example, forecasts will be very close to 0 un-
der the SE kernel, which is what we would desire
from the model.
The periodic kernel best models hashtags that ex-
hibit an oscillating pattern. For example, this best
fits words that are used frequently during the day
and less so during the night, like #fail. Here, the pe-
riod is chosen to be one week (168) rather than one
day (24) because of the weekly effect superimposed
on the daily one. Our model recovers that there is
a daily pattern with people tweeting about their or
others’ failures during the day. On weekends how-
ever, and especially on Friday evenings, people have
better things to do.
The PS kernel introduced in this paper models
best hashtags that have a large and short lived burst
in usage. We show this by two examples. First, we
choose #breakfast which has a daily and weekly pat-
tern. As we would expect, a big rise in usage oc-
curs during the early hours of the day, with very
few occurrences at other times. Our model discov-
ers a weekly pattern as well. This is used mainly
for modelling the difference between weekends and
weekdays. On weekends, the breakfast tag is more
evenly spread during the hours of the morning, be-
cause people do not have to wake up for work and
can have breakfast at a more flexible time than dur-
ing the week. In the second example, we present a
hashtag that is associated to a weekly event: #raw
is used to discuss a wrestling show that airs ev-
ery week for 2 hours on Monday evenings in the
U.S.. With the exception of these 2 hours and the
hour building up to it, the hashtag is rarely used.
This behaviour is modelled very well using our ker-
nel, with a very high value for the shape parame-
ter (s = 200) compared to the previous example
(s = 11) which captures the abrupt trend in usage. In
all cases, our GP model chosen by the evidence per-
forms better than the Lag+ model, which is a very
strong method if presented with the correct period.
This further demonstrates the power of the Gaussian
Process framework to deal with noise in the training
data and to find the underlying function of the time
variation of words.
In Table 3 we present sample tags identified as
</bodyText>
<page confidence="0.994709">
984
</page>
<table confidence="0.953207076923077">
Const SE PER PS
#funny #2011 #brb #ff
#lego #backintheday #coffee #followfriday
#likeaboss #confessionhour #facebook #goodnight
#money #februarywish #facepalm #jobs
#nbd #haiti #funny #news
#nf #makeachange #love #nowplaying
#notetoself #questionsidontlike #rock #tgif
#priorities #savelibraries #running #twitterafterdark
#social #snow #xbox #twitteroff
#true #snowday #youtube #ww
49 268 493 366
Sample Autocorrelation Function
</table>
<figure confidence="0.951526">
0.8
0.6
0.4
0.2
0
Sample Autocorrelation
</figure>
<tableCaption confidence="0.9752545">
Table 3: Sample hashtags for each category. The last line
shows the total number of hashtags of each type.
</tableCaption>
<table confidence="0.9982915">
Lag+ GP-Lin GP-SE GP-PER GP-PS GP+
7.29% -3.99% -34.5% 0.22% 7.37% 9.22%
</table>
<tableCaption confidence="0.89731">
Table 4: Average relative gain over mean (M) prediction
for forecasting on the entire month using the different
models
</tableCaption>
<bodyText confidence="0.999938333333333">
being part of the 4 hashtag categories, and the total
number of hashtags in each.
As a means of quantitative evaluation we com-
pute the relative NRMSE compared to the Mean (M)
method for forecasting. We choose this, because we
consider that NRMSE is not comparable between re-
gression tasks due to the presence of large peaks in
many time series, which distort the NRMSE values.
The results are presented in Table 4 and show that
our Gaussian Process model using model selection
is best. Remarkably, it consistently outperforms the
Lag+ model, which shows the effectiveness of the
GP models to incorporate uncertainty. The GP-PS
model does very well on its own. Although chosen in
the model selection phase in only a third of the tasks,
it performs consistently well across tasks because of
its ability to model well all the periodic hashtags,
be they smooth or abrupt. The GP-Lin model does
worse than the average, mostly due to uni-modal
time series which don’t have high occurrences in the
testing part of the data.
</bodyText>
<subsectionHeader confidence="0.993217">
5.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999724125">
Let us now turn to why the GP model is better
for discovering periodicities than classic time series
modelling methods. Measuring autocorrelation be-
tween points in the time series is used to discover
the hidden periodicities in the data and in building
AR models. However, the downsides of this method
are: a) the incapacity of accurately finding the cor-
rect periods, because all integer multiples of the cor-
</bodyText>
<figure confidence="0.8273455">
0.2 0 100 200 300
Lag
</figure>
<figureCaption confidence="0.992741">
Figure 6: Sample autocorrelation for #confessionhour
</figureCaption>
<figure confidence="0.991349375">
Periodogram Power Spectral Density Estimate
−10
−20
−30
−40
−50
−60
70
</figure>
<figureCaption confidence="0.999948">
Figure 7: Power spectral density for #raw
</figureCaption>
<bodyText confidence="0.999791681818182">
rect period will be feasible candidates and b) it leads
to incorrect conclusions when there is autocorre-
lated noise. The second case is illustrated in Fig-
ure 6 where #confessionhour shows autocorrelation
but, as seen in Figure 5, lacks a periodic component.
Another approach to discovering periods in data
is by computing the power spectral density. This
has been used in the GP framework by Wilson and
Adams (2013). For some time series, this gives a
good indication of the period, as represented by a
peak in the periodogram at that value. This fails to
discover the correct period when dealing with large
bursts like those exhibited by the #raw time series
as shown in Figure 7. The lowest frequency spike
corresponds to the correct period of 168, but also
other candidate periods are shown as possible. The
reason for this is its reliance on the Fourier Trans-
form which decomposes the time series into a sum
of oscillating patterns. These cannot model step-
functions and other non-smoothly varying signals.
A further discussion falls out of the scope and space
constraints of this paper.
</bodyText>
<figure confidence="0.448053">
Power/frequency (dB/rad/sample)
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Normalized Frequency (×π rad/sample)
</figure>
<page confidence="0.587799">
985
</page>
<table confidence="0.997651571428571">
Tweet Time Prior Rank Prediction
Bruins Goal!!! Patrice Bergeron makes it 3-1 Boston 2-3am, 2 Feb 2011 E: 0.00017 7 #fb
P: 0.00086 1 #bruins
i need some of Malik people 3-4am, 2 Feb 2011 E: 0.00021 7 #ff
P: 0.00420 1 #thegame
Alfie u doughnut! U didn’t confront Kay? SMH 7-8pm, 3 Feb 2011 E: 0.00027 8 #nowplaying
P: 0.00360 1 #eastenders
</table>
<tableCaption confidence="0.9815765">
Table 5: Example of tweet classification using the Naive Bayes model with the two different priors (E - empirical, P -
GP forecast). Rank shows the rank in probability of the correct class (hashtag) under the model. Time is G.M.T.
</tableCaption>
<sectionHeader confidence="0.979758" genericHeader="method">
6 Text based prediction
</sectionHeader>
<bodyText confidence="0.999950388888889">
In this section we demonstrate the usefulness of
our method of modelling in an NLP task: predict-
ing the hashtag of a tweet based on its text. In con-
trast to this classification approach for suggesting a
tweet’s hashtag, information retrieval methods based
on computing similarities between tweets are very
hard to scale to large data (Zangerle et al., 2011).
We choose a simple model for prediction, the
Naive Bayes Classifier. This method provides us
with a straightforward way to incorporate our prior
knowledge of how frequent a hashtag is in a certain
time frame. This Naive Bayes model (NB-P) uses
the forecasted values for the respective hour as the
prior on the hashtags.
For comparison we use the Most Frequent (MF)
baseline and the Naive Bayes with empirical prior
(NB-E) which doesn’t use any temporal forecasting
information. Because there are more than 1000 pos-
sible classes we show the accuracy of the correct
hashtag being amongst the top 1,5 or 50 hashtags
as well as the Mean Reciprocal Rank (MRR). The
results are shown in Table 6.
The results show that incorporating the forecasted
values as a more informative prior for classification
we obtain better predictions. The improvements are
consistent in all the Match values. Also, we high-
light that a 9% improvement in the forecasting task
carries over to about a 2% improvement in classifi-
cation. We show a few examples in which the GP
learned prior makes a difference in classification in
Table 5 together with the values for both priors.
With these experiments, we highlighted that there
are performance gains even with only adding a more
informative prior that uses periodicity information.
This motivates future work to add this information
to discriminative classifiers thus avoiding the need
</bodyText>
<table confidence="0.97456">
MF NB-E NB-P
Match@1 7.28% 16.04% 17.39%
Match@5 19.90% 29.51% 31.91%
Match@50 44.92% 59.17% 60.85%
MRR 0.144 0.237 0.252
</table>
<tableCaption confidence="0.995905">
Table 6: Results for hashtag classification.
</tableCaption>
<bodyText confidence="0.999924428571428">
for the Naive Bayes decomposition. The modelling
framework offered by the GPs can accommodate
classification, although scaling issues arise when us-
ing a large number of features or output classes. Ef-
forts to scale GPs to a large number of variables
are well understood (Candela and Rasmussen, 2005)
and we will try to incorporate this in future work.
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999938642857143">
Periodicities play an important role when analysing
the temporal dimension of text. We have presented
a framework based on Gaussian Process regression
for identifying periodic patterns and their parame-
ters using only training data. We divided the periodic
patterns into 2 categories: oscillating and periodic
bursts by performing model selection using bayesian
evidence. The periodicities we have discovered have
proven useful in an NLP classification task.
In future work, we aim to model time continu-
ously and to perform discriminative clustering in or-
der to make better use of the learned periodicites.
We will consider incorporating periodicities in other
applications, such as topic models.
</bodyText>
<sectionHeader confidence="0.997751" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999916166666667">
This research was funded by the Trendminer
project, EU FP7-ICT Programme, grant agreement
no.287863. The authors would like to thank James
Hensman, Nicolas Durrande and Neil Lawrence for
advice on Gaussian Processes, Chris Dyer and Noah
Smith for discussions about periodicities in NLP.
</bodyText>
<page confidence="0.996479">
986
</page>
<sectionHeader confidence="0.996239" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999719173076923">
David Blei and John Lafferty. 2006. Dynamic topic
models. In Proceedings of the 23rd International con-
ference on Machine learning, ICML ’06.
Joaquin Qui˜nonero Candela and Carl Edward Ras-
mussen. 2005. A Unifying View of Sparse Approx-
imate Gaussian Process Regression. Journal of Ma-
chine Learning Research (JMLR), 6:1939–1959, De-
cember.
Trevor Cohn and Lucia Specia. 2013. Modelling An-
notator Bias with Multi-task Gaussian Processes: An
Application to Machine Translation Quality Estima-
tion. In Proceedings of the Association of Computa-
tional Linguistics, ACL ’13.
Nicolas Durrande, James Hensman, Magnus Rattray, and
Neil Lawrence. 2013. Gaussian Process models
for periodicity detection. In Submitted to JRSSb,
http://arxiv.org/abs/1303.7090.
David Duvenaud, James Robert Lloyd, Roger Grosse,
Joshua B. Tenenbaum, and Zoubin Ghahramani.
2013. Structure discovery in nonparametric regression
through compositional kernel search. In Proceedings
of the International Conference on Machine Learning,
ICML ’13.
Mehmet G¨onen and Ethem Alpaydin. 2011. Multi-
ple Kernel Learning Algorithms. Journal of Machine
Learning Research (JMLR), 12:2211–2268, July.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America,
(Suppl 1):5228–5235, April.
Philipp Hennig, David H. Stern, Ralf Herbrich, and Thore
Graepel. 2012. Kernel topic models. Journal of Ma-
chine Learning Research (JMLR) - Proceedings Track,
22:511–519.
Jure Leskovec, Lars Backstrom, and Jon Kleinberg.
2009. Meme-tracking and the dynamics of the news
cycle. In Proceedings of the 15th ACM SIGKDD Inter-
national conference on Knowledge discovery and data
mining, KDD ’09.
Zongyang Ma, Aixin Sun, and Gao Cong. 2012. Will
this #hashtag be popular tomorrow? In Proceedings of
the 35th International ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ’12.
Zongyang Ma, Aixin Sun, and Gao Cong. 2013. On pre-
dicting the popularity of newly emerging hashtags in
Twitter. Journal of the American Society for Informa-
tion Science and Technology, 64(7):1399–1410.
Allie Mazzia and James Juett. 2011. Sug-
gesting hashtags on Twitter. In http://www-
personal.umich.edu/ amazzia/pubs/545-inal.pdf.
James McInerney, Alex Rogers, and Nicholas R Jennings.
2013. Learning periodic human behaviour models
from sparse data for crowdsourcing aid delivery in
developing countries. In Proceedings of the Twenty-
Ninth Conference on Uncertainty in Artiicial Intelli-
gence, UAI ’13.
Tamara Polajnar, Simon Rogers, and Mark Girolami.
2011. Protein interaction detection in sentences via
Gaussian Processes: a preliminary evaluation. In-
ternational Journal Data Mining and Bioinformatics,
5(1):52–72, February.
Daniel Preot¸iuc-Pietro and Trevor Cohn. 2013. Mining
User Behaviours: A Study of Check-in Patterns in Lo-
cation Based Social Networks. In Proceedings of the
ACM Web Science Conference, Web Science ’13.
Daniel Preot¸iuc-Pietro, Sina Samangooei, Trevor Cohn,
Nicholas Gibbins, and Mahesan Niranjan. 2012.
Trendminer: An architecture for real time analysis of
social media text. Proceedings of the Sixth Interna-
tional AAAI Conference on Weblogs and Social Media,
Workshop on Real-Time Analysis and Mining of Social
Streams.
Carl Edward Rasmussen and Zoubin Ghahramani. 2000.
Occam’s razor. In Advances in Neural Information
Processing Systems, NIPS 13.
Carl Edward Rasmussen and Christopher K. I. Williams.
2005. Gaussian Processes for Machine Learning.
MIT Press.
Daniel M. Romero, Brendan Meeder, and Jon Klein-
berg. 2011. Differences in the mechanics of informa-
tion diffusion across topics: idioms, political hashtags,
and complex contagion on Twitter. In Proceedings of
the 20th International conference on World wide web,
WWW ’11.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013.
An Investigation on the Effectiveness of Features for
Translation Quality Estimation. In MT Summit ’13.
Oren Tsur and Ari Rappoport. 2012. What’s in a hash-
tag? Content based prediction of the spread of ideas
in microblogging communities. In Proceedings of the
ifth ACM International conference on Web search and
data mining, WSDM ’12.
Xuerui Wang and Andrew McCallum. 2006. Topics over
time: a non-Markov continuous-time model of topi-
cal trends. In Proceedings of the 12th ACM SIGKDD
International conference on Knowledge discovery and
data mining, KDD ’06.
Chong Wang, David M. Blei, and David Heckerman.
2008. Continuous time Dynamic topic models. In
Proceedings of the Twenty-Fourth Conference on Un-
certainty in Artiicial Intelligence, UAI ’08.
Andrew Gordon Wilson and Ryan Prescott Adams. 2013.
Gaussian Process covariance kernels for pattern dis-
</reference>
<page confidence="0.978147">
987
</page>
<reference confidence="0.99966045">
covery and extrapolation. In Proceedings of the Inter-
national Conference on Machine Learning, ICML ’13.
Jaewon Yang and Jure Leskovec. 2011. Patterns of tem-
poral variation in online media. In Proceedings of the
fourth ACM International conference on Web search
and data mining, WSDM ’11.
Lei Yang, Tao Sun, Ming Zhang, and Qiaozhu Mei. 2012.
We know what @you #tag: does the dual role affect
hashtag adoption? In Proceedings of the 21st Interna-
tional conference on World Wide Web, WWW ’12.
Dani Yogatama, Michael Heilman, Brendan O’Connor,
Chris Dyer, Bryan R. Routledge, and Noah A. Smith.
2011. Predicting a scientific community’s response
to an article. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ’11.
Eva Zangerle, Wolfgang Gassler, and Gunther Specht.
2011. Recommending #-tags in twitter. In Proceed-
ings of the Workshop on Semantic Adaptive Social
Web, UMAP ’11.
</reference>
<page confidence="0.997615">
988
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.482555">
<title confidence="0.999765">A temporal model of text periodicities using Gaussian Processes</title>
<author confidence="0.995586">Trevor</author>
<affiliation confidence="0.994273">Department of Computer University of</affiliation>
<address confidence="0.648267">Regent Court, 211 Portobello Sheffield, S1 4DP, United</address>
<abstract confidence="0.999529647058823">Temporal variations of text are usually ignored in NLP applications. However, text use changes with time, which can affect many applications. In this paper we model periodic distributions of words over time. Focusing on hashtag frequency in Twitter, we first automatically identify the periodic patterns. We use this for regression in order to forecast the volume of a hashtag based on past data. We use Gaussian Processes, a state-ofthe-art bayesian non-parametric model, with a novel periodic kernel. We demonstrate this in a text classification setting, assigning the tweet hashtag based on the rest of its text. This method shows significant improvements over competitive baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>John Lafferty</author>
</authors>
<title>Dynamic topic models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd International conference on Machine learning, ICML ’06.</booktitle>
<contexts>
<context position="5684" citStr="Blei and Lafferty, 2006" startWordPosition="889" endWordPosition="892">he first paper to use GP regression for forecasting and model selection within a NLP task. All the hashtag time series data and the implementation of the PS kernel in the popular opensource Gaussian Processes packages GPML1 and GPy2 are available on the author’s website3. 2 Related Work Time varying text patterns have been of particular interest in topic modelling. Griffiths and Steyvers (2004) analyse evolution of topics over time, but without modelling time explicitly. Extensions that model time make different assumptions, usually regarding smoothing proprieties in (Wang and McCallum, 2006; Blei and Lafferty, 2006; Wang et al., 2008; Hennig et al., 2012). Yogatama et al. (2011) proposed a regulariser for generalised linear models that encourages local temporal smoothness. 1http://www.gaussianprocess.org/gpml/ code 2https://github.com/SheffieldML/GPy 3http://www.preotiuc.ro Modelling periodicities is one of the standard applications of Gaussian Processes (Rasmussen and Williams, 2005). Recent work by Wilson and Adams (2013) and Durrande et al. (2013) show how different periods can be identified from data. In general, methods that assume certain periodicities at daily or weekly levels were proposed e.g. </context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>David Blei and John Lafferty. 2006. Dynamic topic models. In Proceedings of the 23rd International conference on Machine learning, ICML ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joaquin Qui˜nonero Candela</author>
<author>Carl Edward Rasmussen</author>
</authors>
<title>A Unifying View of Sparse Approximate Gaussian Process Regression.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>6--1939</pages>
<contexts>
<context position="34424" citStr="Candela and Rasmussen, 2005" startWordPosition="5828" endWordPosition="5831"> more informative prior that uses periodicity information. This motivates future work to add this information to discriminative classifiers thus avoiding the need MF NB-E NB-P Match@1 7.28% 16.04% 17.39% Match@5 19.90% 29.51% 31.91% Match@50 44.92% 59.17% 60.85% MRR 0.144 0.237 0.252 Table 6: Results for hashtag classification. for the Naive Bayes decomposition. The modelling framework offered by the GPs can accommodate classification, although scaling issues arise when using a large number of features or output classes. Efforts to scale GPs to a large number of variables are well understood (Candela and Rasmussen, 2005) and we will try to incorporate this in future work. 7 Conclusion Periodicities play an important role when analysing the temporal dimension of text. We have presented a framework based on Gaussian Process regression for identifying periodic patterns and their parameters using only training data. We divided the periodic patterns into 2 categories: oscillating and periodic bursts by performing model selection using bayesian evidence. The periodicities we have discovered have proven useful in an NLP classification task. In future work, we aim to model time continuously and to perform discriminat</context>
</contexts>
<marker>Candela, Rasmussen, 2005</marker>
<rawString>Joaquin Qui˜nonero Candela and Carl Edward Rasmussen. 2005. A Unifying View of Sparse Approximate Gaussian Process Regression. Journal of Machine Learning Research (JMLR), 6:1939–1959, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Association of Computational Linguistics, ACL ’13.</booktitle>
<contexts>
<context position="6426" citStr="Cohn and Specia, 2013" startWordPosition="995" endWordPosition="998">at encourages local temporal smoothness. 1http://www.gaussianprocess.org/gpml/ code 2https://github.com/SheffieldML/GPy 3http://www.preotiuc.ro Modelling periodicities is one of the standard applications of Gaussian Processes (Rasmussen and Williams, 2005). Recent work by Wilson and Adams (2013) and Durrande et al. (2013) show how different periods can be identified from data. In general, methods that assume certain periodicities at daily or weekly levels were proposed e.g. in (McInerney et al., 2013). GPs were used with text by Polajnar et al. (2011) and for Quality Estimation regression in (Cohn and Specia, 2013; Shah et al., 2013). Temporal patterns for short, distinctive lexical items such as hashtags and memes were quantitatively studied (Leskovec et al., 2009) and clustered (Yang and Leskovec, 2011) in Social Media. (Yang et al., 2012) studies the dual role of hashtags, of bookmarks of content and symbols of community membership, in the context of hashtag adoption. (Romero et al., 2011) analyses the patterns of temporal diffusion in Social Media finding that hashtags have also a persistence factor. For predicting future popularity of hashtags, Tsur and Rappoport (2012) use linear regression with </context>
</contexts>
<marker>Cohn, Specia, 2013</marker>
<rawString>Trevor Cohn and Lucia Specia. 2013. Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation. In Proceedings of the Association of Computational Linguistics, ACL ’13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolas Durrande</author>
<author>James Hensman</author>
<author>Magnus Rattray</author>
<author>Neil Lawrence</author>
</authors>
<title>Gaussian Process models for periodicity detection.</title>
<date>2013</date>
<note>In Submitted to JRSSb, http://arxiv.org/abs/1303.7090.</note>
<contexts>
<context position="6128" citStr="Durrande et al. (2013)" startWordPosition="945" endWordPosition="948">out modelling time explicitly. Extensions that model time make different assumptions, usually regarding smoothing proprieties in (Wang and McCallum, 2006; Blei and Lafferty, 2006; Wang et al., 2008; Hennig et al., 2012). Yogatama et al. (2011) proposed a regulariser for generalised linear models that encourages local temporal smoothness. 1http://www.gaussianprocess.org/gpml/ code 2https://github.com/SheffieldML/GPy 3http://www.preotiuc.ro Modelling periodicities is one of the standard applications of Gaussian Processes (Rasmussen and Williams, 2005). Recent work by Wilson and Adams (2013) and Durrande et al. (2013) show how different periods can be identified from data. In general, methods that assume certain periodicities at daily or weekly levels were proposed e.g. in (McInerney et al., 2013). GPs were used with text by Polajnar et al. (2011) and for Quality Estimation regression in (Cohn and Specia, 2013; Shah et al., 2013). Temporal patterns for short, distinctive lexical items such as hashtags and memes were quantitatively studied (Leskovec et al., 2009) and clustered (Yang and Leskovec, 2011) in Social Media. (Yang et al., 2012) studies the dual role of hashtags, of bookmarks of content and symbol</context>
</contexts>
<marker>Durrande, Hensman, Rattray, Lawrence, 2013</marker>
<rawString>Nicolas Durrande, James Hensman, Magnus Rattray, and Neil Lawrence. 2013. Gaussian Process models for periodicity detection. In Submitted to JRSSb, http://arxiv.org/abs/1303.7090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Duvenaud</author>
<author>James Robert Lloyd</author>
<author>Roger Grosse</author>
<author>Joshua B Tenenbaum</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Structure discovery in nonparametric regression through compositional kernel search.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference on Machine Learning, ICML ’13.</booktitle>
<contexts>
<context position="18673" citStr="Duvenaud et al., 2013" startWordPosition="3132" endWordPosition="3135"> for a sample regression in Figure 4. The likelihood shows that there are multiple canyons in the likelihood, which can lead a convex optimisation method to local optima. These appear when p is equal or an integer multiple of the main period of the data, in this case 24. The lowest values are obtained when p = 168, allowing the model to accommodate the day of week effect. Our procedure is not guaranteed to reach a global optima, but 981 Figure 4: NLML for #goodmorning on the training set as a function of the 2 kernel parameters. is a relatively standard technique for fitting periodic kernels (Duvenaud et al., 2013). The flexibility of the GP framework allows us to combine kernels (e.g. SE-PS or PS+Lin) in order to identify a combination of trends (Duvenaud et al., 2013; G¨onen and Alpaydin, 2011). Experiments on a subset of data showed no major benefits of combining kernels, but the computational time and model complexity increased drastically due to the extra hyperparameters. Because we will model a proportion of words within a limited time frame, there are few linear trends in the data. It might seem limiting that we only learn a single period, although we could combine periodic kernels with different</context>
</contexts>
<marker>Duvenaud, Lloyd, Grosse, Tenenbaum, Ghahramani, 2013</marker>
<rawString>David Duvenaud, James Robert Lloyd, Roger Grosse, Joshua B. Tenenbaum, and Zoubin Ghahramani. 2013. Structure discovery in nonparametric regression through compositional kernel search. In Proceedings of the International Conference on Machine Learning, ICML ’13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehmet G¨onen</author>
<author>Ethem Alpaydin</author>
</authors>
<title>Multiple Kernel Learning Algorithms.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>12--2211</pages>
<marker>G¨onen, Alpaydin, 2011</marker>
<rawString>Mehmet G¨onen and Ethem Alpaydin. 2011. Multiple Kernel Learning Algorithms. Journal of Machine Learning Research (JMLR), 12:2211–2268, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America, (Suppl 1):5228–5235,</booktitle>
<contexts>
<context position="5458" citStr="Griffiths and Steyvers (2004)" startWordPosition="856" endWordPosition="859">pproach extends to more general uses, e.g. to discriminative text regression and classification. More broadly, we aim to establish GPs as a state-of-the-art model for regression and classification in NLP. To our knowledge, this is the first paper to use GP regression for forecasting and model selection within a NLP task. All the hashtag time series data and the implementation of the PS kernel in the popular opensource Gaussian Processes packages GPML1 and GPy2 are available on the author’s website3. 2 Related Work Time varying text patterns have been of particular interest in topic modelling. Griffiths and Steyvers (2004) analyse evolution of topics over time, but without modelling time explicitly. Extensions that model time make different assumptions, usually regarding smoothing proprieties in (Wang and McCallum, 2006; Blei and Lafferty, 2006; Wang et al., 2008; Hennig et al., 2012). Yogatama et al. (2011) proposed a regulariser for generalised linear models that encourages local temporal smoothness. 1http://www.gaussianprocess.org/gpml/ code 2https://github.com/SheffieldML/GPy 3http://www.preotiuc.ro Modelling periodicities is one of the standard applications of Gaussian Processes (Rasmussen and Williams, 20</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences of the United States of America, (Suppl 1):5228–5235, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Hennig</author>
<author>David H Stern</author>
<author>Ralf Herbrich</author>
<author>Thore Graepel</author>
</authors>
<title>Kernel topic models.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research (JMLR) - Proceedings Track,</journal>
<pages>22--511</pages>
<contexts>
<context position="5725" citStr="Hennig et al., 2012" startWordPosition="897" endWordPosition="900">casting and model selection within a NLP task. All the hashtag time series data and the implementation of the PS kernel in the popular opensource Gaussian Processes packages GPML1 and GPy2 are available on the author’s website3. 2 Related Work Time varying text patterns have been of particular interest in topic modelling. Griffiths and Steyvers (2004) analyse evolution of topics over time, but without modelling time explicitly. Extensions that model time make different assumptions, usually regarding smoothing proprieties in (Wang and McCallum, 2006; Blei and Lafferty, 2006; Wang et al., 2008; Hennig et al., 2012). Yogatama et al. (2011) proposed a regulariser for generalised linear models that encourages local temporal smoothness. 1http://www.gaussianprocess.org/gpml/ code 2https://github.com/SheffieldML/GPy 3http://www.preotiuc.ro Modelling periodicities is one of the standard applications of Gaussian Processes (Rasmussen and Williams, 2005). Recent work by Wilson and Adams (2013) and Durrande et al. (2013) show how different periods can be identified from data. In general, methods that assume certain periodicities at daily or weekly levels were proposed e.g. in (McInerney et al., 2013). GPs were use</context>
</contexts>
<marker>Hennig, Stern, Herbrich, Graepel, 2012</marker>
<rawString>Philipp Hennig, David H. Stern, Ralf Herbrich, and Thore Graepel. 2012. Kernel topic models. Journal of Machine Learning Research (JMLR) - Proceedings Track, 22:511–519.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jure Leskovec</author>
<author>Lars Backstrom</author>
<author>Jon Kleinberg</author>
</authors>
<title>Meme-tracking and the dynamics of the news cycle.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM SIGKDD International conference on Knowledge discovery and data mining, KDD ’09.</booktitle>
<contexts>
<context position="6581" citStr="Leskovec et al., 2009" startWordPosition="1019" endWordPosition="1022">periodicities is one of the standard applications of Gaussian Processes (Rasmussen and Williams, 2005). Recent work by Wilson and Adams (2013) and Durrande et al. (2013) show how different periods can be identified from data. In general, methods that assume certain periodicities at daily or weekly levels were proposed e.g. in (McInerney et al., 2013). GPs were used with text by Polajnar et al. (2011) and for Quality Estimation regression in (Cohn and Specia, 2013; Shah et al., 2013). Temporal patterns for short, distinctive lexical items such as hashtags and memes were quantitatively studied (Leskovec et al., 2009) and clustered (Yang and Leskovec, 2011) in Social Media. (Yang et al., 2012) studies the dual role of hashtags, of bookmarks of content and symbols of community membership, in the context of hashtag adoption. (Romero et al., 2011) analyses the patterns of temporal diffusion in Social Media finding that hashtags have also a persistence factor. For predicting future popularity of hashtags, Tsur and Rappoport (2012) use linear regression with a wide range of features. (Ma et al., 2012; Ma et al., 2013) frame the problem as classification into a number of fixed intervals and applies all the stand</context>
</contexts>
<marker>Leskovec, Backstrom, Kleinberg, 2009</marker>
<rawString>Jure Leskovec, Lars Backstrom, and Jon Kleinberg. 2009. Meme-tracking and the dynamics of the news cycle. In Proceedings of the 15th ACM SIGKDD International conference on Knowledge discovery and data mining, KDD ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zongyang Ma</author>
<author>Aixin Sun</author>
<author>Gao Cong</author>
</authors>
<title>Will this #hashtag be popular tomorrow?</title>
<date>2012</date>
<booktitle>In Proceedings of the 35th International ACM SIGIR conference on Research and development in information retrieval, SIGIR ’12.</booktitle>
<contexts>
<context position="7068" citStr="Ma et al., 2012" startWordPosition="1100" endWordPosition="1103">ral patterns for short, distinctive lexical items such as hashtags and memes were quantitatively studied (Leskovec et al., 2009) and clustered (Yang and Leskovec, 2011) in Social Media. (Yang et al., 2012) studies the dual role of hashtags, of bookmarks of content and symbols of community membership, in the context of hashtag adoption. (Romero et al., 2011) analyses the patterns of temporal diffusion in Social Media finding that hashtags have also a persistence factor. For predicting future popularity of hashtags, Tsur and Rappoport (2012) use linear regression with a wide range of features. (Ma et al., 2012; Ma et al., 2013) frame the problem as classification into a number of fixed intervals and applies all the standard classifiers. None of these studies model periodicities, although the former stresses their importance for accurate predictions. For predicting the hashtag given the tweet text, Mazzia and Juett (2011) uses the Naive Bayes classifier with the uniform and empirical prior or TF-IDF weighting. 3 Gaussian Processes In this paper we consider Gaussian Process (GP) models of regression (Rasmussen and Williams, 2005). GP is a probabilistic machine learning framework incorporating kernels</context>
</contexts>
<marker>Ma, Sun, Cong, 2012</marker>
<rawString>Zongyang Ma, Aixin Sun, and Gao Cong. 2012. Will this #hashtag be popular tomorrow? In Proceedings of the 35th International ACM SIGIR conference on Research and development in information retrieval, SIGIR ’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zongyang Ma</author>
<author>Aixin Sun</author>
<author>Gao Cong</author>
</authors>
<title>On predicting the popularity of newly emerging hashtags in Twitter.</title>
<date>2013</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>64</volume>
<issue>7</issue>
<contexts>
<context position="7086" citStr="Ma et al., 2013" startWordPosition="1104" endWordPosition="1107">short, distinctive lexical items such as hashtags and memes were quantitatively studied (Leskovec et al., 2009) and clustered (Yang and Leskovec, 2011) in Social Media. (Yang et al., 2012) studies the dual role of hashtags, of bookmarks of content and symbols of community membership, in the context of hashtag adoption. (Romero et al., 2011) analyses the patterns of temporal diffusion in Social Media finding that hashtags have also a persistence factor. For predicting future popularity of hashtags, Tsur and Rappoport (2012) use linear regression with a wide range of features. (Ma et al., 2012; Ma et al., 2013) frame the problem as classification into a number of fixed intervals and applies all the standard classifiers. None of these studies model periodicities, although the former stresses their importance for accurate predictions. For predicting the hashtag given the tweet text, Mazzia and Juett (2011) uses the Naive Bayes classifier with the uniform and empirical prior or TF-IDF weighting. 3 Gaussian Processes In this paper we consider Gaussian Process (GP) models of regression (Rasmussen and Williams, 2005). GP is a probabilistic machine learning framework incorporating kernels and Bayesian nonp</context>
</contexts>
<marker>Ma, Sun, Cong, 2013</marker>
<rawString>Zongyang Ma, Aixin Sun, and Gao Cong. 2013. On predicting the popularity of newly emerging hashtags in Twitter. Journal of the American Society for Information Science and Technology, 64(7):1399–1410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allie Mazzia</author>
<author>James Juett</author>
</authors>
<title>Suggesting hashtags on Twitter.</title>
<date>2011</date>
<booktitle>In http://wwwpersonal.umich.edu/ amazzia/pubs/545-inal.pdf.</booktitle>
<contexts>
<context position="7385" citStr="Mazzia and Juett (2011)" startWordPosition="1150" endWordPosition="1153">context of hashtag adoption. (Romero et al., 2011) analyses the patterns of temporal diffusion in Social Media finding that hashtags have also a persistence factor. For predicting future popularity of hashtags, Tsur and Rappoport (2012) use linear regression with a wide range of features. (Ma et al., 2012; Ma et al., 2013) frame the problem as classification into a number of fixed intervals and applies all the standard classifiers. None of these studies model periodicities, although the former stresses their importance for accurate predictions. For predicting the hashtag given the tweet text, Mazzia and Juett (2011) uses the Naive Bayes classifier with the uniform and empirical prior or TF-IDF weighting. 3 Gaussian Processes In this paper we consider Gaussian Process (GP) models of regression (Rasmussen and Williams, 2005). GP is a probabilistic machine learning framework incorporating kernels and Bayesian nonparametrics which is widely considered as state-ofthe-art for regression. The GP defines a prior over functions which applied at each input point gives a response value. Given data, we can analytically infer the posterior distribution of these functions assuming Gaussian noise. The kernel of the GP </context>
</contexts>
<marker>Mazzia, Juett, 2011</marker>
<rawString>Allie Mazzia and James Juett. 2011. Suggesting hashtags on Twitter. In http://wwwpersonal.umich.edu/ amazzia/pubs/545-inal.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James McInerney</author>
<author>Alex Rogers</author>
<author>Nicholas R Jennings</author>
</authors>
<title>Learning periodic human behaviour models from sparse data for crowdsourcing aid delivery in developing countries.</title>
<date>2013</date>
<booktitle>In Proceedings of the TwentyNinth Conference on Uncertainty in Artiicial Intelligence, UAI ’13.</booktitle>
<contexts>
<context position="6311" citStr="McInerney et al., 2013" startWordPosition="975" endWordPosition="978">ng et al., 2008; Hennig et al., 2012). Yogatama et al. (2011) proposed a regulariser for generalised linear models that encourages local temporal smoothness. 1http://www.gaussianprocess.org/gpml/ code 2https://github.com/SheffieldML/GPy 3http://www.preotiuc.ro Modelling periodicities is one of the standard applications of Gaussian Processes (Rasmussen and Williams, 2005). Recent work by Wilson and Adams (2013) and Durrande et al. (2013) show how different periods can be identified from data. In general, methods that assume certain periodicities at daily or weekly levels were proposed e.g. in (McInerney et al., 2013). GPs were used with text by Polajnar et al. (2011) and for Quality Estimation regression in (Cohn and Specia, 2013; Shah et al., 2013). Temporal patterns for short, distinctive lexical items such as hashtags and memes were quantitatively studied (Leskovec et al., 2009) and clustered (Yang and Leskovec, 2011) in Social Media. (Yang et al., 2012) studies the dual role of hashtags, of bookmarks of content and symbols of community membership, in the context of hashtag adoption. (Romero et al., 2011) analyses the patterns of temporal diffusion in Social Media finding that hashtags have also a pers</context>
</contexts>
<marker>McInerney, Rogers, Jennings, 2013</marker>
<rawString>James McInerney, Alex Rogers, and Nicholas R Jennings. 2013. Learning periodic human behaviour models from sparse data for crowdsourcing aid delivery in developing countries. In Proceedings of the TwentyNinth Conference on Uncertainty in Artiicial Intelligence, UAI ’13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara Polajnar</author>
<author>Simon Rogers</author>
<author>Mark Girolami</author>
</authors>
<title>Protein interaction detection in sentences via Gaussian Processes: a preliminary evaluation.</title>
<date>2011</date>
<journal>International Journal Data Mining and Bioinformatics,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="6362" citStr="Polajnar et al. (2011)" startWordPosition="985" endWordPosition="988">l. (2011) proposed a regulariser for generalised linear models that encourages local temporal smoothness. 1http://www.gaussianprocess.org/gpml/ code 2https://github.com/SheffieldML/GPy 3http://www.preotiuc.ro Modelling periodicities is one of the standard applications of Gaussian Processes (Rasmussen and Williams, 2005). Recent work by Wilson and Adams (2013) and Durrande et al. (2013) show how different periods can be identified from data. In general, methods that assume certain periodicities at daily or weekly levels were proposed e.g. in (McInerney et al., 2013). GPs were used with text by Polajnar et al. (2011) and for Quality Estimation regression in (Cohn and Specia, 2013; Shah et al., 2013). Temporal patterns for short, distinctive lexical items such as hashtags and memes were quantitatively studied (Leskovec et al., 2009) and clustered (Yang and Leskovec, 2011) in Social Media. (Yang et al., 2012) studies the dual role of hashtags, of bookmarks of content and symbols of community membership, in the context of hashtag adoption. (Romero et al., 2011) analyses the patterns of temporal diffusion in Social Media finding that hashtags have also a persistence factor. For predicting future popularity of</context>
</contexts>
<marker>Polajnar, Rogers, Girolami, 2011</marker>
<rawString>Tamara Polajnar, Simon Rogers, and Mark Girolami. 2011. Protein interaction detection in sentences via Gaussian Processes: a preliminary evaluation. International Journal Data Mining and Bioinformatics, 5(1):52–72, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Preot¸iuc-Pietro</author>
<author>Trevor Cohn</author>
</authors>
<title>Mining User Behaviours: A Study of Check-in Patterns in Location Based Social Networks.</title>
<date>2013</date>
<booktitle>In Proceedings of the ACM Web Science Conference, Web Science ’13.</booktitle>
<marker>Preot¸iuc-Pietro, Cohn, 2013</marker>
<rawString>Daniel Preot¸iuc-Pietro and Trevor Cohn. 2013. Mining User Behaviours: A Study of Check-in Patterns in Location Based Social Networks. In Proceedings of the ACM Web Science Conference, Web Science ’13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Preot¸iuc-Pietro</author>
<author>Sina Samangooei</author>
<author>Trevor Cohn</author>
<author>Nicholas Gibbins</author>
<author>Mahesan Niranjan</author>
</authors>
<title>Trendminer: An architecture for real time analysis of social media text.</title>
<date>2012</date>
<booktitle>Proceedings of the Sixth International AAAI Conference on Weblogs and Social Media, Workshop on Real-Time Analysis and Mining of Social Streams.</booktitle>
<marker>Preot¸iuc-Pietro, Samangooei, Cohn, Gibbins, Niranjan, 2012</marker>
<rawString>Daniel Preot¸iuc-Pietro, Sina Samangooei, Trevor Cohn, Nicholas Gibbins, and Mahesan Niranjan. 2012. Trendminer: An architecture for real time analysis of social media text. Proceedings of the Sixth International AAAI Conference on Weblogs and Social Media, Workshop on Real-Time Analysis and Mining of Social Streams.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Edward Rasmussen</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Occam’s razor.</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems, NIPS 13.</booktitle>
<contexts>
<context position="16522" citStr="Rasmussen and Ghahramani, 2000" startWordPosition="2754" endWordPosition="2757">the marginal likelihood or evidence and is useful for model selection using only the training set: p(x |D, θ, hi) = ifp(x|D, f, hi)p(f|θ,hi) (7) Our first goal is to fit the kernel by minimizing the negative log marginal likelihood (NLML) with respect to the kernel parameters θ. This approximation is also known as type II maximum likelihood (ML-II). Conditioned on kernel parameters, the evidence of a GP can be computed analytically. Our second goal is to use the evidence for model selection because it balances the data fit and the model complexity by automatically incorporating Occam’s Razor (Rasmussen and Ghahramani, 2000). Because the evidence must normalise, complex models which can account for many datasets achieve low evidence. One can think of the evidence as the probability that a random draw of the parameter values from the model class would generate the dataset D. This way, complex models are penalised because they can describe many datasets, while the simple models can describe only a few datasets, thus the chance of a good data fit being very low. This is for example the case of the periodic bursts in Figure 3. Although the periodic kernel can fit the data, it will incur a high model complexity penalt</context>
</contexts>
<marker>Rasmussen, Ghahramani, 2000</marker>
<rawString>Carl Edward Rasmussen and Zoubin Ghahramani. 2000. Occam’s razor. In Advances in Neural Information Processing Systems, NIPS 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Edward Rasmussen</author>
<author>Christopher K I Williams</author>
</authors>
<title>Gaussian Processes for Machine Learning.</title>
<date>2005</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3557" citStr="Rasmussen and Williams, 2005" startWordPosition="563" endWordPosition="566">types of temporal patterns: a) periodic, b) constant in time, c) falling out of use after enjoying a brief spell of popularity (e.g. internet memes, news). This is performed automatically only using training data and makes no assumptions on the existence or the length of the periods we aim to model. We demonstrate the approach by modelling frequencies of hashtag occurrences in Twitter. Hashtags are user-generated labels included in tweets by their authors in order to assign them to a conversation and can be considered as a proxy for topics. To this end, we make use of Gaussian Processes (GP) (Rasmussen and Williams, 2005), a 977 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 977–988, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Bayesian non-parametric model for regression. Using the Bayesian evidence we automatically perform model selection to classify temporal patterns. We aim to use the most suitable model for extrapolation, i.e. predicting future values from past observations. The GP is fully defined by the covariance structure assumed between the observed points, and its hyperparameters, which can be automatica</context>
<context position="6061" citStr="Rasmussen and Williams, 2005" startWordPosition="933" endWordPosition="936">ffiths and Steyvers (2004) analyse evolution of topics over time, but without modelling time explicitly. Extensions that model time make different assumptions, usually regarding smoothing proprieties in (Wang and McCallum, 2006; Blei and Lafferty, 2006; Wang et al., 2008; Hennig et al., 2012). Yogatama et al. (2011) proposed a regulariser for generalised linear models that encourages local temporal smoothness. 1http://www.gaussianprocess.org/gpml/ code 2https://github.com/SheffieldML/GPy 3http://www.preotiuc.ro Modelling periodicities is one of the standard applications of Gaussian Processes (Rasmussen and Williams, 2005). Recent work by Wilson and Adams (2013) and Durrande et al. (2013) show how different periods can be identified from data. In general, methods that assume certain periodicities at daily or weekly levels were proposed e.g. in (McInerney et al., 2013). GPs were used with text by Polajnar et al. (2011) and for Quality Estimation regression in (Cohn and Specia, 2013; Shah et al., 2013). Temporal patterns for short, distinctive lexical items such as hashtags and memes were quantitatively studied (Leskovec et al., 2009) and clustered (Yang and Leskovec, 2011) in Social Media. (Yang et al., 2012) st</context>
<context position="7596" citStr="Rasmussen and Williams, 2005" startWordPosition="1183" endWordPosition="1186">tags, Tsur and Rappoport (2012) use linear regression with a wide range of features. (Ma et al., 2012; Ma et al., 2013) frame the problem as classification into a number of fixed intervals and applies all the standard classifiers. None of these studies model periodicities, although the former stresses their importance for accurate predictions. For predicting the hashtag given the tweet text, Mazzia and Juett (2011) uses the Naive Bayes classifier with the uniform and empirical prior or TF-IDF weighting. 3 Gaussian Processes In this paper we consider Gaussian Process (GP) models of regression (Rasmussen and Williams, 2005). GP is a probabilistic machine learning framework incorporating kernels and Bayesian nonparametrics which is widely considered as state-ofthe-art for regression. The GP defines a prior over functions which applied at each input point gives a response value. Given data, we can analytically infer the posterior distribution of these functions assuming Gaussian noise. The kernel of the GP defines the covariance in response values as a function of its inputs. We can identify two different set-ups for a regression problem. If the range of values to be predicted 978 lies within the bounds of the tra</context>
<context position="8940" citStr="Rasmussen and Williams, 2005" startWordPosition="1403" endWordPosition="1407">our problem that of extrapolation. In this respect, extrapolation is considered a more difficult task and the covariance kernel which incorporates our prior knowledge plays a major role in the prediction. There is the case when multiple covariance kernels can describe our data. For choosing the right kernel and its hyperparameters only using the training data we employ Bayesian model selection which makes a trade-off between the fit of the training data and model complexity. We now briefly give an overview of GP regression, kernel choice and model selection. We refer the interested reader to (Rasmussen and Williams, 2005) for a detailed introduction to GPs. 3.1 Gaussian Process Regression Consider a time series regression task where we only have one feature, the value xt at time t. Our training data consists of n pairs D = {(t, xt)}. The model will need to predict values xt for values of t greater than those in the dataset. GP regression assumes a latent function f that is drawn from a GP prior f(t) — !9P(m, k(t, t&apos;)) where m is the mean and k a kernel. The prediction value is obtained by the function evaluated at the corresponding data point, xt = f(t) + c, where E — N(0, σ2) is white-noise. The GP is defined</context>
<context position="13095" citStr="Rasmussen and Williams, 2005" startWordPosition="2137" endWordPosition="2140">lation settings. kSE(t, t0) = s2 · exp −(t − t0)2 (3) 2l2 This gives a smooth transition between neighbouring points and best describes time series with a smooth shape e.g. a uni-modal burst with a steady decrease. However, its uncertainty grows with for predictions well into the future. Its two parameters s and l are the characteristic lengthscales along the two axes. Intuitively, they control the distance of inputs on a particular axis from which the function values become uncorrelated. Using the SE kernel corresponds to Bayesian linear regression with an infinite number of basis functions (Rasmussen and Williams, 2005). Linear (Lin): The linear kernel describes a linear relationship between outputs. kLin(t, t0) = s2 + kt · t0k (4) This can be obtained from linear regression by having N(0,1) priors on the corresponding regression weights and a prior of N(0, s2) on the bias. Periodic (PER): The periodic kernel represents a SE kernel in polar coordinates. sin2(2π(t − t0)2/p) � l2 (5) It has a sinusoidal shape and is good at modelling periodically patterns that oscillate between low and high frequency. s and l are characteristic lengthscales as in the SE kernel and p is the period. 1 s=1 s=5 s=50 0.6 0.4 0.2 02</context>
</contexts>
<marker>Rasmussen, Williams, 2005</marker>
<rawString>Carl Edward Rasmussen and Christopher K. I. Williams. 2005. Gaussian Processes for Machine Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Romero</author>
<author>Brendan Meeder</author>
<author>Jon Kleinberg</author>
</authors>
<title>Differences in the mechanics of information diffusion across topics: idioms, political hashtags, and complex contagion on Twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th International conference on World wide web, WWW ’11.</booktitle>
<contexts>
<context position="6812" citStr="Romero et al., 2011" startWordPosition="1059" endWordPosition="1062">ral, methods that assume certain periodicities at daily or weekly levels were proposed e.g. in (McInerney et al., 2013). GPs were used with text by Polajnar et al. (2011) and for Quality Estimation regression in (Cohn and Specia, 2013; Shah et al., 2013). Temporal patterns for short, distinctive lexical items such as hashtags and memes were quantitatively studied (Leskovec et al., 2009) and clustered (Yang and Leskovec, 2011) in Social Media. (Yang et al., 2012) studies the dual role of hashtags, of bookmarks of content and symbols of community membership, in the context of hashtag adoption. (Romero et al., 2011) analyses the patterns of temporal diffusion in Social Media finding that hashtags have also a persistence factor. For predicting future popularity of hashtags, Tsur and Rappoport (2012) use linear regression with a wide range of features. (Ma et al., 2012; Ma et al., 2013) frame the problem as classification into a number of fixed intervals and applies all the standard classifiers. None of these studies model periodicities, although the former stresses their importance for accurate predictions. For predicting the hashtag given the tweet text, Mazzia and Juett (2011) uses the Naive Bayes class</context>
</contexts>
<marker>Romero, Meeder, Kleinberg, 2011</marker>
<rawString>Daniel M. Romero, Brendan Meeder, and Jon Kleinberg. 2011. Differences in the mechanics of information diffusion across topics: idioms, political hashtags, and complex contagion on Twitter. In Proceedings of the 20th International conference on World wide web, WWW ’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kashif Shah</author>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>An Investigation on the Effectiveness of Features for Translation Quality Estimation. In</title>
<date>2013</date>
<journal>MT Summit</journal>
<volume>13</volume>
<contexts>
<context position="6446" citStr="Shah et al., 2013" startWordPosition="999" endWordPosition="1002">poral smoothness. 1http://www.gaussianprocess.org/gpml/ code 2https://github.com/SheffieldML/GPy 3http://www.preotiuc.ro Modelling periodicities is one of the standard applications of Gaussian Processes (Rasmussen and Williams, 2005). Recent work by Wilson and Adams (2013) and Durrande et al. (2013) show how different periods can be identified from data. In general, methods that assume certain periodicities at daily or weekly levels were proposed e.g. in (McInerney et al., 2013). GPs were used with text by Polajnar et al. (2011) and for Quality Estimation regression in (Cohn and Specia, 2013; Shah et al., 2013). Temporal patterns for short, distinctive lexical items such as hashtags and memes were quantitatively studied (Leskovec et al., 2009) and clustered (Yang and Leskovec, 2011) in Social Media. (Yang et al., 2012) studies the dual role of hashtags, of bookmarks of content and symbols of community membership, in the context of hashtag adoption. (Romero et al., 2011) analyses the patterns of temporal diffusion in Social Media finding that hashtags have also a persistence factor. For predicting future popularity of hashtags, Tsur and Rappoport (2012) use linear regression with a wide range of feat</context>
</contexts>
<marker>Shah, Cohn, Specia, 2013</marker>
<rawString>Kashif Shah, Trevor Cohn, and Lucia Specia. 2013. An Investigation on the Effectiveness of Features for Translation Quality Estimation. In MT Summit ’13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>What’s in a hashtag? Content based prediction of the spread of ideas in microblogging communities.</title>
<date>2012</date>
<journal>WSDM</journal>
<booktitle>In Proceedings of the ifth ACM International conference on Web</booktitle>
<volume>12</volume>
<contexts>
<context position="6998" citStr="Tsur and Rappoport (2012)" startWordPosition="1087" endWordPosition="1090">ality Estimation regression in (Cohn and Specia, 2013; Shah et al., 2013). Temporal patterns for short, distinctive lexical items such as hashtags and memes were quantitatively studied (Leskovec et al., 2009) and clustered (Yang and Leskovec, 2011) in Social Media. (Yang et al., 2012) studies the dual role of hashtags, of bookmarks of content and symbols of community membership, in the context of hashtag adoption. (Romero et al., 2011) analyses the patterns of temporal diffusion in Social Media finding that hashtags have also a persistence factor. For predicting future popularity of hashtags, Tsur and Rappoport (2012) use linear regression with a wide range of features. (Ma et al., 2012; Ma et al., 2013) frame the problem as classification into a number of fixed intervals and applies all the standard classifiers. None of these studies model periodicities, although the former stresses their importance for accurate predictions. For predicting the hashtag given the tweet text, Mazzia and Juett (2011) uses the Naive Bayes classifier with the uniform and empirical prior or TF-IDF weighting. 3 Gaussian Processes In this paper we consider Gaussian Process (GP) models of regression (Rasmussen and Williams, 2005). </context>
<context position="20295" citStr="Tsur and Rappoport (2012)" startWordPosition="3400" endWordPosition="3403">ion interval was 1 January – 28 February 2011. For simplicity in the classification task, we filtered the stream to include only tweets that have exactly one hashtag. These represent approximately 7.8% of our stream. As text processing steps, we have tokenised all the tweets and filtered them to be written in English using the Trendminer pipeline (Preoiiuc-Pietro et al., 2012). We also remove duplicate tweets (retweets and tweets that had the same first 6 content tokens) because they likely represent duplicate content, automated messages or spam which would bias the dataset, as also stated by Tsur and Rappoport (2012). In our experiments we use the first month of data as training and the second month as testing. Note the challenging nature of this testing configuration where predictions must be made for up to 28 days into the future. We keep a total 1176 of hashtags which appear at least 500 times in both splits of the data. The vocabulary consists of all the tokens that occur more than 100 times in the dataset and start with an alphabetic letter. After processing, our dataset consists of 6,416,591 tweets with each having on average 9.55 tokens. 5 Forecasting hashtag frequency We treat our task of forecast</context>
</contexts>
<marker>Tsur, Rappoport, 2012</marker>
<rawString>Oren Tsur and Ari Rappoport. 2012. What’s in a hashtag? Content based prediction of the spread of ideas in microblogging communities. In Proceedings of the ifth ACM International conference on Web search and data mining, WSDM ’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuerui Wang</author>
<author>Andrew McCallum</author>
</authors>
<title>Topics over time: a non-Markov continuous-time model of topical trends.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD International conference on Knowledge discovery and data mining, KDD ’06.</booktitle>
<contexts>
<context position="1601" citStr="Wang and McCallum, 2006" startWordPosition="238" endWordPosition="241">changes in text corpora are central to our understanding of many linguistic and social phenomena. Social Media platforms and the digitalization of libraries provides a vast body of timestamped data. This allows studying of the complex temporal patterns exhibited by text usage including highly non-stationary distributions and periodicities. However, temporal effects have been mostly ignored by previous work on text analysis or at best dealt with by making strong assumptions such as smoothly varying parameters with time (Yogatama et al., 2011) or modelled using a simple uni-modal distri bution (Wang and McCallum, 2006). This paper develops a temporal model for classifying microblog posts which explicitly incorporates multimodal periodic behaviours using Gaussian Processes (GPs). We expect text usage to follow multiple periodicities at different scales. For example, people on Social Media might talk about different topics during and after work on weekdays, talk every Friday about the weekend ahead, or comment about their favorite weekly TV show during its air time. Given this, text frequencies will display periodic patterns. This applies to other text related quantities like cooccurrence values or topic dist</context>
<context position="5659" citStr="Wang and McCallum, 2006" startWordPosition="884" endWordPosition="888"> our knowledge, this is the first paper to use GP regression for forecasting and model selection within a NLP task. All the hashtag time series data and the implementation of the PS kernel in the popular opensource Gaussian Processes packages GPML1 and GPy2 are available on the author’s website3. 2 Related Work Time varying text patterns have been of particular interest in topic modelling. Griffiths and Steyvers (2004) analyse evolution of topics over time, but without modelling time explicitly. Extensions that model time make different assumptions, usually regarding smoothing proprieties in (Wang and McCallum, 2006; Blei and Lafferty, 2006; Wang et al., 2008; Hennig et al., 2012). Yogatama et al. (2011) proposed a regulariser for generalised linear models that encourages local temporal smoothness. 1http://www.gaussianprocess.org/gpml/ code 2https://github.com/SheffieldML/GPy 3http://www.preotiuc.ro Modelling periodicities is one of the standard applications of Gaussian Processes (Rasmussen and Williams, 2005). Recent work by Wilson and Adams (2013) and Durrande et al. (2013) show how different periods can be identified from data. In general, methods that assume certain periodicities at daily or weekly l</context>
</contexts>
<marker>Wang, McCallum, 2006</marker>
<rawString>Xuerui Wang and Andrew McCallum. 2006. Topics over time: a non-Markov continuous-time model of topical trends. In Proceedings of the 12th ACM SIGKDD International conference on Knowledge discovery and data mining, KDD ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chong Wang</author>
<author>David M Blei</author>
<author>David Heckerman</author>
</authors>
<title>Continuous time Dynamic topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twenty-Fourth Conference on Uncertainty in Artiicial Intelligence, UAI ’08.</booktitle>
<contexts>
<context position="5703" citStr="Wang et al., 2008" startWordPosition="893" endWordPosition="896">regression for forecasting and model selection within a NLP task. All the hashtag time series data and the implementation of the PS kernel in the popular opensource Gaussian Processes packages GPML1 and GPy2 are available on the author’s website3. 2 Related Work Time varying text patterns have been of particular interest in topic modelling. Griffiths and Steyvers (2004) analyse evolution of topics over time, but without modelling time explicitly. Extensions that model time make different assumptions, usually regarding smoothing proprieties in (Wang and McCallum, 2006; Blei and Lafferty, 2006; Wang et al., 2008; Hennig et al., 2012). Yogatama et al. (2011) proposed a regulariser for generalised linear models that encourages local temporal smoothness. 1http://www.gaussianprocess.org/gpml/ code 2https://github.com/SheffieldML/GPy 3http://www.preotiuc.ro Modelling periodicities is one of the standard applications of Gaussian Processes (Rasmussen and Williams, 2005). Recent work by Wilson and Adams (2013) and Durrande et al. (2013) show how different periods can be identified from data. In general, methods that assume certain periodicities at daily or weekly levels were proposed e.g. in (McInerney et al</context>
</contexts>
<marker>Wang, Blei, Heckerman, 2008</marker>
<rawString>Chong Wang, David M. Blei, and David Heckerman. 2008. Continuous time Dynamic topic models. In Proceedings of the Twenty-Fourth Conference on Uncertainty in Artiicial Intelligence, UAI ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Gordon Wilson</author>
<author>Ryan Prescott Adams</author>
</authors>
<title>Gaussian Process covariance kernels for pattern discovery and extrapolation.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference on Machine Learning, ICML ’13.</booktitle>
<contexts>
<context position="6101" citStr="Wilson and Adams (2013)" startWordPosition="940" endWordPosition="943">f topics over time, but without modelling time explicitly. Extensions that model time make different assumptions, usually regarding smoothing proprieties in (Wang and McCallum, 2006; Blei and Lafferty, 2006; Wang et al., 2008; Hennig et al., 2012). Yogatama et al. (2011) proposed a regulariser for generalised linear models that encourages local temporal smoothness. 1http://www.gaussianprocess.org/gpml/ code 2https://github.com/SheffieldML/GPy 3http://www.preotiuc.ro Modelling periodicities is one of the standard applications of Gaussian Processes (Rasmussen and Williams, 2005). Recent work by Wilson and Adams (2013) and Durrande et al. (2013) show how different periods can be identified from data. In general, methods that assume certain periodicities at daily or weekly levels were proposed e.g. in (McInerney et al., 2013). GPs were used with text by Polajnar et al. (2011) and for Quality Estimation regression in (Cohn and Specia, 2013; Shah et al., 2013). Temporal patterns for short, distinctive lexical items such as hashtags and memes were quantitatively studied (Leskovec et al., 2009) and clustered (Yang and Leskovec, 2011) in Social Media. (Yang et al., 2012) studies the dual role of hashtags, of book</context>
<context position="30813" citStr="Wilson and Adams (2013)" startWordPosition="5225" endWordPosition="5228">multiples of the cor0.2 0 100 200 300 Lag Figure 6: Sample autocorrelation for #confessionhour Periodogram Power Spectral Density Estimate −10 −20 −30 −40 −50 −60 70 Figure 7: Power spectral density for #raw rect period will be feasible candidates and b) it leads to incorrect conclusions when there is autocorrelated noise. The second case is illustrated in Figure 6 where #confessionhour shows autocorrelation but, as seen in Figure 5, lacks a periodic component. Another approach to discovering periods in data is by computing the power spectral density. This has been used in the GP framework by Wilson and Adams (2013). For some time series, this gives a good indication of the period, as represented by a peak in the periodogram at that value. This fails to discover the correct period when dealing with large bursts like those exhibited by the #raw time series as shown in Figure 7. The lowest frequency spike corresponds to the correct period of 168, but also other candidate periods are shown as possible. The reason for this is its reliance on the Fourier Transform which decomposes the time series into a sum of oscillating patterns. These cannot model stepfunctions and other non-smoothly varying signals. A fur</context>
</contexts>
<marker>Wilson, Adams, 2013</marker>
<rawString>Andrew Gordon Wilson and Ryan Prescott Adams. 2013. Gaussian Process covariance kernels for pattern discovery and extrapolation. In Proceedings of the International Conference on Machine Learning, ICML ’13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaewon Yang</author>
<author>Jure Leskovec</author>
</authors>
<title>Patterns of temporal variation in online media.</title>
<date>2011</date>
<booktitle>In Proceedings of the fourth ACM International conference on Web search and data mining, WSDM ’11.</booktitle>
<contexts>
<context position="6621" citStr="Yang and Leskovec, 2011" startWordPosition="1026" endWordPosition="1029">pplications of Gaussian Processes (Rasmussen and Williams, 2005). Recent work by Wilson and Adams (2013) and Durrande et al. (2013) show how different periods can be identified from data. In general, methods that assume certain periodicities at daily or weekly levels were proposed e.g. in (McInerney et al., 2013). GPs were used with text by Polajnar et al. (2011) and for Quality Estimation regression in (Cohn and Specia, 2013; Shah et al., 2013). Temporal patterns for short, distinctive lexical items such as hashtags and memes were quantitatively studied (Leskovec et al., 2009) and clustered (Yang and Leskovec, 2011) in Social Media. (Yang et al., 2012) studies the dual role of hashtags, of bookmarks of content and symbols of community membership, in the context of hashtag adoption. (Romero et al., 2011) analyses the patterns of temporal diffusion in Social Media finding that hashtags have also a persistence factor. For predicting future popularity of hashtags, Tsur and Rappoport (2012) use linear regression with a wide range of features. (Ma et al., 2012; Ma et al., 2013) frame the problem as classification into a number of fixed intervals and applies all the standard classifiers. None of these studies m</context>
<context position="25476" citStr="Yang and Leskovec (2011)" startWordPosition="4303" endWordPosition="4306">E on the held-out testing set. While functions learned using this kernel never clearly outperform others on NRMSE on held-out data, this is very useful for interpretation of the time series, separating noisy time series from those that have an underlying periodic behaviour. The #confessionhour example illustrates a behaviour best suited for modelling using the SE kernel. We notice a sudden burst in volume which decays over the next 2 days. This is actually the behaviour typical of ‘internet memes’ (this hashtag tags tweets of people posting things they would never tell anyone) as presented in Yang and Leskovec (2011). These cannot be modelled with a constant kernel or a periodic one as shown by the results on held-out data and the time series plot. The periodic kernels will fail in trying to match the large burst with others in the training data and will attribute to noise the lack of a similar peak, thus discovering wrong periods and making bad predictions. In this example, forecasts will be very close to 0 under the SE kernel, which is what we would desire from the model. The periodic kernel best models hashtags that exhibit an oscillating pattern. For example, this best fits words that are used frequen</context>
</contexts>
<marker>Yang, Leskovec, 2011</marker>
<rawString>Jaewon Yang and Jure Leskovec. 2011. Patterns of temporal variation in online media. In Proceedings of the fourth ACM International conference on Web search and data mining, WSDM ’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Yang</author>
<author>Tao Sun</author>
<author>Ming Zhang</author>
<author>Qiaozhu Mei</author>
</authors>
<title>We know what @you #tag: does the dual role affect hashtag adoption?</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st International conference on World Wide Web, WWW ’12.</booktitle>
<contexts>
<context position="6658" citStr="Yang et al., 2012" startWordPosition="1033" endWordPosition="1036">n and Williams, 2005). Recent work by Wilson and Adams (2013) and Durrande et al. (2013) show how different periods can be identified from data. In general, methods that assume certain periodicities at daily or weekly levels were proposed e.g. in (McInerney et al., 2013). GPs were used with text by Polajnar et al. (2011) and for Quality Estimation regression in (Cohn and Specia, 2013; Shah et al., 2013). Temporal patterns for short, distinctive lexical items such as hashtags and memes were quantitatively studied (Leskovec et al., 2009) and clustered (Yang and Leskovec, 2011) in Social Media. (Yang et al., 2012) studies the dual role of hashtags, of bookmarks of content and symbols of community membership, in the context of hashtag adoption. (Romero et al., 2011) analyses the patterns of temporal diffusion in Social Media finding that hashtags have also a persistence factor. For predicting future popularity of hashtags, Tsur and Rappoport (2012) use linear regression with a wide range of features. (Ma et al., 2012; Ma et al., 2013) frame the problem as classification into a number of fixed intervals and applies all the standard classifiers. None of these studies model periodicities, although the form</context>
</contexts>
<marker>Yang, Sun, Zhang, Mei, 2012</marker>
<rawString>Lei Yang, Tao Sun, Ming Zhang, and Qiaozhu Mei. 2012. We know what @you #tag: does the dual role affect hashtag adoption? In Proceedings of the 21st International conference on World Wide Web, WWW ’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dani Yogatama</author>
<author>Michael Heilman</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>Predicting a scientific community’s response to an article.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11.</booktitle>
<marker>Yogatama, Heilman, O’Connor, Dyer, Routledge, Smith, 2011</marker>
<rawString>Dani Yogatama, Michael Heilman, Brendan O’Connor, Chris Dyer, Bryan R. Routledge, and Noah A. Smith. 2011. Predicting a scientific community’s response to an article. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Zangerle</author>
<author>Wolfgang Gassler</author>
<author>Gunther Specht</author>
</authors>
<title>Recommending #-tags in twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Semantic Adaptive Social Web, UMAP ’11.</booktitle>
<contexts>
<context position="32551" citStr="Zangerle et al., 2011" startWordPosition="5524" endWordPosition="5527">astenders Table 5: Example of tweet classification using the Naive Bayes model with the two different priors (E - empirical, P - GP forecast). Rank shows the rank in probability of the correct class (hashtag) under the model. Time is G.M.T. 6 Text based prediction In this section we demonstrate the usefulness of our method of modelling in an NLP task: predicting the hashtag of a tweet based on its text. In contrast to this classification approach for suggesting a tweet’s hashtag, information retrieval methods based on computing similarities between tweets are very hard to scale to large data (Zangerle et al., 2011). We choose a simple model for prediction, the Naive Bayes Classifier. This method provides us with a straightforward way to incorporate our prior knowledge of how frequent a hashtag is in a certain time frame. This Naive Bayes model (NB-P) uses the forecasted values for the respective hour as the prior on the hashtags. For comparison we use the Most Frequent (MF) baseline and the Naive Bayes with empirical prior (NB-E) which doesn’t use any temporal forecasting information. Because there are more than 1000 possible classes we show the accuracy of the correct hashtag being amongst the top 1,5 </context>
</contexts>
<marker>Zangerle, Gassler, Specht, 2011</marker>
<rawString>Eva Zangerle, Wolfgang Gassler, and Gunther Specht. 2011. Recommending #-tags in twitter. In Proceedings of the Workshop on Semantic Adaptive Social Web, UMAP ’11.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
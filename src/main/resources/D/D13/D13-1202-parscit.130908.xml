<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.057205">
<title confidence="0.997236">
Of words, eyes and brains:
Correlating image-based distributional semantic models
with neural representations of concepts
</title>
<author confidence="0.984618">
Andrew J. Anderson, Elia Bruni, Ulisse Bordignon, Massimo Poesio and Marco Baroni
</author>
<affiliation confidence="0.960034">
Center for Mind/Brain Sciences (University of Trento, C.so Bettini 31, Rovereto, Italy)
</affiliation>
<email confidence="0.994459">
first.last@unitn.it
</email>
<sectionHeader confidence="0.995527" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999117965517241">
Traditional distributional semantic models ex-
tract word meaning representations from co-
occurrence patterns of words in text cor-
pora. Recently, the distributional approach has
been extended to models that record the co-
occurrence of words with visual features in
image collections. These image-based models
should be complementary to text-based ones,
providing a more cognitively plausible view
of meaning grounded in visual perception. In
this study, we test whether image-based mod-
els capture the semantic patterns that emerge
from fMRI recordings of the neural signal.
Our results indicate that, indeed, there is a
significant correlation between image-based
and brain-based semantic similarities, and that
image-based models complement text-based
ones, so that the best correlations are achieved
when the two modalities are combined. De-
spite some unsatisfactory, but explained out-
comes (in particular, failure to detect differ-
ential association of models with brain areas),
the results show, on the one hand, that image-
based distributional semantic models can be a
precious new tool to explore semantic repre-
sentation in the brain, and, on the other, that
neural data can be used as the ultimate test set
to validate artificial semantic models in terms
of their cognitive plausibility.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955702702703">
Many recent neuroscientific studies have brought
support to the view that concepts are represented
in terms of patterns of neural activation over broad
areas, naturally encoded as vectors in a neural se-
mantic space (Haxby et al., 2001; Huth et al., 2012).
Similar representations are also widely used in com-
putational linguistics, and in particular in distribu-
tional semantics (Clark, 2012; Erk, 2012; Turney
and Pantel, 2010), that captures meaning in terms
of vectors recording the patterns of co-occurrence
of words in large corpora, under the hypothesis that
words that occur in similar contexts are similar in
meaning.
Since the seminal work of Mitchell et al. (2008),
there has thus being interest in investigating whether
corpus-harvested semantic representations can con-
tribute to the study of concepts in the brain. The
relation is mutually beneficial: From the point of
view of brain activity decoding, a strong correlation
between corpus-based and brain-derived conceptual
representations would mean that we could use the
former (much easier to construct on a very large
scale) to make inferences about the second: e.g., us-
ing corpus-based representations to reconstruct the
likely neural signal associated to words we have no
direct brain data for. From the point of view of com-
putational linguistics, neural data provide the ulti-
mate testing ground for models that strive to cap-
ture important aspects of human semantic mem-
ory (much more so than the commonly used ex-
plicit semantic rating benchmarks). If we found that
a corpus-based model of meaning can make non-
trivial predictions about the structure of the semantic
space in the brain, that would make a pretty strong
case for the intriguing idea that the model is approx-
imating, in interesting ways, the way in which hu-
mans acquire and represent semantic knowledge.
</bodyText>
<page confidence="0.935263">
1960
</page>
<note confidence="0.7313475">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1960–1970,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999988366666667">
We take as our starting point the extensive experi-
ments reported in Murphy et al. (2012), who showed
that purely corpus-based distributional models are at
least as good at brain signal prediction tasks as ear-
lier models that made use of manually-generated or
controlled knowledge sources (Chang et al., 2011;
Palatucci et al., 2009; Pereira et al., 2011), and we
evaluate a very recent type of distributional model,
namely one that is not extracted from textual data
but from image collections through automated vi-
sual feature extraction techniques. It has been ar-
gued that this new generation of image-based dis-
tributional models (Bruni et al., 2011; Bruni et al.,
2012b; Feng and Lapata, 2010; Leong and Mihal-
cea, 2011) provides a more realistic view of mean-
ing, since humans obviously acquire a large propor-
tion of their semantic knowledge from perceptual
data. The first question that we ask, thus, is whether
the more “grounded” image-based models can help
us in interpreting conceptual representations in the
brain. More specifically, we will compare the per-
formance of different image-based representations,
and we will test whether text- and image-based rep-
resentations are complementary, so that when used
together they can better account for patterns in neu-
ral data. Finally, we will check for differences be-
tween anatomical regions in the degree to which text
and/or image models are effective, as one might ex-
pect given the well-known functional specializations
of different anatomical regions.
</bodyText>
<sectionHeader confidence="0.834695" genericHeader="method">
2 Brain data
</sectionHeader>
<bodyText confidence="0.999948571428572">
We use the data that were recorded and preprocessed
by Mitchell et al. (2008), available for download in
their supporting online material.1 Full details of the
experimental protocol, data acquisition and prepro-
cessing can be found in Mitchell et al. (2008) and
the supporting material. Key points are that there
were nine right-handed adult participants (5 female,
age between 18 and 32). The experimental task was
to actively think about the properties of sixty objects
that were presented visually, each as a line drawing
in combination with a text label. The entire set of
objects was presented in a random order in six ses-
sions, each object remained on screen for 3 seconds
with a seven second fixation gap between presenta-
</bodyText>
<footnote confidence="0.934469">
1http://www.cs.cmu.edu/˜tom/science2008/
</footnote>
<bodyText confidence="0.998889595744681">
tions.
Mitchell and colleagues examined 12 categories,
five objects per category, for a total of 60 concepts
(words). Due to coverage limitations, we use 51/60
words representing 11/12 categories. Table 1 con-
tains the full list of 51 words organized by category.
fMRI acquisition and preprocessing Mitchell et
al. (2008) acquired functional images on a Siemens
Allegra 3.0T scanner using a gradient echo EPI
pulse sequence with TR=1000 ms, TE=30 ms and
a 60◦ angle. Seventeen 5-mm thick oblique-axial
slices were imaged with a gap of 1-mm between
slices. The acquisition matrix was 64x64 with
3.125x3.125x5-mm voxels. They subsequently
corrected data for slice timing, motion, linear trend,
and performed temporal smoothing with a high-pass
filter at 190s cutoff. The data were normalized to
the MNI template brain image, spatially normalized
into MNI space and resampled to 3x3x6 mm3 vox-
els. The voxel-wise percent signal change relative to
the fixation condition was computed for each object
presentation. The mean of the four images acquired
4s post stimulus presentation was used for analysis.
To create a single representation per object per
participant, we took the voxel-wise mean of the six
presentations of each word. Likewise to create a sin-
gle representation per category per participant, we
took the voxel-wise mean of all word models per
category, per participant.
Anatomical parcellation Analysis was conducted
on the whole brain, and to address the question of
whether there are differences in models’ effective-
ness between anatomical regions, brains were fur-
ther partitioned into frontal, parietal, temporal and
occipital lobes. This partitioning is coarse (each lobe
is large and serves many diverse functions), but, for
an initial test, appropriate, given that each lobe has
specialisms that on face value are amenable to inter-
pretation by our different distributional models and
the exact nature of specialist processing in localised
areas is often subject to debate (so being overly re-
strictive may be risky). Formulation of the distribu-
tional models is described in detail in the Section 3,
but for now it is sufficient to know that the Object
model is derived from image statistics of the object
depicted in images, Context from image statistics of
the background scene, Object&amp;Context is a com-
</bodyText>
<page confidence="0.995105">
1961
</page>
<table confidence="0.667912545454546">
Animals Bear, Cat, Cow, Dog Horse
Building Apartment, Barn, Church, House
Building parts Arch, Chimney, Closet, Door, Window
Clothing Coat, Dress, Pants, Shirt, Skirt
Furniture Bed, Chair, Desk, Dresser, Table
Insect Ant, Bee, Beetle, Butterfly, Fly
Kitchen utensils Bottle, Cup, Glass, Knife, Spoon
Man made objects Bell, Key, Refrigerator, Telephone, Watch
Tool Chisel, Hammer, Screwdriver
Vegetable Celery, Corn, Lettuce, Tomato
Vehicle Airplane, Bicycle, Car, Train, Truck
</table>
<tableCaption confidence="0.999902">
Table 1: The 51 words represented by the brain and the distributional models, organized by category.
</tableCaption>
<bodyText confidence="0.999930367647059">
bination of the two, and Window2 is a text-based
model.
The occipital lobe houses the primary visual pro-
cessing system and consequently it is reasonable
to expect some bias toward image-based semantic
models. Furthermore, given that experimental stim-
uli incorporated line drawings of the object,and the
visual cortex has a well-established role in process-
ing low-level visual statistics including edge detec-
tion (Bruce et al., 2003), we naturally expected a
good performance from Object (formulated from
edge orientation histograms of similar objects).
Following Goodale and Milner (1992)’s influ-
ential perception-action model (see McIntosh and
Schenk (2009) for recent discussion), visual infor-
mation is channeled from the occipital lobe in two
streams: a perceptual stream, serving object identi-
fication and recognition; and an action stream, spe-
cialist in processing egocentric spatial relationships
and ultimately supporting interaction with the world.
The perceptual stream leads to the temporal lobe.
Here the fusiform gyrus (shared with the occipital
lobe) plays a general role in object categorisation
(e.g., animals and tools (Chao et al., 1999), faces
(Kanwisher and Yovel, 2006), body parts (Peelen
and Downing, 2005) and even word form percep-
tion (McCandliss et al., 2003)). As the parahip-
pocampus is strongly associated with scene repre-
sentation (Epstein, 2008), we expect both the Object
and Context models to capture variability in the tem-
poral lobe. Of wider relevance to semantic process-
ing, the medial temporal gyrus, inferior temporal
gyrus and ventral temporal lobe have generally been
implicated to have roles in supramodal integration
and concept retrieval (Binder et al., 2009). Given
this, we expected that incorporating text would also
be valuable and that the Window2&amp;Object&amp;Context
combination would be a good model.
The visual action stream leads from the occipi-
tal lobe to the parietal lobe to support spatial cog-
nition tasks and action control (Sack, 2009). In
that there seems to be an egocentric frame of ref-
erence, placing actor in environment, it is tempt-
ing to speculate that the Context model is more ap-
propriate than the Object model here. As the pari-
etal lobe also contains the angular gyrus, thought
to be involved in complex, supra-modal information
integration and knowledge retrieval (Binder et al.,
2009), we might again forecast that integrating text
and image information would boost performance, so
Window2&amp;Context was earmarked as a strong can-
didate.
The frontal lobe, is traditionally associated with
high-level processing and manipulation of abstract
knowledge and rules and controlled behaviour
(Miller et al., 2002). Regarding semantics, the dor-
somedial prefrontal cortex has been implicated in
self-guided retrieval of semantic information (e.g.,
uncued speech production), the ventromedial pre-
frontal cortex in motivation and emotional process-
ing, the inferior frontal gyrus in phonological and
syntactic processing, (Binder et al., 2009) and in-
tegration of lexical information (Hagoort, 2005).
Given the association with linguistic processing we
anticipated a bias in favour of Window2.
The four lobes were identified and partitioned
using Tzourio-Mazoyer et al. (2002)’s automatic
anatomical labelling scheme.
</bodyText>
<page confidence="0.980934">
1962
</page>
<bodyText confidence="0.999940230769231">
Voxel selection The set of 500 most stable voxels,
both within the whole brain and from within each
region of interest were identified for analysis. The
most stable voxels were those showing consistent
variation across the different stimuli between scan-
ning sessions. Specifically, and following a similar
strategy to Mitchell et al. (2008), for each voxel, the
set of 51 words from each unique pair of scanning
sessions were correlated using Pearson’s correlation
(6 sessions and therefore 15 unique pairs), and the
mean of the 15 resulting correlation coefficients was
taken as the measure of stability. The 500 voxels
with highest mean correlations were selected.
</bodyText>
<sectionHeader confidence="0.997309" genericHeader="method">
3 Distributional models
</sectionHeader>
<bodyText confidence="0.999984161290323">
Distributional semantic models approximate word
meaning by keeping track of word co-occurrence
statistics from large textual input, relying on the dis-
tributional hypothesis: The meaning of a word can
be induced by the context in which it occurs (Turney
and Pantel, 2010). Despite their great success, these
models still rely on verbal input only, while humans
base their meaning representation also on perceptual
information (Louwerse, 2011).
Thanks to recent developments in computer vi-
sion, it is nowadays possible to take the visual per-
ceptual channel into account, and build new com-
putational models of semantics enhanced with vi-
sual information (Feng and Lapata, 2010; Bruni et
al., 2011; Leong and Mihalcea, 2011; Bergsma and
Goebel, 2011; Bruni et al., 2012a). Given a set of
target concepts and a collection of images depicting
those concepts, it is indeed possible to first encode
the image content into low-level features, and subse-
quently convert it into a higher-level representation
based on the bag-of-visual-words method (Grauman
and Leibe, 2011). Recently, Bruni et al. (2012b)
have shown that better semantic representations can
be extracted if we first localize the concept in the
image, and then extract distinct higher-level features
(visual words) from the box containing the concept
and from the surrounding context. We also follow
this strategy here.
In our experiments we utilize both traditional text-
based models and experimental image-based mod-
els, as well as their combination.
</bodyText>
<subsectionHeader confidence="0.998589">
3.1 Textual models
</subsectionHeader>
<bodyText confidence="0.999986911764705">
Verb We experiment with the original text-based
semantic model used to predict fMRI patterns by
Mitchell et al. (2008). Each object stimulus word
is represented as a 25-dimensional vector, with each
value corresponding to the normalized sentence-
wide co-occurrence of that word with one of 25
manually-picked sensorimotor verbs (such as see,
hear, eat, ...) in a trillion word text corpus.
Window2 To create this model, we collect text
co-occurrence statistics from the freely available
ukWaC and Wackypedia corpora combined (about 3
billion words in total).2 As collocates of our distri-
butional model we select a set of 30K words, namely
the top 20K most frequent nouns, 5K most frequent
adjectives and 5K most frequent verbs.
In the tradition of HAL (Lund and Burgess, 1996),
the model is based on co-occurrence statistics with
collocates within a fixed-size window of 2 to the left
and right of each target word. Despite their sim-
plicity, narrow-window-based models have shown
to achieve state-of-the-art results in various stan-
dard semantic tasks (Bullinaria and Levy, 2007)
and to outperform both document-based and syntax-
based models trained on the same corpus (Bruni et
al., 2012a). Moreover, in Murphy et al. (2012) a
window-based model very similar to ours was not
significantly worse than their best model for brain
decoding. We tried also a few variations, e.g., us-
ing a larger window or different transformations on
the raw co-occurrences from those presented below,
but with little, insignificant changes in performance.
Given that our focus here is on visual information,
we only report results for Window2 and its combi-
nation with visual models.
</bodyText>
<subsectionHeader confidence="0.99934">
3.2 Visual models
</subsectionHeader>
<bodyText confidence="0.999932555555555">
Our visual models are inspired by Bruni et al.
(2012b), that have explored to what extent extract-
ing features from images where objects are local-
ized results in better semantic representations. They
found that extracting visual features separately from
the object and its surrounding context leads to bet-
ter performance than not using localization, and us-
ing only object- and, more surprisingly, context-
extracted features also results in performant models
</bodyText>
<footnote confidence="0.967123">
2http://wacky.sslmit.unibo.it/
</footnote>
<page confidence="0.962835">
1963
</page>
<bodyText confidence="0.999941934782609">
(especially when evaluating inter-object similarity,
the context in which an object is located can signif-
icantly contribute to semantic representation, in cer-
tain cases carrying even more information than the
depicted object itself).
More in detail, with localization the visual fea-
tures (visual words) can be extracted from the ob-
ject bounding box (in our experiments, the Object
model) or from only outside the object box (the
Context model). A combined model is obtained
by concatenating the two feature vectors (the Ob-
ject&amp;Context model).
Visual model construction pipeline To extract
visual co-occurrence statistics, we use images from
ImageNet (Deng et al., 2009),3 a very large im-
age database organized on top of the WordNet hi-
erarchy (Fellbaum, 1998). ImageNet has more than
14 million images, covering 21K WordNet nominal
synsets. ImageNet stands out for the high quality of
its images, both in terms of resolution and concept
annotations. Moreover, for around 3K concepts, an-
notations of object bounding boxes is provided. This
last feature allows us to exploit object localization
within our experiments.
To build visual distributional models, we utilize
the bag-of-visual-words (BoVW) representation of
images (Sivic and Zisserman, 2003; Csurka et al.,
2004). Inspired by NLP, BoVW discretizes the im-
age content in terms of a histogram of visual word
counts. Differently from NLP, in vision there is not a
natural notion of visual words, hence a visual vocab-
ulary has to be built from scratch. The process works
as follows. First, a large set of low-level features is
extracted from a corpus of images. The low-level
feature vectors are subsequently clustered into dif-
ferent regions (visual words). Given then a new im-
age, each of the low-level feature vectors extracted
from the patches that compose it is mapped to the
nearest visual word (e.g., in terms of Euclidean dis-
tance from the cluster centroid) such that the image
can be represented with a histogram counting the in-
stances of each visual word in the image.
As low-level features we use SIFT, the Scale In-
variant Feature Transform (Lowe, 2004). SIFT fea-
tures are good at capturing parts of objects and are
designed to be invariant to image transformations
</bodyText>
<footnote confidence="0.805657">
3http://www.image-net.org/
</footnote>
<bodyText confidence="0.99977875">
such as change in scale, rotation and illumination.
To construct the visual vocabulary, we cluster the
SIFT features into 25K different clusters.4 We add
also spatial information by dividing the image into
several subregions, representing each of them in
terms of BoVW and then stacking the resulting his-
tograms (Lazebnik et al., 2006). We use in total 8
different regions, obtaining a final vector of 200K
dimensions (25K visual words x 8 regions). Since
each concept in our dataset is represented by mul-
tiple images, we pool the visual word occurrences
across images by summing them up into a single
vector.
To perform the entire visual pipeline we use
VSEM, an open library for visual semantics (Bruni
et al., 2013).5
</bodyText>
<subsectionHeader confidence="0.999753">
3.3 Model transformations and combination
</subsectionHeader>
<bodyText confidence="0.999894666666666">
Once both the textual and the visual models are built,
we perform two different transformations on the raw
co-occurrence counts. First, we transform them into
nonnegative Pointwise Mutual Information (PMI)
association scores (Church and Hanks, 1990). As a
second transformation, we apply dimensionality re-
duction to the two matrices. In particular, we adopt
the Singular Value Decomposition (SVD), one of the
most effective methods to approximate the original
data in lower dimensionality space (Sch¨utze, 1997),
and reduce the vectors to 50 dimensions.
To combine text- and image-based semantic mod-
els in a joint representation, we separately normalize
their vectors to unit length, and concatenate them,
along the lines of Bruni et al. (2011). More sophis-
ticated combination models have been proposed in
the recent literature on multimodal semantics. For
example, Bruni et al. (2012a) use SVD as a mix-
ing strategy, given its ability to smooth the matrices
and uncover latent dimensions. Another example is
Silberer and Lapata (2013), where Canonical Corre-
lation Analysis is used. We reserve the exploration
of more advanced combination methods for further
studies.
Finally, to represent the 11 categories we experi-
ment with (see Table 1), we average the vectors of
the concepts they include.
</bodyText>
<footnote confidence="0.993608">
4We use k-means, the most commonly employed clustering
algorithm for this task.
5http://clic.cimec.unitn.it/vsem/
</footnote>
<page confidence="0.994897">
1964
</page>
<sectionHeader confidence="0.999322" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999993321428571">
A question is posed over how to evaluate the rela-
tionship between the different distributional models
and brain data. Comparing each model’s predictive
performance using the same strategy as Mitchell et
al. (2008) (also followed by Murphy et al. (2012))
is one possibility: they used multiple regression to
relate distributional codes to individual voxel activa-
tions, thus allowing brain states to be estimated from
previously unseen distributional codes. Regression
models were trained on 58/60 words and in testing
the regression models estimated the brain state as-
sociated with the 2 unseen distributional codes. The
predicted brain states were compared with the actual
fMRI data, and the process repeated for each per-
mutation of left-out words, to build a metric of pre-
diction accuracy. For our purposes, a fair compari-
son of models using this strategy is complicated by
differences in dimensionality between both seman-
tic models and lobes (which we compare to other
lobes) in association with the comparatively small
number of words in the fMRI data set. Large dimen-
sionality models risk overfitting the data, and it is a
nuisance to try to reliably correct for the effects of
overfitting in performance comparisons. Not least,
to thoroughly evaluate all possible cross-validation
permutations is demanding in processing time, and
we have many models to compare.
An alternative approach, and that which we
have adopted, is representational similarity analy-
sis (Kriegeskorte et al., 2008). Representational
similarity analysis circumvents the previous prob-
lems by abstracting each fMRI/distributional data
source to a common structure capturing the inter-
relationships between each pair of data items (e.g.,
words). Specifically, for each model/participant’s
fMRI data/anatomical region, the similarity struc-
ture was evaluated by taking the pairwise correla-
tion (Pearson’s correlation coefficient) between all
unique category or word combinations. This pro-
duced a list of 55 category pair correlations and 121
word pair correlations for each data source. For all
brain data, correlation lists were averaged across the
nine participants to produce a single list of mean
word pair correlations and a single list of mean cat-
egory pair correlations for each anatomical region
and the whole brain. Then to provide a measure of
similarity between models and brain data, the cor-
relation lists for respective data sources were them-
selves correlated using Spearman’s rank correlation.
Statistical significance was tested using a permuta-
tion test: The word-pair (or category-pair) labels
were randomly shuffled 10,000 times to estimate a
null distribution when the two similarity lists are
not correlated. The p-value is calculated as the pro-
portion of random correlation coefficients that are
greater than or equal to the observed coefficient.
</bodyText>
<sectionHeader confidence="0.999979" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.999073">
5.1 Category-level analyses
</subsectionHeader>
<bodyText confidence="0.999765235294118">
Do image models correlate with brain data? Ta-
ble 2 displays results of Spearman’s correlations be-
tween the per-category similarity structure of dis-
tributional models and brain data. There is a sig-
nificant correlation between every purely image-
based model and the occipital, parietal and tempo-
ral lobes, and also the whole brain (.38&lt; p &gt;.51,
all p &lt;.01). The frontal lobe is less well described.
Still, whilst not significant, correlations are only
marginally above the conventional p = .05 cutoff
(all are less than p = .064). This strongly suggests
that the answer to our first question is yes: distri-
butional models derived from images can be used
to explain concept fMRI data. Otherwise Window2
significantly correlates with the whole brain and all
anatomical regions except for the frontal lobe where
p=.34, p = .07. In contrast Verb (the original, par-
tially hand-crafted model used by Mitchell and col-
leagues) captures inter-relationships poorly and nei-
ther correlates with the whole brain or any lobe.
Do different models correlate with different
anatomical regions? 2-way ANOVA without
replication was used to test for differences in cor-
relation coefficients between the five pure-modality
models (Verb, Window2, Object, Context and Ob-
ject&amp;Context), and the four brain lobes. This re-
vealed a highly significant difference between mod-
els F(4,12)=45.2, p&lt;.001. Post-hoc 2-tailed t-tests
comparing model pairs found that Verb differed sig-
nificantly from all other models (correlations were
lower). There was a clear difference even when Verb
(mean±sd over lobes = .1±.1) was compared to the
second weakest model, Object (mean±sd=.4±.09),
where t =-7.7, p &lt;.01, df=4. There were no
</bodyText>
<page confidence="0.984748">
1965
</page>
<table confidence="0.999962">
Frontal Parietal Occipital Temporal Whole-Brain
Verb 0.00 (0.51) 0.06 (0.37) 0.24 (0.10) 0.07 (0.35) 0.17 (0.17)
Window2 0.34 (0.06) 0.49 (0.00) 0.47 (0.01) 0.47 (0.00) 0.44 (0.00)
Object 0.27 (0.07) 0.38 (0.02) 0.45 (0.00) 0.47 (0.00) 0.43 (0.01)
Context 0.33 (0.06) 0.50 (0.00) 0.44 (0.00) 0.44 (0.01) 0.44 (0.01)
Object&amp;Context 0.32 (0.05) 0.48 (0.00) 0.51 (0.00) 0.49 (0.00) 0.49 (0.00)
Window2&amp;Object 0.32 (0.06) 0.45 (0.00) 0.52 (0.00) 0.53 (0.00) 0.49 (0.00)
Window2&amp;Context 0.39 (0.04) 0.57 (0.00) 0.53 (0.00) 0.55 (0.00) 0.51 (0.00)
Window2&amp;Object&amp;Context 0.37 (0.04) 0.52 (0.00) 0.55 (0.00) 0.55 (0.00) 0.53 (0.00)
</table>
<tableCaption confidence="0.992755666666667">
Table 2: Matrix of correlations between each pairwise combination of distributional semantic models and brain data.
Correlations correspond to the pairwise similarity between the 11 categories. In each column the first value corre-
sponds to Spearman’s rank correlation coefficient and the value in parenthesis is the p-value.
</tableCaption>
<table confidence="0.999973888888889">
Frontal Parietal Occipital Temporal Whole-Brain
Verb -0.04 (0.72) 0.09 (0.06) 0.07 (0.20) 0.03 (0.31) 0.07 (0.18)
Window2 0.07 (0.13) 0.19 (0.00) 0.12 (0.06) 0.21 (0.00) 0.13 (0.04)
Object 0.01 (0.40) 0.08 (0.07) 0.17 (0.01) 0.18 (0.00) 0.17 (0.01)
Context 0.04 (0.24) 0.14 (0.01) 0.01 (0.44) 0.12 (0.02) 0.02 (0.38)
Object&amp;Context 0.03 (0.31) 0.13 (0.01) 0.10 (0.07) 0.17 (0.00) 0.11 (0.06)
Window2&amp;Object 0.04 (0.24) 0.16 (0.00) 0.16 (0.01) 0.23 (0.00) 0.17 (0.00)
Window2&amp;Context 0.07 (0.12) 0.20 (0.00) 0.09 (0.11) 0.22 (0.00) 0.11 (0.07)
Window2&amp;Object&amp;Context 0.05 (0.18) 0.18 (0.00) 0.12 (0.05) 0.23 (0.00) 0.13 (0.02)
</table>
<tableCaption confidence="0.974236666666667">
Table 3: Matrix of correlations between each pairwise combination of distributional semantic models and brain data.
Correlations correspond to the pairwise similarity between the 51 words. In each column the first value corresponds
to Spearman’s rank correlation coefficient and the value in parenthesis is the p-value.
</tableCaption>
<bodyText confidence="0.99990575">
other significant differences between models. How-
ever there was a highly significant difference be-
tween lobes F(3,12)=13.77, p &lt;.001. Post-hoc 2-
tailed t-tests comparing lobe pairs found that the
frontal lobe yielded significantly different correla-
tions (lower) than each other lobe. When the frontal
lobe (mean±sd over models = .25±.14) was com-
pared to the second weakest anatomical region, the
parietal lobe (mean±sd=.38±.19), the difference
was highly significant, t =-8, df=3, p &lt;.01. This
introduces the question of whether this difference in
correlations is the result of differences in neural cat-
egory organisation and representation, or differences
in the quality of the signal, which we address next.
Category-level inter-correlations between lobes
were all relatively strong and highly significant. The
occipital lobe was found to be the most distinct, be-
ing similar to the temporal lobe (p=.71, p &lt;.001),
but less so to the parietal and frontal lobes (p=.53,
p &lt;.001 and p=.57, p &lt;.001 respectively). The
temporal lobe shows roughly similar levels of cor-
relation to each other lobe (all .71&lt; p &gt;.73, all
p &lt;.001). The frontal and parietal lobes are related
most strongly to each other (p=.77, p &lt;.001), to a
slightly lesser extent to the temporal lobe (in both
cases p=.73, p &lt;.001) and least so to the occipital
lobe. These strong relationships are consistent with
there being a broadly similar category organisation
across lobes.
To appraise this assertion in the context of the
previously detected difference between the frontal
lobe and all other lobes, we examine the raw cat-
egory pair similarity matrices derived from the oc-
cipital lobe and the frontal lobe (Figure 1). All the
below observations are qualitative. Although it is
difficult to have intuitions about the relative differ-
ences between all category pairs (e.g., whether tools
or furniture should be more similar to animals), we
might reasonably expect some obvious similarities.
For instance, for animals to be visually similar to in-
</bodyText>
<page confidence="0.988612">
1966
</page>
<bodyText confidence="0.999777425531915">
sects and clothing, because all have legs and arms
and curves (of course we would not expect a strong
relationship between insects and clothes in function
or other modalities such as sound), buildings to be
similar to building parts and vehicles (hard edges
and windows), building parts to be similar to furni-
ture (e.g., from Table 1 we see there is some overlap
in category membership between these categories,
such as closet and door) and tools to be similar to
kitchen utensils. All of these relationships are main-
tained in the occipital lobe, and many are visible in
the frontal lobe (including the similarity between in-
sects and clothes), however there are exceptions that
are difficult to explain e.g., within the frontal lobe,
building parts are not similar to furniture, kitchen
utensils are closer to clothing than to tools and ve-
hicles are more similar to clothing than anything
else. As such we conclude that category-level rep-
resentations were similar across lobes with differ-
ences likely due to variation in signal quality be-
tween lobes.
Are text- and image-based semantic models com-
plementary? Turning to the question of whether
text- and image-derived semantic information can
be complementary, we observe from Table 2 that
there is not a single instance of a joint model with
a weaker correlation than its pure-image counter-
part. The Window2 model showed a stronger cor-
relation than the Window2&amp;Object model for the
frontal and parietal lobes, but was weaker than Win-
dow2&amp;Object&amp;Context and Window2&amp;Context in
all tests and was also weaker than any joint model
in whole-brain comparisons. The mean±sd correla-
tions for all purely image-based results pooled over
lobes (3 models * 4 lobes) was .42±.08 in com-
parison to .49±.08 for the joint models. The rel-
ative performance of Object vs. Context vs. Ob-
ject&amp;Context on the four different lobes is preserved
between image-based and joint models: correlating
the 12 combinations using Spearman’s correlation
gives p=.85, p &lt;.001. Differences can be statis-
tically quantified by pooling all image related cor-
relation coefficients for each anatomical region (3
models * 4 regions), as for the respective joint mod-
els, and comparing with a 2-tailed Wilcoxon signed
rank test. Differences were highly significant (W=0,
p &lt;.001,n=12). This evidence accumulates to sug-
</bodyText>
<figureCaption confidence="0.9990155">
Figure 1: Similarity (Pearson correlation) between each
category pair in (top) occipital and (bottom) frontal lobes.
</figureCaption>
<bodyText confidence="0.995425333333333">
gest that text and image-derived semantic informa-
tion can be complementary in interpreting concept
fMRI data.
</bodyText>
<subsectionHeader confidence="0.999579">
5.2 Word-level analyses
</subsectionHeader>
<bodyText confidence="0.998953888888889">
Do image models capture word pair similari-
ties? Per-word results generally corroborate the
relationships observed in the previous section in
the sense that Spearman’s correlation between per-
word and per-category results for the 40 combina-
tions of models and lobes was p=.78, p &lt;.001.
There were differences, most obviously a dramatic
drop in the strength of correlation coefficients for
the per-word results, visible in Table 3. Subsets
</bodyText>
<page confidence="0.991355">
1967
</page>
<bodyText confidence="0.999644339622642">
of per-word image-based models correlated with
three lobes and the whole brain. Correlations corre-
sponding to significance values of p &lt;.05 were ob-
served in the temporal and parietal lobes, for Con-
text, Object&amp;Context and Window2 whereas Ob-
ject was correlated with the occipital and temporal
lobes (p &lt;.05). 2-way ANOVA without replica-
tion was used to test for differences between mod-
els and lobes. This revealed a significant differ-
ence between models (F(4,12)=4.05, p=.027). Post-
hoc t-tests showed that the Window2 model signifi-
cantly differed from (was stronger than) the Context
(t=3.8, p =.03, df=3) and Object&amp;Context models
(t =4.5, p =.02, df=3). There were no other signifi-
cant differences between models. There was again a
significant difference between lobes (F(3,12)=7.89,
p &lt; .01), with the frontal lobe showing the weak-
est correlations. Post-hoc 2-tailed t-tests comparing
lobe-pairs found that the frontal lobe differed signif-
icantly (correlations were weaker) from the parietal
(t =-9, p &lt;.001, df=4) and temporal lobes (t =-6.4,
p &lt;.01, df=4) but not from the occipital lobe (t =-
2.18, p =.09, df=4). No other significant differences
between lobes were observed.
Are there differences between models/lobes?
Word-level inter-correlations between lobes were all
significant and the pattern of differences in correla-
tion strength largely resembled that of the category-
level analyses. The occipital lobe was again most
similar to the temporal lobe (p=.57, p &lt;.001), but
less so to the parietal and frontal lobes (p=.47,
p &lt;.001 and p=.34, p &lt;.001 respectively). The
temporal lobe this time showed stronger correlation
to the parietal (p=.68, p &lt;.001) and frontal lobes
(p=.61, p &lt;.001) than the occipital lobe. The frontal
and parietal lobes were again strongly related to one
another (p=.67, p &lt;.001). These results echo the
category-level findings, that word-level brain activ-
ity is also organised in a similar way across lobes.
Consequently this diminishes our chances of uncov-
ering neat interactions between models and brain ar-
eas (where for instance the Window2 model corre-
lates with the frontal lobe and Object model matches
the occipital lobe). It is however noteworthy that
we can observe some interpretable selectivity in
lobe*model combinations. In particular the Con-
text model better matches the parietal lobe than the
Object model, which in turn better captures the oc-
cipital and temporal lobes (Observations are quali-
tative). Also as we see next, adding text informa-
tion boosts performance in both parietal and tempo-
ral lobes (see Section 2 on our expectations about
information encoded in the lobes).
</bodyText>
<subsectionHeader confidence="0.571699">
Does joining text and image models help word-
</subsectionHeader>
<bodyText confidence="0.999971470588235">
level interpretation? As concerns the benefits of
joining Text and Image information, per-word joint
models were generally stronger than the respective
image-based models. There was one exception:
adding text to the Object model weakened corre-
lation with the occipital lobe. Joint models were
exclusively stronger than Window2 for the tempo-
ral and occipital lobes, and were stronger in 1/3 of
cases for the frontal and parietal lobes. In an anal-
ogous comparison to the per-category analysis, a
Wilcoxon signed rank test was used to examine the
difference made by adding text information to image
models (pooling 3 models over 4 anatomical areas
for both image and joint models). The mean±sd of
image models was .1±.06 whereas for Joint models
it was .15±.07. The difference was highly signifi-
cant (W=1, p &lt;.001, n=12).
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999976285714286">
This study brought together, for the first time, two
recent research lines: The exploration of “seman-
tic spaces” in the brain using distributional semantic
models extracted from corpora, and the extension
of the latter to image-based features. We showed
that image-based distributional semantic measures
significantly correlate with fMRI-based neural sim-
ilarity patterns pertaining to categories of concrete
concepts as well as concrete basic-level concepts ex-
pressed by specific words (although correlations, es-
pecially at the basic-concept level, are rather low,
which might signify the need to develop still more
performant distributional models and/or noise inher-
ent to neural data). Moreover, image-based mod-
els complement a state-of-the-art text-based model,
with the best performance achieved when the two
modalities are combined. This not only presents an
optimistic outlook for the future use of image-based
models as an interpretative tool to explore issues of
cognitive grounding, but also demonstrates that they
are capturing useful additional aspects of meaning to
</bodyText>
<page confidence="0.98437">
1968
</page>
<bodyText confidence="0.999909814814815">
the text models, which are likely relevant for com-
putational semantic tasks.
The weak comparative performance of the origi-
nal Mitchell et al.’s Verb model is perhaps surprising
given its previous success in prediction (Mitchell et
al., 2008), but a useful reminder that a good predic-
tor does not necessarily have to capture the internal
structure of the data it predicts.
The lack of finding organisational differences be-
tween anatomical regions differentially described by
the various models is perhaps disappointing, but not
uncontroversial, given that the dataset was not origi-
nally designed to tease apart visual information from
linguistic context. It is however interesting that
in the more challenging word-level analysis some
meaningful trend was visible. In future experiments
it may prove valuable to configure a fMRI stimulus
set where text-based and image-based interrelation-
ships are maximally different. Collecting our own
fMRI data will also allow us to move beyond ex-
ploratory analysis, to test sharper predictions about
distributional models and their brain area correlates.
There are also many opportunities for focusing anal-
yses on different subsets of brain regions, with the
semantic system identified by Binder et al. (2009) in
particular presenting one interesting avenue for in-
vestigation.
</bodyText>
<sectionHeader confidence="0.997663" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9864095">
This research was partially funded by a Google Re-
search Award to the fifth author.
</bodyText>
<sectionHeader confidence="0.998314" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998854358208956">
Shane Bergsma and Randy Goebel. 2011. Using visual
information to predict lexical preference. In Proceed-
ings of RANLP, pages 399–405, Hissar, Bulgaria.
Jeffrey R. Binder, Rutvik H. Desai, William W. Graves,
and Lisa L. Conant. 2009. Where is the semantic
system? a critical review and meta-analysis of 120
functional neuroimaging studies. Cerebral Cortexl,
12:2767–2796.
Vicki Bruce, Patrick R Green, and Georgeson Mark A.
2003. Visual perception: Physiology, psychology, and
ecology. Psychology Pr.
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In Pro-
ceedings of the EMNLP GEMS Workshop, pages 22–
32, Edinburgh, UK.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012a. Distributional semantics in
Technicolor. In Proceedings of ACL, pages 136–145,
Jeju Island, Korea.
Elia Bruni, Jasper Uijlings, Marco Baroni, and Nicu
Sebe. 2012b. Distributional semantics with eyes: Us-
ing image analysis to improve computational represen-
tations of word meaning. In Proceedings ofACMMul-
timedia, pages 1219–1228, Nara, Japan.
Elia Bruni, Ulisse Bordignon, Adam Liska, Jasper Ui-
jlings, and Irina Sergienya. 2013. Vsem: An open li-
brary for visual semantics representation. In Proceed-
ings of ACL, Sofia, Bulgaria.
John Bullinaria and Joseph Levy. 2007. Extract-
ing semantic representations from word co-occurrence
statistics: A computational study. Behavior Research
Methods, 39:510–526.
Kai-min Chang, Tom Mitchell, and Marcel Just. 2011.
Quantitative modeling of the neural representation of
objects: How semantic feature norms can account for
fMRI activation. NeuroImage, 56:716–727.
Linda L Chao, James V Haxby, and Alex Martin. 1999.
Attribute-based neural substrates in temporal cortex
for perceiving and knowing about objects. Nature neu-
roscience, 2(10):913–919.
Kenneth Church and Peter Hanks. 1990. Word asso-
ciation norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22–29.
Stephen Clark. 2012. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.
Gabriella Csurka, Christopher Dance, Lixin Fan, Jutta
Willamowski, and C´edric Bray. 2004. Visual cate-
gorization with bags of keypoints. In In Workshop on
Statistical Learning in Computer Vision, ECCV, pages
1–22, Prague, Czech Republic.
Jia Deng, Wei Dong, Richard Socher, Lia-Ji Li, and
Li Fei-Fei. 2009. Imagenet: A large-scale hierarchi-
cal image database. In Proceedings of CVPR, pages
248–255, Miami Beach, FL.
Russell A Epstein. 2008. Parahippocampal and ret-
rosplenial contributions to human spatial navigation.
Trends in cognitive sciences, 12(10):388–396.
Katrin Erk. 2012. Vector space models of word meaning
and phrase meaning: A survey. Language and Lin-
guistics Compass, 6(10):635–653.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Proceedings of
HLT-NAACL, pages 91–99, Los Angeles, CA.
</reference>
<page confidence="0.923167">
1969
</page>
<reference confidence="0.999741928571429">
Melvyn A. Goodale and David Milner. 1992. Separate
visual pathways for perception and action. Trends in
Neurosciences, 15:20–25.
Kristen Grauman and Bastian Leibe. 2011. Visual Object
Recognition. Morgan &amp; Claypool, San Francisco.
Peter Hagoort. 2005. On Broca, brain, and bind-
ing: a new framework. Trends in cognitive sciences,
9(9):416–423.
James Haxby, Ida Gobbini, Maura Furey, Alumit Ishai,
Jennifer Schouten, and Pietro Pietrini. 2001. Dis-
tributed and overlapping representations of faces and
objects in ventral temporal cortex. Science, 293:2425–
2430.
Alexander Huth, Shinji Nishimoto, An Vu, and Jack Gal-
lant. 2012. A continuous semantic space describes the
representation of thousands of object and action cate-
gories across the human brain. Neuron, 76(6):1210–
1224.
Nancy Kanwisher and Galit Yovel. 2006. The fusiform
face area: a cortical region specialized for the percep-
tion of faces. Philosophical Transactions of the Royal
Society B: Biological Sciences, 361(1476):2109–2128.
Nikolaus Kriegeskorte, Marieke Mur, and Peter Ban-
dettini. 2008. Representational similarity analysis–
connecting the branches of systems neuroscience.
Frontiers in systems neuroscience, 2.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.
2006. Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories. In
Proceedings of CVPR, pages 2169–2178, Washington,
DC.
Chee Wee Leong and Rada Mihalcea. 2011. Going
beyond text: A hybrid image-text approach for mea-
suring word relatedness. In Proceedings of IJCNLP,
pages 1403–1407.
Max Louwerse. 2011. Symbol interdependency in sym-
bolic and embodied cognition. Topics in Cognitive
Science, 3:273–302.
David G Lowe. 2004. Distinctive Image Features from
Scale-Invariant Keypoints. International Journal of
Computer Vision, 60:91–110.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28:203–208.
Bruce D McCandliss, Laurent Cohen, and Stanislas De-
haene. 2003. The visual word form area: expertise
for reading in the fusiform gyrus. Trends in cognitive
sciences, 7(7):293–299.
Robert D McIntosh and Thomas Schenk. 2009. Two vi-
sual streams for perception and action: Current trends.
Neuropsychologia, 47(6):1391–1396.
Earl K Miller, David J Freedman, and Jonathan D Wal-
lis. 2002. The prefrontal cortex: categories, con-
cepts and cognition. Philosophical Transactions of
the Royal Society of London. Series B: Biological Sci-
ences, 357(1424):1123–1136.
Tom Mitchell, Svetlana Shinkareva, Andrew Carlson,
Kai-Min Chang, Vincente Malave, Robert Mason, and
Marcel Just. 2008. Predicting human brain activ-
ity associated with the meanings of nouns. Science,
320:1191–1195.
Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012.
Selecting corpus-semantic models for neurolinguistic
decoding. In Proceedings of *SEM, pages 114–123,
Montreal, Canada.
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton, and
Tom Mitchell. 2009. Zero-shot learning with seman-
tic output codes. In Proceedings of NIPS, pages 1410–
1418, Vancouver, Canada.
Marius V Peelen and Paul E Downing. 2005. Selectivity
for the human body in the fusiform gyrus. Journal of
Neurophysiology, 93(1):603–608.
Francisco Pereira, Greg Detre, and Matthew Botvinick.
2011. Generating text from functional brain images.
Frontiers in Human Neuroscience, 5(72). Published
online: http://www.frontiersin.org/
human_neuroscience/10.3389/fnhum.
2011.00072/abstract.
Alexander T Sack. 2009. Parietal cortex and spatial cog-
nition. Behavioural brain research, 202(2):153–161.
Hinrich Sch¨utze. 1997. Ambiguity Resolution in Natural
Language Learning. CSLI, Stanford, CA.
Carina Silberer and Mirella Lapata. 2013. Models of
semantic representation with visual attributes. In Pro-
ceedings of ACL, Sofia, Bulgaria.
Josef Sivic and Andrew Zisserman. 2003. Video Google:
A text retrieval approach to object matching in videos.
In Proceedings of ICCV, pages 1470–1477, Nice,
France.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141–188.
N Tzourio-Mazoyer, B Landeau, D Papathanassiou,
F Crivello, O Etard, N Delcroix, B Mazoyer, and M Jo-
liot. 2002. Automated anatomical labeling of activa-
tions in SPM using a macroscopic anatomical parcel-
lation of the MNI MRI single-subject brain. Neuroim-
age, 15(1):273–289.
</reference>
<page confidence="0.99278">
1970
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.749489">
<title confidence="0.935505">Of words, eyes and brains: Correlating image-based distributional semantic with neural representations of concepts</title>
<author confidence="0.990162">Andrew J Anderson</author>
<author confidence="0.990162">Elia Bruni</author>
<author confidence="0.990162">Ulisse Bordignon</author>
<author confidence="0.990162">Massimo Poesio</author>
<author confidence="0.990162">Marco</author>
<affiliation confidence="0.86254">Center for Mind/Brain Sciences (University of Trento, C.so Bettini 31, Rovereto,</affiliation>
<email confidence="0.999246">first.last@unitn.it</email>
<abstract confidence="0.999745966666667">Traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora. Recently, the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections. These image-based models should be complementary to text-based ones, providing a more cognitively plausible view of meaning grounded in visual perception. In this study, we test whether image-based models capture the semantic patterns that emerge from fMRI recordings of the neural signal. Our results indicate that, indeed, there is a significant correlation between image-based and brain-based semantic similarities, and that image-based models complement text-based ones, so that the best correlations are achieved when the two modalities are combined. Despite some unsatisfactory, but explained outcomes (in particular, failure to detect differential association of models with brain areas), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Randy Goebel</author>
</authors>
<title>Using visual information to predict lexical preference.</title>
<date>2011</date>
<booktitle>In Proceedings of RANLP,</booktitle>
<pages>399--405</pages>
<location>Hissar, Bulgaria.</location>
<contexts>
<context position="13553" citStr="Bergsma and Goebel, 2011" startWordPosition="2080" endWordPosition="2083">tual input, relying on the distributional hypothesis: The meaning of a word can be induced by the context in which it occurs (Turney and Pantel, 2010). Despite their great success, these models still rely on verbal input only, while humans base their meaning representation also on perceptual information (Louwerse, 2011). Thanks to recent developments in computer vision, it is nowadays possible to take the visual perceptual channel into account, and build new computational models of semantics enhanced with visual information (Feng and Lapata, 2010; Bruni et al., 2011; Leong and Mihalcea, 2011; Bergsma and Goebel, 2011; Bruni et al., 2012a). Given a set of target concepts and a collection of images depicting those concepts, it is indeed possible to first encode the image content into low-level features, and subsequently convert it into a higher-level representation based on the bag-of-visual-words method (Grauman and Leibe, 2011). Recently, Bruni et al. (2012b) have shown that better semantic representations can be extracted if we first localize the concept in the image, and then extract distinct higher-level features (visual words) from the box containing the concept and from the surrounding context. We al</context>
</contexts>
<marker>Bergsma, Goebel, 2011</marker>
<rawString>Shane Bergsma and Randy Goebel. 2011. Using visual information to predict lexical preference. In Proceedings of RANLP, pages 399–405, Hissar, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey R Binder</author>
<author>Rutvik H Desai</author>
<author>William W Graves</author>
<author>Lisa L Conant</author>
</authors>
<title>Where is the semantic system? a critical review and meta-analysis of 120 functional neuroimaging studies. Cerebral Cortexl,</title>
<date>2009</date>
<pages>12--2767</pages>
<contexts>
<context position="10548" citStr="Binder et al., 2009" startWordPosition="1621" endWordPosition="1624">role in object categorisation (e.g., animals and tools (Chao et al., 1999), faces (Kanwisher and Yovel, 2006), body parts (Peelen and Downing, 2005) and even word form perception (McCandliss et al., 2003)). As the parahippocampus is strongly associated with scene representation (Epstein, 2008), we expect both the Object and Context models to capture variability in the temporal lobe. Of wider relevance to semantic processing, the medial temporal gyrus, inferior temporal gyrus and ventral temporal lobe have generally been implicated to have roles in supramodal integration and concept retrieval (Binder et al., 2009). Given this, we expected that incorporating text would also be valuable and that the Window2&amp;Object&amp;Context combination would be a good model. The visual action stream leads from the occipital lobe to the parietal lobe to support spatial cognition tasks and action control (Sack, 2009). In that there seems to be an egocentric frame of reference, placing actor in environment, it is tempting to speculate that the Context model is more appropriate than the Object model here. As the parietal lobe also contains the angular gyrus, thought to be involved in complex, supra-modal information integratio</context>
<context position="11839" citStr="Binder et al., 2009" startWordPosition="1817" endWordPosition="1820">t that integrating text and image information would boost performance, so Window2&amp;Context was earmarked as a strong candidate. The frontal lobe, is traditionally associated with high-level processing and manipulation of abstract knowledge and rules and controlled behaviour (Miller et al., 2002). Regarding semantics, the dorsomedial prefrontal cortex has been implicated in self-guided retrieval of semantic information (e.g., uncued speech production), the ventromedial prefrontal cortex in motivation and emotional processing, the inferior frontal gyrus in phonological and syntactic processing, (Binder et al., 2009) and integration of lexical information (Hagoort, 2005). Given the association with linguistic processing we anticipated a bias in favour of Window2. The four lobes were identified and partitioned using Tzourio-Mazoyer et al. (2002)’s automatic anatomical labelling scheme. 1962 Voxel selection The set of 500 most stable voxels, both within the whole brain and from within each region of interest were identified for analysis. The most stable voxels were those showing consistent variation across the different stimuli between scanning sessions. Specifically, and following a similar strategy to Mit</context>
</contexts>
<marker>Binder, Desai, Graves, Conant, 2009</marker>
<rawString>Jeffrey R. Binder, Rutvik H. Desai, William W. Graves, and Lisa L. Conant. 2009. Where is the semantic system? a critical review and meta-analysis of 120 functional neuroimaging studies. Cerebral Cortexl, 12:2767–2796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vicki Bruce</author>
<author>Patrick R Green</author>
<author>Georgeson Mark A</author>
</authors>
<title>Visual perception: Physiology, psychology, and ecology.</title>
<date>2003</date>
<publisher>Psychology Pr.</publisher>
<contexts>
<context position="9280" citStr="Bruce et al., 2003" startWordPosition="1432" endWordPosition="1435"> Vegetable Celery, Corn, Lettuce, Tomato Vehicle Airplane, Bicycle, Car, Train, Truck Table 1: The 51 words represented by the brain and the distributional models, organized by category. bination of the two, and Window2 is a text-based model. The occipital lobe houses the primary visual processing system and consequently it is reasonable to expect some bias toward image-based semantic models. Furthermore, given that experimental stimuli incorporated line drawings of the object,and the visual cortex has a well-established role in processing low-level visual statistics including edge detection (Bruce et al., 2003), we naturally expected a good performance from Object (formulated from edge orientation histograms of similar objects). Following Goodale and Milner (1992)’s influential perception-action model (see McIntosh and Schenk (2009) for recent discussion), visual information is channeled from the occipital lobe in two streams: a perceptual stream, serving object identification and recognition; and an action stream, specialist in processing egocentric spatial relationships and ultimately supporting interaction with the world. The perceptual stream leads to the temporal lobe. Here the fusiform gyrus (</context>
</contexts>
<marker>Bruce, Green, A, 2003</marker>
<rawString>Vicki Bruce, Patrick R Green, and Georgeson Mark A. 2003. Visual perception: Physiology, psychology, and ecology. Psychology Pr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Giang Binh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Distributional semantics from text and images.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP GEMS Workshop,</booktitle>
<pages>22--32</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="4311" citStr="Bruni et al., 2011" startWordPosition="659" endWordPosition="662">e extensive experiments reported in Murphy et al. (2012), who showed that purely corpus-based distributional models are at least as good at brain signal prediction tasks as earlier models that made use of manually-generated or controlled knowledge sources (Chang et al., 2011; Palatucci et al., 2009; Pereira et al., 2011), and we evaluate a very recent type of distributional model, namely one that is not extracted from textual data but from image collections through automated visual feature extraction techniques. It has been argued that this new generation of image-based distributional models (Bruni et al., 2011; Bruni et al., 2012b; Feng and Lapata, 2010; Leong and Mihalcea, 2011) provides a more realistic view of meaning, since humans obviously acquire a large proportion of their semantic knowledge from perceptual data. The first question that we ask, thus, is whether the more “grounded” image-based models can help us in interpreting conceptual representations in the brain. More specifically, we will compare the performance of different image-based representations, and we will test whether text- and image-based representations are complementary, so that when used together they can better account fo</context>
<context position="13501" citStr="Bruni et al., 2011" startWordPosition="2072" endWordPosition="2075">f word co-occurrence statistics from large textual input, relying on the distributional hypothesis: The meaning of a word can be induced by the context in which it occurs (Turney and Pantel, 2010). Despite their great success, these models still rely on verbal input only, while humans base their meaning representation also on perceptual information (Louwerse, 2011). Thanks to recent developments in computer vision, it is nowadays possible to take the visual perceptual channel into account, and build new computational models of semantics enhanced with visual information (Feng and Lapata, 2010; Bruni et al., 2011; Leong and Mihalcea, 2011; Bergsma and Goebel, 2011; Bruni et al., 2012a). Given a set of target concepts and a collection of images depicting those concepts, it is indeed possible to first encode the image content into low-level features, and subsequently convert it into a higher-level representation based on the bag-of-visual-words method (Grauman and Leibe, 2011). Recently, Bruni et al. (2012b) have shown that better semantic representations can be extracted if we first localize the concept in the image, and then extract distinct higher-level features (visual words) from the box containing</context>
<context position="20266" citStr="Bruni et al. (2011)" startWordPosition="3139" endWordPosition="3142">ts. First, we transform them into nonnegative Pointwise Mutual Information (PMI) association scores (Church and Hanks, 1990). As a second transformation, we apply dimensionality reduction to the two matrices. In particular, we adopt the Singular Value Decomposition (SVD), one of the most effective methods to approximate the original data in lower dimensionality space (Sch¨utze, 1997), and reduce the vectors to 50 dimensions. To combine text- and image-based semantic models in a joint representation, we separately normalize their vectors to unit length, and concatenate them, along the lines of Bruni et al. (2011). More sophisticated combination models have been proposed in the recent literature on multimodal semantics. For example, Bruni et al. (2012a) use SVD as a mixing strategy, given its ability to smooth the matrices and uncover latent dimensions. Another example is Silberer and Lapata (2013), where Canonical Correlation Analysis is used. We reserve the exploration of more advanced combination methods for further studies. Finally, to represent the 11 categories we experiment with (see Table 1), we average the vectors of the concepts they include. 4We use k-means, the most commonly employed cluste</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2011</marker>
<rawString>Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011. Distributional semantics from text and images. In Proceedings of the EMNLP GEMS Workshop, pages 22– 32, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>Nam Khanh Tran</author>
</authors>
<title>Distributional semantics in Technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>136--145</pages>
<location>Jeju Island,</location>
<contexts>
<context position="4331" citStr="Bruni et al., 2012" startWordPosition="663" endWordPosition="666">nts reported in Murphy et al. (2012), who showed that purely corpus-based distributional models are at least as good at brain signal prediction tasks as earlier models that made use of manually-generated or controlled knowledge sources (Chang et al., 2011; Palatucci et al., 2009; Pereira et al., 2011), and we evaluate a very recent type of distributional model, namely one that is not extracted from textual data but from image collections through automated visual feature extraction techniques. It has been argued that this new generation of image-based distributional models (Bruni et al., 2011; Bruni et al., 2012b; Feng and Lapata, 2010; Leong and Mihalcea, 2011) provides a more realistic view of meaning, since humans obviously acquire a large proportion of their semantic knowledge from perceptual data. The first question that we ask, thus, is whether the more “grounded” image-based models can help us in interpreting conceptual representations in the brain. More specifically, we will compare the performance of different image-based representations, and we will test whether text- and image-based representations are complementary, so that when used together they can better account for patterns in neural</context>
<context position="13573" citStr="Bruni et al., 2012" startWordPosition="2084" endWordPosition="2087"> distributional hypothesis: The meaning of a word can be induced by the context in which it occurs (Turney and Pantel, 2010). Despite their great success, these models still rely on verbal input only, while humans base their meaning representation also on perceptual information (Louwerse, 2011). Thanks to recent developments in computer vision, it is nowadays possible to take the visual perceptual channel into account, and build new computational models of semantics enhanced with visual information (Feng and Lapata, 2010; Bruni et al., 2011; Leong and Mihalcea, 2011; Bergsma and Goebel, 2011; Bruni et al., 2012a). Given a set of target concepts and a collection of images depicting those concepts, it is indeed possible to first encode the image content into low-level features, and subsequently convert it into a higher-level representation based on the bag-of-visual-words method (Grauman and Leibe, 2011). Recently, Bruni et al. (2012b) have shown that better semantic representations can be extracted if we first localize the concept in the image, and then extract distinct higher-level features (visual words) from the box containing the concept and from the surrounding context. We also follow this strat</context>
<context position="15518" citStr="Bruni et al., 2012" startWordPosition="2389" endWordPosition="2392">cates of our distributional model we select a set of 30K words, namely the top 20K most frequent nouns, 5K most frequent adjectives and 5K most frequent verbs. In the tradition of HAL (Lund and Burgess, 1996), the model is based on co-occurrence statistics with collocates within a fixed-size window of 2 to the left and right of each target word. Despite their simplicity, narrow-window-based models have shown to achieve state-of-the-art results in various standard semantic tasks (Bullinaria and Levy, 2007) and to outperform both document-based and syntaxbased models trained on the same corpus (Bruni et al., 2012a). Moreover, in Murphy et al. (2012) a window-based model very similar to ours was not significantly worse than their best model for brain decoding. We tried also a few variations, e.g., using a larger window or different transformations on the raw co-occurrences from those presented below, but with little, insignificant changes in performance. Given that our focus here is on visual information, we only report results for Window2 and its combination with visual models. 3.2 Visual models Our visual models are inspired by Bruni et al. (2012b), that have explored to what extent extracting featur</context>
<context position="20406" citStr="Bruni et al. (2012" startWordPosition="3160" endWordPosition="3163">nsformation, we apply dimensionality reduction to the two matrices. In particular, we adopt the Singular Value Decomposition (SVD), one of the most effective methods to approximate the original data in lower dimensionality space (Sch¨utze, 1997), and reduce the vectors to 50 dimensions. To combine text- and image-based semantic models in a joint representation, we separately normalize their vectors to unit length, and concatenate them, along the lines of Bruni et al. (2011). More sophisticated combination models have been proposed in the recent literature on multimodal semantics. For example, Bruni et al. (2012a) use SVD as a mixing strategy, given its ability to smooth the matrices and uncover latent dimensions. Another example is Silberer and Lapata (2013), where Canonical Correlation Analysis is used. We reserve the exploration of more advanced combination methods for further studies. Finally, to represent the 11 categories we experiment with (see Table 1), we average the vectors of the concepts they include. 4We use k-means, the most commonly employed clustering algorithm for this task. 5http://clic.cimec.unitn.it/vsem/ 1964 4 Experiments A question is posed over how to evaluate the relationship</context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Elia Bruni, Gemma Boleda, Marco Baroni, and Nam Khanh Tran. 2012a. Distributional semantics in Technicolor. In Proceedings of ACL, pages 136–145, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Jasper Uijlings</author>
<author>Marco Baroni</author>
<author>Nicu Sebe</author>
</authors>
<title>Distributional semantics with eyes: Using image analysis to improve computational representations of word meaning.</title>
<date>2012</date>
<booktitle>In Proceedings ofACMMultimedia,</booktitle>
<pages>1219--1228</pages>
<location>Nara, Japan.</location>
<contexts>
<context position="4331" citStr="Bruni et al., 2012" startWordPosition="663" endWordPosition="666">nts reported in Murphy et al. (2012), who showed that purely corpus-based distributional models are at least as good at brain signal prediction tasks as earlier models that made use of manually-generated or controlled knowledge sources (Chang et al., 2011; Palatucci et al., 2009; Pereira et al., 2011), and we evaluate a very recent type of distributional model, namely one that is not extracted from textual data but from image collections through automated visual feature extraction techniques. It has been argued that this new generation of image-based distributional models (Bruni et al., 2011; Bruni et al., 2012b; Feng and Lapata, 2010; Leong and Mihalcea, 2011) provides a more realistic view of meaning, since humans obviously acquire a large proportion of their semantic knowledge from perceptual data. The first question that we ask, thus, is whether the more “grounded” image-based models can help us in interpreting conceptual representations in the brain. More specifically, we will compare the performance of different image-based representations, and we will test whether text- and image-based representations are complementary, so that when used together they can better account for patterns in neural</context>
<context position="13573" citStr="Bruni et al., 2012" startWordPosition="2084" endWordPosition="2087"> distributional hypothesis: The meaning of a word can be induced by the context in which it occurs (Turney and Pantel, 2010). Despite their great success, these models still rely on verbal input only, while humans base their meaning representation also on perceptual information (Louwerse, 2011). Thanks to recent developments in computer vision, it is nowadays possible to take the visual perceptual channel into account, and build new computational models of semantics enhanced with visual information (Feng and Lapata, 2010; Bruni et al., 2011; Leong and Mihalcea, 2011; Bergsma and Goebel, 2011; Bruni et al., 2012a). Given a set of target concepts and a collection of images depicting those concepts, it is indeed possible to first encode the image content into low-level features, and subsequently convert it into a higher-level representation based on the bag-of-visual-words method (Grauman and Leibe, 2011). Recently, Bruni et al. (2012b) have shown that better semantic representations can be extracted if we first localize the concept in the image, and then extract distinct higher-level features (visual words) from the box containing the concept and from the surrounding context. We also follow this strat</context>
<context position="15518" citStr="Bruni et al., 2012" startWordPosition="2389" endWordPosition="2392">cates of our distributional model we select a set of 30K words, namely the top 20K most frequent nouns, 5K most frequent adjectives and 5K most frequent verbs. In the tradition of HAL (Lund and Burgess, 1996), the model is based on co-occurrence statistics with collocates within a fixed-size window of 2 to the left and right of each target word. Despite their simplicity, narrow-window-based models have shown to achieve state-of-the-art results in various standard semantic tasks (Bullinaria and Levy, 2007) and to outperform both document-based and syntaxbased models trained on the same corpus (Bruni et al., 2012a). Moreover, in Murphy et al. (2012) a window-based model very similar to ours was not significantly worse than their best model for brain decoding. We tried also a few variations, e.g., using a larger window or different transformations on the raw co-occurrences from those presented below, but with little, insignificant changes in performance. Given that our focus here is on visual information, we only report results for Window2 and its combination with visual models. 3.2 Visual models Our visual models are inspired by Bruni et al. (2012b), that have explored to what extent extracting featur</context>
<context position="20406" citStr="Bruni et al. (2012" startWordPosition="3160" endWordPosition="3163">nsformation, we apply dimensionality reduction to the two matrices. In particular, we adopt the Singular Value Decomposition (SVD), one of the most effective methods to approximate the original data in lower dimensionality space (Sch¨utze, 1997), and reduce the vectors to 50 dimensions. To combine text- and image-based semantic models in a joint representation, we separately normalize their vectors to unit length, and concatenate them, along the lines of Bruni et al. (2011). More sophisticated combination models have been proposed in the recent literature on multimodal semantics. For example, Bruni et al. (2012a) use SVD as a mixing strategy, given its ability to smooth the matrices and uncover latent dimensions. Another example is Silberer and Lapata (2013), where Canonical Correlation Analysis is used. We reserve the exploration of more advanced combination methods for further studies. Finally, to represent the 11 categories we experiment with (see Table 1), we average the vectors of the concepts they include. 4We use k-means, the most commonly employed clustering algorithm for this task. 5http://clic.cimec.unitn.it/vsem/ 1964 4 Experiments A question is posed over how to evaluate the relationship</context>
</contexts>
<marker>Bruni, Uijlings, Baroni, Sebe, 2012</marker>
<rawString>Elia Bruni, Jasper Uijlings, Marco Baroni, and Nicu Sebe. 2012b. Distributional semantics with eyes: Using image analysis to improve computational representations of word meaning. In Proceedings ofACMMultimedia, pages 1219–1228, Nara, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Ulisse Bordignon</author>
<author>Adam Liska</author>
<author>Jasper Uijlings</author>
<author>Irina Sergienya</author>
</authors>
<title>Vsem: An open library for visual semantics representation.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="19477" citStr="Bruni et al., 2013" startWordPosition="3021" endWordPosition="3024">the SIFT features into 25K different clusters.4 We add also spatial information by dividing the image into several subregions, representing each of them in terms of BoVW and then stacking the resulting histograms (Lazebnik et al., 2006). We use in total 8 different regions, obtaining a final vector of 200K dimensions (25K visual words x 8 regions). Since each concept in our dataset is represented by multiple images, we pool the visual word occurrences across images by summing them up into a single vector. To perform the entire visual pipeline we use VSEM, an open library for visual semantics (Bruni et al., 2013).5 3.3 Model transformations and combination Once both the textual and the visual models are built, we perform two different transformations on the raw co-occurrence counts. First, we transform them into nonnegative Pointwise Mutual Information (PMI) association scores (Church and Hanks, 1990). As a second transformation, we apply dimensionality reduction to the two matrices. In particular, we adopt the Singular Value Decomposition (SVD), one of the most effective methods to approximate the original data in lower dimensionality space (Sch¨utze, 1997), and reduce the vectors to 50 dimensions. T</context>
</contexts>
<marker>Bruni, Bordignon, Liska, Uijlings, Sergienya, 2013</marker>
<rawString>Elia Bruni, Ulisse Bordignon, Adam Liska, Jasper Uijlings, and Irina Sergienya. 2013. Vsem: An open library for visual semantics representation. In Proceedings of ACL, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Bullinaria</author>
<author>Joseph Levy</author>
</authors>
<title>Extracting semantic representations from word co-occurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<pages>39--510</pages>
<contexts>
<context position="15410" citStr="Bullinaria and Levy, 2007" startWordPosition="2371" endWordPosition="2374">atistics from the freely available ukWaC and Wackypedia corpora combined (about 3 billion words in total).2 As collocates of our distributional model we select a set of 30K words, namely the top 20K most frequent nouns, 5K most frequent adjectives and 5K most frequent verbs. In the tradition of HAL (Lund and Burgess, 1996), the model is based on co-occurrence statistics with collocates within a fixed-size window of 2 to the left and right of each target word. Despite their simplicity, narrow-window-based models have shown to achieve state-of-the-art results in various standard semantic tasks (Bullinaria and Levy, 2007) and to outperform both document-based and syntaxbased models trained on the same corpus (Bruni et al., 2012a). Moreover, in Murphy et al. (2012) a window-based model very similar to ours was not significantly worse than their best model for brain decoding. We tried also a few variations, e.g., using a larger window or different transformations on the raw co-occurrences from those presented below, but with little, insignificant changes in performance. Given that our focus here is on visual information, we only report results for Window2 and its combination with visual models. 3.2 Visual models</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>John Bullinaria and Joseph Levy. 2007. Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior Research Methods, 39:510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai-min Chang</author>
<author>Tom Mitchell</author>
<author>Marcel Just</author>
</authors>
<title>Quantitative modeling of the neural representation of objects: How semantic feature norms can account for fMRI activation.</title>
<date>2011</date>
<journal>NeuroImage,</journal>
<pages>56--716</pages>
<contexts>
<context position="3968" citStr="Chang et al., 2011" startWordPosition="603" endWordPosition="606"> is approximating, in interesting ways, the way in which humans acquire and represent semantic knowledge. 1960 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1960–1970, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics We take as our starting point the extensive experiments reported in Murphy et al. (2012), who showed that purely corpus-based distributional models are at least as good at brain signal prediction tasks as earlier models that made use of manually-generated or controlled knowledge sources (Chang et al., 2011; Palatucci et al., 2009; Pereira et al., 2011), and we evaluate a very recent type of distributional model, namely one that is not extracted from textual data but from image collections through automated visual feature extraction techniques. It has been argued that this new generation of image-based distributional models (Bruni et al., 2011; Bruni et al., 2012b; Feng and Lapata, 2010; Leong and Mihalcea, 2011) provides a more realistic view of meaning, since humans obviously acquire a large proportion of their semantic knowledge from perceptual data. The first question that we ask, thus, is w</context>
</contexts>
<marker>Chang, Mitchell, Just, 2011</marker>
<rawString>Kai-min Chang, Tom Mitchell, and Marcel Just. 2011. Quantitative modeling of the neural representation of objects: How semantic feature norms can account for fMRI activation. NeuroImage, 56:716–727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linda L Chao</author>
<author>James V Haxby</author>
<author>Alex Martin</author>
</authors>
<title>Attribute-based neural substrates in temporal cortex for perceiving and knowing about objects.</title>
<date>1999</date>
<journal>Nature neuroscience,</journal>
<volume>2</volume>
<issue>10</issue>
<contexts>
<context position="10002" citStr="Chao et al., 1999" startWordPosition="1536" endWordPosition="1539">imilar objects). Following Goodale and Milner (1992)’s influential perception-action model (see McIntosh and Schenk (2009) for recent discussion), visual information is channeled from the occipital lobe in two streams: a perceptual stream, serving object identification and recognition; and an action stream, specialist in processing egocentric spatial relationships and ultimately supporting interaction with the world. The perceptual stream leads to the temporal lobe. Here the fusiform gyrus (shared with the occipital lobe) plays a general role in object categorisation (e.g., animals and tools (Chao et al., 1999), faces (Kanwisher and Yovel, 2006), body parts (Peelen and Downing, 2005) and even word form perception (McCandliss et al., 2003)). As the parahippocampus is strongly associated with scene representation (Epstein, 2008), we expect both the Object and Context models to capture variability in the temporal lobe. Of wider relevance to semantic processing, the medial temporal gyrus, inferior temporal gyrus and ventral temporal lobe have generally been implicated to have roles in supramodal integration and concept retrieval (Binder et al., 2009). Given this, we expected that incorporating text woul</context>
</contexts>
<marker>Chao, Haxby, Martin, 1999</marker>
<rawString>Linda L Chao, James V Haxby, and Alex Martin. 1999. Attribute-based neural substrates in temporal cortex for perceiving and knowing about objects. Nature neuroscience, 2(10):913–919.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Peter Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="19771" citStr="Church and Hanks, 1990" startWordPosition="3062" endWordPosition="3065">al vector of 200K dimensions (25K visual words x 8 regions). Since each concept in our dataset is represented by multiple images, we pool the visual word occurrences across images by summing them up into a single vector. To perform the entire visual pipeline we use VSEM, an open library for visual semantics (Bruni et al., 2013).5 3.3 Model transformations and combination Once both the textual and the visual models are built, we perform two different transformations on the raw co-occurrence counts. First, we transform them into nonnegative Pointwise Mutual Information (PMI) association scores (Church and Hanks, 1990). As a second transformation, we apply dimensionality reduction to the two matrices. In particular, we adopt the Singular Value Decomposition (SVD), one of the most effective methods to approximate the original data in lower dimensionality space (Sch¨utze, 1997), and reduce the vectors to 50 dimensions. To combine text- and image-based semantic models in a joint representation, we separately normalize their vectors to unit length, and concatenate them, along the lines of Bruni et al. (2011). More sophisticated combination models have been proposed in the recent literature on multimodal semanti</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Church and Peter Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>Vector space models of lexical meaning.</title>
<date>2012</date>
<booktitle>Handbook of Contemporary Semantics, 2nd edition.</booktitle>
<editor>In Shalom Lappin and Chris Fox, editors,</editor>
<publisher>In press.</publisher>
<location>Blackwell, Malden, MA.</location>
<contexts>
<context position="2013" citStr="Clark, 2012" startWordPosition="296" endWordPosition="297">ool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility. 1 Introduction Many recent neuroscientific studies have brought support to the view that concepts are represented in terms of patterns of neural activation over broad areas, naturally encoded as vectors in a neural semantic space (Haxby et al., 2001; Huth et al., 2012). Similar representations are also widely used in computational linguistics, and in particular in distributional semantics (Clark, 2012; Erk, 2012; Turney and Pantel, 2010), that captures meaning in terms of vectors recording the patterns of co-occurrence of words in large corpora, under the hypothesis that words that occur in similar contexts are similar in meaning. Since the seminal work of Mitchell et al. (2008), there has thus being interest in investigating whether corpus-harvested semantic representations can contribute to the study of concepts in the brain. The relation is mutually beneficial: From the point of view of brain activity decoding, a strong correlation between corpus-based and brain-derived conceptual repre</context>
</contexts>
<marker>Clark, 2012</marker>
<rawString>Stephen Clark. 2012. Vector space models of lexical meaning. In Shalom Lappin and Chris Fox, editors, Handbook of Contemporary Semantics, 2nd edition. Blackwell, Malden, MA. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriella Csurka</author>
<author>Christopher Dance</author>
<author>Lixin Fan</author>
<author>Jutta Willamowski</author>
<author>C´edric Bray</author>
</authors>
<title>Visual categorization with bags of keypoints.</title>
<date>2004</date>
<booktitle>In In Workshop on Statistical Learning in Computer Vision, ECCV,</booktitle>
<pages>1--22</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="17773" citStr="Csurka et al., 2004" startWordPosition="2734" endWordPosition="2737">(Deng et al., 2009),3 a very large image database organized on top of the WordNet hierarchy (Fellbaum, 1998). ImageNet has more than 14 million images, covering 21K WordNet nominal synsets. ImageNet stands out for the high quality of its images, both in terms of resolution and concept annotations. Moreover, for around 3K concepts, annotations of object bounding boxes is provided. This last feature allows us to exploit object localization within our experiments. To build visual distributional models, we utilize the bag-of-visual-words (BoVW) representation of images (Sivic and Zisserman, 2003; Csurka et al., 2004). Inspired by NLP, BoVW discretizes the image content in terms of a histogram of visual word counts. Differently from NLP, in vision there is not a natural notion of visual words, hence a visual vocabulary has to be built from scratch. The process works as follows. First, a large set of low-level features is extracted from a corpus of images. The low-level feature vectors are subsequently clustered into different regions (visual words). Given then a new image, each of the low-level feature vectors extracted from the patches that compose it is mapped to the nearest visual word (e.g., in terms o</context>
</contexts>
<marker>Csurka, Dance, Fan, Willamowski, Bray, 2004</marker>
<rawString>Gabriella Csurka, Christopher Dance, Lixin Fan, Jutta Willamowski, and C´edric Bray. 2004. Visual categorization with bags of keypoints. In In Workshop on Statistical Learning in Computer Vision, ECCV, pages 1–22, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Wei Dong</author>
<author>Richard Socher</author>
<author>Lia-Ji Li</author>
<author>Li Fei-Fei</author>
</authors>
<title>Imagenet: A large-scale hierarchical image database.</title>
<date>2009</date>
<booktitle>In Proceedings of CVPR,</booktitle>
<pages>248--255</pages>
<location>Miami Beach, FL.</location>
<contexts>
<context position="17172" citStr="Deng et al., 2009" startWordPosition="2642" endWordPosition="2645">, the context in which an object is located can significantly contribute to semantic representation, in certain cases carrying even more information than the depicted object itself). More in detail, with localization the visual features (visual words) can be extracted from the object bounding box (in our experiments, the Object model) or from only outside the object box (the Context model). A combined model is obtained by concatenating the two feature vectors (the Object&amp;Context model). Visual model construction pipeline To extract visual co-occurrence statistics, we use images from ImageNet (Deng et al., 2009),3 a very large image database organized on top of the WordNet hierarchy (Fellbaum, 1998). ImageNet has more than 14 million images, covering 21K WordNet nominal synsets. ImageNet stands out for the high quality of its images, both in terms of resolution and concept annotations. Moreover, for around 3K concepts, annotations of object bounding boxes is provided. This last feature allows us to exploit object localization within our experiments. To build visual distributional models, we utilize the bag-of-visual-words (BoVW) representation of images (Sivic and Zisserman, 2003; Csurka et al., 2004</context>
</contexts>
<marker>Deng, Dong, Socher, Li, Fei-Fei, 2009</marker>
<rawString>Jia Deng, Wei Dong, Richard Socher, Lia-Ji Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In Proceedings of CVPR, pages 248–255, Miami Beach, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Russell A Epstein</author>
</authors>
<title>Parahippocampal and retrosplenial contributions to human spatial navigation. Trends in cognitive sciences,</title>
<date>2008</date>
<pages>12--10</pages>
<contexts>
<context position="10222" citStr="Epstein, 2008" startWordPosition="1572" endWordPosition="1573">ceptual stream, serving object identification and recognition; and an action stream, specialist in processing egocentric spatial relationships and ultimately supporting interaction with the world. The perceptual stream leads to the temporal lobe. Here the fusiform gyrus (shared with the occipital lobe) plays a general role in object categorisation (e.g., animals and tools (Chao et al., 1999), faces (Kanwisher and Yovel, 2006), body parts (Peelen and Downing, 2005) and even word form perception (McCandliss et al., 2003)). As the parahippocampus is strongly associated with scene representation (Epstein, 2008), we expect both the Object and Context models to capture variability in the temporal lobe. Of wider relevance to semantic processing, the medial temporal gyrus, inferior temporal gyrus and ventral temporal lobe have generally been implicated to have roles in supramodal integration and concept retrieval (Binder et al., 2009). Given this, we expected that incorporating text would also be valuable and that the Window2&amp;Object&amp;Context combination would be a good model. The visual action stream leads from the occipital lobe to the parietal lobe to support spatial cognition tasks and action control </context>
</contexts>
<marker>Epstein, 2008</marker>
<rawString>Russell A Epstein. 2008. Parahippocampal and retrosplenial contributions to human spatial navigation. Trends in cognitive sciences, 12(10):388–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Vector space models of word meaning and phrase meaning: A survey.</title>
<date>2012</date>
<journal>Language and Linguistics Compass,</journal>
<volume>6</volume>
<issue>10</issue>
<contexts>
<context position="2024" citStr="Erk, 2012" startWordPosition="298" endWordPosition="299">e semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility. 1 Introduction Many recent neuroscientific studies have brought support to the view that concepts are represented in terms of patterns of neural activation over broad areas, naturally encoded as vectors in a neural semantic space (Haxby et al., 2001; Huth et al., 2012). Similar representations are also widely used in computational linguistics, and in particular in distributional semantics (Clark, 2012; Erk, 2012; Turney and Pantel, 2010), that captures meaning in terms of vectors recording the patterns of co-occurrence of words in large corpora, under the hypothesis that words that occur in similar contexts are similar in meaning. Since the seminal work of Mitchell et al. (2008), there has thus being interest in investigating whether corpus-harvested semantic representations can contribute to the study of concepts in the brain. The relation is mutually beneficial: From the point of view of brain activity decoding, a strong correlation between corpus-based and brain-derived conceptual representations </context>
</contexts>
<marker>Erk, 2012</marker>
<rawString>Katrin Erk. 2012. Vector space models of word meaning and phrase meaning: A survey. Language and Linguistics Compass, 6(10):635–653.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Visual information in semantic representation.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>91--99</pages>
<location>Los Angeles, CA.</location>
<contexts>
<context position="4355" citStr="Feng and Lapata, 2010" startWordPosition="667" endWordPosition="670">y et al. (2012), who showed that purely corpus-based distributional models are at least as good at brain signal prediction tasks as earlier models that made use of manually-generated or controlled knowledge sources (Chang et al., 2011; Palatucci et al., 2009; Pereira et al., 2011), and we evaluate a very recent type of distributional model, namely one that is not extracted from textual data but from image collections through automated visual feature extraction techniques. It has been argued that this new generation of image-based distributional models (Bruni et al., 2011; Bruni et al., 2012b; Feng and Lapata, 2010; Leong and Mihalcea, 2011) provides a more realistic view of meaning, since humans obviously acquire a large proportion of their semantic knowledge from perceptual data. The first question that we ask, thus, is whether the more “grounded” image-based models can help us in interpreting conceptual representations in the brain. More specifically, we will compare the performance of different image-based representations, and we will test whether text- and image-based representations are complementary, so that when used together they can better account for patterns in neural data. Finally, we will </context>
<context position="13481" citStr="Feng and Lapata, 2010" startWordPosition="2068" endWordPosition="2071">ning by keeping track of word co-occurrence statistics from large textual input, relying on the distributional hypothesis: The meaning of a word can be induced by the context in which it occurs (Turney and Pantel, 2010). Despite their great success, these models still rely on verbal input only, while humans base their meaning representation also on perceptual information (Louwerse, 2011). Thanks to recent developments in computer vision, it is nowadays possible to take the visual perceptual channel into account, and build new computational models of semantics enhanced with visual information (Feng and Lapata, 2010; Bruni et al., 2011; Leong and Mihalcea, 2011; Bergsma and Goebel, 2011; Bruni et al., 2012a). Given a set of target concepts and a collection of images depicting those concepts, it is indeed possible to first encode the image content into low-level features, and subsequently convert it into a higher-level representation based on the bag-of-visual-words method (Grauman and Leibe, 2011). Recently, Bruni et al. (2012b) have shown that better semantic representations can be extracted if we first localize the concept in the image, and then extract distinct higher-level features (visual words) fro</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010. Visual information in semantic representation. In Proceedings of HLT-NAACL, pages 91–99, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melvyn A Goodale</author>
<author>David Milner</author>
</authors>
<title>Separate visual pathways for perception and action. Trends in Neurosciences,</title>
<date>1992</date>
<pages>15--20</pages>
<contexts>
<context position="9436" citStr="Goodale and Milner (1992)" startWordPosition="1453" endWordPosition="1456">ional models, organized by category. bination of the two, and Window2 is a text-based model. The occipital lobe houses the primary visual processing system and consequently it is reasonable to expect some bias toward image-based semantic models. Furthermore, given that experimental stimuli incorporated line drawings of the object,and the visual cortex has a well-established role in processing low-level visual statistics including edge detection (Bruce et al., 2003), we naturally expected a good performance from Object (formulated from edge orientation histograms of similar objects). Following Goodale and Milner (1992)’s influential perception-action model (see McIntosh and Schenk (2009) for recent discussion), visual information is channeled from the occipital lobe in two streams: a perceptual stream, serving object identification and recognition; and an action stream, specialist in processing egocentric spatial relationships and ultimately supporting interaction with the world. The perceptual stream leads to the temporal lobe. Here the fusiform gyrus (shared with the occipital lobe) plays a general role in object categorisation (e.g., animals and tools (Chao et al., 1999), faces (Kanwisher and Yovel, 2006</context>
</contexts>
<marker>Goodale, Milner, 1992</marker>
<rawString>Melvyn A. Goodale and David Milner. 1992. Separate visual pathways for perception and action. Trends in Neurosciences, 15:20–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristen Grauman</author>
<author>Bastian Leibe</author>
</authors>
<title>Visual Object Recognition.</title>
<date>2011</date>
<publisher>Morgan &amp; Claypool,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="13870" citStr="Grauman and Leibe, 2011" startWordPosition="2129" endWordPosition="2132">011). Thanks to recent developments in computer vision, it is nowadays possible to take the visual perceptual channel into account, and build new computational models of semantics enhanced with visual information (Feng and Lapata, 2010; Bruni et al., 2011; Leong and Mihalcea, 2011; Bergsma and Goebel, 2011; Bruni et al., 2012a). Given a set of target concepts and a collection of images depicting those concepts, it is indeed possible to first encode the image content into low-level features, and subsequently convert it into a higher-level representation based on the bag-of-visual-words method (Grauman and Leibe, 2011). Recently, Bruni et al. (2012b) have shown that better semantic representations can be extracted if we first localize the concept in the image, and then extract distinct higher-level features (visual words) from the box containing the concept and from the surrounding context. We also follow this strategy here. In our experiments we utilize both traditional textbased models and experimental image-based models, as well as their combination. 3.1 Textual models Verb We experiment with the original text-based semantic model used to predict fMRI patterns by Mitchell et al. (2008). Each object stimu</context>
</contexts>
<marker>Grauman, Leibe, 2011</marker>
<rawString>Kristen Grauman and Bastian Leibe. 2011. Visual Object Recognition. Morgan &amp; Claypool, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Hagoort</author>
</authors>
<title>On Broca, brain, and binding: a new framework. Trends in cognitive sciences,</title>
<date>2005</date>
<pages>9--9</pages>
<contexts>
<context position="11894" citStr="Hagoort, 2005" startWordPosition="1827" endWordPosition="1828">formance, so Window2&amp;Context was earmarked as a strong candidate. The frontal lobe, is traditionally associated with high-level processing and manipulation of abstract knowledge and rules and controlled behaviour (Miller et al., 2002). Regarding semantics, the dorsomedial prefrontal cortex has been implicated in self-guided retrieval of semantic information (e.g., uncued speech production), the ventromedial prefrontal cortex in motivation and emotional processing, the inferior frontal gyrus in phonological and syntactic processing, (Binder et al., 2009) and integration of lexical information (Hagoort, 2005). Given the association with linguistic processing we anticipated a bias in favour of Window2. The four lobes were identified and partitioned using Tzourio-Mazoyer et al. (2002)’s automatic anatomical labelling scheme. 1962 Voxel selection The set of 500 most stable voxels, both within the whole brain and from within each region of interest were identified for analysis. The most stable voxels were those showing consistent variation across the different stimuli between scanning sessions. Specifically, and following a similar strategy to Mitchell et al. (2008), for each voxel, the set of 51 word</context>
</contexts>
<marker>Hagoort, 2005</marker>
<rawString>Peter Hagoort. 2005. On Broca, brain, and binding: a new framework. Trends in cognitive sciences, 9(9):416–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Haxby</author>
<author>Ida Gobbini</author>
<author>Maura Furey</author>
<author>Alumit Ishai</author>
<author>Jennifer Schouten</author>
<author>Pietro Pietrini</author>
</authors>
<title>Distributed and overlapping representations of faces and objects in ventral temporal cortex.</title>
<date>2001</date>
<journal>Science,</journal>
<volume>293</volume>
<pages>2430</pages>
<contexts>
<context position="1858" citStr="Haxby et al., 2001" startWordPosition="271" endWordPosition="274">ct differential association of models with brain areas), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility. 1 Introduction Many recent neuroscientific studies have brought support to the view that concepts are represented in terms of patterns of neural activation over broad areas, naturally encoded as vectors in a neural semantic space (Haxby et al., 2001; Huth et al., 2012). Similar representations are also widely used in computational linguistics, and in particular in distributional semantics (Clark, 2012; Erk, 2012; Turney and Pantel, 2010), that captures meaning in terms of vectors recording the patterns of co-occurrence of words in large corpora, under the hypothesis that words that occur in similar contexts are similar in meaning. Since the seminal work of Mitchell et al. (2008), there has thus being interest in investigating whether corpus-harvested semantic representations can contribute to the study of concepts in the brain. The relat</context>
</contexts>
<marker>Haxby, Gobbini, Furey, Ishai, Schouten, Pietrini, 2001</marker>
<rawString>James Haxby, Ida Gobbini, Maura Furey, Alumit Ishai, Jennifer Schouten, and Pietro Pietrini. 2001. Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science, 293:2425– 2430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Huth</author>
<author>Shinji Nishimoto</author>
<author>An Vu</author>
<author>Jack Gallant</author>
</authors>
<title>A continuous semantic space describes the representation of thousands of object and action categories across the human brain.</title>
<date>2012</date>
<journal>Neuron,</journal>
<volume>76</volume>
<issue>6</issue>
<pages>1224</pages>
<contexts>
<context position="1878" citStr="Huth et al., 2012" startWordPosition="275" endWordPosition="278">ciation of models with brain areas), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility. 1 Introduction Many recent neuroscientific studies have brought support to the view that concepts are represented in terms of patterns of neural activation over broad areas, naturally encoded as vectors in a neural semantic space (Haxby et al., 2001; Huth et al., 2012). Similar representations are also widely used in computational linguistics, and in particular in distributional semantics (Clark, 2012; Erk, 2012; Turney and Pantel, 2010), that captures meaning in terms of vectors recording the patterns of co-occurrence of words in large corpora, under the hypothesis that words that occur in similar contexts are similar in meaning. Since the seminal work of Mitchell et al. (2008), there has thus being interest in investigating whether corpus-harvested semantic representations can contribute to the study of concepts in the brain. The relation is mutually bene</context>
</contexts>
<marker>Huth, Nishimoto, Vu, Gallant, 2012</marker>
<rawString>Alexander Huth, Shinji Nishimoto, An Vu, and Jack Gallant. 2012. A continuous semantic space describes the representation of thousands of object and action categories across the human brain. Neuron, 76(6):1210– 1224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Kanwisher</author>
<author>Galit Yovel</author>
</authors>
<title>The fusiform face area: a cortical region specialized for the perception of faces.</title>
<date>2006</date>
<journal>Philosophical Transactions of the Royal Society B: Biological Sciences,</journal>
<volume>361</volume>
<issue>1476</issue>
<contexts>
<context position="10037" citStr="Kanwisher and Yovel, 2006" startWordPosition="1541" endWordPosition="1544">Goodale and Milner (1992)’s influential perception-action model (see McIntosh and Schenk (2009) for recent discussion), visual information is channeled from the occipital lobe in two streams: a perceptual stream, serving object identification and recognition; and an action stream, specialist in processing egocentric spatial relationships and ultimately supporting interaction with the world. The perceptual stream leads to the temporal lobe. Here the fusiform gyrus (shared with the occipital lobe) plays a general role in object categorisation (e.g., animals and tools (Chao et al., 1999), faces (Kanwisher and Yovel, 2006), body parts (Peelen and Downing, 2005) and even word form perception (McCandliss et al., 2003)). As the parahippocampus is strongly associated with scene representation (Epstein, 2008), we expect both the Object and Context models to capture variability in the temporal lobe. Of wider relevance to semantic processing, the medial temporal gyrus, inferior temporal gyrus and ventral temporal lobe have generally been implicated to have roles in supramodal integration and concept retrieval (Binder et al., 2009). Given this, we expected that incorporating text would also be valuable and that the Win</context>
</contexts>
<marker>Kanwisher, Yovel, 2006</marker>
<rawString>Nancy Kanwisher and Galit Yovel. 2006. The fusiform face area: a cortical region specialized for the perception of faces. Philosophical Transactions of the Royal Society B: Biological Sciences, 361(1476):2109–2128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikolaus Kriegeskorte</author>
<author>Marieke Mur</author>
<author>Peter Bandettini</author>
</authors>
<title>Representational similarity analysis– connecting the branches of systems neuroscience. Frontiers in systems neuroscience,</title>
<date>2008</date>
<pages>2</pages>
<contexts>
<context position="22441" citStr="Kriegeskorte et al., 2008" startWordPosition="3475" endWordPosition="3478">cated by differences in dimensionality between both semantic models and lobes (which we compare to other lobes) in association with the comparatively small number of words in the fMRI data set. Large dimensionality models risk overfitting the data, and it is a nuisance to try to reliably correct for the effects of overfitting in performance comparisons. Not least, to thoroughly evaluate all possible cross-validation permutations is demanding in processing time, and we have many models to compare. An alternative approach, and that which we have adopted, is representational similarity analysis (Kriegeskorte et al., 2008). Representational similarity analysis circumvents the previous problems by abstracting each fMRI/distributional data source to a common structure capturing the interrelationships between each pair of data items (e.g., words). Specifically, for each model/participant’s fMRI data/anatomical region, the similarity structure was evaluated by taking the pairwise correlation (Pearson’s correlation coefficient) between all unique category or word combinations. This produced a list of 55 category pair correlations and 121 word pair correlations for each data source. For all brain data, correlation li</context>
</contexts>
<marker>Kriegeskorte, Mur, Bandettini, 2008</marker>
<rawString>Nikolaus Kriegeskorte, Marieke Mur, and Peter Bandettini. 2008. Representational similarity analysis– connecting the branches of systems neuroscience. Frontiers in systems neuroscience, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Lazebnik</author>
<author>Cordelia Schmid</author>
<author>Jean Ponce</author>
</authors>
<title>Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories.</title>
<date>2006</date>
<booktitle>In Proceedings of CVPR,</booktitle>
<pages>2169--2178</pages>
<location>Washington, DC.</location>
<contexts>
<context position="19094" citStr="Lazebnik et al., 2006" startWordPosition="2954" endWordPosition="2957">gram counting the instances of each visual word in the image. As low-level features we use SIFT, the Scale Invariant Feature Transform (Lowe, 2004). SIFT features are good at capturing parts of objects and are designed to be invariant to image transformations 3http://www.image-net.org/ such as change in scale, rotation and illumination. To construct the visual vocabulary, we cluster the SIFT features into 25K different clusters.4 We add also spatial information by dividing the image into several subregions, representing each of them in terms of BoVW and then stacking the resulting histograms (Lazebnik et al., 2006). We use in total 8 different regions, obtaining a final vector of 200K dimensions (25K visual words x 8 regions). Since each concept in our dataset is represented by multiple images, we pool the visual word occurrences across images by summing them up into a single vector. To perform the entire visual pipeline we use VSEM, an open library for visual semantics (Bruni et al., 2013).5 3.3 Model transformations and combination Once both the textual and the visual models are built, we perform two different transformations on the raw co-occurrence counts. First, we transform them into nonnegative P</context>
</contexts>
<marker>Lazebnik, Schmid, Ponce, 2006</marker>
<rawString>Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. 2006. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In Proceedings of CVPR, pages 2169–2178, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chee Wee Leong</author>
<author>Rada Mihalcea</author>
</authors>
<title>Going beyond text: A hybrid image-text approach for measuring word relatedness.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>1403--1407</pages>
<contexts>
<context position="4382" citStr="Leong and Mihalcea, 2011" startWordPosition="671" endWordPosition="675">owed that purely corpus-based distributional models are at least as good at brain signal prediction tasks as earlier models that made use of manually-generated or controlled knowledge sources (Chang et al., 2011; Palatucci et al., 2009; Pereira et al., 2011), and we evaluate a very recent type of distributional model, namely one that is not extracted from textual data but from image collections through automated visual feature extraction techniques. It has been argued that this new generation of image-based distributional models (Bruni et al., 2011; Bruni et al., 2012b; Feng and Lapata, 2010; Leong and Mihalcea, 2011) provides a more realistic view of meaning, since humans obviously acquire a large proportion of their semantic knowledge from perceptual data. The first question that we ask, thus, is whether the more “grounded” image-based models can help us in interpreting conceptual representations in the brain. More specifically, we will compare the performance of different image-based representations, and we will test whether text- and image-based representations are complementary, so that when used together they can better account for patterns in neural data. Finally, we will check for differences betwe</context>
<context position="13527" citStr="Leong and Mihalcea, 2011" startWordPosition="2076" endWordPosition="2079"> statistics from large textual input, relying on the distributional hypothesis: The meaning of a word can be induced by the context in which it occurs (Turney and Pantel, 2010). Despite their great success, these models still rely on verbal input only, while humans base their meaning representation also on perceptual information (Louwerse, 2011). Thanks to recent developments in computer vision, it is nowadays possible to take the visual perceptual channel into account, and build new computational models of semantics enhanced with visual information (Feng and Lapata, 2010; Bruni et al., 2011; Leong and Mihalcea, 2011; Bergsma and Goebel, 2011; Bruni et al., 2012a). Given a set of target concepts and a collection of images depicting those concepts, it is indeed possible to first encode the image content into low-level features, and subsequently convert it into a higher-level representation based on the bag-of-visual-words method (Grauman and Leibe, 2011). Recently, Bruni et al. (2012b) have shown that better semantic representations can be extracted if we first localize the concept in the image, and then extract distinct higher-level features (visual words) from the box containing the concept and from the </context>
</contexts>
<marker>Leong, Mihalcea, 2011</marker>
<rawString>Chee Wee Leong and Rada Mihalcea. 2011. Going beyond text: A hybrid image-text approach for measuring word relatedness. In Proceedings of IJCNLP, pages 1403–1407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Louwerse</author>
</authors>
<title>Symbol interdependency in symbolic and embodied cognition. Topics in Cognitive Science,</title>
<date>2011</date>
<pages>3--273</pages>
<contexts>
<context position="13250" citStr="Louwerse, 2011" startWordPosition="2032" endWordPosition="2033">mean of the 15 resulting correlation coefficients was taken as the measure of stability. The 500 voxels with highest mean correlations were selected. 3 Distributional models Distributional semantic models approximate word meaning by keeping track of word co-occurrence statistics from large textual input, relying on the distributional hypothesis: The meaning of a word can be induced by the context in which it occurs (Turney and Pantel, 2010). Despite their great success, these models still rely on verbal input only, while humans base their meaning representation also on perceptual information (Louwerse, 2011). Thanks to recent developments in computer vision, it is nowadays possible to take the visual perceptual channel into account, and build new computational models of semantics enhanced with visual information (Feng and Lapata, 2010; Bruni et al., 2011; Leong and Mihalcea, 2011; Bergsma and Goebel, 2011; Bruni et al., 2012a). Given a set of target concepts and a collection of images depicting those concepts, it is indeed possible to first encode the image content into low-level features, and subsequently convert it into a higher-level representation based on the bag-of-visual-words method (Grau</context>
</contexts>
<marker>Louwerse, 2011</marker>
<rawString>Max Louwerse. 2011. Symbol interdependency in symbolic and embodied cognition. Topics in Cognitive Science, 3:273–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Lowe</author>
</authors>
<title>Distinctive Image Features from Scale-Invariant Keypoints.</title>
<date>2004</date>
<journal>International Journal of Computer Vision,</journal>
<pages>60--91</pages>
<contexts>
<context position="18619" citStr="Lowe, 2004" startWordPosition="2883" endWordPosition="2884"> The process works as follows. First, a large set of low-level features is extracted from a corpus of images. The low-level feature vectors are subsequently clustered into different regions (visual words). Given then a new image, each of the low-level feature vectors extracted from the patches that compose it is mapped to the nearest visual word (e.g., in terms of Euclidean distance from the cluster centroid) such that the image can be represented with a histogram counting the instances of each visual word in the image. As low-level features we use SIFT, the Scale Invariant Feature Transform (Lowe, 2004). SIFT features are good at capturing parts of objects and are designed to be invariant to image transformations 3http://www.image-net.org/ such as change in scale, rotation and illumination. To construct the visual vocabulary, we cluster the SIFT features into 25K different clusters.4 We add also spatial information by dividing the image into several subregions, representing each of them in terms of BoVW and then stacking the resulting histograms (Lazebnik et al., 2006). We use in total 8 different regions, obtaining a final vector of 200K dimensions (25K visual words x 8 regions). Since each</context>
</contexts>
<marker>Lowe, 2004</marker>
<rawString>David G Lowe. 2004. Distinctive Image Features from Scale-Invariant Keypoints. International Journal of Computer Vision, 60:91–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods,</title>
<date>1996</date>
<pages>28--203</pages>
<contexts>
<context position="15108" citStr="Lund and Burgess, 1996" startWordPosition="2325" endWordPosition="2328">resented as a 25-dimensional vector, with each value corresponding to the normalized sentencewide co-occurrence of that word with one of 25 manually-picked sensorimotor verbs (such as see, hear, eat, ...) in a trillion word text corpus. Window2 To create this model, we collect text co-occurrence statistics from the freely available ukWaC and Wackypedia corpora combined (about 3 billion words in total).2 As collocates of our distributional model we select a set of 30K words, namely the top 20K most frequent nouns, 5K most frequent adjectives and 5K most frequent verbs. In the tradition of HAL (Lund and Burgess, 1996), the model is based on co-occurrence statistics with collocates within a fixed-size window of 2 to the left and right of each target word. Despite their simplicity, narrow-window-based models have shown to achieve state-of-the-art results in various standard semantic tasks (Bullinaria and Levy, 2007) and to outperform both document-based and syntaxbased models trained on the same corpus (Bruni et al., 2012a). Moreover, in Murphy et al. (2012) a window-based model very similar to ours was not significantly worse than their best model for brain decoding. We tried also a few variations, e.g., us</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, 28:203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce D McCandliss</author>
<author>Laurent Cohen</author>
<author>Stanislas Dehaene</author>
</authors>
<title>The visual word form area: expertise for reading in the fusiform gyrus. Trends in cognitive sciences,</title>
<date>2003</date>
<pages>7--7</pages>
<contexts>
<context position="10132" citStr="McCandliss et al., 2003" startWordPosition="1557" endWordPosition="1560">or recent discussion), visual information is channeled from the occipital lobe in two streams: a perceptual stream, serving object identification and recognition; and an action stream, specialist in processing egocentric spatial relationships and ultimately supporting interaction with the world. The perceptual stream leads to the temporal lobe. Here the fusiform gyrus (shared with the occipital lobe) plays a general role in object categorisation (e.g., animals and tools (Chao et al., 1999), faces (Kanwisher and Yovel, 2006), body parts (Peelen and Downing, 2005) and even word form perception (McCandliss et al., 2003)). As the parahippocampus is strongly associated with scene representation (Epstein, 2008), we expect both the Object and Context models to capture variability in the temporal lobe. Of wider relevance to semantic processing, the medial temporal gyrus, inferior temporal gyrus and ventral temporal lobe have generally been implicated to have roles in supramodal integration and concept retrieval (Binder et al., 2009). Given this, we expected that incorporating text would also be valuable and that the Window2&amp;Object&amp;Context combination would be a good model. The visual action stream leads from the </context>
</contexts>
<marker>McCandliss, Cohen, Dehaene, 2003</marker>
<rawString>Bruce D McCandliss, Laurent Cohen, and Stanislas Dehaene. 2003. The visual word form area: expertise for reading in the fusiform gyrus. Trends in cognitive sciences, 7(7):293–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert D McIntosh</author>
<author>Thomas Schenk</author>
</authors>
<title>Two visual streams for perception and action: Current trends.</title>
<date>2009</date>
<journal>Neuropsychologia,</journal>
<volume>47</volume>
<issue>6</issue>
<contexts>
<context position="9506" citStr="McIntosh and Schenk (2009)" startWordPosition="1462" endWordPosition="1465"> is a text-based model. The occipital lobe houses the primary visual processing system and consequently it is reasonable to expect some bias toward image-based semantic models. Furthermore, given that experimental stimuli incorporated line drawings of the object,and the visual cortex has a well-established role in processing low-level visual statistics including edge detection (Bruce et al., 2003), we naturally expected a good performance from Object (formulated from edge orientation histograms of similar objects). Following Goodale and Milner (1992)’s influential perception-action model (see McIntosh and Schenk (2009) for recent discussion), visual information is channeled from the occipital lobe in two streams: a perceptual stream, serving object identification and recognition; and an action stream, specialist in processing egocentric spatial relationships and ultimately supporting interaction with the world. The perceptual stream leads to the temporal lobe. Here the fusiform gyrus (shared with the occipital lobe) plays a general role in object categorisation (e.g., animals and tools (Chao et al., 1999), faces (Kanwisher and Yovel, 2006), body parts (Peelen and Downing, 2005) and even word form perception</context>
</contexts>
<marker>McIntosh, Schenk, 2009</marker>
<rawString>Robert D McIntosh and Thomas Schenk. 2009. Two visual streams for perception and action: Current trends. Neuropsychologia, 47(6):1391–1396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Earl K Miller</author>
<author>David J Freedman</author>
<author>Jonathan D Wallis</author>
</authors>
<title>The prefrontal cortex: categories, concepts and cognition.</title>
<date>2002</date>
<journal>Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences,</journal>
<volume>357</volume>
<issue>1424</issue>
<contexts>
<context position="11514" citStr="Miller et al., 2002" startWordPosition="1773" endWordPosition="1776">, placing actor in environment, it is tempting to speculate that the Context model is more appropriate than the Object model here. As the parietal lobe also contains the angular gyrus, thought to be involved in complex, supra-modal information integration and knowledge retrieval (Binder et al., 2009), we might again forecast that integrating text and image information would boost performance, so Window2&amp;Context was earmarked as a strong candidate. The frontal lobe, is traditionally associated with high-level processing and manipulation of abstract knowledge and rules and controlled behaviour (Miller et al., 2002). Regarding semantics, the dorsomedial prefrontal cortex has been implicated in self-guided retrieval of semantic information (e.g., uncued speech production), the ventromedial prefrontal cortex in motivation and emotional processing, the inferior frontal gyrus in phonological and syntactic processing, (Binder et al., 2009) and integration of lexical information (Hagoort, 2005). Given the association with linguistic processing we anticipated a bias in favour of Window2. The four lobes were identified and partitioned using Tzourio-Mazoyer et al. (2002)’s automatic anatomical labelling scheme. 1</context>
</contexts>
<marker>Miller, Freedman, Wallis, 2002</marker>
<rawString>Earl K Miller, David J Freedman, and Jonathan D Wallis. 2002. The prefrontal cortex: categories, concepts and cognition. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 357(1424):1123–1136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Mitchell</author>
<author>Svetlana Shinkareva</author>
<author>Andrew Carlson</author>
<author>Kai-Min Chang</author>
<author>Vincente Malave</author>
<author>Robert Mason</author>
<author>Marcel Just</author>
</authors>
<title>Predicting human brain activity associated with the meanings of nouns.</title>
<date>2008</date>
<journal>Science,</journal>
<pages>320--1191</pages>
<contexts>
<context position="2296" citStr="Mitchell et al. (2008)" startWordPosition="340" endWordPosition="343">upport to the view that concepts are represented in terms of patterns of neural activation over broad areas, naturally encoded as vectors in a neural semantic space (Haxby et al., 2001; Huth et al., 2012). Similar representations are also widely used in computational linguistics, and in particular in distributional semantics (Clark, 2012; Erk, 2012; Turney and Pantel, 2010), that captures meaning in terms of vectors recording the patterns of co-occurrence of words in large corpora, under the hypothesis that words that occur in similar contexts are similar in meaning. Since the seminal work of Mitchell et al. (2008), there has thus being interest in investigating whether corpus-harvested semantic representations can contribute to the study of concepts in the brain. The relation is mutually beneficial: From the point of view of brain activity decoding, a strong correlation between corpus-based and brain-derived conceptual representations would mean that we could use the former (much easier to construct on a very large scale) to make inferences about the second: e.g., using corpus-based representations to reconstruct the likely neural signal associated to words we have no direct brain data for. From the po</context>
<context position="5258" citStr="Mitchell et al. (2008)" startWordPosition="810" endWordPosition="813">eting conceptual representations in the brain. More specifically, we will compare the performance of different image-based representations, and we will test whether text- and image-based representations are complementary, so that when used together they can better account for patterns in neural data. Finally, we will check for differences between anatomical regions in the degree to which text and/or image models are effective, as one might expect given the well-known functional specializations of different anatomical regions. 2 Brain data We use the data that were recorded and preprocessed by Mitchell et al. (2008), available for download in their supporting online material.1 Full details of the experimental protocol, data acquisition and preprocessing can be found in Mitchell et al. (2008) and the supporting material. Key points are that there were nine right-handed adult participants (5 female, age between 18 and 32). The experimental task was to actively think about the properties of sixty objects that were presented visually, each as a line drawing in combination with a text label. The entire set of objects was presented in a random order in six sessions, each object remained on screen for 3 seconds</context>
<context position="12458" citStr="Mitchell et al. (2008)" startWordPosition="1909" endWordPosition="1912">09) and integration of lexical information (Hagoort, 2005). Given the association with linguistic processing we anticipated a bias in favour of Window2. The four lobes were identified and partitioned using Tzourio-Mazoyer et al. (2002)’s automatic anatomical labelling scheme. 1962 Voxel selection The set of 500 most stable voxels, both within the whole brain and from within each region of interest were identified for analysis. The most stable voxels were those showing consistent variation across the different stimuli between scanning sessions. Specifically, and following a similar strategy to Mitchell et al. (2008), for each voxel, the set of 51 words from each unique pair of scanning sessions were correlated using Pearson’s correlation (6 sessions and therefore 15 unique pairs), and the mean of the 15 resulting correlation coefficients was taken as the measure of stability. The 500 voxels with highest mean correlations were selected. 3 Distributional models Distributional semantic models approximate word meaning by keeping track of word co-occurrence statistics from large textual input, relying on the distributional hypothesis: The meaning of a word can be induced by the context in which it occurs (Tur</context>
<context position="14451" citStr="Mitchell et al. (2008)" startWordPosition="2219" endWordPosition="2222">al-words method (Grauman and Leibe, 2011). Recently, Bruni et al. (2012b) have shown that better semantic representations can be extracted if we first localize the concept in the image, and then extract distinct higher-level features (visual words) from the box containing the concept and from the surrounding context. We also follow this strategy here. In our experiments we utilize both traditional textbased models and experimental image-based models, as well as their combination. 3.1 Textual models Verb We experiment with the original text-based semantic model used to predict fMRI patterns by Mitchell et al. (2008). Each object stimulus word is represented as a 25-dimensional vector, with each value corresponding to the normalized sentencewide co-occurrence of that word with one of 25 manually-picked sensorimotor verbs (such as see, hear, eat, ...) in a trillion word text corpus. Window2 To create this model, we collect text co-occurrence statistics from the freely available ukWaC and Wackypedia corpora combined (about 3 billion words in total).2 As collocates of our distributional model we select a set of 30K words, namely the top 20K most frequent nouns, 5K most frequent adjectives and 5K most frequen</context>
<context position="21162" citStr="Mitchell et al. (2008)" startWordPosition="3275" endWordPosition="3278">nd Lapata (2013), where Canonical Correlation Analysis is used. We reserve the exploration of more advanced combination methods for further studies. Finally, to represent the 11 categories we experiment with (see Table 1), we average the vectors of the concepts they include. 4We use k-means, the most commonly employed clustering algorithm for this task. 5http://clic.cimec.unitn.it/vsem/ 1964 4 Experiments A question is posed over how to evaluate the relationship between the different distributional models and brain data. Comparing each model’s predictive performance using the same strategy as Mitchell et al. (2008) (also followed by Murphy et al. (2012)) is one possibility: they used multiple regression to relate distributional codes to individual voxel activations, thus allowing brain states to be estimated from previously unseen distributional codes. Regression models were trained on 58/60 words and in testing the regression models estimated the brain state associated with the 2 unseen distributional codes. The predicted brain states were compared with the actual fMRI data, and the process repeated for each permutation of left-out words, to build a metric of prediction accuracy. For our purposes, a fa</context>
<context position="37273" citStr="Mitchell et al., 2008" startWordPosition="5775" endWordPosition="5778">e-based models complement a state-of-the-art text-based model, with the best performance achieved when the two modalities are combined. This not only presents an optimistic outlook for the future use of image-based models as an interpretative tool to explore issues of cognitive grounding, but also demonstrates that they are capturing useful additional aspects of meaning to 1968 the text models, which are likely relevant for computational semantic tasks. The weak comparative performance of the original Mitchell et al.’s Verb model is perhaps surprising given its previous success in prediction (Mitchell et al., 2008), but a useful reminder that a good predictor does not necessarily have to capture the internal structure of the data it predicts. The lack of finding organisational differences between anatomical regions differentially described by the various models is perhaps disappointing, but not uncontroversial, given that the dataset was not originally designed to tease apart visual information from linguistic context. It is however interesting that in the more challenging word-level analysis some meaningful trend was visible. In future experiments it may prove valuable to configure a fMRI stimulus set </context>
</contexts>
<marker>Mitchell, Shinkareva, Carlson, Chang, Malave, Mason, Just, 2008</marker>
<rawString>Tom Mitchell, Svetlana Shinkareva, Andrew Carlson, Kai-Min Chang, Vincente Malave, Robert Mason, and Marcel Just. 2008. Predicting human brain activity associated with the meanings of nouns. Science, 320:1191–1195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Murphy</author>
<author>Partha Talukdar</author>
<author>Tom Mitchell</author>
</authors>
<title>Selecting corpus-semantic models for neurolinguistic decoding.</title>
<date>2012</date>
<booktitle>In Proceedings of *SEM,</booktitle>
<pages>114--123</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="3749" citStr="Murphy et al. (2012)" startWordPosition="569" endWordPosition="572">hmarks). If we found that a corpus-based model of meaning can make nontrivial predictions about the structure of the semantic space in the brain, that would make a pretty strong case for the intriguing idea that the model is approximating, in interesting ways, the way in which humans acquire and represent semantic knowledge. 1960 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1960–1970, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics We take as our starting point the extensive experiments reported in Murphy et al. (2012), who showed that purely corpus-based distributional models are at least as good at brain signal prediction tasks as earlier models that made use of manually-generated or controlled knowledge sources (Chang et al., 2011; Palatucci et al., 2009; Pereira et al., 2011), and we evaluate a very recent type of distributional model, namely one that is not extracted from textual data but from image collections through automated visual feature extraction techniques. It has been argued that this new generation of image-based distributional models (Bruni et al., 2011; Bruni et al., 2012b; Feng and Lapata</context>
<context position="15555" citStr="Murphy et al. (2012)" startWordPosition="2395" endWordPosition="2398">e select a set of 30K words, namely the top 20K most frequent nouns, 5K most frequent adjectives and 5K most frequent verbs. In the tradition of HAL (Lund and Burgess, 1996), the model is based on co-occurrence statistics with collocates within a fixed-size window of 2 to the left and right of each target word. Despite their simplicity, narrow-window-based models have shown to achieve state-of-the-art results in various standard semantic tasks (Bullinaria and Levy, 2007) and to outperform both document-based and syntaxbased models trained on the same corpus (Bruni et al., 2012a). Moreover, in Murphy et al. (2012) a window-based model very similar to ours was not significantly worse than their best model for brain decoding. We tried also a few variations, e.g., using a larger window or different transformations on the raw co-occurrences from those presented below, but with little, insignificant changes in performance. Given that our focus here is on visual information, we only report results for Window2 and its combination with visual models. 3.2 Visual models Our visual models are inspired by Bruni et al. (2012b), that have explored to what extent extracting features from images where objects are loca</context>
<context position="21201" citStr="Murphy et al. (2012)" startWordPosition="3282" endWordPosition="3285">tion Analysis is used. We reserve the exploration of more advanced combination methods for further studies. Finally, to represent the 11 categories we experiment with (see Table 1), we average the vectors of the concepts they include. 4We use k-means, the most commonly employed clustering algorithm for this task. 5http://clic.cimec.unitn.it/vsem/ 1964 4 Experiments A question is posed over how to evaluate the relationship between the different distributional models and brain data. Comparing each model’s predictive performance using the same strategy as Mitchell et al. (2008) (also followed by Murphy et al. (2012)) is one possibility: they used multiple regression to relate distributional codes to individual voxel activations, thus allowing brain states to be estimated from previously unseen distributional codes. Regression models were trained on 58/60 words and in testing the regression models estimated the brain state associated with the 2 unseen distributional codes. The predicted brain states were compared with the actual fMRI data, and the process repeated for each permutation of left-out words, to build a metric of prediction accuracy. For our purposes, a fair comparison of models using this stra</context>
</contexts>
<marker>Murphy, Talukdar, Mitchell, 2012</marker>
<rawString>Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012. Selecting corpus-semantic models for neurolinguistic decoding. In Proceedings of *SEM, pages 114–123, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Palatucci</author>
<author>Dean Pomerleau</author>
<author>Geoffrey Hinton</author>
<author>Tom Mitchell</author>
</authors>
<title>Zero-shot learning with semantic output codes.</title>
<date>2009</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>1410--1418</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="3992" citStr="Palatucci et al., 2009" startWordPosition="607" endWordPosition="610">n interesting ways, the way in which humans acquire and represent semantic knowledge. 1960 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1960–1970, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics We take as our starting point the extensive experiments reported in Murphy et al. (2012), who showed that purely corpus-based distributional models are at least as good at brain signal prediction tasks as earlier models that made use of manually-generated or controlled knowledge sources (Chang et al., 2011; Palatucci et al., 2009; Pereira et al., 2011), and we evaluate a very recent type of distributional model, namely one that is not extracted from textual data but from image collections through automated visual feature extraction techniques. It has been argued that this new generation of image-based distributional models (Bruni et al., 2011; Bruni et al., 2012b; Feng and Lapata, 2010; Leong and Mihalcea, 2011) provides a more realistic view of meaning, since humans obviously acquire a large proportion of their semantic knowledge from perceptual data. The first question that we ask, thus, is whether the more “grounde</context>
</contexts>
<marker>Palatucci, Pomerleau, Hinton, Mitchell, 2009</marker>
<rawString>Mark Palatucci, Dean Pomerleau, Geoffrey Hinton, and Tom Mitchell. 2009. Zero-shot learning with semantic output codes. In Proceedings of NIPS, pages 1410– 1418, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius V Peelen</author>
<author>Paul E Downing</author>
</authors>
<title>Selectivity for the human body in the fusiform gyrus.</title>
<date>2005</date>
<journal>Journal of Neurophysiology,</journal>
<volume>93</volume>
<issue>1</issue>
<contexts>
<context position="10076" citStr="Peelen and Downing, 2005" startWordPosition="1547" endWordPosition="1550">perception-action model (see McIntosh and Schenk (2009) for recent discussion), visual information is channeled from the occipital lobe in two streams: a perceptual stream, serving object identification and recognition; and an action stream, specialist in processing egocentric spatial relationships and ultimately supporting interaction with the world. The perceptual stream leads to the temporal lobe. Here the fusiform gyrus (shared with the occipital lobe) plays a general role in object categorisation (e.g., animals and tools (Chao et al., 1999), faces (Kanwisher and Yovel, 2006), body parts (Peelen and Downing, 2005) and even word form perception (McCandliss et al., 2003)). As the parahippocampus is strongly associated with scene representation (Epstein, 2008), we expect both the Object and Context models to capture variability in the temporal lobe. Of wider relevance to semantic processing, the medial temporal gyrus, inferior temporal gyrus and ventral temporal lobe have generally been implicated to have roles in supramodal integration and concept retrieval (Binder et al., 2009). Given this, we expected that incorporating text would also be valuable and that the Window2&amp;Object&amp;Context combination would b</context>
</contexts>
<marker>Peelen, Downing, 2005</marker>
<rawString>Marius V Peelen and Paul E Downing. 2005. Selectivity for the human body in the fusiform gyrus. Journal of Neurophysiology, 93(1):603–608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Pereira</author>
<author>Greg Detre</author>
<author>Matthew Botvinick</author>
</authors>
<title>Generating text from functional brain images.</title>
<date>2011</date>
<journal>Frontiers in Human Neuroscience,</journal>
<volume>5</volume>
<issue>72</issue>
<note>Published online: http://www.frontiersin.org/ human_neuroscience/10.3389/fnhum. 2011.00072/abstract.</note>
<contexts>
<context position="4015" citStr="Pereira et al., 2011" startWordPosition="611" endWordPosition="614">way in which humans acquire and represent semantic knowledge. 1960 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1960–1970, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics We take as our starting point the extensive experiments reported in Murphy et al. (2012), who showed that purely corpus-based distributional models are at least as good at brain signal prediction tasks as earlier models that made use of manually-generated or controlled knowledge sources (Chang et al., 2011; Palatucci et al., 2009; Pereira et al., 2011), and we evaluate a very recent type of distributional model, namely one that is not extracted from textual data but from image collections through automated visual feature extraction techniques. It has been argued that this new generation of image-based distributional models (Bruni et al., 2011; Bruni et al., 2012b; Feng and Lapata, 2010; Leong and Mihalcea, 2011) provides a more realistic view of meaning, since humans obviously acquire a large proportion of their semantic knowledge from perceptual data. The first question that we ask, thus, is whether the more “grounded” image-based models c</context>
</contexts>
<marker>Pereira, Detre, Botvinick, 2011</marker>
<rawString>Francisco Pereira, Greg Detre, and Matthew Botvinick. 2011. Generating text from functional brain images. Frontiers in Human Neuroscience, 5(72). Published online: http://www.frontiersin.org/ human_neuroscience/10.3389/fnhum. 2011.00072/abstract.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander T Sack</author>
</authors>
<title>Parietal cortex and spatial cognition. Behavioural brain research,</title>
<date>2009</date>
<pages>202--2</pages>
<contexts>
<context position="10834" citStr="Sack, 2009" startWordPosition="1669" endWordPosition="1670"> we expect both the Object and Context models to capture variability in the temporal lobe. Of wider relevance to semantic processing, the medial temporal gyrus, inferior temporal gyrus and ventral temporal lobe have generally been implicated to have roles in supramodal integration and concept retrieval (Binder et al., 2009). Given this, we expected that incorporating text would also be valuable and that the Window2&amp;Object&amp;Context combination would be a good model. The visual action stream leads from the occipital lobe to the parietal lobe to support spatial cognition tasks and action control (Sack, 2009). In that there seems to be an egocentric frame of reference, placing actor in environment, it is tempting to speculate that the Context model is more appropriate than the Object model here. As the parietal lobe also contains the angular gyrus, thought to be involved in complex, supra-modal information integration and knowledge retrieval (Binder et al., 2009), we might again forecast that integrating text and image information would boost performance, so Window2&amp;Context was earmarked as a strong candidate. The frontal lobe, is traditionally associated with high-level processing and manipulatio</context>
</contexts>
<marker>Sack, 2009</marker>
<rawString>Alexander T Sack. 2009. Parietal cortex and spatial cognition. Behavioural brain research, 202(2):153–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Ambiguity Resolution in Natural Language Learning.</title>
<date>1997</date>
<location>CSLI, Stanford, CA.</location>
<marker>Sch¨utze, 1997</marker>
<rawString>Hinrich Sch¨utze. 1997. Ambiguity Resolution in Natural Language Learning. CSLI, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Models of semantic representation with visual attributes.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="20556" citStr="Silberer and Lapata (2013)" startWordPosition="3185" endWordPosition="3188"> most effective methods to approximate the original data in lower dimensionality space (Sch¨utze, 1997), and reduce the vectors to 50 dimensions. To combine text- and image-based semantic models in a joint representation, we separately normalize their vectors to unit length, and concatenate them, along the lines of Bruni et al. (2011). More sophisticated combination models have been proposed in the recent literature on multimodal semantics. For example, Bruni et al. (2012a) use SVD as a mixing strategy, given its ability to smooth the matrices and uncover latent dimensions. Another example is Silberer and Lapata (2013), where Canonical Correlation Analysis is used. We reserve the exploration of more advanced combination methods for further studies. Finally, to represent the 11 categories we experiment with (see Table 1), we average the vectors of the concepts they include. 4We use k-means, the most commonly employed clustering algorithm for this task. 5http://clic.cimec.unitn.it/vsem/ 1964 4 Experiments A question is posed over how to evaluate the relationship between the different distributional models and brain data. Comparing each model’s predictive performance using the same strategy as Mitchell et al. </context>
</contexts>
<marker>Silberer, Lapata, 2013</marker>
<rawString>Carina Silberer and Mirella Lapata. 2013. Models of semantic representation with visual attributes. In Proceedings of ACL, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Sivic</author>
<author>Andrew Zisserman</author>
</authors>
<title>Video Google: A text retrieval approach to object matching in videos.</title>
<date>2003</date>
<booktitle>In Proceedings of ICCV,</booktitle>
<pages>1470--1477</pages>
<location>Nice, France.</location>
<contexts>
<context position="17751" citStr="Sivic and Zisserman, 2003" startWordPosition="2730" endWordPosition="2733">e use images from ImageNet (Deng et al., 2009),3 a very large image database organized on top of the WordNet hierarchy (Fellbaum, 1998). ImageNet has more than 14 million images, covering 21K WordNet nominal synsets. ImageNet stands out for the high quality of its images, both in terms of resolution and concept annotations. Moreover, for around 3K concepts, annotations of object bounding boxes is provided. This last feature allows us to exploit object localization within our experiments. To build visual distributional models, we utilize the bag-of-visual-words (BoVW) representation of images (Sivic and Zisserman, 2003; Csurka et al., 2004). Inspired by NLP, BoVW discretizes the image content in terms of a histogram of visual word counts. Differently from NLP, in vision there is not a natural notion of visual words, hence a visual vocabulary has to be built from scratch. The process works as follows. First, a large set of low-level features is extracted from a corpus of images. The low-level feature vectors are subsequently clustered into different regions (visual words). Given then a new image, each of the low-level feature vectors extracted from the patches that compose it is mapped to the nearest visual </context>
</contexts>
<marker>Sivic, Zisserman, 2003</marker>
<rawString>Josef Sivic and Andrew Zisserman. 2003. Video Google: A text retrieval approach to object matching in videos. In Proceedings of ICCV, pages 1470–1477, Nice, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="2050" citStr="Turney and Pantel, 2010" startWordPosition="300" endWordPosition="303">representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility. 1 Introduction Many recent neuroscientific studies have brought support to the view that concepts are represented in terms of patterns of neural activation over broad areas, naturally encoded as vectors in a neural semantic space (Haxby et al., 2001; Huth et al., 2012). Similar representations are also widely used in computational linguistics, and in particular in distributional semantics (Clark, 2012; Erk, 2012; Turney and Pantel, 2010), that captures meaning in terms of vectors recording the patterns of co-occurrence of words in large corpora, under the hypothesis that words that occur in similar contexts are similar in meaning. Since the seminal work of Mitchell et al. (2008), there has thus being interest in investigating whether corpus-harvested semantic representations can contribute to the study of concepts in the brain. The relation is mutually beneficial: From the point of view of brain activity decoding, a strong correlation between corpus-based and brain-derived conceptual representations would mean that we could u</context>
<context position="13079" citStr="Turney and Pantel, 2010" startWordPosition="2006" endWordPosition="2009">08), for each voxel, the set of 51 words from each unique pair of scanning sessions were correlated using Pearson’s correlation (6 sessions and therefore 15 unique pairs), and the mean of the 15 resulting correlation coefficients was taken as the measure of stability. The 500 voxels with highest mean correlations were selected. 3 Distributional models Distributional semantic models approximate word meaning by keeping track of word co-occurrence statistics from large textual input, relying on the distributional hypothesis: The meaning of a word can be induced by the context in which it occurs (Turney and Pantel, 2010). Despite their great success, these models still rely on verbal input only, while humans base their meaning representation also on perceptual information (Louwerse, 2011). Thanks to recent developments in computer vision, it is nowadays possible to take the visual perceptual channel into account, and build new computational models of semantics enhanced with visual information (Feng and Lapata, 2010; Bruni et al., 2011; Leong and Mihalcea, 2011; Bergsma and Goebel, 2011; Bruni et al., 2012a). Given a set of target concepts and a collection of images depicting those concepts, it is indeed possi</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Tzourio-Mazoyer</author>
<author>B Landeau</author>
<author>D Papathanassiou</author>
<author>F Crivello</author>
<author>O Etard</author>
<author>N Delcroix</author>
<author>B Mazoyer</author>
<author>M Joliot</author>
</authors>
<title>Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain.</title>
<date>2002</date>
<journal>Neuroimage,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="12071" citStr="Tzourio-Mazoyer et al. (2002)" startWordPosition="1851" endWordPosition="1854">tract knowledge and rules and controlled behaviour (Miller et al., 2002). Regarding semantics, the dorsomedial prefrontal cortex has been implicated in self-guided retrieval of semantic information (e.g., uncued speech production), the ventromedial prefrontal cortex in motivation and emotional processing, the inferior frontal gyrus in phonological and syntactic processing, (Binder et al., 2009) and integration of lexical information (Hagoort, 2005). Given the association with linguistic processing we anticipated a bias in favour of Window2. The four lobes were identified and partitioned using Tzourio-Mazoyer et al. (2002)’s automatic anatomical labelling scheme. 1962 Voxel selection The set of 500 most stable voxels, both within the whole brain and from within each region of interest were identified for analysis. The most stable voxels were those showing consistent variation across the different stimuli between scanning sessions. Specifically, and following a similar strategy to Mitchell et al. (2008), for each voxel, the set of 51 words from each unique pair of scanning sessions were correlated using Pearson’s correlation (6 sessions and therefore 15 unique pairs), and the mean of the 15 resulting correlation</context>
</contexts>
<marker>Tzourio-Mazoyer, Landeau, Papathanassiou, Crivello, Etard, Delcroix, Mazoyer, Joliot, 2002</marker>
<rawString>N Tzourio-Mazoyer, B Landeau, D Papathanassiou, F Crivello, O Etard, N Delcroix, B Mazoyer, and M Joliot. 2002. Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain. Neuroimage, 15(1):273–289.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
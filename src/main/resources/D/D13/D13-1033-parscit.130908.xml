<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000180">
<title confidence="0.973083">
The Effects of Syntactic Features in Automatic Prediction of Morphology
</title>
<author confidence="0.998701">
Wolfgang Seeker and Jonas Kuhn
</author>
<affiliation confidence="0.996873">
Institute for Natural Language Processing
University of Stuttgart
</affiliation>
<email confidence="0.998028">
{seeker,jonas}@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.998592" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999519272727273">
Morphology and syntax interact considerably
in many languages and language processing
should pay attention to these interdependen-
cies. We analyze the effect of syntactic fea-
tures when used in automatic morphology pre-
diction on four typologically different lan-
guages. We show that predicting morphology
for languages with highly ambiguous word
forms profits from taking the syntactic context
of words into account and results in state-of-
the-art models.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.977049645833334">
In this paper, we investigate the interplay between
syntax and morphology with respect to the task of
assigning morphological descriptions (or tags) to
each token of a sentence. Specifically, we examine
the effect of syntactic information when it is inte-
grated into the feature model of a morphological tag-
ger. We test the effect of syntactic features on four
languages – Czech, German, Hungarian, and Span-
ish – and find that syntactic features improve our tag-
ger considerably for Czech and German, but not for
Hungarian and Spanish. Our analysis of construc-
tions that show morpho-syntactic agreement sug-
gests that syntactic features are important if the lan-
guage shows frequent word form syncretisms1 that
can be disambiguated by the syntactic context.
The meaning of a sentence is structurally encoded
1Syncretism describes the situation where a word form is
ambiguous between several different morphological descrip-
tions within its inflection paradigm.
by morphological and syntactic means.2 Different
languages, however, use them to a different extent.
Languages like English encode grammatical infor-
mation (like the subject vs object status of an argu-
ment) via word order, whereas languages like Czech
or Hungarian use different word forms. Automatic
analysis of languages with rich morphology needs
to pay attention to the interaction between morphol-
ogy and syntax in order to arrive at suitable com-
putational models. Linguistic theory (e. g., Bresnan
(2001), Melˇcuk (2009)) suggests many interactions
between morphology and syntax. For example, lan-
guages with a case system use different forms of the
same word to mark different syntactic (or seman-
tic) relations (Blake, 2001). In many languages, two
words that participate in a syntactic relation show
covariance in some or all of their morphological fea-
tures (so-called agreement, Corbett (2006)).3
Automatic annotation of morphology assigns
morphological descriptions (e. g., nominative-
singular-masculine) to word forms. It is usually
modeled as a sequence model, often in combination
with part-of-speech tagging and lemmatization
(Collins, 2002; Hajiˇc, 2004; Smith et al., 2005;
Chrupała et al., 2008, and others). Sequence models
achieve high accuracy and coverage but since they
only use linear context they only approximate some
of the underlying hierarchical relationships. As
an example for these hierarchical relationships,
</bodyText>
<footnote confidence="0.9928668">
2And also by prosodic means, which we will not discuss
since text-based tools rarely have access to this information.
3For example, in English, the subject of a sentence and the
finite verb agree with respect to their number and person fea-
ture.
</footnote>
<page confidence="0.971563">
333
</page>
<note confidence="0.80648">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 333–344,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.960594333333333">
PM MO
MO
MO
CJ
CD NK MO
NK
NK
CJ
die wirtschaftlich am weitesten entwickelten , modernen und zum Teil katholisch gepr¨agten Regionen
nom/acc.pl.fem nom/acc.pl.fem
the economic - most developed , modern and to part catholic influenced regions
’the regions that are economically most developed, modern, and partly catholic’
</figure>
<figureCaption confidence="0.999995">
Figure 1: Example of a German noun phrase. First and last word agree in number, gender, and case value.
</figureCaption>
<bodyText confidence="0.999216771428571">
Figure 1 shows a German noun phrase taken from
the German TiGer corpus (Brants et al., 2002).
The two bold-faced words are the determiner and
the head noun of the phrase, and they agree in
their gender, number, and case values. The word
Regionen (regions) is four-way ambiguous for its
case value, which is reduced to a two-way ambi-
guity between nominative and accusative by the
determiner. Further disambiguation would require
information about the syntactic role of the noun
phrase in a sentence. There are 11 tokens between
these two words, which would require a context
window of at least 13 to capture the agreement
relation within a sequence model. Syntactically,
however, as indicated by the dependency tree,
the determiner and the head are linked directly.
The interdependency between morphology and
syntax in the example thus manifests itself in the
morphological disambiguation of a highly syncretic
word form because of its government or agreement
relation to its respective syntactic head/dependents.
Of course, the sequence model is most of the
time a reasonable approximation, because the ma-
jority of noun phrases in the TiGer corpus are not
as long as the example in Figure 1.4 Furthermore,
not all languages show this kind of relationship be-
tween morphological forms and syntactic relation as
demonstrated for German. But taking advantage of
the morphosyntactic dependencies in a language can
give us better models that may even be capable of
handling the more difficult or rare cases. We there-
fore advocate that models for predicting morphology
should be designed with the typological characteris-
tics of a language and its morphosyntactic properties
in mind, and should, where appropriate, integrate
</bodyText>
<footnote confidence="0.881689">
4We find 57,551 noun phrases with less than three tokens
between determiner and noun and 4,670 with three or more.
</footnote>
<bodyText confidence="0.99815145">
syntactic information in order to better model the
morphosyntactic interdependencies of the language.
In the remainder of the paper, we show empiri-
cally that taking syntactic information into account
produces state-of-the-art models for languages with
a high interdependency between morphology and
syntax. We use a simple setup, where we combine
a morphological tagger and a dependency parser in
a bootstrapping architecture in order to analyze the
effect of syntactic information on the performance
of the morphological tagger (Section 2). Using syn-
tactic features in morphology prediction requires a
syntactically annotated corpus for training a statisti-
cal parser, which may not be available for languages
with few resources. We show in Section 3 that only
very little syntactically annotated data is required to
achieve the improvements. We furthermore expect
that the improved morphological information also
improves parsing performance and present a prelim-
inary experiment in Section 4.
</bodyText>
<sectionHeader confidence="0.998773" genericHeader="introduction">
2 Experiments
</sectionHeader>
<bodyText confidence="0.9999076">
In this section, we present a series of experiments
that investigate the effect of syntactic information on
the prediction of morphological features. We start
by describing our data sets and the system that we
used for the experiments.
</bodyText>
<subsectionHeader confidence="0.998177">
2.1 Languages and Data Sets
</subsectionHeader>
<bodyText confidence="0.999787571428571">
We test our hypotheses on four different languages:
Czech, German, Hungarian, and Spanish.
Spanish, a Romance language, and German, a
Germanic language, constitute inflecting languages
that show verbal and nominal morphology, but not
as sophisticated as Czech and Hungarian. As we
will see in the experiments, it is relatively easy to
</bodyText>
<page confidence="0.998288">
334
</page>
<bodyText confidence="0.999775722222222">
predict the morphological information annotated in
the Spanish data set.
Czech and Hungarian represent languages with
very rich morphological systems both in verbal and
nominal morphological paradigms. They differ sig-
nificantly in the way in which morphological infor-
mation is encoded in word forms. Czech, a Slavic
language, is an inflecting language, where one suf-
fix may signal several different morphological cate-
gories simultaneously (e. g., number, gender, case).
In contrast, Hungarian, a Finno-Ugric language, is
of the agglutinating type, where each morphological
category is marked by its own morpheme.
Both German and Czech show various form syn-
cretisms in their inflection paradigms. Form syn-
cretisms emerge when the same word form is am-
biguous between several different morphological de-
scriptions, and they are a major challenge to auto-
matic morphological analysis. Spanish shows syn-
cretism in the verbal inflection paradigms. In Hun-
garian, form syncretisms are much less frequent.
The case paradigm of Hungarian only shows one
form syncretism between dative and genitive case
(out of about 18 case suffixes).
All languages show agreement between subject
and verb, and within the noun phrase. The word or-
der in Czech and Hungarian is very variable whereas
it is more restrictive in Spanish and German.
As our data, we use the CoNLL 2009 Shared
Task data sets (Hajiˇc et al., 2009) for Czech and
Spanish. For German, we use the dependency
conversion of the TiGer treebank by Seeker and
Kuhn (2012), splitting it into 40k/5k/5k sentences
for training/development/test. For Hungarian, we
use the Szeged Dependency Treebank (Vincze et al.,
2010), with the split of Farkas et al. (2012).
</bodyText>
<subsectionHeader confidence="0.998827">
2.2 System Description
</subsectionHeader>
<bodyText confidence="0.9998505">
To test our hypotheses, we implemented a tagger
that assigns full morphological descriptions to each
token in a sentence. The system was inspired by the
morphological tagger included in mate-tools.5 Like
the tagger provided with mate-tools, it is a classifier
that tags each token using the surrounding tokens in
</bodyText>
<footnote confidence="0.967361333333333">
5A collection of language independent, data-driven analysis
tools for lemmatization, pos-tagging, morphological analysis,
and dependency parsing: http://code.google.com/p/mate-tools
</footnote>
<bodyText confidence="0.9999024375">
its feature model. Models are trained using passive-
aggressive online training (Crammer et al., 2003).
The system makes two passes over each sentence:
The first pass provides predicted tags that are used
as features during the second pass. We also adopted
the idea of a tag filter, which deterministically as-
signs tags for words that always occur with the same
tag in the training data.
For all matters of syntactic annotation in this pa-
per, we use the graph-based dependency parser by
Bohnet (2010), also included in mate-tools. All data
sets are annotated with gold syntactic information,
which is used to train the parsing models.
For our experiments, we use a bootstrapping ap-
proach: the parser uses the output of the morphology
in its feature set, and the morphological tagger we
want to analyze uses the output of the parser as syn-
tactic features. Since it is best to keep the training
setting as similar as possible to the test setting, we
use 10-fold jackknifing to annotate our training data
with predicted morphology or syntax respectively.
Jackknifing differs from cross-validation only in
its purpose. Cross-validation is used for evaluating
data, jackknifing is used to annotate data. The data
set is split into n parts, and n-1 parts are used to train
a model for annotating the nth part. This is then
rotated n times such that each part is annotated by
the automatic tool without training it on its own test
data. Jackknifing is important for creating a realis-
tic training scenario that provides automatic prepro-
cessing. For annotating development and test sets,
models are trained on the jackknifed training set.
</bodyText>
<subsectionHeader confidence="0.998927">
2.3 The Effects of Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999992857142857">
In the first experiment, we use the system described
in Section 2.2 to predict morphological information
on all four languages. We start with describing the
general setup and the feature set, and continue with
a discussion of the results.
The experimental setup is as follows: the German
and Spanish data sets are annotated with lemma and
part-of-speech information using 10-fold jackknif-
ing. The annotation is done with mate-tools’ lem-
matizer and pos-tagger. For Czech and Hungarian,
we keep the annotation provided with the data sets.
Note that our experimental setup does not include
lemmas or part-of-speech tags as part of the predic-
tion of the morphology but annotates them in a pre-
</bodyText>
<page confidence="0.998519">
335
</page>
<bodyText confidence="0.999609">
processing step. It is not necessary to separate part-
of-speech and lemma from the prediction of mor-
phology and, in fact, many systems perform these
steps simultaneously (e. g. Spoustov´a et al. (2009)).
Doing morphology prediction as a separate step al-
lows us to use lemma and part-of-speech informa-
tion in the feature set.6
</bodyText>
<equation confidence="0.964911304347826">
static features
form form1b form2b
form3b form1a lemma2a
pos1b pos2b pos1a
form+pos pos+s1 pos+s2
pos+s3 pos+s4 lemma+p2
lemma+p3 pos+number form+form1b
pos+pos1a pos+pos1b+pos2b s1+s1 1b
s1+s1 1a s2+s2 1a last-verb-lemma
last-verb-pos next-verb-lemma next-verb-pos
dynamic features
tag1b+tag2b tag2b+tag3b tag1a
tag1a+tag1b tag1a+tag2a tag2a+tag3a
pos1b+case1b last-verb-tag next-verb-tag
pos1b+case1b+pos2b+case2b
Hungarian only features
pos+uppercase
Czech only features
pos+p2
Spanish only features
s5 p1 p4
p5 s2 1a s3 1a
s4 1a
</equation>
<tableCaption confidence="0.811658">
Table 1: Baseline feature set. form means word form,
</tableCaption>
<bodyText confidence="0.944444029411765">
lemma is lemma, pos is part-of-speech, s1/p1 stand for
suffix and prefix of length 1 (characters), tag is the mor-
phological tag predicted by the system, 1b/1a means 1
token before/after the current token, and + marks feature
conjunctions. number marks if the form contains a digit.
After preprocessing the data, our baseline system
is trained using the feature set shown in Table 1. The
baseline system does not make use of any syntactic
information but predicts morphological information
based solely on tokens and their linear context. The
features are divided into static features, which can be
computed on the input, and dynamic features, which
are computed also on previous output of the system
(cf. two passes in Section 2.2).
6Lemma and part-of-speech prediction may also profit from
syntactic information, see e.g. Prins (2004) or Bohnet and Nivre
(2012).
The feature sets in Table 1 were developed specif-
ically for our experiments and are the result of an
automatic forward/backward feature selection pro-
cess. The purpose of the feature selection was to ar-
rive at a baseline system that performs well without
any syntactic information. With such an optimized
baseline system, we can measure the contribution of
syntactic features more reliably.
The last-verb/next-verb and pos+case features are
variants of the features proposed in Votrubec (2006).
They extract information about the first verb within
the last 10/the next 30 tokens in the sentence. The
case feature extracts the case value from previously
assigned morphological tags. Note that the verb
features are approximating syntactic information by
making the assumption that the closest verbs are
likely to be syntactic heads for many words.
</bodyText>
<figure confidence="0.646618666666667">
static features
h lemma h s2 h s3 pos+h pos s1+h s1
h dir h dir+h pos
ld s1 ld s2 ld p1 ld p4
dynamic features
h tag ld tag
</figure>
<tableCaption confidence="0.611233">
Table 2: Syntactic features. h and ld mark features from
</tableCaption>
<bodyText confidence="0.985251772727273">
the head and the left-most daughter, dir is a binary fea-
ture marking the direction of the head with respect to the
current token.
After training the baseline models, we use them to
annotate the whole data set with morphological in-
formation (using 10-fold jackknifing for the training
portions). We then use 10-fold jackknifing again to
annotate the data sets with the dependency parser.
At this point, all our data sets are annotated with
predicted morphology from our baseline system and
with syntactic information from the parser, which
uses the morphological information from our base-
line system in its feature set. We can now retrain our
morphological tagger using features that are derived
from the dependency trees provided by the parser.
Note that this is not a stacking architecture, since
the second system does not use the predicted mor-
phology output from the baseline system. The loop
simply ensures that we get the best possible syntac-
tic features.
We extract two kinds of syntactic features: fea-
tures of the syntactic head of the current token, and
</bodyText>
<page confidence="0.995765">
336
</page>
<table confidence="0.833615578947368">
dev set test set
all oov all oov
Czech
94.75 84.12 94.78 84.23
93.80 80.47 93.57 80.53
*94.40 81.51 *94.24 81.61
*94.80 82.45 *94.64 82.80
German
90.63 72.11 89.04 70.80
92.59 80.73 91.48 78.83
*93.70 82.71 *92.51 80.20
*94.28 *84.12 *93.32 *82.35
Hungarian
97.27 92.61 97.03 91.28
97.38 92.39 97.19 91.50
*97.63 92.79 *97.45 91.92
Spanish
98.23 92.46 98.02 93.15
98.24 92.30 98.07 93.03
</table>
<figure confidence="0.9723568">
98.40 92.82 *98.22 93.64
featurama
our baseline
pred syntax
gold syntax
RFTagger
our baseline
pred syntax
gold syntax
our baseline
pred syntax
gold syntax
our baseline
pred syntax
gold syntax
</figure>
<figureCaption confidence="0.6081885">
Table 4: The effect of syntactic features when predicting
morphology using lexicons. * mark statistically signifi-
cantly better models compared to our baseline (sentence-
based t-test with α = 0.05).
</figureCaption>
<table confidence="0.999191772727273">
dev set test set
all oov all oov
Czech
morfette 90.37 68.66 90.01 67.25
our baseline 92.51 73.12 92.29 72.58
pred syntax *93.18 74.04 *92.82 73.11
gold syntax *93.64 75.20 *93.30 74.96
German
morfette 86.78 66.37 84.58 61.05
our baseline 90.92 72.52 89.11 69.67
pred syntax *92.07 75.06 *90.10 71.18
gold syntax *92.70 *76.29 *90.87 *73.20
Hungarian
morfette *96.19 *85.82 95.99 *85.43
our baseline 96.08 84.49 95.94 83.76
pred syntax 96.18 84.70 96.11 83.85
gold syntax *96.46 85.30 *96.35 84.50
Spanish
morfette 97.83 89.67 97.76 91.00
our baseline 97.83 89.05 97.59 90.88
pred syntax 97.84 89.08 97.67 90.91
gold syntax 98.11 90.34 97.88 91.61
</table>
<tableCaption confidence="0.79569">
Table 3: The effect of syntactic features when predicting
morphological information. * mark statistically signifi-
cantly better models compared to our baseline (sentence-
based t-test with α = 0.05).
</tableCaption>
<bodyText confidence="0.998519204545455">
features of the left-most daughter of the current to-
ken. We also experimented with other types, e. g.
the right-most daughter, but these features did not
improve the model. This is likely due to the way
these languages encode morphological information
and may be different for other languages. From the
head and the left-most daughter, we construct fea-
tures about form, lemma, affixes, and tags. Table 2
lists the syntactic features that we use in the model.
With the syntactic features available due to the
parsing step, we train new models with the full sys-
tem. For each language, we run four experiments.
The first two are baseline experiments, where we
use the off-the-shelf morphological tagger morfette
(Chrupała et al., 2008) and our own baseline sys-
tem, both of which do not use any syntactic features.
In the third experiment, we evaluate our full system
using the syntactic features provided by the depen-
dency parser. As an oracle experiment, we also re-
port results on the full system when using the gold
standard syntax from the treebank. Table 3 presents
all results in terms of accuracy on all tokens (all)
and out-of-vocabulary tokens only (oov). Out-of-
vocabulary tokens do not occur in the training data.
We find trends along several axes: Generally, the
syntactic features work well for Czech and Ger-
man, whereas for Hungarian and Spanish, they do
not yield any significant improvement. The im-
provements for German and Czech are between 0.5
(Czech) and 1.0 (German) percentage points abso-
lute in token accuracy, and between 0.2 (Czech test
set) and 2.5 (German dev set) percentage points ab-
solute in accuracy of unknown words. There are no
obvious differences between the development and
the test set in any of the languages.
Compared to the morfette baseline, we find our
systems to be either superior or equal to morfette in
terms of token accuracy. Regarding accuracy on un-
known words, morfette outperforms our systems for
Hungarian, but is outperformed on Czech and Ger-
man. For Spanish, all systems yield similar results.
Looking at the oracle experiment, we see that for
all languages, the system can learn something from
syntax. For Czech and German, this is clearly the
</bodyText>
<page confidence="0.992488">
337
</page>
<bodyText confidence="0.999901666666667">
case, for Hungarian and Spanish, the differences are
small but visible. There are pronounced differences
between the predicted and the gold syntax experi-
ments in Czech and German. Clearly, the parser
makes mistakes that propagate through to the pre-
diction of the morphology.
</bodyText>
<subsectionHeader confidence="0.997468">
2.4 Syntax vs Lexicon
</subsectionHeader>
<bodyText confidence="0.999960621621622">
The current state-of-the-art in predicting morpho-
logical features makes use of morphological lexi-
cons (e.g. Hajiˇc (2000), Hakkani-T¨ur et al. (2002),
Hajiˇc (2004)). Lexicons define the possible morpho-
logical descriptions of a word and a statistical model
selects the most probable one among them. In the
following experiment, we test whether the contribu-
tion of syntactic features is similar or different to the
contribution of morphological lexicons.
Lexicons encode important knowledge that is dif-
ficult to pick up in a purely statistical system, e. g.
the gender of nouns, which often cannot be deduced
from the word form (Corbett, 1991).7
We extend our system from the previous experi-
ment to include information from a morphological
dictionaries. For Czech, we use the morphologi-
cal analyzer distributed with the Prague Dependency
Treebank 2 (Hajiˇc et al., 2006). For German, we
use DMor (Schiller, 1994). For Hungarian, we use
(Tr´on et al., 2006), and for Spanish, we use the mor-
phological analyzer included in Freeling (Carreras et
al., 2004). The output of the analyzers is given to the
system as features that simply record the presence of
a particular morphological analysis for the current
word. The system can thus use the output of any
tool regardless of its annotation scheme, especially
if the annotation scheme of the treebank is different
from the one of the morphological analyzer.
Table 4 presents the results of experiments where
we add the output of the morphological analyzers
to our system. Again, we run experiments with and
without syntactic features. For Czech, we also show
results from featurama8 with the feature set devel-
oped by Votrubec (2006). For German, we show re-
sults for RFTagger (Schmid and Laws, 2008).
As expected, the information from the morpho-
logical lexicon improves the overall performance
</bodyText>
<footnote confidence="0.999905333333333">
7Lexicons are also often used to speed up processing con-
siderably by restricting the search space of the statistical model.
8http://sourceforge.net/projects/featurama/
</footnote>
<bodyText confidence="0.999885642857143">
considerably compared to the results in Table 3, es-
pecially on unknown tokens. This shows that even
with the considerable amounts of training data avail-
able nowadays, rule-based morphological analyzers
are important resources for morphological descrip-
tion (cf. Hajiˇc (2000)). The contribution of syn-
tactic features in German and Czech is almost the
same as in the previous experiment, indicating that
the syntactic features contribute information that is
orthogonal to that of the morphological lexicon. The
lexicon provides lexical knowledge about a word
form, while the syntactic features provide the syn-
tactic context that is needed in German and Czech
to decide on the right morphological tag.
</bodyText>
<subsectionHeader confidence="0.961227">
2.5 Language Differences
</subsectionHeader>
<bodyText confidence="0.9997265">
From the previous experiments, we conclude that
syntactic features help in the prediction of morphol-
ogy for Czech and German, but not for Hungarian
and Spanish. To further investigate the difference
between Czech and German on the one hand, and
Hungarian and Spanish on the other, we take a closer
look at the output of the tagger.
We find an interesting difference between the
two pairs of languages, namely the performance
with respect to agreement. Agreement is a phe-
nomenon where morphology and syntax strongly in-
teract. Morphological features co-vary between two
items in the sentence, but the relation between these
items can occur at various linguistic levels (Corbett,
2006). If the syntactic information helps with pre-
dicting morphological information, we expect this
to be particularly helpful with getting agreement
right. All languages show agreement to some ex-
tent. Specifically, all languages show agreement in
number (and person) between the subject and the
verb of a clause. Czech, German, and Spanish show
agreement in number, gender, and case (not Span-
ish) within a noun phrase. Hungarian shows case
agreement within the noun phrase only rarely, e.g.
for attributively used demonstrative pronouns.
In order to test the effect on agreement, we mea-
sure the accuracy on tokens that are in an agreement
relation with their syntactic head. We counted sub-
ject verb agreement as well as agreement with re-
spect to number, gender, and case (where applicable)
between a noun and its dependent adjective and de-
terminer. Table 5 displays the counts from the devel-
</bodyText>
<page confidence="0.994891">
338
</page>
<bodyText confidence="0.9971444">
opment sets of each language. We compare the base-
line system that does not use any syntactic informa-
tion with the output of the morphological tagger that
uses the gold syntax. We use the gold syntax rather
than the predicted one in order to eliminate any in-
fluence from parsing errors. As can be seen from the
results, the level of agreement relations in Czech and
German improves when using syntactic information,
whereas in Spanish and Hungarian, only very tiny
changes occur.
</bodyText>
<table confidence="0.998991555555556">
agreement baseline gold syntax
Czech
sbj-verb 3199/4044 = 79.10 3264/4044 = 80.71
NP case 8719/9132 = 95.48 8821/9132 = 96.59
NP num 8933/9132 = 97.82 9016/9132 = 98.73
NP gen 8493/9132 = 93.00 8768/9132 = 96.01
German
sbj-verb 4412/4696 = 93.95 4562/4696 = 97.15
NP case 13340/13951 = 95.62 13510/13951 = 96.84
NP num 13631/13951 = 97.71 13788/13951 = 98.83
NP gen 13253/13951 = 95.00 13528/13951 = 96.97
Hungarian
sbj-verb 8653/10219 = 84.68 8655/10219 = 84.70
NP case 402/891 = 45.12 412/891 = 46.24
Spanish
sbj-verb 1930/2004 = 96.31 1932/2004 = 96.41
NP num 8810/8849 = 99.56 8816/8849 = 99.63
NP gen 8810/8849 = 99.56 8821/8849 = 99.68
</table>
<tableCaption confidence="0.995866">
Table 5: Agreement counts in morphological annotation
compared between the baseline system and the oracle
system using gold syntax.
</tableCaption>
<bodyText confidence="0.999841952380953">
For Czech and German, these results sugguest
that syntactic information helps with agreement. We
believe that the reasons why it does not help for
Hungarian and Spanish are the following: for Span-
ish, we see that also the baseline model achieves
very high accuracies (cf. Table 3) and also high rates
of correct agreement. It seems that for Spanish, syn-
tactic context is simply not necessary to make the
correct prediction. For Hungarian, the reason lies
within the inflectional paradigms of the language,
which do not show any form syncretism, mean-
ing that word forms in Hungarian are usually not
ambiguous within one morphological category (e.g.
case). Making a morphological tag prediction, how-
ever, is difficult only if the word form itself is am-
biguous between several morphological tags. In this
case, using the agreement relation between the word
and its syntactic head can help the system making
the proper prediction. This is the situation that we
find in Czech and German, where form syncretism
is pervasive in the inflectional paradigms.
</bodyText>
<subsectionHeader confidence="0.996866">
2.6 Syntactic Features in Czech
</subsectionHeader>
<bodyText confidence="0.999921608695652">
In Section 2.4 we compared the performance of our
system on Czech to another system, featurama (see
Table 4). Featurama outperforms our baseline sys-
tem by a percentage point in token accuracy (and
even more for unknown tokens). Syntactic informa-
tion closes that gap to a large extent but only using
gold syntax gets our system on a par with featurama.
The question then arises whether the syntactic
features actually contribute something new to the
task, or whether the same effect could also be
achieved with linear context features alone as in fea-
turama. In order to test this we run an additional
experiment, where we add some of the syntax fea-
tures to the feature set of featurama. Specifically,
we add the static features from Table 2 that do not
use lemma or part-of-speech information. Due to the
way featurama works, we cannot use features from
the morphological tags (the dynamic features).
The results in Table 6 show that also featurama
profits from syntactic features, which corroborates
the findings from the previous experiments. We also
note again that better syntax would improve results
even more.
</bodyText>
<table confidence="0.9106686">
dev set test set
all oov all oov
featurama 94.75 84.12 94.78 84.23
pred syntax 95.18 84.65 95.09 84.52
gold syntax *95.39 84.62 *95.34 85.03
</table>
<tableCaption confidence="0.966428">
Table 6: Syntactic features for featurama (Czech). * mark
statistically significantly better models compared to feat-
urama (sentence-based t-test with α = 0.05).
</tableCaption>
<sectionHeader confidence="0.958615" genericHeader="method">
3 How Much Syntax is Needed?
</sectionHeader>
<bodyText confidence="0.999858">
Syntactic features require syntactically annotated
corpora. Without a treebank to train the parser, the
morphology cannot profit from syntactic features.9
This may be problematic for languages for which
there is no treebank, because creating a treebank is
expensive. Fortunately, it turns out that very small
amounts of syntactically annotated data are enough
</bodyText>
<footnote confidence="0.979487">
9Which is of course only a problem for statistical parsers.
</footnote>
<page confidence="0.99803">
339
</page>
<figure confidence="0.9982696">
German Czech
0 5000 10000 15000 20000 25000 30000 35000 40000
# of sentences in training data of syntactic parser
0 5000 10000 15000 20000 25000 30000 35000 40000
# of sentences in training data of syntactic parser
accuracy of morphology
94
93
92
91
90
89
88
dev
test
accuracy of morphology
94
93
92
91
90
89
88
dev
test
</figure>
<figureCaption confidence="0.996956">
Figure 2: Dependency between amount of training data for syntactic parser and quality of morphological prediction.
</figureCaption>
<bodyText confidence="0.994767463414635">
to provide a parsing quality that is sufficient for the
morphological tagger.
In order to test what amount of training data is
needed, we train several parsing models on increas-
ing amounts of syntactically annotated data. For ex-
ample, the first experiment uses the first 1,000 sen-
tences of the treebank. We perform 5-fold jackknif-
ing with the parser on these sentences to annotate
them with syntax. Then we train one parsing model
on these 1,000 sentences and use it to annotate the
rest of the training data as well as the development
and the test set. This gives us the full data set an-
notated with syntax that was learned from the first
1,000 sentences of the treebank. The morphologi-
cal tagger is then trained on the full training set and
applied to development and test set.
Figure 2 shows the dependency between the
amount of training data given to the parser and the
quality of the morphological tagger using syntac-
tic features provided by this parser. The left-most
point corresponds to a model that does not use syn-
tactic information. For both languages, German
and Czech, we find that already 1,000 sentences are
enough training data for the parser to provide useful
syntactic information to the morphological tagger.
After 5,000 sentences, both curves flatten out and
stay on the same level. We conclude that using syn-
tactic features for morphological prediction is viable
even if there is only small amounts of syntactic data
available to train the parser.
As a related experiment, we also test if we can get
the same effect with a very simple and thus much
faster parser. We use the brute-force algorithm de-
scribed in Covington (2001), which selects for each
token in the sentence another token as the head. It
does not have any tree requirements, so it is not even
guaranteed to yield a cycle-free tree structure. In Ta-
ble 7, we compare the simple parser with the mate-
parser, both trained on the first 5,000 sentences of
the treebank. Evaluation is done in terms of labeled
(LAS) and unlabeled attachment score (UAS).10
</bodyText>
<table confidence="0.98644775">
dev set test set
LAS UAS LAS UAS
Czech
71.57 78.96 69.09 77.23
76.77 84.38 74.70 83.00
German
83.06 85.23 78.56 81.18
87.56 90.08 83.69 86.58
</table>
<tableCaption confidence="0.993328">
Table 7: Simple parser vs full parser – syntactic quality.
Trained on first 5,000 sentences of the training set.
</tableCaption>
<bodyText confidence="0.9190214375">
As expected, the simple parser performs much
worse in terms of syntactic quality. Table 8 shows
the performance of the morphological tagger when
using the output of both parsers as syntactic fea-
tures. For Czech, both parsers seem to supply sim-
ilar information to the morphological tagger, while
for German, using the full parser is clearly better.
In both cases, the morphological tagger outperforms
the models that do not use syntactic information (cf.
Table 3). The performance on unknown words is
however much worse for both languages. We con-
clude that even with a simple parser and little train-
ing data, the morphology can make use of syntactic
information to some extent.
10LAS: correct edges with correct labels UAS: correct edges
all edges,all edges
</bodyText>
<figure confidence="0.74373175">
simple parser (5k)
full parser (5k)
simple parser (5k)
full parser (5k)
</figure>
<page confidence="0.958426">
340
</page>
<table confidence="0.9873701">
dev set test set
all oov all oov
Czech
92.51 73.12 92.29 72.58
92.96 73.45 92.53 72.66
93.08 73.64 92.69 73.39
German
90.92 72.52 89.11 69.67
91.52 73.34 89.66 70.52
91.92 83.46 89.91 80.50
</table>
<tableCaption confidence="0.993614">
Table 8: Simple parser vs full parser – morphological
quality. The parsing models were trained on the first
5,000 sentences of the training data, the morphological
tagger was trained on the full training set.
</tableCaption>
<sectionHeader confidence="0.976299" genericHeader="method">
4 Does Better Morphology lead to Better
Parses?
</sectionHeader>
<bodyText confidence="0.999958193548387">
In the previous sections, we show that syntactic in-
formation improves a model for predicting morphol-
ogy for Czech and German, where syntax and mor-
phology interact considerably. A natural question
then is whether the improvement also occurs in the
other direction, namely whether the improved mor-
phology also leads to better parsing models.
In the previous experiments, we run a 10-fold
jackknifing process to annotate the training data with
morphological information using no syntactic fea-
tures and afterwards use jackknifing with the parser
to annotate syntax. The syntax is subsequently used
as features for our predicted-syntax experiments.
We can apply the same process once more with the
morphology prediction in order to annotate the train-
ing data with morphological information that is pre-
dicted using the syntactic features. A parser trained
on this data will then use the improved morphology
as features. If the improved morphology has an im-
pact on the parser, the quality of the second parsing
model should then be superior to the first parsing
model, which uses the morphology predicted with-
out syntactic information. Note that for the follow-
ing experiments, neither morphology model uses the
morphological lexicon.
Table 9 presents the evaluation of the two pars-
ing models (one using morphology without syntactic
features, the other one using the improved morphol-
ogy). The results show no improvement in parsing
performance when using the improved morphology.
Looking closer at the output, we find differences be-
</bodyText>
<table confidence="0.97905375">
dev set test set
LAS UAS LAS UAS
Czech
81.73 88.45 81.02 87.77
81.63 88.37 80.83 87.61
German
91.16 92.97 88.06 90.24
91.20 92.97 88.15 90.34
</table>
<tableCaption confidence="0.973264">
Table 9: Impact of the improved morphology on the qual-
ity of the dependency parser for Czech and German.
</tableCaption>
<bodyText confidence="0.9999261">
tween the two parsing models with respect to gram-
matical functions that are morphologically marked.
For example, in German, performance on subjects
and accusative objects improves while performance
for dative objects and genitives decreases. This sug-
gests different strengths in the two parsing models.
However, the question how to make use of the im-
proved morphology in parsing clearly needs more
research in the future. A promising avenue may be
the approach by Hohensee and Bender (2012).
</bodyText>
<sectionHeader confidence="0.999923" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9998965">
Morphological taggers have been developed for
many languages. The most common approach is the
combination of a morphological lexicon with a sta-
tistical disambiguation model (Hakkani-T¨ur et al.,
2002; Hajiˇc, 2004; Smith et al., 2005; Spoustov´a
et al., 2009; Zsibrita et al., 2013).
Our work has been inspired by Versley et al.
(2010), who annotate a treebank with morphologi-
cal information after the syntax had been annotated
already. The system used a finite-state morphology
to propose a set of candidate tags for each word,
which is then further restricted using hand-crafted
rules over the already available syntax tree.
Lee et al. (2011) pursue the idea of jointly predict-
ing syntax and morphology, out of the motivation
that joint models should model the problem more
faithfully. They demonstrate that both sides can use
information from each other. However, their model
is computationally quite demanding and its overall
performance falls far behind the standard pipeline
approach where both tasks are done in sequence.
The problem of modeling the interaction between
morphology and syntax has recently attracted some
attention in the SPMRL workshops (Tsarfaty et al.,
</bodyText>
<figure confidence="0.9765562">
no syntax
simple syntax
full syntax
no syntax
simple syntax
full syntax
baseline morph
morph w/ syntax
baseline morph
morph w/ syntax
</figure>
<page confidence="0.993068">
341
</page>
<bodyText confidence="0.999595923076923">
2010). Modeling morphosyntactic relations explic-
itly has been shown to improve statistical parsing
models (Tsarfaty and Sima’an, 2010; Goldberg and
Elhadad, 2010; Seeker and Kuhn, 2013), but the co-
dependency between morphology and syntax makes
it a difficult problem, and linguistic intuition is often
contradicted by the empirical findings. For example,
Marton et al. (2013) show that case information is
the most helpful morphological feature for parsing
Arabic, but only if it is given as gold information,
whereas using case information from an automatic
system may even harm the performance.
Morphologically rich languages pose different
challenges for automatic systems. In this paper, we
work with European languages, where the problem
of predicting morphology can be reduced to a tag-
ging problem. In languages like Arabic, Hebrew,
or Turkish, widespread ambiguity in segmentation
of single words into meaningful morphemes adds an
additional complexity. Given a good segmentation
tool that takes care of this, our approach is appli-
cable to these languages as well. For Hebrew, this
problem has also been addressed by jointly mod-
eling segmentation, morphological prediction, and
syntax (Cohen and Smith, 2007; Goldberg and Tsar-
faty, 2008; Goldberg and Elhadad, 2013).
</bodyText>
<sectionHeader confidence="0.999543" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999971269230769">
In this paper, we have demonstrated that using syn-
tactic information for predicting morphological in-
formation is helpful if the language shows form syn-
cretism in combination with morphosyntactic phe-
nomena like agreement. A model that uses syntactic
information is superior to a sequence model because
it leverages the syntactic dependencies that may hold
between morphologically dependent words as sug-
gested by linguistic theory. We also showed that
only small amounts of training data for a statistical
parser would be needed to improve the morphologi-
cal tagger. Making use of the improved morphology
in the dependency parser is not straight-forward and
requires more investigation in the future.
Modeling the interaction between morphology
and syntax is important for building successful pars-
ing pipelines for languages with free word order and
rich morphology. Moreover, our experiments show
that paying attention to the individual properties of a
language can help us explain and predict the behav-
ior of automatic tools. Thus, the term ”morpholog-
ically rich language” should be viewed as a broad
term that covers many different languages, whose
differences among each other may be as important as
the difference with languages with a less rich mor-
phology.
</bodyText>
<sectionHeader confidence="0.99809" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999641888888889">
We would like to thank Jan Hajiˇc and Jan ˇStˇep´anek
for their kind help with the Czech morphology and
featurama. We would also like to thank Thomas
M¨uller for sharing resources and thoughts with us,
and Anders Bj¨orkelund for commenting on earlier
versions of this paper. This work was funded by
the Deutsche Forschungsgemeinschaft (DFG) via
SFB 732 ”Incremental Specification in Context”,
project D8.
</bodyText>
<sectionHeader confidence="0.998928" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999720142857143">
Barry J. Blake. 2001. Case. Cambridge University
Press, Cambridge, New York, 2nd edition.
Bernd Bohnet and Joakim Nivre. 2012. A Transition-
Based System for Joint Part-of-Speech Tagging and
Labeled Non-Projective Dependency Parsing. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1455–
1465, Jeju, South Korea. Association for Computa-
tional Linguistics.
Bernd Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings of
the 23rd International Conference on Computational
Linguistics, pages 89–97, Beijing, China. International
Committee on Computational Linguistics.
Sabine Brants, Stefanie Dipper, Silvia Hansen-Shirra,
Wolfgang Lezius, and George Smith. 2002. The
TIGER treebank. In Proceedings of the 1st Workshop
on Treebanks and Linguistic Theories, pages 24–41,
Sozopol, Bulgaria.
Joan Bresnan. 2001. Lexical-Functional Syntax. Black-
well Publishers.
Xavier Carreras, Isaac Chao, Llus Padr, and Muntsa Padr.
2004. Freeling: An open-source suite of language
analyzers. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC’04), pages 239–242. European Language Re-
sources Association (ELRA).
</reference>
<page confidence="0.990881">
342
</page>
<reference confidence="0.998304028037383">
Grzegorz Chrupała, Georgiana Dinu, and Josef van
Genabith. 2008. Learning morphology with mor-
fette. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC’08), pages 2362–2367, Marrakech, Morocco.
European Language Resources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.
Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
logical and syntactic disambiguation. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 208–217, Prague,
Czech Republic. Association for Computational Lin-
guistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1–8. Association for
Computational Linguistics, July.
Greville G. Corbett. 1991. Gender. Cambridge Text-
books in Linguistics. Cambridge University Press.
Greville G. Corbett. 2006. Agreement. Cambridge Text-
books in Linguistics. Cambridge University Press.
Michael A. Covington. 2001. A fundamental algorithm
for dependency parsing (with corrections). In Pro-
ceedings of the 39th Annual ACM Southeast Confer-
ence, Athens, Gorgia. ACM.
Koby Crammer, Ofer Dekel, Shai Shalev-Shwartz, and
Yoram Singer. 2003. Online passive-aggressive algo-
rithms. In Proceedings of the 16th Annual Conference
on Neural Information Processing Systems, volume 7,
pages 1217–1224, Cambridge, Massachusetts, USA.
MIT Press.
Rich´ard Farkas, Veronika Vincze, and Helmut Schmid.
2012. Dependency parsing of hungarian: Baseline re-
sults and challenges. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 55–65, Avignon,
France. Association for Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. Easy first
dependency parsing of modern Hebrew. In Proceed-
ings of the NAACL HLT 2010 First Workshop on Sta-
tistical Parsing of Morphologically-Rich Languages,
pages 103–107, Los Angeles, California, USA. Asso-
ciation for Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2013. Word seg-
mentation, unknown-word resolution, and morpholog-
ical agreement in a hebrew parsing system. Computa-
tional Linguistics, 39(1):121–160.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics, pages 371–379, Columbus, Ohio. Association for
Computational Linguistics.
Jan Hajiˇc. 2000. Morphological Tagging: Data vs. Dic-
tionaries. In Proceedings of the 6th ANLP Conference
/ 1st NAACL Meeting, pages 94—101, Seattle, Wash-
ington. Association for Computational Linguistics.
Jan Hajiˇc. 2004. Disambiguation of Rich Inflection
(Computational Morphology of Czech). Nakladatel-
stv´ı Karolinum, Prague, Czech Republic.
Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr Sgall,
Petr Pajas, Jan ˇStˇep´anek, Ji´ı Havelka, and Marie
Mikulov´a. 2006. Prague Dependency Treebank 2.0.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan Step´anek, Pavel Stran´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and Semantic dependen-
cies in multiple languages. In Proceedings of the
13th Conference on Computational Natural Language
Learning: Shared Task, pages 1–18, Boulder, Col-
orado, USA. Association for Computational Linguis-
tics.
Dilek Z. Hakkani-T¨ur, Kemal Oflazer, and G¨okhan T¨ur.
2002. Statistical morphological disambiguation for
agglutinative languages. Computers and the Humani-
ties, 36(4):381–410.
Matt Hohensee and Emily M. Bender. 2012. Getting
more from morphology in multilingual dependency
parsing. In Proceedings of the 2012 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 315–326, Montr´eal, Canada. Association
for Computational Linguistics.
John Lee, Jason Naradowsky, and David A. Smith. 2011.
A discriminative model for joint morphological disam-
biguation and dependency parsing. In Proceedings of
the 49th annual meeting of the Association for Compu-
tational Linguistics, pages 885–894, Portland, USA.
Association for Computational Linguistics.
Yuval Marton, Nizar Habash, and Owen Rambow. 2013.
Dependency parsing of modern standard arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161–194.
Igor Melˇcuk. 2009. Dependency in linguistic descrip-
tion.
Robbert Prins. 2004. Beyond N in N-gram tagging. In
Leonoor Van Der Beek, Dmitriy Genzel, and Daniel
Midgley, editors, Proceedings of the ACL 2004 Student
Research Workshop, pages 61–66, Barcelona, Spain.
Association for Computational Linguistics.
Anne Schiller. 1994. Dmor - user’s guide. Technical
report, University of Stuttgart.
</reference>
<page confidence="0.991495">
343
</page>
<reference confidence="0.998905657534247">
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics, pages 777–784, Morristown, NJ,
USA. Association for Computational Linguistics.
Wolfgang Seeker and Jonas Kuhn. 2012. Making El-
lipses Explicit in Dependency Conversion for a Ger-
man Treebank. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation, pages 3132–3139, Istanbul, Turkey. European
Language Resources Association (ELRA).
Wolfgang Seeker and Jonas Kuhn. 2013. Morphologi-
cal and syntactic case in statistical dependency pars-
ing. Computational Linguistics, 39(1):23–55.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 475–482, Vancouver, British Columbia, Canada,
October. Association for Computational Linguistics.
Drahom´ıra ”Johanka” Spoustov´a, Jan Hajiˇc, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 763–771, Athens, Greece. Association for
Computational Linguistics.
Viktor Tr´on, P´eter Hal´acsy, P´eter Rebrus, Andr´as Rung,
P´eter Vajda, and Eszter Simon. 2006. Morphdb.hu:
Hungarian lexical database and morphological gram-
mar. In Proceedings of the 5th International Confer-
ence on Language Resources and Evaluation, pages
1670–1673, Genoa, Italy.
Reut Tsarfaty and Khalil Sima’an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 40–48, Los
Angeles, California, USA. Association for Computa-
tional Linguistics.
Reut Tsarfaty, Djam´e Seddah, Yoav Goldberg, Sandra
K¨ubler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing of morphologically rich languages (SPMRL):
what, how and whither. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 1–12, Los
Angeles, California, USA. Association for Computa-
tional Linguistics.
Yannick Versley, Kathrin Beck, Erhard Hinrichs, and
Heike Telljohann. 2010. A syntax-first approach to
high-quality morphological analysis and lemma dis-
ambiguation for the tba-d/z treebank. In 9th Confer-
ence on Treebanks and Linguistic Theories (TLT9),
pages 233–244.
Veronika Vincze, D´ora Szauter, Attila Alm´asi, Gy¨orgy
M´ora, Zolt´an Alexin, and J´anos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of the 7th
Conference on International Language Resources and
Evaluation, pages 1855–1862, Valletta, Malta. Euro-
pean Language Resources Association (ELRA).
Jan Votrubec. 2006. Morphological tagging based on
averaged perceptron. In WDS’06 Proceedings of Con-
tributed Papers, pages 191–195, Praha, Czechia. Mat-
fyzpress, Charles University.
J´anos Zsibrita, Veronika Vincze, and Rich´ard Farkas.
2013. magyarlanc 2.0: szintaktikai elemz´es ´es felgy-
ors´ıtott sz´ofaji egy´ertelms´ıt´es. In Attila Tan´acs and
Veronika Vincze, editors, IX. Magyar Sz´amit´og´epes
Nyelv´eszeti Konferencia, pages 368–374, Szeged,
Hungary.
</reference>
<page confidence="0.998985">
344
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.700493">
<title confidence="0.980494">The Effects of Syntactic Features in Automatic Prediction of Morphology</title>
<author confidence="0.739321">Seeker</author>
<affiliation confidence="0.989646">Institute for Natural Language University of</affiliation>
<abstract confidence="0.995942083333333">Morphology and syntax interact considerably in many languages and language processing should pay attention to these interdependencies. We analyze the effect of syntactic features when used in automatic morphology prediction on four typologically different languages. We show that predicting morphology for languages with highly ambiguous word forms profits from taking the syntactic context of words into account and results in state-ofthe-art models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Barry J Blake</author>
</authors>
<date>2001</date>
<publisher>Case. Cambridge University Press,</publisher>
<location>Cambridge, New York,</location>
<note>2nd edition.</note>
<contexts>
<context position="2365" citStr="Blake, 2001" startWordPosition="353" endWordPosition="354">English encode grammatical information (like the subject vs object status of an argument) via word order, whereas languages like Czech or Hungarian use different word forms. Automatic analysis of languages with rich morphology needs to pay attention to the interaction between morphology and syntax in order to arrive at suitable computational models. Linguistic theory (e. g., Bresnan (2001), Melˇcuk (2009)) suggests many interactions between morphology and syntax. For example, languages with a case system use different forms of the same word to mark different syntactic (or semantic) relations (Blake, 2001). In many languages, two words that participate in a syntactic relation show covariance in some or all of their morphological features (so-called agreement, Corbett (2006)).3 Automatic annotation of morphology assigns morphological descriptions (e. g., nominativesingular-masculine) to word forms. It is usually modeled as a sequence model, often in combination with part-of-speech tagging and lemmatization (Collins, 2002; Hajiˇc, 2004; Smith et al., 2005; Chrupała et al., 2008, and others). Sequence models achieve high accuracy and coverage but since they only use linear context they only approx</context>
</contexts>
<marker>Blake, 2001</marker>
<rawString>Barry J. Blake. 2001. Case. Cambridge University Press, Cambridge, New York, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Joakim Nivre</author>
</authors>
<title>A TransitionBased System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1455--1465</pages>
<institution>Jeju, South Korea. Association for Computational Linguistics.</institution>
<contexts>
<context position="13734" citStr="Bohnet and Nivre (2012)" startWordPosition="2117" endWordPosition="2120">number marks if the form contains a digit. After preprocessing the data, our baseline system is trained using the feature set shown in Table 1. The baseline system does not make use of any syntactic information but predicts morphological information based solely on tokens and their linear context. The features are divided into static features, which can be computed on the input, and dynamic features, which are computed also on previous output of the system (cf. two passes in Section 2.2). 6Lemma and part-of-speech prediction may also profit from syntactic information, see e.g. Prins (2004) or Bohnet and Nivre (2012). The feature sets in Table 1 were developed specifically for our experiments and are the result of an automatic forward/backward feature selection process. The purpose of the feature selection was to arrive at a baseline system that performs well without any syntactic information. With such an optimized baseline system, we can measure the contribution of syntactic features more reliably. The last-verb/next-verb and pos+case features are variants of the features proposed in Votrubec (2006). They extract information about the first verb within the last 10/the next 30 tokens in the sentence. The</context>
</contexts>
<marker>Bohnet, Nivre, 2012</marker>
<rawString>Bernd Bohnet and Joakim Nivre. 2012. A TransitionBased System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1455– 1465, Jeju, South Korea. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Very high accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<journal>International Committee on Computational Linguistics.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>89--97</pages>
<location>Beijing,</location>
<contexts>
<context position="10101" citStr="Bohnet (2010)" startWordPosition="1546" endWordPosition="1547">or lemmatization, pos-tagging, morphological analysis, and dependency parsing: http://code.google.com/p/mate-tools its feature model. Models are trained using passiveaggressive online training (Crammer et al., 2003). The system makes two passes over each sentence: The first pass provides predicted tags that are used as features during the second pass. We also adopted the idea of a tag filter, which deterministically assigns tags for words that always occur with the same tag in the training data. For all matters of syntactic annotation in this paper, we use the graph-based dependency parser by Bohnet (2010), also included in mate-tools. All data sets are annotated with gold syntactic information, which is used to train the parsing models. For our experiments, we use a bootstrapping approach: the parser uses the output of the morphology in its feature set, and the morphological tagger we want to analyze uses the output of the parser as syntactic features. Since it is best to keep the training setting as similar as possible to the test setting, we use 10-fold jackknifing to annotate our training data with predicted morphology or syntax respectively. Jackknifing differs from cross-validation only i</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Very high accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 89–97, Beijing, China. International Committee on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen-Shirra</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the 1st Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>24--41</pages>
<location>Sozopol, Bulgaria.</location>
<contexts>
<context position="4039" citStr="Brants et al., 2002" startWordPosition="605" endWordPosition="608">3–344, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics PM MO MO MO CJ CD NK MO NK NK CJ die wirtschaftlich am weitesten entwickelten , modernen und zum Teil katholisch gepr¨agten Regionen nom/acc.pl.fem nom/acc.pl.fem the economic - most developed , modern and to part catholic influenced regions ’the regions that are economically most developed, modern, and partly catholic’ Figure 1: Example of a German noun phrase. First and last word agree in number, gender, and case value. Figure 1 shows a German noun phrase taken from the German TiGer corpus (Brants et al., 2002). The two bold-faced words are the determiner and the head noun of the phrase, and they agree in their gender, number, and case values. The word Regionen (regions) is four-way ambiguous for its case value, which is reduced to a two-way ambiguity between nominative and accusative by the determiner. Further disambiguation would require information about the syntactic role of the noun phrase in a sentence. There are 11 tokens between these two words, which would require a context window of at least 13 to capture the agreement relation within a sequence model. Syntactically, however, as indicated </context>
</contexts>
<marker>Brants, Dipper, Hansen-Shirra, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen-Shirra, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank. In Proceedings of the 1st Workshop on Treebanks and Linguistic Theories, pages 24–41, Sozopol, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
</authors>
<title>Lexical-Functional Syntax.</title>
<date>2001</date>
<publisher>Blackwell Publishers.</publisher>
<contexts>
<context position="2145" citStr="Bresnan (2001)" startWordPosition="319" endWordPosition="320">rd form is ambiguous between several different morphological descriptions within its inflection paradigm. by morphological and syntactic means.2 Different languages, however, use them to a different extent. Languages like English encode grammatical information (like the subject vs object status of an argument) via word order, whereas languages like Czech or Hungarian use different word forms. Automatic analysis of languages with rich morphology needs to pay attention to the interaction between morphology and syntax in order to arrive at suitable computational models. Linguistic theory (e. g., Bresnan (2001), Melˇcuk (2009)) suggests many interactions between morphology and syntax. For example, languages with a case system use different forms of the same word to mark different syntactic (or semantic) relations (Blake, 2001). In many languages, two words that participate in a syntactic relation show covariance in some or all of their morphological features (so-called agreement, Corbett (2006)).3 Automatic annotation of morphology assigns morphological descriptions (e. g., nominativesingular-masculine) to word forms. It is usually modeled as a sequence model, often in combination with part-of-speec</context>
</contexts>
<marker>Bresnan, 2001</marker>
<rawString>Joan Bresnan. 2001. Lexical-Functional Syntax. Blackwell Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Isaac Chao</author>
<author>Llus Padr</author>
<author>Muntsa Padr</author>
</authors>
<title>Freeling: An open-source suite of language analyzers.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC’04),</booktitle>
<pages>239--242</pages>
<contexts>
<context position="20996" citStr="Carreras et al., 2004" startWordPosition="3304" endWordPosition="3307">morphological lexicons. Lexicons encode important knowledge that is difficult to pick up in a purely statistical system, e. g. the gender of nouns, which often cannot be deduced from the word form (Corbett, 1991).7 We extend our system from the previous experiment to include information from a morphological dictionaries. For Czech, we use the morphological analyzer distributed with the Prague Dependency Treebank 2 (Hajiˇc et al., 2006). For German, we use DMor (Schiller, 1994). For Hungarian, we use (Tr´on et al., 2006), and for Spanish, we use the morphological analyzer included in Freeling (Carreras et al., 2004). The output of the analyzers is given to the system as features that simply record the presence of a particular morphological analysis for the current word. The system can thus use the output of any tool regardless of its annotation scheme, especially if the annotation scheme of the treebank is different from the one of the morphological analyzer. Table 4 presents the results of experiments where we add the output of the morphological analyzers to our system. Again, we run experiments with and without syntactic features. For Czech, we also show results from featurama8 with the feature set dev</context>
</contexts>
<marker>Carreras, Chao, Padr, Padr, 2004</marker>
<rawString>Xavier Carreras, Isaac Chao, Llus Padr, and Muntsa Padr. 2004. Freeling: An open-source suite of language analyzers. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC’04), pages 239–242. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Chrupała</author>
<author>Georgiana Dinu</author>
<author>Josef van Genabith</author>
</authors>
<title>Learning morphology with morfette.</title>
<date>2008</date>
<journal>European Language Resources Association</journal>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08),</booktitle>
<pages>2362--2367</pages>
<location>Marrakech,</location>
<marker>Chrupała, Dinu, van Genabith, 2008</marker>
<rawString>Grzegorz Chrupała, Georgiana Dinu, and Josef van Genabith. 2008. Learning morphology with morfette. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08), pages 2362–2367, Marrakech, Morocco. European Language Resources Association (ELRA). http://www.lrec-conf.org/proceedings/lrec2008/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Joint morphological and syntactic disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>208--217</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="37264" citStr="Cohen and Smith, 2007" startWordPosition="5947" endWordPosition="5950">ce. Morphologically rich languages pose different challenges for automatic systems. In this paper, we work with European languages, where the problem of predicting morphology can be reduced to a tagging problem. In languages like Arabic, Hebrew, or Turkish, widespread ambiguity in segmentation of single words into meaningful morphemes adds an additional complexity. Given a good segmentation tool that takes care of this, our approach is applicable to these languages as well. For Hebrew, this problem has also been addressed by jointly modeling segmentation, morphological prediction, and syntax (Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2013). 6 Conclusion In this paper, we have demonstrated that using syntactic information for predicting morphological information is helpful if the language shows form syncretism in combination with morphosyntactic phenomena like agreement. A model that uses syntactic information is superior to a sequence model because it leverages the syntactic dependencies that may hold between morphologically dependent words as suggested by linguistic theory. We also showed that only small amounts of training data for a statistical parser would be needed </context>
</contexts>
<marker>Cohen, Smith, 2007</marker>
<rawString>Shay B. Cohen and Noah A. Smith. 2007. Joint morphological and syntactic disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 208–217, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="2787" citStr="Collins, 2002" startWordPosition="411" endWordPosition="412">ny interactions between morphology and syntax. For example, languages with a case system use different forms of the same word to mark different syntactic (or semantic) relations (Blake, 2001). In many languages, two words that participate in a syntactic relation show covariance in some or all of their morphological features (so-called agreement, Corbett (2006)).3 Automatic annotation of morphology assigns morphological descriptions (e. g., nominativesingular-masculine) to word forms. It is usually modeled as a sequence model, often in combination with part-of-speech tagging and lemmatization (Collins, 2002; Hajiˇc, 2004; Smith et al., 2005; Chrupała et al., 2008, and others). Sequence models achieve high accuracy and coverage but since they only use linear context they only approximate some of the underlying hierarchical relationships. As an example for these hierarchical relationships, 2And also by prosodic means, which we will not discuss since text-based tools rarely have access to this information. 3For example, in English, the subject of a sentence and the finite verb agree with respect to their number and person feature. 333 Proceedings of the 2013 Conference on Empirical Methods in Natur</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 1–8. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greville G Corbett</author>
</authors>
<title>Gender. Cambridge Textbooks in Linguistics.</title>
<date>1991</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="20586" citStr="Corbett, 1991" startWordPosition="3240" endWordPosition="3241"> in predicting morphological features makes use of morphological lexicons (e.g. Hajiˇc (2000), Hakkani-T¨ur et al. (2002), Hajiˇc (2004)). Lexicons define the possible morphological descriptions of a word and a statistical model selects the most probable one among them. In the following experiment, we test whether the contribution of syntactic features is similar or different to the contribution of morphological lexicons. Lexicons encode important knowledge that is difficult to pick up in a purely statistical system, e. g. the gender of nouns, which often cannot be deduced from the word form (Corbett, 1991).7 We extend our system from the previous experiment to include information from a morphological dictionaries. For Czech, we use the morphological analyzer distributed with the Prague Dependency Treebank 2 (Hajiˇc et al., 2006). For German, we use DMor (Schiller, 1994). For Hungarian, we use (Tr´on et al., 2006), and for Spanish, we use the morphological analyzer included in Freeling (Carreras et al., 2004). The output of the analyzers is given to the system as features that simply record the presence of a particular morphological analysis for the current word. The system can thus use the outp</context>
</contexts>
<marker>Corbett, 1991</marker>
<rawString>Greville G. Corbett. 1991. Gender. Cambridge Textbooks in Linguistics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greville G Corbett</author>
</authors>
<title>Agreement. Cambridge Textbooks in Linguistics.</title>
<date>2006</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2536" citStr="Corbett (2006)" startWordPosition="379" endWordPosition="380">rms. Automatic analysis of languages with rich morphology needs to pay attention to the interaction between morphology and syntax in order to arrive at suitable computational models. Linguistic theory (e. g., Bresnan (2001), Melˇcuk (2009)) suggests many interactions between morphology and syntax. For example, languages with a case system use different forms of the same word to mark different syntactic (or semantic) relations (Blake, 2001). In many languages, two words that participate in a syntactic relation show covariance in some or all of their morphological features (so-called agreement, Corbett (2006)).3 Automatic annotation of morphology assigns morphological descriptions (e. g., nominativesingular-masculine) to word forms. It is usually modeled as a sequence model, often in combination with part-of-speech tagging and lemmatization (Collins, 2002; Hajiˇc, 2004; Smith et al., 2005; Chrupała et al., 2008, and others). Sequence models achieve high accuracy and coverage but since they only use linear context they only approximate some of the underlying hierarchical relationships. As an example for these hierarchical relationships, 2And also by prosodic means, which we will not discuss since t</context>
<context position="23356" citStr="Corbett, 2006" startWordPosition="3675" endWordPosition="3676">elp in the prediction of morphology for Czech and German, but not for Hungarian and Spanish. To further investigate the difference between Czech and German on the one hand, and Hungarian and Spanish on the other, we take a closer look at the output of the tagger. We find an interesting difference between the two pairs of languages, namely the performance with respect to agreement. Agreement is a phenomenon where morphology and syntax strongly interact. Morphological features co-vary between two items in the sentence, but the relation between these items can occur at various linguistic levels (Corbett, 2006). If the syntactic information helps with predicting morphological information, we expect this to be particularly helpful with getting agreement right. All languages show agreement to some extent. Specifically, all languages show agreement in number (and person) between the subject and the verb of a clause. Czech, German, and Spanish show agreement in number, gender, and case (not Spanish) within a noun phrase. Hungarian shows case agreement within the noun phrase only rarely, e.g. for attributively used demonstrative pronouns. In order to test the effect on agreement, we measure the accuracy </context>
</contexts>
<marker>Corbett, 2006</marker>
<rawString>Greville G. Corbett. 2006. Agreement. Cambridge Textbooks in Linguistics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>A fundamental algorithm for dependency parsing (with corrections).</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual ACM Southeast Conference,</booktitle>
<publisher>ACM.</publisher>
<location>Athens, Gorgia.</location>
<contexts>
<context position="30534" citStr="Covington (2001)" startWordPosition="4869" endWordPosition="4870">information. For both languages, German and Czech, we find that already 1,000 sentences are enough training data for the parser to provide useful syntactic information to the morphological tagger. After 5,000 sentences, both curves flatten out and stay on the same level. We conclude that using syntactic features for morphological prediction is viable even if there is only small amounts of syntactic data available to train the parser. As a related experiment, we also test if we can get the same effect with a very simple and thus much faster parser. We use the brute-force algorithm described in Covington (2001), which selects for each token in the sentence another token as the head. It does not have any tree requirements, so it is not even guaranteed to yield a cycle-free tree structure. In Table 7, we compare the simple parser with the mateparser, both trained on the first 5,000 sentences of the treebank. Evaluation is done in terms of labeled (LAS) and unlabeled attachment score (UAS).10 dev set test set LAS UAS LAS UAS Czech 71.57 78.96 69.09 77.23 76.77 84.38 74.70 83.00 German 83.06 85.23 78.56 81.18 87.56 90.08 83.69 86.58 Table 7: Simple parser vs full parser – syntactic quality. Trained on f</context>
</contexts>
<marker>Covington, 2001</marker>
<rawString>Michael A. Covington. 2001. A fundamental algorithm for dependency parsing (with corrections). In Proceedings of the 39th Annual ACM Southeast Conference, Athens, Gorgia. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2003</date>
<booktitle>In Proceedings of the 16th Annual Conference on Neural Information Processing Systems,</booktitle>
<volume>7</volume>
<pages>1217--1224</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="9703" citStr="Crammer et al., 2003" startWordPosition="1475" endWordPosition="1478">). 2.2 System Description To test our hypotheses, we implemented a tagger that assigns full morphological descriptions to each token in a sentence. The system was inspired by the morphological tagger included in mate-tools.5 Like the tagger provided with mate-tools, it is a classifier that tags each token using the surrounding tokens in 5A collection of language independent, data-driven analysis tools for lemmatization, pos-tagging, morphological analysis, and dependency parsing: http://code.google.com/p/mate-tools its feature model. Models are trained using passiveaggressive online training (Crammer et al., 2003). The system makes two passes over each sentence: The first pass provides predicted tags that are used as features during the second pass. We also adopted the idea of a tag filter, which deterministically assigns tags for words that always occur with the same tag in the training data. For all matters of syntactic annotation in this paper, we use the graph-based dependency parser by Bohnet (2010), also included in mate-tools. All data sets are annotated with gold syntactic information, which is used to train the parsing models. For our experiments, we use a bootstrapping approach: the parser us</context>
</contexts>
<marker>Crammer, Dekel, Shalev-Shwartz, Singer, 2003</marker>
<rawString>Koby Crammer, Ofer Dekel, Shai Shalev-Shwartz, and Yoram Singer. 2003. Online passive-aggressive algorithms. In Proceedings of the 16th Annual Conference on Neural Information Processing Systems, volume 7, pages 1217–1224, Cambridge, Massachusetts, USA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich´ard Farkas</author>
<author>Veronika Vincze</author>
<author>Helmut Schmid</author>
</authors>
<title>Dependency parsing of hungarian: Baseline results and challenges.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>55--65</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France.</location>
<contexts>
<context position="9083" citStr="Farkas et al. (2012)" startWordPosition="1391" endWordPosition="1394">d genitive case (out of about 18 case suffixes). All languages show agreement between subject and verb, and within the noun phrase. The word order in Czech and Hungarian is very variable whereas it is more restrictive in Spanish and German. As our data, we use the CoNLL 2009 Shared Task data sets (Hajiˇc et al., 2009) for Czech and Spanish. For German, we use the dependency conversion of the TiGer treebank by Seeker and Kuhn (2012), splitting it into 40k/5k/5k sentences for training/development/test. For Hungarian, we use the Szeged Dependency Treebank (Vincze et al., 2010), with the split of Farkas et al. (2012). 2.2 System Description To test our hypotheses, we implemented a tagger that assigns full morphological descriptions to each token in a sentence. The system was inspired by the morphological tagger included in mate-tools.5 Like the tagger provided with mate-tools, it is a classifier that tags each token using the surrounding tokens in 5A collection of language independent, data-driven analysis tools for lemmatization, pos-tagging, morphological analysis, and dependency parsing: http://code.google.com/p/mate-tools its feature model. Models are trained using passiveaggressive online training (C</context>
</contexts>
<marker>Farkas, Vincze, Schmid, 2012</marker>
<rawString>Rich´ard Farkas, Veronika Vincze, and Helmut Schmid. 2012. Dependency parsing of hungarian: Baseline results and challenges. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 55–65, Avignon, France. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>Easy first dependency parsing of modern Hebrew.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages,</booktitle>
<pages>103--107</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California, USA.</location>
<contexts>
<context position="36211" citStr="Goldberg and Elhadad, 2010" startWordPosition="5786" endWordPosition="5789">other. However, their model is computationally quite demanding and its overall performance falls far behind the standard pipeline approach where both tasks are done in sequence. The problem of modeling the interaction between morphology and syntax has recently attracted some attention in the SPMRL workshops (Tsarfaty et al., no syntax simple syntax full syntax no syntax simple syntax full syntax baseline morph morph w/ syntax baseline morph morph w/ syntax 341 2010). Modeling morphosyntactic relations explicitly has been shown to improve statistical parsing models (Tsarfaty and Sima’an, 2010; Goldberg and Elhadad, 2010; Seeker and Kuhn, 2013), but the codependency between morphology and syntax makes it a difficult problem, and linguistic intuition is often contradicted by the empirical findings. For example, Marton et al. (2013) show that case information is the most helpful morphological feature for parsing Arabic, but only if it is given as gold information, whereas using case information from an automatic system may even harm the performance. Morphologically rich languages pose different challenges for automatic systems. In this paper, we work with European languages, where the problem of predicting morp</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2010. Easy first dependency parsing of modern Hebrew. In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 103–107, Los Angeles, California, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>Word segmentation, unknown-word resolution, and morphological agreement in a hebrew parsing system.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="37322" citStr="Goldberg and Elhadad, 2013" startWordPosition="5956" endWordPosition="5959">allenges for automatic systems. In this paper, we work with European languages, where the problem of predicting morphology can be reduced to a tagging problem. In languages like Arabic, Hebrew, or Turkish, widespread ambiguity in segmentation of single words into meaningful morphemes adds an additional complexity. Given a good segmentation tool that takes care of this, our approach is applicable to these languages as well. For Hebrew, this problem has also been addressed by jointly modeling segmentation, morphological prediction, and syntax (Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2013). 6 Conclusion In this paper, we have demonstrated that using syntactic information for predicting morphological information is helpful if the language shows form syncretism in combination with morphosyntactic phenomena like agreement. A model that uses syntactic information is superior to a sequence model because it leverages the syntactic dependencies that may hold between morphologically dependent words as suggested by linguistic theory. We also showed that only small amounts of training data for a statistical parser would be needed to improve the morphological tagger. Making use of the imp</context>
</contexts>
<marker>Goldberg, Elhadad, 2013</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2013. Word segmentation, unknown-word resolution, and morphological agreement in a hebrew parsing system. Computational Linguistics, 39(1):121–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Reut Tsarfaty</author>
</authors>
<title>A single generative model for joint morphological segmentation and syntactic parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>371--379</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<contexts>
<context position="37293" citStr="Goldberg and Tsarfaty, 2008" startWordPosition="5951" endWordPosition="5955">h languages pose different challenges for automatic systems. In this paper, we work with European languages, where the problem of predicting morphology can be reduced to a tagging problem. In languages like Arabic, Hebrew, or Turkish, widespread ambiguity in segmentation of single words into meaningful morphemes adds an additional complexity. Given a good segmentation tool that takes care of this, our approach is applicable to these languages as well. For Hebrew, this problem has also been addressed by jointly modeling segmentation, morphological prediction, and syntax (Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2013). 6 Conclusion In this paper, we have demonstrated that using syntactic information for predicting morphological information is helpful if the language shows form syncretism in combination with morphosyntactic phenomena like agreement. A model that uses syntactic information is superior to a sequence model because it leverages the syntactic dependencies that may hold between morphologically dependent words as suggested by linguistic theory. We also showed that only small amounts of training data for a statistical parser would be needed to improve the morphological </context>
</contexts>
<marker>Goldberg, Tsarfaty, 2008</marker>
<rawString>Yoav Goldberg and Reut Tsarfaty. 2008. A single generative model for joint morphological segmentation and syntactic parsing. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, pages 371–379, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Morphological Tagging: Data vs. Dictionaries.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th ANLP Conference / 1st NAACL Meeting,</booktitle>
<pages>94--101</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington.</location>
<marker>Hajiˇc, 2000</marker>
<rawString>Jan Hajiˇc. 2000. Morphological Tagging: Data vs. Dictionaries. In Proceedings of the 6th ANLP Conference / 1st NAACL Meeting, pages 94—101, Seattle, Washington. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Disambiguation of Rich Inflection (Computational Morphology of Czech). Nakladatelstv´ı Karolinum,</title>
<date>2004</date>
<location>Prague, Czech Republic.</location>
<marker>Hajiˇc, 2004</marker>
<rawString>Jan Hajiˇc. 2004. Disambiguation of Rich Inflection (Computational Morphology of Czech). Nakladatelstv´ı Karolinum, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Jarmila Panevov´a</author>
<author>Eva Hajiˇcov´a</author>
<author>Petr Sgall</author>
<author>Petr Pajas</author>
<author>Jan ˇStˇep´anek</author>
<author>Ji´ı Havelka</author>
<author>Marie Mikulov´a</author>
</authors>
<date>2006</date>
<journal>Prague Dependency Treebank</journal>
<volume>2</volume>
<marker>Hajiˇc, Panevov´a, Hajiˇcov´a, Sgall, Pajas, ˇStˇep´anek, Havelka, Mikulov´a, 2006</marker>
<rawString>Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr Sgall, Petr Pajas, Jan ˇStˇep´anek, Ji´ı Havelka, and Marie Mikulov´a. 2006. Prague Dependency Treebank 2.0.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Mart´ı</author>
<author>Llu´ıs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan Step´anek</author>
<author>Pavel Stran´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL2009 shared task: Syntactic and Semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado, USA.</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Mart´ı, M`arquez, Meyers, Nivre, Pad´o, Step´anek, Stran´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan Step´anek, Pavel Stran´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 shared task: Syntactic and Semantic dependencies in multiple languages. In Proceedings of the 13th Conference on Computational Natural Language Learning: Shared Task, pages 1–18, Boulder, Colorado, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dilek Z Hakkani-T¨ur</author>
<author>Kemal Oflazer</author>
<author>G¨okhan T¨ur</author>
</authors>
<title>Statistical morphological disambiguation for agglutinative languages. Computers and the Humanities,</title>
<date>2002</date>
<pages>36--4</pages>
<marker>Hakkani-T¨ur, Oflazer, T¨ur, 2002</marker>
<rawString>Dilek Z. Hakkani-T¨ur, Kemal Oflazer, and G¨okhan T¨ur. 2002. Statistical morphological disambiguation for agglutinative languages. Computers and the Humanities, 36(4):381–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Hohensee</author>
<author>Emily M Bender</author>
</authors>
<title>Getting more from morphology in multilingual dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>315--326</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="34717" citStr="Hohensee and Bender (2012)" startWordPosition="5555" endWordPosition="5558"> 90.24 91.20 92.97 88.15 90.34 Table 9: Impact of the improved morphology on the quality of the dependency parser for Czech and German. tween the two parsing models with respect to grammatical functions that are morphologically marked. For example, in German, performance on subjects and accusative objects improves while performance for dative objects and genitives decreases. This suggests different strengths in the two parsing models. However, the question how to make use of the improved morphology in parsing clearly needs more research in the future. A promising avenue may be the approach by Hohensee and Bender (2012). 5 Related Work Morphological taggers have been developed for many languages. The most common approach is the combination of a morphological lexicon with a statistical disambiguation model (Hakkani-T¨ur et al., 2002; Hajiˇc, 2004; Smith et al., 2005; Spoustov´a et al., 2009; Zsibrita et al., 2013). Our work has been inspired by Versley et al. (2010), who annotate a treebank with morphological information after the syntax had been annotated already. The system used a finite-state morphology to propose a set of candidate tags for each word, which is then further restricted using hand-crafted ru</context>
</contexts>
<marker>Hohensee, Bender, 2012</marker>
<rawString>Matt Hohensee and Emily M. Bender. 2012. Getting more from morphology in multilingual dependency parsing. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 315–326, Montr´eal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
<author>Jason Naradowsky</author>
<author>David A Smith</author>
</authors>
<title>A discriminative model for joint morphological disambiguation and dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>885--894</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, USA.</location>
<contexts>
<context position="35378" citStr="Lee et al. (2011)" startWordPosition="5660" endWordPosition="5663">been developed for many languages. The most common approach is the combination of a morphological lexicon with a statistical disambiguation model (Hakkani-T¨ur et al., 2002; Hajiˇc, 2004; Smith et al., 2005; Spoustov´a et al., 2009; Zsibrita et al., 2013). Our work has been inspired by Versley et al. (2010), who annotate a treebank with morphological information after the syntax had been annotated already. The system used a finite-state morphology to propose a set of candidate tags for each word, which is then further restricted using hand-crafted rules over the already available syntax tree. Lee et al. (2011) pursue the idea of jointly predicting syntax and morphology, out of the motivation that joint models should model the problem more faithfully. They demonstrate that both sides can use information from each other. However, their model is computationally quite demanding and its overall performance falls far behind the standard pipeline approach where both tasks are done in sequence. The problem of modeling the interaction between morphology and syntax has recently attracted some attention in the SPMRL workshops (Tsarfaty et al., no syntax simple syntax full syntax no syntax simple syntax full s</context>
</contexts>
<marker>Lee, Naradowsky, Smith, 2011</marker>
<rawString>John Lee, Jason Naradowsky, and David A. Smith. 2011. A discriminative model for joint morphological disambiguation and dependency parsing. In Proceedings of the 49th annual meeting of the Association for Computational Linguistics, pages 885–894, Portland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Dependency parsing of modern standard arabic with lexical and inflectional features.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="36425" citStr="Marton et al. (2013)" startWordPosition="5819" endWordPosition="5822">tween morphology and syntax has recently attracted some attention in the SPMRL workshops (Tsarfaty et al., no syntax simple syntax full syntax no syntax simple syntax full syntax baseline morph morph w/ syntax baseline morph morph w/ syntax 341 2010). Modeling morphosyntactic relations explicitly has been shown to improve statistical parsing models (Tsarfaty and Sima’an, 2010; Goldberg and Elhadad, 2010; Seeker and Kuhn, 2013), but the codependency between morphology and syntax makes it a difficult problem, and linguistic intuition is often contradicted by the empirical findings. For example, Marton et al. (2013) show that case information is the most helpful morphological feature for parsing Arabic, but only if it is given as gold information, whereas using case information from an automatic system may even harm the performance. Morphologically rich languages pose different challenges for automatic systems. In this paper, we work with European languages, where the problem of predicting morphology can be reduced to a tagging problem. In languages like Arabic, Hebrew, or Turkish, widespread ambiguity in segmentation of single words into meaningful morphemes adds an additional complexity. Given a good s</context>
</contexts>
<marker>Marton, Habash, Rambow, 2013</marker>
<rawString>Yuval Marton, Nizar Habash, and Owen Rambow. 2013. Dependency parsing of modern standard arabic with lexical and inflectional features. Computational Linguistics, 39(1):161–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Melˇcuk</author>
</authors>
<date>2009</date>
<note>Dependency in linguistic description.</note>
<marker>Melˇcuk, 2009</marker>
<rawString>Igor Melˇcuk. 2009. Dependency in linguistic description.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robbert Prins</author>
</authors>
<title>Beyond N in N-gram tagging.</title>
<date>2004</date>
<booktitle>Proceedings of the ACL 2004 Student Research Workshop,</booktitle>
<pages>61--66</pages>
<editor>In Leonoor Van Der Beek, Dmitriy Genzel, and Daniel Midgley, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain.</location>
<contexts>
<context position="13707" citStr="Prins (2004)" startWordPosition="2114" endWordPosition="2115">e conjunctions. number marks if the form contains a digit. After preprocessing the data, our baseline system is trained using the feature set shown in Table 1. The baseline system does not make use of any syntactic information but predicts morphological information based solely on tokens and their linear context. The features are divided into static features, which can be computed on the input, and dynamic features, which are computed also on previous output of the system (cf. two passes in Section 2.2). 6Lemma and part-of-speech prediction may also profit from syntactic information, see e.g. Prins (2004) or Bohnet and Nivre (2012). The feature sets in Table 1 were developed specifically for our experiments and are the result of an automatic forward/backward feature selection process. The purpose of the feature selection was to arrive at a baseline system that performs well without any syntactic information. With such an optimized baseline system, we can measure the contribution of syntactic features more reliably. The last-verb/next-verb and pos+case features are variants of the features proposed in Votrubec (2006). They extract information about the first verb within the last 10/the next 30 </context>
</contexts>
<marker>Prins, 2004</marker>
<rawString>Robbert Prins. 2004. Beyond N in N-gram tagging. In Leonoor Van Der Beek, Dmitriy Genzel, and Daniel Midgley, editors, Proceedings of the ACL 2004 Student Research Workshop, pages 61–66, Barcelona, Spain. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Schiller</author>
</authors>
<title>Dmor - user’s guide.</title>
<date>1994</date>
<tech>Technical report,</tech>
<institution>University of Stuttgart.</institution>
<contexts>
<context position="20855" citStr="Schiller, 1994" startWordPosition="3282" endWordPosition="3283">m. In the following experiment, we test whether the contribution of syntactic features is similar or different to the contribution of morphological lexicons. Lexicons encode important knowledge that is difficult to pick up in a purely statistical system, e. g. the gender of nouns, which often cannot be deduced from the word form (Corbett, 1991).7 We extend our system from the previous experiment to include information from a morphological dictionaries. For Czech, we use the morphological analyzer distributed with the Prague Dependency Treebank 2 (Hajiˇc et al., 2006). For German, we use DMor (Schiller, 1994). For Hungarian, we use (Tr´on et al., 2006), and for Spanish, we use the morphological analyzer included in Freeling (Carreras et al., 2004). The output of the analyzers is given to the system as features that simply record the presence of a particular morphological analysis for the current word. The system can thus use the output of any tool regardless of its annotation scheme, especially if the annotation scheme of the treebank is different from the one of the morphological analyzer. Table 4 presents the results of experiments where we add the output of the morphological analyzers to our sy</context>
</contexts>
<marker>Schiller, 1994</marker>
<rawString>Anne Schiller. 1994. Dmor - user’s guide. Technical report, University of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
<author>Florian Laws</author>
</authors>
<title>Estimation of conditional probabilities with decision trees and an application to fine-grained POS tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>777--784</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="21687" citStr="Schmid and Laws, 2008" startWordPosition="3419" endWordPosition="3422"> simply record the presence of a particular morphological analysis for the current word. The system can thus use the output of any tool regardless of its annotation scheme, especially if the annotation scheme of the treebank is different from the one of the morphological analyzer. Table 4 presents the results of experiments where we add the output of the morphological analyzers to our system. Again, we run experiments with and without syntactic features. For Czech, we also show results from featurama8 with the feature set developed by Votrubec (2006). For German, we show results for RFTagger (Schmid and Laws, 2008). As expected, the information from the morphological lexicon improves the overall performance 7Lexicons are also often used to speed up processing considerably by restricting the search space of the statistical model. 8http://sourceforge.net/projects/featurama/ considerably compared to the results in Table 3, especially on unknown tokens. This shows that even with the considerable amounts of training data available nowadays, rule-based morphological analyzers are important resources for morphological description (cf. Hajiˇc (2000)). The contribution of syntactic features in German and Czech i</context>
</contexts>
<marker>Schmid, Laws, 2008</marker>
<rawString>Helmut Schmid and Florian Laws. 2008. Estimation of conditional probabilities with decision trees and an application to fine-grained POS tagging. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 777–784, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Seeker</author>
<author>Jonas Kuhn</author>
</authors>
<title>Making Ellipses Explicit in Dependency Conversion for a German Treebank.</title>
<date>2012</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation,</booktitle>
<pages>3132--3139</pages>
<location>Istanbul,</location>
<contexts>
<context position="8898" citStr="Seeker and Kuhn (2012)" startWordPosition="1364" endWordPosition="1367">shows syncretism in the verbal inflection paradigms. In Hungarian, form syncretisms are much less frequent. The case paradigm of Hungarian only shows one form syncretism between dative and genitive case (out of about 18 case suffixes). All languages show agreement between subject and verb, and within the noun phrase. The word order in Czech and Hungarian is very variable whereas it is more restrictive in Spanish and German. As our data, we use the CoNLL 2009 Shared Task data sets (Hajiˇc et al., 2009) for Czech and Spanish. For German, we use the dependency conversion of the TiGer treebank by Seeker and Kuhn (2012), splitting it into 40k/5k/5k sentences for training/development/test. For Hungarian, we use the Szeged Dependency Treebank (Vincze et al., 2010), with the split of Farkas et al. (2012). 2.2 System Description To test our hypotheses, we implemented a tagger that assigns full morphological descriptions to each token in a sentence. The system was inspired by the morphological tagger included in mate-tools.5 Like the tagger provided with mate-tools, it is a classifier that tags each token using the surrounding tokens in 5A collection of language independent, data-driven analysis tools for lemmati</context>
</contexts>
<marker>Seeker, Kuhn, 2012</marker>
<rawString>Wolfgang Seeker and Jonas Kuhn. 2012. Making Ellipses Explicit in Dependency Conversion for a German Treebank. In Proceedings of the 8th International Conference on Language Resources and Evaluation, pages 3132–3139, Istanbul, Turkey. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Seeker</author>
<author>Jonas Kuhn</author>
</authors>
<title>Morphological and syntactic case in statistical dependency parsing.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="36235" citStr="Seeker and Kuhn, 2013" startWordPosition="5790" endWordPosition="5793">is computationally quite demanding and its overall performance falls far behind the standard pipeline approach where both tasks are done in sequence. The problem of modeling the interaction between morphology and syntax has recently attracted some attention in the SPMRL workshops (Tsarfaty et al., no syntax simple syntax full syntax no syntax simple syntax full syntax baseline morph morph w/ syntax baseline morph morph w/ syntax 341 2010). Modeling morphosyntactic relations explicitly has been shown to improve statistical parsing models (Tsarfaty and Sima’an, 2010; Goldberg and Elhadad, 2010; Seeker and Kuhn, 2013), but the codependency between morphology and syntax makes it a difficult problem, and linguistic intuition is often contradicted by the empirical findings. For example, Marton et al. (2013) show that case information is the most helpful morphological feature for parsing Arabic, but only if it is given as gold information, whereas using case information from an automatic system may even harm the performance. Morphologically rich languages pose different challenges for automatic systems. In this paper, we work with European languages, where the problem of predicting morphology can be reduced to</context>
</contexts>
<marker>Seeker, Kuhn, 2013</marker>
<rawString>Wolfgang Seeker and Jonas Kuhn. 2013. Morphological and syntactic case in statistical dependency parsing. Computational Linguistics, 39(1):23–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>David A Smith</author>
<author>Roy W Tromble</author>
</authors>
<title>Context-based morphological disambiguation with random fields.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>475--482</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="2821" citStr="Smith et al., 2005" startWordPosition="415" endWordPosition="418">ology and syntax. For example, languages with a case system use different forms of the same word to mark different syntactic (or semantic) relations (Blake, 2001). In many languages, two words that participate in a syntactic relation show covariance in some or all of their morphological features (so-called agreement, Corbett (2006)).3 Automatic annotation of morphology assigns morphological descriptions (e. g., nominativesingular-masculine) to word forms. It is usually modeled as a sequence model, often in combination with part-of-speech tagging and lemmatization (Collins, 2002; Hajiˇc, 2004; Smith et al., 2005; Chrupała et al., 2008, and others). Sequence models achieve high accuracy and coverage but since they only use linear context they only approximate some of the underlying hierarchical relationships. As an example for these hierarchical relationships, 2And also by prosodic means, which we will not discuss since text-based tools rarely have access to this information. 3For example, in English, the subject of a sentence and the finite verb agree with respect to their number and person feature. 333 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 333–</context>
<context position="34967" citStr="Smith et al., 2005" startWordPosition="5593" endWordPosition="5596">, performance on subjects and accusative objects improves while performance for dative objects and genitives decreases. This suggests different strengths in the two parsing models. However, the question how to make use of the improved morphology in parsing clearly needs more research in the future. A promising avenue may be the approach by Hohensee and Bender (2012). 5 Related Work Morphological taggers have been developed for many languages. The most common approach is the combination of a morphological lexicon with a statistical disambiguation model (Hakkani-T¨ur et al., 2002; Hajiˇc, 2004; Smith et al., 2005; Spoustov´a et al., 2009; Zsibrita et al., 2013). Our work has been inspired by Versley et al. (2010), who annotate a treebank with morphological information after the syntax had been annotated already. The system used a finite-state morphology to propose a set of candidate tags for each word, which is then further restricted using hand-crafted rules over the already available syntax tree. Lee et al. (2011) pursue the idea of jointly predicting syntax and morphology, out of the motivation that joint models should model the problem more faithfully. They demonstrate that both sides can use info</context>
</contexts>
<marker>Smith, Smith, Tromble, 2005</marker>
<rawString>Noah A. Smith, David A. Smith, and Roy W. Tromble. 2005. Context-based morphological disambiguation with random fields. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 475–482, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drahom´ıra ”Johanka” Spoustov´a</author>
<author>Jan Hajiˇc</author>
<author>Jan Raab</author>
<author>Miroslav Spousta</author>
</authors>
<title>Semi-supervised training for the averaged perceptron POS tagger.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>763--771</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece.</location>
<marker>Spoustov´a, Hajiˇc, Raab, Spousta, 2009</marker>
<rawString>Drahom´ıra ”Johanka” Spoustov´a, Jan Hajiˇc, Jan Raab, and Miroslav Spousta. 2009. Semi-supervised training for the averaged perceptron POS tagger. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 763–771, Athens, Greece. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viktor Tr´on</author>
<author>P´eter Hal´acsy</author>
<author>P´eter Rebrus</author>
<author>Andr´as Rung</author>
<author>P´eter Vajda</author>
<author>Eszter Simon</author>
</authors>
<title>Morphdb.hu: Hungarian lexical database and morphological grammar.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation,</booktitle>
<pages>1670--1673</pages>
<location>Genoa, Italy.</location>
<marker>Tr´on, Hal´acsy, Rebrus, Rung, Vajda, Simon, 2006</marker>
<rawString>Viktor Tr´on, P´eter Hal´acsy, P´eter Rebrus, Andr´as Rung, P´eter Vajda, and Eszter Simon. 2006. Morphdb.hu: Hungarian lexical database and morphological grammar. In Proceedings of the 5th International Conference on Language Resources and Evaluation, pages 1670–1673, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Khalil Sima’an</author>
</authors>
<title>Modeling morphosyntactic agreement in constituency-based parsing of Modern Hebrew.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages,</booktitle>
<pages>40--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California, USA.</location>
<marker>Tsarfaty, Sima’an, 2010</marker>
<rawString>Reut Tsarfaty and Khalil Sima’an. 2010. Modeling morphosyntactic agreement in constituency-based parsing of Modern Hebrew. In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 40–48, Los Angeles, California, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Reut Tsarfaty</author>
<author>Djam´e Seddah</author>
<author>Yoav Goldberg</author>
<author>Sandra K¨ubler</author>
<author>Marie Candito</author>
<author>Jennifer Foster</author>
<author>Yannick Versley</author>
<author>Ines Rehbein</author>
<author>Lamia Tounsi</author>
</authors>
<title>Statistical parsing of morphologically rich languages (SPMRL): what, how and whither.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages,</booktitle>
<pages>1--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California, USA.</location>
<marker>Tsarfaty, Seddah, Goldberg, K¨ubler, Candito, Foster, Versley, Rehbein, Tounsi, 2010</marker>
<rawString>Reut Tsarfaty, Djam´e Seddah, Yoav Goldberg, Sandra K¨ubler, Marie Candito, Jennifer Foster, Yannick Versley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical parsing of morphologically rich languages (SPMRL): what, how and whither. In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 1–12, Los Angeles, California, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannick Versley</author>
<author>Kathrin Beck</author>
<author>Erhard Hinrichs</author>
<author>Heike Telljohann</author>
</authors>
<title>A syntax-first approach to high-quality morphological analysis and lemma disambiguation for the tba-d/z treebank.</title>
<date>2010</date>
<booktitle>In 9th Conference on Treebanks and Linguistic Theories (TLT9),</booktitle>
<pages>233--244</pages>
<contexts>
<context position="35069" citStr="Versley et al. (2010)" startWordPosition="5611" endWordPosition="5614">genitives decreases. This suggests different strengths in the two parsing models. However, the question how to make use of the improved morphology in parsing clearly needs more research in the future. A promising avenue may be the approach by Hohensee and Bender (2012). 5 Related Work Morphological taggers have been developed for many languages. The most common approach is the combination of a morphological lexicon with a statistical disambiguation model (Hakkani-T¨ur et al., 2002; Hajiˇc, 2004; Smith et al., 2005; Spoustov´a et al., 2009; Zsibrita et al., 2013). Our work has been inspired by Versley et al. (2010), who annotate a treebank with morphological information after the syntax had been annotated already. The system used a finite-state morphology to propose a set of candidate tags for each word, which is then further restricted using hand-crafted rules over the already available syntax tree. Lee et al. (2011) pursue the idea of jointly predicting syntax and morphology, out of the motivation that joint models should model the problem more faithfully. They demonstrate that both sides can use information from each other. However, their model is computationally quite demanding and its overall perfo</context>
</contexts>
<marker>Versley, Beck, Hinrichs, Telljohann, 2010</marker>
<rawString>Yannick Versley, Kathrin Beck, Erhard Hinrichs, and Heike Telljohann. 2010. A syntax-first approach to high-quality morphological analysis and lemma disambiguation for the tba-d/z treebank. In 9th Conference on Treebanks and Linguistic Theories (TLT9), pages 233–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
<author>D´ora Szauter</author>
<author>Attila Alm´asi</author>
<author>Gy¨orgy M´ora</author>
<author>Zolt´an Alexin</author>
<author>J´anos Csirik</author>
</authors>
<title>Hungarian Dependency Treebank.</title>
<date>2010</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the 7th Conference on International Language Resources and Evaluation,</booktitle>
<pages>1855--1862</pages>
<location>Valletta,</location>
<marker>Vincze, Szauter, Alm´asi, M´ora, Alexin, Csirik, 2010</marker>
<rawString>Veronika Vincze, D´ora Szauter, Attila Alm´asi, Gy¨orgy M´ora, Zolt´an Alexin, and J´anos Csirik. 2010. Hungarian Dependency Treebank. In Proceedings of the 7th Conference on International Language Resources and Evaluation, pages 1855–1862, Valletta, Malta. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Votrubec</author>
</authors>
<title>Morphological tagging based on averaged perceptron.</title>
<date>2006</date>
<booktitle>In WDS’06 Proceedings of Contributed Papers,</booktitle>
<pages>191--195</pages>
<location>Praha, Czechia. Matfyzpress, Charles University.</location>
<contexts>
<context position="14228" citStr="Votrubec (2006)" startWordPosition="2195" endWordPosition="2196">nd part-of-speech prediction may also profit from syntactic information, see e.g. Prins (2004) or Bohnet and Nivre (2012). The feature sets in Table 1 were developed specifically for our experiments and are the result of an automatic forward/backward feature selection process. The purpose of the feature selection was to arrive at a baseline system that performs well without any syntactic information. With such an optimized baseline system, we can measure the contribution of syntactic features more reliably. The last-verb/next-verb and pos+case features are variants of the features proposed in Votrubec (2006). They extract information about the first verb within the last 10/the next 30 tokens in the sentence. The case feature extracts the case value from previously assigned morphological tags. Note that the verb features are approximating syntactic information by making the assumption that the closest verbs are likely to be syntactic heads for many words. static features h lemma h s2 h s3 pos+h pos s1+h s1 h dir h dir+h pos ld s1 ld s2 ld p1 ld p4 dynamic features h tag ld tag Table 2: Syntactic features. h and ld mark features from the head and the left-most daughter, dir is a binary feature mark</context>
<context position="21621" citStr="Votrubec (2006)" startWordPosition="3409" endWordPosition="3410">ut of the analyzers is given to the system as features that simply record the presence of a particular morphological analysis for the current word. The system can thus use the output of any tool regardless of its annotation scheme, especially if the annotation scheme of the treebank is different from the one of the morphological analyzer. Table 4 presents the results of experiments where we add the output of the morphological analyzers to our system. Again, we run experiments with and without syntactic features. For Czech, we also show results from featurama8 with the feature set developed by Votrubec (2006). For German, we show results for RFTagger (Schmid and Laws, 2008). As expected, the information from the morphological lexicon improves the overall performance 7Lexicons are also often used to speed up processing considerably by restricting the search space of the statistical model. 8http://sourceforge.net/projects/featurama/ considerably compared to the results in Table 3, especially on unknown tokens. This shows that even with the considerable amounts of training data available nowadays, rule-based morphological analyzers are important resources for morphological description (cf. Hajiˇc (20</context>
</contexts>
<marker>Votrubec, 2006</marker>
<rawString>Jan Votrubec. 2006. Morphological tagging based on averaged perceptron. In WDS’06 Proceedings of Contributed Papers, pages 191–195, Praha, Czechia. Matfyzpress, Charles University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J´anos Zsibrita</author>
<author>Veronika Vincze</author>
<author>Rich´ard Farkas</author>
</authors>
<title>magyarlanc 2.0: szintaktikai elemz´es ´es felgyors´ıtott sz´ofaji egy´ertelms´ıt´es.</title>
<date>2013</date>
<booktitle>In Attila Tan´acs and Veronika Vincze, editors, IX. Magyar Sz´amit´og´epes Nyelv´eszeti Konferencia,</booktitle>
<pages>368--374</pages>
<location>Szeged, Hungary.</location>
<contexts>
<context position="35016" citStr="Zsibrita et al., 2013" startWordPosition="5601" endWordPosition="5604">cts improves while performance for dative objects and genitives decreases. This suggests different strengths in the two parsing models. However, the question how to make use of the improved morphology in parsing clearly needs more research in the future. A promising avenue may be the approach by Hohensee and Bender (2012). 5 Related Work Morphological taggers have been developed for many languages. The most common approach is the combination of a morphological lexicon with a statistical disambiguation model (Hakkani-T¨ur et al., 2002; Hajiˇc, 2004; Smith et al., 2005; Spoustov´a et al., 2009; Zsibrita et al., 2013). Our work has been inspired by Versley et al. (2010), who annotate a treebank with morphological information after the syntax had been annotated already. The system used a finite-state morphology to propose a set of candidate tags for each word, which is then further restricted using hand-crafted rules over the already available syntax tree. Lee et al. (2011) pursue the idea of jointly predicting syntax and morphology, out of the motivation that joint models should model the problem more faithfully. They demonstrate that both sides can use information from each other. However, their model is </context>
</contexts>
<marker>Zsibrita, Vincze, Farkas, 2013</marker>
<rawString>J´anos Zsibrita, Veronika Vincze, and Rich´ard Farkas. 2013. magyarlanc 2.0: szintaktikai elemz´es ´es felgyors´ıtott sz´ofaji egy´ertelms´ıt´es. In Attila Tan´acs and Veronika Vincze, editors, IX. Magyar Sz´amit´og´epes Nyelv´eszeti Konferencia, pages 368–374, Szeged, Hungary.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
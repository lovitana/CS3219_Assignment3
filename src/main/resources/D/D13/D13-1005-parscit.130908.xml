<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9989745">
A Joint Learning Model of Word Segmentation, Lexical Acquisition,
and Phonetic Variability
</title>
<author confidence="0.786011">
Micha Elsner
</author>
<email confidence="0.615535">
melsner0@gmail.com
</email>
<affiliation confidence="0.8609965">
Dept. of Linguistics
The Ohio State University
</affiliation>
<address confidence="0.429725">
Naomi H. Feldman
</address>
<email confidence="0.946529">
nhf@umd.edu
</email>
<affiliation confidence="0.9886915">
Dept. of Linguistics
University of Maryland
</affiliation>
<sectionHeader confidence="0.994821" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999470052631579">
We present a cognitive model of early lexi-
cal acquisition which jointly performs word
segmentation and learns an explicit model of
phonetic variation. We define the model as a
Bayesian noisy channel; we sample segmen-
tations and word forms simultaneously from
the posterior, using beam sampling to control
the size of the search space. Compared to a
pipelined approach in which segmentation is
performed first, our model is qualitatively more
similar to human learners. On data with vari-
able pronunciations, the pipelined approach
learns to treat syllables or morphemes as words.
In contrast, our joint model, like infant learners,
tends to learn multiword collocations. We also
conduct analyses of the phonetic variations that
the model learns to accept and its patterns of
word recognition errors, and relate these to de-
velopmental evidence.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997965">
By the end of their first year, infants have acquired
many of the basic elements of their native language.
Their sensitivity to phonetic contrasts has become
language-specific (Werker and Tees, 1984), and they
have begun detecting words in fluent speech (Jusczyk
and Aslin, 1995; Jusczyk et al., 1999) and learn-
ing word meanings (Bergelson and Swingley, 2012).
These developmental cooccurrences lead some re-
searchers to propose that phonetic and word learning
occur jointly, each one informing the other (Swingley,
2009; Feldman et al., 2013). Previous computational
models capture some aspects of this joint learning
</bodyText>
<sectionHeader confidence="0.225883" genericHeader="introduction">
Sharon Goldwater
</sectionHeader>
<email confidence="0.756323">
sgwater@inf.ed.ac.uk
</email>
<affiliation confidence="0.757114">
ILCC, School of Informatics
University of Edinburgh
</affiliation>
<author confidence="0.679698">
Frank Wood
</author>
<email confidence="0.88647">
fwood@robots.ox.ac.uk
</email>
<affiliation confidence="0.9802235">
Dept. of Engineering
University of Oxford
</affiliation>
<bodyText confidence="0.999901727272728">
problem, but typically simplify the problem consid-
erably, either by assuming an unrealistic degree of
phonetic regularity for word segmentation (Goldwa-
ter et al., 2009) or assuming pre-segmented input
for phonetic and lexical acquisition (Feldman et al.,
2009; Feldman et al., in press; Elsner et al., 2012).
This paper presents, to our knowledge, the first broad-
coverage model that learns to segment phonetically
variable input into words, while simultaneously learn-
ing an explicit model of phonetic variation that allows
it to cluster together segmented tokens with different
phonetic realizations (e.g., [ju] and [jI]) into lexical
items (/ju/).
We base our model on the Bayesian word segmen-
tation model of Goldwater et al. (2009) (henceforth
GGJ), using a noisy-channel setup where phonetic
variation is introduced by a finite-state transducer
(Neubig et al., 2010; Elsner et al., 2012). This in-
tegrated model allows us to examine how solving
the word segmentation problem should affect infants’
strategies for learning about phonetic variability and
how phonetic learning can allow word segmentation
to proceed in ways that mimic the idealized input
used in previous models.
In particular, although the GGJ model achieves
high segmentation accuracy on phonemic (non-
variable) input and makes errors that are qualitatively
similar to human learners (tending to undersegment
the input), its accuracy drops considerably on phonet-
ically noisy data and it tends to oversegment rather
than undersegment. Here, we demonstrate that when
the model is augmented to account for phonetic vari-
ability, it is able to learn common phonetic changes
</bodyText>
<page confidence="0.990123">
42
</page>
<note confidence="0.733712">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 42–54,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999539307692308">
and by doing so, its accuracy improves and its errors
return to the more human-like undersegmentation
pattern. In addition, we find small improvements
in lexicon accuracy over a pipeline model that seg-
ments first and then performs lexical-phonetic learn-
ing (Elsner et al., 2012). We analyze the model’s
phonetic and lexical representations in detail, draw-
ing comparisons to experimental results on adult and
infant speech processing. Taken together, our results
support the idea that a Bayesian model that jointly
performs word segmentation and phonetic learning
provides a plausible explanation for many aspects of
early phonetic and word learning in infants.
</bodyText>
<sectionHeader confidence="0.999943" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999844677419355">
Nearly all computational models used to explore the
problems addressed here have treated the learning
tasks in isolation. Examples include models of word
segmentation from phonemic input (Christiansen et
al., 1998; Brent, 1999; Venkataraman, 2001; Swing-
ley, 2005) or phonetic input (Fleck, 2008; Rytting,
2007; Daland and Pierrehumbert, 2011; Boruta et
al., 2011), models of phonetic clustering (Vallabha
et al., 2007; Varadarajan et al., 2008; Dupoux et al.,
2011) and phonological rule learning (Peperkamp et
al., 2006; Martin et al., 2013).
Elsner et al. (2012) present a model that is similar
to ours, using a noisy channel model implemented
with a finite-state transducer to learn about phonetic
variability while clustering distinct tokens into lexi-
cal items. However (like the earlier lexical-phonetic
learning model of Feldman et al. (2009; in press))
their model assumes known word boundaries, so
to perform both segmentation and lexical-phonetic
learning, they use a pipeline that first segments using
GGJ and then applies their model to the results.
Neubig et al. (2010) also present a transducer-
based noisy channel model that performs joint in-
ference on two out of the three tasks we consider
here; their model assumes fixed probabilities for pho-
netic changes (the noise model) and jointly infers
the word segmentation and lexical items, as in our
‘oracle’ model below (though unlike our system their
model learns from phone lattices rather than a single
transcription). They evaluate only on phone recogni-
tion, not scoring the inferred lexical items.
</bodyText>
<note confidence="0.874601">
Recently, B¨orschinger et al. (2013) did present a
</note>
<figureCaption confidence="0.85459825">
Figure 1: The graphical model for our system (Eq. 1-
4). Note that the si are not distinct observations; they
are concatenated together into a continuous sequence of
characters which constitute the observations.
</figureCaption>
<bodyText confidence="0.99950775">
joint learner for segmentation, phonetic learning, and
lexical clustering, but the model and inference are
tailored to investigate word-final /t/-deletion, rather
than aiming for a broad coverage system as we do.
</bodyText>
<sectionHeader confidence="0.998479" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999761818181818">
We follow several previous models of lexical acquisi-
tion in adopting a Bayesian noisy channel framework
(Eq. 1-4; Fig. 1). The model has two components:
a source distribution P(X) over utterances without
phonetic variability X, i.e., intended forms (Elsner et
al., 2012) and a channel or noise distribution T(S|X)
that translates them into the observed surface forms
S. The boundaries between surface forms are then
deterministically removed so that the actual observa-
tions are just the unsegmented string of characters in
the surface forms.
</bodyText>
<equation confidence="0.61545275">
G0|α0, pstop — DP(α0,Geom(pstop))
G.|G0, α1 — DP (α1, G0)
Xi|Xi−1 — GXi_1
S|X; 0 — T(S|X; 0)
</equation>
<bodyText confidence="0.975917">
The source model is an exact copy of GGJ1: to
generate the intended-form word sequences X, we
</bodyText>
<footnote confidence="0.598169">
1We use their best reported parameter values: α0 =
3000, α1 = 100, pstap = .2 and for unigrams, α0 = 20.
</footnote>
<table confidence="0.342819333333333">
Generator for possible words
Geoma, b, ..., ju, ... want, ... juwant, ...
Probabilities for each word
</table>
<equation confidence="0.8767255">
(sparse)
p(ði) = .1, p(a) = .05, p(want) = .01...
a0
G0
Conditional probabilities
for each word after each word
p(ði  |want) = .3, p(a  |want) = .1,
p(want  |want) = .0001...
</equation>
<figure confidence="0.957118352941176">
Intended forms
ju want a kuki
ju want it
Surface forms
ja wan a kuki
ju wand it
GGJ
09
a1
Gx
� contexts
x x
T
2 ...
1
s1 s2 ...
n utterances
</figure>
<page confidence="0.999662">
43
</page>
<bodyText confidence="0.999922979166667">
sample a random language model from a hierarchi-
cal Dirichlet process (Teh et al., 2006) with char-
acter strings as atoms. To do so, we first draw a
unigram distribution G0 from a Dirichlet process
prior whose base distribution generates intended form
word strings by drawing each phone in turn until the
stop character is drawn (with probability pstop). Then,
for each possible context word x, we draw a condi-
tional distribution on words following that context
Gx = P(Xi = ~|Xi_1 = x) using G0 as a prior.
Finally, we sample word sequences x1 ... x,,, from
the bigram model.
The channel model is a finite transducer with pa-
rameters 0 which independently rewrites single char-
acters from the intended string into characters of the
surface string. We use MAP point estimates of these
parameters; single characters (without n-gram con-
text) are used for computational efficiency. Also for
efficiency, the transducer can insert characters into
the surface string, but cannot delete characters from
the intended string. As in several previous phonolog-
ical models (Dreyer et al., 2008; Hayes and Wilson,
2008), the probabilities are learned using a feature-
based log-linear model. For features, we use all the
unigram features from Elsner et al. (2012), which
check faithfulness to voicing, place and manner of
articulation (for example, for k ! g, active features
are faith-manner, faith-place, output-g and voiceless-
to-voiced).
Below, we present two methods for learning the
transducer parameters 0. The oracle transducer is es-
timated using the gold-standard word segmentations
and intended forms for the dataset; it represents the
best possible approximation under our model of the
actual phonetics of the dataset. We can also estimate
the transducer using the EM algorithm. We first ini-
tialize a simple transducer by putting small weights
on the faithfulness features to encourage phonologi-
cally plausible changes. With this initial model, we
begin running the sampler used to learn word segmen-
tations. After several hundred sampler iterations, we
start re-estimating the transducer by maximum likeli-
hood after each iteration. We regularize our estimates
by adding 200 pseudocounts for the rewrite x ! x
during training (rather than regularizing the weights
for particular features). We also show segment only
results for a model without the transducer component
(i.e., S = X); this recovers the GGJ baseline.
</bodyText>
<sectionHeader confidence="0.999754" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.9997053">
Inference for this model is complicated for two rea-
sons. First, the hypothesis space is extremely large.
Since we allow the input string to be probabilistically
lengthened, we cannot be sure how long it is, nor
which characters it contains. Second, our hypothe-
ses about nearby characters are highly correlated due
to lexical effects. When deciding how to interpret
[w@nt], if we posit that the intended vowel is /A/, the
word is likely to be /wAn/ “one” and the next word
begins with /t/; if instead we posit that the vowel
is /O/, the word is probably /wOnt/ “want”. Thus,
inference methods that change only one character at
a time are unlikely to mix well. Since they cannot
simultaneously change the vowel and resegment the
/t/, they must pass through a low-probability inter-
mediate state to get from one state to the other, so
will tend to get stuck in a bad local minimum. A
Gibbs sampler which inserts or deletes a single seg-
ment boundary in each step (Goldwater et al., 2009)
suffers from this problem.
Mochihashi et al. (2009) describe an inference
method with higher mobility: a block sampler for
the GGJ model that samples from the posterior over
analyses of a whole utterance at once. This method
encodes the model as a large HMM, using dynamic
programming to select an analysis. We encode our
own model in the same way, constructing the HMM
and composing it with the transducer (Mohri, 2004)
to form a larger finite-state machine which is still
amenable to forward-backward sampling.
</bodyText>
<subsectionHeader confidence="0.992522">
4.1 Finite-state encoding
</subsectionHeader>
<bodyText confidence="0.997429916666667">
Following Mochihashi et al. (2009) and Neubig et
al. (2010), we can write the original GGJ model
as a Hidden Semi-Markov model. States in the
HMM, written ST:[w][C ], are labeled with the
previous word w and the sequence of characters C
which have so far been incorporated into the current
word. To produce a word boundary, we transition
from ST:[w][C] to ST:[C][] with probability
P(xi = C|xi_1 = w). We can also add the next
character s to the current word, transitioning from
ST:[w][C] to ST:[w][C : s], at no cost (since
the full cost of the word is paid at its boundary, there
</bodyText>
<page confidence="0.999477">
44
</page>
<figureCaption confidence="0.991829375">
Figure 2: A fragment of the composed finite-state machine
for word segmentation and character replacement for the
surface string ju. The start state [s] is followed by a word
boundary (filled circle); the next intended character is
probably j but can be d or others with lower probability.
After j can be a word boundary (forming the intended
word j), or another character such as u, @ or other (not
shown) alternatives.
</figureCaption>
<bodyText confidence="0.997915911111111">
is no cost for the individual characters)2.
In addition to analyses using known words, we
can also encode the uniform-geometric prior over
unknown words using a finite-state machine. We
can choose to select a word from the prior by tran-
sitioning to a state ST:[Geom][] with probability
P(new word|xi_1 = w) immediately after a word
boundary. While in Geom, we can transition to a new
Geom state and produce any character with uniform
probability P(c) = (1−Pstop) 1
|C|; otherwise, we can
end the word, transitioning to ST:[unk.word][],
with probability Pstop.
This construction is also approximate; it ignores
the possibility that the prior will generate a known
word w, in which case our final transition ought to
be to ST:[w][] instead of ST:[unk.word][]. This
approximation means we do not need to add context
to the Geom state to remember the sequence of char-
acters it produced, which allows us to keep only a
single Geom state on the chart at each timestep.
When we compose this model with the channel
model, the number of states expands. Each state must
now keep track of the previous word, what intended
characters C have been posited and what surface char-
acters S have been recognized, ST:[w][C ][S].
2Though not mentioned by Mochihashi et al. (2009) or Neu-
big et al. (2010), this construction is not exact, since transitions
in a Bayesian HMM are exchangeable but not independent (Beal
et al., 2001): if a word occurs twice in an utterance, its probabil-
ity is slightly higher the second time. For single utterances, this
bias is small and easy to correct for using a Metropolis-Hastings
acceptance check (B¨orschinger and Johnson, 2012) using the
path probability from the HMM as the proposal.
To recognize the current word, we transition to
ST:[C ][][] with probability P(xi = C|xi_1 =
w). To parse a new surface character s by positing
intended character x (note that x might be e), we
transition to ST:[w][C : x][S : 8] with probabil-
ity T(s|x). (As above, we pay no cost for our choice
of x, which is paid for when we recognize the word;
however, we must pay for s.) For efficiency, we do
not allow the G0 states to hypothesize different sur-
face and intended characters, so when we initially
propose an unknown word, it must surface as itself.3
</bodyText>
<subsectionHeader confidence="0.999638">
4.2 Beam sampler
</subsectionHeader>
<bodyText confidence="0.9938814375">
This machine has too many states to fully fill the chart
before backward sampling, so we restrict the set of
trajectories under consideration using beam sampling
(Van Gael et al., 2008) and simulated annealing.
The beam sampler is closely related to the standard
beam search technique, which uses a probability cut-
off to discard parts of the FST which are unlikely to
figure in the eventual solution. Unlike conventional
beam search, the sampler explores using stochastic
cutoffs, so that all trajectories are explored, but most
of the bad ones are explored infrequently, leading to
higher efficiency.
We design our beam sampler to restrict the set
of potential intended characters at each timestep.
In particular, given a stream of input characters
S = s1 ... s, we introduce a set of auxiliary cutoff
variables U = u1 ... u, The ui variables represent
limits on the probability of the emission of surface
character si; we exclude any hypothesized xi whose
probability of generating si, T (si|xi), is less than
ui. To create a beam sampling scheme, we must de-
vise a distribution for U given a state sequence Q (as
discussed above, the sequence of states encodes the
intended character sequence and the segmentation
of the surface string), Pu(U|Q) and then incorporate
the probability of U into the forward messages.
If qi is the state in Q at which si is generated, and
xi the corresponding intended character, we require
that Pu &lt; T(si|xi); that is, the cutoffs must not
exclude any states in the sequence Q. We define Pu
3Again, this approximation is corrected for by the Metropolis-
Hastings step.
</bodyText>
<figure confidence="0.998040611111111">
p(j|[s])
u/u p(u|j)
word j
word u
u
word ju
[s]
ə/u
p(ju|[s])
j/j
d/j
d
ə
j
u/u
u
word jə
p(jə|[s])
</figure>
<page confidence="0.965085">
45
</page>
<bodyText confidence="0.6173935">
as a A-mixture of two distributions: on each sweep). This is similar to the scheme for
altering Pu in Huggins and Wood (2013).
</bodyText>
<equation confidence="0.9982275">
Pu(u|si,xi) = AU[0,min(.05,T(si|xi))]+
(1 − A)T(si|xi)Beta(5, 1e − 5)
</equation>
<bodyText confidence="0.999418714285714">
The former distribution is quite unrestrictive, while
the latter prefers to prune away nearly all the states.
Thus, for most characters in the string, we do not
permit radical changes, while for a fraction, we do.
We follow Huggins and Wood (2013), who ex-
tended Van Gael et al. (2008) to the case of a non-
uniform Pu, to define our forward message a as:
</bodyText>
<equation confidence="0.993063666666667">
a(qi, i) a P(qi, S0..i, U0..i) (5)
�= Pu(ui|si, xi)T (si|xi)a(qi−1, i − 1)
qz−1
</equation>
<bodyText confidence="0.999993677419355">
This is the standard HMM forward message, aug-
mented with the probability of u. Since Pu(·|si, xi)
is required to be less than T (si|xi), it will be 0 when-
ever T(si|xi) &lt; u; this is how the u variables func-
tion as cutoffs. In practice, we use the u variables to
filter the lexical items that begin at each position i
in advance, using a simple 0/1 edit distance Markov
model which runs faster than our full model. (For ex-
ample, we can quickly check if the current U allows
want as the intended form for wOlk at i; if not, we can
avoid constructing the prefix ST:[xi−1 ][wa][wO]
since the continuation will fail.)
The algorithm’s speed depends on the size and
uncertainty of the inferred LM: large numbers of
plausible words mean more states to explore. When
inference starts, and the system is highly uncertain
about word boundaries, it is therefore reasonable to
limit the exploration of the character sequence. We
do so by annealing in two ways: as in Goldwater
et al. (2009), we raise P(X) (Eq. 3) to a power t
which increases linearly from .3. To sample from
the posterior, we would want to end with t = 1, but
as in previous noisy-channel models (Elsner et al.,
2012; Bahl et al., 1980) we get better results when we
emphasize the LM at the expense of the channel and
so end at t = 2. Meanwhile, as t rises and we explore
fewer implausible lexical sequences, we can explore
the character sequence more. We begin by setting
the A interpolation parameter of Pu to 0 to minimize
exploration and increase it linearly to .3 (allowing
the system to change about a third of the characters
</bodyText>
<subsectionHeader confidence="0.990571">
4.3 Dataset and metrics
</subsectionHeader>
<bodyText confidence="0.999967305555556">
We use the corpus released by Elsner et al. (2012),
which contains 9790 child-directed English utter-
ances originally from the Bernstein-Ratner corpus
(Bernstein-Ratner, 1987) and later transcribed phone-
mically (Brent, 1999). This standard word segmenta-
tion dataset was modified by Elsner et al. (2012) to
include phonetic variation by assigning each token a
pronunciation independently selected from the empir-
ical distribution of pronunciations of that word type
in the closely-transcribed Buckeye Speech Corpus
(Pitt et al., 2007). Following previous work, we hold
out the last 1790 utterances as unseen test data during
development. In the results presented here, we run
the model on all 9790 utterances but score only these
1790. We average results over 5 runs of the model
with different random seeds.
We use standard metrics for segmentation and lex-
icon recovery. For segmentation, we report precision,
recall and F-score for word boundaries (bds), and for
the positions of word tokens in the surface string (srf;
both boundaries must be correct).
For normalization of the pronunciation variation,
we follow Elsner et al. (2012) in measuring how well
the system clusters together variant pronunciations
of the same lexical item, without insisting that the
intended form the system proposes for them match
the one in our corpus. For example, if the system
correctly clusters [ju] and [jI] together but assigns
them the incorrect intended form /jI/, we can still
give credit to this cluster if it is the one that overlaps
best with the gold-standard /ju/ cluster. To compute
these scores, we find the optimal one-to-one map-
ping between our clusters of pronunciations and the
true lexical entries, then report scores for mapped to-
kens (mtk; boundaries and mapping to gold standard
cluster must be correct) and mapped types4 (mlx).
</bodyText>
<footnote confidence="0.9382998">
4Elsner et al. (2012) calls the mlx metric lexicon F, which
is possibly confusing. We map the clusters to a gold-standard
lexicon (plus potentially some words that don’t correspond to
anything in the gold standard) and compute a type-level F-score
on this lexicon.
</footnote>
<page confidence="0.989345">
46
</page>
<table confidence="0.999979461538462">
Prec Rec F-score
Pipeline (segment, then cluster): (Elsner et al., 2012)
Bds 70.4 93.5 80.3
Srf 56.5 69.7 62.4
Mtk 44.2 54.5 48.8
Mlx 48.6 43.1 45.7
Bigram model, segment only
Bds 73.9 (-0.6:0.7) 91.0 (-0.6:0.4) 81.6 (-0.5:0.6)
Srf 60.8 (-0.7:1.1) 70.8 (-0.8:0.9) 65.4 (-0.6:1.0)
Mtk 41.6 (-0.6:1.2) 48.4 (-0.5:1.2) 44.8 (-0.6:1.2)
Mlx 36.6 (-0.7:0.8) 49.8 (-1.0:0.8) 42.2 (-0.9:0.8)
Unigram model, oracle transducer
Bds 81.4 (-0.8:0.4) 72.1 (-0.9:0.8) 76.4 (-0.5:0.7)
Srf 63.6 (-1.0:1.1) 58.5 (-1.2:1.2) 60.9 (-0.9:1.2)
Mtk 46.8 (-1.0:1.1) 43.0 (-1.1:1.2) 44.8 (-1.0:1.2)
Mlx 56.7 (-1.1:1.0) 47.6 (-1.4:0.8) 51.7 (-1.2:0.8)
Bigram model, oracle transducer
Bds 76.1 (-0.6:0.6) 83.8 (-0.9:1.0) 79.8 (-0.8:0.4)
Srf 62.2 (-0.9:1.0) 66.7 (-1.2:1.1) 64.4 (-1.1:0.8)
Mtk 47.2 (-0.7:0.9) 50.6 (-1.0:0.8) 48.8 (-0.8:0.7)
Mlx 40.1 (-1.0:1.2) 43.7 (-0.6:0.7) 41.8 (-0.8:0.6)
Bigram model, EM transducer
Bds 80.1 (-0.5:0.8) 83.0 (-1.4:1.3) 81.5 (-0.5:0.7)
Srf 66.1 (-0.8:1.4) 67.8 (-1.4:1.7) 66.9 (-0.9:1.4)
Mtk 49.0 (-0.9:0.7) 50.3 (-1.1:1.4) 49.6 (-1.0:1.0)
Mlx 43.0 (-1.0:1.4) 49.5 (-1.5:1.1) 46.0 (-1.0:1.3)
</table>
<tableCaption confidence="0.910216666666667">
Table 1: Mean segmentation (bds, srf) and normalization
(mtk, mlx) scores on the test set over 5 runs. Parentheses
show min and max scores as differences from the mean.
</tableCaption>
<sectionHeader confidence="0.998267" genericHeader="evaluation">
5 Results and discussion
</sectionHeader>
<bodyText confidence="0.9994986">
In the following sections, we analyze how our model
with variability compares to GGJ on noisy data. We
give quantitative scores and also show that qualitative
patterns of errors are often similar to those of human
learners and listeners.
</bodyText>
<subsectionHeader confidence="0.974991">
5.1 Clean versus variable input
</subsectionHeader>
<bodyText confidence="0.999924833333333">
We begin by evaluating our model as a word seg-
mentation system. (Table 1 gives segmentation and
normalization scores for various models and base-
lines on the 1790 test utterances.) We first confirm
that our inference method is reasonable. The bigram
model without variability (“segment only”) should
have the same segmentation performance as the stan-
dard dpseg implementation of GGJ. This is the case:
dpseg has boundary F of 80.3 and token F of 62.4;
we get 81.6 and 65.4. Thus, our sampler is finding
good solutions, at least for the no-variability model.
We compare segmentation scores between the
“segment only” system and the two bigram models
with transducers (“oracle” and “EM”). While these
systems all achieve similar segmentation scores, they
do so in different ways. “Segment only” finds a so-
lution with boundary precision 73.9% and boundary
recall 91.0% for a total F of 81.6%. The low pre-
cision and high recall here indicate a tendency to
oversegment; when the analysis of a given subse-
quence is unclear, the system prefers to chop it into
small chunks. The bigram models which incorporate
transducers score P: 76.1, R: 83.8 (oracle) and P:
80.1, R: 83.0 (EM), indicating that they prefer to find
longer sequences (undersegment) more.
In previous experiments on datasets without varia-
tion, GGJ also has a strong tendency to undersegment
the data (boundary P: 90.1, R: 80.3), which Gold-
water et al. argue is rational behavior for an ideal
learner seeking a parsimonious explanation for the
data. Undersegmentation occurs especially when ig-
noring lexical context (a unigram model), but to some
extent even in bigram models. Human learners also
tend to learn collocations as single words (Peters,
1983; Tomasello, 2000), and the GGJ model has been
shown to capture several other effects seen in labora-
tory segmentation tasks (Frank et al., 2010). Together,
these findings support the idea that human learners
may behave in important respects like the Bayesian
ideal learners that Goldwater et al. presented.
However, experiments on data with variation have
called these conclusions into question. In particu-
lar, GGJ has previously been shown to oversegment
rather than undersegment as the input grows noisier
(Fleck, 2008), and our results replicate this finding
(oversegmentation for the “segment only” model).
In addition, the GGJ bigram model, which achieves
much higher segmentation accuracy than the unigram
model on clean data, actually performs worse on very
noisy data (Jansen et al., 2013). Infants are known to
track statistical dependencies across words (G´omez
and Maye, 2005), so it is worrisome that these de-
pendencies hurt GGJ’s segmentation accuracy when
learning from noisy data.
Our results show that modeling phonetic variabil-
ity reverses the problematic trends described above.
Although the models with phonetic variability show
similar overall segmentation accuracy on noisy data
to the original GGJ model, the pattern of errors
changes, with less oversegmentation and more un-
</bodyText>
<page confidence="0.996816">
47
</page>
<bodyText confidence="0.998087666666667">
dersegmentation. Thus, their qualitative performance
on variable data resembles GGJ’s on clean data, and
therefore the behavior of human learners.
</bodyText>
<subsectionHeader confidence="0.996387">
5.2 Phonetic variability
</subsectionHeader>
<bodyText confidence="0.9999083">
We next analyze the model’s ability to normalize vari-
ations in the pronunciation of tokens, by inspecting
the mtk score. The “segment only” baseline is pre-
dictably poor, F: 44.8. The pipeline model scores
48.8, and our oracle transducer model matches this
exactly. The EM transducer scores better, F: 49.6.
Although the confidence intervals overlap slightly,
the EM system also outperforms the pipeline on the
other F-measures; altogether, these results suggest
at least a weak learning synergy (Johnson, 2008) be-
tween segmentation and phonetic learning.
It is interesting that EM can perform better than
the oracle. However, EM is more conservative about
which sound changes it will allow, and thus tends to
avoid mistakes caused by the simplicity of the trans-
ducer model. Since the transducer works segment-
by-segment, it can apply rare contextual variations
out of context. EM benefits from not learning these
variations to begin with.
We can also compare the bigram and unigram ver-
sions of the model. The unigram model is a rea-
sonable segmenter, though not quite as good as the
bigram model, with boundary F of 76.4 and token
F of 60.9 (compared to 79.8 and 64.4 using the bi-
gram model). However, it is not good at normalizing
variation; its mtk score is comparable to the baseline
at 44.8%5. Although bigram context is only moder-
ately effective for telling where words are, the model
seems heavily reliant on lexical context to decide
what words it is hearing.
</bodyText>
<subsectionHeader confidence="0.995275">
5.3 Error analysis
</subsectionHeader>
<bodyText confidence="0.993927875">
To gain more insight into the differing behavior of
our model versus a pipelined system, we inspect the
intended word strings X proposed by each one in
detail. Below, we categorize the kinds of intended
word strings that the model might propose to span a
given gold-standard word token:
Correct Correctly segmented, mapped to the correct
lexical item (e.g., gold intended /ju/, surface
</bodyText>
<footnote confidence="0.8420445">
5Elsner et al. (2012) show a similar result for a unigram
version of their pipelined system.
</footnote>
<table confidence="0.999237666666667">
EM-learned Segment only
Correct 49.88 47.61
Wrong form 17.96 23.73
Collocation 14.25 7.59
Split 8.26 15.18
One bound 7.11 15.18
Corr. colloc. 1.35 &lt; 0.01
Other 0.75 0.22
Corr. split 0.43 0.66
</table>
<tableCaption confidence="0.755051">
Table 2: Distribution (%) of error types (see text) in a
single run on the full dataset.
segmentation [ju], intended /ju/)
</tableCaption>
<bodyText confidence="0.995586941176471">
Wrong form Correctly segmented, mapped to the
wrong lexical item (/ju/, surf. [ju], int. /jEs/)
Colloc Missegmented as part of a sequence whose
boundaries correspond to real word boundaries
(/ju•want/, surf. [juwant], int. /juwant/)
Corr. colloc As above, but proposed lexical item
maps to this word (/ar•ju/, surf. [arj@] int.
/ju/)
Split Missegmented with a word-internal boundary
(/dOgiz/, surf. [dO•giz], int. /dO•giz/)
Corr. split As above, but one proposed word maps
correctly (/dOgi/, surf. [dOg•i], int. /dOgi•@/)
One boundary One boundary correct, the other
wrong (/ju•wa.../, surf. [juw], int. /juw/)
Other Not a collocation, both boundaries are wrong
(/du•ju•wa.../, surf. [ujuw], int. /ujuw/)
Table 2 shows the distribution over intended word
strings proposed by the “segment only” baseline and
the EM-learned transducer. Both systems propose
a large number of correct forms, and the most com-
mon error category is “wrong form” (lexical error
without segmentation error), an error which could
potentially be repaired in a pipeline system. How-
ever, the remaining errors represent segmentation
mistakes which a pipeline could not repair. Here
the two systems behave quite differently. The EM-
learned transducer analyses 14% of real tokens as
parts of multiword collocations like “doyou”; in an-
other 1.35%, the underlying content word is even
correctly detected. The non-variable system, on the
other hand, analyses 15% of real tokens by splitting
them into pieces. Since infant learners tend to learn
collocations, this supports our analysis that the model
with variation better models human behavior.
</bodyText>
<page confidence="0.999497">
48
</page>
<tableCaption confidence="0.936574">
EM ju: 805, duju: 239, juwan: 88, jI: 58, e-ju: 54, judu:
47, jx: 39, jul2k: 39, fu: 30, u: 23, 3u: 18, j: 17,
je-: 16, tfu: 15, aj:15, Derjugo: 12, d3u: 12
GGJ ju: 498, jI: 280, j@: 165, ji: 119, duju: 106, dujI: 44,
kInju: 39, i: 32, u: 29, kInjI: 29, jul2k: 24, juwan:
23, j: 22, fu: 19, jU: 18, e-ju: 18, I:16, 3u: 15, d3•u:
13, jE: 12, fI: 11, TxrJkju: 11
Table 3: Forms proposed with frequency &gt; 10 for
gold-standard tokens of “you” in one sample from EM-
transducer and segment-only (GGJ) system.
</tableCaption>
<bodyText confidence="0.999943527777778">
To illustrate this behavior anecdotally, we present
the distribution of intended word strings spanning
tokens whose gold intended form is /ju/ “you” (Table
3). The EM-learned solution proposes 805 tokens
of /ju/, which is the correct analysis6; the “segment
only” system instead finds varying forms like /jI/,
/jx/ etc. This is unsurprising and could be repaired
by a suitable pipelined system. However, the EM
system also proposes 239 instances of “doyou”, 88
instances of “youwant”, 54 instances of “areyou” and
several other collocations. The “segment only” sys-
tem finds some of these collocations, split into dif-
ferent versions: for instance 106 instances of /duju/
and 44 of /dujI/. In a pipelined system, we could
combine these variants to find 150 instances— but
this is still 89 instances short of the 239 found when
allowing for variability. The same pattern holds for
“youlike” and “youwant”. Because the non-variable
system must learn each variant separately, it learns
only the most common instances of these long collo-
cations, and analyzes infrequent variants differently.
We also perform this analysis specifically for
words beginning with vowels. Infants show a delay
in their ability to segment these words from continu-
ous speech (Mattys and Jusczyk, 2001; Nazzi et al.,
2005; Seidl and Johnson, 2008), and Seidl and John-
son (2008) suggest a perceptual explanation— initial
vowels can be hard to hear and often exhibit variation
due to coarticulation or resyllabification. Although
our dataset does not contain coarticulation as such, it
should show this pattern of greater variation, which
we hypothesize might lead to difficulty in segmenting
and recognizing vowel-initial words.
The model’s behavior is consistent with this hy-
pothesis (Table 4). Both the “segment only” and
EM transducer models find approximately the same
</bodyText>
<footnote confidence="0.983282">
6Not all the variants are merged, however. jI, jx, fu etc. are
still occasionally analyzed as separate lexical items.
</footnote>
<table confidence="0.995459571428571">
Segment only Vow. init Cons. init
Correct 47.5 51.7
Wrong form 18.6 15.7
Collocation 14.6 12.2
Split 6.2 10.8
Right bd. corr. 5.8 3.6
Left bd. corr. 4.6 3.8
EM transducer Vow. init Cons. init
Correct 41.5 52.1
Wrong form 20.4 17.3
Collocation 19.2 12.5
Split 5.2 9.1
Right bd. corr. 6.2 2.7
Left bd. corr. 2.7 3.1
</table>
<tableCaption confidence="0.965469">
Table 4: Most common error types (%; see text) for in-
tended forms beginning with vowels or consonants. Rare
error types are not shown. “One bound” errors are split up
by which boundary is correct.
</tableCaption>
<bodyText confidence="0.999950615384615">
proportion of vowel-initial tokens, and both systems
do somewhat better on consonant-initial words than
vowel-initial words. The advantage is stronger for
the transducer model, which gets only 41.5% of
vowel-initial tokens correct as opposed to 52.1% of
consonant-initial words. It proposes more colloca-
tions for vowel-initial words (19.2%) than for conso-
nants (12.5%). In cases where they do not propose a
collocation, both systems are somewhat more likely
to find the right boundary of a vowel-initial token
than the left boundary (although again this difference
is larger for the EM system); this suggests that the
problem is indeed caused by the initial segment.
</bodyText>
<subsectionHeader confidence="0.987135">
5.4 Phonetic Learning
</subsectionHeader>
<bodyText confidence="0.999958214285714">
We next compare phonetic variations learned by the
model to characteristics of infant speech perception.
Infants show an asymmetry between consonants and
vowels, losing sensitivity to non-native vowel con-
trasts by eight months (Kuhl et al., 1992; Bosch
and Sebasti´an-Gall´es, 2003) but to non-native con-
sonant contrasts only by 10-12 months (Werker and
Tees, 1984). The observed ordering is somewhat
puzzling when one considers the availability for dis-
tributional information (Maye et al., 2002), which is
much stronger for stop consonants than for vowels
(Lisker and Abramson, 1964; Peterson and Barney,
1952). Infants are also conservative in generalizing
across phonetic variability, showing a delayed abil-
</bodyText>
<page confidence="0.9987">
49
</page>
<bodyText confidence="0.999974942857143">
ity to generalize across talkers, affects, and dialects.
They have difficulty recognizing word tokens that are
spoken by a different talker or in a different tone of
voice until 11 months (Houston and Jusczyk, 2000;
Singh et al., 2004), and the ability to adapt to unfa-
miliar dialects appears to develop even later, between
15 and 19 months (Best et al., 2009; Heugten and
Johnson, in press; White and Aslin, 2011).
Similar to infants, our model shows both a vowel-
consonant asymmetry and a reluctance to accept the
full range of adult phonetic variability. Table 5 shows
some segment-to-segment alternations learned in var-
ious transducers. The oracle learns a large amount
of variation (u surfaces as itself only 68% of the
time) involving many different segments, whereas
EM is similar to infant learners in learning a more
conservative solution with fewer alternations over-
all. Moreover, EM appears to identify patterns of
variability in vowels before consonants. It learns a
similar range of alternations for u as in the oracle,
although it treats the sound as less variable than it
actually is. It learns much less variability for con-
sonants; it picks up the alternation of d with s and
z, but predicts that d will surface as itself 91% of
the time when the true figure is only 69%. And it
fails to learn any meaningful alternations involving
k. These results suggest that patterns of variability in
vowels are more evident than patterns of variability
in consonants when infants are beginning to solve the
word segmentation problem.
To investigate the effect of data size on this con-
servativism, we ran the system on 1000 utterances
instead of 9790. This leads to an even more conser-
vative solution, with variations for u but none of the
others (although i and d still vary more than k).
</bodyText>
<subsectionHeader confidence="0.997492">
5.5 Segmentation and recognition errors
</subsectionHeader>
<bodyText confidence="0.98868375">
A particularly interesting set of errors are those that
involve both a missegmentation and a simultaneous
misrecognition, since the joint model is prone to
such errors while the pipelined model is not. Rel-
atively little is known about infants’ misrecognitions
of words in fluent speech, although it is clear that they
find words in medial position harder (Plunkett, 2005;
Seidl and Johnson, 2006). However, adults make
missegmentation/misrecognition errors fairly often,
especially when listening to noisy audio (Butterfield
and Cutler, 1988). Such errors are more common
top 4 outputs s
</bodyText>
<table confidence="0.999558666666667">
u .68 @ .05 a .04 U .04
i .85 I .03 @ .03 E .02
d .69 s .07 [0] .07 z .04
k .93 d .02 g .02
r .21 h .11 d .01 @ .07
u .75 @ .08 I .04 U .03
i .90 I .04 E .02
d .91 s .03 z 0.1
k .98
@ .32 I .14 n .13 t .13
u .82 I .04 @ .04 a .02
i .97
d .95
k .99
@ .21 I .18 t .12 s .12
</table>
<tableCaption confidence="0.7211378">
Table 5: Learned phonetic alternations: top 4 outputs s
with p &gt; .001 for inputs x = uw (/u/), iy (/i/), dh (/d/),
k (/k/) and [0], the null character. Outputs from [0] are
insertions. The oracle allows [0] as an output (deletion)
but for computational reasons, the model does not.
</tableCaption>
<bodyText confidence="0.999817318181818">
when the misrecognized word belongs to a prosod-
ically rare class and when the incorrectly hypothe-
sized string contains frequent words (Cutler, 1990);
phonetically ambiguous words are also more com-
monly recognized as the more frequent of two op-
tions (Connine et al., 1993). For the indefinite article
“a” (often reduced to [@]), lexical context is the main
factor in deciding between ambiguous interpretations
(Kim et al., 2012). In rapid speech, listeners have few
phonetic cues to indicate whether it is present at all
(Dilley and Pitt, 2010). Below, we analyze various
misrecognitions made by our system (using the EM
transducer), and find some similar effects.
The easiest cases to analyze are those with no mis-
segmentation: the proposed boundaries are correct,
and the proposed lexical entry corresponds to a real
word7, but not the correct one. Most of them corre-
spond to homophones (Table 6).
Common cases with a missegmentation include it
and is, a and is, it’s and is, who, who’s and whose,
that’s and what’s, and there and there’s. In general,
these errors involve words which sometimes appear
</bodyText>
<footnote confidence="0.79980075">
7The one-to-one mapping can be misleading, as it may map
a large cluster to a real word on the basis of one or two tokens if
all other tokens correspond to a different word already used for
another cluster. We manually filter out a few cases like this.
</footnote>
<figure confidence="0.998613875">
System x
Oracle u
EM i
(full) d
k
[0]
u
i
d
k
[0]
EM u
(only i
1000 d
utts) k
[0]
</figure>
<page confidence="0.873073">
50
</page>
<table confidence="0.920682833333333">
Actual proposed count
/tu/ “two” /t@/ “to” 95
/kin/ “can” /kxnt/ “can’t” 67
/En/ “and”
/xn/ “an” 61
/hIz/ “his” /Iz/ “is” 57
/D@/ “the” /@/ “ah” 51
/w@ts/ “what’s” /wants/ “wants” 40
/wan/ “want” /won/ “won’t” 39
/yu/ “you” /yx/ “yeah” 39
/f@~/ “for” /fOr/ “four” 30
/hir/ “here” /hil/ “he’ll” 28
</table>
<tableCaption confidence="0.85355125">
Table 6: Top ten errors involving confusion between real,
correctly segmented words: the most common pronunci-
ation of the actual token and its orthographic form, the
same for the proposed token, and the frequency.
</tableCaption>
<bodyText confidence="0.999910529411765">
with a morpheme or clitic (which can easily be mis-
segmented as part of something else), words which
differ by one segment, and frequent function words
which often appear in similar contexts. These tenden-
cies match those shown by adult human listeners.
A particularly distinctive set of joint recognition
and segmentation errors are those where an entire
real token is treated as phonetic “noise”— that is, it
is segmented along with an adjacent word, and the
system clusters the whole sequence as a token of
that word. The most common examples are “that’s a”
identified as “that’s”, “have a” identified as “have”,
“sees a” identified as “sees” and other examples in-
volving “a”, a word which also frequently confuses
humans (Kim et al., 2012; Dilley and Pitt, 2010).
However, there are also instances of “who’s in” as
“who’s”, “does it” as “does”, and “can you” as “can”.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999878">
We have presented a model that jointly infers word
segmentation, lexical items, and a model of phonetic
variability; we believe this is the first model to do
so on a broad-coverage naturalistic corpus8. Our re-
sults show a small improvement in both segmentation
and normalization over a pipeline model, providing
evidence for a synergistic interaction between these
learning tasks and supporting claims of interactive
learning from the developmental literature on infants.
We also reproduced several experimental findings;
our results suggest that two vowel-consonant asym-
</bodyText>
<footnote confidence="0.944082333333333">
8Software is available from the ACL archive; updated
versions may be posted at https://bitbucket.org/
melsner/beamseg.
</footnote>
<bodyText confidence="0.999817714285714">
metries, one from the word segmentation literature
and another from the phonetic learning literature, are
linked to the large variability in vowels found in nat-
ural corpora. The model’s correspondence with hu-
man behavioral results is by no means exact, but we
believe these kinds of predictions might help guide
future research on infant phonetic and word learning.
</bodyText>
<sectionHeader confidence="0.996413" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997536666666667">
Thanks to Mary Beckman for comments. This work
was supported by EPSRC grant EP/H050442/1 to the
second author.
</bodyText>
<sectionHeader confidence="0.999404" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999376944444445">
Lalit Bahl, Raimo Bakis, Frederick Jelinek, and Robert
Mercer. 1980. Language-model/acoustic-channel-
model balance mechanism. Technical disclosure bul-
letin Vol. 23, No. 7b, IBM, December.
Matthew J. Beal, Zoubin Ghahramani, and Carl Edward
Rasmussen. 2001. The infinite Hidden Markov Model.
In NIPS, pages 577–584.
Elika Bergelson and Daniel Swingley. 2012. At 6-9
months, human infants know the meanings of many
common nouns. Proceedings of the National Academy
of Sciences, 109:3253–3258.
Nan Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, editors,
Children’s Language, volume 6. Erlbaum, Hillsdale,
NJ.
Catherine T. Best, Michael D. Tyler, Tiffany N. Good-
ing, Corey B. Orlando, and Chelsea A. Quann. 2009.
Development of phonological constancy: Toddlers’ per-
ception of native- and jamaican-accented words. Psy-
chological Science, 20(5):539–542.
Benjamin B¨orschinger and Mark Johnson. 2012. Using
rejuvenation to improve particle filtering for Bayesian
word segmentation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 85–89, Jeju Island,
Korea, July. Association for Computational Linguistics.
Benjamin B¨orschinger, Mark Johnson, and Katherine De-
muth. 2013. A joint model of word segmentation
and phonological variation for English word-final /t/-
deletion. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics, Sofia,
Bulgaria, August. Association for Computational Lin-
guistics.
Luc Boruta, Sharon Peperkamp, Benoit Crabb´e, and Em-
manuel Dupoux. 2011. Testing the robustness of online
word segmentation: Effects of linguistic diversity and
</reference>
<page confidence="0.992026">
51
</page>
<reference confidence="0.998817971698113">
phonetic variation. In Proceedings of the 2nd Workshop
on Cognitive Modeling and Computational Linguistics,
pages 1–9.
Laura Bosch and N´uria Sebasti´an-Gall´es. 2003. Simulta-
neous bilingualism and the perception of a language-
specific vowel contrast in the first year of life. Lan-
guage and Speech, 46(2-3):217–243.
Michael R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discovery.
Machine Learning, 34:71–105, February.
Sally Butterfield and Anne Cutler. 1988. Segmentation
errors by human listeners: Evidence for a prosodic
segmentation strategy. In Proceedings of SPEECH
‘88: Seventh Symposium of the Federation of Acoustic
Societies of Europe, vol. 3, pages 827–833, Edinburgh.
Morten H. Christiansen, Joseph Allen, and Mark S. Sei-
denberg. 1998. Learning to Segment Speech Using
Multiple Cues: A Connectionist Model. Language and
Cognitive Processes, 13(2/3):221–269.
C. M. Connine, D. Titone, and J. Wang. 1993. Audi-
tory word recognition: Extrinsic and intrinsic effects of
word frequency. Journal of Experimental Psychology:
Learning, Memory and Cognition, 19:81–94.
Anne Cutler. 1990. Exploiting prosodic probabilities in
speech segmentation. In G. A. Altmann, editor, Cog-
nitive models of speech processing: Psycholinguistic
and computational perspectives, pages 105–121. MIT
Press, Cambridge, MA.
Robert Daland and Janet B. Pierrehumbert. 2011. Learn-
ing diphone-based segmentation. Cognitive Science,
35(1):119–155.
Laura C. Dilley and Mark Pitt. 2010. Altering context
speech rate can cause words to appear or disappear.
Psychological Science, 21(11):1664–1670.
Markus Dreyer, Jason R. Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing,
EMNLP ’08, pages 1080–1089, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Emmanuel Dupoux, Guillaume Beraud-Sudreau, and
Shigeki Sagayama. 2011. Templatic features for mod-
eling phoneme acquisition. In Proceedings of the 33rd
Annual Cognitive Science Society.
Micha Elsner, Sharon Goldwater, and Jacob Eisenstein.
2012. Bootstrapping a unified model of lexical and pho-
netic acquisition. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 184–193, Jeju
Island, Korea, July. Association for Computational Lin-
guistics.
Naomi Feldman, Thomas Griffiths, and James Morgan.
2009. Learning phonetic categories by learning a lexi-
con. In Proceedings of the 31st Annual Conference of
the Cognitive Science Society.
Naomi H. Feldman, Emily B. Myers, Katherine S. White,
Thomas L. Griffiths, and James L. Morgan. 2013.
Word-level information influences phonetic learning
in adults and infants. Cognition, 127(3):427–438.
Naomi H. Feldman, Thomas L. Griffiths, Sharon Gold-
water, and James L. Morgan. in press. A role for the
developing lexicon in phonetic category acquisition.
Psychological Review.
Margaret M. Fleck. 2008. Lexicalized phonotactic word
segmentation. In Proceedings ofACL-08: HLT, pages
130–138, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Michael C. Frank, Sharon Goldwater, Thomas L. Griffiths,
and Joshua B. Tenenbaum. 2010. Modeling human per-
formance in statistical word segmentation. Cognition,
117(2):107–125.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21–54.
Rebecca G´omez and Jessica Maye. 2005. The develop-
mental trajectory of nonadjacent dependency learning.
Infancy, 7:183–206.
Bruce Hayes and Colin Wilson. 2008. A maximum en-
tropy model of phonotactics and phonotactic learning.
Linguistic Inquiry, 39(3):379–440.
Marieke van Heugten and Elizabeth K. Johnson. in press.
Learning to contend with accents in infancy: Benefits
of brief speaker exposure. Journal of Experimental
Psychology: General.
Derek M. Houston and Peter W. Jusczyk. 2000. The role
of talker-specific information in word segmentation by
infants. Journal of Experimental Psychology: Human
Perception and Performance, 26:1570–1582.
Jonathan Huggins and Frank Wood. 2013. Infinite struc-
tured hidden semi-Markov models. Transactions on
Pattern Analysis and Machine Intelligence (TPAMI), to
appear, September.
Aren Jansen, Emmanuel Dupoux, Sharon Goldwater,
Mark Johnson, Sanjeev Khudanpur, Kenneth Church,
Naomi Feldman, Hynek Hermansky, Florian Metze,
Richard Rose, Mike Seltzer, Pascal Clark, Ian McGraw,
Balakrishnan Varadarajan, Erin Bennett, Benjamin
Borschinger, Justin Chiu, Ewan Dunbar, Abdellah Four-
tassi, David Harwath, Chia-ying Lee, Keith Levin,
Atta Norouzian, Vijay Peddinti, Rachael Richardson,
Thomas Schatz, and Samuel Thomas. 2013. A sum-
mary of the 2012 JHU CLSP workshop on zero re-
source speech technologies and early language acqui-
sition. Proceedings of the IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing.
</reference>
<page confidence="0.973676">
52
</page>
<reference confidence="0.999765775700935">
Mark Johnson. 2008. Using adaptor grammars to identify
synergies in the unsupervised acquisition of linguis-
tic structure. In Proceedings of ACL-08: HLT, pages
398–406, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Peter W. Jusczyk and Richard N. Aslin. 1995. Infants’ de-
tection of the sound patterns of words in fluent speech.
Cognitive Psychology, 29:1–23.
Peter W. Jusczyk, Derek M. Houston, and Mary Newsome.
1999. The beginnings of word segmentation in English-
learning infants. Cognitive Psychology, 39:159–207.
Dahee Kim, Joseph D.W. Stephens, and Mark A. Pitt.
2012. How does context play a part in splitting words
apart? Production and perception of word boundaries
in casual speech. Journal of Memory and Language,
66(4):509 – 529.
Patricia K. Kuhl, Karen A. Williams, Francisco Lacerda,
Kenneth N. Stevens, and Bjorn Lindblom. 1992. Lin-
guistic experience alters phonetic perception in infants
by 6 months of age. Science, 255(5044):606–608.
Leigh Lisker and Arthur S. Abramson. 1964. A cross-
language study of voicing in initial stops: Acoustical
measurements. Word, 20:384–422.
Andrew Martin, Sharon Peperkamp, and Emmanuel
Dupoux. 2013. Learning phonemes with a proto-
lexicon. Cognitive Science, 37:103–124.
Sven L. Mattys and Peter W. Jusczyk. 2001. Do infants
segment words or recurring contiguous patterns? Jour-
nal of Experimental Psychology: Human Perception
and Performance, 27(3):644–655+.
Jessica Maye, Janet F. Werker, and LouAnn Gerken. 2002.
Infant sensitivity to distributional information can affect
phonetic discrimination. Cognition, 82(3):B101–11.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested pitman-yor language modeling. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
100–108, Suntec, Singapore, August. Association for
Computational Linguistics.
Mehryar Mohri, 2004. Weighted Finite-State Transducer
Algorithms: An Overview, chapter 29, pages 551–564.
Physica-Verlag.
Thierry Nazzi, Laura C. Dilley, Ann Marie Jusczyk, Ste-
fanie Shattuck-Hufnagel, and Peter W. Jusczyk. 2005.
English-learning infants’ segmentation of verbs from
fluent speech. Language and Speech, 48(3):279–298+.
Graham Neubig, Masato Mimura, Shinsuke Mori, and
Tatsuya Kawahara. 2010. Learning a language model
from continuous speech. In 11th Annual Conference
of the International Speech Communication Associa-
tion (InterSpeech 2010), pages 1053–1056, Makuhari,
Japan, 9.
Sharon Peperkamp, Rozenn Le Calvez, Jean-Pierre Nadal,
and Emmanuel Dupoux. 2006. The acquisition of
allophonic rules: Statistical learning with linguistic
constraints. Cognition, 101(3):B31–B41.
Ann M. Peters. 1983. The Units of Language Acquisi-
tion. Cambridge Monographs and Texts in Applied
Psycholinguistics. Cambridge University Press.
Gordon E. Peterson and Harold L. Barney. 1952. Control
methods used in a study of the vowels. Journal of the
Acoustical Society of America, 24(2):175–184.
Mark A. Pitt, Laura Dilley, Keith Johnson, Scott Kies-
ling, William Raymond, Elizabeth Hume, and Eric
Fosler-Lussier. 2007. Buckeye corpus of conversa-
tional speech (2nd release).
Kim Plunkett. 2005. Learning how to be flexible with
words. Attention and Performance, XXI:233–248.
Anton Rytting. 2007. Preserving Subsegmental Varia-
tion in Modeling Word Segmentation (Or, the Raising
of Baby Mondegreen). Ph.D. thesis, The Ohio State
University.
Amanda Seidl and Elizabeth Johnson. 2006. Infant word
segmentation revisited: Edge alignment facilitates tar-
get extraction. Developmental Science, 9:565–573.
Amanda Seidl and Elizabeth Johnson. 2008. Perceptual
factors influence infants’ extraction of onsetless words
from continuous speech. Journal of Child Language,
34.
Leher Singh, James Morgan, and Katherine White. 2004.
Preference and processing: The role of speech affect
in early spoken word recognition. Journal of Memory
and Language, 51:173–189.
Daniel Swingley. 2005. Statistical clustering and the con-
tents of the infant vocabulary. Cognitive Psychology,
50:86–132.
Daniel Swingley. 2009. Contributions of infant word
learning to language development. Philosophical
Transactions of the Royal Society B: Biological Sci-
ences, 364(1536):3617–3632, December.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Ameri-
can Statistical Association, 101(476):1566–1581.
Michael Tomasello. 2000. The item-based nature of chil-
dren’s early syntactic development. Trends in Cognitive
Sciences, 4(4):156 – 163.
Gautam K. Vallabha, James L. McClelland, Ferran Pons,
Janet F. Werker, and Shigeaki Amano. 2007. Unsuper-
vised learning of vowel categories from infant-directed
speech. Proceedings of the National Academy of Sci-
ences, 104(33):13273–13278.
Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam sampling for the
infinite Hidden Markov model. In Proceedings of the
25th International Conference on Machine learning,
</reference>
<page confidence="0.987349">
53
</page>
<reference confidence="0.995402294117647">
ICML ’08, pages 1088–1095, New York, NY, USA.
ACM.
Balakrishnan Varadarajan, Sanjeev Khudanpur, and Em-
manuel Dupoux. 2008. Unsupervised learning of
acoustic sub-word units. In Proceedings of the As-
sociation for Computational Linguistics: Short Papers,
pages 165–168.
Anand Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational Lin-
guistics, 27(3):351–372.
Janet F. Werker and Richard C. Tees. 1984. Cross-
language speech perception: Evidence for perceptual
reorganization during the first year of life. Infant Be-
havior and Development, 7(1):49 – 63.
Katherine S. White and Richard N. Aslin. 2011. Adap-
tation to novel accents by toddlers. Developmental
Science, 14(2):372–384.
</reference>
<page confidence="0.999015">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.762501">
<title confidence="0.9814975">A Joint Learning Model of Word Segmentation, Lexical and Phonetic Variability</title>
<author confidence="0.994698">Micha</author>
<affiliation confidence="0.9684615">Dept. of The Ohio State University</affiliation>
<author confidence="0.99473">H Naomi</author>
<affiliation confidence="0.999324">Dept. of University of Maryland</affiliation>
<abstract confidence="0.99252275">We present a cognitive model of early lexical acquisition which jointly performs word segmentation and learns an explicit model of phonetic variation. We define the model as a Bayesian noisy channel; we sample segmentations and word forms simultaneously from the posterior, using beam sampling to control the size of the search space. Compared to a pipelined approach in which segmentation is performed first, our model is qualitatively more similar to human learners. On data with variable pronunciations, the pipelined approach learns to treat syllables or morphemes as words. In contrast, our joint model, like infant learners, tends to learn multiword collocations. We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lalit Bahl</author>
<author>Raimo Bakis</author>
<author>Frederick Jelinek</author>
<author>Robert Mercer</author>
</authors>
<title>Language-model/acoustic-channelmodel balance mechanism.</title>
<date>1980</date>
<journal>Technical disclosure bulletin</journal>
<volume>23</volume>
<contexts>
<context position="18446" citStr="Bahl et al., 1980" startWordPosition="3037" endWordPosition="3040">[wO] since the continuation will fail.) The algorithm’s speed depends on the size and uncertainty of the inferred LM: large numbers of plausible words mean more states to explore. When inference starts, and the system is highly uncertain about word boundaries, it is therefore reasonable to limit the exploration of the character sequence. We do so by annealing in two ways: as in Goldwater et al. (2009), we raise P(X) (Eq. 3) to a power t which increases linearly from .3. To sample from the posterior, we would want to end with t = 1, but as in previous noisy-channel models (Elsner et al., 2012; Bahl et al., 1980) we get better results when we emphasize the LM at the expense of the channel and so end at t = 2. Meanwhile, as t rises and we explore fewer implausible lexical sequences, we can explore the character sequence more. We begin by setting the A interpolation parameter of Pu to 0 to minimize exploration and increase it linearly to .3 (allowing the system to change about a third of the characters 4.3 Dataset and metrics We use the corpus released by Elsner et al. (2012), which contains 9790 child-directed English utterances originally from the Bernstein-Ratner corpus (Bernstein-Ratner, 1987) and l</context>
</contexts>
<marker>Bahl, Bakis, Jelinek, Mercer, 1980</marker>
<rawString>Lalit Bahl, Raimo Bakis, Frederick Jelinek, and Robert Mercer. 1980. Language-model/acoustic-channelmodel balance mechanism. Technical disclosure bulletin Vol. 23, No. 7b, IBM, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew J Beal</author>
<author>Zoubin Ghahramani</author>
<author>Carl Edward Rasmussen</author>
</authors>
<title>The infinite Hidden Markov Model. In</title>
<date>2001</date>
<booktitle>NIPS,</booktitle>
<pages>577--584</pages>
<contexts>
<context position="14057" citStr="Beal et al., 2001" startWordPosition="2259" endWordPosition="2262">ot need to add context to the Geom state to remember the sequence of characters it produced, which allows us to keep only a single Geom state on the chart at each timestep. When we compose this model with the channel model, the number of states expands. Each state must now keep track of the previous word, what intended characters C have been posited and what surface characters S have been recognized, ST:[w][C ][S]. 2Though not mentioned by Mochihashi et al. (2009) or Neubig et al. (2010), this construction is not exact, since transitions in a Bayesian HMM are exchangeable but not independent (Beal et al., 2001): if a word occurs twice in an utterance, its probability is slightly higher the second time. For single utterances, this bias is small and easy to correct for using a Metropolis-Hastings acceptance check (B¨orschinger and Johnson, 2012) using the path probability from the HMM as the proposal. To recognize the current word, we transition to ST:[C ][][] with probability P(xi = C|xi_1 = w). To parse a new surface character s by positing intended character x (note that x might be e), we transition to ST:[w][C : x][S : 8] with probability T(s|x). (As above, we pay no cost for our choice of x, whic</context>
</contexts>
<marker>Beal, Ghahramani, Rasmussen, 2001</marker>
<rawString>Matthew J. Beal, Zoubin Ghahramani, and Carl Edward Rasmussen. 2001. The infinite Hidden Markov Model. In NIPS, pages 577–584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elika Bergelson</author>
<author>Daniel Swingley</author>
</authors>
<title>At 6-9 months, human infants know the meanings of many common nouns.</title>
<date>2012</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<pages>109--3253</pages>
<contexts>
<context position="1469" citStr="Bergelson and Swingley, 2012" startWordPosition="220" endWordPosition="223">st, our joint model, like infant learners, tends to learn multiword collocations. We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence. 1 Introduction By the end of their first year, infants have acquired many of the basic elements of their native language. Their sensitivity to phonetic contrasts has become language-specific (Werker and Tees, 1984), and they have begun detecting words in fluent speech (Jusczyk and Aslin, 1995; Jusczyk et al., 1999) and learning word meanings (Bergelson and Swingley, 2012). These developmental cooccurrences lead some researchers to propose that phonetic and word learning occur jointly, each one informing the other (Swingley, 2009; Feldman et al., 2013). Previous computational models capture some aspects of this joint learning Sharon Goldwater sgwater@inf.ed.ac.uk ILCC, School of Informatics University of Edinburgh Frank Wood fwood@robots.ox.ac.uk Dept. of Engineering University of Oxford problem, but typically simplify the problem considerably, either by assuming an unrealistic degree of phonetic regularity for word segmentation (Goldwater et al., 2009) or assu</context>
</contexts>
<marker>Bergelson, Swingley, 2012</marker>
<rawString>Elika Bergelson and Daniel Swingley. 2012. At 6-9 months, human infants know the meanings of many common nouns. Proceedings of the National Academy of Sciences, 109:3253–3258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Bernstein-Ratner</author>
</authors>
<title>The phonology of parentchild speech.</title>
<date>1987</date>
<booktitle>Children’s Language,</booktitle>
<volume>6</volume>
<editor>In K. Nelson and A. van Kleeck, editors,</editor>
<publisher>Erlbaum,</publisher>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="19040" citStr="Bernstein-Ratner, 1987" startWordPosition="3139" endWordPosition="3140">al., 2012; Bahl et al., 1980) we get better results when we emphasize the LM at the expense of the channel and so end at t = 2. Meanwhile, as t rises and we explore fewer implausible lexical sequences, we can explore the character sequence more. We begin by setting the A interpolation parameter of Pu to 0 to minimize exploration and increase it linearly to .3 (allowing the system to change about a third of the characters 4.3 Dataset and metrics We use the corpus released by Elsner et al. (2012), which contains 9790 child-directed English utterances originally from the Bernstein-Ratner corpus (Bernstein-Ratner, 1987) and later transcribed phonemically (Brent, 1999). This standard word segmentation dataset was modified by Elsner et al. (2012) to include phonetic variation by assigning each token a pronunciation independently selected from the empirical distribution of pronunciations of that word type in the closely-transcribed Buckeye Speech Corpus (Pitt et al., 2007). Following previous work, we hold out the last 1790 utterances as unseen test data during development. In the results presented here, we run the model on all 9790 utterances but score only these 1790. We average results over 5 runs of the mod</context>
</contexts>
<marker>Bernstein-Ratner, 1987</marker>
<rawString>Nan Bernstein-Ratner. 1987. The phonology of parentchild speech. In K. Nelson and A. van Kleeck, editors, Children’s Language, volume 6. Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine T Best</author>
<author>Michael D Tyler</author>
<author>Tiffany N Gooding</author>
<author>Corey B Orlando</author>
<author>Chelsea A Quann</author>
</authors>
<title>Development of phonological constancy: Toddlers’ perception of native- and jamaican-accented words.</title>
<date>2009</date>
<journal>Psychological Science,</journal>
<volume>20</volume>
<issue>5</issue>
<contexts>
<context position="34338" citStr="Best et al., 2009" startWordPosition="5576" endWordPosition="5579">ibutional information (Maye et al., 2002), which is much stronger for stop consonants than for vowels (Lisker and Abramson, 1964; Peterson and Barney, 1952). Infants are also conservative in generalizing across phonetic variability, showing a delayed abil49 ity to generalize across talkers, affects, and dialects. They have difficulty recognizing word tokens that are spoken by a different talker or in a different tone of voice until 11 months (Houston and Jusczyk, 2000; Singh et al., 2004), and the ability to adapt to unfamiliar dialects appears to develop even later, between 15 and 19 months (Best et al., 2009; Heugten and Johnson, in press; White and Aslin, 2011). Similar to infants, our model shows both a vowelconsonant asymmetry and a reluctance to accept the full range of adult phonetic variability. Table 5 shows some segment-to-segment alternations learned in various transducers. The oracle learns a large amount of variation (u surfaces as itself only 68% of the time) involving many different segments, whereas EM is similar to infant learners in learning a more conservative solution with fewer alternations overall. Moreover, EM appears to identify patterns of variability in vowels before conso</context>
</contexts>
<marker>Best, Tyler, Gooding, Orlando, Quann, 2009</marker>
<rawString>Catherine T. Best, Michael D. Tyler, Tiffany N. Gooding, Corey B. Orlando, and Chelsea A. Quann. 2009. Development of phonological constancy: Toddlers’ perception of native- and jamaican-accented words. Psychological Science, 20(5):539–542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin B¨orschinger</author>
<author>Mark Johnson</author>
</authors>
<title>Using rejuvenation to improve particle filtering for Bayesian word segmentation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>85--89</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<marker>B¨orschinger, Johnson, 2012</marker>
<rawString>Benjamin B¨orschinger and Mark Johnson. 2012. Using rejuvenation to improve particle filtering for Bayesian word segmentation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 85–89, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin B¨orschinger</author>
<author>Mark Johnson</author>
<author>Katherine Demuth</author>
</authors>
<title>A joint model of word segmentation and phonological variation for English word-final /t/-deletion.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>B¨orschinger, Johnson, Demuth, 2013</marker>
<rawString>Benjamin B¨orschinger, Mark Johnson, and Katherine Demuth. 2013. A joint model of word segmentation and phonological variation for English word-final /t/-deletion. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luc Boruta</author>
<author>Sharon Peperkamp</author>
<author>Benoit Crabb´e</author>
<author>Emmanuel Dupoux</author>
</authors>
<title>Testing the robustness of online word segmentation: Effects of linguistic diversity and phonetic variation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics,</booktitle>
<pages>1--9</pages>
<marker>Boruta, Peperkamp, Crabb´e, Dupoux, 2011</marker>
<rawString>Luc Boruta, Sharon Peperkamp, Benoit Crabb´e, and Emmanuel Dupoux. 2011. Testing the robustness of online word segmentation: Effects of linguistic diversity and phonetic variation. In Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Bosch</author>
<author>N´uria Sebasti´an-Gall´es</author>
</authors>
<title>Simultaneous bilingualism and the perception of a languagespecific vowel contrast in the first year of life.</title>
<date>2003</date>
<journal>Language and Speech,</journal>
<pages>46--2</pages>
<marker>Bosch, Sebasti´an-Gall´es, 2003</marker>
<rawString>Laura Bosch and N´uria Sebasti´an-Gall´es. 2003. Simultaneous bilingualism and the perception of a languagespecific vowel contrast in the first year of life. Language and Speech, 46(2-3):217–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
</authors>
<title>An efficient, probabilistically sound algorithm for segmentation and word discovery.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--71</pages>
<contexts>
<context position="4631" citStr="Brent, 1999" startWordPosition="690" endWordPosition="691">the model’s phonetic and lexical representations in detail, drawing comparisons to experimental results on adult and infant speech processing. Taken together, our results support the idea that a Bayesian model that jointly performs word segmentation and phonetic learning provides a plausible explanation for many aspects of early phonetic and word learning in infants. 2 Related Work Nearly all computational models used to explore the problems addressed here have treated the learning tasks in isolation. Examples include models of word segmentation from phonemic input (Christiansen et al., 1998; Brent, 1999; Venkataraman, 2001; Swingley, 2005) or phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2011; Boruta et al., 2011), models of phonetic clustering (Vallabha et al., 2007; Varadarajan et al., 2008; Dupoux et al., 2011) and phonological rule learning (Peperkamp et al., 2006; Martin et al., 2013). Elsner et al. (2012) present a model that is similar to ours, using a noisy channel model implemented with a finite-state transducer to learn about phonetic variability while clustering distinct tokens into lexical items. However (like the earlier lexical-phonetic learning model of</context>
<context position="19089" citStr="Brent, 1999" startWordPosition="3146" endWordPosition="3147">emphasize the LM at the expense of the channel and so end at t = 2. Meanwhile, as t rises and we explore fewer implausible lexical sequences, we can explore the character sequence more. We begin by setting the A interpolation parameter of Pu to 0 to minimize exploration and increase it linearly to .3 (allowing the system to change about a third of the characters 4.3 Dataset and metrics We use the corpus released by Elsner et al. (2012), which contains 9790 child-directed English utterances originally from the Bernstein-Ratner corpus (Bernstein-Ratner, 1987) and later transcribed phonemically (Brent, 1999). This standard word segmentation dataset was modified by Elsner et al. (2012) to include phonetic variation by assigning each token a pronunciation independently selected from the empirical distribution of pronunciations of that word type in the closely-transcribed Buckeye Speech Corpus (Pitt et al., 2007). Following previous work, we hold out the last 1790 utterances as unseen test data during development. In the results presented here, we run the model on all 9790 utterances but score only these 1790. We average results over 5 runs of the model with different random seeds. We use standard m</context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>Michael R. Brent. 1999. An efficient, probabilistically sound algorithm for segmentation and word discovery. Machine Learning, 34:71–105, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sally Butterfield</author>
<author>Anne Cutler</author>
</authors>
<title>Segmentation errors by human listeners: Evidence for a prosodic segmentation strategy.</title>
<date>1988</date>
<booktitle>In Proceedings of SPEECH ‘88: Seventh Symposium of the Federation of Acoustic Societies of Europe,</booktitle>
<volume>3</volume>
<pages>827--833</pages>
<location>Edinburgh.</location>
<contexts>
<context position="36354" citStr="Butterfield and Cutler, 1988" startWordPosition="5904" endWordPosition="5907">ers (although i and d still vary more than k). 5.5 Segmentation and recognition errors A particularly interesting set of errors are those that involve both a missegmentation and a simultaneous misrecognition, since the joint model is prone to such errors while the pipelined model is not. Relatively little is known about infants’ misrecognitions of words in fluent speech, although it is clear that they find words in medial position harder (Plunkett, 2005; Seidl and Johnson, 2006). However, adults make missegmentation/misrecognition errors fairly often, especially when listening to noisy audio (Butterfield and Cutler, 1988). Such errors are more common top 4 outputs s u .68 @ .05 a .04 U .04 i .85 I .03 @ .03 E .02 d .69 s .07 [0] .07 z .04 k .93 d .02 g .02 r .21 h .11 d .01 @ .07 u .75 @ .08 I .04 U .03 i .90 I .04 E .02 d .91 s .03 z 0.1 k .98 @ .32 I .14 n .13 t .13 u .82 I .04 @ .04 a .02 i .97 d .95 k .99 @ .21 I .18 t .12 s .12 Table 5: Learned phonetic alternations: top 4 outputs s with p &gt; .001 for inputs x = uw (/u/), iy (/i/), dh (/d/), k (/k/) and [0], the null character. Outputs from [0] are insertions. The oracle allows [0] as an output (deletion) but for computational reasons, the model does not. </context>
</contexts>
<marker>Butterfield, Cutler, 1988</marker>
<rawString>Sally Butterfield and Anne Cutler. 1988. Segmentation errors by human listeners: Evidence for a prosodic segmentation strategy. In Proceedings of SPEECH ‘88: Seventh Symposium of the Federation of Acoustic Societies of Europe, vol. 3, pages 827–833, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morten H Christiansen</author>
<author>Joseph Allen</author>
<author>Mark S Seidenberg</author>
</authors>
<title>Learning to Segment Speech Using Multiple Cues: A Connectionist Model. Language and Cognitive Processes,</title>
<date>1998</date>
<pages>13--2</pages>
<contexts>
<context position="4618" citStr="Christiansen et al., 1998" startWordPosition="686" endWordPosition="689"> et al., 2012). We analyze the model’s phonetic and lexical representations in detail, drawing comparisons to experimental results on adult and infant speech processing. Taken together, our results support the idea that a Bayesian model that jointly performs word segmentation and phonetic learning provides a plausible explanation for many aspects of early phonetic and word learning in infants. 2 Related Work Nearly all computational models used to explore the problems addressed here have treated the learning tasks in isolation. Examples include models of word segmentation from phonemic input (Christiansen et al., 1998; Brent, 1999; Venkataraman, 2001; Swingley, 2005) or phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2011; Boruta et al., 2011), models of phonetic clustering (Vallabha et al., 2007; Varadarajan et al., 2008; Dupoux et al., 2011) and phonological rule learning (Peperkamp et al., 2006; Martin et al., 2013). Elsner et al. (2012) present a model that is similar to ours, using a noisy channel model implemented with a finite-state transducer to learn about phonetic variability while clustering distinct tokens into lexical items. However (like the earlier lexical-phonetic lear</context>
</contexts>
<marker>Christiansen, Allen, Seidenberg, 1998</marker>
<rawString>Morten H. Christiansen, Joseph Allen, and Mark S. Seidenberg. 1998. Learning to Segment Speech Using Multiple Cues: A Connectionist Model. Language and Cognitive Processes, 13(2/3):221–269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Connine</author>
<author>D Titone</author>
<author>J Wang</author>
</authors>
<title>Auditory word recognition: Extrinsic and intrinsic effects of word frequency.</title>
<date>1993</date>
<journal>Journal of Experimental Psychology: Learning, Memory and Cognition,</journal>
<pages>19--81</pages>
<contexts>
<context position="37225" citStr="Connine et al., 1993" startWordPosition="6098" endWordPosition="6101">3 u .82 I .04 @ .04 a .02 i .97 d .95 k .99 @ .21 I .18 t .12 s .12 Table 5: Learned phonetic alternations: top 4 outputs s with p &gt; .001 for inputs x = uw (/u/), iy (/i/), dh (/d/), k (/k/) and [0], the null character. Outputs from [0] are insertions. The oracle allows [0] as an output (deletion) but for computational reasons, the model does not. when the misrecognized word belongs to a prosodically rare class and when the incorrectly hypothesized string contains frequent words (Cutler, 1990); phonetically ambiguous words are also more commonly recognized as the more frequent of two options (Connine et al., 1993). For the indefinite article “a” (often reduced to [@]), lexical context is the main factor in deciding between ambiguous interpretations (Kim et al., 2012). In rapid speech, listeners have few phonetic cues to indicate whether it is present at all (Dilley and Pitt, 2010). Below, we analyze various misrecognitions made by our system (using the EM transducer), and find some similar effects. The easiest cases to analyze are those with no missegmentation: the proposed boundaries are correct, and the proposed lexical entry corresponds to a real word7, but not the correct one. Most of them correspo</context>
</contexts>
<marker>Connine, Titone, Wang, 1993</marker>
<rawString>C. M. Connine, D. Titone, and J. Wang. 1993. Auditory word recognition: Extrinsic and intrinsic effects of word frequency. Journal of Experimental Psychology: Learning, Memory and Cognition, 19:81–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Cutler</author>
</authors>
<title>Exploiting prosodic probabilities in speech segmentation.</title>
<date>1990</date>
<booktitle>Cognitive models of speech processing: Psycholinguistic and computational perspectives,</booktitle>
<pages>105--121</pages>
<editor>In G. A. Altmann, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="37102" citStr="Cutler, 1990" startWordPosition="6079" endWordPosition="6080">02 r .21 h .11 d .01 @ .07 u .75 @ .08 I .04 U .03 i .90 I .04 E .02 d .91 s .03 z 0.1 k .98 @ .32 I .14 n .13 t .13 u .82 I .04 @ .04 a .02 i .97 d .95 k .99 @ .21 I .18 t .12 s .12 Table 5: Learned phonetic alternations: top 4 outputs s with p &gt; .001 for inputs x = uw (/u/), iy (/i/), dh (/d/), k (/k/) and [0], the null character. Outputs from [0] are insertions. The oracle allows [0] as an output (deletion) but for computational reasons, the model does not. when the misrecognized word belongs to a prosodically rare class and when the incorrectly hypothesized string contains frequent words (Cutler, 1990); phonetically ambiguous words are also more commonly recognized as the more frequent of two options (Connine et al., 1993). For the indefinite article “a” (often reduced to [@]), lexical context is the main factor in deciding between ambiguous interpretations (Kim et al., 2012). In rapid speech, listeners have few phonetic cues to indicate whether it is present at all (Dilley and Pitt, 2010). Below, we analyze various misrecognitions made by our system (using the EM transducer), and find some similar effects. The easiest cases to analyze are those with no missegmentation: the proposed boundar</context>
</contexts>
<marker>Cutler, 1990</marker>
<rawString>Anne Cutler. 1990. Exploiting prosodic probabilities in speech segmentation. In G. A. Altmann, editor, Cognitive models of speech processing: Psycholinguistic and computational perspectives, pages 105–121. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Daland</author>
<author>Janet B Pierrehumbert</author>
</authors>
<title>Learning diphone-based segmentation.</title>
<date>2011</date>
<journal>Cognitive Science,</journal>
<volume>35</volume>
<issue>1</issue>
<contexts>
<context position="4746" citStr="Daland and Pierrehumbert, 2011" startWordPosition="704" endWordPosition="707">results on adult and infant speech processing. Taken together, our results support the idea that a Bayesian model that jointly performs word segmentation and phonetic learning provides a plausible explanation for many aspects of early phonetic and word learning in infants. 2 Related Work Nearly all computational models used to explore the problems addressed here have treated the learning tasks in isolation. Examples include models of word segmentation from phonemic input (Christiansen et al., 1998; Brent, 1999; Venkataraman, 2001; Swingley, 2005) or phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2011; Boruta et al., 2011), models of phonetic clustering (Vallabha et al., 2007; Varadarajan et al., 2008; Dupoux et al., 2011) and phonological rule learning (Peperkamp et al., 2006; Martin et al., 2013). Elsner et al. (2012) present a model that is similar to ours, using a noisy channel model implemented with a finite-state transducer to learn about phonetic variability while clustering distinct tokens into lexical items. However (like the earlier lexical-phonetic learning model of Feldman et al. (2009; in press)) their model assumes known word boundaries, so to perform both segmentation and le</context>
</contexts>
<marker>Daland, Pierrehumbert, 2011</marker>
<rawString>Robert Daland and Janet B. Pierrehumbert. 2011. Learning diphone-based segmentation. Cognitive Science, 35(1):119–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura C Dilley</author>
<author>Mark Pitt</author>
</authors>
<title>Altering context speech rate can cause words to appear or disappear.</title>
<date>2010</date>
<journal>Psychological Science,</journal>
<volume>21</volume>
<issue>11</issue>
<contexts>
<context position="37497" citStr="Dilley and Pitt, 2010" startWordPosition="6142" endWordPosition="6145">[0] as an output (deletion) but for computational reasons, the model does not. when the misrecognized word belongs to a prosodically rare class and when the incorrectly hypothesized string contains frequent words (Cutler, 1990); phonetically ambiguous words are also more commonly recognized as the more frequent of two options (Connine et al., 1993). For the indefinite article “a” (often reduced to [@]), lexical context is the main factor in deciding between ambiguous interpretations (Kim et al., 2012). In rapid speech, listeners have few phonetic cues to indicate whether it is present at all (Dilley and Pitt, 2010). Below, we analyze various misrecognitions made by our system (using the EM transducer), and find some similar effects. The easiest cases to analyze are those with no missegmentation: the proposed boundaries are correct, and the proposed lexical entry corresponds to a real word7, but not the correct one. Most of them correspond to homophones (Table 6). Common cases with a missegmentation include it and is, a and is, it’s and is, who, who’s and whose, that’s and what’s, and there and there’s. In general, these errors involve words which sometimes appear 7The one-to-one mapping can be misleadin</context>
<context position="39670" citStr="Dilley and Pitt, 2010" startWordPosition="6523" endWordPosition="6526">on words which often appear in similar contexts. These tendencies match those shown by adult human listeners. A particularly distinctive set of joint recognition and segmentation errors are those where an entire real token is treated as phonetic “noise”— that is, it is segmented along with an adjacent word, and the system clusters the whole sequence as a token of that word. The most common examples are “that’s a” identified as “that’s”, “have a” identified as “have”, “sees a” identified as “sees” and other examples involving “a”, a word which also frequently confuses humans (Kim et al., 2012; Dilley and Pitt, 2010). However, there are also instances of “who’s in” as “who’s”, “does it” as “does”, and “can you” as “can”. 6 Conclusion We have presented a model that jointly infers word segmentation, lexical items, and a model of phonetic variability; we believe this is the first model to do so on a broad-coverage naturalistic corpus8. Our results show a small improvement in both segmentation and normalization over a pipeline model, providing evidence for a synergistic interaction between these learning tasks and supporting claims of interactive learning from the developmental literature on infants. We also </context>
</contexts>
<marker>Dilley, Pitt, 2010</marker>
<rawString>Laura C. Dilley and Mark Pitt. 2010. Altering context speech rate can cause words to appear or disappear. Psychological Science, 21(11):1664–1670.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason R Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Latent-variable modeling of string transductions with finite-state methods.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>1080--1089</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8780" citStr="Dreyer et al., 2008" startWordPosition="1376" endWordPosition="1379">ext Gx = P(Xi = ~|Xi_1 = x) using G0 as a prior. Finally, we sample word sequences x1 ... x,,, from the bigram model. The channel model is a finite transducer with parameters 0 which independently rewrites single characters from the intended string into characters of the surface string. We use MAP point estimates of these parameters; single characters (without n-gram context) are used for computational efficiency. Also for efficiency, the transducer can insert characters into the surface string, but cannot delete characters from the intended string. As in several previous phonological models (Dreyer et al., 2008; Hayes and Wilson, 2008), the probabilities are learned using a featurebased log-linear model. For features, we use all the unigram features from Elsner et al. (2012), which check faithfulness to voicing, place and manner of articulation (for example, for k ! g, active features are faith-manner, faith-place, output-g and voicelessto-voiced). Below, we present two methods for learning the transducer parameters 0. The oracle transducer is estimated using the gold-standard word segmentations and intended forms for the dataset; it represents the best possible approximation under our model of the </context>
</contexts>
<marker>Dreyer, Smith, Eisner, 2008</marker>
<rawString>Markus Dreyer, Jason R. Smith, and Jason Eisner. 2008. Latent-variable modeling of string transductions with finite-state methods. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 1080–1089, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Dupoux</author>
<author>Guillaume Beraud-Sudreau</author>
<author>Shigeki Sagayama</author>
</authors>
<title>Templatic features for modeling phoneme acquisition.</title>
<date>2011</date>
<booktitle>In Proceedings of the 33rd Annual Cognitive Science Society.</booktitle>
<contexts>
<context position="4870" citStr="Dupoux et al., 2011" startWordPosition="724" endWordPosition="727">s word segmentation and phonetic learning provides a plausible explanation for many aspects of early phonetic and word learning in infants. 2 Related Work Nearly all computational models used to explore the problems addressed here have treated the learning tasks in isolation. Examples include models of word segmentation from phonemic input (Christiansen et al., 1998; Brent, 1999; Venkataraman, 2001; Swingley, 2005) or phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2011; Boruta et al., 2011), models of phonetic clustering (Vallabha et al., 2007; Varadarajan et al., 2008; Dupoux et al., 2011) and phonological rule learning (Peperkamp et al., 2006; Martin et al., 2013). Elsner et al. (2012) present a model that is similar to ours, using a noisy channel model implemented with a finite-state transducer to learn about phonetic variability while clustering distinct tokens into lexical items. However (like the earlier lexical-phonetic learning model of Feldman et al. (2009; in press)) their model assumes known word boundaries, so to perform both segmentation and lexical-phonetic learning, they use a pipeline that first segments using GGJ and then applies their model to the results. Neub</context>
</contexts>
<marker>Dupoux, Beraud-Sudreau, Sagayama, 2011</marker>
<rawString>Emmanuel Dupoux, Guillaume Beraud-Sudreau, and Shigeki Sagayama. 2011. Templatic features for modeling phoneme acquisition. In Proceedings of the 33rd Annual Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Sharon Goldwater</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Bootstrapping a unified model of lexical and phonetic acquisition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>184--193</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2200" citStr="Elsner et al., 2012" startWordPosition="323" endWordPosition="326">tly, each one informing the other (Swingley, 2009; Feldman et al., 2013). Previous computational models capture some aspects of this joint learning Sharon Goldwater sgwater@inf.ed.ac.uk ILCC, School of Informatics University of Edinburgh Frank Wood fwood@robots.ox.ac.uk Dept. of Engineering University of Oxford problem, but typically simplify the problem considerably, either by assuming an unrealistic degree of phonetic regularity for word segmentation (Goldwater et al., 2009) or assuming pre-segmented input for phonetic and lexical acquisition (Feldman et al., 2009; Feldman et al., in press; Elsner et al., 2012). This paper presents, to our knowledge, the first broadcoverage model that learns to segment phonetically variable input into words, while simultaneously learning an explicit model of phonetic variation that allows it to cluster together segmented tokens with different phonetic realizations (e.g., [ju] and [jI]) into lexical items (/ju/). We base our model on the Bayesian word segmentation model of Goldwater et al. (2009) (henceforth GGJ), using a noisy-channel setup where phonetic variation is introduced by a finite-state transducer (Neubig et al., 2010; Elsner et al., 2012). This integrated</context>
<context position="4007" citStr="Elsner et al., 2012" startWordPosition="596" endWordPosition="599">egment. Here, we demonstrate that when the model is augmented to account for phonetic variability, it is able to learn common phonetic changes 42 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 42–54, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics and by doing so, its accuracy improves and its errors return to the more human-like undersegmentation pattern. In addition, we find small improvements in lexicon accuracy over a pipeline model that segments first and then performs lexical-phonetic learning (Elsner et al., 2012). We analyze the model’s phonetic and lexical representations in detail, drawing comparisons to experimental results on adult and infant speech processing. Taken together, our results support the idea that a Bayesian model that jointly performs word segmentation and phonetic learning provides a plausible explanation for many aspects of early phonetic and word learning in infants. 2 Related Work Nearly all computational models used to explore the problems addressed here have treated the learning tasks in isolation. Examples include models of word segmentation from phonemic input (Christiansen e</context>
<context position="6723" citStr="Elsner et al., 2012" startWordPosition="1014" endWordPosition="1017">istinct observations; they are concatenated together into a continuous sequence of characters which constitute the observations. joint learner for segmentation, phonetic learning, and lexical clustering, but the model and inference are tailored to investigate word-final /t/-deletion, rather than aiming for a broad coverage system as we do. 3 Model We follow several previous models of lexical acquisition in adopting a Bayesian noisy channel framework (Eq. 1-4; Fig. 1). The model has two components: a source distribution P(X) over utterances without phonetic variability X, i.e., intended forms (Elsner et al., 2012) and a channel or noise distribution T(S|X) that translates them into the observed surface forms S. The boundaries between surface forms are then deterministically removed so that the actual observations are just the unsegmented string of characters in the surface forms. G0|α0, pstop — DP(α0,Geom(pstop)) G.|G0, α1 — DP (α1, G0) Xi|Xi−1 — GXi_1 S|X; 0 — T(S|X; 0) The source model is an exact copy of GGJ1: to generate the intended-form word sequences X, we 1We use their best reported parameter values: α0 = 3000, α1 = 100, pstap = .2 and for unigrams, α0 = 20. Generator for possible words Geoma, </context>
<context position="8947" citStr="Elsner et al. (2012)" startWordPosition="1403" endWordPosition="1406">rameters 0 which independently rewrites single characters from the intended string into characters of the surface string. We use MAP point estimates of these parameters; single characters (without n-gram context) are used for computational efficiency. Also for efficiency, the transducer can insert characters into the surface string, but cannot delete characters from the intended string. As in several previous phonological models (Dreyer et al., 2008; Hayes and Wilson, 2008), the probabilities are learned using a featurebased log-linear model. For features, we use all the unigram features from Elsner et al. (2012), which check faithfulness to voicing, place and manner of articulation (for example, for k ! g, active features are faith-manner, faith-place, output-g and voicelessto-voiced). Below, we present two methods for learning the transducer parameters 0. The oracle transducer is estimated using the gold-standard word segmentations and intended forms for the dataset; it represents the best possible approximation under our model of the actual phonetics of the dataset. We can also estimate the transducer using the EM algorithm. We first initialize a simple transducer by putting small weights on the fa</context>
<context position="18426" citStr="Elsner et al., 2012" startWordPosition="3033" endWordPosition="3036">prefix ST:[xi−1 ][wa][wO] since the continuation will fail.) The algorithm’s speed depends on the size and uncertainty of the inferred LM: large numbers of plausible words mean more states to explore. When inference starts, and the system is highly uncertain about word boundaries, it is therefore reasonable to limit the exploration of the character sequence. We do so by annealing in two ways: as in Goldwater et al. (2009), we raise P(X) (Eq. 3) to a power t which increases linearly from .3. To sample from the posterior, we would want to end with t = 1, but as in previous noisy-channel models (Elsner et al., 2012; Bahl et al., 1980) we get better results when we emphasize the LM at the expense of the channel and so end at t = 2. Meanwhile, as t rises and we explore fewer implausible lexical sequences, we can explore the character sequence more. We begin by setting the A interpolation parameter of Pu to 0 to minimize exploration and increase it linearly to .3 (allowing the system to change about a third of the characters 4.3 Dataset and metrics We use the corpus released by Elsner et al. (2012), which contains 9790 child-directed English utterances originally from the Bernstein-Ratner corpus (Bernstein</context>
<context position="19999" citStr="Elsner et al. (2012)" startWordPosition="3288" endWordPosition="3291">Pitt et al., 2007). Following previous work, we hold out the last 1790 utterances as unseen test data during development. In the results presented here, we run the model on all 9790 utterances but score only these 1790. We average results over 5 runs of the model with different random seeds. We use standard metrics for segmentation and lexicon recovery. For segmentation, we report precision, recall and F-score for word boundaries (bds), and for the positions of word tokens in the surface string (srf; both boundaries must be correct). For normalization of the pronunciation variation, we follow Elsner et al. (2012) in measuring how well the system clusters together variant pronunciations of the same lexical item, without insisting that the intended form the system proposes for them match the one in our corpus. For example, if the system correctly clusters [ju] and [jI] together but assigns them the incorrect intended form /jI/, we can still give credit to this cluster if it is the one that overlaps best with the gold-standard /ju/ cluster. To compute these scores, we find the optimal one-to-one mapping between our clusters of pronunciations and the true lexical entries, then report scores for mapped tok</context>
<context position="27604" citStr="Elsner et al. (2012)" startWordPosition="4493" endWordPosition="4496"> the baseline at 44.8%5. Although bigram context is only moderately effective for telling where words are, the model seems heavily reliant on lexical context to decide what words it is hearing. 5.3 Error analysis To gain more insight into the differing behavior of our model versus a pipelined system, we inspect the intended word strings X proposed by each one in detail. Below, we categorize the kinds of intended word strings that the model might propose to span a given gold-standard word token: Correct Correctly segmented, mapped to the correct lexical item (e.g., gold intended /ju/, surface 5Elsner et al. (2012) show a similar result for a unigram version of their pipelined system. EM-learned Segment only Correct 49.88 47.61 Wrong form 17.96 23.73 Collocation 14.25 7.59 Split 8.26 15.18 One bound 7.11 15.18 Corr. colloc. 1.35 &lt; 0.01 Other 0.75 0.22 Corr. split 0.43 0.66 Table 2: Distribution (%) of error types (see text) in a single run on the full dataset. segmentation [ju], intended /ju/) Wrong form Correctly segmented, mapped to the wrong lexical item (/ju/, surf. [ju], int. /jEs/) Colloc Missegmented as part of a sequence whose boundaries correspond to real word boundaries (/ju•want/, surf. [juwa</context>
</contexts>
<marker>Elsner, Goldwater, Eisenstein, 2012</marker>
<rawString>Micha Elsner, Sharon Goldwater, and Jacob Eisenstein. 2012. Bootstrapping a unified model of lexical and phonetic acquisition. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 184–193, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naomi Feldman</author>
<author>Thomas Griffiths</author>
<author>James Morgan</author>
</authors>
<title>Learning phonetic categories by learning a lexicon.</title>
<date>2009</date>
<booktitle>In Proceedings of the 31st Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="2152" citStr="Feldman et al., 2009" startWordPosition="314" endWordPosition="317">opose that phonetic and word learning occur jointly, each one informing the other (Swingley, 2009; Feldman et al., 2013). Previous computational models capture some aspects of this joint learning Sharon Goldwater sgwater@inf.ed.ac.uk ILCC, School of Informatics University of Edinburgh Frank Wood fwood@robots.ox.ac.uk Dept. of Engineering University of Oxford problem, but typically simplify the problem considerably, either by assuming an unrealistic degree of phonetic regularity for word segmentation (Goldwater et al., 2009) or assuming pre-segmented input for phonetic and lexical acquisition (Feldman et al., 2009; Feldman et al., in press; Elsner et al., 2012). This paper presents, to our knowledge, the first broadcoverage model that learns to segment phonetically variable input into words, while simultaneously learning an explicit model of phonetic variation that allows it to cluster together segmented tokens with different phonetic realizations (e.g., [ju] and [jI]) into lexical items (/ju/). We base our model on the Bayesian word segmentation model of Goldwater et al. (2009) (henceforth GGJ), using a noisy-channel setup where phonetic variation is introduced by a finite-state transducer (Neubig et </context>
<context position="5252" citStr="Feldman et al. (2009" startWordPosition="783" endWordPosition="786"> Venkataraman, 2001; Swingley, 2005) or phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2011; Boruta et al., 2011), models of phonetic clustering (Vallabha et al., 2007; Varadarajan et al., 2008; Dupoux et al., 2011) and phonological rule learning (Peperkamp et al., 2006; Martin et al., 2013). Elsner et al. (2012) present a model that is similar to ours, using a noisy channel model implemented with a finite-state transducer to learn about phonetic variability while clustering distinct tokens into lexical items. However (like the earlier lexical-phonetic learning model of Feldman et al. (2009; in press)) their model assumes known word boundaries, so to perform both segmentation and lexical-phonetic learning, they use a pipeline that first segments using GGJ and then applies their model to the results. Neubig et al. (2010) also present a transducerbased noisy channel model that performs joint inference on two out of the three tasks we consider here; their model assumes fixed probabilities for phonetic changes (the noise model) and jointly infers the word segmentation and lexical items, as in our ‘oracle’ model below (though unlike our system their model learns from phone lattices r</context>
</contexts>
<marker>Feldman, Griffiths, Morgan, 2009</marker>
<rawString>Naomi Feldman, Thomas Griffiths, and James Morgan. 2009. Learning phonetic categories by learning a lexicon. In Proceedings of the 31st Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naomi H Feldman</author>
<author>Emily B Myers</author>
<author>Katherine S White</author>
<author>Thomas L Griffiths</author>
<author>James L Morgan</author>
</authors>
<title>Word-level information influences phonetic learning in adults and infants.</title>
<date>2013</date>
<journal>Cognition,</journal>
<volume>127</volume>
<issue>3</issue>
<contexts>
<context position="1652" citStr="Feldman et al., 2013" startWordPosition="247" endWordPosition="250">recognition errors, and relate these to developmental evidence. 1 Introduction By the end of their first year, infants have acquired many of the basic elements of their native language. Their sensitivity to phonetic contrasts has become language-specific (Werker and Tees, 1984), and they have begun detecting words in fluent speech (Jusczyk and Aslin, 1995; Jusczyk et al., 1999) and learning word meanings (Bergelson and Swingley, 2012). These developmental cooccurrences lead some researchers to propose that phonetic and word learning occur jointly, each one informing the other (Swingley, 2009; Feldman et al., 2013). Previous computational models capture some aspects of this joint learning Sharon Goldwater sgwater@inf.ed.ac.uk ILCC, School of Informatics University of Edinburgh Frank Wood fwood@robots.ox.ac.uk Dept. of Engineering University of Oxford problem, but typically simplify the problem considerably, either by assuming an unrealistic degree of phonetic regularity for word segmentation (Goldwater et al., 2009) or assuming pre-segmented input for phonetic and lexical acquisition (Feldman et al., 2009; Feldman et al., in press; Elsner et al., 2012). This paper presents, to our knowledge, the first b</context>
</contexts>
<marker>Feldman, Myers, White, Griffiths, Morgan, 2013</marker>
<rawString>Naomi H. Feldman, Emily B. Myers, Katherine S. White, Thomas L. Griffiths, and James L. Morgan. 2013. Word-level information influences phonetic learning in adults and infants. Cognition, 127(3):427–438.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Naomi H Feldman</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
<author>James L Morgan</author>
</authors>
<title>in press. A role for the developing lexicon in phonetic category acquisition. Psychological Review.</title>
<marker>Feldman, Griffiths, Goldwater, Morgan, </marker>
<rawString>Naomi H. Feldman, Thomas L. Griffiths, Sharon Goldwater, and James L. Morgan. in press. A role for the developing lexicon in phonetic category acquisition. Psychological Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret M Fleck</author>
</authors>
<title>Lexicalized phonotactic word segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>130--138</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="4699" citStr="Fleck, 2008" startWordPosition="700" endWordPosition="701">comparisons to experimental results on adult and infant speech processing. Taken together, our results support the idea that a Bayesian model that jointly performs word segmentation and phonetic learning provides a plausible explanation for many aspects of early phonetic and word learning in infants. 2 Related Work Nearly all computational models used to explore the problems addressed here have treated the learning tasks in isolation. Examples include models of word segmentation from phonemic input (Christiansen et al., 1998; Brent, 1999; Venkataraman, 2001; Swingley, 2005) or phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2011; Boruta et al., 2011), models of phonetic clustering (Vallabha et al., 2007; Varadarajan et al., 2008; Dupoux et al., 2011) and phonological rule learning (Peperkamp et al., 2006; Martin et al., 2013). Elsner et al. (2012) present a model that is similar to ours, using a noisy channel model implemented with a finite-state transducer to learn about phonetic variability while clustering distinct tokens into lexical items. However (like the earlier lexical-phonetic learning model of Feldman et al. (2009; in press)) their model assumes known word bou</context>
<context position="24763" citStr="Fleck, 2008" startWordPosition="4041" endWordPosition="4042">bigram models. Human learners also tend to learn collocations as single words (Peters, 1983; Tomasello, 2000), and the GGJ model has been shown to capture several other effects seen in laboratory segmentation tasks (Frank et al., 2010). Together, these findings support the idea that human learners may behave in important respects like the Bayesian ideal learners that Goldwater et al. presented. However, experiments on data with variation have called these conclusions into question. In particular, GGJ has previously been shown to oversegment rather than undersegment as the input grows noisier (Fleck, 2008), and our results replicate this finding (oversegmentation for the “segment only” model). In addition, the GGJ bigram model, which achieves much higher segmentation accuracy than the unigram model on clean data, actually performs worse on very noisy data (Jansen et al., 2013). Infants are known to track statistical dependencies across words (G´omez and Maye, 2005), so it is worrisome that these dependencies hurt GGJ’s segmentation accuracy when learning from noisy data. Our results show that modeling phonetic variability reverses the problematic trends described above. Although the models with</context>
</contexts>
<marker>Fleck, 2008</marker>
<rawString>Margaret M. Fleck. 2008. Lexicalized phonotactic word segmentation. In Proceedings ofACL-08: HLT, pages 130–138, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C Frank</author>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Modeling human performance in statistical word segmentation.</title>
<date>2010</date>
<journal>Cognition,</journal>
<volume>117</volume>
<issue>2</issue>
<contexts>
<context position="24386" citStr="Frank et al., 2010" startWordPosition="3983" endWordPosition="3986">ment) more. In previous experiments on datasets without variation, GGJ also has a strong tendency to undersegment the data (boundary P: 90.1, R: 80.3), which Goldwater et al. argue is rational behavior for an ideal learner seeking a parsimonious explanation for the data. Undersegmentation occurs especially when ignoring lexical context (a unigram model), but to some extent even in bigram models. Human learners also tend to learn collocations as single words (Peters, 1983; Tomasello, 2000), and the GGJ model has been shown to capture several other effects seen in laboratory segmentation tasks (Frank et al., 2010). Together, these findings support the idea that human learners may behave in important respects like the Bayesian ideal learners that Goldwater et al. presented. However, experiments on data with variation have called these conclusions into question. In particular, GGJ has previously been shown to oversegment rather than undersegment as the input grows noisier (Fleck, 2008), and our results replicate this finding (oversegmentation for the “segment only” model). In addition, the GGJ bigram model, which achieves much higher segmentation accuracy than the unigram model on clean data, actually pe</context>
</contexts>
<marker>Frank, Goldwater, Griffiths, Tenenbaum, 2010</marker>
<rawString>Michael C. Frank, Sharon Goldwater, Thomas L. Griffiths, and Joshua B. Tenenbaum. 2010. Modeling human performance in statistical word segmentation. Cognition, 117(2):107–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<issue>1</issue>
<contexts>
<context position="2061" citStr="Goldwater et al., 2009" startWordPosition="300" endWordPosition="304"> (Bergelson and Swingley, 2012). These developmental cooccurrences lead some researchers to propose that phonetic and word learning occur jointly, each one informing the other (Swingley, 2009; Feldman et al., 2013). Previous computational models capture some aspects of this joint learning Sharon Goldwater sgwater@inf.ed.ac.uk ILCC, School of Informatics University of Edinburgh Frank Wood fwood@robots.ox.ac.uk Dept. of Engineering University of Oxford problem, but typically simplify the problem considerably, either by assuming an unrealistic degree of phonetic regularity for word segmentation (Goldwater et al., 2009) or assuming pre-segmented input for phonetic and lexical acquisition (Feldman et al., 2009; Feldman et al., in press; Elsner et al., 2012). This paper presents, to our knowledge, the first broadcoverage model that learns to segment phonetically variable input into words, while simultaneously learning an explicit model of phonetic variation that allows it to cluster together segmented tokens with different phonetic realizations (e.g., [ju] and [jI]) into lexical items (/ju/). We base our model on the Bayesian word segmentation model of Goldwater et al. (2009) (henceforth GGJ), using a noisy-ch</context>
<context position="11103" citStr="Goldwater et al., 2009" startWordPosition="1756" endWordPosition="1759">rpret [w@nt], if we posit that the intended vowel is /A/, the word is likely to be /wAn/ “one” and the next word begins with /t/; if instead we posit that the vowel is /O/, the word is probably /wOnt/ “want”. Thus, inference methods that change only one character at a time are unlikely to mix well. Since they cannot simultaneously change the vowel and resegment the /t/, they must pass through a low-probability intermediate state to get from one state to the other, so will tend to get stuck in a bad local minimum. A Gibbs sampler which inserts or deletes a single segment boundary in each step (Goldwater et al., 2009) suffers from this problem. Mochihashi et al. (2009) describe an inference method with higher mobility: a block sampler for the GGJ model that samples from the posterior over analyses of a whole utterance at once. This method encodes the model as a large HMM, using dynamic programming to select an analysis. We encode our own model in the same way, constructing the HMM and composing it with the transducer (Mohri, 2004) to form a larger finite-state machine which is still amenable to forward-backward sampling. 4.1 Finite-state encoding Following Mochihashi et al. (2009) and Neubig et al. (2010),</context>
<context position="18232" citStr="Goldwater et al. (2009)" startWordPosition="2995" endWordPosition="2998">istance Markov model which runs faster than our full model. (For example, we can quickly check if the current U allows want as the intended form for wOlk at i; if not, we can avoid constructing the prefix ST:[xi−1 ][wa][wO] since the continuation will fail.) The algorithm’s speed depends on the size and uncertainty of the inferred LM: large numbers of plausible words mean more states to explore. When inference starts, and the system is highly uncertain about word boundaries, it is therefore reasonable to limit the exploration of the character sequence. We do so by annealing in two ways: as in Goldwater et al. (2009), we raise P(X) (Eq. 3) to a power t which increases linearly from .3. To sample from the posterior, we would want to end with t = 1, but as in previous noisy-channel models (Elsner et al., 2012; Bahl et al., 1980) we get better results when we emphasize the LM at the expense of the channel and so end at t = 2. Meanwhile, as t rises and we explore fewer implausible lexical sequences, we can explore the character sequence more. We begin by setting the A interpolation parameter of Pu to 0 to minimize exploration and increase it linearly to .3 (allowing the system to change about a third of the c</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2009. A Bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112(1):21–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca G´omez</author>
<author>Jessica Maye</author>
</authors>
<title>The developmental trajectory of nonadjacent dependency learning.</title>
<date>2005</date>
<journal>Infancy,</journal>
<pages>7--183</pages>
<marker>G´omez, Maye, 2005</marker>
<rawString>Rebecca G´omez and Jessica Maye. 2005. The developmental trajectory of nonadjacent dependency learning. Infancy, 7:183–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Hayes</author>
<author>Colin Wilson</author>
</authors>
<title>A maximum entropy model of phonotactics and phonotactic learning.</title>
<date>2008</date>
<journal>Linguistic Inquiry,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="8805" citStr="Hayes and Wilson, 2008" startWordPosition="1380" endWordPosition="1383">1 = x) using G0 as a prior. Finally, we sample word sequences x1 ... x,,, from the bigram model. The channel model is a finite transducer with parameters 0 which independently rewrites single characters from the intended string into characters of the surface string. We use MAP point estimates of these parameters; single characters (without n-gram context) are used for computational efficiency. Also for efficiency, the transducer can insert characters into the surface string, but cannot delete characters from the intended string. As in several previous phonological models (Dreyer et al., 2008; Hayes and Wilson, 2008), the probabilities are learned using a featurebased log-linear model. For features, we use all the unigram features from Elsner et al. (2012), which check faithfulness to voicing, place and manner of articulation (for example, for k ! g, active features are faith-manner, faith-place, output-g and voicelessto-voiced). Below, we present two methods for learning the transducer parameters 0. The oracle transducer is estimated using the gold-standard word segmentations and intended forms for the dataset; it represents the best possible approximation under our model of the actual phonetics of the d</context>
</contexts>
<marker>Hayes, Wilson, 2008</marker>
<rawString>Bruce Hayes and Colin Wilson. 2008. A maximum entropy model of phonotactics and phonotactic learning. Linguistic Inquiry, 39(3):379–440.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Marieke van Heugten</author>
<author>Elizabeth K Johnson</author>
</authors>
<title>in press. Learning to contend with accents in infancy: Benefits of brief speaker exposure.</title>
<journal>Journal of Experimental Psychology: General.</journal>
<marker>van Heugten, Johnson, </marker>
<rawString>Marieke van Heugten and Elizabeth K. Johnson. in press. Learning to contend with accents in infancy: Benefits of brief speaker exposure. Journal of Experimental Psychology: General.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derek M Houston</author>
<author>Peter W Jusczyk</author>
</authors>
<title>The role of talker-specific information in word segmentation by infants. Journal of Experimental Psychology: Human Perception and Performance,</title>
<date>2000</date>
<pages>26--1570</pages>
<contexts>
<context position="34193" citStr="Houston and Jusczyk, 2000" startWordPosition="5549" endWordPosition="5552">onsonant contrasts only by 10-12 months (Werker and Tees, 1984). The observed ordering is somewhat puzzling when one considers the availability for distributional information (Maye et al., 2002), which is much stronger for stop consonants than for vowels (Lisker and Abramson, 1964; Peterson and Barney, 1952). Infants are also conservative in generalizing across phonetic variability, showing a delayed abil49 ity to generalize across talkers, affects, and dialects. They have difficulty recognizing word tokens that are spoken by a different talker or in a different tone of voice until 11 months (Houston and Jusczyk, 2000; Singh et al., 2004), and the ability to adapt to unfamiliar dialects appears to develop even later, between 15 and 19 months (Best et al., 2009; Heugten and Johnson, in press; White and Aslin, 2011). Similar to infants, our model shows both a vowelconsonant asymmetry and a reluctance to accept the full range of adult phonetic variability. Table 5 shows some segment-to-segment alternations learned in various transducers. The oracle learns a large amount of variation (u surfaces as itself only 68% of the time) involving many different segments, whereas EM is similar to infant learners in learn</context>
</contexts>
<marker>Houston, Jusczyk, 2000</marker>
<rawString>Derek M. Houston and Peter W. Jusczyk. 2000. The role of talker-specific information in word segmentation by infants. Journal of Experimental Psychology: Human Perception and Performance, 26:1570–1582.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Huggins</author>
<author>Frank Wood</author>
</authors>
<title>Infinite structured hidden semi-Markov models.</title>
<date>2013</date>
<booktitle>Transactions on Pattern Analysis and Machine Intelligence (TPAMI),</booktitle>
<note>to appear,</note>
<contexts>
<context position="16751" citStr="Huggins and Wood (2013)" startWordPosition="2730" endWordPosition="2733">gmentation of the surface string), Pu(U|Q) and then incorporate the probability of U into the forward messages. If qi is the state in Q at which si is generated, and xi the corresponding intended character, we require that Pu &lt; T(si|xi); that is, the cutoffs must not exclude any states in the sequence Q. We define Pu 3Again, this approximation is corrected for by the MetropolisHastings step. p(j|[s]) u/u p(u|j) word j word u u word ju [s] ə/u p(ju|[s]) j/j d/j d ə j u/u u word jə p(jə|[s]) 45 as a A-mixture of two distributions: on each sweep). This is similar to the scheme for altering Pu in Huggins and Wood (2013). Pu(u|si,xi) = AU[0,min(.05,T(si|xi))]+ (1 − A)T(si|xi)Beta(5, 1e − 5) The former distribution is quite unrestrictive, while the latter prefers to prune away nearly all the states. Thus, for most characters in the string, we do not permit radical changes, while for a fraction, we do. We follow Huggins and Wood (2013), who extended Van Gael et al. (2008) to the case of a nonuniform Pu, to define our forward message a as: a(qi, i) a P(qi, S0..i, U0..i) (5) �= Pu(ui|si, xi)T (si|xi)a(qi−1, i − 1) qz−1 This is the standard HMM forward message, augmented with the probability of u. Since Pu(·|si, x</context>
</contexts>
<marker>Huggins, Wood, 2013</marker>
<rawString>Jonathan Huggins and Frank Wood. 2013. Infinite structured hidden semi-Markov models. Transactions on Pattern Analysis and Machine Intelligence (TPAMI), to appear, September.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Aren Jansen</author>
<author>Emmanuel Dupoux</author>
<author>Sharon Goldwater</author>
<author>Mark Johnson</author>
<author>Sanjeev Khudanpur</author>
<author>Kenneth Church</author>
<author>Naomi Feldman</author>
<author>Hynek Hermansky</author>
<author>Florian Metze</author>
<author>Richard Rose</author>
<author>Mike Seltzer</author>
<author>Pascal Clark</author>
<author>Ian McGraw</author>
<author>Balakrishnan Varadarajan</author>
<author>Erin Bennett</author>
<author>Benjamin Borschinger</author>
<author>Justin Chiu</author>
</authors>
<title>Ewan Dunbar, Abdellah Fourtassi, David Harwath, Chia-ying Lee,</title>
<date>2013</date>
<booktitle>Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<location>Keith Levin, Atta Norouzian, Vijay Peddinti, Rachael Richardson, Thomas</location>
<contexts>
<context position="25039" citStr="Jansen et al., 2013" startWordPosition="4081" endWordPosition="4084">t the idea that human learners may behave in important respects like the Bayesian ideal learners that Goldwater et al. presented. However, experiments on data with variation have called these conclusions into question. In particular, GGJ has previously been shown to oversegment rather than undersegment as the input grows noisier (Fleck, 2008), and our results replicate this finding (oversegmentation for the “segment only” model). In addition, the GGJ bigram model, which achieves much higher segmentation accuracy than the unigram model on clean data, actually performs worse on very noisy data (Jansen et al., 2013). Infants are known to track statistical dependencies across words (G´omez and Maye, 2005), so it is worrisome that these dependencies hurt GGJ’s segmentation accuracy when learning from noisy data. Our results show that modeling phonetic variability reverses the problematic trends described above. Although the models with phonetic variability show similar overall segmentation accuracy on noisy data to the original GGJ model, the pattern of errors changes, with less oversegmentation and more un47 dersegmentation. Thus, their qualitative performance on variable data resembles GGJ’s on clean dat</context>
</contexts>
<marker>Jansen, Dupoux, Goldwater, Johnson, Khudanpur, Church, Feldman, Hermansky, Metze, Rose, Seltzer, Clark, McGraw, Varadarajan, Bennett, Borschinger, Chiu, 2013</marker>
<rawString>Aren Jansen, Emmanuel Dupoux, Sharon Goldwater, Mark Johnson, Sanjeev Khudanpur, Kenneth Church, Naomi Feldman, Hynek Hermansky, Florian Metze, Richard Rose, Mike Seltzer, Pascal Clark, Ian McGraw, Balakrishnan Varadarajan, Erin Bennett, Benjamin Borschinger, Justin Chiu, Ewan Dunbar, Abdellah Fourtassi, David Harwath, Chia-ying Lee, Keith Levin, Atta Norouzian, Vijay Peddinti, Rachael Richardson, Thomas Schatz, and Samuel Thomas. 2013. A summary of the 2012 JHU CLSP workshop on zero resource speech technologies and early language acquisition. Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>398--406</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="26223" citStr="Johnson, 2008" startWordPosition="4260" endWordPosition="4261">a resembles GGJ’s on clean data, and therefore the behavior of human learners. 5.2 Phonetic variability We next analyze the model’s ability to normalize variations in the pronunciation of tokens, by inspecting the mtk score. The “segment only” baseline is predictably poor, F: 44.8. The pipeline model scores 48.8, and our oracle transducer model matches this exactly. The EM transducer scores better, F: 49.6. Although the confidence intervals overlap slightly, the EM system also outperforms the pipeline on the other F-measures; altogether, these results suggest at least a weak learning synergy (Johnson, 2008) between segmentation and phonetic learning. It is interesting that EM can perform better than the oracle. However, EM is more conservative about which sound changes it will allow, and thus tends to avoid mistakes caused by the simplicity of the transducer model. Since the transducer works segmentby-segment, it can apply rare contextual variations out of context. EM benefits from not learning these variations to begin with. We can also compare the bigram and unigram versions of the model. The unigram model is a reasonable segmenter, though not quite as good as the bigram model, with boundary F</context>
<context position="31423" citStr="Johnson, 2008" startWordPosition="5113" endWordPosition="5114">system, we could combine these variants to find 150 instances— but this is still 89 instances short of the 239 found when allowing for variability. The same pattern holds for “youlike” and “youwant”. Because the non-variable system must learn each variant separately, it learns only the most common instances of these long collocations, and analyzes infrequent variants differently. We also perform this analysis specifically for words beginning with vowels. Infants show a delay in their ability to segment these words from continuous speech (Mattys and Jusczyk, 2001; Nazzi et al., 2005; Seidl and Johnson, 2008), and Seidl and Johnson (2008) suggest a perceptual explanation— initial vowels can be hard to hear and often exhibit variation due to coarticulation or resyllabification. Although our dataset does not contain coarticulation as such, it should show this pattern of greater variation, which we hypothesize might lead to difficulty in segmenting and recognizing vowel-initial words. The model’s behavior is consistent with this hypothesis (Table 4). Both the “segment only” and EM transducer models find approximately the same 6Not all the variants are merged, however. jI, jx, fu etc. are still occasi</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>Mark Johnson. 2008. Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure. In Proceedings of ACL-08: HLT, pages 398–406, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Jusczyk</author>
<author>Richard N Aslin</author>
</authors>
<title>Infants’ detection of the sound patterns of words in fluent speech.</title>
<date>1995</date>
<pages>29--1</pages>
<publisher>Cognitive Psychology,</publisher>
<contexts>
<context position="1388" citStr="Jusczyk and Aslin, 1995" startWordPosition="207" endWordPosition="210">pelined approach learns to treat syllables or morphemes as words. In contrast, our joint model, like infant learners, tends to learn multiword collocations. We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence. 1 Introduction By the end of their first year, infants have acquired many of the basic elements of their native language. Their sensitivity to phonetic contrasts has become language-specific (Werker and Tees, 1984), and they have begun detecting words in fluent speech (Jusczyk and Aslin, 1995; Jusczyk et al., 1999) and learning word meanings (Bergelson and Swingley, 2012). These developmental cooccurrences lead some researchers to propose that phonetic and word learning occur jointly, each one informing the other (Swingley, 2009; Feldman et al., 2013). Previous computational models capture some aspects of this joint learning Sharon Goldwater sgwater@inf.ed.ac.uk ILCC, School of Informatics University of Edinburgh Frank Wood fwood@robots.ox.ac.uk Dept. of Engineering University of Oxford problem, but typically simplify the problem considerably, either by assuming an unrealistic deg</context>
</contexts>
<marker>Jusczyk, Aslin, 1995</marker>
<rawString>Peter W. Jusczyk and Richard N. Aslin. 1995. Infants’ detection of the sound patterns of words in fluent speech. Cognitive Psychology, 29:1–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Jusczyk</author>
<author>Derek M Houston</author>
<author>Mary Newsome</author>
</authors>
<title>The beginnings of word segmentation in Englishlearning infants.</title>
<date>1999</date>
<pages>39--159</pages>
<publisher>Cognitive Psychology,</publisher>
<contexts>
<context position="1411" citStr="Jusczyk et al., 1999" startWordPosition="211" endWordPosition="214">o treat syllables or morphemes as words. In contrast, our joint model, like infant learners, tends to learn multiword collocations. We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence. 1 Introduction By the end of their first year, infants have acquired many of the basic elements of their native language. Their sensitivity to phonetic contrasts has become language-specific (Werker and Tees, 1984), and they have begun detecting words in fluent speech (Jusczyk and Aslin, 1995; Jusczyk et al., 1999) and learning word meanings (Bergelson and Swingley, 2012). These developmental cooccurrences lead some researchers to propose that phonetic and word learning occur jointly, each one informing the other (Swingley, 2009; Feldman et al., 2013). Previous computational models capture some aspects of this joint learning Sharon Goldwater sgwater@inf.ed.ac.uk ILCC, School of Informatics University of Edinburgh Frank Wood fwood@robots.ox.ac.uk Dept. of Engineering University of Oxford problem, but typically simplify the problem considerably, either by assuming an unrealistic degree of phonetic regular</context>
</contexts>
<marker>Jusczyk, Houston, Newsome, 1999</marker>
<rawString>Peter W. Jusczyk, Derek M. Houston, and Mary Newsome. 1999. The beginnings of word segmentation in Englishlearning infants. Cognitive Psychology, 39:159–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dahee Kim</author>
<author>Joseph D W Stephens</author>
<author>Mark A Pitt</author>
</authors>
<title>How does context play a part in splitting words apart? Production and perception of word boundaries in casual speech.</title>
<date>2012</date>
<journal>Journal of Memory and Language,</journal>
<volume>66</volume>
<issue>4</issue>
<pages>529</pages>
<contexts>
<context position="37381" citStr="Kim et al., 2012" startWordPosition="6122" endWordPosition="6125">), iy (/i/), dh (/d/), k (/k/) and [0], the null character. Outputs from [0] are insertions. The oracle allows [0] as an output (deletion) but for computational reasons, the model does not. when the misrecognized word belongs to a prosodically rare class and when the incorrectly hypothesized string contains frequent words (Cutler, 1990); phonetically ambiguous words are also more commonly recognized as the more frequent of two options (Connine et al., 1993). For the indefinite article “a” (often reduced to [@]), lexical context is the main factor in deciding between ambiguous interpretations (Kim et al., 2012). In rapid speech, listeners have few phonetic cues to indicate whether it is present at all (Dilley and Pitt, 2010). Below, we analyze various misrecognitions made by our system (using the EM transducer), and find some similar effects. The easiest cases to analyze are those with no missegmentation: the proposed boundaries are correct, and the proposed lexical entry corresponds to a real word7, but not the correct one. Most of them correspond to homophones (Table 6). Common cases with a missegmentation include it and is, a and is, it’s and is, who, who’s and whose, that’s and what’s, and there</context>
<context position="39646" citStr="Kim et al., 2012" startWordPosition="6519" endWordPosition="6522">nd frequent function words which often appear in similar contexts. These tendencies match those shown by adult human listeners. A particularly distinctive set of joint recognition and segmentation errors are those where an entire real token is treated as phonetic “noise”— that is, it is segmented along with an adjacent word, and the system clusters the whole sequence as a token of that word. The most common examples are “that’s a” identified as “that’s”, “have a” identified as “have”, “sees a” identified as “sees” and other examples involving “a”, a word which also frequently confuses humans (Kim et al., 2012; Dilley and Pitt, 2010). However, there are also instances of “who’s in” as “who’s”, “does it” as “does”, and “can you” as “can”. 6 Conclusion We have presented a model that jointly infers word segmentation, lexical items, and a model of phonetic variability; we believe this is the first model to do so on a broad-coverage naturalistic corpus8. Our results show a small improvement in both segmentation and normalization over a pipeline model, providing evidence for a synergistic interaction between these learning tasks and supporting claims of interactive learning from the developmental literat</context>
</contexts>
<marker>Kim, Stephens, Pitt, 2012</marker>
<rawString>Dahee Kim, Joseph D.W. Stephens, and Mark A. Pitt. 2012. How does context play a part in splitting words apart? Production and perception of word boundaries in casual speech. Journal of Memory and Language, 66(4):509 – 529.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patricia K Kuhl</author>
<author>Karen A Williams</author>
<author>Francisco Lacerda</author>
<author>Kenneth N Stevens</author>
<author>Bjorn Lindblom</author>
</authors>
<title>Linguistic experience alters phonetic perception in infants by 6 months of age.</title>
<date>1992</date>
<journal>Science,</journal>
<volume>255</volume>
<issue>5044</issue>
<contexts>
<context position="33511" citStr="Kuhl et al., 1992" startWordPosition="5445" endWordPosition="5448">l-initial words (19.2%) than for consonants (12.5%). In cases where they do not propose a collocation, both systems are somewhat more likely to find the right boundary of a vowel-initial token than the left boundary (although again this difference is larger for the EM system); this suggests that the problem is indeed caused by the initial segment. 5.4 Phonetic Learning We next compare phonetic variations learned by the model to characteristics of infant speech perception. Infants show an asymmetry between consonants and vowels, losing sensitivity to non-native vowel contrasts by eight months (Kuhl et al., 1992; Bosch and Sebasti´an-Gall´es, 2003) but to non-native consonant contrasts only by 10-12 months (Werker and Tees, 1984). The observed ordering is somewhat puzzling when one considers the availability for distributional information (Maye et al., 2002), which is much stronger for stop consonants than for vowels (Lisker and Abramson, 1964; Peterson and Barney, 1952). Infants are also conservative in generalizing across phonetic variability, showing a delayed abil49 ity to generalize across talkers, affects, and dialects. They have difficulty recognizing word tokens that are spoken by a different</context>
</contexts>
<marker>Kuhl, Williams, Lacerda, Stevens, Lindblom, 1992</marker>
<rawString>Patricia K. Kuhl, Karen A. Williams, Francisco Lacerda, Kenneth N. Stevens, and Bjorn Lindblom. 1992. Linguistic experience alters phonetic perception in infants by 6 months of age. Science, 255(5044):606–608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leigh Lisker</author>
<author>Arthur S Abramson</author>
</authors>
<title>A crosslanguage study of voicing in initial stops: Acoustical measurements.</title>
<date>1964</date>
<pages>20--384</pages>
<publisher>Word,</publisher>
<contexts>
<context position="33849" citStr="Lisker and Abramson, 1964" startWordPosition="5496" endWordPosition="5499">he initial segment. 5.4 Phonetic Learning We next compare phonetic variations learned by the model to characteristics of infant speech perception. Infants show an asymmetry between consonants and vowels, losing sensitivity to non-native vowel contrasts by eight months (Kuhl et al., 1992; Bosch and Sebasti´an-Gall´es, 2003) but to non-native consonant contrasts only by 10-12 months (Werker and Tees, 1984). The observed ordering is somewhat puzzling when one considers the availability for distributional information (Maye et al., 2002), which is much stronger for stop consonants than for vowels (Lisker and Abramson, 1964; Peterson and Barney, 1952). Infants are also conservative in generalizing across phonetic variability, showing a delayed abil49 ity to generalize across talkers, affects, and dialects. They have difficulty recognizing word tokens that are spoken by a different talker or in a different tone of voice until 11 months (Houston and Jusczyk, 2000; Singh et al., 2004), and the ability to adapt to unfamiliar dialects appears to develop even later, between 15 and 19 months (Best et al., 2009; Heugten and Johnson, in press; White and Aslin, 2011). Similar to infants, our model shows both a vowelconson</context>
</contexts>
<marker>Lisker, Abramson, 1964</marker>
<rawString>Leigh Lisker and Arthur S. Abramson. 1964. A crosslanguage study of voicing in initial stops: Acoustical measurements. Word, 20:384–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Martin</author>
<author>Sharon Peperkamp</author>
<author>Emmanuel Dupoux</author>
</authors>
<title>Learning phonemes with a protolexicon. Cognitive Science,</title>
<date>2013</date>
<pages>37--103</pages>
<contexts>
<context position="4947" citStr="Martin et al., 2013" startWordPosition="736" endWordPosition="739">r many aspects of early phonetic and word learning in infants. 2 Related Work Nearly all computational models used to explore the problems addressed here have treated the learning tasks in isolation. Examples include models of word segmentation from phonemic input (Christiansen et al., 1998; Brent, 1999; Venkataraman, 2001; Swingley, 2005) or phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2011; Boruta et al., 2011), models of phonetic clustering (Vallabha et al., 2007; Varadarajan et al., 2008; Dupoux et al., 2011) and phonological rule learning (Peperkamp et al., 2006; Martin et al., 2013). Elsner et al. (2012) present a model that is similar to ours, using a noisy channel model implemented with a finite-state transducer to learn about phonetic variability while clustering distinct tokens into lexical items. However (like the earlier lexical-phonetic learning model of Feldman et al. (2009; in press)) their model assumes known word boundaries, so to perform both segmentation and lexical-phonetic learning, they use a pipeline that first segments using GGJ and then applies their model to the results. Neubig et al. (2010) also present a transducerbased noisy channel model that perf</context>
</contexts>
<marker>Martin, Peperkamp, Dupoux, 2013</marker>
<rawString>Andrew Martin, Sharon Peperkamp, and Emmanuel Dupoux. 2013. Learning phonemes with a protolexicon. Cognitive Science, 37:103–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven L Mattys</author>
<author>Peter W Jusczyk</author>
</authors>
<title>Do infants segment words or recurring contiguous patterns?</title>
<date>2001</date>
<journal>Journal of Experimental Psychology: Human Perception and Performance,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="31377" citStr="Mattys and Jusczyk, 2001" startWordPosition="5103" endWordPosition="5106">06 instances of /duju/ and 44 of /dujI/. In a pipelined system, we could combine these variants to find 150 instances— but this is still 89 instances short of the 239 found when allowing for variability. The same pattern holds for “youlike” and “youwant”. Because the non-variable system must learn each variant separately, it learns only the most common instances of these long collocations, and analyzes infrequent variants differently. We also perform this analysis specifically for words beginning with vowels. Infants show a delay in their ability to segment these words from continuous speech (Mattys and Jusczyk, 2001; Nazzi et al., 2005; Seidl and Johnson, 2008), and Seidl and Johnson (2008) suggest a perceptual explanation— initial vowels can be hard to hear and often exhibit variation due to coarticulation or resyllabification. Although our dataset does not contain coarticulation as such, it should show this pattern of greater variation, which we hypothesize might lead to difficulty in segmenting and recognizing vowel-initial words. The model’s behavior is consistent with this hypothesis (Table 4). Both the “segment only” and EM transducer models find approximately the same 6Not all the variants are mer</context>
</contexts>
<marker>Mattys, Jusczyk, 2001</marker>
<rawString>Sven L. Mattys and Peter W. Jusczyk. 2001. Do infants segment words or recurring contiguous patterns? Journal of Experimental Psychology: Human Perception and Performance, 27(3):644–655+.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jessica Maye</author>
<author>Janet F Werker</author>
<author>LouAnn Gerken</author>
</authors>
<title>Infant sensitivity to distributional information can affect phonetic discrimination.</title>
<date>2002</date>
<journal>Cognition,</journal>
<volume>82</volume>
<issue>3</issue>
<contexts>
<context position="33762" citStr="Maye et al., 2002" startWordPosition="5482" endWordPosition="5485"> larger for the EM system); this suggests that the problem is indeed caused by the initial segment. 5.4 Phonetic Learning We next compare phonetic variations learned by the model to characteristics of infant speech perception. Infants show an asymmetry between consonants and vowels, losing sensitivity to non-native vowel contrasts by eight months (Kuhl et al., 1992; Bosch and Sebasti´an-Gall´es, 2003) but to non-native consonant contrasts only by 10-12 months (Werker and Tees, 1984). The observed ordering is somewhat puzzling when one considers the availability for distributional information (Maye et al., 2002), which is much stronger for stop consonants than for vowels (Lisker and Abramson, 1964; Peterson and Barney, 1952). Infants are also conservative in generalizing across phonetic variability, showing a delayed abil49 ity to generalize across talkers, affects, and dialects. They have difficulty recognizing word tokens that are spoken by a different talker or in a different tone of voice until 11 months (Houston and Jusczyk, 2000; Singh et al., 2004), and the ability to adapt to unfamiliar dialects appears to develop even later, between 15 and 19 months (Best et al., 2009; Heugten and Johnson, i</context>
</contexts>
<marker>Maye, Werker, Gerken, 2002</marker>
<rawString>Jessica Maye, Janet F. Werker, and LouAnn Gerken. 2002. Infant sensitivity to distributional information can affect phonetic discrimination. Cognition, 82(3):B101–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daichi Mochihashi</author>
<author>Takeshi Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Bayesian unsupervised word segmentation with nested pitman-yor language modeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>100--108</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="11155" citStr="Mochihashi et al. (2009)" startWordPosition="1764" endWordPosition="1767">s /A/, the word is likely to be /wAn/ “one” and the next word begins with /t/; if instead we posit that the vowel is /O/, the word is probably /wOnt/ “want”. Thus, inference methods that change only one character at a time are unlikely to mix well. Since they cannot simultaneously change the vowel and resegment the /t/, they must pass through a low-probability intermediate state to get from one state to the other, so will tend to get stuck in a bad local minimum. A Gibbs sampler which inserts or deletes a single segment boundary in each step (Goldwater et al., 2009) suffers from this problem. Mochihashi et al. (2009) describe an inference method with higher mobility: a block sampler for the GGJ model that samples from the posterior over analyses of a whole utterance at once. This method encodes the model as a large HMM, using dynamic programming to select an analysis. We encode our own model in the same way, constructing the HMM and composing it with the transducer (Mohri, 2004) to form a larger finite-state machine which is still amenable to forward-backward sampling. 4.1 Finite-state encoding Following Mochihashi et al. (2009) and Neubig et al. (2010), we can write the original GGJ model as a Hidden Sem</context>
<context position="13907" citStr="Mochihashi et al. (2009)" startWordPosition="2233" endWordPosition="2236"> prior will generate a known word w, in which case our final transition ought to be to ST:[w][] instead of ST:[unk.word][]. This approximation means we do not need to add context to the Geom state to remember the sequence of characters it produced, which allows us to keep only a single Geom state on the chart at each timestep. When we compose this model with the channel model, the number of states expands. Each state must now keep track of the previous word, what intended characters C have been posited and what surface characters S have been recognized, ST:[w][C ][S]. 2Though not mentioned by Mochihashi et al. (2009) or Neubig et al. (2010), this construction is not exact, since transitions in a Bayesian HMM are exchangeable but not independent (Beal et al., 2001): if a word occurs twice in an utterance, its probability is slightly higher the second time. For single utterances, this bias is small and easy to correct for using a Metropolis-Hastings acceptance check (B¨orschinger and Johnson, 2012) using the path probability from the HMM as the proposal. To recognize the current word, we transition to ST:[C ][][] with probability P(xi = C|xi_1 = w). To parse a new surface character s by positing intended ch</context>
</contexts>
<marker>Mochihashi, Yamada, Ueda, 2009</marker>
<rawString>Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian unsupervised word segmentation with nested pitman-yor language modeling. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 100–108, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Weighted Finite-State Transducer Algorithms: An Overview, chapter 29,</title>
<date>2004</date>
<pages>551--564</pages>
<publisher>Physica-Verlag.</publisher>
<contexts>
<context position="11524" citStr="Mohri, 2004" startWordPosition="1829" endWordPosition="1830"> to get from one state to the other, so will tend to get stuck in a bad local minimum. A Gibbs sampler which inserts or deletes a single segment boundary in each step (Goldwater et al., 2009) suffers from this problem. Mochihashi et al. (2009) describe an inference method with higher mobility: a block sampler for the GGJ model that samples from the posterior over analyses of a whole utterance at once. This method encodes the model as a large HMM, using dynamic programming to select an analysis. We encode our own model in the same way, constructing the HMM and composing it with the transducer (Mohri, 2004) to form a larger finite-state machine which is still amenable to forward-backward sampling. 4.1 Finite-state encoding Following Mochihashi et al. (2009) and Neubig et al. (2010), we can write the original GGJ model as a Hidden Semi-Markov model. States in the HMM, written ST:[w][C ], are labeled with the previous word w and the sequence of characters C which have so far been incorporated into the current word. To produce a word boundary, we transition from ST:[w][C] to ST:[C][] with probability P(xi = C|xi_1 = w). We can also add the next character s to the current word, transitioning from ST</context>
</contexts>
<marker>Mohri, 2004</marker>
<rawString>Mehryar Mohri, 2004. Weighted Finite-State Transducer Algorithms: An Overview, chapter 29, pages 551–564. Physica-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thierry Nazzi</author>
<author>Laura C Dilley</author>
<author>Ann Marie Jusczyk</author>
<author>Stefanie Shattuck-Hufnagel</author>
<author>Peter W Jusczyk</author>
</authors>
<title>English-learning infants’ segmentation of verbs from fluent speech.</title>
<date>2005</date>
<journal>Language and Speech,</journal>
<volume>48</volume>
<issue>3</issue>
<contexts>
<context position="31397" citStr="Nazzi et al., 2005" startWordPosition="5107" endWordPosition="5110"> 44 of /dujI/. In a pipelined system, we could combine these variants to find 150 instances— but this is still 89 instances short of the 239 found when allowing for variability. The same pattern holds for “youlike” and “youwant”. Because the non-variable system must learn each variant separately, it learns only the most common instances of these long collocations, and analyzes infrequent variants differently. We also perform this analysis specifically for words beginning with vowels. Infants show a delay in their ability to segment these words from continuous speech (Mattys and Jusczyk, 2001; Nazzi et al., 2005; Seidl and Johnson, 2008), and Seidl and Johnson (2008) suggest a perceptual explanation— initial vowels can be hard to hear and often exhibit variation due to coarticulation or resyllabification. Although our dataset does not contain coarticulation as such, it should show this pattern of greater variation, which we hypothesize might lead to difficulty in segmenting and recognizing vowel-initial words. The model’s behavior is consistent with this hypothesis (Table 4). Both the “segment only” and EM transducer models find approximately the same 6Not all the variants are merged, however. jI, jx</context>
</contexts>
<marker>Nazzi, Dilley, Jusczyk, Shattuck-Hufnagel, Jusczyk, 2005</marker>
<rawString>Thierry Nazzi, Laura C. Dilley, Ann Marie Jusczyk, Stefanie Shattuck-Hufnagel, and Peter W. Jusczyk. 2005. English-learning infants’ segmentation of verbs from fluent speech. Language and Speech, 48(3):279–298+.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Masato Mimura</author>
<author>Shinsuke Mori</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>Learning a language model from continuous speech.</title>
<date>2010</date>
<booktitle>In 11th Annual Conference of the International Speech Communication Association (InterSpeech</booktitle>
<pages>1053--1056</pages>
<location>Makuhari,</location>
<contexts>
<context position="2761" citStr="Neubig et al., 2010" startWordPosition="408" endWordPosition="411"> al., 2009; Feldman et al., in press; Elsner et al., 2012). This paper presents, to our knowledge, the first broadcoverage model that learns to segment phonetically variable input into words, while simultaneously learning an explicit model of phonetic variation that allows it to cluster together segmented tokens with different phonetic realizations (e.g., [ju] and [jI]) into lexical items (/ju/). We base our model on the Bayesian word segmentation model of Goldwater et al. (2009) (henceforth GGJ), using a noisy-channel setup where phonetic variation is introduced by a finite-state transducer (Neubig et al., 2010; Elsner et al., 2012). This integrated model allows us to examine how solving the word segmentation problem should affect infants’ strategies for learning about phonetic variability and how phonetic learning can allow word segmentation to proceed in ways that mimic the idealized input used in previous models. In particular, although the GGJ model achieves high segmentation accuracy on phonemic (nonvariable) input and makes errors that are qualitatively similar to human learners (tending to undersegment the input), its accuracy drops considerably on phonetically noisy data and it tends to over</context>
<context position="5486" citStr="Neubig et al. (2010)" startWordPosition="820" endWordPosition="823">011) and phonological rule learning (Peperkamp et al., 2006; Martin et al., 2013). Elsner et al. (2012) present a model that is similar to ours, using a noisy channel model implemented with a finite-state transducer to learn about phonetic variability while clustering distinct tokens into lexical items. However (like the earlier lexical-phonetic learning model of Feldman et al. (2009; in press)) their model assumes known word boundaries, so to perform both segmentation and lexical-phonetic learning, they use a pipeline that first segments using GGJ and then applies their model to the results. Neubig et al. (2010) also present a transducerbased noisy channel model that performs joint inference on two out of the three tasks we consider here; their model assumes fixed probabilities for phonetic changes (the noise model) and jointly infers the word segmentation and lexical items, as in our ‘oracle’ model below (though unlike our system their model learns from phone lattices rather than a single transcription). They evaluate only on phone recognition, not scoring the inferred lexical items. Recently, B¨orschinger et al. (2013) did present a Figure 1: The graphical model for our system (Eq. 1- 4). Note that</context>
<context position="11702" citStr="Neubig et al. (2010)" startWordPosition="1853" endWordPosition="1856">ldwater et al., 2009) suffers from this problem. Mochihashi et al. (2009) describe an inference method with higher mobility: a block sampler for the GGJ model that samples from the posterior over analyses of a whole utterance at once. This method encodes the model as a large HMM, using dynamic programming to select an analysis. We encode our own model in the same way, constructing the HMM and composing it with the transducer (Mohri, 2004) to form a larger finite-state machine which is still amenable to forward-backward sampling. 4.1 Finite-state encoding Following Mochihashi et al. (2009) and Neubig et al. (2010), we can write the original GGJ model as a Hidden Semi-Markov model. States in the HMM, written ST:[w][C ], are labeled with the previous word w and the sequence of characters C which have so far been incorporated into the current word. To produce a word boundary, we transition from ST:[w][C] to ST:[C][] with probability P(xi = C|xi_1 = w). We can also add the next character s to the current word, transitioning from ST:[w][C] to ST:[w][C : s], at no cost (since the full cost of the word is paid at its boundary, there 44 Figure 2: A fragment of the composed finite-state machine for word segment</context>
<context position="13931" citStr="Neubig et al. (2010)" startWordPosition="2238" endWordPosition="2242"> word w, in which case our final transition ought to be to ST:[w][] instead of ST:[unk.word][]. This approximation means we do not need to add context to the Geom state to remember the sequence of characters it produced, which allows us to keep only a single Geom state on the chart at each timestep. When we compose this model with the channel model, the number of states expands. Each state must now keep track of the previous word, what intended characters C have been posited and what surface characters S have been recognized, ST:[w][C ][S]. 2Though not mentioned by Mochihashi et al. (2009) or Neubig et al. (2010), this construction is not exact, since transitions in a Bayesian HMM are exchangeable but not independent (Beal et al., 2001): if a word occurs twice in an utterance, its probability is slightly higher the second time. For single utterances, this bias is small and easy to correct for using a Metropolis-Hastings acceptance check (B¨orschinger and Johnson, 2012) using the path probability from the HMM as the proposal. To recognize the current word, we transition to ST:[C ][][] with probability P(xi = C|xi_1 = w). To parse a new surface character s by positing intended character x (note that x m</context>
</contexts>
<marker>Neubig, Mimura, Mori, Kawahara, 2010</marker>
<rawString>Graham Neubig, Masato Mimura, Shinsuke Mori, and Tatsuya Kawahara. 2010. Learning a language model from continuous speech. In 11th Annual Conference of the International Speech Communication Association (InterSpeech 2010), pages 1053–1056, Makuhari, Japan, 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Peperkamp</author>
<author>Rozenn Le Calvez</author>
<author>Jean-Pierre Nadal</author>
<author>Emmanuel Dupoux</author>
</authors>
<title>The acquisition of allophonic rules: Statistical learning with linguistic constraints.</title>
<date>2006</date>
<journal>Cognition,</journal>
<volume>101</volume>
<issue>3</issue>
<marker>Peperkamp, Le Calvez, Nadal, Dupoux, 2006</marker>
<rawString>Sharon Peperkamp, Rozenn Le Calvez, Jean-Pierre Nadal, and Emmanuel Dupoux. 2006. The acquisition of allophonic rules: Statistical learning with linguistic constraints. Cognition, 101(3):B31–B41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann M Peters</author>
</authors>
<title>The Units of Language Acquisition. Cambridge Monographs and Texts in Applied Psycholinguistics.</title>
<date>1983</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="24242" citStr="Peters, 1983" startWordPosition="3961" endWordPosition="3962">ate transducers score P: 76.1, R: 83.8 (oracle) and P: 80.1, R: 83.0 (EM), indicating that they prefer to find longer sequences (undersegment) more. In previous experiments on datasets without variation, GGJ also has a strong tendency to undersegment the data (boundary P: 90.1, R: 80.3), which Goldwater et al. argue is rational behavior for an ideal learner seeking a parsimonious explanation for the data. Undersegmentation occurs especially when ignoring lexical context (a unigram model), but to some extent even in bigram models. Human learners also tend to learn collocations as single words (Peters, 1983; Tomasello, 2000), and the GGJ model has been shown to capture several other effects seen in laboratory segmentation tasks (Frank et al., 2010). Together, these findings support the idea that human learners may behave in important respects like the Bayesian ideal learners that Goldwater et al. presented. However, experiments on data with variation have called these conclusions into question. In particular, GGJ has previously been shown to oversegment rather than undersegment as the input grows noisier (Fleck, 2008), and our results replicate this finding (oversegmentation for the “segment onl</context>
</contexts>
<marker>Peters, 1983</marker>
<rawString>Ann M. Peters. 1983. The Units of Language Acquisition. Cambridge Monographs and Texts in Applied Psycholinguistics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gordon E Peterson</author>
<author>Harold L Barney</author>
</authors>
<title>Control methods used in a study of the vowels.</title>
<date>1952</date>
<journal>Journal of the Acoustical Society of America,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="33877" citStr="Peterson and Barney, 1952" startWordPosition="5500" endWordPosition="5503">netic Learning We next compare phonetic variations learned by the model to characteristics of infant speech perception. Infants show an asymmetry between consonants and vowels, losing sensitivity to non-native vowel contrasts by eight months (Kuhl et al., 1992; Bosch and Sebasti´an-Gall´es, 2003) but to non-native consonant contrasts only by 10-12 months (Werker and Tees, 1984). The observed ordering is somewhat puzzling when one considers the availability for distributional information (Maye et al., 2002), which is much stronger for stop consonants than for vowels (Lisker and Abramson, 1964; Peterson and Barney, 1952). Infants are also conservative in generalizing across phonetic variability, showing a delayed abil49 ity to generalize across talkers, affects, and dialects. They have difficulty recognizing word tokens that are spoken by a different talker or in a different tone of voice until 11 months (Houston and Jusczyk, 2000; Singh et al., 2004), and the ability to adapt to unfamiliar dialects appears to develop even later, between 15 and 19 months (Best et al., 2009; Heugten and Johnson, in press; White and Aslin, 2011). Similar to infants, our model shows both a vowelconsonant asymmetry and a reluctan</context>
</contexts>
<marker>Peterson, Barney, 1952</marker>
<rawString>Gordon E. Peterson and Harold L. Barney. 1952. Control methods used in a study of the vowels. Journal of the Acoustical Society of America, 24(2):175–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Pitt</author>
<author>Laura Dilley</author>
<author>Keith Johnson</author>
<author>Scott Kiesling</author>
<author>William Raymond</author>
<author>Elizabeth Hume</author>
<author>Eric Fosler-Lussier</author>
</authors>
<title>Buckeye corpus of conversational speech (2nd release).</title>
<date>2007</date>
<contexts>
<context position="19397" citStr="Pitt et al., 2007" startWordPosition="3190" endWordPosition="3193">(allowing the system to change about a third of the characters 4.3 Dataset and metrics We use the corpus released by Elsner et al. (2012), which contains 9790 child-directed English utterances originally from the Bernstein-Ratner corpus (Bernstein-Ratner, 1987) and later transcribed phonemically (Brent, 1999). This standard word segmentation dataset was modified by Elsner et al. (2012) to include phonetic variation by assigning each token a pronunciation independently selected from the empirical distribution of pronunciations of that word type in the closely-transcribed Buckeye Speech Corpus (Pitt et al., 2007). Following previous work, we hold out the last 1790 utterances as unseen test data during development. In the results presented here, we run the model on all 9790 utterances but score only these 1790. We average results over 5 runs of the model with different random seeds. We use standard metrics for segmentation and lexicon recovery. For segmentation, we report precision, recall and F-score for word boundaries (bds), and for the positions of word tokens in the surface string (srf; both boundaries must be correct). For normalization of the pronunciation variation, we follow Elsner et al. (201</context>
</contexts>
<marker>Pitt, Dilley, Johnson, Kiesling, Raymond, Hume, Fosler-Lussier, 2007</marker>
<rawString>Mark A. Pitt, Laura Dilley, Keith Johnson, Scott Kiesling, William Raymond, Elizabeth Hume, and Eric Fosler-Lussier. 2007. Buckeye corpus of conversational speech (2nd release).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kim Plunkett</author>
</authors>
<title>Learning how to be flexible with words. Attention and Performance,</title>
<date>2005</date>
<contexts>
<context position="36182" citStr="Plunkett, 2005" startWordPosition="5885" endWordPosition="5886">ervativism, we ran the system on 1000 utterances instead of 9790. This leads to an even more conservative solution, with variations for u but none of the others (although i and d still vary more than k). 5.5 Segmentation and recognition errors A particularly interesting set of errors are those that involve both a missegmentation and a simultaneous misrecognition, since the joint model is prone to such errors while the pipelined model is not. Relatively little is known about infants’ misrecognitions of words in fluent speech, although it is clear that they find words in medial position harder (Plunkett, 2005; Seidl and Johnson, 2006). However, adults make missegmentation/misrecognition errors fairly often, especially when listening to noisy audio (Butterfield and Cutler, 1988). Such errors are more common top 4 outputs s u .68 @ .05 a .04 U .04 i .85 I .03 @ .03 E .02 d .69 s .07 [0] .07 z .04 k .93 d .02 g .02 r .21 h .11 d .01 @ .07 u .75 @ .08 I .04 U .03 i .90 I .04 E .02 d .91 s .03 z 0.1 k .98 @ .32 I .14 n .13 t .13 u .82 I .04 @ .04 a .02 i .97 d .95 k .99 @ .21 I .18 t .12 s .12 Table 5: Learned phonetic alternations: top 4 outputs s with p &gt; .001 for inputs x = uw (/u/), iy (/i/), dh (/</context>
</contexts>
<marker>Plunkett, 2005</marker>
<rawString>Kim Plunkett. 2005. Learning how to be flexible with words. Attention and Performance, XXI:233–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anton Rytting</author>
</authors>
<title>Preserving Subsegmental Variation in Modeling Word Segmentation (Or, the Raising of Baby Mondegreen).</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>The Ohio State University.</institution>
<contexts>
<context position="4714" citStr="Rytting, 2007" startWordPosition="702" endWordPosition="703">o experimental results on adult and infant speech processing. Taken together, our results support the idea that a Bayesian model that jointly performs word segmentation and phonetic learning provides a plausible explanation for many aspects of early phonetic and word learning in infants. 2 Related Work Nearly all computational models used to explore the problems addressed here have treated the learning tasks in isolation. Examples include models of word segmentation from phonemic input (Christiansen et al., 1998; Brent, 1999; Venkataraman, 2001; Swingley, 2005) or phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2011; Boruta et al., 2011), models of phonetic clustering (Vallabha et al., 2007; Varadarajan et al., 2008; Dupoux et al., 2011) and phonological rule learning (Peperkamp et al., 2006; Martin et al., 2013). Elsner et al. (2012) present a model that is similar to ours, using a noisy channel model implemented with a finite-state transducer to learn about phonetic variability while clustering distinct tokens into lexical items. However (like the earlier lexical-phonetic learning model of Feldman et al. (2009; in press)) their model assumes known word boundaries, so to </context>
</contexts>
<marker>Rytting, 2007</marker>
<rawString>Anton Rytting. 2007. Preserving Subsegmental Variation in Modeling Word Segmentation (Or, the Raising of Baby Mondegreen). Ph.D. thesis, The Ohio State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amanda Seidl</author>
<author>Elizabeth Johnson</author>
</authors>
<title>Infant word segmentation revisited: Edge alignment facilitates target extraction.</title>
<date>2006</date>
<journal>Developmental Science,</journal>
<pages>9--565</pages>
<contexts>
<context position="36208" citStr="Seidl and Johnson, 2006" startWordPosition="5887" endWordPosition="5890">an the system on 1000 utterances instead of 9790. This leads to an even more conservative solution, with variations for u but none of the others (although i and d still vary more than k). 5.5 Segmentation and recognition errors A particularly interesting set of errors are those that involve both a missegmentation and a simultaneous misrecognition, since the joint model is prone to such errors while the pipelined model is not. Relatively little is known about infants’ misrecognitions of words in fluent speech, although it is clear that they find words in medial position harder (Plunkett, 2005; Seidl and Johnson, 2006). However, adults make missegmentation/misrecognition errors fairly often, especially when listening to noisy audio (Butterfield and Cutler, 1988). Such errors are more common top 4 outputs s u .68 @ .05 a .04 U .04 i .85 I .03 @ .03 E .02 d .69 s .07 [0] .07 z .04 k .93 d .02 g .02 r .21 h .11 d .01 @ .07 u .75 @ .08 I .04 U .03 i .90 I .04 E .02 d .91 s .03 z 0.1 k .98 @ .32 I .14 n .13 t .13 u .82 I .04 @ .04 a .02 i .97 d .95 k .99 @ .21 I .18 t .12 s .12 Table 5: Learned phonetic alternations: top 4 outputs s with p &gt; .001 for inputs x = uw (/u/), iy (/i/), dh (/d/), k (/k/) and [0], the </context>
</contexts>
<marker>Seidl, Johnson, 2006</marker>
<rawString>Amanda Seidl and Elizabeth Johnson. 2006. Infant word segmentation revisited: Edge alignment facilitates target extraction. Developmental Science, 9:565–573.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amanda Seidl</author>
<author>Elizabeth Johnson</author>
</authors>
<title>Perceptual factors influence infants’ extraction of onsetless words from continuous speech.</title>
<date>2008</date>
<journal>Journal of Child Language,</journal>
<volume>34</volume>
<contexts>
<context position="31423" citStr="Seidl and Johnson, 2008" startWordPosition="5111" endWordPosition="5114">pipelined system, we could combine these variants to find 150 instances— but this is still 89 instances short of the 239 found when allowing for variability. The same pattern holds for “youlike” and “youwant”. Because the non-variable system must learn each variant separately, it learns only the most common instances of these long collocations, and analyzes infrequent variants differently. We also perform this analysis specifically for words beginning with vowels. Infants show a delay in their ability to segment these words from continuous speech (Mattys and Jusczyk, 2001; Nazzi et al., 2005; Seidl and Johnson, 2008), and Seidl and Johnson (2008) suggest a perceptual explanation— initial vowels can be hard to hear and often exhibit variation due to coarticulation or resyllabification. Although our dataset does not contain coarticulation as such, it should show this pattern of greater variation, which we hypothesize might lead to difficulty in segmenting and recognizing vowel-initial words. The model’s behavior is consistent with this hypothesis (Table 4). Both the “segment only” and EM transducer models find approximately the same 6Not all the variants are merged, however. jI, jx, fu etc. are still occasi</context>
</contexts>
<marker>Seidl, Johnson, 2008</marker>
<rawString>Amanda Seidl and Elizabeth Johnson. 2008. Perceptual factors influence infants’ extraction of onsetless words from continuous speech. Journal of Child Language, 34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leher Singh</author>
<author>James Morgan</author>
<author>Katherine White</author>
</authors>
<title>Preference and processing: The role of speech affect in early spoken word recognition.</title>
<date>2004</date>
<journal>Journal of Memory and Language,</journal>
<pages>51--173</pages>
<contexts>
<context position="34214" citStr="Singh et al., 2004" startWordPosition="5553" endWordPosition="5556">10-12 months (Werker and Tees, 1984). The observed ordering is somewhat puzzling when one considers the availability for distributional information (Maye et al., 2002), which is much stronger for stop consonants than for vowels (Lisker and Abramson, 1964; Peterson and Barney, 1952). Infants are also conservative in generalizing across phonetic variability, showing a delayed abil49 ity to generalize across talkers, affects, and dialects. They have difficulty recognizing word tokens that are spoken by a different talker or in a different tone of voice until 11 months (Houston and Jusczyk, 2000; Singh et al., 2004), and the ability to adapt to unfamiliar dialects appears to develop even later, between 15 and 19 months (Best et al., 2009; Heugten and Johnson, in press; White and Aslin, 2011). Similar to infants, our model shows both a vowelconsonant asymmetry and a reluctance to accept the full range of adult phonetic variability. Table 5 shows some segment-to-segment alternations learned in various transducers. The oracle learns a large amount of variation (u surfaces as itself only 68% of the time) involving many different segments, whereas EM is similar to infant learners in learning a more conservati</context>
</contexts>
<marker>Singh, Morgan, White, 2004</marker>
<rawString>Leher Singh, James Morgan, and Katherine White. 2004. Preference and processing: The role of speech affect in early spoken word recognition. Journal of Memory and Language, 51:173–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Swingley</author>
</authors>
<title>Statistical clustering and the contents of the infant vocabulary. Cognitive Psychology,</title>
<date>2005</date>
<pages>50--86</pages>
<contexts>
<context position="4668" citStr="Swingley, 2005" startWordPosition="694" endWordPosition="696">representations in detail, drawing comparisons to experimental results on adult and infant speech processing. Taken together, our results support the idea that a Bayesian model that jointly performs word segmentation and phonetic learning provides a plausible explanation for many aspects of early phonetic and word learning in infants. 2 Related Work Nearly all computational models used to explore the problems addressed here have treated the learning tasks in isolation. Examples include models of word segmentation from phonemic input (Christiansen et al., 1998; Brent, 1999; Venkataraman, 2001; Swingley, 2005) or phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2011; Boruta et al., 2011), models of phonetic clustering (Vallabha et al., 2007; Varadarajan et al., 2008; Dupoux et al., 2011) and phonological rule learning (Peperkamp et al., 2006; Martin et al., 2013). Elsner et al. (2012) present a model that is similar to ours, using a noisy channel model implemented with a finite-state transducer to learn about phonetic variability while clustering distinct tokens into lexical items. However (like the earlier lexical-phonetic learning model of Feldman et al. (2009; in press)) the</context>
</contexts>
<marker>Swingley, 2005</marker>
<rawString>Daniel Swingley. 2005. Statistical clustering and the contents of the infant vocabulary. Cognitive Psychology, 50:86–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Swingley</author>
</authors>
<title>Contributions of infant word learning to language development.</title>
<date>2009</date>
<journal>Philosophical Transactions of the Royal Society B: Biological Sciences,</journal>
<volume>364</volume>
<issue>1536</issue>
<contexts>
<context position="1629" citStr="Swingley, 2009" startWordPosition="245" endWordPosition="246">atterns of word recognition errors, and relate these to developmental evidence. 1 Introduction By the end of their first year, infants have acquired many of the basic elements of their native language. Their sensitivity to phonetic contrasts has become language-specific (Werker and Tees, 1984), and they have begun detecting words in fluent speech (Jusczyk and Aslin, 1995; Jusczyk et al., 1999) and learning word meanings (Bergelson and Swingley, 2012). These developmental cooccurrences lead some researchers to propose that phonetic and word learning occur jointly, each one informing the other (Swingley, 2009; Feldman et al., 2013). Previous computational models capture some aspects of this joint learning Sharon Goldwater sgwater@inf.ed.ac.uk ILCC, School of Informatics University of Edinburgh Frank Wood fwood@robots.ox.ac.uk Dept. of Engineering University of Oxford problem, but typically simplify the problem considerably, either by assuming an unrealistic degree of phonetic regularity for word segmentation (Goldwater et al., 2009) or assuming pre-segmented input for phonetic and lexical acquisition (Feldman et al., 2009; Feldman et al., in press; Elsner et al., 2012). This paper presents, to our</context>
</contexts>
<marker>Swingley, 2009</marker>
<rawString>Daniel Swingley. 2009. Contributions of infant word learning to language development. Philosophical Transactions of the Royal Society B: Biological Sciences, 364(1536):3617–3632, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
<author>M I Jordan</author>
<author>M J Beal</author>
<author>D M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="7792" citStr="Teh et al., 2006" startWordPosition="1213" endWordPosition="1216"> X, we 1We use their best reported parameter values: α0 = 3000, α1 = 100, pstap = .2 and for unigrams, α0 = 20. Generator for possible words Geoma, b, ..., ju, ... want, ... juwant, ... Probabilities for each word (sparse) p(ði) = .1, p(a) = .05, p(want) = .01... a0 G0 Conditional probabilities for each word after each word p(ði |want) = .3, p(a |want) = .1, p(want |want) = .0001... Intended forms ju want a kuki ju want it Surface forms ja wan a kuki ju wand it GGJ 09 a1 Gx � contexts x x T 2 ... 1 s1 s2 ... n utterances 43 sample a random language model from a hierarchical Dirichlet process (Teh et al., 2006) with character strings as atoms. To do so, we first draw a unigram distribution G0 from a Dirichlet process prior whose base distribution generates intended form word strings by drawing each phone in turn until the stop character is drawn (with probability pstop). Then, for each possible context word x, we draw a conditional distribution on words following that context Gx = P(Xi = ~|Xi_1 = x) using G0 as a prior. Finally, we sample word sequences x1 ... x,,, from the bigram model. The channel model is a finite transducer with parameters 0 which independently rewrites single characters from th</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Tomasello</author>
</authors>
<title>The item-based nature of children’s early syntactic development.</title>
<date>2000</date>
<journal>Trends in Cognitive Sciences,</journal>
<volume>4</volume>
<issue>4</issue>
<pages>163</pages>
<contexts>
<context position="24260" citStr="Tomasello, 2000" startWordPosition="3963" endWordPosition="3964">s score P: 76.1, R: 83.8 (oracle) and P: 80.1, R: 83.0 (EM), indicating that they prefer to find longer sequences (undersegment) more. In previous experiments on datasets without variation, GGJ also has a strong tendency to undersegment the data (boundary P: 90.1, R: 80.3), which Goldwater et al. argue is rational behavior for an ideal learner seeking a parsimonious explanation for the data. Undersegmentation occurs especially when ignoring lexical context (a unigram model), but to some extent even in bigram models. Human learners also tend to learn collocations as single words (Peters, 1983; Tomasello, 2000), and the GGJ model has been shown to capture several other effects seen in laboratory segmentation tasks (Frank et al., 2010). Together, these findings support the idea that human learners may behave in important respects like the Bayesian ideal learners that Goldwater et al. presented. However, experiments on data with variation have called these conclusions into question. In particular, GGJ has previously been shown to oversegment rather than undersegment as the input grows noisier (Fleck, 2008), and our results replicate this finding (oversegmentation for the “segment only” model). In addi</context>
</contexts>
<marker>Tomasello, 2000</marker>
<rawString>Michael Tomasello. 2000. The item-based nature of children’s early syntactic development. Trends in Cognitive Sciences, 4(4):156 – 163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gautam K Vallabha</author>
<author>James L McClelland</author>
<author>Ferran Pons</author>
<author>Janet F Werker</author>
<author>Shigeaki Amano</author>
</authors>
<title>Unsupervised learning of vowel categories from infant-directed speech.</title>
<date>2007</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>104</volume>
<issue>33</issue>
<contexts>
<context position="4822" citStr="Vallabha et al., 2007" startWordPosition="716" endWordPosition="719">e idea that a Bayesian model that jointly performs word segmentation and phonetic learning provides a plausible explanation for many aspects of early phonetic and word learning in infants. 2 Related Work Nearly all computational models used to explore the problems addressed here have treated the learning tasks in isolation. Examples include models of word segmentation from phonemic input (Christiansen et al., 1998; Brent, 1999; Venkataraman, 2001; Swingley, 2005) or phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2011; Boruta et al., 2011), models of phonetic clustering (Vallabha et al., 2007; Varadarajan et al., 2008; Dupoux et al., 2011) and phonological rule learning (Peperkamp et al., 2006; Martin et al., 2013). Elsner et al. (2012) present a model that is similar to ours, using a noisy channel model implemented with a finite-state transducer to learn about phonetic variability while clustering distinct tokens into lexical items. However (like the earlier lexical-phonetic learning model of Feldman et al. (2009; in press)) their model assumes known word boundaries, so to perform both segmentation and lexical-phonetic learning, they use a pipeline that first segments using GGJ a</context>
</contexts>
<marker>Vallabha, McClelland, Pons, Werker, Amano, 2007</marker>
<rawString>Gautam K. Vallabha, James L. McClelland, Ferran Pons, Janet F. Werker, and Shigeaki Amano. 2007. Unsupervised learning of vowel categories from infant-directed speech. Proceedings of the National Academy of Sciences, 104(33):13273–13278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Van Gael</author>
<author>Yunus Saatci</author>
<author>Yee Whye Teh</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Beam sampling for the infinite Hidden Markov model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine learning, ICML ’08,</booktitle>
<pages>1088--1095</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Van Gael, Saatci, Teh, Ghahramani, 2008</marker>
<rawString>Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and Zoubin Ghahramani. 2008. Beam sampling for the infinite Hidden Markov model. In Proceedings of the 25th International Conference on Machine learning, ICML ’08, pages 1088–1095, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Balakrishnan Varadarajan</author>
<author>Sanjeev Khudanpur</author>
<author>Emmanuel Dupoux</author>
</authors>
<title>Unsupervised learning of acoustic sub-word units.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics: Short Papers,</booktitle>
<pages>165--168</pages>
<contexts>
<context position="4848" citStr="Varadarajan et al., 2008" startWordPosition="720" endWordPosition="723">model that jointly performs word segmentation and phonetic learning provides a plausible explanation for many aspects of early phonetic and word learning in infants. 2 Related Work Nearly all computational models used to explore the problems addressed here have treated the learning tasks in isolation. Examples include models of word segmentation from phonemic input (Christiansen et al., 1998; Brent, 1999; Venkataraman, 2001; Swingley, 2005) or phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2011; Boruta et al., 2011), models of phonetic clustering (Vallabha et al., 2007; Varadarajan et al., 2008; Dupoux et al., 2011) and phonological rule learning (Peperkamp et al., 2006; Martin et al., 2013). Elsner et al. (2012) present a model that is similar to ours, using a noisy channel model implemented with a finite-state transducer to learn about phonetic variability while clustering distinct tokens into lexical items. However (like the earlier lexical-phonetic learning model of Feldman et al. (2009; in press)) their model assumes known word boundaries, so to perform both segmentation and lexical-phonetic learning, they use a pipeline that first segments using GGJ and then applies their mode</context>
</contexts>
<marker>Varadarajan, Khudanpur, Dupoux, 2008</marker>
<rawString>Balakrishnan Varadarajan, Sanjeev Khudanpur, and Emmanuel Dupoux. 2008. Unsupervised learning of acoustic sub-word units. In Proceedings of the Association for Computational Linguistics: Short Papers, pages 165–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anand Venkataraman</author>
</authors>
<title>A statistical model for word discovery in transcribed speech.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="4651" citStr="Venkataraman, 2001" startWordPosition="692" endWordPosition="693">honetic and lexical representations in detail, drawing comparisons to experimental results on adult and infant speech processing. Taken together, our results support the idea that a Bayesian model that jointly performs word segmentation and phonetic learning provides a plausible explanation for many aspects of early phonetic and word learning in infants. 2 Related Work Nearly all computational models used to explore the problems addressed here have treated the learning tasks in isolation. Examples include models of word segmentation from phonemic input (Christiansen et al., 1998; Brent, 1999; Venkataraman, 2001; Swingley, 2005) or phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2011; Boruta et al., 2011), models of phonetic clustering (Vallabha et al., 2007; Varadarajan et al., 2008; Dupoux et al., 2011) and phonological rule learning (Peperkamp et al., 2006; Martin et al., 2013). Elsner et al. (2012) present a model that is similar to ours, using a noisy channel model implemented with a finite-state transducer to learn about phonetic variability while clustering distinct tokens into lexical items. However (like the earlier lexical-phonetic learning model of Feldman et al. (200</context>
</contexts>
<marker>Venkataraman, 2001</marker>
<rawString>Anand Venkataraman. 2001. A statistical model for word discovery in transcribed speech. Computational Linguistics, 27(3):351–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet F Werker</author>
<author>Richard C Tees</author>
</authors>
<title>Crosslanguage speech perception: Evidence for perceptual reorganization during the first year of life.</title>
<date>1984</date>
<journal>Infant Behavior and Development,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="1309" citStr="Werker and Tees, 1984" startWordPosition="194" endWordPosition="197">y more similar to human learners. On data with variable pronunciations, the pipelined approach learns to treat syllables or morphemes as words. In contrast, our joint model, like infant learners, tends to learn multiword collocations. We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence. 1 Introduction By the end of their first year, infants have acquired many of the basic elements of their native language. Their sensitivity to phonetic contrasts has become language-specific (Werker and Tees, 1984), and they have begun detecting words in fluent speech (Jusczyk and Aslin, 1995; Jusczyk et al., 1999) and learning word meanings (Bergelson and Swingley, 2012). These developmental cooccurrences lead some researchers to propose that phonetic and word learning occur jointly, each one informing the other (Swingley, 2009; Feldman et al., 2013). Previous computational models capture some aspects of this joint learning Sharon Goldwater sgwater@inf.ed.ac.uk ILCC, School of Informatics University of Edinburgh Frank Wood fwood@robots.ox.ac.uk Dept. of Engineering University of Oxford problem, but typ</context>
<context position="33631" citStr="Werker and Tees, 1984" startWordPosition="5463" endWordPosition="5466"> are somewhat more likely to find the right boundary of a vowel-initial token than the left boundary (although again this difference is larger for the EM system); this suggests that the problem is indeed caused by the initial segment. 5.4 Phonetic Learning We next compare phonetic variations learned by the model to characteristics of infant speech perception. Infants show an asymmetry between consonants and vowels, losing sensitivity to non-native vowel contrasts by eight months (Kuhl et al., 1992; Bosch and Sebasti´an-Gall´es, 2003) but to non-native consonant contrasts only by 10-12 months (Werker and Tees, 1984). The observed ordering is somewhat puzzling when one considers the availability for distributional information (Maye et al., 2002), which is much stronger for stop consonants than for vowels (Lisker and Abramson, 1964; Peterson and Barney, 1952). Infants are also conservative in generalizing across phonetic variability, showing a delayed abil49 ity to generalize across talkers, affects, and dialects. They have difficulty recognizing word tokens that are spoken by a different talker or in a different tone of voice until 11 months (Houston and Jusczyk, 2000; Singh et al., 2004), and the ability</context>
</contexts>
<marker>Werker, Tees, 1984</marker>
<rawString>Janet F. Werker and Richard C. Tees. 1984. Crosslanguage speech perception: Evidence for perceptual reorganization during the first year of life. Infant Behavior and Development, 7(1):49 – 63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katherine S White</author>
<author>Richard N Aslin</author>
</authors>
<title>Adaptation to novel accents by toddlers.</title>
<date>2011</date>
<journal>Developmental Science,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="34393" citStr="White and Aslin, 2011" startWordPosition="5585" endWordPosition="5588">s much stronger for stop consonants than for vowels (Lisker and Abramson, 1964; Peterson and Barney, 1952). Infants are also conservative in generalizing across phonetic variability, showing a delayed abil49 ity to generalize across talkers, affects, and dialects. They have difficulty recognizing word tokens that are spoken by a different talker or in a different tone of voice until 11 months (Houston and Jusczyk, 2000; Singh et al., 2004), and the ability to adapt to unfamiliar dialects appears to develop even later, between 15 and 19 months (Best et al., 2009; Heugten and Johnson, in press; White and Aslin, 2011). Similar to infants, our model shows both a vowelconsonant asymmetry and a reluctance to accept the full range of adult phonetic variability. Table 5 shows some segment-to-segment alternations learned in various transducers. The oracle learns a large amount of variation (u surfaces as itself only 68% of the time) involving many different segments, whereas EM is similar to infant learners in learning a more conservative solution with fewer alternations overall. Moreover, EM appears to identify patterns of variability in vowels before consonants. It learns a similar range of alternations for u </context>
</contexts>
<marker>White, Aslin, 2011</marker>
<rawString>Katherine S. White and Richard N. Aslin. 2011. Adaptation to novel accents by toddlers. Developmental Science, 14(2):372–384.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
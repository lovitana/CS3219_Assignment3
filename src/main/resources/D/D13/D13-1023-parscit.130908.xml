<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005300">
<title confidence="0.981517">
An Efficient Language Model Using Double-Array Structures
</title>
<author confidence="0.99425">
Makoto Yasuhara Toru Tanaka †Jun-ya Norimatsu Mikio Yamamoto
</author>
<affiliation confidence="0.9997485">
Department of Computer Science
University of Tsukuba, Japan
</affiliation>
<email confidence="0.998871">
†norimatsu@mibel.cs.tsukuba.ac.jp
</email>
<sectionHeader confidence="0.9986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999718863636364">
Ngram language models tend to increase in
size with inflating the corpus size, and con-
sume considerable resources. In this pa-
per, we propose an efficient method for im-
plementing ngram models based on double-
array structures. First, we propose a method
for representing backwards suffix trees using
double-array structures and demonstrate its ef-
ficiency. Next, we propose two optimization
methods for improving the efficiency of data
representation in the double-array structures.
Embedding probabilities into unused spaces
in double-array structures reduces the model
size. Moreover, tuning the word IDs in the
language model makes the model smaller and
faster. We also show that our method can be
used for building large language models using
the division method. Lastly, we show that our
method outperforms methods based on recent
related works from the viewpoints of model
size and query speed when both optimization
methods are used.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99951425">
Ngram language models (F. Jelinek, 1990) are
widely used as probabilistic models of sentence in
natural language processing. The wide use of the
Internet has entailed a dramatic increase in size of
the available corpora, which can be harnessed to ob-
tain a significant improvement in model quality. In
particular, Brants et al. (2007) have shown that the
performance of statistical machine translation sys-
tems is monotonically improved with the increas-
ing size of training corpora for the language model.
However, models using larger corpora also consume
more resources. In recent years, many methods for
improving the efficiency of language models have
been proposed to tackle this problem (Pauls and
Klein, 2011; Kenneth Heafield, 2011). Such meth-
ods not only reduce the required memory size but
also raise query speed.
In this paper, we propose the double-array lan-
guage model (DALM) which uses double-array
structures (Aoe, 1989). Double-array structures
are widely used in text processing, especially for
Japanese. They are known to provide a compact
representation of tries (Fredkin, 1960) and fast tran-
sitions between trie nodes. The ability to store and
manipulate tries efficiently is expected to increase
the performance of language models (i.e., improving
query speed and reducing the model size in terms of
memory) because tries are one of the most common
representations of data structures in language mod-
els. We use double-array structures to implement
a language model since we can utilize their speed
and compactness when querying the model about an
ngram.
In order to utilize of double-array structures as
language models, we modify them to be able to
store probabilities and backoff weights. We also
propose two optimization methods: embedding
and ordering. These methods reduce model size
and increase query speed. Embedding is an ef-
ficient method for storing ngram probabilities and
backoff weights, whereby we find vacant spaces in
the double-array language model structure and pop-
ulate them with language model information, such
as probabilities and backoff weights. Ordering is
</bodyText>
<page confidence="0.962558">
222
</page>
<note confidence="0.734025">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 222–232,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.99947325">
a method for compacting the double-array structure.
DALM uses word IDs for all words of the ngram,
and ordering assigns a word ID to each word
to reduce the model size. These two optimization
methods can be used simultaneously and are also ex-
pected to work well.
In our experiments, we use a language model
based on corpora of the NTCIR patent retrieval
task (Atsushi Fujii et al., 2007; Atsushi Fujii et al.,
2005; Atsushi Fujii et al., 2004; Makoto Iwayama et
al., 2003). The model size is 31 GB in the ARPA
file format. We conducted experiments focusing on
query speed and model size. The results indicate
that when the abovementioned optimization meth-
ods are used together, DALM outperforms state-of-
the-art methods on those points.
</bodyText>
<sectionHeader confidence="0.999972" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.982748">
2.1 Tries and Backwards Suffix Trees
</subsectionHeader>
<bodyText confidence="0.999288222222222">
Tries (Fredkin, 1960) are one of the most widely
used tree structures in ngram language models since
they can reduce memory requirements by sharing
common prefix. Moreover, since the query speed
for tries depends only on the number of input words,
the query speed remains constant even if the ngram
model increases in size.
Backwards suffix trees (Bell et al., 1990; Stolcke,
2002; Germann et al., 2009) are among the most
efficient representations of tries for language mod-
els. They contain ngrams in reverse order of history
words.
Figure 1 shows an example of a backwards suf-
fix tree representation. In this paper, we denote an
ngram: by the form w1, w2, · · · , w,,, as w,,,1. In this
example, word lists (represented as rectangular ta-
bles) contain target words (here, w,,,) of ngrams, and
circled words in the tree denote history words (here,
</bodyText>
<equation confidence="0.65044">
w,,,−1
</equation>
<bodyText confidence="0.9713676">
1 ) associated with target words. The history
words “I eat,” “you eat”, and “do you eat” are stored
in reverse order. Querying this trie about an ngram is
simple: just trace history words in reverse and then
find the target word in a list. For example, consider
querying about the trigram “I eat fish”. First, simply
trace the history in the trie in reverse order (“eat” →
“I”); then, find “fish” in list &lt;1&gt;. Similarly, query-
ing a backwards suffix tree about unknown ngrams
is also efficient, because the backwards suffix tree
</bodyText>
<figureCaption confidence="0.99267575">
Figure 1: Example of a backwards suffix tree. There
are two branch types in a backwards suffix tree: history
words and target words. History words are shown in cir-
cles and target words are stored in word lists.
</figureCaption>
<bodyText confidence="0.999923176470588">
representation is highly suitable for the backoff cal-
culation. For example, in querying about the 4gram
“do you eat soup”, we first trace “eat” → “you” →
“do” in a manner similar to above. However, search-
ing for the word “soup” in list &lt;3&gt; fails because
list &lt;3&gt; does not contain the word “soup”. In this
case, we return to the node “you” to search the list
&lt;2&gt;, where we find “soup”. This means that the tri-
gram “you eat soup” is contained in the tree while
the 4gram “do you eat soup” is not. This behavior
can be efficiently used for backoff calculation.
SRILM (Stolcke, 2002) is a widely used language
model toolkit. It utilizes backwards suffix trees for
its data structures. In SRILM, tries are implemented
as 64-bit pointer links, which wastes a lot of mem-
ory. On the other hand, the access speed for ngram
probabilities is relatively high.
</bodyText>
<subsectionHeader confidence="0.999209">
2.2 Efficient Language Models
</subsectionHeader>
<bodyText confidence="0.9997285">
In recent years, several methods have been proposed
for storing language models efficiently in memory.
Talbot and Osborne (2007) have proposed an effi-
cient method based on bloom filters. This method
modifies bloom filters to store count information
about training sets. In prior work, bloom filters
have been used for checking whether certain data
are contained in a set. To store the count informa-
tion, pairs from &lt;ngram,1&gt; to &lt;ngram,count&gt; are
all added to the set for each ngram. To query this
language model about the probability of an ngram,
probabilities are calculated during querying by us-
ing these counts. Talbot and Brants (2008) have pro-
posed a method based on perfect hash functions and
bloomier filters. This method uses perfect hash func-
tions to store ngrams and encode values (for exam-
</bodyText>
<page confidence="0.997772">
223
</page>
<bodyText confidence="0.999635458333333">
ple, probabilities or counts of ngrams in the training
corpus) to a large array. Guthrie and Hepple (2010)
have proposed a language model called ShefLM that
uses minimal perfect hash functions (Belazzougui et
al., 2009), which can store ngrams without vacant
spaces. Furthermore, values are compressed by sim-
ple dense coding (Fredriksson and Nikitin, 2007).
ShefLM achieves a high compression ratio when
it stores counts of ngrams in the training corpus.
However, when this method stores probabilities of
ngrams, the advantage of using compression is lim-
ited because floating-point numbers are difficult to
compress. Generally, compression is performed by
combining the same values but, two floating-point
numbers are rarely the same, especially in the case
of probability values1. These methods implement
lossy language models, meaning that, we can re-
duce the model size at the expense of model qual-
ity. These methods also reduce the model perfor-
mance (perplexity).
Pauls and Klein (2011) have proposed Berke-
leyLM which is based on an implicit encoding struc-
ture, where ngrams are separated according to their
order, and are sorted by word ID. The sorted ngrams
are linked to each other like a trie structure. Berke-
leyLM provides rather efficient methods. Variable-
length coding and block compression are used if
small model size is more important than query
speed. In addition, Heafield (2011) has proposed
an efficient language model toolkit called KenLM
that has been recently used in machine translation
systems, for which large language models are of-
ten needed. KenLM has two different main structure
types: trie and probing. The trie structure is
compact but relatively slower to query, whereas the
probing structure is relatively larger but faster to
query.
In this paper, we propose a language model struc-
ture based on double-array structures. As we de-
scribe in Section 3, double-array structures can be
used as fast and compact representations of tries.
We propose several techniques for maximizing the
performance of double-array structures from the per-
spective of query speed and model size.
1In our experience, it is considerably easier to compress
backoff weights than to compress probabilities, although both
are represented with floating-point numbers. We use this knowl-
edge in our methods.
</bodyText>
<sectionHeader confidence="0.994108" genericHeader="method">
3 Double-Array
</sectionHeader>
<subsectionHeader confidence="0.999862">
3.1 Double-Array Structure
</subsectionHeader>
<bodyText confidence="0.999427658536585">
In DALM, we use a double-array structure (Aoe,
1989) to represent the trie of a language model.
Double-array structures are trie representations con-
sisting of two parallel arrays: BASE and CHECK.
They are not only fast to query, but also provide a
compact way to store tries. In the structure, nodes in
the trie are represented by slots with the same index
in both arrays. Before proposing several efficient
language model representation techniques in Section
4, we introduce double-array themselves. In addi-
tion, the construction algorithms for double-arrays
are described in Section 3.2 and Section 3.3.
The most naive implementation of a trie will have
a two-dimensional array NEXT. Let WORDID (w)
be a function that returns a word ID as a
number corresponding to its argument word w;
then NEXT [n][WORDID (w)] (that presents the
WORDID(w)-th slot of the nth row in the NEXT
array) stores the node number which can be transit
from the node number n by the word w, and we can
traverse the trie efficiently and easily to serialize the
array in memory. This idea is simple but wastes the
most of the used memory because almost all of the
slots are unused and this results in occupying mem-
ory space. The double-array structures solve this
problem by taking advantage of the sparseness of the
NEXT array. The two-dimensional array NEXT
is merged into a one-dimensional array BASE by
shifting the entries of each row of the NEXT array
and combining the set of resulting arrays. We can
store this result in much less memory than the se-
rialization of the naive implementation above. Ad-
ditionally, a CHECK array is introduced to check
whether the transition is valid or not because we can-
not distinguish which node the information in a par-
ticular slot comes from. Using a CHECK array, we
can avoid transition errors and move safely to the
child node of any chosen node.
As a definition, a node link from a node ns with
a word w to the next node nn,,,tin the trie is defined
as follows:
</bodyText>
<construct confidence="0.9006635">
next &lt;-- BASE[s] + WORDID (w)
if CHECK [next] == s
</construct>
<bodyText confidence="0.99822">
where s denotes the index of the slot in the double-
</bodyText>
<page confidence="0.997705">
224
</page>
<figureCaption confidence="0.99896075">
Figure 2: A trie and a corresponding double-array struc-
ture. Node ns is represented by the slots BASE[s] and
CHECK[s]. A link from a node ns with a word w is
indicated by CHECK [next] == s.
</figureCaption>
<bodyText confidence="0.999568166666667">
array structure which represents ns. The trie tran-
sition from a node ns with a word w is applied ac-
cording to the following steps:
Step 1 Calculating the “next” destination and
Step 2 Checking whether the transition is correct.
Step 2 specifically means the following:
</bodyText>
<listItem confidence="0.9182892">
1. If CHECK [next] == s, then we can “move”
to the node nn,xt;
2. otherwise, we can detect that the transition
from the node ns with the word w is not con-
tained in the trie.
</listItem>
<bodyText confidence="0.995317263157895">
Figure 2 shows an example of a transition from a
parent node ns with a word w.
Next, we describe how the existence of an ngram
history can be determined (Aoe, 1989). We can it-
erate over the nodes by the transitions shown above
and may find the node representing an ngram his-
tory. But we should check that it is valid because
nodes except for leaf nodes possiblly represent a
fragment of some total ngram history. We can use
endmarker symbols to determine whether an ngram
history is in the trie. We add nodes meaning the end-
marker symbol after the last node of each ngram his-
tory. When querying about wn−1
1 , we transit repeat-
edly; in other words, we set s = 0 and start by ap-
plying Step 1 and 2 repeatedly for each word. When
we reach the node wn−1, we continue searching for
an endmarker symbol. If the symbol is found, we
know that the ngram history wn−1
</bodyText>
<equation confidence="0.735767">
1 is in the trie.
</equation>
<bodyText confidence="0.683836666666667">
The double-array structure consumes 8 bytes per
node because the BASE and CHECK arrays are
4 byte array variables. Therefore, the structure can
</bodyText>
<figureCaption confidence="0.995740666666667">
Figure 3: Greedy insertion of trie elements. The children
of a node are collectively inserted into the double-array
when the BASE value of the node is fixed.
</figureCaption>
<bodyText confidence="0.9996732">
store nodes compactly in case of a high filling rate.
Moreover, node transitions are very fast because
they require only one addition and one comparison
per transition. We use a double-array structure in
DALM, which can maximize its potential.
</bodyText>
<subsectionHeader confidence="0.996699">
3.2 Greedy Construction
</subsectionHeader>
<bodyText confidence="0.999909666666667">
Greedy algorithms are widely used for construct-
ing static double-array structures2. The construction
steps are as follows:
</bodyText>
<listItem confidence="0.9701576">
1. Define the root node of a trie to correspond to
index 0 of the double-array structure and
2. Find the BASE value greedily (i.e., in order
1, 2, 3, · · ·) for all nodes which have fixed their
indices in the double-array structure.
</listItem>
<bodyText confidence="0.999735444444445">
In practice, once the BASE value of a node is fixed,
the positions of its children are fixed at the same
time, and we can find the BASE values for each
child recursively.
Figure 3 shows an example of such construc-
tion. In this example, three nodes (“I”, “you” and
“they”) are inserted at the same time. This is be-
cause the above three node positions are fixed by
the BASE value of the node “eat”. To insert nodes
</bodyText>
<footnote confidence="0.982843">
2We were unable to find an original source for this tech-
nique. However, this method is commonly used in double-array
implementations.
</footnote>
<page confidence="0.996836">
225
</page>
<bodyText confidence="0.937583">
“I”, “you” and “they”, the following three slots must
be empty (i.e., the slots must not be used by other
nodes.):
</bodyText>
<listItem confidence="0.999900333333333">
• BASE[s] + WORDID(“I”)
• BASE[s] + WORDID(“you”)
• BASE[s] + WORDID(“they”)
</listItem>
<bodyText confidence="0.999805666666667">
where s is the index of the node “eat”. At the con-
struction step, we need to find BASE[s] which sat-
isfies the above conditions.
</bodyText>
<subsectionHeader confidence="0.995402">
3.3 Efficient Construction Algorithm
</subsectionHeader>
<bodyText confidence="0.999562741935484">
The construction time for a double-array structure
poses the greatest challenge. We use a more effi-
cient method (Nakamura and Mochizuki, 2006) in-
stead of the naive method for constructing a double-
array structure because the naive method requires a
long time. We call the method “empty doubly-linked
list”. This algorithm is one of the most efficient con-
struction methods devised to date. Figure 4 shows
an example of an empty doubly-linked list. We can
efficiently define the BASE value of each node by
using the CHECK array to store the next empty slot.
In this example, in searching the BASE value of a
node, the first child node can be set to position 1,
and if that fails, we can successively try positions
3, 4, 6, 8, • • • by tracing the list instead of searching
all BASE values 0, 1, 2, 3, 4, 5, • • •.
As analyzed by Nakamura and Mochizuki(2006),
the computational cost of a node insertion is less
than in the naive method. The original naive method
requires O(NM) time for a node insertion, where
M is a number of unique word types and N is a
number of nodes of the trie; the algorithm using an
empty double-linked list requires O(UM), where U
is the number of unused slots.
As described in Section 5, we divide the trie into
several smaller tries and apply the efficient method
for constructing our largest models. This is because
it is not feasible to wait several weeks for the large
language model structure to be built. The dividing
method is currently the only method allowing us to
build them.
</bodyText>
<figureCaption confidence="0.997373666666666">
Figure 4: Empty doubly-linked list. Unused CHECK
slots are used to indicate the next unused slots, and un-
used BASE slots are used to indicate previous unused
slots. Thus, the BASE and CHECK arrays are used as a
doubly-linked list which can reduce the number of inef-
fective trials.
</figureCaption>
<sectionHeader confidence="0.8138845" genericHeader="method">
4 Proposed Methods
4.1 DALM
</sectionHeader>
<bodyText confidence="0.999985">
In this section, we present the application of the
double-array structure to backwards suffix trees. As
this is the most basic structure based on double-array
structures, we refer to it as the simple structure
and improve its performance as described in the fol-
lowing sections.
To represent a backwards suffix tree as a double-
array structure, we should modify the tree because it
has two types of branches (target words and history
nodes), which must be distinguished in the double-
array structure. Instead, we should distinguish the
branch type which indicates whether the node is a
target word or a history word. We use the endmarker
symbol (&lt;#&gt;) for branch discrimination. In prior
work, the endmarker symbol has been used to indi-
cate whether an ngram is in the trie. However, there
is no need to distinguish whether the node of the tree
is included in the language model because all nodes
of a backwards suffix tree which represents ngrams
surely exist in the model. We use the endmarker
symbol to indicate nodes which are end-of-history
words. Therefore, target words of ngrams are chil-
dren of the endmarker symbols that they follow.
By using the endmarker symbol, target words can
be treated the same as ordinary nodes because all tar-
get words are positioned after &lt;#&gt;. Figure 5 shows
an example of such construction. We can clearly dis-
tinguish target words and history words in the back-
wards suffix tree.
Querying in the tree is rather simple. For exam-
ple, consider the case of a query trigram “I eat fish”
in the trie of Figure 5. We can trace this trigram in
</bodyText>
<page confidence="0.998771">
226
</page>
<figureCaption confidence="0.9982892">
Figure 5: An example of converting a backwards suffix
tree. We introduce endmarker symbols to distinguish the
two branch types. We can treat the tree as an ordinary trie
that can be represented by a double-array structure while
retaining the advantages of the tree structure.
</figureCaption>
<bodyText confidence="0.999618">
the same way as the original backwards suffix tree.
First, we trace “eat” —* “I”, then trace that to the end-
marker symbol &lt;#&gt; and finally find the word “fish”.
Next, we describe the procedure for storing prob-
abilities and backoff weights. We prepare a VALUE
array to store the probabilities and backoff weights
of ngrams. Figure 6 shows the simple DALM
structure. The backwards suffix tree stores a back-
off weight for each node and a probability for each
target word. In simple DALM, each value is
stored for the respective position of the correspond-
ing node.
</bodyText>
<subsectionHeader confidence="0.972969">
4.2 Embedding
</subsectionHeader>
<bodyText confidence="0.9999545">
Embedding is a method for reducing model size.
In the simple DALM structure, there are many va-
cant spaces in the BASE and CHECK arrays. We
use these vacant spaces to store backoff weights and
probabilities. Figure 7 shows vacant spaces in the
simple DALM structure.
First, the BASE array slots of target word nodes
are unused because target words are always in leaf
positions in the backwards suffix tree and do not
have any children nodes. In the example of Figure 7,
BASE[9] is not used, and therefore can be used for
storing a probability value. This method can reduce
the model size because all probabilities are stored
into the BASE array. As a result, the VALUE array
</bodyText>
<figureCaption confidence="0.994983833333333">
Figure 6: The simple DALM data structure. The
BASE and CHECK arrays are used in the same way
as in a double-array structure. To return probabilities and
backoff weights, a VALUE array is introduced.
Figure 7: Unused slots in the simple DALM structure
used for other types of information, such as probabilities.
</figureCaption>
<bodyText confidence="0.996599684210527">
contains only backoff weights.
Next, the CHECK array slots of endmarker sym-
bols are also vacant. We do not need to check for
endmarker symbol transition because the endmarker
symbol &lt;#&gt; is seen for all nodes except target word
nodes. This means that all endmarker symbol tran-
sitions are ensured to be correct and the CHECK
array slots of endmarker symbols do not need to be
used. We use this space to store backoff weights.
In order to avoid false positives, we cannot store
backoff weights directly. Instead, we store the po-
sitions of the backoff weights in the VALUE array
as negative numbers. When a query for an unknown
ngram encounters an endmarker symbol node, the
value of the CHECK array is never matched be-
cause the corresponding value stored there is neg-
ative. The same values in the VALUE array can be
unified to reduce the memory requirements. Figure
8 illustrates an example of the embedding method.
</bodyText>
<page confidence="0.997279">
227
</page>
<figureCaption confidence="0.988001857142857">
Figure 8: Implementation of the embedding method.
We use vacant spaces in the VALUE array to store the
probabilities and indices of backoff weights. The in-
dices of backoff weights are taken with a negative sign
to avoid false positives. Backoff weights are stored in the
VALUE array, and the same values in the VALUE array
can be unified.
</figureCaption>
<subsectionHeader confidence="0.988913">
4.3 Ordering
</subsectionHeader>
<bodyText confidence="0.989887321428571">
Ordering is a method for shortening the double-
array structure and increasing the query speed. In
ordering, word IDs are assigned in order of un-
igram probability. This is done at a preprocessing
stage, before the DALM is built.
Before explaining the reasons why this method
is effective, we present an interpretation of double-
array construction in Figure 9 which corresponds to
the case presented in Figure 3. In the previous
section, we pointed out that the insertion problem
is equivalent to the problem of finding the BASE
value of the parent node. Here, we expand this fur-
ther into the idea that finding the BASE value is
equivalent to the problem of finding the shift length
of an insertion array. We can create an insertion
array which is an array of flag bits set to 1 at the po-
sitions of word IDs of children nodes’ words. More-
over, we prepare a used array which is also a flag
bit array denoting whether the original slots in the
double-array structure are occupied. In this situ-
ation, finding the shift length is equivalent to the
problem of finding the BASE value of the slot for
the node “eat”, and the combined used array denotes
the size of the double-array structure after insertion.
Figure 10 shows an intuitive example illustrating
the efficiency of the ordering method. When
word IDs are assigned in order of unigram proba-
bility, 1s in the insertion array are gathered toward
</bodyText>
<figureCaption confidence="0.91636">
Figure 9: Interpretation of a double-array construction.
</figureCaption>
<bodyText confidence="0.882162111111111">
The insertion problem for the double-array structure is
interpreted as a finding problem of a shift length of the
insertion array. We can measure the size of the double-
array structure in the used array.
the beginning of the array. This means that 1s in
the insertion array form clusters, which makes in-
sertion easier than for unordered insertion arrays.
This shortens the shift lengths for each insertion ar-
ray: a shorter double-array structure results.
</bodyText>
<sectionHeader confidence="0.999494" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<subsectionHeader confidence="0.998647">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9997348">
To compare the performance of DALM with other
methods, we conduct experiments on two ngram
models built from small and large training corpora.
Table 1 shows the specifications of the model.
Training data are extracted from the Publication
of unexamined Japanese patent applications, which
is distributed with the NTCIR 3,4,5,6 patent retrieval
task (Atsushi Fujii et al., 2007; Atsushi Fujii et al.,
2005; Atsushi Fujii et al., 2004; Makoto Iwayama
et al., 2003). We used data for the period from
</bodyText>
<page confidence="0.993619">
228
</page>
<figureCaption confidence="0.99655475">
Figure 11: Comparison between tuned and non-tuned
double-array structures.
Figure 10: An example of word ID ordering effi-
ciency. Word IDs in the insertion array are packed to
the front in advance. Therefore, shift lengths for ordered
arrays are often shorter than unordered ones. The result-
ing size of the double-array structure is expected to be
smaller than that of an unordered array.
</figureCaption>
<tableCaption confidence="0.99716">
Table 1: Corpus and model specifications.
</tableCaption>
<table confidence="0.997088333333333">
Model Corpus Unique Ngram
Size Type Type
(words) (words) (1-5gram)
100 Mwords 100 M 195 K 31 M
5 Gwords 5 G 2,140 K 936 M
Test set 100 M 198 K -
</table>
<bodyText confidence="0.998051857142857">
1,993 to 2,002 and extracted paragraphs containing
“background” and “example”. This method is simi-
lar to the NTCIR 7 Patent Translation Task(Fujii et
al., 2008). The small and large training data sets
contained 100 Mwords and 5 Gwords, respectively.
Furthermore, we sampled another 100 Mwords as
a test set to measure the access speed for extract-
ing ngram probabilities. We used an Intel ⃝R Xeon ⃝R
X5675 (3.07 GHz) 24-core server with 142 GB of
RAM.
Our experiments were performed from the view-
points of speed and model size. We executed each
program twice, and the results of the second run
were taken as the final performance.
</bodyText>
<tableCaption confidence="0.994799">
Table 2: Comparison between tuned and non-tuned
double-array structures.
</tableCaption>
<table confidence="0.999507833333333">
Method Size Speed
(MB) (queries/s)
Simple 1,152 1,065,536
Embedding 782 1,004,555
Ordering 726 1,083,703
Both 498 1,057,607
</table>
<subsectionHeader confidence="0.962353">
5.2 Optimization Methods
</subsectionHeader>
<bodyText confidence="0.999994">
We compared the performance of the DALMs
proposed here, namely simple, embedding,
ordering and both, where both indicates that
the language model uses both embedding and
ordering. We conducted experiments examin-
ing how these methods affect the size of the double-
array structures and the query speeds. We used the
100 Mwords model in the comparison because it
was difficult to build a DALM using the 5 Gwords
model.
The results are shown in Figure 11 and Table 2.
While both ordering and embedding decreased
the model size, the query speed was increased by the
former and decreased by the latter. Both was the
smallest and most balanced method.
</bodyText>
<subsectionHeader confidence="0.997191">
5.3 Divided Double-Array Structure
</subsectionHeader>
<bodyText confidence="0.9997148">
Building a double-array structure requires a long
time, which can sometimes be impractical. In fact,
as mentioned above, waiting on construction of the
double-array structure of the 5 Gwords model is in-
feasible.
</bodyText>
<page confidence="0.994935">
229
</page>
<table confidence="0.564661142857143">
Size Speed
Number of parts
(MB) (queries/s)
1 498 1,057,607
2 502 1,105,358
4 510 1,087,619
8 540 1,098,594
</table>
<tableCaption confidence="0.7620455">
Table 3: Comparison between divided and original
double-array structures.
</tableCaption>
<figureCaption confidence="0.9805615">
Figure 12: Comparison between divided and original
double-array structures.
</figureCaption>
<bodyText confidence="0.999119738095238">
As described in Section 3.3, the efficient algo-
rithm requires O(UM) time to insert one node and
the insertion is iterated N (the total number of inser-
tions) times. If we assume that the number of unused
slots at the ith insertion, Ui, is proportional to i, or
that Ui = c x i where c is a proportionality con-
stant, we can calculate the building time as follows:
∑N1 UiM = O (MN2).
To shorten the build time, we divided the original
trie into several parts. Building parts of the origi-
nal trie is possible because N is reduced. Moreover,
these double-array structures can be built in parallel.
Note that query results for both original and divided
tries are completely equivalent because divided tries
hold all the ngram statistics of the original trie. This
method is similar to that used in randomized lan-
guage models (Talbot and Brants, 2008).
We compared the differences between the meth-
ods using the original and divided double-array
structures. In the comparison, we also used the 100
Mwords model with the both optimization method
described in the previous section (Figure 12 and Ta-
ble 3).
Although dividing the trie increased the size of
the DALM slightly, the model size was still smaller
than that without optimization. Query speed in-
creased as the number of parts was increased. We
attributed this to the divided DALM consisting of
several double-array structures, each smaller than
the undivided structure which results in an increase.
Figure 12 shows that there is a trade-off relation be-
tween model size and query speed.
Below, we use the 5 Gwords model in our exper-
iments. In our environment, building a 5 Gwords
double-array structure required about 4 days when
the double-array structures were divided into 8 parts,
even though we used the more efficient algorithm
described in Section 3.3. The time required for
building the model when the original structure was
divided into less than 8 parts was too long. Thus,
a more efficient building algorithm is essential for
advancing this research further.
</bodyText>
<subsectionHeader confidence="0.997678">
5.4 Comparison with Other Methods
</subsectionHeader>
<bodyText confidence="0.999975074074074">
Using the 100 Mwords and 5 Gwords mod-
els, we compared DALM with other meth-
ods (KenLM (Kenneth Heafield, 2011) and
SRILM (Stolcke, 2002)). In this experiment, we
used the both method (which is mentioned above)
for DALM and divided the original trie into 8 parts
and built double-array structures.
The results are shown in Figure 13 and Table 4;
the group on the left shows the results for the 100
Mwords model and the group on the right shows the
results for the 5 Gwords model.
The experimental results clearly indicate that
DALM is the fastest of all the compared methods
and that the model size is nearly the same or slightly
smaller than that of KenLM (Probing). Whereas
KenLM (Trie) is the smallest model, it is slower
than DALM.
The differences between the 5 Gwords versions
of DALM and KenLM (Probing) are smaller in
comparison with the 100 Mwords models. This is
because hash-based language models have an ad-
vantage when storing higher-order ngrams. Large
language models have more 5grams, which leads to
shorter backoff times. On the other hand, trie-based
language models have to trace higher-order ngrams
for every query, which requires more time.
Finally, we discuss practical situations. We con-
</bodyText>
<page confidence="0.997851">
230
</page>
<tableCaption confidence="0.999951">
Table 4: Comparison between DALM and other methods.
</tableCaption>
<table confidence="0.999232857142857">
100 Mwords Model 5 Gwords Model
LM Size Speed Size Speed
(MB) (queries/s) (MB) (queries/s)
SRILM 1,194 894,138 31,747 729,447
KenLM (Probing) 665 1,002,489 18,685 913,208
KenLM (Trie) 340 804,513 9,606 635,300
DALM (8 parts) 540 1,098,594 15,345 953,186
</table>
<figureCaption confidence="0.9872285">
Figure 13: Comparison between DALM and other lan-
guage model systems.
</figureCaption>
<bodyText confidence="0.999967230769231">
ducted this study’s experiments using test set text
written by humans. In some applications such as
statistical machine translations, language model sys-
tems should compute probabilities of many unnatu-
ral ngrams which will be unknown. This may affect
query speed because querying unknown and unnat-
ural ngrams generate many backoffs. They may re-
sults in trie-based LM being slightly faster, because
traversing the trie can stop immediately when it de-
tects that a queried ngram history is not contained in
the trie. On the other hand, hash-based LM such as
KenLM probing would repeat queries until finding
truncated ngram histories in the trie.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999982428571428">
We proposed a method for implementing language
models based on double-array structures. We call
this method DALM. Moreover, we proposed two
methods for optimizing DALM: embedding and
ordering. Embedding is a method whereby
empty spaces in arrays are used to store ngram prob-
abilities and backoff weights, and ordering is a
method for numbering word IDs; these methods re-
duce model size and increase query speed. These
two optimization methods work well independently,
but even better performance can be achieved if they
are combined.
We also used a division method to build the model
structure in several parts in order to speed up the
construction of double-array structures. Although
this procedure results in a slight increase in model
size, the divided double-array structures mostly re-
tained the compactness and speed of the original
structure. The time required for building double-
array structures is the bottleneck of DALM as it is
sometimes too long to be practical, even though the
model structure itself achieves high performance. In
future work, we will develop a faster algorithm for
building double-array structures.
While DALM has outperformed state-of-the-art
language model implementations methods in our ex-
periments, we should continue to consider ways to
optimize the method for higher-order ngrams.
</bodyText>
<sectionHeader confidence="0.999196" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998729">
We thank the anonymous reviewers for many valu-
able comments. This work is supported by JSPS
KAKENHI Grant Number 24650063.
</bodyText>
<sectionHeader confidence="0.999452" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998999111111111">
J.-I. Aoe. 1989. An Efficient Digital Search Algorithm
by Using a Double-Array Structure. IEEE Transac-
tions on Software Engineering, 15(9):1066–1077.
Atsushi Fujii, Makoto Iwayama, and Noriko Kando.
2004. Overview of the Patent Retrieval Task at
NTCIR-4.
Atsushi Fujii, Makoto Iwayama, and Noriko Kando.
2005. Overview of Patent Retrieval Task at NTCIR-
5.
</reference>
<page confidence="0.963248">
231
</page>
<reference confidence="0.999900551724138">
Atsushi Fujii, Makoto Iwayama, and Noriko Kando.
2007. Overview of the Patent Retrieval Task at the
NTCIR-6 Workshop. pages 359–365.
Djamal Belazzougui, Fabiano C. Botelho, and Martin Di-
etzfelbinger. 2009. Hash, displace, and compress. In
ESA, pages 682–693.
Timothy C. Bell, John G. Cleary, and Ian H. Witten.
1990. Text compression.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large Language Models in
Machine Translation. In Proceedings of the 2007 Joint
Conference on EMNLP-CoNLL, pages 858–867. ACL.
B. Merialdo F. Jelinek. 1990. Self-organized language
modeling for speech recognition.
Edward Fredkin. 1960. Trie memory. Communications
of the ACM, 3(9):490–499.
Kimmo Fredriksson and Fedor Nikitin. 2007. Simple
Compression Code Supporting Random Access and
Fast String Matching. In Proceedings of the 6th in-
ternational conference on Experimental algorithms,
WEA’07, pages 203–216. Springer-Verlag.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the Patent Trans-
lation Task at the NTCIR-7 Workshop.
Ulrich Germann, Eric Joanis, and Samuel Larkin. 2009.
Tightly Packed Tries: How to Fit Large Models into
Memory, and Make them Load Fast, Too. In Proceed-
ings of the Workshop on SETQA-NLP, pages 31–39.
ACL.
David Guthrie and Mark Hepple. 2010. Storing the Web
in Memory: Space Efficient Language Models with
Constant Time Retrieval. In Proceedings of the 2010
Conference on EMNLP, pages 262–272. ACL.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation. ACL.
Makoto Iwayama, Atsushi Fujii, Noriko Kando, and Aki-
hiko Takano. 2003. Overview of Patent Retrieval Task
at NTCIR-3.
Yasumasa Nakamura and Hisatoshi Mochizuki. 2006.
Fast Computation of Updating Method of a Dictio-
nary for Compression Digital Search Tree. Trans-
actions of Information Processing Society of Japan.
Data, 47(13):16–27.
Adam Pauls and Dan Klein. 2011. Faster and Smaller
N-Gram Language Models. In Proceedings of the
49th Annual Meeting of the ACL-HLT, pages 258–267.
ACL.
A. Stolcke. 2002. SRILM-an Extensible Language Mod-
eling Toolkit. Seventh International Conference on
Spoken Language Processing.
David Talbot and Thorsten Brants. 2008. Randomized
Language Models via Perfect Hash Functions. In Pro-
ceedings of ACL-08: HLT.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom Filter Language Models: Tera-Scale LMs on
the Cheap. In Proceedings of the 2007 Joint Confer-
ence on EMNLP-CoNLL, pages 468–476.
</reference>
<page confidence="0.995174">
232
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.991016">
<title confidence="0.999932">An Efficient Language Model Using Double-Array Structures</title>
<author confidence="0.994979">Yasuhara Toru Tanaka Norimatsu Mikio</author>
<affiliation confidence="0.9998775">Department of Computer University of Tsukuba,</affiliation>
<abstract confidence="0.999837391304348">language models tend to increase in size with inflating the corpus size, and consume considerable resources. In this paper, we propose an efficient method for immodels based on doublearray structures. First, we propose a method for representing backwards suffix trees using double-array structures and demonstrate its efficiency. Next, we propose two optimization methods for improving the efficiency of data representation in the double-array structures. Embedding probabilities into unused spaces in double-array structures reduces the model size. Moreover, tuning the word IDs in the language model makes the model smaller and faster. We also show that our method can be used for building large language models using the division method. Lastly, we show that our method outperforms methods based on recent related works from the viewpoints of model size and query speed when both optimization methods are used.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J-I Aoe</author>
</authors>
<title>An Efficient Digital Search Algorithm by Using a Double-Array Structure.</title>
<date>1989</date>
<journal>IEEE Transactions on Software Engineering,</journal>
<volume>15</volume>
<issue>9</issue>
<contexts>
<context position="2106" citStr="Aoe, 1989" startWordPosition="317" endWordPosition="318"> al. (2007) have shown that the performance of statistical machine translation systems is monotonically improved with the increasing size of training corpora for the language model. However, models using larger corpora also consume more resources. In recent years, many methods for improving the efficiency of language models have been proposed to tackle this problem (Pauls and Klein, 2011; Kenneth Heafield, 2011). Such methods not only reduce the required memory size but also raise query speed. In this paper, we propose the double-array language model (DALM) which uses double-array structures (Aoe, 1989). Double-array structures are widely used in text processing, especially for Japanese. They are known to provide a compact representation of tries (Fredkin, 1960) and fast transitions between trie nodes. The ability to store and manipulate tries efficiently is expected to increase the performance of language models (i.e., improving query speed and reducing the model size in terms of memory) because tries are one of the most common representations of data structures in language models. We use double-array structures to implement a language model since we can utilize their speed and compactness </context>
<context position="9940" citStr="Aoe, 1989" startWordPosition="1601" endWordPosition="1602"> a language model structure based on double-array structures. As we describe in Section 3, double-array structures can be used as fast and compact representations of tries. We propose several techniques for maximizing the performance of double-array structures from the perspective of query speed and model size. 1In our experience, it is considerably easier to compress backoff weights than to compress probabilities, although both are represented with floating-point numbers. We use this knowledge in our methods. 3 Double-Array 3.1 Double-Array Structure In DALM, we use a double-array structure (Aoe, 1989) to represent the trie of a language model. Double-array structures are trie representations consisting of two parallel arrays: BASE and CHECK. They are not only fast to query, but also provide a compact way to store tries. In the structure, nodes in the trie are represented by slots with the same index in both arrays. Before proposing several efficient language model representation techniques in Section 4, we introduce double-array themselves. In addition, the construction algorithms for double-arrays are described in Section 3.2 and Section 3.3. The most naive implementation of a trie will h</context>
<context position="12771" citStr="Aoe, 1989" startWordPosition="2113" endWordPosition="2114">s. array structure which represents ns. The trie transition from a node ns with a word w is applied according to the following steps: Step 1 Calculating the “next” destination and Step 2 Checking whether the transition is correct. Step 2 specifically means the following: 1. If CHECK [next] == s, then we can “move” to the node nn,xt; 2. otherwise, we can detect that the transition from the node ns with the word w is not contained in the trie. Figure 2 shows an example of a transition from a parent node ns with a word w. Next, we describe how the existence of an ngram history can be determined (Aoe, 1989). We can iterate over the nodes by the transitions shown above and may find the node representing an ngram history. But we should check that it is valid because nodes except for leaf nodes possiblly represent a fragment of some total ngram history. We can use endmarker symbols to determine whether an ngram history is in the trie. We add nodes meaning the endmarker symbol after the last node of each ngram history. When querying about wn−1 1 , we transit repeatedly; in other words, we set s = 0 and start by applying Step 1 and 2 repeatedly for each word. When we reach the node wn−1, we continue </context>
</contexts>
<marker>Aoe, 1989</marker>
<rawString>J.-I. Aoe. 1989. An Efficient Digital Search Algorithm by Using a Double-Array Structure. IEEE Transactions on Software Engineering, 15(9):1066–1077.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Makoto Iwayama</author>
<author>Noriko Kando</author>
</authors>
<title>Overview of the Patent Retrieval Task at NTCIR-4.</title>
<date>2004</date>
<contexts>
<context position="3918" citStr="Fujii et al., 2004" startWordPosition="600" endWordPosition="603">3 Conference on Empirical Methods in Natural Language Processing, pages 222–232, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics a method for compacting the double-array structure. DALM uses word IDs for all words of the ngram, and ordering assigns a word ID to each word to reduce the model size. These two optimization methods can be used simultaneously and are also expected to work well. In our experiments, we use a language model based on corpora of the NTCIR patent retrieval task (Atsushi Fujii et al., 2007; Atsushi Fujii et al., 2005; Atsushi Fujii et al., 2004; Makoto Iwayama et al., 2003). The model size is 31 GB in the ARPA file format. We conducted experiments focusing on query speed and model size. The results indicate that when the abovementioned optimization methods are used together, DALM outperforms state-ofthe-art methods on those points. 2 Related Work 2.1 Tries and Backwards Suffix Trees Tries (Fredkin, 1960) are one of the most widely used tree structures in ngram language models since they can reduce memory requirements by sharing common prefix. Moreover, since the query speed for tries depends only on the number of input words, the qu</context>
<context position="24185" citStr="Fujii et al., 2004" startWordPosition="4102" endWordPosition="4105">rtion easier than for unordered insertion arrays. This shortens the shift lengths for each insertion array: a shorter double-array structure results. 5 Experiment 5.1 Experimental Setup To compare the performance of DALM with other methods, we conduct experiments on two ngram models built from small and large training corpora. Table 1 shows the specifications of the model. Training data are extracted from the Publication of unexamined Japanese patent applications, which is distributed with the NTCIR 3,4,5,6 patent retrieval task (Atsushi Fujii et al., 2007; Atsushi Fujii et al., 2005; Atsushi Fujii et al., 2004; Makoto Iwayama et al., 2003). We used data for the period from 228 Figure 11: Comparison between tuned and non-tuned double-array structures. Figure 10: An example of word ID ordering efficiency. Word IDs in the insertion array are packed to the front in advance. Therefore, shift lengths for ordered arrays are often shorter than unordered ones. The resulting size of the double-array structure is expected to be smaller than that of an unordered array. Table 1: Corpus and model specifications. Model Corpus Unique Ngram Size Type Type (words) (words) (1-5gram) 100 Mwords 100 M 195 K 31 M 5 Gwor</context>
</contexts>
<marker>Fujii, Iwayama, Kando, 2004</marker>
<rawString>Atsushi Fujii, Makoto Iwayama, and Noriko Kando. 2004. Overview of the Patent Retrieval Task at NTCIR-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Makoto Iwayama</author>
<author>Noriko Kando</author>
</authors>
<date>2005</date>
<booktitle>Overview of Patent Retrieval Task at NTCIR5.</booktitle>
<contexts>
<context position="3890" citStr="Fujii et al., 2005" startWordPosition="595" endWordPosition="598">s 222 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 222–232, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics a method for compacting the double-array structure. DALM uses word IDs for all words of the ngram, and ordering assigns a word ID to each word to reduce the model size. These two optimization methods can be used simultaneously and are also expected to work well. In our experiments, we use a language model based on corpora of the NTCIR patent retrieval task (Atsushi Fujii et al., 2007; Atsushi Fujii et al., 2005; Atsushi Fujii et al., 2004; Makoto Iwayama et al., 2003). The model size is 31 GB in the ARPA file format. We conducted experiments focusing on query speed and model size. The results indicate that when the abovementioned optimization methods are used together, DALM outperforms state-ofthe-art methods on those points. 2 Related Work 2.1 Tries and Backwards Suffix Trees Tries (Fredkin, 1960) are one of the most widely used tree structures in ngram language models since they can reduce memory requirements by sharing common prefix. Moreover, since the query speed for tries depends only on the n</context>
<context position="24157" citStr="Fujii et al., 2005" startWordPosition="4097" endWordPosition="4100">m clusters, which makes insertion easier than for unordered insertion arrays. This shortens the shift lengths for each insertion array: a shorter double-array structure results. 5 Experiment 5.1 Experimental Setup To compare the performance of DALM with other methods, we conduct experiments on two ngram models built from small and large training corpora. Table 1 shows the specifications of the model. Training data are extracted from the Publication of unexamined Japanese patent applications, which is distributed with the NTCIR 3,4,5,6 patent retrieval task (Atsushi Fujii et al., 2007; Atsushi Fujii et al., 2005; Atsushi Fujii et al., 2004; Makoto Iwayama et al., 2003). We used data for the period from 228 Figure 11: Comparison between tuned and non-tuned double-array structures. Figure 10: An example of word ID ordering efficiency. Word IDs in the insertion array are packed to the front in advance. Therefore, shift lengths for ordered arrays are often shorter than unordered ones. The resulting size of the double-array structure is expected to be smaller than that of an unordered array. Table 1: Corpus and model specifications. Model Corpus Unique Ngram Size Type Type (words) (words) (1-5gram) 100 Mw</context>
</contexts>
<marker>Fujii, Iwayama, Kando, 2005</marker>
<rawString>Atsushi Fujii, Makoto Iwayama, and Noriko Kando. 2005. Overview of Patent Retrieval Task at NTCIR5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Makoto Iwayama</author>
<author>Noriko Kando</author>
</authors>
<date>2007</date>
<booktitle>Overview of the Patent Retrieval Task at the NTCIR-6 Workshop.</booktitle>
<pages>359--365</pages>
<contexts>
<context position="3862" citStr="Fujii et al., 2007" startWordPosition="590" endWordPosition="593"> backoff weights. Ordering is 222 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 222–232, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics a method for compacting the double-array structure. DALM uses word IDs for all words of the ngram, and ordering assigns a word ID to each word to reduce the model size. These two optimization methods can be used simultaneously and are also expected to work well. In our experiments, we use a language model based on corpora of the NTCIR patent retrieval task (Atsushi Fujii et al., 2007; Atsushi Fujii et al., 2005; Atsushi Fujii et al., 2004; Makoto Iwayama et al., 2003). The model size is 31 GB in the ARPA file format. We conducted experiments focusing on query speed and model size. The results indicate that when the abovementioned optimization methods are used together, DALM outperforms state-ofthe-art methods on those points. 2 Related Work 2.1 Tries and Backwards Suffix Trees Tries (Fredkin, 1960) are one of the most widely used tree structures in ngram language models since they can reduce memory requirements by sharing common prefix. Moreover, since the query speed for</context>
<context position="24129" citStr="Fujii et al., 2007" startWordPosition="4092" endWordPosition="4095">s in the insertion array form clusters, which makes insertion easier than for unordered insertion arrays. This shortens the shift lengths for each insertion array: a shorter double-array structure results. 5 Experiment 5.1 Experimental Setup To compare the performance of DALM with other methods, we conduct experiments on two ngram models built from small and large training corpora. Table 1 shows the specifications of the model. Training data are extracted from the Publication of unexamined Japanese patent applications, which is distributed with the NTCIR 3,4,5,6 patent retrieval task (Atsushi Fujii et al., 2007; Atsushi Fujii et al., 2005; Atsushi Fujii et al., 2004; Makoto Iwayama et al., 2003). We used data for the period from 228 Figure 11: Comparison between tuned and non-tuned double-array structures. Figure 10: An example of word ID ordering efficiency. Word IDs in the insertion array are packed to the front in advance. Therefore, shift lengths for ordered arrays are often shorter than unordered ones. The resulting size of the double-array structure is expected to be smaller than that of an unordered array. Table 1: Corpus and model specifications. Model Corpus Unique Ngram Size Type Type (wor</context>
</contexts>
<marker>Fujii, Iwayama, Kando, 2007</marker>
<rawString>Atsushi Fujii, Makoto Iwayama, and Noriko Kando. 2007. Overview of the Patent Retrieval Task at the NTCIR-6 Workshop. pages 359–365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Djamal Belazzougui</author>
<author>Fabiano C Botelho</author>
<author>Martin Dietzfelbinger</author>
</authors>
<title>Hash, displace, and compress.</title>
<date>2009</date>
<booktitle>In ESA,</booktitle>
<pages>682--693</pages>
<contexts>
<context position="7759" citStr="Belazzougui et al., 2009" startWordPosition="1258" endWordPosition="1261">tion, pairs from &lt;ngram,1&gt; to &lt;ngram,count&gt; are all added to the set for each ngram. To query this language model about the probability of an ngram, probabilities are calculated during querying by using these counts. Talbot and Brants (2008) have proposed a method based on perfect hash functions and bloomier filters. This method uses perfect hash functions to store ngrams and encode values (for exam223 ple, probabilities or counts of ngrams in the training corpus) to a large array. Guthrie and Hepple (2010) have proposed a language model called ShefLM that uses minimal perfect hash functions (Belazzougui et al., 2009), which can store ngrams without vacant spaces. Furthermore, values are compressed by simple dense coding (Fredriksson and Nikitin, 2007). ShefLM achieves a high compression ratio when it stores counts of ngrams in the training corpus. However, when this method stores probabilities of ngrams, the advantage of using compression is limited because floating-point numbers are difficult to compress. Generally, compression is performed by combining the same values but, two floating-point numbers are rarely the same, especially in the case of probability values1. These methods implement lossy languag</context>
</contexts>
<marker>Belazzougui, Botelho, Dietzfelbinger, 2009</marker>
<rawString>Djamal Belazzougui, Fabiano C. Botelho, and Martin Dietzfelbinger. 2009. Hash, displace, and compress. In ESA, pages 682–693.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy C Bell</author>
<author>John G Cleary</author>
<author>Ian H Witten</author>
</authors>
<date>1990</date>
<note>Text compression.</note>
<contexts>
<context position="4629" citStr="Bell et al., 1990" startWordPosition="717" endWordPosition="720">experiments focusing on query speed and model size. The results indicate that when the abovementioned optimization methods are used together, DALM outperforms state-ofthe-art methods on those points. 2 Related Work 2.1 Tries and Backwards Suffix Trees Tries (Fredkin, 1960) are one of the most widely used tree structures in ngram language models since they can reduce memory requirements by sharing common prefix. Moreover, since the query speed for tries depends only on the number of input words, the query speed remains constant even if the ngram model increases in size. Backwards suffix trees (Bell et al., 1990; Stolcke, 2002; Germann et al., 2009) are among the most efficient representations of tries for language models. They contain ngrams in reverse order of history words. Figure 1 shows an example of a backwards suffix tree representation. In this paper, we denote an ngram: by the form w1, w2, · · · , w,,, as w,,,1. In this example, word lists (represented as rectangular tables) contain target words (here, w,,,) of ngrams, and circled words in the tree denote history words (here, w,,,−1 1 ) associated with target words. The history words “I eat,” “you eat”, and “do you eat” are stored in reverse</context>
</contexts>
<marker>Bell, Cleary, Witten, 1990</marker>
<rawString>Timothy C. Bell, John G. Cleary, and Ian H. Witten. 1990. Text compression.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large Language Models in Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on EMNLP-CoNLL,</booktitle>
<pages>858--867</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1507" citStr="Brants et al. (2007)" startWordPosition="222" endWordPosition="225">lso show that our method can be used for building large language models using the division method. Lastly, we show that our method outperforms methods based on recent related works from the viewpoints of model size and query speed when both optimization methods are used. 1 Introduction Ngram language models (F. Jelinek, 1990) are widely used as probabilistic models of sentence in natural language processing. The wide use of the Internet has entailed a dramatic increase in size of the available corpora, which can be harnessed to obtain a significant improvement in model quality. In particular, Brants et al. (2007) have shown that the performance of statistical machine translation systems is monotonically improved with the increasing size of training corpora for the language model. However, models using larger corpora also consume more resources. In recent years, many methods for improving the efficiency of language models have been proposed to tackle this problem (Pauls and Klein, 2011; Kenneth Heafield, 2011). Such methods not only reduce the required memory size but also raise query speed. In this paper, we propose the double-array language model (DALM) which uses double-array structures (Aoe, 1989).</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large Language Models in Machine Translation. In Proceedings of the 2007 Joint Conference on EMNLP-CoNLL, pages 858–867. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo F Jelinek</author>
</authors>
<title>Self-organized language modeling for speech recognition.</title>
<date>1990</date>
<contexts>
<context position="1214" citStr="Jelinek, 1990" startWordPosition="176" endWordPosition="177">mization methods for improving the efficiency of data representation in the double-array structures. Embedding probabilities into unused spaces in double-array structures reduces the model size. Moreover, tuning the word IDs in the language model makes the model smaller and faster. We also show that our method can be used for building large language models using the division method. Lastly, we show that our method outperforms methods based on recent related works from the viewpoints of model size and query speed when both optimization methods are used. 1 Introduction Ngram language models (F. Jelinek, 1990) are widely used as probabilistic models of sentence in natural language processing. The wide use of the Internet has entailed a dramatic increase in size of the available corpora, which can be harnessed to obtain a significant improvement in model quality. In particular, Brants et al. (2007) have shown that the performance of statistical machine translation systems is monotonically improved with the increasing size of training corpora for the language model. However, models using larger corpora also consume more resources. In recent years, many methods for improving the efficiency of language</context>
</contexts>
<marker>Jelinek, 1990</marker>
<rawString>B. Merialdo F. Jelinek. 1990. Self-organized language modeling for speech recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Fredkin</author>
</authors>
<title>Trie memory.</title>
<date>1960</date>
<journal>Communications of the ACM,</journal>
<volume>3</volume>
<issue>9</issue>
<contexts>
<context position="2268" citStr="Fredkin, 1960" startWordPosition="340" endWordPosition="341">for the language model. However, models using larger corpora also consume more resources. In recent years, many methods for improving the efficiency of language models have been proposed to tackle this problem (Pauls and Klein, 2011; Kenneth Heafield, 2011). Such methods not only reduce the required memory size but also raise query speed. In this paper, we propose the double-array language model (DALM) which uses double-array structures (Aoe, 1989). Double-array structures are widely used in text processing, especially for Japanese. They are known to provide a compact representation of tries (Fredkin, 1960) and fast transitions between trie nodes. The ability to store and manipulate tries efficiently is expected to increase the performance of language models (i.e., improving query speed and reducing the model size in terms of memory) because tries are one of the most common representations of data structures in language models. We use double-array structures to implement a language model since we can utilize their speed and compactness when querying the model about an ngram. In order to utilize of double-array structures as language models, we modify them to be able to store probabilities and ba</context>
<context position="4285" citStr="Fredkin, 1960" startWordPosition="661" endWordPosition="662">hods can be used simultaneously and are also expected to work well. In our experiments, we use a language model based on corpora of the NTCIR patent retrieval task (Atsushi Fujii et al., 2007; Atsushi Fujii et al., 2005; Atsushi Fujii et al., 2004; Makoto Iwayama et al., 2003). The model size is 31 GB in the ARPA file format. We conducted experiments focusing on query speed and model size. The results indicate that when the abovementioned optimization methods are used together, DALM outperforms state-ofthe-art methods on those points. 2 Related Work 2.1 Tries and Backwards Suffix Trees Tries (Fredkin, 1960) are one of the most widely used tree structures in ngram language models since they can reduce memory requirements by sharing common prefix. Moreover, since the query speed for tries depends only on the number of input words, the query speed remains constant even if the ngram model increases in size. Backwards suffix trees (Bell et al., 1990; Stolcke, 2002; Germann et al., 2009) are among the most efficient representations of tries for language models. They contain ngrams in reverse order of history words. Figure 1 shows an example of a backwards suffix tree representation. In this paper, we </context>
</contexts>
<marker>Fredkin, 1960</marker>
<rawString>Edward Fredkin. 1960. Trie memory. Communications of the ACM, 3(9):490–499.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Fredriksson</author>
<author>Fedor Nikitin</author>
</authors>
<title>Simple Compression Code Supporting Random Access and Fast String Matching.</title>
<date>2007</date>
<booktitle>In Proceedings of the 6th international conference on Experimental algorithms, WEA’07,</booktitle>
<pages>203--216</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="7896" citStr="Fredriksson and Nikitin, 2007" startWordPosition="1278" endWordPosition="1281">ity of an ngram, probabilities are calculated during querying by using these counts. Talbot and Brants (2008) have proposed a method based on perfect hash functions and bloomier filters. This method uses perfect hash functions to store ngrams and encode values (for exam223 ple, probabilities or counts of ngrams in the training corpus) to a large array. Guthrie and Hepple (2010) have proposed a language model called ShefLM that uses minimal perfect hash functions (Belazzougui et al., 2009), which can store ngrams without vacant spaces. Furthermore, values are compressed by simple dense coding (Fredriksson and Nikitin, 2007). ShefLM achieves a high compression ratio when it stores counts of ngrams in the training corpus. However, when this method stores probabilities of ngrams, the advantage of using compression is limited because floating-point numbers are difficult to compress. Generally, compression is performed by combining the same values but, two floating-point numbers are rarely the same, especially in the case of probability values1. These methods implement lossy language models, meaning that, we can reduce the model size at the expense of model quality. These methods also reduce the model performance (pe</context>
</contexts>
<marker>Fredriksson, Nikitin, 2007</marker>
<rawString>Kimmo Fredriksson and Fedor Nikitin. 2007. Simple Compression Code Supporting Random Access and Fast String Matching. In Proceedings of the 6th international conference on Experimental algorithms, WEA’07, pages 203–216. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
</authors>
<title>Masao Utiyama, Mikio Yamamoto, and Takehito Utsuro.</title>
<date>2008</date>
<booktitle>Overview of the Patent Translation Task at the NTCIR-7 Workshop.</booktitle>
<marker>Fujii, 2008</marker>
<rawString>Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and Takehito Utsuro. 2008. Overview of the Patent Translation Task at the NTCIR-7 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Eric Joanis</author>
<author>Samuel Larkin</author>
</authors>
<title>Tightly Packed Tries: How to Fit Large Models into Memory, and Make them Load Fast, Too.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on SETQA-NLP,</booktitle>
<pages>31--39</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4667" citStr="Germann et al., 2009" startWordPosition="723" endWordPosition="726">d and model size. The results indicate that when the abovementioned optimization methods are used together, DALM outperforms state-ofthe-art methods on those points. 2 Related Work 2.1 Tries and Backwards Suffix Trees Tries (Fredkin, 1960) are one of the most widely used tree structures in ngram language models since they can reduce memory requirements by sharing common prefix. Moreover, since the query speed for tries depends only on the number of input words, the query speed remains constant even if the ngram model increases in size. Backwards suffix trees (Bell et al., 1990; Stolcke, 2002; Germann et al., 2009) are among the most efficient representations of tries for language models. They contain ngrams in reverse order of history words. Figure 1 shows an example of a backwards suffix tree representation. In this paper, we denote an ngram: by the form w1, w2, · · · , w,,, as w,,,1. In this example, word lists (represented as rectangular tables) contain target words (here, w,,,) of ngrams, and circled words in the tree denote history words (here, w,,,−1 1 ) associated with target words. The history words “I eat,” “you eat”, and “do you eat” are stored in reverse order. Querying this trie about an ng</context>
</contexts>
<marker>Germann, Joanis, Larkin, 2009</marker>
<rawString>Ulrich Germann, Eric Joanis, and Samuel Larkin. 2009. Tightly Packed Tries: How to Fit Large Models into Memory, and Make them Load Fast, Too. In Proceedings of the Workshop on SETQA-NLP, pages 31–39. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Guthrie</author>
<author>Mark Hepple</author>
</authors>
<title>Storing the Web in Memory: Space Efficient Language Models with Constant Time Retrieval.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on EMNLP,</booktitle>
<pages>262--272</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="7646" citStr="Guthrie and Hepple (2010)" startWordPosition="1241" endWordPosition="1244">bloom filters have been used for checking whether certain data are contained in a set. To store the count information, pairs from &lt;ngram,1&gt; to &lt;ngram,count&gt; are all added to the set for each ngram. To query this language model about the probability of an ngram, probabilities are calculated during querying by using these counts. Talbot and Brants (2008) have proposed a method based on perfect hash functions and bloomier filters. This method uses perfect hash functions to store ngrams and encode values (for exam223 ple, probabilities or counts of ngrams in the training corpus) to a large array. Guthrie and Hepple (2010) have proposed a language model called ShefLM that uses minimal perfect hash functions (Belazzougui et al., 2009), which can store ngrams without vacant spaces. Furthermore, values are compressed by simple dense coding (Fredriksson and Nikitin, 2007). ShefLM achieves a high compression ratio when it stores counts of ngrams in the training corpus. However, when this method stores probabilities of ngrams, the advantage of using compression is limited because floating-point numbers are difficult to compress. Generally, compression is performed by combining the same values but, two floating-point </context>
</contexts>
<marker>Guthrie, Hepple, 2010</marker>
<rawString>David Guthrie and Mark Hepple. 2010. Storing the Web in Memory: Space Efficient Language Models with Constant Time Retrieval. In Proceedings of the 2010 Conference on EMNLP, pages 262–272. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and Smaller Language Model Queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation. ACL.</booktitle>
<contexts>
<context position="1911" citStr="Heafield, 2011" startWordPosition="285" endWordPosition="286">g. The wide use of the Internet has entailed a dramatic increase in size of the available corpora, which can be harnessed to obtain a significant improvement in model quality. In particular, Brants et al. (2007) have shown that the performance of statistical machine translation systems is monotonically improved with the increasing size of training corpora for the language model. However, models using larger corpora also consume more resources. In recent years, many methods for improving the efficiency of language models have been proposed to tackle this problem (Pauls and Klein, 2011; Kenneth Heafield, 2011). Such methods not only reduce the required memory size but also raise query speed. In this paper, we propose the double-array language model (DALM) which uses double-array structures (Aoe, 1989). Double-array structures are widely used in text processing, especially for Japanese. They are known to provide a compact representation of tries (Fredkin, 1960) and fast transitions between trie nodes. The ability to store and manipulate tries efficiently is expected to increase the performance of language models (i.e., improving query speed and reducing the model size in terms of memory) because tri</context>
<context position="8934" citStr="Heafield (2011)" startWordPosition="1445" endWordPosition="1446">1. These methods implement lossy language models, meaning that, we can reduce the model size at the expense of model quality. These methods also reduce the model performance (perplexity). Pauls and Klein (2011) have proposed BerkeleyLM which is based on an implicit encoding structure, where ngrams are separated according to their order, and are sorted by word ID. The sorted ngrams are linked to each other like a trie structure. BerkeleyLM provides rather efficient methods. Variablelength coding and block compression are used if small model size is more important than query speed. In addition, Heafield (2011) has proposed an efficient language model toolkit called KenLM that has been recently used in machine translation systems, for which large language models are often needed. KenLM has two different main structure types: trie and probing. The trie structure is compact but relatively slower to query, whereas the probing structure is relatively larger but faster to query. In this paper, we propose a language model structure based on double-array structures. As we describe in Section 3, double-array structures can be used as fast and compact representations of tries. We propose several techniques f</context>
<context position="28994" citStr="Heafield, 2011" startWordPosition="4905" endWordPosition="4906">se the 5 Gwords model in our experiments. In our environment, building a 5 Gwords double-array structure required about 4 days when the double-array structures were divided into 8 parts, even though we used the more efficient algorithm described in Section 3.3. The time required for building the model when the original structure was divided into less than 8 parts was too long. Thus, a more efficient building algorithm is essential for advancing this research further. 5.4 Comparison with Other Methods Using the 100 Mwords and 5 Gwords models, we compared DALM with other methods (KenLM (Kenneth Heafield, 2011) and SRILM (Stolcke, 2002)). In this experiment, we used the both method (which is mentioned above) for DALM and divided the original trie into 8 parts and built double-array structures. The results are shown in Figure 13 and Table 4; the group on the left shows the results for the 100 Mwords model and the group on the right shows the results for the 5 Gwords model. The experimental results clearly indicate that DALM is the fastest of all the compared methods and that the model size is nearly the same or slightly smaller than that of KenLM (Probing). Whereas KenLM (Trie) is the smallest model,</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and Smaller Language Model Queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Iwayama</author>
</authors>
<title>Atsushi Fujii, Noriko Kando, and Akihiko Takano.</title>
<date>2003</date>
<booktitle>Overview of Patent Retrieval Task at NTCIR-3.</booktitle>
<marker>Iwayama, 2003</marker>
<rawString>Makoto Iwayama, Atsushi Fujii, Noriko Kando, and Akihiko Takano. 2003. Overview of Patent Retrieval Task at NTCIR-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasumasa Nakamura</author>
<author>Hisatoshi Mochizuki</author>
</authors>
<title>Fast Computation of Updating Method of a Dictionary for Compression Digital Search Tree.</title>
<date>2006</date>
<journal>Transactions of Information Processing Society of Japan. Data,</journal>
<volume>47</volume>
<issue>13</issue>
<contexts>
<context position="15460" citStr="Nakamura and Mochizuki, 2006" startWordPosition="2588" endWordPosition="2591"> unable to find an original source for this technique. However, this method is commonly used in double-array implementations. 225 “I”, “you” and “they”, the following three slots must be empty (i.e., the slots must not be used by other nodes.): • BASE[s] + WORDID(“I”) • BASE[s] + WORDID(“you”) • BASE[s] + WORDID(“they”) where s is the index of the node “eat”. At the construction step, we need to find BASE[s] which satisfies the above conditions. 3.3 Efficient Construction Algorithm The construction time for a double-array structure poses the greatest challenge. We use a more efficient method (Nakamura and Mochizuki, 2006) instead of the naive method for constructing a doublearray structure because the naive method requires a long time. We call the method “empty doubly-linked list”. This algorithm is one of the most efficient construction methods devised to date. Figure 4 shows an example of an empty doubly-linked list. We can efficiently define the BASE value of each node by using the CHECK array to store the next empty slot. In this example, in searching the BASE value of a node, the first child node can be set to position 1, and if that fails, we can successively try positions 3, 4, 6, 8, • • • by tracing th</context>
</contexts>
<marker>Nakamura, Mochizuki, 2006</marker>
<rawString>Yasumasa Nakamura and Hisatoshi Mochizuki. 2006. Fast Computation of Updating Method of a Dictionary for Compression Digital Search Tree. Transactions of Information Processing Society of Japan. Data, 47(13):16–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Faster and Smaller N-Gram Language Models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the ACL-HLT,</booktitle>
<pages>258--267</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1886" citStr="Pauls and Klein, 2011" startWordPosition="280" endWordPosition="283">e in natural language processing. The wide use of the Internet has entailed a dramatic increase in size of the available corpora, which can be harnessed to obtain a significant improvement in model quality. In particular, Brants et al. (2007) have shown that the performance of statistical machine translation systems is monotonically improved with the increasing size of training corpora for the language model. However, models using larger corpora also consume more resources. In recent years, many methods for improving the efficiency of language models have been proposed to tackle this problem (Pauls and Klein, 2011; Kenneth Heafield, 2011). Such methods not only reduce the required memory size but also raise query speed. In this paper, we propose the double-array language model (DALM) which uses double-array structures (Aoe, 1989). Double-array structures are widely used in text processing, especially for Japanese. They are known to provide a compact representation of tries (Fredkin, 1960) and fast transitions between trie nodes. The ability to store and manipulate tries efficiently is expected to increase the performance of language models (i.e., improving query speed and reducing the model size in ter</context>
<context position="8529" citStr="Pauls and Klein (2011)" startWordPosition="1376" endWordPosition="1379">hieves a high compression ratio when it stores counts of ngrams in the training corpus. However, when this method stores probabilities of ngrams, the advantage of using compression is limited because floating-point numbers are difficult to compress. Generally, compression is performed by combining the same values but, two floating-point numbers are rarely the same, especially in the case of probability values1. These methods implement lossy language models, meaning that, we can reduce the model size at the expense of model quality. These methods also reduce the model performance (perplexity). Pauls and Klein (2011) have proposed BerkeleyLM which is based on an implicit encoding structure, where ngrams are separated according to their order, and are sorted by word ID. The sorted ngrams are linked to each other like a trie structure. BerkeleyLM provides rather efficient methods. Variablelength coding and block compression are used if small model size is more important than query speed. In addition, Heafield (2011) has proposed an efficient language model toolkit called KenLM that has been recently used in machine translation systems, for which large language models are often needed. KenLM has two differen</context>
</contexts>
<marker>Pauls, Klein, 2011</marker>
<rawString>Adam Pauls and Dan Klein. 2011. Faster and Smaller N-Gram Language Models. In Proceedings of the 49th Annual Meeting of the ACL-HLT, pages 258–267. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM-an Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>Seventh International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="4644" citStr="Stolcke, 2002" startWordPosition="721" endWordPosition="722">g on query speed and model size. The results indicate that when the abovementioned optimization methods are used together, DALM outperforms state-ofthe-art methods on those points. 2 Related Work 2.1 Tries and Backwards Suffix Trees Tries (Fredkin, 1960) are one of the most widely used tree structures in ngram language models since they can reduce memory requirements by sharing common prefix. Moreover, since the query speed for tries depends only on the number of input words, the query speed remains constant even if the ngram model increases in size. Backwards suffix trees (Bell et al., 1990; Stolcke, 2002; Germann et al., 2009) are among the most efficient representations of tries for language models. They contain ngrams in reverse order of history words. Figure 1 shows an example of a backwards suffix tree representation. In this paper, we denote an ngram: by the form w1, w2, · · · , w,,, as w,,,1. In this example, word lists (represented as rectangular tables) contain target words (here, w,,,) of ngrams, and circled words in the tree denote history words (here, w,,,−1 1 ) associated with target words. The history words “I eat,” “you eat”, and “do you eat” are stored in reverse order. Queryin</context>
<context position="6437" citStr="Stolcke, 2002" startWordPosition="1043" endWordPosition="1044">et words are stored in word lists. representation is highly suitable for the backoff calculation. For example, in querying about the 4gram “do you eat soup”, we first trace “eat” → “you” → “do” in a manner similar to above. However, searching for the word “soup” in list &lt;3&gt; fails because list &lt;3&gt; does not contain the word “soup”. In this case, we return to the node “you” to search the list &lt;2&gt;, where we find “soup”. This means that the trigram “you eat soup” is contained in the tree while the 4gram “do you eat soup” is not. This behavior can be efficiently used for backoff calculation. SRILM (Stolcke, 2002) is a widely used language model toolkit. It utilizes backwards suffix trees for its data structures. In SRILM, tries are implemented as 64-bit pointer links, which wastes a lot of memory. On the other hand, the access speed for ngram probabilities is relatively high. 2.2 Efficient Language Models In recent years, several methods have been proposed for storing language models efficiently in memory. Talbot and Osborne (2007) have proposed an efficient method based on bloom filters. This method modifies bloom filters to store count information about training sets. In prior work, bloom filters ha</context>
<context position="29020" citStr="Stolcke, 2002" startWordPosition="4909" endWordPosition="4910">r experiments. In our environment, building a 5 Gwords double-array structure required about 4 days when the double-array structures were divided into 8 parts, even though we used the more efficient algorithm described in Section 3.3. The time required for building the model when the original structure was divided into less than 8 parts was too long. Thus, a more efficient building algorithm is essential for advancing this research further. 5.4 Comparison with Other Methods Using the 100 Mwords and 5 Gwords models, we compared DALM with other methods (KenLM (Kenneth Heafield, 2011) and SRILM (Stolcke, 2002)). In this experiment, we used the both method (which is mentioned above) for DALM and divided the original trie into 8 parts and built double-array structures. The results are shown in Figure 13 and Table 4; the group on the left shows the results for the 100 Mwords model and the group on the right shows the results for the 5 Gwords model. The experimental results clearly indicate that DALM is the fastest of all the compared methods and that the model size is nearly the same or slightly smaller than that of KenLM (Probing). Whereas KenLM (Trie) is the smallest model, it is slower than DALM. T</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM-an Extensible Language Modeling Toolkit. Seventh International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Thorsten Brants</author>
</authors>
<title>Randomized Language Models via Perfect Hash Functions.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<contexts>
<context position="7375" citStr="Talbot and Brants (2008)" startWordPosition="1194" endWordPosition="1197">, several methods have been proposed for storing language models efficiently in memory. Talbot and Osborne (2007) have proposed an efficient method based on bloom filters. This method modifies bloom filters to store count information about training sets. In prior work, bloom filters have been used for checking whether certain data are contained in a set. To store the count information, pairs from &lt;ngram,1&gt; to &lt;ngram,count&gt; are all added to the set for each ngram. To query this language model about the probability of an ngram, probabilities are calculated during querying by using these counts. Talbot and Brants (2008) have proposed a method based on perfect hash functions and bloomier filters. This method uses perfect hash functions to store ngrams and encode values (for exam223 ple, probabilities or counts of ngrams in the training corpus) to a large array. Guthrie and Hepple (2010) have proposed a language model called ShefLM that uses minimal perfect hash functions (Belazzougui et al., 2009), which can store ngrams without vacant spaces. Furthermore, values are compressed by simple dense coding (Fredriksson and Nikitin, 2007). ShefLM achieves a high compression ratio when it stores counts of ngrams in t</context>
<context position="27678" citStr="Talbot and Brants, 2008" startWordPosition="4687" endWordPosition="4690">insertion, Ui, is proportional to i, or that Ui = c x i where c is a proportionality constant, we can calculate the building time as follows: ∑N1 UiM = O (MN2). To shorten the build time, we divided the original trie into several parts. Building parts of the original trie is possible because N is reduced. Moreover, these double-array structures can be built in parallel. Note that query results for both original and divided tries are completely equivalent because divided tries hold all the ngram statistics of the original trie. This method is similar to that used in randomized language models (Talbot and Brants, 2008). We compared the differences between the methods using the original and divided double-array structures. In the comparison, we also used the 100 Mwords model with the both optimization method described in the previous section (Figure 12 and Table 3). Although dividing the trie increased the size of the DALM slightly, the model size was still smaller than that without optimization. Query speed increased as the number of parts was increased. We attributed this to the divided DALM consisting of several double-array structures, each smaller than the undivided structure which results in an increas</context>
</contexts>
<marker>Talbot, Brants, 2008</marker>
<rawString>David Talbot and Thorsten Brants. 2008. Randomized Language Models via Perfect Hash Functions. In Proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Smoothed Bloom Filter Language Models: Tera-Scale LMs on the Cheap.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on EMNLP-CoNLL,</booktitle>
<pages>468--476</pages>
<contexts>
<context position="6864" citStr="Talbot and Osborne (2007)" startWordPosition="1109" endWordPosition="1112">p”. This means that the trigram “you eat soup” is contained in the tree while the 4gram “do you eat soup” is not. This behavior can be efficiently used for backoff calculation. SRILM (Stolcke, 2002) is a widely used language model toolkit. It utilizes backwards suffix trees for its data structures. In SRILM, tries are implemented as 64-bit pointer links, which wastes a lot of memory. On the other hand, the access speed for ngram probabilities is relatively high. 2.2 Efficient Language Models In recent years, several methods have been proposed for storing language models efficiently in memory. Talbot and Osborne (2007) have proposed an efficient method based on bloom filters. This method modifies bloom filters to store count information about training sets. In prior work, bloom filters have been used for checking whether certain data are contained in a set. To store the count information, pairs from &lt;ngram,1&gt; to &lt;ngram,count&gt; are all added to the set for each ngram. To query this language model about the probability of an ngram, probabilities are calculated during querying by using these counts. Talbot and Brants (2008) have proposed a method based on perfect hash functions and bloomier filters. This method</context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007. Smoothed Bloom Filter Language Models: Tera-Scale LMs on the Cheap. In Proceedings of the 2007 Joint Conference on EMNLP-CoNLL, pages 468–476.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
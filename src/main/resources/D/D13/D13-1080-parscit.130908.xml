<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.032856">
<title confidence="0.9974565">
Improving Learning and Inference in a Large Knowledge-base
using Latent Syntactic Cues
</title>
<author confidence="0.993001">
Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel, and Tom Mitchell
</author>
<affiliation confidence="0.9931">
Carnegie Mellon University
</affiliation>
<address confidence="0.620537">
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
</address>
<email confidence="0.999045">
{mg1,ppt,bkisiel,tom.mitchell}@cs.cmu.edu
</email>
<sectionHeader confidence="0.995552" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996520533333333">
Automatically constructed Knowledge Bases
(KBs) are often incomplete and there is a gen-
uine need to improve their coverage. Path
Ranking Algorithm (PRA) is a recently pro-
posed method which aims to improve KB cov-
erage by performing inference directly over
the KB graph. For the first time, we demon-
strate that addition of edges labeled with la-
tent features mined from a large dependency
parsed corpus of 500 million Web documents
can significantly outperform previous PRA-
based approaches on the KB inference task.
We present extensive experimental results val-
idating this finding. The resources presented
in this paper are publicly available.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999008125">
Over the last few years, several large scale Knowl-
edge Bases (KBs) such as Freebase (Bollacker et
al., 2008), NELL (Carlson et al., 2010), and YAGO
(Suchanek et al., 2007) have been developed. Each
such KB consists of millions of facts (e.g., (Tiger
Woods, playsSport, Golf)) spanning over multiple
relations. Unfortunately, these KBs are often incom-
plete and there is a need to increase their coverage of
facts to make them useful in practical applications.
A strategy to increase coverage might be to per-
form inference directly over the KB represented as a
graph. For example, if the KB contained the follow-
ing facts, (Tiger Woods, participatesIn, PGA Tour))
and (Golf, sportOfTournament, PGA Tour), then by
putting these two facts together, we could potentially
infer that (Tiger Woods, playsSport, Golf). The
</bodyText>
<figureCaption confidence="0.9845735">
Figure 1: Example demonstrating how lexicalized syn-
tactic edges can improve connectivity in the KB enabling
PRA (Lao and Cohen, 2010) to discover relationships be-
tween Alex Rodriguez and World Series. Edges with la-
tent labels can improve inference performance by reduc-
ing data sparsity. See Section 1.1 for details.
</figureCaption>
<bodyText confidence="0.999937210526316">
recently proposed Path Ranking Algorithm (PRA)
(Lao and Cohen, 2010) performs such inference by
automatically learning semantic inference rules over
the KB (Lao et al., 2011). PRA uses features based
off of sequences of edge types, e.g., (playsSport,
sportOfTournament), to predict missing facts in the
KB.
PRA was extended by (Lao et al., 2012) to per-
form inference over a KB augmented with depen-
dency parsed sentences. While this opens up the
possibility of learning syntactic-semantic inference
rules, the set of syntactic edge labels used are
just the unlexicalized dependency role labels (e.g.,
nobj, dobj, etc., without the corresponding words),
thereby limiting overall expressitivity of the learned
inference rules. To overcome this limitation, in this
paper we augment the KB graph by adding edges
with more expressive lexicalized syntactic labels
(where the labels are words instead of dependen-
</bodyText>
<page confidence="0.983188">
833
</page>
<bodyText confidence="0.931712230769231">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 833–838,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
cies). These additional edges, e.g., (Alex Rodriguez,
“plays for”, NY Yankees), are mined by extracting
600 million Subject-Verb-Object (SVO) triples from
a large corpus of 500m dependency parsed docu-
ments, which would have been prohibitively expen-
sive to add directly as in (Lao et al., 2012). In order
to overcome the explosion of path features and data
sparsity, we derive edge labels by learning latent em-
beddings of the lexicalized edges. Through exten-
sive experiments on real world datasets, we demon-
strate effectiveness of the proposed approach.
</bodyText>
<subsectionHeader confidence="0.999887">
1.1 Motivating Example
</subsectionHeader>
<bodyText confidence="0.9999873">
In Figure 1, the KB graph (only solid edges) is dis-
connected, thereby making it impossible for PRA to
discover any relationship between Alex Rodriguez
and World Series. However, addition of the two
edges with SVO-based lexicalized syntactic edges
(e.g., (Alex Rodriguez, plays for, NY Yankees)) re-
stores this inference possibility. For example, PRA
might use the edge sequence (“plays for”, team-
PlaysIn) as evidence for predicting the relation in-
stance (Alex Rodriguez, athleteWonChampionship,
World Series). Unfortunately, such naive addition
of lexicalized edges may result in significant data
sparsity, which can be overcome by mapping lexi-
calized edge labels to some latent embedding (e.g.,
(Alex Rodriguez, LatentFeat#5, NY Yankees) and
running PRA over this augmented graph. Using la-
tent embeddings, PRA could then use the following
edge sequence as a feature in its prediction models:
(LatentFeat#5, teamPlaysIn). We find this strategy
to be very effective as described in Section 4.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999967078431373">
There is a long history of methods using suface-level
lexical patterns for extracting relational facts from
text corpora (Hearst, 1992; Brin, 1999; Agichtein
and Gravano, 2000; Ravichandran and Hovy, 2002;
Etzioni et al., 2004). Syntactic information in the
form of dependency paths have been explored in
(Snow et al., 2006; Suchanek et al., 2006). A
method of latent embedding of relation instances
for sentence-level relation extraction was shown in
(Wang et al., 2011). However, none of this prior
work makes explicit use of the background KBs as
we explore in this paper.
Path Ranking Algorithm (PRA) (Lao and Cohen,
2010) has been used previously to perform inference
over graph-structured KBs (Lao et al., 2011), and to
learn formation of online communities (Settles and
Dow, 2013). In (Lao et al., 2012), PRA is extended
to perform inference over a KB using syntactic in-
formation from parsed text. In contrast to these pre-
vious PRA-based approaches where all edge labels
are either KB labels or at surface-level, in this pa-
per we explore using latent edge labels in addition
to surface-level labels in the graph over which PRA
is applied. In particular, we focus on the problem of
performing inference over a large KB and learn la-
tent edge labels by mining dependency syntax statis-
tics from a large text corpus.
Though we use Principal Components Analysis
(PCA) for dimensionality reduction for the experi-
ments in this paper, this is by no means the only
choice. Various other dimensionality reduction tech-
niques, and in particular, other verb clustering tech-
niques (Korhonen et al., 2003), may also be used.
OpenIE systems such as Reverb (Etzioni et al.,
2011) also extract verb-anchored dependency triples
from large text corpus. In contrast to such ap-
proaches, we focus on how latent embedding of
verbs in such triples can be combined with explicit
background knowledge to improve coverage of ex-
isting KBs. This has the added capability of infer-
ring facts which are not explicitly mentioned in text.
The recently proposed Universal Schema (Riedel
et al., 2013) also demonstrates the benefit of us-
ing latent features for increasing coverage of KBs.
Key differences between that approach and ours in-
clude our use of syntactic information as opposed to
surface-level patterns in theirs, and also the ability
of the proposed PRA-based method to generate use-
ful inference rules which is beyond the capability of
the matrix factorization approach in (Riedel et al.,
2013).
</bodyText>
<sectionHeader confidence="0.998071" genericHeader="method">
3 Method
</sectionHeader>
<subsectionHeader confidence="0.999955">
3.1 Path Ranking Algorithm (PRA)
</subsectionHeader>
<bodyText confidence="0.999963166666667">
In this section, we present a brief overview of the
Path Ranking Algorithm (PRA) (Lao and Cohen,
2010), building on the notations in (Lao et al., 2012).
Let G = (V, E, T) be the graph, where V is the set
of vertices, E is the set of edges, and T is the set of
edge types. For each edge (v1, t, v2) E E, we have
</bodyText>
<page confidence="0.989441">
834
</page>
<bodyText confidence="0.9970995">
v1, v2 ∈ V and t ∈ T. Let R ⊂ T be the set of types
predicted by PRA. R could in principal equal T, but
in this paper we restrict prediction to KB relations,
while T also includes types derived from surface text
and latent embeddings. Let 7r = ht1, t2, ... , twi be
a path type of length w over graph G, where ti ∈ T
is the type of the ith edge in the path. Each such
path type is also a feature in the PRA model. For
a given source and target node pair s, t ∈ V , let
P(s → t; 7r) be the value of the feature 7r specify-
ing the probability of reaching node t starting from
node s and following a path constrained by path type
7r. We approximate these probabilities using random
walks. A value of 0 indicates unreachability from s
to t using path type 7r.
Let B = {7r1, ... , 7r,,,} be the set of all features
(path types). The score that relation r holds between
node s and node t is given by the following function:
</bodyText>
<equation confidence="0.9929465">
�ScorePRA(s, t, r) = P(s → t; 7r) B,&apos;
,EB
</equation>
<bodyText confidence="0.984335277777778">
where 0&apos;, is the weight of feature 7r in class r ∈ R.
Feature Selection: The set B of possible path
types grows exponentially in the length of the paths
that are considered. In order to have a manageable
set of features to compute, we first perform a feature
selection step. The goal of this step is to select for
computation only those path types that commonly
connect sources and targets of relation r. We per-
form this feature selection by doing length-bounded
random walks from a given list of source and tar-
get nodes, keeping track of how frequently each path
type leads from a source node to a target node. The
most common m path types are selected for the set
B.
Training: We perform standard logistic regres-
sion with L2 regularization to learn the weights B,&apos;.
We follow the strategy in (Lao and Cohen, 2010) to
generate positive and negative training instances.
</bodyText>
<subsectionHeader confidence="0.976168">
3.2 PRAsyntactic
</subsectionHeader>
<bodyText confidence="0.998078185185185">
In this section, we shall extend the knowledge graph
G = (V, E, T) from the previous section with an
augmented graph G0 = (V, E0, T0), where E ⊂ E0
and T ⊂ T0, with the set of vertices unchanged.
In order to get the edges in E0 − E, we first
collect a set of Subject-Verb-Object (SVO) triples
D = {(s, v, o, c)} from a large dependency parsed
text corpus, with c ∈ R+ denoting the frequency
of this triple in the corpus. The additional edge
set is then defined as Esyntactic = E0 − E =
{(s, v, o)  |∃(s, v, o, c) ∈ D, s, o ∈ V }. We de-
fine S = {v  |∃(s, v, o) ∈ Esyntactic} and set
T0 = T ∪ S. In other words, for each pair of
directly connected nodes in the KB graph G, we add
an additional edge between those two nodes for each
verb which takes the NPs represented by two nodes
as subjects and objects (or vice versa) as observed in
a text corpus. In Figure 1, (Alex Rodriguez, “plays
for”, NY Yankees) is an example of such an edge.
PRA is then applied over this augmented graph
G0, over the same set of prediction types R as be-
fore. We shall refer to this version of PRA as
PRAsyntactic. For the experiments in this paper, we
collected |D |= 600 million SVO triples1 from the
entire ClueWeb corpus (Callan et al., 2009), parsed
using the Malt parser (Nivre et al., 2007) by the
Hazy project (Kumar et al., 2013).
</bodyText>
<subsectionHeader confidence="0.982579">
3.3 PRAlatent
</subsectionHeader>
<bodyText confidence="0.997507947368421">
In this section we construct G00 = (V, E&amp;quot;, T00),
another syntactic-information-induced extension of
the knowledge graph G, but instead of using the sur-
face forms of verbs in S (see previous section) as
edge types, we derive those edges types T00 based
on latent embeddings of those verbs. We note that
E ⊂ E00, and T ⊂ T00.
In order to learn the latent or low dimensional em-
beddings of the verbs in S, we first define QS =
{(s, o)  |∃(s, v, o, c) ∈ D, v ∈ S}, the set of
subject-object tuples in D which are connected by
at least one verb in S. We now construct a matrix
X|S|x|QS |whose entry X„ e = c, where v ∈ S, q =
(s, o) ∈ QS, and (s, v, o, c) ∈ D. After row normal-
izing and centering matrix X, we apply PCA on this
matrix. Let A|S|xd with d &lt;&lt; |QS |be the low di-
mensional embeddings of the verbs in S as induced
by PCA. We use two strategies to derive mappings
for verbs from matrix A.
</bodyText>
<listItem confidence="0.88796675">
• PRAlatent.: The verb is mapped to concatena-
tion of the 2 most positive columns in the row
in A that corresponds to the verb. Similarly, for
the most negative 2 columns.
</listItem>
<footnote confidence="0.993386">
1This data and other resources from the paper are publicly
available at http://rtw.ml.cmu.edu/emnlp2013 pra/.
</footnote>
<page confidence="0.97707">
835
</page>
<table confidence="0.9999764">
Precision Recall F1
PRA 0.800 0.331 0.468
PRAgyntactic 0.804 0.271 0.405
PRAlatent, 0.885 0.334 0.485
PRAlatentd 0.868 0.424 0.570
</table>
<tableCaption confidence="0.99979">
Table 1: Comparison of performance of different variants
</tableCaption>
<bodyText confidence="0.62208075">
of PRA micro averaged across 15 NELL relations. We
find that use of latent edge labels, in particular the pro-
posed approach PRAlatentd, significantly outperforms
other approaches. This is our main result. (See Section 4)
</bodyText>
<listItem confidence="0.959416333333333">
• PRAlatentd: The verb is mapped to disjunction
of top-k most positive and negative columns in
the row in A that corresponds to the verb.
</listItem>
<sectionHeader confidence="0.998732" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999954645161291">
We compared the various methods using 15 NELL
relations. For each relation, we split NELL’s known
relation instances into 90% training and 10% testing.
For each method, we then selected 750 path features
and trained the model, as described in Section 3, us-
ing GraphChi (Kyrola et al., 2012) to perform the
random walk graph computations. To evaluate the
model, we took all source nodes in the testing data
and used the model to predict target nodes. We re-
port the precision and recall (on the set of known tar-
get nodes) of the set of predictions for each model
that are above a certain confidence threshold. Be-
cause we used strong regularization, we picked for
our threshold a model score of 0.405, correspond-
ing to 60% probability of the relation instance being
true; values higher than this left many relations with-
out any predictions. Table 1 contains the results.
As can be seen in the table, PRAgyntactic on av-
erage performs slightly worse than PRA. While
the extra syntactic features are very informative for
some relations, they also introduce a lot of spar-
sity, which makes the model perform worse on other
relations. When using latent factorization meth-
ods to reduce the sparsity of the syntactic features,
we see a significant improvement in performance.
PRAlatent, has a 45% reduction in precision er-
rors vs. PRA while maintaining the same recall,
and PRAlatentd reduces precision errors by 35%
while improving recall by 27%. Section 4.1 con-
tains some qualitative analysis of how sparsity is re-
duced with the latent methods. As a piece quanti-
</bodyText>
<figure confidence="0.9130555">
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
</figure>
<figureCaption confidence="0.9953624">
Figure 2: Precision (y axis) - Recall (x axis) plots for the
relations cityLiesOnRiver (top) and athletePlaysForTeam
(bottom). PRAlatentd (rightmost plot), the proposed ap-
proach which exploits latent edge labels, outperforms
other alternatives.
</figureCaption>
<bodyText confidence="0.9994888">
tative analysis, there were 908 possible path types
found in the feature selection step with PRA on the
relation cityLiesOnRiver (of which we then selected
750). For PRAgyntactic, there were 73,820, while
PRAlatent, had 47,554 and PRAlatentd had 58,414.
Table 2 shows F1 scores for each model on
each relation, and Figure 2 shows representative
Precision-Recall plots for two NELL relations. In
both cases, we find that PRAlatentd significantly
outperforms other baselines.
</bodyText>
<subsectionHeader confidence="0.940862">
4.1 Discussion
</subsectionHeader>
<bodyText confidence="0.999520454545455">
While examining the model weights for each of
the methods, we saw a few occasions where sur-
face relations and NELL relations combined to form
interpretable path types. For example, in ath-
letePlaysForTeam, some highly weighted features
took the form of (athletePlaysSport, “(sport) played
by (team)”). A high weight on this feature would
bias the prediction towards teams that are known to
play the same sport as the athlete.
For PRA, the top features for the best performing
relations are path types that contain a single edge
</bodyText>
<figure confidence="0.9987073">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
PRA
PRAsyntactic
PRAlatentc
PRAlatentd
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
PRA
PRAsyntactic
PRAlatentc
PRAlatentd
</figure>
<page confidence="0.994446">
836
</page>
<table confidence="0.9999791875">
PRA PRAgyntactic PRAlatent� PRAlatentd
animalIsTypeOfAnimal 0.52 0.50 0.47 0.53
athletePlaysForTeam 0.22 0.21 0.56 0.64
athletePlaysInLeague 0.81 0.75 0.73 0.74
cityLiesOnRiver 0.05 0 0.07 0.31
cityLocatedInCountry 0.15 0.20 0.45 0.55
companyCeo 0.29 0.18 0.25 0.35
countryHasCompanyOffice 0 0 0 0
drugHasSideEffect 0.96 0.95 0.94 0.94
headquarteredIn 0.31 0.11 0.41 0.64
locationLocatedWithinLocation 0.40 0.38 0.38 0.41
publicationJournalist 0.10 0.06 0.10 0.16
roomCanContainFurniture 0.72 0.70 0.71 0.73
stadiumLocatedInCity 0.53 0 0.13 0.67
teamPlaysAgainstTeam 0.47 0.24 0.26 0.21
writerWroteBook 0.59 0.62 0.73 0.80
</table>
<tableCaption confidence="0.999766">
Table 2: F1 performance of different variants of PRA for all 15 relations tested.
</tableCaption>
<bodyText confidence="0.999950964285715">
which is a supertype or subtype of the relation be-
ing predicted. For instance, for the relation ath-
letePlaysForTeam (shown in Figure 2), the highest-
weighted features in PRA are athleteLedSport-
sTeam (more specific than athletePlaysForTeam)
and personBelongsToOrganization (more general
than athletePlaysForTeam). For the same rela-
tion, PRAgyntactic has features like “scored for”,
“signed”, “have”, and “led”. When using a latent
embedding of these verb phrases, “signed”, “have”,
and “led” all have the same representation in the la-
tent space, and so it seems clear that PRAlatent gains
a lot by reducing the sparsity inherent in using sur-
face verb forms.
For cityLiesOnRiver, where PRA does not per-
form as well, there is no NELL relation that is an im-
mediate supertype or subtype, and so PRA does not
have as much evidence to use. It finds features that,
e.g., are analogous to the statement “cities in the
same state probably lie on the same river”. Adding
lexical labels gives the model edges to use like “lies
on”, “runs through”, “flows through”, “starts in”
and “reaches”, and these features give a significant
boost in performance to PRAgyntactic. Once again,
almost all of those verb phrases share the same latent
embedding, and so PRAlatent gains another signifi-
cant boost in performance by combining them into a
single feature.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999845">
In this paper, we introduced the use of latent lexi-
cal edge labels for PRA-based inference over knowl-
edge bases. We obtained such latent edge labels
by mining a large dependency parsed corpus of
500 million web documents and performing PCA
on the result. Through extensive experiments on
real datasets, we demonstrated that the proposed ap-
proach significantly outperforms previous state-of-
the-art baselines.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999916333333333">
We thank William Cohen (CMU) for enlightening
conversations on topics discussed in this paper. We
thank the ClueWeb project (CMU) and the Hazy
Research Group (http://hazy.cs.wisc.edu/hazy/) for
their generous help with data sets; and to the anony-
mous reviewers for their constructive comments.
This research has been supported in part by DARPA
(under contract number FA8750-13-2-0005), and
Google. Any opinions, findings, conclusions and
recommendations expressed in this paper are the au-
thors’ and do not necessarily reflect those of the
sponsors.
</bodyText>
<page confidence="0.9958">
837
</page>
<sectionHeader confidence="0.989073" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997658259259259">
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the Fifth ACM conference on Digital
libraries.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collabo-
ratively created graph database for structuring human
knowledge. In Proceedings of SIGMOD.
Sergey Brin. 1999. Extracting patterns and relations
from the world wide web.
J. Callan, M. Hoy, C. Yoo, and L. Zhao. 2009.
Clueweb09 data set. boston.lti.cs.cmu.edu.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R Hruschka Jr, and Tom M Mitchell.
2010. Toward an architecture for never-ending lan-
guage learning. In AAAI.
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S Weld, and Alexander Yates.
2004. Web-scale information extraction in know-
itall:(preliminary results). In Proceedings of WWW.
Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam Mausam. 2011.
Open information extraction: The second generation.
In Proceedings of IJCAI.
Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th conference on Computational Linguistics.
Anna Korhonen, Yuval Krymolowski, and Zvika Marx.
2003. Clustering polysemic subcategorization frame
distributions semantically. In Proceedings of ACL.
Arun Kumar, Feng Niu, and Christopher R´e. 2013.
Hazy: making it easier to build and maintain big-data
analytics. Communications of the ACM, 56(3):40–49.
Aapo Kyrola, Guy Blelloch, and Carlos Guestrin. 2012.
Graphchi: Large-scale graph computation on just a pc.
In Proceedings of the 10th USENIX Symposium on Op-
erating Systems Design and Implementation (OSDI),
pages 31–46.
Ni Lao and William W Cohen. 2010. Relational re-
trieval using a combination of path-constrained ran-
dom walks. Machine learning, 81(1):53–67.
Ni Lao, Tom Mitchell, and William W Cohen. 2011.
Random walk inference and learning in a large scale
knowledge base. In Proceedings of EMNLP. Associa-
tion for Computational Linguistics.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of EMNLP-CoNLL.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. K¨ubler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13(02).
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of NAACL-HLT.
Burr Settles and Steven Dow. 2013. Let’s get together:
the formation and success of online creative collabora-
tions. In Proceedings of CHI.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of ACL.
Fabian M Suchanek, Georgiana Ifrim, and Gerhard
Weikum. 2006. Combining linguistic and statistical
analysis to extract relations from web documents. In
Proceedings of KDD.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In Proceedings of WWW.
Chang Wang, James Fan, Aditya Kalyanpur, and David
Gondek. 2011. Relation extraction with relation top-
ics. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1426–1436. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.997613">
838
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.940736">
<title confidence="0.999059">Improving Learning and Inference in a Large using Latent Syntactic Cues</title>
<author confidence="0.999071">Matt Gardner</author>
<author confidence="0.999071">Partha Pratim Talukdar</author>
<author confidence="0.999071">Bryan Kisiel</author>
<author confidence="0.999071">Tom</author>
<affiliation confidence="0.99558">Carnegie Mellon</affiliation>
<address confidence="0.9974155">5000 Forbes Pittsburgh, PA 15213,</address>
<abstract confidence="0.99673675">Automatically constructed Knowledge Bases (KBs) are often incomplete and there is a genuine need to improve their coverage. Path Ranking Algorithm (PRA) is a recently proposed method which aims to improve KB coverage by performing inference directly over the KB graph. For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task. We present extensive experimental results validating this finding. The resources presented in this paper are publicly available.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fifth ACM conference on Digital libraries.</booktitle>
<contexts>
<context position="4928" citStr="Agichtein and Gravano, 2000" startWordPosition="748" endWordPosition="751">ized edges may result in significant data sparsity, which can be overcome by mapping lexicalized edge labels to some latent embedding (e.g., (Alex Rodriguez, LatentFeat#5, NY Yankees) and running PRA over this augmented graph. Using latent embeddings, PRA could then use the following edge sequence as a feature in its prediction models: (LatentFeat#5, teamPlaysIn). We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles an</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In Proceedings of the Fifth ACM conference on Digital libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGMOD.</booktitle>
<contexts>
<context position="1044" citStr="Bollacker et al., 2008" startWordPosition="153" endWordPosition="156">ithm (PRA) is a recently proposed method which aims to improve KB coverage by performing inference directly over the KB graph. For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task. We present extensive experimental results validating this finding. The resources presented in this paper are publicly available. 1 Introduction Over the last few years, several large scale Knowledge Bases (KBs) such as Freebase (Bollacker et al., 2008), NELL (Carlson et al., 2010), and YAGO (Suchanek et al., 2007) have been developed. Each such KB consists of millions of facts (e.g., (Tiger Woods, playsSport, Golf)) spanning over multiple relations. Unfortunately, these KBs are often incomplete and there is a need to increase their coverage of facts to make them useful in practical applications. A strategy to increase coverage might be to perform inference directly over the KB represented as a graph. For example, if the KB contained the following facts, (Tiger Woods, participatesIn, PGA Tour)) and (Golf, sportOfTournament, PGA Tour), then b</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of SIGMOD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
</authors>
<title>Extracting patterns and relations from the world wide web.</title>
<date>1999</date>
<contexts>
<context position="4899" citStr="Brin, 1999" startWordPosition="746" endWordPosition="747">n of lexicalized edges may result in significant data sparsity, which can be overcome by mapping lexicalized edge labels to some latent embedding (e.g., (Alex Rodriguez, LatentFeat#5, NY Yankees) and running PRA over this augmented graph. Using latent embeddings, PRA could then use the following edge sequence as a feature in its prediction models: (LatentFeat#5, teamPlaysIn). We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of o</context>
</contexts>
<marker>Brin, 1999</marker>
<rawString>Sergey Brin. 1999. Extracting patterns and relations from the world wide web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Callan</author>
<author>M Hoy</author>
<author>C Yoo</author>
<author>L Zhao</author>
</authors>
<date>2009</date>
<note>Clueweb09 data set. boston.lti.cs.cmu.edu.</note>
<contexts>
<context position="10654" citStr="Callan et al., 2009" startWordPosition="1810" endWordPosition="1813">words, for each pair of directly connected nodes in the KB graph G, we add an additional edge between those two nodes for each verb which takes the NPs represented by two nodes as subjects and objects (or vice versa) as observed in a text corpus. In Figure 1, (Alex Rodriguez, “plays for”, NY Yankees) is an example of such an edge. PRA is then applied over this augmented graph G0, over the same set of prediction types R as before. We shall refer to this version of PRA as PRAsyntactic. For the experiments in this paper, we collected |D |= 600 million SVO triples1 from the entire ClueWeb corpus (Callan et al., 2009), parsed using the Malt parser (Nivre et al., 2007) by the Hazy project (Kumar et al., 2013). 3.3 PRAlatent In this section we construct G00 = (V, E&amp;quot;, T00), another syntactic-information-induced extension of the knowledge graph G, but instead of using the surface forms of verbs in S (see previous section) as edge types, we derive those edges types T00 based on latent embeddings of those verbs. We note that E ⊂ E00, and T ⊂ T00. In order to learn the latent or low dimensional embeddings of the verbs in S, we first define QS = {(s, o) |∃(s, v, o, c) ∈ D, v ∈ S}, the set of subject-object tuples </context>
</contexts>
<marker>Callan, Hoy, Yoo, Zhao, 2009</marker>
<rawString>J. Callan, M. Hoy, C. Yoo, and L. Zhao. 2009. Clueweb09 data set. boston.lti.cs.cmu.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Toward an architecture for never-ending language learning.</title>
<date>2010</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="1073" citStr="Carlson et al., 2010" startWordPosition="158" endWordPosition="161">ed method which aims to improve KB coverage by performing inference directly over the KB graph. For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task. We present extensive experimental results validating this finding. The resources presented in this paper are publicly available. 1 Introduction Over the last few years, several large scale Knowledge Bases (KBs) such as Freebase (Bollacker et al., 2008), NELL (Carlson et al., 2010), and YAGO (Suchanek et al., 2007) have been developed. Each such KB consists of millions of facts (e.g., (Tiger Woods, playsSport, Golf)) spanning over multiple relations. Unfortunately, these KBs are often incomplete and there is a need to increase their coverage of facts to make them useful in practical applications. A strategy to increase coverage might be to perform inference directly over the KB represented as a graph. For example, if the KB contained the following facts, (Tiger Woods, participatesIn, PGA Tour)) and (Golf, sportOfTournament, PGA Tour), then by putting these two facts tog</context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hruschka Jr, and Tom M Mitchell. 2010. Toward an architecture for never-ending language learning. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>Stanley Kok</author>
<author>Ana-Maria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Web-scale information extraction in knowitall:(preliminary results).</title>
<date>2004</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="4980" citStr="Etzioni et al., 2004" startWordPosition="756" endWordPosition="759">can be overcome by mapping lexicalized edge labels to some latent embedding (e.g., (Alex Rodriguez, LatentFeat#5, NY Yankees) and running PRA over this augmented graph. Using latent embeddings, PRA could then use the following edge sequence as a feature in its prediction models: (LatentFeat#5, teamPlaysIn). We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Kok, Popescu, Shaked, Soderland, Weld, Yates, 2004</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, Stanley Kok, Ana-Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S Weld, and Alexander Yates. 2004. Web-scale information extraction in knowitall:(preliminary results). In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Anthony Fader</author>
<author>Janara Christensen</author>
<author>Stephen Soderland</author>
<author>Mausam Mausam</author>
</authors>
<title>Open information extraction: The second generation.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="6421" citStr="Etzioni et al., 2011" startWordPosition="997" endWordPosition="1000">ing latent edge labels in addition to surface-level labels in the graph over which PRA is applied. In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus. Though we use Principal Components Analysis (PCA) for dimensionality reduction for the experiments in this paper, this is by no means the only choice. Various other dimensionality reduction techniques, and in particular, other verb clustering techniques (Korhonen et al., 2003), may also be used. OpenIE systems such as Reverb (Etzioni et al., 2011) also extract verb-anchored dependency triples from large text corpus. In contrast to such approaches, we focus on how latent embedding of verbs in such triples can be combined with explicit background knowledge to improve coverage of existing KBs. This has the added capability of inferring facts which are not explicitly mentioned in text. The recently proposed Universal Schema (Riedel et al., 2013) also demonstrates the benefit of using latent features for increasing coverage of KBs. Key differences between that approach and ours include our use of syntactic information as opposed to surface-</context>
</contexts>
<marker>Etzioni, Fader, Christensen, Soderland, Mausam, 2011</marker>
<rawString>Oren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, and Mausam Mausam. 2011. Open information extraction: The second generation. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th conference on Computational Linguistics.</booktitle>
<contexts>
<context position="4887" citStr="Hearst, 1992" startWordPosition="744" endWordPosition="745"> naive addition of lexicalized edges may result in significant data sparsity, which can be overcome by mapping lexicalized edge labels to some latent embedding (e.g., (Alex Rodriguez, LatentFeat#5, NY Yankees) and running PRA over this augmented graph. Using latent embeddings, PRA could then use the following edge sequence as a feature in its prediction models: (LatentFeat#5, teamPlaysIn). We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn fo</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
<author>Yuval Krymolowski</author>
<author>Zvika Marx</author>
</authors>
<title>Clustering polysemic subcategorization frame distributions semantically.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6349" citStr="Korhonen et al., 2003" startWordPosition="984" endWordPosition="987">els are either KB labels or at surface-level, in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied. In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus. Though we use Principal Components Analysis (PCA) for dimensionality reduction for the experiments in this paper, this is by no means the only choice. Various other dimensionality reduction techniques, and in particular, other verb clustering techniques (Korhonen et al., 2003), may also be used. OpenIE systems such as Reverb (Etzioni et al., 2011) also extract verb-anchored dependency triples from large text corpus. In contrast to such approaches, we focus on how latent embedding of verbs in such triples can be combined with explicit background knowledge to improve coverage of existing KBs. This has the added capability of inferring facts which are not explicitly mentioned in text. The recently proposed Universal Schema (Riedel et al., 2013) also demonstrates the benefit of using latent features for increasing coverage of KBs. Key differences between that approach </context>
</contexts>
<marker>Korhonen, Krymolowski, Marx, 2003</marker>
<rawString>Anna Korhonen, Yuval Krymolowski, and Zvika Marx. 2003. Clustering polysemic subcategorization frame distributions semantically. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arun Kumar</author>
<author>Feng Niu</author>
<author>Christopher R´e</author>
</authors>
<title>Hazy: making it easier to build and maintain big-data analytics.</title>
<date>2013</date>
<journal>Communications of the ACM,</journal>
<volume>56</volume>
<issue>3</issue>
<marker>Kumar, Niu, R´e, 2013</marker>
<rawString>Arun Kumar, Feng Niu, and Christopher R´e. 2013. Hazy: making it easier to build and maintain big-data analytics. Communications of the ACM, 56(3):40–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aapo Kyrola</author>
<author>Guy Blelloch</author>
<author>Carlos Guestrin</author>
</authors>
<title>Graphchi: Large-scale graph computation on just a pc.</title>
<date>2012</date>
<booktitle>In Proceedings of the 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI),</booktitle>
<pages>31--46</pages>
<contexts>
<context position="12790" citStr="Kyrola et al., 2012" startWordPosition="2198" endWordPosition="2201">ons. We find that use of latent edge labels, in particular the proposed approach PRAlatentd, significantly outperforms other approaches. This is our main result. (See Section 4) • PRAlatentd: The verb is mapped to disjunction of top-k most positive and negative columns in the row in A that corresponds to the verb. 4 Experiments We compared the various methods using 15 NELL relations. For each relation, we split NELL’s known relation instances into 90% training and 10% testing. For each method, we then selected 750 path features and trained the model, as described in Section 3, using GraphChi (Kyrola et al., 2012) to perform the random walk graph computations. To evaluate the model, we took all source nodes in the testing data and used the model to predict target nodes. We report the precision and recall (on the set of known target nodes) of the set of predictions for each model that are above a certain confidence threshold. Because we used strong regularization, we picked for our threshold a model score of 0.405, corresponding to 60% probability of the relation instance being true; values higher than this left many relations without any predictions. Table 1 contains the results. As can be seen in the </context>
</contexts>
<marker>Kyrola, Blelloch, Guestrin, 2012</marker>
<rawString>Aapo Kyrola, Guy Blelloch, and Carlos Guestrin. 2012. Graphchi: Large-scale graph computation on just a pc. In Proceedings of the 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI), pages 31–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>William W Cohen</author>
</authors>
<title>Relational retrieval using a combination of path-constrained random walks.</title>
<date>2010</date>
<booktitle>Machine learning,</booktitle>
<pages>81--1</pages>
<contexts>
<context position="1882" citStr="Lao and Cohen, 2010" startWordPosition="286" endWordPosition="289">hese KBs are often incomplete and there is a need to increase their coverage of facts to make them useful in practical applications. A strategy to increase coverage might be to perform inference directly over the KB represented as a graph. For example, if the KB contained the following facts, (Tiger Woods, participatesIn, PGA Tour)) and (Golf, sportOfTournament, PGA Tour), then by putting these two facts together, we could potentially infer that (Tiger Woods, playsSport, Golf). The Figure 1: Example demonstrating how lexicalized syntactic edges can improve connectivity in the KB enabling PRA (Lao and Cohen, 2010) to discover relationships between Alex Rodriguez and World Series. Edges with latent labels can improve inference performance by reducing data sparsity. See Section 1.1 for details. recently proposed Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) performs such inference by automatically learning semantic inference rules over the KB (Lao et al., 2011). PRA uses features based off of sequences of edge types, e.g., (playsSport, sportOfTournament), to predict missing facts in the KB. PRA was extended by (Lao et al., 2012) to perform inference over a KB augmented with dependency parsed sentenc</context>
<context position="5379" citStr="Lao and Cohen, 2010" startWordPosition="822" endWordPosition="825">is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to these previous PRA-based approaches where all edge labels are either KB labels or at surface-level, in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied. In particular, we focus on the problem of performing inference over a large KB a</context>
<context position="7374" citStr="Lao and Cohen, 2010" startWordPosition="1152" endWordPosition="1155">in text. The recently proposed Universal Schema (Riedel et al., 2013) also demonstrates the benefit of using latent features for increasing coverage of KBs. Key differences between that approach and ours include our use of syntactic information as opposed to surface-level patterns in theirs, and also the ability of the proposed PRA-based method to generate useful inference rules which is beyond the capability of the matrix factorization approach in (Riedel et al., 2013). 3 Method 3.1 Path Ranking Algorithm (PRA) In this section, we present a brief overview of the Path Ranking Algorithm (PRA) (Lao and Cohen, 2010), building on the notations in (Lao et al., 2012). Let G = (V, E, T) be the graph, where V is the set of vertices, E is the set of edges, and T is the set of edge types. For each edge (v1, t, v2) E E, we have 834 v1, v2 ∈ V and t ∈ T. Let R ⊂ T be the set of types predicted by PRA. R could in principal equal T, but in this paper we restrict prediction to KB relations, while T also includes types derived from surface text and latent embeddings. Let 7r = ht1, t2, ... , twi be a path type of length w over graph G, where ti ∈ T is the type of the ith edge in the path. Each such path type is also a</context>
<context position="9361" citStr="Lao and Cohen, 2010" startWordPosition="1552" endWordPosition="1555">nageable set of features to compute, we first perform a feature selection step. The goal of this step is to select for computation only those path types that commonly connect sources and targets of relation r. We perform this feature selection by doing length-bounded random walks from a given list of source and target nodes, keeping track of how frequently each path type leads from a source node to a target node. The most common m path types are selected for the set B. Training: We perform standard logistic regression with L2 regularization to learn the weights B,&apos;. We follow the strategy in (Lao and Cohen, 2010) to generate positive and negative training instances. 3.2 PRAsyntactic In this section, we shall extend the knowledge graph G = (V, E, T) from the previous section with an augmented graph G0 = (V, E0, T0), where E ⊂ E0 and T ⊂ T0, with the set of vertices unchanged. In order to get the edges in E0 − E, we first collect a set of Subject-Verb-Object (SVO) triples D = {(s, v, o, c)} from a large dependency parsed text corpus, with c ∈ R+ denoting the frequency of this triple in the corpus. The additional edge set is then defined as Esyntactic = E0 − E = {(s, v, o) |∃(s, v, o, c) ∈ D, s, o ∈ V }.</context>
</contexts>
<marker>Lao, Cohen, 2010</marker>
<rawString>Ni Lao and William W Cohen. 2010. Relational retrieval using a combination of path-constrained random walks. Machine learning, 81(1):53–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>Tom Mitchell</author>
<author>William W Cohen</author>
</authors>
<title>Random walk inference and learning in a large scale knowledge base.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2239" citStr="Lao et al., 2011" startWordPosition="341" endWordPosition="344">ent, PGA Tour), then by putting these two facts together, we could potentially infer that (Tiger Woods, playsSport, Golf). The Figure 1: Example demonstrating how lexicalized syntactic edges can improve connectivity in the KB enabling PRA (Lao and Cohen, 2010) to discover relationships between Alex Rodriguez and World Series. Edges with latent labels can improve inference performance by reducing data sparsity. See Section 1.1 for details. recently proposed Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) performs such inference by automatically learning semantic inference rules over the KB (Lao et al., 2011). PRA uses features based off of sequences of edge types, e.g., (playsSport, sportOfTournament), to predict missing facts in the KB. PRA was extended by (Lao et al., 2012) to perform inference over a KB augmented with dependency parsed sentences. While this opens up the possibility of learning syntactic-semantic inference rules, the set of syntactic edge labels used are just the unlexicalized dependency role labels (e.g., nobj, dobj, etc., without the corresponding words), thereby limiting overall expressitivity of the learned inference rules. To overcome this limitation, in this paper we augm</context>
<context position="5470" citStr="Lao et al., 2011" startWordPosition="836" endWordPosition="839">ts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to these previous PRA-based approaches where all edge labels are either KB labels or at surface-level, in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied. In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus</context>
</contexts>
<marker>Lao, Mitchell, Cohen, 2011</marker>
<rawString>Ni Lao, Tom Mitchell, and William W Cohen. 2011. Random walk inference and learning in a large scale knowledge base. In Proceedings of EMNLP. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>Amarnag Subramanya</author>
<author>Fernando Pereira</author>
<author>William W Cohen</author>
</authors>
<title>Reading the web with learned syntactic-semantic inference rules.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="2410" citStr="Lao et al., 2012" startWordPosition="369" endWordPosition="372">zed syntactic edges can improve connectivity in the KB enabling PRA (Lao and Cohen, 2010) to discover relationships between Alex Rodriguez and World Series. Edges with latent labels can improve inference performance by reducing data sparsity. See Section 1.1 for details. recently proposed Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) performs such inference by automatically learning semantic inference rules over the KB (Lao et al., 2011). PRA uses features based off of sequences of edge types, e.g., (playsSport, sportOfTournament), to predict missing facts in the KB. PRA was extended by (Lao et al., 2012) to perform inference over a KB augmented with dependency parsed sentences. While this opens up the possibility of learning syntactic-semantic inference rules, the set of syntactic edge labels used are just the unlexicalized dependency role labels (e.g., nobj, dobj, etc., without the corresponding words), thereby limiting overall expressitivity of the learned inference rules. To overcome this limitation, in this paper we augment the KB graph by adding edges with more expressive lexicalized syntactic labels (where the labels are words instead of dependen833 Proceedings of the 2013 Conference on</context>
<context position="5563" citStr="Lao et al., 2012" startWordPosition="852" endWordPosition="855"> Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to these previous PRA-based approaches where all edge labels are either KB labels or at surface-level, in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied. In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus. Though we use Principal Components Analysis (PCA) for dimensionality reduction for the expe</context>
<context position="7423" citStr="Lao et al., 2012" startWordPosition="1161" endWordPosition="1164">del et al., 2013) also demonstrates the benefit of using latent features for increasing coverage of KBs. Key differences between that approach and ours include our use of syntactic information as opposed to surface-level patterns in theirs, and also the ability of the proposed PRA-based method to generate useful inference rules which is beyond the capability of the matrix factorization approach in (Riedel et al., 2013). 3 Method 3.1 Path Ranking Algorithm (PRA) In this section, we present a brief overview of the Path Ranking Algorithm (PRA) (Lao and Cohen, 2010), building on the notations in (Lao et al., 2012). Let G = (V, E, T) be the graph, where V is the set of vertices, E is the set of edges, and T is the set of edge types. For each edge (v1, t, v2) E E, we have 834 v1, v2 ∈ V and t ∈ T. Let R ⊂ T be the set of types predicted by PRA. R could in principal equal T, but in this paper we restrict prediction to KB relations, while T also includes types derived from surface text and latent embeddings. Let 7r = ht1, t2, ... , twi be a path type of length w over graph G, where ti ∈ T is the type of the ith edge in the path. Each such path type is also a feature in the PRA model. For a given source and</context>
</contexts>
<marker>Lao, Subramanya, Pereira, Cohen, 2012</marker>
<rawString>Ni Lao, Amarnag Subramanya, Fernando Pereira, and William W Cohen. 2012. Reading the web with learned syntactic-semantic inference rules. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>A Chanev</author>
<author>G Eryigit</author>
<author>S K¨ubler</author>
<author>S Marinov</author>
<author>E Marsi</author>
</authors>
<title>Maltparser: A language-independent system for datadriven dependency parsing.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>02</issue>
<marker>Nivre, Hall, Nilsson, Chanev, Eryigit, K¨ubler, Marinov, Marsi, 2007</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. K¨ubler, S. Marinov, and E. Marsi. 2007. Maltparser: A language-independent system for datadriven dependency parsing. Natural Language Engineering, 13(02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4957" citStr="Ravichandran and Hovy, 2002" startWordPosition="752" endWordPosition="755">ificant data sparsity, which can be overcome by mapping lexicalized edge labels to some latent embedding (e.g., (Alex Rodriguez, LatentFeat#5, NY Yankees) and running PRA over this augmented graph. Using latent embeddings, PRA could then use the following edge sequence as a feature in its prediction models: (LatentFeat#5, teamPlaysIn). We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al.,</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
<author>Benjamin M Marlin</author>
</authors>
<title>Relation extraction with matrix factorization and universal schemas.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="6823" citStr="Riedel et al., 2013" startWordPosition="1062" endWordPosition="1065">ns the only choice. Various other dimensionality reduction techniques, and in particular, other verb clustering techniques (Korhonen et al., 2003), may also be used. OpenIE systems such as Reverb (Etzioni et al., 2011) also extract verb-anchored dependency triples from large text corpus. In contrast to such approaches, we focus on how latent embedding of verbs in such triples can be combined with explicit background knowledge to improve coverage of existing KBs. This has the added capability of inferring facts which are not explicitly mentioned in text. The recently proposed Universal Schema (Riedel et al., 2013) also demonstrates the benefit of using latent features for increasing coverage of KBs. Key differences between that approach and ours include our use of syntactic information as opposed to surface-level patterns in theirs, and also the ability of the proposed PRA-based method to generate useful inference rules which is beyond the capability of the matrix factorization approach in (Riedel et al., 2013). 3 Method 3.1 Path Ranking Algorithm (PRA) In this section, we present a brief overview of the Path Ranking Algorithm (PRA) (Lao and Cohen, 2010), building on the notations in (Lao et al., 2012)</context>
</contexts>
<marker>Riedel, Yao, McCallum, Marlin, 2013</marker>
<rawString>Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M Marlin. 2013. Relation extraction with matrix factorization and universal schemas. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
<author>Steven Dow</author>
</authors>
<title>Let’s get together: the formation and success of online creative collaborations.</title>
<date>2013</date>
<booktitle>In Proceedings of CHI.</booktitle>
<contexts>
<context position="5540" citStr="Settles and Dow, 2013" startWordPosition="847" endWordPosition="850">vano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to these previous PRA-based approaches where all edge labels are either KB labels or at surface-level, in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied. In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus. Though we use Principal Components Analysis (PCA) for dimensionality</context>
</contexts>
<marker>Settles, Dow, 2013</marker>
<rawString>Burr Settles and Steven Dow. 2013. Let’s get together: the formation and success of online creative collaborations. In Proceedings of CHI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5076" citStr="Snow et al., 2006" startWordPosition="772" endWordPosition="775">LatentFeat#5, NY Yankees) and running PRA over this augmented graph. Using latent embeddings, PRA could then use the following edge sequence as a feature in its prediction models: (LatentFeat#5, teamPlaysIn). We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to the</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Georgiana Ifrim</author>
<author>Gerhard Weikum</author>
</authors>
<title>Combining linguistic and statistical analysis to extract relations from web documents.</title>
<date>2006</date>
<booktitle>In Proceedings of KDD.</booktitle>
<contexts>
<context position="5100" citStr="Suchanek et al., 2006" startWordPosition="776" endWordPosition="779">nkees) and running PRA over this augmented graph. Using latent embeddings, PRA could then use the following edge sequence as a feature in its prediction models: (LatentFeat#5, teamPlaysIn). We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to these previous PRA-based ap</context>
</contexts>
<marker>Suchanek, Ifrim, Weikum, 2006</marker>
<rawString>Fabian M Suchanek, Georgiana Ifrim, and Gerhard Weikum. 2006. Combining linguistic and statistical analysis to extract relations from web documents. In Proceedings of KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: a core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="1107" citStr="Suchanek et al., 2007" startWordPosition="164" endWordPosition="167">B coverage by performing inference directly over the KB graph. For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRAbased approaches on the KB inference task. We present extensive experimental results validating this finding. The resources presented in this paper are publicly available. 1 Introduction Over the last few years, several large scale Knowledge Bases (KBs) such as Freebase (Bollacker et al., 2008), NELL (Carlson et al., 2010), and YAGO (Suchanek et al., 2007) have been developed. Each such KB consists of millions of facts (e.g., (Tiger Woods, playsSport, Golf)) spanning over multiple relations. Unfortunately, these KBs are often incomplete and there is a need to increase their coverage of facts to make them useful in practical applications. A strategy to increase coverage might be to perform inference directly over the KB represented as a graph. For example, if the KB contained the following facts, (Tiger Woods, participatesIn, PGA Tour)) and (Golf, sportOfTournament, PGA Tour), then by putting these two facts together, we could potentially infer </context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Wang</author>
<author>James Fan</author>
<author>Aditya Kalyanpur</author>
<author>David Gondek</author>
</authors>
<title>Relation extraction with relation topics.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1426--1436</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5224" citStr="Wang et al., 2011" startWordPosition="795" endWordPosition="798">eature in its prediction models: (LatentFeat#5, teamPlaysIn). We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to these previous PRA-based approaches where all edge labels are either KB labels or at surface-level, in this paper we explore using latent edge labels i</context>
</contexts>
<marker>Wang, Fan, Kalyanpur, Gondek, 2011</marker>
<rawString>Chang Wang, James Fan, Aditya Kalyanpur, and David Gondek. 2011. Relation extraction with relation topics. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1426–1436. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000122">
<title confidence="0.932261">
Identifying Phrasal Verbs Using Many Bilingual Corpora
</title>
<author confidence="0.994247">
Karl Pichotta* John DeNero
</author>
<affiliation confidence="0.985648">
Department of Computer Science Google, Inc.
University of Texas at Austin denero@google.com
</affiliation>
<email confidence="0.987266">
pichotta@cs.utexas.edu
</email>
<sectionHeader confidence="0.994585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999410083333333">
We address the problem of identifying mul-
tiword expressions in a language, focus-
ing on English phrasal verbs. Our poly-
glot ranking approach integrates frequency
statistics from translated corpora in 50 dif-
ferent languages. Our experimental eval-
uation demonstrates that combining statisti-
cal evidence from many parallel corpora us-
ing a novel ranking-oriented boosting algo-
rithm produces a comprehensive set of English
phrasal verbs, achieving performance compa-
rable to a human-curated set.
</bodyText>
<sectionHeader confidence="0.998419" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999421589285715">
A multiword expression (MWE), or noncomposi-
tional compound, is a sequence of words whose
meaning cannot be composed directly from the
meanings of its constituent words. These idiosyn-
cratic phrases are prevalent in the lexicon of a lan-
guage; Jackendoff (1993) estimates that their num-
ber is on the same order of magnitude as that of sin-
gle words, and Sag et al. (2002) suggest that they
are much more common, though quantifying them
is challenging (Church, 2011). The task of identify-
ing MWEs is relevant not only to lexical semantics
applications, but also machine translation (Koehn et
al., 2003; Ren et al., 2009; Pal et al., 2010), informa-
tion retrieval (Xu et al., 2010; Acosta et al., 2011),
and syntactic parsing (Sag et al., 2002). Awareness
of MWEs has empirically proven useful in a num-
ber of domains: Finlayson and Kulkarni (2011), for
example, use MWEs to attain a significant perfor-
mance improvement in word sense disambiguation;
Venkatapathy and Joshi (2006) use features associ-
ated with MWEs to improve word alignment.
&amp;quot;Research conducted during an internship at Google.
We focus on a particular subset of MWEs, English
phrasal verbs. A phrasal verb consists of a head
verb followed by one or more particles, such that
the meaning of the phrase cannot be determined by
combining the simplex meanings of its constituent
words (Baldwin and Villavicencio, 2002; Dixon,
1982; Bannard et al., 2003).1 Examples of phrasal
verbs include count on [rely], look after [tend], or
take off [remove], the meanings of which do not in-
volve counting, looking, or taking. In contrast, there
are verbs followed by particles that are not phrasal
verbs, because their meaning is compositional, such
as walk towards, sit behind, or paint on.
We identify phrasal verbs by using frequency
statistics calculated from parallel corpora, consist-
ing of bilingual pairs of documents such that one
is a translation of the other, with one document in
English. We leverage the observation that a verb
will translate in an atypical way when occurring as
the head of a phrasal verb. For example, the word
look in the context of look after will tend to trans-
late differently from how look translates generally.
In order to characterize this difference, we calculate
a frequency distribution over translations of look,
then compare it to the distribution of translations of
look when followed by the word after. We expect
that idiomatic phrasal verbs will tend to have unex-
pected translation of their head verbs, measured by
the Kullback-Leibler divergence between those dis-
tributions.
Our polyglot ranking approach is motivated by the
hypothesis that using many parallel corpora of dif-
ferent languages will help determine the degree of
semantic idiomaticity of a phrase. In order to com-
</bodyText>
<footnote confidence="0.9672625">
1Nomenclature varies: the term verb-particle construction
is also used to denote what we call phrasal verbs; further, the
term phrasal verb is sometimes used to denote a broader class
of constructions.
</footnote>
<page confidence="0.94134">
636
</page>
<note confidence="0.736481">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 636–646,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.99981325">
bine evidence from multiple languages, we develop
a novel boosting algorithm tailored to the task of
ranking multiword expressions by their degree of id-
iomaticity. We train and evaluate on disjoint subsets
of the phrasal verbs in English Wiktionary2. In our
experiments, the set of phrasal verbs identified au-
tomatically by our method achieves held-out recall
that nears the performance of the phrasal verbs in
WordNet 3.0, a human-curated set. Our approach
strongly outperforms a monolingual system, and
continues to improve when incrementally adding
translation statistics for 50 different languages.
</bodyText>
<sectionHeader confidence="0.984382" genericHeader="method">
2 Identifying Phrasal Verbs
</sectionHeader>
<bodyText confidence="0.993521433333333">
The task of identifying phrasal verbs using corpus
information raises several issues of experimental de-
sign. We consider four central issues below in moti-
vating our approach.
Types vs. Tokens. When a phrase is used in con-
text, it takes a particular meaning among its pos-
sible senses. Many phrasal verbs admit composi-
tional senses in addition to idiomatic ones—contrast
idiomatic “look down on him for his politics” with
compositional “look down on him from the balcony.”
In this paper, we focus on the task of determining
whether a phrase type is a phrasal verb, meaning that
it frequently expresses an idiomatic meaning across
its many token usages in a corpus. We do not at-
tempt to distinguish which individual phrase tokens
in the corpus have idiomatic senses.
Ranking vs. Classification. Identifying phrasal
verbs involves relative, rather than categorical, judg-
ments: some phrasal verbs are more compositional
than others, but retain a degree of noncomposition-
ality (McCarthy et al., 2003). Moreover, a poly-
semous phrasal verb may express an idiosyncratic
sense more or less often than a compositional sense
in a particular corpus. Therefore, we should expect
a corpus-driven system not to classify phrases as
strictly idiomatic or compositional, but instead as-
sign a ranking or relative scoring to a set of candi-
dates.
Candidate Phrases. We distinguish between the
task of identifying candidate multiword expressions
</bodyText>
<footnote confidence="0.991572">
2http://en.wiktionary.org
</footnote>
<table confidence="0.9561388">
Feature Description
cOL (x50) KL Divergence for each language L
µ1 frequency of phrase given verb
µ2 PMI of verb and particles
µ3 µ1 with interposed pronouns
</table>
<tableCaption confidence="0.999716">
Table 1: Features used by the polyglot ranking system.
</tableCaption>
<bodyText confidence="0.996271315789474">
and the task of ranking those candidates by their se-
mantic idiosyncracy. With English phrasal verbs, it
is straightforward to enumerate all desired verbs fol-
lowed by one or more particles, and rank the entire
set.
Using Parallel Corpora. There have been a num-
ber of approaches proposed for the use of multilin-
gual resources for MWE identification (Melamed,
1997; Villada Moir´on and Tiedemann, 2006; Caseli
et al., 2010; Tsvetkov and Wintner, 2012; Salehi
and Cook, 2013). Our approach differs from pre-
vious work in that we identify MWEs using transla-
tion distributions of verbs, as opposed to 1–1, 1–m,
or m–n word alignments, most-likely translations,
bilingual dictionaries, or distributional entropy. To
the best of our knowledge, ours is the first approach
to use translational distributions to leverage the ob-
servation that a verb typically translates differently
when it heads a phrasal verb.
</bodyText>
<sectionHeader confidence="0.987104" genericHeader="method">
3 The Polyglot Ranking Approach
</sectionHeader>
<bodyText confidence="0.999972818181818">
Our approach uses bilingual and monolingual statis-
tics as features, computed over unlabeled corpora.
Each statistic characterizes the degree of idiosyn-
crasy of a candidate phrasal verb, using a single
monolingual or bilingual corpus. We combine fea-
tures for many language pairs using a boosting algo-
rithm that optimizes a ranking objective using a su-
pervised training set of English phrasal verbs. Each
of these aspects of our approach is described in de-
tail below; for reference, Table 1 provides a list of
the features used.
</bodyText>
<subsectionHeader confidence="0.999678">
3.1 Bilingual Statistics
</subsectionHeader>
<bodyText confidence="0.9994135">
One of the intuitive properties of an MWE is that
its individual words likely do not translate literally
when the whole expression is translated into another
language (Melamed, 1997). We capture this effect
</bodyText>
<page confidence="0.997257">
637
</page>
<bodyText confidence="0.999266260869565">
by measuring the divergence between how a verb
translates generally and how it translates when head-
ing a candidate phrasal verb.
A parallel corpus is a collection of document
pairs hDE, DFi, where DE is in English, DF is in
another language, one document is a translation of
the other, and all documents DF are in the same
language. A phrase-aligned parallel corpus aligns
those documents at a sentence, phrase, and word
level. A phrase e aligns to another phrase f if some
word in e aligns to some word in f and no word in
e or f aligns outside of f or e, respectively. As a
result of this definition, the words within an aligned
phrase pair are themselves connected by word-level
alignments.
Given an English phrase e, define F(e) to be the
set of all foreign phrases observed aligned to e in a
parallel corpus. For any f ∈ F(e), let P(f|e) be the
conditional probability of the phrase e translating to
the phrase f. This probability is estimated as the
relative frequency of observing f and e as an aligned
phrase pair, conditioned on observing e aligned to
any phrase in the corpus:
</bodyText>
<equation confidence="0.9925045">
P(f|e) = E f� N(e, f&apos;)
N(e, f)
</equation>
<bodyText confidence="0.999758206896552">
with N(e, f) the number of times e and f are ob-
served occurring as an aligned phrase pair.
Next, we assign statistics to individual verbs
within phrases. The first word of a candidate phrasal
verb e is a verb. For a candidate phrasal verb e and
a foreign phrase f, let π1(e, f) be the subphrase of
f that is most commonly word-aligned to the first
word of e. As an example, consider the phrase pair
e = talk down to and f = hablar con menosprecio.
Suppose that when e is aligned to f, the word talk is
most frequently aligned to hablar. Then π1(e, f) =
hablar.
For a phrase e and its set F(e) of aligned trans-
lations, we define the constituent translation proba-
bility of a foreign subphrase x as:
to its subphrase most commonly aligned to the verb
in e. It expresses how this verb is translated in the
context of a phrasal verb construction.3 Equation (1)
defines a distribution over all phrases x of a foreign
language.
We also assign statistics to verbs as they are trans-
lated outside of the context of a phrase. Let v(e)
be the verb of a phrasal verb candidate e, which
is always its first word. For a single-word verb
phrase v(e), we can compute the constituent transla-
tion probability Pv(e)(x), again using Equation (1).
The difference between Pe(x) and Pv(e)(x) is that
the latter sums over all translations of the verb v(e),
regardless of whether it appears in the context of e:
</bodyText>
<equation confidence="0.9995375">
Pv(e)(x) = � P(f|v(e)) · δ (π1(v(e),f),x)
fEF(v(e))
</equation>
<bodyText confidence="0.999754166666667">
For a one-word phrase such as v(e), π1(v(e), f)
is the subphrase of f that most commonly directly
word-aligns to the one word of v(e).
Finally, for a phrase e and its verb v(e), we calcu-
late the Kullback-Leibler (KL) divergence between
the translation distribution of v(e) and e:
</bodyText>
<equation confidence="0.986675">
�
DKL (Pv(e)kPe ) =
x
</equation>
<bodyText confidence="0.999655454545454">
where the sum ranges over all x such that Pv(e)(x) &gt;
0. This quantifies the difference between the trans-
lations of e’s verb when it occurs in e, and when it
occurs in general. Figure 1 illustrates this computa-
tion on a toy corpus.
Smoothing. Equation (2) is defined only if, for ev-
ery x such that Pv(e)(x) &gt; 0, it is also the case
that Pe(x) &gt; 0. In order to ensure that this con-
dition holds, we smooth the translation distributions
toward uniform. Let D be the set of phrases with
non-zero probability under either distribution:
</bodyText>
<equation confidence="0.956087714285714">
D = {x : Pv(e)(x) &gt; 0 or Pe(x) &gt; 0}
Then, let UD be the uniform distribution over D:
Pv(e)(x) ln
Pe(x) (2)
Pv(e)(x)
�Pe(x) = P(f|e) · δ (π1(e, f), x) (1) UD(x) = f 1/|D |if x ∈ D
fEF(e) l 0 if x ∈/D
</equation>
<bodyText confidence="0.894521666666667">
where δ is the Kronecker delta function, taking value
1 if its arguments are equal and 0 otherwise. Intu-
itively, Pe assigns the probability mass for every f
</bodyText>
<footnote confidence="0.666535666666667">
3To extend this statistic to other types of multiword expres-
sions, one could compute a similar distribution for other content
words in the phrase.
</footnote>
<page confidence="0.983351">
638
</page>
<figure confidence="0.9602455">
Aligned Phrase Pair N(e, f) ⇡ (e, f)
DKL(Pv(e;)kP0e;) = —0.109 + —0.045 + 1.159 = 1.005
</figure>
<figureCaption confidence="0.75819925">
Figure 1: The computation of DKL(P 0 �(e�)kP0 e�) using a
toy corpus, for e = looking forward to. Note that the sec-
ond aligned phrase pair contains the third, so the second’s
count of 3 must be included in the third’s count of 5.
</figureCaption>
<bodyText confidence="0.998455">
When computing divergence in Equation (2), we use
the smoothed distributions Pe0 and P0v(e):
</bodyText>
<equation confidence="0.999996">
P0e(x) = αPe(x) + (1 − α)UD(x)
P0v(e)(x) = αPv(e)(x) + (1 − α)UD(x).
</equation>
<bodyText confidence="0.997519230769231">
We use α = 0.95, which distributes 5% of the total
probability mass evenly among all events in D.
Morphology. We calculate statistics for morpho-
logical variants of an English phrase. For a candi-
date English phrasal verb e (for example, look up),
let E denote the set of inflections of that phrasal verb
(for look up, this will be [look|looks|looked|looking]
up). We extract the variants in E from the verb en-
tries in English Wiktionary. The final score com-
puted from a phrase-aligned parallel corpus translat-
ing English sentences into a language L is the aver-
age KL divergence of smoothed constituent transla-
tion distributions for any inflected form ei ∈ E:
</bodyText>
<equation confidence="0.867173">
� �
DKL P 0 v(ei)kP 0 ei
</equation>
<subsectionHeader confidence="0.999659">
3.2 Monolingual Statistics
</subsectionHeader>
<bodyText confidence="0.999985363636364">
We also collect a number of monolingual statistics
for each phrasal verb candidate, motivated by the
considerable body of previous work on the topic
(Church and Hanks, 1990; Lin, 1999; McCarthy et
al., 2003). The monolingual statistics are designed
to identify frequent collocations in a language. This
set of monolingual features is not comprehensive, as
we focus our attention primarily on bilingual fea-
tures in this paper.
As above, define E toxbe thefset of morpholog-
ically inflectedvariants of a candidate e, and let
</bodyText>
<equation confidence="0.919651875">
\hline \\ [-1ex]
V be the set of inflected variants of the head verb
$P_{(e)})&amp;. $
\hline \\ 1ex]
v(e) of e. We define three statistics calculated from
$Pe(x)$ &amp;$\frac{3}{4}=0.75 $ &amp;$\fra
the phrase counts of a monolingual English corpus.
\hline \\ [1ex]
</equation>
<bodyText confidence="0.9962555">
First, we define p1_(e) to be the relative frequency of [1ex]
the candidate e, given e’s head verb, summed over
</bodyText>
<equation confidence="0.98409">
\nd{taular}
morphological variants:
µ1(e) = ln P (E|V )
D_{KL} (P_{v(e_i)
P
ei∈E N(ei)
= ln Pvi∈V N(vi)
</equation>
<bodyText confidence="0.999985">
where N(x) is the number of times phrase x was
observed in the monolingual corpus.
Second, define µ2(e) to be the pointwise mutual
information (PMI) between V (the event that one of
the inflections of the verb in e is observed) and R,
the event of observing the rest of the phrase:
</bodyText>
<equation confidence="0.9564478">
µ2(e)
= PMI(V, R)
= lg P(V, R) − lg (P(V)P(R))
= lg P(E) − lg (P(V)P(R))
=
</equation>
<bodyText confidence="0.911355142857143">
N(vi)−lg N(r)+lg N
where N is the total number of tokens in the corpus,
and logarithms are base-2. This statistic character-
izes the degree of association between a verb and
its phrasal extension. We only calculate µ2 for two-
word phrases, as it did not prove helpful for longer
phrases.
</bodyText>
<figure confidence="0.968599941176471">
1
3
5
3
looking forward to
deseando
deseando
mirando
looking
mirando
looking
buscando a
mirando
buscando
looking forward to
mirando adelante a
mirando deseando buscando
</figure>
<equation confidence="0.9720105">
Pv(e)(x) = 0. 25 0 = 0.3 5
P0v(e)(x) 0. 10 0.02 0.3 3
Pe(x) = 0. 5 = 0.25 0
P0e(x) 0. 29 0.254 0.02
1 X
ϕL(e) = |E|
ei∈E
X X
lg N(ei)−lg
ei∈E vi∈V
</equation>
<page confidence="0.988122">
639
</page>
<bodyText confidence="0.9998243">
Finally, define µ3(e) to be the relative frequency
of the phrasal verb e augmented by an accusative
pronoun, conditioned on the verb. Let A be the
set of phrases in E with an accusative pronoun (it,
them, him, her, me, you) optionally inserted either at
the end of the phrase or directly after the verb. For
e = look up, A = {look up, look X up, look up X,
looks up, looks X up, looks up X, ... }, with X an
accusative pronoun. The µ3 statistic is similar to µ1,
but allows for an intervening or following pronoun:
</bodyText>
<equation confidence="0.998560666666667">
µ3(e) = lnP(A|V )
= ln EeiEA N(ei)
EviEV N(vi)
</equation>
<bodyText confidence="0.99968325">
This statistic is designed to exploit the intuition that
phrasal verbs frequently have accusative pronouns
either inserted into the middle (e.g. look it up) or at
the end (e.g. look down on him).
</bodyText>
<subsectionHeader confidence="0.999397">
3.3 Ranking Phrasal Verb Candidates
</subsectionHeader>
<bodyText confidence="0.980378142857143">
Our goal is to assign a single real-valued score to
each candidate e, by which we can rank candidates
according to semantic idiosyncrasy. For each lan-
guage L for which we have a parallel corpus, we
defined, in section 3.1, a function cOL(e) assigning
real values to candidate phrasal verbs e, which we
hypothesize is higher on average for more idiomatic
compounds. Further, in section 3.2, we defined real-
valued monolingual functions µ1, µ2, and µ3 for
which we hypothesize the same trend holds. Be-
cause each score individually ranks all candidates,
it is natural to view each cOL and µi as a weak rank-
ing function that we can combine with a supervised
boosting objective. We use a modified version of
AdaBoost (Freund and Schapire, 1995) that opti-
mizes for recall.
For each cOL and µi, we compute a ranked list
of candidate phrasal verbs, ordered from highest to
lowest value. To simplify learning, we consider only
the top 5000 candidate phrasal verbs according to
µ1, µ2, and µ3. This pruning procedure excludes
candidates that do not appear in our monolingual
corpus.
We optimize the ranker using an unranked, in-
complete training set of phrasal verbs. We can eval-
uate the quality of the ranker by outputting the top
N ranked candidates and measuring recall relative
Algorithm 1 Recall-Oriented Ranking AdaBoost
</bodyText>
<listItem confidence="0.962179136363637">
1: for i = 1 : |X |do
2: w[i] +— 1/|X|
3: end for
4: for t = 1 : T do
5: for all h E H do
6: Eh +— 0
7: for i = 1 : |X |do
8: if xi E� h then
9: Eh +— Eh + w[i]
10: end if
11: end for
12: end for
13: ht +— argmaxhEH |EB − Eh|
14: αt +— ln(EB/Eht)
15: for i = 1 : |X |do
16: if xi E ht then
17: w[i] +— 1 � w[i] exp (−αt)
18: else
19: w[i]�1�w[i] exp (αt)
20: end if
21: end for
22: end for
</listItem>
<bodyText confidence="0.999181217391304">
to this gold-standard training set. We choose this
recall-at-N metric so as to not directly penalize pre-
cision errors, as our training set is incomplete.
Define H to be the set of N-element sets contain-
ing the top proposals for each weak ranker (we use
N = 2000). That is, each element of H is a set con-
taining the 2000 highest values for some cOL or µi.
We define the baseline error EB to be 1 − ]E[R], with
R the recall-at-N of a ranker ordering the candidate
phrases in the set UH at random. The value ]E[R] is
estimated by averaging the recall-at-N of 1000 ran-
dom orderings of UH.
Algorithm 1 gives the formulation of the Ada-
Boost training algorithm that we use to combine
weak rankers. The algorithm maintains a weight
vector w (summing to 1) which contains a positive
real number for each gold standard phrasal verb in
the training set X. Initially, w is uniformly set to
1/|X|. At each iteration of the algorithm, w is mod-
ified to take higher values for recently misclassi-
fied examples. We repeatedly choose weak rankers
ht E H (and corresponding real-valued coefficients
αt) that correctly rank examples with high w values.
</bodyText>
<page confidence="0.988896">
640
</page>
<bodyText confidence="0.999012">
Lines 5–12 of Algorithm 1 calculate the weighted
error values Eh for every weak ranker set h E H.
The error Eh will be 1 if h contains none of X and 0
if h contains all of X, as w always sums to 1. Line
13 picks the ranker ht E H whose weighted error is
as far as possible from the random baseline error EB.
Line 14 calculates a coefficient αt for ht, which will
be positive if Eht &lt; EB and negative if Eht &gt; EB.
Intuitively, αt encodes the importance of ht—it will
be high if ht performs well, and low if it performs
poorly. The Z in lines 17 and 19 is the normalizing
constant ensuring the vector w sums to 1.
After termination of Algorithm 1, we have
weights α1, ... , αT and lists h1, ... , hT. Define ft
as the function that generated the list ht (each ft will
be some cOL or pi). Now, we define a final combined
function co, taking a phrase e and returning a real
number:
</bodyText>
<equation confidence="0.998929">
T
�0(e) _ αtft(e).
t=1
</equation>
<bodyText confidence="0.9999857">
We standardize the scores of individual weak
rankers to have mean 0 and variance 1, so that their
scores are comparable.
The final learned ranker outputs a real value, in-
stead of the class labels frequently found in Ada-
Boost. This follows previous work using boosting
for learning to rank (Freund et al., 2003; Xu and Li,
2007). Our algorithm differs from previous methods
because we are seeking to optimize for Recall-at-N,
rather than a ranking loss.
</bodyText>
<sectionHeader confidence="0.997867" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.988911">
4.1 Training and Test Set
</subsectionHeader>
<bodyText confidence="0.997099">
In order to train and evaluate our system, we con-
struct a gold-standard list of phrasal verbs from
the freely available English Wiktionary. We gather
phrasal verbs from three sources within Wiktionary:
</bodyText>
<listItem confidence="0.993555">
1. Entries labeled as English phrasal verbs4,
2. Entries labeled as English idioms5, and
3. The derived terms6 of English verb entries.
</listItem>
<footnote confidence="0.982983833333333">
4http://en.wiktionary.org/wiki/Category:
English—phrasal—verbs
5http://en.wiktionary.org/wiki/Category:
English—idioms
6For example, see http://en.wiktionary.org/
wiki/take#Derived—terms
</footnote>
<bodyText confidence="0.504585">
about across after against along
among around at before behind
between beyond by down for
from in into like off
on onto outside over past
round through to towards under
up upon with within without
</bodyText>
<tableCaption confidence="0.9902535">
Table 2: Particles and prepositions allowed in phrasal
verbs gathered from Wiktionary.
</tableCaption>
<bodyText confidence="0.999961545454546">
Many of the idioms and derived terms are not
phrasal verbs (e.g. kick the bucket, make-or-break).
We filter out any phrases not of the form V P+, with
V a verb, and P+ denoting one or more occurrences
of particles and prepositions from the list in Table 2.
We omit prepositions that do not productively form
English phrasal verbs, such as amid and as. This
process also omits some compounds that are some-
times called phrasal verbs, such as light verb con-
structions, e.g. have a go (Butt, 2003), and noncom-
positional verb-adverb collocations, e.g. look for-
ward.
There are a number of extant phrasal verb cor-
pora. For example, McCarthy et al. (2003) present
graded human compositionality judgments for 116
phrasal verbs, and Baldwin (2008) presents a large
set of candidates produced by an automated system,
with false positives manually removed. We use Wik-
tionary instead, in an attempt to construct a maxi-
mally comprehensive data set that is free from any
possible biases introduced by automatic extraction
processes.
</bodyText>
<subsectionHeader confidence="0.998339">
4.2 Filtering and Data Partition
</subsectionHeader>
<bodyText confidence="0.9999845">
The merged list of phrasal verbs extracted from Wik-
tionary included some common collocations that
have compositional semantics (e.g. know about), as
well as some very rare constructions (e.g. cheese
down). We removed these spurious results system-
atically by filtering out very frequent and very infre-
quent entries. First, we calculated the log probability
of each phrase, according to a language model built
from a large monolingual corpus of news documents
and web documents, smoothed with stupid back-
off (Brants et al., 2007). We sorted all Wiktionary
phrasal verbs according to this value. Then, we se-
lected the contiguous 75% of the sorted phrases that
minimize the variance of this statistic. This method
</bodyText>
<page confidence="0.997087">
641
</page>
<table confidence="0.997039375">
Recall-at-1220
Dev Test
Frequent Candidates 17.0 19.3
WordNet 3.0 Frequent 41.6 43.7
WordNet 3.0 Filtered 49.4 48.8
Monolingual Only 30.1 30.2
Bilingual Only 47.1 43.9
Monolingual+Bilingual 50.8 47.9
</table>
<tableCaption confidence="0.95022325">
Table 3: Our boosted ranker combining monolingual
and bilingual features (bottom) compared to three base-
lines (top) gives comparable performance to the human-
curated upper bound.
</tableCaption>
<bodyText confidence="0.9998175">
removed a few very frequent phrases and a large
number of rare phrases. The remaining phrases were
split randomly into a development set of 694 items
and a held-out test set of 695 items.
</bodyText>
<subsectionHeader confidence="0.997273">
4.3 Corpora
</subsectionHeader>
<bodyText confidence="0.998454357142857">
Our monolingual English corpus consists of news ar-
ticles and documents collected from the web. Our
parallel corpora from English to each of 50 lan-
guages also consist of documents collected from
the web via distributed data mining of parallel doc-
uments based on the text content of web pages
(Uszkoreit et al., 2010).
The parallel corpora were segmented into aligned
sentence pairs and word-aligned using two iterations
of IBM Model 1 (Brown et al., 1993) and two iter-
ations of the HMM-based alignment model (Vogel
et al., 1996) with posterior symmetrization (Liang et
al., 2006). This training recipe is common in large-
scale machine translation systems.
</bodyText>
<subsectionHeader confidence="0.997942">
4.4 Generating Candidates
</subsectionHeader>
<bodyText confidence="0.999904777777778">
To generate the set of candidate phrasal verbs con-
sidered during evaluation, we exhaustively enumer-
ated the Cartesian product of all verbs present in the
previously described Wiktionary set (V), all parti-
cles in Table 2 (P) and a small set of second parti-
cles T = {with, to, on, c}, with c the empty string.
The set of candidate phrasal verbs we consider dur-
ing evaluation is the product V x P x T, which con-
tains 96,880 items.
</bodyText>
<subsectionHeader confidence="0.573365">
4.5 Results
</subsectionHeader>
<bodyText confidence="0.999797630434783">
We optimize a ranker using the boosting algorithm
described in section 3.3, using the features from Ta-
ble 1, optimizing performance on the Wiktionary de-
velopment set described in section 4.2. Monolingual
and bilingual statistics are calculated using the cor-
pora described in section 4.3, with candidate phrasal
verbs being drawn from the set described in section
4.4.
We evaluate our method of identifying phrasal
verbs by computing recall-at-N. This statistic is the
fraction of the Wiktionary test set that appears in the
top N proposed phrasal verbs by the method, where
N is an arbitrary number of top-ranked candidates
held constant when comparing different approaches
(we use N = 1220). We do not compute precision,
because the test set to which we compare is not an
exhaustive list of phrasal verbs, due to the develop-
ment/test split, frequency filtering, and omissions in
the original lexical resource. Proposing a phrasal
verb not in the test set is not necessarily an error, but
identifying many phrasal verbs from the test set is an
indication of an effective method. Recall-at-N is a
natural way to evaluate a ranking system where the
gold-standard data is an incomplete, unranked set.
Table 3 compares our approach to three baselines
using the Recall-at-1220 metric evaluated on both
the development and test sets. As a lower bound, we
evaluated the 1220 most frequent candidates in our
Monolingual corpus (Frequent Candidates).
As a competitive baseline, we evaluated the set of
phrasal verbs in WordNet 3.0 (Fellbaum, 1998). We
selected the most frequent 1220 out of 1781 verb-
particle constructions in WordNet (WordNet 3.0 Fre-
quent). A stronger baseline resulted from apply-
ing the same filtering procedure to WordNet that
we did to Wiktionary: sorting all verb-particle en-
tries by their language model score and retaining the
1220 consecutive entries that minimized language
model variance (WordNet 3.0 Filtered). WordNet
is a human-curated resource, and yet its recall-at-N
compared to our Wiktionary test set is only 48.8%,
indicating substantial divergence between the two
resources. Such divergence is typical: lexical re-
sources often disagree about what multiword expres-
sions to include (Lin, 1999).
The three final lines in Table 3 evaluate our
</bodyText>
<figure confidence="0.8626392">
Baseline
Boosted
642
0 5 10 15 20 25 30 35 40 45 50
Number of languages (k)
</figure>
<figureCaption confidence="0.9985944">
Figure 2: The solid line shows recall-at-1220 when com-
bining the k best-performing bilingual statistics and three
monolingual statistics. The dotted line shows the indi-
vidual performance of the kth best-performing bilingual
statistic, when applied in isolation to rank candidates.
</figureCaption>
<bodyText confidence="0.999783272727273">
boosted ranker. Automatically detecting phrasal
verbs using monolingual features alone strongly out-
performed the frequency-based lower bound, but un-
derperformed the WordNet baseline. Bilingual fea-
tures, using features from 50 languages, proved sub-
stantially more effective. The combination of both
types of features yielded the best performance, out-
performing the human-curated WordNet baseline on
the development set (on which our ranker was opti-
mized) and approaching its performance on the held-
out test set.
</bodyText>
<subsectionHeader confidence="0.999238">
4.6 Feature Analysis
</subsectionHeader>
<bodyText confidence="0.999854384615385">
The solid line in Figure 2 shows the recall-at-1220
for a boosted ranker using all monolingual statistics
and k bilingual statistics, for increasing k. Bilin-
gual statistics are added according to their individual
recall, from best-performing to worst. That is, the
point at k = 0 uses only µ1, µ2, and µ3, the point at
k = 1 adds the best individually-performing bilin-
gual statistic (Spanish) as a weak ranker, the next
point adds the second-best bilingual statistic (Ger-
man), etc. Boosting maximizes performance on the
development set, and evaluation is performed on the
test set. We use T = 53 (equal to the total number
of weak rankers).
</bodyText>
<table confidence="0.9966998">
Recall-at-1220
Dev Test
Bilingual only 47.1 43.9
Bilingual+µ1 48.1 46.9
Bilingual+µ2 50.1 48.3
Bilingual+µ3 48.4 46.3
Bilingual+µ1 + µ2 50.2 47.9
Bilingual+µ1 + µ3 49.0 47.4
Bilingual+µ2 + µ3 50.4 49.4
Bilingual+µ1 + µ2 + µ3 50.8 47.9
</table>
<tableCaption confidence="0.990733">
Table 4: An ablation of monolingual statistics shows that
they are useful in addition to the 50 bilingual statistics
combined, and no single statistic provides maximal per-
formance.
</tableCaption>
<bodyText confidence="0.999923583333333">
The dotted line in Figure 2 shows that individual
bilingual statistics have recall-at-1220 ranging from
34.4% to 5.0%. This difference reflects the differ-
ent sizes of parallel corpora and usefulness of dif-
ferent languages in identifying English semantic id-
iosyncrasy. Combining together the signal of mul-
tiple languages is clearly beneficial, and including
many low-performing languages still offers overall
improvements.
Table 4 shows the effect of adding different sub-
sets of the monolingual statistics to the set of all
50 bilingual statistics. Monolingual statistics give
a performance improvement of up to 5.5% recall
on the test set, but the comparative behavior of the
various combinations of the µz is somewhat unpre-
dictable when training on the development set and
evaluating on the test set. The pointwise mutual in-
formation of a verb and its particles (µ2) appears to
be the most useful feature. In fact, the test set per-
formance of using µ2 alone outperforms the combi-
nation of all three. The best combination even out-
performs the WordNet 3.0 baseline on the test set,
though optimizing on the development set would not
select this model.
</bodyText>
<subsectionHeader confidence="0.978031">
4.7 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999895">
Table 5 shows the 100 highest ranked phrasal verb
candidates by our system that do not appear in either
the development or test sets. Most of these candi-
dates are in fact English phrasal verbs that happened
to be missing from Wiktionary; some are present
in Wiktionary but were removed from the reference
</bodyText>
<figure confidence="0.995576333333334">
50%
40%
Combined with AdaBoost
Individual Bilingual Statistics
Test set recall-at-1220
30%
20%
10%
0%
</figure>
<page confidence="0.996807">
643
</page>
<tableCaption confidence="0.987160315789474">
pick up pat on tap into fit for charge with suit against
catch up burst into muck up haul up give up get off
get through get up get in tack on buzz about do like
plump for haul in keep up with strap on catch up with suck into
get round chop off slap on pitch into get into inquire into
drop behind get on catch up on pass on cue from carry around
get around get over shoot at pick over shoot by shoot in
make up to get past cast down set up with rule off hand round
piss on hit by break down move for lead off pluck off
flip through edge over strike off plug into keep up go past
set off pull round see about stay on put up sidle up to
buzz around take off set up slap in head towards shoot past
inquire for tuck up lie with well before go on with reel from
drive along snap off barge into whip on put down instance through
bar from cut down on let in tune in to move off suit in
lean against well beyond get down to go across sail into lie over
hit with chow down on look after catch at
Table 5: The highest ranked phrasal verb candidates from our full system that do not appear in either Wiktionary set.
Candidates are presented in decreasing rank; “pat on” is the second highest ranked candidate.
</tableCaption>
<bodyText confidence="0.999912">
sets during filtering, and the remainder are in fact
not phrasal verbs (true precision errors).
These errors fall largely into two categories.
Some candidates are compositional, but contain pol-
ysemous verbs, such as hit by, drive along, and head
towards. In these cases, prepositions disambiguate
the verb, which naturally affects translation distri-
butions. Other candidates are not phrasal verbs, but
instead phrases that tend to have a different syntac-
tic role, such as suit against, instance through, fit
for, and lie over (conjugated as lay over). A care-
ful treatment of part-of-speech tags when computing
corpus statistics might address this issue.
</bodyText>
<sectionHeader confidence="0.999962" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999938804878049">
The idea of using word-aligned parallel corpora
to identify idiomatic expressions has been pur-
sued in a number of different ways. Melamed
(1997) tests candidate MWEs by collapsing them
into single tokens, training a new translation model
with these tokens, and using the performance of
the new model to judge candidates’ noncomposi-
tionality. Villada Moir´on and Tiedemann (2006)
use word-aligned parallel corpora to identify Dutch
MWEs, testing the assumption that the distributions
of alignments of MWEs will generally have higher
entropies than those of fully compositional com-
pounds. Caseli et al. (2010) generate candidate mul-
tiword expressions by picking out sufficiently com-
mon phrases that align to single target-side tokens.
Tsvetkov and Wintner (2012) generate candidate
MWEs by finding one-to-one alignments in paral-
lel corpora which are not in a bilingual dictionary,
and ranking them based on monolingual statistics.
The system of Salehi and Cook (2013) is perhaps
the closest to the current work, judging noncompo-
sitionality using string edit distance between a can-
didate phrase’s automatic translation and its com-
ponents’ individual translations. Unlike the current
work, their method does not use distributions over
translations or combine individual bilingual values
with boosting; however, they find, as we do, that in-
corporating many languages is beneficial to MWE
identification.
A large body of work has investigated the identifi-
cation of noncompositional compounds from mono-
lingual sources (Lin, 1999; Schone and Jurafsky,
2001; Fazly and Stevenson, 2006; McCarthy et
al., 2003; Baldwin et al., 2003; Villavicencio,
2003). Many of these monolingual statistics could
be viewed as weak rankers and fruitfully incorpo-
rated into our framework.
There has also been a substantial amount of work
addressing the problem of differentiating between
literal and idiomatic instances of phrases in con-
text (Katz and Giesbrecht, 2006; Li et al., 2010;
</bodyText>
<page confidence="0.996774">
644
</page>
<bodyText confidence="0.9966376">
Sporleder and Li, 2009; Birke and Sarkar, 2006;
Diab and Bhutada, 2009). We do not attempt this
task; however, techniques for token identification
could be used to improve type identification (Bald-
win, 2005).
</bodyText>
<sectionHeader confidence="0.998208" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999979066666667">
We have presented the polyglot ranking approach
to phrasal verb identification, using parallel corpora
from many languages to identify phrasal verbs. We
proposed an evaluation metric that acknowledges the
inherent incompleteness of reference sets, but dis-
tinguishes among competing systems in a manner
aligned to the goals of the task. We developed a
recall-oriented learning method that integrates mul-
tiple weak ranking signals, and demonstrated exper-
imentally that combining statistical evidence from a
large number of bilingual corpora, as well as from
monolingual corpora, produces the most effective
system overall. We look forward to generalizing
our approach to other types of noncompositional
phrases.
</bodyText>
<sectionHeader confidence="0.99724" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99973">
Special thanks to Ivan Sag, who argued for the
importance of handling multi-word expressions in
natural language processing applications, and who
taught the authors about natural language syntax
once upon a time. We would also like to thank the
anonymous reviewers for their helpful suggestions.
</bodyText>
<sectionHeader confidence="0.995938" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99663452173913">
Otavio Acosta, Aline Villavicencio, and Viviane Moreira.
2011. Identification and treatment of multiword ex-
pressions applied to information retrieval. In Proceed-
ings of the ACL Workshop on Multiword Expressions.
Timothy Baldwin and Aline Villavicencio. 2002. Ex-
tracting the unextractable: A case study on verb-
particles. In Proceedings of the Sixth Conference on
Natural Language Learning.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL Workshop on Multiword Expressions.
Timothy Baldwin. 2005. Deep lexical acquisition of
verb-particle constructions. Computer Speech &amp; Lan-
guage, Special Issue on Multiword Expressions.
Timothy Baldwin. 2008. A resource for evaluating the
deep lexical acquisition of english verb-particle con-
structions. In Proceedings of the LREC Workshop To-
wards a Shared Task for Multiword Expressions.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL Workshop on
Multiword Expressions.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for the nearly unsupervised recognition of non-
literal language. In Proceedings of European Chapter
of the Association for Computational Linguistics.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics.
Miriam Butt. 2003. The light verb jungle. In Proceed-
ings of the Workshop on Multi-Verb Constructions.
Helena de Medeiros Caseli, Carlos Ramisch, Maria das
Grac¸as Volpe Nunes, and Aline Villavicencio. 2010.
Alignment-based extraction of multiword expressions.
Language Resources and Evaluation.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1).
Kenneth Church. 2011. How many multiword expres-
sions do people know? In Proceedings of the ACL
Workshop on Multiword Expressions.
Mona T. Diab and Pravin Bhutada. 2009. Verb noun
construction MWE token supervised classification. In
Proceedings of the ACL Workshop on Multiword Ex-
pressions.
Robert Dixon. 1982. The grammar of english phrasal
verbs. Australian Journal of Linguistics.
Afsaneh Fazly and Suzanne Stevenson. 2006. Automat-
ically constructing a lexicon of verb phrase idiomatic
combinations. In Proceedings of the European Chap-
ter of the Association for Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Mark Finlayson and Nidhi Kulkarni. 2011. Detecting
multiword expressions improves word sense disam-
biguation. In Proceedings of the ACL Workshop on
Multiword Expressions.
Yoav Freund and Robert E. Schapire. 1995. A decision-
theoretic generalization of on-line learning and an ap-
plication to boosting. In Proceedings of the Confer-
ence on Computational Learning Theory.
</reference>
<page confidence="0.985899">
645
</page>
<reference confidence="0.99959964893617">
Yoav Freund, Raj Iyer, Robert E Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm for
combining preferences. The Journal of Machine
Learning Research.
Ray Jackendoff. 1993. The Architecture of the Language
Faculty. MIT Press.
Graham Katz and Eugenie Giesbrecht. 2006. Auto-
matic identification of non-compositional multi-word
expressions using latent semantic analysis. In Pro-
ceedings of the ACL Workshop on Multiword Expres-
sions.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the North American Chapter of the Association
for Computational Linguistics.
Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010.
Topic models for word sense disambiguation and
token-based idiom detection. In Proceedings of the
Association for Computational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the Asso-
ciation for Computational Linguistics.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasal
verbs. In Proceedings of the ACL Workshop on Multi-
word Expressions.
I. Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Santanu Pal, Sudip Kumar Naskar, Pavel Pecina, Sivaji
Bandyopadhyay, and Andy Way. 2010. Handling
named entities and compound verbs in phrase-based
statistical machine translation. In Proceedings of the
COLING 2010 Workshop on Multiword Expressions.
Zhixiang Ren, Yajuan Lu, Jie Cao, Qun Liu, and Yun
Huang. 2009. Improving statistical machine transla-
tion using domain bilingual multiword expressions. In
Proceedings of the ACL Workshop on Multiword Ex-
pressions.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In Proceedings
of the CICLING Conference on Intelligent Text Pro-
cessing and Computational Linguistics.
Bahar Salehi and Paul Cook. 2013. Predicting the com-
positionality of multiword expressions using transla-
tions in multiple languages. In Second Joint Confer-
ence on Lexical and Computational Semantics.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictionary
headwords a solved problem? In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Caroline Sporleder and Linlin Li. 2009. Unsupervised
recognition of literal and non-literal use of idiomatic
expressions. In Proceedings of the European Chapter
of the Association for Computational Linguistics.
Yulia Tsvetkov and Shuly Wintner. 2012. Extraction
of multi-word expressions from small parallel corpora.
In Natural Language Engineering.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Du-
biner. 2010. Large scale parallel document mining for
machine translation. In Proceedings of the Conference
on Computational Linguistics.
Sriram Venkatapathy and Aravind K Joshi. 2006. Us-
ing information about multi-word expressions for the
word-alignment task. In Proceedings of the ACL
Workshop on Multiword Expressions.
Bego˜na Villada Moir´on and J¨org Tiedemann. 2006.
Identifying idiomatic expressions using automatic
word-alignment. In Proceedings of the EACL Work-
shop on Multiword Expressions in a Multilingual Con-
text.
Aline Villavicencio. 2003. Verb-particle constructions
and lexical resources. In Proceedings of the ACL
workshop on Multiword expressions.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the Conference on Computa-
tional linguistics.
Jun Xu and Hang Li. 2007. AdaRank: a boosting al-
gorithm for information retrieval. In Proceedings of
the SIGIR Conference on Research and Development
in Information Retrieval.
Ying Xu, Randy Goebel, Christoph Ringlstetter, and
Grzegorz Kondrak. 2010. Application of the tightness
continuum measure to chinese information retrieval.
In Proceedings of the COLING Workshop on Multi-
word Expressions.
</reference>
<page confidence="0.998849">
646
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.733204">
<title confidence="0.990397">Identifying Phrasal Verbs Using Many Bilingual Corpora</title>
<author confidence="0.859931">DeNero</author>
<affiliation confidence="0.948887">Department of Computer Science Google, Inc. of Texas at Austin</affiliation>
<email confidence="0.999193">pichotta@cs.utexas.edu</email>
<abstract confidence="0.996006692307692">We address the problem of identifying multiword expressions in a language, focuson English phrasal verbs. Our polyranking integrates frequency statistics from translated corpora in 50 different languages. Our experimental evaluation demonstrates that combining statistical evidence from many parallel corpora using a novel ranking-oriented boosting algorithm produces a comprehensive set of English phrasal verbs, achieving performance comparable to a human-curated set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Otavio Acosta</author>
<author>Aline Villavicencio</author>
<author>Viviane Moreira</author>
</authors>
<title>Identification and treatment of multiword expressions applied to information retrieval.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL Workshop on Multiword Expressions.</booktitle>
<contexts>
<context position="1405" citStr="Acosta et al., 2011" startWordPosition="214" endWordPosition="217">f words whose meaning cannot be composed directly from the meanings of its constituent words. These idiosyncratic phrases are prevalent in the lexicon of a language; Jackendoff (1993) estimates that their number is on the same order of magnitude as that of single words, and Sag et al. (2002) suggest that they are much more common, though quantifying them is challenging (Church, 2011). The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002). Awareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. &amp;quot;Research conducted during an internship at Google. We focus on a particular subset of MWEs, English phrasal verbs. A phrasal verb consists of a head verb followed by one or more particles, such that the meaning of the phrase cannot be determined by combining th</context>
</contexts>
<marker>Acosta, Villavicencio, Moreira, 2011</marker>
<rawString>Otavio Acosta, Aline Villavicencio, and Viviane Moreira. 2011. Identification and treatment of multiword expressions applied to information retrieval. In Proceedings of the ACL Workshop on Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Aline Villavicencio</author>
</authors>
<title>Extracting the unextractable: A case study on verbparticles.</title>
<date>2002</date>
<booktitle>In Proceedings of the Sixth Conference on Natural Language Learning.</booktitle>
<contexts>
<context position="2081" citStr="Baldwin and Villavicencio, 2002" startWordPosition="322" endWordPosition="325">wareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. &amp;quot;Research conducted during an internship at Google. We focus on a particular subset of MWEs, English phrasal verbs. A phrasal verb consists of a head verb followed by one or more particles, such that the meaning of the phrase cannot be determined by combining the simplex meanings of its constituent words (Baldwin and Villavicencio, 2002; Dixon, 1982; Bannard et al., 2003).1 Examples of phrasal verbs include count on [rely], look after [tend], or take off [remove], the meanings of which do not involve counting, looking, or taking. In contrast, there are verbs followed by particles that are not phrasal verbs, because their meaning is compositional, such as walk towards, sit behind, or paint on. We identify phrasal verbs by using frequency statistics calculated from parallel corpora, consisting of bilingual pairs of documents such that one is a translation of the other, with one document in English. We leverage the observation </context>
</contexts>
<marker>Baldwin, Villavicencio, 2002</marker>
<rawString>Timothy Baldwin and Aline Villavicencio. 2002. Extracting the unextractable: A case study on verbparticles. In Proceedings of the Sixth Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Colin Bannard</author>
<author>Takaaki Tanaka</author>
<author>Dominic Widdows</author>
</authors>
<title>An empirical model of multiword expression decomposability.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL Workshop on Multiword Expressions.</booktitle>
<contexts>
<context position="33503" citStr="Baldwin et al., 2003" startWordPosition="5679" endWordPosition="5682">ent work, judging noncompositionality using string edit distance between a candidate phrase’s automatic translation and its components’ individual translations. Unlike the current work, their method does not use distributions over translations or combine individual bilingual values with boosting; however, they find, as we do, that incorporating many languages is beneficial to MWE identification. A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; 644 Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identification (Baldwin, 2005). 6 Conclusion We have presented the polyglot rank</context>
</contexts>
<marker>Baldwin, Bannard, Tanaka, Widdows, 2003</marker>
<rawString>Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and Dominic Widdows. 2003. An empirical model of multiword expression decomposability. In Proceedings of the ACL Workshop on Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
</authors>
<title>Deep lexical acquisition of verb-particle constructions.</title>
<date>2005</date>
<journal>Computer Speech &amp; Language, Special Issue on Multiword Expressions.</journal>
<contexts>
<context position="34053" citStr="Baldwin, 2005" startWordPosition="5766" endWordPosition="5768">and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; 644 Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identification (Baldwin, 2005). 6 Conclusion We have presented the polyglot ranking approach to phrasal verb identification, using parallel corpora from many languages to identify phrasal verbs. We proposed an evaluation metric that acknowledges the inherent incompleteness of reference sets, but distinguishes among competing systems in a manner aligned to the goals of the task. We developed a recall-oriented learning method that integrates multiple weak ranking signals, and demonstrated experimentally that combining statistical evidence from a large number of bilingual corpora, as well as from monolingual corpora, produces</context>
</contexts>
<marker>Baldwin, 2005</marker>
<rawString>Timothy Baldwin. 2005. Deep lexical acquisition of verb-particle constructions. Computer Speech &amp; Language, Special Issue on Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
</authors>
<title>A resource for evaluating the deep lexical acquisition of english verb-particle constructions.</title>
<date>2008</date>
<booktitle>In Proceedings of the LREC Workshop Towards</booktitle>
<contexts>
<context position="21495" citStr="Baldwin (2008)" startWordPosition="3714" endWordPosition="3715"> not of the form V P+, with V a verb, and P+ denoting one or more occurrences of particles and prepositions from the list in Table 2. We omit prepositions that do not productively form English phrasal verbs, such as amid and as. This process also omits some compounds that are sometimes called phrasal verbs, such as light verb constructions, e.g. have a go (Butt, 2003), and noncompositional verb-adverb collocations, e.g. look forward. There are a number of extant phrasal verb corpora. For example, McCarthy et al. (2003) present graded human compositionality judgments for 116 phrasal verbs, and Baldwin (2008) presents a large set of candidates produced by an automated system, with false positives manually removed. We use Wiktionary instead, in an attempt to construct a maximally comprehensive data set that is free from any possible biases introduced by automatic extraction processes. 4.2 Filtering and Data Partition The merged list of phrasal verbs extracted from Wiktionary included some common collocations that have compositional semantics (e.g. know about), as well as some very rare constructions (e.g. cheese down). We removed these spurious results systematically by filtering out very frequent </context>
</contexts>
<marker>Baldwin, 2008</marker>
<rawString>Timothy Baldwin. 2008. A resource for evaluating the deep lexical acquisition of english verb-particle constructions. In Proceedings of the LREC Workshop Towards a Shared Task for Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Timothy Baldwin</author>
<author>Alex Lascarides</author>
</authors>
<title>A statistical approach to the semantics of verbparticles.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL Workshop on Multiword Expressions.</booktitle>
<contexts>
<context position="2117" citStr="Bannard et al., 2003" startWordPosition="328" endWordPosition="331"> in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. &amp;quot;Research conducted during an internship at Google. We focus on a particular subset of MWEs, English phrasal verbs. A phrasal verb consists of a head verb followed by one or more particles, such that the meaning of the phrase cannot be determined by combining the simplex meanings of its constituent words (Baldwin and Villavicencio, 2002; Dixon, 1982; Bannard et al., 2003).1 Examples of phrasal verbs include count on [rely], look after [tend], or take off [remove], the meanings of which do not involve counting, looking, or taking. In contrast, there are verbs followed by particles that are not phrasal verbs, because their meaning is compositional, such as walk towards, sit behind, or paint on. We identify phrasal verbs by using frequency statistics calculated from parallel corpora, consisting of bilingual pairs of documents such that one is a translation of the other, with one document in English. We leverage the observation that a verb will translate in an aty</context>
</contexts>
<marker>Bannard, Baldwin, Lascarides, 2003</marker>
<rawString>Colin Bannard, Timothy Baldwin, and Alex Lascarides. 2003. A statistical approach to the semantics of verbparticles. In Proceedings of the ACL Workshop on Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Birke</author>
<author>Anoop Sarkar</author>
</authors>
<title>A clustering approach for the nearly unsupervised recognition of nonliteral language.</title>
<date>2006</date>
<booktitle>In Proceedings of European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="33892" citStr="Birke and Sarkar, 2006" startWordPosition="5740" endWordPosition="5743">tification. A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; 644 Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identification (Baldwin, 2005). 6 Conclusion We have presented the polyglot ranking approach to phrasal verb identification, using parallel corpora from many languages to identify phrasal verbs. We proposed an evaluation metric that acknowledges the inherent incompleteness of reference sets, but distinguishes among competing systems in a manner aligned to the goals of the task. We developed a recall-oriented learning method that integrates multiple weak ranking sig</context>
</contexts>
<marker>Birke, Sarkar, 2006</marker>
<rawString>Julia Birke and Anoop Sarkar. 2006. A clustering approach for the nearly unsupervised recognition of nonliteral language. In Proceedings of European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="22336" citStr="Brants et al., 2007" startWordPosition="3844" endWordPosition="3847">ssible biases introduced by automatic extraction processes. 4.2 Filtering and Data Partition The merged list of phrasal verbs extracted from Wiktionary included some common collocations that have compositional semantics (e.g. know about), as well as some very rare constructions (e.g. cheese down). We removed these spurious results systematically by filtering out very frequent and very infrequent entries. First, we calculated the log probability of each phrase, according to a language model built from a large monolingual corpus of news documents and web documents, smoothed with stupid backoff (Brants et al., 2007). We sorted all Wiktionary phrasal verbs according to this value. Then, we selected the contiguous 75% of the sorted phrases that minimize the variance of this statistic. This method 641 Recall-at-1220 Dev Test Frequent Candidates 17.0 19.3 WordNet 3.0 Frequent 41.6 43.7 WordNet 3.0 Filtered 49.4 48.8 Monolingual Only 30.1 30.2 Bilingual Only 47.1 43.9 Monolingual+Bilingual 50.8 47.9 Table 3: Our boosted ranker combining monolingual and bilingual features (bottom) compared to three baselines (top) gives comparable performance to the humancurated upper bound. removed a few very frequent phrases</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="23555" citStr="Brown et al., 1993" startWordPosition="4041" endWordPosition="4044"> and a large number of rare phrases. The remaining phrases were split randomly into a development set of 694 items and a held-out test set of 695 items. 4.3 Corpora Our monolingual English corpus consists of news articles and documents collected from the web. Our parallel corpora from English to each of 50 languages also consist of documents collected from the web via distributed data mining of parallel documents based on the text content of web pages (Uszkoreit et al., 2010). The parallel corpora were segmented into aligned sentence pairs and word-aligned using two iterations of IBM Model 1 (Brown et al., 1993) and two iterations of the HMM-based alignment model (Vogel et al., 1996) with posterior symmetrization (Liang et al., 2006). This training recipe is common in largescale machine translation systems. 4.4 Generating Candidates To generate the set of candidate phrasal verbs considered during evaluation, we exhaustively enumerated the Cartesian product of all verbs present in the previously described Wiktionary set (V), all particles in Table 2 (P) and a small set of second particles T = {with, to, on, c}, with c the empty string. The set of candidate phrasal verbs we consider during evaluation i</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miriam Butt</author>
</authors>
<title>The light verb jungle.</title>
<date>2003</date>
<booktitle>In Proceedings of the Workshop on Multi-Verb Constructions.</booktitle>
<contexts>
<context position="21251" citStr="Butt, 2003" startWordPosition="3677" endWordPosition="3678">er up upon with within without Table 2: Particles and prepositions allowed in phrasal verbs gathered from Wiktionary. Many of the idioms and derived terms are not phrasal verbs (e.g. kick the bucket, make-or-break). We filter out any phrases not of the form V P+, with V a verb, and P+ denoting one or more occurrences of particles and prepositions from the list in Table 2. We omit prepositions that do not productively form English phrasal verbs, such as amid and as. This process also omits some compounds that are sometimes called phrasal verbs, such as light verb constructions, e.g. have a go (Butt, 2003), and noncompositional verb-adverb collocations, e.g. look forward. There are a number of extant phrasal verb corpora. For example, McCarthy et al. (2003) present graded human compositionality judgments for 116 phrasal verbs, and Baldwin (2008) presents a large set of candidates produced by an automated system, with false positives manually removed. We use Wiktionary instead, in an attempt to construct a maximally comprehensive data set that is free from any possible biases introduced by automatic extraction processes. 4.2 Filtering and Data Partition The merged list of phrasal verbs extracted</context>
</contexts>
<marker>Butt, 2003</marker>
<rawString>Miriam Butt. 2003. The light verb jungle. In Proceedings of the Workshop on Multi-Verb Constructions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helena de Medeiros Caseli</author>
</authors>
<title>Carlos Ramisch, Maria das Grac¸as Volpe Nunes, and Aline Villavicencio.</title>
<date>2010</date>
<marker>Caseli, 2010</marker>
<rawString>Helena de Medeiros Caseli, Carlos Ramisch, Maria das Grac¸as Volpe Nunes, and Aline Villavicencio. 2010. Alignment-based extraction of multiword expressions. Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="13124" citStr="Church and Hanks, 1990" startWordPosition="2199" endWordPosition="2202">he set of inflections of that phrasal verb (for look up, this will be [look|looks|looked|looking] up). We extract the variants in E from the verb entries in English Wiktionary. The final score computed from a phrase-aligned parallel corpus translating English sentences into a language L is the average KL divergence of smoothed constituent translation distributions for any inflected form ei ∈ E: � � DKL P 0 v(ei)kP 0 ei 3.2 Monolingual Statistics We also collect a number of monolingual statistics for each phrasal verb candidate, motivated by the considerable body of previous work on the topic (Church and Hanks, 1990; Lin, 1999; McCarthy et al., 2003). The monolingual statistics are designed to identify frequent collocations in a language. This set of monolingual features is not comprehensive, as we focus our attention primarily on bilingual features in this paper. As above, define E toxbe thefset of morphologically inflectedvariants of a candidate e, and let \hline \\ [-1ex] V be the set of inflected variants of the head verb $P_{(e)})&amp;. $ \hline \\ 1ex] v(e) of e. We define three statistics calculated from $Pe(x)$ &amp;$\frac{3}{4}=0.75 $ &amp;$\fra the phrase counts of a monolingual English corpus. \hline \\ [</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>How many multiword expressions do people know?</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL Workshop on Multiword Expressions.</booktitle>
<contexts>
<context position="1171" citStr="Church, 2011" startWordPosition="175" endWordPosition="176">ented boosting algorithm produces a comprehensive set of English phrasal verbs, achieving performance comparable to a human-curated set. 1 Introduction A multiword expression (MWE), or noncompositional compound, is a sequence of words whose meaning cannot be composed directly from the meanings of its constituent words. These idiosyncratic phrases are prevalent in the lexicon of a language; Jackendoff (1993) estimates that their number is on the same order of magnitude as that of single words, and Sag et al. (2002) suggest that they are much more common, though quantifying them is challenging (Church, 2011). The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002). Awareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. &amp;quot;Research conducted during a</context>
</contexts>
<marker>Church, 2011</marker>
<rawString>Kenneth Church. 2011. How many multiword expressions do people know? In Proceedings of the ACL Workshop on Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona T Diab</author>
<author>Pravin Bhutada</author>
</authors>
<title>Verb noun construction MWE token supervised classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL Workshop on Multiword Expressions.</booktitle>
<contexts>
<context position="33917" citStr="Diab and Bhutada, 2009" startWordPosition="5744" endWordPosition="5747"> of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; 644 Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identification (Baldwin, 2005). 6 Conclusion We have presented the polyglot ranking approach to phrasal verb identification, using parallel corpora from many languages to identify phrasal verbs. We proposed an evaluation metric that acknowledges the inherent incompleteness of reference sets, but distinguishes among competing systems in a manner aligned to the goals of the task. We developed a recall-oriented learning method that integrates multiple weak ranking signals, and demonstrated ex</context>
</contexts>
<marker>Diab, Bhutada, 2009</marker>
<rawString>Mona T. Diab and Pravin Bhutada. 2009. Verb noun construction MWE token supervised classification. In Proceedings of the ACL Workshop on Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dixon</author>
</authors>
<title>The grammar of english phrasal verbs.</title>
<date>1982</date>
<journal>Australian Journal of Linguistics.</journal>
<contexts>
<context position="2094" citStr="Dixon, 1982" startWordPosition="326" endWordPosition="327">proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. &amp;quot;Research conducted during an internship at Google. We focus on a particular subset of MWEs, English phrasal verbs. A phrasal verb consists of a head verb followed by one or more particles, such that the meaning of the phrase cannot be determined by combining the simplex meanings of its constituent words (Baldwin and Villavicencio, 2002; Dixon, 1982; Bannard et al., 2003).1 Examples of phrasal verbs include count on [rely], look after [tend], or take off [remove], the meanings of which do not involve counting, looking, or taking. In contrast, there are verbs followed by particles that are not phrasal verbs, because their meaning is compositional, such as walk towards, sit behind, or paint on. We identify phrasal verbs by using frequency statistics calculated from parallel corpora, consisting of bilingual pairs of documents such that one is a translation of the other, with one document in English. We leverage the observation that a verb w</context>
</contexts>
<marker>Dixon, 1982</marker>
<rawString>Robert Dixon. 1982. The grammar of english phrasal verbs. Australian Journal of Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Afsaneh Fazly</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Automatically constructing a lexicon of verb phrase idiomatic combinations.</title>
<date>2006</date>
<booktitle>In Proceedings of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="33458" citStr="Fazly and Stevenson, 2006" startWordPosition="5671" endWordPosition="5674">and Cook (2013) is perhaps the closest to the current work, judging noncompositionality using string edit distance between a candidate phrase’s automatic translation and its components’ individual translations. Unlike the current work, their method does not use distributions over translations or combine individual bilingual values with boosting; however, they find, as we do, that incorporating many languages is beneficial to MWE identification. A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; 644 Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identification (Baldwin, 2005). 6 C</context>
</contexts>
<marker>Fazly, Stevenson, 2006</marker>
<rawString>Afsaneh Fazly and Suzanne Stevenson. 2006. Automatically constructing a lexicon of verb phrase idiomatic combinations. In Proceedings of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="25759" citStr="Fellbaum, 1998" startWordPosition="4408" endWordPosition="4409">phrasal verb not in the test set is not necessarily an error, but identifying many phrasal verbs from the test set is an indication of an effective method. Recall-at-N is a natural way to evaluate a ranking system where the gold-standard data is an incomplete, unranked set. Table 3 compares our approach to three baselines using the Recall-at-1220 metric evaluated on both the development and test sets. As a lower bound, we evaluated the 1220 most frequent candidates in our Monolingual corpus (Frequent Candidates). As a competitive baseline, we evaluated the set of phrasal verbs in WordNet 3.0 (Fellbaum, 1998). We selected the most frequent 1220 out of 1781 verbparticle constructions in WordNet (WordNet 3.0 Frequent). A stronger baseline resulted from applying the same filtering procedure to WordNet that we did to Wiktionary: sorting all verb-particle entries by their language model score and retaining the 1220 consecutive entries that minimized language model variance (WordNet 3.0 Filtered). WordNet is a human-curated resource, and yet its recall-at-N compared to our Wiktionary test set is only 48.8%, indicating substantial divergence between the two resources. Such divergence is typical: lexical </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Finlayson</author>
<author>Nidhi Kulkarni</author>
</authors>
<title>Detecting multiword expressions improves word sense disambiguation.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL Workshop on Multiword Expressions.</booktitle>
<contexts>
<context position="1550" citStr="Finlayson and Kulkarni (2011)" startWordPosition="238" endWordPosition="241">in the lexicon of a language; Jackendoff (1993) estimates that their number is on the same order of magnitude as that of single words, and Sag et al. (2002) suggest that they are much more common, though quantifying them is challenging (Church, 2011). The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002). Awareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. &amp;quot;Research conducted during an internship at Google. We focus on a particular subset of MWEs, English phrasal verbs. A phrasal verb consists of a head verb followed by one or more particles, such that the meaning of the phrase cannot be determined by combining the simplex meanings of its constituent words (Baldwin and Villavicencio, 2002; Dixon, 1982; Bannard et al., 2003).1 Examples of phrasal verbs incl</context>
</contexts>
<marker>Finlayson, Kulkarni, 2011</marker>
<rawString>Mark Finlayson and Nidhi Kulkarni. 2011. Detecting multiword expressions improves word sense disambiguation. In Proceedings of the ACL Workshop on Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>A decisiontheoretic generalization of on-line learning and an application to boosting.</title>
<date>1995</date>
<booktitle>In Proceedings of the Conference on Computational Learning Theory.</booktitle>
<contexts>
<context position="16440" citStr="Freund and Schapire, 1995" startWordPosition="2795" endWordPosition="2798">to semantic idiosyncrasy. For each language L for which we have a parallel corpus, we defined, in section 3.1, a function cOL(e) assigning real values to candidate phrasal verbs e, which we hypothesize is higher on average for more idiomatic compounds. Further, in section 3.2, we defined realvalued monolingual functions µ1, µ2, and µ3 for which we hypothesize the same trend holds. Because each score individually ranks all candidates, it is natural to view each cOL and µi as a weak ranking function that we can combine with a supervised boosting objective. We use a modified version of AdaBoost (Freund and Schapire, 1995) that optimizes for recall. For each cOL and µi, we compute a ranked list of candidate phrasal verbs, ordered from highest to lowest value. To simplify learning, we consider only the top 5000 candidate phrasal verbs according to µ1, µ2, and µ3. This pruning procedure excludes candidates that do not appear in our monolingual corpus. We optimize the ranker using an unranked, incomplete training set of phrasal verbs. We can evaluate the quality of the ranker by outputting the top N ranked candidates and measuring recall relative Algorithm 1 Recall-Oriented Ranking AdaBoost 1: for i = 1 : |X |do 2</context>
</contexts>
<marker>Freund, Schapire, 1995</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1995. A decisiontheoretic generalization of on-line learning and an application to boosting. In Proceedings of the Conference on Computational Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Raj Iyer</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>An efficient boosting algorithm for combining preferences.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research.</journal>
<contexts>
<context position="19753" citStr="Freund et al., 2003" startWordPosition="3445" endWordPosition="3448"> vector w sums to 1. After termination of Algorithm 1, we have weights α1, ... , αT and lists h1, ... , hT. Define ft as the function that generated the list ht (each ft will be some cOL or pi). Now, we define a final combined function co, taking a phrase e and returning a real number: T �0(e) _ αtft(e). t=1 We standardize the scores of individual weak rankers to have mean 0 and variance 1, so that their scores are comparable. The final learned ranker outputs a real value, instead of the class labels frequently found in AdaBoost. This follows previous work using boosting for learning to rank (Freund et al., 2003; Xu and Li, 2007). Our algorithm differs from previous methods because we are seeking to optimize for Recall-at-N, rather than a ranking loss. 4 Experimental Evaluation 4.1 Training and Test Set In order to train and evaluate our system, we construct a gold-standard list of phrasal verbs from the freely available English Wiktionary. We gather phrasal verbs from three sources within Wiktionary: 1. Entries labeled as English phrasal verbs4, 2. Entries labeled as English idioms5, and 3. The derived terms6 of English verb entries. 4http://en.wiktionary.org/wiki/Category: English—phrasal—verbs 5ht</context>
</contexts>
<marker>Freund, Iyer, Schapire, Singer, 2003</marker>
<rawString>Yoav Freund, Raj Iyer, Robert E Schapire, and Yoram Singer. 2003. An efficient boosting algorithm for combining preferences. The Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>The Architecture of the Language Faculty.</title>
<date>1993</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="968" citStr="Jackendoff (1993)" startWordPosition="138" endWordPosition="139">rates frequency statistics from translated corpora in 50 different languages. Our experimental evaluation demonstrates that combining statistical evidence from many parallel corpora using a novel ranking-oriented boosting algorithm produces a comprehensive set of English phrasal verbs, achieving performance comparable to a human-curated set. 1 Introduction A multiword expression (MWE), or noncompositional compound, is a sequence of words whose meaning cannot be composed directly from the meanings of its constituent words. These idiosyncratic phrases are prevalent in the lexicon of a language; Jackendoff (1993) estimates that their number is on the same order of magnitude as that of single words, and Sag et al. (2002) suggest that they are much more common, though quantifying them is challenging (Church, 2011). The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002). Awareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use</context>
</contexts>
<marker>Jackendoff, 1993</marker>
<rawString>Ray Jackendoff. 1993. The Architecture of the Language Faculty. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Katz</author>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Automatic identification of non-compositional multi-word expressions using latent semantic analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL Workshop on Multiword Expressions.</booktitle>
<contexts>
<context position="33823" citStr="Katz and Giesbrecht, 2006" startWordPosition="5727" endWordPosition="5730">d, as we do, that incorporating many languages is beneficial to MWE identification. A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; 644 Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identification (Baldwin, 2005). 6 Conclusion We have presented the polyglot ranking approach to phrasal verb identification, using parallel corpora from many languages to identify phrasal verbs. We proposed an evaluation metric that acknowledges the inherent incompleteness of reference sets, but distinguishes among competing systems in a manner aligned to the goals of the task. We developed a reca</context>
</contexts>
<marker>Katz, Giesbrecht, 2006</marker>
<rawString>Graham Katz and Eugenie Giesbrecht. 2006. Automatic identification of non-compositional multi-word expressions using latent semantic analysis. In Proceedings of the ACL Workshop on Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1306" citStr="Koehn et al., 2003" startWordPosition="195" endWordPosition="198">d set. 1 Introduction A multiword expression (MWE), or noncompositional compound, is a sequence of words whose meaning cannot be composed directly from the meanings of its constituent words. These idiosyncratic phrases are prevalent in the lexicon of a language; Jackendoff (1993) estimates that their number is on the same order of magnitude as that of single words, and Sag et al. (2002) suggest that they are much more common, though quantifying them is challenging (Church, 2011). The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002). Awareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. &amp;quot;Research conducted during an internship at Google. We focus on a particular subset of MWEs, English phrasal verbs. A phrasal verb consists of a head verb followed</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linlin Li</author>
<author>Benjamin Roth</author>
<author>Caroline Sporleder</author>
</authors>
<title>Topic models for word sense disambiguation and token-based idiom detection.</title>
<date>2010</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="33840" citStr="Li et al., 2010" startWordPosition="5731" endWordPosition="5734">ting many languages is beneficial to MWE identification. A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; 644 Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identification (Baldwin, 2005). 6 Conclusion We have presented the polyglot ranking approach to phrasal verb identification, using parallel corpora from many languages to identify phrasal verbs. We proposed an evaluation metric that acknowledges the inherent incompleteness of reference sets, but distinguishes among competing systems in a manner aligned to the goals of the task. We developed a recall-oriented learn</context>
</contexts>
<marker>Li, Roth, Sporleder, 2010</marker>
<rawString>Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010. Topic models for word sense disambiguation and token-based idiom detection. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="23679" citStr="Liang et al., 2006" startWordPosition="4061" endWordPosition="4064">d-out test set of 695 items. 4.3 Corpora Our monolingual English corpus consists of news articles and documents collected from the web. Our parallel corpora from English to each of 50 languages also consist of documents collected from the web via distributed data mining of parallel documents based on the text content of web pages (Uszkoreit et al., 2010). The parallel corpora were segmented into aligned sentence pairs and word-aligned using two iterations of IBM Model 1 (Brown et al., 1993) and two iterations of the HMM-based alignment model (Vogel et al., 1996) with posterior symmetrization (Liang et al., 2006). This training recipe is common in largescale machine translation systems. 4.4 Generating Candidates To generate the set of candidate phrasal verbs considered during evaluation, we exhaustively enumerated the Cartesian product of all verbs present in the previously described Wiktionary set (V), all particles in Table 2 (P) and a small set of second particles T = {with, to, on, c}, with c the empty string. The set of candidate phrasal verbs we consider during evaluation is the product V x P x T, which contains 96,880 items. 4.5 Results We optimize a ranker using the boosting algorithm describe</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic identification of noncompositional phrases.</title>
<date>1999</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="13135" citStr="Lin, 1999" startWordPosition="2203" endWordPosition="2204"> that phrasal verb (for look up, this will be [look|looks|looked|looking] up). We extract the variants in E from the verb entries in English Wiktionary. The final score computed from a phrase-aligned parallel corpus translating English sentences into a language L is the average KL divergence of smoothed constituent translation distributions for any inflected form ei ∈ E: � � DKL P 0 v(ei)kP 0 ei 3.2 Monolingual Statistics We also collect a number of monolingual statistics for each phrasal verb candidate, motivated by the considerable body of previous work on the topic (Church and Hanks, 1990; Lin, 1999; McCarthy et al., 2003). The monolingual statistics are designed to identify frequent collocations in a language. This set of monolingual features is not comprehensive, as we focus our attention primarily on bilingual features in this paper. As above, define E toxbe thefset of morphologically inflectedvariants of a candidate e, and let \hline \\ [-1ex] V be the set of inflected variants of the head verb $P_{(e)})&amp;. $ \hline \\ 1ex] v(e) of e. We define three statistics calculated from $Pe(x)$ &amp;$\frac{3}{4}=0.75 $ &amp;$\fra the phrase counts of a monolingual English corpus. \hline \\ [1ex] First,</context>
<context position="26439" citStr="Lin, 1999" startWordPosition="4511" endWordPosition="4512">tions in WordNet (WordNet 3.0 Frequent). A stronger baseline resulted from applying the same filtering procedure to WordNet that we did to Wiktionary: sorting all verb-particle entries by their language model score and retaining the 1220 consecutive entries that minimized language model variance (WordNet 3.0 Filtered). WordNet is a human-curated resource, and yet its recall-at-N compared to our Wiktionary test set is only 48.8%, indicating substantial divergence between the two resources. Such divergence is typical: lexical resources often disagree about what multiword expressions to include (Lin, 1999). The three final lines in Table 3 evaluate our Baseline Boosted 642 0 5 10 15 20 25 30 35 40 45 50 Number of languages (k) Figure 2: The solid line shows recall-at-1220 when combining the k best-performing bilingual statistics and three monolingual statistics. The dotted line shows the individual performance of the kth best-performing bilingual statistic, when applied in isolation to rank candidates. boosted ranker. Automatically detecting phrasal verbs using monolingual features alone strongly outperformed the frequency-based lower bound, but underperformed the WordNet baseline. Bilingual fe</context>
<context position="33404" citStr="Lin, 1999" startWordPosition="5665" endWordPosition="5666">gual statistics. The system of Salehi and Cook (2013) is perhaps the closest to the current work, judging noncompositionality using string edit distance between a candidate phrase’s automatic translation and its components’ individual translations. Unlike the current work, their method does not use distributions over translations or combine individual bilingual values with boosting; however, they find, as we do, that incorporating many languages is beneficial to MWE identification. A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; 644 Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be us</context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>Dekang Lin. 1999. Automatic identification of noncompositional phrases. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Bill Keller</author>
<author>John Carroll</author>
</authors>
<title>Detecting a continuum of compositionality in phrasal verbs.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL Workshop on Multiword Expressions.</booktitle>
<contexts>
<context position="5504" citStr="McCarthy et al., 2003" startWordPosition="860" endWordPosition="863">omatic “look down on him for his politics” with compositional “look down on him from the balcony.” In this paper, we focus on the task of determining whether a phrase type is a phrasal verb, meaning that it frequently expresses an idiomatic meaning across its many token usages in a corpus. We do not attempt to distinguish which individual phrase tokens in the corpus have idiomatic senses. Ranking vs. Classification. Identifying phrasal verbs involves relative, rather than categorical, judgments: some phrasal verbs are more compositional than others, but retain a degree of noncompositionality (McCarthy et al., 2003). Moreover, a polysemous phrasal verb may express an idiosyncratic sense more or less often than a compositional sense in a particular corpus. Therefore, we should expect a corpus-driven system not to classify phrases as strictly idiomatic or compositional, but instead assign a ranking or relative scoring to a set of candidates. Candidate Phrases. We distinguish between the task of identifying candidate multiword expressions 2http://en.wiktionary.org Feature Description cOL (x50) KL Divergence for each language L µ1 frequency of phrase given verb µ2 PMI of verb and particles µ3 µ1 with interpo</context>
<context position="13159" citStr="McCarthy et al., 2003" startWordPosition="2205" endWordPosition="2208">al verb (for look up, this will be [look|looks|looked|looking] up). We extract the variants in E from the verb entries in English Wiktionary. The final score computed from a phrase-aligned parallel corpus translating English sentences into a language L is the average KL divergence of smoothed constituent translation distributions for any inflected form ei ∈ E: � � DKL P 0 v(ei)kP 0 ei 3.2 Monolingual Statistics We also collect a number of monolingual statistics for each phrasal verb candidate, motivated by the considerable body of previous work on the topic (Church and Hanks, 1990; Lin, 1999; McCarthy et al., 2003). The monolingual statistics are designed to identify frequent collocations in a language. This set of monolingual features is not comprehensive, as we focus our attention primarily on bilingual features in this paper. As above, define E toxbe thefset of morphologically inflectedvariants of a candidate e, and let \hline \\ [-1ex] V be the set of inflected variants of the head verb $P_{(e)})&amp;. $ \hline \\ 1ex] v(e) of e. We define three statistics calculated from $Pe(x)$ &amp;$\frac{3}{4}=0.75 $ &amp;$\fra the phrase counts of a monolingual English corpus. \hline \\ [1ex] First, we define p1_(e) to be </context>
<context position="21405" citStr="McCarthy et al. (2003)" startWordPosition="3700" endWordPosition="3703">rived terms are not phrasal verbs (e.g. kick the bucket, make-or-break). We filter out any phrases not of the form V P+, with V a verb, and P+ denoting one or more occurrences of particles and prepositions from the list in Table 2. We omit prepositions that do not productively form English phrasal verbs, such as amid and as. This process also omits some compounds that are sometimes called phrasal verbs, such as light verb constructions, e.g. have a go (Butt, 2003), and noncompositional verb-adverb collocations, e.g. look forward. There are a number of extant phrasal verb corpora. For example, McCarthy et al. (2003) present graded human compositionality judgments for 116 phrasal verbs, and Baldwin (2008) presents a large set of candidates produced by an automated system, with false positives manually removed. We use Wiktionary instead, in an attempt to construct a maximally comprehensive data set that is free from any possible biases introduced by automatic extraction processes. 4.2 Filtering and Data Partition The merged list of phrasal verbs extracted from Wiktionary included some common collocations that have compositional semantics (e.g. know about), as well as some very rare constructions (e.g. chee</context>
<context position="33481" citStr="McCarthy et al., 2003" startWordPosition="5675" endWordPosition="5678">the closest to the current work, judging noncompositionality using string edit distance between a candidate phrase’s automatic translation and its components’ individual translations. Unlike the current work, their method does not use distributions over translations or combine individual bilingual values with boosting; however, they find, as we do, that incorporating many languages is beneficial to MWE identification. A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; 644 Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identification (Baldwin, 2005). 6 Conclusion We have prese</context>
</contexts>
<marker>McCarthy, Keller, Carroll, 2003</marker>
<rawString>Diana McCarthy, Bill Keller, and John Carroll. 2003. Detecting a continuum of compositionality in phrasal verbs. In Proceedings of the ACL Workshop on Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Automatic discovery of noncompositional compounds in parallel data.</title>
<date>1997</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="6533" citStr="Melamed, 1997" startWordPosition="1025" endWordPosition="1026"> expressions 2http://en.wiktionary.org Feature Description cOL (x50) KL Divergence for each language L µ1 frequency of phrase given verb µ2 PMI of verb and particles µ3 µ1 with interposed pronouns Table 1: Features used by the polyglot ranking system. and the task of ranking those candidates by their semantic idiosyncracy. With English phrasal verbs, it is straightforward to enumerate all desired verbs followed by one or more particles, and rank the entire set. Using Parallel Corpora. There have been a number of approaches proposed for the use of multilingual resources for MWE identification (Melamed, 1997; Villada Moir´on and Tiedemann, 2006; Caseli et al., 2010; Tsvetkov and Wintner, 2012; Salehi and Cook, 2013). Our approach differs from previous work in that we identify MWEs using translation distributions of verbs, as opposed to 1–1, 1–m, or m–n word alignments, most-likely translations, bilingual dictionaries, or distributional entropy. To the best of our knowledge, ours is the first approach to use translational distributions to leverage the observation that a verb typically translates differently when it heads a phrasal verb. 3 The Polyglot Ranking Approach Our approach uses bilingual a</context>
<context position="7838" citStr="Melamed, 1997" startWordPosition="1231" endWordPosition="1232">acterizes the degree of idiosyncrasy of a candidate phrasal verb, using a single monolingual or bilingual corpus. We combine features for many language pairs using a boosting algorithm that optimizes a ranking objective using a supervised training set of English phrasal verbs. Each of these aspects of our approach is described in detail below; for reference, Table 1 provides a list of the features used. 3.1 Bilingual Statistics One of the intuitive properties of an MWE is that its individual words likely do not translate literally when the whole expression is translated into another language (Melamed, 1997). We capture this effect 637 by measuring the divergence between how a verb translates generally and how it translates when heading a candidate phrasal verb. A parallel corpus is a collection of document pairs hDE, DFi, where DE is in English, DF is in another language, one document is a translation of the other, and all documents DF are in the same language. A phrase-aligned parallel corpus aligns those documents at a sentence, phrase, and word level. A phrase e aligns to another phrase f if some word in e aligns to some word in f and no word in e or f aligns outside of f or e, respectively. </context>
<context position="32027" citStr="Melamed (1997)" startWordPosition="5460" endWordPosition="5461">s verbs, such as hit by, drive along, and head towards. In these cases, prepositions disambiguate the verb, which naturally affects translation distributions. Other candidates are not phrasal verbs, but instead phrases that tend to have a different syntactic role, such as suit against, instance through, fit for, and lie over (conjugated as lay over). A careful treatment of part-of-speech tags when computing corpus statistics might address this issue. 5 Related Work The idea of using word-aligned parallel corpora to identify idiomatic expressions has been pursued in a number of different ways. Melamed (1997) tests candidate MWEs by collapsing them into single tokens, training a new translation model with these tokens, and using the performance of the new model to judge candidates’ noncompositionality. Villada Moir´on and Tiedemann (2006) use word-aligned parallel corpora to identify Dutch MWEs, testing the assumption that the distributions of alignments of MWEs will generally have higher entropies than those of fully compositional compounds. Caseli et al. (2010) generate candidate multiword expressions by picking out sufficiently common phrases that align to single target-side tokens. Tsvetkov an</context>
</contexts>
<marker>Melamed, 1997</marker>
<rawString>I. Dan Melamed. 1997. Automatic discovery of noncompositional compounds in parallel data. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Santanu Pal</author>
<author>Sudip Kumar Naskar</author>
<author>Pavel Pecina</author>
<author>Sivaji Bandyopadhyay</author>
<author>Andy Way</author>
</authors>
<title>Handling named entities and compound verbs in phrase-based statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the COLING 2010 Workshop on Multiword Expressions.</booktitle>
<contexts>
<context position="1343" citStr="Pal et al., 2010" startWordPosition="203" endWordPosition="206">ession (MWE), or noncompositional compound, is a sequence of words whose meaning cannot be composed directly from the meanings of its constituent words. These idiosyncratic phrases are prevalent in the lexicon of a language; Jackendoff (1993) estimates that their number is on the same order of magnitude as that of single words, and Sag et al. (2002) suggest that they are much more common, though quantifying them is challenging (Church, 2011). The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002). Awareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. &amp;quot;Research conducted during an internship at Google. We focus on a particular subset of MWEs, English phrasal verbs. A phrasal verb consists of a head verb followed by one or more particles, such that </context>
</contexts>
<marker>Pal, Naskar, Pecina, Bandyopadhyay, Way, 2010</marker>
<rawString>Santanu Pal, Sudip Kumar Naskar, Pavel Pecina, Sivaji Bandyopadhyay, and Andy Way. 2010. Handling named entities and compound verbs in phrase-based statistical machine translation. In Proceedings of the COLING 2010 Workshop on Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhixiang Ren</author>
<author>Yajuan Lu</author>
<author>Jie Cao</author>
<author>Qun Liu</author>
<author>Yun Huang</author>
</authors>
<title>Improving statistical machine translation using domain bilingual multiword expressions.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL Workshop on Multiword Expressions.</booktitle>
<contexts>
<context position="1324" citStr="Ren et al., 2009" startWordPosition="199" endWordPosition="202">n A multiword expression (MWE), or noncompositional compound, is a sequence of words whose meaning cannot be composed directly from the meanings of its constituent words. These idiosyncratic phrases are prevalent in the lexicon of a language; Jackendoff (1993) estimates that their number is on the same order of magnitude as that of single words, and Sag et al. (2002) suggest that they are much more common, though quantifying them is challenging (Church, 2011). The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002). Awareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. &amp;quot;Research conducted during an internship at Google. We focus on a particular subset of MWEs, English phrasal verbs. A phrasal verb consists of a head verb followed by one or more pa</context>
</contexts>
<marker>Ren, Lu, Cao, Liu, Huang, 2009</marker>
<rawString>Zhixiang Ren, Yajuan Lu, Jie Cao, Qun Liu, and Yun Huang. 2009. Improving statistical machine translation using domain bilingual multiword expressions. In Proceedings of the ACL Workshop on Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann A Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multiword expressions: A pain in the neck for NLP.</title>
<date>2002</date>
<booktitle>In Proceedings of the CICLING Conference on Intelligent Text Processing and Computational Linguistics.</booktitle>
<contexts>
<context position="1077" citStr="Sag et al. (2002)" startWordPosition="159" endWordPosition="162">onstrates that combining statistical evidence from many parallel corpora using a novel ranking-oriented boosting algorithm produces a comprehensive set of English phrasal verbs, achieving performance comparable to a human-curated set. 1 Introduction A multiword expression (MWE), or noncompositional compound, is a sequence of words whose meaning cannot be composed directly from the meanings of its constituent words. These idiosyncratic phrases are prevalent in the lexicon of a language; Jackendoff (1993) estimates that their number is on the same order of magnitude as that of single words, and Sag et al. (2002) suggest that they are much more common, though quantifying them is challenging (Church, 2011). The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002). Awareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A. Copestake, and Dan Flickinger. 2002. Multiword expressions: A pain in the neck for NLP. In Proceedings of the CICLING Conference on Intelligent Text Processing and Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bahar Salehi</author>
<author>Paul Cook</author>
</authors>
<title>Predicting the compositionality of multiword expressions using translations in multiple languages.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics.</booktitle>
<contexts>
<context position="6643" citStr="Salehi and Cook, 2013" startWordPosition="1040" endWordPosition="1043">L µ1 frequency of phrase given verb µ2 PMI of verb and particles µ3 µ1 with interposed pronouns Table 1: Features used by the polyglot ranking system. and the task of ranking those candidates by their semantic idiosyncracy. With English phrasal verbs, it is straightforward to enumerate all desired verbs followed by one or more particles, and rank the entire set. Using Parallel Corpora. There have been a number of approaches proposed for the use of multilingual resources for MWE identification (Melamed, 1997; Villada Moir´on and Tiedemann, 2006; Caseli et al., 2010; Tsvetkov and Wintner, 2012; Salehi and Cook, 2013). Our approach differs from previous work in that we identify MWEs using translation distributions of verbs, as opposed to 1–1, 1–m, or m–n word alignments, most-likely translations, bilingual dictionaries, or distributional entropy. To the best of our knowledge, ours is the first approach to use translational distributions to leverage the observation that a verb typically translates differently when it heads a phrasal verb. 3 The Polyglot Ranking Approach Our approach uses bilingual and monolingual statistics as features, computed over unlabeled corpora. Each statistic characterizes the degre</context>
<context position="32848" citStr="Salehi and Cook (2013)" startWordPosition="5581" endWordPosition="5584">illada Moir´on and Tiedemann (2006) use word-aligned parallel corpora to identify Dutch MWEs, testing the assumption that the distributions of alignments of MWEs will generally have higher entropies than those of fully compositional compounds. Caseli et al. (2010) generate candidate multiword expressions by picking out sufficiently common phrases that align to single target-side tokens. Tsvetkov and Wintner (2012) generate candidate MWEs by finding one-to-one alignments in parallel corpora which are not in a bilingual dictionary, and ranking them based on monolingual statistics. The system of Salehi and Cook (2013) is perhaps the closest to the current work, judging noncompositionality using string edit distance between a candidate phrase’s automatic translation and its components’ individual translations. Unlike the current work, their method does not use distributions over translations or combine individual bilingual values with boosting; however, they find, as we do, that incorporating many languages is beneficial to MWE identification. A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Steve</context>
</contexts>
<marker>Salehi, Cook, 2013</marker>
<rawString>Bahar Salehi and Paul Cook. 2013. Predicting the compositionality of multiword expressions using translations in multiple languages. In Second Joint Conference on Lexical and Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Schone</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Is knowledge-free induction of multiword unit dictionary headwords a solved problem?</title>
<date>2001</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="33431" citStr="Schone and Jurafsky, 2001" startWordPosition="5667" endWordPosition="5670">tics. The system of Salehi and Cook (2013) is perhaps the closest to the current work, judging noncompositionality using string edit distance between a candidate phrase’s automatic translation and its components’ individual translations. Unlike the current work, their method does not use distributions over translations or combine individual bilingual values with boosting; however, they find, as we do, that incorporating many languages is beneficial to MWE identification. A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; 644 Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identifi</context>
</contexts>
<marker>Schone, Jurafsky, 2001</marker>
<rawString>Patrick Schone and Daniel Jurafsky. 2001. Is knowledge-free induction of multiword unit dictionary headwords a solved problem? In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Linlin Li</author>
</authors>
<title>Unsupervised recognition of literal and non-literal use of idiomatic expressions.</title>
<date>2009</date>
<booktitle>In Proceedings of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="33868" citStr="Sporleder and Li, 2009" startWordPosition="5736" endWordPosition="5739">s beneficial to MWE identification. A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; 644 Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identification (Baldwin, 2005). 6 Conclusion We have presented the polyglot ranking approach to phrasal verb identification, using parallel corpora from many languages to identify phrasal verbs. We proposed an evaluation metric that acknowledges the inherent incompleteness of reference sets, but distinguishes among competing systems in a manner aligned to the goals of the task. We developed a recall-oriented learning method that integrates m</context>
</contexts>
<marker>Sporleder, Li, 2009</marker>
<rawString>Caroline Sporleder and Linlin Li. 2009. Unsupervised recognition of literal and non-literal use of idiomatic expressions. In Proceedings of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Shuly Wintner</author>
</authors>
<title>Extraction of multi-word expressions from small parallel corpora.</title>
<date>2012</date>
<journal>In Natural Language Engineering.</journal>
<contexts>
<context position="6619" citStr="Tsvetkov and Wintner, 2012" startWordPosition="1036" endWordPosition="1039">ivergence for each language L µ1 frequency of phrase given verb µ2 PMI of verb and particles µ3 µ1 with interposed pronouns Table 1: Features used by the polyglot ranking system. and the task of ranking those candidates by their semantic idiosyncracy. With English phrasal verbs, it is straightforward to enumerate all desired verbs followed by one or more particles, and rank the entire set. Using Parallel Corpora. There have been a number of approaches proposed for the use of multilingual resources for MWE identification (Melamed, 1997; Villada Moir´on and Tiedemann, 2006; Caseli et al., 2010; Tsvetkov and Wintner, 2012; Salehi and Cook, 2013). Our approach differs from previous work in that we identify MWEs using translation distributions of verbs, as opposed to 1–1, 1–m, or m–n word alignments, most-likely translations, bilingual dictionaries, or distributional entropy. To the best of our knowledge, ours is the first approach to use translational distributions to leverage the observation that a verb typically translates differently when it heads a phrasal verb. 3 The Polyglot Ranking Approach Our approach uses bilingual and monolingual statistics as features, computed over unlabeled corpora. Each statistic</context>
<context position="32643" citStr="Tsvetkov and Wintner (2012)" startWordPosition="5549" endWordPosition="5552">amed (1997) tests candidate MWEs by collapsing them into single tokens, training a new translation model with these tokens, and using the performance of the new model to judge candidates’ noncompositionality. Villada Moir´on and Tiedemann (2006) use word-aligned parallel corpora to identify Dutch MWEs, testing the assumption that the distributions of alignments of MWEs will generally have higher entropies than those of fully compositional compounds. Caseli et al. (2010) generate candidate multiword expressions by picking out sufficiently common phrases that align to single target-side tokens. Tsvetkov and Wintner (2012) generate candidate MWEs by finding one-to-one alignments in parallel corpora which are not in a bilingual dictionary, and ranking them based on monolingual statistics. The system of Salehi and Cook (2013) is perhaps the closest to the current work, judging noncompositionality using string edit distance between a candidate phrase’s automatic translation and its components’ individual translations. Unlike the current work, their method does not use distributions over translations or combine individual bilingual values with boosting; however, they find, as we do, that incorporating many language</context>
</contexts>
<marker>Tsvetkov, Wintner, 2012</marker>
<rawString>Yulia Tsvetkov and Shuly Wintner. 2012. Extraction of multi-word expressions from small parallel corpora. In Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Jay Ponte</author>
<author>Ashok Popat</author>
<author>Moshe Dubiner</author>
</authors>
<title>Large scale parallel document mining for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="23416" citStr="Uszkoreit et al., 2010" startWordPosition="4019" endWordPosition="4022">es (bottom) compared to three baselines (top) gives comparable performance to the humancurated upper bound. removed a few very frequent phrases and a large number of rare phrases. The remaining phrases were split randomly into a development set of 694 items and a held-out test set of 695 items. 4.3 Corpora Our monolingual English corpus consists of news articles and documents collected from the web. Our parallel corpora from English to each of 50 languages also consist of documents collected from the web via distributed data mining of parallel documents based on the text content of web pages (Uszkoreit et al., 2010). The parallel corpora were segmented into aligned sentence pairs and word-aligned using two iterations of IBM Model 1 (Brown et al., 1993) and two iterations of the HMM-based alignment model (Vogel et al., 1996) with posterior symmetrization (Liang et al., 2006). This training recipe is common in largescale machine translation systems. 4.4 Generating Candidates To generate the set of candidate phrasal verbs considered during evaluation, we exhaustively enumerated the Cartesian product of all verbs present in the previously described Wiktionary set (V), all particles in Table 2 (P) and a small</context>
</contexts>
<marker>Uszkoreit, Ponte, Popat, Dubiner, 2010</marker>
<rawString>Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Dubiner. 2010. Large scale parallel document mining for machine translation. In Proceedings of the Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sriram Venkatapathy</author>
<author>Aravind K Joshi</author>
</authors>
<title>Using information about multi-word expressions for the word-alignment task.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL Workshop on Multiword Expressions.</booktitle>
<contexts>
<context position="1681" citStr="Venkatapathy and Joshi (2006)" startWordPosition="257" endWordPosition="260">ds, and Sag et al. (2002) suggest that they are much more common, though quantifying them is challenging (Church, 2011). The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002). Awareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. &amp;quot;Research conducted during an internship at Google. We focus on a particular subset of MWEs, English phrasal verbs. A phrasal verb consists of a head verb followed by one or more particles, such that the meaning of the phrase cannot be determined by combining the simplex meanings of its constituent words (Baldwin and Villavicencio, 2002; Dixon, 1982; Bannard et al., 2003).1 Examples of phrasal verbs include count on [rely], look after [tend], or take off [remove], the meanings of which do not involve counting, looking, or taking. In</context>
</contexts>
<marker>Venkatapathy, Joshi, 2006</marker>
<rawString>Sriram Venkatapathy and Aravind K Joshi. 2006. Using information about multi-word expressions for the word-alignment task. In Proceedings of the ACL Workshop on Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bego˜na Villada Moir´on</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Identifying idiomatic expressions using automatic word-alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL Workshop on Multiword Expressions in a Multilingual Context.</booktitle>
<marker>Moir´on, Tiedemann, 2006</marker>
<rawString>Bego˜na Villada Moir´on and J¨org Tiedemann. 2006. Identifying idiomatic expressions using automatic word-alignment. In Proceedings of the EACL Workshop on Multiword Expressions in a Multilingual Context.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aline Villavicencio</author>
</authors>
<title>Verb-particle constructions and lexical resources.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL workshop on Multiword expressions.</booktitle>
<contexts>
<context position="33525" citStr="Villavicencio, 2003" startWordPosition="5683" endWordPosition="5684">ompositionality using string edit distance between a candidate phrase’s automatic translation and its components’ individual translations. Unlike the current work, their method does not use distributions over translations or combine individual bilingual values with boosting; however, they find, as we do, that incorporating many languages is beneficial to MWE identification. A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; 644 Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identification (Baldwin, 2005). 6 Conclusion We have presented the polyglot ranking approach to phrasa</context>
</contexts>
<marker>Villavicencio, 2003</marker>
<rawString>Aline Villavicencio. 2003. Verb-particle constructions and lexical resources. In Proceedings of the ACL workshop on Multiword expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Computational linguistics.</booktitle>
<contexts>
<context position="23628" citStr="Vogel et al., 1996" startWordPosition="4054" endWordPosition="4057">domly into a development set of 694 items and a held-out test set of 695 items. 4.3 Corpora Our monolingual English corpus consists of news articles and documents collected from the web. Our parallel corpora from English to each of 50 languages also consist of documents collected from the web via distributed data mining of parallel documents based on the text content of web pages (Uszkoreit et al., 2010). The parallel corpora were segmented into aligned sentence pairs and word-aligned using two iterations of IBM Model 1 (Brown et al., 1993) and two iterations of the HMM-based alignment model (Vogel et al., 1996) with posterior symmetrization (Liang et al., 2006). This training recipe is common in largescale machine translation systems. 4.4 Generating Candidates To generate the set of candidate phrasal verbs considered during evaluation, we exhaustively enumerated the Cartesian product of all verbs present in the previously described Wiktionary set (V), all particles in Table 2 (P) and a small set of second particles T = {with, to, on, c}, with c the empty string. The set of candidate phrasal verbs we consider during evaluation is the product V x P x T, which contains 96,880 items. 4.5 Results We opti</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of the Conference on Computational linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Xu</author>
<author>Hang Li</author>
</authors>
<title>AdaRank: a boosting algorithm for information retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of the SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<contexts>
<context position="19771" citStr="Xu and Li, 2007" startWordPosition="3449" endWordPosition="3452">After termination of Algorithm 1, we have weights α1, ... , αT and lists h1, ... , hT. Define ft as the function that generated the list ht (each ft will be some cOL or pi). Now, we define a final combined function co, taking a phrase e and returning a real number: T �0(e) _ αtft(e). t=1 We standardize the scores of individual weak rankers to have mean 0 and variance 1, so that their scores are comparable. The final learned ranker outputs a real value, instead of the class labels frequently found in AdaBoost. This follows previous work using boosting for learning to rank (Freund et al., 2003; Xu and Li, 2007). Our algorithm differs from previous methods because we are seeking to optimize for Recall-at-N, rather than a ranking loss. 4 Experimental Evaluation 4.1 Training and Test Set In order to train and evaluate our system, we construct a gold-standard list of phrasal verbs from the freely available English Wiktionary. We gather phrasal verbs from three sources within Wiktionary: 1. Entries labeled as English phrasal verbs4, 2. Entries labeled as English idioms5, and 3. The derived terms6 of English verb entries. 4http://en.wiktionary.org/wiki/Category: English—phrasal—verbs 5http://en.wiktionary</context>
</contexts>
<marker>Xu, Li, 2007</marker>
<rawString>Jun Xu and Hang Li. 2007. AdaRank: a boosting algorithm for information retrieval. In Proceedings of the SIGIR Conference on Research and Development in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Xu</author>
<author>Randy Goebel</author>
<author>Christoph Ringlstetter</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Application of the tightness continuum measure to chinese information retrieval.</title>
<date>2010</date>
<booktitle>In Proceedings of the COLING Workshop on Multiword Expressions.</booktitle>
<contexts>
<context position="1383" citStr="Xu et al., 2010" startWordPosition="210" endWordPosition="213">, is a sequence of words whose meaning cannot be composed directly from the meanings of its constituent words. These idiosyncratic phrases are prevalent in the lexicon of a language; Jackendoff (1993) estimates that their number is on the same order of magnitude as that of single words, and Sag et al. (2002) suggest that they are much more common, though quantifying them is challenging (Church, 2011). The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002). Awareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. &amp;quot;Research conducted during an internship at Google. We focus on a particular subset of MWEs, English phrasal verbs. A phrasal verb consists of a head verb followed by one or more particles, such that the meaning of the phrase cannot be dete</context>
</contexts>
<marker>Xu, Goebel, Ringlstetter, Kondrak, 2010</marker>
<rawString>Ying Xu, Randy Goebel, Christoph Ringlstetter, and Grzegorz Kondrak. 2010. Application of the tightness continuum measure to chinese information retrieval. In Proceedings of the COLING Workshop on Multiword Expressions.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
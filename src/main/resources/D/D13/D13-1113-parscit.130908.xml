<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995393">
Identifying Multiple Userids of the Same Author
</title>
<author confidence="0.978818">
Tieyun Qian Bing Liu
</author>
<affiliation confidence="0.9567545">
State Key Laboratory of Software Eng. Department of Computer Science
Wuhan University University of Illinois at Chicago
</affiliation>
<address confidence="0.982906">
16 Luojiashan Road 851 South Morgan St., Chicago
Wuhan, Hubei 430072, China IL, USA, 60607
</address>
<email confidence="0.986476">
qty@whu.edu.cn liub@cs.uic.edu
</email>
<sectionHeader confidence="0.994681" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999978666666667">
This paper studies the problem of identifying
users who use multiple userids to post in so-
cial media. Since multiple userids may belong
to the same author, it is hard to directly apply
supervised learning to solve the problem. This
paper proposes a new method, which still uses
supervised learning but does not require train-
ing documents from the involved userids. In-
stead, it uses documents from other userids
for classifier building. The classifier can be
applied to documents of the involved userids.
This is possible because we transform the
document space to a similarity space and
learning is performed in this new space. Our
evaluation is done in the online review do-
main. The experimental results using a large
number of userids and their reviews show that
the proposed method is highly effective.
</bodyText>
<sectionHeader confidence="0.99813" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.963548769230769">
It is common knowledge that some users in social
media register multiple accounts/userids to post
articles, blogs, reviews, etc. There are many rea-
sons for doing this. For example, due to past post-
ings, a user may become despised by others.
He/she then registers another userid in order to
regain his/her status. A user may also use multiple
userids to instigate controversy or debates to popu-
larize a topic to make it “hot” or even just to pro-
mote activities at a website. Yet, a user may also
use multiple userids to post fake or deceptive opin-
ions to promote or demote some products (Liu,
2012). It is thus important to develop technologies
* The work was mainly done when the first author was visit-
ing the University of Illinois at Chicago.
to identify such multi-id users. This paper deals
with this problem based on writing style and other
linguistic clues.
Problem definition: Given a set of userids ID =
{id1, ..., idn} and each idi has a set of documents
Di, we want to identify userids that belong to the
same physical author.
The main related works to ours are in the area of
authorship attribution (AA), which aims to identify
authors of documents. AA is often solved using
supervised learning. Let A = {a1, ..., ak} be a set of
authors (or classes) and each author ai  A has a
set of training documents Di. A classifier is then
built to decide the author a of each test document
d, where a  A. We will discuss this and other re-
lated works in Section 2.
This supervised AA formulation, however, is
not suitable for our task because we only have
userids but not real authors. Since some of the
userids may belong to the same author, we cannot
treat each userid as a class because in that case, we
will be classifying based on userids, which won’t
help us find authors with multiple userids (see Sec-
tion 7 also).
This paper proposes a novel algorithm. To sim-
plify the presentation, we assume that at most two
userids can belong to a single author, but the algo-
rithm can be extended to handle more than two
userids from the same author. Using this assump-
tion, the algorithm works in two steps:
1. Candidate identification: For each userid idi,
we first find the most likely userid idj (i ≠ j) that
may have the same author as idi. We call idj the
candidate of idi. We also call this function can-
did-iden, i.e., idj = candid-iden(idi). For easy
presentation, here we only use one argument for
1124
</bodyText>
<note confidence="0.9044365">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1124–1135,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.981243350515464">
candid-iden. In the computation, it needs more
arguments (see Section 4).
2. Candidate confirmation: In the reverse order,
we apply the function candid-iden on idj, which
produces idk, i.e., idk = candid-iden(idj).
Decision making: If k = i, we conclude that idi
and idj are from the same author. Otherwise, idi
and idj are not from the same author.
The key of the algorithm is candid-iden. An ob-
vious approach for candid-iden is to use an infor-
mation retrieval method. We can first split the
documents Di of each idi into two subsets, a query
set Qi and a sample set Si. We then compare each
query document in Qi with each sample document
in Sj from other userids idj ( ID – {idi}). Cosine
can be used here for similarity comparison. All the
similarity scores are then aggregated and used to
rank the userids in ID – {idi}. The top ranked
userid is the candidate for idi. Note that partition-
ing the documents of a userid idi into the query set
Qi and the sample set Si is crucial here. We cannot
use all documents in Di to compare with all docu-
ments in Dj. If so and we get candid-iden(idi) = idj,
we will definitely get candid-iden(idj) = idi since
the similarity function is symmetric.
This cosine similarity based method, however,
does not work well (see Section 7). We propose a
supervised learning method to compute the scores.
For this, we need to reformulate the problem.
The idea of this reformulation is to learn in a
similarity space rather than in the original docu-
ment space as in traditional AA. In the new formu-
lation, each document d is still represented as a
feature vector, but the vector no longer represents
the document d itself. Instead, it represents a set of
similarities between the document d and a query q.
We call this method learning in the similarity
space (LSS).
Specifically, in LSS, each document d is first
represented with a document space vector (called a
d-vector) based on the document itself as in the
traditional classification learning of AA. Each fea-
ture in the d-vector is called a d-feature (docu-
ment-feature). A query document q is represented
in the same way. We then produce a similarity vec-
tor sv (called s-vector) for d. sv consists of a set of
similarity values between document d (in a d-
vector) and query q (in a d-vector):
sv =Sim(d, q),
where Sim is a similarity function consists of a set
of similarity measures. Thus, the d-vector for doc-
ument d in the document space is transformed to
an s-vector sv for d in the similarity space. Each
feature in sv is called an s-feature. For example,
we have the following d-vector for query q:
q: 1:1 2:1 6:2
where x:z represents a d-feature x (a word) and its
frequency z in q. We also have two non-query
documents, one is d1 which is written by the author
of query q and the other is d2 which is not written
by query author q. Their d-vectors are:
d1: 1:2 2:1 3:1 d2: 2:2 3:1 5:2
If we use cosine as the first similarity measure in
Sim, we can generate an s-feature 1:0.50 for d1
(cosine(q, d1) = 0.50) and an s-feature 1:0.27 for d2
(cosine(q, d2) = 0.27). If we have more similarity
measures more s-features can be produced. The
resulting two s-vectors for d1 and d2 with their
class labels, 1 and -1, are as follows:
d1: 1 1:0.50 ... d2: -1 1:0.27 ...
Class 1 means “written by author of query q”, also
called q-positive, and class -1 means “not written
by author of query q”, also called q-negative.
LSS gives us a two-class classification problem.
In this formulation, a test userid and his/her docu-
ments do not have to be seen in training as long as
a set of known documents from this userid is
available. Any supervised learning method can be
used to build a classifier. We use SVM. The result-
ing classifier is employed to compute a score for
each review to be used in the two-step algorithm
above to find the candidate for each userid and
then the userids with the same authors.
Due to the use of query documents, the LSS
formulation has some resemblance to document
ranking based on learning to rank (Li, 2011; Liu,
2011). However, LSS is very different because we
turn the problem into a supervised classification
problem. The key difference between learning to
rank and classification is that ranking will always
put some documents at the top even if the desired
documents do not exist. However, classification
will not return any document if the desired docu-
ments do not exist in the test data (unless there are
classification errors). Our Type II experiments in
Section 7 were specifically designed for testing
such non-existence situations.
</bodyText>
<page confidence="0.539642">
1125
</page>
<bodyText confidence="0.999928166666667">
Using online review as the application domain,
we conduct experiments on a large number of re-
views and their author/reviewer userids from Am-
azon.com. The results show that the proposed
algorithm is highly accurate and outperforms three
strong baselines markedly.
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99992403125">
A similar problem was attempted in (Chen et al.,
2004) in the context of open forums where users
interact with each other in their discussions. Their
method is based on post relationships and intervals
between posts. It does not use any linguistic clues.
It is thus not applicable to domains like online re-
views. Reviews do not involve user interactions
since each review is independent of other reviews.
Novak et al. also solved the same problem under
the name of “Anti-aliasing” (Novak et al., 2004).
They used a clustering based method which as-
sumed the number of actual authors is known. This
is unrealistic in practice as there is no way to know
which author has and does not have multiple ids.
Our work is also related to authorship attribu-
tion (AA). However, to our knowledge, our prob-
lem has not been attempted in AA. Existing works
focused on two main themes: finding good writing
style features, and developing effective classifica-
tion methods. On finding good features (d-features
in our case), it was found that the most promising
features are function words (Mosteller, 1964; Ar-
gamon and Levitan, 2004; Argamon et al., 2007)
and rewrite rules (Halteren et al., 1996). Length
(Gamon 2004; Graham et al., 2005), richness (Hal-
teren et al., 1996; Koppel and Schler, 2004), punc-
tuations (Graham et al., 2005), character n-grams
(Grieve, 2007; Hedegaard and Simonsen, 2011),
word n-grams (Burrows, 1992; Sanderson and
Guenter 2006), POS n-grams (Gamon, 2004; Hirst
and Feiguina, 2007), syntactic category pairs (Na-
rayanan et al., 2012) are also useful.
On classification, numerous methods have been
tried, e.g., Bayesian analysis (Mosteller, 1964),
discriminant analysis (Stamatatos et al., 2000),
PCA (Hoover, 2001), neural networks (Graham et
al., 2005; Zheng et al., 2006; Graham et al., 2005),
clustering (Sanderson and Guenter, 2006), decision
trees (Uzuner and Katz, 2005; Zhao and Zobel,
2005), regularized least squares classification
(Narayanan et al., 2012), and SVM (Diederich et
al., 2000; Gamon 2004; Koppel and Schler, 2004;
Hedegaard and Simonsen, 2011). Among them,
SVM was found to be most accurate (Li et al.,
2006; Kim et al., 2011). Although we also use
supervised learning, we do not learn in the original
document space as these existing methods do. The
transformation is important because it enables us
to use documents from other authors in training.
The traditional supervised learning (TSL) cannot
do that. In our case, the only documents that TSL
can use for training are the queries in the testing
set. However, as we will see in our experiments,
such a method performs poorly.
Since we use online reviews as our experiment
domain, our work is related to fake review detec-
tion (Jindal and Liu, 2008) as imposters can use
multiple userids to post fake reviews. Existing re-
search has proposed many methods to detect fake
reviewers (Lim et al., 2010; Wang et al., 2011;
Mukherjee et al., 2012) and fake reviews (Jindal
and Liu, 2008; Ott et al., 2011, 2012; Li et al.,
2011; Feng et al., 2012). However, none of them
identifies userids belonging to the same person.
</bodyText>
<sectionHeader confidence="0.838987" genericHeader="method">
3 Learning in the Similarity Space
</sectionHeader>
<bodyText confidence="0.999426875">
We now formulate the proposed supervised
learning in the similarity space (LSS), which will
be used in the candid-iden function in our algo-
rithm to be discussed in Section 4.
The key difference between LSS and the classic
document space learning is in the document repre-
sentation. Another difference is in the testing
phase. We discuss testing first.
</bodyText>
<listItem confidence="0.816214">
Test data: We are given:
• A query q from query author (userid) aq
• A set of test documents DT = {dt1, ..., dtm}.
Goal: classify the test documents into those au-
thored by aq and those not authored by aq.
</listItem>
<bodyText confidence="0.7638565">
We note the following points:
i) This is like a retrieval scenario, but we use su-
pervised learning to perform the task.
ii) Unlike traditional supervised classification,
here the test query author aq does not have to
be used in training. But we are given a query
document q from aq. Clearly, in practice, we
can have multiple query documents from aq,
which we will discuss in Section 4.
Training document representation: As noted
earlier, each document is represented with a simi-
larity vector (s-vector) computed using a similarity
</bodyText>
<page confidence="0.501502">
1126
</page>
<listItem confidence="0.981758466666667">
1. For each author ari a AR
2. select a set of query documents Qi c DRi
3. For each query qij a Qi
// produce positive s-training examples
4. select a set of documents from author ari
DRij c DRi – {qij}
5. For each document drijk a DRij
6. produce an s-training example for drijk,
(Sim(drijk, qij), 1)
// produce negative s-training examples
7. select a set of documents from the rest of authors
DRij,rest c (DR1 v ... v DRn) – DRi
8. For each document drijk,rest a DRij,rest
9. produce an s-training example for drijk,rest,
(Sim(drijk,rest, qij), -1)
</listItem>
<figureCaption confidence="0.996882">
Figure 1: Generating s-training examples
</figureCaption>
<figure confidence="0.8401774">
// Author ar1 –
// positive (1) s-training examples
(Sim(dr111, q11), 1), ..., (Sim(dr11p, q11), 1)
...
(Sim(dr1k1, q1k), 1), ..., (Sim(dr1kp, q1k), 1)
// negative (-1) s-training examples
(Sim(dr111.rest, q11), -1), ..., (Sim(dr11u.rest, q11), -1)
...
(Sim(dr1k1.rest, q1k), -1), ..., (Sim(dr1ku.rest, q1k), -1)
...
</figure>
<figureCaption confidence="0.996941">
Figure 2: s-training examples
</figureCaption>
<bodyText confidence="0.999263923076923">
function Sim. Sim takes a query document and a
non-query document and produces a vector of sim-
ilarity values or s-features to represent the non-
query document. We present the detail below:
Let the set of training authors be AR = {ar1, ..,
arn}. Each author ari has a set of documents DRi.
Each document in DRi is first represented with a
document vector (or d-vector). The algorithm for
producing the training set, called s-training set, is
given in Figure 1.
We randomly select a small set of queries Qi
from documents DRi of each author ari (lines 1,
and 2). For each query qij a Qi (line 3), it selects a
set of documents DRij also from DRi (excluding qij)
of the same author (line 4) to be the positive doc-
uments for qij, called q-positive and labeled 1.
Then, for each document drijk in DRij, a q-positive
s-training example with the label 1 is generated for
drijk by computing the similarities of qij and drijk
using the similarity function Sim (lines 5, 6). In
line 7, it selects a set of documents DRij,rest from
other authors to be the negative documents for qij,
called q-negative and labeled -1. For each docu-
ment drijk,rest in DRij,rest (line 8), a q-negative s-
training example with label -1 is generated for drijk
by computing the similarities of qij and drijk,rest us-
</bodyText>
<listItem confidence="0.99963">
1. For each document set Di of idi a ID do
2. partition Di into two subsets:
(1) query set Qi and (2) sample set Si;
3. For each document set Di of idi a ID do
// step 1: candidate identification
4. idj = candid-iden(idi, ID), i &lt; j;
// step 2: candidate confirmation
5. idk = candid-iden(idj, ID), k ≠ j;
6. If k = i then idi and idj are from the same author
8. else idi and idj are not from the same author
</listItem>
<figureCaption confidence="0.964995">
Figure 3: Identifying userids from the same authors
</figureCaption>
<bodyText confidence="0.364983">
Function candidate-iden(idi, ID)
</bodyText>
<listItem confidence="0.924743777777778">
1. For each sample document set Sj of idj a ID-{idi} do
2. pcount[idj], psum[idj], psqsum[idj], max[idj] = 0;
3. For each query qi a Qi do
4. For each sample sjf a Sj do
5. ssjf = &lt;(idi, qi), (Sim(sjf, qi), ?)&gt;;
6. Classify ssjf using the classifier built earlier;
7. If ssjf is classified positive, i.e., 1 then
8. pcount[idj] = pcount[idj] + 1;
9. psum[idj] = psum[idj] + ssjf.score
</listItem>
<figure confidence="0.994710470588235">
10 psqsum[idj] = psqsum[idj] + (ssjf.score)2
11. If ssif.score &gt; max[idj] then
12. max[idj] = srjf.score
// Four methods to decide which idj is the candidate for idi
13. If for all idj a ID-{idi}, pcount[idi] = 0 then
14. ..:,a _ „.,,., i,,., F J,4 IN
idj ID{idi} ||
15. Elsepcount[id ]
cid  arg max(j)
idjID{idi}  |Sj |
]
16. cid  arg max ( psum[id j )
Sj
17. (psum[id j ])2
cid  arg max}(  |Sij  |)
18. // 4. ScoreMax
19. return cid;
</figure>
<figureCaption confidence="0.999927">
Figure 4: Identifying the candidate
</figureCaption>
<bodyText confidence="0.987952066666667">
ing Sim (line 9). How to select Qi, DRij and DRij,rest
(lines 2, 4 and 7) is left open intentionally to give
flexibility in implementation.
This formulation gives us a two-class classifica-
tion problem. The classes are 1 (q-positive mean-
ing “written by author of query qij”) and -1 (q-
negative meaning “not written by author of query
qij.” Figure 2 shows what the s-training data looks
like. For easy presentation, we assume that there
are k queries in every Qi, and p documents in every
DRij and u documents in every DRij,rest. The num-
ber of authors is n. Each author ari generates
k×(p+u) s-training examples. As we will see in
Section 7, k can be very small, even 1.
Complexity: In the worst case, every document
</bodyText>
<equation confidence="0.7567418">
// 1. Voting
// 2. ScoreSum
// 3. ScoreSqSum
idj ID
1127
</equation>
<bodyText confidence="0.999796555555556">
can serve as a query or a non-query document.
Then we need to compute all pairwise document
similarities. If the number of training documents is
m, the complexity is O(m2), which is both space
and computation expensive. However, in practice,
we don’t need all pairwise comparisons. Only a
small subset is sufficient (see Section 7).
Test document representation: Like training
documents, test documents are represented as s-
vectors as well in the similarity space.
Given a query q from author aq and a set of test
documents DT, each test document dti is converted
to a s-vector svi = Sim(dti, q). To reflect svi is com-
puted based on query q from author aq, a s-test
case is thus represented as &lt;(aq, q), (svi, ?)&gt;.
Training: A binary classifier is learned using the
s-training data. Each s-training example is repre-
sented with (sv, y), where sv is an s-vector and y
( {1, -1}) is its class. Any supervised learning
algorithm, e.g., SVM, can be applied.
Testing: The classifier is applied to each s-test
case &lt;(aq, q), (svi, ?)&gt; (where svi = S(dti, q)) to
give it a class q-positive or q-negative. Note that
the classifier is only applied on svi.
In most cases, classification based on a single que-
ry is inaccurate. Using multiple queries of an au-
thor can classify much more accurately.
</bodyText>
<sectionHeader confidence="0.993452" genericHeader="method">
4 Identify Userids of the Same Author
</sectionHeader>
<bodyText confidence="0.999916525">
We now expand the sketch of the two-step algo-
rithm in Section 1 based on the problem statement
in Section 1. The algorithm is given in Figure 3.
Lines 1-2 partitions the documents set Di of
each idi in ID = {id1, id2, ..., idn}, the set of userids
that we are working on. How to do the partition is
flexible (see Section 7). Line 4 is the step 1 of
candidate identification, and line 5 is the step2 of
candidate confirmation. Lines 6-8 is the decision
making of step 2 (see Section 1). Line 6 produced
a classification score using the classifier described
in Section 3. The key function here is candid-iden.
Its algorithm is in Figure 4.
The candid-iden function takes two arguments:
the query userid idi and the whole set of userids
ID. It classifies each sample ssjf in sample set Sj of
idj  ID-{idi} to positive (qi-positive) or negative
(qi-negative) (lines 4, 5, 6). We then aggregate the
classification results to determine which userid is
likely to have the same author as idi.
One simple aggregation method is voting. We
count the total number of positive classifications of
the sample documents of each userid in ID-{idi}.
The userid idj with the highest count is the candi-
date cid which may share the same author as query
idi. cid is returned as the candidate.
There are also other methods, which can depend
on what output value the classifier produces. Here
we propose four methods including the voting
method above. The other three methods requires
the classifier to produces a prediction score, which
reflects the positive and negative certainty. Many
classification algorithms produce such a score.
Here we use SVM. For each classification, SVM
outputs a positive or negative score indicating the
certainty that the test case is positive or negative.
To save space, all four alternative methods are
given in Figure 4. Line 2 initializes some variables
for recording the aggregated values for the final
decision making. The four methods are as follows:
</bodyText>
<listItem confidence="0.705202086956522">
1). Voting: For each sample from userid idj, if it is
classified as positive, one vote/count is added
to pcount[idj]. The userid with the highest
pcount is regarded as the candidate userid, cid
(line 15). Note that the normalization is ap-
plied because the sizes of the sample sets Sj
can be different for different userids. Lines 13
and 14 mean that if all documents of all
userids are classified as negative (pcount[idj] =
0, which also implies psum[idj] = psqsum[idj]
= 0), we use method 4).
2). ScoreSum: This method works similarly to the
voting method above except that instead of
counting positive classifications, this method
sums up all scores of positive classifications in
psum[idj] for each userid (line 9). The decision
is also made similarly (line 16).
3). ScoreSqSum: This method works similarly to
ScoreSum above except that it sums up the
squared scores of positive classifications in
psqsum[idj] for each userid (line 10). The deci-
sion is also made similarly (line 17).
4). ScoreMax: This method works similarly to the
</listItem>
<bodyText confidence="0.954801">
voting method as well except that it finds the
maximum classification score for the docu-
ments of each userid (lines 11 and 12). The
decision is made in line 18.
</bodyText>
<sectionHeader confidence="0.985402" genericHeader="method">
5 D-features
</sectionHeader>
<bodyText confidence="0.976575304347826">
We now compute s-features (similarity features)
1128
for each non-query document based on a query
document. Since s-features are calculated using d-
features of a non-query document and a query
document, we thus discuss d-features first, which
are extracted from each document itself. We em-
ploy 26 d-features in four categories: length d-
features, frequency based d-features, tf.idf based d-
features, and richness d-features. Although many
features below have been used in various tasks
before, our key contribution is solving a new prob-
lem based on a new learning formulation (LSS).
Length d-feature: We derive three length d-
features from each raw document: (1) average
sentence length (in terms of word count); (2)
average word length (in terms of character count
in one word); (3) average document length (in
terms of word count in one document).
Frequency based d-features: We extract lexical,
syntactic, and stylistic tokens from the raw docu-
ments and the parsed syntactic trees to produce the
following features:
</bodyText>
<listItem confidence="0.832152695652174">
• Lexical tokens: word unigrams
• Syntactic tokens: content-independent struc-
tures: POS n-grams (1 &lt; n &lt; 3) and rewrite rules
(Halteren et al., 1996; Hirst and Feiguina, 2007).
A rewrite rule is a combination of a node and
its immediate constituents in a syntactic tree.
For example, the rewrite rule for &amp;quot;the best
book&amp;quot; is NP-&gt;DT+JJS+NN.
• Common stylistic token: K-length word (1 &lt; K &lt;
15), punctuations, and 157 function words
(www.flesl.net/Vocabulary/SinglewordLists/fun
ctionwordlist.php).
• Review specific stylistic tokens: These tokens
reflect styles of reviews: all cap words, pairs of
quotation marks, pairs of brackets, exclamatory
marks, contractions, two or more consecutive
non-alphanumeric characters, model auxilia-
ries (e.g., should, must), word “recommend” or
“recommended”, sentences with the first letter
capitalized, sentences starting with This is (this
is) or This was (this was). We then treat these
tokens as pseudo-words and count their fre-
quency to form frequency d-features.
</listItem>
<bodyText confidence="0.999873291666667">
TF-IDF based d-feature: For the tokens listed in
the frequency based features above, we also com-
pute their tf.idf values. We list these two kinds of
d-features separately because they will be used for
different s-features later.
Richness d-features: This is a set of vocabulary
richness functions used to quantify the diversity of
vocabulary in text (Holmes and Forsyth, 1995). In
this paper, we apply them to the counts of word
unigrams, POS n-grams (1 &lt; n &lt; 3), and rewrite
rules. Here POS n-grams and rewrite rules are
treated as pseudo-words. Let T be the total number
of tokens (words or pseudo-words), and V(T) be
the number of different tokens in a document, v be
the highest frequency of occurrence of a token, and
V(m, T) be the number of tokens which occur m
times in the document. We use the following six
richness measures (Yule, 1944; Burrows, 1992;
Halteren et al., 1996) given in Table 1: Yule’s
characteristic (K), Hapax dislegomena (S), Simp-
son’s index (D), Honorës measure (R), Brunet’s
measure (W), and Hapax legomena (H). They give
us a set of richness d-features about word uni-
grams, POS n-grams, and rewrite rules.
</bodyText>
<tableCaption confidence="0.920205">
Table 1. Richness metrics
</tableCaption>
<equation confidence="0.953371">
K 104* m1
m m - V m T
 ( *( 1)* ( , ))
m  1
D 
T *( T-1)
E(m2*V(m,T)T) S=V(2&apos;T)
</equation>
<table confidence="0.756221714285714">

V(T)
T2
v R= 100*log(T)
1-V(l, T)/V(T)
H=V(1,T) W=T&amp;quot;(T)-&apos;,a=0.17
v
</table>
<sectionHeader confidence="0.852058" genericHeader="method">
6 S-Features
</sectionHeader>
<bodyText confidence="0.9998106">
The extracted d-features are transformed into s-
features, which are a set of similarity functions on
two documents. We adopt five types of s-features.
Sim4 Length s-features: This is a set of four simi-
larity functions defined by us. They are used for d-
feature vectors of length. The four formulae are
given in Table 2, where lwq. (lwd), lsq. (lsd), and lrq.
(lrd) denote the average word, sentence, and docu-
ment length respectively, either in query q or non-
query document d. They produce four s-features.
</bodyText>
<tableCaption confidence="0.726512">
Table 2. Sim4 for computing length s-features
</tableCaption>
<equation confidence="0.740444571428571">
m w s r  { , , }
1/ (1 + log(1+  |&amp;quot; ,�  &amp;quot;,d |))
m  1/ (1 + log(1+  |lsq  lsd |))
w s r (lmq*ld)/ (lmq)2*(lmd)2
{ , , }  { , , } m w s r
1/ (1 + log(1+ |l,E  l,d |))
1129
</equation>
<bodyText confidence="0.996298">
Sim3 Sentence s-features: This is a set of three
sentence similarity functions (Metzler et al., 2005).
We apply them (called Sim3) to documents. Sim3
s-features are used for frequency based d-features.
The three formulae are given in Table 3, where f(t,
s) is the frequency count of token t in a document s,
and lq and ld are the average document length of
the query and non-query document, respectively.
</bodyText>
<tableCaption confidence="0.435309">
Table 3. Sim3 for computing sentence s-features
</tableCaption>
<equation confidence="0.9139452">
t q d
e n
f (t , d)
teq ted t q d
e n
</equation>
<bodyText confidence="0.98553775">
Sim7 Retrieval s-features: This is a set of seven
similarity functions (Table 4) applicable to all fre-
quency based d-features. These functions were
used in information retrieval (Cao et al., 2006).
</bodyText>
<tableCaption confidence="0.928644">
Table 4. Sim7 for computing retrieval s-features
</tableCaption>
<table confidence="0.992868111111111">
log(f(t, d) + 1) {IDI +1)
tagnd E
log(
tegnd J (t, d)
log(idf(t))
to qnd
 1)
E log( fid
d)
t qd
log( f * idf (t)  1) log(BM25score)
 i d d)
� d
log( f (t, d) *
{ID |
1)

tqd  |d  |J (t, d)
</table>
<bodyText confidence="0.999688545454546">
In Table 4, f(t, d) denotes the frequency count of
token t in a non-query document d, q denotes the
query, D is the entire collection, |. |is the size of a
set, and idf is the inverse document frequency.
These 7 formulae can produce 7 s-features.
SimC tf-idf s-feature: This is the cosine similarity
used for d-vectors represented by the tf.idf based
d-features. SimC tf-idf produces one s-feature.
SimC Richness s-feature: This is also cosine sim-
ilarity. However, it is applied to the richness d-
feature vectors, and produces one s-feature.
</bodyText>
<sectionHeader confidence="0.995605" genericHeader="evaluation">
7 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999842">
We now evaluate the proposed approach and com-
pare it with baselines. All our experiments use the
SVMperf classifier (Joachims, 2006).
</bodyText>
<subsectionHeader confidence="0.99089">
7.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.999718346938776">
Experiment Data: We use a set of reviews and
their authors/reviewers from Amazon.com as our
experiment data. We select the authors who have
posted more than 30 reviews in the book category.
After cleaning, we have 831 authors, 731 authors
for training and 100 authors for testing. The num-
bers of reviews in the training and test author set
are 59256 and 14308, respectively. We use the
Stanford parser (Klein and Manning, 2003) to gen-
erate the grammar structure of review sentences
for extracting syntactic d-features. Note that the
authors here are in fact userids. However, since
they are randomly selected from a large number of
userids, the probability that two sampled userids
belong to the same person is very small. Thus, it
should be safe to assume that each userid here rep-
resents a unique author.
Training data: We randomly choose 1 (one) re-
view for each author as the query and all of his/her
other reviews as q-positive reviews. The q-
negative reviews consist of reviews randomly se-
lected from the other 730 authors, two reviews per
author. We also tried to use more queries from
each author, but they make little difference.
Test data: The test authors are all unseen, i.e.,
their reviews have not been used in training. We
prepare the test case for each author as follows.
We first divide the reviews of each author into
two equal subsets. The purpose is to simulate the
situation where there are two userids idia and idib
from the same author ai. Our objective is that giv-
en one userid idia and its query set, we want to find
the other userid idib from the same author.
For the review subset of idia (or idib), we ran-
domly select 9 reviews as the query set and anoth-
er 10 reviews as the sample set for the userid. The
two sets are disjoint. We don’t use more queries or
sample reviews from each author since in the re-
view domain most authors do not have many re-
views (Jindal and Liu, 2008). In the experiments,
we will vary the number of test userids, the num-
ber of queries, and the number of samples. We use
the following format to describe each test data:
T&lt;n&gt;_Q&lt;n&gt;S&lt;n&gt;, where T denotes the total num-
ber of test userids, Q the query set and S the sam-
ple set, and &lt;n&gt; a number. For example,
T50_Q9S10 stands for a test data with 50 userids,
and for each userid, 9 reviews are selected as que-
ries and 10 reviews are selected as samples. * rep-
</bodyText>
<equation confidence="0.96932825">
E
N *
logteqnd(f(t,d)) Ef(t,q)+Ef(t,d)-Ef(t,d)
1130
</equation>
<bodyText confidence="0.999786277777778">
resents a wildcard whose value we can vary.
Note that we use this “artificial” data rather than
manually labeled data for our experiments because
it is very hard to reliably label any gold-standard
data manually in this case. The problem is similar
to labeling fake reviews. In the fake review detec-
tion research, researchers have manually label fake
reviews and reviewers (Yoo and Gretzel 2009;
Lim et al., 2010; Li et al., 2011; Wang et al., 2011).
However, based on the actual fake reviews written
using Amazon Mechanical Turk, Ott et al. (2011)
have showed that the accuracy of human labeling
of fake reviews is very poor. We also believe that
our test data is realistic for evaluation as we can
image that the two sets of reviews are from two
accounts (userids) of the same author (reviewer).
Two types of experiments: For each author with
two userids, we conduct two types of tests.
</bodyText>
<listItem confidence="0.972797">
• Type I: Identify two userids belong to the same
author. The experiment runs iteratively to test
every userid. In each iteration, we plant one
userid of an author in the test set and use the
other userid of the same author as the query
userid. That is, in the ith run, the test data con-
sist of the following two components:
1. Query userid idia and its query set Qia
2. Test userids {id1a, ..., id(i-1)a, idib, ..., idma}
and their corresponding sample review sets
{S1a, ..., S(i-1)a, Sib, ..., Sma}.
</listItem>
<bodyText confidence="0.999096428571429">
Note that the query userid idia and the test
userid idib are from the same author. Our objec-
tive is to use Qia to find idib through Sib.
Evaluation measure: We use precision, recall,
and F1 score to evaluate Type I experiments as
we want to identify all matching pairs. The er-
rors are “no pair” and “wrong pair” found.
</bodyText>
<listItem confidence="0.9946125">
• Type II: Type II experiments test the cases
when no pair exists. That is, we do not plant
any matching userid for the query userid. Then,
the algorithm should not find anything. For the
ith run, the test data has these components:
1. Query userid idia and its query set Qia
</listItem>
<bodyText confidence="0.999354217391304">
2.Test userids {id1a, ..., id(i-1)a, id(i+1)a, ..., idma}
and their sample review sets {S1a, ..., S(i-1)a,
S(i+1)a, ..., Sma}. idib is not planted.
Evaluation measure: Here we cannot use pre-
cision and recall because we are not trying to
find any pairs. We thus use accuracy as our
measure. For each idi, if no pair is found, it is
correct. If a pair is found, it is wrong.
Baseline methods: As mentioned eariler, there
are only two works that tried to identify multi-id
users. The first is that in (Chen et al., 2004).
However, as we discussed in related work, their
approach is not applicable to reviews. The other is
that in (Novak et al., 2004), which used clustering
but assumed that the number of actual authors (or
clusters) is known. This is unrealistic in practice.
Thus we designed three new baselines:
TSL: This baseline is based on the traditional su-
pervised learning (TSL). We use it to evaluate
how the traditional approach performs in the
original feature space. In this case, each docu-
ment in TSL has to be represented as a vector of
d-features or traditional n-gram features. For
each test userid id, we build a SVM classifier
based on the one vs. all strategy. That is, for
training we use id’s queries in T*_Q*S10 as the
positive documents, and all queries of the other
test userids (e.g., 99 userids if the test data has
100 userids) as the negative documents. Note
that TSL cannot use the 731 userids for training
as in LSS because they do not appear in the test
data. In testing, userid id’s sample (non-query)
documents in T*_Q*S10 are used as positive
documents, and the sample documents of all oth-
er test userids are used as negative documents.
SimUG: It uses the word unigrams to compare the
cosine similarity of queries and samples. Cosine
similarity with unigrams is the most widely used
document similarity measure.
SimAD: It uses all d-features to compare the cosine
similarity of queries and samples.
For both SimUG and SimAD, their cosine simi-
larity values are used in place of SVM scores of
LSS or TSL. We then apply the same 4 strategies
to decide the final author attribution except voting
as cosine similarity cannot classify.
</bodyText>
<subsectionHeader confidence="0.964672">
7.2 Results and analysis
</subsectionHeader>
<bodyText confidence="0.9982285">
1) Effects of positive/total ratio in training set:
Since our data is highly skewed and too many neg-
ative cases may not be good for classification, we
thus performed this experiment to find a good ratio.
Table 5 shows the results for Type I experiments.
From Table 5, we can see that the results are high-
ly accurate. Even for 100 userids, our method can
correctly identify 85% cases. Here we use the data
sets T*_Q9S10 and the decision method is
ScoreSqSum, which produces the best result. The
</bodyText>
<page confidence="0.493719">
1131
</page>
<bodyText confidence="0.99973525">
results for Type II experiments (Table 6) are also
accurate. In most cases, the values of accuracy are
higher than 90%. For all our experiments below,
we use the model/classifier trained with 0.4 ratio.
</bodyText>
<tableCaption confidence="0.998878">
Table 5. Positive(p)/total(t) ratio in training (Type I)
Table 6. Positive(p)/total(t) ratio in training (Type II)
</tableCaption>
<table confidence="0.9907095">
Accuracy p/t 10 30 50 80 100
0.3 90.00 90.00 92.00 97.50 94.00
0.4 90.00 90.00 94.00 98.75 95.00
0.5 80.00 86.67 94.00 97.75 95.00
0.6 80.00 86.67 90.00 93.75 92.00
0.7 80.00 86.67 90.00 95.00 92.00
</table>
<listItem confidence="0.9058945">
(2) Effects of different decision methods: We
show the results of the four proposed decision
methods: Voting, ScoreSum, ScoreSqSum, and
ScoreMax, using our basic data of T*_Q9S10 with
varied number of test userids. Figure 5(a) shows
that ScoreSqSum is the best for Type I experi-
ments. Figure 5(b) shows ScoreMax is the best for
Type II, but ScoreSqSum also does very well. Be-
low, ScoreSqSum is used as our default method
because Type I is more important than Type II.
</listItem>
<figure confidence="0.984006">
(a) Type I (b) Type II
</figure>
<figureCaption confidence="0.999201">
Figure 5: Effect of different decision methods
</figureCaption>
<bodyText confidence="0.953192333333333">
(3) Effects of number of queries per userid:
Figure 6 shows the results of different numbers of
queries. We see that more queries give better re-
sults, which is easy to understand because more
queries give more information. We use 9 queries
per userid in all other experiments.
</bodyText>
<figure confidence="0.996738">
(a) Type I (b) Type II
</figure>
<figureCaption confidence="0.99987">
Figure 6: Effect of different numbers of queries
</figureCaption>
<bodyText confidence="0.978248">
(4) Effects of number of samples per userid: We
tried 2, 4, 6, 8, 10 samples per userid. Although
there are some fluctuations for Type II (Fig.7(b)),
we can see an upward trend for Type I in Fig. 7(a).
This indicates that more sample documents give
better results in general. The main reason again is
that more samples from a userid give more identi-
fying information about the userid. We use 10 test
documents (samples) per userid in all experiments.
</bodyText>
<figure confidence="0.996732">
(a) Type I (b) Type II
</figure>
<figureCaption confidence="0.999972">
Figure 7: Effect of different number of samples
</figureCaption>
<bodyText confidence="0.995331181818182">
(5) Impact of individual s-feature sets: Here we
show the effectiveness of individual s-feature sets.
From Table 7, we see that Sim7Retrieval s-
features are extremely important for Type I test.
Removing Sim7Retrieval causes about 10% to
20% F1 score drop on different datasets. SimCT-
fidf s-features are also useful. The impacts of other
s-features are small. The same applies to Type II
test (Table 8). On average, using all features is the
best. Hence we use all features in all other experi-
ments above.
</bodyText>
<tableCaption confidence="0.996031">
Table 7. Using different s-features (Type I)
</tableCaption>
<table confidence="0.99981375">
T*_Q9S10 F1 F1 F1 F1 F1
10 30 50 80 100
All features 100.00 90.91 90.11 88.89 85.71
No Sim4Len 100.00 88.89 86.36 87.32 85.06
No SimCRichness 100.00 88.89 91.30 88.89 85.71
No SimCTfidf 100.00 80.00 86.36 86.53 83.72
No Sim7Retrieval 82.35 72.34 75.80 78.79 77.30
No Sim3Sent 94.74 84.62 86.36 88.11 87.64
</table>
<tableCaption confidence="0.994758">
Table 8. Using different s-features (Type II)
</tableCaption>
<table confidence="0.99719375">
T*_Q9S10 Acc. Acc Acc. Acc Acc.
10 30 50 80 100
All features 90.00 90.00 94.00 98.75 99.00
No Sim4Len 90.00 93.33 96.00 96.25 96.00
No SimCRichness 90.00 90.00 94.00 96.25 96.00
No SimCTfidf 90.00 86.67 94.00 93.75 97.00
No Sim7Retrieval 80.00 90.00 94.00 94.00 96.00
No Sim3Sent 90.00 93.33 92.00 98.75 93.00
</table>
<listItem confidence="0.848175">
(6) Comparing with the three baselines: Similar
to our method, the training data for TSL is highly
skewed as it uses a one-vs.-all strategy. Hence we
also investigate the effect of p/t ratio in training for
TSL. Results show that 0.4 ratio is the best setting.
</listItem>
<figure confidence="0.934954125">
p/t 10 30 50 80 100
0.3 100.00 84.62 86.36 88.89 83.72
0.4 100.00 91.91 90.11 88.89 85.71
0.5 100.00 90.91 91.30 88.89 87.01
0.6 94.74 82.35 87.64 85.71 86.36
0.7 94.74 84.62 86.36 86.53 87.64
F1
1132
</figure>
<bodyText confidence="0.999622">
Thus this setting is adopted for TSL in the follow-
ing experiments. Note that we cannot conduct p/t
ratio experiments for SimAD and SimUG as they
are unsupervised methods. We use ScoreMax for
TSL, ScoreSqSum for SimUG and SimAD, re-
spectively, since they perform the best for their
corresponding approaches. Tables 9 and 10 show
the results of our LSS method and the baseline
methods for Type I and II tests respectively. For
TSL, we use all d-features. Unigram features gave
TSL much worse results and are thus not included
here.
</bodyText>
<tableCaption confidence="0.999703">
Table 9: Comparison with baselines (Type I)
</tableCaption>
<table confidence="0.999943153846154">
10 30 50 80 100
LSS Pre 100.00 100.00 100.00 100.00 98.68
Rec 100.00 83.33 82.00 80.00 75.76
F1 100.00 90.91 90.11 88.89 85.71
TSL Pre 50.00 50.00 33.33 0.00 0.00
Rec 11.11 3.45 2.08 0.00 0.00
F1 18.18 6.45 3.92 0.00 0.00
SimUG Pre 100.00 100.00 100.00 100.00 100.00
Rec 70.00 46.67 48.00 48.75 43.00
F1 82.35 63.64 64.86 65.55 60.14
SimAD Pre 100.00 75.00 100.00 33.33 0.00
Rec 20.00 10.35 2.00 1.28 0.00
F1 33.33 18.18 3.92 2.47 0.00
</table>
<tableCaption confidence="0.999229">
Table 10: Comparison with baselines (Type II)
</tableCaption>
<table confidence="0.991297">
Accuracy 10 30 50 80 100
LSS 90.00 90.00 94.00 98.75 95.00
TSL 90.00 96.67 98.00 98.75 99.00
SimUG 96.00 93.33 96.00 96.25 97.00
SimAD 90.00 96.67 98.00 98.75 99.00
</table>
<bodyText confidence="0.976770714285714">
From Tables 9 and 10, we can make the follow-
ing observations.
• For Type I, F1 scores of LSS are markedly bet-
ter than those of the three baselines. The results
of SimUG also drop more quickly than LSS
with the increased number of userids. SimAD’s
results are extremely poor. These show that
LSS is much more superior to the unsupervised
methods. TSL performed the worst, indicating
that traditional supervised learning is inappro-
priate for this task. There are two main reasons:
First, for one vs. all learning, the negative train-
ing data actually contain positive documents
which are written by the same author using an-
other userid as the positive data, which confus-
es the classifier. Second, TSL is unable to build
an accurate classifier using the small number of
queries (which are training data). In contrast,
our LSS method can exploit a large number of
other authors who do not have to appear in test-
ing and thus achieves the huge improvements.
• For Type II, LSS also performs very well. The
baselines perform well too and even better,
which is not surprising because they have diffi-
culty in finding matching pairs for Type I.
Since Type II datasets have no author with mul-
tiple userids, naturally the baselines will do
well for Type II. But that is useless because
when there are authors with multiple usersids
(Type I), they are unable to find them well.
In summary, we can conclude that for Type I tests
(there are authors with multiple userids), LSS is
dramatically better than all baseline methods. For
Type II tests (there is no author with multiple
userids), it also performs very well.
</bodyText>
<sectionHeader confidence="0.997806" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999981375">
This paper proposed a novel method to identify
userids that may be from the same author. The
core of the method is a supervised learning method
which learns in a similarity space rather than the
document space. This learning method is able to
better determine whether a document may be writ-
ten by a known author, although no document
from the author has been used in training (as long
as we have some documents from the author to
serve as queries). To the best of our knowledge,
there is no existing method based on linguistic
analysis for solving the problem. Our experimental
results based on a large number of reviewers and
their reviews show that the proposed algorithm is
highly accurate. It outperforms three baselines
markedly.
</bodyText>
<sectionHeader confidence="0.995577" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999967285714286">
We are grateful to the anonymous reviewers for
their thoughtful comments. Tieyun Qian was sup-
ported in part by the NSFC Projects (61272275,
61272110, 61202036), and the 111 Project
(B07037). Bing Liu was supported in part by a
grant from National Science Foundation (NSF)
under no. IIS-1111092.
</bodyText>
<sectionHeader confidence="0.968078" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.97962325">
Shlomo Argamon and Shlomo Levitan. 2004.
Measuring the usefulness of function words for
authorship attribution. Literary and Linguistic
Computing 1-3.
</bodyText>
<page confidence="0.572886">
1133
</page>
<reference confidence="0.998544364130435">
Shlomo Argamon, Casey Whitelaw, Paul Chase,
Sobhan Raj Hota, Navendu Garg, and Shlomo
Levitan. 2007. Stylistic text classification using
functional lexical features: Research articles. J.
Am. Soc. Inf. Sci. Technol. 58:802-822.
John F. Burrows. 1992. Not unless you ask nicely:
The interpretative nexus between analysis and
information. Literary and Linguistic Computing
7:91-109.
Yunbo Cao, Jun Xu, Tie-Yan Liu, Hang Li, Yalou
Huang, and Hsiao-Wuen Hon. 2006. Adapting
ranking svm to document retrieval. Proc. of
SIGIR, Pages 186-193.
Hung-Ching Chen, Mark K. Goldberg, Malik
Magdon-Ismail. 2004. Identifying multi-ID users
in open forums. Intelligence and Security
Informatics, Pages 176-186.
Joachim Diederich, Jörg Kindermann, Edda
Leopold, and Gerhard Paass, 2000. Authorship
attribution with support vector machines.
Applied Intelligence 19:109-123.
Hugo Jair Escalante, Thamar Solorio, and Manuel
Montes-y-Gómez. 2011. Local histograms of
character n-grams for authorship attribution.
Proc. of ACL-HLT, Volume I: 288-298.
Song Feng, Longfei Xing, Anupam Gogar, and
Yejin Choi. 2012. Distributional Footprints of
Deceptive Product Reviews. Proc. of ICWSM.
Michael Gamon. 2004. Linguistic correlates of
style: authorship classification with deep
linguistic analysis features. Proc. of Coling.
Neil Graham, Graeme Hirst, and Bhaskara Marthi.
2005. Segmenting documents by stylistic
character. Natural Language Engineering,
11:397-415.
Jack Grieve. 2007. Quantitative authorship
attribution: An evaluation of techniques. Literary
and Linguistic Computing 22:251-270.
Hans van Halteren, Fiona Tweedie, and Harald
Baayen. 1996. Outside the cave of shadows:
using syntactic annotation to enhance authorship
attribution. Literary and Linguistic Computing
11:121-132.
Steffen Hedegaard and Jakob Grue Simonsen.
2011. Lost in translation: authorship attribution
using frame semantics. Proc. of ACL-HLT, short
papers - Volume 2, 65-70.
Graeme Hirst and Ol’ga Feiguina. 2007. Bigrams
of syntactic labels for authorship discrimination
of short texts. Literary and Linguistic Computing
22:405-417.
David I. Holmes and R. S. Forsyth. 1995. The
Federalist Revisited: New Directions in
Authorship Attribution, Literary and Linguistic
Computing, 10(2): 111-127.
David L. Hoover. 2001. Statistical stylistics and
authorship attribution: an empirical
investigation. Literary and Linguistic Computing
16:421-424.
Nitin Jindal and Bing Liu. 2008. Opinion Spam
and Analysis. Proc. of WSDM, California, USA.
Thorsten Joachims. 2006. Training linear svms in
linear time. Proc. of KDD.
Sangkyum Kim, Hyungsul Kim, Tim Weninger,
Jiawei Han, and Hyun Duk Kim. 2011.
Authorship classification: a discriminative
syntactic tree mining approach. Proc. of SIGIR,
Pages 455-464.
Dan Klein, and Christopher D. Manning. 2003.
Accurate unlexicalized parsing. Proc. of ACL,
423-430.
Moshe Koppel and Jonathan Schler. 2004.
Authorship verification as a one-class
classification problem. Proc. of ICML.
Moshe Koppel, Jonathan Schler, Shlomo
Argamon. 2011. Authorship attribution in the
wild. Lang Resources &amp; Evaluation, 45:83-94
Fangtao Li, Minlie Huang, Yi Yang and Xiaoyan
Zhu. 2011. Learning to identify review Spam.
Proc. of IJCAI.
Hang Li. 2011. Learning to Rank for Information
Retrieval and Natural Language Processing.
Morgan &amp; Claypool publishers.
Jiexun Li, Rong Zheng, and Hsinchun Chen. 2006.
From fingerprint to writeprint. Communications
of the ACM, 49:76-82.
Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing
Liu, Hady W. Lauw. 2010. Detecting product
review spammers using rating behaviors. Proc.
of CIKM, 2010.
Bing Liu. 2012. Sentiment Analysis and Opinion
Mining, Morgan &amp; Claypool publishers.
1134
Tieyan Liu. 2011. Learning to Rank for Infor-
mation Retrieval. Springer.
Kim Luyckx, Walter Daelemans. 2008. Authorship
Attribution and Verification with Many Authors
and Limited Data. Proc. of Coling, pages 513-
520.
David Madigan, Alexander Genkin, David D.
Lewis, Shlomo Argamon, Dmitriy Fradkin, and
Li Ye. 2005. Author Identification on the Large
Scale. Proc. of CSNA.
Donald Metzler, Yaniv Bernstein, W. Bruce Croft,
Alistair Moffat, and Justin Zobel. 2005.
Similarity measures for tracking information
flow. Proc. of CIKM. Pages 517-524.
Frederick Mosteller, David Lee Wallace. 1964.
Inference and disputed authorship: The
Federalist. Addison-Wesley.
Arjun Mukherjee, Bing Liu, and Natalie Glance.
2012. Spotting Fake Reviewer Groups in Con-
sumer Reviews. Proc. of WWW, Pages 191-200.
Arvind Narayanan, Hristo Paskov, Neil Zhenqiang
Gong, et al. 2012. On the feasibility of internet-
scale author identification. Proceedings of the
2012 IEEE Symposium on Security and Privacy.
Pages 300-314
Jasmine Novak, Prabhakar Raghavan, Andrew
Tomkins. 2004. Anti-aliasing on the web. Proc.
of WWW, Pages 30-39
Myle Ott, Yejin Choi, Claire Cardie, Jeffrey T.
Hancock. 2011. Finding Deceptive Opinion
Spam by Any Stretch of the Imagination. Proc.
of ACL.
Myle Ott, Claire Cardie, Jeffrey T. Hancock. 2012.
Estimating the prevalence of deception in online
review communities. Proc. of WWW.
Fuchun Peng, Dale Schuurmans, Shaojun Wang,
and Vlado Keselj. 2003. Language independent
authorship attribution using character level
language models. Proc. of EACL, Pages 267-
274.
Conrad Sanderson and Simon Guenter. 2006.
Short text authorship attribution via sequence
kernels, markov chains and author unmasking:
an investigation. Proc. of EMNLP, Pages 482-
491.
Yanir Seroussi, Fabian Bohnert, Ingrid Zukerman.
2012. Authorship Attribution with Author-
aware Topic Models. Proc. of ACL, 2:264-269.
Thamar Solorio, Sangita Pillay, Sindhu Raghavan,
Manuel Montes y G´omez. 2011. Modality
Specific Meta Features for Authorship
Attribution in Web Forum Posts. Proc. of
IJCNLP, Pages 156-164.
Efstathios Stamatatos. 2009. A Survey of Modern
Authorship Attribution Methods. Journal of the
American Society for Information Science and
Technology, 60(3):538-556, Wiley.
Efstathios Stamatatos, George Kokkinakis, and
Nikos Fakotakis. 2000. Automatic text
categorization in terms of genre and author.
Comput. Linguist. 26:471-495.
Özlem Uzuner and Boris Katz. 2005. A
comparative study of language models for book
and author recognition. Proc. of IJCNLP, Pages
969-980.
Vladimir N. Vapnik. 1998. Statistical Learning
Theory. Wiley-Interscience, NY.
O. de Vel, A. Anderson, M. Corney and G.
Mohay. 2001. Mining Email Content for Author
Identification Forensics. Sigmod Record, 30:55-
64.
Kyung-Hyan Yoo and Ulrike Gretzel. 2009.
Comparison of Deceptive and Truthful Travel
Reviews. Information and Communication
Technologies in Tourism, Pages 37-47.
Georgy Udnv Yule. 1944. The statistical study of
literary vocabulary. Cambridge University
Press.
Guan Wang, Sihong Xie, Bing Liu, Philip S. Yu.
2011. Review Graph based Online Store
Review Spammer Detection. Proc. of ICDM.
Ying Zhao and Justin Zobel. 2005. Effective and
scalable authorship attribution using function
words. Proceeding of Information Retrival
Technology, Pages 174-189.
Rong Zheng, Jiexun Li, Hsinchun Chen, and Zan
Huang. 2006. A framework for authorship iden-
tification of online messages: Writing style fea-
tures and classification techniques. Journal of
the American Society of Information Science
and Technology 57:378-393.
</reference>
<page confidence="0.798441">
1135
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.928385">
<title confidence="0.998553">Identifying Multiple Userids of the Same Author</title>
<author confidence="0.989697">Tieyun Qian Bing Liu</author>
<affiliation confidence="0.9975815">Key Laboratory of Software Eng. of Computer Science Wuhan University University of Illinois at Chicago</affiliation>
<address confidence="0.9825535">16 Luojiashan Road 851 South Morgan St., Chicago Hubei 430072, China USA, 60607</address>
<email confidence="0.99718">qty@whu.edu.cnliub@cs.uic.edu</email>
<abstract confidence="0.998860684210526">This paper studies the problem of identifying users who use multiple userids to post in social media. Since multiple userids may belong to the same author, it is hard to directly apply supervised learning to solve the problem. This paper proposes a new method, which still uses supervised learning but does not require training documents from the involved userids. Instead, it uses documents from other userids for classifier building. The classifier can be applied to documents of the involved userids. This is possible because we transform the document space to a similarity space and learning is performed in this new space. Our evaluation is done in the online review domain. The experimental results using a large number of userids and their reviews show that the proposed method is highly effective.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shlomo Argamon</author>
<author>Casey Whitelaw</author>
<author>Paul Chase</author>
<author>Sobhan Raj Hota</author>
<author>Navendu Garg</author>
<author>Shlomo Levitan</author>
</authors>
<title>Stylistic text classification using functional lexical features: Research articles.</title>
<date>2007</date>
<journal>J. Am. Soc. Inf. Sci. Technol.</journal>
<pages>58--802</pages>
<contexts>
<context position="9720" citStr="Argamon et al., 2007" startWordPosition="1694" endWordPosition="1697">lustering based method which assumed the number of actual authors is known. This is unrealistic in practice as there is no way to know which author has and does not have multiple ids. Our work is also related to authorship attribution (AA). However, to our knowledge, our problem has not been attempted in AA. Existing works focused on two main themes: finding good writing style features, and developing effective classification methods. On finding good features (d-features in our case), it was found that the most promising features are function words (Mosteller, 1964; Argamon and Levitan, 2004; Argamon et al., 2007) and rewrite rules (Halteren et al., 1996). Length (Gamon 2004; Graham et al., 2005), richness (Halteren et al., 1996; Koppel and Schler, 2004), punctuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural netwo</context>
</contexts>
<marker>Argamon, Whitelaw, Chase, Hota, Garg, Levitan, 2007</marker>
<rawString>Shlomo Argamon, Casey Whitelaw, Paul Chase, Sobhan Raj Hota, Navendu Garg, and Shlomo Levitan. 2007. Stylistic text classification using functional lexical features: Research articles. J. Am. Soc. Inf. Sci. Technol. 58:802-822.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John F Burrows</author>
</authors>
<title>Not unless you ask nicely: The interpretative nexus between analysis and information.</title>
<date>1992</date>
<booktitle>Literary and Linguistic Computing</booktitle>
<pages>7--91</pages>
<contexts>
<context position="9992" citStr="Burrows, 1992" startWordPosition="1738" endWordPosition="1739">has not been attempted in AA. Existing works focused on two main themes: finding good writing style features, and developing effective classification methods. On finding good features (d-features in our case), it was found that the most promising features are function words (Mosteller, 1964; Argamon and Levitan, 2004; Argamon et al., 2007) and rewrite rules (Halteren et al., 1996). Length (Gamon 2004; Graham et al., 2005), richness (Halteren et al., 1996; Koppel and Schler, 2004), punctuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sanderson and Guenter, 2006), decision trees (Uzuner and Katz, 2005; Zhao and Zobel, 2005), regularized least squares classification (Narayanan et al., 2012), and SVM (Diederich et al., 2000; </context>
<context position="24537" citStr="Burrows, 1992" startWordPosition="4234" endWordPosition="4235">: This is a set of vocabulary richness functions used to quantify the diversity of vocabulary in text (Holmes and Forsyth, 1995). In this paper, we apply them to the counts of word unigrams, POS n-grams (1 &lt; n &lt; 3), and rewrite rules. Here POS n-grams and rewrite rules are treated as pseudo-words. Let T be the total number of tokens (words or pseudo-words), and V(T) be the number of different tokens in a document, v be the highest frequency of occurrence of a token, and V(m, T) be the number of tokens which occur m times in the document. We use the following six richness measures (Yule, 1944; Burrows, 1992; Halteren et al., 1996) given in Table 1: Yule’s characteristic (K), Hapax dislegomena (S), Simpson’s index (D), Honorës measure (R), Brunet’s measure (W), and Hapax legomena (H). They give us a set of richness d-features about word unigrams, POS n-grams, and rewrite rules. Table 1. Richness metrics K 104* m1 m m - V m T  ( *( 1)* ( , )) m  1 D  T *( T-1) E(m2*V(m,T)T) S=V(2&apos;T)  V(T) T2 v R= 100*log(T) 1-V(l, T)/V(T) H=V(1,T) W=T&amp;quot;(T)-&apos;,a=0.17 v 6 S-Features The extracted d-features are transformed into sfeatures, which are a set of similarity functions on two documents. We adopt five ty</context>
</contexts>
<marker>Burrows, 1992</marker>
<rawString>John F. Burrows. 1992. Not unless you ask nicely: The interpretative nexus between analysis and information. Literary and Linguistic Computing 7:91-109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunbo Cao</author>
<author>Jun Xu</author>
<author>Tie-Yan Liu</author>
<author>Hang Li</author>
<author>Yalou Huang</author>
<author>Hsiao-Wuen Hon</author>
</authors>
<title>Adapting ranking svm to document retrieval.</title>
<date>2006</date>
<booktitle>Proc. of SIGIR,</booktitle>
<pages>186--193</pages>
<contexts>
<context position="26419" citStr="Cao et al., 2006" startWordPosition="4592" endWordPosition="4595">unctions (Metzler et al., 2005). We apply them (called Sim3) to documents. Sim3 s-features are used for frequency based d-features. The three formulae are given in Table 3, where f(t, s) is the frequency count of token t in a document s, and lq and ld are the average document length of the query and non-query document, respectively. Table 3. Sim3 for computing sentence s-features t q d e n f (t , d) teq ted t q d e n Sim7 Retrieval s-features: This is a set of seven similarity functions (Table 4) applicable to all frequency based d-features. These functions were used in information retrieval (Cao et al., 2006). Table 4. Sim7 for computing retrieval s-features log(f(t, d) + 1) {IDI +1) tagnd E log( tegnd J (t, d) log(idf(t)) to qnd  1) E log( fid d) t qd log( f * idf (t)  1) log(BM25score)  i d d) � d log( f (t, d) * {ID | 1)  tqd |d |J (t, d) In Table 4, f(t, d) denotes the frequency count of token t in a non-query document d, q denotes the query, D is the entire collection, |. |is the size of a set, and idf is the inverse document frequency. These 7 formulae can produce 7 s-features. SimC tf-idf s-feature: This is the cosine similarity used for d-vectors represented by the tf.idf based d-</context>
</contexts>
<marker>Cao, Xu, Liu, Li, Huang, Hon, 2006</marker>
<rawString>Yunbo Cao, Jun Xu, Tie-Yan Liu, Hang Li, Yalou Huang, and Hsiao-Wuen Hon. 2006. Adapting ranking svm to document retrieval. Proc. of SIGIR, Pages 186-193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hung-Ching Chen</author>
<author>Mark K Goldberg</author>
<author>Malik Magdon-Ismail</author>
</authors>
<title>Identifying multi-ID users in open forums. Intelligence and Security Informatics,</title>
<date>2004</date>
<pages>176--186</pages>
<contexts>
<context position="8637" citStr="Chen et al., 2004" startWordPosition="1511" endWordPosition="1514">documents do not exist. However, classification will not return any document if the desired documents do not exist in the test data (unless there are classification errors). Our Type II experiments in Section 7 were specifically designed for testing such non-existence situations. 1125 Using online review as the application domain, we conduct experiments on a large number of reviews and their author/reviewer userids from Amazon.com. The results show that the proposed algorithm is highly accurate and outperforms three strong baselines markedly. 2 Related Work A similar problem was attempted in (Chen et al., 2004) in the context of open forums where users interact with each other in their discussions. Their method is based on post relationships and intervals between posts. It does not use any linguistic clues. It is thus not applicable to domains like online reviews. Reviews do not involve user interactions since each review is independent of other reviews. Novak et al. also solved the same problem under the name of “Anti-aliasing” (Novak et al., 2004). They used a clustering based method which assumed the number of actual authors is known. This is unrealistic in practice as there is no way to know whi</context>
<context position="32299" citStr="Chen et al., 2004" startWordPosition="5657" endWordPosition="5660">ing. For the ith run, the test data has these components: 1. Query userid idia and its query set Qia 2.Test userids {id1a, ..., id(i-1)a, id(i+1)a, ..., idma} and their sample review sets {S1a, ..., S(i-1)a, S(i+1)a, ..., Sma}. idib is not planted. Evaluation measure: Here we cannot use precision and recall because we are not trying to find any pairs. We thus use accuracy as our measure. For each idi, if no pair is found, it is correct. If a pair is found, it is wrong. Baseline methods: As mentioned eariler, there are only two works that tried to identify multi-id users. The first is that in (Chen et al., 2004). However, as we discussed in related work, their approach is not applicable to reviews. The other is that in (Novak et al., 2004), which used clustering but assumed that the number of actual authors (or clusters) is known. This is unrealistic in practice. Thus we designed three new baselines: TSL: This baseline is based on the traditional supervised learning (TSL). We use it to evaluate how the traditional approach performs in the original feature space. In this case, each document in TSL has to be represented as a vector of d-features or traditional n-gram features. For each test userid id, </context>
</contexts>
<marker>Chen, Goldberg, Magdon-Ismail, 2004</marker>
<rawString>Hung-Ching Chen, Mark K. Goldberg, Malik Magdon-Ismail. 2004. Identifying multi-ID users in open forums. Intelligence and Security Informatics, Pages 176-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Diederich</author>
<author>Jörg Kindermann</author>
<author>Edda Leopold</author>
<author>Gerhard Paass</author>
</authors>
<title>Authorship attribution with support vector machines.</title>
<date>2000</date>
<journal>Applied Intelligence</journal>
<pages>19--109</pages>
<contexts>
<context position="10590" citStr="Diederich et al., 2000" startWordPosition="1823" endWordPosition="1826">d n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sanderson and Guenter, 2006), decision trees (Uzuner and Katz, 2005; Zhao and Zobel, 2005), regularized least squares classification (Narayanan et al., 2012), and SVM (Diederich et al., 2000; Gamon 2004; Koppel and Schler, 2004; Hedegaard and Simonsen, 2011). Among them, SVM was found to be most accurate (Li et al., 2006; Kim et al., 2011). Although we also use supervised learning, we do not learn in the original document space as these existing methods do. The transformation is important because it enables us to use documents from other authors in training. The traditional supervised learning (TSL) cannot do that. In our case, the only documents that TSL can use for training are the queries in the testing set. However, as we will see in our experiments, such a method performs po</context>
</contexts>
<marker>Diederich, Kindermann, Leopold, Paass, 2000</marker>
<rawString>Joachim Diederich, Jörg Kindermann, Edda Leopold, and Gerhard Paass, 2000. Authorship attribution with support vector machines. Applied Intelligence 19:109-123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Jair Escalante</author>
<author>Thamar Solorio</author>
<author>Manuel Montes-y-Gómez</author>
</authors>
<title>Local histograms of character n-grams for authorship attribution.</title>
<date>2011</date>
<booktitle>Proc. of ACL-HLT, Volume I:</booktitle>
<pages>288--298</pages>
<marker>Escalante, Solorio, Montes-y-Gómez, 2011</marker>
<rawString>Hugo Jair Escalante, Thamar Solorio, and Manuel Montes-y-Gómez. 2011. Local histograms of character n-grams for authorship attribution. Proc. of ACL-HLT, Volume I: 288-298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Song Feng</author>
<author>Longfei Xing</author>
<author>Anupam Gogar</author>
<author>Yejin Choi</author>
</authors>
<title>Distributional Footprints of Deceptive Product Reviews.</title>
<date>2012</date>
<booktitle>Proc. of ICWSM.</booktitle>
<contexts>
<context position="11608" citStr="Feng et al., 2012" startWordPosition="2001" endWordPosition="2004">d learning (TSL) cannot do that. In our case, the only documents that TSL can use for training are the queries in the testing set. However, as we will see in our experiments, such a method performs poorly. Since we use online reviews as our experiment domain, our work is related to fake review detection (Jindal and Liu, 2008) as imposters can use multiple userids to post fake reviews. Existing research has proposed many methods to detect fake reviewers (Lim et al., 2010; Wang et al., 2011; Mukherjee et al., 2012) and fake reviews (Jindal and Liu, 2008; Ott et al., 2011, 2012; Li et al., 2011; Feng et al., 2012). However, none of them identifies userids belonging to the same person. 3 Learning in the Similarity Space We now formulate the proposed supervised learning in the similarity space (LSS), which will be used in the candid-iden function in our algorithm to be discussed in Section 4. The key difference between LSS and the classic document space learning is in the document representation. Another difference is in the testing phase. We discuss testing first. Test data: We are given: • A query q from query author (userid) aq • A set of test documents DT = {dt1, ..., dtm}. Goal: classify the test do</context>
</contexts>
<marker>Feng, Xing, Gogar, Choi, 2012</marker>
<rawString>Song Feng, Longfei Xing, Anupam Gogar, and Yejin Choi. 2012. Distributional Footprints of Deceptive Product Reviews. Proc. of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
</authors>
<title>Linguistic correlates of style: authorship classification with deep linguistic analysis features.</title>
<date>2004</date>
<booktitle>Proc. of Coling.</booktitle>
<contexts>
<context position="9782" citStr="Gamon 2004" startWordPosition="1706" endWordPosition="1707">n. This is unrealistic in practice as there is no way to know which author has and does not have multiple ids. Our work is also related to authorship attribution (AA). However, to our knowledge, our problem has not been attempted in AA. Existing works focused on two main themes: finding good writing style features, and developing effective classification methods. On finding good features (d-features in our case), it was found that the most promising features are function words (Mosteller, 1964; Argamon and Levitan, 2004; Argamon et al., 2007) and rewrite rules (Halteren et al., 1996). Length (Gamon 2004; Graham et al., 2005), richness (Halteren et al., 1996; Koppel and Schler, 2004), punctuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2</context>
</contexts>
<marker>Gamon, 2004</marker>
<rawString>Michael Gamon. 2004. Linguistic correlates of style: authorship classification with deep linguistic analysis features. Proc. of Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neil Graham</author>
<author>Graeme Hirst</author>
<author>Bhaskara Marthi</author>
</authors>
<title>Segmenting documents by stylistic character.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<pages>11--397</pages>
<contexts>
<context position="9804" citStr="Graham et al., 2005" startWordPosition="1708" endWordPosition="1711">nrealistic in practice as there is no way to know which author has and does not have multiple ids. Our work is also related to authorship attribution (AA). However, to our knowledge, our problem has not been attempted in AA. Existing works focused on two main themes: finding good writing style features, and developing effective classification methods. On finding good features (d-features in our case), it was found that the most promising features are function words (Mosteller, 1964; Argamon and Levitan, 2004; Argamon et al., 2007) and rewrite rules (Halteren et al., 1996). Length (Gamon 2004; Graham et al., 2005), richness (Halteren et al., 1996; Koppel and Schler, 2004), punctuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sand</context>
</contexts>
<marker>Graham, Hirst, Marthi, 2005</marker>
<rawString>Neil Graham, Graeme Hirst, and Bhaskara Marthi. 2005. Segmenting documents by stylistic character. Natural Language Engineering, 11:397-415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jack Grieve</author>
</authors>
<title>Quantitative authorship attribution: An evaluation of techniques.</title>
<date>2007</date>
<journal>Literary and Linguistic Computing</journal>
<pages>22--251</pages>
<contexts>
<context position="9932" citStr="Grieve, 2007" startWordPosition="1730" endWordPosition="1731">p attribution (AA). However, to our knowledge, our problem has not been attempted in AA. Existing works focused on two main themes: finding good writing style features, and developing effective classification methods. On finding good features (d-features in our case), it was found that the most promising features are function words (Mosteller, 1964; Argamon and Levitan, 2004; Argamon et al., 2007) and rewrite rules (Halteren et al., 1996). Length (Gamon 2004; Graham et al., 2005), richness (Halteren et al., 1996; Koppel and Schler, 2004), punctuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sanderson and Guenter, 2006), decision trees (Uzuner and Katz, 2005; Zhao and Zobel, 2005), regularized least squares classification</context>
</contexts>
<marker>Grieve, 2007</marker>
<rawString>Jack Grieve. 2007. Quantitative authorship attribution: An evaluation of techniques. Literary and Linguistic Computing 22:251-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Halteren</author>
<author>Fiona Tweedie</author>
<author>Harald Baayen</author>
</authors>
<title>Outside the cave of shadows: using syntactic annotation to enhance authorship attribution. Literary and Linguistic Computing</title>
<date>1996</date>
<pages>11--121</pages>
<marker>van Halteren, Tweedie, Baayen, 1996</marker>
<rawString>Hans van Halteren, Fiona Tweedie, and Harald Baayen. 1996. Outside the cave of shadows: using syntactic annotation to enhance authorship attribution. Literary and Linguistic Computing 11:121-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen Hedegaard</author>
<author>Jakob Grue Simonsen</author>
</authors>
<title>Lost in translation: authorship attribution using frame semantics.</title>
<date>2011</date>
<booktitle>Proc. of ACL-HLT, short papers -</booktitle>
<volume>2</volume>
<pages>65--70</pages>
<contexts>
<context position="9963" citStr="Hedegaard and Simonsen, 2011" startWordPosition="1732" endWordPosition="1735">(AA). However, to our knowledge, our problem has not been attempted in AA. Existing works focused on two main themes: finding good writing style features, and developing effective classification methods. On finding good features (d-features in our case), it was found that the most promising features are function words (Mosteller, 1964; Argamon and Levitan, 2004; Argamon et al., 2007) and rewrite rules (Halteren et al., 1996). Length (Gamon 2004; Graham et al., 2005), richness (Halteren et al., 1996; Koppel and Schler, 2004), punctuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sanderson and Guenter, 2006), decision trees (Uzuner and Katz, 2005; Zhao and Zobel, 2005), regularized least squares classification (Narayanan et al., 2012), and </context>
</contexts>
<marker>Hedegaard, Simonsen, 2011</marker>
<rawString>Steffen Hedegaard and Jakob Grue Simonsen. 2011. Lost in translation: authorship attribution using frame semantics. Proc. of ACL-HLT, short papers - Volume 2, 65-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>Ol’ga Feiguina</author>
</authors>
<title>Bigrams of syntactic labels for authorship discrimination of short texts.</title>
<date>2007</date>
<journal>Literary and Linguistic Computing</journal>
<pages>22--405</pages>
<contexts>
<context position="10074" citStr="Hirst and Feiguina, 2007" startWordPosition="1748" endWordPosition="1751">s: finding good writing style features, and developing effective classification methods. On finding good features (d-features in our case), it was found that the most promising features are function words (Mosteller, 1964; Argamon and Levitan, 2004; Argamon et al., 2007) and rewrite rules (Halteren et al., 1996). Length (Gamon 2004; Graham et al., 2005), richness (Halteren et al., 1996; Koppel and Schler, 2004), punctuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sanderson and Guenter, 2006), decision trees (Uzuner and Katz, 2005; Zhao and Zobel, 2005), regularized least squares classification (Narayanan et al., 2012), and SVM (Diederich et al., 2000; Gamon 2004; Koppel and Schler, 2004; Hedegaard and Simonsen, 2011). Among them, SV</context>
<context position="22848" citStr="Hirst and Feiguina, 2007" startWordPosition="3962" endWordPosition="3965">ulation (LSS). Length d-feature: We derive three length dfeatures from each raw document: (1) average sentence length (in terms of word count); (2) average word length (in terms of character count in one word); (3) average document length (in terms of word count in one document). Frequency based d-features: We extract lexical, syntactic, and stylistic tokens from the raw documents and the parsed syntactic trees to produce the following features: • Lexical tokens: word unigrams • Syntactic tokens: content-independent structures: POS n-grams (1 &lt; n &lt; 3) and rewrite rules (Halteren et al., 1996; Hirst and Feiguina, 2007). A rewrite rule is a combination of a node and its immediate constituents in a syntactic tree. For example, the rewrite rule for &amp;quot;the best book&amp;quot; is NP-&gt;DT+JJS+NN. • Common stylistic token: K-length word (1 &lt; K &lt; 15), punctuations, and 157 function words (www.flesl.net/Vocabulary/SinglewordLists/fun ctionwordlist.php). • Review specific stylistic tokens: These tokens reflect styles of reviews: all cap words, pairs of quotation marks, pairs of brackets, exclamatory marks, contractions, two or more consecutive non-alphanumeric characters, model auxiliaries (e.g., should, must), word “recommend” </context>
</contexts>
<marker>Hirst, Feiguina, 2007</marker>
<rawString>Graeme Hirst and Ol’ga Feiguina. 2007. Bigrams of syntactic labels for authorship discrimination of short texts. Literary and Linguistic Computing 22:405-417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David I Holmes</author>
<author>R S Forsyth</author>
</authors>
<title>The Federalist Revisited: New Directions</title>
<date>1995</date>
<booktitle>in Authorship Attribution, Literary and Linguistic Computing,</booktitle>
<volume>10</volume>
<issue>2</issue>
<pages>111--127</pages>
<contexts>
<context position="24052" citStr="Holmes and Forsyth, 1995" startWordPosition="4142" endWordPosition="4145">t), word “recommend” or “recommended”, sentences with the first letter capitalized, sentences starting with This is (this is) or This was (this was). We then treat these tokens as pseudo-words and count their frequency to form frequency d-features. TF-IDF based d-feature: For the tokens listed in the frequency based features above, we also compute their tf.idf values. We list these two kinds of d-features separately because they will be used for different s-features later. Richness d-features: This is a set of vocabulary richness functions used to quantify the diversity of vocabulary in text (Holmes and Forsyth, 1995). In this paper, we apply them to the counts of word unigrams, POS n-grams (1 &lt; n &lt; 3), and rewrite rules. Here POS n-grams and rewrite rules are treated as pseudo-words. Let T be the total number of tokens (words or pseudo-words), and V(T) be the number of different tokens in a document, v be the highest frequency of occurrence of a token, and V(m, T) be the number of tokens which occur m times in the document. We use the following six richness measures (Yule, 1944; Burrows, 1992; Halteren et al., 1996) given in Table 1: Yule’s characteristic (K), Hapax dislegomena (S), Simpson’s index (D), H</context>
</contexts>
<marker>Holmes, Forsyth, 1995</marker>
<rawString>David I. Holmes and R. S. Forsyth. 1995. The Federalist Revisited: New Directions in Authorship Attribution, Literary and Linguistic Computing, 10(2): 111-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Hoover</author>
</authors>
<title>Statistical stylistics and authorship attribution: an empirical investigation.</title>
<date>2001</date>
<booktitle>Literary and Linguistic Computing</booktitle>
<pages>16--421</pages>
<contexts>
<context position="10306" citStr="Hoover, 2001" startWordPosition="1782" endWordPosition="1783">2004; Argamon et al., 2007) and rewrite rules (Halteren et al., 1996). Length (Gamon 2004; Graham et al., 2005), richness (Halteren et al., 1996; Koppel and Schler, 2004), punctuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sanderson and Guenter, 2006), decision trees (Uzuner and Katz, 2005; Zhao and Zobel, 2005), regularized least squares classification (Narayanan et al., 2012), and SVM (Diederich et al., 2000; Gamon 2004; Koppel and Schler, 2004; Hedegaard and Simonsen, 2011). Among them, SVM was found to be most accurate (Li et al., 2006; Kim et al., 2011). Although we also use supervised learning, we do not learn in the original document space as these existing methods do. The transformation is important because it e</context>
</contexts>
<marker>Hoover, 2001</marker>
<rawString>David L. Hoover. 2001. Statistical stylistics and authorship attribution: an empirical investigation. Literary and Linguistic Computing 16:421-424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
</authors>
<title>Opinion Spam and Analysis.</title>
<date>2008</date>
<booktitle>Proc. of WSDM,</booktitle>
<location>California, USA.</location>
<contexts>
<context position="11317" citStr="Jindal and Liu, 2008" startWordPosition="1948" endWordPosition="1951"> accurate (Li et al., 2006; Kim et al., 2011). Although we also use supervised learning, we do not learn in the original document space as these existing methods do. The transformation is important because it enables us to use documents from other authors in training. The traditional supervised learning (TSL) cannot do that. In our case, the only documents that TSL can use for training are the queries in the testing set. However, as we will see in our experiments, such a method performs poorly. Since we use online reviews as our experiment domain, our work is related to fake review detection (Jindal and Liu, 2008) as imposters can use multiple userids to post fake reviews. Existing research has proposed many methods to detect fake reviewers (Lim et al., 2010; Wang et al., 2011; Mukherjee et al., 2012) and fake reviews (Jindal and Liu, 2008; Ott et al., 2011, 2012; Li et al., 2011; Feng et al., 2012). However, none of them identifies userids belonging to the same person. 3 Learning in the Similarity Space We now formulate the proposed supervised learning in the similarity space (LSS), which will be used in the candid-iden function in our algorithm to be discussed in Section 4. The key difference between</context>
<context position="29291" citStr="Jindal and Liu, 2008" startWordPosition="5113" endWordPosition="5116">ws. We first divide the reviews of each author into two equal subsets. The purpose is to simulate the situation where there are two userids idia and idib from the same author ai. Our objective is that given one userid idia and its query set, we want to find the other userid idib from the same author. For the review subset of idia (or idib), we randomly select 9 reviews as the query set and another 10 reviews as the sample set for the userid. The two sets are disjoint. We don’t use more queries or sample reviews from each author since in the review domain most authors do not have many reviews (Jindal and Liu, 2008). In the experiments, we will vary the number of test userids, the number of queries, and the number of samples. We use the following format to describe each test data: T&lt;n&gt;_Q&lt;n&gt;S&lt;n&gt;, where T denotes the total number of test userids, Q the query set and S the sample set, and &lt;n&gt; a number. For example, T50_Q9S10 stands for a test data with 50 userids, and for each userid, 9 reviews are selected as queries and 10 reviews are selected as samples. * repE N * logteqnd(f(t,d)) Ef(t,q)+Ef(t,d)-Ef(t,d) 1130 resents a wildcard whose value we can vary. Note that we use this “artificial” data rather than</context>
</contexts>
<marker>Jindal, Liu, 2008</marker>
<rawString>Nitin Jindal and Bing Liu. 2008. Opinion Spam and Analysis. Proc. of WSDM, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training linear svms in linear time.</title>
<date>2006</date>
<booktitle>Proc. of KDD.</booktitle>
<contexts>
<context position="27365" citStr="Joachims, 2006" startWordPosition="4769" endWordPosition="4770">, q denotes the query, D is the entire collection, |. |is the size of a set, and idf is the inverse document frequency. These 7 formulae can produce 7 s-features. SimC tf-idf s-feature: This is the cosine similarity used for d-vectors represented by the tf.idf based d-features. SimC tf-idf produces one s-feature. SimC Richness s-feature: This is also cosine similarity. However, it is applied to the richness dfeature vectors, and produces one s-feature. 7 Experimental Evaluation We now evaluate the proposed approach and compare it with baselines. All our experiments use the SVMperf classifier (Joachims, 2006). 7.1 Experiment Setup Experiment Data: We use a set of reviews and their authors/reviewers from Amazon.com as our experiment data. We select the authors who have posted more than 30 reviews in the book category. After cleaning, we have 831 authors, 731 authors for training and 100 authors for testing. The numbers of reviews in the training and test author set are 59256 and 14308, respectively. We use the Stanford parser (Klein and Manning, 2003) to generate the grammar structure of review sentences for extracting syntactic d-features. Note that the authors here are in fact userids. However, s</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Thorsten Joachims. 2006. Training linear svms in linear time. Proc. of KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sangkyum Kim</author>
<author>Hyungsul Kim</author>
<author>Tim Weninger</author>
<author>Jiawei Han</author>
<author>Hyun Duk Kim</author>
</authors>
<title>Authorship classification: a discriminative syntactic tree mining approach.</title>
<date>2011</date>
<booktitle>Proc. of SIGIR,</booktitle>
<pages>455--464</pages>
<contexts>
<context position="10741" citStr="Kim et al., 2011" startWordPosition="1850" endWordPosition="1853">) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sanderson and Guenter, 2006), decision trees (Uzuner and Katz, 2005; Zhao and Zobel, 2005), regularized least squares classification (Narayanan et al., 2012), and SVM (Diederich et al., 2000; Gamon 2004; Koppel and Schler, 2004; Hedegaard and Simonsen, 2011). Among them, SVM was found to be most accurate (Li et al., 2006; Kim et al., 2011). Although we also use supervised learning, we do not learn in the original document space as these existing methods do. The transformation is important because it enables us to use documents from other authors in training. The traditional supervised learning (TSL) cannot do that. In our case, the only documents that TSL can use for training are the queries in the testing set. However, as we will see in our experiments, such a method performs poorly. Since we use online reviews as our experiment domain, our work is related to fake review detection (Jindal and Liu, 2008) as imposters can use mu</context>
</contexts>
<marker>Kim, Kim, Weninger, Han, Kim, 2011</marker>
<rawString>Sangkyum Kim, Hyungsul Kim, Tim Weninger, Jiawei Han, and Hyun Duk Kim. 2011. Authorship classification: a discriminative syntactic tree mining approach. Proc. of SIGIR, Pages 455-464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>Proc. of ACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="27815" citStr="Klein and Manning, 2003" startWordPosition="4843" endWordPosition="4846"> one s-feature. 7 Experimental Evaluation We now evaluate the proposed approach and compare it with baselines. All our experiments use the SVMperf classifier (Joachims, 2006). 7.1 Experiment Setup Experiment Data: We use a set of reviews and their authors/reviewers from Amazon.com as our experiment data. We select the authors who have posted more than 30 reviews in the book category. After cleaning, we have 831 authors, 731 authors for training and 100 authors for testing. The numbers of reviews in the training and test author set are 59256 and 14308, respectively. We use the Stanford parser (Klein and Manning, 2003) to generate the grammar structure of review sentences for extracting syntactic d-features. Note that the authors here are in fact userids. However, since they are randomly selected from a large number of userids, the probability that two sampled userids belong to the same person is very small. Thus, it should be safe to assume that each userid here represents a unique author. Training data: We randomly choose 1 (one) review for each author as the query and all of his/her other reviews as q-positive reviews. The qnegative reviews consist of reviews randomly selected from the other 730 authors,</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein, and Christopher D. Manning. 2003. Accurate unlexicalized parsing. Proc. of ACL, 423-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
</authors>
<title>Authorship verification as a one-class classification problem.</title>
<date>2004</date>
<booktitle>Proc. of ICML.</booktitle>
<contexts>
<context position="9863" citStr="Koppel and Schler, 2004" startWordPosition="1718" endWordPosition="1721"> author has and does not have multiple ids. Our work is also related to authorship attribution (AA). However, to our knowledge, our problem has not been attempted in AA. Existing works focused on two main themes: finding good writing style features, and developing effective classification methods. On finding good features (d-features in our case), it was found that the most promising features are function words (Mosteller, 1964; Argamon and Levitan, 2004; Argamon et al., 2007) and rewrite rules (Halteren et al., 1996). Length (Gamon 2004; Graham et al., 2005), richness (Halteren et al., 1996; Koppel and Schler, 2004), punctuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sanderson and Guenter, 2006), decision trees (Uzuner and Katz, </context>
</contexts>
<marker>Koppel, Schler, 2004</marker>
<rawString>Moshe Koppel and Jonathan Schler. 2004. Authorship verification as a one-class classification problem. Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
<author>Shlomo Argamon</author>
</authors>
<date>2011</date>
<booktitle>Authorship attribution in the wild. Lang Resources &amp; Evaluation,</booktitle>
<pages>45--83</pages>
<marker>Koppel, Schler, Argamon, 2011</marker>
<rawString>Moshe Koppel, Jonathan Schler, Shlomo Argamon. 2011. Authorship attribution in the wild. Lang Resources &amp; Evaluation, 45:83-94</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangtao Li</author>
<author>Minlie Huang</author>
<author>Yi Yang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Learning to identify review Spam.</title>
<date>2011</date>
<booktitle>Proc. of IJCAI.</booktitle>
<contexts>
<context position="11588" citStr="Li et al., 2011" startWordPosition="1997" endWordPosition="2000">itional supervised learning (TSL) cannot do that. In our case, the only documents that TSL can use for training are the queries in the testing set. However, as we will see in our experiments, such a method performs poorly. Since we use online reviews as our experiment domain, our work is related to fake review detection (Jindal and Liu, 2008) as imposters can use multiple userids to post fake reviews. Existing research has proposed many methods to detect fake reviewers (Lim et al., 2010; Wang et al., 2011; Mukherjee et al., 2012) and fake reviews (Jindal and Liu, 2008; Ott et al., 2011, 2012; Li et al., 2011; Feng et al., 2012). However, none of them identifies userids belonging to the same person. 3 Learning in the Similarity Space We now formulate the proposed supervised learning in the similarity space (LSS), which will be used in the candid-iden function in our algorithm to be discussed in Section 4. The key difference between LSS and the classic document space learning is in the document representation. Another difference is in the testing phase. We discuss testing first. Test data: We are given: • A query q from query author (userid) aq • A set of test documents DT = {dt1, ..., dtm}. Goal: </context>
<context position="30225" citStr="Li et al., 2011" startWordPosition="5279" endWordPosition="5282"> for a test data with 50 userids, and for each userid, 9 reviews are selected as queries and 10 reviews are selected as samples. * repE N * logteqnd(f(t,d)) Ef(t,q)+Ef(t,d)-Ef(t,d) 1130 resents a wildcard whose value we can vary. Note that we use this “artificial” data rather than manually labeled data for our experiments because it is very hard to reliably label any gold-standard data manually in this case. The problem is similar to labeling fake reviews. In the fake review detection research, researchers have manually label fake reviews and reviewers (Yoo and Gretzel 2009; Lim et al., 2010; Li et al., 2011; Wang et al., 2011). However, based on the actual fake reviews written using Amazon Mechanical Turk, Ott et al. (2011) have showed that the accuracy of human labeling of fake reviews is very poor. We also believe that our test data is realistic for evaluation as we can image that the two sets of reviews are from two accounts (userids) of the same author (reviewer). Two types of experiments: For each author with two userids, we conduct two types of tests. • Type I: Identify two userids belong to the same author. The experiment runs iteratively to test every userid. In each iteration, we plant </context>
</contexts>
<marker>Li, Huang, Yang, Zhu, 2011</marker>
<rawString>Fangtao Li, Minlie Huang, Yi Yang and Xiaoyan Zhu. 2011. Learning to identify review Spam. Proc. of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
</authors>
<title>Learning to Rank for Information Retrieval and Natural Language Processing.</title>
<date>2011</date>
<publisher>Morgan &amp; Claypool publishers.</publisher>
<contexts>
<context position="7763" citStr="Li, 2011" startWordPosition="1375" endWordPosition="1376">ves us a two-class classification problem. In this formulation, a test userid and his/her documents do not have to be seen in training as long as a set of known documents from this userid is available. Any supervised learning method can be used to build a classifier. We use SVM. The resulting classifier is employed to compute a score for each review to be used in the two-step algorithm above to find the candidate for each userid and then the userids with the same authors. Due to the use of query documents, the LSS formulation has some resemblance to document ranking based on learning to rank (Li, 2011; Liu, 2011). However, LSS is very different because we turn the problem into a supervised classification problem. The key difference between learning to rank and classification is that ranking will always put some documents at the top even if the desired documents do not exist. However, classification will not return any document if the desired documents do not exist in the test data (unless there are classification errors). Our Type II experiments in Section 7 were specifically designed for testing such non-existence situations. 1125 Using online review as the application domain, we conduct </context>
</contexts>
<marker>Li, 2011</marker>
<rawString>Hang Li. 2011. Learning to Rank for Information Retrieval and Natural Language Processing. Morgan &amp; Claypool publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiexun Li</author>
<author>Rong Zheng</author>
<author>Hsinchun Chen</author>
</authors>
<title>From fingerprint to writeprint.</title>
<date>2006</date>
<journal>Communications of the ACM,</journal>
<pages>49--76</pages>
<contexts>
<context position="10722" citStr="Li et al., 2006" startWordPosition="1846" endWordPosition="1849">anan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sanderson and Guenter, 2006), decision trees (Uzuner and Katz, 2005; Zhao and Zobel, 2005), regularized least squares classification (Narayanan et al., 2012), and SVM (Diederich et al., 2000; Gamon 2004; Koppel and Schler, 2004; Hedegaard and Simonsen, 2011). Among them, SVM was found to be most accurate (Li et al., 2006; Kim et al., 2011). Although we also use supervised learning, we do not learn in the original document space as these existing methods do. The transformation is important because it enables us to use documents from other authors in training. The traditional supervised learning (TSL) cannot do that. In our case, the only documents that TSL can use for training are the queries in the testing set. However, as we will see in our experiments, such a method performs poorly. Since we use online reviews as our experiment domain, our work is related to fake review detection (Jindal and Liu, 2008) as i</context>
</contexts>
<marker>Li, Zheng, Chen, 2006</marker>
<rawString>Jiexun Li, Rong Zheng, and Hsinchun Chen. 2006. From fingerprint to writeprint. Communications of the ACM, 49:76-82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ee-Peng Lim</author>
<author>Viet-An Nguyen</author>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
<author>Hady W Lauw</author>
</authors>
<title>Detecting product review spammers using rating behaviors.</title>
<date>2010</date>
<booktitle>Proc. of CIKM,</booktitle>
<contexts>
<context position="11464" citStr="Lim et al., 2010" startWordPosition="1973" endWordPosition="1976">g methods do. The transformation is important because it enables us to use documents from other authors in training. The traditional supervised learning (TSL) cannot do that. In our case, the only documents that TSL can use for training are the queries in the testing set. However, as we will see in our experiments, such a method performs poorly. Since we use online reviews as our experiment domain, our work is related to fake review detection (Jindal and Liu, 2008) as imposters can use multiple userids to post fake reviews. Existing research has proposed many methods to detect fake reviewers (Lim et al., 2010; Wang et al., 2011; Mukherjee et al., 2012) and fake reviews (Jindal and Liu, 2008; Ott et al., 2011, 2012; Li et al., 2011; Feng et al., 2012). However, none of them identifies userids belonging to the same person. 3 Learning in the Similarity Space We now formulate the proposed supervised learning in the similarity space (LSS), which will be used in the candid-iden function in our algorithm to be discussed in Section 4. The key difference between LSS and the classic document space learning is in the document representation. Another difference is in the testing phase. We discuss testing firs</context>
<context position="30208" citStr="Lim et al., 2010" startWordPosition="5275" endWordPosition="5278">, T50_Q9S10 stands for a test data with 50 userids, and for each userid, 9 reviews are selected as queries and 10 reviews are selected as samples. * repE N * logteqnd(f(t,d)) Ef(t,q)+Ef(t,d)-Ef(t,d) 1130 resents a wildcard whose value we can vary. Note that we use this “artificial” data rather than manually labeled data for our experiments because it is very hard to reliably label any gold-standard data manually in this case. The problem is similar to labeling fake reviews. In the fake review detection research, researchers have manually label fake reviews and reviewers (Yoo and Gretzel 2009; Lim et al., 2010; Li et al., 2011; Wang et al., 2011). However, based on the actual fake reviews written using Amazon Mechanical Turk, Ott et al. (2011) have showed that the accuracy of human labeling of fake reviews is very poor. We also believe that our test data is realistic for evaluation as we can image that the two sets of reviews are from two accounts (userids) of the same author (reviewer). Two types of experiments: For each author with two userids, we conduct two types of tests. • Type I: Identify two userids belong to the same author. The experiment runs iteratively to test every userid. In each ite</context>
</contexts>
<marker>Lim, Nguyen, Jindal, Liu, Lauw, 2010</marker>
<rawString>Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu, Hady W. Lauw. 2010. Detecting product review spammers using rating behaviors. Proc. of CIKM, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining,</title>
<date>2012</date>
<pages>1134</pages>
<publisher>Morgan &amp; Claypool</publisher>
<contexts>
<context position="1738" citStr="Liu, 2012" startWordPosition="286" endWordPosition="287">Introduction It is common knowledge that some users in social media register multiple accounts/userids to post articles, blogs, reviews, etc. There are many reasons for doing this. For example, due to past postings, a user may become despised by others. He/she then registers another userid in order to regain his/her status. A user may also use multiple userids to instigate controversy or debates to popularize a topic to make it “hot” or even just to promote activities at a website. Yet, a user may also use multiple userids to post fake or deceptive opinions to promote or demote some products (Liu, 2012). It is thus important to develop technologies * The work was mainly done when the first author was visiting the University of Illinois at Chicago. to identify such multi-id users. This paper deals with this problem based on writing style and other linguistic clues. Problem definition: Given a set of userids ID = {id1, ..., idn} and each idi has a set of documents Di, we want to identify userids that belong to the same physical author. The main related works to ours are in the area of authorship attribution (AA), which aims to identify authors of documents. AA is often solved using supervised </context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment Analysis and Opinion Mining, Morgan &amp; Claypool publishers. 1134</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tieyan Liu</author>
</authors>
<title>Learning to Rank for Information Retrieval.</title>
<date>2011</date>
<publisher>Springer.</publisher>
<contexts>
<context position="7775" citStr="Liu, 2011" startWordPosition="1377" endWordPosition="1378">wo-class classification problem. In this formulation, a test userid and his/her documents do not have to be seen in training as long as a set of known documents from this userid is available. Any supervised learning method can be used to build a classifier. We use SVM. The resulting classifier is employed to compute a score for each review to be used in the two-step algorithm above to find the candidate for each userid and then the userids with the same authors. Due to the use of query documents, the LSS formulation has some resemblance to document ranking based on learning to rank (Li, 2011; Liu, 2011). However, LSS is very different because we turn the problem into a supervised classification problem. The key difference between learning to rank and classification is that ranking will always put some documents at the top even if the desired documents do not exist. However, classification will not return any document if the desired documents do not exist in the test data (unless there are classification errors). Our Type II experiments in Section 7 were specifically designed for testing such non-existence situations. 1125 Using online review as the application domain, we conduct experiments </context>
</contexts>
<marker>Liu, 2011</marker>
<rawString>Tieyan Liu. 2011. Learning to Rank for Information Retrieval. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kim Luyckx</author>
<author>Walter Daelemans</author>
</authors>
<date>2008</date>
<booktitle>Authorship Attribution and Verification with Many Authors and Limited Data. Proc. of Coling,</booktitle>
<pages>513--520</pages>
<marker>Luyckx, Daelemans, 2008</marker>
<rawString>Kim Luyckx, Walter Daelemans. 2008. Authorship Attribution and Verification with Many Authors and Limited Data. Proc. of Coling, pages 513-520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Madigan</author>
<author>Alexander Genkin</author>
<author>David D Lewis</author>
<author>Shlomo Argamon</author>
<author>Dmitriy Fradkin</author>
<author>Li Ye</author>
</authors>
<title>Author Identification on the Large Scale.</title>
<date>2005</date>
<booktitle>Proc. of CSNA.</booktitle>
<marker>Madigan, Genkin, Lewis, Argamon, Fradkin, Ye, 2005</marker>
<rawString>David Madigan, Alexander Genkin, David D. Lewis, Shlomo Argamon, Dmitriy Fradkin, and Li Ye. 2005. Author Identification on the Large Scale. Proc. of CSNA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Metzler</author>
<author>Yaniv Bernstein</author>
<author>W Bruce Croft</author>
<author>Alistair Moffat</author>
<author>Justin Zobel</author>
</authors>
<title>Similarity measures for tracking information flow.</title>
<date>2005</date>
<booktitle>Proc. of CIKM.</booktitle>
<pages>517--524</pages>
<contexts>
<context position="25833" citStr="Metzler et al., 2005" startWordPosition="4485" endWordPosition="4488"> functions defined by us. They are used for dfeature vectors of length. The four formulae are given in Table 2, where lwq. (lwd), lsq. (lsd), and lrq. (lrd) denote the average word, sentence, and document length respectively, either in query q or nonquery document d. They produce four s-features. Table 2. Sim4 for computing length s-features m w s r  { , , } 1/ (1 + log(1+ |&amp;quot; ,�  &amp;quot;,d |)) m  1/ (1 + log(1+ |lsq  lsd |)) w s r (lmq*ld)/ (lmq)2*(lmd)2 { , , }  { , , } m w s r 1/ (1 + log(1+ |l,E  l,d |)) 1129 Sim3 Sentence s-features: This is a set of three sentence similarity functions (Metzler et al., 2005). We apply them (called Sim3) to documents. Sim3 s-features are used for frequency based d-features. The three formulae are given in Table 3, where f(t, s) is the frequency count of token t in a document s, and lq and ld are the average document length of the query and non-query document, respectively. Table 3. Sim3 for computing sentence s-features t q d e n f (t , d) teq ted t q d e n Sim7 Retrieval s-features: This is a set of seven similarity functions (Table 4) applicable to all frequency based d-features. These functions were used in information retrieval (Cao et al., 2006). Table 4. Sim</context>
</contexts>
<marker>Metzler, Bernstein, Croft, Moffat, Zobel, 2005</marker>
<rawString>Donald Metzler, Yaniv Bernstein, W. Bruce Croft, Alistair Moffat, and Justin Zobel. 2005. Similarity measures for tracking information flow. Proc. of CIKM. Pages 517-524.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Mosteller</author>
<author>David Lee Wallace</author>
</authors>
<title>Inference and disputed authorship: The Federalist.</title>
<date>1964</date>
<publisher>Addison-Wesley.</publisher>
<marker>Mosteller, Wallace, 1964</marker>
<rawString>Frederick Mosteller, David Lee Wallace. 1964. Inference and disputed authorship: The Federalist. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjun Mukherjee</author>
<author>Bing Liu</author>
<author>Natalie Glance</author>
</authors>
<title>Spotting Fake Reviewer Groups in Consumer Reviews.</title>
<date>2012</date>
<booktitle>Proc. of WWW,</booktitle>
<pages>191--200</pages>
<contexts>
<context position="11508" citStr="Mukherjee et al., 2012" startWordPosition="1981" endWordPosition="1984">mportant because it enables us to use documents from other authors in training. The traditional supervised learning (TSL) cannot do that. In our case, the only documents that TSL can use for training are the queries in the testing set. However, as we will see in our experiments, such a method performs poorly. Since we use online reviews as our experiment domain, our work is related to fake review detection (Jindal and Liu, 2008) as imposters can use multiple userids to post fake reviews. Existing research has proposed many methods to detect fake reviewers (Lim et al., 2010; Wang et al., 2011; Mukherjee et al., 2012) and fake reviews (Jindal and Liu, 2008; Ott et al., 2011, 2012; Li et al., 2011; Feng et al., 2012). However, none of them identifies userids belonging to the same person. 3 Learning in the Similarity Space We now formulate the proposed supervised learning in the similarity space (LSS), which will be used in the candid-iden function in our algorithm to be discussed in Section 4. The key difference between LSS and the classic document space learning is in the document representation. Another difference is in the testing phase. We discuss testing first. Test data: We are given: • A query q from</context>
</contexts>
<marker>Mukherjee, Liu, Glance, 2012</marker>
<rawString>Arjun Mukherjee, Bing Liu, and Natalie Glance. 2012. Spotting Fake Reviewer Groups in Consumer Reviews. Proc. of WWW, Pages 191-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arvind Narayanan</author>
<author>Hristo Paskov</author>
<author>Neil Zhenqiang Gong</author>
</authors>
<title>On the feasibility of internetscale author identification.</title>
<date>2012</date>
<booktitle>Proceedings of the 2012 IEEE Symposium on Security and Privacy.</booktitle>
<pages>300--314</pages>
<contexts>
<context position="10125" citStr="Narayanan et al., 2012" startWordPosition="1755" endWordPosition="1759">g effective classification methods. On finding good features (d-features in our case), it was found that the most promising features are function words (Mosteller, 1964; Argamon and Levitan, 2004; Argamon et al., 2007) and rewrite rules (Halteren et al., 1996). Length (Gamon 2004; Graham et al., 2005), richness (Halteren et al., 1996; Koppel and Schler, 2004), punctuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sanderson and Guenter, 2006), decision trees (Uzuner and Katz, 2005; Zhao and Zobel, 2005), regularized least squares classification (Narayanan et al., 2012), and SVM (Diederich et al., 2000; Gamon 2004; Koppel and Schler, 2004; Hedegaard and Simonsen, 2011). Among them, SVM was found to be most accurate (Li et al., 2006; K</context>
</contexts>
<marker>Narayanan, Paskov, Gong, 2012</marker>
<rawString>Arvind Narayanan, Hristo Paskov, Neil Zhenqiang Gong, et al. 2012. On the feasibility of internetscale author identification. Proceedings of the 2012 IEEE Symposium on Security and Privacy. Pages 300-314</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jasmine Novak</author>
<author>Prabhakar Raghavan</author>
<author>Andrew Tomkins</author>
</authors>
<title>Anti-aliasing on the web.</title>
<date>2004</date>
<booktitle>Proc. of WWW,</booktitle>
<pages>30--39</pages>
<contexts>
<context position="9084" citStr="Novak et al., 2004" startWordPosition="1586" endWordPosition="1589">s show that the proposed algorithm is highly accurate and outperforms three strong baselines markedly. 2 Related Work A similar problem was attempted in (Chen et al., 2004) in the context of open forums where users interact with each other in their discussions. Their method is based on post relationships and intervals between posts. It does not use any linguistic clues. It is thus not applicable to domains like online reviews. Reviews do not involve user interactions since each review is independent of other reviews. Novak et al. also solved the same problem under the name of “Anti-aliasing” (Novak et al., 2004). They used a clustering based method which assumed the number of actual authors is known. This is unrealistic in practice as there is no way to know which author has and does not have multiple ids. Our work is also related to authorship attribution (AA). However, to our knowledge, our problem has not been attempted in AA. Existing works focused on two main themes: finding good writing style features, and developing effective classification methods. On finding good features (d-features in our case), it was found that the most promising features are function words (Mosteller, 1964; Argamon and </context>
<context position="32429" citStr="Novak et al., 2004" startWordPosition="5680" endWordPosition="5683">d(i-1)a, id(i+1)a, ..., idma} and their sample review sets {S1a, ..., S(i-1)a, S(i+1)a, ..., Sma}. idib is not planted. Evaluation measure: Here we cannot use precision and recall because we are not trying to find any pairs. We thus use accuracy as our measure. For each idi, if no pair is found, it is correct. If a pair is found, it is wrong. Baseline methods: As mentioned eariler, there are only two works that tried to identify multi-id users. The first is that in (Chen et al., 2004). However, as we discussed in related work, their approach is not applicable to reviews. The other is that in (Novak et al., 2004), which used clustering but assumed that the number of actual authors (or clusters) is known. This is unrealistic in practice. Thus we designed three new baselines: TSL: This baseline is based on the traditional supervised learning (TSL). We use it to evaluate how the traditional approach performs in the original feature space. In this case, each document in TSL has to be represented as a vector of d-features or traditional n-gram features. For each test userid id, we build a SVM classifier based on the one vs. all strategy. That is, for training we use id’s queries in T*_Q*S10 as the positive</context>
</contexts>
<marker>Novak, Raghavan, Tomkins, 2004</marker>
<rawString>Jasmine Novak, Prabhakar Raghavan, Andrew Tomkins. 2004. Anti-aliasing on the web. Proc. of WWW, Pages 30-39</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myle Ott</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Jeffrey T Hancock</author>
</authors>
<title>Finding Deceptive Opinion Spam by Any Stretch of the Imagination.</title>
<date>2011</date>
<booktitle>Proc. of ACL.</booktitle>
<contexts>
<context position="11565" citStr="Ott et al., 2011" startWordPosition="1992" endWordPosition="1995">rs in training. The traditional supervised learning (TSL) cannot do that. In our case, the only documents that TSL can use for training are the queries in the testing set. However, as we will see in our experiments, such a method performs poorly. Since we use online reviews as our experiment domain, our work is related to fake review detection (Jindal and Liu, 2008) as imposters can use multiple userids to post fake reviews. Existing research has proposed many methods to detect fake reviewers (Lim et al., 2010; Wang et al., 2011; Mukherjee et al., 2012) and fake reviews (Jindal and Liu, 2008; Ott et al., 2011, 2012; Li et al., 2011; Feng et al., 2012). However, none of them identifies userids belonging to the same person. 3 Learning in the Similarity Space We now formulate the proposed supervised learning in the similarity space (LSS), which will be used in the candid-iden function in our algorithm to be discussed in Section 4. The key difference between LSS and the classic document space learning is in the document representation. Another difference is in the testing phase. We discuss testing first. Test data: We are given: • A query q from query author (userid) aq • A set of test documents DT = </context>
<context position="30344" citStr="Ott et al. (2011)" startWordPosition="5299" endWordPosition="5302">as samples. * repE N * logteqnd(f(t,d)) Ef(t,q)+Ef(t,d)-Ef(t,d) 1130 resents a wildcard whose value we can vary. Note that we use this “artificial” data rather than manually labeled data for our experiments because it is very hard to reliably label any gold-standard data manually in this case. The problem is similar to labeling fake reviews. In the fake review detection research, researchers have manually label fake reviews and reviewers (Yoo and Gretzel 2009; Lim et al., 2010; Li et al., 2011; Wang et al., 2011). However, based on the actual fake reviews written using Amazon Mechanical Turk, Ott et al. (2011) have showed that the accuracy of human labeling of fake reviews is very poor. We also believe that our test data is realistic for evaluation as we can image that the two sets of reviews are from two accounts (userids) of the same author (reviewer). Two types of experiments: For each author with two userids, we conduct two types of tests. • Type I: Identify two userids belong to the same author. The experiment runs iteratively to test every userid. In each iteration, we plant one userid of an author in the test set and use the other userid of the same author as the query userid. That is, in th</context>
</contexts>
<marker>Ott, Choi, Cardie, Hancock, 2011</marker>
<rawString>Myle Ott, Yejin Choi, Claire Cardie, Jeffrey T. Hancock. 2011. Finding Deceptive Opinion Spam by Any Stretch of the Imagination. Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myle Ott</author>
<author>Claire Cardie</author>
<author>Jeffrey T Hancock</author>
</authors>
<title>Estimating the prevalence of deception in online review communities.</title>
<date>2012</date>
<booktitle>Proc. of WWW.</booktitle>
<marker>Ott, Cardie, Hancock, 2012</marker>
<rawString>Myle Ott, Claire Cardie, Jeffrey T. Hancock. 2012. Estimating the prevalence of deception in online review communities. Proc. of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Dale Schuurmans</author>
<author>Shaojun Wang</author>
<author>Vlado Keselj</author>
</authors>
<title>Language independent authorship attribution using character level language models.</title>
<date>2003</date>
<booktitle>Proc. of EACL,</booktitle>
<pages>267--274</pages>
<marker>Peng, Schuurmans, Wang, Keselj, 2003</marker>
<rawString>Fuchun Peng, Dale Schuurmans, Shaojun Wang, and Vlado Keselj. 2003. Language independent authorship attribution using character level language models. Proc. of EACL, Pages 267-274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Conrad Sanderson</author>
<author>Simon Guenter</author>
</authors>
<title>Short text authorship attribution via sequence kernels, markov chains and author unmasking: an investigation.</title>
<date>2006</date>
<booktitle>Proc. of EMNLP,</booktitle>
<pages>482--491</pages>
<contexts>
<context position="10021" citStr="Sanderson and Guenter 2006" startWordPosition="1740" endWordPosition="1743">tempted in AA. Existing works focused on two main themes: finding good writing style features, and developing effective classification methods. On finding good features (d-features in our case), it was found that the most promising features are function words (Mosteller, 1964; Argamon and Levitan, 2004; Argamon et al., 2007) and rewrite rules (Halteren et al., 1996). Length (Gamon 2004; Graham et al., 2005), richness (Halteren et al., 1996; Koppel and Schler, 2004), punctuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sanderson and Guenter, 2006), decision trees (Uzuner and Katz, 2005; Zhao and Zobel, 2005), regularized least squares classification (Narayanan et al., 2012), and SVM (Diederich et al., 2000; Gamon 2004; Koppel and Schler</context>
</contexts>
<marker>Sanderson, Guenter, 2006</marker>
<rawString>Conrad Sanderson and Simon Guenter. 2006. Short text authorship attribution via sequence kernels, markov chains and author unmasking: an investigation. Proc. of EMNLP, Pages 482-491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanir Seroussi</author>
<author>Fabian Bohnert</author>
<author>Ingrid Zukerman</author>
</authors>
<title>Authorship Attribution with Authoraware Topic Models.</title>
<date>2012</date>
<booktitle>Proc. of ACL,</booktitle>
<pages>2--264</pages>
<marker>Seroussi, Bohnert, Zukerman, 2012</marker>
<rawString>Yanir Seroussi, Fabian Bohnert, Ingrid Zukerman. 2012. Authorship Attribution with Authoraware Topic Models. Proc. of ACL, 2:264-269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thamar Solorio</author>
</authors>
<title>Sangita Pillay, Sindhu Raghavan, Manuel Montes y G´omez.</title>
<date>2011</date>
<booktitle>Proc. of IJCNLP,</booktitle>
<pages>156--164</pages>
<marker>Solorio, 2011</marker>
<rawString>Thamar Solorio, Sangita Pillay, Sindhu Raghavan, Manuel Montes y G´omez. 2011. Modality Specific Meta Features for Authorship Attribution in Web Forum Posts. Proc. of IJCNLP, Pages 156-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efstathios Stamatatos</author>
</authors>
<title>A Survey of Modern Authorship Attribution Methods.</title>
<date>2009</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<pages>60--3</pages>
<publisher>Wiley.</publisher>
<marker>Stamatatos, 2009</marker>
<rawString>Efstathios Stamatatos. 2009. A Survey of Modern Authorship Attribution Methods. Journal of the American Society for Information Science and Technology, 60(3):538-556, Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efstathios Stamatatos</author>
<author>George Kokkinakis</author>
<author>Nikos Fakotakis</author>
</authors>
<title>Automatic text categorization in terms of genre and author.</title>
<date>2000</date>
<journal>Comput. Linguist.</journal>
<pages>26--471</pages>
<contexts>
<context position="10286" citStr="Stamatatos et al., 2000" startWordPosition="1777" endWordPosition="1780">er, 1964; Argamon and Levitan, 2004; Argamon et al., 2007) and rewrite rules (Halteren et al., 1996). Length (Gamon 2004; Graham et al., 2005), richness (Halteren et al., 1996; Koppel and Schler, 2004), punctuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sanderson and Guenter, 2006), decision trees (Uzuner and Katz, 2005; Zhao and Zobel, 2005), regularized least squares classification (Narayanan et al., 2012), and SVM (Diederich et al., 2000; Gamon 2004; Koppel and Schler, 2004; Hedegaard and Simonsen, 2011). Among them, SVM was found to be most accurate (Li et al., 2006; Kim et al., 2011). Although we also use supervised learning, we do not learn in the original document space as these existing methods do. The transformation is im</context>
</contexts>
<marker>Stamatatos, Kokkinakis, Fakotakis, 2000</marker>
<rawString>Efstathios Stamatatos, George Kokkinakis, and Nikos Fakotakis. 2000. Automatic text categorization in terms of genre and author. Comput. Linguist. 26:471-495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Özlem Uzuner</author>
<author>Boris Katz</author>
</authors>
<title>A comparative study of language models for book and author recognition.</title>
<date>2005</date>
<booktitle>Proc. of IJCNLP,</booktitle>
<pages>969--980</pages>
<contexts>
<context position="10467" citStr="Uzuner and Katz, 2005" startWordPosition="1805" endWordPosition="1808">and Schler, 2004), punctuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sanderson and Guenter, 2006), decision trees (Uzuner and Katz, 2005; Zhao and Zobel, 2005), regularized least squares classification (Narayanan et al., 2012), and SVM (Diederich et al., 2000; Gamon 2004; Koppel and Schler, 2004; Hedegaard and Simonsen, 2011). Among them, SVM was found to be most accurate (Li et al., 2006; Kim et al., 2011). Although we also use supervised learning, we do not learn in the original document space as these existing methods do. The transformation is important because it enables us to use documents from other authors in training. The traditional supervised learning (TSL) cannot do that. In our case, the only documents that TSL can</context>
</contexts>
<marker>Uzuner, Katz, 2005</marker>
<rawString>Özlem Uzuner and Boris Katz. 2005. A comparative study of language models for book and author recognition. Proc. of IJCNLP, Pages 969-980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<location>Wiley-Interscience, NY.</location>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. 1998. Statistical Learning Theory. Wiley-Interscience, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O de Vel</author>
<author>A Anderson</author>
<author>M Corney</author>
<author>G Mohay</author>
</authors>
<title>Mining Email Content for Author Identification Forensics. Sigmod Record,</title>
<date>2001</date>
<pages>30--55</pages>
<marker>de Vel, Anderson, Corney, Mohay, 2001</marker>
<rawString>O. de Vel, A. Anderson, M. Corney and G. Mohay. 2001. Mining Email Content for Author Identification Forensics. Sigmod Record, 30:55-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyung-Hyan Yoo</author>
<author>Ulrike Gretzel</author>
</authors>
<title>Comparison of Deceptive and Truthful Travel Reviews. Information and Communication Technologies in Tourism,</title>
<date>2009</date>
<pages>37--47</pages>
<contexts>
<context position="30190" citStr="Yoo and Gretzel 2009" startWordPosition="5271" endWordPosition="5274"> a number. For example, T50_Q9S10 stands for a test data with 50 userids, and for each userid, 9 reviews are selected as queries and 10 reviews are selected as samples. * repE N * logteqnd(f(t,d)) Ef(t,q)+Ef(t,d)-Ef(t,d) 1130 resents a wildcard whose value we can vary. Note that we use this “artificial” data rather than manually labeled data for our experiments because it is very hard to reliably label any gold-standard data manually in this case. The problem is similar to labeling fake reviews. In the fake review detection research, researchers have manually label fake reviews and reviewers (Yoo and Gretzel 2009; Lim et al., 2010; Li et al., 2011; Wang et al., 2011). However, based on the actual fake reviews written using Amazon Mechanical Turk, Ott et al. (2011) have showed that the accuracy of human labeling of fake reviews is very poor. We also believe that our test data is realistic for evaluation as we can image that the two sets of reviews are from two accounts (userids) of the same author (reviewer). Two types of experiments: For each author with two userids, we conduct two types of tests. • Type I: Identify two userids belong to the same author. The experiment runs iteratively to test every u</context>
</contexts>
<marker>Yoo, Gretzel, 2009</marker>
<rawString>Kyung-Hyan Yoo and Ulrike Gretzel. 2009. Comparison of Deceptive and Truthful Travel Reviews. Information and Communication Technologies in Tourism, Pages 37-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgy Udnv Yule</author>
</authors>
<title>The statistical study of literary vocabulary.</title>
<date>1944</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="24522" citStr="Yule, 1944" startWordPosition="4232" endWordPosition="4233">s d-features: This is a set of vocabulary richness functions used to quantify the diversity of vocabulary in text (Holmes and Forsyth, 1995). In this paper, we apply them to the counts of word unigrams, POS n-grams (1 &lt; n &lt; 3), and rewrite rules. Here POS n-grams and rewrite rules are treated as pseudo-words. Let T be the total number of tokens (words or pseudo-words), and V(T) be the number of different tokens in a document, v be the highest frequency of occurrence of a token, and V(m, T) be the number of tokens which occur m times in the document. We use the following six richness measures (Yule, 1944; Burrows, 1992; Halteren et al., 1996) given in Table 1: Yule’s characteristic (K), Hapax dislegomena (S), Simpson’s index (D), Honorës measure (R), Brunet’s measure (W), and Hapax legomena (H). They give us a set of richness d-features about word unigrams, POS n-grams, and rewrite rules. Table 1. Richness metrics K 104* m1 m m - V m T  ( *( 1)* ( , )) m  1 D  T *( T-1) E(m2*V(m,T)T) S=V(2&apos;T)  V(T) T2 v R= 100*log(T) 1-V(l, T)/V(T) H=V(1,T) W=T&amp;quot;(T)-&apos;,a=0.17 v 6 S-Features The extracted d-features are transformed into sfeatures, which are a set of similarity functions on two documents. W</context>
</contexts>
<marker>Yule, 1944</marker>
<rawString>Georgy Udnv Yule. 1944. The statistical study of literary vocabulary. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guan Wang</author>
<author>Sihong Xie</author>
<author>Bing Liu</author>
<author>Philip S Yu</author>
</authors>
<title>Review Graph based Online Store Review Spammer Detection.</title>
<date>2011</date>
<booktitle>Proc. of ICDM.</booktitle>
<contexts>
<context position="11483" citStr="Wang et al., 2011" startWordPosition="1977" endWordPosition="1980">transformation is important because it enables us to use documents from other authors in training. The traditional supervised learning (TSL) cannot do that. In our case, the only documents that TSL can use for training are the queries in the testing set. However, as we will see in our experiments, such a method performs poorly. Since we use online reviews as our experiment domain, our work is related to fake review detection (Jindal and Liu, 2008) as imposters can use multiple userids to post fake reviews. Existing research has proposed many methods to detect fake reviewers (Lim et al., 2010; Wang et al., 2011; Mukherjee et al., 2012) and fake reviews (Jindal and Liu, 2008; Ott et al., 2011, 2012; Li et al., 2011; Feng et al., 2012). However, none of them identifies userids belonging to the same person. 3 Learning in the Similarity Space We now formulate the proposed supervised learning in the similarity space (LSS), which will be used in the candid-iden function in our algorithm to be discussed in Section 4. The key difference between LSS and the classic document space learning is in the document representation. Another difference is in the testing phase. We discuss testing first. Test data: We ar</context>
<context position="30245" citStr="Wang et al., 2011" startWordPosition="5283" endWordPosition="5286">with 50 userids, and for each userid, 9 reviews are selected as queries and 10 reviews are selected as samples. * repE N * logteqnd(f(t,d)) Ef(t,q)+Ef(t,d)-Ef(t,d) 1130 resents a wildcard whose value we can vary. Note that we use this “artificial” data rather than manually labeled data for our experiments because it is very hard to reliably label any gold-standard data manually in this case. The problem is similar to labeling fake reviews. In the fake review detection research, researchers have manually label fake reviews and reviewers (Yoo and Gretzel 2009; Lim et al., 2010; Li et al., 2011; Wang et al., 2011). However, based on the actual fake reviews written using Amazon Mechanical Turk, Ott et al. (2011) have showed that the accuracy of human labeling of fake reviews is very poor. We also believe that our test data is realistic for evaluation as we can image that the two sets of reviews are from two accounts (userids) of the same author (reviewer). Two types of experiments: For each author with two userids, we conduct two types of tests. • Type I: Identify two userids belong to the same author. The experiment runs iteratively to test every userid. In each iteration, we plant one userid of an aut</context>
</contexts>
<marker>Wang, Xie, Liu, Yu, 2011</marker>
<rawString>Guan Wang, Sihong Xie, Bing Liu, Philip S. Yu. 2011. Review Graph based Online Store Review Spammer Detection. Proc. of ICDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhao</author>
<author>Justin Zobel</author>
</authors>
<title>Effective and scalable authorship attribution using function words.</title>
<date>2005</date>
<booktitle>Proceeding of Information Retrival Technology,</booktitle>
<pages>174--189</pages>
<contexts>
<context position="10490" citStr="Zhao and Zobel, 2005" startWordPosition="1809" endWordPosition="1812">tuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sanderson and Guenter, 2006), decision trees (Uzuner and Katz, 2005; Zhao and Zobel, 2005), regularized least squares classification (Narayanan et al., 2012), and SVM (Diederich et al., 2000; Gamon 2004; Koppel and Schler, 2004; Hedegaard and Simonsen, 2011). Among them, SVM was found to be most accurate (Li et al., 2006; Kim et al., 2011). Although we also use supervised learning, we do not learn in the original document space as these existing methods do. The transformation is important because it enables us to use documents from other authors in training. The traditional supervised learning (TSL) cannot do that. In our case, the only documents that TSL can use for training are t</context>
</contexts>
<marker>Zhao, Zobel, 2005</marker>
<rawString>Ying Zhao and Justin Zobel. 2005. Effective and scalable authorship attribution using function words. Proceeding of Information Retrival Technology, Pages 174-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong Zheng</author>
<author>Jiexun Li</author>
<author>Hsinchun Chen</author>
<author>Zan Huang</author>
</authors>
<title>A framework for authorship identification of online messages: Writing style features and classification techniques.</title>
<date>2006</date>
<journal>Journal of the American Society of Information Science and Technology</journal>
<pages>57--378</pages>
<contexts>
<context position="10364" citStr="Zheng et al., 2006" startWordPosition="1790" endWordPosition="1793">en et al., 1996). Length (Gamon 2004; Graham et al., 2005), richness (Halteren et al., 1996; Koppel and Schler, 2004), punctuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sanderson and Guenter, 2006), decision trees (Uzuner and Katz, 2005; Zhao and Zobel, 2005), regularized least squares classification (Narayanan et al., 2012), and SVM (Diederich et al., 2000; Gamon 2004; Koppel and Schler, 2004; Hedegaard and Simonsen, 2011). Among them, SVM was found to be most accurate (Li et al., 2006; Kim et al., 2011). Although we also use supervised learning, we do not learn in the original document space as these existing methods do. The transformation is important because it enables us to use documents from other authors in training.</context>
</contexts>
<marker>Zheng, Li, Chen, Huang, 2006</marker>
<rawString>Rong Zheng, Jiexun Li, Hsinchun Chen, and Zan Huang. 2006. A framework for authorship identification of online messages: Writing style features and classification techniques. Journal of the American Society of Information Science and Technology 57:378-393.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
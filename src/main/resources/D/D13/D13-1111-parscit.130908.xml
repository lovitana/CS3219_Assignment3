<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000187">
<title confidence="0.99874">
A Systematic Exploration of Diversity in Machine Translation
</title>
<author confidence="0.993691">
Kevin Gimpel* Dhruv Batra† Chris Dyer$ Gregory Shakhnarovich*
</author>
<affiliation confidence="0.899533">
*Toyota Technological Institute at Chicago, Chicago, IL 60637, USA
†Virginia Tech, Blacksburg, VA 24061, USA
$Carnegie Mellon University, Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.984256">
Corresponding author: kgimpel@ttic.edu
</email>
<sectionHeader confidence="0.993613" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99960725">
This paper addresses the problem of produc-
ing a diverse set of plausible translations. We
present a simple procedure that can be used
with any statistical machine translation (MT)
system. We explore three ways of using di-
verse translations: (1) system combination,
(2) discriminative reranking with rich features,
and (3) a novel post-editing scenario in which
multiple translations are presented to users.
We find that diversity can improve perfor-
mance on these tasks, especially for sentences
that are difficult for MT.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999867">
From the perspective of user interaction, the ideal
machine translator is an agent that reads documents
in one language and produces accurate, high qual-
ity translations in another. This interaction ideal
has been implicit in machine translation (MT) re-
search since the field’s inception. It is the way
we interact with commercial MT services (such as
Google Translate and Microsoft Translator), and the
way MT systems are evaluated (Bojar et al., 2013).
Unfortunately, when a real, imperfect MT system
makes an error, the user is left trying to guess what
the original sentence means.
Multiple Hypotheses. In contrast, when we look
at the way other computer systems consume out-
put from MT systems (or similarly unreliable tools),
we see a different pattern. In a pipeline setting
it is commonplace to propagate not just a single-
best output but the M-best hypotheses (Venugopal
et al., 2008). Multiple solutions are also used for
reranking (Collins, 2000; Shen and Joshi, 2003;
Collins and Koo, 2005; Charniak and Johnson,
2005), tuning (Och, 2003), minimum Bayes risk de-
coding (Kumar and Byrne, 2004), and system com-
bination (Rosti et al., 2007). When dealing with
error-prone systems, knowing about alternatives has
benefits over relying on only a single output (Finkel
et al., 2006; Dyer, 2010).
Need for Diversity. Unfortunately, M-best lists are
a poor surrogate for structured output spaces (Finkel
et al., 2006; Huang, 2008). In MT, for exam-
ple, many translations on M-best lists are extremely
similar, often differing only by a single punctua-
tion mark or minor morphological variation. Re-
cent work has explored reasoning about sets using
packed representations such as lattices and hyper-
graphs (Macherey et al., 2008; Tromble et al., 2008;
Kumar et al., 2009), or sampling translations propor-
tional to their probability (Chatterjee and Cancedda,
2010). We argue that the implicit goal behind these
techniques is to better explore the output space by
introducing diversity into the surrogate set.
Overview and Contributions. In this work, we el-
evate diversity to a first-class status and directly ad-
dress the problem of generating a set of diverse,
plausible translations. We use the recently pro-
posed technique of Batra et al. (2012), which pro-
duces diverse M-best solutions from a probabilistic
model using a generic dissimilarity function A(, ·)
that specifies how two solutions differ. Our first con-
tribution is a family of dissimilarity functions for
MT that admit simple algorithms for generating di-
verse translations. Other contributions are empiri-
cal: we show that diverse translations can lead to
improvements for system combination and discrim-
inative reranking. We also perform a novel human
</bodyText>
<page confidence="0.927641">
1100
</page>
<note confidence="0.7308485">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1100–1111,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.998548">
post-editing evaluation in order to measure whether
diverse translations can help users make sense of
noisy MT output. We find that diverse translations
can help post-editors produce better outputs for sen-
tences that are the most difficult for MT. While we
focus on machine translation in this paper, we note
that our approach is applicable to other structure pre-
diction problems in NLP.
</bodyText>
<sectionHeader confidence="0.895225" genericHeader="introduction">
2 Preliminaries and Notation
</sectionHeader>
<bodyText confidence="0.999977214285714">
Let X denote the set of all strings in a source lan-
guage. For an x E X, let �x denote the set of its pos-
sible translations y in the target language. MT mod-
els typically include a latent variable that captures
the derivational structure of the translation process.
Regardless of its specific form, we refer to this vari-
able as a derivation h E xx, where xx is the set of
possible values of h for x. Derivations are coupled
with translations and we define Tx C �x x xx as
the set of possible (y, h) pairs for x.
We use a linear model with a parameter vector w
and a vector O(x, y, h) of feature functions on x, y,
and h (Och and Ney, 2002). The translation of x is
selected using a simple decision rule:
</bodyText>
<equation confidence="0.9522285">
(ˆy, ˆh) = argmax wTO(x, y, h) (1)
(y,h)ET.
</equation>
<bodyText confidence="0.999567428571429">
where we also maximize over the latent variable h
for efficiency. Translation models differ in the form
of Tx and the choice of the feature functions O. In
this paper we focus on phrase-based (Koehn et al.,
2003) and hierarchical phrase-based (Chiang, 2007)
models, which include several bilingual and mono-
lingual features, including n-gram language models.
</bodyText>
<sectionHeader confidence="0.992645" genericHeader="method">
3 Diversity in Machine Translation
</sectionHeader>
<bodyText confidence="0.999248">
We now address the task of producing a set of di-
verse high-scoring translations.
</bodyText>
<subsectionHeader confidence="0.992272">
3.1 Generating Diverse Translations
</subsectionHeader>
<bodyText confidence="0.9996166">
We use a recently proposed technique (Batra et al.,
2012) that constructs diverse lists via a greedy itera-
tive procedure as follows. Let y1 be the model-best
translation (Eq. 1). On the m-th iteration, the m-th
best (diverse) translation is obtained as (ym, hm) =
</bodyText>
<equation confidence="0.601327">
AjΔ(yj, y) (2)
</equation>
<bodyText confidence="0.999931875">
where Δ is a dissimilarity function and Aj is the
weight placed on dissimilarity to previous trans-
lation j relative to the model score. Intuitively,
we seek a translation that is highly-scoring under
the model while being different (as measured by
Δ) from all previous translations. The A param-
eters determine the trade-off between model score
and diversity. We refer to Eq. (2) as dissimilarity-
augmented decoding.
The objective in Eq. (2) is a Lagrangian relax-
ation for an intractable constrained objective speci-
fying a minimum dissimilarity Δmin between trans-
lations in the list, i.e., Δ(yj, y) &gt; Δmin (Batra et
al., 2012). Instead of setting the dissimilarity thresh-
old Δmin, we set the weights Aj. While the formu-
lation allows for a different Aj for each previous so-
lution j, we simply use a single A = Aj for all j.
This was also done in the experiments in (Batra et
al., 2012).
Note that if the dissimilarity function factors
across the parts of the output variables (y, h) in the
same way as the features O, then the same decod-
ing algorithm can be used as for Eq. (1). We discuss
design choices for Δ next.
</bodyText>
<subsectionHeader confidence="0.9996">
3.2 Dissimilarity Functions for MT
</subsectionHeader>
<bodyText confidence="0.999946875">
When designing a dissimilarity function Δ(·, ·) for
MT, we want to consider variation both in individ-
ual word choice and longer-range sentence structure.
We also want a function that can be easily incorpo-
rated into extant statistical MT systems. We propose
a dissimilarity function that simply counts the num-
ber of times any n-gram is present in both transla-
tions, then negates. Letting q = n − 1:
</bodyText>
<equation confidence="0.983008">
|y|−q |y&apos;|−q
Δn(y, y0) = − E [[yi:i+q = y0j:j+q]] (3)
i=1 j=1
</equation>
<bodyText confidence="0.999317">
where [[·]] is the Iverson bracket (1 if input condition
is true, 0 otherwise) and yi:j is the subsequence of y
from word i to word j (inclusive).
Importantly, Eq. (2) can be solved with no change
to the decoding algorithm. The dissimilarity terms
can simply be incorporated as an additional lan-
guage model in ARPA format that sets the log-
probability to the negated count for each n-gram
in previous diverse translations, and sets to zero
all other n-grams’ log-probabilities and back-off
weights.
</bodyText>
<equation confidence="0.670774">
argmax wTO(x, y, h) + m−1�
(y,h)ET. j=1
</equation>
<page confidence="0.957439">
1101
</page>
<bodyText confidence="0.999979">
The advantage of this dissimilarity function is its
simplicity. It can be easily used with any transla-
tion system that uses n-gram language models with-
out any change to the decoder. Indeed, we use both
phrase-based and hierarchical phrase-based models
in our experiments below.
</bodyText>
<sectionHeader confidence="0.999945" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999957658536585">
MT researchers have recently started to con-
sider diversity in the context of system combina-
tion (Macherey and Och, 2007). Most closely-
related is work by Devlin and Matsoukas (2012),
who proposed a way to generate diverse transla-
tions by varying particular “traits,” such as transla-
tion length, number of rules applied, etc. Their ap-
proach can be viewed as solving Eq. (2) with a richer
dissimilarity function that requires a special-purpose
decoding algorithm. We chose our n-gram dissimi-
larity function due to its simplicity and applicability
to most MT systems without requiring any change
to decoders.
Among other work, Xiao et al. (2013) used bag-
ging and boosting to get diverse system outputs for
system combination and Cer et al. (2013) used mul-
tiple identical systems trained jointly with an objec-
tive function that encourages the systems to generate
complementary translations.
There is also similarity between our approach and
minimum Bayes risk decoding (Kumar and Byrne,
2004), variational decoding (Li et al., 2009), and
other “consensus” decoding algorithms (DeNero et
al., 2009). These all seek a single translation that
is most similar on average to the model’s preferred
translations. In this way, they try to capture the
model’s range of beliefs in a single translation. We
instead seek a set of translations that, when consid-
ered as a whole, similarly express the full range of
the model’s beliefs about plausible translations for
the input.
Also related is work on determinantal point pro-
cesses (DPPs; Kulesza and Taskar, 2010), an ele-
gant probabilistic model over sets of items that nat-
urally prefers diverse sets. DPPs have been ap-
plied to summarization (Kulesza and Taskar, 2011)
and discovery of topical threads in document collec-
tions (Gillenwater et al., 2012). Unfortunately, in
the structured setting, DPPs make severely restric-
tive assumptions on the scoring function, while our
framework does not.
</bodyText>
<sectionHeader confidence="0.995951" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9999932">
We now embark on an extensive empirical evalua-
tion of the framework presented above. We begin
by analyzing our diverse sets of translations, show-
ing how they differ from standard M-best lists (Sec-
tion 6), followed by three tasks that illustrate how di-
versity can be exploited to improve translation qual-
ity: system combination (Section 7), discrimina-
tive reranking (Section 8), and a novel human post-
editing task (Section 9). In the remainder of this sec-
tion, we describe details of our experimental setup.
</bodyText>
<subsectionHeader confidence="0.999159">
5.1 Language Pairs and Datasets
</subsectionHeader>
<bodyText confidence="0.999950870967742">
We use three language pairs: Arabic-to-English
(AR—*EN), Chinese-to-English (ZH—*EN), and
German-to-English (DE—*EN). For AR—*EN and
DE—*EN, we used a phrase-based model (Koehn et
al., 2003) and for ZH—*EN we used a hierarchical
phrase-based model (Chiang, 2007).
Each language pair has two tuning and one test
set: TUNE1 is used for tuning the baseline sys-
tems with minimum error rate training (MERT; Och,
2003), TUNE2 is used for training system combin-
ers and rerankers, and TEST is used for evaluation.
There are four references for AR—*EN and ZH—*EN
and one for DE—*EN.
For AR—*EN, we used data provided by the LDC
for the NIST evaluations, which includes 3.3M sen-
tences of UN data and 982K sentences from other
(mostly news) sources. Arabic text was prepro-
cessed using an HMM segmenter that splits attached
prepositional phrases, personal pronouns, and the
future marker (Lee et al., 2003). The common stylis-
tic sentence-initial w+ (and) clitic was removed.
The resulting corpus contained 130M Arabic tokens
and 130M English tokens. We used the NIST MT06
test set as TUNE1, a 764-sentence subset of MT05 as
TUNE2, and MT08 as TEST.
For ZH—*EN, we used 303k sentence pairs from
the FBIS corpus (LDC2003E14). We segmented
the Chinese data using the Stanford Chinese seg-
menter (Chang et al., 2008) in “CTB” mode, giving
us 7.9M Chinese tokens and 9.4M English tokens.
We used the NIST MT02 test set as TUNE1, MT05
</bodyText>
<page confidence="0.995761">
1102
</page>
<bodyText confidence="0.998982285714286">
as TUNE2, and MT03 as TEST.
For DE→EN, we used data released for the
WMT2011 shared task (Callison-Burch et al., 2011).
German compound words were split using a CRF
segmenter (Dyer, 2009). We used the WMT2010
test set as TUNE1, the 2009 test set as TUNE2, and
the 2011 test set as TEST.
</bodyText>
<subsectionHeader confidence="0.999357">
5.2 Baseline Systems
</subsectionHeader>
<bodyText confidence="0.99997208">
We used the Moses MT toolkit (Koehn et al.,
2007; Hoang et al., 2009) with default settings
and features for both phrase-based and hierarchi-
cal systems. Word alignment was done using
GIZA++ (Och and Ney, 2003) in both directions,
with the grow-diag-final-and heuristic used
to symmetrize the alignments and a max phrase
length of 7 used for phrase extraction.
Language models used the target side of the paral-
lel corpus in each case augmented with 24.8M lines
(601M tokens) of randomly-selected sentences from
the Gigaword v4 corpus (excluding the NY Times
and LA Times). We used 5-gram models, estimated
using the SRI Language Modeling toolkit (Stolcke,
2002) with modified Kneser-Ney smoothing (Chen
and Goodman, 1998). The minimum count cut-off
for unigrams, bigrams, and trigrams was 1 and the
cut-off for 4-grams and 5-grams was 3. Language
model inference used KenLM (Heafield, 2011).
Uncased IBM BLEU was used for evaluation (Pa-
pineni et al., 2002). MERT was used to train the fea-
ture weights for the baseline systems on TUNE1. We
used the learned parameters to generate M-best and
diverse lists for TUNE2 and TEST to use for subse-
quent experiments.
</bodyText>
<subsectionHeader confidence="0.996575">
5.3 Diverse List Generation
</subsectionHeader>
<bodyText confidence="0.9999396">
Generating diverse translations depends on two hy-
perparameters: the n-gram order used by the dissim-
ilarity function An (§3.2) and the Aj weights on the
dissimilarity terms in Eq. (2). Though our frame-
work permits different Aj for each j, we use a sin-
gle A value for simplicity, as was also done in (Ba-
tra et al., 2012). The values of n and A were tuned
on a 200 sentence subset of TUNE1 separately for
each language pair (which we call TUNE200), so as
to maximize the oracle BLEU score of the diverse
</bodyText>
<table confidence="0.9986108">
AR→EN ZH→EN DE→EN
1 best 50.1 36.9 21.8
20 best 54.0 40.3 24.7
200 best 57.5 43.8 27.7
1000 best 59.8 46.4 29.8
unique 20 best 56.6 44.1 26.7
unique 200 best 59.6 46.4 29.5
20 diverse 58.5 46.4 28.6
20 div x 10 best 61.3 48.7 30.3
20 div x 50 best 63.2 50.6 31.6
</table>
<tableCaption confidence="0.9281795">
Table 1: Oracle BLEU scores on TEST for various sizes
of M-best and diverse lists. Unique lists were obtained
from 1,000-best lists and therefore may not contain the
target number of unique translations for all sentences.
</tableCaption>
<bodyText confidence="0.999600352941177">
lists.1 We considered n values in {2, 3, ... , 9} and
A values in {0.005, 0.01, 0.05, 0.1}. We give details
on optimal values for these hyperparameters when
discussing particular tasks below.
Though simple, our approach is computationally
expensive as M grows because it requires decoding
M times for each sentence. So, we assume M G 20.
But we also extract an N-best list for each of the M
diverse translations.2 Many MT decoders, including
the phrase-based and hierarchical implementations
in Moses, permit efficient extraction of N-best lists,
so we exploit this to obtain larger lists that still ex-
hibit diversity. But we note that these N-best lists
for each diverse solution are not in themselves di-
verse; with more computational power or more effi-
cient algorithms (Devlin and Matsoukas, 2012) we
could potentially generate larger, more diverse lists.
</bodyText>
<sectionHeader confidence="0.660243" genericHeader="method">
6 Analysis of Diverse Lists
</sectionHeader>
<bodyText confidence="0.848809933333333">
We now characterize our diverse lists by compar-
ing them to M-best lists. Table 1 shows oracle
BLEU scores on TEST for M-best lists, unique M-
best lists, and diverse lists of several sizes. To get
unique lists, we first generated 1000-best lists, then
retained only the highest-scoring derivation for each
unique translation. When comparing M-best and di-
verse lists of comparable size, the diverse lists al-
1Since BLEU does not decompose additively across seg-
ments, we chose translations for individual sentences that max-
imized BLEU+1 (Lin and Och, 2004), then computed “oracle”
corpus BLEU of these translations.
2We did not consider n-grams from previous N-best lists
when computing the dissimilarity function, but only those from
the previous diverse translations.
</bodyText>
<page confidence="0.965963">
1103
</page>
<figure confidence="0.9871815">
0-25 25-36 36-47 47-94
1-best BLEU bin
</figure>
<figureCaption confidence="0.9993295">
Figure 1: Median, min, and max BLEU+1 of 20-best
and 20-diverse lists for the ZH→EN test set, divided into
quartiles according to the BLEU+1 score of the 1-best
translation, and averaged across sentences in each quar-
tile. Heights of the bars show median and “error bars”
indicate max and min.
</figureCaption>
<bodyText confidence="0.994520891891892">
ways have higher oracle BLEU. The differences are
largest when comparing 20-best lists and 20-diverse
lists, where they range from 4 to 6 BLEU points.
When generating these diverse lists, we used the
n and A values that were tuned for each language
pair to maximize oracle BLEU on TUNE200 for the
“20 div × 50 best” configuration. The optimal val-
ues of n were 6 for ZH→EN and AR→EN and 7 for
DE→EN.3 When instead tuning to maximize oracle
BLEU for 20-diverse lists, the optimal n stayed at
7 for DE→EN, but increased to 7 for AR→EN and 9
for ZH→EN. These values are noticeably larger than
n-gram sizes typically used in language modeling
and evaluation. They suggest that for optimal ora-
cle BLEU, translations with long-spanning amounts
of repeated material should be avoided, while short
overlapping n-grams are permitted.
Figure 1 shows other statistics on TEST for
ZH→EN. Plots for AR→EN and DE→EN are quali-
tatively similar. We divided the TEST sentences into
quartiles based on BLEU+1 of the 1-best transla-
tions from the baseline system. We computed the
median, min, and max BLEU+1 on each list and av-
eraged over the sentences in each quartile. As shown
in the plot, the ranges of 20-diverse lists subsume
those of 20-best lists, though the medians of diverse
3The optimal values of A were 0.005 for AR--+EN and 0.01
for ZH--+EN and DE--+EN. Since these values depend on the
scale of the weights learned by MERT, they are difficult to in-
terpret in isolation.
lists drop when the baseline system has high BLEU
score. This matches intuition: when the baseline
system is performing well, forcing it to find different
translations is likely to result in worse translations.
So we may expect diverse lists to be most helpful for
more difficult sentences, a point we return to in our
experiments below.
</bodyText>
<sectionHeader confidence="0.979093" genericHeader="method">
7 System Combination Experiments
</sectionHeader>
<bodyText confidence="0.999161766666667">
One way to evaluate the quality of our diverse lists
is to use them in system combination, as was sim-
ilarly done by Devlin and Matsoukas (2012) and
Cer et al. (2013). We use the system combination
framework of Heafield and Lavie (2010b), which
has an open-source implementation (Heafield and
Lavie, 2010a).4
We use our baseline systems (trained on TUNE1)
to generate lists for system combination on TUNE2
and TEST. We compare M-best lists, unique M-best
lists, and M-diverse lists, with M ∈ {10,15, 20}.5
For each choice of list type and M, we trained the
system combiner on TUNE2 and tested on TEST with
the learned parameters. System combination hyper-
parameters (whether to use feature length normal-
ization; the size of the k-best lists generated by the
system combiner during tuning, k ∈ {300, 600})
were chosen to maximize BLEU on TUNE200. Also,
we removed the individual features from the
default feature set because they correspond to in-
dividual systems in the combination; they did not
seem appropriate for us since our hypotheses all
come from the same system.
The results are shown in Table 2. Like Devlin and
Matsoukas (2012), we see no gain from system com-
bination using M-best lists. We see some improve-
ment with unique lists, particularly for AR→EN, al-
though it is not consistent across M values. But
we see larger improvements with diverse lists for
AR→EN and ZH→EN. For these language pairs, our
</bodyText>
<tableCaption confidence="0.792315888888889">
4The implementation uses MERT to tune parameters, but we
found this to be time-consuming and noisy for the larger feature
sets. So we used a structured support vector machine learning
framework instead (described in Section 8), using multiple it-
erations of learning interleaved with (system combiner) N-best
list generation, and accumulating N-best lists across iterations.
5Dissimilarity hyperparameters n and A were again chosen
to maximize oracle BLEU on TUNE200, separately for each M
and for each language pair.
</tableCaption>
<figure confidence="0.9993262">
20 best
20 diverse
%BLEU 70
60
50
40
30
20
10
0
</figure>
<page confidence="0.97584">
1104
</page>
<table confidence="0.999678833333333">
AR→EN ZH→EN DE→EN
10 15 20 10 15 20 10 15 20
baseline (no system combination) 50.1 36.9 21.8
M-best 50.2 50.1 50.0 36.7 36.9 37.0 21.7 21.7 21.8
unique M-best (from 1000-best list) 50.6 50.0 50.8 37.1 36.9 37.1 21.8 21.9 21.9
M-diverse 51.4 51.2 51.2 37.6 37.6 37.5 22.0 21.8 21.6
</table>
<tableCaption confidence="0.87898">
Table 2: System combination results (%BLEU on TEST). Size of lists is M E 110,15, 201. Highest score in each
column is bold.
</tableCaption>
<table confidence="0.999939833333333">
q1 AR→EN q4 q1 ZH→EN q4 q1 DE→EN q4
q2 q3 q2 q3 q2 q3
baseline 30.1 44.1 55.1 70.0 15.2 28.9 41.0 57.5 5.3 14.4 23.7 40.9
15-best 30.1 44.6 55.5 68.8 15.9 29.2 40.5 56.8 6.0 15.0 23.6 40.0
unique 15-best 30.4 44.7 55.2 68.4 16.7 29.0 41.2 56.6 5.9 14.9 23.8 40.6
15-diverse 31.3 45.3 57.8 69.1 17.7 30.6 41.7 56.9 7.6 15.2 23.4 39.6
</table>
<tableCaption confidence="0.995930333333333">
Table 3: System combination results (%BLEU on quartiles of TEST, M = 15). Source sentences were divided into
quartiles (numbered “qn”) according to BLEU+1 of the 1-best translations of the baseline system. Highest score in
each column is bold.
</tableCaption>
<bodyText confidence="0.999970333333333">
gains are similar to those seen by Devlin and Mat-
soukas, but use our simpler dissimilarity function.6
For DE→EN, results are similar for all settings and
do not show much improvement from system com-
bination.
In Table 3, we break down the scores according
to 1-best BLEU+1 quartiles, as done in Figure 1.7
In general, we find the largest gains for the low-
BLEU translations. For the two worst BLEU quar-
tiles, we see gains of 1.2 to 2.5 BLEU points, while
the gains shrink or disappear entirely for the best
quartile. This may be a worthwhile trade-off: a
large improvement in the worst translations may be
more significant to users than a smaller degredation
on sentences that are already being translated well.
In addition, quality estimation (Specia et al., 2011;
Bach et al., 2011) could be used to automatically de-
termine the BLEU quartile for each sentence. Then
system combination of diverse translations might be
used only when the 1-best translation is predicted to
be of low quality.
</bodyText>
<sectionHeader confidence="0.98358" genericHeader="method">
8 Reranking Experiments
</sectionHeader>
<bodyText confidence="0.998705333333333">
We now turn to discriminative reranking, which has
frequently been used to easily add rich features to
a model. It has been used for MT with varying de-
</bodyText>
<footnote confidence="0.56608">
6They reported +0.8 BLEU from system combination for
AR--+EN, and saw a further +0.5–0.7 from their new features.
7Quartile points are: 39, 49, 61 for AR--+EN; 25, 36, and 47
for ZH--+EN; and 14.5, 21.1, and 30.3 for DE--+EN.
</footnote>
<bodyText confidence="0.9948884">
gree of success (Och et al., 2004; Shen et al., 2004;
Hildebrand and Vogel, 2008); some have attributed
its mixed results to a lack of diversity in the M-best
lists traditionally used. We propose diverse lists as a
way to address this concern.
</bodyText>
<subsectionHeader confidence="0.992756">
8.1 Learning Framework
</subsectionHeader>
<bodyText confidence="0.999945">
Several learning formulations have been proposed
for M-best reranking. One commonly-used ap-
proach in MT is MERT, used in the reranking ex-
periments of Och et al. (2004) and Hildebrand and
Vogel (2008), among others. We experimented with
MERT and other algorithms, including pairwise
ranking optimization (Hopkins and May, 2011), but
we found best results using the approach of Yadol-
lahpour et al. (2013), who used a slack-rescaled
structured support vector machine (Tsochantaridis
et al., 2005) with L2 regularization. As a sentence-
level loss, we used negated BLEU+1. We used the
1-slack cutting-plane algorithm of Joachims et al.
(2009) for optimization during learning.8 A more
detailed description of the reranker is provided in the
supplementary material.
We used 5-fold cross-validation on TUNE2 to
choose the regularization parameter C from the set
10.01, 0.1,1,101. We selected the value yielding
the highest average BLEU score across the held-out
</bodyText>
<footnote confidence="0.943677">
8Our implementation uses OOQP (Gertz and Wright, 2003)
to solve the quadratic program in the inner loop, which uses
HSL, a collection of Fortran codes for large-scale scientific
computation (www.hsl.rl.ac.uk).
</footnote>
<page confidence="0.991958">
1105
</page>
<bodyText confidence="0.999988">
folds. This value was then used for one final round
of training on the entirety of TUNE2. Additionally,
we tuned the decision to return the parameters at
convergence or those that produced the highest train-
ing corpus BLEU score. Since we use a sentence-
level metric during training (BLEU+1) and a corpus-
level metric for final evaluation (BLEU), we found
that it was often better to return parameters that pro-
duced the highest training BLEU score.
This tuning procedure was repeated for each fea-
ture set and for each list type (M-best or diverse).
The test set was not used for any of this tuning.
</bodyText>
<subsectionHeader confidence="0.83663">
8.2 Features
</subsectionHeader>
<bodyText confidence="0.999396935483871">
In addition to the features from the baseline models
(14 for phrase-based, 8 for hierarchical), we add 36
more for reranking:
Inverse Model 1 (INVMOD1): We added the “in-
verse” versions of the three IBM Model 1 features
described in Section 2.2 of Hildebrand and Vogel
(2008). The first is the probability of the source sen-
tence given the translation under IBM Model 1, the
second replaces the E with a max in the first fea-
ture, and the third computes the percentage of words
whose lexical translation probability falls below a
threshold. We also include versions of the first 2
features normalized by the translation length, for a
total of 5 INVMOD1 features.
Large LM (LLM): We created a large 4-gram LM
by interpolating LMs from the WMT news data, Gi-
gaword, Europarl, and the DE→EN news commen-
tary (NC) corpus to maximize likelihood of a held-
out development set (WMT08 test set). We used the
average per-word log-probability as the single fea-
ture function in this category.
Syntactic LM (SYN): We used the syntactic treelet
language model of Pauls and Klein (2012) to com-
pute two features: the translation log probability and
the length-normalized log probability.
Finite/Non-Finite Verbs (VERB): We ran the Stan-
ford part-of-speech (POS) tagger (Toutanova et al.,
2003) on each translation and added four features:
the fraction of words tagged as finite/non-finite
verbs, and the fraction of verbs that are finite/non-
finite.9
</bodyText>
<footnote confidence="0.886521">
9Words tagged as MD, VBP, VBZ, and VBD were counted
</footnote>
<table confidence="0.998184222222222">
Reranking AR→EN ZH→EN DE→EN
features best div best div best div
N/A (baseline) 50.1 36.9 21.8
None 50.5 50.7 37.3 37.1 21.9 21.6
+ INVMOD1 50.3 50.8 37.6 37.1 22.0 21.8
+ LLM, SYN 50.5 51.1 37.4 37.3 21.7 21.7
+ VERB, DISC 50.4 51.3 37.3 37.3 21.9 22.2
+ GOOG 50.7 51.3 36.8 37.1 21.9 22.2
+ WCLM 51.2 51.8 37.3 37.4 22.2 22.3
</table>
<tableCaption confidence="0.999815">
Table 4: Reranking results (%BLEU on TEST).
</tableCaption>
<bodyText confidence="0.995298257142857">
Discriminative Word/Tag LMs (DISC): For each
language pair, we generated 10,000-best lists for
TUNE1 and computed BLEU+1 for each. From
these lists, we estimated 3- and 5-gram LMs,
weighting the n-gram counts by the BLEU+1
scores.10 We repeated this procedure except using
1 minus BLEU+1 as the weight (learning a language
model of “bad” translations). This yielded 4 fea-
tures. The procedure was then repeated using POS
tags instead of words, for 8 features in total.
Google 5-Grams (GOOG): Translations were com-
pared to the Google 5-gram corpus (LDC2006T13)
to compute: the number of 5-grams that matched,
the number of 5-grams that missed, and a set of
indicator features that fire if the fraction of 5-
grams that matched in the sentence was greater than
10.05, 0.1, 0.2, ... , 0.91, for a total of 12 features.
Word Cluster LMs (WCLM): Using an imple-
mentation provided by Liang (2005), we performed
Brown clustering (Brown et al., 1992) on 900k En-
glish sentences, including the NC corpus and ran-
dom sentences from Gigaword. We clustered words
that appeared at least twice, once with 300 clus-
ters and again with 1000. We then replaced words
with their clusters in a large corpus consisting of
the WMT news data, Gigaword, and the NC data.
An additional cluster label was used for unknown
words. For each of the clusterings (300 and 1000),
we estimated 5- and 7-gram LMs with Witten-Bell
smoothing (Witten and Bell, 1991). We added 4 fea-
tures to the reranker, one for the log-probability of
the translation under each of the word cluster LMs.
as finite verbs, and VB, VBG, and VBN were non-finite verbs.
10Before estimating LMs, we projected the sentence weights
so that the min and max per source sentence were 0 and 1.
</bodyText>
<page confidence="0.984367">
1106
</page>
<table confidence="0.999823538461538">
List type Features
None All
20 best 50.3 50.6
100 best 50.6 50.8
200 best 50.4 51.2
1000 best 50.5 51.2
unique 20 best 50.5 51.2
unique 100 best 50.6 51.2
unique 200 best 50.4 51.3
20 diverse 50.5 51.1
20 div × 5 best 50.6 51.4
20 div × 10 best 50.7 51.3
20 div × 50 best 50.7 51.8
</table>
<tableCaption confidence="0.998934">
Table 5: List comparison for AR→EN reranking.
</tableCaption>
<sectionHeader confidence="0.595617" genericHeader="method">
8.3 Results
</sectionHeader>
<bodyText confidence="0.97275440625">
Our results are shown in Table 4. We report results
using the baseline system alone (labeled “N/A (base-
line)”), and reranking standard M-best lists and our
diverse lists. For diverse lists, we use the “20 div ×
50 best” lists described in Section 5.3, with the tuned
dissimilarity hyperparameters reported in Section 6.
In the reranking settings, we also report results with-
out adding any additional features (the row labeled
“None”).11
The remaining rows add features. For AR→EN,
we see the largest gains, both over the baseline as
well as differences between M-best lists and diverse
lists. When using all features, we achieve a gain
of 0.6 BLEU over M-best reranking and 1.7 BLEU
points over the baseline system. The difference of
0.6 BLEU is consistent across feature subsets. We
found the WCLM features to give the largest in-
dividual improvement, with the remaining feature
sets each contributing a small amount. For Chinese
and German, the gains and individual differences are
smaller. Nonetheless, diverse lists appear to be more
robust for these language pairs as features are added.
In Table 5, we compare several sizes and types of
lists for AR→EN reranking both with no additional
features and with the full set. We see that using 20-
diverse lists nearly matches the performance of 200-
best lists. Also, retaining 50-best lists for each di-
verse solution improves BLEU by 0.7.
11Though such results have not always been reported in prior
work on reranking, we generally found them to improve over
the baseline, presumably because seeing more data improves
generalization ability.
</bodyText>
<figure confidence="0.61891125">
Train
best div
51.2 51.7
50.5 51.8
</figure>
<tableCaption confidence="0.985347">
Table 6: Comparing M-best and diverse lists for train-
ing/testing (AR→EN, all features).
</tableCaption>
<bodyText confidence="0.999575882352941">
Thus far, when training the reranker on M-best
lists, we tested it on M-best lists, and similarly for
diverse lists. Table 6 shows what happens with the
other two pairings for AR→EN with the full feature
set. When training on diverse lists, we see very lit-
tle difference in BLEU whether testing on M-best
or diverse lists. This has a practical benefit: we can
use (computationally-expensive) diverse lists during
offline training and then use fast M-best lists at test
time. When training on M-best lists and testing
on diverse lists, we see a substantial drop (51.2 vs
50.5). The reranker may be overfitting to the limited
scope of translations present in typical M-best lists,
thereby hindering its ability to correctly rank diverse
lists at test time. These results suggest that part of
the benefit of using diverse lists comes from seeing
a larger portion of the output space during training.
</bodyText>
<sectionHeader confidence="0.573813" genericHeader="method">
9 Human Post-Editing Experiments
</sectionHeader>
<bodyText confidence="0.999790045454546">
We wanted to determine whether diverse translations
could be helpful to users struggling to understand
the output of an imperfect MT system. We con-
sider a post-editing task in which users are presented
with translation output without the source sentence,
and are asked to improve it. This setting has been
studied; e.g., Koehn (2010) presented evidence that
monolingual speakers could often produce improved
translations for this task, occasionally reaching the
level of an expert translator.
Here, we use a novel variation of this task in
which multiple translations are shown to editors. We
compare the use of entries from an M-best list and
entries from a diverse list. Again, the original source
sentence is not provided. Our goal is to determine
whether multiple, diverse translations can help users
to more accurately guess the meaning of the original
sentence than entries from a standard M-best list. If
so, commercial MT systems might permit users to
request additional diverse translations for those sen-
tences whose model-best translations are difficult to
understand.
</bodyText>
<figure confidence="0.826818">
best
div
Test
</figure>
<page confidence="0.973999">
1107
</page>
<subsectionHeader confidence="0.995706">
9.1 Translation List Post-Editing
</subsectionHeader>
<bodyText confidence="0.999899875">
We use Amazon Mechanical Turk (MTurk) for this
experiment. Workers are shown 3 outputs from an
MT system. They are not shown the original sen-
tence, nor are they shown a reference. Based on
the 3 imperfect translations, they are asked to write
a single fluent English translation that best cap-
tures the understood meaning. Half of the time, the
worker is shown 3 entries from an M-best list, and
the other half of the time 3 entries from a diverse
list. We then compare the outputs produced under
the two conditions. The goal is to measure whether
workers are able to produce translations that are
closer in meaning to the (unseen) references when
shown diverse translations. We refer to this task as
the EDITING task.
To evaluate the outputs, we use a second task in
which users are shown a reference translation along
with two outputs from the first task: one created
from M-best lists and one from diverse lists. Work-
ers in this task are asked to choose which translation
is a better match to the reference in terms of mean-
ing, or they can indicate that the translations are of
the same quality. We refer to this second task as the
EVAL task.
</bodyText>
<subsectionHeader confidence="0.998353">
9.2 Dissimilarity Functions
</subsectionHeader>
<bodyText confidence="0.998541">
To generate diverse lists for the EDITING task, we
use the same dissimilarity function as in reranking,
but we tune the hyperparameters n and A differently.
Since our expectation here is that workers may com-
bine information from multiple translations to pro-
duce a superior output, we are interested in the cov-
erage of the translations in the diverse list, rather
than the oracle BLEU score.
We designed a metric based on coverage of entire
lists of translations. It is similar to BLEU+1, except
(1) it uses n-gram recalls instead of n-gram preci-
sions, (2) there is no brevity penalty term, and (3) it
compares a list to a set of references and any trans-
lation in the list can contribute a match of an n-gram
in any reference. Like BLEU, counts are clipped
based on those in the references. We maximized
this metric over diverse lists of length 5, for n E
12, 3, ... , 9} and A E 10.005, 0.01, 0.05, 0.1, 0.2}.
The optimal values for AR—*EN were n = 4 and
A = 0.1, while for ZH—*EN they were n = 4 and
A = 0.2. These n values are smaller than for rerank-
ing, and the A values are larger. This suggests that,
when maximizing coverage of a small diverse list,
more dissimilarity is desired among the translations.
</bodyText>
<subsectionHeader confidence="0.99557">
9.3 Detailed Procedure
</subsectionHeader>
<bodyText confidence="0.9999563">
We focused on AR—*EN and ZH—*EN for this study.
We sampled 200 sentences from their test sets, cho-
sen from among those whose reference translation
was between 5 and 25 words. We generated a unique
5-best list for each sentence using our baseline sys-
tem (described in Section 5.2) and also generated a
diverse list of length 5 using the dissimilarity func-
tion A with hyperparameters tuned using the proce-
dure from the previous section. We untokenized and
truecased the translations. We dropped non-ASCII
characters because we feared they would confuse
our workers. As a result, workers must contend with
missing words in the output, often proper nouns.
Given the 2 lists for each sentence, we sampled
two integers i, j E 12, 3, 4, 5} without replacement.
The indices i and j indicate two entries from the
lists. We took translations 1, i, and j from the 5-best
list and created an EDITING task from them. We did
the same using entries 1, i, and j from the diverse
list. We repeated this process 3 times for each sen-
tence, obtaining 3 x 2 = 6 tasks for each, giving us
a total of 1,200 EDITING tasks per language pair.
The outputs of the EDITING tasks were evaluated
with EVAL tasks. For each sentence, we had 3 post-
edited outputs generated using entries in 5-best lists
and 3 post-edited outputs from diverse lists. We cre-
ated EVAL tasks for all 9 output pairs, for all 200
sentences per language pair. We additionally gave
each task to three MTurk workers. This gave us
10,800 evaluation judgments for the EVAL task.
</bodyText>
<subsectionHeader confidence="0.745232">
9.4 Results
</subsectionHeader>
<bodyText confidence="0.9998527">
Figure 2 shows the quartile breakdown for judg-
ments collected from the EVAL task. The Y axis
represents the percentage of judgments for which
best/diverse outputs were preferred; the missing per-
centage for each bin is accounted for by “same”
judgments.
We observe an interesting phenomenon. Overall,
there is a slight preference for the post-edited out-
puts of M-best entries (“best”) over those from di-
verse translations (“div”); this preference is clearest
</bodyText>
<page confidence="0.985909">
1108
</page>
<figure confidence="0.983928">
Arabic−English
BLEU Bin
</figure>
<figureCaption confidence="0.999000714285714">
Figure 2: Percentages in which post-edited output given
M-best entries (“best”) was preferred by human eval-
uators as compared to post-edited output given diverse
translations (“div”), broken down by the BLEU+1 score
of the 1-best translation for the sentences. When the base-
line system is doing poorly, diversity helps post-editors to
produce better translations.
</figureCaption>
<bodyText confidence="0.999937761904762">
when the baseline system’s 1-best translation had a
high BLEU score. However, we see this trend re-
versed for sentences in which the baseline system’s
1-best translation had a low BLEU score. In general,
when the BLEU score of the baseline system is be-
low 35, it is preferable to give diverse translations to
users for post-editing. But when the baseline system
does very well, diverse translations do not contribute
anything, and in fact hurt because they may distract
users from the high-quality (and typically very sim-
ilar) translations from the 5-best lists.
Estimation of the quality of the output (“confi-
dence estimation”) has recently gained interest in
the MT community (Specia et al., 2011; Bach et
al., 2011; Callison-Burch et al., 2012; Bojar et al.,
2013), including specifically for post-editing (Tat-
sumi, 2009; Specia, 2011; Koponen, 2012). Future
work could investigate whether such automatic con-
fidence estimation could be used to identify situa-
tions in which diverse translations can be helpful for
aiding user understanding.
</bodyText>
<sectionHeader confidence="0.996304" genericHeader="discussions">
10 Future Work
</sectionHeader>
<bodyText confidence="0.99997108">
Our dissimilarity function captures diversity in the
particular phrases used by an MT system, but for
certain applications we may prefer other types of di-
versity. Defining the dissimilarity function on POS
tags or word clusters would help us to capture stylis-
tic patterns in sentence structure, as would targeting
syntactic structures in syntax-based translation.
A weakness of our approach is its computational
expense; by contrast, the method of Devlin and Mat-
soukas (2012) obtains diverse translations more ef-
ficiently by extracting them from a single decoding
of an input sentence (albeit with a wide beam). We
expect their ideas to be directly applicable to our set-
ting in order to get diverse solutions more cheaply.
We also plan to explore methods of explicitly target-
ing multiple, diverse solutions as part of the search
algorithm.
Finally, M-best lists are currently used to ap-
proximate structured spaces for many areas of MT,
including tuning (Och, 2003), minimum Bayes
risk decoding (Kumar and Byrne, 2004), and
pipelines (Venugopal et al., 2008). Future work
could replace M-best lists with diverse lists in these
and related tasks, whether for MT or other areas of
structured NLP.
</bodyText>
<sectionHeader confidence="0.998417" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.5137684">
We thank the anonymous reviewers as well as Colin
Cherry, Kenneth Heafield, Silja Hildebrand, Fei
Huang, Dan Klein, Adam Pauls, and Bing Xiang.
DB was partially supported by the National Science
Foundation under Grant No. 1353694.
</bodyText>
<sectionHeader confidence="0.99859" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998752333333333">
N. Bach, F. Huang, and Y. Al-Onaizan. 2011. Goodness:
A method for measuring machine translation confi-
dence. In Proc. of ACL.
D. Batra, P. Yadollahpour, A. Guzman-Rivera, and
G. Shakhnarovich. 2012. Diverse M-best solutions
in Markov random fields. In Proc. of ECCV.
O. Bojar, C. Buck, C. Callison-Burch, C. Federmann,
B. Haddow, P. Koehn, C. Monz, M. Post, R. Soricut,
and L. Specia. 2013. Findings of the 2013 Workshop
on Statistical Machine Translation. In Proc. of WMT.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della
Pietra, and J. C. Lai. 1992. Class-based N-gram mod-
</reference>
<figure confidence="0.999156125">
0−23 23−36 36−50 50−94
% Chosen
45
40
25
20
35
30
0−34 34−46 46−62 62−94
% Chosen
40
20
30
best
div
Chinese−English
</figure>
<page confidence="0.981587">
1109
</page>
<reference confidence="0.99893456122449">
els of natural language. Computational Linguistics,
18.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 Workshop on Statistical
Machine Translation. In Proc. of WMT.
C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Sori-
cut, and L. Specia. 2012. Findings of the 2012 Work-
shop on Statistical Machine Translation. In Proc. of
WMT.
D. Cer, C. D. Manning, and D. Jurafsky. 2013. Positive
diversity tuning for machine translation system com-
bination. In Proc. of WMT.
P. Chang, M. Galley, and C. D. Manning. 2008. Opti-
mizing Chinese word segmentation for machine trans-
lation performance. In Proc. of WMT.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proc. of ACL.
S. Chatterjee and N. Cancedda. 2010. Minimum error
rate training by sampling the translation lattice. In
Proc. of EMNLP.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal report 10-98, Harvard University.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1).
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. of ICML.
J. DeNero, D. Chiang, and K. Knight. 2009. Fast con-
sensus decoding over translation forests. In Proc. of
ACL.
J. Devlin and S. Matsoukas. 2012. Trait-based hypoth-
esis selection for machine translation. In Proc. of
NAACL.
C. Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for MT. In Proc. of HLT-
NAACL.
C. Dyer. 2010. A Formal Model ofAmbiguity and its Ap-
plications in Machine Translation. Ph.D. thesis, Uni-
versity of Maryland.
J. R. Finkel, C. D. Manning, and A. Y. Ng. 2006. Solv-
ing the problem of cascading errors: Approximate
Bayesian inference for linguistic annotation pipelines.
In Proc. of EMNLP.
E. M. Gertz and S. J. Wright. 2003. Object-oriented soft-
ware for quadratic programming. ACM Transactions
on Mathematical Software, 29(1).
J. Gillenwater, A. Kulesza, and B. Taskar. 2012. Discov-
ering diverse and salient threads in document collec-
tions. In Proc. of EMNLP.
K. Heafield and A. Lavie. 2010a. Combining machine
translation output with open source: The Carnegie
Mellon multi-engine machine translation scheme. The
Prague Bulletin of Mathematical Linguistics, 93.
K. Heafield and A. Lavie. 2010b. Voting on n-grams for
machine translation system combination. In Proc. of
AMTA.
K. Heafield. 2011. Kenlm: Faster and smaller language
model queries. In Proc. of WMT.
A. Hildebrand and S. Vogel. 2008. Combination of
machine translation systems via hypothesis selection
from combined n-best lists. In Proc. of AMTA.
H. Hoang, P. Koehn, and A. Lopez. 2009. A Uni-
fied Framework for Phrase-Based, Hierarchical, and
Syntax-Based Statistical Machine Translation. In
Proc. of IWSLT.
M. Hopkins and J. May. 2011. Tuning as ranking. In
Proc. of EMNLP.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proc. of ACL.
T. Joachims, T. Finley, and C. Yu. 2009. Cutting-
plane training of structural SVMs. Machine Learning,
77(1).
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL (demo
session).
P. Koehn. 2010. Enabling monolingual translators: Post-
editing vs. options. In Proc. of NAACL.
M. Koponen. 2012. Comparing human perceptions of
post-editing effort with post-editing operations. In
Proc. of WMT.
A. Kulesza and B. Taskar. 2010. Structured determinan-
tal point processes. In Proc. of NIPS.
A. Kulesza and B. Taskar. 2011. Learning determinantal
point processes. In Proc. of UAI.
S. Kumar and W. Byrne. 2004. Minimum bayes-risk
decoding for statistical machine translation. In Proc.
of HLT-NAACL.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009.
Efficient minimum error rate training and minimum
</reference>
<page confidence="0.827283">
1110
</page>
<reference confidence="0.999703841463415">
Bayes-risk decoding for translation hypergraphs and
lattices. In Proc. of ACL-IJCNLP.
Y. Lee, K. Papineni, S. Roukos, O. Emam, and H. Hassan.
2003. Language model based Arabic word segmenta-
tion. In Proc. of ACL.
Z. Li, J. Eisner, and S. Khudanpur. 2009. Variational
decoding for statistical machine translation. In Proc.
of ACL.
P. Liang. 2005. Semi-supervised learning for natural
language. Master’s thesis, Massachusetts Institute of
Technology.
C. Lin and F. J. Och. 2004. Orange: a method for evalu-
ating automatic evaluation metrics for machine trans-
lation. In Proc. of COLING.
W. Macherey and F. J. Och. 2007. An empirical study
on computing consensus translations from multiple
machine translation systems. In Proc. of EMNLP-
CoNLL.
W. Macherey, F. J. Och, I. Thayer, and J. Uszkoreit. 2008.
Lattice-based minimum error rate training for statisti-
cal machine translation. In Proc. of EMNLP.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of ACL.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord
of features for statistical machine translation. In HLT-
NAACL.
F. J. Och. 2003. Minimum error rate training for statisti-
cal machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
A. Pauls and D. Klein. 2012. Large-scale syntactic lan-
guage modeling with treelets. In Proc. of ACL.
A.-V. Rosti, N. F. Ayan, B. Xiang, S. Matsoukas,
R. Schwartz, and B. Dorr. 2007. Combining outputs
from multiple machine translation systems. In HLT-
NAACL.
L. Shen and A. K. Joshi. 2003. An SVM-based voting
algorithm with application to parse reranking. In Proc.
of CoNLL.
L. Shen, A. Sarkar, and F. J. Och. 2004. Discriminative
reranking for machine translation. In Proc. of HLT-
NAACL.
L. Specia, N. Hajlaoui, C. Hallett, and W. Aziz. 2011.
Predicting machine translation adequacy. In Proc. of
MT Summit XIII.
L. Specia. 2011. Exploiting objective annotations for
measuring translation post-editing effort. In Proc. of
EAMT.
A. Stolcke. 2002. SRILM—an extensible language mod-
eling toolkit. In Proc. of ICSLP.
M. Tatsumi. 2009. Correlation between automatic evalu-
ation metric scores, post-editing speed, and some other
factors. In Proc. of MT Summit XII.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proc. of HLT-NAACL.
R. Tromble, S. Kumar, F. J. Och, and W. Macherey. 2008.
Lattice Minimum Bayes-Risk decoding for statistical
machine translation. In Proc. of EMNLP.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. JMLR, 6.
A. Venugopal, A. Zollmann, N.A. Smith, and S. Vogel.
2008. Wider pipelines: N-best alignments and parses
in MT training. In Proc. ofAMTA.
I. H. Witten and T. C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events
in adaptive text compression. IEEE Transactions on
Information Theory, 37(4).
T. Xiao, J. Zhu, and T. Liu. 2013. Bagging and boosting
statistical machine translation systems. Artif. Intell.,
195.
P. Yadollahpour, D. Batra, and G. Shakhnarovich. 2013.
Discriminative re-ranking of diverse segmentations. In
Proc. of CVPR.
</reference>
<page confidence="0.995319">
1111
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.412697">
<title confidence="0.999774">A Systematic Exploration of Diversity in Machine Translation</title>
<author confidence="0.760791">Dhruv</author>
<affiliation confidence="0.965491">Technological Institute at Chicago, Chicago, IL 60637,</affiliation>
<address confidence="0.981727">Tech, Blacksburg, VA 24061, Mellon University, Pittsburgh, PA 15213,</address>
<note confidence="0.597259">author:</note>
<abstract confidence="0.997585384615385">This paper addresses the problem of producing a diverse set of plausible translations. We present a simple procedure that can be used with any statistical machine translation (MT) system. We explore three ways of using diverse translations: (1) system combination, (2) discriminative reranking with rich features, and (3) a novel post-editing scenario in which multiple translations are presented to users. We find that diversity can improve performance on these tasks, especially for sentences that are difficult for MT.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Bach</author>
<author>F Huang</author>
<author>Y Al-Onaizan</author>
</authors>
<title>Goodness: A method for measuring machine translation confidence.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="22137" citStr="Bach et al., 2011" startWordPosition="3704" endWordPosition="3707">w much improvement from system combination. In Table 3, we break down the scores according to 1-best BLEU+1 quartiles, as done in Figure 1.7 In general, we find the largest gains for the lowBLEU translations. For the two worst BLEU quartiles, we see gains of 1.2 to 2.5 BLEU points, while the gains shrink or disappear entirely for the best quartile. This may be a worthwhile trade-off: a large improvement in the worst translations may be more significant to users than a smaller degredation on sentences that are already being translated well. In addition, quality estimation (Specia et al., 2011; Bach et al., 2011) could be used to automatically determine the BLEU quartile for each sentence. Then system combination of diverse translations might be used only when the 1-best translation is predicted to be of low quality. 8 Reranking Experiments We now turn to discriminative reranking, which has frequently been used to easily add rich features to a model. It has been used for MT with varying de6They reported +0.8 BLEU from system combination for AR--+EN, and saw a further +0.5–0.7 from their new features. 7Quartile points are: 39, 49, 61 for AR--+EN; 25, 36, and 47 for ZH--+EN; and 14.5, 21.1, and 30.3 for</context>
<context position="37990" citStr="Bach et al., 2011" startWordPosition="6391" endWordPosition="6394">ersed for sentences in which the baseline system’s 1-best translation had a low BLEU score. In general, when the BLEU score of the baseline system is below 35, it is preferable to give diverse translations to users for post-editing. But when the baseline system does very well, diverse translations do not contribute anything, and in fact hurt because they may distract users from the high-quality (and typically very similar) translations from the 5-best lists. Estimation of the quality of the output (“confidence estimation”) has recently gained interest in the MT community (Specia et al., 2011; Bach et al., 2011; Callison-Burch et al., 2012; Bojar et al., 2013), including specifically for post-editing (Tatsumi, 2009; Specia, 2011; Koponen, 2012). Future work could investigate whether such automatic confidence estimation could be used to identify situations in which diverse translations can be helpful for aiding user understanding. 10 Future Work Our dissimilarity function captures diversity in the particular phrases used by an MT system, but for certain applications we may prefer other types of diversity. Defining the dissimilarity function on POS tags or word clusters would help us to capture stylis</context>
</contexts>
<marker>Bach, Huang, Al-Onaizan, 2011</marker>
<rawString>N. Bach, F. Huang, and Y. Al-Onaizan. 2011. Goodness: A method for measuring machine translation confidence. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Batra</author>
<author>P Yadollahpour</author>
<author>A Guzman-Rivera</author>
<author>G Shakhnarovich</author>
</authors>
<title>Diverse M-best solutions in Markov random fields.</title>
<date>2012</date>
<booktitle>In Proc. of ECCV.</booktitle>
<contexts>
<context position="3112" citStr="Batra et al. (2012)" startWordPosition="480" endWordPosition="483">soning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the implicit goal behind these techniques is to better explore the output space by introducing diversity into the surrogate set. Overview and Contributions. In this work, we elevate diversity to a first-class status and directly address the problem of generating a set of diverse, plausible translations. We use the recently proposed technique of Batra et al. (2012), which produces diverse M-best solutions from a probabilistic model using a generic dissimilarity function A(, ·) that specifies how two solutions differ. Our first contribution is a family of dissimilarity functions for MT that admit simple algorithms for generating diverse translations. Other contributions are empirical: we show that diverse translations can lead to improvements for system combination and discriminative reranking. We also perform a novel human 1100 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1100–1111, Seattle, Washington, U</context>
<context position="5515" citStr="Batra et al., 2012" startWordPosition="883" endWordPosition="886">n rule: (ˆy, ˆh) = argmax wTO(x, y, h) (1) (y,h)ET. where we also maximize over the latent variable h for efficiency. Translation models differ in the form of Tx and the choice of the feature functions O. In this paper we focus on phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) models, which include several bilingual and monolingual features, including n-gram language models. 3 Diversity in Machine Translation We now address the task of producing a set of diverse high-scoring translations. 3.1 Generating Diverse Translations We use a recently proposed technique (Batra et al., 2012) that constructs diverse lists via a greedy iterative procedure as follows. Let y1 be the model-best translation (Eq. 1). On the m-th iteration, the m-th best (diverse) translation is obtained as (ym, hm) = AjΔ(yj, y) (2) where Δ is a dissimilarity function and Aj is the weight placed on dissimilarity to previous translation j relative to the model score. Intuitively, we seek a translation that is highly-scoring under the model while being different (as measured by Δ) from all previous translations. The A parameters determine the trade-off between model score and diversity. We refer to Eq. (2)</context>
<context position="13901" citStr="Batra et al., 2012" startWordPosition="2289" endWordPosition="2293">2011). Uncased IBM BLEU was used for evaluation (Papineni et al., 2002). MERT was used to train the feature weights for the baseline systems on TUNE1. We used the learned parameters to generate M-best and diverse lists for TUNE2 and TEST to use for subsequent experiments. 5.3 Diverse List Generation Generating diverse translations depends on two hyperparameters: the n-gram order used by the dissimilarity function An (§3.2) and the Aj weights on the dissimilarity terms in Eq. (2). Though our framework permits different Aj for each j, we use a single A value for simplicity, as was also done in (Batra et al., 2012). The values of n and A were tuned on a 200 sentence subset of TUNE1 separately for each language pair (which we call TUNE200), so as to maximize the oracle BLEU score of the diverse AR→EN ZH→EN DE→EN 1 best 50.1 36.9 21.8 20 best 54.0 40.3 24.7 200 best 57.5 43.8 27.7 1000 best 59.8 46.4 29.8 unique 20 best 56.6 44.1 26.7 unique 200 best 59.6 46.4 29.5 20 diverse 58.5 46.4 28.6 20 div x 10 best 61.3 48.7 30.3 20 div x 50 best 63.2 50.6 31.6 Table 1: Oracle BLEU scores on TEST for various sizes of M-best and diverse lists. Unique lists were obtained from 1,000-best lists and therefore may not </context>
</contexts>
<marker>Batra, Yadollahpour, Guzman-Rivera, Shakhnarovich, 2012</marker>
<rawString>D. Batra, P. Yadollahpour, A. Guzman-Rivera, and G. Shakhnarovich. 2012. Diverse M-best solutions in Markov random fields. In Proc. of ECCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Bojar</author>
<author>C Buck</author>
<author>C Callison-Burch</author>
<author>C Federmann</author>
<author>B Haddow</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>M Post</author>
<author>R Soricut</author>
<author>L Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proc. of WMT.</booktitle>
<contexts>
<context position="1324" citStr="Bojar et al., 2013" startWordPosition="193" endWordPosition="196">translations are presented to users. We find that diversity can improve performance on these tasks, especially for sentences that are difficult for MT. 1 Introduction From the perspective of user interaction, the ideal machine translator is an agent that reads documents in one language and produces accurate, high quality translations in another. This interaction ideal has been implicit in machine translation (MT) research since the field’s inception. It is the way we interact with commercial MT services (such as Google Translate and Microsoft Translator), and the way MT systems are evaluated (Bojar et al., 2013). Unfortunately, when a real, imperfect MT system makes an error, the user is left trying to guess what the original sentence means. Multiple Hypotheses. In contrast, when we look at the way other computer systems consume output from MT systems (or similarly unreliable tools), we see a different pattern. In a pipeline setting it is commonplace to propagate not just a singlebest output but the M-best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Collins and Koo, 2005; Charniak and Johnson, 2005), tuning (Och, 2003), min</context>
<context position="38040" citStr="Bojar et al., 2013" startWordPosition="6399" endWordPosition="6402">’s 1-best translation had a low BLEU score. In general, when the BLEU score of the baseline system is below 35, it is preferable to give diverse translations to users for post-editing. But when the baseline system does very well, diverse translations do not contribute anything, and in fact hurt because they may distract users from the high-quality (and typically very similar) translations from the 5-best lists. Estimation of the quality of the output (“confidence estimation”) has recently gained interest in the MT community (Specia et al., 2011; Bach et al., 2011; Callison-Burch et al., 2012; Bojar et al., 2013), including specifically for post-editing (Tatsumi, 2009; Specia, 2011; Koponen, 2012). Future work could investigate whether such automatic confidence estimation could be used to identify situations in which diverse translations can be helpful for aiding user understanding. 10 Future Work Our dissimilarity function captures diversity in the particular phrases used by an MT system, but for certain applications we may prefer other types of diversity. Defining the dissimilarity function on POS tags or word clusters would help us to capture stylistic patterns in sentence structure, as would targe</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>O. Bojar, C. Buck, C. Callison-Burch, C. Federmann, B. Haddow, P. Koehn, C. Monz, M. Post, R. Soricut, and L. Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>P V deSouza</author>
<author>R L Mercer</author>
<author>V J Della Pietra</author>
<author>J C Lai</author>
</authors>
<title>Class-based N-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<contexts>
<context position="27583" citStr="Brown et al., 1992" startWordPosition="4614" endWordPosition="4617">e model of “bad” translations). This yielded 4 features. The procedure was then repeated using POS tags instead of words, for 8 features in total. Google 5-Grams (GOOG): Translations were compared to the Google 5-gram corpus (LDC2006T13) to compute: the number of 5-grams that matched, the number of 5-grams that missed, and a set of indicator features that fire if the fraction of 5- grams that matched in the sentence was greater than 10.05, 0.1, 0.2, ... , 0.91, for a total of 12 features. Word Cluster LMs (WCLM): Using an implementation provided by Liang (2005), we performed Brown clustering (Brown et al., 1992) on 900k English sentences, including the NC corpus and random sentences from Gigaword. We clustered words that appeared at least twice, once with 300 clusters and again with 1000. We then replaced words with their clusters in a large corpus consisting of the WMT news data, Gigaword, and the NC data. An additional cluster label was used for unknown words. For each of the clusterings (300 and 1000), we estimated 5- and 7-gram LMs with Witten-Bell smoothing (Witten and Bell, 1991). We added 4 features to the reranker, one for the log-probability of the translation under each of the word cluster </context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della Pietra, and J. C. Lai. 1992. Class-based N-gram models of natural language. Computational Linguistics, 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>O Zaidan</author>
</authors>
<date>2011</date>
<booktitle>Findings of the 2011 Workshop on Statistical Machine Translation. In Proc. of WMT.</booktitle>
<contexts>
<context position="12208" citStr="Callison-Burch et al., 2011" startWordPosition="2002" endWordPosition="2005">istic sentence-initial w+ (and) clitic was removed. The resulting corpus contained 130M Arabic tokens and 130M English tokens. We used the NIST MT06 test set as TUNE1, a 764-sentence subset of MT05 as TUNE2, and MT08 as TEST. For ZH—*EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter (Chang et al., 2008) in “CTB” mode, giving us 7.9M Chinese tokens and 9.4M English tokens. We used the NIST MT02 test set as TUNE1, MT05 1102 as TUNE2, and MT03 as TEST. For DE→EN, we used data released for the WMT2011 shared task (Callison-Burch et al., 2011). German compound words were split using a CRF segmenter (Dyer, 2009). We used the WMT2010 test set as TUNE1, the 2009 test set as TUNE2, and the 2011 test set as TEST. 5.2 Baseline Systems We used the Moses MT toolkit (Koehn et al., 2007; Hoang et al., 2009) with default settings and features for both phrase-based and hierarchical systems. Word alignment was done using GIZA++ (Och and Ney, 2003) in both directions, with the grow-diag-final-and heuristic used to symmetrize the alignments and a max phrase length of 7 used for phrase extraction. Language models used the target side of the parall</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan. 2011. Findings of the 2011 Workshop on Statistical Machine Translation. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>M Post</author>
<author>R Soricut</author>
<author>L Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Proc. of WMT.</booktitle>
<contexts>
<context position="38019" citStr="Callison-Burch et al., 2012" startWordPosition="6395" endWordPosition="6398"> in which the baseline system’s 1-best translation had a low BLEU score. In general, when the BLEU score of the baseline system is below 35, it is preferable to give diverse translations to users for post-editing. But when the baseline system does very well, diverse translations do not contribute anything, and in fact hurt because they may distract users from the high-quality (and typically very similar) translations from the 5-best lists. Estimation of the quality of the output (“confidence estimation”) has recently gained interest in the MT community (Specia et al., 2011; Bach et al., 2011; Callison-Burch et al., 2012; Bojar et al., 2013), including specifically for post-editing (Tatsumi, 2009; Specia, 2011; Koponen, 2012). Future work could investigate whether such automatic confidence estimation could be used to identify situations in which diverse translations can be helpful for aiding user understanding. 10 Future Work Our dissimilarity function captures diversity in the particular phrases used by an MT system, but for certain applications we may prefer other types of diversity. Defining the dissimilarity function on POS tags or word clusters would help us to capture stylistic patterns in sentence stru</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Soricut, and L. Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cer</author>
<author>C D Manning</author>
<author>D Jurafsky</author>
</authors>
<title>Positive diversity tuning for machine translation system combination.</title>
<date>2013</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="8924" citStr="Cer et al. (2013)" startWordPosition="1468" endWordPosition="1471">t closelyrelated is work by Devlin and Matsoukas (2012), who proposed a way to generate diverse translations by varying particular “traits,” such as translation length, number of rules applied, etc. Their approach can be viewed as solving Eq. (2) with a richer dissimilarity function that requires a special-purpose decoding algorithm. We chose our n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al. (2013) used bagging and boosting to get diverse system outputs for system combination and Cer et al. (2013) used multiple identical systems trained jointly with an objective function that encourages the systems to generate complementary translations. There is also similarity between our approach and minimum Bayes risk decoding (Kumar and Byrne, 2004), variational decoding (Li et al., 2009), and other “consensus” decoding algorithms (DeNero et al., 2009). These all seek a single translation that is most similar on average to the model’s preferred translations. In this way, they try to capture the model’s range of beliefs in a single translation. We instead seek a set of translations that, when consi</context>
<context position="18556" citStr="Cer et al. (2013)" startWordPosition="3086" endWordPosition="3089">le of the weights learned by MERT, they are difficult to interpret in isolation. lists drop when the baseline system has high BLEU score. This matches intuition: when the baseline system is performing well, forcing it to find different translations is likely to result in worse translations. So we may expect diverse lists to be most helpful for more difficult sentences, a point we return to in our experiments below. 7 System Combination Experiments One way to evaluate the quality of our diverse lists is to use them in system combination, as was similarly done by Devlin and Matsoukas (2012) and Cer et al. (2013). We use the system combination framework of Heafield and Lavie (2010b), which has an open-source implementation (Heafield and Lavie, 2010a).4 We use our baseline systems (trained on TUNE1) to generate lists for system combination on TUNE2 and TEST. We compare M-best lists, unique M-best lists, and M-diverse lists, with M ∈ {10,15, 20}.5 For each choice of list type and M, we trained the system combiner on TUNE2 and tested on TEST with the learned parameters. System combination hyperparameters (whether to use feature length normalization; the size of the k-best lists generated by the system co</context>
</contexts>
<marker>Cer, Manning, Jurafsky, 2013</marker>
<rawString>D. Cer, C. D. Manning, and D. Jurafsky. 2013. Positive diversity tuning for machine translation system combination. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Chang</author>
<author>M Galley</author>
<author>C D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="11968" citStr="Chang et al., 2008" startWordPosition="1958" endWordPosition="1961"> data and 982K sentences from other (mostly news) sources. Arabic text was preprocessed using an HMM segmenter that splits attached prepositional phrases, personal pronouns, and the future marker (Lee et al., 2003). The common stylistic sentence-initial w+ (and) clitic was removed. The resulting corpus contained 130M Arabic tokens and 130M English tokens. We used the NIST MT06 test set as TUNE1, a 764-sentence subset of MT05 as TUNE2, and MT08 as TEST. For ZH—*EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter (Chang et al., 2008) in “CTB” mode, giving us 7.9M Chinese tokens and 9.4M English tokens. We used the NIST MT02 test set as TUNE1, MT05 1102 as TUNE2, and MT03 as TEST. For DE→EN, we used data released for the WMT2011 shared task (Callison-Burch et al., 2011). German compound words were split using a CRF segmenter (Dyer, 2009). We used the WMT2010 test set as TUNE1, the 2009 test set as TUNE2, and the 2011 test set as TEST. 5.2 Baseline Systems We used the Moses MT toolkit (Koehn et al., 2007; Hoang et al., 2009) with default settings and features for both phrase-based and hierarchical systems. Word alignment wa</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>P. Chang, M. Galley, and C. D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine nbest parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1899" citStr="Charniak and Johnson, 2005" startWordPosition="287" endWordPosition="290">he way MT systems are evaluated (Bojar et al., 2013). Unfortunately, when a real, imperfect MT system makes an error, the user is left trying to guess what the original sentence means. Multiple Hypotheses. In contrast, when we look at the way other computer systems consume output from MT systems (or similarly unreliable tools), we see a different pattern. In a pipeline setting it is commonplace to propagate not just a singlebest output but the M-best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Collins and Koo, 2005; Charniak and Johnson, 2005), tuning (Och, 2003), minimum Bayes risk decoding (Kumar and Byrne, 2004), and system combination (Rosti et al., 2007). When dealing with error-prone systems, knowing about alternatives has benefits over relying on only a single output (Finkel et al., 2006; Dyer, 2010). Need for Diversity. Unfortunately, M-best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M-best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-fine nbest parsing and maxent discriminative reranking. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chatterjee</author>
<author>N Cancedda</author>
</authors>
<title>Minimum error rate training by sampling the translation lattice.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2730" citStr="Chatterjee and Cancedda, 2010" startWordPosition="417" endWordPosition="420">er relying on only a single output (Finkel et al., 2006; Dyer, 2010). Need for Diversity. Unfortunately, M-best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M-best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the implicit goal behind these techniques is to better explore the output space by introducing diversity into the surrogate set. Overview and Contributions. In this work, we elevate diversity to a first-class status and directly address the problem of generating a set of diverse, plausible translations. We use the recently proposed technique of Batra et al. (2012), which produces diverse M-best solutions from a probabilistic model using a generic dissimilarity function A(, ·) that specifies how two solutions differ. Our first contribution is a family of dissimilarity functions </context>
</contexts>
<marker>Chatterjee, Cancedda, 2010</marker>
<rawString>S. Chatterjee and N. Cancedda. 2010. Minimum error rate training by sampling the translation lattice. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical report 10-98,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="13118" citStr="Chen and Goodman, 1998" startWordPosition="2152" endWordPosition="2155">eatures for both phrase-based and hierarchical systems. Word alignment was done using GIZA++ (Och and Ney, 2003) in both directions, with the grow-diag-final-and heuristic used to symmetrize the alignments and a max phrase length of 7 used for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used 5-gram models, estimated using the SRI Language Modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The minimum count cut-off for unigrams, bigrams, and trigrams was 1 and the cut-off for 4-grams and 5-grams was 3. Language model inference used KenLM (Heafield, 2011). Uncased IBM BLEU was used for evaluation (Papineni et al., 2002). MERT was used to train the feature weights for the baseline systems on TUNE1. We used the learned parameters to generate M-best and diverse lists for TUNE2 and TEST to use for subsequent experiments. 5.3 Diverse List Generation Generating diverse translations depends on two hyperparameters: the n-gram order used by the dissimilarity function An (§3.2) and the A</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>S. Chen and J. Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical report 10-98, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="5205" citStr="Chiang, 2007" startWordPosition="839" endWordPosition="840"> for x. Derivations are coupled with translations and we define Tx C �x x xx as the set of possible (y, h) pairs for x. We use a linear model with a parameter vector w and a vector O(x, y, h) of feature functions on x, y, and h (Och and Ney, 2002). The translation of x is selected using a simple decision rule: (ˆy, ˆh) = argmax wTO(x, y, h) (1) (y,h)ET. where we also maximize over the latent variable h for efficiency. Translation models differ in the form of Tx and the choice of the feature functions O. In this paper we focus on phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) models, which include several bilingual and monolingual features, including n-gram language models. 3 Diversity in Machine Translation We now address the task of producing a set of diverse high-scoring translations. 3.1 Generating Diverse Translations We use a recently proposed technique (Batra et al., 2012) that constructs diverse lists via a greedy iterative procedure as follows. Let y1 be the model-best translation (Eq. 1). On the m-th iteration, the m-th best (diverse) translation is obtained as (ym, hm) = AjΔ(yj, y) (2) where Δ is a dissimilarity function and Aj is the weight placed on d</context>
<context position="10931" citStr="Chiang, 2007" startWordPosition="1784" endWordPosition="1785">sts (Section 6), followed by three tasks that illustrate how diversity can be exploited to improve translation quality: system combination (Section 7), discriminative reranking (Section 8), and a novel human postediting task (Section 9). In the remainder of this section, we describe details of our experimental setup. 5.1 Language Pairs and Datasets We use three language pairs: Arabic-to-English (AR—*EN), Chinese-to-English (ZH—*EN), and German-to-English (DE—*EN). For AR—*EN and DE—*EN, we used a phrase-based model (Koehn et al., 2003) and for ZH—*EN we used a hierarchical phrase-based model (Chiang, 2007). Each language pair has two tuning and one test set: TUNE1 is used for tuning the baseline systems with minimum error rate training (MERT; Och, 2003), TUNE2 is used for training system combiners and rerankers, and TEST is used for evaluation. There are four references for AR—*EN and ZH—*EN and one for DE—*EN. For AR—*EN, we used data provided by the LDC for the NIST evaluations, which includes 3.3M sentences of UN data and 982K sentences from other (mostly news) sources. Arabic text was preprocessed using an HMM segmenter that splits attached prepositional phrases, personal pronouns, and the </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1870" citStr="Collins and Koo, 2005" startWordPosition="283" endWordPosition="286">soft Translator), and the way MT systems are evaluated (Bojar et al., 2013). Unfortunately, when a real, imperfect MT system makes an error, the user is left trying to guess what the original sentence means. Multiple Hypotheses. In contrast, when we look at the way other computer systems consume output from MT systems (or similarly unreliable tools), we see a different pattern. In a pipeline setting it is commonplace to propagate not just a singlebest output but the M-best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Collins and Koo, 2005; Charniak and Johnson, 2005), tuning (Och, 2003), minimum Bayes risk decoding (Kumar and Byrne, 2004), and system combination (Rosti et al., 2007). When dealing with error-prone systems, knowing about alternatives has benefits over relying on only a single output (Finkel et al., 2006; Dyer, 2010). Need for Diversity. Unfortunately, M-best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M-best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recen</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>M. Collins and T. Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="1825" citStr="Collins, 2000" startWordPosition="277" endWordPosition="278">s (such as Google Translate and Microsoft Translator), and the way MT systems are evaluated (Bojar et al., 2013). Unfortunately, when a real, imperfect MT system makes an error, the user is left trying to guess what the original sentence means. Multiple Hypotheses. In contrast, when we look at the way other computer systems consume output from MT systems (or similarly unreliable tools), we see a different pattern. In a pipeline setting it is commonplace to propagate not just a singlebest output but the M-best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Collins and Koo, 2005; Charniak and Johnson, 2005), tuning (Och, 2003), minimum Bayes risk decoding (Kumar and Byrne, 2004), and system combination (Rosti et al., 2007). When dealing with error-prone systems, knowing about alternatives has benefits over relying on only a single output (Finkel et al., 2006; Dyer, 2010). Need for Diversity. Unfortunately, M-best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M-best lists are extremely similar, often differing only by a single punctuation</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>M. Collins. 2000. Discriminative reranking for natural language parsing. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>D Chiang</author>
<author>K Knight</author>
</authors>
<title>Fast consensus decoding over translation forests.</title>
<date>2009</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="9274" citStr="DeNero et al., 2009" startWordPosition="1519" endWordPosition="1522">r n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al. (2013) used bagging and boosting to get diverse system outputs for system combination and Cer et al. (2013) used multiple identical systems trained jointly with an objective function that encourages the systems to generate complementary translations. There is also similarity between our approach and minimum Bayes risk decoding (Kumar and Byrne, 2004), variational decoding (Li et al., 2009), and other “consensus” decoding algorithms (DeNero et al., 2009). These all seek a single translation that is most similar on average to the model’s preferred translations. In this way, they try to capture the model’s range of beliefs in a single translation. We instead seek a set of translations that, when considered as a whole, similarly express the full range of the model’s beliefs about plausible translations for the input. Also related is work on determinantal point processes (DPPs; Kulesza and Taskar, 2010), an elegant probabilistic model over sets of items that naturally prefers diverse sets. DPPs have been applied to summarization (Kulesza and Task</context>
</contexts>
<marker>DeNero, Chiang, Knight, 2009</marker>
<rawString>J. DeNero, D. Chiang, and K. Knight. 2009. Fast consensus decoding over translation forests. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Devlin</author>
<author>S Matsoukas</author>
</authors>
<title>Trait-based hypothesis selection for machine translation.</title>
<date>2012</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="8362" citStr="Devlin and Matsoukas (2012)" startWordPosition="1376" endWordPosition="1379">n previous diverse translations, and sets to zero all other n-grams’ log-probabilities and back-off weights. argmax wTO(x, y, h) + m−1� (y,h)ET. j=1 1101 The advantage of this dissimilarity function is its simplicity. It can be easily used with any translation system that uses n-gram language models without any change to the decoder. Indeed, we use both phrase-based and hierarchical phrase-based models in our experiments below. 4 Related Work MT researchers have recently started to consider diversity in the context of system combination (Macherey and Och, 2007). Most closelyrelated is work by Devlin and Matsoukas (2012), who proposed a way to generate diverse translations by varying particular “traits,” such as translation length, number of rules applied, etc. Their approach can be viewed as solving Eq. (2) with a richer dissimilarity function that requires a special-purpose decoding algorithm. We chose our n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al. (2013) used bagging and boosting to get diverse system outputs for system combination and Cer et al. (2013) used multiple identical systems train</context>
<context position="15368" citStr="Devlin and Matsoukas, 2012" startWordPosition="2549" endWordPosition="2552">icular tasks below. Though simple, our approach is computationally expensive as M grows because it requires decoding M times for each sentence. So, we assume M G 20. But we also extract an N-best list for each of the M diverse translations.2 Many MT decoders, including the phrase-based and hierarchical implementations in Moses, permit efficient extraction of N-best lists, so we exploit this to obtain larger lists that still exhibit diversity. But we note that these N-best lists for each diverse solution are not in themselves diverse; with more computational power or more efficient algorithms (Devlin and Matsoukas, 2012) we could potentially generate larger, more diverse lists. 6 Analysis of Diverse Lists We now characterize our diverse lists by comparing them to M-best lists. Table 1 shows oracle BLEU scores on TEST for M-best lists, unique Mbest lists, and diverse lists of several sizes. To get unique lists, we first generated 1000-best lists, then retained only the highest-scoring derivation for each unique translation. When comparing M-best and diverse lists of comparable size, the diverse lists al1Since BLEU does not decompose additively across segments, we chose translations for individual sentences tha</context>
<context position="18534" citStr="Devlin and Matsoukas (2012)" startWordPosition="3081" endWordPosition="3084">e these values depend on the scale of the weights learned by MERT, they are difficult to interpret in isolation. lists drop when the baseline system has high BLEU score. This matches intuition: when the baseline system is performing well, forcing it to find different translations is likely to result in worse translations. So we may expect diverse lists to be most helpful for more difficult sentences, a point we return to in our experiments below. 7 System Combination Experiments One way to evaluate the quality of our diverse lists is to use them in system combination, as was similarly done by Devlin and Matsoukas (2012) and Cer et al. (2013). We use the system combination framework of Heafield and Lavie (2010b), which has an open-source implementation (Heafield and Lavie, 2010a).4 We use our baseline systems (trained on TUNE1) to generate lists for system combination on TUNE2 and TEST. We compare M-best lists, unique M-best lists, and M-diverse lists, with M ∈ {10,15, 20}.5 For each choice of list type and M, we trained the system combiner on TUNE2 and tested on TEST with the learned parameters. System combination hyperparameters (whether to use feature length normalization; the size of the k-best lists gene</context>
<context position="38806" citStr="Devlin and Matsoukas (2012)" startWordPosition="6514" endWordPosition="6518">c confidence estimation could be used to identify situations in which diverse translations can be helpful for aiding user understanding. 10 Future Work Our dissimilarity function captures diversity in the particular phrases used by an MT system, but for certain applications we may prefer other types of diversity. Defining the dissimilarity function on POS tags or word clusters would help us to capture stylistic patterns in sentence structure, as would targeting syntactic structures in syntax-based translation. A weakness of our approach is its computational expense; by contrast, the method of Devlin and Matsoukas (2012) obtains diverse translations more efficiently by extracting them from a single decoding of an input sentence (albeit with a wide beam). We expect their ideas to be directly applicable to our setting in order to get diverse solutions more cheaply. We also plan to explore methods of explicitly targeting multiple, diverse solutions as part of the search algorithm. Finally, M-best lists are currently used to approximate structured spaces for many areas of MT, including tuning (Och, 2003), minimum Bayes risk decoding (Kumar and Byrne, 2004), and pipelines (Venugopal et al., 2008). Future work coul</context>
</contexts>
<marker>Devlin, Matsoukas, 2012</marker>
<rawString>J. Devlin and S. Matsoukas. 2012. Trait-based hypothesis selection for machine translation. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
</authors>
<title>Using a maximum entropy model to build segmentation lattices for MT.</title>
<date>2009</date>
<booktitle>In Proc. of HLTNAACL.</booktitle>
<contexts>
<context position="12277" citStr="Dyer, 2009" startWordPosition="2015" endWordPosition="2016">M Arabic tokens and 130M English tokens. We used the NIST MT06 test set as TUNE1, a 764-sentence subset of MT05 as TUNE2, and MT08 as TEST. For ZH—*EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter (Chang et al., 2008) in “CTB” mode, giving us 7.9M Chinese tokens and 9.4M English tokens. We used the NIST MT02 test set as TUNE1, MT05 1102 as TUNE2, and MT03 as TEST. For DE→EN, we used data released for the WMT2011 shared task (Callison-Burch et al., 2011). German compound words were split using a CRF segmenter (Dyer, 2009). We used the WMT2010 test set as TUNE1, the 2009 test set as TUNE2, and the 2011 test set as TEST. 5.2 Baseline Systems We used the Moses MT toolkit (Koehn et al., 2007; Hoang et al., 2009) with default settings and features for both phrase-based and hierarchical systems. Word alignment was done using GIZA++ (Och and Ney, 2003) in both directions, with the grow-diag-final-and heuristic used to symmetrize the alignments and a max phrase length of 7 used for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of ra</context>
</contexts>
<marker>Dyer, 2009</marker>
<rawString>C. Dyer. 2009. Using a maximum entropy model to build segmentation lattices for MT. In Proc. of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
</authors>
<title>A Formal Model ofAmbiguity and its Applications in Machine Translation.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Maryland.</institution>
<contexts>
<context position="2168" citStr="Dyer, 2010" startWordPosition="332" endWordPosition="333"> MT systems (or similarly unreliable tools), we see a different pattern. In a pipeline setting it is commonplace to propagate not just a singlebest output but the M-best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Collins and Koo, 2005; Charniak and Johnson, 2005), tuning (Och, 2003), minimum Bayes risk decoding (Kumar and Byrne, 2004), and system combination (Rosti et al., 2007). When dealing with error-prone systems, knowing about alternatives has benefits over relying on only a single output (Finkel et al., 2006; Dyer, 2010). Need for Diversity. Unfortunately, M-best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M-best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the implicit goal behi</context>
</contexts>
<marker>Dyer, 2010</marker>
<rawString>C. Dyer. 2010. A Formal Model ofAmbiguity and its Applications in Machine Translation. Ph.D. thesis, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Solving the problem of cascading errors: Approximate Bayesian inference for linguistic annotation pipelines.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2155" citStr="Finkel et al., 2006" startWordPosition="328" endWordPosition="331">s consume output from MT systems (or similarly unreliable tools), we see a different pattern. In a pipeline setting it is commonplace to propagate not just a singlebest output but the M-best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Collins and Koo, 2005; Charniak and Johnson, 2005), tuning (Och, 2003), minimum Bayes risk decoding (Kumar and Byrne, 2004), and system combination (Rosti et al., 2007). When dealing with error-prone systems, knowing about alternatives has benefits over relying on only a single output (Finkel et al., 2006; Dyer, 2010). Need for Diversity. Unfortunately, M-best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M-best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the impli</context>
</contexts>
<marker>Finkel, Manning, Ng, 2006</marker>
<rawString>J. R. Finkel, C. D. Manning, and A. Y. Ng. 2006. Solving the problem of cascading errors: Approximate Bayesian inference for linguistic annotation pipelines. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Gertz</author>
<author>S J Wright</author>
</authors>
<title>Object-oriented software for quadratic programming.</title>
<date>2003</date>
<journal>ACM Transactions on Mathematical Software,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="24022" citStr="Gertz and Wright, 2003" startWordPosition="4008" endWordPosition="4011">our et al. (2013), who used a slack-rescaled structured support vector machine (Tsochantaridis et al., 2005) with L2 regularization. As a sentencelevel loss, we used negated BLEU+1. We used the 1-slack cutting-plane algorithm of Joachims et al. (2009) for optimization during learning.8 A more detailed description of the reranker is provided in the supplementary material. We used 5-fold cross-validation on TUNE2 to choose the regularization parameter C from the set 10.01, 0.1,1,101. We selected the value yielding the highest average BLEU score across the held-out 8Our implementation uses OOQP (Gertz and Wright, 2003) to solve the quadratic program in the inner loop, which uses HSL, a collection of Fortran codes for large-scale scientific computation (www.hsl.rl.ac.uk). 1105 folds. This value was then used for one final round of training on the entirety of TUNE2. Additionally, we tuned the decision to return the parameters at convergence or those that produced the highest training corpus BLEU score. Since we use a sentencelevel metric during training (BLEU+1) and a corpuslevel metric for final evaluation (BLEU), we found that it was often better to return parameters that produced the highest training BLEU </context>
</contexts>
<marker>Gertz, Wright, 2003</marker>
<rawString>E. M. Gertz and S. J. Wright. 2003. Object-oriented software for quadratic programming. ACM Transactions on Mathematical Software, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gillenwater</author>
<author>A Kulesza</author>
<author>B Taskar</author>
</authors>
<title>Discovering diverse and salient threads in document collections.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="9967" citStr="Gillenwater et al., 2012" startWordPosition="1634" endWordPosition="1637">e to the model’s preferred translations. In this way, they try to capture the model’s range of beliefs in a single translation. We instead seek a set of translations that, when considered as a whole, similarly express the full range of the model’s beliefs about plausible translations for the input. Also related is work on determinantal point processes (DPPs; Kulesza and Taskar, 2010), an elegant probabilistic model over sets of items that naturally prefers diverse sets. DPPs have been applied to summarization (Kulesza and Taskar, 2011) and discovery of topical threads in document collections (Gillenwater et al., 2012). Unfortunately, in the structured setting, DPPs make severely restrictive assumptions on the scoring function, while our framework does not. 5 Experimental Setup We now embark on an extensive empirical evaluation of the framework presented above. We begin by analyzing our diverse sets of translations, showing how they differ from standard M-best lists (Section 6), followed by three tasks that illustrate how diversity can be exploited to improve translation quality: system combination (Section 7), discriminative reranking (Section 8), and a novel human postediting task (Section 9). In the rema</context>
</contexts>
<marker>Gillenwater, Kulesza, Taskar, 2012</marker>
<rawString>J. Gillenwater, A. Kulesza, and B. Taskar. 2012. Discovering diverse and salient threads in document collections. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Heafield</author>
<author>A Lavie</author>
</authors>
<title>Combining machine translation output with open source: The Carnegie Mellon multi-engine machine translation scheme.</title>
<date>2010</date>
<journal>The Prague Bulletin of Mathematical Linguistics,</journal>
<volume>93</volume>
<contexts>
<context position="18625" citStr="Heafield and Lavie (2010" startWordPosition="3097" endWordPosition="3100">pret in isolation. lists drop when the baseline system has high BLEU score. This matches intuition: when the baseline system is performing well, forcing it to find different translations is likely to result in worse translations. So we may expect diverse lists to be most helpful for more difficult sentences, a point we return to in our experiments below. 7 System Combination Experiments One way to evaluate the quality of our diverse lists is to use them in system combination, as was similarly done by Devlin and Matsoukas (2012) and Cer et al. (2013). We use the system combination framework of Heafield and Lavie (2010b), which has an open-source implementation (Heafield and Lavie, 2010a).4 We use our baseline systems (trained on TUNE1) to generate lists for system combination on TUNE2 and TEST. We compare M-best lists, unique M-best lists, and M-diverse lists, with M ∈ {10,15, 20}.5 For each choice of list type and M, we trained the system combiner on TUNE2 and tested on TEST with the learned parameters. System combination hyperparameters (whether to use feature length normalization; the size of the k-best lists generated by the system combiner during tuning, k ∈ {300, 600}) were chosen to maximize BLEU on</context>
</contexts>
<marker>Heafield, Lavie, 2010</marker>
<rawString>K. Heafield and A. Lavie. 2010a. Combining machine translation output with open source: The Carnegie Mellon multi-engine machine translation scheme. The Prague Bulletin of Mathematical Linguistics, 93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Heafield</author>
<author>A Lavie</author>
</authors>
<title>Voting on n-grams for machine translation system combination.</title>
<date>2010</date>
<booktitle>In Proc. of AMTA.</booktitle>
<contexts>
<context position="18625" citStr="Heafield and Lavie (2010" startWordPosition="3097" endWordPosition="3100">pret in isolation. lists drop when the baseline system has high BLEU score. This matches intuition: when the baseline system is performing well, forcing it to find different translations is likely to result in worse translations. So we may expect diverse lists to be most helpful for more difficult sentences, a point we return to in our experiments below. 7 System Combination Experiments One way to evaluate the quality of our diverse lists is to use them in system combination, as was similarly done by Devlin and Matsoukas (2012) and Cer et al. (2013). We use the system combination framework of Heafield and Lavie (2010b), which has an open-source implementation (Heafield and Lavie, 2010a).4 We use our baseline systems (trained on TUNE1) to generate lists for system combination on TUNE2 and TEST. We compare M-best lists, unique M-best lists, and M-diverse lists, with M ∈ {10,15, 20}.5 For each choice of list type and M, we trained the system combiner on TUNE2 and tested on TEST with the learned parameters. System combination hyperparameters (whether to use feature length normalization; the size of the k-best lists generated by the system combiner during tuning, k ∈ {300, 600}) were chosen to maximize BLEU on</context>
</contexts>
<marker>Heafield, Lavie, 2010</marker>
<rawString>K. Heafield and A. Lavie. 2010b. Voting on n-grams for machine translation system combination. In Proc. of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Heafield</author>
</authors>
<title>Kenlm: Faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="13287" citStr="Heafield, 2011" startWordPosition="2181" endWordPosition="2182">to symmetrize the alignments and a max phrase length of 7 used for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used 5-gram models, estimated using the SRI Language Modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The minimum count cut-off for unigrams, bigrams, and trigrams was 1 and the cut-off for 4-grams and 5-grams was 3. Language model inference used KenLM (Heafield, 2011). Uncased IBM BLEU was used for evaluation (Papineni et al., 2002). MERT was used to train the feature weights for the baseline systems on TUNE1. We used the learned parameters to generate M-best and diverse lists for TUNE2 and TEST to use for subsequent experiments. 5.3 Diverse List Generation Generating diverse translations depends on two hyperparameters: the n-gram order used by the dissimilarity function An (§3.2) and the Aj weights on the dissimilarity terms in Eq. (2). Though our framework permits different Aj for each j, we use a single A value for simplicity, as was also done in (Batra</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>K. Heafield. 2011. Kenlm: Faster and smaller language model queries. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hildebrand</author>
<author>S Vogel</author>
</authors>
<title>Combination of machine translation systems via hypothesis selection from combined n-best lists.</title>
<date>2008</date>
<booktitle>In Proc. of AMTA.</booktitle>
<contexts>
<context position="22828" citStr="Hildebrand and Vogel, 2008" startWordPosition="3823" endWordPosition="3826">each sentence. Then system combination of diverse translations might be used only when the 1-best translation is predicted to be of low quality. 8 Reranking Experiments We now turn to discriminative reranking, which has frequently been used to easily add rich features to a model. It has been used for MT with varying de6They reported +0.8 BLEU from system combination for AR--+EN, and saw a further +0.5–0.7 from their new features. 7Quartile points are: 39, 49, 61 for AR--+EN; 25, 36, and 47 for ZH--+EN; and 14.5, 21.1, and 30.3 for DE--+EN. gree of success (Och et al., 2004; Shen et al., 2004; Hildebrand and Vogel, 2008); some have attributed its mixed results to a lack of diversity in the M-best lists traditionally used. We propose diverse lists as a way to address this concern. 8.1 Learning Framework Several learning formulations have been proposed for M-best reranking. One commonly-used approach in MT is MERT, used in the reranking experiments of Och et al. (2004) and Hildebrand and Vogel (2008), among others. We experimented with MERT and other algorithms, including pairwise ranking optimization (Hopkins and May, 2011), but we found best results using the approach of Yadollahpour et al. (2013), who used a</context>
<context position="25066" citStr="Hildebrand and Vogel (2008)" startWordPosition="4185" endWordPosition="4188">l metric during training (BLEU+1) and a corpuslevel metric for final evaluation (BLEU), we found that it was often better to return parameters that produced the highest training BLEU score. This tuning procedure was repeated for each feature set and for each list type (M-best or diverse). The test set was not used for any of this tuning. 8.2 Features In addition to the features from the baseline models (14 for phrase-based, 8 for hierarchical), we add 36 more for reranking: Inverse Model 1 (INVMOD1): We added the “inverse” versions of the three IBM Model 1 features described in Section 2.2 of Hildebrand and Vogel (2008). The first is the probability of the source sentence given the translation under IBM Model 1, the second replaces the E with a max in the first feature, and the third computes the percentage of words whose lexical translation probability falls below a threshold. We also include versions of the first 2 features normalized by the translation length, for a total of 5 INVMOD1 features. Large LM (LLM): We created a large 4-gram LM by interpolating LMs from the WMT news data, Gigaword, Europarl, and the DE→EN news commentary (NC) corpus to maximize likelihood of a heldout development set (WMT08 tes</context>
</contexts>
<marker>Hildebrand, Vogel, 2008</marker>
<rawString>A. Hildebrand and S. Vogel. 2008. Combination of machine translation systems via hypothesis selection from combined n-best lists. In Proc. of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hoang</author>
<author>P Koehn</author>
<author>A Lopez</author>
</authors>
<title>A Unified Framework for Phrase-Based, Hierarchical, and Syntax-Based Statistical Machine Translation.</title>
<date>2009</date>
<booktitle>In Proc. of IWSLT.</booktitle>
<contexts>
<context position="12467" citStr="Hoang et al., 2009" startWordPosition="2051" endWordPosition="2054">om the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter (Chang et al., 2008) in “CTB” mode, giving us 7.9M Chinese tokens and 9.4M English tokens. We used the NIST MT02 test set as TUNE1, MT05 1102 as TUNE2, and MT03 as TEST. For DE→EN, we used data released for the WMT2011 shared task (Callison-Burch et al., 2011). German compound words were split using a CRF segmenter (Dyer, 2009). We used the WMT2010 test set as TUNE1, the 2009 test set as TUNE2, and the 2011 test set as TEST. 5.2 Baseline Systems We used the Moses MT toolkit (Koehn et al., 2007; Hoang et al., 2009) with default settings and features for both phrase-based and hierarchical systems. Word alignment was done using GIZA++ (Och and Ney, 2003) in both directions, with the grow-diag-final-and heuristic used to symmetrize the alignments and a max phrase length of 7 used for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used 5-gram models, estimated using the SRI Language Modeling toolkit (Stolcke, 2002) with mod</context>
</contexts>
<marker>Hoang, Koehn, Lopez, 2009</marker>
<rawString>H. Hoang, P. Koehn, and A. Lopez. 2009. A Unified Framework for Phrase-Based, Hierarchical, and Syntax-Based Statistical Machine Translation. In Proc. of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hopkins</author>
<author>J May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="23340" citStr="Hopkins and May, 2011" startWordPosition="3904" endWordPosition="3907"> 21.1, and 30.3 for DE--+EN. gree of success (Och et al., 2004; Shen et al., 2004; Hildebrand and Vogel, 2008); some have attributed its mixed results to a lack of diversity in the M-best lists traditionally used. We propose diverse lists as a way to address this concern. 8.1 Learning Framework Several learning formulations have been proposed for M-best reranking. One commonly-used approach in MT is MERT, used in the reranking experiments of Och et al. (2004) and Hildebrand and Vogel (2008), among others. We experimented with MERT and other algorithms, including pairwise ranking optimization (Hopkins and May, 2011), but we found best results using the approach of Yadollahpour et al. (2013), who used a slack-rescaled structured support vector machine (Tsochantaridis et al., 2005) with L2 regularization. As a sentencelevel loss, we used negated BLEU+1. We used the 1-slack cutting-plane algorithm of Joachims et al. (2009) for optimization during learning.8 A more detailed description of the reranker is provided in the supplementary material. We used 5-fold cross-validation on TUNE2 to choose the regularization parameter C from the set 10.01, 0.1,1,101. We selected the value yielding the highest average BLE</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>M. Hopkins and J. May. 2011. Tuning as ranking. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2302" citStr="Huang, 2008" startWordPosition="352" endWordPosition="353">t a singlebest output but the M-best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Collins and Koo, 2005; Charniak and Johnson, 2005), tuning (Och, 2003), minimum Bayes risk decoding (Kumar and Byrne, 2004), and system combination (Rosti et al., 2007). When dealing with error-prone systems, knowing about alternatives has benefits over relying on only a single output (Finkel et al., 2006; Dyer, 2010). Need for Diversity. Unfortunately, M-best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M-best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the implicit goal behind these techniques is to better explore the output space by introducing diversity into the surrogate set. Overview and Contributions.</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>L. Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
<author>T Finley</author>
<author>C Yu</author>
</authors>
<title>Cuttingplane training of structural SVMs.</title>
<date>2009</date>
<booktitle>Machine Learning,</booktitle>
<volume>77</volume>
<issue>1</issue>
<contexts>
<context position="23650" citStr="Joachims et al. (2009)" startWordPosition="3953" endWordPosition="3956">ng formulations have been proposed for M-best reranking. One commonly-used approach in MT is MERT, used in the reranking experiments of Och et al. (2004) and Hildebrand and Vogel (2008), among others. We experimented with MERT and other algorithms, including pairwise ranking optimization (Hopkins and May, 2011), but we found best results using the approach of Yadollahpour et al. (2013), who used a slack-rescaled structured support vector machine (Tsochantaridis et al., 2005) with L2 regularization. As a sentencelevel loss, we used negated BLEU+1. We used the 1-slack cutting-plane algorithm of Joachims et al. (2009) for optimization during learning.8 A more detailed description of the reranker is provided in the supplementary material. We used 5-fold cross-validation on TUNE2 to choose the regularization parameter C from the set 10.01, 0.1,1,101. We selected the value yielding the highest average BLEU score across the held-out 8Our implementation uses OOQP (Gertz and Wright, 2003) to solve the quadratic program in the inner loop, which uses HSL, a collection of Fortran codes for large-scale scientific computation (www.hsl.rl.ac.uk). 1105 folds. This value was then used for one final round of training on </context>
</contexts>
<marker>Joachims, Finley, Yu, 2009</marker>
<rawString>T. Joachims, T. Finley, and C. Yu. 2009. Cuttingplane training of structural SVMs. Machine Learning, 77(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="5160" citStr="Koehn et al., 2003" startWordPosition="832" endWordPosition="835">h E xx, where xx is the set of possible values of h for x. Derivations are coupled with translations and we define Tx C �x x xx as the set of possible (y, h) pairs for x. We use a linear model with a parameter vector w and a vector O(x, y, h) of feature functions on x, y, and h (Och and Ney, 2002). The translation of x is selected using a simple decision rule: (ˆy, ˆh) = argmax wTO(x, y, h) (1) (y,h)ET. where we also maximize over the latent variable h for efficiency. Translation models differ in the form of Tx and the choice of the feature functions O. In this paper we focus on phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) models, which include several bilingual and monolingual features, including n-gram language models. 3 Diversity in Machine Translation We now address the task of producing a set of diverse high-scoring translations. 3.1 Generating Diverse Translations We use a recently proposed technique (Batra et al., 2012) that constructs diverse lists via a greedy iterative procedure as follows. Let y1 be the model-best translation (Eq. 1). On the m-th iteration, the m-th best (diverse) translation is obtained as (ym, hm) = AjΔ(yj, y) (2) where Δ is a dissimilar</context>
<context position="10859" citStr="Koehn et al., 2003" startWordPosition="1771" endWordPosition="1774"> diverse sets of translations, showing how they differ from standard M-best lists (Section 6), followed by three tasks that illustrate how diversity can be exploited to improve translation quality: system combination (Section 7), discriminative reranking (Section 8), and a novel human postediting task (Section 9). In the remainder of this section, we describe details of our experimental setup. 5.1 Language Pairs and Datasets We use three language pairs: Arabic-to-English (AR—*EN), Chinese-to-English (ZH—*EN), and German-to-English (DE—*EN). For AR—*EN and DE—*EN, we used a phrase-based model (Koehn et al., 2003) and for ZH—*EN we used a hierarchical phrase-based model (Chiang, 2007). Each language pair has two tuning and one test set: TUNE1 is used for tuning the baseline systems with minimum error rate training (MERT; Och, 2003), TUNE2 is used for training system combiners and rerankers, and TEST is used for evaluation. There are four references for AR—*EN and ZH—*EN and one for DE—*EN. For AR—*EN, we used data provided by the LDC for the NIST evaluations, which includes 3.3M sentences of UN data and 982K sentences from other (mostly news) sources. Arabic text was preprocessed using an HMM segmenter</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL</booktitle>
<note>(demo session).</note>
<contexts>
<context position="12446" citStr="Koehn et al., 2007" startWordPosition="2047" endWordPosition="2050">3k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter (Chang et al., 2008) in “CTB” mode, giving us 7.9M Chinese tokens and 9.4M English tokens. We used the NIST MT02 test set as TUNE1, MT05 1102 as TUNE2, and MT03 as TEST. For DE→EN, we used data released for the WMT2011 shared task (Callison-Burch et al., 2011). German compound words were split using a CRF segmenter (Dyer, 2009). We used the WMT2010 test set as TUNE1, the 2009 test set as TUNE2, and the 2011 test set as TEST. 5.2 Baseline Systems We used the Moses MT toolkit (Koehn et al., 2007; Hoang et al., 2009) with default settings and features for both phrase-based and hierarchical systems. Word alignment was done using GIZA++ (Och and Ney, 2003) in both directions, with the grow-diag-final-and heuristic used to symmetrize the alignments and a max phrase length of 7 used for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used 5-gram models, estimated using the SRI Language Modeling toolkit (St</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL (demo session).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Enabling monolingual translators: Postediting vs. options.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="31689" citStr="Koehn (2010)" startWordPosition="5313" endWordPosition="5314">nt in typical M-best lists, thereby hindering its ability to correctly rank diverse lists at test time. These results suggest that part of the benefit of using diverse lists comes from seeing a larger portion of the output space during training. 9 Human Post-Editing Experiments We wanted to determine whether diverse translations could be helpful to users struggling to understand the output of an imperfect MT system. We consider a post-editing task in which users are presented with translation output without the source sentence, and are asked to improve it. This setting has been studied; e.g., Koehn (2010) presented evidence that monolingual speakers could often produce improved translations for this task, occasionally reaching the level of an expert translator. Here, we use a novel variation of this task in which multiple translations are shown to editors. We compare the use of entries from an M-best list and entries from a diverse list. Again, the original source sentence is not provided. Our goal is to determine whether multiple, diverse translations can help users to more accurately guess the meaning of the original sentence than entries from a standard M-best list. If so, commercial MT sys</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>P. Koehn. 2010. Enabling monolingual translators: Postediting vs. options. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Koponen</author>
</authors>
<title>Comparing human perceptions of post-editing effort with post-editing operations.</title>
<date>2012</date>
<booktitle>In Proc. of WMT.</booktitle>
<contexts>
<context position="38126" citStr="Koponen, 2012" startWordPosition="6412" endWordPosition="6413"> system is below 35, it is preferable to give diverse translations to users for post-editing. But when the baseline system does very well, diverse translations do not contribute anything, and in fact hurt because they may distract users from the high-quality (and typically very similar) translations from the 5-best lists. Estimation of the quality of the output (“confidence estimation”) has recently gained interest in the MT community (Specia et al., 2011; Bach et al., 2011; Callison-Burch et al., 2012; Bojar et al., 2013), including specifically for post-editing (Tatsumi, 2009; Specia, 2011; Koponen, 2012). Future work could investigate whether such automatic confidence estimation could be used to identify situations in which diverse translations can be helpful for aiding user understanding. 10 Future Work Our dissimilarity function captures diversity in the particular phrases used by an MT system, but for certain applications we may prefer other types of diversity. Defining the dissimilarity function on POS tags or word clusters would help us to capture stylistic patterns in sentence structure, as would targeting syntactic structures in syntax-based translation. A weakness of our approach is i</context>
</contexts>
<marker>Koponen, 2012</marker>
<rawString>M. Koponen. 2012. Comparing human perceptions of post-editing effort with post-editing operations. In Proc. of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kulesza</author>
<author>B Taskar</author>
</authors>
<title>Structured determinantal point processes.</title>
<date>2010</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="9728" citStr="Kulesza and Taskar, 2010" startWordPosition="1595" endWordPosition="1598">r approach and minimum Bayes risk decoding (Kumar and Byrne, 2004), variational decoding (Li et al., 2009), and other “consensus” decoding algorithms (DeNero et al., 2009). These all seek a single translation that is most similar on average to the model’s preferred translations. In this way, they try to capture the model’s range of beliefs in a single translation. We instead seek a set of translations that, when considered as a whole, similarly express the full range of the model’s beliefs about plausible translations for the input. Also related is work on determinantal point processes (DPPs; Kulesza and Taskar, 2010), an elegant probabilistic model over sets of items that naturally prefers diverse sets. DPPs have been applied to summarization (Kulesza and Taskar, 2011) and discovery of topical threads in document collections (Gillenwater et al., 2012). Unfortunately, in the structured setting, DPPs make severely restrictive assumptions on the scoring function, while our framework does not. 5 Experimental Setup We now embark on an extensive empirical evaluation of the framework presented above. We begin by analyzing our diverse sets of translations, showing how they differ from standard M-best lists (Secti</context>
</contexts>
<marker>Kulesza, Taskar, 2010</marker>
<rawString>A. Kulesza and B. Taskar. 2010. Structured determinantal point processes. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kulesza</author>
<author>B Taskar</author>
</authors>
<title>Learning determinantal point processes.</title>
<date>2011</date>
<booktitle>In Proc. of UAI.</booktitle>
<contexts>
<context position="9883" citStr="Kulesza and Taskar, 2011" startWordPosition="1621" endWordPosition="1624">ro et al., 2009). These all seek a single translation that is most similar on average to the model’s preferred translations. In this way, they try to capture the model’s range of beliefs in a single translation. We instead seek a set of translations that, when considered as a whole, similarly express the full range of the model’s beliefs about plausible translations for the input. Also related is work on determinantal point processes (DPPs; Kulesza and Taskar, 2010), an elegant probabilistic model over sets of items that naturally prefers diverse sets. DPPs have been applied to summarization (Kulesza and Taskar, 2011) and discovery of topical threads in document collections (Gillenwater et al., 2012). Unfortunately, in the structured setting, DPPs make severely restrictive assumptions on the scoring function, while our framework does not. 5 Experimental Setup We now embark on an extensive empirical evaluation of the framework presented above. We begin by analyzing our diverse sets of translations, showing how they differ from standard M-best lists (Section 6), followed by three tasks that illustrate how diversity can be exploited to improve translation quality: system combination (Section 7), discriminativ</context>
</contexts>
<marker>Kulesza, Taskar, 2011</marker>
<rawString>A. Kulesza and B. Taskar. 2011. Learning determinantal point processes. In Proc. of UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>Minimum bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="1972" citStr="Kumar and Byrne, 2004" startWordPosition="299" endWordPosition="302">al, imperfect MT system makes an error, the user is left trying to guess what the original sentence means. Multiple Hypotheses. In contrast, when we look at the way other computer systems consume output from MT systems (or similarly unreliable tools), we see a different pattern. In a pipeline setting it is commonplace to propagate not just a singlebest output but the M-best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Collins and Koo, 2005; Charniak and Johnson, 2005), tuning (Och, 2003), minimum Bayes risk decoding (Kumar and Byrne, 2004), and system combination (Rosti et al., 2007). When dealing with error-prone systems, knowing about alternatives has benefits over relying on only a single output (Finkel et al., 2006; Dyer, 2010). Need for Diversity. Unfortunately, M-best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M-best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs</context>
<context position="9169" citStr="Kumar and Byrne, 2004" startWordPosition="1504" endWordPosition="1507">q. (2) with a richer dissimilarity function that requires a special-purpose decoding algorithm. We chose our n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al. (2013) used bagging and boosting to get diverse system outputs for system combination and Cer et al. (2013) used multiple identical systems trained jointly with an objective function that encourages the systems to generate complementary translations. There is also similarity between our approach and minimum Bayes risk decoding (Kumar and Byrne, 2004), variational decoding (Li et al., 2009), and other “consensus” decoding algorithms (DeNero et al., 2009). These all seek a single translation that is most similar on average to the model’s preferred translations. In this way, they try to capture the model’s range of beliefs in a single translation. We instead seek a set of translations that, when considered as a whole, similarly express the full range of the model’s beliefs about plausible translations for the input. Also related is work on determinantal point processes (DPPs; Kulesza and Taskar, 2010), an elegant probabilistic model over set</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>S. Kumar and W. Byrne. 2004. Minimum bayes-risk decoding for statistical machine translation. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Macherey</author>
<author>C Dyer</author>
<author>F Och</author>
</authors>
<title>Efficient minimum error rate training and minimum Bayes-risk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP.</booktitle>
<contexts>
<context position="2638" citStr="Kumar et al., 2009" startWordPosition="405" endWordPosition="408">When dealing with error-prone systems, knowing about alternatives has benefits over relying on only a single output (Finkel et al., 2006; Dyer, 2010). Need for Diversity. Unfortunately, M-best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M-best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the implicit goal behind these techniques is to better explore the output space by introducing diversity into the surrogate set. Overview and Contributions. In this work, we elevate diversity to a first-class status and directly address the problem of generating a set of diverse, plausible translations. We use the recently proposed technique of Batra et al. (2012), which produces diverse M-best solutions from a probabilistic model using a generic dissimilarity function A(, ·) that specif</context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Efficient minimum error rate training and minimum Bayes-risk decoding for translation hypergraphs and lattices. In Proc. of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lee</author>
<author>K Papineni</author>
<author>S Roukos</author>
<author>O Emam</author>
<author>H Hassan</author>
</authors>
<title>Language model based Arabic word segmentation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="11563" citStr="Lee et al., 2003" startWordPosition="1890" endWordPosition="1893"> pair has two tuning and one test set: TUNE1 is used for tuning the baseline systems with minimum error rate training (MERT; Och, 2003), TUNE2 is used for training system combiners and rerankers, and TEST is used for evaluation. There are four references for AR—*EN and ZH—*EN and one for DE—*EN. For AR—*EN, we used data provided by the LDC for the NIST evaluations, which includes 3.3M sentences of UN data and 982K sentences from other (mostly news) sources. Arabic text was preprocessed using an HMM segmenter that splits attached prepositional phrases, personal pronouns, and the future marker (Lee et al., 2003). The common stylistic sentence-initial w+ (and) clitic was removed. The resulting corpus contained 130M Arabic tokens and 130M English tokens. We used the NIST MT06 test set as TUNE1, a 764-sentence subset of MT05 as TUNE2, and MT08 as TEST. For ZH—*EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter (Chang et al., 2008) in “CTB” mode, giving us 7.9M Chinese tokens and 9.4M English tokens. We used the NIST MT02 test set as TUNE1, MT05 1102 as TUNE2, and MT03 as TEST. For DE→EN, we used data released for the WMT2</context>
</contexts>
<marker>Lee, Papineni, Roukos, Emam, Hassan, 2003</marker>
<rawString>Y. Lee, K. Papineni, S. Roukos, O. Emam, and H. Hassan. 2003. Language model based Arabic word segmentation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Li</author>
<author>J Eisner</author>
<author>S Khudanpur</author>
</authors>
<title>Variational decoding for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="9209" citStr="Li et al., 2009" startWordPosition="1510" endWordPosition="1513">at requires a special-purpose decoding algorithm. We chose our n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al. (2013) used bagging and boosting to get diverse system outputs for system combination and Cer et al. (2013) used multiple identical systems trained jointly with an objective function that encourages the systems to generate complementary translations. There is also similarity between our approach and minimum Bayes risk decoding (Kumar and Byrne, 2004), variational decoding (Li et al., 2009), and other “consensus” decoding algorithms (DeNero et al., 2009). These all seek a single translation that is most similar on average to the model’s preferred translations. In this way, they try to capture the model’s range of beliefs in a single translation. We instead seek a set of translations that, when considered as a whole, similarly express the full range of the model’s beliefs about plausible translations for the input. Also related is work on determinantal point processes (DPPs; Kulesza and Taskar, 2010), an elegant probabilistic model over sets of items that naturally prefers divers</context>
</contexts>
<marker>Li, Eisner, Khudanpur, 2009</marker>
<rawString>Z. Li, J. Eisner, and S. Khudanpur. 2009. Variational decoding for statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
</authors>
<title>Semi-supervised learning for natural language. Master’s thesis,</title>
<date>2005</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="27531" citStr="Liang (2005)" startWordPosition="4608" endWordPosition="4609">inus BLEU+1 as the weight (learning a language model of “bad” translations). This yielded 4 features. The procedure was then repeated using POS tags instead of words, for 8 features in total. Google 5-Grams (GOOG): Translations were compared to the Google 5-gram corpus (LDC2006T13) to compute: the number of 5-grams that matched, the number of 5-grams that missed, and a set of indicator features that fire if the fraction of 5- grams that matched in the sentence was greater than 10.05, 0.1, 0.2, ... , 0.91, for a total of 12 features. Word Cluster LMs (WCLM): Using an implementation provided by Liang (2005), we performed Brown clustering (Brown et al., 1992) on 900k English sentences, including the NC corpus and random sentences from Gigaword. We clustered words that appeared at least twice, once with 300 clusters and again with 1000. We then replaced words with their clusters in a large corpus consisting of the WMT news data, Gigaword, and the NC data. An additional cluster label was used for unknown words. For each of the clusterings (300 and 1000), we estimated 5- and 7-gram LMs with Witten-Bell smoothing (Witten and Bell, 1991). We added 4 features to the reranker, one for the log-probabilit</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>P. Liang. 2005. Semi-supervised learning for natural language. Master’s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
<author>F J Och</author>
</authors>
<title>Orange: a method for evaluating automatic evaluation metrics for machine translation.</title>
<date>2004</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="16006" citStr="Lin and Och, 2004" startWordPosition="2652" endWordPosition="2655">y generate larger, more diverse lists. 6 Analysis of Diverse Lists We now characterize our diverse lists by comparing them to M-best lists. Table 1 shows oracle BLEU scores on TEST for M-best lists, unique Mbest lists, and diverse lists of several sizes. To get unique lists, we first generated 1000-best lists, then retained only the highest-scoring derivation for each unique translation. When comparing M-best and diverse lists of comparable size, the diverse lists al1Since BLEU does not decompose additively across segments, we chose translations for individual sentences that maximized BLEU+1 (Lin and Och, 2004), then computed “oracle” corpus BLEU of these translations. 2We did not consider n-grams from previous N-best lists when computing the dissimilarity function, but only those from the previous diverse translations. 1103 0-25 25-36 36-47 47-94 1-best BLEU bin Figure 1: Median, min, and max BLEU+1 of 20-best and 20-diverse lists for the ZH→EN test set, divided into quartiles according to the BLEU+1 score of the 1-best translation, and averaged across sentences in each quartile. Heights of the bars show median and “error bars” indicate max and min. ways have higher oracle BLEU. The differences are</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>C. Lin and F. J. Och. 2004. Orange: a method for evaluating automatic evaluation metrics for machine translation. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Macherey</author>
<author>F J Och</author>
</authors>
<title>An empirical study on computing consensus translations from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLPCoNLL.</booktitle>
<contexts>
<context position="8302" citStr="Macherey and Och, 2007" startWordPosition="1366" endWordPosition="1369">he logprobability to the negated count for each n-gram in previous diverse translations, and sets to zero all other n-grams’ log-probabilities and back-off weights. argmax wTO(x, y, h) + m−1� (y,h)ET. j=1 1101 The advantage of this dissimilarity function is its simplicity. It can be easily used with any translation system that uses n-gram language models without any change to the decoder. Indeed, we use both phrase-based and hierarchical phrase-based models in our experiments below. 4 Related Work MT researchers have recently started to consider diversity in the context of system combination (Macherey and Och, 2007). Most closelyrelated is work by Devlin and Matsoukas (2012), who proposed a way to generate diverse translations by varying particular “traits,” such as translation length, number of rules applied, etc. Their approach can be viewed as solving Eq. (2) with a richer dissimilarity function that requires a special-purpose decoding algorithm. We chose our n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al. (2013) used bagging and boosting to get diverse system outputs for system combination</context>
</contexts>
<marker>Macherey, Och, 2007</marker>
<rawString>W. Macherey and F. J. Och. 2007. An empirical study on computing consensus translations from multiple machine translation systems. In Proc. of EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Macherey</author>
<author>F J Och</author>
<author>I Thayer</author>
<author>J Uszkoreit</author>
</authors>
<title>Lattice-based minimum error rate training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2595" citStr="Macherey et al., 2008" startWordPosition="397" endWordPosition="400">and system combination (Rosti et al., 2007). When dealing with error-prone systems, knowing about alternatives has benefits over relying on only a single output (Finkel et al., 2006; Dyer, 2010). Need for Diversity. Unfortunately, M-best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M-best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the implicit goal behind these techniques is to better explore the output space by introducing diversity into the surrogate set. Overview and Contributions. In this work, we elevate diversity to a first-class status and directly address the problem of generating a set of diverse, plausible translations. We use the recently proposed technique of Batra et al. (2012), which produces diverse M-best solutions from a probabilistic model using a generi</context>
</contexts>
<marker>Macherey, Och, Thayer, Uszkoreit, 2008</marker>
<rawString>W. Macherey, F. J. Och, I. Thayer, and J. Uszkoreit. 2008. Lattice-based minimum error rate training for statistical machine translation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="4839" citStr="Och and Ney, 2002" startWordPosition="774" endWordPosition="777">t of all strings in a source language. For an x E X, let �x denote the set of its possible translations y in the target language. MT models typically include a latent variable that captures the derivational structure of the translation process. Regardless of its specific form, we refer to this variable as a derivation h E xx, where xx is the set of possible values of h for x. Derivations are coupled with translations and we define Tx C �x x xx as the set of possible (y, h) pairs for x. We use a linear model with a parameter vector w and a vector O(x, y, h) of feature functions on x, y, and h (Och and Ney, 2002). The translation of x is selected using a simple decision rule: (ˆy, ˆh) = argmax wTO(x, y, h) (1) (y,h)ET. where we also maximize over the latent variable h for efficiency. Translation models differ in the form of Tx and the choice of the feature functions O. In this paper we focus on phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) models, which include several bilingual and monolingual features, including n-gram language models. 3 Diversity in Machine Translation We now address the task of producing a set of diverse high-scoring translations. 3.1 Generating Di</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F. J. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="12607" citStr="Och and Ney, 2003" startWordPosition="2073" endWordPosition="2076">g us 7.9M Chinese tokens and 9.4M English tokens. We used the NIST MT02 test set as TUNE1, MT05 1102 as TUNE2, and MT03 as TEST. For DE→EN, we used data released for the WMT2011 shared task (Callison-Burch et al., 2011). German compound words were split using a CRF segmenter (Dyer, 2009). We used the WMT2010 test set as TUNE1, the 2009 test set as TUNE2, and the 2011 test set as TEST. 5.2 Baseline Systems We used the Moses MT toolkit (Koehn et al., 2007; Hoang et al., 2009) with default settings and features for both phrase-based and hierarchical systems. Word alignment was done using GIZA++ (Och and Ney, 2003) in both directions, with the grow-diag-final-and heuristic used to symmetrize the alignments and a max phrase length of 7 used for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used 5-gram models, estimated using the SRI Language Modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The minimum count cut-off for unigrams, bigrams, and trigrams was 1 and the cut-off for</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>D Gildea</author>
<author>S Khudanpur</author>
<author>A Sarkar</author>
<author>K Yamada</author>
<author>A Fraser</author>
<author>S Kumar</author>
<author>L Shen</author>
<author>D Smith</author>
<author>K Eng</author>
<author>V Jain</author>
<author>Z Jin</author>
<author>D Radev</author>
</authors>
<title>A smorgasbord of features for statistical machine translation.</title>
<date>2004</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="22780" citStr="Och et al., 2004" startWordPosition="3815" endWordPosition="3818">ally determine the BLEU quartile for each sentence. Then system combination of diverse translations might be used only when the 1-best translation is predicted to be of low quality. 8 Reranking Experiments We now turn to discriminative reranking, which has frequently been used to easily add rich features to a model. It has been used for MT with varying de6They reported +0.8 BLEU from system combination for AR--+EN, and saw a further +0.5–0.7 from their new features. 7Quartile points are: 39, 49, 61 for AR--+EN; 25, 36, and 47 for ZH--+EN; and 14.5, 21.1, and 30.3 for DE--+EN. gree of success (Och et al., 2004; Shen et al., 2004; Hildebrand and Vogel, 2008); some have attributed its mixed results to a lack of diversity in the M-best lists traditionally used. We propose diverse lists as a way to address this concern. 8.1 Learning Framework Several learning formulations have been proposed for M-best reranking. One commonly-used approach in MT is MERT, used in the reranking experiments of Och et al. (2004) and Hildebrand and Vogel (2008), among others. We experimented with MERT and other algorithms, including pairwise ranking optimization (Hopkins and May, 2011), but we found best results using the ap</context>
</contexts>
<marker>Och, Gildea, Khudanpur, Sarkar, Yamada, Fraser, Kumar, Shen, Smith, Eng, Jain, Jin, Radev, 2004</marker>
<rawString>F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord of features for statistical machine translation. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1919" citStr="Och, 2003" startWordPosition="292" endWordPosition="293">r et al., 2013). Unfortunately, when a real, imperfect MT system makes an error, the user is left trying to guess what the original sentence means. Multiple Hypotheses. In contrast, when we look at the way other computer systems consume output from MT systems (or similarly unreliable tools), we see a different pattern. In a pipeline setting it is commonplace to propagate not just a singlebest output but the M-best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Collins and Koo, 2005; Charniak and Johnson, 2005), tuning (Och, 2003), minimum Bayes risk decoding (Kumar and Byrne, 2004), and system combination (Rosti et al., 2007). When dealing with error-prone systems, knowing about alternatives has benefits over relying on only a single output (Finkel et al., 2006; Dyer, 2010). Need for Diversity. Unfortunately, M-best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M-best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using pa</context>
<context position="11081" citStr="Och, 2003" startWordPosition="1811" endWordPosition="1812">iscriminative reranking (Section 8), and a novel human postediting task (Section 9). In the remainder of this section, we describe details of our experimental setup. 5.1 Language Pairs and Datasets We use three language pairs: Arabic-to-English (AR—*EN), Chinese-to-English (ZH—*EN), and German-to-English (DE—*EN). For AR—*EN and DE—*EN, we used a phrase-based model (Koehn et al., 2003) and for ZH—*EN we used a hierarchical phrase-based model (Chiang, 2007). Each language pair has two tuning and one test set: TUNE1 is used for tuning the baseline systems with minimum error rate training (MERT; Och, 2003), TUNE2 is used for training system combiners and rerankers, and TEST is used for evaluation. There are four references for AR—*EN and ZH—*EN and one for DE—*EN. For AR—*EN, we used data provided by the LDC for the NIST evaluations, which includes 3.3M sentences of UN data and 982K sentences from other (mostly news) sources. Arabic text was preprocessed using an HMM segmenter that splits attached prepositional phrases, personal pronouns, and the future marker (Lee et al., 2003). The common stylistic sentence-initial w+ (and) clitic was removed. The resulting corpus contained 130M Arabic tokens</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training for statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="13353" citStr="Papineni et al., 2002" startWordPosition="2190" endWordPosition="2194">sed for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used 5-gram models, estimated using the SRI Language Modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The minimum count cut-off for unigrams, bigrams, and trigrams was 1 and the cut-off for 4-grams and 5-grams was 3. Language model inference used KenLM (Heafield, 2011). Uncased IBM BLEU was used for evaluation (Papineni et al., 2002). MERT was used to train the feature weights for the baseline systems on TUNE1. We used the learned parameters to generate M-best and diverse lists for TUNE2 and TEST to use for subsequent experiments. 5.3 Diverse List Generation Generating diverse translations depends on two hyperparameters: the n-gram order used by the dissimilarity function An (§3.2) and the Aj weights on the dissimilarity terms in Eq. (2). Though our framework permits different Aj for each j, we use a single A value for simplicity, as was also done in (Batra et al., 2012). The values of n and A were tuned on a 200 sentence</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pauls</author>
<author>D Klein</author>
</authors>
<title>Large-scale syntactic language modeling with treelets.</title>
<date>2012</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="25858" citStr="Pauls and Klein (2012)" startWordPosition="4322" endWordPosition="4325">mputes the percentage of words whose lexical translation probability falls below a threshold. We also include versions of the first 2 features normalized by the translation length, for a total of 5 INVMOD1 features. Large LM (LLM): We created a large 4-gram LM by interpolating LMs from the WMT news data, Gigaword, Europarl, and the DE→EN news commentary (NC) corpus to maximize likelihood of a heldout development set (WMT08 test set). We used the average per-word log-probability as the single feature function in this category. Syntactic LM (SYN): We used the syntactic treelet language model of Pauls and Klein (2012) to compute two features: the translation log probability and the length-normalized log probability. Finite/Non-Finite Verbs (VERB): We ran the Stanford part-of-speech (POS) tagger (Toutanova et al., 2003) on each translation and added four features: the fraction of words tagged as finite/non-finite verbs, and the fraction of verbs that are finite/nonfinite.9 9Words tagged as MD, VBP, VBZ, and VBD were counted Reranking AR→EN ZH→EN DE→EN features best div best div best div N/A (baseline) 50.1 36.9 21.8 None 50.5 50.7 37.3 37.1 21.9 21.6 + INVMOD1 50.3 50.8 37.6 37.1 22.0 21.8 + LLM, SYN 50.5 5</context>
</contexts>
<marker>Pauls, Klein, 2012</marker>
<rawString>A. Pauls and D. Klein. 2012. Large-scale syntactic language modeling with treelets. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A-V Rosti</author>
<author>N F Ayan</author>
<author>B Xiang</author>
<author>S Matsoukas</author>
<author>R Schwartz</author>
<author>B Dorr</author>
</authors>
<title>Combining outputs from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="2017" citStr="Rosti et al., 2007" startWordPosition="307" endWordPosition="310"> is left trying to guess what the original sentence means. Multiple Hypotheses. In contrast, when we look at the way other computer systems consume output from MT systems (or similarly unreliable tools), we see a different pattern. In a pipeline setting it is commonplace to propagate not just a singlebest output but the M-best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Collins and Koo, 2005; Charniak and Johnson, 2005), tuning (Och, 2003), minimum Bayes risk decoding (Kumar and Byrne, 2004), and system combination (Rosti et al., 2007). When dealing with error-prone systems, knowing about alternatives has benefits over relying on only a single output (Finkel et al., 2006; Dyer, 2010). Need for Diversity. Unfortunately, M-best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M-best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008</context>
</contexts>
<marker>Rosti, Ayan, Xiang, Matsoukas, Schwartz, Dorr, 2007</marker>
<rawString>A.-V. Rosti, N. F. Ayan, B. Xiang, S. Matsoukas, R. Schwartz, and B. Dorr. 2007. Combining outputs from multiple machine translation systems. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>A K Joshi</author>
</authors>
<title>An SVM-based voting algorithm with application to parse reranking.</title>
<date>2003</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="1847" citStr="Shen and Joshi, 2003" startWordPosition="279" endWordPosition="282">le Translate and Microsoft Translator), and the way MT systems are evaluated (Bojar et al., 2013). Unfortunately, when a real, imperfect MT system makes an error, the user is left trying to guess what the original sentence means. Multiple Hypotheses. In contrast, when we look at the way other computer systems consume output from MT systems (or similarly unreliable tools), we see a different pattern. In a pipeline setting it is commonplace to propagate not just a singlebest output but the M-best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Collins and Koo, 2005; Charniak and Johnson, 2005), tuning (Och, 2003), minimum Bayes risk decoding (Kumar and Byrne, 2004), and system combination (Rosti et al., 2007). When dealing with error-prone systems, knowing about alternatives has benefits over relying on only a single output (Finkel et al., 2006; Dyer, 2010). Need for Diversity. Unfortunately, M-best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M-best lists are extremely similar, often differing only by a single punctuation mark or minor morphol</context>
</contexts>
<marker>Shen, Joshi, 2003</marker>
<rawString>L. Shen and A. K. Joshi. 2003. An SVM-based voting algorithm with application to parse reranking. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>A Sarkar</author>
<author>F J Och</author>
</authors>
<title>Discriminative reranking for machine translation.</title>
<date>2004</date>
<booktitle>In Proc. of HLTNAACL.</booktitle>
<contexts>
<context position="22799" citStr="Shen et al., 2004" startWordPosition="3819" endWordPosition="3822"> BLEU quartile for each sentence. Then system combination of diverse translations might be used only when the 1-best translation is predicted to be of low quality. 8 Reranking Experiments We now turn to discriminative reranking, which has frequently been used to easily add rich features to a model. It has been used for MT with varying de6They reported +0.8 BLEU from system combination for AR--+EN, and saw a further +0.5–0.7 from their new features. 7Quartile points are: 39, 49, 61 for AR--+EN; 25, 36, and 47 for ZH--+EN; and 14.5, 21.1, and 30.3 for DE--+EN. gree of success (Och et al., 2004; Shen et al., 2004; Hildebrand and Vogel, 2008); some have attributed its mixed results to a lack of diversity in the M-best lists traditionally used. We propose diverse lists as a way to address this concern. 8.1 Learning Framework Several learning formulations have been proposed for M-best reranking. One commonly-used approach in MT is MERT, used in the reranking experiments of Och et al. (2004) and Hildebrand and Vogel (2008), among others. We experimented with MERT and other algorithms, including pairwise ranking optimization (Hopkins and May, 2011), but we found best results using the approach of Yadollahp</context>
</contexts>
<marker>Shen, Sarkar, Och, 2004</marker>
<rawString>L. Shen, A. Sarkar, and F. J. Och. 2004. Discriminative reranking for machine translation. In Proc. of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Specia</author>
<author>N Hajlaoui</author>
<author>C Hallett</author>
<author>W Aziz</author>
</authors>
<title>Predicting machine translation adequacy.</title>
<date>2011</date>
<booktitle>In Proc. of MT Summit XIII.</booktitle>
<contexts>
<context position="22117" citStr="Specia et al., 2011" startWordPosition="3700" endWordPosition="3703">ttings and do not show much improvement from system combination. In Table 3, we break down the scores according to 1-best BLEU+1 quartiles, as done in Figure 1.7 In general, we find the largest gains for the lowBLEU translations. For the two worst BLEU quartiles, we see gains of 1.2 to 2.5 BLEU points, while the gains shrink or disappear entirely for the best quartile. This may be a worthwhile trade-off: a large improvement in the worst translations may be more significant to users than a smaller degredation on sentences that are already being translated well. In addition, quality estimation (Specia et al., 2011; Bach et al., 2011) could be used to automatically determine the BLEU quartile for each sentence. Then system combination of diverse translations might be used only when the 1-best translation is predicted to be of low quality. 8 Reranking Experiments We now turn to discriminative reranking, which has frequently been used to easily add rich features to a model. It has been used for MT with varying de6They reported +0.8 BLEU from system combination for AR--+EN, and saw a further +0.5–0.7 from their new features. 7Quartile points are: 39, 49, 61 for AR--+EN; 25, 36, and 47 for ZH--+EN; and 14.5</context>
<context position="37971" citStr="Specia et al., 2011" startWordPosition="6387" endWordPosition="6390">we see this trend reversed for sentences in which the baseline system’s 1-best translation had a low BLEU score. In general, when the BLEU score of the baseline system is below 35, it is preferable to give diverse translations to users for post-editing. But when the baseline system does very well, diverse translations do not contribute anything, and in fact hurt because they may distract users from the high-quality (and typically very similar) translations from the 5-best lists. Estimation of the quality of the output (“confidence estimation”) has recently gained interest in the MT community (Specia et al., 2011; Bach et al., 2011; Callison-Burch et al., 2012; Bojar et al., 2013), including specifically for post-editing (Tatsumi, 2009; Specia, 2011; Koponen, 2012). Future work could investigate whether such automatic confidence estimation could be used to identify situations in which diverse translations can be helpful for aiding user understanding. 10 Future Work Our dissimilarity function captures diversity in the particular phrases used by an MT system, but for certain applications we may prefer other types of diversity. Defining the dissimilarity function on POS tags or word clusters would help u</context>
</contexts>
<marker>Specia, Hajlaoui, Hallett, Aziz, 2011</marker>
<rawString>L. Specia, N. Hajlaoui, C. Hallett, and W. Aziz. 2011. Predicting machine translation adequacy. In Proc. of MT Summit XIII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Specia</author>
</authors>
<title>Exploiting objective annotations for measuring translation post-editing effort.</title>
<date>2011</date>
<booktitle>In Proc. of EAMT.</booktitle>
<contexts>
<context position="38110" citStr="Specia, 2011" startWordPosition="6410" endWordPosition="6411">f the baseline system is below 35, it is preferable to give diverse translations to users for post-editing. But when the baseline system does very well, diverse translations do not contribute anything, and in fact hurt because they may distract users from the high-quality (and typically very similar) translations from the 5-best lists. Estimation of the quality of the output (“confidence estimation”) has recently gained interest in the MT community (Specia et al., 2011; Bach et al., 2011; Callison-Burch et al., 2012; Bojar et al., 2013), including specifically for post-editing (Tatsumi, 2009; Specia, 2011; Koponen, 2012). Future work could investigate whether such automatic confidence estimation could be used to identify situations in which diverse translations can be helpful for aiding user understanding. 10 Future Work Our dissimilarity function captures diversity in the particular phrases used by an MT system, but for certain applications we may prefer other types of diversity. Defining the dissimilarity function on POS tags or word clusters would help us to capture stylistic patterns in sentence structure, as would targeting syntactic structures in syntax-based translation. A weakness of o</context>
</contexts>
<marker>Specia, 2011</marker>
<rawString>L. Specia. 2011. Exploiting objective annotations for measuring translation post-editing effort. In Proc. of EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM—an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="13058" citStr="Stolcke, 2002" startWordPosition="2146" endWordPosition="2147">07; Hoang et al., 2009) with default settings and features for both phrase-based and hierarchical systems. Word alignment was done using GIZA++ (Och and Ney, 2003) in both directions, with the grow-diag-final-and heuristic used to symmetrize the alignments and a max phrase length of 7 used for phrase extraction. Language models used the target side of the parallel corpus in each case augmented with 24.8M lines (601M tokens) of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used 5-gram models, estimated using the SRI Language Modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The minimum count cut-off for unigrams, bigrams, and trigrams was 1 and the cut-off for 4-grams and 5-grams was 3. Language model inference used KenLM (Heafield, 2011). Uncased IBM BLEU was used for evaluation (Papineni et al., 2002). MERT was used to train the feature weights for the baseline systems on TUNE1. We used the learned parameters to generate M-best and diverse lists for TUNE2 and TEST to use for subsequent experiments. 5.3 Diverse List Generation Generating diverse translations depends on two hyperparameters: the n-gram </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM—an extensible language modeling toolkit. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tatsumi</author>
</authors>
<title>Correlation between automatic evaluation metric scores, post-editing speed, and some other factors.</title>
<date>2009</date>
<booktitle>In Proc. of MT Summit XII.</booktitle>
<contexts>
<context position="38096" citStr="Tatsumi, 2009" startWordPosition="6407" endWordPosition="6409">he BLEU score of the baseline system is below 35, it is preferable to give diverse translations to users for post-editing. But when the baseline system does very well, diverse translations do not contribute anything, and in fact hurt because they may distract users from the high-quality (and typically very similar) translations from the 5-best lists. Estimation of the quality of the output (“confidence estimation”) has recently gained interest in the MT community (Specia et al., 2011; Bach et al., 2011; Callison-Burch et al., 2012; Bojar et al., 2013), including specifically for post-editing (Tatsumi, 2009; Specia, 2011; Koponen, 2012). Future work could investigate whether such automatic confidence estimation could be used to identify situations in which diverse translations can be helpful for aiding user understanding. 10 Future Work Our dissimilarity function captures diversity in the particular phrases used by an MT system, but for certain applications we may prefer other types of diversity. Defining the dissimilarity function on POS tags or word clusters would help us to capture stylistic patterns in sentence structure, as would targeting syntactic structures in syntax-based translation. A</context>
</contexts>
<marker>Tatsumi, 2009</marker>
<rawString>M. Tatsumi. 2009. Correlation between automatic evaluation metric scores, post-editing speed, and some other factors. In Proc. of MT Summit XII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C D Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="26063" citStr="Toutanova et al., 2003" startWordPosition="4351" endWordPosition="4354">1 features. Large LM (LLM): We created a large 4-gram LM by interpolating LMs from the WMT news data, Gigaword, Europarl, and the DE→EN news commentary (NC) corpus to maximize likelihood of a heldout development set (WMT08 test set). We used the average per-word log-probability as the single feature function in this category. Syntactic LM (SYN): We used the syntactic treelet language model of Pauls and Klein (2012) to compute two features: the translation log probability and the length-normalized log probability. Finite/Non-Finite Verbs (VERB): We ran the Stanford part-of-speech (POS) tagger (Toutanova et al., 2003) on each translation and added four features: the fraction of words tagged as finite/non-finite verbs, and the fraction of verbs that are finite/nonfinite.9 9Words tagged as MD, VBP, VBZ, and VBD were counted Reranking AR→EN ZH→EN DE→EN features best div best div best div N/A (baseline) 50.1 36.9 21.8 None 50.5 50.7 37.3 37.1 21.9 21.6 + INVMOD1 50.3 50.8 37.6 37.1 22.0 21.8 + LLM, SYN 50.5 51.1 37.4 37.3 21.7 21.7 + VERB, DISC 50.4 51.3 37.3 37.3 21.9 22.2 + GOOG 50.7 51.3 36.8 37.1 21.9 22.2 + WCLM 51.2 51.8 37.3 37.4 22.2 22.3 Table 4: Reranking results (%BLEU on TEST). Discriminative Word/</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tromble</author>
<author>S Kumar</author>
<author>F J Och</author>
<author>W Macherey</author>
</authors>
<title>Lattice Minimum Bayes-Risk decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2617" citStr="Tromble et al., 2008" startWordPosition="401" endWordPosition="404">(Rosti et al., 2007). When dealing with error-prone systems, knowing about alternatives has benefits over relying on only a single output (Finkel et al., 2006; Dyer, 2010). Need for Diversity. Unfortunately, M-best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M-best lists are extremely similar, often differing only by a single punctuation mark or minor morphological variation. Recent work has explored reasoning about sets using packed representations such as lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar et al., 2009), or sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). We argue that the implicit goal behind these techniques is to better explore the output space by introducing diversity into the surrogate set. Overview and Contributions. In this work, we elevate diversity to a first-class status and directly address the problem of generating a set of diverse, plausible translations. We use the recently proposed technique of Batra et al. (2012), which produces diverse M-best solutions from a probabilistic model using a generic dissimilarity functi</context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>R. Tromble, S. Kumar, F. J. Och, and W. Macherey. 2008. Lattice Minimum Bayes-Risk decoding for statistical machine translation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Joachims</author>
<author>T Hofmann</author>
<author>Y Altun</author>
</authors>
<title>Large margin methods for structured and interdependent output variables.</title>
<date>2005</date>
<journal>JMLR,</journal>
<volume>6</volume>
<contexts>
<context position="23507" citStr="Tsochantaridis et al., 2005" startWordPosition="3930" endWordPosition="3933">of diversity in the M-best lists traditionally used. We propose diverse lists as a way to address this concern. 8.1 Learning Framework Several learning formulations have been proposed for M-best reranking. One commonly-used approach in MT is MERT, used in the reranking experiments of Och et al. (2004) and Hildebrand and Vogel (2008), among others. We experimented with MERT and other algorithms, including pairwise ranking optimization (Hopkins and May, 2011), but we found best results using the approach of Yadollahpour et al. (2013), who used a slack-rescaled structured support vector machine (Tsochantaridis et al., 2005) with L2 regularization. As a sentencelevel loss, we used negated BLEU+1. We used the 1-slack cutting-plane algorithm of Joachims et al. (2009) for optimization during learning.8 A more detailed description of the reranker is provided in the supplementary material. We used 5-fold cross-validation on TUNE2 to choose the regularization parameter C from the set 10.01, 0.1,1,101. We selected the value yielding the highest average BLEU score across the held-out 8Our implementation uses OOQP (Gertz and Wright, 2003) to solve the quadratic program in the inner loop, which uses HSL, a collection of Fo</context>
</contexts>
<marker>Tsochantaridis, Joachims, Hofmann, Altun, 2005</marker>
<rawString>I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. 2005. Large margin methods for structured and interdependent output variables. JMLR, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Venugopal</author>
<author>A Zollmann</author>
<author>N A Smith</author>
<author>S Vogel</author>
</authors>
<title>Wider pipelines: N-best alignments and parses in MT training.</title>
<date>2008</date>
<booktitle>In Proc. ofAMTA.</booktitle>
<contexts>
<context position="1762" citStr="Venugopal et al., 2008" startWordPosition="266" endWordPosition="269">e field’s inception. It is the way we interact with commercial MT services (such as Google Translate and Microsoft Translator), and the way MT systems are evaluated (Bojar et al., 2013). Unfortunately, when a real, imperfect MT system makes an error, the user is left trying to guess what the original sentence means. Multiple Hypotheses. In contrast, when we look at the way other computer systems consume output from MT systems (or similarly unreliable tools), we see a different pattern. In a pipeline setting it is commonplace to propagate not just a singlebest output but the M-best hypotheses (Venugopal et al., 2008). Multiple solutions are also used for reranking (Collins, 2000; Shen and Joshi, 2003; Collins and Koo, 2005; Charniak and Johnson, 2005), tuning (Och, 2003), minimum Bayes risk decoding (Kumar and Byrne, 2004), and system combination (Rosti et al., 2007). When dealing with error-prone systems, knowing about alternatives has benefits over relying on only a single output (Finkel et al., 2006; Dyer, 2010). Need for Diversity. Unfortunately, M-best lists are a poor surrogate for structured output spaces (Finkel et al., 2006; Huang, 2008). In MT, for example, many translations on M-best lists are </context>
</contexts>
<marker>Venugopal, Zollmann, Smith, Vogel, 2008</marker>
<rawString>A. Venugopal, A. Zollmann, N.A. Smith, and S. Vogel. 2008. Wider pipelines: N-best alignments and parses in MT training. In Proc. ofAMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>T C Bell</author>
</authors>
<title>The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="28066" citStr="Witten and Bell, 1991" startWordPosition="4698" endWordPosition="4701"> 12 features. Word Cluster LMs (WCLM): Using an implementation provided by Liang (2005), we performed Brown clustering (Brown et al., 1992) on 900k English sentences, including the NC corpus and random sentences from Gigaword. We clustered words that appeared at least twice, once with 300 clusters and again with 1000. We then replaced words with their clusters in a large corpus consisting of the WMT news data, Gigaword, and the NC data. An additional cluster label was used for unknown words. For each of the clusterings (300 and 1000), we estimated 5- and 7-gram LMs with Witten-Bell smoothing (Witten and Bell, 1991). We added 4 features to the reranker, one for the log-probability of the translation under each of the word cluster LMs. as finite verbs, and VB, VBG, and VBN were non-finite verbs. 10Before estimating LMs, we projected the sentence weights so that the min and max per source sentence were 0 and 1. 1106 List type Features None All 20 best 50.3 50.6 100 best 50.6 50.8 200 best 50.4 51.2 1000 best 50.5 51.2 unique 20 best 50.5 51.2 unique 100 best 50.6 51.2 unique 200 best 50.4 51.3 20 diverse 50.5 51.1 20 div × 5 best 50.6 51.4 20 div × 10 best 50.7 51.3 20 div × 50 best 50.7 51.8 Table 5: List</context>
</contexts>
<marker>Witten, Bell, 1991</marker>
<rawString>I. H. Witten and T. C. Bell. 1991. The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE Transactions on Information Theory, 37(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Xiao</author>
<author>J Zhu</author>
<author>T Liu</author>
</authors>
<title>Bagging and boosting statistical machine translation systems.</title>
<date>2013</date>
<journal>Artif. Intell.,</journal>
<volume>195</volume>
<contexts>
<context position="8823" citStr="Xiao et al. (2013)" startWordPosition="1450" endWordPosition="1453">ently started to consider diversity in the context of system combination (Macherey and Och, 2007). Most closelyrelated is work by Devlin and Matsoukas (2012), who proposed a way to generate diverse translations by varying particular “traits,” such as translation length, number of rules applied, etc. Their approach can be viewed as solving Eq. (2) with a richer dissimilarity function that requires a special-purpose decoding algorithm. We chose our n-gram dissimilarity function due to its simplicity and applicability to most MT systems without requiring any change to decoders. Among other work, Xiao et al. (2013) used bagging and boosting to get diverse system outputs for system combination and Cer et al. (2013) used multiple identical systems trained jointly with an objective function that encourages the systems to generate complementary translations. There is also similarity between our approach and minimum Bayes risk decoding (Kumar and Byrne, 2004), variational decoding (Li et al., 2009), and other “consensus” decoding algorithms (DeNero et al., 2009). These all seek a single translation that is most similar on average to the model’s preferred translations. In this way, they try to capture the mod</context>
</contexts>
<marker>Xiao, Zhu, Liu, 2013</marker>
<rawString>T. Xiao, J. Zhu, and T. Liu. 2013. Bagging and boosting statistical machine translation systems. Artif. Intell., 195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Yadollahpour</author>
<author>D Batra</author>
<author>G Shakhnarovich</author>
</authors>
<title>Discriminative re-ranking of diverse segmentations.</title>
<date>2013</date>
<booktitle>In Proc. of CVPR.</booktitle>
<contexts>
<context position="23416" citStr="Yadollahpour et al. (2013)" startWordPosition="3917" endWordPosition="3921">al., 2004; Hildebrand and Vogel, 2008); some have attributed its mixed results to a lack of diversity in the M-best lists traditionally used. We propose diverse lists as a way to address this concern. 8.1 Learning Framework Several learning formulations have been proposed for M-best reranking. One commonly-used approach in MT is MERT, used in the reranking experiments of Och et al. (2004) and Hildebrand and Vogel (2008), among others. We experimented with MERT and other algorithms, including pairwise ranking optimization (Hopkins and May, 2011), but we found best results using the approach of Yadollahpour et al. (2013), who used a slack-rescaled structured support vector machine (Tsochantaridis et al., 2005) with L2 regularization. As a sentencelevel loss, we used negated BLEU+1. We used the 1-slack cutting-plane algorithm of Joachims et al. (2009) for optimization during learning.8 A more detailed description of the reranker is provided in the supplementary material. We used 5-fold cross-validation on TUNE2 to choose the regularization parameter C from the set 10.01, 0.1,1,101. We selected the value yielding the highest average BLEU score across the held-out 8Our implementation uses OOQP (Gertz and Wright,</context>
</contexts>
<marker>Yadollahpour, Batra, Shakhnarovich, 2013</marker>
<rawString>P. Yadollahpour, D. Batra, and G. Shakhnarovich. 2013. Discriminative re-ranking of diverse segmentations. In Proc. of CVPR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
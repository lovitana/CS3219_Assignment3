<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008481">
<title confidence="0.9533">
Studying the recursive behaviour of adjectival modification
with compositional distributional semantics
</title>
<author confidence="0.900706">
Eva Maria Vecchi and Roberto Zamparelli and Marco Baroni
</author>
<affiliation confidence="0.897666">
Center for Mind/Brain Sciences (University of Trento, Italy)
</affiliation>
<email confidence="0.995606">
(evamaria.vecchi|roberto.zamparelli|marco.baroni)@unitn.it
</email>
<sectionHeader confidence="0.998569" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999483882352941">
In this study, we use compositional distribu-
tional semantic methods to investigate restric-
tions in adjective ordering. Specifically, we
focus on properties distinguishing Adjective-
Adjective-Noun phrases in which there is flex-
ibility in the adjective ordering from those
bound to a rigid order. We explore a number
of measures extracted from the distributional
representation of AAN phrases which may in-
dicate a word order restriction. We find that
we are able to distinguish the relevant classes
and the correct order based primarily on the
degree of modification of the adjectives. Our
results offer fresh insight into the semantic
properties that determine adjective ordering,
building a bridge between syntax and distri-
butional semantics.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999845291666667">
A prominent approach for representing the meaning
of a word in Natural Language Processing (NLP) is
to treat it as a numerical vector that codes the pat-
tern of co-occurrence of that word with other ex-
pressions in a large corpus of language (Sahlgren,
2006; Turney and Pantel, 2010). This approach to
semantics (sometimes called distributional seman-
tics) scales well to large lexicons and does not re-
quire words to be manually disambiguated (Sch¨utze,
1997). Until recently, however, this method had
been almost exclusively limited to the level of sin-
gle content words (nouns, adjectives, verbs), and had
not directly addressed the problem of composition-
ality (Frege, 1892; Montague, 1970; Partee, 2004),
the crucial property of natural language which al-
lows speakers to derive the meaning of a complex
linguistic constituent from the meaning of its imme-
diate syntactic subconstituents.
Several recent proposals have strived to ex-
tend distributional semantics with a component that
also generates vectors for complex linguistic con-
stituents, using compositional operations in the vec-
tor space (Baroni and Zamparelli, 2010; Guevara,
2010; Mitchell and Lapata, 2010; Grefenstette and
Sadrzadeh, 2011; Socher et al., 2012). All of
these approaches construct distributional represen-
tations for novel phrases starting from the corpus-
derived vectors for their lexical constituents and
exploiting the geometric quality of the representa-
tion. Such methods are able to capture complex se-
mantic information of adjective-noun (AN) phrases,
such as characterizing modification (Boleda et al.,
2012; Boleda et al., 2013), and can detect seman-
tic deviance in novel phrases (Vecchi et al., 2011).
Furthermore, these methods are naturally recursive:
they can derive a representation not only for, e.g.,
red car, but also for new red car, fast new red car,
etc. This aspect is appealing since trying to extract
meaningful representations for all recursive phrases
directly from a corpus will result in a problem of
sparsity, since most large phrases will never occur in
any finite sample.
Once we start seriously looking into recursive
modification, however, the issue of modifier order-
ing restrictions naturally arises. Such restrictions
have often been discussed in the theoretical linguis-
tic literature (Sproat and Shih, 1990; Crisma, 1991;
Scott, 2002), and have become one of the key in-
</bodyText>
<page confidence="0.976831">
141
</page>
<note confidence="0.7337135">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 141–151,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999882217391304">
gredients of the ‘cartographic’ approach to syntax
(Cinque, 2002). In this paradigm, the ordering is
derived by assigning semantically different classes
of modifiers to the specifiers of distinct functional
projections, whose sequence is hard-wired. While
it is accepted that in different languages movement
can lead to a principled rearrangement of the linear
order of the modifiers (Cinque, 2010; Steddy and
Samek-Lodovici, 2011), one key assumption of the
cartographic literature is that exactly one intonation-
ally unmarked order for stacked adjectives should
be possible in languages like English. The possi-
bility of alternative orders, when discussed at all,
is attributed to the presence of idioms (high Amer-
ican building, but American high officer), to asyn-
detic conjunctive meanings (e.g. new creative idea
parsed as [new &amp; creative] idea, rather than [new
[creative idea]]), or to semantic category ambiguity
for any adjective which appears in different orders
(see Cinque (2004) for discussion).
In this study, we show that the existence of both
rigid and flexible order cases is robustly attested at
least for adjectival modification, and that flexible or-
dering is unlikely to reduce to idioms, coordination
or ambiguity. Moreover, we show that at least for
some recursively constructed adjective-adjective-
noun phrases (AANs) we can extract meaning-
ful representations from the corpus, approximating
them reasonably well by means of compositional
distributional semantic models, and that the seman-
tic information contained in these models character-
izes which AA will have rigid order (as with rapid
social change vs. *social rapid change), or flexible
order (e.g. total estimated population vs. estimated
total population). In the former case, we find that
the same distributional semantic cues discriminate
between correct and wrong orders.
To achieve these goals, we consider various
properties of the distributional representation of
AANs (both corpus-extracted and compositionally-
derived), and explore their correlation with restric-
tions in adjective ordering. We conclude that mea-
sures that quantify the degree to which the modifiers
have an impact on the distributional meaning of the
AAN can be good predictors of ordering restrictions
in AANs.
</bodyText>
<sectionHeader confidence="0.989626" genericHeader="introduction">
2 Materials and methods
</sectionHeader>
<subsectionHeader confidence="0.971597">
2.1 Semantic space
</subsectionHeader>
<bodyText confidence="0.999974547619048">
Our initial step was to construct a semantic space for
our experiments, consisting of a matrix where each
row represents the meaning of an adjective, noun,
AN or AAN as a distributional vector, each column
a semantic dimension of meaning. We first introduce
the source corpus, then the vocabulary of words and
phrases that we represent in the space, and finally the
procedure adopted to build the vectors representing
the vocabulary items from corpus statistics, and ob-
tain the semantic space matrix. We work here with a
traditional, window-based semantic space, since our
focus is on the effect of different composition meth-
ods given a common semantic space. In addition,
Blacoe and Lapata (2012) found that a vanilla space
of this sort performed best in their composition ex-
periments, when compared to a syntax-aware space
and to neural language model vectors such as those
used for composition by Socher et al. (2011).
Source corpus We use as our source corpus the
concatenation of the Web-derived ukWaC corpus, a
mid-2009 dump of the English Wikipedia and the
British National Corpus1. The corpus has been tok-
enized, POS-tagged and lemmatized with the Tree-
Tagger (Schmid, 1995), and it contains about 2.8 bil-
lion tokens. We extract all statistics at the lemma
level, meaning that we consider only the canonical
form of each word ignoring inflectional information,
such as pluralization and verb inflection.
Semantic space vocabulary The words/phrases
in the semantic space must of course include the
items that we need for our experiments (adjectives,
nouns, ANs and AANs used for model training, as
input to composition and for evaluation). Therefore,
we first populate our semantic space with a core vo-
cabulary containing the 8K most frequent nouns and
the 4K most frequent adjectives from the corpus.
The ANs included in the semantic space are com-
posed of adjectives with very high frequency in the
corpus so that they are generally able to combine
with many classes of nouns. They are composed
of the 700 most frequent adjectives and 4K most
frequent nouns in the corpus, which were manually
</bodyText>
<footnote confidence="0.999254">
1http://wacky.sslmit.unibo.it, http://en.
wikipedia.org,http://www.natcorp.ox.ac.uk
</footnote>
<page confidence="0.997151">
142
</page>
<bodyText confidence="0.99994548">
controlled for problematic cases – excluding adjec-
tives such as above, less, or very, and nouns such
as cant, mph, or yours – often due to tagging errors.
We generated the set of ANs by crossing the filtered
663 adjectives and 3,910 nouns. We include those
ANs that occur at least 100 times in the corpus in
our vocabulary, which amounted to a total of 128K
ANs.
Finally, we created a set of AAN phrases com-
posed of the adjectives and nouns used to gener-
ate the ANs. Additional preprocessing of the gen-
erated AxAyNs includes: (i) control that both AxN
and AyN are attested in the corpus; (ii) discard any
AxAyN in which AxN or AyN are among the top
200 most frequent ANs in the source corpus (as in
this case, order will be affected by the fact that such
phrases are almost certainly highly lexicalized); and
(iii) discard AANs seen as part of a conjunction in
the source corpus (i.e., where the two adjectives ap-
pear separated by comma, and, or or; this addresses
the objection that a flexible order AAN might be a
hidden A(&amp;)A conjunction: we would expect that
such a conjunction should also appear overtly else-
where). The set of AANs thus generated is then di-
vided into two types of adjective ordering:
</bodyText>
<listItem confidence="0.937654">
1. Flexible Order (FO): phrases where both or-
ders, AxAyN and AyAxN, are attested (f&gt;10
in both orders).
2. Rigid Order (RO): phrases with one order,
AxAyN, attested (20&lt;f&lt;200)2 and AyAxN
unattested.
</listItem>
<bodyText confidence="0.960154875">
All AANs that did not meet either condition were
excluded from our semantic space vocabulary. The
preserved set resulted in 1,438 AANs: 621 flexible
order and 817 rigid order. Note that there are almost
as many flexible as rigid order cases; this speaks
against the idea that free order is a marginal phe-
nomenon, due to occasional ambiguities that reas-
sign the adjective to a different semantic class. The
existence of freely ordered stacked adjectives is a ro-
bust phenomenon, which needs to be addressed.
2The upper threshold was included as an additional filter
against potential multiword expressions. Of course, the bound-
ary between phrases that are at least partially compositional and
those that are fully lexicalized is not sharp, and we leave it to
further work to explore the interplay between the semantic fac-
tors we study here and patterns of lexicalization.
</bodyText>
<table confidence="0.998627">
Model p M&amp;L
CORP 0.41 0.43
W.ADD 0.41 0.44
F.ADD 0.40 –
MULT 0.33 0.46
LFM 0.40 –
</table>
<tableCaption confidence="0.9880348">
Table 1: Correlation scores (Spearman’s p, all signif-
icant at p &lt;0.001) between cosines of corpus-extracted
or model-generated AN vectors and phrase similarity rat-
ings collected in Mitchell and Lapata (2010), as well as
best reported results from Mitchell &amp; Lapata (M&amp;L).
</tableCaption>
<bodyText confidence="0.9995043">
Semantic vector construction For each of
the items in our vocabulary, we first build 10K-
dimensional vectors by recording the item’s
sentence-internal co-occurrence with the top 10K
most frequent content lemmas (nouns, adjectives,
verbs or adverbs) in the corpus. We built a rank
of these co-occurrence counts, and excluded as
stop words from the dimensions any element of
any POS whose rank was from 0 to 300. The raw
co-occurrence counts were then transformed into
(positive) Pointwise Mutual Information (pPMI)
scores (Church and Hanks, 1990). Next, we reduce
the full co-occurrence matrix to 300 dimensions
applying the Non-negative Matrix Factorization
(NMF) operation (Lin, 2007). We did not tune the
semantic vector construction parameters, since we
found them to work best in a number of independent
earlier experiments.
Corpus-extracted vectors (corp) were computed
for the ANs and for the flexible order and attested
rigid order AANs, and then mapped onto the 300-
dimension NMF-reduced semantic space. As a san-
ity check, the first row of Table 1 reports the corre-
lation between the AN phrase similarity ratings col-
lected in Mitchell and Lapata (2010) and the cosines
of corpus-extracted vectors in our space, for the
same ANs. For the AAN vectors, which are sparser,
we used human judgements to build a reliable sub-
set to serve as our gold standard, as detailed in Sec-
tion 2.4.
</bodyText>
<subsectionHeader confidence="0.999015">
2.2 Composition models
</subsectionHeader>
<bodyText confidence="0.999994">
We focus on four composition functions proposed
in recent literature with high performance in a num-
ber of semantic tasks. We first consider meth-
ods proposed by Mitchell and Lapata (2010) in
</bodyText>
<page confidence="0.995177">
143
</page>
<bodyText confidence="0.9997552">
which the model-generated vectors are simply ob-
tained through component-wise operations on the
constituent vectors. Given input vectors u~ and ~v, the
multiplicative model (MULT) computes a composed
vector by component-wise multiplication (0) of the
constituent vectors, where the i-th component of the
composed vector is given by pi = uivi.3 Given an
A.AyN phrase, this model extends naturally to the
recursive setting of this experiment, as seen in Equa-
tion (1).
</bodyText>
<equation confidence="0.725556">
p~= ~a. O~ay O n~ (1)
</equation>
<bodyText confidence="0.959586705882353">
This composition method is order-insensitive, the
formula above corresponding to the representation
of both A.AyN and AyA.N.
In the weighted additive model (W.ADD), we ob-
tain the composed vector as a weighted sum of the
two component vectors: p~ = α~u + β~v, where α and
β are scalars. Again, we can easily apply this func-
tion recursively, as in Equation (2).
p~= α ~a. + β(α~ay + β~n) = α~a. + αβ~ay + β2~n
(2)
We also consider the full extension of the addi-
tive model (F.ADD), presented in Guevara (2010)
and Zanzotto et al. (2010), such that the component
vectors are pre-multiplied by weight matrices before
being added: p~ = W1~u + W2~v. Similarly to the
W.ADD model, Equation (3) describes how we apply
this function recursively.
</bodyText>
<equation confidence="0.9659025">
p~ = W1~a. + W2(W1~ay + W2~n) (3)
= W1~a. + W2W1~ay + W22~n
</equation>
<bodyText confidence="0.970887882352941">
Finally, we consider the lexical function model
(LFM), first introduced in Baroni and Zamparelli
(2010), in which attributive adjectives are treated as
functions from noun meanings to noun meanings.
This is a standard approach in Montague semantics
(Thomason, 1974), except noun meanings here are
distributional vectors, not denotations, and adjec-
tives are (linear) functions learned from a large cor-
pus. In this model, predicted vectors are generated
3We conjecture that the different performance of our multi-
plicative model and M&amp;L’s (cf. Table 1) is due to the fact that
we use log-transformed pPMI scores, making their multiplica-
tive model more akin to our additive approach.
by multiplying a function matrix U with a compo-
nent vector: p~ = U~v. Given a weight matrix, A, for
each adjective in the phrase, we apply the functions
in sequence recursively as shown in Equation (4).
</bodyText>
<equation confidence="0.981836">
p~ = A.(Ay~n) (4)
</equation>
<bodyText confidence="0.9999315">
Composition model estimation Parameters for
W.ADD, F.ADD and LFM were estimated following
the strategy proposed by Guevara (2010) and Ba-
roni and Zamparelli (2010), recently extended to all
composition models by Dinu et al. (2013b). Specif-
ically, we learn parameter values that optimize the
mapping from the noun to the AN as seen in ex-
amples of corpus-extracted N-AN vector pairs, us-
ing least-squares methods. All parameter estima-
tions and phrase compositions were implemented
using the DISSECT toolkit4 (Dinu et al., 2013a),
with a training set of 74,767 corpus-extracted N-
AN vector pairs, ranging from 100 to over 1K items
across the 663 adjectives. Importantly, while below
we report experimental results on capturing various
properties of recursive AAN constructions, no AAN
was seen during training, which was based entirely
on mapping from N to AN. Table 1 reports the re-
sults attained by our model implementations on the
Mitchell and Lapata AN similarity data set.
</bodyText>
<subsectionHeader confidence="0.99957">
2.3 Measures of adjective ordering
</subsectionHeader>
<bodyText confidence="0.999991647058824">
Our general goal is to determine which
linguistically-motivated factors distinguish the
two types of adjective ordering. We hypothesize
that in cases of flexible order, the two adjectives
will have a similarly strong effect on the noun, thus
transforming the meaning of the noun equivalently
in the direction of both adjectives and component
ANs. For example, in the phrase creative new idea,
the idea is both new and creative, so we would
expect a similar impact of modification by both
adjectives.
On the other hand, we predict that in rigid order
cases, one adjective, the one closer to the noun, will
dominate the meaning of the phrase, distorting the
meaning of the noun by a significant amount. For
example, the phrase different architectural style in-
tuitively describes an architectural style that is dif-
</bodyText>
<footnote confidence="0.992075">
4http://clic.cimec.unitn.it/composes/
toolkit
</footnote>
<page confidence="0.997441">
144
</page>
<bodyText confidence="0.999947577777778">
ferent, rather than a style that is to the same extent
architectural and different.
We consider a number of measures that could cap-
ture our intuitions and quantify this difference, ex-
ploring the distance relationship between the AAN
vectors and each of the AAN subparts. First, we
examine how the similarity of an AAN to its com-
ponent adjectives affects the ordering, using the co-
sine between the AxAyN vector and each of the
component A vectors as an expression of similarity
(we abbreviate this as cosAx and cosAy for the first
and second adjective, respectively).5 Our hypothe-
sis predicts that flexible order AANs should remain
similarly close to both component As, while rigid
order AANs should remain systematically closer to
their Ay than to their Ax.
Next, we consider the similarity between the
AxAyN vector and its component N vector (cosN).
This measure is aimed at verifying if the degree to
which the meaning of the head noun is distorted
could be a property that distinguishes the two types
of adjective ordering. Again, vectors for flexible or-
der AANs should remain closer to their component
nouns in the semantic space, while rigid order AANs
should distort the meaning of the head noun more
notably.
We also inspect how the similarity of the AAN
to its component AN vectors affects the type of ad-
jective ordering (cosAxN and cosAyN). Consid-
ering the examples above, we predict that the flex-
ible order AAN creative new idea will share many
properties with both creative idea and new idea, as
represented in our semantic space, while rigid or-
der AANs, like different architectural style, should
remain quite similar to the AyN, i.e., architectural
style, and relatively distant from the AxN, i.e., dif-
ferent style.
Finally, we consider a measure that does not ex-
ploit distributional semantic representations, namely
the difference in PMI between AxN and AyN
(DPMI). Based on our hypothesis described for the
other measures, we expect the association in the cor-
pus of AyN to be much greater than AxN for rigid
order AANs, resulting in a large negative DPMI val-
ues. While flexible order AANs should have similar
</bodyText>
<footnote confidence="0.870867666666667">
5In the case of LFM, we compare the similarity of the AAN
with the AN centroids for each adjective, since the model does
not make use of A vectors (Baroni and Zamparelli, 2010).
</footnote>
<bodyText confidence="0.966238333333333">
association strengths for both AxN and AyN, thus
we expect DPMI to be closer to 0 than for rigid or-
der AANs.
</bodyText>
<subsectionHeader confidence="0.988656">
2.4 Gold standard
</subsectionHeader>
<bodyText confidence="0.999781894736842">
To our knowledge, this is the first study to use
distributional representations of recursive modifi-
cation; therefore we must first determine if the
composed AAN vector representations are seman-
tically coherent objects. Thus, for vector analysis,
a gold standard of 320 corpus-extracted AAN vec-
tors were selected and their quality was established
by inspecting their nearest neighbors. In order to
create the gold standard, we ran a crowdsourcing
experiment on CrowdFlower6 (Callison-Burch and
Dredze, 2010; Munro et al., 2010), as follows.
First, we gathered a randomly selected set of 600
corpus-extracted AANs, containing 300 flexible or-
der and 300 attested rigid order AANs. We then
extracted the top 3 nearest neighbors to the corpus-
extracted AAN vectors as represented in the seman-
tic space7. Each AAN was then presented with each
of the nearest neighbors, and participants were asked
to judge “how strongly related are the two phrases?”
on a scale of 1-7. The rationale was that if we
obtained a good distributional representation of the
AAN, its nearest neighbors should be closely related
words and phrases. Each pair was judged 10 times,
and we calculated a relatedness score for the AAN
by taking the average of the 30 judgments (10 for
each of the three neighbors).
The final set for the gold standard contains the 320
AANs (152 flexible order and 168 attested rigid or-
der) which had a relatedness score over the median-
split (3.9). Table 2 shows examples of gold stan-
dard AANs and their nearest neighbors. As these
example indicate, the gold standard AANs reside in
semantic neighborhoods that are populated by in-
tuitively strongly related expressions, which makes
them a sensible target for the compositional models
to approximate.
We also find that the neighbors for the AANs rep-
resent an interesting variety of types of semantic
</bodyText>
<footnote confidence="0.9968842">
6http://www.crowdflower.com
7The top 3 neighbors included adjectives, nouns, ANs and
AANs. The preference for ANs and AANs, as seen in Table 2,
is likely a result of the dominance of those elements in the se-
mantic space (c.f. Section 2.1).
</footnote>
<page confidence="0.972151">
145
</page>
<table confidence="0.999264791666667">
medieval old town contemp. political issue
fascinating town cultural topic
impressive cathedral contemporary debate
medieval street contemporary politics
rural poor people British naval power
poor rural people naval war
rural infrastructure British navy
rural people naval power
friendly helpful staff last live performance
near hotel final gig
helpful staff live dvd
quick service live release
creative new idea rapid social change
innovative effort social conflict
creative design social transition
dynamic part cultural consequence
national daily newspaper new regional government
national newspaper regional government
major newspaper local reform
daily newspaper regional council
daily national newspaper fresh organic vegetable
national daily newspaper organic vegetable
well-known journalist organic fruit
weekly column organic product
</table>
<tableCaption confidence="0.948399666666667">
Table 2: Examples of the nearest neighbors of the gold
standard, both flexible order (left column) and rigid order
(right column) AANs.
</tableCaption>
<bodyText confidence="0.999846923076923">
similarity. For example, the nearest neighbors to the
corpus-extracted vectors for medieval old town and
rapid social change include phrases which describe
quite complex associations, cf. Table 2. In addition,
we find that the nearest neighbors for flexible order
AAN vectors are not necessarily the same for both
adjective orders, as seen in the difference in neigh-
bors of national daily newspaper and daily national
newspaper. We can expect that the change in or-
der, when acceptable and frequent, does not neces-
sarily yield synonymous phrases, and that corpus-
extracted vector representations capture subtle dif-
ferences in meaning.
</bodyText>
<sectionHeader confidence="0.999996" genericHeader="background">
3 Results
</sectionHeader>
<subsectionHeader confidence="0.999954">
3.1 Quality of model-generated AAN vectors
</subsectionHeader>
<bodyText confidence="0.99385">
Our nearest neighbor analysis suggests that the
corpus-extracted AAN vectors in the gold standard
are meaningful, semantically coherent objects. We
can thus assess the quality of AANs recursively gen-
erated by composition models by how closely they
</bodyText>
<table confidence="0.9994878">
Gold FO RO
W.ADD 0.565 0.572 0.558
F.ADD 0.618 0.622 0.614
MULT 0.424 0.468 0.384
LFM 0.655 0.675 0.637
</table>
<tableCaption confidence="0.772592571428571">
Table 3: Mean cosine similarities between the corpus-
extracted and model-generated gold AAN vectors. All
pairwise differences between models are significant ac-
cording to Bonferroni-corrected paired t-tests (p&lt;0.001).
For MULT and LFM, the difference between mean flexible
order (FO) and rigid order (RO) cosines is also signifi-
cant.
</tableCaption>
<bodyText confidence="0.999336086956522">
approximate these vectors. We find that the perfor-
mances of most composition models in approximat-
ing the vectors for the gold AANs is quite satisfac-
tory (cf. Table 3). To put this evaluation into per-
spective, note that 99% of the simulated distribu-
tion of pairwise cosines of corpus-extracted AANs
is below the mean cosine of the worst-performing
model (MULT), that is, a cosine of 0.424 is very sig-
nificantly above what is expected by chance for two
random corpus-extracted AAN vectors. Also, ob-
serve that the two more parameter-rich models are
better than W.ADD, and that LFM also significantly
outperforms F.ADD.
Further, the results show that the models are able
to approximate flexible order AAN vectors better
than rigid order AANs, significantly so for LFM and
MULT. This result is quite interesting because it sug-
gests that flexible order AANs express a more lit-
eral (or intersective) modification by both adjectives,
which is what we would expect to be better captured
by compositional models. Clearly, a more complex
modification process is occurring in the case of rigid
order AANs, as we predicted to be the case.
</bodyText>
<subsectionHeader confidence="0.999521">
3.2 Distinguishing flexible vs. rigid order
</subsectionHeader>
<bodyText confidence="0.999534">
In the results reported below, we test how both our
baseline DPMI measure and the distance from the
AAN and its component parts changes depending on
the type of adjective ordering to which the AAN be-
longs. From this point forward, we only use gold
standard items, where we are sure of the quality of
the corpus-extracted vectors. The first block of Ta-
ble 4 reports the t-normalized difference between
flexible order and rigid order mean cosines for the
corpus-extracted vectors.
</bodyText>
<page confidence="0.99709">
146
</page>
<table confidence="0.999239931034483">
Measure t sig.
cosA., 2.478
cosAy -4.348 * RO&gt;FO
CORP cosN 4.656 * FO&gt;RO
cosA.,N 5.913 * FO&gt;RO
cosAyN 1.970
cosA., 4.805 * FO&gt;RO
cosAy -1.109
cosN 1.140
W.ADD cosA.,N 1.059
cosAyN 0.584
cosA., 2.050
cosAy -1.451
cosN 4.493 * FO&gt;RO
F.ADD cosA.,N -0.445
cosAyN 2.300
cosA., 3.830 * FO&gt;RO
cosAy -0.503
cosN 5.090 * FO&gt;RO
MULT
cosA.,N 4.435 * FO&gt;RO
cosAyN 3.900 * FO&gt;RO
cosA., -1.649
cosAy -1.272
cosN 5.539 * FO&gt;RO
LFM
cosA.,N 3.336 * FO&gt;RO
cosAyN 4.215 * FO&gt;RO
OPMI 8.701 * FO&gt;RO
</table>
<tableCaption confidence="0.962403">
Table 4: Flexible vs. Rigid Order AANs. t-normalized
</tableCaption>
<figureCaption confidence="0.85546">
differences between flexible order (FO) and rigid order
(FO) mean cosines (or mean OPMI values) for corpus-
extracted and model-generated vectors. For significant
differences (p&lt;0.05 after Bonferroni correction), the last
column reports whether mean cosine (or OPMI) is larger
for flexible order (FO) or rigid order (RO) class.
</figureCaption>
<bodyText confidence="0.998895859375">
These results show, in accordance with our con-
siderations in Section 2.3 above: (i) flexible or-
der AxAyNs are closer to AxN and the component
N than rigid order AxAyNs, and (ii) rigid order
AxAyNs are closer to their Ay (flexible order AANs
are also closer to Ax but the effect does not reach
significance).8 The results imply that the degree of
modification of the Ay on the noun is a significant
indicator of the type of ordering present.
8As an aside, the fact that mean cosines are significantly
larger for the flexible order class in two cases but for the rigid or-
der class in another addresses the concern, raised by a reviewer,
that the words and phrases in one of the two classes might sys-
tematically inhabit denser regions of the space than those of the
other class, thus distorting results based on comparing mean
cosines.
In particular, rigid order AxAyNs are heavily
modified by Ay, distorting the meaning of the head
noun in the direction of the closest adjective quite
drastically, and only undergoing a slight modifica-
tion when the Ax is added. In other words, in rigid
order phrases, for example rapid social change, the
AyN expresses a single concept (probably a “kind”,
in the terminology of formal semantics), strongly re-
lated to social, social change, which is then mod-
ified by the Ax. Thus, the change is not both so-
cial and rapid, rather, the social change is rapid. On
the other hand, flexible order AANs maintain the se-
mantic value of the head noun while being modi-
fied only slightly by both adjectives, almost equiv-
alently. For example, in the phrase friendly help-
ful staff, one is saying that the staff is both friendly
and helpful. Most importantly, the corpus-extracted
distributional representations are able to model this
phenomenon inherently and can significantly distin-
guish the two adjective orders.
The results of the composition models (cf. Ta-
ble 4) show that for all models at least some prop-
erties do distinguish flexible and rigid order AANs,
although only MULT and LFM capture the two prop-
erties that show the largest effect for the corpus-
extracted vectors, namely the asymmetry in similar-
ity to the noun and the AxN (flexible order AANs
being more similar to both).
It is worth remarking that MULT approximated the
patterns observed in the corpus vectors quite well,
despite producing order-insensitive representations
of recursive structures. For flexible order AANs, or-
der is indeed only slightly affecting the meaning, so
it stands to reason that MULT has no problems mod-
eling this class. For rigid order AANs, where we
consider here the attested-order only, evidently the
order-insensitive MULT representation is sufficient
to capture their relations to their constituents.
Finally, we see that the OPMI measure is the best
at distinguishing between the two classes of AAN
ordering. This confirms our hypothesis that a lot has
to do with how integrated Ay and N are. While it
is somewhat disappointing that OPMI outperforms
all distributional semantic cues, note that this mea-
sure conflates semantic and lexical factors, as the
high PMI of AyN in at least some rigid order AANs
might be also a cue of the fact that the latter bigram
is a lexicalized phrase (as discussed in footnote 2, it
</bodyText>
<page confidence="0.996133">
147
</page>
<bodyText confidence="0.999598777777778">
is unlikely that our filtering strategies sifted out all
multiword expressions). Moreover, OPMI does not
produce a semantic representation of the phrase (see
how composed distributional vectors approximate of
high quality AAN vectors in Table 3). Finally, this
measure will not scale up to cases where the ANs
are not attested, whereas measures based on compo-
sition only need corpus-harvested representations of
adjectives and nouns.
</bodyText>
<subsectionHeader confidence="0.99792">
3.3 Properties of the correct adjective order
</subsectionHeader>
<bodyText confidence="0.999989594594595">
Having shown that flexible order and rigid order
AANs are significantly distinguished by various
properties, we proceed now to test whether those
same properties also allow us to distinguish between
correct (corpus-attested) and wrong (unattested) ad-
jective ordering in rigid AANs (recall that we are
working with cases where the attested-order occurs
more than 20 times in the corpus, and both adjec-
tives modify the nouns at least 10 times, so we are
confident that there is a true asymmetry).
We expect that the fundamental property that dis-
tinguishes the orders is again found in the degree
of modification of both component adjectives. We
predict that the single concept created by the AyN
in attested-order rigid AANs, such as legal status
informal legal status, is an effect of the modifica-
tion strength of the Ay on the head noun, and when
seen in the incorrect ordering, i.e.,&apos;legal formal sta-
tus, the strong modification of legal will still domi-
nate the meaning of the AAN. Composition models
should be able to capture this effect based on the dis-
tance from both the component adjectives and ANs.
Clearly, we cannot run these analyses on corpus-
extracted vectors since the unattested order, by def-
inition, is not seen in our corpus, and therefore we
cannot collect co-occurrence statistics for the AAN
phrase. Thus, we test our measures of adjective or-
dering on the model-generated AAN vectors, for all
gold rigid order AANs in both orders.
We also consider the OPMI measure which was
so effective in distinguishing flexible vs. rigid or-
der AANs. We expect that the greater association
with AyN for attested-order AANs will again lead
to large, negative differences in PMI scores, while
the expectation that unattested-order AANs will be
highly associated with their AxN will correspond to
large, positive differences in PMI.
</bodyText>
<table confidence="0.999451">
Measure t sig.
cosA., -7.840 * U&gt;A
cosAy 7.924 * A&gt;U
cosN 2.394
W.ADD cosA.,N -5.462 * U&gt;A
cosAyN 3.627 * A&gt;U
cosA., -8.418 * U&gt;A
cosAy 6.534 * A&gt;U
cosN -1.927
F.ADD
cosA.,N -3.583 * U&gt;A
cosAyN -2.185
cosA., -5.100 * U&gt;A
cosAy 5.100 * A&gt;U
cosN 0.000
MULT cosA.,N -0.598
cosAyN 0.598
cosA., -7.498 * U&gt;A
cosAy 7.227 * A&gt;U
cosN -2.172
LFM
cosA.,N -5.792 * U&gt;A
cosAyN 0.774
OPMI -11.448 * U&gt;A
</table>
<tableCaption confidence="0.999342">
Table 5: Attested- vs. unattested-order rigid order
</tableCaption>
<figureCaption confidence="0.417092666666667">
AANs. t-normalized mean paired cosine (or OPMI) dif-
ferences between attested (A) and unattested (U) AANs
with their components. For significant differences (paired
t-test p&lt;0.05 after Bonferroni correction), last column
reports whether cosines (or OPMI) are on average larger
for A or U.
</figureCaption>
<bodyText confidence="0.9995409">
Across all composition models, we find that the
distance between the model-generated AAN and its
component adjectives, Ax and Ay, are significant in-
dicators of attested vs. unattested adjective ordering
(cf. Table 5). Specifically, we find that rigid order
AANs in the correct order are closest to their Ay,
while we can detect the unattested order when the
rigid order AAN is closer to its Ax. This finding
is quite interesting, since it shows that the order in
which the composition functions are applied does
not alter the fact that the modification of one ad-
jective in rigid order AANs (the Ay in the case of
attested-order rigid order AANs) is much stronger
than the other. Unlike the measures that differenti-
ated flexible and rigid order AANs, here we see that
the distance from the component N is not an indi-
cator of the correct adjective ordering (trivially so
for MULT, where attested and unattested AANs are
identical).
Next, we find that for W.ADD, F.ADD and LFM,
</bodyText>
<page confidence="0.996249">
148
</page>
<bodyText confidence="0.999955705882353">
the distance from the component A„N is a strong
indicator of attested- vs. unattested-order rigid order
AANs. Specifically, attested-order AANs are further
from their A„N than unattested-order AANs. This
finding is in line with our predictions and follows
the findings of the impact of the distance from the
component adjectives.
DPMI, as seen in the ability to distinguish flexi-
ble vs. rigid order AANs, is the strongest indicator
of correct vs wrong adjective ordering. This mea-
sure confirms that the association of one adjective
(the Ay in attested-order AANs) with the head noun
is indeed the most significant factor distinguishing
these two classes. However, as we mentioned be-
fore, this measure has its limitations and is likely not
to be entirely sufficient for future steps in modeling
recursive modification.
</bodyText>
<sectionHeader confidence="0.99974" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999829013157895">
While AN constructions have been extensively stud-
ied within the framework of compositional distri-
butional semantics (Baroni and Zamparelli, 2010;
Boleda et al., 2012; Boleda et al., 2013; Guevara,
2010; Mitchell and Lapata, 2010; Turney, 2012;
Vecchi et al., 2011), for the first time, we extended
the investigation to recursively built AAN phrases.
First, we showed that composition functions ap-
plied recursively can approximate corpus-extracted
AAN vectors that we know to be of high semantic
quality.
Next, we looked at some properties of the same
high-quality corpus-extracted AAN vectors, finding
that the distinction between “flexible” AANs, where
the adjective order can be flipped, and “rigid” ones,
where the order is fixed, is reflected in distributional
cues. These results all derive from the intuition that
the most embedded adjective in a rigid AAN has a
very strong effect on the distributional semantic rep-
resentation of the AAN. Most compositional models
were able to capture at least some of the same cues
that emerged in the analysis of the corpus-extracted
vectors.
Finally, similar cues were also shown to distin-
guish (compositional) representations of rigid AANs
in the “correct” (corpus-attested) and “wrong”
(unattested) orders, again pointing to the degree to
which the (attested-order) closest adjective affects
the overall AAN meaning as an important factor.
Comparing the composition functions, we find
that the linguistically motivated LFM approach has
the most consistent performance across all our tests.
This model significantly outperformed all others in
approximating high-quality corpus-extracted AAN
vectors, it provided the closest approximation to the
corpus-observed patterns when distinguishing flexi-
ble and rigid AANs, and it was one of the models
with the strongest cues distinguishing attested and
unattested orders of rigid AANs.
From an applied point of view, a natural next step
would be to use the cues we proposed as features to
train a classifier to predict the preferred order of ad-
jectives, to be tested also in cases where neither or-
der is found in the corpus, so direct corpus evidence
cannot help. For a full account of adjectival order-
ing, non-semantic factors should also be taken into
account. As shown by the effectiveness in our ex-
periments of PMI, which is a classic measure used
to harvest idioms and other multiword expressions
(Church and Hanks, 1990), ordering is affected by
arbitrary lexicalization patterns. Metrical effects are
also likely to play a role, like they do in the well-
studied case of “binomials” such as salt and pep-
per (Benor and Levy, 2006; Copestake and Herbe-
lot, 2011). In a pilot study, we found that indeed
word length (roughly quantified by number of let-
ters) is a significant factor in predicting adjective
ordering (the shorter adjective being more likely to
occur first), but its effect is not nearly as strong as
that of the semantic measures we considered here.
In our future work, we would like to develop an or-
der model that exploits semantic, metrical and lexi-
calization features jointly for maximal classification
accuracy.
Adjectival ordering information could be useful
in parsing: in English, it could tell whether an
AANN sequence should be parsed as A[[AN]N]
or A[A[NN]]; in languages with pre- and post-
N adjectives, like Italian or Spanish, it could tell
whether ANA sequences should be parsed as A[NA]
or [AN]A. The ability to detect ordering restric-
tions could also help Natural Language Generation
tasks (Malouf, 2000), especially for the generation
of unattested combinations of As and Ns.
From a theoretical point of view, we would like to
extend our analysis to adjective coordination (what’s
</bodyText>
<page confidence="0.996994">
149
</page>
<bodyText confidence="0.999983555555556">
the difference between new and creative idea and
new creative idea?). Additionally, we could go more
granular, looking at whether compositional models
can help us to understand why certain classes of ad-
jectives are more likely to precede or follow others
(why is size more likely to take scope over color,
so that big red car sounds more natural than red big
car?) or studying the behaviour of specific adjectives
(can our approach capture the fact that strong alco-
holic drink is preferable to alcoholic strong drink
because strong pertains to the alcoholic properties
of the drink?).
In the meantime, we hope that the results we re-
ported here provide convincing evidence of the use-
fulness of compositional distributional semantics in
tackling topics, such as recursive adjectival modifi-
cation, that have been of traditional interest to theo-
retical linguists from a new perspective.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9969504">
We would like to thank the anonymous reviewers,
Fabio Massimo Zanzotto, Yao-Zhong Zhang and the
members of the COMPOSES team. This research
was supported by the ERC 2011 Starting Indepen-
dent Research Grant n. 283554 (COMPOSES).
</bodyText>
<sectionHeader confidence="0.999098" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998508972972973">
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183–1193, Boston,
MA.
Sarah Bunin Benor and Roger Levy. 2006. The chicken
or the egg? A probabilistic analysis of english binomi-
als. Language, pages 233–278.
William Blacoe and Mirella Lapata. 2012. A comparison
of vector-based representations for semantic composi-
tion. In Proceedings of the 2012 Joint Conference on
EMNLP and CoNLL, pages 546–556, Jeju Island, Ko-
rea.
Gemma Boleda, Eva Maria Vecchi, Miquel Cornudella,
and Louise McNally. 2012. First-order vs. higher-
order modification in distributional semantics. In Pro-
ceedings of the 2012 Joint Conference on EMNLP and
CoNLL, pages 1223–1233, Jeju Island, Korea.
Gemma Boleda, Marco Baroni, Louise McNally, and
Nghia Pham. 2013. Intensionality was only alleged:
On adjective-noun composition in distributional se-
mantics. In Proceedings of IWCS, pages 35–46, Pots-
dam, Germany.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with amazon’s mechanical
turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon’s Mechanical Turk, pages 1–12, Los Angeles,
CA.
Kenneth Church and Peter Hanks. 1990. Word asso-
ciation norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22–29.
Guglielmo Cinque, editor. 2002. Functional Structure in
DP and IP - The Carthography of Syntactic Structures,
volume 1. Oxford University Press.
Guglielmo Cinque. 2004. Issues in adverbial syntax.
Lingua, 114:683–710.
Guglielmo Cinque. 2010. The syntax of adjectives: a
comparative study. MIT Press.
Ann Copestake and Aur´elie Herbelot. 2011. Exciting
and interesting: issues in the generation of binomials.
In Proceedings of the UCNLG+ Eval: Language Gen-
eration and Evaluation Workshop, pages 45–53, Edin-
burgh, UK.
Paola Crisma. 1991. Functional categories inside the
noun phrase: A study on the distribution of nominal
modifiers. “Tesi di Laurea”, University of Venice.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013a. DISSECT: DIStributional SEmantics Compo-
sition Toolkit. In Proceedings of the System Demon-
strations of ACL 2013, East Stroudsburg, PA.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013b. General estimation and evaluation of compo-
sitional distributional semantic models. In Proceed-
ings of the ACL 2013 Workshop on Continuous Vec-
tor Space Models and their Compositionality (CVSC
2013), East Stroudsburg, PA.
Gottlob Frege. 1892. ¨Uber sinn und bedeutung.
Zeitschrift fuer Philosophie un philosophische Kritik,
100.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
EMNLP, Edinburgh, UK.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the ACL GEMS Workshop,
pages 33–37, Uppsala, Sweden.
Chih-Jen Lin. 2007. Projected gradient methods for
Nonnegative Matrix Factorization. Neural Computa-
tion, 19(10):2756–2779.
Robert Malouf. 2000. The order of prenominal adjec-
tives in natural language generation. In Proceedings
of ACL, pages 85–92, East Stroudsburg, PA.
</reference>
<page confidence="0.979359">
150
</page>
<reference confidence="0.999903268656717">
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388–1429.
Richard Montague. 1970. Universal Grammar. Theoria,
36:373–398.
Robert Munro, Steven Bethard, Victor Kuperman,
Vicky Tzuyin Lai, Robin Melnick, Christopher Potts,
Tyler Schnoebelen, and Harry Tily. 2010. Crowd-
sourcing and language studies: the new generation of
linguistic data. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk, pages 122–130,
Los Angeles, CA.
Barbara Partee. 2004. Compositionality. In Compo-
sitionality in Formal Semantics: Selected Papers by
Barbara H. Partee. Blackwell, Oxford.
Magnus Sahlgren. 2006. The Word-Space Model. Dis-
sertation, Stockholm University.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceed-
ings of the EACL-SIGDAT Workshop, Dublin, Ireland.
Hinrich Sch¨utze. 1997. Ambiguity Resolution in Natural
Language Learning. CSLI, Stanford, CA.
Gary-John Scott. 2002. Stacked adjectival modification
and the structure of nominal phrases. In Guglielmo
Cinque, editor, Functional Structure in DP and IP. The
Carthography of Syntactic Structures, volume 1. Ox-
ford University Press.
Richard Socher, E.H. Huang, J. Pennington, Andrew Y.
Ng, and C.D. Manning. 2011. Dynamic pooling and
unfolding recursive autoencoders for paraphrase de-
tection. Advances in Neural Information Processing
Systems, 24:801–809.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201–1211, Edinburgh, UK.
Richard Sproat and Chilin Shih. 1990. The cross-
linguistics distribution of adjective ordering restric-
tions. In C. Georgopoulos and Ishihara R., editors,
Interdisciplinary approaches to language: essays in
honor of Yuki Kuroda, pages 565–593. Kluver, Dor-
drecht.
Sam Steddy and Vieri Samek-Lodovici. 2011. On the
ungrammaticality of remnant movement in the deriva-
tion of greenberg’s universal 20. Linguistic Inquiry,
42(3):445–469.
Richmond H. Thomason, editor. 1974. Formal Philoso-
phy: Selected Papers of Richard Montague. Yale Uni-
versity Press, New York.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141–188.
Peter Turney. 2012. Domain and function: A dual-space
model of semantic relations and compositions. Jour-
nal of Artificial Intelligence Research, 44:533–585.
Eva Maria Vecchi, Marco Baroni, and Roberto Zampar-
elli. 2011. (Linear) maps of the impossible: Cap-
turing semantic anomalies in distributional space. In
Proceedings of the ACL Workshop on Distributional
Semantics and Compositionality, pages 1–9, Portland,
OR.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca Faluc-
chi, and Suresh Manandhar. 2010. Estimating linear
models for compositional distributional semantics. In
Proceedings of COLING, pages 1263–1271, Beijing,
China.
</reference>
<page confidence="0.998343">
151
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.827478">
<title confidence="0.958625">Studying the recursive behaviour of adjectival with compositional distributional semantics</title>
<author confidence="0.999896">Maria Vecchi Zamparelli</author>
<affiliation confidence="0.965651">Center for Mind/Brain Sciences (University of Trento, Italy)</affiliation>
<email confidence="0.997212">(evamaria.vecchi|roberto.zamparelli|marco.baroni)@unitn.it</email>
<abstract confidence="0.994607944444444">In this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. Specifically, we focus on properties distinguishing Adjective- Adjective-Noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order. We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction. We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1183--1193</pages>
<location>Boston, MA.</location>
<contexts>
<context position="2167" citStr="Baroni and Zamparelli, 2010" startWordPosition="314" endWordPosition="317">d had been almost exclusively limited to the level of single content words (nouns, adjectives, verbs), and had not directly addressed the problem of compositionality (Frege, 1892; Montague, 1970; Partee, 2004), the crucial property of natural language which allows speakers to derive the meaning of a complex linguistic constituent from the meaning of its immediate syntactic subconstituents. Several recent proposals have strived to extend distributional semantics with a component that also generates vectors for complex linguistic constituents, using compositional operations in the vector space (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012). All of these approaches construct distributional representations for novel phrases starting from the corpusderived vectors for their lexical constituents and exploiting the geometric quality of the representation. Such methods are able to capture complex semantic information of adjective-noun (AN) phrases, such as characterizing modification (Boleda et al., 2012; Boleda et al., 2013), and can detect semantic deviance in novel phrases (Vecchi et al., 2011). Furthermore, these methods are naturall</context>
<context position="13735" citStr="Baroni and Zamparelli (2010)" startWordPosition="2179" endWordPosition="2182">β are scalars. Again, we can easily apply this function recursively, as in Equation (2). p~= α ~a. + β(α~ay + β~n) = α~a. + αβ~ay + β2~n (2) We also consider the full extension of the additive model (F.ADD), presented in Guevara (2010) and Zanzotto et al. (2010), such that the component vectors are pre-multiplied by weight matrices before being added: p~ = W1~u + W2~v. Similarly to the W.ADD model, Equation (3) describes how we apply this function recursively. p~ = W1~a. + W2(W1~ay + W2~n) (3) = W1~a. + W2W1~ay + W22~n Finally, we consider the lexical function model (LFM), first introduced in Baroni and Zamparelli (2010), in which attributive adjectives are treated as functions from noun meanings to noun meanings. This is a standard approach in Montague semantics (Thomason, 1974), except noun meanings here are distributional vectors, not denotations, and adjectives are (linear) functions learned from a large corpus. In this model, predicted vectors are generated 3We conjecture that the different performance of our multiplicative model and M&amp;L’s (cf. Table 1) is due to the fact that we use log-transformed pPMI scores, making their multiplicative model more akin to our additive approach. by multiplying a functi</context>
<context position="18701" citStr="Baroni and Zamparelli, 2010" startWordPosition="2996" endWordPosition="2999"> relatively distant from the AxN, i.e., different style. Finally, we consider a measure that does not exploit distributional semantic representations, namely the difference in PMI between AxN and AyN (DPMI). Based on our hypothesis described for the other measures, we expect the association in the corpus of AyN to be much greater than AxN for rigid order AANs, resulting in a large negative DPMI values. While flexible order AANs should have similar 5In the case of LFM, we compare the similarity of the AAN with the AN centroids for each adjective, since the model does not make use of A vectors (Baroni and Zamparelli, 2010). association strengths for both AxN and AyN, thus we expect DPMI to be closer to 0 than for rigid order AANs. 2.4 Gold standard To our knowledge, this is the first study to use distributional representations of recursive modification; therefore we must first determine if the composed AAN vector representations are semantically coherent objects. Thus, for vector analysis, a gold standard of 320 corpus-extracted AAN vectors were selected and their quality was established by inspecting their nearest neighbors. In order to create the gold standard, we ran a crowdsourcing experiment on CrowdFlower</context>
<context position="34012" citStr="Baroni and Zamparelli, 2010" startWordPosition="5490" endWordPosition="5493">he ability to distinguish flexible vs. rigid order AANs, is the strongest indicator of correct vs wrong adjective ordering. This measure confirms that the association of one adjective (the Ay in attested-order AANs) with the head noun is indeed the most significant factor distinguishing these two classes. However, as we mentioned before, this measure has its limitations and is likely not to be entirely sufficient for future steps in modeling recursive modification. 4 Conclusion While AN constructions have been extensively studied within the framework of compositional distributional semantics (Baroni and Zamparelli, 2010; Boleda et al., 2012; Boleda et al., 2013; Guevara, 2010; Mitchell and Lapata, 2010; Turney, 2012; Vecchi et al., 2011), for the first time, we extended the investigation to recursively built AAN phrases. First, we showed that composition functions applied recursively can approximate corpus-extracted AAN vectors that we know to be of high semantic quality. Next, we looked at some properties of the same high-quality corpus-extracted AAN vectors, finding that the distinction between “flexible” AANs, where the adjective order can be flipped, and “rigid” ones, where the order is fixed, is reflect</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of EMNLP, pages 1183–1193, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Bunin Benor</author>
<author>Roger Levy</author>
</authors>
<title>The chicken or the egg? A probabilistic analysis of english binomials.</title>
<date>2006</date>
<journal>Language,</journal>
<pages>233--278</pages>
<contexts>
<context position="36489" citStr="Benor and Levy, 2006" startWordPosition="5882" endWordPosition="5885">edict the preferred order of adjectives, to be tested also in cases where neither order is found in the corpus, so direct corpus evidence cannot help. For a full account of adjectival ordering, non-semantic factors should also be taken into account. As shown by the effectiveness in our experiments of PMI, which is a classic measure used to harvest idioms and other multiword expressions (Church and Hanks, 1990), ordering is affected by arbitrary lexicalization patterns. Metrical effects are also likely to play a role, like they do in the wellstudied case of “binomials” such as salt and pepper (Benor and Levy, 2006; Copestake and Herbelot, 2011). In a pilot study, we found that indeed word length (roughly quantified by number of letters) is a significant factor in predicting adjective ordering (the shorter adjective being more likely to occur first), but its effect is not nearly as strong as that of the semantic measures we considered here. In our future work, we would like to develop an order model that exploits semantic, metrical and lexicalization features jointly for maximal classification accuracy. Adjectival ordering information could be useful in parsing: in English, it could tell whether an AANN</context>
</contexts>
<marker>Benor, Levy, 2006</marker>
<rawString>Sarah Bunin Benor and Roger Levy. 2006. The chicken or the egg? A probabilistic analysis of english binomials. Language, pages 233–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on EMNLP and CoNLL,</booktitle>
<pages>546--556</pages>
<location>Jeju Island,</location>
<contexts>
<context position="6624" citStr="Blacoe and Lapata (2012)" startWordPosition="992" endWordPosition="995">onsisting of a matrix where each row represents the meaning of an adjective, noun, AN or AAN as a distributional vector, each column a semantic dimension of meaning. We first introduce the source corpus, then the vocabulary of words and phrases that we represent in the space, and finally the procedure adopted to build the vectors representing the vocabulary items from corpus statistics, and obtain the semantic space matrix. We work here with a traditional, window-based semantic space, since our focus is on the effect of different composition methods given a common semantic space. In addition, Blacoe and Lapata (2012) found that a vanilla space of this sort performed best in their composition experiments, when compared to a syntax-aware space and to neural language model vectors such as those used for composition by Socher et al. (2011). Source corpus We use as our source corpus the concatenation of the Web-derived ukWaC corpus, a mid-2009 dump of the English Wikipedia and the British National Corpus1. The corpus has been tokenized, POS-tagged and lemmatized with the TreeTagger (Schmid, 1995), and it contains about 2.8 billion tokens. We extract all statistics at the lemma level, meaning that we consider o</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composition. In Proceedings of the 2012 Joint Conference on EMNLP and CoNLL, pages 546–556, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gemma Boleda</author>
<author>Eva Maria Vecchi</author>
<author>Miquel Cornudella</author>
<author>Louise McNally</author>
</authors>
<title>First-order vs. higherorder modification in distributional semantics.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on EMNLP and CoNLL,</booktitle>
<pages>1223--1233</pages>
<location>Jeju Island,</location>
<contexts>
<context position="2631" citStr="Boleda et al., 2012" startWordPosition="380" endWordPosition="383"> component that also generates vectors for complex linguistic constituents, using compositional operations in the vector space (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012). All of these approaches construct distributional representations for novel phrases starting from the corpusderived vectors for their lexical constituents and exploiting the geometric quality of the representation. Such methods are able to capture complex semantic information of adjective-noun (AN) phrases, such as characterizing modification (Boleda et al., 2012; Boleda et al., 2013), and can detect semantic deviance in novel phrases (Vecchi et al., 2011). Furthermore, these methods are naturally recursive: they can derive a representation not only for, e.g., red car, but also for new red car, fast new red car, etc. This aspect is appealing since trying to extract meaningful representations for all recursive phrases directly from a corpus will result in a problem of sparsity, since most large phrases will never occur in any finite sample. Once we start seriously looking into recursive modification, however, the issue of modifier ordering restrictions</context>
<context position="34033" citStr="Boleda et al., 2012" startWordPosition="5494" endWordPosition="5497">xible vs. rigid order AANs, is the strongest indicator of correct vs wrong adjective ordering. This measure confirms that the association of one adjective (the Ay in attested-order AANs) with the head noun is indeed the most significant factor distinguishing these two classes. However, as we mentioned before, this measure has its limitations and is likely not to be entirely sufficient for future steps in modeling recursive modification. 4 Conclusion While AN constructions have been extensively studied within the framework of compositional distributional semantics (Baroni and Zamparelli, 2010; Boleda et al., 2012; Boleda et al., 2013; Guevara, 2010; Mitchell and Lapata, 2010; Turney, 2012; Vecchi et al., 2011), for the first time, we extended the investigation to recursively built AAN phrases. First, we showed that composition functions applied recursively can approximate corpus-extracted AAN vectors that we know to be of high semantic quality. Next, we looked at some properties of the same high-quality corpus-extracted AAN vectors, finding that the distinction between “flexible” AANs, where the adjective order can be flipped, and “rigid” ones, where the order is fixed, is reflected in distributional </context>
</contexts>
<marker>Boleda, Vecchi, Cornudella, McNally, 2012</marker>
<rawString>Gemma Boleda, Eva Maria Vecchi, Miquel Cornudella, and Louise McNally. 2012. First-order vs. higherorder modification in distributional semantics. In Proceedings of the 2012 Joint Conference on EMNLP and CoNLL, pages 1223–1233, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>Louise McNally</author>
<author>Nghia Pham</author>
</authors>
<title>Intensionality was only alleged: On adjective-noun composition in distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of IWCS,</booktitle>
<pages>35--46</pages>
<location>Potsdam, Germany.</location>
<contexts>
<context position="2653" citStr="Boleda et al., 2013" startWordPosition="384" endWordPosition="387">generates vectors for complex linguistic constituents, using compositional operations in the vector space (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012). All of these approaches construct distributional representations for novel phrases starting from the corpusderived vectors for their lexical constituents and exploiting the geometric quality of the representation. Such methods are able to capture complex semantic information of adjective-noun (AN) phrases, such as characterizing modification (Boleda et al., 2012; Boleda et al., 2013), and can detect semantic deviance in novel phrases (Vecchi et al., 2011). Furthermore, these methods are naturally recursive: they can derive a representation not only for, e.g., red car, but also for new red car, fast new red car, etc. This aspect is appealing since trying to extract meaningful representations for all recursive phrases directly from a corpus will result in a problem of sparsity, since most large phrases will never occur in any finite sample. Once we start seriously looking into recursive modification, however, the issue of modifier ordering restrictions naturally arises. Suc</context>
<context position="34054" citStr="Boleda et al., 2013" startWordPosition="5498" endWordPosition="5501"> AANs, is the strongest indicator of correct vs wrong adjective ordering. This measure confirms that the association of one adjective (the Ay in attested-order AANs) with the head noun is indeed the most significant factor distinguishing these two classes. However, as we mentioned before, this measure has its limitations and is likely not to be entirely sufficient for future steps in modeling recursive modification. 4 Conclusion While AN constructions have been extensively studied within the framework of compositional distributional semantics (Baroni and Zamparelli, 2010; Boleda et al., 2012; Boleda et al., 2013; Guevara, 2010; Mitchell and Lapata, 2010; Turney, 2012; Vecchi et al., 2011), for the first time, we extended the investigation to recursively built AAN phrases. First, we showed that composition functions applied recursively can approximate corpus-extracted AAN vectors that we know to be of high semantic quality. Next, we looked at some properties of the same high-quality corpus-extracted AAN vectors, finding that the distinction between “flexible” AANs, where the adjective order can be flipped, and “rigid” ones, where the order is fixed, is reflected in distributional cues. These results a</context>
</contexts>
<marker>Boleda, Baroni, McNally, Pham, 2013</marker>
<rawString>Gemma Boleda, Marco Baroni, Louise McNally, and Nghia Pham. 2013. Intensionality was only alleged: On adjective-noun composition in distributional semantics. In Proceedings of IWCS, pages 35–46, Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating speech and language data with amazon’s mechanical turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>1--12</pages>
<location>Los Angeles, CA.</location>
<contexts>
<context position="19335" citStr="Callison-Burch and Dredze, 2010" startWordPosition="3096" endWordPosition="3099">ssociation strengths for both AxN and AyN, thus we expect DPMI to be closer to 0 than for rigid order AANs. 2.4 Gold standard To our knowledge, this is the first study to use distributional representations of recursive modification; therefore we must first determine if the composed AAN vector representations are semantically coherent objects. Thus, for vector analysis, a gold standard of 320 corpus-extracted AAN vectors were selected and their quality was established by inspecting their nearest neighbors. In order to create the gold standard, we ran a crowdsourcing experiment on CrowdFlower6 (Callison-Burch and Dredze, 2010; Munro et al., 2010), as follows. First, we gathered a randomly selected set of 600 corpus-extracted AANs, containing 300 flexible order and 300 attested rigid order AANs. We then extracted the top 3 nearest neighbors to the corpusextracted AAN vectors as represented in the semantic space7. Each AAN was then presented with each of the nearest neighbors, and participants were asked to judge “how strongly related are the two phrases?” on a scale of 1-7. The rationale was that if we obtained a good distributional representation of the AAN, its nearest neighbors should be closely related words an</context>
</contexts>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with amazon’s mechanical turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 1–12, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Peter Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="11288" citStr="Church and Hanks, 1990" startWordPosition="1767" endWordPosition="1770"> Lapata (2010), as well as best reported results from Mitchell &amp; Lapata (M&amp;L). Semantic vector construction For each of the items in our vocabulary, we first build 10Kdimensional vectors by recording the item’s sentence-internal co-occurrence with the top 10K most frequent content lemmas (nouns, adjectives, verbs or adverbs) in the corpus. We built a rank of these co-occurrence counts, and excluded as stop words from the dimensions any element of any POS whose rank was from 0 to 300. The raw co-occurrence counts were then transformed into (positive) Pointwise Mutual Information (pPMI) scores (Church and Hanks, 1990). Next, we reduce the full co-occurrence matrix to 300 dimensions applying the Non-negative Matrix Factorization (NMF) operation (Lin, 2007). We did not tune the semantic vector construction parameters, since we found them to work best in a number of independent earlier experiments. Corpus-extracted vectors (corp) were computed for the ANs and for the flexible order and attested rigid order AANs, and then mapped onto the 300- dimension NMF-reduced semantic space. As a sanity check, the first row of Table 1 reports the correlation between the AN phrase similarity ratings collected in Mitchell a</context>
<context position="36282" citStr="Church and Hanks, 1990" startWordPosition="5846" endWordPosition="5849">th the strongest cues distinguishing attested and unattested orders of rigid AANs. From an applied point of view, a natural next step would be to use the cues we proposed as features to train a classifier to predict the preferred order of adjectives, to be tested also in cases where neither order is found in the corpus, so direct corpus evidence cannot help. For a full account of adjectival ordering, non-semantic factors should also be taken into account. As shown by the effectiveness in our experiments of PMI, which is a classic measure used to harvest idioms and other multiword expressions (Church and Hanks, 1990), ordering is affected by arbitrary lexicalization patterns. Metrical effects are also likely to play a role, like they do in the wellstudied case of “binomials” such as salt and pepper (Benor and Levy, 2006; Copestake and Herbelot, 2011). In a pilot study, we found that indeed word length (roughly quantified by number of letters) is a significant factor in predicting adjective ordering (the shorter adjective being more likely to occur first), but its effect is not nearly as strong as that of the semantic measures we considered here. In our future work, we would like to develop an order model </context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Church and Peter Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<date>2002</date>
<booktitle>Functional Structure in DP and IP - The Carthography of Syntactic Structures,</booktitle>
<volume>1</volume>
<editor>Guglielmo Cinque, editor.</editor>
<publisher>Oxford University Press.</publisher>
<marker>2002</marker>
<rawString>Guglielmo Cinque, editor. 2002. Functional Structure in DP and IP - The Carthography of Syntactic Structures, volume 1. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guglielmo Cinque</author>
</authors>
<date>2004</date>
<booktitle>Issues in adverbial syntax. Lingua,</booktitle>
<pages>114--683</pages>
<contexts>
<context position="4610" citStr="Cinque (2004)" startWordPosition="683" endWordPosition="684">odifiers (Cinque, 2010; Steddy and Samek-Lodovici, 2011), one key assumption of the cartographic literature is that exactly one intonationally unmarked order for stacked adjectives should be possible in languages like English. The possibility of alternative orders, when discussed at all, is attributed to the presence of idioms (high American building, but American high officer), to asyndetic conjunctive meanings (e.g. new creative idea parsed as [new &amp; creative] idea, rather than [new [creative idea]]), or to semantic category ambiguity for any adjective which appears in different orders (see Cinque (2004) for discussion). In this study, we show that the existence of both rigid and flexible order cases is robustly attested at least for adjectival modification, and that flexible ordering is unlikely to reduce to idioms, coordination or ambiguity. Moreover, we show that at least for some recursively constructed adjective-adjectivenoun phrases (AANs) we can extract meaningful representations from the corpus, approximating them reasonably well by means of compositional distributional semantic models, and that the semantic information contained in these models characterizes which AA will have rigid </context>
</contexts>
<marker>Cinque, 2004</marker>
<rawString>Guglielmo Cinque. 2004. Issues in adverbial syntax. Lingua, 114:683–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guglielmo Cinque</author>
</authors>
<title>The syntax of adjectives: a comparative study.</title>
<date>2010</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4019" citStr="Cinque, 2010" startWordPosition="593" endWordPosition="594">of the key in141 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 141–151, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics gredients of the ‘cartographic’ approach to syntax (Cinque, 2002). In this paradigm, the ordering is derived by assigning semantically different classes of modifiers to the specifiers of distinct functional projections, whose sequence is hard-wired. While it is accepted that in different languages movement can lead to a principled rearrangement of the linear order of the modifiers (Cinque, 2010; Steddy and Samek-Lodovici, 2011), one key assumption of the cartographic literature is that exactly one intonationally unmarked order for stacked adjectives should be possible in languages like English. The possibility of alternative orders, when discussed at all, is attributed to the presence of idioms (high American building, but American high officer), to asyndetic conjunctive meanings (e.g. new creative idea parsed as [new &amp; creative] idea, rather than [new [creative idea]]), or to semantic category ambiguity for any adjective which appears in different orders (see Cinque (2004) for disc</context>
</contexts>
<marker>Cinque, 2010</marker>
<rawString>Guglielmo Cinque. 2010. The syntax of adjectives: a comparative study. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Aur´elie Herbelot</author>
</authors>
<title>Exciting and interesting: issues in the generation of binomials.</title>
<date>2011</date>
<booktitle>In Proceedings of the UCNLG+ Eval: Language Generation and Evaluation Workshop,</booktitle>
<pages>45--53</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="36520" citStr="Copestake and Herbelot, 2011" startWordPosition="5886" endWordPosition="5890">der of adjectives, to be tested also in cases where neither order is found in the corpus, so direct corpus evidence cannot help. For a full account of adjectival ordering, non-semantic factors should also be taken into account. As shown by the effectiveness in our experiments of PMI, which is a classic measure used to harvest idioms and other multiword expressions (Church and Hanks, 1990), ordering is affected by arbitrary lexicalization patterns. Metrical effects are also likely to play a role, like they do in the wellstudied case of “binomials” such as salt and pepper (Benor and Levy, 2006; Copestake and Herbelot, 2011). In a pilot study, we found that indeed word length (roughly quantified by number of letters) is a significant factor in predicting adjective ordering (the shorter adjective being more likely to occur first), but its effect is not nearly as strong as that of the semantic measures we considered here. In our future work, we would like to develop an order model that exploits semantic, metrical and lexicalization features jointly for maximal classification accuracy. Adjectival ordering information could be useful in parsing: in English, it could tell whether an AANN sequence should be parsed as A</context>
</contexts>
<marker>Copestake, Herbelot, 2011</marker>
<rawString>Ann Copestake and Aur´elie Herbelot. 2011. Exciting and interesting: issues in the generation of binomials. In Proceedings of the UCNLG+ Eval: Language Generation and Evaluation Workshop, pages 45–53, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Crisma</author>
</authors>
<title>Functional categories inside the noun phrase: A study on the distribution of nominal modifiers. “Tesi di Laurea”,</title>
<date>1991</date>
<institution>University of Venice.</institution>
<contexts>
<context position="3371" citStr="Crisma, 1991" startWordPosition="499" endWordPosition="500">naturally recursive: they can derive a representation not only for, e.g., red car, but also for new red car, fast new red car, etc. This aspect is appealing since trying to extract meaningful representations for all recursive phrases directly from a corpus will result in a problem of sparsity, since most large phrases will never occur in any finite sample. Once we start seriously looking into recursive modification, however, the issue of modifier ordering restrictions naturally arises. Such restrictions have often been discussed in the theoretical linguistic literature (Sproat and Shih, 1990; Crisma, 1991; Scott, 2002), and have become one of the key in141 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 141–151, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics gredients of the ‘cartographic’ approach to syntax (Cinque, 2002). In this paradigm, the ordering is derived by assigning semantically different classes of modifiers to the specifiers of distinct functional projections, whose sequence is hard-wired. While it is accepted that in different languages movement can lead to a principled rearrangement of</context>
</contexts>
<marker>Crisma, 1991</marker>
<rawString>Paola Crisma. 1991. Functional categories inside the noun phrase: A study on the distribution of nominal modifiers. “Tesi di Laurea”, University of Venice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>DISSECT: DIStributional SEmantics Composition Toolkit.</title>
<date>2013</date>
<booktitle>In Proceedings of the System Demonstrations of ACL 2013,</booktitle>
<location>East Stroudsburg, PA.</location>
<contexts>
<context position="14761" citStr="Dinu et al. (2013" startWordPosition="2345" endWordPosition="2348">icative model and M&amp;L’s (cf. Table 1) is due to the fact that we use log-transformed pPMI scores, making their multiplicative model more akin to our additive approach. by multiplying a function matrix U with a component vector: p~ = U~v. Given a weight matrix, A, for each adjective in the phrase, we apply the functions in sequence recursively as shown in Equation (4). p~ = A.(Ay~n) (4) Composition model estimation Parameters for W.ADD, F.ADD and LFM were estimated following the strategy proposed by Guevara (2010) and Baroni and Zamparelli (2010), recently extended to all composition models by Dinu et al. (2013b). Specifically, we learn parameter values that optimize the mapping from the noun to the AN as seen in examples of corpus-extracted N-AN vector pairs, using least-squares methods. All parameter estimations and phrase compositions were implemented using the DISSECT toolkit4 (Dinu et al., 2013a), with a training set of 74,767 corpus-extracted NAN vector pairs, ranging from 100 to over 1K items across the 663 adjectives. Importantly, while below we report experimental results on capturing various properties of recursive AAN constructions, no AAN was seen during training, which was based entirel</context>
</contexts>
<marker>Dinu, Pham, Baroni, 2013</marker>
<rawString>Georgiana Dinu, Nghia The Pham, and Marco Baroni. 2013a. DISSECT: DIStributional SEmantics Composition Toolkit. In Proceedings of the System Demonstrations of ACL 2013, East Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>General estimation and evaluation of compositional distributional semantic models.</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL 2013 Workshop on Continuous Vector Space Models and their Compositionality (CVSC 2013),</booktitle>
<location>East Stroudsburg, PA.</location>
<contexts>
<context position="14761" citStr="Dinu et al. (2013" startWordPosition="2345" endWordPosition="2348">icative model and M&amp;L’s (cf. Table 1) is due to the fact that we use log-transformed pPMI scores, making their multiplicative model more akin to our additive approach. by multiplying a function matrix U with a component vector: p~ = U~v. Given a weight matrix, A, for each adjective in the phrase, we apply the functions in sequence recursively as shown in Equation (4). p~ = A.(Ay~n) (4) Composition model estimation Parameters for W.ADD, F.ADD and LFM were estimated following the strategy proposed by Guevara (2010) and Baroni and Zamparelli (2010), recently extended to all composition models by Dinu et al. (2013b). Specifically, we learn parameter values that optimize the mapping from the noun to the AN as seen in examples of corpus-extracted N-AN vector pairs, using least-squares methods. All parameter estimations and phrase compositions were implemented using the DISSECT toolkit4 (Dinu et al., 2013a), with a training set of 74,767 corpus-extracted NAN vector pairs, ranging from 100 to over 1K items across the 663 adjectives. Importantly, while below we report experimental results on capturing various properties of recursive AAN constructions, no AAN was seen during training, which was based entirel</context>
</contexts>
<marker>Dinu, Pham, Baroni, 2013</marker>
<rawString>Georgiana Dinu, Nghia The Pham, and Marco Baroni. 2013b. General estimation and evaluation of compositional distributional semantic models. In Proceedings of the ACL 2013 Workshop on Continuous Vector Space Models and their Compositionality (CVSC 2013), East Stroudsburg, PA.</rawString>
</citation>
<citation valid="false">
<title>Uber sinn und bedeutung. Zeitschrift fuer Philosophie un philosophische Kritik,</title>
<pages>100</pages>
<marker></marker>
<rawString>Gottlob Frege. 1892. ¨Uber sinn und bedeutung. Zeitschrift fuer Philosophie un philosophische Kritik, 100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Edinburgh, UK.</location>
<contexts>
<context position="2243" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="324" endWordPosition="327">rds (nouns, adjectives, verbs), and had not directly addressed the problem of compositionality (Frege, 1892; Montague, 1970; Partee, 2004), the crucial property of natural language which allows speakers to derive the meaning of a complex linguistic constituent from the meaning of its immediate syntactic subconstituents. Several recent proposals have strived to extend distributional semantics with a component that also generates vectors for complex linguistic constituents, using compositional operations in the vector space (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012). All of these approaches construct distributional representations for novel phrases starting from the corpusderived vectors for their lexical constituents and exploiting the geometric quality of the representation. Such methods are able to capture complex semantic information of adjective-noun (AN) phrases, such as characterizing modification (Boleda et al., 2012; Boleda et al., 2013), and can detect semantic deviance in novel phrases (Vecchi et al., 2011). Furthermore, these methods are naturally recursive: they can derive a representation not only for, e.g., red car, b</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of EMNLP, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>A regression model of adjective-noun compositionality in distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL GEMS Workshop,</booktitle>
<pages>33--37</pages>
<location>Uppsala,</location>
<contexts>
<context position="2182" citStr="Guevara, 2010" startWordPosition="318" endWordPosition="319"> limited to the level of single content words (nouns, adjectives, verbs), and had not directly addressed the problem of compositionality (Frege, 1892; Montague, 1970; Partee, 2004), the crucial property of natural language which allows speakers to derive the meaning of a complex linguistic constituent from the meaning of its immediate syntactic subconstituents. Several recent proposals have strived to extend distributional semantics with a component that also generates vectors for complex linguistic constituents, using compositional operations in the vector space (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012). All of these approaches construct distributional representations for novel phrases starting from the corpusderived vectors for their lexical constituents and exploiting the geometric quality of the representation. Such methods are able to capture complex semantic information of adjective-noun (AN) phrases, such as characterizing modification (Boleda et al., 2012; Boleda et al., 2013), and can detect semantic deviance in novel phrases (Vecchi et al., 2011). Furthermore, these methods are naturally recursive: th</context>
<context position="13342" citStr="Guevara (2010)" startWordPosition="2115" endWordPosition="2116">tends naturally to the recursive setting of this experiment, as seen in Equation (1). p~= ~a. O~ay O n~ (1) This composition method is order-insensitive, the formula above corresponding to the representation of both A.AyN and AyA.N. In the weighted additive model (W.ADD), we obtain the composed vector as a weighted sum of the two component vectors: p~ = α~u + β~v, where α and β are scalars. Again, we can easily apply this function recursively, as in Equation (2). p~= α ~a. + β(α~ay + β~n) = α~a. + αβ~ay + β2~n (2) We also consider the full extension of the additive model (F.ADD), presented in Guevara (2010) and Zanzotto et al. (2010), such that the component vectors are pre-multiplied by weight matrices before being added: p~ = W1~u + W2~v. Similarly to the W.ADD model, Equation (3) describes how we apply this function recursively. p~ = W1~a. + W2(W1~ay + W2~n) (3) = W1~a. + W2W1~ay + W22~n Finally, we consider the lexical function model (LFM), first introduced in Baroni and Zamparelli (2010), in which attributive adjectives are treated as functions from noun meanings to noun meanings. This is a standard approach in Montague semantics (Thomason, 1974), except noun meanings here are distributiona</context>
<context position="14662" citStr="Guevara (2010)" startWordPosition="2330" endWordPosition="2331">el, predicted vectors are generated 3We conjecture that the different performance of our multiplicative model and M&amp;L’s (cf. Table 1) is due to the fact that we use log-transformed pPMI scores, making their multiplicative model more akin to our additive approach. by multiplying a function matrix U with a component vector: p~ = U~v. Given a weight matrix, A, for each adjective in the phrase, we apply the functions in sequence recursively as shown in Equation (4). p~ = A.(Ay~n) (4) Composition model estimation Parameters for W.ADD, F.ADD and LFM were estimated following the strategy proposed by Guevara (2010) and Baroni and Zamparelli (2010), recently extended to all composition models by Dinu et al. (2013b). Specifically, we learn parameter values that optimize the mapping from the noun to the AN as seen in examples of corpus-extracted N-AN vector pairs, using least-squares methods. All parameter estimations and phrase compositions were implemented using the DISSECT toolkit4 (Dinu et al., 2013a), with a training set of 74,767 corpus-extracted NAN vector pairs, ranging from 100 to over 1K items across the 663 adjectives. Importantly, while below we report experimental results on capturing various </context>
<context position="34069" citStr="Guevara, 2010" startWordPosition="5502" endWordPosition="5503">st indicator of correct vs wrong adjective ordering. This measure confirms that the association of one adjective (the Ay in attested-order AANs) with the head noun is indeed the most significant factor distinguishing these two classes. However, as we mentioned before, this measure has its limitations and is likely not to be entirely sufficient for future steps in modeling recursive modification. 4 Conclusion While AN constructions have been extensively studied within the framework of compositional distributional semantics (Baroni and Zamparelli, 2010; Boleda et al., 2012; Boleda et al., 2013; Guevara, 2010; Mitchell and Lapata, 2010; Turney, 2012; Vecchi et al., 2011), for the first time, we extended the investigation to recursively built AAN phrases. First, we showed that composition functions applied recursively can approximate corpus-extracted AAN vectors that we know to be of high semantic quality. Next, we looked at some properties of the same high-quality corpus-extracted AAN vectors, finding that the distinction between “flexible” AANs, where the adjective order can be flipped, and “rigid” ones, where the order is fixed, is reflected in distributional cues. These results all derive from </context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Emiliano Guevara. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceedings of the ACL GEMS Workshop, pages 33–37, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Jen Lin</author>
</authors>
<title>Projected gradient methods for Nonnegative Matrix Factorization.</title>
<date>2007</date>
<journal>Neural Computation,</journal>
<volume>19</volume>
<issue>10</issue>
<contexts>
<context position="11428" citStr="Lin, 2007" startWordPosition="1788" endWordPosition="1789">irst build 10Kdimensional vectors by recording the item’s sentence-internal co-occurrence with the top 10K most frequent content lemmas (nouns, adjectives, verbs or adverbs) in the corpus. We built a rank of these co-occurrence counts, and excluded as stop words from the dimensions any element of any POS whose rank was from 0 to 300. The raw co-occurrence counts were then transformed into (positive) Pointwise Mutual Information (pPMI) scores (Church and Hanks, 1990). Next, we reduce the full co-occurrence matrix to 300 dimensions applying the Non-negative Matrix Factorization (NMF) operation (Lin, 2007). We did not tune the semantic vector construction parameters, since we found them to work best in a number of independent earlier experiments. Corpus-extracted vectors (corp) were computed for the ANs and for the flexible order and attested rigid order AANs, and then mapped onto the 300- dimension NMF-reduced semantic space. As a sanity check, the first row of Table 1 reports the correlation between the AN phrase similarity ratings collected in Mitchell and Lapata (2010) and the cosines of corpus-extracted vectors in our space, for the same ANs. For the AAN vectors, which are sparser, we used</context>
</contexts>
<marker>Lin, 2007</marker>
<rawString>Chih-Jen Lin. 2007. Projected gradient methods for Nonnegative Matrix Factorization. Neural Computation, 19(10):2756–2779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>The order of prenominal adjectives in natural language generation.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>85--92</pages>
<location>East Stroudsburg, PA.</location>
<contexts>
<context position="37391" citStr="Malouf, 2000" startWordPosition="6031" endWordPosition="6032">f the semantic measures we considered here. In our future work, we would like to develop an order model that exploits semantic, metrical and lexicalization features jointly for maximal classification accuracy. Adjectival ordering information could be useful in parsing: in English, it could tell whether an AANN sequence should be parsed as A[[AN]N] or A[A[NN]]; in languages with pre- and postN adjectives, like Italian or Spanish, it could tell whether ANA sequences should be parsed as A[NA] or [AN]A. The ability to detect ordering restrictions could also help Natural Language Generation tasks (Malouf, 2000), especially for the generation of unattested combinations of As and Ns. From a theoretical point of view, we would like to extend our analysis to adjective coordination (what’s 149 the difference between new and creative idea and new creative idea?). Additionally, we could go more granular, looking at whether compositional models can help us to understand why certain classes of adjectives are more likely to precede or follow others (why is size more likely to take scope over color, so that big red car sounds more natural than red big car?) or studying the behaviour of specific adjectives (can</context>
</contexts>
<marker>Malouf, 2000</marker>
<rawString>Robert Malouf. 2000. The order of prenominal adjectives in natural language generation. In Proceedings of ACL, pages 85–92, East Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="2209" citStr="Mitchell and Lapata, 2010" startWordPosition="320" endWordPosition="323"> level of single content words (nouns, adjectives, verbs), and had not directly addressed the problem of compositionality (Frege, 1892; Montague, 1970; Partee, 2004), the crucial property of natural language which allows speakers to derive the meaning of a complex linguistic constituent from the meaning of its immediate syntactic subconstituents. Several recent proposals have strived to extend distributional semantics with a component that also generates vectors for complex linguistic constituents, using compositional operations in the vector space (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012). All of these approaches construct distributional representations for novel phrases starting from the corpusderived vectors for their lexical constituents and exploiting the geometric quality of the representation. Such methods are able to capture complex semantic information of adjective-noun (AN) phrases, such as characterizing modification (Boleda et al., 2012; Boleda et al., 2013), and can detect semantic deviance in novel phrases (Vecchi et al., 2011). Furthermore, these methods are naturally recursive: they can derive a representat</context>
<context position="10679" citStr="Mitchell and Lapata (2010)" startWordPosition="1671" endWordPosition="1674">cluded as an additional filter against potential multiword expressions. Of course, the boundary between phrases that are at least partially compositional and those that are fully lexicalized is not sharp, and we leave it to further work to explore the interplay between the semantic factors we study here and patterns of lexicalization. Model p M&amp;L CORP 0.41 0.43 W.ADD 0.41 0.44 F.ADD 0.40 – MULT 0.33 0.46 LFM 0.40 – Table 1: Correlation scores (Spearman’s p, all significant at p &lt;0.001) between cosines of corpus-extracted or model-generated AN vectors and phrase similarity ratings collected in Mitchell and Lapata (2010), as well as best reported results from Mitchell &amp; Lapata (M&amp;L). Semantic vector construction For each of the items in our vocabulary, we first build 10Kdimensional vectors by recording the item’s sentence-internal co-occurrence with the top 10K most frequent content lemmas (nouns, adjectives, verbs or adverbs) in the corpus. We built a rank of these co-occurrence counts, and excluded as stop words from the dimensions any element of any POS whose rank was from 0 to 300. The raw co-occurrence counts were then transformed into (positive) Pointwise Mutual Information (pPMI) scores (Church and Han</context>
<context position="12341" citStr="Mitchell and Lapata (2010)" startWordPosition="1941" endWordPosition="1944">0- dimension NMF-reduced semantic space. As a sanity check, the first row of Table 1 reports the correlation between the AN phrase similarity ratings collected in Mitchell and Lapata (2010) and the cosines of corpus-extracted vectors in our space, for the same ANs. For the AAN vectors, which are sparser, we used human judgements to build a reliable subset to serve as our gold standard, as detailed in Section 2.4. 2.2 Composition models We focus on four composition functions proposed in recent literature with high performance in a number of semantic tasks. We first consider methods proposed by Mitchell and Lapata (2010) in 143 which the model-generated vectors are simply obtained through component-wise operations on the constituent vectors. Given input vectors u~ and ~v, the multiplicative model (MULT) computes a composed vector by component-wise multiplication (0) of the constituent vectors, where the i-th component of the composed vector is given by pi = uivi.3 Given an A.AyN phrase, this model extends naturally to the recursive setting of this experiment, as seen in Equation (1). p~= ~a. O~ay O n~ (1) This composition method is order-insensitive, the formula above corresponding to the representation of bo</context>
<context position="34096" citStr="Mitchell and Lapata, 2010" startWordPosition="5504" endWordPosition="5507"> correct vs wrong adjective ordering. This measure confirms that the association of one adjective (the Ay in attested-order AANs) with the head noun is indeed the most significant factor distinguishing these two classes. However, as we mentioned before, this measure has its limitations and is likely not to be entirely sufficient for future steps in modeling recursive modification. 4 Conclusion While AN constructions have been extensively studied within the framework of compositional distributional semantics (Baroni and Zamparelli, 2010; Boleda et al., 2012; Boleda et al., 2013; Guevara, 2010; Mitchell and Lapata, 2010; Turney, 2012; Vecchi et al., 2011), for the first time, we extended the investigation to recursively built AAN phrases. First, we showed that composition functions applied recursively can approximate corpus-extracted AAN vectors that we know to be of high semantic quality. Next, we looked at some properties of the same high-quality corpus-extracted AAN vectors, finding that the distinction between “flexible” AANs, where the adjective order can be flipped, and “rigid” ones, where the order is fixed, is reflected in distributional cues. These results all derive from the intuition that the most</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<date>1970</date>
<journal>Universal Grammar. Theoria,</journal>
<pages>36--373</pages>
<contexts>
<context position="1734" citStr="Montague, 1970" startWordPosition="252" endWordPosition="253">nguage Processing (NLP) is to treat it as a numerical vector that codes the pattern of co-occurrence of that word with other expressions in a large corpus of language (Sahlgren, 2006; Turney and Pantel, 2010). This approach to semantics (sometimes called distributional semantics) scales well to large lexicons and does not require words to be manually disambiguated (Sch¨utze, 1997). Until recently, however, this method had been almost exclusively limited to the level of single content words (nouns, adjectives, verbs), and had not directly addressed the problem of compositionality (Frege, 1892; Montague, 1970; Partee, 2004), the crucial property of natural language which allows speakers to derive the meaning of a complex linguistic constituent from the meaning of its immediate syntactic subconstituents. Several recent proposals have strived to extend distributional semantics with a component that also generates vectors for complex linguistic constituents, using compositional operations in the vector space (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012). All of these approaches construct distributional representations fo</context>
</contexts>
<marker>Montague, 1970</marker>
<rawString>Richard Montague. 1970. Universal Grammar. Theoria, 36:373–398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Munro</author>
<author>Steven Bethard</author>
<author>Victor Kuperman</author>
<author>Vicky Tzuyin Lai</author>
<author>Robin Melnick</author>
<author>Christopher Potts</author>
<author>Tyler Schnoebelen</author>
<author>Harry Tily</author>
</authors>
<title>Crowdsourcing and language studies: the new generation of linguistic data.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>122--130</pages>
<location>Los Angeles, CA.</location>
<contexts>
<context position="19356" citStr="Munro et al., 2010" startWordPosition="3100" endWordPosition="3103"> and AyN, thus we expect DPMI to be closer to 0 than for rigid order AANs. 2.4 Gold standard To our knowledge, this is the first study to use distributional representations of recursive modification; therefore we must first determine if the composed AAN vector representations are semantically coherent objects. Thus, for vector analysis, a gold standard of 320 corpus-extracted AAN vectors were selected and their quality was established by inspecting their nearest neighbors. In order to create the gold standard, we ran a crowdsourcing experiment on CrowdFlower6 (Callison-Burch and Dredze, 2010; Munro et al., 2010), as follows. First, we gathered a randomly selected set of 600 corpus-extracted AANs, containing 300 flexible order and 300 attested rigid order AANs. We then extracted the top 3 nearest neighbors to the corpusextracted AAN vectors as represented in the semantic space7. Each AAN was then presented with each of the nearest neighbors, and participants were asked to judge “how strongly related are the two phrases?” on a scale of 1-7. The rationale was that if we obtained a good distributional representation of the AAN, its nearest neighbors should be closely related words and phrases. Each pair </context>
</contexts>
<marker>Munro, Bethard, Kuperman, Lai, Melnick, Potts, Schnoebelen, Tily, 2010</marker>
<rawString>Robert Munro, Steven Bethard, Victor Kuperman, Vicky Tzuyin Lai, Robin Melnick, Christopher Potts, Tyler Schnoebelen, and Harry Tily. 2010. Crowdsourcing and language studies: the new generation of linguistic data. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 122–130, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Partee</author>
</authors>
<date>2004</date>
<booktitle>Compositionality. In Compositionality in Formal Semantics: Selected Papers by</booktitle>
<publisher>Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="1749" citStr="Partee, 2004" startWordPosition="254" endWordPosition="255">g (NLP) is to treat it as a numerical vector that codes the pattern of co-occurrence of that word with other expressions in a large corpus of language (Sahlgren, 2006; Turney and Pantel, 2010). This approach to semantics (sometimes called distributional semantics) scales well to large lexicons and does not require words to be manually disambiguated (Sch¨utze, 1997). Until recently, however, this method had been almost exclusively limited to the level of single content words (nouns, adjectives, verbs), and had not directly addressed the problem of compositionality (Frege, 1892; Montague, 1970; Partee, 2004), the crucial property of natural language which allows speakers to derive the meaning of a complex linguistic constituent from the meaning of its immediate syntactic subconstituents. Several recent proposals have strived to extend distributional semantics with a component that also generates vectors for complex linguistic constituents, using compositional operations in the vector space (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012). All of these approaches construct distributional representations for novel phrases</context>
</contexts>
<marker>Partee, 2004</marker>
<rawString>Barbara Partee. 2004. Compositionality. In Compositionality in Formal Semantics: Selected Papers by Barbara H. Partee. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model. Dissertation,</title>
<date>2006</date>
<location>Stockholm University.</location>
<contexts>
<context position="1302" citStr="Sahlgren, 2006" startWordPosition="187" endWordPosition="188">y indicate a word order restriction. We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics. 1 Introduction A prominent approach for representing the meaning of a word in Natural Language Processing (NLP) is to treat it as a numerical vector that codes the pattern of co-occurrence of that word with other expressions in a large corpus of language (Sahlgren, 2006; Turney and Pantel, 2010). This approach to semantics (sometimes called distributional semantics) scales well to large lexicons and does not require words to be manually disambiguated (Sch¨utze, 1997). Until recently, however, this method had been almost exclusively limited to the level of single content words (nouns, adjectives, verbs), and had not directly addressed the problem of compositionality (Frege, 1892; Montague, 1970; Partee, 2004), the crucial property of natural language which allows speakers to derive the meaning of a complex linguistic constituent from the meaning of its immedi</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model. Dissertation, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Improvements in part-of-speech tagging with an application to German.</title>
<date>1995</date>
<booktitle>In Proceedings of the EACL-SIGDAT Workshop,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="7108" citStr="Schmid, 1995" startWordPosition="1074" endWordPosition="1075"> our focus is on the effect of different composition methods given a common semantic space. In addition, Blacoe and Lapata (2012) found that a vanilla space of this sort performed best in their composition experiments, when compared to a syntax-aware space and to neural language model vectors such as those used for composition by Socher et al. (2011). Source corpus We use as our source corpus the concatenation of the Web-derived ukWaC corpus, a mid-2009 dump of the English Wikipedia and the British National Corpus1. The corpus has been tokenized, POS-tagged and lemmatized with the TreeTagger (Schmid, 1995), and it contains about 2.8 billion tokens. We extract all statistics at the lemma level, meaning that we consider only the canonical form of each word ignoring inflectional information, such as pluralization and verb inflection. Semantic space vocabulary The words/phrases in the semantic space must of course include the items that we need for our experiments (adjectives, nouns, ANs and AANs used for model training, as input to composition and for evaluation). Therefore, we first populate our semantic space with a core vocabulary containing the 8K most frequent nouns and the 4K most frequent a</context>
</contexts>
<marker>Schmid, 1995</marker>
<rawString>Helmut Schmid. 1995. Improvements in part-of-speech tagging with an application to German. In Proceedings of the EACL-SIGDAT Workshop, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Ambiguity Resolution in Natural Language Learning.</title>
<date>1997</date>
<location>CSLI, Stanford, CA.</location>
<marker>Sch¨utze, 1997</marker>
<rawString>Hinrich Sch¨utze. 1997. Ambiguity Resolution in Natural Language Learning. CSLI, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary-John Scott</author>
</authors>
<title>Stacked adjectival modification and the structure of nominal phrases.</title>
<date>2002</date>
<booktitle>Functional Structure in DP and IP. The Carthography of Syntactic Structures,</booktitle>
<volume>1</volume>
<editor>In Guglielmo Cinque, editor,</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="3385" citStr="Scott, 2002" startWordPosition="501" endWordPosition="502">rsive: they can derive a representation not only for, e.g., red car, but also for new red car, fast new red car, etc. This aspect is appealing since trying to extract meaningful representations for all recursive phrases directly from a corpus will result in a problem of sparsity, since most large phrases will never occur in any finite sample. Once we start seriously looking into recursive modification, however, the issue of modifier ordering restrictions naturally arises. Such restrictions have often been discussed in the theoretical linguistic literature (Sproat and Shih, 1990; Crisma, 1991; Scott, 2002), and have become one of the key in141 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 141–151, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics gredients of the ‘cartographic’ approach to syntax (Cinque, 2002). In this paradigm, the ordering is derived by assigning semantically different classes of modifiers to the specifiers of distinct functional projections, whose sequence is hard-wired. While it is accepted that in different languages movement can lead to a principled rearrangement of the linear or</context>
</contexts>
<marker>Scott, 2002</marker>
<rawString>Gary-John Scott. 2002. Stacked adjectival modification and the structure of nominal phrases. In Guglielmo Cinque, editor, Functional Structure in DP and IP. The Carthography of Syntactic Structures, volume 1. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>E H Huang</author>
<author>J Pennington</author>
<author>Andrew Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>24--801</pages>
<contexts>
<context position="6847" citStr="Socher et al. (2011)" startWordPosition="1030" endWordPosition="1033">words and phrases that we represent in the space, and finally the procedure adopted to build the vectors representing the vocabulary items from corpus statistics, and obtain the semantic space matrix. We work here with a traditional, window-based semantic space, since our focus is on the effect of different composition methods given a common semantic space. In addition, Blacoe and Lapata (2012) found that a vanilla space of this sort performed best in their composition experiments, when compared to a syntax-aware space and to neural language model vectors such as those used for composition by Socher et al. (2011). Source corpus We use as our source corpus the concatenation of the Web-derived ukWaC corpus, a mid-2009 dump of the English Wikipedia and the British National Corpus1. The corpus has been tokenized, POS-tagged and lemmatized with the TreeTagger (Schmid, 1995), and it contains about 2.8 billion tokens. We extract all statistics at the lemma level, meaning that we consider only the canonical form of each word ignoring inflectional information, such as pluralization and verb inflection. Semantic space vocabulary The words/phrases in the semantic space must of course include the items that we ne</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, E.H. Huang, J. Pennington, Andrew Y. Ng, and C.D. Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. Advances in Neural Information Processing Systems, 24:801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1201--1211</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="2265" citStr="Socher et al., 2012" startWordPosition="328" endWordPosition="331">d had not directly addressed the problem of compositionality (Frege, 1892; Montague, 1970; Partee, 2004), the crucial property of natural language which allows speakers to derive the meaning of a complex linguistic constituent from the meaning of its immediate syntactic subconstituents. Several recent proposals have strived to extend distributional semantics with a component that also generates vectors for complex linguistic constituents, using compositional operations in the vector space (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012). All of these approaches construct distributional representations for novel phrases starting from the corpusderived vectors for their lexical constituents and exploiting the geometric quality of the representation. Such methods are able to capture complex semantic information of adjective-noun (AN) phrases, such as characterizing modification (Boleda et al., 2012; Boleda et al., 2013), and can detect semantic deviance in novel phrases (Vecchi et al., 2011). Furthermore, these methods are naturally recursive: they can derive a representation not only for, e.g., red car, but also for new red ca</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of EMNLP, pages 1201–1211, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chilin Shih</author>
</authors>
<title>The crosslinguistics distribution of adjective ordering restrictions.</title>
<date>1990</date>
<pages>565--593</pages>
<editor>In C. Georgopoulos and Ishihara R., editors,</editor>
<location>Kluver, Dordrecht.</location>
<contexts>
<context position="3357" citStr="Sproat and Shih, 1990" startWordPosition="495" endWordPosition="498">ore, these methods are naturally recursive: they can derive a representation not only for, e.g., red car, but also for new red car, fast new red car, etc. This aspect is appealing since trying to extract meaningful representations for all recursive phrases directly from a corpus will result in a problem of sparsity, since most large phrases will never occur in any finite sample. Once we start seriously looking into recursive modification, however, the issue of modifier ordering restrictions naturally arises. Such restrictions have often been discussed in the theoretical linguistic literature (Sproat and Shih, 1990; Crisma, 1991; Scott, 2002), and have become one of the key in141 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 141–151, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics gredients of the ‘cartographic’ approach to syntax (Cinque, 2002). In this paradigm, the ordering is derived by assigning semantically different classes of modifiers to the specifiers of distinct functional projections, whose sequence is hard-wired. While it is accepted that in different languages movement can lead to a principled re</context>
</contexts>
<marker>Sproat, Shih, 1990</marker>
<rawString>Richard Sproat and Chilin Shih. 1990. The crosslinguistics distribution of adjective ordering restrictions. In C. Georgopoulos and Ishihara R., editors, Interdisciplinary approaches to language: essays in honor of Yuki Kuroda, pages 565–593. Kluver, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sam Steddy</author>
<author>Vieri Samek-Lodovici</author>
</authors>
<title>On the ungrammaticality of remnant movement in the derivation of greenberg’s universal 20. Linguistic Inquiry,</title>
<date>2011</date>
<volume>42</volume>
<issue>3</issue>
<contexts>
<context position="4053" citStr="Steddy and Samek-Lodovici, 2011" startWordPosition="595" endWordPosition="598">41 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 141–151, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics gredients of the ‘cartographic’ approach to syntax (Cinque, 2002). In this paradigm, the ordering is derived by assigning semantically different classes of modifiers to the specifiers of distinct functional projections, whose sequence is hard-wired. While it is accepted that in different languages movement can lead to a principled rearrangement of the linear order of the modifiers (Cinque, 2010; Steddy and Samek-Lodovici, 2011), one key assumption of the cartographic literature is that exactly one intonationally unmarked order for stacked adjectives should be possible in languages like English. The possibility of alternative orders, when discussed at all, is attributed to the presence of idioms (high American building, but American high officer), to asyndetic conjunctive meanings (e.g. new creative idea parsed as [new &amp; creative] idea, rather than [new [creative idea]]), or to semantic category ambiguity for any adjective which appears in different orders (see Cinque (2004) for discussion). In this study, we show th</context>
</contexts>
<marker>Steddy, Samek-Lodovici, 2011</marker>
<rawString>Sam Steddy and Vieri Samek-Lodovici. 2011. On the ungrammaticality of remnant movement in the derivation of greenberg’s universal 20. Linguistic Inquiry, 42(3):445–469.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richmond H Thomason</author>
<author>editor</author>
</authors>
<title>Formal Philosophy: Selected Papers of Richard Montague.</title>
<date>1974</date>
<publisher>Yale University Press,</publisher>
<location>New York.</location>
<marker>Thomason, editor, 1974</marker>
<rawString>Richmond H. Thomason, editor. 1974. Formal Philosophy: Selected Papers of Richard Montague. Yale University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1328" citStr="Turney and Pantel, 2010" startWordPosition="189" endWordPosition="192">d order restriction. We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics. 1 Introduction A prominent approach for representing the meaning of a word in Natural Language Processing (NLP) is to treat it as a numerical vector that codes the pattern of co-occurrence of that word with other expressions in a large corpus of language (Sahlgren, 2006; Turney and Pantel, 2010). This approach to semantics (sometimes called distributional semantics) scales well to large lexicons and does not require words to be manually disambiguated (Sch¨utze, 1997). Until recently, however, this method had been almost exclusively limited to the level of single content words (nouns, adjectives, verbs), and had not directly addressed the problem of compositionality (Frege, 1892; Montague, 1970; Partee, 2004), the crucial property of natural language which allows speakers to derive the meaning of a complex linguistic constituent from the meaning of its immediate syntactic subconstitue</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Domain and function: A dual-space model of semantic relations and compositions.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>44--533</pages>
<contexts>
<context position="34110" citStr="Turney, 2012" startWordPosition="5508" endWordPosition="5509"> ordering. This measure confirms that the association of one adjective (the Ay in attested-order AANs) with the head noun is indeed the most significant factor distinguishing these two classes. However, as we mentioned before, this measure has its limitations and is likely not to be entirely sufficient for future steps in modeling recursive modification. 4 Conclusion While AN constructions have been extensively studied within the framework of compositional distributional semantics (Baroni and Zamparelli, 2010; Boleda et al., 2012; Boleda et al., 2013; Guevara, 2010; Mitchell and Lapata, 2010; Turney, 2012; Vecchi et al., 2011), for the first time, we extended the investigation to recursively built AAN phrases. First, we showed that composition functions applied recursively can approximate corpus-extracted AAN vectors that we know to be of high semantic quality. Next, we looked at some properties of the same high-quality corpus-extracted AAN vectors, finding that the distinction between “flexible” AANs, where the adjective order can be flipped, and “rigid” ones, where the order is fixed, is reflected in distributional cues. These results all derive from the intuition that the most embedded adje</context>
</contexts>
<marker>Turney, 2012</marker>
<rawString>Peter Turney. 2012. Domain and function: A dual-space model of semantic relations and compositions. Journal of Artificial Intelligence Research, 44:533–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Maria Vecchi</author>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>(Linear) maps of the impossible: Capturing semantic anomalies in distributional space.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL Workshop on Distributional Semantics and Compositionality,</booktitle>
<pages>1--9</pages>
<location>Portland, OR.</location>
<contexts>
<context position="2726" citStr="Vecchi et al., 2011" startWordPosition="397" endWordPosition="400">l operations in the vector space (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012). All of these approaches construct distributional representations for novel phrases starting from the corpusderived vectors for their lexical constituents and exploiting the geometric quality of the representation. Such methods are able to capture complex semantic information of adjective-noun (AN) phrases, such as characterizing modification (Boleda et al., 2012; Boleda et al., 2013), and can detect semantic deviance in novel phrases (Vecchi et al., 2011). Furthermore, these methods are naturally recursive: they can derive a representation not only for, e.g., red car, but also for new red car, fast new red car, etc. This aspect is appealing since trying to extract meaningful representations for all recursive phrases directly from a corpus will result in a problem of sparsity, since most large phrases will never occur in any finite sample. Once we start seriously looking into recursive modification, however, the issue of modifier ordering restrictions naturally arises. Such restrictions have often been discussed in the theoretical linguistic li</context>
<context position="34132" citStr="Vecchi et al., 2011" startWordPosition="5510" endWordPosition="5513">s measure confirms that the association of one adjective (the Ay in attested-order AANs) with the head noun is indeed the most significant factor distinguishing these two classes. However, as we mentioned before, this measure has its limitations and is likely not to be entirely sufficient for future steps in modeling recursive modification. 4 Conclusion While AN constructions have been extensively studied within the framework of compositional distributional semantics (Baroni and Zamparelli, 2010; Boleda et al., 2012; Boleda et al., 2013; Guevara, 2010; Mitchell and Lapata, 2010; Turney, 2012; Vecchi et al., 2011), for the first time, we extended the investigation to recursively built AAN phrases. First, we showed that composition functions applied recursively can approximate corpus-extracted AAN vectors that we know to be of high semantic quality. Next, we looked at some properties of the same high-quality corpus-extracted AAN vectors, finding that the distinction between “flexible” AANs, where the adjective order can be flipped, and “rigid” ones, where the order is fixed, is reflected in distributional cues. These results all derive from the intuition that the most embedded adjective in a rigid AAN h</context>
</contexts>
<marker>Vecchi, Baroni, Zamparelli, 2011</marker>
<rawString>Eva Maria Vecchi, Marco Baroni, and Roberto Zamparelli. 2011. (Linear) maps of the impossible: Capturing semantic anomalies in distributional space. In Proceedings of the ACL Workshop on Distributional Semantics and Compositionality, pages 1–9, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Zanzotto</author>
<author>Ioannis Korkontzelos</author>
<author>Francesca Falucchi</author>
<author>Suresh Manandhar</author>
</authors>
<title>Estimating linear models for compositional distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1263--1271</pages>
<location>Beijing, China.</location>
<contexts>
<context position="13369" citStr="Zanzotto et al. (2010)" startWordPosition="2118" endWordPosition="2121">the recursive setting of this experiment, as seen in Equation (1). p~= ~a. O~ay O n~ (1) This composition method is order-insensitive, the formula above corresponding to the representation of both A.AyN and AyA.N. In the weighted additive model (W.ADD), we obtain the composed vector as a weighted sum of the two component vectors: p~ = α~u + β~v, where α and β are scalars. Again, we can easily apply this function recursively, as in Equation (2). p~= α ~a. + β(α~ay + β~n) = α~a. + αβ~ay + β2~n (2) We also consider the full extension of the additive model (F.ADD), presented in Guevara (2010) and Zanzotto et al. (2010), such that the component vectors are pre-multiplied by weight matrices before being added: p~ = W1~u + W2~v. Similarly to the W.ADD model, Equation (3) describes how we apply this function recursively. p~ = W1~a. + W2(W1~ay + W2~n) (3) = W1~a. + W2W1~ay + W22~n Finally, we consider the lexical function model (LFM), first introduced in Baroni and Zamparelli (2010), in which attributive adjectives are treated as functions from noun meanings to noun meanings. This is a standard approach in Montague semantics (Thomason, 1974), except noun meanings here are distributional vectors, not denotations,</context>
</contexts>
<marker>Zanzotto, Korkontzelos, Falucchi, Manandhar, 2010</marker>
<rawString>Fabio Zanzotto, Ioannis Korkontzelos, Francesca Falucchi, and Suresh Manandhar. 2010. Estimating linear models for compositional distributional semantics. In Proceedings of COLING, pages 1263–1271, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
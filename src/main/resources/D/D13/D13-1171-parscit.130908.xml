<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.975221">
Open Domain Targeted Sentiment
</title>
<author confidence="0.988815">
Margaret Mitchell Jacqueline Aguilar Theresa Wilson Benjamin Van Durme
</author>
<affiliation confidence="0.9406085">
Human Language Technology Center of Excellence
Johns Hopkins University
</affiliation>
<address confidence="0.823361">
Baltimore, MD 21218, USA
</address>
<email confidence="0.997775">
{m.mitchell,jacqui.aguilar}@jhu.edu, Theresa.Wilson@oberlin.edu, vandurme@cs.jhu.edu
</email>
<sectionHeader confidence="0.99739" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99503847826087">
We propose a novel approach to sentiment
analysis for a low resource setting. The in-
tuition behind this work is that sentiment
expressed towards an entity, targeted senti-
ment, may be viewed as a span of sentiment
expressed across the entity. This represen-
tation allows us to model sentiment detec-
tion as a sequence tagging problem, jointly
discovering people and organizations along
with whether there is sentiment directed to-
wards them. We compare performance in
both Spanish and English on microblog data,
using only a sentiment lexicon as an exter-
nal resource. By leveraging linguistically-
informed features within conditional random
fields (CRFs) trained to minimize empiri-
cal risk, our best models in Spanish signifi-
cantly outperform a strong baseline, and reach
around 90% accuracy on the combined task of
named entity recognition and sentiment pre-
diction. Our models in English, trained on a
much smaller dataset, are not yet statistically
significant against their baselines.
</bodyText>
<sectionHeader confidence="0.999464" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999076714285714">
Sentiment analysis is a multi-faceted problem. De-
termining when a positive or negative sentiment is
being expressed is a large part of the challenge, but
identifying other attributes, such as the target of the
sentiment, is also crucial if the ultimate goal is to
pinpoint and extract opinions. Consider the exam-
ples below, all of which contain a positive sentiment:
</bodyText>
<listItem confidence="0.9971155">
(1) So happy that Kentucky lost to Tennessee!
(2) Kentucky versus Kansas I can hardly wait...
(3) Kentucky is the best alley-oop throwing team
since Sherman Douglas’ Syracuse squads!!
</listItem>
<bodyText confidence="0.999943375">
The entities in these examples are college basket-
ball teams, and the events referred to are games. In
(1), although there is a positive sentiment, the tar-
get of the sentiment is an event (Kentucky losing to
Tennessee). However, from the positive sentiment
toward this event, we can infer that the speaker has
a negative sentiment toward Kentucky and a positive
sentiment toward Tennessee. In (2), the positive sen-
timent is toward a future event, but we are not given
enough information to infer a sentiment toward the
mentioned entities. In (3), Kentucky is the direct
target of the positive sentiment. We can also in-
fer a positive sentiment toward Douglas’s Syracuse
teams, and even toward Douglas himself.
These examples illustrate the importance of the
target when interpreting sentiment in context. If we
are looking for sentiments toward Kentucky, for ex-
ample, we would want to identify (1) as negative, (2)
as neutral (no sentiment) and (3) as positive. How-
ever, if we are looking for sentiment toward Ten-
nessee, we would want to identify (1) as positive,
and (2) and (3) as neutral.
The expression of these and other kinds of sen-
timent can be understood as involving three items:
</bodyText>
<listItem confidence="0.999534666666667">
(1) An experiencer
(2) An attitude
(3) A target (optionally)
</listItem>
<bodyText confidence="0.999855875">
Research in sentiment analysis often focuses on (2),
predicting overall sentiment polarity (Agarwal et al.,
2011; Bora, 2012). Recent work has begun to com-
bine (2) with (3), examining how to automatically
predict the sentiment polarity expressed towards a
target entity (Jiang et al., 2011; Chen et al., 2012)
for a fixed set of targets. This topic-dependent sen-
timent classification requires that the target entity be
</bodyText>
<page confidence="0.862713">
1643
</page>
<note confidence="0.879303">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1643–1654,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999305">
Figure 1: Sentiment expressed across an entity.
</figureCaption>
<bodyText confidence="0.999557974358974">
given, and returns statements expressing sentiment
towards the given entity.
In this paper, we take a step towards open-domain,
targeted sentiment analysis by investigating how to
detect both the named entity and the sentiment ex-
pressed toward it. We observe that sentiment ex-
pressed towards a target entity may be possible to
learn in a graphical model along the span of the en-
tity itself: Similar to how named entity recognition
(NER) learns labels along the span of each word in
an entity name, sentiment may be expressed along
the entity as well. A small example is shown in Fig-
ure 1. We focus on people and organizations (voli-
tional named entities), which are the primary targets
of sentiment in our microblog data (see Table 1).
Both NER and opinion expression extraction have
achieved impressive results using conditional ran-
dom fields (CRFs) (Lafferty et al., 2001) to define
the conditional probability of entity categories (Mc-
Callum and Li, 2003; Choi et al., 2006; Yang and
Cardie, 2013). We develop such models to jointly
predict the NE and the sentiment expressed towards
it using minimum risk training (Stoyanov and Eis-
ner, 2012). We learn our models on informal Span-
ish and English language taken from the social net-
work Twitter,1 where the language variety makes
NLP particularly challenging (see Figure 2).
Our ultimate goal is to develop models that will
be useful for low resource languages, where a sen-
timent lexicon may be known or bootstrapped, but
more sophisticated linguistic tools may not be read-
ily available. We therefore do not rely on an external
part-of-speech tagger or parser, which are often used
for features in fine-grained sentiment analysis; such
tools are not available in many languages, and if they
are, are not usually adapted for noisy social media.
Instead, we use information from sentiment lex-
icons and some simple hand-written features, and
otherwise use only features of the word that can be
</bodyText>
<footnote confidence="0.969224">
1www.twitter.com
</footnote>
<bodyText confidence="0.770053">
@[user] le dijo erralo muy por lo bajo jaja un grande
juancito grandes amigos mios
@[user] he told him it was very on the dl haha a great
juancito great friends of mine
@[user] buenos dias Profe!! Nos quedamos acciden-
tados otra vez en la carretera vieja guarenas echando
gasoil, estamos a la interperie
@[user] good morning, Prof!! We were wrecked again
on the old guarenas highway while getting diesel, we’re
out in the open
Sin ´animo de ofender a los Militares, que realmente
se merecen ese aumento y m´as. Pero, d´onde queda la
misma recompensa para M´edicos.
I do not intend to offend the military in the slightest,
they truly deserve the raise and more. However, I’m
wondering whether doctors will ever receive a similar
compensation.
</bodyText>
<figureCaption confidence="0.82808">
Figure 2: Messages on Twitter use a wide range of
</figureCaption>
<bodyText confidence="0.871801">
formality, style, and errors, which makes extracting in-
formation particularly difficult. Examples from Spanish
(screen names anonymized), with approximate transla-
tions in English.
extracted without supervision. These include fea-
tures based on unsupervised word tags (Brown clus-
ters) and a method that automatically syllabifies a
word based on the orthography of the language. All
tools and code used for this research are released
with this paper.2
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999802">
As the scale of social media has grown, using
sources such as Twitter to mine public sentiment
has become increasingly promising. Commer-
cial systems include Sentiment1403 (products and
brands) and tweetfeel4 (suggests searching for pop-
ular movies, celebrities and companies).
The majority of academic research has focused on
supervised classification of message sentiment irre-
spective of target (Barbosa and Feng, 2010; Pak and
Paroubek, 2010; Bifet and Frank, 2010; Davidov et
al., 2010; Kouloumpis et al., 2011; Agarwal et al.,
2011). Large datasets are collected for this work by
leveraging the sentiment inherent in emoticons (e.g.,
smilies and frownies) and/or select Twitter hashtags
(e.g., #bestdayever, #fail), resulting in noisy collec-
</bodyText>
<footnote confidence="0.999979333333333">
2www.m-mitchell.com/code
3www.sentiment140.com
4www.tweetfeel.com
</footnote>
<page confidence="0.996252">
1644
</page>
<bodyText confidence="0.99991998630137">
tions appropriate for initial exploration. Prior work
includes: the use of a social network (Speriosu et
al., 2011; Tan et al., 2011; Calais Guerra et al.,
2011; Jiang et al., 2011; Li et al., 2012; Hu et
al., 2013); user-adapted models based on collabo-
rative online-learning (Li et al., 2010b); unsuper-
vised, joint sentiment-topic modeling (Saif et al.,
2012); tracking changing sentiment during debates
(Diakopoulos and Shamma, 2010); and how ortho-
graphic conventions such as word-lengthening can
be used to adapt a Twitter-specific sentiment lexicon
(Brody and Diakopoulos, 2011).
Efforts in targeted sentiment (Bermingham and
Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a;
Jiang et al., 2011; Tan et al., 2011; Wang et al.,
2011; Li et al., 2012; Chen et al., 2012), have mostly
focused on topic-dependent analysis. In these ap-
proaches, messages are collected on a fixed set of
topics/targets, such as products or sports teams, and
sentiment is learned for the given set. In contrast,
we aim to predict sentiment in tweets for any named
person or organization. We refer to this task as open
domain targeted sentiment analysis.
Within topic-dependent sentiment analysis, sev-
eral approaches have explored applying CRFs or
HMMs to extract sentiment and target words from
text (Jin and Ho, 2009; Li et al., 2010a). In these
approaches, opinion expressions are extracted, and
polarity is annotated across the opinion expression.
However, as noted by many researchers in senti-
ment, opinion orientation towards a specific target
is often not equal to the orientation of a neighbor-
ing opinion expression; and opinion expressions in
one context may not be opinion expressions in an-
other (Kim and Hovy, 2006), making open domain
approaches particularly challenging.
The above work by Jiang et al. (2011) is most
similar to our own. They do not use joint learning,
but they do incorporate a number of parse-based fea-
tures designed to capture relationships between sen-
timent terms and topic references. In our work these
relationships are captured by the CRF model, and
we compare against their approach in Section 6.
Recent work by Yang and Cardie (2013) is sim-
ilar in spirit to our own, where the identification
of opinion holders, opinion targets, and opinion ex-
pressions is modeled as a sequence tagging problem
using a CRF. However, similar to previous work ap-
plying CRFs to extract sentiment, Yang and Cardie
use syntactic relations to connect an opinion target
to an opinion expression. In contrast, we model
the expression of sentiment polarity across the senti-
ment target itself, extracting both the sentiment tar-
get and the sentiment expressed towards it within the
same span of words. This allows us to use surround-
ing context to determine sentiment polarity without
identifying explicit opinion expressions or relying
on a parser to help link expression to target.
Most work in targeted sentiment outside the mi-
croblogging domain has been in relation to prod-
uct review mining (e.g., Yi et al. (2003), Hu and
Liu (2004), Popescu and Etzioni (2005), Qiu et al.
(2011)). Rather than identify named entities (NEs),
this work seeks to identify products and their fea-
tures mentioned in reviews, and classify these for
sentiment. Recent work by Qui et al. jointly learns
targets and opinion words, and Jakob and Gurevych
(2010) use CRFs to extract the targets of opinions,
but do not attempt to classify the sentiment toward
these targets. To the best of our knowledge, this is
the first work to approach targeted sentiment in a low
resource setting and to jointly predict NEs and tar-
geted sentiment.
</bodyText>
<sectionHeader confidence="0.997185" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.997383166666667">
Twitter Collection We use the Spanish/English
Twitter dataset of Etter et al. (2013) to train and test
our models. Approximately 30,000 Spanish tweets
and 10,000 English were labeled for named entities
in BIO encoding: The start of an NE is labeled B-
{NE} and the rest of the NE is labeled I-{NE}. The
</bodyText>
<table confidence="0.974123090909091">
NE COUNT NEUTRAL POS NEG
PERSON 5462 80% 20% 0%
ORGANIZATION 4408 80% 20% 0%
LOCATION 1405 100% 0% 0%
URL 1030 100% 0% 0%
TIME 535 70% 10% 20%
DATE 222 100% 0% 0%
MONEY 95 90% 0% 10%
PERCENT 81 80% 20% 0%
TELEPHONE 23 100% 0% 0%
EMAIL 8 100% 0% 0%
</table>
<tableCaption confidence="0.988371833333333">
Table 1: Distribution of named entities in our Spanish
Twitter corpus. Targeted sentiment percentages are based
on expert annotations from a random sample of 10 (or
all) of of each entity. Most entities are not sentiment tar-
gets (NEUTRAL). PERSON and ORGANIZATION are most
frequent, and among the top recipients of sentiment.
</tableCaption>
<page confidence="0.993847">
1645
</page>
<bodyText confidence="0.999828304347826">
full set of NE categories are shown in Table 1. For
example, the sequence “Mark Twain” would be la-
beled B-PERSON, I-PERSON. We are interested in both
PERSON and ORGANIZATION entities, which make
up the majority of named entities in this data, and we
evaluate these using the more general entity category
VOLITIONAL. Removing retweets, 7,105 Spanish
tweets contained a total of 9,870 volitional entities
and 2,350 English tweets contained a total of 3,577
volitional entities.
Sentiment Lexicons We use two sentiment lex-
icon sources in each language. For English, we
use the MPQA lexicon (Wilson et al., 2005), which
identifies 12,296 manually and semi-automatically
produced subjective terms along with their polarity.
For the second lexicon, we use SentiWordNet 3.0
(Baccianella et al., 2010), which assigns positive and
negative polarity scores to WordNet synsets. We use
the majority polarity of all words with a subjectivity
score above 0.5.
For Spanish, the first lexicon is obtained from
Volkova et al. (2013), who automatically trans-
lated strongly subjective terms from the MPQA lex-
icon (Wilson et al., 2005) into Spanish. The re-
sulting Spanish lexicon contains about 65K words.
The second lexicon is available from Perez-Rosas
et al. (2012). This contains approximately 1000
sentiment-bearing words collected leveraging man-
ual resources and 2000 collected leveraging auto-
matic resources.
Annotation To collect sentiment labels, we
use crowdsourcing through Amazon’s Mechanical
Turk.5 Annotators (“Turkers”) were shown six
tweets at a time, each with a single highlighted
named entity. Turkers were instructed to (1) se-
lect the sentiment being expressed towards the en-
tity (positive, negative, or no sentiment); and (2)
rate their level of confidence in their selection. Fol-
lowing best practices on collecting language data
with Mechanical Turk (Callison-Burch and Dredze,
2010), two controls were placed among each set of
six tweets to screen out unreliable judgments. An
example prompt is shown in Figure 3.
Each (tweet, NE) pair was shown to three Turk-
ers, and those with majority consensus on sentiment
polarity were extracted. Tweets without sentiment
</bodyText>
<footnote confidence="0.658309">
5www.mturk.com/mturk
</footnote>
<figure confidence="0.9855296">
Positive
Negative
Neutral
ORGANIZATION PERSON
Named Entity
</figure>
<figureCaption confidence="0.999605">
Figure 4: Targeted sentiment annotated for Spanish.
</figureCaption>
<table confidence="0.9350214">
Majority
POS NEUTRAL NEG
POS 757 1249 130
NEUTRAL 707 2151 473
NEG 129 726 452
</table>
<tableCaption confidence="0.994733">
Table 2: Number of targeted sentiment instances where
</tableCaption>
<bodyText confidence="0.969946517241379">
at least two of the three annotators (Majority) agreed.
Common disagreements with a third annotator (Minority)
were over whether no sentiment or positive sentiment was
expressed, and whether no sentiment or negative sent-
ment was expressed.
consensus on all NEs were removed. In Spanish, this
yielded 6,658 unique (tweet, NE) pairs. In English,
which is a smaller data set, this yielded 3,288 unique
pairs. We split the data into folds for 10-fold cross-
validation, developing on the data from one fold and
reporting results for the remaining nine.
The distribution of sentiment for the named en-
tities annotated by Turkers is shown in Figure 4.
Neutral (no targeted sentiment) dominates, followed
by positive sentiment for both organizations and
people. As shown in Table 2, common disagree-
ments were over whether or not there was targeted
positive sentiment, and whether or not there was
targeted negative sentiment. This is in line with
previous research showing that distinguishing pos-
itive sentiment from no sentiment (and distinguish-
ing negative sentiment from no sentiment) is often
more challenging than distinguishing between pos-
itive and negative sentiment (Wilson et al., 2009).
Indeed, we see that it was more common for annota-
tors to disagree than to agree on targeted sentiment,
particularly for negative targeted sentiment, where
more instances had NEUTRAL/NEGATIVE disagree-
ment than NEGATIVE three-way agreement.
</bodyText>
<figure confidence="0.916683333333333">
Frequency in Tweets
0 500 1000 1500 2000 2500
Minority
</figure>
<page confidence="0.571812">
1646
</page>
<figureCaption confidence="0.996447">
Figure 3: Example Tweet shown to Turkers.
</figureCaption>
<figure confidence="0.825833285714286">
Variable Possible values
Sentiment (s) NOT-TARG, SENT-TARG
(PIPE &amp; JOINT models)
Named Entity (l) O, B-VOLITIONAL, I-VOLITIONAL
(PIPE &amp; JOINT models)
Combined Sent/NE (y) O, B+NOT-TARG, I+NOT-TARG
(COLL models) B+SENT-TARG, I+SENT-TARG
</figure>
<tableCaption confidence="0.987882666666667">
Table 3: Possible values for random variables, targeted
subjectivity (is/is not sentiment target). COLL models
collapse targeted subjectivity and NE label into one node.
</tableCaption>
<figure confidence="0.788844625">
Variable Possible values
Sentiment (s) NOT-TARG, POS, NEG
(PIPE &amp; JOINT models)
Named Entity (l) O, B-VOLITIONAL, I-VOLITIONAL
(PIPE &amp; JOINT models)
Combined Sent/NE (y) O, B+NOT-TARG, I+NOT-TARG
(COLL models) B+POS, I+POS
B+NEG, I+NEG
</figure>
<tableCaption confidence="0.951472666666667">
Table 4: Possible values for random variables, targeted
sentiment. The COLL models collapse both targeted sen-
timent and NE label into one node.
</tableCaption>
<sectionHeader confidence="0.954228" genericHeader="method">
4 Targeted Subjectivity and Sentiment
</sectionHeader>
<bodyText confidence="0.989139666666667">
Formally, we define the problem as follows: Given
an observed message w = (w1 ... wn), where n is
the number of words in the message and wj(1 &lt;
j &lt; n) is a word, we learn the probability of a
label sequence l = (l1 ... ln), where li E the set
of named entity values; and a sentiment sequence
s = (s1 ... sn), where si E the set of sentiment val-
ues. We additionally explore simpler linear-chain
models that learn the probability of a single label
sequence y = (y1 ... yn), where yi E the set of con-
joined entity+sentiment values (Tables 3 and 4).
Our basic model is a linear conditional random
field, an undirected graph that represents the con-
ditional distribution p(l, s|w).6 Sentiment towards
a named entity may be modeled in a CRF as a se-
6For the COLL models, this is instead the conditional distri-
butionp(y1w), where entity and sentiment labels are conjoined
in one sequence assignment y.
quence of random variables for sentiment s con-
nected to named entities l. In all models, entity vari-
ables are connected by a factor to their neighbors
in sequence, and we include skip-chains (Finkel and
Manning, 2010) connecting identical words where
at least one is capitalized. Our model strategies in-
clude: a pipeline that first learns volitional entities
then sentiment directed towards them (PIPE); one
that jointly learns volitional entities along with sen-
timent directed towards them (JOINT); and one that
learns volitional entities and targeted sentiment with
combined labels (COLL) (Figure 5).
Using these models, we explore two primary
tasks: (1) the task of detecting whether sentiment
is targeted at an entity, which we refer to as targeted
subjectivity; and (2) the task of detecting whether
positive, negative, or neutral sentiment (no senti-
ment) is targeted at an entity, which we refer to as
targeted sentiment. Moving from targeted subjectiv-
ity prediction to targeted sentiment prediction is pos-
sible by changing the sentiment target (SENT-TARG)
variable into two variables, one for positive targeted
sentiment (POS) and one for negative (NEG). Possi-
ble values for targeted subjectivity are shown in Ta-
ble 3, and possible values for targeted sentiment are
shown in Table 4.
In the pipeline models (PIPE), we first build a
CRF where each word is connected by a factor to
an entity label li E l. In a second model, every ob-
served volitional entity node is connected by a factor
to a sentiment label si E s. An example is shown in
Figure 5 (1).
In the joint models (JOINT), each si E s is con-
nected by a factor to the corresponding entity label
in the sequence, li E l. Sentiment in this model
is partially observed: All sentiment variables are
treated as latent except for the sentiment connected
to the volitional entity. An example is shown in Fig-
ure 5 (2).
</bodyText>
<page confidence="0.971276">
1647
</page>
<bodyText confidence="0.998379">
In the collapsed models (COLL), we combine sen-
timent and named entity into one label sequence
(e.g., O, B+SENT-TARG, I+SENT-TARG). An example
is shown in Figure 5 (3). The JOINT and PIPE mod-
els therefore predict named entity sequences, their
category labels, and the sentiment expressed towards
volitional named entities.7 The collapsed models
predict volitional labels and targeted sentiment as
combined categories. The COLL and PIPE models
are considerably faster than JOINT models, where
exact inference is intractable.
</bodyText>
<figure confidence="0.6454622">
1. PIPELINE MODEL (PIPE)
Step 1: Volitional Named Step 2: Sentiment
Entity Recognition
2. JOINT MODEL 3. COLLAPSED MODEL
(JOINT) (COLL)
</figure>
<figureCaption confidence="0.994437333333333">
Figure 5: Example CRFs for targeted subjectivity with
observed variables (dark nodes), predicted variables
(white nodes) and hidden variables (light grey nodes).
</figureCaption>
<sectionHeader confidence="0.989627" genericHeader="method">
5 Training
</sectionHeader>
<bodyText confidence="0.954816593220339">
Minimum-Risk CRF Training We use the
ERMA system (Stoyanov et al., 2011) to learn our
models.8 ERMA (Empirical Risk Minimization un-
der Approximations) learns parameters to minimize
loss on the training data. Predicting NE labels using
a linear-chain CRF trained with empirical risk mini-
mization has been shown to result in a statistically
significant improvement over the common approach
of maximum likelihood estimation (Stoyanov and
Eisner, 2012). All models are trained to optimize
7We found that learning the VOLITIONAL categories dur-
ing training rather than maintaining beliefs about separate
named entities during inference (ORGANIZATION, PERSON)
and then post-processing to VOLITIONAL leads to slightly bet-
ter accuracy.
8sites.google.com/site/ermasoftware
log likelihood using 20 iterations of stochastic
gradient descent, and a maximum of 100 iterations
of belief propagation to compute the marginals for
each example.
Features Features of the models are shown in Ta-
ble 5. For an observed word, features are extracted
for the word itself as well as within a context win-
dow of three words in either direction. Words seen
only once are treated as out-of-vocabulary. Surface
features and linguistic features are concatenated in
groups of two and three to create further features.
All algorithms and code that we have developed for
feature extraction are available online.9
Because we aim to develop models that do not
heavily rely on language-specific resources, we are
interested in exploring unsupervised and lightly
supervised methods for learning relevant features.
Rather than use part-of-speech tags, we therefore
use Brown cluster labels as unsupervised word tags
(Brown et al., 1992; Koo et al., 2008). Brown
clustering is a distributional similarity method that
merges pairs of word clusters in the training data10
to create the smallest decrease in corpus likelihood,
using a bigram language model on the clusters. For
our task, we cut clusters at length 3 and length 5,
and these serve as rough part-of-speech tags without
the need to train additional models. For example,
the word hello is tagged as belonging to cluster 011
(length 3) and 01111 (length 5).
During development, we found that being able
to syllabify the word (break the word into sylla-
bles) was a positive indicator of people names, but
a negative indicator of organization names. This
observation can be approximated automatically us-
ing constraints from the sonority sequencing princi-
ple (Hooper, 1976; Clements, 1990; Blevins, 1996;
Morelli, 2003) on a language’s orthography. This
is a phonotactic principle that states that syllables
will tend to have a sonority peak, usually a vowel,
in the center of the syllable, followed on either side
by consonants with decreasing sonority. Although
languages may violate this principle, the core idea
that a vowel forms the nucleus of a syllable with op-
</bodyText>
<footnote confidence="0.9961792">
9www.m-mitchell.com/code
10For Spanish, we train on a sample of ˜7 million Spanish
tweets. For English, we train on the essays (Pennebaker et al.,
2007) and Facebook data (Kosinskia et al., 2013) available from
ICWSM 2013.
</footnote>
<page confidence="0.990893">
1648
</page>
<bodyText confidence="0.999984756756757">
tional consonants before (the onset) and after (the
coda) can be used to begin to automatically learn
syllable structure.11 We learn this in an unsuper-
vised way, using the most frequent (seen more than
1,000 times) word-initial non-vowel sequences from
the Brown cluster data as allowable syllable onset
consonants. Similarly, the most frequent word-final
non-vowel sequences are learned as possible sylla-
ble codas. For each word, we then attempt to seg-
ment syllables using the learned onsets and codas
around each vowel. If a word cannot be syllabified,
it is often an initialism (e.g., CND, lsat).
We follow the approach from the out-of-
vocabulary assignment in the Berkeley parser
(Petrov et al., 2006) to encode common surface
patterns such as capitalization and lexical patterns
such as verb endings as a single feature for words
we have seen once or less. We also use the Jer-
boa toolkit (Van Durme, 2012) to extract further
language-independent features from the data, such
as features for emoticons and binning for repeated
characters (like !!!). In addition, we include features
for whether the word is three or four letters, which
is often used for acronyms and initialisms in several
languages (including Spanish and English); whether
the word is neighbored by a punctuation mark; word
identity; word length; message length; and position
in the sentence.
We utilize a speaker of each language to simply
list word forms for sentiment features that may be
indicative of sentiment, totaling less than two hours
of annotation time. This set includes intensifiers
(e.g., hella, freakin’ in English; e.g., muy, suma-
mente in Spanish), positive/negative abbreviations
(WTF, pso), positive/negative slang words, and pos-
itive/negative prefix and suffixes (e.g., anti- in En-
glish and Spanish, -ito in Spanish).
</bodyText>
<sectionHeader confidence="0.999778" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999969833333333">
We are interested in both PERSON and ORGANIZA-
TION entities, and evaluate these in the collapsed
category VOLITIONAL. This suggests that the data
may be pre-processed to label all volitional entities
as VOLITIONAL NEs, or the models may be learned
with the traditional named entities in place, and post-
</bodyText>
<footnote confidence="0.784919">
11Further development is necessary to extend a similar idea
to languages that do not ordinarily mark all vowels in their or-
thography, such as Hebrew and Arabic.
</footnote>
<note confidence="0.432649">
SURFACE FEATURES
</note>
<bodyText confidence="0.968317">
binned word length, message length, and sen-
tence position; Jerboa features; word identity; word
lengthening; punctuation characters, has digit; has
dash; is lower case; is 3 or 4 letters; first letter capi-
talized; more than one letter capitalized, etc.
</bodyText>
<note confidence="0.353279">
LINGUISTIC FEATURES
</note>
<tableCaption confidence="0.351554">
function words; can syllabify; curse words; laugh
words; words for good, bad, no, my; slang words; ab-
breviations; intensifiers; subjective suffixes and pre-
fixes (such as diminutive forms); common verb end-
ings; common noun endings
</tableCaption>
<table confidence="0.34780425">
BROWN CLUSTERING FEATURES
cluster at length 3; cluster at length 5
SENTIMENT FEATURES
is sentiment-bearing word; prior sentiment polarity
</table>
<tableCaption confidence="0.994791">
Table 5: Features used in model.
</tableCaption>
<bodyText confidence="0.99995021875">
processed to identify those that are VOLITIONAL.
We explored results using both methods, and found
that training models on VOLITIONAL tags yielded
the best performance overall; we report numbers for
this approach below.
We compare against a baseline (BASE-NS) where
we use our volitional entity labels and assign no
sentiment directed towards the entity (the majority
case). This is a strong baseline to isolate how our
methods perform specifically for the task of identi-
fying sentiment targeted at an entity.
We report on precision, recall, and sensitivity for
the tasks of NER and targeted subjectivity/sentiment
prediction in isolation; and we report on accuracy
for the targeted subjectivity and targeted sentiment
models. For sentiment, a true positive is an instance
where the label has sentiment, and a true negative is
an instance where the label has no sentiment (neu-
tral). For NER, a true positive is an instance where
the label is a B- or I- label; a true negative is an
instance where the label is O. The three systems
are evaluated against one another for NER, subjec-
tivity (entity has/does not have sentiment expressed
towards it), and sentiment (positive/negative/no sen-
timent) using paired t-tests across folds, with a Bon-
ferroni correction to set α to 0.02.
NER We include results for the isolated task of vo-
litional named entity recognition in Table 6. In both
Spanish and English, all three models are roughly
comparable for precision, recall, and specificity. The
task of finding O tags – spans that are not named en-
tities – works especially well (NE spec). Common
</bodyText>
<page confidence="0.985349">
1649
</page>
<table confidence="0.9985076">
Spanish
Model Joint Pipe Coll
NE prec 65.2 64.3 65.1
NE rec 65.8 64.7 61.2
NE spec 95.4 95.2 95.6
English
Joint Pipe Coll
59.8 62.3 60.5
60.2 57.2 56.5
94.3 95.1 94.7
Spanish
Model Joint Pipe Coll
Subj prec 58.3 58.8 58.9
Subj rec 40.1 50.9 19.1
Subj spec 79.6 77.5 77.8
English
Joint Pipe Coll
46.6 52.2 45.9
44.5 48.5 16.4
77.6 80.8 74.0
</table>
<tableCaption confidence="0.9937335">
Table 6: Average precision, recall, and specificity for vo-
litional entity NER (in %).
</tableCaption>
<bodyText confidence="0.98959">
mistakes include confusing B- labels with I- labels.
Subjectivity and Sentiment Table 7 shows results
for the isolated task of predicting the presence of
sentiment about a volitional entity. In Spanish, the
pipeline models (PIPE) perform optimally for sub-
jectivity recall (Subj rec), and significantly above
the COLL models (p&lt;.001). Precision and speci-
ficity are comparable across models. In English as
in Spanish, the collapsed model is particularly poor
at subjectivity recall.
As discussed in Section 2, the subtask of predict-
ing whether subjectivity is expressed towards an en-
tity is comparable to the main task of Jiang et al.
(2011), and so we compare our approach here. The
Jiang et al. study is similar to the current study in that
they aim to detect targeted sentiment, but it differs
from the current study in that they focus exclusively
on subjectivity towards five manually selected enti-
ties: {Obama, Google, iPad, Lakers, Lady Gaga}.
They also evaluate on artificially balanced evalu-
ation data, and evaluate sentiment polarity (posi-
tive/negative) separately from subjectivity (has/does
not have sentiment).
Our dataset includes any entity labeled as PERSON
or ORGANIZATION, and is not balanced (most tar-
gets have no sentiment expressed towards them; see
Table 1), thus we can only roughly compare against
their approach. Lakers and Lady Gaga are rare in
our collection (appearing less than 3 times), and so
we updated the comparison set prior to evaluation to:
{Obama, Google, iPad, BBC, Tebow}. On this set, a
baseline that always guesses no sentiment reaches an
accuracy of 66.9%, compared to Jiang et al.’s 65.5%
accuracy on a balanced set (not strictly compara-
ble, but provided for reference). The JOINT mod-
els reach an accuracy of 71.04% on this set, demon-
strating this approach as potentially useful for topic-
dependent targeted sentiment.
Table 8 shows results for the task of predicting
the polarity of the sentiment expressed about an en-
tity. In Spanish, the PIPE models significantly out-
</bodyText>
<tableCaption confidence="0.979001333333333">
Table 7: Average precision, recall, and specificity (in %)
for subjectivity prediction (has/does not have sentiment)
along the target entity.
</tableCaption>
<table confidence="0.9753976">
English
Joint Pipe Coll
31.6 42.9 38.5
36.6 34.8 9.7
72.3 82.0 78.1
</table>
<tableCaption confidence="0.981071666666667">
Table 8: Average precision, recall, and specificity (in %)
for sentiment prediction (positive/negative/no sentiment)
along the target entity.
</tableCaption>
<bodyText confidence="0.998965727272727">
perform the COLL models on sentiment recall, and
the JOINT models on sentiment precision (p&lt;.01).
In English, PIPE significantly outperforms JOINT on
precision (p&lt;.001).
Targeted Subjectivity and Targeted Sentiment
The JOINT and PIPE models work reasonably
well for the isolated tasks of NER and subjectiv-
ity/sentiment prediction. We now examine results
for targeted subjectivity – labeling an entity and pre-
dicting whether there is sentiment directed towards
it – in Table 9; and targeted sentiment – labeling an
entity and predicting what the sentiment directed to-
wards it is – in Table 10.
We evaluate using two accuracy metrics: Acc-all,
which measures the accuracy of the entire named en-
tity span along with the sentiment span; and Acc-
Bsent, which measures the accuracy of identifying
the start of a named entity (B- labels) along with
the sentiment expressed towards it. Acc-all primar-
ily measures the correctness of O labels, while Acc-
Bsent focuses on the beginning of named entities.
For the targeted subjectivity task, our JOINT mod-
els perform optimally in Spanish, and significantly
above their baselines. For the Acc-Bsent task, JOINT
models perform best, significantly outperforming
their baseline for subjectivity prediction. In English,
where our data is half the size, we do not see a statis-
tically significant difference between the predictive
models and the no sentiment baselines.
For the targeted sentiment task, the JOINT mod-
els again perform relatively well in Spanish (Table
10), labeling volitional entities, predicting whether
or not there is sentiment targeted towards them, and
</bodyText>
<table confidence="0.994408384615385">
Spanish
Model Joint Pipe Coll
Sent prec 36.6 45.8 42.5
Sent rec 38.0 40.6 15.5
Sent spec 67.1 75.2 73.3
1650
Joint Joint Pipe Pipe Coll Coll
Base Base Base
89.5* 89.3 89.3** 89.1 89.5* 89.3
32.1*** 29.5 30.9*** 28.3 30.1** 28.1
88.0 88.1 88.6 88.6 87.9 88.1
30.4 30.8 30.7 30.3 28.1 29.2
***p&lt;.001 **p&lt;.01 *p&lt;.05
</table>
<tableCaption confidence="0.989019166666667">
Table 9: Average accuracy on Targeted Subjectivity Pre-
diction: Identifying volitional entities and whether they
are a sentiment target. In the core task, Acc-Bsent, the
best model in Spanish is JOINT, significantly outperform-
ing the baseline. In English, the best model (PIPE) does
not significantly improve over its baseline.
</tableCaption>
<table confidence="0.977928285714286">
Joint Joint Pipe Pipe Coll Coll
Base Base Base
89.4 89.4 89.0 89.0 89.2 89.3
29.7* 29.0 30.0 29.2 28.9 29.0
88.0 88.1 88.2 88.4 87.7 88.1
30.4 30.6 30.5 30.8 27.9 29.8
*p&lt;.05
</table>
<tableCaption confidence="0.979187">
Table 10: Average accuracy on Targeted Sentiment Pre-
</tableCaption>
<bodyText confidence="0.8795445">
diction: Identifying volitional entities and the polarity
of the sentiment expressed towards them. The Spanish
JOINT models significantly improve over their baseline
for the core task. In English, no models outperform their
baseline.
the sentiment polarity above their no sentiment base-
lines. We find this to be the most difficult task: It
may be clear that sentiment is being expressed to-
wards an entity, but it is not always clear what the
polarity of that sentiment is. Error analysis is given
below in this section. In the smaller English set, the
models do not outperform the no sentiment baseline.
</bodyText>
<sectionHeader confidence="0.998346" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999764733333333">
Feature Analysis Examples of some of the top-
weighted features in the Spanish models are shown
in Table 11. In addition to lexical identity and Brown
cluster, we find that positive indicators include pos-
itive suffixes such as diminutive forms, whether the
word can be syllabized (Section 5), and whether it is
three or four letters.
Error Analysis Because it is relatively common
for there not to be sentiment targeted at a named en-
tity, it is difficult to tease out the polarity in instances
where there is targeted sentiment. Similarly, our pre-
dictions are most reliable for detecting the absence
of a named entity (O labels).
Label confusions are shown in Table 12. Mistakes
are often made by confusing B- labels (the start of
</bodyText>
<sectionHeader confidence="0.827734" genericHeader="method">
B-VOLITIONAL FEATURES
</sectionHeader>
<bodyText confidence="0.98331025">
Negative is a function word; jerboa tags; followed by a word
with 3 or 4 letters that cannot be syllabified
Positive ends in -a, -o, or -s; is capitalized; has one non-
initial capital letter; is 3 or 4 letters
</bodyText>
<subsectionHeader confidence="0.488143">
B-VOLITIONAL, POS FEATURES
</subsectionHeader>
<bodyText confidence="0.894392285714286">
Negative preceded by a curse word; followed by a word
with a positive suffix; immediately preceded by a
word with a negative prefix
Positive not in a sentiment lexicon; preceded by a happy
emoticon; followed by an exclamation or a ‘my’
word; immediately preceded by a laugh; has two
or more sentiment-bearing words in the sentence
</bodyText>
<sectionHeader confidence="0.466562" genericHeader="method">
B-VOLITIONAL, NEG FEATURES
</sectionHeader>
<bodyText confidence="0.984706">
Negative is immediately followed by a question mark or
positive abbreviation word
Positive preceded by a ‘bad’ word or curse word; has four
or more sentiment lexicon items
</bodyText>
<sectionHeader confidence="0.932983" genericHeader="method">
B-VOLITIONAL, NOT-TARG FEATURES
</sectionHeader>
<bodyText confidence="0.8842898">
Negative immediately followed by a ‘no’ word or word with
a negative prefix; is preceded by a question mark;
is immediately preceded by a curse word or laugh;
is followed by an exclamation mark
Positive not followed by sentiment lexicon word
</bodyText>
<tableCaption confidence="0.9144578">
Table 11: Example strongly weighted features for a
Spanish joint sentiment model. In addition to lexical
identity, we find that curse words and positive and neg-
ative prefixes are used to detect volitional entities and the
sentiment directed towards them.
</tableCaption>
<bodyText confidence="0.934688545454545">
an entity) with I- labels (inside an entity); and by
predicting sentiment polarity when the gold annota-
tions say there is not sentiment targeted at the entity.
Some example errors are shown in Figure 13. In
(1), “CANSADO” (“TIRED”) was predicted to be
volitional, while “Matthew” was not. In (2), “Ma-
tias del rio” was not predicted to be an entity, likely
due to the fact that the capitalization patterns we see
in this sentence are indicative of the start of a sen-
tence rather than a proper name (similar to 1). In (3),
a. b.
</bodyText>
<subsectionHeader confidence="0.386643">
Observed Observed
</subsectionHeader>
<table confidence="0.89838725">
B I O POS NEG NEUT
B 423 21 186 POS 68 24 42
I 36 236 135 NEG 58 65 102
O 197 90 7168 NEUT 115 61 468
</table>
<tableCaption confidence="0.876830333333333">
Table 12: Predicted vs. observed values for a joint model.
(a) For named entities, most common confusions were
between B-VOLITIONAL and O labels. (b) For sentiment,
most common mistakes were to predict that a positive
sentiment was neutral (no sentiment), and that a neutral
sentiment was negative.
</tableCaption>
<figure confidence="0.913174976190476">
Model
Acc-all
Acc-Bsent
Acc-all
Acc-Bsent
Spa
Eng
Model
Acc-all
Acc-Bsent
Acc-all
Acc-Bsent
Spa
Eng
Predicted
1651
NE prediction errors
Cuando estoy CANSADO , ´el es mi DESCANSO . Mateo . 11 : 29 .
O O B-VOLITIONAL O O O O O O O O O O O O
O O O O O O O O O B-VOLITIONAL O O O O O
When I’m TIRED , he is my REST . Matthew . 11 : 29 .
Spanish: Matias del rio fue una lata ...
Spanish:
1.
Predicted:
Gold:
English:
2. Predicted: O O O O O O ...
Gold: B-VOLITIONAL I-VOLITIONAL I-VOLITIONAL O O O ...
English: Matias del r´ıo was a drag ...
Sentiment prediction errors
Spanish: Mario que dio este contigo Spanish: ... si de verdad estas en cielo , ayudame Superman !!!
3. Predicted: NOT-TARG - - - - 4. Predicted: - - - - - - - - POSITIVE -
Gold: POSITIVE - - - - Gold: - - - - - - - - NOT-TARG -
English: Mario may God be with you English: ... if you really are in the skies, help me Superman !!!
Sentiment and NE prediction errors
Spanish: Salen del gobierno de Humala dos connotados izquierdistas, Giesecke y Eiguiguren
Predicted: O O O O B-VOLITIONAL I-VOLITIONAL O O O B-VOLITIONAL O B-VOLITIONAL
5. - - - - NOT-TARG NOT-TARG - - - NOT-TARG - NOT-TARG
Gold: O O O O B-VOLITIONAL O O O O B-VOLITIONAL O B-VOLITIONAL
- - - - NOT-TARG - - - - NEGATIVE - NOT-TARG
English: Leaving the Humala government are two notorious leftists , Giesecke and Eiguiguren
</figure>
<tableCaption confidence="0.994313">
Table 13: Example errors made by joint models.
</tableCaption>
<bodyText confidence="0.999782076923077">
sentiment may not be clear without spelling correc-
tion: “dio” should be “dios”, meaning “God”; other-
wise, “dio” is the word for “gave”. Humans can eas-
ily fix the spelling error, which changes the overall
reading of the expression. In (4), the positive polar-
ity item “verdad” (“believe”) and the exclamation
marks (!!!) were likely used as indicators of posi-
tive sentiment; however, in this case the annotators
marked the targeted sentiment as neutral. In (5), the
“Humala” entity was predicted to be longer than it is
(“Hamala dos” or “Hamala two”). It was also pre-
dicted that both “Giesecke” and “Eiguiguren” had
no sentiment expressed towards them; annotators
disagreed, with the majority of those who annotated
“Giesecke” marking negative sentiment, and the ma-
jority of those who annotated “Eiguiguren” mark-
ing no sentiment. This highlights some of the diffi-
culty in predicting sentiment discussed in Section 3,
where annotators will often disagree as to whether
there is no sentiment or positive/negative sentiment.
During development, we found that the collapsed
model (COLL) performed best on small amounts of
data. However, as we scaled up the amount of data
we trained on, the PIPE and JOINT models signif-
icantly improved, while the COLL models did not
have significant performance gains.
</bodyText>
<sectionHeader confidence="0.998661" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999974958333333">
We have introduced the task of open domain targeted
sentiment: predicting sentiment directed towards an
entity along with discovering the entity itself. Our
approach is developed to find targeted sentiment to-
wards both person and organization named entities
by modeling sentiment as a span along the entity.
We find that by modeling targeted sentiment in
this way, we can reliably detect entities and whether
or not they are sentiment targets above a no senti-
ment baseline. How best to determine the polarity
of the sentiment expressed towards the entity, how-
ever, is still an open issue. Our data suggests that
it is usually not clear-cut whether sentiment is being
expressed or not; the strong disagreement between
annotators suggests that detecting sentiment polar-
ity in microblogs is difficult even for humans.
In future work, we hope to explore further meth-
ods for teasing apart sentiment polarity expressed to-
wards a target. This research has achieved promis-
ing results for detecting sentiment targets without re-
lying on external supervised models, and we hope
that the features and approaches developed here can
aid in sentiment analysis in noisy text and languages
without rich linguistic resources.
</bodyText>
<page confidence="0.993564">
1652
</page>
<sectionHeader confidence="0.996003" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999554961904762">
A. Agarwal, B. Xie, I. Vovsha, O. Rambow, and R. Pas-
sonneau. 2011. Sentiment analysis of twitter data. In
Proceedings of the Workshop on Language in Social
Media.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC’10).
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proceedings of Coling: Posters.
Adam Bermingham and Alan F Smeaton. 2010. Clas-
sifying sentiment in microblogs: Is brevity an advan-
tage? In Proceedings of CIKM-2010.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in Twitter streaming data. In Proceed-
ings of the International Conference on Discovery Sci-
ence (DS-2010).
Juliette Blevins. 1996. The syllable in phonological the-
ory. In John A. Goldswmith, editor, The Handbook
of Phonological Theory. Blackwell Publishing, Black-
well Reference Online.
N. N. Bora. 2012. Summarizing public opinions in
tweets. In Proceedings of CICLing-2012.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proceedings of EMNLP-2011.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J.C. Lai,
and R.L. Mercer. 1992. Class-based n-gram mod-
els of natural language. Computational Linguistics,
18(4):467–479.
Pedro Henrique Calais Guerra, Adriano Veloso, Wagner
Meira Jr, and Virg´ılio Almeida. 2011. From bias to
opinion: a transfer-learning approach to real-time sen-
timent analysis. In Proceedings of the KDD-2011.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with amazon’s mechanical
turk. In Proceedings of the NAACL:HLT Workshop on
Creating Speech and Language Data with Amazon’s
Mechanical Turk.
Lu Chen, Wenbo Wang, Meenakshi Nagarajan, Shaojun
Wang, and Amit P. Sheth. 2012. Extracting diverse
sentiment expressions with target-dependent polarity
from twitter. In Proceedings of ICWSM-2012.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. Proceedings of EMNLP 2006.
G. N. Clements. 1990. The role of the sonority cycle in
core syllabification. In J. Kingston and M. Beckman,
editors, Papers in Laboratory Phonology, pages 283–
333. CUP, Cambridge.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using Twitter hashtags
and smileys. In Proceedings of Coling: Posters.
Nicholas A Diakopoulos and David A Shamma. 2010.
Characterizing debate performance via aggregated
twitter sentiment. In Proceedings of CHI-2010.
David Etter, Francis Ferraro, Ryan Cotterell, Olivia
Buzek, and Benjamin Van Durme. 2013. Nerit:
Named entity recognition for informal text. Techni-
cal Report 11, Human Language Technology Center
of Excellence, Johns Hopkins University, July.
Jenny Rose Finkel and Christopher D. Manning. 2010.
Hierarchical joint learning: Improving joint parsing
and named entity recognition with non-jointly labeled
data. In Proceedings of ACL-2010.
Joan B. Hooper. 1976. The syllable in phonological the-
ory. Language, 48(3):525–540.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of KDD.
Xia Hu, Lei Tang, Jiliang Tang, and Huan Liu. 2013. Ex-
ploiting social relations for sentiment analysis in mi-
croblogging. In Proceedings of the 6th ACM Inter-
national Conference on Web Search and Data Mining
(WSDM-2013).
Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single-and cross-domain setting
with conditional random fields. In Proceedings of
EMNLP.
Long Jiang, Mo Yu, Xiaohua Liu, and Tiejun Zhao. 2011.
Target-dependent twitter sentiment classification. In
Proceedings of ACL-2011.
Wei Jin and Hung Hay Ho. 2009. A novel lexicalized
hmm-based learning framework for web opinion min-
ing. Proceedings of ICML 2009.
Soo-Min Kim and Eduard Hovy. 2006. Identifying and
analyzing judgment opinions. Proceedings of NAACL
2006.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL/HLT.
Michal Kosinskia, David Stillwell, and Thore Graepel.
2013. Private trains and attributes are predictable from
digital records of human behavior. Proc. of the Na-
tional Academy of Sciences of the USA, 110(5).
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proceedings of ICWSM-
2011.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML-2001.
</reference>
<page confidence="0.565102">
1653
</page>
<reference confidence="0.99984072826087">
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010a.
Structure-aware review mining and summarization.
Proceedings of Coling 2010.
Guangxia Li, Steven CH Hoi, Kuiyu Chang, and Ramesh
Jain. 2010b. Micro-blogging sentiment detection
by collaborative online learning. In Proceedings of
ICDM-2010.
Hao Li, Yu Chen, Heng Ji, Smaranda Muresan, and De-
quan Zheng. 2012. Combining social cognitive theo-
ries with linguistic features for multi-genre sentiment
analysis. In Proceedings of the Pacific Asia Con-
ference on Language, Information and Computation
(PACLIC-2012).
Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction, and web-enhanced lexicons.
In Proceedings of CoNLL-2003.
Frida Morelli. 2003. The relative harmony of /s+stop/
onsets: Obstruent clusters and the sonority sequenc-
ing principle. In C. Fery and R. van de Vijver, edi-
tors, The syllable in optimality theory, pages 356–371.
CUP, New York.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
corpus for sentiment analysis and opinion mining. In
Proceedings of LREC-2010.
James W. Pennebaker, Roger J. Booth, and Martha E.
Francis. 2007. Linguistic inquiry and word count:
Liwc2007, operator’s manual.
Veronica Perez-Rosas, Carmen Banea, and Rada Mihal-
cea. 2012. Learning sentiment lexicons in span-
ish. Proceedings of the Conference on Language Re-
sources and Evaluations (LREC 2012).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
Coling:ACL-2006.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of HLT:EMNLP-2005.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extraction
through double propagation. Computational Linguis-
tics, 37(1).
Hassan Saif, Yulan He, and Harith Alani. 2012. Allevi-
ating data sparsity for twitter sentiment analysis. Pro-
ceedings of the WWW Workshop on Making Sense of
Microposts (# MSM2012).
Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Ja-
son Baldridge. 2011. Twitter polarity classification
with label propagation over lexical links and the fol-
lower graph. In Proceedings of the EMNLP-2011
Workshop on Unsupervised Learning in NLP.
Veselin Stoyanov and Jason Eisner. 2012. Minimum-
risk training of approximate crf-based nlp systems. In
Proceedings of NAACL:HLT-2012.
Veselin Stoyanov, Alexander Ropson, and Jason Eis-
ner. 2011. Empirical risk minimization of graphi-
cal model parameters given approximate inference, de-
coding, and model structure. In AIStats.
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment anal-
ysis incorporating social networks. In Proceedings of
the KDD-2011.
Benjamin Van Durme. 2012. Jerboa: A toolkit for ran-
domized and streaming algorithms. Technical report,
Human Language Technology Center of Excellence,
Johns Hopkins University.
Svitlana Volkova, Theresa Wilson, and David Yarowsky.
2013. Exploring sentiment in social media: Boot-
strapping subjectivity clues from multilingual twitter
streams. In Association for Computational Linguistics
(ACL).
Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou, and
Ming Zhang. 2011. Topic sentiment analysis in Twit-
ter: A graph-based hashtag sentiment classification ap-
proach. In Proceedings of CIKM-2011.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In Proceedings of HLT-EMNLP.
Theresa Wilson, Janyce Wiebe, and Paul Hoffman. 2009.
Recognizing contextual polarity: An exploration of
features for phrase-level sentiment analysis. Compu-
tational Linguistics, 35(3).
Bishan Yang and Claire Cardie. 2013. Joint inference for
fine-grained opinion extraction. Proceedings of ACL
2013.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment analyzer: Ex-
tracting sentiments about a given topic using natural
language processing techniques. In Proceedings of
ICDM-2003.
</reference>
<page confidence="0.994696">
1654
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.510282">
<title confidence="0.998799">Open Domain Targeted Sentiment</title>
<author confidence="0.999698">Margaret Mitchell Jacqueline Aguilar Theresa Wilson Benjamin Van</author>
<affiliation confidence="0.7039995">Human Language Technology Center of Johns Hopkins</affiliation>
<address confidence="0.999781">Baltimore, MD 21218, USA</address>
<email confidence="0.999006">Theresa.Wilson@oberlin.edu,vandurme@cs.jhu.edu</email>
<abstract confidence="0.9996355">We propose a novel approach to sentiment analysis for a low resource setting. The intuition behind this work is that sentiment towards an entity, sentimay be viewed as a span of sentiment expressed across the entity. This representation allows us to model sentiment detection as a sequence tagging problem, jointly discovering people and organizations along with whether there is sentiment directed towards them. We compare performance in both Spanish and English on microblog data, using only a sentiment lexicon as an external resource. By leveraging linguisticallyinformed features within conditional random fields (CRFs) trained to minimize empirical risk, our best models in Spanish significantly outperform a strong baseline, and reach around 90% accuracy on the combined task of named entity recognition and sentiment prediction. Our models in English, trained on a much smaller dataset, are not yet statistically significant against their baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Agarwal</author>
<author>B Xie</author>
<author>I Vovsha</author>
<author>O Rambow</author>
<author>R Passonneau</author>
</authors>
<title>Sentiment analysis of twitter data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Language in Social Media.</booktitle>
<contexts>
<context position="3202" citStr="Agarwal et al., 2011" startWordPosition="498" endWordPosition="501">portance of the target when interpreting sentiment in context. If we are looking for sentiments toward Kentucky, for example, we would want to identify (1) as negative, (2) as neutral (no sentiment) and (3) as positive. However, if we are looking for sentiment toward Tennessee, we would want to identify (1) as positive, and (2) and (3) as neutral. The expression of these and other kinds of sentiment can be understood as involving three items: (1) An experiencer (2) An attitude (3) A target (optionally) Research in sentiment analysis often focuses on (2), predicting overall sentiment polarity (Agarwal et al., 2011; Bora, 2012). Recent work has begun to combine (2) with (3), examining how to automatically predict the sentiment polarity expressed towards a target entity (Jiang et al., 2011; Chen et al., 2012) for a fixed set of targets. This topic-dependent sentiment classification requires that the target entity be 1643 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1643–1654, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Figure 1: Sentiment expressed across an entity. given, and returns statements expressing</context>
<context position="7509" citStr="Agarwal et al., 2011" startWordPosition="1191" endWordPosition="1194">l tools and code used for this research are released with this paper.2 2 Related Work As the scale of social media has grown, using sources such as Twitter to mine public sentiment has become increasingly promising. Commercial systems include Sentiment1403 (products and brands) and tweetfeel4 (suggests searching for popular movies, celebrities and companies). The majority of academic research has focused on supervised classification of message sentiment irrespective of target (Barbosa and Feng, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2www.m-mitchell.com/code 3www.sentiment140.com 4www.tweetfeel.com 1644 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sent</context>
</contexts>
<marker>Agarwal, Xie, Vovsha, Rambow, Passonneau, 2011</marker>
<rawString>A. Agarwal, B. Xie, I. Vovsha, O. Rambow, and R. Passonneau. 2011. Sentiment analysis of twitter data. In Proceedings of the Workshop on Language in Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10).</booktitle>
<contexts>
<context position="13071" citStr="Baccianella et al., 2010" startWordPosition="2101" endWordPosition="2104">ATION entities, which make up the majority of named entities in this data, and we evaluate these using the more general entity category VOLITIONAL. Removing retweets, 7,105 Spanish tweets contained a total of 9,870 volitional entities and 2,350 English tweets contained a total of 3,577 volitional entities. Sentiment Lexicons We use two sentiment lexicon sources in each language. For English, we use the MPQA lexicon (Wilson et al., 2005), which identifies 12,296 manually and semi-automatically produced subjective terms along with their polarity. For the second lexicon, we use SentiWordNet 3.0 (Baccianella et al., 2010), which assigns positive and negative polarity scores to WordNet synsets. We use the majority polarity of all words with a subjectivity score above 0.5. For Spanish, the first lexicon is obtained from Volkova et al. (2013), who automatically translated strongly subjective terms from the MPQA lexicon (Wilson et al., 2005) into Spanish. The resulting Spanish lexicon contains about 65K words. The second lexicon is available from Perez-Rosas et al. (2012). This contains approximately 1000 sentiment-bearing words collected leveraging manual resources and 2000 collected leveraging automatic resource</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust sentiment detection on Twitter from biased and noisy data.</title>
<date>2010</date>
<booktitle>In Proceedings of Coling:</booktitle>
<publisher>Posters.</publisher>
<contexts>
<context position="7392" citStr="Barbosa and Feng, 2010" startWordPosition="1171" endWordPosition="1174"> tags (Brown clusters) and a method that automatically syllabifies a word based on the orthography of the language. All tools and code used for this research are released with this paper.2 2 Related Work As the scale of social media has grown, using sources such as Twitter to mine public sentiment has become increasingly promising. Commercial systems include Sentiment1403 (products and brands) and tweetfeel4 (suggests searching for popular movies, celebrities and companies). The majority of academic research has focused on supervised classification of message sentiment irrespective of target (Barbosa and Feng, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2www.m-mitchell.com/code 3www.sentiment140.com 4www.tweetfeel.com 1644 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu e</context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust sentiment detection on Twitter from biased and noisy data. In Proceedings of Coling: Posters.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Bermingham</author>
<author>Alan F Smeaton</author>
</authors>
<title>Classifying sentiment in microblogs: Is brevity an advantage?</title>
<date>2010</date>
<booktitle>In Proceedings of CIKM-2010.</booktitle>
<contexts>
<context position="8432" citStr="Bermingham and Smeaton, 2010" startWordPosition="1321" endWordPosition="1324">te for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sentiment-topic modeling (Saif et al., 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a; Jiang et al., 2011; Tan et al., 2011; Wang et al., 2011; Li et al., 2012; Chen et al., 2012), have mostly focused on topic-dependent analysis. In these approaches, messages are collected on a fixed set of topics/targets, such as products or sports teams, and sentiment is learned for the given set. In contrast, we aim to predict sentiment in tweets for any named person or organization. We refer to this task as open domain targeted sentiment analysis. Within topic-dependent sentiment analysis, several approaches have explored applying CRFs or HMMs to extract</context>
</contexts>
<marker>Bermingham, Smeaton, 2010</marker>
<rawString>Adam Bermingham and Alan F Smeaton. 2010. Classifying sentiment in microblogs: Is brevity an advantage? In Proceedings of CIKM-2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Bifet</author>
<author>Eibe Frank</author>
</authors>
<title>Sentiment knowledge discovery in Twitter streaming data.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Discovery Science (DS-2010).</booktitle>
<contexts>
<context position="7439" citStr="Bifet and Frank, 2010" startWordPosition="1179" endWordPosition="1182">ically syllabifies a word based on the orthography of the language. All tools and code used for this research are released with this paper.2 2 Related Work As the scale of social media has grown, using sources such as Twitter to mine public sentiment has become increasingly promising. Commercial systems include Sentiment1403 (products and brands) and tweetfeel4 (suggests searching for popular movies, celebrities and companies). The majority of academic research has focused on supervised classification of message sentiment irrespective of target (Barbosa and Feng, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2www.m-mitchell.com/code 3www.sentiment140.com 4www.tweetfeel.com 1644 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on coll</context>
</contexts>
<marker>Bifet, Frank, 2010</marker>
<rawString>Albert Bifet and Eibe Frank. 2010. Sentiment knowledge discovery in Twitter streaming data. In Proceedings of the International Conference on Discovery Science (DS-2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juliette Blevins</author>
</authors>
<title>The syllable in phonological theory. In</title>
<date>1996</date>
<booktitle>The Handbook of Phonological Theory.</booktitle>
<editor>John A. Goldswmith, editor,</editor>
<publisher>Blackwell Publishing, Blackwell Reference Online.</publisher>
<contexts>
<context position="23180" citStr="Blevins, 1996" startWordPosition="3702" endWordPosition="3703">age model on the clusters. For our task, we cut clusters at length 3 and length 5, and these serve as rough part-of-speech tags without the need to train additional models. For example, the word hello is tagged as belonging to cluster 011 (length 3) and 01111 (length 5). During development, we found that being able to syllabify the word (break the word into syllables) was a positive indicator of people names, but a negative indicator of organization names. This observation can be approximated automatically using constraints from the sonority sequencing principle (Hooper, 1976; Clements, 1990; Blevins, 1996; Morelli, 2003) on a language’s orthography. This is a phonotactic principle that states that syllables will tend to have a sonority peak, usually a vowel, in the center of the syllable, followed on either side by consonants with decreasing sonority. Although languages may violate this principle, the core idea that a vowel forms the nucleus of a syllable with op9www.m-mitchell.com/code 10For Spanish, we train on a sample of ˜7 million Spanish tweets. For English, we train on the essays (Pennebaker et al., 2007) and Facebook data (Kosinskia et al., 2013) available from ICWSM 2013. 1648 tional </context>
</contexts>
<marker>Blevins, 1996</marker>
<rawString>Juliette Blevins. 1996. The syllable in phonological theory. In John A. Goldswmith, editor, The Handbook of Phonological Theory. Blackwell Publishing, Blackwell Reference Online.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N N Bora</author>
</authors>
<title>Summarizing public opinions in tweets.</title>
<date>2012</date>
<booktitle>In Proceedings of CICLing-2012.</booktitle>
<contexts>
<context position="3215" citStr="Bora, 2012" startWordPosition="502" endWordPosition="503"> when interpreting sentiment in context. If we are looking for sentiments toward Kentucky, for example, we would want to identify (1) as negative, (2) as neutral (no sentiment) and (3) as positive. However, if we are looking for sentiment toward Tennessee, we would want to identify (1) as positive, and (2) and (3) as neutral. The expression of these and other kinds of sentiment can be understood as involving three items: (1) An experiencer (2) An attitude (3) A target (optionally) Research in sentiment analysis often focuses on (2), predicting overall sentiment polarity (Agarwal et al., 2011; Bora, 2012). Recent work has begun to combine (2) with (3), examining how to automatically predict the sentiment polarity expressed towards a target entity (Jiang et al., 2011; Chen et al., 2012) for a fixed set of targets. This topic-dependent sentiment classification requires that the target entity be 1643 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1643–1654, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Figure 1: Sentiment expressed across an entity. given, and returns statements expressing sentiment to</context>
</contexts>
<marker>Bora, 2012</marker>
<rawString>N. N. Bora. 2012. Summarizing public opinions in tweets. In Proceedings of CICLing-2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Nicholas Diakopoulos</author>
</authors>
<title>Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using word lengthening to detect sentiment in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP-2011.</booktitle>
<contexts>
<context position="8371" citStr="Brody and Diakopoulos, 2011" startWordPosition="1313" endWordPosition="1316">3www.sentiment140.com 4www.tweetfeel.com 1644 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sentiment-topic modeling (Saif et al., 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a; Jiang et al., 2011; Tan et al., 2011; Wang et al., 2011; Li et al., 2012; Chen et al., 2012), have mostly focused on topic-dependent analysis. In these approaches, messages are collected on a fixed set of topics/targets, such as products or sports teams, and sentiment is learned for the given set. In contrast, we aim to predict sentiment in tweets for any named person or organization. We refer to this task as open domain targeted sentiment analysis. Within topic-dependent sentiment analysis, seve</context>
</contexts>
<marker>Brody, Diakopoulos, 2011</marker>
<rawString>Samuel Brody and Nicholas Diakopoulos. 2011. Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using word lengthening to detect sentiment in microblogs. In Proceedings of EMNLP-2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>P V deSouza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="22359" citStr="Brown et al., 1992" startWordPosition="3567" endWordPosition="3570">dow of three words in either direction. Words seen only once are treated as out-of-vocabulary. Surface features and linguistic features are concatenated in groups of two and three to create further features. All algorithms and code that we have developed for feature extraction are available online.9 Because we aim to develop models that do not heavily rely on language-specific resources, we are interested in exploring unsupervised and lightly supervised methods for learning relevant features. Rather than use part-of-speech tags, we therefore use Brown cluster labels as unsupervised word tags (Brown et al., 1992; Koo et al., 2008). Brown clustering is a distributional similarity method that merges pairs of word clusters in the training data10 to create the smallest decrease in corpus likelihood, using a bigram language model on the clusters. For our task, we cut clusters at length 3 and length 5, and these serve as rough part-of-speech tags without the need to train additional models. For example, the word hello is tagged as belonging to cluster 011 (length 3) and 01111 (length 5). During development, we found that being able to syllabify the word (break the word into syllables) was a positive indica</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>P. F. Brown, V. J. Della Pietra, P. V. deSouza, J.C. Lai, and R.L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Henrique Calais Guerra</author>
<author>Adriano Veloso</author>
<author>Wagner Meira Jr</author>
<author>Virg´ılio Almeida</author>
</authors>
<title>From bias to opinion: a transfer-learning approach to real-time sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the KDD-2011.</booktitle>
<contexts>
<context position="7949" citStr="Guerra et al., 2011" startWordPosition="1251" endWordPosition="1254">ssage sentiment irrespective of target (Barbosa and Feng, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2www.m-mitchell.com/code 3www.sentiment140.com 4www.tweetfeel.com 1644 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sentiment-topic modeling (Saif et al., 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a; Jiang et al., 2011; Tan et al., 2011; Wang et al., 2011; Li et al., 2012; Chen </context>
</contexts>
<marker>Guerra, Veloso, Jr, Almeida, 2011</marker>
<rawString>Pedro Henrique Calais Guerra, Adriano Veloso, Wagner Meira Jr, and Virg´ılio Almeida. 2011. From bias to opinion: a transfer-learning approach to real-time sentiment analysis. In Proceedings of the KDD-2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating speech and language data with amazon’s mechanical turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL:HLT Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="14163" citStr="Callison-Burch and Dredze, 2010" startWordPosition="2265" endWordPosition="2268">s contains approximately 1000 sentiment-bearing words collected leveraging manual resources and 2000 collected leveraging automatic resources. Annotation To collect sentiment labels, we use crowdsourcing through Amazon’s Mechanical Turk.5 Annotators (“Turkers”) were shown six tweets at a time, each with a single highlighted named entity. Turkers were instructed to (1) select the sentiment being expressed towards the entity (positive, negative, or no sentiment); and (2) rate their level of confidence in their selection. Following best practices on collecting language data with Mechanical Turk (Callison-Burch and Dredze, 2010), two controls were placed among each set of six tweets to screen out unreliable judgments. An example prompt is shown in Figure 3. Each (tweet, NE) pair was shown to three Turkers, and those with majority consensus on sentiment polarity were extracted. Tweets without sentiment 5www.mturk.com/mturk Positive Negative Neutral ORGANIZATION PERSON Named Entity Figure 4: Targeted sentiment annotated for Spanish. Majority POS NEUTRAL NEG POS 757 1249 130 NEUTRAL 707 2151 473 NEG 129 726 452 Table 2: Number of targeted sentiment instances where at least two of the three annotators (Majority) agreed. </context>
</contexts>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with amazon’s mechanical turk. In Proceedings of the NAACL:HLT Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu Chen</author>
<author>Wenbo Wang</author>
<author>Meenakshi Nagarajan</author>
<author>Shaojun Wang</author>
<author>Amit P Sheth</author>
</authors>
<title>Extracting diverse sentiment expressions with target-dependent polarity from twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of ICWSM-2012.</booktitle>
<contexts>
<context position="3399" citStr="Chen et al., 2012" startWordPosition="531" endWordPosition="534">and (3) as positive. However, if we are looking for sentiment toward Tennessee, we would want to identify (1) as positive, and (2) and (3) as neutral. The expression of these and other kinds of sentiment can be understood as involving three items: (1) An experiencer (2) An attitude (3) A target (optionally) Research in sentiment analysis often focuses on (2), predicting overall sentiment polarity (Agarwal et al., 2011; Bora, 2012). Recent work has begun to combine (2) with (3), examining how to automatically predict the sentiment polarity expressed towards a target entity (Jiang et al., 2011; Chen et al., 2012) for a fixed set of targets. This topic-dependent sentiment classification requires that the target entity be 1643 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1643–1654, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Figure 1: Sentiment expressed across an entity. given, and returns statements expressing sentiment towards the given entity. In this paper, we take a step towards open-domain, targeted sentiment analysis by investigating how to detect both the named entity and the sentiment expressed </context>
<context position="8562" citStr="Chen et al., 2012" startWordPosition="1349" endWordPosition="1352"> 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sentiment-topic modeling (Saif et al., 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a; Jiang et al., 2011; Tan et al., 2011; Wang et al., 2011; Li et al., 2012; Chen et al., 2012), have mostly focused on topic-dependent analysis. In these approaches, messages are collected on a fixed set of topics/targets, such as products or sports teams, and sentiment is learned for the given set. In contrast, we aim to predict sentiment in tweets for any named person or organization. We refer to this task as open domain targeted sentiment analysis. Within topic-dependent sentiment analysis, several approaches have explored applying CRFs or HMMs to extract sentiment and target words from text (Jin and Ho, 2009; Li et al., 2010a). In these approaches, opinion expressions are extracted</context>
</contexts>
<marker>Chen, Wang, Nagarajan, Wang, Sheth, 2012</marker>
<rawString>Lu Chen, Wenbo Wang, Meenakshi Nagarajan, Shaojun Wang, and Amit P. Sheth. 2012. Extracting diverse sentiment expressions with target-dependent polarity from twitter. In Proceedings of ICWSM-2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Eric Breck</author>
<author>Claire Cardie</author>
</authors>
<title>Joint extraction of entities and relations for opinion recognition.</title>
<date>2006</date>
<booktitle>Proceedings of EMNLP</booktitle>
<contexts>
<context position="4736" citStr="Choi et al., 2006" startWordPosition="744" endWordPosition="747">ng the span of the entity itself: Similar to how named entity recognition (NER) learns labels along the span of each word in an entity name, sentiment may be expressed along the entity as well. A small example is shown in Figure 1. We focus on people and organizations (volitional named entities), which are the primary targets of sentiment in our microblog data (see Table 1). Both NER and opinion expression extraction have achieved impressive results using conditional random fields (CRFs) (Lafferty et al., 2001) to define the conditional probability of entity categories (McCallum and Li, 2003; Choi et al., 2006; Yang and Cardie, 2013). We develop such models to jointly predict the NE and the sentiment expressed towards it using minimum risk training (Stoyanov and Eisner, 2012). We learn our models on informal Spanish and English language taken from the social network Twitter,1 where the language variety makes NLP particularly challenging (see Figure 2). Our ultimate goal is to develop models that will be useful for low resource languages, where a sentiment lexicon may be known or bootstrapped, but more sophisticated linguistic tools may not be readily available. We therefore do not rely on an extern</context>
</contexts>
<marker>Choi, Breck, Cardie, 2006</marker>
<rawString>Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint extraction of entities and relations for opinion recognition. Proceedings of EMNLP 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G N Clements</author>
</authors>
<title>The role of the sonority cycle in core syllabification.</title>
<date>1990</date>
<booktitle>Papers in Laboratory Phonology,</booktitle>
<pages>283--333</pages>
<editor>In J. Kingston and M. Beckman, editors,</editor>
<publisher>CUP, Cambridge.</publisher>
<contexts>
<context position="23165" citStr="Clements, 1990" startWordPosition="3700" endWordPosition="3701">g a bigram language model on the clusters. For our task, we cut clusters at length 3 and length 5, and these serve as rough part-of-speech tags without the need to train additional models. For example, the word hello is tagged as belonging to cluster 011 (length 3) and 01111 (length 5). During development, we found that being able to syllabify the word (break the word into syllables) was a positive indicator of people names, but a negative indicator of organization names. This observation can be approximated automatically using constraints from the sonority sequencing principle (Hooper, 1976; Clements, 1990; Blevins, 1996; Morelli, 2003) on a language’s orthography. This is a phonotactic principle that states that syllables will tend to have a sonority peak, usually a vowel, in the center of the syllable, followed on either side by consonants with decreasing sonority. Although languages may violate this principle, the core idea that a vowel forms the nucleus of a syllable with op9www.m-mitchell.com/code 10For Spanish, we train on a sample of ˜7 million Spanish tweets. For English, we train on the essays (Pennebaker et al., 2007) and Facebook data (Kosinskia et al., 2013) available from ICWSM 201</context>
</contexts>
<marker>Clements, 1990</marker>
<rawString>G. N. Clements. 1990. The role of the sonority cycle in core syllabification. In J. Kingston and M. Beckman, editors, Papers in Laboratory Phonology, pages 283– 333. CUP, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using Twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of Coling:</booktitle>
<publisher>Posters.</publisher>
<contexts>
<context position="7461" citStr="Davidov et al., 2010" startWordPosition="1183" endWordPosition="1186">rd based on the orthography of the language. All tools and code used for this research are released with this paper.2 2 Related Work As the scale of social media has grown, using sources such as Twitter to mine public sentiment has become increasingly promising. Commercial systems include Sentiment1403 (products and brands) and tweetfeel4 (suggests searching for popular movies, celebrities and companies). The majority of academic research has focused on supervised classification of message sentiment irrespective of target (Barbosa and Feng, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2www.m-mitchell.com/code 3www.sentiment140.com 4www.tweetfeel.com 1644 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learn</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using Twitter hashtags and smileys. In Proceedings of Coling: Posters.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas A Diakopoulos</author>
<author>David A Shamma</author>
</authors>
<title>Characterizing debate performance via aggregated twitter sentiment.</title>
<date>2010</date>
<booktitle>In Proceedings of CHI-2010.</booktitle>
<contexts>
<context position="8224" citStr="Diakopoulos and Shamma, 2010" startWordPosition="1292" endWordPosition="1295">moticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2www.m-mitchell.com/code 3www.sentiment140.com 4www.tweetfeel.com 1644 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sentiment-topic modeling (Saif et al., 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a; Jiang et al., 2011; Tan et al., 2011; Wang et al., 2011; Li et al., 2012; Chen et al., 2012), have mostly focused on topic-dependent analysis. In these approaches, messages are collected on a fixed set of topics/targets, such as products or sports teams, and sentiment is learned for the given set. In contrast, we aim to predict sentiment in tweets for </context>
</contexts>
<marker>Diakopoulos, Shamma, 2010</marker>
<rawString>Nicholas A Diakopoulos and David A Shamma. 2010. Characterizing debate performance via aggregated twitter sentiment. In Proceedings of CHI-2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Etter</author>
<author>Francis Ferraro</author>
<author>Ryan Cotterell</author>
<author>Olivia Buzek</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Nerit: Named entity recognition for informal text.</title>
<date>2013</date>
<tech>Technical Report 11,</tech>
<institution>Human Language Technology Center of Excellence, Johns Hopkins University,</institution>
<marker>Etter, Ferraro, Cotterell, Buzek, Van Durme, 2013</marker>
<rawString>David Etter, Francis Ferraro, Ryan Cotterell, Olivia Buzek, and Benjamin Van Durme. 2013. Nerit: Named entity recognition for informal text. Technical Report 11, Human Language Technology Center of Excellence, Johns Hopkins University, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Hierarchical joint learning: Improving joint parsing and named entity recognition with non-jointly labeled data.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL-2010.</booktitle>
<contexts>
<context position="18169" citStr="Finkel and Manning, 2010" startWordPosition="2909" endWordPosition="2912">e set of conjoined entity+sentiment values (Tables 3 and 4). Our basic model is a linear conditional random field, an undirected graph that represents the conditional distribution p(l, s|w).6 Sentiment towards a named entity may be modeled in a CRF as a se6For the COLL models, this is instead the conditional distributionp(y1w), where entity and sentiment labels are conjoined in one sequence assignment y. quence of random variables for sentiment s connected to named entities l. In all models, entity variables are connected by a factor to their neighbors in sequence, and we include skip-chains (Finkel and Manning, 2010) connecting identical words where at least one is capitalized. Our model strategies include: a pipeline that first learns volitional entities then sentiment directed towards them (PIPE); one that jointly learns volitional entities along with sentiment directed towards them (JOINT); and one that learns volitional entities and targeted sentiment with combined labels (COLL) (Figure 5). Using these models, we explore two primary tasks: (1) the task of detecting whether sentiment is targeted at an entity, which we refer to as targeted subjectivity; and (2) the task of detecting whether positive, ne</context>
</contexts>
<marker>Finkel, Manning, 2010</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2010. Hierarchical joint learning: Improving joint parsing and named entity recognition with non-jointly labeled data. In Proceedings of ACL-2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan B Hooper</author>
</authors>
<title>The syllable in phonological theory.</title>
<date>1976</date>
<journal>Language,</journal>
<volume>48</volume>
<issue>3</issue>
<contexts>
<context position="23149" citStr="Hooper, 1976" startWordPosition="3698" endWordPosition="3699">kelihood, using a bigram language model on the clusters. For our task, we cut clusters at length 3 and length 5, and these serve as rough part-of-speech tags without the need to train additional models. For example, the word hello is tagged as belonging to cluster 011 (length 3) and 01111 (length 5). During development, we found that being able to syllabify the word (break the word into syllables) was a positive indicator of people names, but a negative indicator of organization names. This observation can be approximated automatically using constraints from the sonority sequencing principle (Hooper, 1976; Clements, 1990; Blevins, 1996; Morelli, 2003) on a language’s orthography. This is a phonotactic principle that states that syllables will tend to have a sonority peak, usually a vowel, in the center of the syllable, followed on either side by consonants with decreasing sonority. Although languages may violate this principle, the core idea that a vowel forms the nucleus of a syllable with op9www.m-mitchell.com/code 10For Spanish, we train on a sample of ˜7 million Spanish tweets. For English, we train on the essays (Pennebaker et al., 2007) and Facebook data (Kosinskia et al., 2013) availabl</context>
</contexts>
<marker>Hooper, 1976</marker>
<rawString>Joan B. Hooper. 1976. The syllable in phonological theory. Language, 48(3):525–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of KDD.</booktitle>
<contexts>
<context position="10815" citStr="Hu and Liu (2004)" startWordPosition="1717" endWordPosition="1720">e use syntactic relations to connect an opinion target to an opinion expression. In contrast, we model the expression of sentiment polarity across the sentiment target itself, extracting both the sentiment target and the sentiment expressed towards it within the same span of words. This allows us to use surrounding context to determine sentiment polarity without identifying explicit opinion expressions or relying on a parser to help link expression to target. Most work in targeted sentiment outside the microblogging domain has been in relation to product review mining (e.g., Yi et al. (2003), Hu and Liu (2004), Popescu and Etzioni (2005), Qiu et al. (2011)). Rather than identify named entities (NEs), this work seeks to identify products and their features mentioned in reviews, and classify these for sentiment. Recent work by Qui et al. jointly learns targets and opinion words, and Jakob and Gurevych (2010) use CRFs to extract the targets of opinions, but do not attempt to classify the sentiment toward these targets. To the best of our knowledge, this is the first work to approach targeted sentiment in a low resource setting and to jointly predict NEs and targeted sentiment. 3 Data Twitter Collectio</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xia Hu</author>
<author>Lei Tang</author>
<author>Jiliang Tang</author>
<author>Huan Liu</author>
</authors>
<title>Exploiting social relations for sentiment analysis in microblogging.</title>
<date>2013</date>
<booktitle>In Proceedings of the 6th ACM International Conference on Web Search and Data Mining (WSDM-2013).</booktitle>
<contexts>
<context position="8004" citStr="Hu et al., 2013" startWordPosition="1263" endWordPosition="1266">2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2www.m-mitchell.com/code 3www.sentiment140.com 4www.tweetfeel.com 1644 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sentiment-topic modeling (Saif et al., 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a; Jiang et al., 2011; Tan et al., 2011; Wang et al., 2011; Li et al., 2012; Chen et al., 2012), have mostly focused on topic-dependent a</context>
</contexts>
<marker>Hu, Tang, Tang, Liu, 2013</marker>
<rawString>Xia Hu, Lei Tang, Jiliang Tang, and Huan Liu. 2013. Exploiting social relations for sentiment analysis in microblogging. In Proceedings of the 6th ACM International Conference on Web Search and Data Mining (WSDM-2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niklas Jakob</author>
<author>Iryna Gurevych</author>
</authors>
<title>Extracting opinion targets in a single-and cross-domain setting with conditional random fields.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="11117" citStr="Jakob and Gurevych (2010)" startWordPosition="1766" endWordPosition="1769"> us to use surrounding context to determine sentiment polarity without identifying explicit opinion expressions or relying on a parser to help link expression to target. Most work in targeted sentiment outside the microblogging domain has been in relation to product review mining (e.g., Yi et al. (2003), Hu and Liu (2004), Popescu and Etzioni (2005), Qiu et al. (2011)). Rather than identify named entities (NEs), this work seeks to identify products and their features mentioned in reviews, and classify these for sentiment. Recent work by Qui et al. jointly learns targets and opinion words, and Jakob and Gurevych (2010) use CRFs to extract the targets of opinions, but do not attempt to classify the sentiment toward these targets. To the best of our knowledge, this is the first work to approach targeted sentiment in a low resource setting and to jointly predict NEs and targeted sentiment. 3 Data Twitter Collection We use the Spanish/English Twitter dataset of Etter et al. (2013) to train and test our models. Approximately 30,000 Spanish tweets and 10,000 English were labeled for named entities in BIO encoding: The start of an NE is labeled B{NE} and the rest of the NE is labeled I-{NE}. The NE COUNT NEUTRAL P</context>
</contexts>
<marker>Jakob, Gurevych, 2010</marker>
<rawString>Niklas Jakob and Iryna Gurevych. 2010. Extracting opinion targets in a single-and cross-domain setting with conditional random fields. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Mo Yu</author>
<author>Xiaohua Liu</author>
<author>Tiejun Zhao</author>
</authors>
<title>Target-dependent twitter sentiment classification.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-2011.</booktitle>
<contexts>
<context position="3379" citStr="Jiang et al., 2011" startWordPosition="527" endWordPosition="530">tral (no sentiment) and (3) as positive. However, if we are looking for sentiment toward Tennessee, we would want to identify (1) as positive, and (2) and (3) as neutral. The expression of these and other kinds of sentiment can be understood as involving three items: (1) An experiencer (2) An attitude (3) A target (optionally) Research in sentiment analysis often focuses on (2), predicting overall sentiment polarity (Agarwal et al., 2011; Bora, 2012). Recent work has begun to combine (2) with (3), examining how to automatically predict the sentiment polarity expressed towards a target entity (Jiang et al., 2011; Chen et al., 2012) for a fixed set of targets. This topic-dependent sentiment classification requires that the target entity be 1643 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1643–1654, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics Figure 1: Sentiment expressed across an entity. given, and returns statements expressing sentiment towards the given entity. In this paper, we take a step towards open-domain, targeted sentiment analysis by investigating how to detect both the named entity and the </context>
<context position="7969" citStr="Jiang et al., 2011" startWordPosition="1255" endWordPosition="1258">pective of target (Barbosa and Feng, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2www.m-mitchell.com/code 3www.sentiment140.com 4www.tweetfeel.com 1644 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sentiment-topic modeling (Saif et al., 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a; Jiang et al., 2011; Tan et al., 2011; Wang et al., 2011; Li et al., 2012; Chen et al., 2012), have </context>
<context position="9590" citStr="Jiang et al. (2011)" startWordPosition="1512" endWordPosition="1515">ral approaches have explored applying CRFs or HMMs to extract sentiment and target words from text (Jin and Ho, 2009; Li et al., 2010a). In these approaches, opinion expressions are extracted, and polarity is annotated across the opinion expression. However, as noted by many researchers in sentiment, opinion orientation towards a specific target is often not equal to the orientation of a neighboring opinion expression; and opinion expressions in one context may not be opinion expressions in another (Kim and Hovy, 2006), making open domain approaches particularly challenging. The above work by Jiang et al. (2011) is most similar to our own. They do not use joint learning, but they do incorporate a number of parse-based features designed to capture relationships between sentiment terms and topic references. In our work these relationships are captured by the CRF model, and we compare against their approach in Section 6. Recent work by Yang and Cardie (2013) is similar in spirit to our own, where the identification of opinion holders, opinion targets, and opinion expressions is modeled as a sequence tagging problem using a CRF. However, similar to previous work applying CRFs to extract sentiment, Yang a</context>
<context position="29404" citStr="Jiang et al. (2011)" startWordPosition="4706" endWordPosition="4709">fusing B- labels with I- labels. Subjectivity and Sentiment Table 7 shows results for the isolated task of predicting the presence of sentiment about a volitional entity. In Spanish, the pipeline models (PIPE) perform optimally for subjectivity recall (Subj rec), and significantly above the COLL models (p&lt;.001). Precision and specificity are comparable across models. In English as in Spanish, the collapsed model is particularly poor at subjectivity recall. As discussed in Section 2, the subtask of predicting whether subjectivity is expressed towards an entity is comparable to the main task of Jiang et al. (2011), and so we compare our approach here. The Jiang et al. study is similar to the current study in that they aim to detect targeted sentiment, but it differs from the current study in that they focus exclusively on subjectivity towards five manually selected entities: {Obama, Google, iPad, Lakers, Lady Gaga}. They also evaluate on artificially balanced evaluation data, and evaluate sentiment polarity (positive/negative) separately from subjectivity (has/does not have sentiment). Our dataset includes any entity labeled as PERSON or ORGANIZATION, and is not balanced (most targets have no sentiment</context>
</contexts>
<marker>Jiang, Yu, Liu, Zhao, 2011</marker>
<rawString>Long Jiang, Mo Yu, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter sentiment classification. In Proceedings of ACL-2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Jin</author>
<author>Hung Hay Ho</author>
</authors>
<title>A novel lexicalized hmm-based learning framework for web opinion mining.</title>
<date>2009</date>
<booktitle>Proceedings of ICML</booktitle>
<contexts>
<context position="8450" citStr="Jin and Ho, 2009" startWordPosition="1325" endWordPosition="1328">ior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sentiment-topic modeling (Saif et al., 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a; Jiang et al., 2011; Tan et al., 2011; Wang et al., 2011; Li et al., 2012; Chen et al., 2012), have mostly focused on topic-dependent analysis. In these approaches, messages are collected on a fixed set of topics/targets, such as products or sports teams, and sentiment is learned for the given set. In contrast, we aim to predict sentiment in tweets for any named person or organization. We refer to this task as open domain targeted sentiment analysis. Within topic-dependent sentiment analysis, several approaches have explored applying CRFs or HMMs to extract sentiment and tar</context>
</contexts>
<marker>Jin, Ho, 2009</marker>
<rawString>Wei Jin and Hung Hay Ho. 2009. A novel lexicalized hmm-based learning framework for web opinion mining. Proceedings of ICML 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Identifying and analyzing judgment opinions.</title>
<date>2006</date>
<booktitle>Proceedings of NAACL</booktitle>
<contexts>
<context position="9495" citStr="Kim and Hovy, 2006" startWordPosition="1498" endWordPosition="1501">ask as open domain targeted sentiment analysis. Within topic-dependent sentiment analysis, several approaches have explored applying CRFs or HMMs to extract sentiment and target words from text (Jin and Ho, 2009; Li et al., 2010a). In these approaches, opinion expressions are extracted, and polarity is annotated across the opinion expression. However, as noted by many researchers in sentiment, opinion orientation towards a specific target is often not equal to the orientation of a neighboring opinion expression; and opinion expressions in one context may not be opinion expressions in another (Kim and Hovy, 2006), making open domain approaches particularly challenging. The above work by Jiang et al. (2011) is most similar to our own. They do not use joint learning, but they do incorporate a number of parse-based features designed to capture relationships between sentiment terms and topic references. In our work these relationships are captured by the CRF model, and we compare against their approach in Section 6. Recent work by Yang and Cardie (2013) is similar in spirit to our own, where the identification of opinion holders, opinion targets, and opinion expressions is modeled as a sequence tagging pr</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2006. Identifying and analyzing judgment opinions. Proceedings of NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT.</booktitle>
<contexts>
<context position="22378" citStr="Koo et al., 2008" startWordPosition="3571" endWordPosition="3574">n either direction. Words seen only once are treated as out-of-vocabulary. Surface features and linguistic features are concatenated in groups of two and three to create further features. All algorithms and code that we have developed for feature extraction are available online.9 Because we aim to develop models that do not heavily rely on language-specific resources, we are interested in exploring unsupervised and lightly supervised methods for learning relevant features. Rather than use part-of-speech tags, we therefore use Brown cluster labels as unsupervised word tags (Brown et al., 1992; Koo et al., 2008). Brown clustering is a distributional similarity method that merges pairs of word clusters in the training data10 to create the smallest decrease in corpus likelihood, using a bigram language model on the clusters. For our task, we cut clusters at length 3 and length 5, and these serve as rough part-of-speech tags without the need to train additional models. For example, the word hello is tagged as belonging to cluster 011 (length 3) and 01111 (length 5). During development, we found that being able to syllabify the word (break the word into syllables) was a positive indicator of people names</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of ACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michal Kosinskia</author>
<author>David Stillwell</author>
<author>Thore Graepel</author>
</authors>
<title>Private trains and attributes are predictable from digital records of human behavior.</title>
<date>2013</date>
<booktitle>Proc. of the National Academy of Sciences of the USA,</booktitle>
<volume>110</volume>
<issue>5</issue>
<contexts>
<context position="23740" citStr="Kosinskia et al., 2013" startWordPosition="3791" endWordPosition="3794">quencing principle (Hooper, 1976; Clements, 1990; Blevins, 1996; Morelli, 2003) on a language’s orthography. This is a phonotactic principle that states that syllables will tend to have a sonority peak, usually a vowel, in the center of the syllable, followed on either side by consonants with decreasing sonority. Although languages may violate this principle, the core idea that a vowel forms the nucleus of a syllable with op9www.m-mitchell.com/code 10For Spanish, we train on a sample of ˜7 million Spanish tweets. For English, we train on the essays (Pennebaker et al., 2007) and Facebook data (Kosinskia et al., 2013) available from ICWSM 2013. 1648 tional consonants before (the onset) and after (the coda) can be used to begin to automatically learn syllable structure.11 We learn this in an unsupervised way, using the most frequent (seen more than 1,000 times) word-initial non-vowel sequences from the Brown cluster data as allowable syllable onset consonants. Similarly, the most frequent word-final non-vowel sequences are learned as possible syllable codas. For each word, we then attempt to segment syllables using the learned onsets and codas around each vowel. If a word cannot be syllabified, it is often </context>
</contexts>
<marker>Kosinskia, Stillwell, Graepel, 2013</marker>
<rawString>Michal Kosinskia, David Stillwell, and Thore Graepel. 2013. Private trains and attributes are predictable from digital records of human behavior. Proc. of the National Academy of Sciences of the USA, 110(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efthymios Kouloumpis</author>
<author>Theresa Wilson</author>
<author>Johanna Moore</author>
</authors>
<title>Twitter sentiment analysis: The good the bad and the OMG!</title>
<date>2011</date>
<booktitle>In Proceedings of ICWSM2011.</booktitle>
<contexts>
<context position="7486" citStr="Kouloumpis et al., 2011" startWordPosition="1187" endWordPosition="1190">raphy of the language. All tools and code used for this research are released with this paper.2 2 Related Work As the scale of social media has grown, using sources such as Twitter to mine public sentiment has become increasingly promising. Commercial systems include Sentiment1403 (products and brands) and tweetfeel4 (suggests searching for popular movies, celebrities and companies). The majority of academic research has focused on supervised classification of message sentiment irrespective of target (Barbosa and Feng, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2www.m-mitchell.com/code 3www.sentiment140.com 4www.tweetfeel.com 1644 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); u</context>
</contexts>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. 2011. Twitter sentiment analysis: The good the bad and the OMG! In Proceedings of ICWSM2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML-2001.</booktitle>
<contexts>
<context position="4635" citStr="Lafferty et al., 2001" startWordPosition="727" endWordPosition="730">observe that sentiment expressed towards a target entity may be possible to learn in a graphical model along the span of the entity itself: Similar to how named entity recognition (NER) learns labels along the span of each word in an entity name, sentiment may be expressed along the entity as well. A small example is shown in Figure 1. We focus on people and organizations (volitional named entities), which are the primary targets of sentiment in our microblog data (see Table 1). Both NER and opinion expression extraction have achieved impressive results using conditional random fields (CRFs) (Lafferty et al., 2001) to define the conditional probability of entity categories (McCallum and Li, 2003; Choi et al., 2006; Yang and Cardie, 2013). We develop such models to jointly predict the NE and the sentiment expressed towards it using minimum risk training (Stoyanov and Eisner, 2012). We learn our models on informal Spanish and English language taken from the social network Twitter,1 where the language variety makes NLP particularly challenging (see Figure 2). Our ultimate goal is to develop models that will be useful for low resource languages, where a sentiment lexicon may be known or bootstrapped, but mo</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangtao Li</author>
<author>Chao Han</author>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
<author>Ying-Ju Xia</author>
<author>Shu Zhang</author>
<author>Hao Yu</author>
</authors>
<title>Structure-aware review mining and summarization.</title>
<date>2010</date>
<booktitle>Proceedings of Coling</booktitle>
<contexts>
<context position="8081" citStr="Li et al., 2010" startWordPosition="1274" endWordPosition="1277">loumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2www.m-mitchell.com/code 3www.sentiment140.com 4www.tweetfeel.com 1644 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sentiment-topic modeling (Saif et al., 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a; Jiang et al., 2011; Tan et al., 2011; Wang et al., 2011; Li et al., 2012; Chen et al., 2012), have mostly focused on topic-dependent analysis. In these approaches, messages are collected on a fixed set of topics</context>
</contexts>
<marker>Li, Han, Huang, Zhu, Xia, Zhang, Yu, 2010</marker>
<rawString>Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu, Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010a. Structure-aware review mining and summarization. Proceedings of Coling 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangxia Li</author>
<author>Steven CH Hoi</author>
<author>Kuiyu Chang</author>
<author>Ramesh Jain</author>
</authors>
<title>Micro-blogging sentiment detection by collaborative online learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ICDM-2010.</booktitle>
<contexts>
<context position="8081" citStr="Li et al., 2010" startWordPosition="1274" endWordPosition="1277">loumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2www.m-mitchell.com/code 3www.sentiment140.com 4www.tweetfeel.com 1644 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sentiment-topic modeling (Saif et al., 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a; Jiang et al., 2011; Tan et al., 2011; Wang et al., 2011; Li et al., 2012; Chen et al., 2012), have mostly focused on topic-dependent analysis. In these approaches, messages are collected on a fixed set of topics</context>
</contexts>
<marker>Li, Hoi, Chang, Jain, 2010</marker>
<rawString>Guangxia Li, Steven CH Hoi, Kuiyu Chang, and Ramesh Jain. 2010b. Micro-blogging sentiment detection by collaborative online learning. In Proceedings of ICDM-2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Li</author>
<author>Yu Chen</author>
<author>Heng Ji</author>
<author>Smaranda Muresan</author>
<author>Dequan Zheng</author>
</authors>
<title>Combining social cognitive theories with linguistic features for multi-genre sentiment analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the Pacific Asia Conference on Language, Information and Computation (PACLIC-2012).</booktitle>
<contexts>
<context position="7986" citStr="Li et al., 2012" startWordPosition="1259" endWordPosition="1262">arbosa and Feng, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2www.m-mitchell.com/code 3www.sentiment140.com 4www.tweetfeel.com 1644 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sentiment-topic modeling (Saif et al., 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a; Jiang et al., 2011; Tan et al., 2011; Wang et al., 2011; Li et al., 2012; Chen et al., 2012), have mostly focused on</context>
</contexts>
<marker>Li, Chen, Ji, Muresan, Zheng, 2012</marker>
<rawString>Hao Li, Yu Chen, Heng Ji, Smaranda Muresan, and Dequan Zheng. 2012. Combining social cognitive theories with linguistic features for multi-genre sentiment analysis. In Proceedings of the Pacific Asia Conference on Language, Information and Computation (PACLIC-2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Wei Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction, and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003.</booktitle>
<contexts>
<context position="4717" citStr="McCallum and Li, 2003" startWordPosition="739" endWordPosition="743">n a graphical model along the span of the entity itself: Similar to how named entity recognition (NER) learns labels along the span of each word in an entity name, sentiment may be expressed along the entity as well. A small example is shown in Figure 1. We focus on people and organizations (volitional named entities), which are the primary targets of sentiment in our microblog data (see Table 1). Both NER and opinion expression extraction have achieved impressive results using conditional random fields (CRFs) (Lafferty et al., 2001) to define the conditional probability of entity categories (McCallum and Li, 2003; Choi et al., 2006; Yang and Cardie, 2013). We develop such models to jointly predict the NE and the sentiment expressed towards it using minimum risk training (Stoyanov and Eisner, 2012). We learn our models on informal Spanish and English language taken from the social network Twitter,1 where the language variety makes NLP particularly challenging (see Figure 2). Our ultimate goal is to develop models that will be useful for low resource languages, where a sentiment lexicon may be known or bootstrapped, but more sophisticated linguistic tools may not be readily available. We therefore do no</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>Andrew McCallum and Wei Li. 2003. Early results for named entity recognition with conditional random fields, feature induction, and web-enhanced lexicons. In Proceedings of CoNLL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frida Morelli</author>
</authors>
<title>The relative harmony of /s+stop/ onsets: Obstruent clusters and the sonority sequencing principle.</title>
<date>2003</date>
<booktitle>The syllable in optimality theory,</booktitle>
<pages>356--371</pages>
<editor>In C. Fery and R. van de Vijver, editors,</editor>
<publisher>CUP,</publisher>
<location>New York.</location>
<contexts>
<context position="23196" citStr="Morelli, 2003" startWordPosition="3704" endWordPosition="3705">e clusters. For our task, we cut clusters at length 3 and length 5, and these serve as rough part-of-speech tags without the need to train additional models. For example, the word hello is tagged as belonging to cluster 011 (length 3) and 01111 (length 5). During development, we found that being able to syllabify the word (break the word into syllables) was a positive indicator of people names, but a negative indicator of organization names. This observation can be approximated automatically using constraints from the sonority sequencing principle (Hooper, 1976; Clements, 1990; Blevins, 1996; Morelli, 2003) on a language’s orthography. This is a phonotactic principle that states that syllables will tend to have a sonority peak, usually a vowel, in the center of the syllable, followed on either side by consonants with decreasing sonority. Although languages may violate this principle, the core idea that a vowel forms the nucleus of a syllable with op9www.m-mitchell.com/code 10For Spanish, we train on a sample of ˜7 million Spanish tweets. For English, we train on the essays (Pennebaker et al., 2007) and Facebook data (Kosinskia et al., 2013) available from ICWSM 2013. 1648 tional consonants befor</context>
</contexts>
<marker>Morelli, 2003</marker>
<rawString>Frida Morelli. 2003. The relative harmony of /s+stop/ onsets: Obstruent clusters and the sonority sequencing principle. In C. Fery and R. van de Vijver, editors, The syllable in optimality theory, pages 356–371. CUP, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC-2010.</booktitle>
<contexts>
<context position="7416" citStr="Pak and Paroubek, 2010" startWordPosition="1175" endWordPosition="1178">nd a method that automatically syllabifies a word based on the orthography of the language. All tools and code used for this research are released with this paper.2 2 Related Work As the scale of social media has grown, using sources such as Twitter to mine public sentiment has become increasingly promising. Commercial systems include Sentiment1403 (products and brands) and tweetfeel4 (suggests searching for popular movies, celebrities and companies). The majority of academic research has focused on supervised classification of message sentiment irrespective of target (Barbosa and Feng, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2www.m-mitchell.com/code 3www.sentiment140.com 4www.tweetfeel.com 1644 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapt</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Proceedings of LREC-2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Pennebaker</author>
<author>Roger J Booth</author>
<author>Martha E Francis</author>
</authors>
<title>Linguistic inquiry and word count: Liwc2007,</title>
<date>2007</date>
<note>operator’s manual.</note>
<contexts>
<context position="23697" citStr="Pennebaker et al., 2007" startWordPosition="3784" endWordPosition="3787">cally using constraints from the sonority sequencing principle (Hooper, 1976; Clements, 1990; Blevins, 1996; Morelli, 2003) on a language’s orthography. This is a phonotactic principle that states that syllables will tend to have a sonority peak, usually a vowel, in the center of the syllable, followed on either side by consonants with decreasing sonority. Although languages may violate this principle, the core idea that a vowel forms the nucleus of a syllable with op9www.m-mitchell.com/code 10For Spanish, we train on a sample of ˜7 million Spanish tweets. For English, we train on the essays (Pennebaker et al., 2007) and Facebook data (Kosinskia et al., 2013) available from ICWSM 2013. 1648 tional consonants before (the onset) and after (the coda) can be used to begin to automatically learn syllable structure.11 We learn this in an unsupervised way, using the most frequent (seen more than 1,000 times) word-initial non-vowel sequences from the Brown cluster data as allowable syllable onset consonants. Similarly, the most frequent word-final non-vowel sequences are learned as possible syllable codas. For each word, we then attempt to segment syllables using the learned onsets and codas around each vowel. If</context>
</contexts>
<marker>Pennebaker, Booth, Francis, 2007</marker>
<rawString>James W. Pennebaker, Roger J. Booth, and Martha E. Francis. 2007. Linguistic inquiry and word count: Liwc2007, operator’s manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronica Perez-Rosas</author>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
</authors>
<title>Learning sentiment lexicons in spanish.</title>
<date>2012</date>
<booktitle>Proceedings of the Conference on Language Resources and Evaluations (LREC</booktitle>
<contexts>
<context position="13526" citStr="Perez-Rosas et al. (2012)" startWordPosition="2174" endWordPosition="2177">ies 12,296 manually and semi-automatically produced subjective terms along with their polarity. For the second lexicon, we use SentiWordNet 3.0 (Baccianella et al., 2010), which assigns positive and negative polarity scores to WordNet synsets. We use the majority polarity of all words with a subjectivity score above 0.5. For Spanish, the first lexicon is obtained from Volkova et al. (2013), who automatically translated strongly subjective terms from the MPQA lexicon (Wilson et al., 2005) into Spanish. The resulting Spanish lexicon contains about 65K words. The second lexicon is available from Perez-Rosas et al. (2012). This contains approximately 1000 sentiment-bearing words collected leveraging manual resources and 2000 collected leveraging automatic resources. Annotation To collect sentiment labels, we use crowdsourcing through Amazon’s Mechanical Turk.5 Annotators (“Turkers”) were shown six tweets at a time, each with a single highlighted named entity. Turkers were instructed to (1) select the sentiment being expressed towards the entity (positive, negative, or no sentiment); and (2) rate their level of confidence in their selection. Following best practices on collecting language data with Mechanical T</context>
</contexts>
<marker>Perez-Rosas, Banea, Mihalcea, 2012</marker>
<rawString>Veronica Perez-Rosas, Carmen Banea, and Rada Mihalcea. 2012. Learning sentiment lexicons in spanish. Proceedings of the Conference on Language Resources and Evaluations (LREC 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of Coling:ACL-2006.</booktitle>
<contexts>
<context position="24477" citStr="Petrov et al., 2006" startWordPosition="3909" endWordPosition="3912">tomatically learn syllable structure.11 We learn this in an unsupervised way, using the most frequent (seen more than 1,000 times) word-initial non-vowel sequences from the Brown cluster data as allowable syllable onset consonants. Similarly, the most frequent word-final non-vowel sequences are learned as possible syllable codas. For each word, we then attempt to segment syllables using the learned onsets and codas around each vowel. If a word cannot be syllabified, it is often an initialism (e.g., CND, lsat). We follow the approach from the out-ofvocabulary assignment in the Berkeley parser (Petrov et al., 2006) to encode common surface patterns such as capitalization and lexical patterns such as verb endings as a single feature for words we have seen once or less. We also use the Jerboa toolkit (Van Durme, 2012) to extract further language-independent features from the data, such as features for emoticons and binning for repeated characters (like !!!). In addition, we include features for whether the word is three or four letters, which is often used for acronyms and initialisms in several languages (including Spanish and English); whether the word is neighbored by a punctuation mark; word identity;</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of Coling:ACL-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT:EMNLP-2005.</booktitle>
<contexts>
<context position="10843" citStr="Popescu and Etzioni (2005)" startWordPosition="1721" endWordPosition="1724">ations to connect an opinion target to an opinion expression. In contrast, we model the expression of sentiment polarity across the sentiment target itself, extracting both the sentiment target and the sentiment expressed towards it within the same span of words. This allows us to use surrounding context to determine sentiment polarity without identifying explicit opinion expressions or relying on a parser to help link expression to target. Most work in targeted sentiment outside the microblogging domain has been in relation to product review mining (e.g., Yi et al. (2003), Hu and Liu (2004), Popescu and Etzioni (2005), Qiu et al. (2011)). Rather than identify named entities (NEs), this work seeks to identify products and their features mentioned in reviews, and classify these for sentiment. Recent work by Qui et al. jointly learns targets and opinion words, and Jakob and Gurevych (2010) use CRFs to extract the targets of opinions, but do not attempt to classify the sentiment toward these targets. To the best of our knowledge, this is the first work to approach targeted sentiment in a low resource setting and to jointly predict NEs and targeted sentiment. 3 Data Twitter Collection We use the Spanish/English</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of HLT:EMNLP-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guang Qiu</author>
<author>Bing Liu</author>
<author>Jiajun Bu</author>
<author>Chun Chen</author>
</authors>
<title>Opinion word expansion and target extraction through double propagation.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="10862" citStr="Qiu et al. (2011)" startWordPosition="1725" endWordPosition="1728"> target to an opinion expression. In contrast, we model the expression of sentiment polarity across the sentiment target itself, extracting both the sentiment target and the sentiment expressed towards it within the same span of words. This allows us to use surrounding context to determine sentiment polarity without identifying explicit opinion expressions or relying on a parser to help link expression to target. Most work in targeted sentiment outside the microblogging domain has been in relation to product review mining (e.g., Yi et al. (2003), Hu and Liu (2004), Popescu and Etzioni (2005), Qiu et al. (2011)). Rather than identify named entities (NEs), this work seeks to identify products and their features mentioned in reviews, and classify these for sentiment. Recent work by Qui et al. jointly learns targets and opinion words, and Jakob and Gurevych (2010) use CRFs to extract the targets of opinions, but do not attempt to classify the sentiment toward these targets. To the best of our knowledge, this is the first work to approach targeted sentiment in a low resource setting and to jointly predict NEs and targeted sentiment. 3 Data Twitter Collection We use the Spanish/English Twitter dataset of</context>
</contexts>
<marker>Qiu, Liu, Bu, Chen, 2011</marker>
<rawString>Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through double propagation. Computational Linguistics, 37(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Saif</author>
<author>Yulan He</author>
<author>Harith Alani</author>
</authors>
<title>Alleviating data sparsity for twitter sentiment analysis.</title>
<date>2012</date>
<booktitle>Proceedings of the WWW Workshop on Making Sense of Microposts (# MSM2012).</booktitle>
<contexts>
<context position="8149" citStr="Saif et al., 2012" startWordPosition="1283" endWordPosition="1286">ollected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2www.m-mitchell.com/code 3www.sentiment140.com 4www.tweetfeel.com 1644 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sentiment-topic modeling (Saif et al., 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a; Jiang et al., 2011; Tan et al., 2011; Wang et al., 2011; Li et al., 2012; Chen et al., 2012), have mostly focused on topic-dependent analysis. In these approaches, messages are collected on a fixed set of topics/targets, such as products or sports teams, and sentiment is learned</context>
</contexts>
<marker>Saif, He, Alani, 2012</marker>
<rawString>Hassan Saif, Yulan He, and Harith Alani. 2012. Alleviating data sparsity for twitter sentiment analysis. Proceedings of the WWW Workshop on Making Sense of Microposts (# MSM2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Speriosu</author>
<author>Nikita Sudan</author>
<author>Sid Upadhyay</author>
<author>Jason Baldridge</author>
</authors>
<title>Twitter polarity classification with label propagation over lexical links and the follower graph.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP-2011 Workshop on Unsupervised Learning in NLP.</booktitle>
<contexts>
<context position="7903" citStr="Speriosu et al., 2011" startWordPosition="1242" endWordPosition="1245">h has focused on supervised classification of message sentiment irrespective of target (Barbosa and Feng, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2www.m-mitchell.com/code 3www.sentiment140.com 4www.tweetfeel.com 1644 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sentiment-topic modeling (Saif et al., 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a; Jiang et al., 2011; Tan et al., 2</context>
</contexts>
<marker>Speriosu, Sudan, Upadhyay, Baldridge, 2011</marker>
<rawString>Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Jason Baldridge. 2011. Twitter polarity classification with label propagation over lexical links and the follower graph. In Proceedings of the EMNLP-2011 Workshop on Unsupervised Learning in NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Jason Eisner</author>
</authors>
<title>Minimumrisk training of approximate crf-based nlp systems.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL:HLT-2012.</booktitle>
<contexts>
<context position="4905" citStr="Stoyanov and Eisner, 2012" startWordPosition="771" endWordPosition="775">pressed along the entity as well. A small example is shown in Figure 1. We focus on people and organizations (volitional named entities), which are the primary targets of sentiment in our microblog data (see Table 1). Both NER and opinion expression extraction have achieved impressive results using conditional random fields (CRFs) (Lafferty et al., 2001) to define the conditional probability of entity categories (McCallum and Li, 2003; Choi et al., 2006; Yang and Cardie, 2013). We develop such models to jointly predict the NE and the sentiment expressed towards it using minimum risk training (Stoyanov and Eisner, 2012). We learn our models on informal Spanish and English language taken from the social network Twitter,1 where the language variety makes NLP particularly challenging (see Figure 2). Our ultimate goal is to develop models that will be useful for low resource languages, where a sentiment lexicon may be known or bootstrapped, but more sophisticated linguistic tools may not be readily available. We therefore do not rely on an external part-of-speech tagger or parser, which are often used for features in fine-grained sentiment analysis; such tools are not available in many languages, and if they are</context>
<context position="21112" citStr="Stoyanov and Eisner, 2012" startWordPosition="3380" endWordPosition="3383">OLL) Figure 5: Example CRFs for targeted subjectivity with observed variables (dark nodes), predicted variables (white nodes) and hidden variables (light grey nodes). 5 Training Minimum-Risk CRF Training We use the ERMA system (Stoyanov et al., 2011) to learn our models.8 ERMA (Empirical Risk Minimization under Approximations) learns parameters to minimize loss on the training data. Predicting NE labels using a linear-chain CRF trained with empirical risk minimization has been shown to result in a statistically significant improvement over the common approach of maximum likelihood estimation (Stoyanov and Eisner, 2012). All models are trained to optimize 7We found that learning the VOLITIONAL categories during training rather than maintaining beliefs about separate named entities during inference (ORGANIZATION, PERSON) and then post-processing to VOLITIONAL leads to slightly better accuracy. 8sites.google.com/site/ermasoftware log likelihood using 20 iterations of stochastic gradient descent, and a maximum of 100 iterations of belief propagation to compute the marginals for each example. Features Features of the models are shown in Table 5. For an observed word, features are extracted for the word itself as</context>
</contexts>
<marker>Stoyanov, Eisner, 2012</marker>
<rawString>Veselin Stoyanov and Jason Eisner. 2012. Minimumrisk training of approximate crf-based nlp systems. In Proceedings of NAACL:HLT-2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Alexander Ropson</author>
<author>Jason Eisner</author>
</authors>
<title>Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure. In AIStats.</title>
<date>2011</date>
<contexts>
<context position="20736" citStr="Stoyanov et al., 2011" startWordPosition="3325" endWordPosition="3328">ssed towards volitional named entities.7 The collapsed models predict volitional labels and targeted sentiment as combined categories. The COLL and PIPE models are considerably faster than JOINT models, where exact inference is intractable. 1. PIPELINE MODEL (PIPE) Step 1: Volitional Named Step 2: Sentiment Entity Recognition 2. JOINT MODEL 3. COLLAPSED MODEL (JOINT) (COLL) Figure 5: Example CRFs for targeted subjectivity with observed variables (dark nodes), predicted variables (white nodes) and hidden variables (light grey nodes). 5 Training Minimum-Risk CRF Training We use the ERMA system (Stoyanov et al., 2011) to learn our models.8 ERMA (Empirical Risk Minimization under Approximations) learns parameters to minimize loss on the training data. Predicting NE labels using a linear-chain CRF trained with empirical risk minimization has been shown to result in a statistically significant improvement over the common approach of maximum likelihood estimation (Stoyanov and Eisner, 2012). All models are trained to optimize 7We found that learning the VOLITIONAL categories during training rather than maintaining beliefs about separate named entities during inference (ORGANIZATION, PERSON) and then post-proce</context>
</contexts>
<marker>Stoyanov, Ropson, Eisner, 2011</marker>
<rawString>Veselin Stoyanov, Alexander Ropson, and Jason Eisner. 2011. Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure. In AIStats.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenhao Tan</author>
<author>Lillian Lee</author>
<author>Jie Tang</author>
<author>Long Jiang</author>
<author>Ming Zhou</author>
<author>Ping Li</author>
</authors>
<title>User-level sentiment analysis incorporating social networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the KDD-2011.</booktitle>
<contexts>
<context position="7921" citStr="Tan et al., 2011" startWordPosition="1246" endWordPosition="1249">ised classification of message sentiment irrespective of target (Barbosa and Feng, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Kouloumpis et al., 2011; Agarwal et al., 2011). Large datasets are collected for this work by leveraging the sentiment inherent in emoticons (e.g., smilies and frownies) and/or select Twitter hashtags (e.g., #bestdayever, #fail), resulting in noisy collec2www.m-mitchell.com/code 3www.sentiment140.com 4www.tweetfeel.com 1644 tions appropriate for initial exploration. Prior work includes: the use of a social network (Speriosu et al., 2011; Tan et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sentiment-topic modeling (Saif et al., 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a; Jiang et al., 2011; Tan et al., 2011; Wang et al., </context>
</contexts>
<marker>Tan, Lee, Tang, Jiang, Zhou, Li, 2011</marker>
<rawString>Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming Zhou, and Ping Li. 2011. User-level sentiment analysis incorporating social networks. In Proceedings of the KDD-2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
</authors>
<title>Jerboa: A toolkit for randomized and streaming algorithms.</title>
<date>2012</date>
<tech>Technical report,</tech>
<institution>Human Language Technology Center of Excellence, Johns Hopkins University.</institution>
<marker>Van Durme, 2012</marker>
<rawString>Benjamin Van Durme. 2012. Jerboa: A toolkit for randomized and streaming algorithms. Technical report, Human Language Technology Center of Excellence, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svitlana Volkova</author>
<author>Theresa Wilson</author>
<author>David Yarowsky</author>
</authors>
<title>Exploring sentiment in social media: Bootstrapping subjectivity clues from multilingual twitter streams.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="13293" citStr="Volkova et al. (2013)" startWordPosition="2137" endWordPosition="2140">entities and 2,350 English tweets contained a total of 3,577 volitional entities. Sentiment Lexicons We use two sentiment lexicon sources in each language. For English, we use the MPQA lexicon (Wilson et al., 2005), which identifies 12,296 manually and semi-automatically produced subjective terms along with their polarity. For the second lexicon, we use SentiWordNet 3.0 (Baccianella et al., 2010), which assigns positive and negative polarity scores to WordNet synsets. We use the majority polarity of all words with a subjectivity score above 0.5. For Spanish, the first lexicon is obtained from Volkova et al. (2013), who automatically translated strongly subjective terms from the MPQA lexicon (Wilson et al., 2005) into Spanish. The resulting Spanish lexicon contains about 65K words. The second lexicon is available from Perez-Rosas et al. (2012). This contains approximately 1000 sentiment-bearing words collected leveraging manual resources and 2000 collected leveraging automatic resources. Annotation To collect sentiment labels, we use crowdsourcing through Amazon’s Mechanical Turk.5 Annotators (“Turkers”) were shown six tweets at a time, each with a single highlighted named entity. Turkers were instructe</context>
</contexts>
<marker>Volkova, Wilson, Yarowsky, 2013</marker>
<rawString>Svitlana Volkova, Theresa Wilson, and David Yarowsky. 2013. Exploring sentiment in social media: Bootstrapping subjectivity clues from multilingual twitter streams. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaolong Wang</author>
<author>Furu Wei</author>
<author>Xiaohua Liu</author>
<author>Ming Zhou</author>
<author>Ming Zhang</author>
</authors>
<title>Topic sentiment analysis in Twitter: A graph-based hashtag sentiment classification approach.</title>
<date>2011</date>
<booktitle>In Proceedings of CIKM-2011.</booktitle>
<contexts>
<context position="8525" citStr="Wang et al., 2011" startWordPosition="1341" endWordPosition="1344"> et al., 2011; Calais Guerra et al., 2011; Jiang et al., 2011; Li et al., 2012; Hu et al., 2013); user-adapted models based on collaborative online-learning (Li et al., 2010b); unsupervised, joint sentiment-topic modeling (Saif et al., 2012); tracking changing sentiment during debates (Diakopoulos and Shamma, 2010); and how orthographic conventions such as word-lengthening can be used to adapt a Twitter-specific sentiment lexicon (Brody and Diakopoulos, 2011). Efforts in targeted sentiment (Bermingham and Smeaton, 2010; Jin and Ho, 2009; Li et al., 2010a; Jiang et al., 2011; Tan et al., 2011; Wang et al., 2011; Li et al., 2012; Chen et al., 2012), have mostly focused on topic-dependent analysis. In these approaches, messages are collected on a fixed set of topics/targets, such as products or sports teams, and sentiment is learned for the given set. In contrast, we aim to predict sentiment in tweets for any named person or organization. We refer to this task as open domain targeted sentiment analysis. Within topic-dependent sentiment analysis, several approaches have explored applying CRFs or HMMs to extract sentiment and target words from text (Jin and Ho, 2009; Li et al., 2010a). In these approach</context>
</contexts>
<marker>Wang, Wei, Liu, Zhou, Zhang, 2011</marker>
<rawString>Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou, and Ming Zhang. 2011. Topic sentiment analysis in Twitter: A graph-based hashtag sentiment classification approach. In Proceedings of CIKM-2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP.</booktitle>
<contexts>
<context position="12886" citStr="Wilson et al., 2005" startWordPosition="2076" endWordPosition="2079">iment. 1645 full set of NE categories are shown in Table 1. For example, the sequence “Mark Twain” would be labeled B-PERSON, I-PERSON. We are interested in both PERSON and ORGANIZATION entities, which make up the majority of named entities in this data, and we evaluate these using the more general entity category VOLITIONAL. Removing retweets, 7,105 Spanish tweets contained a total of 9,870 volitional entities and 2,350 English tweets contained a total of 3,577 volitional entities. Sentiment Lexicons We use two sentiment lexicon sources in each language. For English, we use the MPQA lexicon (Wilson et al., 2005), which identifies 12,296 manually and semi-automatically produced subjective terms along with their polarity. For the second lexicon, we use SentiWordNet 3.0 (Baccianella et al., 2010), which assigns positive and negative polarity scores to WordNet synsets. We use the majority polarity of all words with a subjectivity score above 0.5. For Spanish, the first lexicon is obtained from Volkova et al. (2013), who automatically translated strongly subjective terms from the MPQA lexicon (Wilson et al., 2005) into Spanish. The resulting Spanish lexicon contains about 65K words. The second lexicon is </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffman</author>
</authors>
<title>Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="15892" citStr="Wilson et al., 2009" startWordPosition="2536" endWordPosition="2539">f sentiment for the named entities annotated by Turkers is shown in Figure 4. Neutral (no targeted sentiment) dominates, followed by positive sentiment for both organizations and people. As shown in Table 2, common disagreements were over whether or not there was targeted positive sentiment, and whether or not there was targeted negative sentiment. This is in line with previous research showing that distinguishing positive sentiment from no sentiment (and distinguishing negative sentiment from no sentiment) is often more challenging than distinguishing between positive and negative sentiment (Wilson et al., 2009). Indeed, we see that it was more common for annotators to disagree than to agree on targeted sentiment, particularly for negative targeted sentiment, where more instances had NEUTRAL/NEGATIVE disagreement than NEGATIVE three-way agreement. Frequency in Tweets 0 500 1000 1500 2000 2500 Minority 1646 Figure 3: Example Tweet shown to Turkers. Variable Possible values Sentiment (s) NOT-TARG, SENT-TARG (PIPE &amp; JOINT models) Named Entity (l) O, B-VOLITIONAL, I-VOLITIONAL (PIPE &amp; JOINT models) Combined Sent/NE (y) O, B+NOT-TARG, I+NOT-TARG (COLL models) B+SENT-TARG, I+SENT-TARG Table 3: Possible val</context>
</contexts>
<marker>Wilson, Wiebe, Hoffman, 2009</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffman. 2009. Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis. Computational Linguistics, 35(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Claire Cardie</author>
</authors>
<title>Joint inference for fine-grained opinion extraction.</title>
<date>2013</date>
<booktitle>Proceedings of ACL</booktitle>
<contexts>
<context position="4760" citStr="Yang and Cardie, 2013" startWordPosition="748" endWordPosition="751">entity itself: Similar to how named entity recognition (NER) learns labels along the span of each word in an entity name, sentiment may be expressed along the entity as well. A small example is shown in Figure 1. We focus on people and organizations (volitional named entities), which are the primary targets of sentiment in our microblog data (see Table 1). Both NER and opinion expression extraction have achieved impressive results using conditional random fields (CRFs) (Lafferty et al., 2001) to define the conditional probability of entity categories (McCallum and Li, 2003; Choi et al., 2006; Yang and Cardie, 2013). We develop such models to jointly predict the NE and the sentiment expressed towards it using minimum risk training (Stoyanov and Eisner, 2012). We learn our models on informal Spanish and English language taken from the social network Twitter,1 where the language variety makes NLP particularly challenging (see Figure 2). Our ultimate goal is to develop models that will be useful for low resource languages, where a sentiment lexicon may be known or bootstrapped, but more sophisticated linguistic tools may not be readily available. We therefore do not rely on an external part-of-speech tagger</context>
<context position="9940" citStr="Yang and Cardie (2013)" startWordPosition="1572" endWordPosition="1575"> is often not equal to the orientation of a neighboring opinion expression; and opinion expressions in one context may not be opinion expressions in another (Kim and Hovy, 2006), making open domain approaches particularly challenging. The above work by Jiang et al. (2011) is most similar to our own. They do not use joint learning, but they do incorporate a number of parse-based features designed to capture relationships between sentiment terms and topic references. In our work these relationships are captured by the CRF model, and we compare against their approach in Section 6. Recent work by Yang and Cardie (2013) is similar in spirit to our own, where the identification of opinion holders, opinion targets, and opinion expressions is modeled as a sequence tagging problem using a CRF. However, similar to previous work applying CRFs to extract sentiment, Yang and Cardie use syntactic relations to connect an opinion target to an opinion expression. In contrast, we model the expression of sentiment polarity across the sentiment target itself, extracting both the sentiment target and the sentiment expressed towards it within the same span of words. This allows us to use surrounding context to determine sent</context>
</contexts>
<marker>Yang, Cardie, 2013</marker>
<rawString>Bishan Yang and Claire Cardie. 2013. Joint inference for fine-grained opinion extraction. Proceedings of ACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeonghee Yi</author>
<author>Tetsuya Nasukawa</author>
<author>Razvan Bunescu</author>
<author>Wayne Niblack</author>
</authors>
<title>Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques.</title>
<date>2003</date>
<booktitle>In Proceedings of ICDM-2003.</booktitle>
<contexts>
<context position="10796" citStr="Yi et al. (2003)" startWordPosition="1713" endWordPosition="1716">nt, Yang and Cardie use syntactic relations to connect an opinion target to an opinion expression. In contrast, we model the expression of sentiment polarity across the sentiment target itself, extracting both the sentiment target and the sentiment expressed towards it within the same span of words. This allows us to use surrounding context to determine sentiment polarity without identifying explicit opinion expressions or relying on a parser to help link expression to target. Most work in targeted sentiment outside the microblogging domain has been in relation to product review mining (e.g., Yi et al. (2003), Hu and Liu (2004), Popescu and Etzioni (2005), Qiu et al. (2011)). Rather than identify named entities (NEs), this work seeks to identify products and their features mentioned in reviews, and classify these for sentiment. Recent work by Qui et al. jointly learns targets and opinion words, and Jakob and Gurevych (2010) use CRFs to extract the targets of opinions, but do not attempt to classify the sentiment toward these targets. To the best of our knowledge, this is the first work to approach targeted sentiment in a low resource setting and to jointly predict NEs and targeted sentiment. 3 Dat</context>
</contexts>
<marker>Yi, Nasukawa, Bunescu, Niblack, 2003</marker>
<rawString>Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and Wayne Niblack. 2003. Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques. In Proceedings of ICDM-2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9946045">
Exploring Representations from Unlabeled Data with Co-training
for Chinese Word Segmentation
</title>
<author confidence="0.990462">
Longkai Zhang Houfeng Wang* Xu Sun Mairgup Mansur
</author>
<affiliation confidence="0.826471">
Key Laboratory of Computational Linguistics (Peking University) Ministry of Education, China
</affiliation>
<email confidence="0.979916">
zhlongk@qq.com, wanghf@pku.edu.cn, xusun@pku.edu.cn, mairgup@gmail.com,
</email>
<sectionHeader confidence="0.99539" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999269523809524">
Nowadays supervised sequence labeling
models can reach competitive performance
on the task of Chinese word segmenta-
tion. However, the ability of these mod-
els is restricted by the availability of an-
notated data and the design of features.
We propose a scalable semi-supervised fea-
ture engineering approach. In contrast
to previous works using pre-defined task-
specific features with fixed values, we dy-
namically extract representations of label
distributions from both an in-domain cor-
pus and an out-of-domain corpus. We
update the representation values with a
semi-supervised approach. Experiments
on the benchmark datasets show that our
approach achieve good results and reach
an f-score of 0.961. The feature engineer-
ing approach proposed here is a general
iterative semi-supervised method and not
limited to the word segmentation task.
</bodyText>
<sectionHeader confidence="0.998732" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.984569416666667">
Chinese is a language without natural word
delimiters. Therefore, Chinese Word Segmen-
tation (CWS) is an essential task required by
further language processing. Previous research
shows that sequence labeling models trained on
labeled data can reach competitive accuracy on
the CWS task, and supervised models are more
accurate than unsupervised models (Xue, 2003;
Low et al., 2005). However, the resource of man-
ually labeled training corpora is limited. There-
fore, semi-supervised learning has become one
*Corresponding author
of the most natural forms of training for CWS.
Traditional semi-supervised methods focus on
adding new unlabeled instances to the training
set by a given criterion. The possible mislabeled
instances, which are introduced from the auto-
matically labeled raw data, can hurt the per-
formance and not easy to exclude by setting a
sound selecting criterion.
In this paper, we propose a simple and scal-
able semi-supervised strategy that works by pro-
viding semi-supervision at the level of represen-
tation. Previous works mainly assume that con-
text features are helpful to decide the potential
label of a character. However, when some of the
context features do not appear in the training
corpus, this assumption may fail. An example is
shown in table 1. Although the context of “水”
and “篮” is totally different, they share a homo-
geneous structure as “verb-noun”. Therefore. A
much better way is to map the context informa-
tion to a kind of representation. More precisely,
the mapping should let the similar contexts map
to similar representations, while let the distinct
contexts map to distinct representations.
</bodyText>
<table confidence="0.969895666666667">
吃水果 打篮球
Label B B
Character 吃 水 果 打 篮 球
Context C-1= 吃 C-1= 打
Features C0= 水 C0= 篮
C1= 果 C1= 球
</table>
<tableCaption confidence="0.968850666666667">
Table 1: Example of the context of “水” in “吃水
果 (Eat fruits)” and the context of “篮” in “打篮球
(Play basketball)”
</tableCaption>
<bodyText confidence="0.978641">
We use the label distribution information that
</bodyText>
<page confidence="0.983556">
311
</page>
<note confidence="0.733572">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 311–321,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999656780487805">
is extracted from the unlabeled corpus as this
representation to enhance the supervised model.
We add “pseudo-labels” by tagging the unla-
beled data with the trained model on the train-
ing corpus. These “pseudo-labels” are not accu-
rate enough. Therefore, we use the label distri-
bution, which is much more accurate.
To accurately calculate the precise label dis-
tribution, we use a framework similar to the co-
training algorithm to adjust the feature values
iteratively. Generally speaking, unlabeled data
can be classified as in-domain data and out-of-
domain data. In previous works these two kinds
of unlabeled data are used separately for differ-
ent purposes. In-domain data is mainly used to
solve the problem of data sparseness (Sun and
Xu, 2011). On the other hand, out-of domain
data is used for domain adaptation (Chang and
Han, 2010). In our work, we use in-domain and
out-of-domain data together to adjust the labels
of the unlabeled corpus.
We evaluate the performance of CWS on the
benchmark dataset of Peking University in the
second International Chinese Word Segmenta-
tion Bakeoff. Experiment results show that our
approach yields improvements compared with
the state-of-art systems. Even when the la-
beled data is insufficient, our methods can still
work better than traditional methods. Com-
pared to the baseline CWS model, which has
already achieved an f-score above 0.95, we fur-
ther reduce the error rate by 15%.
Our method is not limited to word segmen-
tation. It is also applicable to other problems
which can be solved by sequence labeling mod-
els. We also applied our method to the Chi-
nese Named Entity Recognition task, and also
achieved better results compared to traditional
methods.
The main contributions of our work are as fol-
lows:
</bodyText>
<listItem confidence="0.9934118">
• We proposed a general method to utilize
the label distribution given text contexts as
representations in a semi-supervised frame-
work. We let the co-training process ad-
just the representation values from label
distribution instead of using manually pre-
defined feature templates.
• Compared with previous work, our method
achieved a new state-of-art accuracy on the
CWS task as well as on the NER task.
</listItem>
<bodyText confidence="0.999526166666667">
The remaining part of this paper is organized
as follows. Section 2 describes the details of the
problem and our algorithm. Section 3 describes
the experiment and presents the results. Section
4 reviews the related work. Section 5 concludes
this paper.
</bodyText>
<sectionHeader confidence="0.905396" genericHeader="method">
2 System Architecture
</sectionHeader>
<subsectionHeader confidence="0.995317">
2.1 Sequence Labeling
</subsectionHeader>
<bodyText confidence="0.996304923076923">
Nowadays the character-based sequence label-
ing approach is widely used for the Chinese word
segmentation problem. It was first proposed in
Xue (2003), which assigns each character a label
to indicate its position in the word. The most
prevalent tag set is the BMES tag set, which
uses 4 tags to carry word boundary information.
This tag set uses B, M, E and S to represent the
Beginning, the Middle, the End of a word and
a Single character forming a word respectively.
We use this tag set in our method. An example
of the “BMES” representation is shown in table
2.
</bodyText>
<table confidence="0.766749">
Character: 我 爱 北 京 天 安 门
Tag: S S B E B M E
</table>
<tableCaption confidence="0.967103">
Table 2: An example for the “BMES” representa-
tion. The sentence is “我爱北京天安门” (I love Bei-
jing Tian-an-men square), which consists of 4 Chi-
nese words: “我” (I), “爱” (love), “北京” (Beijing),
and “天安门” (Tian-an-men square).
</tableCaption>
<subsectionHeader confidence="0.997756">
2.2 Unlabeled Data
</subsectionHeader>
<bodyText confidence="0.999957666666667">
Unlabeled data can be divided into in-domain
data and out-of-domain data. In previous works,
these two kinds of unlabeled data are used sep-
arately for different purposes. In-domain data
only solves the problem of data sparseness (Sun
and Xu, 2011). Out-of domain data is used
only for domain adaptation (Chang and Han,
2010). These two functionalities are not contra-
dictory but complementary. Our study shows
</bodyText>
<page confidence="0.997941">
312
</page>
<bodyText confidence="0.999276964285714">
that by correctly designing features and algo-
rithms, both in-domain unlabeled data and out-
of-domain unlabeled data can work together to
help enhancing the segmentation model. In our
algorithm, the dynamic features learned from
one corpus can be adjusted incrementally with
the dynamic features learned from the other cor-
pus.
As for the out-of-domain data, it will be even
better if the corpus is not limited to a specific
domain. We choose a Chinese encyclopedia cor-
pus which meets exactly this requirement. We
use the corpus to learn a large set of informative
features. In our experiment, two different views
of features on unlabeled data are considered:
Static Statistical Features (SSFs): These
features capture statistical information of char-
acters and character n-grams from the unlabeled
corpus. The values of these features are fixed
during the training process once the unlabeled
corpus is given.
Dynamic Statistical Features (DSFs):
These features capture label distribution infor-
mation from the unlabeled corpus given fixed
text contexts. As the training process proceeds,
the value of these features will change, since the
trained tagger at each training iteration may as-
sign different labels to the unlabeled data.
</bodyText>
<subsectionHeader confidence="0.981902">
2.3 Framework
</subsectionHeader>
<bodyText confidence="0.674230555555555">
Suppose we have labeled data L, two unla-
beled corpora Ua and Ub (one is an in-domain
corpus and the other is an out-of-domain cor-
pus). Our algorithm is shown in Table 3.
During each iteration, we tag the unlabeled
corpus Ua using Tb to get pseudo-labels. Then
we extract features from the pseudo-labels. We
use the label distribution information as dy-
namic features. We add these features to the
training data to train a new tagger Ta. To adjust
the feature values, we extract features from one
corpus and then apply the statistics to the other
corpus. This is similar to the principle of co-
training (Yarowsky, 1995; Blum and Mitchell,
1998; Dasgupta et al., 2002). The difference is
that there are not different views of features, but
different kinds of unlabeled data. Detailed de-
scription of features is given in the next section.
</bodyText>
<figure confidence="0.527409066666667">
Algorithm
Init:
Using baseline features only:
Train an initial tagger T0 based on L ()
Label Ua and Ub individually using T0
BEGIN LOOP:
Generate DSFs from tagged Ua
Augment L with DSFs to get La
Generate DSFs from tagged Ub
Augment L with DSFs to get Lb
Using baseline features, SSFs and DSFs:
Train new tagger Ta using La
Train new tagger Tb using Lb
Label Ua using Tb
Label Ub using Ta
</figure>
<tableCaption confidence="0.6420115">
LOOP until performance does not improve
RETURN the tagger which is trained with
in-domain features.
Table 3: Algorithm description
</tableCaption>
<subsectionHeader confidence="0.5088185">
2.4 Features
2.4.1 Baseline Features
</subsectionHeader>
<bodyText confidence="0.995298166666667">
Our baseline feature templates include the
features described in previous works (Sun and
Xu, 2011; Sun et al., 2012). These features are
widely used in the CWS task. To be convenient,
for a character ci with context ... ci_1cici+1...,
its baseline features are listed below:
</bodyText>
<listItem confidence="0.991856714285714">
• Character uni-grams: ck (i − 3 &lt; k &lt; i + 3)
• Character bi-grams: ckck+1 (i − 3 &lt; k &lt;
i + 2)
• Whether ck and ck+1 are identical (i − 2 &lt;
k &lt; i + 2)
• Whether ck and ck+2 are identical (i − 4 &lt;
k &lt; i + 2)
</listItem>
<bodyText confidence="0.9998106">
The last two feature templates are designed to
detect character reduplication, which is a mor-
phological phenomenon in Chinese language.
An example is “+全+美” (Perfect), which is
a Chinese idiom with structure “ABAC”.
</bodyText>
<page confidence="0.99665">
313
</page>
<bodyText confidence="0.98787521978022">
2.4.2 Static statistical features
Statistical features are statistics that distilled
from the large unlabeled corpus. They are
proved useful in the Chinese word segmenta-
tion task. We define Static Statistical Features
(SSFs) as features whose value do not change
during the training process. The SSFs in our
approach includes Mutual information, Punctu-
ation information and Accessor variety. Previ-
ous works have already explored the functions
of the three static statistics in the Chinese word
segmentation task, e.g. Feng et al. (2004); Sun
and Xu (2011). We mainly follow their defini-
tions while considering more details and giving
some modification.
Mutual information
Mutual information (MI) is a quantity that
measures the mutual dependence of two random
variables. Previous works showed that larger MI
of two strings claims higher probability that the
two strings should be combined. Therefore, MI
can show the tendency of two strings forming
one word. However, previous works mainly fo-
cused on the balanced case, i.e., the MI of strings
with the same length. In our study we find that,
in Chinese, there remains large amount of imbal-
anced cases, like a string with length 1 followed
by a string with length 2, and vice versa. We
further considered the MI of these string pairs
to capture more information.
Punctuation information
Punctuations can provide implicit labels for
the characters before and after them. The char-
acter after punctuations must be the first char-
acter of a word. The character before punctua-
tions must be the last character of a word. When
a string appears frequently after punctuations,
it tends to be the beginning of a word. The situ-
ation is similar when a string appears frequently
preceding punctuations. Besides, the probabil-
ity of a string appears in the corpus also affects
this tendency. Considering all these factors,
we propose “punctuation rate” (PR) to capture
this information. For a string with length len
and probability p in the corpus, we define the
left punctuation rate LPRlen as the number of
times the string appears after punctuations, di-
vided by p. Similarly, the right punctuation
rate RPRlen is defines as the number of times
it appears preceding punctuations divided by its
probability p. The length of string we consider
is from 1 to 4.
Accessor variety
Accessor variety (AV) is also known as letter
successor variety (LSV) (Harris, 1955; Hafer and
Weiss, 1974). If a string appears after or pre-
ceding many different characters, this may pro-
vide some information of the string itself. Pre-
vious work of Feng et al. (2004), Sun and Xu
(2011) used AV to represent this statistic. Sim-
ilar to punctuation rate, we also consider both
left AV and right AV. For a string s with length
l, we define the left accessor variety (LAV) as
the types of distinct characters preceding s in
the corpus, and the right accessor variety (RAV)
as the types of distinct characters after s in the
corpus. The length of string we consider is also
from 1 to 4.
2.4.3 Dynamic statistical features
The unlabeled corpus lacks precise labels. We
can use the trained tagger to give the unla-
beled data “pseudo-labels”. These labels can-
not guarantee an acceptable precision. How-
ever, the label distribution will not be largely
affected by small mistakes. Using the label dis-
tribution information is more accurate than us-
ing the pseudo-labels directly.
Based on this assumption, we propose “dy-
namic statistical features” (DSFs). The DSFs
are intended to capture label distribution infor-
mation given a text context. The word “Dy-
namic” is in accordance with the fact that these
feature values will change during the training
process.
We give a formal description of DSFs. Sup-
pose there are K labels in our task. For example,
K = 4 if we take BMES labeling method. We
define the whole character sequence with length
n as X = (x1, x2 · · · xj · · · xn). Given a text con-
text Ci, where i is current character position,
the DSFs can be represented as a list,
</bodyText>
<equation confidence="0.995739">
DSF(Ci) = (DSF(Ci)1, · · · , DSF(Ci)K)
</equation>
<page confidence="0.976998">
314
</page>
<bodyText confidence="0.976225142857143">
Each element in the list represents the proba-
bility of the corresponding label in the distribu-
tion.
For convenience, we further define function
‘count(condition)’ as the total number of times
a ‘condition’ is true in the unlabeled corpus.
For example, count (current=‘a’) represents the
times the current character equals ‘a’, which is
exactly the number of times character ‘a’ ap-
pears in the unlabeled corpus.
According to different types of text context
Ci, we can divide DSFs into 3 types:
1.Basic DSF
For Basic DSF of Ci, we define D(Ci):
</bodyText>
<equation confidence="0.997899">
D(Ci) = (D(Ci)1, ... , D(Ci)K)
</equation>
<bodyText confidence="0.999913666666667">
We define Basic DSF with current character po-
sition i, text context Ci and label l (the lth di-
mension in the list) as:
</bodyText>
<equation confidence="0.9997675">
D(Ci)l = P(y = l|Ci = xi)
count(Ci = xi ∧ y = l)
=
count(Ci = xi)
</equation>
<bodyText confidence="0.999134238095238">
In this equation, the numerator counts the num-
ber of times current character is xi with label l.
The denominator counts the number of times
current character is xi.
We use the term “Basic” because this kind of
DSFs only considers the character of position i
as its context. The text context refers to the cur-
rent character itself. This feature captures the
label distribution information given the charac-
ter itself.
2.BigramDSF
Basic DSF is simple and very easy to imple-
ment. The weakness is that it is less power-
ful to describe word-building features. Although
characters convey context information, charac-
ters themselves in Chinese is sometimes mean-
ingless. Character bi-grams can carry more con-
text information than uni-grams. We modify
Basic DSFs to bi-gram level and propose Bigram
DSFs.
For Bigram DSF of Ci, we define B(Ci):
</bodyText>
<equation confidence="0.995602">
B(Ci) = (B(Ci)1, ... , B(Ci)K)
</equation>
<bodyText confidence="0.999938">
We define Bigram DSF with current character
position i, text context Ci and label l (the lth
dimension in the list) as:
</bodyText>
<equation confidence="0.990108333333333">
B(Ci)l = P(y = l|Ci = xi−jxi−j+1)
count(Ci = xi−jxi−j+1 ∧ y = l)
= count(Ci = xi−jxi−j+1)
</equation>
<bodyText confidence="0.9883572">
j can take value 0 and 1.
In this equation, the numerator counts the
number of times current context is xi−jxi−j+1
with label l. The denominator counts the num-
ber of times current context is xi−jxi−j+1.
</bodyText>
<equation confidence="0.455236">
3.WindowDSF
</equation>
<bodyText confidence="0.994360142857143">
Considering Basic DSF and Bigram DSF only
might cause the over-fitting problem, therefore
we introduce another kind of DSF. We call it
Window DSF, which considers the surrounding
context of a character and omits the character
itself.
For Window DSF, we define W(Ci):
</bodyText>
<equation confidence="0.992261">
W(Ci) = (W(Ci)1,...,W(Ci)K)
</equation>
<bodyText confidence="0.999935666666667">
We define Window DSF with current character
position i, text context Ci and label l (the lth
dimension in the list) as:
</bodyText>
<equation confidence="0.99890325">
W (Ci)l = P(y = l|Ci = xi−1xi+1)
count(Ci = xi−1xi+1 ∧ y = l)
=
count(Ci = xi−1xi+1)
</equation>
<bodyText confidence="0.9996505">
In this equation, the numerator counts the
number of times current context is xi−1xi+1
with label l. The denominator counts the num-
ber of times current context is xi−1xi+1.
</bodyText>
<subsubsectionHeader confidence="0.36864">
2.4.4 Discrete features VS. Continuous
</subsubsectionHeader>
<bodyText confidence="0.95307">
features
The statistical features may be expressed as
real values. A more natural way is to use dis-
crete values to incorporate them into the se-
quence labeling models . Previous works like
Sun and Xu (2011) solve this problem by set-
ting thresholds and converting the real value
into boolean values. We use a different method
to solve this, which does not need to consider
tuning thresholds. In our method, we process
static and dynamic statistical features using dif-
ferent strategies.
</bodyText>
<page confidence="0.996781">
315
</page>
<bodyText confidence="0.999647857142857">
For static statistical value:
For mutual information, we round the real
value to their nearest integer. For punctuation
rate and accessor variety, as the values tend to
be large, we first get the log value of the feature
and then use the nearest integer as the corre-
sponding discrete value.
For dynamic statistical value:
Dynamic statistical features are distributions
of a label. The values of DSFs are all percentage
values. We can solve this by multiply the proba-
bility by an integer N and then take the integer
part as the final feature value. We set the value
of N by cross-validation..
</bodyText>
<subsectionHeader confidence="0.915988">
2.5 Conditional Random Fields
</subsectionHeader>
<bodyText confidence="0.9941255">
Our algorithm is not necessarily limited to
a specific baseline tagger. For simplicity and
reliability, we use a simple Conditional Ran-
dom Field (CRF) tagger, although other se-
quence labeling models like Semi-Markov CRF
Gao et al. (2007) and Latent-variable CRF Sun
et al. (2009) may provide better results than
a single CRF. Detailed definition of CRF can
be found in Lafferty et al. (2001); McCallum
(2002); Pinto et al. (2003).
</bodyText>
<sectionHeader confidence="0.998856" genericHeader="method">
3 Experiment
</sectionHeader>
<subsectionHeader confidence="0.99336">
3.1 Data and metrics
</subsectionHeader>
<bodyText confidence="0.999894">
We used the benchmark datasets provided by
the second International Chinese Word Segmen-
tation Bakeoff1 to test our approach. We chose
the Peking University (PKU) data in our exper-
iment. Although the benchmark provides an-
other three data sets, two of them are data of
traditional Chinese, which is quite different from
simplified Chinese. Another is the data from Mi-
crosoft Research (MSR). We experimented on
this data and got 97.45% in f-score compared
to the state-of-art 97.4% reported in Sun et al.
(2012). However, this corpus is much larger
than the PKU corpus. Using the labeled data
alone can get a relatively good tagger and the
unlabeled data contributes little to the perfor-
mance. For simplicity and efficiency, our further
</bodyText>
<footnote confidence="0.925625">
1http://www.sighan.org/bakeoff2005/
</footnote>
<bodyText confidence="0.999451952380952">
experiments are all conducted on the PKU data.
Details of the PKU data are listed in table 4.
We also used two un-segmented corpora as
unlabeled data. The first one is Chinese Giga-
word2 corpus. It is a comprehensive archive of
newswire data. The second one is articles from
Baike3 of baidu.com. It is a Chinese encyclope-
dia similar to Wikipedia but contains more Chi-
nese items and their descriptions. In the exper-
iment we used about 5 million characters from
each corpus for efficiency. Details of unlabeled
data can be found in table 5.
In our experiment, we did not use any ex-
tra resources such as common surnames, part-
of-speech or other dictionaries.
F-score is used as the accuracy measure. We
define precision P as the percentage of words
in the output that are segmented correctly. We
define recall R as the percentage of the words
in reference that are correctly segmented. Then
F-score is as follows:
</bodyText>
<equation confidence="0.993154">
2 x P x R
F=
</equation>
<bodyText confidence="0.9995935">
The recall of out-of-vocabulary is also taken into
consideration, which measures the ability of the
model to correctly segment out of vocabulary
words.
</bodyText>
<subsectionHeader confidence="0.998715">
3.2 Main Results
</subsectionHeader>
<bodyText confidence="0.9999898">
Table 6 summarizes the segmentation results
on test data with different feature combinations.
We performed incremental evaluation. In this
table, we first present the results of the tagger
only using baseline features. Then we show the
results of adding SSF and DSF individually. In
the end we compare the results of combining
SSF and DSF with baseline features.
Because the baseline features is strong to
reach a relative good result, it is not easy to
largely enhance the performance. Neverthe-
less, there are significant increases in f-score and
OOV-Recall when adding these features. From
table 6 we can see that by adding SSF and DSF
individually, the F-score is improved by +1.1%
</bodyText>
<footnote confidence="0.998980666666667">
2http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2003T09
3http://baike.baidu.com/
</footnote>
<equation confidence="0.861244">
P + R
</equation>
<page confidence="0.953705">
316
</page>
<table confidence="0.9645925">
Identical words Total word Identical Character Total character
5.5 x 104 1.1 x 106 5 x 103 1.8 x 106
</table>
<tableCaption confidence="0.98101">
Table 4: Details of the PKU data
</tableCaption>
<table confidence="0.983834333333333">
Corpus Character used
Gigaword 5000193
Baike 5000147
</table>
<tableCaption confidence="0.980412">
Table 5: Details of the unlabeled data.
</tableCaption>
<table confidence="0.999967">
P R F OOV
Baseline 0.950 0.943 0.946 0.676
+SSF 0.961 0.953 0.957 0.728
+DSF 0.958 0.953 0.955 0.678
+SSF+DSF 0.965 0.958 0.961 0.731
</table>
<tableCaption confidence="0.739721">
Table 6: Segmentation results on test data with
different feature combinations. The symbol “+”
means this feature configuration contains features set
containing the baseline features and all features after
‘+’. The size of unlabeled data is fixed as 5 million
characters.
</tableCaption>
<bodyText confidence="0.9937416">
and +0.9%. The OOV-Recall is also improved,
especially after adding SSFs. When considering
SSF and DSF together, the f-score is improved
by +1.5% while the OOV-Recall is improved by
+5.5%.
To compare the contribution of unlabeled
data, we conduct experiments of using differ-
ent sizes of unlabeled data. Note that the SSFs
are still calculated using all the unlabeled data.
However, each iteration in the algorithm uses
unlabeled data with different sizes.
Table 7 shows the results when changing the
size of unlabeled data. We experimented on
three different sizes: 0.5 million, 1 million and 5
million characters.
</bodyText>
<table confidence="0.99767675">
P R F OOV
DSF(0.5M) 0.962 0.954 0.958 0.727
DSF(1M) 0.963 0.955 0.959 0.728
DSF(5M) 0.965 0.958 0.961 0.731
</table>
<tableCaption confidence="0.994089666666667">
Table 7: Comparison of results when changing the
size of unlabeled data. (0.5 million, 1 million and 5
million characters).
</tableCaption>
<bodyText confidence="0.998997">
We further experimented on unlabeled corpus
with larger size (up to 100 million characters).
However the performance did not change signif-
icantly. Besides, because the number of features
in our method is very large, using too large un-
labeled corpus is intractable in real applications
due to the limitation of memory.
Our method can keep working well even when
the labeled data are insufficient. Table 8 shows
the comparison of f-scores when changing the
size of labeled data. We compared the results
of using all labeled data with 3 different situa-
tions: using 1/10, 1/2 and 1/4 of all the labeled
data. In fact, the best system on the Second In-
ternational Chinese Word Segmentation bakeoff
reached 0.95 in f-score by using all labeled data.
From table 8 we can see that our algorithm only
needs 1/4 of all labeled data to achieve the same
f-score.
</bodyText>
<table confidence="0.9996484">
Baseline +SSF+DSF Improve
1/10 0.934 0.943 +0.96%
1/4 0.946 0.951 +0.53%
1/2 0.952 0.956 +0.42%
All 0.957 0.961 +0.42%
</table>
<tableCaption confidence="0.80186975">
Table 8: Comparison of f-scores when changing the
size of labeled data. (1/10, 1/4, 1/2 and all labeled
data. The size of unlabeled data is fixed as 5 million
characters.)
</tableCaption>
<bodyText confidence="0.999965357142857">
We also explored how the performance
changes as iteration increases. Figure 1 shows
the change of F-score during the first 10 itera-
tions. From figure 1 we find that f-score has a
fast improvement in the first few iterations, and
then stables at a fixed point. Besides, as the size
of labeled data increases, it converges faster.
Using an in-domain corpus and an out-of-
domain corpus is better than use one corpus
alone. We compared our approach with the
method which uses only one unlabeled corpus.
To use only one corpus, we modify our algorithm
to extract DSFs from the Chinese Giga word
corpus and apply the learned features to itself.
</bodyText>
<page confidence="0.996807">
317
</page>
<figureCaption confidence="0.985855">
Figure 1: Learning curve of using different size of
labeled data
</figureCaption>
<bodyText confidence="0.9998248">
Table 9 shows the result. We can see that our
method outperforms by +0.2% in f-score and
+0.7% in OOV-Recall.
Finally, we compared our method with the
state-of-art systems reported in the previous pa-
pers. Table 10 listed the results. Best05 repre-
sents the best system reported on the Second In-
ternational Chinese Word Segmentation Bake-
off. CRF + Rule system represents a combina-
tion of CRF model and rule based model pre-
sented in Zhang et al. (2006). Other three sys-
tems all represent the methods using their cor-
responding model in the corresponding papers.
Note that these state-of-art systems are either
using complicated models with semi-Markov re-
laxations or latent variables, or modifying mod-
els to fit special conditions. Our system uses a
single CRF model. As we can see in table 10,
our method achieved higher F-scores than the
previous best systems.
</bodyText>
<subsectionHeader confidence="0.984448">
3.3 Results on NER task
</subsectionHeader>
<bodyText confidence="0.999906866666667">
Our method is not limited to the CWS prob-
lem. It is applicable to all sequence labeling
problems. We applied our method on the Chi-
nese NER task. We used the MSR corpus of
the sixth SIGHAN Workshop on Chinese Lan-
guage Processing. It is the only NER corpus
using simplified Chinese in that workshop. We
compared our method with the pure sequence la-
beling approach in He and Wang (2008). We re-
implemented their method to eliminate the dif-
ference of various CRFs implementations. Ex-
periment results are shown in table 11. We can
see that our methods works better, especially
when handling the out-of-vocabulary named en-
tities;
</bodyText>
<sectionHeader confidence="0.99831" genericHeader="method">
4 Related work
</sectionHeader>
<bodyText confidence="0.998654860465117">
Recent studies show that character sequence
labeling is an effective method of Chinese word
segmentation for machine learning (Xue, 2003;
Low et al., 2005; Zhao et al., 2006a,b). These su-
pervised methods show good results. Unsuper-
vised word segmentation (Maosong et al., 1998;
Peng and Schuurmans, 2001; Feng et al., 2004;
Goldwater et al., 2006; Jin and Tanaka-Ishii,
2006) takes advantage of the huge amount of raw
text to solve Chinese word segmentation prob-
lems. These methods need no annotated corpus,
and most of them use statistics to help model
the problem. However, they usually are less ac-
curate than supervised ones.
Currently “feature-engineering” methods
have been successfully applied into NLP ap-
plications. Miller et al. (2004) applied this
method to named entity recognition. Koo et al.
(2008) applied this method to dependency pars-
ing. Turian et al. (2010) applied this method to
both named entity recognition and text chunk-
ing. These papers shared the same concept of
word clustering. However, we cannot simply
equal Chinese character to English word because
characters in Chinese carry much less informa-
tion than words in English and the clustering
results is less meaningful.
Features extracted from large unlabeled cor-
pus in previous works mainly focus on statisti-
cal information of characters. Feng et al. (2004)
used the accessor variety criterion to extract
word types. Li and Sun (2009) used punctua-
tion information in Chinese word segmentation
by introducing extra labels ’L’ and ’R’. Chang
and Han (2010), Sun and Xu (2011) used rich
statistical information as discrete features in
a sequence labeling framework. All these ap-
proaches can be viewed as using static statistics
features in a supervised approach. Our method
is different from theirs. For the static statistics
features in our approach, we not only consider
richer string pairs with the different lengths, but
also consider term frequency when processing
</bodyText>
<page confidence="0.99594">
318
</page>
<table confidence="0.999697333333333">
P R F OOV
Using one corpus 0.963 0.955 0.959 0.724
Our method 0.965 0.958 0.961 0.731
</table>
<tableCaption confidence="0.997514">
Table 9: Comparison of our approach with using only the Gigaword corpus
</tableCaption>
<table confidence="0.999912285714286">
Method P R F-score
Best05 (Chen et al. (2005)) 0.953 0.946 0.950
CRF + rule-system (Zhang et al. (2006)) 0.947 0.955 0.951
Semi-perceptron (Zhang and Clark (2007)) N/A N/A 0.945
Latent-variable CRF (Sun et al. (2009)) 0.956 0.948 0.952
ADF-CRF (Sun et al. (2012)) 0.958 0.949 0.954
Our method 0.965 0.958 0.961
</table>
<tableCaption confidence="0.998772">
Table 10: Comparison of our approach with the state-of-art systems
</tableCaption>
<table confidence="0.999816">
P R F OOV
Traditional 0.925 0.872 0.898 0.712
Our method 0.916 0.887 0.902 0.737
</table>
<tableCaption confidence="0.971437">
Table 11: Comparison of our approach with tradi-
tional NER systems
</tableCaption>
<bodyText confidence="0.998220789473684">
punctuation features.
There are previous works using features ex-
tracted from label distribution of unlabeled cor-
pus in NLP tasks. Schapire et al. (2002) use a
set of features annotated with majority labels
to boost a logistic regression model. We are
different from their approach because there is
no pseudo-example labeling process in our ap-
proach. Qi et al. (2009) investigated on large
set of distribution features and used these fea-
tures in a self-training way. They applied the
method on three tasks: named entity recogni-
tion, POS tagging and gene name recognition
and got relatively good results. Our approach is
different from theirs. Although we all consider
label distribution, the way we use features are
different. Besides, our approach uses two unla-
beled corpora which can mutually enhancing to
get better result.
</bodyText>
<sectionHeader confidence="0.990543" genericHeader="method">
5 Conclusion and Perspectives
</sectionHeader>
<bodyText confidence="0.999966045454545">
In this paper, we presented a semi-supervised
method for Chinese word segmentation. Two
kinds of new features are used for the itera-
tive modeling: static statistical features and dy-
namic statistical features. The dynamic statis-
tical features use label distribution information
for text contexts, and can be adjusted automat-
ically during the co-training process. Experi-
mental results show that the new features can
improve the performance on the Chinese word
segmentation task. We further conducted exper-
iments to show that the performance is largely
improved, especially when the labeled data is
insufficient.
The proposed iterative semi-supervised
method is not limited to the Chinese word
segmentation task. It can be easily extended
to any sequence labeling task. For example, it
works well on the NER task as well. As our
future work, we plan to apply our method to
other natural language processing tasks, such
as text chunking.
</bodyText>
<sectionHeader confidence="0.998682" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99981275">
This research was partly supported by Ma-
jor National Social Science Fund of China(No.
12&amp;ZD227),National High Technology Research
and Development Program of China (863 Pro-
gram) (No. 2012AA011101) and National Natu-
ral Science Foundation of China (No.91024009).
We also thank Xu Sun and Qiuye Zhao for proof-
reading the paper.
</bodyText>
<page confidence="0.998842">
319
</page>
<sectionHeader confidence="0.977542" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.931061185185185">
Blum, A. and Mitchell, T. (1998). Combining
labeled and unlabeled data with co-training.
In Proceedings of the eleventh annual confer-
ence on Computational learning theory, pages
92–100. ACM.
Chang, B. and Han, D. (2010). Enhancing
domain portability of chinese segmentation
model using chi-square statistics and boot-
strapping. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 789–798. Association
for Computational Linguistics.
Chen, A., Zhou, Y., Zhang, A., and Sun, G.
(2005). Unigram language model for chinese
word segmentation. In Proceedings of the
4th SIGHAN Workshop on Chinese Language
Processing, pages 138–141. Association for
Computational Linguistics Jeju Island, Korea.
Dasgupta, S., Littman, M. L., and McAllester,
D. (2002). Pac generalization bounds for co-
training. Advances in neural information pro-
cessing systems, 1:375–382.
Feng, H., Chen, K., Deng, X., and Zheng, W.
(2004). Accessor variety criteria for chinese
word extraction. Computational Linguistics,
30(1):75–93.
Gao, J., Andrew, G., Johnson, M., and
</reference>
<bodyText confidence="0.558263333333333">
Toutanova, K. (2007). A comparative study
of parameter estimation methods for statisti-
cal natural language processing. In ANNUAL
</bodyText>
<sectionHeader confidence="0.443304" genericHeader="method">
MEETING-ASSOCIATION FOR COMPU-
</sectionHeader>
<bodyText confidence="0.916087045454546">
TATIONAL LINGUISTICS, volume 45, page
824.
Goldwater, S., Griffiths, T., and Johnson, M.
(2006). Contextual dependencies in unsuper-
vised word segmentation. In Proceedings of
the 21st International Conference on Compu-
tational Linguistics and the 44th annual meet-
ing of the Association for Computational Lin-
guistics, pages 673–680. Association for Com-
putational Linguistics.
Hafer, M. A. and Weiss, S. F. (1974). Word seg-
mentation by letter successor varieties. Infor-
mation storage and retrieval, 10(11):371–385.
Harris, Z. S. (1955). From phoneme to mor-
pheme. Language, 31(2):190–222.
He, J. and Wang, H. (2008). Chinese named en-
tity recognition and word segmentation based
on character. In Sixth SIGHAN Workshop on
Chinese Language Processing, page 128.
Jin, Z. and Tanaka-Ishii, K. (2006). Unsu-
pervised segmentation of chinese text by use
of branching entropy. In Proceedings of the
</bodyText>
<reference confidence="0.979793527777778">
COLING/ACL on Main conference poster
sessions, pages 428–435. Association for Com-
putational Linguistics.
Koo, T., Carreras, X., and Collins, M. (2008).
Simple semi-supervised dependency parsing.
Lafferty, J., McCallum, A., and Pereira, F.
(2001). Conditional random fields: Proba-
bilistic models for segmenting and labeling se-
quence data.
Li, Z. and Sun, M. (2009). Punctuation
as implicit annotations for chinese word
segmentation. Computational Linguistics,
35(4):505–512.
Low, J., Ng, H., and Guo, W. (2005). A
maximum entropy approach to chinese word
segmentation. In Proceedings of the Fourth
SIGHAN Workshop on Chinese Language
Processing, volume 164. Jeju Island, Korea.
Maosong, S., Dayang, S., and Tsou, B. (1998).
Chinese word segmentation without using lex-
icon and hand-crafted training data. In Pro-
ceedings of the 17th international confer-
ence on Computational linguistics-Volume 2,
pages 1265–1271. Association for Computa-
tional Linguistics.
McCallum, A. (2002). Efficiently inducing fea-
tures of conditional random fields. In Proceed-
ings of the Nineteenth Conference on Uncer-
tainty in Artificial Intelligence, pages 403–410.
Morgan Kaufmann Publishers Inc.
Miller, S., Guinness, J., and Zamanian, A.
(2004). Name tagging with word clusters
and discriminative training. In Proceedings of
HLT-NAACL, volume 4.
Peng, F. and Schuurmans, D. (2001). Self-
supervised chinese word segmentation. Ad-
</reference>
<page confidence="0.976135">
320
</page>
<reference confidence="0.998654134146341">
vances in Intelligent Data Analysis, pages
238–247.
Pinto, D., McCallum, A., Wei, X., and Croft,
W. (2003). Table extraction using conditional
random fields. In Proceedings of the 26th an-
nual international ACM SIGIR conference on
Research and development in informaion re-
trieval, pages 235–242. ACM.
Qi, Y., Kuksa, P., Collobert, R., Sadamasa,
K., Kavukcuoglu, K., and Weston, J. (2009).
Semi-supervised sequence labeling with self-
learned features. In Data Mining, 2009.
ICDM’09. Ninth IEEE International Confer-
ence on, pages 428–437. IEEE.
Schapire, R., Rochery, M., Rahim, M., and
Gupta, N. (2002). Incorporating prior
knowledge into boosting. In MACHINE
LEARNING-INTERNATIONAL WORK-
SHOP THEN CONFERENCE-, pages
538–545.
Sun, W. and Xu, J. (2011). Enhancing chi-
nese word segmentation using unlabeled data.
In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing,
pages 970–979. Association for Computational
Linguistics.
Sun, X., Wang, H., and Li, W. (2012). Fast on-
line training with frequency-adaptive learning
rates for chinese word segmentation and new
word detection. In Proceedings of the 50th An-
nual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers),
pages 253–262, Jeju Island, Korea. Associa-
tion for Computational Linguistics.
Sun, X., Zhang, Y., Matsuzaki, T., Tsuruoka,
Y., and Tsujii, J. (2009). A discriminative
latent variable chinese segmenter with hybrid
word/character information. In Proceedings of
Human Language Technologies: The 2009 An-
nual Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics, pages 56–64. Association for Compu-
tational Linguistics.
Turian, J., Ratinov, L., and Bengio, Y. (2010).
Word representations: a simple and gen-
eral method for semi-supervised learning. In
Proceedings of the 48th Annual Meeting of
the Association for Computational Linguis-
tics, pages 384–394. Association for Compu-
tational Linguistics.
Xue, N. (2003). Chinese word segmentation as
character tagging. Computational Linguistics
and Chinese Language Processing, 8(1):29–48.
Yarowsky, D. (1995). Unsupervised word sense
disambiguation rivaling supervised methods.
In Proceedings of the 33rd annual meeting
on Association for Computational Linguistics,
pages 189–196. Association for Computational
Linguistics.
Zhang, R., Kikui, G., and Sumita, E. (2006).
Subword-based tagging by conditional ran-
dom fields for chinese word segmentation. In
Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Companion
Volume: Short Papers, pages 193–196. Asso-
ciation for Computational Linguistics.
Zhang, Y. and Clark, S. (2007). Chi-
nese segmentation with a word-based percep-
tron algorithm. In ANNUAL MEETING-
ASSOCIATION FOR COMPUTATIONAL
LINGUISTICS, volume 45, page 840.
Zhao, H., Huang, C., and Li, M. (2006a). An
improved chinese word segmentation system
with conditional random field. In Proceed-
ings of the Fifth SIGHAN Workshop on Chi-
nese Language Processing, volume 117. Syd-
ney: July.
Zhao, H., Huang, C., Li, M., and Lu, B. (2006b).
Effective tag set selection in chinese word seg-
mentation via conditional random field mod-
eling. In Proceedings of PACLIC, volume 20,
pages 87–94.
</reference>
<page confidence="0.998763">
321
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.787483">
<title confidence="0.9996925">Exploring Representations from Unlabeled Data with for Chinese Word Segmentation</title>
<author confidence="0.99906">Zhang Houfeng Xu Sun Mairgup Mansur</author>
<affiliation confidence="0.994387">Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,</affiliation>
<email confidence="0.87371">zhlongk@qq.com,wanghf@pku.edu.cn,xusun@pku.edu.cn,mairgup@gmail.com,</email>
<abstract confidence="0.995711136363636">Nowadays supervised sequence labeling models can reach competitive performance on the task of Chinese word segmentation. However, the ability of these models is restricted by the availability of annotated data and the design of features. We propose a scalable semi-supervised feature engineering approach. In contrast to previous works using pre-defined taskspecific features with fixed values, we dynamically extract representations of label distributions from both an in-domain corpus and an out-of-domain corpus. We update the representation values with a semi-supervised approach. Experiments on the benchmark datasets show that our approach achieve good results and reach an f-score of 0.961. The feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the eleventh annual conference on Computational learning theory,</booktitle>
<pages>92--100</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8846" citStr="Blum and Mitchell, 1998" startWordPosition="1426" endWordPosition="1429">abeled data L, two unlabeled corpora Ua and Ub (one is an in-domain corpus and the other is an out-of-domain corpus). Our algorithm is shown in Table 3. During each iteration, we tag the unlabeled corpus Ua using Tb to get pseudo-labels. Then we extract features from the pseudo-labels. We use the label distribution information as dynamic features. We add these features to the training data to train a new tagger Ta. To adjust the feature values, we extract features from one corpus and then apply the statistics to the other corpus. This is similar to the principle of cotraining (Yarowsky, 1995; Blum and Mitchell, 1998; Dasgupta et al., 2002). The difference is that there are not different views of features, but different kinds of unlabeled data. Detailed description of features is given in the next section. Algorithm Init: Using baseline features only: Train an initial tagger T0 based on L () Label Ua and Ub individually using T0 BEGIN LOOP: Generate DSFs from tagged Ua Augment L with DSFs to get La Generate DSFs from tagged Ub Augment L with DSFs to get Lb Using baseline features, SSFs and DSFs: Train new tagger Ta using La Train new tagger Tb using Lb Label Ua using Tb Label Ub using Ta LOOP until perfor</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Blum, A. and Mitchell, T. (1998). Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory, pages 92–100. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Chang</author>
<author>D Han</author>
</authors>
<title>Enhancing domain portability of chinese segmentation model using chi-square statistics and bootstrapping.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>789--798</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4090" citStr="Chang and Han, 2010" startWordPosition="634" endWordPosition="637">” are not accurate enough. Therefore, we use the label distribution, which is much more accurate. To accurately calculate the precise label distribution, we use a framework similar to the cotraining algorithm to adjust the feature values iteratively. Generally speaking, unlabeled data can be classified as in-domain data and out-ofdomain data. In previous works these two kinds of unlabeled data are used separately for different purposes. In-domain data is mainly used to solve the problem of data sparseness (Sun and Xu, 2011). On the other hand, out-of domain data is used for domain adaptation (Chang and Han, 2010). In our work, we use in-domain and out-of-domain data together to adjust the labels of the unlabeled corpus. We evaluate the performance of CWS on the benchmark dataset of Peking University in the second International Chinese Word Segmentation Bakeoff. Experiment results show that our approach yields improvements compared with the state-of-art systems. Even when the labeled data is insufficient, our methods can still work better than traditional methods. Compared to the baseline CWS model, which has already achieved an f-score above 0.95, we further reduce the error rate by 15%. Our method is</context>
<context position="6875" citStr="Chang and Han, 2010" startWordPosition="1107" endWordPosition="1110">wn in table 2. Character: 我 爱 北 京 天 安 门 Tag: S S B E B M E Table 2: An example for the “BMES” representation. The sentence is “我爱北京天安门” (I love Beijing Tian-an-men square), which consists of 4 Chinese words: “我” (I), “爱” (love), “北京” (Beijing), and “天安门” (Tian-an-men square). 2.2 Unlabeled Data Unlabeled data can be divided into in-domain data and out-of-domain data. In previous works, these two kinds of unlabeled data are used separately for different purposes. In-domain data only solves the problem of data sparseness (Sun and Xu, 2011). Out-of domain data is used only for domain adaptation (Chang and Han, 2010). These two functionalities are not contradictory but complementary. Our study shows 312 that by correctly designing features and algorithms, both in-domain unlabeled data and outof-domain unlabeled data can work together to help enhancing the segmentation model. In our algorithm, the dynamic features learned from one corpus can be adjusted incrementally with the dynamic features learned from the other corpus. As for the out-of-domain data, it will be even better if the corpus is not limited to a specific domain. We choose a Chinese encyclopedia corpus which meets exactly this requirement. We </context>
<context position="27686" citStr="Chang and Han (2010)" startWordPosition="4620" endWordPosition="4623">ity recognition and text chunking. These papers shared the same concept of word clustering. However, we cannot simply equal Chinese character to English word because characters in Chinese carry much less information than words in English and the clustering results is less meaningful. Features extracted from large unlabeled corpus in previous works mainly focus on statistical information of characters. Feng et al. (2004) used the accessor variety criterion to extract word types. Li and Sun (2009) used punctuation information in Chinese word segmentation by introducing extra labels ’L’ and ’R’. Chang and Han (2010), Sun and Xu (2011) used rich statistical information as discrete features in a sequence labeling framework. All these approaches can be viewed as using static statistics features in a supervised approach. Our method is different from theirs. For the static statistics features in our approach, we not only consider richer string pairs with the different lengths, but also consider term frequency when processing 318 P R F OOV Using one corpus 0.963 0.955 0.959 0.724 Our method 0.965 0.958 0.961 0.731 Table 9: Comparison of our approach with using only the Gigaword corpus Method P R F-score Best05</context>
</contexts>
<marker>Chang, Han, 2010</marker>
<rawString>Chang, B. and Han, D. (2010). Enhancing domain portability of chinese segmentation model using chi-square statistics and bootstrapping. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 789–798. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Chen</author>
<author>Y Zhou</author>
<author>A Zhang</author>
<author>G Sun</author>
</authors>
<title>Unigram language model for chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 4th SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>138--141</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics Jeju Island,</institution>
<contexts>
<context position="28306" citStr="Chen et al. (2005)" startWordPosition="4723" endWordPosition="4726">Sun and Xu (2011) used rich statistical information as discrete features in a sequence labeling framework. All these approaches can be viewed as using static statistics features in a supervised approach. Our method is different from theirs. For the static statistics features in our approach, we not only consider richer string pairs with the different lengths, but also consider term frequency when processing 318 P R F OOV Using one corpus 0.963 0.955 0.959 0.724 Our method 0.965 0.958 0.961 0.731 Table 9: Comparison of our approach with using only the Gigaword corpus Method P R F-score Best05 (Chen et al. (2005)) 0.953 0.946 0.950 CRF + rule-system (Zhang et al. (2006)) 0.947 0.955 0.951 Semi-perceptron (Zhang and Clark (2007)) N/A N/A 0.945 Latent-variable CRF (Sun et al. (2009)) 0.956 0.948 0.952 ADF-CRF (Sun et al. (2012)) 0.958 0.949 0.954 Our method 0.965 0.958 0.961 Table 10: Comparison of our approach with the state-of-art systems P R F OOV Traditional 0.925 0.872 0.898 0.712 Our method 0.916 0.887 0.902 0.737 Table 11: Comparison of our approach with traditional NER systems punctuation features. There are previous works using features extracted from label distribution of unlabeled corpus in N</context>
</contexts>
<marker>Chen, Zhou, Zhang, Sun, 2005</marker>
<rawString>Chen, A., Zhou, Y., Zhang, A., and Sun, G. (2005). Unigram language model for chinese word segmentation. In Proceedings of the 4th SIGHAN Workshop on Chinese Language Processing, pages 138–141. Association for Computational Linguistics Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dasgupta</author>
<author>M L Littman</author>
<author>D McAllester</author>
</authors>
<title>Pac generalization bounds for cotraining. Advances in neural information processing systems,</title>
<date>2002</date>
<pages>1--375</pages>
<contexts>
<context position="8870" citStr="Dasgupta et al., 2002" startWordPosition="1430" endWordPosition="1433">led corpora Ua and Ub (one is an in-domain corpus and the other is an out-of-domain corpus). Our algorithm is shown in Table 3. During each iteration, we tag the unlabeled corpus Ua using Tb to get pseudo-labels. Then we extract features from the pseudo-labels. We use the label distribution information as dynamic features. We add these features to the training data to train a new tagger Ta. To adjust the feature values, we extract features from one corpus and then apply the statistics to the other corpus. This is similar to the principle of cotraining (Yarowsky, 1995; Blum and Mitchell, 1998; Dasgupta et al., 2002). The difference is that there are not different views of features, but different kinds of unlabeled data. Detailed description of features is given in the next section. Algorithm Init: Using baseline features only: Train an initial tagger T0 based on L () Label Ua and Ub individually using T0 BEGIN LOOP: Generate DSFs from tagged Ua Augment L with DSFs to get La Generate DSFs from tagged Ub Augment L with DSFs to get Lb Using baseline features, SSFs and DSFs: Train new tagger Ta using La Train new tagger Tb using Lb Label Ua using Tb Label Ub using Ta LOOP until performance does not improve R</context>
</contexts>
<marker>Dasgupta, Littman, McAllester, 2002</marker>
<rawString>Dasgupta, S., Littman, M. L., and McAllester, D. (2002). Pac generalization bounds for cotraining. Advances in neural information processing systems, 1:375–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Feng</author>
<author>K Chen</author>
<author>X Deng</author>
<author>W Zheng</author>
</authors>
<title>Accessor variety criteria for chinese word extraction.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="10835" citStr="Feng et al. (2004)" startWordPosition="1774" endWordPosition="1777"> example is “+全+美” (Perfect), which is a Chinese idiom with structure “ABAC”. 313 2.4.2 Static statistical features Statistical features are statistics that distilled from the large unlabeled corpus. They are proved useful in the Chinese word segmentation task. We define Static Statistical Features (SSFs) as features whose value do not change during the training process. The SSFs in our approach includes Mutual information, Punctuation information and Accessor variety. Previous works have already explored the functions of the three static statistics in the Chinese word segmentation task, e.g. Feng et al. (2004); Sun and Xu (2011). We mainly follow their definitions while considering more details and giving some modification. Mutual information Mutual information (MI) is a quantity that measures the mutual dependence of two random variables. Previous works showed that larger MI of two strings claims higher probability that the two strings should be combined. Therefore, MI can show the tendency of two strings forming one word. However, previous works mainly focused on the balanced case, i.e., the MI of strings with the same length. In our study we find that, in Chinese, there remains large amount of i</context>
<context position="12876" citStr="Feng et al. (2004)" startWordPosition="2115" endWordPosition="2118">bility p in the corpus, we define the left punctuation rate LPRlen as the number of times the string appears after punctuations, divided by p. Similarly, the right punctuation rate RPRlen is defines as the number of times it appears preceding punctuations divided by its probability p. The length of string we consider is from 1 to 4. Accessor variety Accessor variety (AV) is also known as letter successor variety (LSV) (Harris, 1955; Hafer and Weiss, 1974). If a string appears after or preceding many different characters, this may provide some information of the string itself. Previous work of Feng et al. (2004), Sun and Xu (2011) used AV to represent this statistic. Similar to punctuation rate, we also consider both left AV and right AV. For a string s with length l, we define the left accessor variety (LAV) as the types of distinct characters preceding s in the corpus, and the right accessor variety (RAV) as the types of distinct characters after s in the corpus. The length of string we consider is also from 1 to 4. 2.4.3 Dynamic statistical features The unlabeled corpus lacks precise labels. We can use the trained tagger to give the unlabeled data “pseudo-labels”. These labels cannot guarantee an </context>
<context position="26476" citStr="Feng et al., 2004" startWordPosition="4427" endWordPosition="4430">e labeling approach in He and Wang (2008). We reimplemented their method to eliminate the difference of various CRFs implementations. Experiment results are shown in table 11. We can see that our methods works better, especially when handling the out-of-vocabulary named entities; 4 Related work Recent studies show that character sequence labeling is an effective method of Chinese word segmentation for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named entity recognition. Koo et al. (2008) applied this method to dependency parsing. Turian et al. (2010) applied this method to both named entity recogn</context>
</contexts>
<marker>Feng, Chen, Deng, Zheng, 2004</marker>
<rawString>Feng, H., Chen, K., Deng, X., and Zheng, W. (2004). Accessor variety criteria for chinese word extraction. Computational Linguistics, 30(1):75–93.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Gao</author>
<author>G Andrew</author>
<author>M Johnson</author>
</authors>
<booktitle>and COLING/ACL on Main conference poster sessions,</booktitle>
<pages>428--435</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Gao, Andrew, Johnson, </marker>
<rawString>Gao, J., Andrew, G., Johnson, M., and COLING/ACL on Main conference poster sessions, pages 428–435. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<date>2008</date>
<note>Simple semi-supervised dependency parsing.</note>
<contexts>
<context position="26964" citStr="Koo et al. (2008)" startWordPosition="4504" endWordPosition="4507">sed methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named entity recognition. Koo et al. (2008) applied this method to dependency parsing. Turian et al. (2010) applied this method to both named entity recognition and text chunking. These papers shared the same concept of word clustering. However, we cannot simply equal Chinese character to English word because characters in Chinese carry much less information than words in English and the clustering results is less meaningful. Features extracted from large unlabeled corpus in previous works mainly focus on statistical information of characters. Feng et al. (2004) used the accessor variety criterion to extract word types. Li and Sun (200</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Koo, T., Carreras, X., and Collins, M. (2008). Simple semi-supervised dependency parsing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<contexts>
<context position="18578" citStr="Lafferty et al. (2001)" startWordPosition="3109" endWordPosition="3112"> of DSFs are all percentage values. We can solve this by multiply the probability by an integer N and then take the integer part as the final feature value. We set the value of N by cross-validation.. 2.5 Conditional Random Fields Our algorithm is not necessarily limited to a specific baseline tagger. For simplicity and reliability, we use a simple Conditional Random Field (CRF) tagger, although other sequence labeling models like Semi-Markov CRF Gao et al. (2007) and Latent-variable CRF Sun et al. (2009) may provide better results than a single CRF. Detailed definition of CRF can be found in Lafferty et al. (2001); McCallum (2002); Pinto et al. (2003). 3 Experiment 3.1 Data and metrics We used the benchmark datasets provided by the second International Chinese Word Segmentation Bakeoff1 to test our approach. We chose the Peking University (PKU) data in our experiment. Although the benchmark provides another three data sets, two of them are data of traditional Chinese, which is quite different from simplified Chinese. Another is the data from Microsoft Research (MSR). We experimented on this data and got 97.45% in f-score compared to the state-of-art 97.4% reported in Sun et al. (2012). However, this co</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, J., McCallum, A., and Pereira, F. (2001). Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Li</author>
<author>M Sun</author>
</authors>
<title>Punctuation as implicit annotations for chinese word segmentation.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="27566" citStr="Li and Sun (2009)" startWordPosition="4601" endWordPosition="4604">o et al. (2008) applied this method to dependency parsing. Turian et al. (2010) applied this method to both named entity recognition and text chunking. These papers shared the same concept of word clustering. However, we cannot simply equal Chinese character to English word because characters in Chinese carry much less information than words in English and the clustering results is less meaningful. Features extracted from large unlabeled corpus in previous works mainly focus on statistical information of characters. Feng et al. (2004) used the accessor variety criterion to extract word types. Li and Sun (2009) used punctuation information in Chinese word segmentation by introducing extra labels ’L’ and ’R’. Chang and Han (2010), Sun and Xu (2011) used rich statistical information as discrete features in a sequence labeling framework. All these approaches can be viewed as using static statistics features in a supervised approach. Our method is different from theirs. For the static statistics features in our approach, we not only consider richer string pairs with the different lengths, but also consider term frequency when processing 318 P R F OOV Using one corpus 0.963 0.955 0.959 0.724 Our method 0</context>
</contexts>
<marker>Li, Sun, 2009</marker>
<rawString>Li, Z. and Sun, M. (2009). Punctuation as implicit annotations for chinese word segmentation. Computational Linguistics, 35(4):505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Low</author>
<author>H Ng</author>
<author>W Guo</author>
</authors>
<title>A maximum entropy approach to chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<volume>164</volume>
<location>Island,</location>
<contexts>
<context position="1550" citStr="Low et al., 2005" startWordPosition="216" endWordPosition="219">tasets show that our approach achieve good results and reach an f-score of 0.961. The feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task. 1 Introduction Chinese is a language without natural word delimiters. Therefore, Chinese Word Segmentation (CWS) is an essential task required by further language processing. Previous research shows that sequence labeling models trained on labeled data can reach competitive accuracy on the CWS task, and supervised models are more accurate than unsupervised models (Xue, 2003; Low et al., 2005). However, the resource of manually labeled training corpora is limited. Therefore, semi-supervised learning has become one *Corresponding author of the most natural forms of training for CWS. Traditional semi-supervised methods focus on adding new unlabeled instances to the training set by a given criterion. The possible mislabeled instances, which are introduced from the automatically labeled raw data, can hurt the performance and not easy to exclude by setting a sound selecting criterion. In this paper, we propose a simple and scalable semi-supervised strategy that works by providing semi-s</context>
<context position="26309" citStr="Low et al., 2005" startWordPosition="4400" endWordPosition="4403">ixth SIGHAN Workshop on Chinese Language Processing. It is the only NER corpus using simplified Chinese in that workshop. We compared our method with the pure sequence labeling approach in He and Wang (2008). We reimplemented their method to eliminate the difference of various CRFs implementations. Experiment results are shown in table 11. We can see that our methods works better, especially when handling the out-of-vocabulary named entities; 4 Related work Recent studies show that character sequence labeling is an effective method of Chinese word segmentation for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied thi</context>
</contexts>
<marker>Low, Ng, Guo, 2005</marker>
<rawString>Low, J., Ng, H., and Guo, W. (2005). A maximum entropy approach to chinese word segmentation. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, volume 164. Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Maosong</author>
<author>S Dayang</author>
<author>B Tsou</author>
</authors>
<title>Chinese word segmentation without using lexicon and hand-crafted training data.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics-Volume 2,</booktitle>
<pages>1265--1271</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="26430" citStr="Maosong et al., 1998" startWordPosition="4419" endWordPosition="4422">hop. We compared our method with the pure sequence labeling approach in He and Wang (2008). We reimplemented their method to eliminate the difference of various CRFs implementations. Experiment results are shown in table 11. We can see that our methods works better, especially when handling the out-of-vocabulary named entities; 4 Related work Recent studies show that character sequence labeling is an effective method of Chinese word segmentation for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named entity recognition. Koo et al. (2008) applied this method to dependency parsing. Turian et al. (2010) a</context>
</contexts>
<marker>Maosong, Dayang, Tsou, 1998</marker>
<rawString>Maosong, S., Dayang, S., and Tsou, B. (1998). Chinese word segmentation without using lexicon and hand-crafted training data. In Proceedings of the 17th international conference on Computational linguistics-Volume 2, pages 1265–1271. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
</authors>
<title>Efficiently inducing features of conditional random fields.</title>
<date>2002</date>
<booktitle>In Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>403--410</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="18595" citStr="McCallum (2002)" startWordPosition="3113" endWordPosition="3114">age values. We can solve this by multiply the probability by an integer N and then take the integer part as the final feature value. We set the value of N by cross-validation.. 2.5 Conditional Random Fields Our algorithm is not necessarily limited to a specific baseline tagger. For simplicity and reliability, we use a simple Conditional Random Field (CRF) tagger, although other sequence labeling models like Semi-Markov CRF Gao et al. (2007) and Latent-variable CRF Sun et al. (2009) may provide better results than a single CRF. Detailed definition of CRF can be found in Lafferty et al. (2001); McCallum (2002); Pinto et al. (2003). 3 Experiment 3.1 Data and metrics We used the benchmark datasets provided by the second International Chinese Word Segmentation Bakeoff1 to test our approach. We chose the Peking University (PKU) data in our experiment. Although the benchmark provides another three data sets, two of them are data of traditional Chinese, which is quite different from simplified Chinese. Another is the data from Microsoft Research (MSR). We experimented on this data and got 97.45% in f-score compared to the state-of-art 97.4% reported in Sun et al. (2012). However, this corpus is much larg</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>McCallum, A. (2002). Efficiently inducing features of conditional random fields. In Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence, pages 403–410. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>J Guinness</author>
<author>A Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<volume>4</volume>
<contexts>
<context position="26897" citStr="Miller et al. (2004)" startWordPosition="4493" endWordPosition="4496">ing (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named entity recognition. Koo et al. (2008) applied this method to dependency parsing. Turian et al. (2010) applied this method to both named entity recognition and text chunking. These papers shared the same concept of word clustering. However, we cannot simply equal Chinese character to English word because characters in Chinese carry much less information than words in English and the clustering results is less meaningful. Features extracted from large unlabeled corpus in previous works mainly focus on statistical information of characters. Feng et al. (2004) used th</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>Miller, S., Guinness, J., and Zamanian, A. (2004). Name tagging with word clusters and discriminative training. In Proceedings of HLT-NAACL, volume 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>D Schuurmans</author>
</authors>
<title>Selfsupervised chinese word segmentation.</title>
<date>2001</date>
<booktitle>Advances in Intelligent Data Analysis,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="26457" citStr="Peng and Schuurmans, 2001" startWordPosition="4423" endWordPosition="4426">ethod with the pure sequence labeling approach in He and Wang (2008). We reimplemented their method to eliminate the difference of various CRFs implementations. Experiment results are shown in table 11. We can see that our methods works better, especially when handling the out-of-vocabulary named entities; 4 Related work Recent studies show that character sequence labeling is an effective method of Chinese word segmentation for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named entity recognition. Koo et al. (2008) applied this method to dependency parsing. Turian et al. (2010) applied this method to both </context>
</contexts>
<marker>Peng, Schuurmans, 2001</marker>
<rawString>Peng, F. and Schuurmans, D. (2001). Selfsupervised chinese word segmentation. Advances in Intelligent Data Analysis, pages 238–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pinto</author>
<author>A McCallum</author>
<author>X Wei</author>
<author>W Croft</author>
</authors>
<title>Table extraction using conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,</booktitle>
<pages>235--242</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="18616" citStr="Pinto et al. (2003)" startWordPosition="3115" endWordPosition="3118">n solve this by multiply the probability by an integer N and then take the integer part as the final feature value. We set the value of N by cross-validation.. 2.5 Conditional Random Fields Our algorithm is not necessarily limited to a specific baseline tagger. For simplicity and reliability, we use a simple Conditional Random Field (CRF) tagger, although other sequence labeling models like Semi-Markov CRF Gao et al. (2007) and Latent-variable CRF Sun et al. (2009) may provide better results than a single CRF. Detailed definition of CRF can be found in Lafferty et al. (2001); McCallum (2002); Pinto et al. (2003). 3 Experiment 3.1 Data and metrics We used the benchmark datasets provided by the second International Chinese Word Segmentation Bakeoff1 to test our approach. We chose the Peking University (PKU) data in our experiment. Although the benchmark provides another three data sets, two of them are data of traditional Chinese, which is quite different from simplified Chinese. Another is the data from Microsoft Research (MSR). We experimented on this data and got 97.45% in f-score compared to the state-of-art 97.4% reported in Sun et al. (2012). However, this corpus is much larger than the PKU corpu</context>
</contexts>
<marker>Pinto, McCallum, Wei, Croft, 2003</marker>
<rawString>Pinto, D., McCallum, A., Wei, X., and Croft, W. (2003). Table extraction using conditional random fields. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 235–242. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Qi</author>
<author>P Kuksa</author>
<author>R Collobert</author>
<author>K Sadamasa</author>
<author>K Kavukcuoglu</author>
<author>J Weston</author>
</authors>
<title>Semi-supervised sequence labeling with selflearned features.</title>
<date>2009</date>
<booktitle>In Data Mining, 2009. ICDM’09. Ninth IEEE International Conference on,</booktitle>
<pages>428--437</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="29152" citStr="Qi et al. (2009)" startWordPosition="4862" endWordPosition="4865"> 0.954 Our method 0.965 0.958 0.961 Table 10: Comparison of our approach with the state-of-art systems P R F OOV Traditional 0.925 0.872 0.898 0.712 Our method 0.916 0.887 0.902 0.737 Table 11: Comparison of our approach with traditional NER systems punctuation features. There are previous works using features extracted from label distribution of unlabeled corpus in NLP tasks. Schapire et al. (2002) use a set of features annotated with majority labels to boost a logistic regression model. We are different from their approach because there is no pseudo-example labeling process in our approach. Qi et al. (2009) investigated on large set of distribution features and used these features in a self-training way. They applied the method on three tasks: named entity recognition, POS tagging and gene name recognition and got relatively good results. Our approach is different from theirs. Although we all consider label distribution, the way we use features are different. Besides, our approach uses two unlabeled corpora which can mutually enhancing to get better result. 5 Conclusion and Perspectives In this paper, we presented a semi-supervised method for Chinese word segmentation. Two kinds of new features </context>
</contexts>
<marker>Qi, Kuksa, Collobert, Sadamasa, Kavukcuoglu, Weston, 2009</marker>
<rawString>Qi, Y., Kuksa, P., Collobert, R., Sadamasa, K., Kavukcuoglu, K., and Weston, J. (2009). Semi-supervised sequence labeling with selflearned features. In Data Mining, 2009. ICDM’09. Ninth IEEE International Conference on, pages 428–437. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schapire</author>
<author>M Rochery</author>
<author>M Rahim</author>
</authors>
<date></date>
<marker>Schapire, Rochery, Rahim, </marker>
<rawString>Schapire, R., Rochery, M., Rahim, M., and</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Gupta</author>
</authors>
<title>Incorporating prior knowledge into boosting.</title>
<date>2002</date>
<booktitle>In MACHINE LEARNING-INTERNATIONAL WORKSHOP THEN CONFERENCE-,</booktitle>
<pages>538--545</pages>
<marker>Gupta, 2002</marker>
<rawString>Gupta, N. (2002). Incorporating prior knowledge into boosting. In MACHINE LEARNING-INTERNATIONAL WORKSHOP THEN CONFERENCE-, pages 538–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Sun</author>
<author>J Xu</author>
</authors>
<title>Enhancing chinese word segmentation using unlabeled data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>970--979</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3999" citStr="Sun and Xu, 2011" startWordPosition="618" endWordPosition="621">g the unlabeled data with the trained model on the training corpus. These “pseudo-labels” are not accurate enough. Therefore, we use the label distribution, which is much more accurate. To accurately calculate the precise label distribution, we use a framework similar to the cotraining algorithm to adjust the feature values iteratively. Generally speaking, unlabeled data can be classified as in-domain data and out-ofdomain data. In previous works these two kinds of unlabeled data are used separately for different purposes. In-domain data is mainly used to solve the problem of data sparseness (Sun and Xu, 2011). On the other hand, out-of domain data is used for domain adaptation (Chang and Han, 2010). In our work, we use in-domain and out-of-domain data together to adjust the labels of the unlabeled corpus. We evaluate the performance of CWS on the benchmark dataset of Peking University in the second International Chinese Word Segmentation Bakeoff. Experiment results show that our approach yields improvements compared with the state-of-art systems. Even when the labeled data is insufficient, our methods can still work better than traditional methods. Compared to the baseline CWS model, which has alr</context>
<context position="6798" citStr="Sun and Xu, 2011" startWordPosition="1094" endWordPosition="1097">this tag set in our method. An example of the “BMES” representation is shown in table 2. Character: 我 爱 北 京 天 安 门 Tag: S S B E B M E Table 2: An example for the “BMES” representation. The sentence is “我爱北京天安门” (I love Beijing Tian-an-men square), which consists of 4 Chinese words: “我” (I), “爱” (love), “北京” (Beijing), and “天安门” (Tian-an-men square). 2.2 Unlabeled Data Unlabeled data can be divided into in-domain data and out-of-domain data. In previous works, these two kinds of unlabeled data are used separately for different purposes. In-domain data only solves the problem of data sparseness (Sun and Xu, 2011). Out-of domain data is used only for domain adaptation (Chang and Han, 2010). These two functionalities are not contradictory but complementary. Our study shows 312 that by correctly designing features and algorithms, both in-domain unlabeled data and outof-domain unlabeled data can work together to help enhancing the segmentation model. In our algorithm, the dynamic features learned from one corpus can be adjusted incrementally with the dynamic features learned from the other corpus. As for the out-of-domain data, it will be even better if the corpus is not limited to a specific domain. We c</context>
<context position="9694" citStr="Sun and Xu, 2011" startWordPosition="1571" endWordPosition="1574">s only: Train an initial tagger T0 based on L () Label Ua and Ub individually using T0 BEGIN LOOP: Generate DSFs from tagged Ua Augment L with DSFs to get La Generate DSFs from tagged Ub Augment L with DSFs to get Lb Using baseline features, SSFs and DSFs: Train new tagger Ta using La Train new tagger Tb using Lb Label Ua using Tb Label Ub using Ta LOOP until performance does not improve RETURN the tagger which is trained with in-domain features. Table 3: Algorithm description 2.4 Features 2.4.1 Baseline Features Our baseline feature templates include the features described in previous works (Sun and Xu, 2011; Sun et al., 2012). These features are widely used in the CWS task. To be convenient, for a character ci with context ... ci_1cici+1..., its baseline features are listed below: • Character uni-grams: ck (i − 3 &lt; k &lt; i + 3) • Character bi-grams: ckck+1 (i − 3 &lt; k &lt; i + 2) • Whether ck and ck+1 are identical (i − 2 &lt; k &lt; i + 2) • Whether ck and ck+2 are identical (i − 4 &lt; k &lt; i + 2) The last two feature templates are designed to detect character reduplication, which is a morphological phenomenon in Chinese language. An example is “+全+美” (Perfect), which is a Chinese idiom with structure “ABAC”.</context>
<context position="12895" citStr="Sun and Xu (2011)" startWordPosition="2119" endWordPosition="2122">us, we define the left punctuation rate LPRlen as the number of times the string appears after punctuations, divided by p. Similarly, the right punctuation rate RPRlen is defines as the number of times it appears preceding punctuations divided by its probability p. The length of string we consider is from 1 to 4. Accessor variety Accessor variety (AV) is also known as letter successor variety (LSV) (Harris, 1955; Hafer and Weiss, 1974). If a string appears after or preceding many different characters, this may provide some information of the string itself. Previous work of Feng et al. (2004), Sun and Xu (2011) used AV to represent this statistic. Similar to punctuation rate, we also consider both left AV and right AV. For a string s with length l, we define the left accessor variety (LAV) as the types of distinct characters preceding s in the corpus, and the right accessor variety (RAV) as the types of distinct characters after s in the corpus. The length of string we consider is also from 1 to 4. 2.4.3 Dynamic statistical features The unlabeled corpus lacks precise labels. We can use the trained tagger to give the unlabeled data “pseudo-labels”. These labels cannot guarantee an acceptable precisio</context>
<context position="17282" citStr="Sun and Xu (2011)" startWordPosition="2892" endWordPosition="2895">fine Window DSF with current character position i, text context Ci and label l (the lth dimension in the list) as: W (Ci)l = P(y = l|Ci = xi−1xi+1) count(Ci = xi−1xi+1 ∧ y = l) = count(Ci = xi−1xi+1) In this equation, the numerator counts the number of times current context is xi−1xi+1 with label l. The denominator counts the number of times current context is xi−1xi+1. 2.4.4 Discrete features VS. Continuous features The statistical features may be expressed as real values. A more natural way is to use discrete values to incorporate them into the sequence labeling models . Previous works like Sun and Xu (2011) solve this problem by setting thresholds and converting the real value into boolean values. We use a different method to solve this, which does not need to consider tuning thresholds. In our method, we process static and dynamic statistical features using different strategies. 315 For static statistical value: For mutual information, we round the real value to their nearest integer. For punctuation rate and accessor variety, as the values tend to be large, we first get the log value of the feature and then use the nearest integer as the corresponding discrete value. For dynamic statistical va</context>
<context position="27705" citStr="Sun and Xu (2011)" startWordPosition="4624" endWordPosition="4627">xt chunking. These papers shared the same concept of word clustering. However, we cannot simply equal Chinese character to English word because characters in Chinese carry much less information than words in English and the clustering results is less meaningful. Features extracted from large unlabeled corpus in previous works mainly focus on statistical information of characters. Feng et al. (2004) used the accessor variety criterion to extract word types. Li and Sun (2009) used punctuation information in Chinese word segmentation by introducing extra labels ’L’ and ’R’. Chang and Han (2010), Sun and Xu (2011) used rich statistical information as discrete features in a sequence labeling framework. All these approaches can be viewed as using static statistics features in a supervised approach. Our method is different from theirs. For the static statistics features in our approach, we not only consider richer string pairs with the different lengths, but also consider term frequency when processing 318 P R F OOV Using one corpus 0.963 0.955 0.959 0.724 Our method 0.965 0.958 0.961 0.731 Table 9: Comparison of our approach with using only the Gigaword corpus Method P R F-score Best05 (Chen et al. (2005</context>
</contexts>
<marker>Sun, Xu, 2011</marker>
<rawString>Sun, W. and Xu, J. (2011). Enhancing chinese word segmentation using unlabeled data. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 970–979. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Sun</author>
<author>H Wang</author>
<author>W Li</author>
</authors>
<title>Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>253--262</pages>
<institution>Jeju Island, Korea. Association for Computational Linguistics.</institution>
<contexts>
<context position="9713" citStr="Sun et al., 2012" startWordPosition="1575" endWordPosition="1578">nitial tagger T0 based on L () Label Ua and Ub individually using T0 BEGIN LOOP: Generate DSFs from tagged Ua Augment L with DSFs to get La Generate DSFs from tagged Ub Augment L with DSFs to get Lb Using baseline features, SSFs and DSFs: Train new tagger Ta using La Train new tagger Tb using Lb Label Ua using Tb Label Ub using Ta LOOP until performance does not improve RETURN the tagger which is trained with in-domain features. Table 3: Algorithm description 2.4 Features 2.4.1 Baseline Features Our baseline feature templates include the features described in previous works (Sun and Xu, 2011; Sun et al., 2012). These features are widely used in the CWS task. To be convenient, for a character ci with context ... ci_1cici+1..., its baseline features are listed below: • Character uni-grams: ck (i − 3 &lt; k &lt; i + 3) • Character bi-grams: ckck+1 (i − 3 &lt; k &lt; i + 2) • Whether ck and ck+1 are identical (i − 2 &lt; k &lt; i + 2) • Whether ck and ck+2 are identical (i − 4 &lt; k &lt; i + 2) The last two feature templates are designed to detect character reduplication, which is a morphological phenomenon in Chinese language. An example is “+全+美” (Perfect), which is a Chinese idiom with structure “ABAC”. 313 2.4.2 Static s</context>
<context position="19160" citStr="Sun et al. (2012)" startWordPosition="3205" endWordPosition="3208"> be found in Lafferty et al. (2001); McCallum (2002); Pinto et al. (2003). 3 Experiment 3.1 Data and metrics We used the benchmark datasets provided by the second International Chinese Word Segmentation Bakeoff1 to test our approach. We chose the Peking University (PKU) data in our experiment. Although the benchmark provides another three data sets, two of them are data of traditional Chinese, which is quite different from simplified Chinese. Another is the data from Microsoft Research (MSR). We experimented on this data and got 97.45% in f-score compared to the state-of-art 97.4% reported in Sun et al. (2012). However, this corpus is much larger than the PKU corpus. Using the labeled data alone can get a relatively good tagger and the unlabeled data contributes little to the performance. For simplicity and efficiency, our further 1http://www.sighan.org/bakeoff2005/ experiments are all conducted on the PKU data. Details of the PKU data are listed in table 4. We also used two un-segmented corpora as unlabeled data. The first one is Chinese Gigaword2 corpus. It is a comprehensive archive of newswire data. The second one is articles from Baike3 of baidu.com. It is a Chinese encyclopedia similar to Wik</context>
<context position="28523" citStr="Sun et al. (2012)" startWordPosition="4758" endWordPosition="4761">different from theirs. For the static statistics features in our approach, we not only consider richer string pairs with the different lengths, but also consider term frequency when processing 318 P R F OOV Using one corpus 0.963 0.955 0.959 0.724 Our method 0.965 0.958 0.961 0.731 Table 9: Comparison of our approach with using only the Gigaword corpus Method P R F-score Best05 (Chen et al. (2005)) 0.953 0.946 0.950 CRF + rule-system (Zhang et al. (2006)) 0.947 0.955 0.951 Semi-perceptron (Zhang and Clark (2007)) N/A N/A 0.945 Latent-variable CRF (Sun et al. (2009)) 0.956 0.948 0.952 ADF-CRF (Sun et al. (2012)) 0.958 0.949 0.954 Our method 0.965 0.958 0.961 Table 10: Comparison of our approach with the state-of-art systems P R F OOV Traditional 0.925 0.872 0.898 0.712 Our method 0.916 0.887 0.902 0.737 Table 11: Comparison of our approach with traditional NER systems punctuation features. There are previous works using features extracted from label distribution of unlabeled corpus in NLP tasks. Schapire et al. (2002) use a set of features annotated with majority labels to boost a logistic regression model. We are different from their approach because there is no pseudo-example labeling process in o</context>
</contexts>
<marker>Sun, Wang, Li, 2012</marker>
<rawString>Sun, X., Wang, H., and Li, W. (2012). Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 253–262, Jeju Island, Korea. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Sun</author>
<author>Y Zhang</author>
<author>T Matsuzaki</author>
<author>Y Tsuruoka</author>
<author>J Tsujii</author>
</authors>
<title>A discriminative latent variable chinese segmenter with hybrid word/character information.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>56--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="18466" citStr="Sun et al. (2009)" startWordPosition="3089" endWordPosition="3092">value. For dynamic statistical value: Dynamic statistical features are distributions of a label. The values of DSFs are all percentage values. We can solve this by multiply the probability by an integer N and then take the integer part as the final feature value. We set the value of N by cross-validation.. 2.5 Conditional Random Fields Our algorithm is not necessarily limited to a specific baseline tagger. For simplicity and reliability, we use a simple Conditional Random Field (CRF) tagger, although other sequence labeling models like Semi-Markov CRF Gao et al. (2007) and Latent-variable CRF Sun et al. (2009) may provide better results than a single CRF. Detailed definition of CRF can be found in Lafferty et al. (2001); McCallum (2002); Pinto et al. (2003). 3 Experiment 3.1 Data and metrics We used the benchmark datasets provided by the second International Chinese Word Segmentation Bakeoff1 to test our approach. We chose the Peking University (PKU) data in our experiment. Although the benchmark provides another three data sets, two of them are data of traditional Chinese, which is quite different from simplified Chinese. Another is the data from Microsoft Research (MSR). We experimented on this d</context>
<context position="28477" citStr="Sun et al. (2009)" startWordPosition="4750" endWordPosition="4753">tures in a supervised approach. Our method is different from theirs. For the static statistics features in our approach, we not only consider richer string pairs with the different lengths, but also consider term frequency when processing 318 P R F OOV Using one corpus 0.963 0.955 0.959 0.724 Our method 0.965 0.958 0.961 0.731 Table 9: Comparison of our approach with using only the Gigaword corpus Method P R F-score Best05 (Chen et al. (2005)) 0.953 0.946 0.950 CRF + rule-system (Zhang et al. (2006)) 0.947 0.955 0.951 Semi-perceptron (Zhang and Clark (2007)) N/A N/A 0.945 Latent-variable CRF (Sun et al. (2009)) 0.956 0.948 0.952 ADF-CRF (Sun et al. (2012)) 0.958 0.949 0.954 Our method 0.965 0.958 0.961 Table 10: Comparison of our approach with the state-of-art systems P R F OOV Traditional 0.925 0.872 0.898 0.712 Our method 0.916 0.887 0.902 0.737 Table 11: Comparison of our approach with traditional NER systems punctuation features. There are previous works using features extracted from label distribution of unlabeled corpus in NLP tasks. Schapire et al. (2002) use a set of features annotated with majority labels to boost a logistic regression model. We are different from their approach because th</context>
</contexts>
<marker>Sun, Zhang, Matsuzaki, Tsuruoka, Tsujii, 2009</marker>
<rawString>Sun, X., Zhang, Y., Matsuzaki, T., Tsuruoka, Y., and Tsujii, J. (2009). A discriminative latent variable chinese segmenter with hybrid word/character information. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 56–64. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="27028" citStr="Turian et al. (2010)" startWordPosition="4515" endWordPosition="4518"> (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named entity recognition. Koo et al. (2008) applied this method to dependency parsing. Turian et al. (2010) applied this method to both named entity recognition and text chunking. These papers shared the same concept of word clustering. However, we cannot simply equal Chinese character to English word because characters in Chinese carry much less information than words in English and the clustering results is less meaningful. Features extracted from large unlabeled corpus in previous works mainly focus on statistical information of characters. Feng et al. (2004) used the accessor variety criterion to extract word types. Li and Sun (2009) used punctuation information in Chinese word segmentation by </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Turian, J., Ratinov, L., and Bengio, Y. (2010). Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="1531" citStr="Xue, 2003" startWordPosition="214" endWordPosition="215">enchmark datasets show that our approach achieve good results and reach an f-score of 0.961. The feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task. 1 Introduction Chinese is a language without natural word delimiters. Therefore, Chinese Word Segmentation (CWS) is an essential task required by further language processing. Previous research shows that sequence labeling models trained on labeled data can reach competitive accuracy on the CWS task, and supervised models are more accurate than unsupervised models (Xue, 2003; Low et al., 2005). However, the resource of manually labeled training corpora is limited. Therefore, semi-supervised learning has become one *Corresponding author of the most natural forms of training for CWS. Traditional semi-supervised methods focus on adding new unlabeled instances to the training set by a given criterion. The possible mislabeled instances, which are introduced from the automatically labeled raw data, can hurt the performance and not easy to exclude by setting a sound selecting criterion. In this paper, we propose a simple and scalable semi-supervised strategy that works </context>
<context position="5853" citStr="Xue (2003)" startWordPosition="922" endWordPosition="923">g manually predefined feature templates. • Compared with previous work, our method achieved a new state-of-art accuracy on the CWS task as well as on the NER task. The remaining part of this paper is organized as follows. Section 2 describes the details of the problem and our algorithm. Section 3 describes the experiment and presents the results. Section 4 reviews the related work. Section 5 concludes this paper. 2 System Architecture 2.1 Sequence Labeling Nowadays the character-based sequence labeling approach is widely used for the Chinese word segmentation problem. It was first proposed in Xue (2003), which assigns each character a label to indicate its position in the word. The most prevalent tag set is the BMES tag set, which uses 4 tags to carry word boundary information. This tag set uses B, M, E and S to represent the Beginning, the Middle, the End of a word and a Single character forming a word respectively. We use this tag set in our method. An example of the “BMES” representation is shown in table 2. Character: 我 爱 北 京 天 安 门 Tag: S S B E B M E Table 2: An example for the “BMES” representation. The sentence is “我爱北京天安门” (I love Beijing Tian-an-men square), which consists of 4 Chine</context>
<context position="26291" citStr="Xue, 2003" startWordPosition="4398" endWordPosition="4399">us of the sixth SIGHAN Workshop on Chinese Language Processing. It is the only NER corpus using simplified Chinese in that workshop. We compared our method with the pure sequence labeling approach in He and Wang (2008). We reimplemented their method to eliminate the difference of various CRFs implementations. Experiment results are shown in table 11. We can see that our methods works better, especially when handling the out-of-vocabulary named entities; 4 Related work Recent studies show that character sequence labeling is an effective method of Chinese word segmentation for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. </context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Xue, N. (2003). Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8(1):29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8821" citStr="Yarowsky, 1995" startWordPosition="1424" endWordPosition="1425">uppose we have labeled data L, two unlabeled corpora Ua and Ub (one is an in-domain corpus and the other is an out-of-domain corpus). Our algorithm is shown in Table 3. During each iteration, we tag the unlabeled corpus Ua using Tb to get pseudo-labels. Then we extract features from the pseudo-labels. We use the label distribution information as dynamic features. We add these features to the training data to train a new tagger Ta. To adjust the feature values, we extract features from one corpus and then apply the statistics to the other corpus. This is similar to the principle of cotraining (Yarowsky, 1995; Blum and Mitchell, 1998; Dasgupta et al., 2002). The difference is that there are not different views of features, but different kinds of unlabeled data. Detailed description of features is given in the next section. Algorithm Init: Using baseline features only: Train an initial tagger T0 based on L () Label Ua and Ub individually using T0 BEGIN LOOP: Generate DSFs from tagged Ua Augment L with DSFs to get La Generate DSFs from tagged Ub Augment L with DSFs to get Lb Using baseline features, SSFs and DSFs: Train new tagger Ta using La Train new tagger Tb using Lb Label Ua using Tb Label Ub u</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>Yarowsky, D. (1995). Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, pages 189–196. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zhang</author>
<author>G Kikui</author>
<author>E Sumita</author>
</authors>
<title>Subword-based tagging by conditional random fields for chinese word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<pages>193--196</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25083" citStr="Zhang et al. (2006)" startWordPosition="4193" endWordPosition="4196">lgorithm to extract DSFs from the Chinese Giga word corpus and apply the learned features to itself. 317 Figure 1: Learning curve of using different size of labeled data Table 9 shows the result. We can see that our method outperforms by +0.2% in f-score and +0.7% in OOV-Recall. Finally, we compared our method with the state-of-art systems reported in the previous papers. Table 10 listed the results. Best05 represents the best system reported on the Second International Chinese Word Segmentation Bakeoff. CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al. (2006). Other three systems all represent the methods using their corresponding model in the corresponding papers. Note that these state-of-art systems are either using complicated models with semi-Markov relaxations or latent variables, or modifying models to fit special conditions. Our system uses a single CRF model. As we can see in table 10, our method achieved higher F-scores than the previous best systems. 3.3 Results on NER task Our method is not limited to the CWS problem. It is applicable to all sequence labeling problems. We applied our method on the Chinese NER task. We used the MSR corpu</context>
<context position="28364" citStr="Zhang et al. (2006)" startWordPosition="4733" endWordPosition="4736">screte features in a sequence labeling framework. All these approaches can be viewed as using static statistics features in a supervised approach. Our method is different from theirs. For the static statistics features in our approach, we not only consider richer string pairs with the different lengths, but also consider term frequency when processing 318 P R F OOV Using one corpus 0.963 0.955 0.959 0.724 Our method 0.965 0.958 0.961 0.731 Table 9: Comparison of our approach with using only the Gigaword corpus Method P R F-score Best05 (Chen et al. (2005)) 0.953 0.946 0.950 CRF + rule-system (Zhang et al. (2006)) 0.947 0.955 0.951 Semi-perceptron (Zhang and Clark (2007)) N/A N/A 0.945 Latent-variable CRF (Sun et al. (2009)) 0.956 0.948 0.952 ADF-CRF (Sun et al. (2012)) 0.958 0.949 0.954 Our method 0.965 0.958 0.961 Table 10: Comparison of our approach with the state-of-art systems P R F OOV Traditional 0.925 0.872 0.898 0.712 Our method 0.916 0.887 0.902 0.737 Table 11: Comparison of our approach with traditional NER systems punctuation features. There are previous works using features extracted from label distribution of unlabeled corpus in NLP tasks. Schapire et al. (2002) use a set of features ann</context>
</contexts>
<marker>Zhang, Kikui, Sumita, 2006</marker>
<rawString>Zhang, R., Kikui, G., and Sumita, E. (2006). Subword-based tagging by conditional random fields for chinese word segmentation. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 193–196. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Clark</author>
</authors>
<title>Chinese segmentation with a word-based perceptron algorithm.</title>
<date>2007</date>
<booktitle>In ANNUAL MEETINGASSOCIATION FOR COMPUTATIONAL LINGUISTICS,</booktitle>
<volume>45</volume>
<pages>840</pages>
<contexts>
<context position="28423" citStr="Zhang and Clark (2007)" startWordPosition="4741" endWordPosition="4744">ese approaches can be viewed as using static statistics features in a supervised approach. Our method is different from theirs. For the static statistics features in our approach, we not only consider richer string pairs with the different lengths, but also consider term frequency when processing 318 P R F OOV Using one corpus 0.963 0.955 0.959 0.724 Our method 0.965 0.958 0.961 0.731 Table 9: Comparison of our approach with using only the Gigaword corpus Method P R F-score Best05 (Chen et al. (2005)) 0.953 0.946 0.950 CRF + rule-system (Zhang et al. (2006)) 0.947 0.955 0.951 Semi-perceptron (Zhang and Clark (2007)) N/A N/A 0.945 Latent-variable CRF (Sun et al. (2009)) 0.956 0.948 0.952 ADF-CRF (Sun et al. (2012)) 0.958 0.949 0.954 Our method 0.965 0.958 0.961 Table 10: Comparison of our approach with the state-of-art systems P R F OOV Traditional 0.925 0.872 0.898 0.712 Our method 0.916 0.887 0.902 0.737 Table 11: Comparison of our approach with traditional NER systems punctuation features. There are previous works using features extracted from label distribution of unlabeled corpus in NLP tasks. Schapire et al. (2002) use a set of features annotated with majority labels to boost a logistic regression </context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Zhang, Y. and Clark, S. (2007). Chinese segmentation with a word-based perceptron algorithm. In ANNUAL MEETINGASSOCIATION FOR COMPUTATIONAL LINGUISTICS, volume 45, page 840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhao</author>
<author>C Huang</author>
<author>M Li</author>
</authors>
<title>An improved chinese word segmentation system with conditional random field.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<volume>117</volume>
<location>Sydney:</location>
<contexts>
<context position="26328" citStr="Zhao et al., 2006" startWordPosition="4404" endWordPosition="4407">op on Chinese Language Processing. It is the only NER corpus using simplified Chinese in that workshop. We compared our method with the pure sequence labeling approach in He and Wang (2008). We reimplemented their method to eliminate the difference of various CRFs implementations. Experiment results are shown in table 11. We can see that our methods works better, especially when handling the out-of-vocabulary named entities; 4 Related work Recent studies show that character sequence labeling is an effective method of Chinese word segmentation for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named e</context>
</contexts>
<marker>Zhao, Huang, Li, 2006</marker>
<rawString>Zhao, H., Huang, C., and Li, M. (2006a). An improved chinese word segmentation system with conditional random field. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, volume 117. Sydney: July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhao</author>
<author>C Huang</author>
<author>M Li</author>
<author>B Lu</author>
</authors>
<title>Effective tag set selection in chinese word segmentation via conditional random field modeling.</title>
<date>2006</date>
<booktitle>In Proceedings of PACLIC,</booktitle>
<volume>20</volume>
<pages>87--94</pages>
<contexts>
<context position="26328" citStr="Zhao et al., 2006" startWordPosition="4404" endWordPosition="4407">op on Chinese Language Processing. It is the only NER corpus using simplified Chinese in that workshop. We compared our method with the pure sequence labeling approach in He and Wang (2008). We reimplemented their method to eliminate the difference of various CRFs implementations. Experiment results are shown in table 11. We can see that our methods works better, especially when handling the out-of-vocabulary named entities; 4 Related work Recent studies show that character sequence labeling is an effective method of Chinese word segmentation for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b). These supervised methods show good results. Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and Tanaka-Ishii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems. These methods need no annotated corpus, and most of them use statistics to help model the problem. However, they usually are less accurate than supervised ones. Currently “feature-engineering” methods have been successfully applied into NLP applications. Miller et al. (2004) applied this method to named e</context>
</contexts>
<marker>Zhao, Huang, Li, Lu, 2006</marker>
<rawString>Zhao, H., Huang, C., Li, M., and Lu, B. (2006b). Effective tag set selection in chinese word segmentation via conditional random field modeling. In Proceedings of PACLIC, volume 20, pages 87–94.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004352">
<title confidence="0.97986">
Automatic Feature Engineering for Answer Selection and Extraction
</title>
<author confidence="0.996095">
Aliaksei Severyn Alessandro Moschitti
</author>
<affiliation confidence="0.7631835">
DISI, University of Trento Qatar Computing Research Institue
38123 Povo (TN), Italy 5825 Doha, Qatar
</affiliation>
<email confidence="0.995633">
severyn@disi.unitn.it amoschitti@qf.org.qa
</email>
<sectionHeader confidence="0.998562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999916714285714">
This paper proposes a framework for automat-
ically engineering features for two important
tasks of question answering: answer sentence
selection and answer extraction. We represent
question and answer sentence pairs with lin-
guistic structures enriched by semantic infor-
mation, where the latter is produced by auto-
matic classifiers, e.g., question classifier and
Named Entity Recognizer. Tree kernels ap-
plied to such structures enable a simple way to
generate highly discriminative structural fea-
tures that combine syntactic and semantic in-
formation encoded in the input trees. We con-
duct experiments on a public benchmark from
TREC to compare with previous systems for
answer sentence selection and answer extrac-
tion. The results show that our models greatly
improve on the state of the art, e.g., up to 22%
on F1 (relative improvement) for answer ex-
traction, while using no additional resources
and no manual feature engineering.
</bodyText>
<sectionHeader confidence="0.999504" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994926125">
Question Answering (QA) systems are typically
built from three main macro-modules: (i) search and
retrieval of candidate passages; (ii) reranking or se-
lection of the most promising passages; and (iii) an-
swer extraction. The last two steps are the most in-
teresting from a Natural Language Processing view-
point since deep linguistic analysis can be carried
out as the input is just a limited set of candidates.
Answer sentence selection refers to the task of se-
lecting the sentence containing the correct answer
among the different sentence candidates retrieved by
a search engine.
Answer extraction is a final step, required for
factoid questions, consisting in extracting multi-
words constituting the synthetic answer, e.g., Barack
Obama for a question: Who is the US president?
The definition of rules for both tasks is conceptually
demanding and involves the use of syntactic and se-
mantic properties of the questions and its related an-
swer passages.
For example, given a question from TREC QA1:
Q: What was Johnny Appleseed’s real
name?
and a relevant passage, e.g., retrieved by a search
engine:
A: Appleseed, whose real name was John
Chapman, planted many trees in the early
1800s.
a rule detecting the semantic links between Johnny
Appleseed’s real name and the correct answer
John Chapman in the answer sentence has to
be engineered. This requires the definition of
other rules that associate the question pattern
real name ?(X) with real name is(X) of
the answer sentence. Although this can be done by
an expert NLP engineer, the effort for achieving the
necessary coverage and a reasonable accuracy is not
negligible.
An alternative to manual rule definition is the use
of machine learning, which often shifts the problem
</bodyText>
<footnote confidence="0.892838">
1We use it as our running example in the rest of the paper.
</footnote>
<page confidence="0.885799">
458
</page>
<note confidence="0.7393465">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 458–467,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999518125">
to the easier task of feature engineering. Unfortu-
nately, when the learning task is semantically dif-
ficult such as in QA, e.g., features have to encode
combinations of syntactic and semantic properties.
Thus their extraction modules basically assume the
shape of high-level rules, which are, in any case, es-
sential to achieve state-of-the-art accuracy. For ex-
ample, the great IBM Watson system (Ferrucci et
al., 2010) uses a learning to rank algorithm fed with
hundreds of features. The extraction of some of the
latter requires articulated rules/algorithms, which,
in terms of complexity, are very similar to those
constituting typical handcrafted QA systems. An
immediate consequence is the reduced adaptability
to new domains, which requires a substantial re-
engineering work.
In this paper, we show that tree kernels (Collins
and Duffy, 2002; Moschitti, 2006) can be applied to
automatically learn complex structural patterns for
both answer sentence selection and answer extrac-
tion. Such patterns are syntactic/semantic structures
occurring in question and answer passages. To make
such information available to the tree kernel func-
tions, we rely on the shallow syntactic trees enriched
with semantic information (Severyn et al., 2013b;
Severyn et al., 2013a), e.g., Named Entities (NEs)
and question focus and category, automatically de-
rived by machine learning modules, e.g., question
classifier (QC) or focus classifier (FC).
More in detail, we (i) design a pair of shallow
syntactic trees (one for the question and one for the
answer sentence); (ii) connect them with relational
nodes (i.e., those matching the same words in the
question and in the answer passages); (iii) label the
tree nodes with semantic information such as ques-
tion category and focus and NEs; and (iv) use the NE
type to establish additional semantic links between
the candidate answer, i.e., an NE, and the focus word
of the question. Finally, for the task of answer ex-
traction we also connect such semantic information
to the answer sentence trees such that we can learn
factoid answer patterns.
We show that our models are very effective in pro-
ducing features for both answer selection and ex-
traction by experimenting with TREC QA corpora
and directly comparing with the state of the art,
e.g., (Wang et al., 2007; Yao et al., 2013). The re-
sults show that our methods greatly improve on both
tasks yielding a large improvement in Mean Average
Precision for answer selection and in F1 for answer
extraction: up to 22% of relative improvement in F1,
when small training data is used. Moreover, in con-
trast to the previous work, our model does not rely
on external resources, e.g., WordNet, or complex
features in addition to the structural kernel model.
The reminder of this paper is organized as fol-
lows, Sec. 2 describes our kernel-based classifiers,
Sec. 3 illustrates our question/answer relational
structures also enriched with semantic information,
Sec. 4 describes our model for answer selection and
extraction, Sec. 5 illustrates our comparative exper-
iments on TREC data, Sec. 6 reports on our error
analysis, Sec. 7 discusses the related work, and fi-
nally, Sec. 8 derives the conclusions.
</bodyText>
<sectionHeader confidence="0.97922" genericHeader="introduction">
2 Structural Kernels for classification
</sectionHeader>
<bodyText confidence="0.999234666666667">
This section describes a kernel framework where the
input question/answer pairs are handled directly in
the form of syntactic/semantic structures.
</bodyText>
<subsectionHeader confidence="0.792495">
2.1 Feature vector approach to object pair
classification
</subsectionHeader>
<bodyText confidence="0.9999783125">
A conventional approach to represent a ques-
tion/answer pairs in linear models consists in defin-
ing a set of similarity features {xi} and computing
the simple scalar product h(x) = w · x = Ei wixi,
where w is the model weight vector learned on the
training data. Hence, the learning problem boils
down to estimating individual weights of each of
the similarity features xi. Such features often en-
code various types of lexical, syntactic and semantic
similarities shared between a question and its can-
didate. Previous work used a rich number of distri-
butional semantic, knowledge-based, translation and
paraphrase resources to build explicit feature vector
representations. One evident potential downside of
using feature vectors is that a great deal of structural
information encoded in a given text pair is lost.
</bodyText>
<subsectionHeader confidence="0.996964">
2.2 Pair Classification using Structural Kernels
</subsectionHeader>
<bodyText confidence="0.9886925">
A more versatile approach in terms of the input
representation relies on kernels. A typical ker-
nel machine, e.g., SVM, classifies a test input x
using the following prediction function: h(x) =
</bodyText>
<equation confidence="0.641566">
E
</equation>
<bodyText confidence="0.7575065">
i aiyiK(x,xi), where ai are the model parame-
ters estimated from the training data, yi are target
</bodyText>
<page confidence="0.997943">
459
</page>
<bodyText confidence="0.998967875">
variables, xi are support vectors, and K(·, ·) is a ker-
nel function. The latter can measure the similarity
between question and answer pairs.
We define each question/answer pair x as a triple
composed of a question tree T q and answer sentence
tree T s and a similarity feature vector v, i.e., x =
(T q,T s,v). Given two triples xi and xj, we define
the following kernel:
</bodyText>
<equation confidence="0.997203666666667">
K(xi,xj) = KTK(T iq,Tjq)
+ KTK(T is,Tjs) (1)
+ Kv(vi,vj),
</equation>
<bodyText confidence="0.999980481481481">
where KTK computes a structural kernel, e.g., tree
kernel, and Kv is a kernel over feature vectors, e.g.,
linear, polynomial, gaussian, etc. Structural kernels
can capture the structural representation of a ques-
tion/answer pair whereas traditional feature vectors
can encode some sort of similarity, e.g., lexical, syn-
tactic, semantic, between a question and its candi-
date answer.
We prefer to split the kernel computation over a
question/answer pair into two terms since tree ker-
nels are very efficient and there are no efficient
graph kernels that can encode exhaustively all graph
fragments. It should be noted that the tree kernel
sum does not capture feature pairs. Theoretically,
for such purpose, a kernel product should be used.
However, our experiments revealed that using the
product is actually worse in practice. In contrast,
we solve the lack of feature pairing by annotating
the trees with relational tags which are supposed
to link the question tree fragments with the related
fragments from the answer sentence.
Such relational information is very important to
improve the quality of the pair representation as well
as the implicitly generated features. In the next sec-
tion, we show simple structural models that we used
in our experiments for question and answer pair clas-
sification.
</bodyText>
<subsectionHeader confidence="0.999176">
2.3 Partial Tree Kernels
</subsectionHeader>
<bodyText confidence="0.99987475">
The above framework can use any kernel for
structural data. We use the Partial Tree Kernel
(PTK) (Moschitti, 2006) to compute KTK(·, ·) as it
is the most general convolution tree kernel, which
at the same time shows rather good efficiency. PTK
can be effectively applied to both constituency and
dependency parse trees. It generalizes the syntactic
tree kernel (STK) (Collins and Duffy, 2002), which
maps a tree into the space of all possible tree frag-
ments constrained by the rule that sibling nodes can-
not be separated. In contrast, the PTK fragments
can contain any subset of siblings, i.e., PTK allows
for breaking the production rules in syntactic trees.
Consequently, PTK generates an extremely rich fea-
ture space, which results in higher generalization
ability.
</bodyText>
<sectionHeader confidence="0.994624" genericHeader="method">
3 Relational Structures
</sectionHeader>
<bodyText confidence="0.9999615">
This section introduces relational structures de-
signed to encode syntactic and shallow semantic
properties of question/answer pairs. We first define a
simple to construct shallow syntactic tree represen-
tation derived from a shallow parser. Next, we in-
troduce a relational linking scheme based on a plain
syntactic matching and further augment it with ad-
ditional semantic information.
</bodyText>
<subsectionHeader confidence="0.999082">
3.1 Shallow syntactic tree
</subsectionHeader>
<bodyText confidence="0.998148">
Our shallow tree structure is a two-level syntactic
hierarchy built from word lemmas (leaves), part-of-
speech tags that organized into chunks identified by
a shallow syntactic parser (Fig. 1). We defined a
similar structure in (Severyn and Moschitti, 2012)
for answer passage reranking, which improved on
feature vector baselines.
This simple linguistic representation is suitable
for building a rather expressive answer sentence se-
lection model. Moreover, the use of a shallow parser
is motivated by the need to generate text spans to
produce candidate answers required by an answer
extraction system.
</bodyText>
<subsectionHeader confidence="0.999671">
3.2 Tree pairs enriched with relational links
</subsectionHeader>
<bodyText confidence="0.9999731">
It is important to establish a correspondence be-
tween question and answer sentence aligning related
concepts from both. We take on a two-level ap-
proach, where we first use plain lexical matching to
connect common lemmas from the question and its
candidate answer sentence. Secondly, we establish
semantic links between NEs extracted from the an-
swer sentence and the question focus word, which
encodes the expected lexical answer type (LAT). We
use the question categories to identify NEs that have
</bodyText>
<page confidence="0.998826">
460
</page>
<figureCaption confidence="0.860722">
Figure 1: Shallow tree representation of the example q/a pair from Sec. 1. Dashed arrows (red) indicate the tree
fragments (red dashed boxes) in the question and its answer sentence linked by the relational REL tag, which is
established via syntactic match on the word lemmas. Solid arrows (blue) connect a question focus word name with the
related named entities of type Person corresponding to the question category (HUM) via a relational tag REL-HUM.
Additional ANS tag is used to mark chunks containing candidate answer (here the correct answer John Chapman).
</figureCaption>
<bodyText confidence="0.99984006060606">
higher probability to be correct answers following a
mapping defined in Table 1.
Next, we briefly introduce our tree kernel-based
models for building question focus and category
classifiers.
Lexical Answer Type. Question Focus represents
a central entity or a property asked by a question
(Prager, 2006). It can be used to search for semanti-
cally compatible candidate answers, thus greatly re-
ducing the search space (Pinchak, 2006). While sev-
eral machine learning approaches based on manual
features and syntactic structures have been recently
explored, e.g. (Quarteroni et al., 2012; Damljanovic
et al., 2010; Bunescu and Huang, 2010), we opt for
the latter approach where tree kernels handle auto-
matic feature engineering.
To build an automatic Question Focus detector we
use a tree kernel approach as follows: we (i) parse
each question; (ii) create a set of positive trees by
labeling the node exactly covering the focus with
FC tag; (iii) build a set of negative trees by labeling
any other constituent node with FC; (iii) we train
the FC node classifier with tree kernels. At the test
time, we try to label each constituent node with FC
generating a set of candidate trees. Finally, we select
the tree and thus the constituent associated with the
highest SVM score.
Question classification. Our question classification
model is simpler than before: we use an SVM multi-
classifier with tree kernels to automatically extract
the question class. To build a multi-class classifier
we train a binary SVM for each of the classes and
apply a one-vs-all strategy to obtain the predicted
</bodyText>
<tableCaption confidence="0.99106">
Table 1: Expected Answer Type (EAT) → named entity
types.
</tableCaption>
<table confidence="0.945862">
EAT Named Entity types
HUM Person
LOCATION Location
ENTITY Organization, Person, Misc
DATE Date, Time, Number
QUANTITY Number, Percentage
CURRENCY Money, Number
</table>
<bodyText confidence="0.999529">
class. We use constituency trees as our input repre-
sentation.
Our question taxonomy is derived from the
UIUIC dataset (Li and Roth, 2002) which defines
6 coarse and 50 fine grain classes. In particular,
our set of question categories is formed by adopt-
ing 3 coarse classes: HUM (human), LOC (loca-
tion), ENTY (entities) and replacing the NUM (nu-
meric) coarse class with 3 fine-grain classes: CUR-
RENCY, DATE, QUANTITY2. This set of question
categories is sufficient to capture the coarse seman-
tic answer type of the candidate answers found in
TREC. Also using fewer question classes results in
a more accurate multi-class classifier.
Semantic tagging. Question focus word specifies
the lexical answer type capturing the target informa-
tion need posed by a question, but to make this piece
of information effective, the focus word needs to
be linked to the target candidate answer. The focus
word can be lexically matched with words present in
</bodyText>
<footnote confidence="0.979462">
2This class is composed by including all the fine-grain
classes from NUMERIC coarse class except for CURRENCY
and DATE.
</footnote>
<page confidence="0.998991">
461
</page>
<bodyText confidence="0.999918545454546">
the answer sentence, or the match can be established
using semantic information. Clearly, the latter ap-
proach is more appealing since it helps to alleviate
the lexical gap problem, i.e., it improves the cover-
age of the n¨aive string matching of words between a
question and its answer.
Hence, we propose to exploit a question focus
along with the related named entities (according to
the mapping from Table 1) of the answer sentence
to establish relational links between the tree frag-
ments. In particular, once the question focus and
question category are determined, we link the fo-
cus word wfocus in the question, with all the named
entities whose type matches the question class (Ta-
ble 1). We perform tagging at the chunk level and
use a relational tag typed with a question class, e.g.,
REL-HUM. Fig. 1 shows an example q/a pair where
the typed relational tag is used in the shallow syntac-
tic tree representation to link the chunk containing
the question focus name with the named entities of
the corresponding type Person, i.e., Appleseed and
John Chapman.
</bodyText>
<sectionHeader confidence="0.969228" genericHeader="method">
4 Answer Sentence Selection and Answer
Keyword Extraction
</sectionHeader>
<bodyText confidence="0.999893">
This section describes our approach to (i) answer
sentence selection used to select the most promising
answer sentences; and (ii) answer extraction which
returns the answer keyword (for factoid questions).
</bodyText>
<subsectionHeader confidence="0.99813">
4.1 Answer Sentence Selection
</subsectionHeader>
<bodyText confidence="0.9771486">
We cast the task of answer sentence selection as
a classification problem. Considering a supervised
learning scenario, we are given a set of questions
{�i}N i=1 where each question Qi is associated with
a list of candidate answer sentences {(ri, si)}Ni=1,
with ri E {−1,+1} indicating if a given candidate
answer sentence si contains a correct answer (+1)
or not (−1). Using this labeled data, our goal is to
learn a classifier model to predict if a given pair of
a question and an answer sentence is correct or not.
We train a binary SVM with tree kernels3 to train an
answer sentence classifier. The prediction scores ob-
tained from a classifier are used to rerank the answer
candidates (pointwise reranking), s.t. the sentences
that are more likely to contain correct answers will
3disi.unitn.it/moschitti/Tree-Kernel.htm
be ranked higher than incorrect candidates. In addi-
tion to the structural representation, we augment our
model with basic bag-of-word features (unigram and
bigrams) computed over lemmas.
</bodyText>
<subsectionHeader confidence="0.994124">
4.2 Answer Sentence Extraction
</subsectionHeader>
<bodyText confidence="0.999979848484849">
The goal of answer extraction is to extract a text span
from a given candidate answer sentence. Such span
represents a correct answer phrase for a given ques-
tion. Different from previous work that casts the an-
swer extraction task as a tagging problem and apply
a CRF to learn an answer phrase tagger (Yao et al.,
2013), we take on a simpler approach using a kernel-
based classifier.
In particular, we rely on the shallow tree represen-
tation, where text spans identified by a shallow syn-
tactic parser serve as a source of candidate answers.
Algorithm 1 specifies the steps to generate training
data for our classifier. In particular, for each ex-
ample representing a triple (a, Tq, Ts) composed of
the answer a, the question and the answer sentence
trees, we generate a set of training examples E with
every candidate chunk marked with an ANS tag (one
at a time). To reduce the number of generated exam-
ples for each answer sentence, we only consider NP
chunks, since other types of chunks, e.g., VP, ADJP,
typically do not contain factoid answers. Finally, an
original untagged tree is used to generate a positive
example (line 8), when the answer sentence contains
a correct answer, and a negative example (line 10),
when it does not contain a correct answer.
At the classification time, given a question and a
candidate answer sentence, all NP nodes of the sen-
tence are marked with ANS (one at a time) as the
possible answer, generating a set of tree candidates.
Then, such trees are classified (using the kernel from
Eq. 1) and the one with the highest score is selected.
If no tree is classified as positive example we do not
extract any answer.
</bodyText>
<sectionHeader confidence="0.999832" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999242166666667">
We provide the results on two related yet different
tasks: answer sentence selection and answer extrac-
tion. The goal of the former is to learn a model
scoring correct question and answer sentence pairs
to bring in the top positions sentences containing the
correct answers. Answer extraction derives the cor-
</bodyText>
<page confidence="0.997085">
462
</page>
<bodyText confidence="0.793165">
Algorithm 1 Generate training data for answer ex-
traction
</bodyText>
<listItem confidence="0.982701769230769">
1: for all (a, Tq, Ts) E D do
2: E +— 0
3: for all chunk E extract chunks(Ts) do
4: if not chunk == NP then
5: continue
6: Ts0 +— tagAnswerChunk(Ts, chunk)
7: if contains answer(a, chunk) then
8: label +— +1
9: else
10: label +— −1
11: e +— build example(Tq,T0s,label)
12: E +— E U fel
13: return E
</listItem>
<bodyText confidence="0.9608925">
rect answer keywords, i.e., a text span such as multi-
words or constituents, from a given sentence.
</bodyText>
<subsectionHeader confidence="0.962568">
5.1 Semantic Annotation
</subsectionHeader>
<bodyText confidence="0.999674208333333">
We briefly describe the experiments of training auto-
matic question category and focus classifiers, which
are more extensively described in (Severyn et al.,
2013b).
Question Focus detection. We used three datasets
for training and evaluating the performance of our
focus detector: SeCo-600 (Quarteroni et al., 2012),
Mooney GeoQuery (Damljanovic et al., 2010) and
the dataset from (Bunescu and Huang, 2010). The
SeCo dataset contains 600 questions. The Mooney
GeoQuery contains 250 question targeted at ge-
ographical information in the U.S. The first two
datasets are very domain specific, while the dataset
from (Bunescu and Huang, 2010) is more generic
containing the first 2,000 questions from the answer
type dataset from Li and Roth annotated with fo-
cus words. We removed questions with implicit and
multiple focuses.
Question Classification. We used the UIUIC
dataset (Li and Roth, 2002) which contains 5,952
factoid questions 4 to train a multi-class question
classifier.
Table 2 summarizes the results of question focus
and category classification.
</bodyText>
<footnote confidence="0.965019">
4We excluded questions from TREC to ensure there is no
overlap with the data used for testing models trained on TREC
QA.
</footnote>
<tableCaption confidence="0.9846555">
Table 2: Accuracy (%) of focus (FC) and question classi-
fiers (QC) using PTK.
</tableCaption>
<table confidence="0.645249285714286">
TASK SET PTK
MOONEY 80.5
FC SECO-600 90.0
BUNESCU 96.9
UIUIC 85.9
QC
TREC 11-12 78.1
</table>
<subsectionHeader confidence="0.994917">
5.2 Answer Sentence Selection
</subsectionHeader>
<bodyText confidence="0.999941090909091">
We used the train and test data from (Wang et al.,
2007) to enable direct comparison with previous
work on answer sentence selection. The training
data is composed by questions drawn from TREC
8-12 while questions from TREC 13 are used for
testing. The data provided for training comes as
two sets: a small set of 94 questions (TRAIN) that
were manually curated for errors5 and 1,229 ques-
tions from the entire TREC 8-12 that contain at least
one correct answer sentence (ALL). The latter set
represents a more noisy setting, since many answer
sentences are marked erroneously as correct as they
simply match a regular expression. Table 3 summa-
rizes the data used for training and testing.
Table 4 compares our kernel-based structural
model with the previous state-of-the-art systems for
answer sentence selection. In particular, we com-
pare with four most recent state of the art answer
sentence reranker models (Wang et al., 2007; Heil-
man and Smith, 2010; Wang and Manning, 2010;
Yao et al., 2013), which report their performance on
the same questions and candidate sets from TREC
13 as provided by (Wang et al., 2007).
Our simple shallow tree representation (Severyn
and Moschitti, 2012) delivers state-of-the-art ac-
curacy largely improving on previous work. Fi-
nally, augmenting the structure with semantic link-
ing (Severyn et al., 2013b) yields additional im-
provement in MAP and MRR. This suggests the
utility of using supervised components, e.g., ques-
tion focus and question category classifiers coupled
with NERs, to establish semantic mapping between
words in a q/a pair.
</bodyText>
<footnote confidence="0.986341">
5In TREC correct answers are identified by regex matching
using the provided answer pattern files
</footnote>
<page confidence="0.999754">
463
</page>
<tableCaption confidence="0.9637915">
Table 3: Summary of TREC data for answer extraction
used in (Yao et al., 2013).
</tableCaption>
<table confidence="0.99955725">
data questions candidates correct
TRAIN 94 4718 348
ALL 1229 53417 6410
TEST 89 1517 284
</table>
<tableCaption confidence="0.995652">
Table 4: Answer sentence reranking on TREC 13.
</tableCaption>
<table confidence="0.9995035">
System MAP MRR
Wang et al. (2007) 0.6029 0.6852
Heilman &amp; Smith (2010) 0.6091 0.6917
Wang &amp; Manning (2010) 0.5951 0.6951
Yao et al. (2013) 0.6319 0.7270
+ WN 0.6371 0.7301
shallow tree (S&amp;M, 2012) 0.6485 0.7244
+ semantic tagging 0.6781 0.7358
</table>
<bodyText confidence="0.99953925">
It is worth noting that our kernel-based classifier
is conceptually simpler than approaches in the previ-
ous work, as it relies on the structural kernels, e.g.,
PTK, to automatically extract salient syntactic pat-
terns relating questions and answers. Our model
only includes the most basic feature vector (uni- and
bi-grams) and does not rely on external sources such
as WordNet.
</bodyText>
<subsectionHeader confidence="0.998168">
5.3 Answer Extraction
</subsectionHeader>
<bodyText confidence="0.999986769230769">
Our experiments on answer extraction replicate the
setting of (Yao et al., 2013), which is the most recent
work on answer extraction reporting state-of-the-art
results.
Table 5 reports the accuracy of our model in re-
covering correct answers from a set of candidate an-
swer sentences for a given question. Here the fo-
cus is on the ability of an answer extraction system
to recuperate as many correct answers as possible
from each answer sentence candidate. The set of
extracted candidate answers can then be used to se-
lect a single best answer, which is the final output
of the QA system for factoid questions. Recall (R)
encodes the percentage of correct answer sentences
for which the system correctly extracts an answer
(for TREC 13 there are a total of 284 correct answer
sentences), while Precision (P) reflects how many
answers extracted by the system are actually correct.
Clearly, having a high recall system, allows for cor-
rectly answering more questions. On the other hand,
a high precision system would attempt to answer less
questions (extracting no answers at all) but get them
right.
We compare our results to a CRF model of (Yao et
al., 2013) augmented with WordNet features (with-
out forced voting) 6. Unlike the CRF model which
obtains higher values of precision, our system acts
as a high recall system able to recover most of the
answers from the correct answer sentences. Having
higher recall is favorable to high precision in answer
extraction since producing more correct answers can
help in the final voting scheme to come up with a
single best answer. To solve the low recall problem
of their CRF model, Yao et al. (2013) apply fairly
complex outlier resolution techniques to force an-
swer predictions, thus aiming at increasing the num-
ber of extracted answers.
To further boost the number of answers produced
by our system we exclude negative examples (an-
swer sentences not containing the correct answer)
from training, which slightly increases the number
of pairs with correctly recovered answers. Never-
theless, it has a substantial effect on the number of
questions that can be answered correctly (assuming
perfect single best answer selection). Clearly, our
system is able to recover a large number of answers
from the correct answer sentences, while low pre-
cision, i.e., extracting answer candidates from sen-
tences that do not contain a correct answer, can be
overcome by further applying various best answer
selection strategies, which we explore in the next
section.
</bodyText>
<subsectionHeader confidence="0.99993">
5.4 Best Answer Selection
</subsectionHeader>
<bodyText confidence="0.999870222222222">
Since the final step of the answer extraction module
is to select for each question a single best answer
from a set of extracted candidate answers, an answer
selection scheme is required.
We adopt a simple majority voting strategy, where
we aggregate the extracted answers produced by our
answer extraction model. Answers sharing simi-
lar lemmas (excluding stop words) are grouped to-
gether. The prediction scores obtained by the an-
</bodyText>
<footnote confidence="0.99153">
6We could not replicate the results obtained in (Yao et al.,
2013) with the forced voting strategy. Thus such result is not
included in Table 5.
</footnote>
<page confidence="0.999243">
464
</page>
<tableCaption confidence="0.966697125">
Table 5: Results on answer extraction. P/R - precision
and recall; pairs - number of QA pairs with a correctly ex-
tracted answer, q - number of questions with at least one
correct answer extracted, F1 sets an upper bound on the
performance assuming the selected best answer among
extracted candidates is always correct. *-marks the set-
ting where we exclude incorrect question answer pairs
from training.
</tableCaption>
<table confidence="0.999966777777778">
set P R pairs q F1
Yao et al. (2013) 25.7 23.4 73 33 -
+ WN 26.7 24.3 76 35 -
TRAIN 29.6 64.4 183 58 65.2
TRAIN* 15.7 71.8 204 66 74.1
Yao et al. (2013) 35.2 35.1 100 38 -
+ WN 34.5 34.7 98 38 -
ALL 29.4 74.6 212 69 77.5
ALL* 15.8 76.7 218 73 82.0
</table>
<tableCaption confidence="0.992608">
Table 6: Results on finding the best answer with voting.
</tableCaption>
<table confidence="0.999089888888889">
system set P R F1
Yao et al. (2013) TRAIN 55.7 43.8 49.1
+ forced 54.5 53.9 54.2
+ WN 55.2 53.9 54.5
this work 66.2 66.2 66.2
Yao et al. (2013) AL L 67.2 50.6 57.7
+ forced 60.9 59.6 60.2
+ WN 63.6 62.9 63.3
this work 70.8 70.8 70.8
</table>
<bodyText confidence="0.996846846153846">
swer extraction classifier are used as votes to decide
on the final rank to select the best single answer.
Table 6 shows the results after the majority vot-
ing is applied to select a single best answer for each
candidate. A rather naive majority voting scheme
already produces satisfactory outcome demonstrat-
ing better results than the previous work. Our vot-
ing scheme is similar to the one used by (Yao et al.,
2013), yet it is much simpler since we do not per-
form any additional hand tuning to account for the
weight of the “forced” votes or take any additional
steps to catch additional answers using outlier detec-
tion techniques applied in the previous work.
</bodyText>
<sectionHeader confidence="0.985226" genericHeader="discussions">
6 Discussion and Error Analysis
</sectionHeader>
<bodyText confidence="0.999979304347826">
There are several sources of errors affecting the fi-
nal performance of our answer extraction system: (i)
chunking, (ii) named entity recognition and seman-
tic linking, (iii) answer extraction, (iv) single best
answer selection.
Chunking. Our system uses text spans identified by
a chunker to extract answer candidates, which makes
it impossible to extract answers that lie outside the
chunk boundaries. Nevertheless, we found this to
be a minor concern since for 279 out of total 284
candidate sentences from TREC 13 the answers are
recoverable within the chunk spans.
Semantic linking. Our structural model relies heav-
ily on the ability of NER to identify the relevant en-
tities in the candidate sentence that can be further
linked to the focus word of the question. While
our answer extraction model is working on all the
NP chunks, the semantic tags from NER serve as a
strong cue for the classifier that a given chunk has
a high probability of containing an answer. Typical
off-the-shelf NER taggers have good precision and
low recall, s.t. many entities as potential answers are
missed. In this respect, a high recall entity linking
system, e.g., linking to wikipedia entities (Ratinov
et al., 2011), is required to boost the quality of can-
didates considered for answer extraction. Finally,
improving the accuracy of question and focus clas-
sifiers would allow for having more accurate input
representations fed to the learning algorithm.
Answer Extraction. Our answer extraction model
acts as a high recall system, while it suffers from
low precision in extracting answers for many incor-
rect sentences. Improving the precision without sac-
rificing the recall would ease the successive task of
best answer selection, since having less incorrect an-
swer candidates would result in a better final per-
formance. Introducing additional constraints in the
form of semantic tags to allow for better selection of
answer candidates could also improve our system.
Best Answer Selection. We apply a naive majority
voting scheme to select a single best answer from
a set of extracted answer candidates. This step has
a dramatic impact on the final performance of the
answer extraction system resulting in a large drop
of recall, i.e., from 82.0 to 70.8 before and after vot-
ing respectively. Hence, a more involved model, i.e.,
</bodyText>
<page confidence="0.998807">
465
</page>
<bodyText confidence="0.996961333333333">
performing joint answer sentence re-ranking and an-
swer extraction, is required to yield a better perfor-
mance.
</bodyText>
<sectionHeader confidence="0.999935" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999962258064517">
Tree kernel methods have found many applications
for the task of answer reranking which are reported
in (Moschitti, 2008; Moschitti, 2009; Moschitti and
Quarteroni, 2008; Severyn and Moschitti, 2012).
However, their methods lack the use of important
relational information between a question and a can-
didate answer, which is essential to learn accurate
relational patterns. In this respect, a solution based
on enumerating relational links was given in (Zan-
zotto and Moschitti, 2006; Zanzotto et al., 2009) for
the textual entailment task but it is computationally
too expensive for the large dataset of QA. A few so-
lutions to overcome computational issues were sug-
gested in (Zanzotto et al., 2010).
In contrast, this paper relies on structures directly
encoding the output of question and focus classifiers
to connect focus word and good candidate answer
keywords (represented by NEs) of the answer pas-
sage. This provides more effective relational infor-
mation, which allows our model to significantly im-
prove on previous rerankers. Additionally, previous
work on kernel-based approaches does not target an-
swer extraction.
One of the best models for answer sentence selec-
tion has been proposed in (Wang et al., 2007). They
use the paradigm of quasi-synchronous grammar to
model relations between a question and a candidate
answer with syntactic transformations. (Heilman
and Smith, 2010) develop an improved Tree Edit
Distance (TED) model for learning tree transforma-
tions in a q/a pair. They search for a good sequence
of tree edit operations using complex and com-
putationally expensive Tree Kernel-based heuristic.
(Wang and Manning, 2010) develop a probabilistic
model to learn tree-edit operations on dependency
parse trees. They cast the problem into the frame-
work of structured output learning with latent vari-
ables. The model of (Yao et al., 2013) has reported
an improvement over the Wang’s et al. (2007) sys-
tem. It applies linear chain CRFs with features de-
rived from TED and WordNet to automatically learn
associations between questions and candidate an-
swers.
Different from previous approaches that use tree-
edit information derived from syntactic trees, our
kernel-based learning approach also use tree struc-
tures but with rather different learning methods, i.e.,
SVMs and structural kernels, to automatically ex-
tract salient syntactic patterns relating questions and
answers. In (Severyn et al., 2013c), we have shown
that such relational structures encoding input text
pairs can be directly used within the kernel learning
framework to build state-of-the-art models for pre-
dicting semantic textual similarity. Furthermore, se-
mantically enriched relational structures, where au-
tomatic have been previously explored for answer
passage reranking in (Severyn et al., 2013b; Sev-
eryn et al., 2013a). This paper demonstrates that this
model also works for building a reranker on the sen-
tence level, and extends the previous work by apply-
ing the idea of automatic feature engineering with
tree kernels to answer extraction.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.99996768">
Our paper demonstrates the effectiveness of han-
dling the input structures representing QA pairs di-
rectly vs. using explicit feature vector representa-
tions, which typically require substantial feature en-
gineering effort. Our approach relies on a kernel-
based learning framework, where structural kernels,
e.g., tree kernels, are used to handle automatic fea-
ture engineering. It is enough to specify the desired
type of structures, e.g., shallow, constituency, de-
pendency trees, representing question and its can-
didate answer sentences and let the kernel learning
framework learn to use discriminative tree fragments
for the target task.
An important feature of our approach is that it
can effectively combine together different types of
syntactic and semantic information, also generated
by additional automatic classifiers, e.g., focus and
question classifiers. We augment the basic struc-
tures with additional relational and semantic infor-
mation by introducing special tag markers into the
tree nodes. Using the structures directly in the ker-
nel learning framework makes it easy to integrate
additional relational constraints and semantic infor-
mation directly in the structures.
The comparison with previous work on a public
</bodyText>
<page confidence="0.998611">
466
</page>
<bodyText confidence="0.999898263157895">
benchmark from TREC suggests that our approach
is very promising as we can improve the state of the
art in both answer selection and extraction by a large
margin (up to 22% of relative improvement in F1 for
answer extraction). Our approach makes it relatively
easy to integrate other sources of semantic informa-
tion, among which the use of Linked Open Data can
be the most promising to enrich the structural repre-
sentation of q/a pairs.
To achieve state-of-the-art results in answer sen-
tence selection and answer extraction, it is sufficient
to provide our model with a suitable tree structure
encoding relevant syntactic information, e.g., using
shallow, constituency or dependency formalisms.
Moreover, additional semantic and relational infor-
mation can be easily plugged in by marking tree
nodes with special tags. We believe this approach
greatly eases the task of tedious feature engineering
that will find its applications well beyond QA tasks.
</bodyText>
<sectionHeader confidence="0.998196" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999177333333333">
This research is partially supported by the EU’s
7th Framework Program (FP7/2007-2013) (#288024
LIMOSINE project) and an Open Collaborative Re-
search (OCR) award from IBM Research. The first
author is supported by the Google Europe Fellow-
ship 2013 award in Machine Learning.
</bodyText>
<sectionHeader confidence="0.999545" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999896256756757">
Razvan Bunescu and Yunfeng Huang. 2010. Towards a
general model of answer typing: Question focus iden-
tification. In CICLing.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over Dis-
crete Structures, and the Voted Perceptron. In ACL.
Danica Damljanovic, Milan Agatonovic, and Hamish
Cunningham. 2010. Identification of the question fo-
cus: Combining syntactic analysis and ontology-based
lookup through the user interaction. In LREC.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya Kalyanpur, Adam Lally,
J. William Murdock, Eric Nyberg, John Prager, Nico
Schlaefer, and Chris Welty. 2010. Building watson:
An overview of the deepqa project. AI Magazine,
31(3).
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In NAACL.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In COLING.
A. Moschitti and S. Quarteroni. 2008. Kernels on Lin-
guistic Structures for Answer Extraction. In ACL.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
ECML.
Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In CIKM.
Alessandro Moschitti. 2009. Syntactic and semantic ker-
nels for short text pair categorization. In EACL.
Christopher Pinchak. 2006. A probabilistic answer type
model. In In EACL.
John M. Prager. 2006. Open-domain question-
answering. Foundations and Trends in Information
Retrieval, 1(2):91–231.
Silvia Quarteroni, Vincenzo Guerrisi, and Pietro La
Torre. 2012. Evaluating multi-focus natural language
queries over data services. In LREC.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In ACL.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of an-
swer re-ranking. In SIGIR.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013a. Building structures from classifiers
for passage reranking. In CIKM.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013b. Learning adaptable patterns for
passage reranking. In CoNLL.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013c. Learning semantic textual similar-
ity with structural representations. In ACL.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In ACL.
Mengqiu Wang, Noah A. Smith, and Teruko Mitaura.
2007. What is the jeopardy model? a quasi-
synchronous grammar for qa. In EMNLP.
Xuchen Yao, Benjamin Van Durme, Peter Clark, and
Chris Callison-Burch. 2013. Answer extraction as se-
quence tagging with tree edit distance. In NAACL.
F. M. Zanzotto and A. Moschitti. 2006. Automatic
Learning of Textual Entailments with Cross-Pair Sim-
ilarities. In COLING.
F. M. Zanzotto, M. Pennacchiotti, and A. Moschitti.
2009. A Machine Learning Approach to Recognizing
Textual Entailment. Natural Language Engineering,
Volume 15 Issue 4, October 2009:551–582.
F. M. Zanzotto, L. Dell’Arciprete, and A. Moschitti.
2010. Efficient graph kernels for textual entail-
ment recognition. FUNDAMENTA INFORMATICAE,
2010.
</reference>
<page confidence="0.999297">
467
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.907902">
<title confidence="0.999966">Automatic Feature Engineering for Answer Selection and Extraction</title>
<author confidence="0.999793">Aliaksei Severyn Alessandro Moschitti</author>
<affiliation confidence="0.999913">DISI, University of Trento Qatar Computing Research Institue</affiliation>
<address confidence="0.999201">38123 Povo (TN), Italy 5825 Doha, Qatar</address>
<email confidence="0.91365">severyn@disi.unitn.itamoschitti@qf.org.qa</email>
<abstract confidence="0.999723727272727">This paper proposes a framework for automatically engineering features for two important tasks of question answering: answer sentence selection and answer extraction. We represent question and answer sentence pairs with linguistic structures enriched by semantic information, where the latter is produced by automatic classifiers, e.g., question classifier and Named Entity Recognizer. Tree kernels applied to such structures enable a simple way to generate highly discriminative structural features that combine syntactic and semantic information encoded in the input trees. We conduct experiments on a public benchmark from TREC to compare with previous systems for answer sentence selection and answer extraction. The results show that our models greatly improve on the state of the art, e.g., up to 22% on F1 (relative improvement) for answer extraction, while using no additional resources and no manual feature engineering.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Yunfeng Huang</author>
</authors>
<title>Towards a general model of answer typing: Question focus identification.</title>
<date>2010</date>
<booktitle>In CICLing.</booktitle>
<contexts>
<context position="13084" citStr="Bunescu and Huang, 2010" startWordPosition="2068" endWordPosition="2071">ability to be correct answers following a mapping defined in Table 1. Next, we briefly introduce our tree kernel-based models for building question focus and category classifiers. Lexical Answer Type. Question Focus represents a central entity or a property asked by a question (Prager, 2006). It can be used to search for semantically compatible candidate answers, thus greatly reducing the search space (Pinchak, 2006). While several machine learning approaches based on manual features and syntactic structures have been recently explored, e.g. (Quarteroni et al., 2012; Damljanovic et al., 2010; Bunescu and Huang, 2010), we opt for the latter approach where tree kernels handle automatic feature engineering. To build an automatic Question Focus detector we use a tree kernel approach as follows: we (i) parse each question; (ii) create a set of positive trees by labeling the node exactly covering the focus with FC tag; (iii) build a set of negative trees by labeling any other constituent node with FC; (iii) we train the FC node classifier with tree kernels. At the test time, we try to label each constituent node with FC generating a set of candidate trees. Finally, we select the tree and thus the constituent as</context>
<context position="20569" citStr="Bunescu and Huang, 2010" startWordPosition="3320" endWordPosition="3323">— +1 9: else 10: label +— −1 11: e +— build example(Tq,T0s,label) 12: E +— E U fel 13: return E rect answer keywords, i.e., a text span such as multiwords or constituents, from a given sentence. 5.1 Semantic Annotation We briefly describe the experiments of training automatic question category and focus classifiers, which are more extensively described in (Severyn et al., 2013b). Question Focus detection. We used three datasets for training and evaluating the performance of our focus detector: SeCo-600 (Quarteroni et al., 2012), Mooney GeoQuery (Damljanovic et al., 2010) and the dataset from (Bunescu and Huang, 2010). The SeCo dataset contains 600 questions. The Mooney GeoQuery contains 250 question targeted at geographical information in the U.S. The first two datasets are very domain specific, while the dataset from (Bunescu and Huang, 2010) is more generic containing the first 2,000 questions from the answer type dataset from Li and Roth annotated with focus words. We removed questions with implicit and multiple focuses. Question Classification. We used the UIUIC dataset (Li and Roth, 2002) which contains 5,952 factoid questions 4 to train a multi-class question classifier. Table 2 summarizes the resul</context>
</contexts>
<marker>Bunescu, Huang, 2010</marker>
<rawString>Razvan Bunescu and Yunfeng Huang. 2010. Towards a general model of answer typing: Question focus identification. In CICLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4037" citStr="Collins and Duffy, 2002" startWordPosition="623" endWordPosition="626">ion modules basically assume the shape of high-level rules, which are, in any case, essential to achieve state-of-the-art accuracy. For example, the great IBM Watson system (Ferrucci et al., 2010) uses a learning to rank algorithm fed with hundreds of features. The extraction of some of the latter requires articulated rules/algorithms, which, in terms of complexity, are very similar to those constituting typical handcrafted QA systems. An immediate consequence is the reduced adaptability to new domains, which requires a substantial reengineering work. In this paper, we show that tree kernels (Collins and Duffy, 2002; Moschitti, 2006) can be applied to automatically learn complex structural patterns for both answer sentence selection and answer extraction. Such patterns are syntactic/semantic structures occurring in question and answer passages. To make such information available to the tree kernel functions, we rely on the shallow syntactic trees enriched with semantic information (Severyn et al., 2013b; Severyn et al., 2013a), e.g., Named Entities (NEs) and question focus and category, automatically derived by machine learning modules, e.g., question classifier (QC) or focus classifier (FC). More in det</context>
<context position="9923" citStr="Collins and Duffy, 2002" startWordPosition="1576" endWordPosition="1579">ty of the pair representation as well as the implicitly generated features. In the next section, we show simple structural models that we used in our experiments for question and answer pair classification. 2.3 Partial Tree Kernels The above framework can use any kernel for structural data. We use the Partial Tree Kernel (PTK) (Moschitti, 2006) to compute KTK(·, ·) as it is the most general convolution tree kernel, which at the same time shows rather good efficiency. PTK can be effectively applied to both constituency and dependency parse trees. It generalizes the syntactic tree kernel (STK) (Collins and Duffy, 2002), which maps a tree into the space of all possible tree fragments constrained by the rule that sibling nodes cannot be separated. In contrast, the PTK fragments can contain any subset of siblings, i.e., PTK allows for breaking the production rules in syntactic trees. Consequently, PTK generates an extremely rich feature space, which results in higher generalization ability. 3 Relational Structures This section introduces relational structures designed to encode syntactic and shallow semantic properties of question/answer pairs. We first define a simple to construct shallow syntactic tree repre</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danica Damljanovic</author>
<author>Milan Agatonovic</author>
<author>Hamish Cunningham</author>
</authors>
<title>Identification of the question focus: Combining syntactic analysis and ontology-based lookup through the user interaction.</title>
<date>2010</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="13058" citStr="Damljanovic et al., 2010" startWordPosition="2064" endWordPosition="2067">John Chapman). higher probability to be correct answers following a mapping defined in Table 1. Next, we briefly introduce our tree kernel-based models for building question focus and category classifiers. Lexical Answer Type. Question Focus represents a central entity or a property asked by a question (Prager, 2006). It can be used to search for semantically compatible candidate answers, thus greatly reducing the search space (Pinchak, 2006). While several machine learning approaches based on manual features and syntactic structures have been recently explored, e.g. (Quarteroni et al., 2012; Damljanovic et al., 2010; Bunescu and Huang, 2010), we opt for the latter approach where tree kernels handle automatic feature engineering. To build an automatic Question Focus detector we use a tree kernel approach as follows: we (i) parse each question; (ii) create a set of positive trees by labeling the node exactly covering the focus with FC tag; (iii) build a set of negative trees by labeling any other constituent node with FC; (iii) we train the FC node classifier with tree kernels. At the test time, we try to label each constituent node with FC generating a set of candidate trees. Finally, we select the tree a</context>
<context position="20522" citStr="Damljanovic et al., 2010" startWordPosition="3312" endWordPosition="3315"> 7: if contains answer(a, chunk) then 8: label +— +1 9: else 10: label +— −1 11: e +— build example(Tq,T0s,label) 12: E +— E U fel 13: return E rect answer keywords, i.e., a text span such as multiwords or constituents, from a given sentence. 5.1 Semantic Annotation We briefly describe the experiments of training automatic question category and focus classifiers, which are more extensively described in (Severyn et al., 2013b). Question Focus detection. We used three datasets for training and evaluating the performance of our focus detector: SeCo-600 (Quarteroni et al., 2012), Mooney GeoQuery (Damljanovic et al., 2010) and the dataset from (Bunescu and Huang, 2010). The SeCo dataset contains 600 questions. The Mooney GeoQuery contains 250 question targeted at geographical information in the U.S. The first two datasets are very domain specific, while the dataset from (Bunescu and Huang, 2010) is more generic containing the first 2,000 questions from the answer type dataset from Li and Roth annotated with focus words. We removed questions with implicit and multiple focuses. Question Classification. We used the UIUIC dataset (Li and Roth, 2002) which contains 5,952 factoid questions 4 to train a multi-class qu</context>
</contexts>
<marker>Damljanovic, Agatonovic, Cunningham, 2010</marker>
<rawString>Danica Damljanovic, Milan Agatonovic, and Hamish Cunningham. 2010. Identification of the question focus: Combining syntactic analysis and ontology-based lookup through the user interaction. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Eric Brown</author>
<author>Jennifer Chu-Carroll</author>
<author>James Fan</author>
<author>David Gondek</author>
<author>Aditya Kalyanpur</author>
<author>Adam Lally</author>
<author>J William Murdock</author>
</authors>
<title>Eric Nyberg,</title>
<date>2010</date>
<journal>AI Magazine,</journal>
<volume>31</volume>
<issue>3</issue>
<location>John Prager, Nico</location>
<contexts>
<context position="3610" citStr="Ferrucci et al., 2010" startWordPosition="558" endWordPosition="561">r. 458 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 458–467, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics to the easier task of feature engineering. Unfortunately, when the learning task is semantically difficult such as in QA, e.g., features have to encode combinations of syntactic and semantic properties. Thus their extraction modules basically assume the shape of high-level rules, which are, in any case, essential to achieve state-of-the-art accuracy. For example, the great IBM Watson system (Ferrucci et al., 2010) uses a learning to rank algorithm fed with hundreds of features. The extraction of some of the latter requires articulated rules/algorithms, which, in terms of complexity, are very similar to those constituting typical handcrafted QA systems. An immediate consequence is the reduced adaptability to new domains, which requires a substantial reengineering work. In this paper, we show that tree kernels (Collins and Duffy, 2002; Moschitti, 2006) can be applied to automatically learn complex structural patterns for both answer sentence selection and answer extraction. Such patterns are syntactic/se</context>
</contexts>
<marker>Ferrucci, Brown, Chu-Carroll, Fan, Gondek, Kalyanpur, Lally, Murdock, 2010</marker>
<rawString>David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, and Chris Welty. 2010. Building watson: An overview of the deepqa project. AI Magazine, 31(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Tree edit models for recognizing textual entailments, paraphrases, and answers to questions.</title>
<date>2010</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="22486" citStr="Heilman and Smith, 2010" startWordPosition="3635" endWordPosition="3639">IN) that were manually curated for errors5 and 1,229 questions from the entire TREC 8-12 that contain at least one correct answer sentence (ALL). The latter set represents a more noisy setting, since many answer sentences are marked erroneously as correct as they simply match a regular expression. Table 3 summarizes the data used for training and testing. Table 4 compares our kernel-based structural model with the previous state-of-the-art systems for answer sentence selection. In particular, we compare with four most recent state of the art answer sentence reranker models (Wang et al., 2007; Heilman and Smith, 2010; Wang and Manning, 2010; Yao et al., 2013), which report their performance on the same questions and candidate sets from TREC 13 as provided by (Wang et al., 2007). Our simple shallow tree representation (Severyn and Moschitti, 2012) delivers state-of-the-art accuracy largely improving on previous work. Finally, augmenting the structure with semantic linking (Severyn et al., 2013b) yields additional improvement in MAP and MRR. This suggests the utility of using supervised components, e.g., question focus and question category classifiers coupled with NERs, to establish semantic mapping betwee</context>
<context position="32636" citStr="Heilman and Smith, 2010" startWordPosition="5332" endWordPosition="5335">the output of question and focus classifiers to connect focus word and good candidate answer keywords (represented by NEs) of the answer passage. This provides more effective relational information, which allows our model to significantly improve on previous rerankers. Additionally, previous work on kernel-based approaches does not target answer extraction. One of the best models for answer sentence selection has been proposed in (Wang et al., 2007). They use the paradigm of quasi-synchronous grammar to model relations between a question and a candidate answer with syntactic transformations. (Heilman and Smith, 2010) develop an improved Tree Edit Distance (TED) model for learning tree transformations in a q/a pair. They search for a good sequence of tree edit operations using complex and computationally expensive Tree Kernel-based heuristic. (Wang and Manning, 2010) develop a probabilistic model to learn tree-edit operations on dependency parse trees. They cast the problem into the framework of structured output learning with latent variables. The model of (Yao et al., 2013) has reported an improvement over the Wang’s et al. (2007) system. It applies linear chain CRFs with features derived from TED and Wo</context>
<context position="23497" citStr="Heilman &amp; Smith (2010)" startWordPosition="3801" endWordPosition="3804">lds additional improvement in MAP and MRR. This suggests the utility of using supervised components, e.g., question focus and question category classifiers coupled with NERs, to establish semantic mapping between words in a q/a pair. 5In TREC correct answers are identified by regex matching using the provided answer pattern files 463 Table 3: Summary of TREC data for answer extraction used in (Yao et al., 2013). data questions candidates correct TRAIN 94 4718 348 ALL 1229 53417 6410 TEST 89 1517 284 Table 4: Answer sentence reranking on TREC 13. System MAP MRR Wang et al. (2007) 0.6029 0.6852 Heilman &amp; Smith (2010) 0.6091 0.6917 Wang &amp; Manning (2010) 0.5951 0.6951 Yao et al. (2013) 0.6319 0.7270 + WN 0.6371 0.7301 shallow tree (S&amp;M, 2012) 0.6485 0.7244 + semantic tagging 0.6781 0.7358 It is worth noting that our kernel-based classifier is conceptually simpler than approaches in the previous work, as it relies on the structural kernels, e.g., PTK, to automatically extract salient syntactic patterns relating questions and answers. Our model only includes the most basic feature vector (uni- and bi-grams) and does not rely on external sources such as WordNet. 5.3 Answer Extraction Our experiments on answer </context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah A. Smith. 2010. Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Dan Roth</author>
</authors>
<title>Learning question classifiers.</title>
<date>2002</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="14389" citStr="Li and Roth, 2002" startWordPosition="2283" endWordPosition="2286">on model is simpler than before: we use an SVM multiclassifier with tree kernels to automatically extract the question class. To build a multi-class classifier we train a binary SVM for each of the classes and apply a one-vs-all strategy to obtain the predicted Table 1: Expected Answer Type (EAT) → named entity types. EAT Named Entity types HUM Person LOCATION Location ENTITY Organization, Person, Misc DATE Date, Time, Number QUANTITY Number, Percentage CURRENCY Money, Number class. We use constituency trees as our input representation. Our question taxonomy is derived from the UIUIC dataset (Li and Roth, 2002) which defines 6 coarse and 50 fine grain classes. In particular, our set of question categories is formed by adopting 3 coarse classes: HUM (human), LOC (location), ENTY (entities) and replacing the NUM (numeric) coarse class with 3 fine-grain classes: CURRENCY, DATE, QUANTITY2. This set of question categories is sufficient to capture the coarse semantic answer type of the candidate answers found in TREC. Also using fewer question classes results in a more accurate multi-class classifier. Semantic tagging. Question focus word specifies the lexical answer type capturing the target information </context>
<context position="21055" citStr="Li and Roth, 2002" startWordPosition="3397" endWordPosition="3400">ector: SeCo-600 (Quarteroni et al., 2012), Mooney GeoQuery (Damljanovic et al., 2010) and the dataset from (Bunescu and Huang, 2010). The SeCo dataset contains 600 questions. The Mooney GeoQuery contains 250 question targeted at geographical information in the U.S. The first two datasets are very domain specific, while the dataset from (Bunescu and Huang, 2010) is more generic containing the first 2,000 questions from the answer type dataset from Li and Roth annotated with focus words. We removed questions with implicit and multiple focuses. Question Classification. We used the UIUIC dataset (Li and Roth, 2002) which contains 5,952 factoid questions 4 to train a multi-class question classifier. Table 2 summarizes the results of question focus and category classification. 4We excluded questions from TREC to ensure there is no overlap with the data used for testing models trained on TREC QA. Table 2: Accuracy (%) of focus (FC) and question classifiers (QC) using PTK. TASK SET PTK MOONEY 80.5 FC SECO-600 90.0 BUNESCU 96.9 UIUIC 85.9 QC TREC 11-12 78.1 5.2 Answer Sentence Selection We used the train and test data from (Wang et al., 2007) to enable direct comparison with previous work on answer sentence </context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>Xin Li and Dan Roth. 2002. Learning question classifiers. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>S Quarteroni</author>
</authors>
<title>Kernels on Linguistic Structures for Answer Extraction.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="31418" citStr="Moschitti and Quarteroni, 2008" startWordPosition="5142" endWordPosition="5145">ly a naive majority voting scheme to select a single best answer from a set of extracted answer candidates. This step has a dramatic impact on the final performance of the answer extraction system resulting in a large drop of recall, i.e., from 82.0 to 70.8 before and after voting respectively. Hence, a more involved model, i.e., 465 performing joint answer sentence re-ranking and answer extraction, is required to yield a better performance. 7 Related Work Tree kernel methods have found many applications for the task of answer reranking which are reported in (Moschitti, 2008; Moschitti, 2009; Moschitti and Quarteroni, 2008; Severyn and Moschitti, 2012). However, their methods lack the use of important relational information between a question and a candidate answer, which is essential to learn accurate relational patterns. In this respect, a solution based on enumerating relational links was given in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009) for the textual entailment task but it is computationally too expensive for the large dataset of QA. A few solutions to overcome computational issues were suggested in (Zanzotto et al., 2010). In contrast, this paper relies on structures directly encoding the ou</context>
</contexts>
<marker>Moschitti, Quarteroni, 2008</marker>
<rawString>A. Moschitti and S. Quarteroni. 2008. Kernels on Linguistic Structures for Answer Extraction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In ECML.</booktitle>
<contexts>
<context position="4055" citStr="Moschitti, 2006" startWordPosition="627" endWordPosition="628">ume the shape of high-level rules, which are, in any case, essential to achieve state-of-the-art accuracy. For example, the great IBM Watson system (Ferrucci et al., 2010) uses a learning to rank algorithm fed with hundreds of features. The extraction of some of the latter requires articulated rules/algorithms, which, in terms of complexity, are very similar to those constituting typical handcrafted QA systems. An immediate consequence is the reduced adaptability to new domains, which requires a substantial reengineering work. In this paper, we show that tree kernels (Collins and Duffy, 2002; Moschitti, 2006) can be applied to automatically learn complex structural patterns for both answer sentence selection and answer extraction. Such patterns are syntactic/semantic structures occurring in question and answer passages. To make such information available to the tree kernel functions, we rely on the shallow syntactic trees enriched with semantic information (Severyn et al., 2013b; Severyn et al., 2013a), e.g., Named Entities (NEs) and question focus and category, automatically derived by machine learning modules, e.g., question classifier (QC) or focus classifier (FC). More in detail, we (i) design</context>
<context position="9645" citStr="Moschitti, 2006" startWordPosition="1533" endWordPosition="1534"> In contrast, we solve the lack of feature pairing by annotating the trees with relational tags which are supposed to link the question tree fragments with the related fragments from the answer sentence. Such relational information is very important to improve the quality of the pair representation as well as the implicitly generated features. In the next section, we show simple structural models that we used in our experiments for question and answer pair classification. 2.3 Partial Tree Kernels The above framework can use any kernel for structural data. We use the Partial Tree Kernel (PTK) (Moschitti, 2006) to compute KTK(·, ·) as it is the most general convolution tree kernel, which at the same time shows rather good efficiency. PTK can be effectively applied to both constituency and dependency parse trees. It generalizes the syntactic tree kernel (STK) (Collins and Duffy, 2002), which maps a tree into the space of all possible tree fragments constrained by the rule that sibling nodes cannot be separated. In contrast, the PTK fragments can contain any subset of siblings, i.e., PTK allows for breaking the production rules in syntactic trees. Consequently, PTK generates an extremely rich feature </context>
<context position="31731" citStr="Moschitti, 2006" startWordPosition="5192" endWordPosition="5193">e., 465 performing joint answer sentence re-ranking and answer extraction, is required to yield a better performance. 7 Related Work Tree kernel methods have found many applications for the task of answer reranking which are reported in (Moschitti, 2008; Moschitti, 2009; Moschitti and Quarteroni, 2008; Severyn and Moschitti, 2012). However, their methods lack the use of important relational information between a question and a candidate answer, which is essential to learn accurate relational patterns. In this respect, a solution based on enumerating relational links was given in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009) for the textual entailment task but it is computationally too expensive for the large dataset of QA. A few solutions to overcome computational issues were suggested in (Zanzotto et al., 2010). In contrast, this paper relies on structures directly encoding the output of question and focus classifiers to connect focus word and good candidate answer keywords (represented by NEs) of the answer passage. This provides more effective relational information, which allows our model to significantly improve on previous rerankers. Additionally, previous work on kernel-based appro</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Kernel methods, syntax and semantics for relational text categorization. In CIKM. Alessandro Moschitti.</title>
<date>2008</date>
<booktitle>In EACL. Christopher Pinchak.</booktitle>
<contexts>
<context position="31369" citStr="Moschitti, 2008" startWordPosition="5138" endWordPosition="5139">tem. Best Answer Selection. We apply a naive majority voting scheme to select a single best answer from a set of extracted answer candidates. This step has a dramatic impact on the final performance of the answer extraction system resulting in a large drop of recall, i.e., from 82.0 to 70.8 before and after voting respectively. Hence, a more involved model, i.e., 465 performing joint answer sentence re-ranking and answer extraction, is required to yield a better performance. 7 Related Work Tree kernel methods have found many applications for the task of answer reranking which are reported in (Moschitti, 2008; Moschitti, 2009; Moschitti and Quarteroni, 2008; Severyn and Moschitti, 2012). However, their methods lack the use of important relational information between a question and a candidate answer, which is essential to learn accurate relational patterns. In this respect, a solution based on enumerating relational links was given in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009) for the textual entailment task but it is computationally too expensive for the large dataset of QA. A few solutions to overcome computational issues were suggested in (Zanzotto et al., 2010). In contrast, this pa</context>
</contexts>
<marker>Moschitti, 2008</marker>
<rawString>Alessandro Moschitti. 2008. Kernel methods, syntax and semantics for relational text categorization. In CIKM. Alessandro Moschitti. 2009. Syntactic and semantic kernels for short text pair categorization. In EACL. Christopher Pinchak. 2006. A probabilistic answer type model. In In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Prager</author>
</authors>
<date>2006</date>
<booktitle>Open-domain questionanswering. Foundations and Trends in Information Retrieval,</booktitle>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="12752" citStr="Prager, 2006" startWordPosition="2019" endWordPosition="2020">ch on the word lemmas. Solid arrows (blue) connect a question focus word name with the related named entities of type Person corresponding to the question category (HUM) via a relational tag REL-HUM. Additional ANS tag is used to mark chunks containing candidate answer (here the correct answer John Chapman). higher probability to be correct answers following a mapping defined in Table 1. Next, we briefly introduce our tree kernel-based models for building question focus and category classifiers. Lexical Answer Type. Question Focus represents a central entity or a property asked by a question (Prager, 2006). It can be used to search for semantically compatible candidate answers, thus greatly reducing the search space (Pinchak, 2006). While several machine learning approaches based on manual features and syntactic structures have been recently explored, e.g. (Quarteroni et al., 2012; Damljanovic et al., 2010; Bunescu and Huang, 2010), we opt for the latter approach where tree kernels handle automatic feature engineering. To build an automatic Question Focus detector we use a tree kernel approach as follows: we (i) parse each question; (ii) create a set of positive trees by labeling the node exact</context>
</contexts>
<marker>Prager, 2006</marker>
<rawString>John M. Prager. 2006. Open-domain questionanswering. Foundations and Trends in Information Retrieval, 1(2):91–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silvia Quarteroni</author>
<author>Vincenzo Guerrisi</author>
<author>Pietro La Torre</author>
</authors>
<title>Evaluating multi-focus natural language queries over data services.</title>
<date>2012</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="13032" citStr="Quarteroni et al., 2012" startWordPosition="2060" endWordPosition="2063">(here the correct answer John Chapman). higher probability to be correct answers following a mapping defined in Table 1. Next, we briefly introduce our tree kernel-based models for building question focus and category classifiers. Lexical Answer Type. Question Focus represents a central entity or a property asked by a question (Prager, 2006). It can be used to search for semantically compatible candidate answers, thus greatly reducing the search space (Pinchak, 2006). While several machine learning approaches based on manual features and syntactic structures have been recently explored, e.g. (Quarteroni et al., 2012; Damljanovic et al., 2010; Bunescu and Huang, 2010), we opt for the latter approach where tree kernels handle automatic feature engineering. To build an automatic Question Focus detector we use a tree kernel approach as follows: we (i) parse each question; (ii) create a set of positive trees by labeling the node exactly covering the focus with FC tag; (iii) build a set of negative trees by labeling any other constituent node with FC; (iii) we train the FC node classifier with tree kernels. At the test time, we try to label each constituent node with FC generating a set of candidate trees. Fin</context>
<context position="20478" citStr="Quarteroni et al., 2012" startWordPosition="3306" endWordPosition="3309">ontinue 6: Ts0 +— tagAnswerChunk(Ts, chunk) 7: if contains answer(a, chunk) then 8: label +— +1 9: else 10: label +— −1 11: e +— build example(Tq,T0s,label) 12: E +— E U fel 13: return E rect answer keywords, i.e., a text span such as multiwords or constituents, from a given sentence. 5.1 Semantic Annotation We briefly describe the experiments of training automatic question category and focus classifiers, which are more extensively described in (Severyn et al., 2013b). Question Focus detection. We used three datasets for training and evaluating the performance of our focus detector: SeCo-600 (Quarteroni et al., 2012), Mooney GeoQuery (Damljanovic et al., 2010) and the dataset from (Bunescu and Huang, 2010). The SeCo dataset contains 600 questions. The Mooney GeoQuery contains 250 question targeted at geographical information in the U.S. The first two datasets are very domain specific, while the dataset from (Bunescu and Huang, 2010) is more generic containing the first 2,000 questions from the answer type dataset from Li and Roth annotated with focus words. We removed questions with implicit and multiple focuses. Question Classification. We used the UIUIC dataset (Li and Roth, 2002) which contains 5,952 f</context>
</contexts>
<marker>Quarteroni, Guerrisi, Torre, 2012</marker>
<rawString>Silvia Quarteroni, Vincenzo Guerrisi, and Pietro La Torre. 2012. Evaluating multi-focus natural language queries over data services. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ratinov</author>
<author>D Roth</author>
<author>D Downey</author>
<author>M Anderson</author>
</authors>
<title>Local and global algorithms for disambiguation to wikipedia.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="30007" citStr="Ratinov et al., 2011" startWordPosition="4918" endWordPosition="4921">ing. Our structural model relies heavily on the ability of NER to identify the relevant entities in the candidate sentence that can be further linked to the focus word of the question. While our answer extraction model is working on all the NP chunks, the semantic tags from NER serve as a strong cue for the classifier that a given chunk has a high probability of containing an answer. Typical off-the-shelf NER taggers have good precision and low recall, s.t. many entities as potential answers are missed. In this respect, a high recall entity linking system, e.g., linking to wikipedia entities (Ratinov et al., 2011), is required to boost the quality of candidates considered for answer extraction. Finally, improving the accuracy of question and focus classifiers would allow for having more accurate input representations fed to the learning algorithm. Answer Extraction. Our answer extraction model acts as a high recall system, while it suffers from low precision in extracting answers for many incorrect sentences. Improving the precision without sacrificing the recall would ease the successive task of best answer selection, since having less incorrect answer candidates would result in a better final perform</context>
</contexts>
<marker>Ratinov, Roth, Downey, Anderson, 2011</marker>
<rawString>L. Ratinov, D. Roth, D. Downey, and M. Anderson. 2011. Local and global algorithms for disambiguation to wikipedia. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Structural relationships for large-scale learning of answer re-ranking.</title>
<date>2012</date>
<booktitle>In SIGIR.</booktitle>
<contexts>
<context position="10990" citStr="Severyn and Moschitti, 2012" startWordPosition="1740" endWordPosition="1743">uctures designed to encode syntactic and shallow semantic properties of question/answer pairs. We first define a simple to construct shallow syntactic tree representation derived from a shallow parser. Next, we introduce a relational linking scheme based on a plain syntactic matching and further augment it with additional semantic information. 3.1 Shallow syntactic tree Our shallow tree structure is a two-level syntactic hierarchy built from word lemmas (leaves), part-ofspeech tags that organized into chunks identified by a shallow syntactic parser (Fig. 1). We defined a similar structure in (Severyn and Moschitti, 2012) for answer passage reranking, which improved on feature vector baselines. This simple linguistic representation is suitable for building a rather expressive answer sentence selection model. Moreover, the use of a shallow parser is motivated by the need to generate text spans to produce candidate answers required by an answer extraction system. 3.2 Tree pairs enriched with relational links It is important to establish a correspondence between question and answer sentence aligning related concepts from both. We take on a two-level approach, where we first use plain lexical matching to connect c</context>
<context position="22720" citStr="Severyn and Moschitti, 2012" startWordPosition="3674" endWordPosition="3677">ked erroneously as correct as they simply match a regular expression. Table 3 summarizes the data used for training and testing. Table 4 compares our kernel-based structural model with the previous state-of-the-art systems for answer sentence selection. In particular, we compare with four most recent state of the art answer sentence reranker models (Wang et al., 2007; Heilman and Smith, 2010; Wang and Manning, 2010; Yao et al., 2013), which report their performance on the same questions and candidate sets from TREC 13 as provided by (Wang et al., 2007). Our simple shallow tree representation (Severyn and Moschitti, 2012) delivers state-of-the-art accuracy largely improving on previous work. Finally, augmenting the structure with semantic linking (Severyn et al., 2013b) yields additional improvement in MAP and MRR. This suggests the utility of using supervised components, e.g., question focus and question category classifiers coupled with NERs, to establish semantic mapping between words in a q/a pair. 5In TREC correct answers are identified by regex matching using the provided answer pattern files 463 Table 3: Summary of TREC data for answer extraction used in (Yao et al., 2013). data questions candidates cor</context>
<context position="31448" citStr="Severyn and Moschitti, 2012" startWordPosition="5146" endWordPosition="5149">e to select a single best answer from a set of extracted answer candidates. This step has a dramatic impact on the final performance of the answer extraction system resulting in a large drop of recall, i.e., from 82.0 to 70.8 before and after voting respectively. Hence, a more involved model, i.e., 465 performing joint answer sentence re-ranking and answer extraction, is required to yield a better performance. 7 Related Work Tree kernel methods have found many applications for the task of answer reranking which are reported in (Moschitti, 2008; Moschitti, 2009; Moschitti and Quarteroni, 2008; Severyn and Moschitti, 2012). However, their methods lack the use of important relational information between a question and a candidate answer, which is essential to learn accurate relational patterns. In this respect, a solution based on enumerating relational links was given in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009) for the textual entailment task but it is computationally too expensive for the large dataset of QA. A few solutions to overcome computational issues were suggested in (Zanzotto et al., 2010). In contrast, this paper relies on structures directly encoding the output of question and focus cla</context>
</contexts>
<marker>Severyn, Moschitti, 2012</marker>
<rawString>Aliaksei Severyn and Alessandro Moschitti. 2012. Structural relationships for large-scale learning of answer re-ranking. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Massimo Nicosia</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Building structures from classifiers for passage reranking.</title>
<date>2013</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="4431" citStr="Severyn et al., 2013" startWordPosition="680" endWordPosition="683">nstituting typical handcrafted QA systems. An immediate consequence is the reduced adaptability to new domains, which requires a substantial reengineering work. In this paper, we show that tree kernels (Collins and Duffy, 2002; Moschitti, 2006) can be applied to automatically learn complex structural patterns for both answer sentence selection and answer extraction. Such patterns are syntactic/semantic structures occurring in question and answer passages. To make such information available to the tree kernel functions, we rely on the shallow syntactic trees enriched with semantic information (Severyn et al., 2013b; Severyn et al., 2013a), e.g., Named Entities (NEs) and question focus and category, automatically derived by machine learning modules, e.g., question classifier (QC) or focus classifier (FC). More in detail, we (i) design a pair of shallow syntactic trees (one for the question and one for the answer sentence); (ii) connect them with relational nodes (i.e., those matching the same words in the question and in the answer passages); (iii) label the tree nodes with semantic information such as question category and focus and NEs; and (iv) use the NE type to establish additional semantic links b</context>
<context position="20324" citStr="Severyn et al., 2013" startWordPosition="3284" endWordPosition="3287">e training data for answer extraction 1: for all (a, Tq, Ts) E D do 2: E +— 0 3: for all chunk E extract chunks(Ts) do 4: if not chunk == NP then 5: continue 6: Ts0 +— tagAnswerChunk(Ts, chunk) 7: if contains answer(a, chunk) then 8: label +— +1 9: else 10: label +— −1 11: e +— build example(Tq,T0s,label) 12: E +— E U fel 13: return E rect answer keywords, i.e., a text span such as multiwords or constituents, from a given sentence. 5.1 Semantic Annotation We briefly describe the experiments of training automatic question category and focus classifiers, which are more extensively described in (Severyn et al., 2013b). Question Focus detection. We used three datasets for training and evaluating the performance of our focus detector: SeCo-600 (Quarteroni et al., 2012), Mooney GeoQuery (Damljanovic et al., 2010) and the dataset from (Bunescu and Huang, 2010). The SeCo dataset contains 600 questions. The Mooney GeoQuery contains 250 question targeted at geographical information in the U.S. The first two datasets are very domain specific, while the dataset from (Bunescu and Huang, 2010) is more generic containing the first 2,000 questions from the answer type dataset from Li and Roth annotated with focus wor</context>
<context position="22869" citStr="Severyn et al., 2013" startWordPosition="3696" endWordPosition="3699">based structural model with the previous state-of-the-art systems for answer sentence selection. In particular, we compare with four most recent state of the art answer sentence reranker models (Wang et al., 2007; Heilman and Smith, 2010; Wang and Manning, 2010; Yao et al., 2013), which report their performance on the same questions and candidate sets from TREC 13 as provided by (Wang et al., 2007). Our simple shallow tree representation (Severyn and Moschitti, 2012) delivers state-of-the-art accuracy largely improving on previous work. Finally, augmenting the structure with semantic linking (Severyn et al., 2013b) yields additional improvement in MAP and MRR. This suggests the utility of using supervised components, e.g., question focus and question category classifiers coupled with NERs, to establish semantic mapping between words in a q/a pair. 5In TREC correct answers are identified by regex matching using the provided answer pattern files 463 Table 3: Summary of TREC data for answer extraction used in (Yao et al., 2013). data questions candidates correct TRAIN 94 4718 348 ALL 1229 53417 6410 TEST 89 1517 284 Table 4: Answer sentence reranking on TREC 13. System MAP MRR Wang et al. (2007) 0.6029 0</context>
<context position="33661" citStr="Severyn et al., 2013" startWordPosition="5491" endWordPosition="5494">earning with latent variables. The model of (Yao et al., 2013) has reported an improvement over the Wang’s et al. (2007) system. It applies linear chain CRFs with features derived from TED and WordNet to automatically learn associations between questions and candidate answers. Different from previous approaches that use treeedit information derived from syntactic trees, our kernel-based learning approach also use tree structures but with rather different learning methods, i.e., SVMs and structural kernels, to automatically extract salient syntactic patterns relating questions and answers. In (Severyn et al., 2013c), we have shown that such relational structures encoding input text pairs can be directly used within the kernel learning framework to build state-of-the-art models for predicting semantic textual similarity. Furthermore, semantically enriched relational structures, where automatic have been previously explored for answer passage reranking in (Severyn et al., 2013b; Severyn et al., 2013a). This paper demonstrates that this model also works for building a reranker on the sentence level, and extends the previous work by applying the idea of automatic feature engineering with tree kernels to an</context>
</contexts>
<marker>Severyn, Nicosia, Moschitti, 2013</marker>
<rawString>Aliaksei Severyn, Massimo Nicosia, and Alessandro Moschitti. 2013a. Building structures from classifiers for passage reranking. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Massimo Nicosia</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Learning adaptable patterns for passage reranking. In CoNLL.</title>
<date>2013</date>
<contexts>
<context position="4431" citStr="Severyn et al., 2013" startWordPosition="680" endWordPosition="683">nstituting typical handcrafted QA systems. An immediate consequence is the reduced adaptability to new domains, which requires a substantial reengineering work. In this paper, we show that tree kernels (Collins and Duffy, 2002; Moschitti, 2006) can be applied to automatically learn complex structural patterns for both answer sentence selection and answer extraction. Such patterns are syntactic/semantic structures occurring in question and answer passages. To make such information available to the tree kernel functions, we rely on the shallow syntactic trees enriched with semantic information (Severyn et al., 2013b; Severyn et al., 2013a), e.g., Named Entities (NEs) and question focus and category, automatically derived by machine learning modules, e.g., question classifier (QC) or focus classifier (FC). More in detail, we (i) design a pair of shallow syntactic trees (one for the question and one for the answer sentence); (ii) connect them with relational nodes (i.e., those matching the same words in the question and in the answer passages); (iii) label the tree nodes with semantic information such as question category and focus and NEs; and (iv) use the NE type to establish additional semantic links b</context>
<context position="20324" citStr="Severyn et al., 2013" startWordPosition="3284" endWordPosition="3287">e training data for answer extraction 1: for all (a, Tq, Ts) E D do 2: E +— 0 3: for all chunk E extract chunks(Ts) do 4: if not chunk == NP then 5: continue 6: Ts0 +— tagAnswerChunk(Ts, chunk) 7: if contains answer(a, chunk) then 8: label +— +1 9: else 10: label +— −1 11: e +— build example(Tq,T0s,label) 12: E +— E U fel 13: return E rect answer keywords, i.e., a text span such as multiwords or constituents, from a given sentence. 5.1 Semantic Annotation We briefly describe the experiments of training automatic question category and focus classifiers, which are more extensively described in (Severyn et al., 2013b). Question Focus detection. We used three datasets for training and evaluating the performance of our focus detector: SeCo-600 (Quarteroni et al., 2012), Mooney GeoQuery (Damljanovic et al., 2010) and the dataset from (Bunescu and Huang, 2010). The SeCo dataset contains 600 questions. The Mooney GeoQuery contains 250 question targeted at geographical information in the U.S. The first two datasets are very domain specific, while the dataset from (Bunescu and Huang, 2010) is more generic containing the first 2,000 questions from the answer type dataset from Li and Roth annotated with focus wor</context>
<context position="22869" citStr="Severyn et al., 2013" startWordPosition="3696" endWordPosition="3699">based structural model with the previous state-of-the-art systems for answer sentence selection. In particular, we compare with four most recent state of the art answer sentence reranker models (Wang et al., 2007; Heilman and Smith, 2010; Wang and Manning, 2010; Yao et al., 2013), which report their performance on the same questions and candidate sets from TREC 13 as provided by (Wang et al., 2007). Our simple shallow tree representation (Severyn and Moschitti, 2012) delivers state-of-the-art accuracy largely improving on previous work. Finally, augmenting the structure with semantic linking (Severyn et al., 2013b) yields additional improvement in MAP and MRR. This suggests the utility of using supervised components, e.g., question focus and question category classifiers coupled with NERs, to establish semantic mapping between words in a q/a pair. 5In TREC correct answers are identified by regex matching using the provided answer pattern files 463 Table 3: Summary of TREC data for answer extraction used in (Yao et al., 2013). data questions candidates correct TRAIN 94 4718 348 ALL 1229 53417 6410 TEST 89 1517 284 Table 4: Answer sentence reranking on TREC 13. System MAP MRR Wang et al. (2007) 0.6029 0</context>
<context position="33661" citStr="Severyn et al., 2013" startWordPosition="5491" endWordPosition="5494">earning with latent variables. The model of (Yao et al., 2013) has reported an improvement over the Wang’s et al. (2007) system. It applies linear chain CRFs with features derived from TED and WordNet to automatically learn associations between questions and candidate answers. Different from previous approaches that use treeedit information derived from syntactic trees, our kernel-based learning approach also use tree structures but with rather different learning methods, i.e., SVMs and structural kernels, to automatically extract salient syntactic patterns relating questions and answers. In (Severyn et al., 2013c), we have shown that such relational structures encoding input text pairs can be directly used within the kernel learning framework to build state-of-the-art models for predicting semantic textual similarity. Furthermore, semantically enriched relational structures, where automatic have been previously explored for answer passage reranking in (Severyn et al., 2013b; Severyn et al., 2013a). This paper demonstrates that this model also works for building a reranker on the sentence level, and extends the previous work by applying the idea of automatic feature engineering with tree kernels to an</context>
</contexts>
<marker>Severyn, Nicosia, Moschitti, 2013</marker>
<rawString>Aliaksei Severyn, Massimo Nicosia, and Alessandro Moschitti. 2013b. Learning adaptable patterns for passage reranking. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Massimo Nicosia</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Learning semantic textual similarity with structural representations.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4431" citStr="Severyn et al., 2013" startWordPosition="680" endWordPosition="683">nstituting typical handcrafted QA systems. An immediate consequence is the reduced adaptability to new domains, which requires a substantial reengineering work. In this paper, we show that tree kernels (Collins and Duffy, 2002; Moschitti, 2006) can be applied to automatically learn complex structural patterns for both answer sentence selection and answer extraction. Such patterns are syntactic/semantic structures occurring in question and answer passages. To make such information available to the tree kernel functions, we rely on the shallow syntactic trees enriched with semantic information (Severyn et al., 2013b; Severyn et al., 2013a), e.g., Named Entities (NEs) and question focus and category, automatically derived by machine learning modules, e.g., question classifier (QC) or focus classifier (FC). More in detail, we (i) design a pair of shallow syntactic trees (one for the question and one for the answer sentence); (ii) connect them with relational nodes (i.e., those matching the same words in the question and in the answer passages); (iii) label the tree nodes with semantic information such as question category and focus and NEs; and (iv) use the NE type to establish additional semantic links b</context>
<context position="20324" citStr="Severyn et al., 2013" startWordPosition="3284" endWordPosition="3287">e training data for answer extraction 1: for all (a, Tq, Ts) E D do 2: E +— 0 3: for all chunk E extract chunks(Ts) do 4: if not chunk == NP then 5: continue 6: Ts0 +— tagAnswerChunk(Ts, chunk) 7: if contains answer(a, chunk) then 8: label +— +1 9: else 10: label +— −1 11: e +— build example(Tq,T0s,label) 12: E +— E U fel 13: return E rect answer keywords, i.e., a text span such as multiwords or constituents, from a given sentence. 5.1 Semantic Annotation We briefly describe the experiments of training automatic question category and focus classifiers, which are more extensively described in (Severyn et al., 2013b). Question Focus detection. We used three datasets for training and evaluating the performance of our focus detector: SeCo-600 (Quarteroni et al., 2012), Mooney GeoQuery (Damljanovic et al., 2010) and the dataset from (Bunescu and Huang, 2010). The SeCo dataset contains 600 questions. The Mooney GeoQuery contains 250 question targeted at geographical information in the U.S. The first two datasets are very domain specific, while the dataset from (Bunescu and Huang, 2010) is more generic containing the first 2,000 questions from the answer type dataset from Li and Roth annotated with focus wor</context>
<context position="22869" citStr="Severyn et al., 2013" startWordPosition="3696" endWordPosition="3699">based structural model with the previous state-of-the-art systems for answer sentence selection. In particular, we compare with four most recent state of the art answer sentence reranker models (Wang et al., 2007; Heilman and Smith, 2010; Wang and Manning, 2010; Yao et al., 2013), which report their performance on the same questions and candidate sets from TREC 13 as provided by (Wang et al., 2007). Our simple shallow tree representation (Severyn and Moschitti, 2012) delivers state-of-the-art accuracy largely improving on previous work. Finally, augmenting the structure with semantic linking (Severyn et al., 2013b) yields additional improvement in MAP and MRR. This suggests the utility of using supervised components, e.g., question focus and question category classifiers coupled with NERs, to establish semantic mapping between words in a q/a pair. 5In TREC correct answers are identified by regex matching using the provided answer pattern files 463 Table 3: Summary of TREC data for answer extraction used in (Yao et al., 2013). data questions candidates correct TRAIN 94 4718 348 ALL 1229 53417 6410 TEST 89 1517 284 Table 4: Answer sentence reranking on TREC 13. System MAP MRR Wang et al. (2007) 0.6029 0</context>
<context position="33661" citStr="Severyn et al., 2013" startWordPosition="5491" endWordPosition="5494">earning with latent variables. The model of (Yao et al., 2013) has reported an improvement over the Wang’s et al. (2007) system. It applies linear chain CRFs with features derived from TED and WordNet to automatically learn associations between questions and candidate answers. Different from previous approaches that use treeedit information derived from syntactic trees, our kernel-based learning approach also use tree structures but with rather different learning methods, i.e., SVMs and structural kernels, to automatically extract salient syntactic patterns relating questions and answers. In (Severyn et al., 2013c), we have shown that such relational structures encoding input text pairs can be directly used within the kernel learning framework to build state-of-the-art models for predicting semantic textual similarity. Furthermore, semantically enriched relational structures, where automatic have been previously explored for answer passage reranking in (Severyn et al., 2013b; Severyn et al., 2013a). This paper demonstrates that this model also works for building a reranker on the sentence level, and extends the previous work by applying the idea of automatic feature engineering with tree kernels to an</context>
</contexts>
<marker>Severyn, Nicosia, Moschitti, 2013</marker>
<rawString>Aliaksei Severyn, Massimo Nicosia, and Alessandro Moschitti. 2013c. Learning semantic textual similarity with structural representations. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Probabilistic tree-edit models with structured latent variables for textual entailment and question answering.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="22510" citStr="Wang and Manning, 2010" startWordPosition="3640" endWordPosition="3643">rated for errors5 and 1,229 questions from the entire TREC 8-12 that contain at least one correct answer sentence (ALL). The latter set represents a more noisy setting, since many answer sentences are marked erroneously as correct as they simply match a regular expression. Table 3 summarizes the data used for training and testing. Table 4 compares our kernel-based structural model with the previous state-of-the-art systems for answer sentence selection. In particular, we compare with four most recent state of the art answer sentence reranker models (Wang et al., 2007; Heilman and Smith, 2010; Wang and Manning, 2010; Yao et al., 2013), which report their performance on the same questions and candidate sets from TREC 13 as provided by (Wang et al., 2007). Our simple shallow tree representation (Severyn and Moschitti, 2012) delivers state-of-the-art accuracy largely improving on previous work. Finally, augmenting the structure with semantic linking (Severyn et al., 2013b) yields additional improvement in MAP and MRR. This suggests the utility of using supervised components, e.g., question focus and question category classifiers coupled with NERs, to establish semantic mapping between words in a q/a pair. 5</context>
<context position="32890" citStr="Wang and Manning, 2010" startWordPosition="5372" endWordPosition="5375">ous rerankers. Additionally, previous work on kernel-based approaches does not target answer extraction. One of the best models for answer sentence selection has been proposed in (Wang et al., 2007). They use the paradigm of quasi-synchronous grammar to model relations between a question and a candidate answer with syntactic transformations. (Heilman and Smith, 2010) develop an improved Tree Edit Distance (TED) model for learning tree transformations in a q/a pair. They search for a good sequence of tree edit operations using complex and computationally expensive Tree Kernel-based heuristic. (Wang and Manning, 2010) develop a probabilistic model to learn tree-edit operations on dependency parse trees. They cast the problem into the framework of structured output learning with latent variables. The model of (Yao et al., 2013) has reported an improvement over the Wang’s et al. (2007) system. It applies linear chain CRFs with features derived from TED and WordNet to automatically learn associations between questions and candidate answers. Different from previous approaches that use treeedit information derived from syntactic trees, our kernel-based learning approach also use tree structures but with rather </context>
<context position="23533" citStr="Wang &amp; Manning (2010)" startWordPosition="3807" endWordPosition="3810"> MRR. This suggests the utility of using supervised components, e.g., question focus and question category classifiers coupled with NERs, to establish semantic mapping between words in a q/a pair. 5In TREC correct answers are identified by regex matching using the provided answer pattern files 463 Table 3: Summary of TREC data for answer extraction used in (Yao et al., 2013). data questions candidates correct TRAIN 94 4718 348 ALL 1229 53417 6410 TEST 89 1517 284 Table 4: Answer sentence reranking on TREC 13. System MAP MRR Wang et al. (2007) 0.6029 0.6852 Heilman &amp; Smith (2010) 0.6091 0.6917 Wang &amp; Manning (2010) 0.5951 0.6951 Yao et al. (2013) 0.6319 0.7270 + WN 0.6371 0.7301 shallow tree (S&amp;M, 2012) 0.6485 0.7244 + semantic tagging 0.6781 0.7358 It is worth noting that our kernel-based classifier is conceptually simpler than approaches in the previous work, as it relies on the structural kernels, e.g., PTK, to automatically extract salient syntactic patterns relating questions and answers. Our model only includes the most basic feature vector (uni- and bi-grams) and does not rely on external sources such as WordNet. 5.3 Answer Extraction Our experiments on answer extraction replicate the setting of </context>
</contexts>
<marker>Wang, Manning, 2010</marker>
<rawString>Mengqiu Wang and Christopher D. Manning. 2010. Probabilistic tree-edit models with structured latent variables for textual entailment and question answering. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Noah A Smith</author>
<author>Teruko Mitaura</author>
</authors>
<title>What is the jeopardy model? a quasisynchronous grammar for qa.</title>
<date>2007</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="5489" citStr="Wang et al., 2007" startWordPosition="858" endWordPosition="861">) label the tree nodes with semantic information such as question category and focus and NEs; and (iv) use the NE type to establish additional semantic links between the candidate answer, i.e., an NE, and the focus word of the question. Finally, for the task of answer extraction we also connect such semantic information to the answer sentence trees such that we can learn factoid answer patterns. We show that our models are very effective in producing features for both answer selection and extraction by experimenting with TREC QA corpora and directly comparing with the state of the art, e.g., (Wang et al., 2007; Yao et al., 2013). The results show that our methods greatly improve on both tasks yielding a large improvement in Mean Average Precision for answer selection and in F1 for answer extraction: up to 22% of relative improvement in F1, when small training data is used. Moreover, in contrast to the previous work, our model does not rely on external resources, e.g., WordNet, or complex features in addition to the structural kernel model. The reminder of this paper is organized as follows, Sec. 2 describes our kernel-based classifiers, Sec. 3 illustrates our question/answer relational structures a</context>
<context position="21588" citStr="Wang et al., 2007" startWordPosition="3488" endWordPosition="3491">tiple focuses. Question Classification. We used the UIUIC dataset (Li and Roth, 2002) which contains 5,952 factoid questions 4 to train a multi-class question classifier. Table 2 summarizes the results of question focus and category classification. 4We excluded questions from TREC to ensure there is no overlap with the data used for testing models trained on TREC QA. Table 2: Accuracy (%) of focus (FC) and question classifiers (QC) using PTK. TASK SET PTK MOONEY 80.5 FC SECO-600 90.0 BUNESCU 96.9 UIUIC 85.9 QC TREC 11-12 78.1 5.2 Answer Sentence Selection We used the train and test data from (Wang et al., 2007) to enable direct comparison with previous work on answer sentence selection. The training data is composed by questions drawn from TREC 8-12 while questions from TREC 13 are used for testing. The data provided for training comes as two sets: a small set of 94 questions (TRAIN) that were manually curated for errors5 and 1,229 questions from the entire TREC 8-12 that contain at least one correct answer sentence (ALL). The latter set represents a more noisy setting, since many answer sentences are marked erroneously as correct as they simply match a regular expression. Table 3 summarizes the dat</context>
<context position="23460" citStr="Wang et al. (2007)" startWordPosition="3795" endWordPosition="3798">nking (Severyn et al., 2013b) yields additional improvement in MAP and MRR. This suggests the utility of using supervised components, e.g., question focus and question category classifiers coupled with NERs, to establish semantic mapping between words in a q/a pair. 5In TREC correct answers are identified by regex matching using the provided answer pattern files 463 Table 3: Summary of TREC data for answer extraction used in (Yao et al., 2013). data questions candidates correct TRAIN 94 4718 348 ALL 1229 53417 6410 TEST 89 1517 284 Table 4: Answer sentence reranking on TREC 13. System MAP MRR Wang et al. (2007) 0.6029 0.6852 Heilman &amp; Smith (2010) 0.6091 0.6917 Wang &amp; Manning (2010) 0.5951 0.6951 Yao et al. (2013) 0.6319 0.7270 + WN 0.6371 0.7301 shallow tree (S&amp;M, 2012) 0.6485 0.7244 + semantic tagging 0.6781 0.7358 It is worth noting that our kernel-based classifier is conceptually simpler than approaches in the previous work, as it relies on the structural kernels, e.g., PTK, to automatically extract salient syntactic patterns relating questions and answers. Our model only includes the most basic feature vector (uni- and bi-grams) and does not rely on external sources such as WordNet. 5.3 Answer </context>
<context position="32465" citStr="Wang et al., 2007" startWordPosition="5308" endWordPosition="5311">et of QA. A few solutions to overcome computational issues were suggested in (Zanzotto et al., 2010). In contrast, this paper relies on structures directly encoding the output of question and focus classifiers to connect focus word and good candidate answer keywords (represented by NEs) of the answer passage. This provides more effective relational information, which allows our model to significantly improve on previous rerankers. Additionally, previous work on kernel-based approaches does not target answer extraction. One of the best models for answer sentence selection has been proposed in (Wang et al., 2007). They use the paradigm of quasi-synchronous grammar to model relations between a question and a candidate answer with syntactic transformations. (Heilman and Smith, 2010) develop an improved Tree Edit Distance (TED) model for learning tree transformations in a q/a pair. They search for a good sequence of tree edit operations using complex and computationally expensive Tree Kernel-based heuristic. (Wang and Manning, 2010) develop a probabilistic model to learn tree-edit operations on dependency parse trees. They cast the problem into the framework of structured output learning with latent vari</context>
</contexts>
<marker>Wang, Smith, Mitaura, 2007</marker>
<rawString>Mengqiu Wang, Noah A. Smith, and Teruko Mitaura. 2007. What is the jeopardy model? a quasisynchronous grammar for qa. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
<author>Peter Clark</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Answer extraction as sequence tagging with tree edit distance.</title>
<date>2013</date>
<booktitle>In NAACL.</booktitle>
<marker>Yao, Van Durme, Clark, Callison-Burch, 2013</marker>
<rawString>Xuchen Yao, Benjamin Van Durme, Peter Clark, and Chris Callison-Burch. 2013. Answer extraction as sequence tagging with tree edit distance. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Zanzotto</author>
<author>A Moschitti</author>
</authors>
<title>Automatic Learning of Textual Entailments with Cross-Pair Similarities.</title>
<date>2006</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="31731" citStr="Zanzotto and Moschitti, 2006" startWordPosition="5189" endWordPosition="5193">ved model, i.e., 465 performing joint answer sentence re-ranking and answer extraction, is required to yield a better performance. 7 Related Work Tree kernel methods have found many applications for the task of answer reranking which are reported in (Moschitti, 2008; Moschitti, 2009; Moschitti and Quarteroni, 2008; Severyn and Moschitti, 2012). However, their methods lack the use of important relational information between a question and a candidate answer, which is essential to learn accurate relational patterns. In this respect, a solution based on enumerating relational links was given in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009) for the textual entailment task but it is computationally too expensive for the large dataset of QA. A few solutions to overcome computational issues were suggested in (Zanzotto et al., 2010). In contrast, this paper relies on structures directly encoding the output of question and focus classifiers to connect focus word and good candidate answer keywords (represented by NEs) of the answer passage. This provides more effective relational information, which allows our model to significantly improve on previous rerankers. Additionally, previous work on kernel-based appro</context>
</contexts>
<marker>Zanzotto, Moschitti, 2006</marker>
<rawString>F. M. Zanzotto and A. Moschitti. 2006. Automatic Learning of Textual Entailments with Cross-Pair Similarities. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Zanzotto</author>
<author>M Pennacchiotti</author>
<author>A Moschitti</author>
</authors>
<title>A Machine Learning Approach to Recognizing Textual Entailment.</title>
<date>2009</date>
<journal>Natural Language Engineering, Volume</journal>
<volume>15</volume>
<contexts>
<context position="31755" citStr="Zanzotto et al., 2009" startWordPosition="5194" endWordPosition="5197">g joint answer sentence re-ranking and answer extraction, is required to yield a better performance. 7 Related Work Tree kernel methods have found many applications for the task of answer reranking which are reported in (Moschitti, 2008; Moschitti, 2009; Moschitti and Quarteroni, 2008; Severyn and Moschitti, 2012). However, their methods lack the use of important relational information between a question and a candidate answer, which is essential to learn accurate relational patterns. In this respect, a solution based on enumerating relational links was given in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009) for the textual entailment task but it is computationally too expensive for the large dataset of QA. A few solutions to overcome computational issues were suggested in (Zanzotto et al., 2010). In contrast, this paper relies on structures directly encoding the output of question and focus classifiers to connect focus word and good candidate answer keywords (represented by NEs) of the answer passage. This provides more effective relational information, which allows our model to significantly improve on previous rerankers. Additionally, previous work on kernel-based approaches does not target an</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Moschitti, 2009</marker>
<rawString>F. M. Zanzotto, M. Pennacchiotti, and A. Moschitti. 2009. A Machine Learning Approach to Recognizing Textual Entailment. Natural Language Engineering, Volume 15 Issue 4, October 2009:551–582.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Zanzotto</author>
<author>L Dell’Arciprete</author>
<author>A Moschitti</author>
</authors>
<title>Efficient graph kernels for textual entailment recognition. FUNDAMENTA INFORMATICAE,</title>
<date>2010</date>
<marker>Zanzotto, Dell’Arciprete, Moschitti, 2010</marker>
<rawString>F. M. Zanzotto, L. Dell’Arciprete, and A. Moschitti. 2010. Efficient graph kernels for textual entailment recognition. FUNDAMENTA INFORMATICAE, 2010.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.040055">
<title confidence="0.995435">
The VerbCorner Project: Toward an Empirically-Based Semantic
Decomposition of Verbs
</title>
<author confidence="0.999101">
Joshua K. Hartshorne Claire Bonial, Martha Palmer
</author>
<affiliation confidence="0.9989245">
Department of Brain and Cognitive Sciences Department of Linguistics
Massachusetts Institute of Technology University of Colorado at Boulder
</affiliation>
<address confidence="0.901296">
77 Massachusetts Avenue Hellems 290, 295 UCB
Cambridge, MA 02139, USA Boulder, CO 80309, USA
</address>
<email confidence="0.997158">
jkhartshorne@gmail.com {CBonial, MPalmer}@colorado.edu
</email>
<sectionHeader confidence="0.995605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999221277777778">
This research describes efforts to use crowd-
sourcing to improve the validity of the seman-
tic predicates in VerbNet, a lexicon of about
6300 English verbs. The current semantic
predicates can be thought of semantic prim-
itives, into which the concepts denoted by a
verb can be decomposed. For example, the
verb spray (of the Spray class), involves the
predicates MOTION, NOT, and LOCATION,
where the event can be decomposed into an
AGENT causing a THEME that was originally
not in a particular location to now be in that
location. Although VerbNet’s predicates are
theoretically well-motivated, systematic em-
pirical data is scarce. This paper describes a
recently-launched attempt to address this issue
with a series of human judgment tasks, posed
to subjects in the form of games.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999880155555556">
One key application of Natural Language Processing
(NLP) is meaning extraction. Of particular impor-
tance is propositional meaning: To understand “Jes-
sica sprayed paint on the wall,” it is not enough to
know who Jessica is, what paint is, and where the
wall is, but that, by the end of the event, some quan-
tity of paint that was not previously on the wall now
is. One must extract not only meanings for individ-
ual words but also the relations between them.
One option is to learn these relations in a largely
bottom-up, data-driven fashion (Chklovski and Pan-
tel, 2004; Poon and Domingos, 2009). For instance,
Poon and Domingos (2009) first extracts depen-
dency trees, converts those into quasi-logical form,
recursively induces lambda expressions from them,
and uses clustering to derive progressively abstract
knowledge.
An alternative is to take a human-inspired ap-
proach, mapping the linguistic input onto the kinds
of representations that linguistic and psychologi-
cal research suggests are the representations em-
ployed by humans. While the exact characteriza-
tion of meaning (and by extension, thought) remains
an area of active research in the cognitive sciences
(Margolis and Laurence, 1999), decades of research
in linguistics and psychology suggests that much of
the meaning of a sentence – as well as its syntactic
structure – can be accounted for by invoking a small
number of highly abstract semantic features (usu-
ally represented as predicates), such as causation,
agency, basic topological relations, and directed mo-
tion (Ambridge et al., 2013; Croft, 2012; Jackend-
off, 1990; Levin and Rappaport Hovav, 2005; Peset-
sky, 1995; Pinker, 1989). For instance, a given verb
can appear in some syntactic frames (Sally broke the
vase. Sally broke the vase with the hammer. The vase
broke.) and not others (*Sally broke the vase to the
floor. *Sally broke John the vase.). When verbs are
classified according to the syntactic frames they can
appear in, most if not all the verbs in a class involve
the same set of abstract semantic features.1
Interestingly, roughly these same features (causa-
tion, etc.) have been singled out by developmental
psychologists as part of “core knowledge” – a set of
early-learned or perhaps innate concepts upon which
</bodyText>
<footnote confidence="0.9975">
1Whether all verbs in a class share the same abstract pred-
icates or merely most is an area of active research (Levin and
Rappaport Hovav, 2005).
</footnote>
<page confidence="0.921813">
1438
</page>
<bodyText confidence="0.895332444444444">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1438–1442,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
the rest of cognition is built (Spelke and Kinzler,
2007). Thus these semantic features/predicates may
be not only crucial to describing linguistic mean-
ing but may be central organizing principles for a
human’s (reasonably successful) thinking about and
conceptualization of the world. As such, they pro-
vide a potentially rewarding target for NLP.
</bodyText>
<sectionHeader confidence="0.997461" genericHeader="introduction">
2 VerbNet
</sectionHeader>
<subsectionHeader confidence="0.99802">
2.1 Overview and Structure
</subsectionHeader>
<bodyText confidence="0.892757527777778">
Perhaps the most comprehensive implementation
of this approach appears in VerbNet (Kipper et al.,
2008; based on Levin, 1993). VerbNet classifies
verbs based on the syntactic frames they can appear
in, providing a semantic description of each frame
for each class. An example entry is shown below:
Syntactic Frame NP V NP PP.DESTINATION
Example Jessica sprayed the wall.
Syntax AGENT V THEME {+LOC|+DEST CONF}
DESTINATION
Semantics MOTION(DURING(E), THEME)
NOT(PREP(START(E), THEME, DESTINATION))
PREP(END(E), THEME, DESTINATION)
CAUSE(AGENT, E)
The “Syntactic Frame” provides a flat syntactic
parse. “Syntax” provides semantic role labels for
each of the NPs and PPs, which are invoked in “Se-
mantics”. VerbNet decomposes the semantics of
this sentence into four separate predicates: 1) the
THEME (the paint) moves doing the event E; 2) at
the start of the event E, the THEME (the paint) is
not at the DESTINATION (on the wall), whereas 3)
at the end of the event E, the THEME (the paint) is
at the DESTINATION (on the wall), and; 4) the event
is caused by the AGENT (Sally). Note that this cap-
tures only the core aspects of semantics shared by all
verbs in the class; differences between verbs in the
same class (e.g., spray vs. splash) are omitted.
Importantly, the semantics of the sentence is de-
pendent on both the matrix verb (paint) and the syn-
tactic frame. Famously, when inserted in the slightly
different frame NP V NP.DESTINATION PP.THEME
– “Sally sprayed the wall with paint” – “spray” en-
tails that destination (the wall) is now fully painted,
an entailment that does not follow in the example
above (Pinker, 1989).
</bodyText>
<subsectionHeader confidence="0.99662">
2.2 Uses and Limitations
</subsectionHeader>
<bodyText confidence="0.999994307692308">
VerbNet has been used in a variety of NLP appli-
cations, such as semantic role labeling (Swier and
Stevenson, 2004), inferencing (Zaenen et al., 2008),
verb classification (Joanis et al., 2008), and informa-
tion extraction (Maynard, Funk, and Peters, 2009).
While such applications have been successful thus
far, an important constraint on how well VerbNet-
based NLP applications can be expected to perform
is the accuracy of the semantics encoded in Verb-
Net. Here, several issues arise. Leaving aside mis-
categorized verbs and other inaccuracies, as noted
above VerbNet assumes that all verbs in the same
class share the same core predicates, which may or
may not be empirically justified. Given the number
of semantic predicates (146),2 verb entries (6580),
and unique verb lemmas (6284) it is not feasible for
a single research team to check, particularly since af-
ter a certain number of verbs, intuitions become less
clear. In any case, it may not be ideal to rely solely
on the intuitions of invested researchers, whose in-
tuitions about subtle judgments may be clouded by
theoretical commitments (Gibson and Federenko,
2013); the only way to ensure this is not the case
is through independent validation. Unfortunately, of
the 280 verb classes in VerbNet, this has been done
for only a few (cf Ambridge et al., 2013).
</bodyText>
<sectionHeader confidence="0.998189" genericHeader="method">
3 VerbCorner
</sectionHeader>
<bodyText confidence="0.999990666666667">
The VerbCorner project was designed to address
these issues by crowd-sourcing the semantic judg-
ments online (gameswithwords.org/VerbCorner/).
Several previous projects have successfully crowd-
sourced linguistic annotations, such as Phrase De-
tectives, where volunteers have contributed 2.5 mil-
lion judgments on anaphoric relations (Poesio et al.,
2012). Below, we outline the VerbCorner project
and describe one specific annotation task in detail.
</bodyText>
<subsectionHeader confidence="0.999831">
3.1 Developing Semantic Annotation Tasks
</subsectionHeader>
<bodyText confidence="0.9998495">
Collecting accurate judgments on subtle questions
from naive participants with limited metalinguistic
</bodyText>
<footnote confidence="0.556973666666667">
2Note that these vary in applicability from those specific to
a small number of verbs (CHARACTERIZE, CONSPIRE) to those
frequently invoked (BEGIN, EXIST).
</footnote>
<page confidence="0.986672">
1439
</page>
<bodyText confidence="0.999994096774193">
skills is difficult. Rare is the non-linguist who can
immediately answer the question, “Does the verb
‘throw,’ when used transitively, entail a change of
location on the part of its THEME?” Thus, we began
by developing tasks that isolate semantic features in
a way accessible to untrained annotators.
We converted the metalinguistic judgments
(“Does this verb entail this abstract predicate?”) into
real-world problems, which previous research sug-
gests should be easier (Cosmides and Tooby, 1992).
Each judgment tasks involved a fanciful backstory.
For instance, in “Simon Says Freeze”, a task de-
signed to elicit judgments about movement, the
Galactic Overlord (Simon) decrees “Galactic Stay
Where You Are Day,” during which nobody is al-
lowed to move from their current location. Partici-
pants read descriptions of events and decide whether
anyone violated the rule. In “Explode on Contact”,
designed to elicit judgments about physical contact,
objects and people explode when they touch one an-
other. The participant reads descriptions of events
and decides whether anything has exploded.3
Each task was piloted until inter-coder reliability
was acceptably high and the modal response nearly
always corresponded with researcher intuitions. As
such, these tasks cannot be used to establish whether
researcher intuitions for the pilot stimuli are correct
(this would be circular); however, there is no guar-
antee that agreement with the researcher will gener-
alize to new items (the pilot stimuli cover a trivial
proportion of all verbs in VerbNet).
</bodyText>
<subsectionHeader confidence="0.999957">
3.2 Crowd-sourcing Semantic Judgments
</subsectionHeader>
<bodyText confidence="0.99778836">
The pilot experiments showed that it is possible to
elicit reliable semantic judgments corresponding to
VerbNet predicates from naive participants (see sec-
tion 3.3). At the project website, volunteers choose
one of the tasks from a list and begin tagging sen-
tences. The sentences are sampled smartly, avoid-
ing sentences already tagged by that volunteer and
biased in favor of of the sentences with the fewest
3Note that each task is designed to elicit judgments about
entailments – things that must be true rather than are merely
likely to be true. If John greeted Bill, they might have come
into contact (e.g., by shaking hands), but perhaps they did not.
Previous work suggests that it is entailments that matter, partic-
ularly for explaining the syntactic behavior of verbs (Levin and
Rappaport Hovav, 2005)
judgments so far. Rather than assessing annotator
quality through gold standard trials with known an-
swers (which wastes data – the answers to these tri-
als are known), approximately 150 sentences were
chosen to be “over-sampled.” As the volunteer tags
sentences, approximately one out of every five are
from this over-sampled set until that volunteer has
tagged all of them. This guarantees that any given
volunteer will have tried some sentences targeted
by many other volunteers, allowing inter-annotator
agreement to be used to assess annotator quality.
Following the example of Zooniverse (zooni-
verse.org), a popular “Citizen Science” platform,
volunteers are encouraged but required to register
(requiring registration prior to seeing the tasks was
found to be a significant barrier to entry). Regis-
tration allows collecting linguistic and educational
background from the volunteer, and also makes it
possible to track the same volunteer across sessions.
Multiple gamification elements were incorporated
into VerbCorner in order to recruit and motivate vol-
unteers. Each task has a leaderboard, where the
volunteer can see his/her rank out of all volunteers
in terms of number of contributions made. In ad-
dition, there is a general leaderboard, which sums
across tasks. Volunteers can earn badges, displayed
on their homepage, for answering certain numbers
of questions in each task. Finally, at random inter-
vals bonus points are awarded, with the explanation
for the bonus points tailored to the task’s backstory.
VerbCorner was launched on May 21, 2013. After
six weeks, 555 volunteers had provided at least one
annotation, for a total of 39,274 annotations, demon-
strating the feasibility of collecting large numbers of
annotations through this method.
</bodyText>
<subsectionHeader confidence="0.999782">
3.3 Case Study: Equilibrium
</subsectionHeader>
<bodyText confidence="0.999880727272727">
“Equilibrium” was designed to elicit judgments
about application of force, frequently argued to be
a core semantic feature in the sense discussed above
(Pinker, 1989). The backstory involves the “Zen Di-
mension,” in which nobody is allowed to exert force
on anything else. The participant reads descriptions
of events (Sally sprayed paint onto the wall) and de-
cides whether they would be allowable in the Zen
Dimension – and, in particular, which participants
in the event are illegally applying force.
In order to minimize unwanted effects of world
</bodyText>
<page confidence="0.979193">
1440
</page>
<bodyText confidence="0.999991583333333">
knowledge, the verb’s arguments are replaced with
nonsense words or randomly chosen proper names
(Sally sprayed the dax onto the blicket). In the
context of the story, this is explained as necessary
anonymization: You are a government official de-
termining whether certain activities are allowable,
and ensuring anonymity is an important safeguard
against favoritism and corruption. An alternative
wouod be to use multiple different content words,
randomly chosen for each annotator. However, this
greatly increases the number of annotators needed
and quickly becomes infeasible.
</bodyText>
<subsectionHeader confidence="0.968241">
3.3.1 Pilot Results
</subsectionHeader>
<bodyText confidence="0.9999933">
The task was piloted on 138 sentences, which com-
prised all possible syntactic frames for three verbs
from each of five verb classes in VerbNet. After
two rounds of piloting (between the first and second,
wording in the backstory was adjusted for clarity
based on pilot subject feedback and results), Kripp’s
alpha reached .76 for 8 annotators, which represents
a reasonably high level of inter-annotator agreement.
Importantly, the modal response matched the intu-
itions of the researchers in 137 of 138 cases.4
</bodyText>
<subsectionHeader confidence="0.991897">
3.3.2 Preliminary VerbCorner Results
</subsectionHeader>
<bodyText confidence="0.999647166666667">
“Equillibrium” was one of the first tasks posted on
VerbCorner, with data currently being collected on
12 of the 280 VerbNet classes, for a total of 5,171
sentences. As of writing, 414 users have submitted
14,294 judgments. Individual annotators annotated
anywhere from 1 to 195 sentences (mean--8, me-
dian--4). While most sentences have relatively few
judgments, each of the 194 over-sampled sentences
has between 15 and 20 judgments.5
Comparing the modal response with the re-
searchers’ intuitions resulted in a match for 184 of
194 sentences. In general, where the modal response
</bodyText>
<footnote confidence="0.732924636363636">
4The remaining case was “The crose smashed sondily.” for
which four pilot subjects thought involved the crose applying
force – matching researcher intuition – and four thought did
not involve any application of force, perhaps interpreting the
sentence was a passive.
5These are the same 15 verbs used in the piloting. The num-
ber of sentences is larger in order to test a wider range of pos-
sible arguments. In particular, wherever appropriate, separate
sentences were constructed using animate and inanimate argu-
ments. Compare Sally sprayed the dax onto Mary and Sally
sprayed the dax onto the blicket.
</footnote>
<bodyText confidence="0.999956583333333">
did not match researcher intuitions, the modal re-
sponse was itself not popular, comprising an aver-
age of 53% of responses, compared with an aver-
age of 77% where the modal response matched re-
searcher intuitions. Thus, these appear to be cases of
disagreement, either because the correct intuition re-
quires more work to obtain or because of differences
across idiolects (at the moment, there is no obvious
pattern as to which sentences caused difficulty, but
the sample size is small). Thus, follow-up investi-
gation of sentences with little inter-coder agreement
may be warranted.
</bodyText>
<sectionHeader confidence="0.983274" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999932727272727">
Data-collection is ongoing. VerbNet identifies ap-
proximately 150 different semantic predicates. An-
notating every verb in each of its syntactic frames for
each semantic predicate would take many millions
of judgments. However, most of the semantic predi-
cates employed in VerbNet are very narrow in scope
and only apply to a few classes. Thus, we have be-
gun with broad predicates that are thought to apply
to many verbs and are adding progressively narrower
predicates as work progresses. At the current rate,
we should complete annotation for the half-dozen
most frequent semantic predicates in the space of a
year.
Future work will explore using an individual
annotator’s history across trials to weight that
user’s contributions, something that VerbCorner was
specifically designed to allow (see above). How to
assess annotator quality without gold standard data
is an active area of research (Passonneau and Car-
penter, 2013; Rzhetsky, Shatkay and Wilbur, 2009;
Whitehill et al., 2009). For instance, Whitehill and
colleagues (2009) provide an algorithm for jointly
estimating both annotator quality and annotation
difficulty (including the latter is important because
some annotators will have low agreement with oth-
ers due to their poor luck in being assigned difficult-
to-annotate sentences). This algorithm is shown to
outperform using the modal response.
Note that this necessarily biases against annota-
tors with few responses. In our case study above, ex-
cluding annotators who contributed small numbers
of annotations led to progressively worse match to
researcher intuition, suggesting that the loss in data
</bodyText>
<page confidence="0.971885">
1441
</page>
<bodyText confidence="0.99977125">
caused by excluding these annotations may not be
worth the increased confidence in annotation quality.
Future research will be needed to assess this trade-
off.
The above work shows the feasibility of crowd-
sourcing VerbNet semantic entailments, as has been
shown for a handful of other linguistic judgments
(Artignan, Hascoet and Lafourcade, 2009; Poesio et
al., 2012; Venhuizen et al., 2013). There are many
domains in which gold standard human judgments
are scarce; crowd-sourcing has considerable poten-
tial at addressing this need.
</bodyText>
<sectionHeader confidence="0.998915" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998786185185185">
B. Ambridge, J. M. Pine, C. F. Rowland, F. Chang, and
A. Bidgood. 2013. The retreat from overgeneral-
ization in child language acquisition: Word learning,
morphology, and verb argument structure. Wiley In-
terdisciplinary Reviews: Cognitive Science. 4:47-62.
G. Artignan, M. Hascoet, and M. Lafourcade. 2009.
Mutliscale visual analysis of lexical networks. Pro-
ceedings of the 13th International Conference on In-
formation Visualisation. Barcelona, Spain.
T. Chklovski and P. Pantel. 2004. VerbOcean: Mining
the Web for fine-grained semantic relations. Proceed-
ings of Empirical Methods in Natural Language Pro-
cessing (EMNLP). Barcelona, Spain.
L. Cosmides and J. Tooby. 1992. Cognitive adaptations
for social exchange. in The Adapted Mind. (J. Barkow,
L. Cosmides, and J. Tooby, Eds.) Oxford University
Press, Oxford, UK.
W. Croft. 2012. Verbs: Aspect and Argument Structure.
Oxford University Press, Oxford, UK.
D. R. Dowty. 1991. Thematic proto-roles and argument
selection. Language. 67:547-619.
E. Gibson and E. Fedorenko. 2013. The need for quanti-
tative methods in syntax and semantics research. Lan-
guage and Cognitive Processes. 28(1-2):88–124.
R. Jackendoff. 1990. Semantic Structures. The MIT
Press, Cambridge, MA.
E. Joanis, S. Stevenson, and D. James. 2008. A general
feature space for automatic verb classification. Natu-
ral Language Engineering. 14(3):337-367.
K. Kipper, A. Korhonen, N. Ryant and M. Palmer. 2008.
A large-scale classification of English verbs. Lan-
guage Resources and Evaluation Journal, 42:21–40
E. Margolis and S. Laurence 1999. Concepts: Core
Readings. The MIT Press, Cambridge, MA.
B. Levin. 1993. English Verb Classes and Alternations:
A Preliminary Investigation. University of Chicago
Press, Chicago.
B. Levin and M. Rappaport Hovav. 2005. Argument
Realization. Cambridge University Press, Cambridge,
UK.
D. Maynard, A. Funk, and W. Peters. 2009. Using
lexico-syntactic ontology design patterns for ontology
creation and population. Proceedings of Workshop on
Ontology Patterns (WOP 2009). Washington, DC
R. J. Passonneau and B. Carpenter 2013. The benefits
of a model of annotation. 7th Linguistic Annotation
Workshop and Interoperability with Discourse. Sofia,
Bulgaria.
D. Pesetsky. 1995. Zero Syntax: Experiencers and Cas-
cades. The MIT Press, Cambridge, MA.
S. Pinker. 1989. Learnability and Cognition. The MIT
Press, Cambridge, MA.
M. Poesio, J. Camberlain, U. Kruschwitz, L. Robaldo,
and L. Ducceschi. 2012. The Phrase Detective Multi-
lingual Corpus, Release 0.1. Proceedings of the Col-
laborative Resource Development and Delivery Work-
shop. Istanbul, Turkey
H. Poon and P. Domingos. 2009. Unsupervised seman-
tic parsing. Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing.
Singapore.
A. Rzhetsky, H. Shatkay, and W. J. Wilbur. 2009. How
to get the most out of your curation effort. PLoS Com-
putational Biology, 5(5):1–13.
E. S. Spelke and K. D. Kinzler. 2007. Core knowledge.
Developmental Science, 10(1):89–96.
R. Swier and S. Stevenson. 2004. Unsupervised seman-
tic role labeling. Proceedings of the Generative Lexi-
con Conference, GenLex-09. Pisa, Italy.
N. Venhuizen, V. Basile, K. Evang, and J. Bos. 2013.
Gamification for word sense labeling. Proceedings of
the 10th International Conference on Computational
Semantics (IWCS 2013). Potsdam, Germany
J. Whitehill, P. Ruvolo, T. F. Wu, J. Bergsma. and J.
Movellan. 2009. Whose vote should count more: Op-
timal integration of labels from labelers of unknown
expertise. Advances in Neural Information Processing
Systems, 22. Vancouver, Canada
A. Zaenen, C. Condoravdi, and D G. Bobrow. 2008. The
encoding of lexical implications in VN. Proceedings
of LREC 2008. Morocco
</reference>
<page confidence="0.994506">
1442
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.983677">
<title confidence="0.9984485">The VerbCorner Project: Toward an Empirically-Based Decomposition of Verbs</title>
<author confidence="0.999992">Joshua K Hartshorne Claire Bonial</author>
<author confidence="0.999992">Martha Palmer</author>
<affiliation confidence="0.9987525">Department of Brain and Cognitive Sciences Department of Linguistics Massachusetts Institute of Technology University of Colorado at Boulder</affiliation>
<address confidence="0.9986815">77 Massachusetts Avenue Hellems 290, 295 UCB Cambridge, MA 02139, USA Boulder, CO 80309,</address>
<abstract confidence="0.999539894736842">This research describes efforts to use crowdsourcing to improve the validity of the semantic predicates in VerbNet, a lexicon of about 6300 English verbs. The current semantic predicates can be thought of semantic primitives, into which the concepts denoted by a verb can be decomposed. For example, the the Spray class), involves the and where the event can be decomposed into an a was originally not in a particular location to now be in that location. Although VerbNet’s predicates are theoretically well-motivated, systematic empirical data is scarce. This paper describes a recently-launched attempt to address this issue with a series of human judgment tasks, posed to subjects in the form of games.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Ambridge</author>
<author>J M Pine</author>
<author>C F Rowland</author>
<author>F Chang</author>
<author>A Bidgood</author>
</authors>
<title>The retreat from overgeneralization in child language acquisition: Word learning, morphology, and verb argument structure. Wiley Interdisciplinary Reviews: Cognitive Science.</title>
<date>2013</date>
<pages>4--47</pages>
<contexts>
<context position="2780" citStr="Ambridge et al., 2013" startWordPosition="428" endWordPosition="431">ons that linguistic and psychological research suggests are the representations employed by humans. While the exact characterization of meaning (and by extension, thought) remains an area of active research in the cognitive sciences (Margolis and Laurence, 1999), decades of research in linguistics and psychology suggests that much of the meaning of a sentence – as well as its syntactic structure – can be accounted for by invoking a small number of highly abstract semantic features (usually represented as predicates), such as causation, agency, basic topological relations, and directed motion (Ambridge et al., 2013; Croft, 2012; Jackendoff, 1990; Levin and Rappaport Hovav, 2005; Pesetsky, 1995; Pinker, 1989). For instance, a given verb can appear in some syntactic frames (Sally broke the vase. Sally broke the vase with the hammer. The vase broke.) and not others (*Sally broke the vase to the floor. *Sally broke John the vase.). When verbs are classified according to the syntactic frames they can appear in, most if not all the verbs in a class involve the same set of abstract semantic features.1 Interestingly, roughly these same features (causation, etc.) have been singled out by developmental psychologi</context>
<context position="7179" citStr="Ambridge et al., 2013" startWordPosition="1143" endWordPosition="1146">e number of semantic predicates (146),2 verb entries (6580), and unique verb lemmas (6284) it is not feasible for a single research team to check, particularly since after a certain number of verbs, intuitions become less clear. In any case, it may not be ideal to rely solely on the intuitions of invested researchers, whose intuitions about subtle judgments may be clouded by theoretical commitments (Gibson and Federenko, 2013); the only way to ensure this is not the case is through independent validation. Unfortunately, of the 280 verb classes in VerbNet, this has been done for only a few (cf Ambridge et al., 2013). 3 VerbCorner The VerbCorner project was designed to address these issues by crowd-sourcing the semantic judgments online (gameswithwords.org/VerbCorner/). Several previous projects have successfully crowdsourced linguistic annotations, such as Phrase Detectives, where volunteers have contributed 2.5 million judgments on anaphoric relations (Poesio et al., 2012). Below, we outline the VerbCorner project and describe one specific annotation task in detail. 3.1 Developing Semantic Annotation Tasks Collecting accurate judgments on subtle questions from naive participants with limited metalinguis</context>
</contexts>
<marker>Ambridge, Pine, Rowland, Chang, Bidgood, 2013</marker>
<rawString>B. Ambridge, J. M. Pine, C. F. Rowland, F. Chang, and A. Bidgood. 2013. The retreat from overgeneralization in child language acquisition: Word learning, morphology, and verb argument structure. Wiley Interdisciplinary Reviews: Cognitive Science. 4:47-62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Artignan</author>
<author>M Hascoet</author>
<author>M Lafourcade</author>
</authors>
<title>Mutliscale visual analysis of lexical networks.</title>
<date>2009</date>
<booktitle>Proceedings of the 13th International Conference on Information Visualisation.</booktitle>
<location>Barcelona,</location>
<marker>Artignan, Hascoet, Lafourcade, 2009</marker>
<rawString>G. Artignan, M. Hascoet, and M. Lafourcade. 2009. Mutliscale visual analysis of lexical networks. Proceedings of the 13th International Conference on Information Visualisation. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Chklovski</author>
<author>P Pantel</author>
</authors>
<title>VerbOcean: Mining the Web for fine-grained semantic relations.</title>
<date>2004</date>
<booktitle>Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1793" citStr="Chklovski and Pantel, 2004" startWordPosition="278" endWordPosition="282">sks, posed to subjects in the form of games. 1 Introduction One key application of Natural Language Processing (NLP) is meaning extraction. Of particular importance is propositional meaning: To understand “Jessica sprayed paint on the wall,” it is not enough to know who Jessica is, what paint is, and where the wall is, but that, by the end of the event, some quantity of paint that was not previously on the wall now is. One must extract not only meanings for individual words but also the relations between them. One option is to learn these relations in a largely bottom-up, data-driven fashion (Chklovski and Pantel, 2004; Poon and Domingos, 2009). For instance, Poon and Domingos (2009) first extracts dependency trees, converts those into quasi-logical form, recursively induces lambda expressions from them, and uses clustering to derive progressively abstract knowledge. An alternative is to take a human-inspired approach, mapping the linguistic input onto the kinds of representations that linguistic and psychological research suggests are the representations employed by humans. While the exact characterization of meaning (and by extension, thought) remains an area of active research in the cognitive sciences (</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>T. Chklovski and P. Pantel. 2004. VerbOcean: Mining the Web for fine-grained semantic relations. Proceedings of Empirical Methods in Natural Language Processing (EMNLP). Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Cosmides</author>
<author>J Tooby</author>
</authors>
<title>Cognitive adaptations for social exchange. in The Adapted</title>
<date>1992</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="8439" citStr="Cosmides and Tooby, 1992" startWordPosition="1321" endWordPosition="1324">ability from those specific to a small number of verbs (CHARACTERIZE, CONSPIRE) to those frequently invoked (BEGIN, EXIST). 1439 skills is difficult. Rare is the non-linguist who can immediately answer the question, “Does the verb ‘throw,’ when used transitively, entail a change of location on the part of its THEME?” Thus, we began by developing tasks that isolate semantic features in a way accessible to untrained annotators. We converted the metalinguistic judgments (“Does this verb entail this abstract predicate?”) into real-world problems, which previous research suggests should be easier (Cosmides and Tooby, 1992). Each judgment tasks involved a fanciful backstory. For instance, in “Simon Says Freeze”, a task designed to elicit judgments about movement, the Galactic Overlord (Simon) decrees “Galactic Stay Where You Are Day,” during which nobody is allowed to move from their current location. Participants read descriptions of events and decide whether anyone violated the rule. In “Explode on Contact”, designed to elicit judgments about physical contact, objects and people explode when they touch one another. The participant reads descriptions of events and decides whether anything has exploded.3 Each ta</context>
</contexts>
<marker>Cosmides, Tooby, 1992</marker>
<rawString>L. Cosmides and J. Tooby. 1992. Cognitive adaptations for social exchange. in The Adapted Mind. (J. Barkow, L. Cosmides, and J. Tooby, Eds.) Oxford University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Croft</author>
</authors>
<title>Verbs: Aspect and Argument Structure.</title>
<date>2012</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="2793" citStr="Croft, 2012" startWordPosition="432" endWordPosition="433"> psychological research suggests are the representations employed by humans. While the exact characterization of meaning (and by extension, thought) remains an area of active research in the cognitive sciences (Margolis and Laurence, 1999), decades of research in linguistics and psychology suggests that much of the meaning of a sentence – as well as its syntactic structure – can be accounted for by invoking a small number of highly abstract semantic features (usually represented as predicates), such as causation, agency, basic topological relations, and directed motion (Ambridge et al., 2013; Croft, 2012; Jackendoff, 1990; Levin and Rappaport Hovav, 2005; Pesetsky, 1995; Pinker, 1989). For instance, a given verb can appear in some syntactic frames (Sally broke the vase. Sally broke the vase with the hammer. The vase broke.) and not others (*Sally broke the vase to the floor. *Sally broke John the vase.). When verbs are classified according to the syntactic frames they can appear in, most if not all the verbs in a class involve the same set of abstract semantic features.1 Interestingly, roughly these same features (causation, etc.) have been singled out by developmental psychologists as part o</context>
</contexts>
<marker>Croft, 2012</marker>
<rawString>W. Croft. 2012. Verbs: Aspect and Argument Structure. Oxford University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Dowty</author>
</authors>
<title>Thematic proto-roles and argument selection.</title>
<date>1991</date>
<journal>Language.</journal>
<pages>67--547</pages>
<marker>Dowty, 1991</marker>
<rawString>D. R. Dowty. 1991. Thematic proto-roles and argument selection. Language. 67:547-619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gibson</author>
<author>E Fedorenko</author>
</authors>
<title>The need for quantitative methods in syntax and semantics research. Language and Cognitive Processes.</title>
<date>2013</date>
<pages>28--1</pages>
<marker>Gibson, Fedorenko, 2013</marker>
<rawString>E. Gibson and E. Fedorenko. 2013. The need for quantitative methods in syntax and semantics research. Language and Cognitive Processes. 28(1-2):88–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jackendoff</author>
</authors>
<title>Semantic Structures.</title>
<date>1990</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2811" citStr="Jackendoff, 1990" startWordPosition="434" endWordPosition="436">l research suggests are the representations employed by humans. While the exact characterization of meaning (and by extension, thought) remains an area of active research in the cognitive sciences (Margolis and Laurence, 1999), decades of research in linguistics and psychology suggests that much of the meaning of a sentence – as well as its syntactic structure – can be accounted for by invoking a small number of highly abstract semantic features (usually represented as predicates), such as causation, agency, basic topological relations, and directed motion (Ambridge et al., 2013; Croft, 2012; Jackendoff, 1990; Levin and Rappaport Hovav, 2005; Pesetsky, 1995; Pinker, 1989). For instance, a given verb can appear in some syntactic frames (Sally broke the vase. Sally broke the vase with the hammer. The vase broke.) and not others (*Sally broke the vase to the floor. *Sally broke John the vase.). When verbs are classified according to the syntactic frames they can appear in, most if not all the verbs in a class involve the same set of abstract semantic features.1 Interestingly, roughly these same features (causation, etc.) have been singled out by developmental psychologists as part of “core knowledge”</context>
</contexts>
<marker>Jackendoff, 1990</marker>
<rawString>R. Jackendoff. 1990. Semantic Structures. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Joanis</author>
<author>S Stevenson</author>
<author>D James</author>
</authors>
<title>A general feature space for automatic verb classification.</title>
<date>2008</date>
<journal>Natural Language Engineering.</journal>
<pages>14--3</pages>
<contexts>
<context position="6054" citStr="Joanis et al., 2008" startWordPosition="957" endWordPosition="960">y vs. splash) are omitted. Importantly, the semantics of the sentence is dependent on both the matrix verb (paint) and the syntactic frame. Famously, when inserted in the slightly different frame NP V NP.DESTINATION PP.THEME – “Sally sprayed the wall with paint” – “spray” entails that destination (the wall) is now fully painted, an entailment that does not follow in the example above (Pinker, 1989). 2.2 Uses and Limitations VerbNet has been used in a variety of NLP applications, such as semantic role labeling (Swier and Stevenson, 2004), inferencing (Zaenen et al., 2008), verb classification (Joanis et al., 2008), and information extraction (Maynard, Funk, and Peters, 2009). While such applications have been successful thus far, an important constraint on how well VerbNetbased NLP applications can be expected to perform is the accuracy of the semantics encoded in VerbNet. Here, several issues arise. Leaving aside miscategorized verbs and other inaccuracies, as noted above VerbNet assumes that all verbs in the same class share the same core predicates, which may or may not be empirically justified. Given the number of semantic predicates (146),2 verb entries (6580), and unique verb lemmas (6284) it is </context>
</contexts>
<marker>Joanis, Stevenson, James, 2008</marker>
<rawString>E. Joanis, S. Stevenson, and D. James. 2008. A general feature space for automatic verb classification. Natural Language Engineering. 14(3):337-367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper</author>
<author>A Korhonen</author>
<author>N Ryant</author>
<author>M Palmer</author>
</authors>
<title>A large-scale classification of English verbs.</title>
<date>2008</date>
<journal>Language Resources and Evaluation Journal,</journal>
<pages>42--21</pages>
<contexts>
<context position="4312" citStr="Kipper et al., 2008" startWordPosition="670" endWordPosition="673">anguage Processing, pages 1438–1442, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics the rest of cognition is built (Spelke and Kinzler, 2007). Thus these semantic features/predicates may be not only crucial to describing linguistic meaning but may be central organizing principles for a human’s (reasonably successful) thinking about and conceptualization of the world. As such, they provide a potentially rewarding target for NLP. 2 VerbNet 2.1 Overview and Structure Perhaps the most comprehensive implementation of this approach appears in VerbNet (Kipper et al., 2008; based on Levin, 1993). VerbNet classifies verbs based on the syntactic frames they can appear in, providing a semantic description of each frame for each class. An example entry is shown below: Syntactic Frame NP V NP PP.DESTINATION Example Jessica sprayed the wall. Syntax AGENT V THEME {+LOC|+DEST CONF} DESTINATION Semantics MOTION(DURING(E), THEME) NOT(PREP(START(E), THEME, DESTINATION)) PREP(END(E), THEME, DESTINATION) CAUSE(AGENT, E) The “Syntactic Frame” provides a flat syntactic parse. “Syntax” provides semantic role labels for each of the NPs and PPs, which are invoked in “Semantics”.</context>
</contexts>
<marker>Kipper, Korhonen, Ryant, Palmer, 2008</marker>
<rawString>K. Kipper, A. Korhonen, N. Ryant and M. Palmer. 2008. A large-scale classification of English verbs. Language Resources and Evaluation Journal, 42:21–40</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Margolis</author>
<author>S Laurence</author>
</authors>
<title>Concepts: Core Readings.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2421" citStr="Margolis and Laurence, 1999" startWordPosition="370" endWordPosition="373">; Poon and Domingos, 2009). For instance, Poon and Domingos (2009) first extracts dependency trees, converts those into quasi-logical form, recursively induces lambda expressions from them, and uses clustering to derive progressively abstract knowledge. An alternative is to take a human-inspired approach, mapping the linguistic input onto the kinds of representations that linguistic and psychological research suggests are the representations employed by humans. While the exact characterization of meaning (and by extension, thought) remains an area of active research in the cognitive sciences (Margolis and Laurence, 1999), decades of research in linguistics and psychology suggests that much of the meaning of a sentence – as well as its syntactic structure – can be accounted for by invoking a small number of highly abstract semantic features (usually represented as predicates), such as causation, agency, basic topological relations, and directed motion (Ambridge et al., 2013; Croft, 2012; Jackendoff, 1990; Levin and Rappaport Hovav, 2005; Pesetsky, 1995; Pinker, 1989). For instance, a given verb can appear in some syntactic frames (Sally broke the vase. Sally broke the vase with the hammer. The vase broke.) and</context>
</contexts>
<marker>Margolis, Laurence, 1999</marker>
<rawString>E. Margolis and S. Laurence 1999. Concepts: Core Readings. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="4335" citStr="Levin, 1993" startWordPosition="676" endWordPosition="677">–1442, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics the rest of cognition is built (Spelke and Kinzler, 2007). Thus these semantic features/predicates may be not only crucial to describing linguistic meaning but may be central organizing principles for a human’s (reasonably successful) thinking about and conceptualization of the world. As such, they provide a potentially rewarding target for NLP. 2 VerbNet 2.1 Overview and Structure Perhaps the most comprehensive implementation of this approach appears in VerbNet (Kipper et al., 2008; based on Levin, 1993). VerbNet classifies verbs based on the syntactic frames they can appear in, providing a semantic description of each frame for each class. An example entry is shown below: Syntactic Frame NP V NP PP.DESTINATION Example Jessica sprayed the wall. Syntax AGENT V THEME {+LOC|+DEST CONF} DESTINATION Semantics MOTION(DURING(E), THEME) NOT(PREP(START(E), THEME, DESTINATION)) PREP(END(E), THEME, DESTINATION) CAUSE(AGENT, E) The “Syntactic Frame” provides a flat syntactic parse. “Syntax” provides semantic role labels for each of the NPs and PPs, which are invoked in “Semantics”. VerbNet decomposes the</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>B. Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
<author>M Rappaport Hovav</author>
</authors>
<title>Argument Realization.</title>
<date>2005</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<marker>Levin, Hovav, 2005</marker>
<rawString>B. Levin and M. Rappaport Hovav. 2005. Argument Realization. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Maynard</author>
<author>A Funk</author>
<author>W Peters</author>
</authors>
<title>Using lexico-syntactic ontology design patterns for ontology creation and population.</title>
<date>2009</date>
<booktitle>Proceedings of Workshop on Ontology Patterns (WOP 2009).</booktitle>
<location>Washington, DC</location>
<contexts>
<context position="6115" citStr="Maynard, Funk, and Peters, 2009" startWordPosition="965" endWordPosition="969">ics of the sentence is dependent on both the matrix verb (paint) and the syntactic frame. Famously, when inserted in the slightly different frame NP V NP.DESTINATION PP.THEME – “Sally sprayed the wall with paint” – “spray” entails that destination (the wall) is now fully painted, an entailment that does not follow in the example above (Pinker, 1989). 2.2 Uses and Limitations VerbNet has been used in a variety of NLP applications, such as semantic role labeling (Swier and Stevenson, 2004), inferencing (Zaenen et al., 2008), verb classification (Joanis et al., 2008), and information extraction (Maynard, Funk, and Peters, 2009). While such applications have been successful thus far, an important constraint on how well VerbNetbased NLP applications can be expected to perform is the accuracy of the semantics encoded in VerbNet. Here, several issues arise. Leaving aside miscategorized verbs and other inaccuracies, as noted above VerbNet assumes that all verbs in the same class share the same core predicates, which may or may not be empirically justified. Given the number of semantic predicates (146),2 verb entries (6580), and unique verb lemmas (6284) it is not feasible for a single research team to check, particularl</context>
</contexts>
<marker>Maynard, Funk, Peters, 2009</marker>
<rawString>D. Maynard, A. Funk, and W. Peters. 2009. Using lexico-syntactic ontology design patterns for ontology creation and population. Proceedings of Workshop on Ontology Patterns (WOP 2009). Washington, DC</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Passonneau</author>
<author>B Carpenter</author>
</authors>
<title>The benefits of a model of annotation.</title>
<date>2013</date>
<booktitle>7th Linguistic Annotation Workshop and Interoperability with Discourse.</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="16539" citStr="Passonneau and Carpenter, 2013" startWordPosition="2588" endWordPosition="2592">n scope and only apply to a few classes. Thus, we have begun with broad predicates that are thought to apply to many verbs and are adding progressively narrower predicates as work progresses. At the current rate, we should complete annotation for the half-dozen most frequent semantic predicates in the space of a year. Future work will explore using an individual annotator’s history across trials to weight that user’s contributions, something that VerbCorner was specifically designed to allow (see above). How to assess annotator quality without gold standard data is an active area of research (Passonneau and Carpenter, 2013; Rzhetsky, Shatkay and Wilbur, 2009; Whitehill et al., 2009). For instance, Whitehill and colleagues (2009) provide an algorithm for jointly estimating both annotator quality and annotation difficulty (including the latter is important because some annotators will have low agreement with others due to their poor luck in being assigned difficultto-annotate sentences). This algorithm is shown to outperform using the modal response. Note that this necessarily biases against annotators with few responses. In our case study above, excluding annotators who contributed small numbers of annotations l</context>
</contexts>
<marker>Passonneau, Carpenter, 2013</marker>
<rawString>R. J. Passonneau and B. Carpenter 2013. The benefits of a model of annotation. 7th Linguistic Annotation Workshop and Interoperability with Discourse. Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pesetsky</author>
</authors>
<title>Zero Syntax: Experiencers and Cascades.</title>
<date>1995</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2860" citStr="Pesetsky, 1995" startWordPosition="442" endWordPosition="444">d by humans. While the exact characterization of meaning (and by extension, thought) remains an area of active research in the cognitive sciences (Margolis and Laurence, 1999), decades of research in linguistics and psychology suggests that much of the meaning of a sentence – as well as its syntactic structure – can be accounted for by invoking a small number of highly abstract semantic features (usually represented as predicates), such as causation, agency, basic topological relations, and directed motion (Ambridge et al., 2013; Croft, 2012; Jackendoff, 1990; Levin and Rappaport Hovav, 2005; Pesetsky, 1995; Pinker, 1989). For instance, a given verb can appear in some syntactic frames (Sally broke the vase. Sally broke the vase with the hammer. The vase broke.) and not others (*Sally broke the vase to the floor. *Sally broke John the vase.). When verbs are classified according to the syntactic frames they can appear in, most if not all the verbs in a class involve the same set of abstract semantic features.1 Interestingly, roughly these same features (causation, etc.) have been singled out by developmental psychologists as part of “core knowledge” – a set of early-learned or perhaps innate conce</context>
</contexts>
<marker>Pesetsky, 1995</marker>
<rawString>D. Pesetsky. 1995. Zero Syntax: Experiencers and Cascades. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pinker</author>
</authors>
<title>Learnability and Cognition.</title>
<date>1989</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2875" citStr="Pinker, 1989" startWordPosition="445" endWordPosition="446">le the exact characterization of meaning (and by extension, thought) remains an area of active research in the cognitive sciences (Margolis and Laurence, 1999), decades of research in linguistics and psychology suggests that much of the meaning of a sentence – as well as its syntactic structure – can be accounted for by invoking a small number of highly abstract semantic features (usually represented as predicates), such as causation, agency, basic topological relations, and directed motion (Ambridge et al., 2013; Croft, 2012; Jackendoff, 1990; Levin and Rappaport Hovav, 2005; Pesetsky, 1995; Pinker, 1989). For instance, a given verb can appear in some syntactic frames (Sally broke the vase. Sally broke the vase with the hammer. The vase broke.) and not others (*Sally broke the vase to the floor. *Sally broke John the vase.). When verbs are classified according to the syntactic frames they can appear in, most if not all the verbs in a class involve the same set of abstract semantic features.1 Interestingly, roughly these same features (causation, etc.) have been singled out by developmental psychologists as part of “core knowledge” – a set of early-learned or perhaps innate concepts upon which </context>
<context position="5835" citStr="Pinker, 1989" startWordPosition="924" endWordPosition="925">on the wall), and; 4) the event is caused by the AGENT (Sally). Note that this captures only the core aspects of semantics shared by all verbs in the class; differences between verbs in the same class (e.g., spray vs. splash) are omitted. Importantly, the semantics of the sentence is dependent on both the matrix verb (paint) and the syntactic frame. Famously, when inserted in the slightly different frame NP V NP.DESTINATION PP.THEME – “Sally sprayed the wall with paint” – “spray” entails that destination (the wall) is now fully painted, an entailment that does not follow in the example above (Pinker, 1989). 2.2 Uses and Limitations VerbNet has been used in a variety of NLP applications, such as semantic role labeling (Swier and Stevenson, 2004), inferencing (Zaenen et al., 2008), verb classification (Joanis et al., 2008), and information extraction (Maynard, Funk, and Peters, 2009). While such applications have been successful thus far, an important constraint on how well VerbNetbased NLP applications can be expected to perform is the accuracy of the semantics encoded in VerbNet. Here, several issues arise. Leaving aside miscategorized verbs and other inaccuracies, as noted above VerbNet assume</context>
<context position="12288" citStr="Pinker, 1989" startWordPosition="1920" endWordPosition="1921">ring certain numbers of questions in each task. Finally, at random intervals bonus points are awarded, with the explanation for the bonus points tailored to the task’s backstory. VerbCorner was launched on May 21, 2013. After six weeks, 555 volunteers had provided at least one annotation, for a total of 39,274 annotations, demonstrating the feasibility of collecting large numbers of annotations through this method. 3.3 Case Study: Equilibrium “Equilibrium” was designed to elicit judgments about application of force, frequently argued to be a core semantic feature in the sense discussed above (Pinker, 1989). The backstory involves the “Zen Dimension,” in which nobody is allowed to exert force on anything else. The participant reads descriptions of events (Sally sprayed paint onto the wall) and decides whether they would be allowable in the Zen Dimension – and, in particular, which participants in the event are illegally applying force. In order to minimize unwanted effects of world 1440 knowledge, the verb’s arguments are replaced with nonsense words or randomly chosen proper names (Sally sprayed the dax onto the blicket). In the context of the story, this is explained as necessary anonymization</context>
</contexts>
<marker>Pinker, 1989</marker>
<rawString>S. Pinker. 1989. Learnability and Cognition. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>J Camberlain</author>
<author>U Kruschwitz</author>
<author>L Robaldo</author>
<author>L Ducceschi</author>
</authors>
<title>The Phrase Detective Multilingual Corpus, Release 0.1.</title>
<date>2012</date>
<booktitle>Proceedings of the Collaborative Resource Development and Delivery Workshop.</booktitle>
<location>Istanbul, Turkey</location>
<contexts>
<context position="7544" citStr="Poesio et al., 2012" startWordPosition="1191" endWordPosition="1194">clouded by theoretical commitments (Gibson and Federenko, 2013); the only way to ensure this is not the case is through independent validation. Unfortunately, of the 280 verb classes in VerbNet, this has been done for only a few (cf Ambridge et al., 2013). 3 VerbCorner The VerbCorner project was designed to address these issues by crowd-sourcing the semantic judgments online (gameswithwords.org/VerbCorner/). Several previous projects have successfully crowdsourced linguistic annotations, such as Phrase Detectives, where volunteers have contributed 2.5 million judgments on anaphoric relations (Poesio et al., 2012). Below, we outline the VerbCorner project and describe one specific annotation task in detail. 3.1 Developing Semantic Annotation Tasks Collecting accurate judgments on subtle questions from naive participants with limited metalinguistic 2Note that these vary in applicability from those specific to a small number of verbs (CHARACTERIZE, CONSPIRE) to those frequently invoked (BEGIN, EXIST). 1439 skills is difficult. Rare is the non-linguist who can immediately answer the question, “Does the verb ‘throw,’ when used transitively, entail a change of location on the part of its THEME?” Thus, we be</context>
</contexts>
<marker>Poesio, Camberlain, Kruschwitz, Robaldo, Ducceschi, 2012</marker>
<rawString>M. Poesio, J. Camberlain, U. Kruschwitz, L. Robaldo, and L. Ducceschi. 2012. The Phrase Detective Multilingual Corpus, Release 0.1. Proceedings of the Collaborative Resource Development and Delivery Workshop. Istanbul, Turkey</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Poon</author>
<author>P Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1819" citStr="Poon and Domingos, 2009" startWordPosition="283" endWordPosition="286">e form of games. 1 Introduction One key application of Natural Language Processing (NLP) is meaning extraction. Of particular importance is propositional meaning: To understand “Jessica sprayed paint on the wall,” it is not enough to know who Jessica is, what paint is, and where the wall is, but that, by the end of the event, some quantity of paint that was not previously on the wall now is. One must extract not only meanings for individual words but also the relations between them. One option is to learn these relations in a largely bottom-up, data-driven fashion (Chklovski and Pantel, 2004; Poon and Domingos, 2009). For instance, Poon and Domingos (2009) first extracts dependency trees, converts those into quasi-logical form, recursively induces lambda expressions from them, and uses clustering to derive progressively abstract knowledge. An alternative is to take a human-inspired approach, mapping the linguistic input onto the kinds of representations that linguistic and psychological research suggests are the representations employed by humans. While the exact characterization of meaning (and by extension, thought) remains an area of active research in the cognitive sciences (Margolis and Laurence, 199</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>H. Poon and P. Domingos. 2009. Unsupervised semantic parsing. Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rzhetsky</author>
<author>H Shatkay</author>
<author>W J Wilbur</author>
</authors>
<title>How to get the most out of your curation effort.</title>
<date>2009</date>
<journal>PLoS Computational Biology,</journal>
<volume>5</volume>
<issue>5</issue>
<marker>Rzhetsky, Shatkay, Wilbur, 2009</marker>
<rawString>A. Rzhetsky, H. Shatkay, and W. J. Wilbur. 2009. How to get the most out of your curation effort. PLoS Computational Biology, 5(5):1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E S Spelke</author>
<author>K D Kinzler</author>
</authors>
<title>Core knowledge.</title>
<date>2007</date>
<journal>Developmental Science,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="3882" citStr="Spelke and Kinzler, 2007" startWordPosition="606" endWordPosition="609">c features.1 Interestingly, roughly these same features (causation, etc.) have been singled out by developmental psychologists as part of “core knowledge” – a set of early-learned or perhaps innate concepts upon which 1Whether all verbs in a class share the same abstract predicates or merely most is an area of active research (Levin and Rappaport Hovav, 2005). 1438 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1438–1442, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics the rest of cognition is built (Spelke and Kinzler, 2007). Thus these semantic features/predicates may be not only crucial to describing linguistic meaning but may be central organizing principles for a human’s (reasonably successful) thinking about and conceptualization of the world. As such, they provide a potentially rewarding target for NLP. 2 VerbNet 2.1 Overview and Structure Perhaps the most comprehensive implementation of this approach appears in VerbNet (Kipper et al., 2008; based on Levin, 1993). VerbNet classifies verbs based on the syntactic frames they can appear in, providing a semantic description of each frame for each class. An exam</context>
</contexts>
<marker>Spelke, Kinzler, 2007</marker>
<rawString>E. S. Spelke and K. D. Kinzler. 2007. Core knowledge. Developmental Science, 10(1):89–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Swier</author>
<author>S Stevenson</author>
</authors>
<title>Unsupervised semantic role labeling.</title>
<date>2004</date>
<booktitle>Proceedings of the Generative Lexicon Conference, GenLex-09.</booktitle>
<location>Pisa, Italy.</location>
<contexts>
<context position="5976" citStr="Swier and Stevenson, 2004" startWordPosition="946" endWordPosition="949">d by all verbs in the class; differences between verbs in the same class (e.g., spray vs. splash) are omitted. Importantly, the semantics of the sentence is dependent on both the matrix verb (paint) and the syntactic frame. Famously, when inserted in the slightly different frame NP V NP.DESTINATION PP.THEME – “Sally sprayed the wall with paint” – “spray” entails that destination (the wall) is now fully painted, an entailment that does not follow in the example above (Pinker, 1989). 2.2 Uses and Limitations VerbNet has been used in a variety of NLP applications, such as semantic role labeling (Swier and Stevenson, 2004), inferencing (Zaenen et al., 2008), verb classification (Joanis et al., 2008), and information extraction (Maynard, Funk, and Peters, 2009). While such applications have been successful thus far, an important constraint on how well VerbNetbased NLP applications can be expected to perform is the accuracy of the semantics encoded in VerbNet. Here, several issues arise. Leaving aside miscategorized verbs and other inaccuracies, as noted above VerbNet assumes that all verbs in the same class share the same core predicates, which may or may not be empirically justified. Given the number of semanti</context>
</contexts>
<marker>Swier, Stevenson, 2004</marker>
<rawString>R. Swier and S. Stevenson. 2004. Unsupervised semantic role labeling. Proceedings of the Generative Lexicon Conference, GenLex-09. Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Venhuizen</author>
<author>V Basile</author>
<author>K Evang</author>
<author>J Bos</author>
</authors>
<title>Gamification for word sense labeling.</title>
<date>2013</date>
<booktitle>Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013).</booktitle>
<location>Potsdam, Germany</location>
<marker>Venhuizen, Basile, Evang, Bos, 2013</marker>
<rawString>N. Venhuizen, V. Basile, K. Evang, and J. Bos. 2013. Gamification for word sense labeling. Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013). Potsdam, Germany</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Movellan</author>
</authors>
<title>Whose vote should count more: Optimal integration of labels from labelers of unknown expertise.</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems, 22.</booktitle>
<location>Vancouver, Canada</location>
<marker>Movellan, 2009</marker>
<rawString>J. Whitehill, P. Ruvolo, T. F. Wu, J. Bergsma. and J. Movellan. 2009. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. Advances in Neural Information Processing Systems, 22. Vancouver, Canada</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zaenen</author>
<author>C Condoravdi</author>
<author>D G Bobrow</author>
</authors>
<title>The encoding of lexical implications in VN.</title>
<date>2008</date>
<booktitle>Proceedings of LREC</booktitle>
<contexts>
<context position="6011" citStr="Zaenen et al., 2008" startWordPosition="951" endWordPosition="954">between verbs in the same class (e.g., spray vs. splash) are omitted. Importantly, the semantics of the sentence is dependent on both the matrix verb (paint) and the syntactic frame. Famously, when inserted in the slightly different frame NP V NP.DESTINATION PP.THEME – “Sally sprayed the wall with paint” – “spray” entails that destination (the wall) is now fully painted, an entailment that does not follow in the example above (Pinker, 1989). 2.2 Uses and Limitations VerbNet has been used in a variety of NLP applications, such as semantic role labeling (Swier and Stevenson, 2004), inferencing (Zaenen et al., 2008), verb classification (Joanis et al., 2008), and information extraction (Maynard, Funk, and Peters, 2009). While such applications have been successful thus far, an important constraint on how well VerbNetbased NLP applications can be expected to perform is the accuracy of the semantics encoded in VerbNet. Here, several issues arise. Leaving aside miscategorized verbs and other inaccuracies, as noted above VerbNet assumes that all verbs in the same class share the same core predicates, which may or may not be empirically justified. Given the number of semantic predicates (146),2 verb entries (</context>
</contexts>
<marker>Zaenen, Condoravdi, Bobrow, 2008</marker>
<rawString>A. Zaenen, C. Condoravdi, and D G. Bobrow. 2008. The encoding of lexical implications in VN. Proceedings of LREC 2008. Morocco</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
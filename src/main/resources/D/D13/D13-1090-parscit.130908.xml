<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000534">
<title confidence="0.994108">
Discriminative Improvements to Distributional Sentence Similarity
</title>
<author confidence="0.999316">
Yangfeng Ji
</author>
<affiliation confidence="0.9988955">
School of Interactive Computing
Georgia Institute of Technology
</affiliation>
<email confidence="0.998062">
jiyfeng@gatech.edu
</email>
<author confidence="0.998156">
Jacob Eisenstein
</author>
<affiliation confidence="0.9988785">
School of Interactive Computing
Georgia Institute of Technology
</affiliation>
<email confidence="0.998484">
jacobe@gatech.edu
</email>
<sectionHeader confidence="0.994794" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.936654176470588">
Matrix and tensor factorization have been ap-
plied to a number of semantic relatedness
tasks, including paraphrase identification. The
key idea is that similarity in the latent space
implies semantic relatedness. We describe
three ways in which labeled data can im-
prove the accuracy of these approaches on
paraphrase classification. First, we design
a new discriminative term-weighting metric
called TF-KLD, which outperforms TF-IDF.
Next, we show that using the latent repre-
sentation from matrix factorization as features
in a classification algorithm substantially im-
proves accuracy. Finally, we combine latent
features with fine-grained n-gram overlap fea-
tures, yielding performance that is 3% more
accurate than the prior state-of-the-art.
</bodyText>
<sectionHeader confidence="0.99885" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999950936170213">
Measuring the semantic similarity of short units
of text is fundamental to many natural language
processing tasks, from evaluating machine transla-
tion (Kauchak and Barzilay, 2006) to grouping re-
dundant event mentions in social media (Petrovi´c
et al., 2010). The task is challenging because of
the infinitely diverse set of possible linguistic real-
izations for any idea (Bhagat and Hovy, 2013), and
because of the short length of individual sentences,
which means that standard bag-of-words representa-
tions will be hopelessly sparse.
Distributional methods address this problem by
transforming the high-dimensional bag-of-words
representation into a lower-dimensional latent space.
This can be accomplished by factoring a matrix
or tensor of term-context counts (Turney and Pan-
tel, 2010); proximity in the induced latent space
has been shown to correlate with semantic similar-
ity (Mihalcea et al., 2006). However, factoring the
term-context matrix means throwing away a consid-
erable amount of information, as the original ma-
trix of size M x N (number of instances by number
of features) is factored into two smaller matrices of
size M x K and N x K, with K « M, N. If the
factorization does not take into account labeled data
about semantic similarity, important information can
be lost.
In this paper, we show how labeled data can con-
siderably improve distributional methods for mea-
suring semantic similarity. First, we develop a
new discriminative term-weighting metric called
TF-KLD, which is applied to the term-context ma-
trix before factorization. On a standard paraphrase
identification task (Dolan et al., 2004), this method
improves on both traditional TF-IDF and Weighted
Textual Matrix Factorization (WTMF; Guo and
Diab, 2012). Next, we convert the latent repre-
sentations of each sentence pair into a feature vec-
tor, which is used as input to a linear SVM clas-
sifier. This yields further improvements and sub-
stantially outperforms the current state-of-the-art
on paraphrase classification. We then add “fine-
grained” features about the lexical similarity of the
sentence pair. The combination of latent and fine-
grained features yields further improvements in ac-
curacy, demonstrating that these feature sets provide
complementary information on semantic similarity.
</bodyText>
<page confidence="0.976308">
891
</page>
<bodyText confidence="0.291942">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891–896,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999838125">
Without attempting to do justice to the entire lit-
erature on paraphrase identification, we note three
high-level approaches: (1) string similarity metrics
such as n-gram overlap and BLEU score (Wan et
al., 2006; Madnani et al., 2012), as well as string
kernels (Bu et al., 2012); (2) syntactic operations
on the parse structure (Wu, 2005; Das and Smith,
2009); and (3) distributional methods, such as la-
tent semantic analysis (LSA; Landauer et al., 1998),
which are most relevant to our work. One appli-
cation of distributional techniques is to replace in-
dividual words with distributionally similar alterna-
tives (Kauchak and Barzilay, 2006). Alternatively,
Blacoe and Lapata (2012) show that latent word rep-
resentations can be combined with simple element-
wise operations to identify the semantic similarity
of larger units of text. Socher et al. (2011) pro-
pose a syntactically-informed approach to combine
word representations, using a recursive auto-encoder
to propagate meaning through the parse tree.
We take a different approach: rather than repre-
senting the meanings of individual words, we di-
rectly obtain a distributional representation for the
entire sentence. This is inspired by Mihalcea et al.
(2006) and Guo and Diab (2012), who treat sen-
tences as pseudo-documents in an LSA framework,
and identify paraphrases using similarity in the la-
tent space. We show that the performance of such
techniques can be improved dramatically by using
supervised information to (1) reweight the individ-
ual distributional features and (2) learn the impor-
tance of each latent dimension.
</bodyText>
<sectionHeader confidence="0.992516" genericHeader="method">
3 Discriminative feature weighting
</sectionHeader>
<bodyText confidence="0.999015027027027">
Distributional representations (Turney and Pantel,
2010) can be induced from a co-occurrence ma-
trix W E RM N, where M is the number of in-
stances and N is the number of distributional fea-
tures. For paraphrase identification, each instance
is a sentence; features may be unigrams, or may
include higher-order n-grams or dependency pairs.
By decomposing the matrix W, we hope to obtain
a latent representation in which semantically-related
sentences are similar. Singular value decomposition
(SVD) is traditionally used to perform this factoriza-
tion. However, recent work has demonstrated the ro-
bustness of nonnegative matrix factorization (NMF;
Lee and Seung, 2001) for text mining tasks (Xu et
al., 2003; Arora et al., 2012); the difference from
SVD is the addition of a non-negativity constraint
in the latent representation based on non-orthogonal
basis.
While W may simply contain counts of distribu-
tional features, prior work has demonstrated the util-
ity of reweighting these counts (Turney and Pantel,
2010). TF-IDF is a standard approach, as the inverse
document frequency (IDF) term increases the impor-
tance of rare words, which may be more discrimi-
native. Guo and Diab (2012) show that applying a
special weight to unseen words can further improve-
ment performance on paraphrase identification.
We present a new weighting scheme, TF-KLD,
based on supervised information. The key idea is
to increase the weights of distributional features that
are discriminative, and to decrease the weights of
features that are not. Conceptually, this is similar
to Linear Discriminant Analysis, a supervised fea-
ture weighting scheme for continuous data (Murphy,
2012).
More formally, we assume labeled sentence pairs
of the form (~w(1)
</bodyText>
<equation confidence="0.835952">
i , ~w(2)
i , ri), where ~w(1)
</equation>
<bodyText confidence="0.981966888888889">
i is the bi-
narized vector of distributional features for the first
sentence, ~w(2)
i is the binarized vector of distribu-
tional features for the second sentence, and ri E
{0,1} indicates whether they are labeled as a para-
phrase pair. Assuming the order of the sentences
within the pair is irrelevant, then for k-th distribu-
tional feature, we define two Bernoulli distributions:
</bodyText>
<equation confidence="0.776333333333333">
• pk = P(w(1)
ik |w(2)
ik = 1, ri = 1). This is the
probability that sentence w(1)
i contains feature
k, given that k appears in w(2)
</equation>
<bodyText confidence="0.7542745">
i and the two sen-
tences are labeled as paraphrases, ri = 1.
</bodyText>
<equation confidence="0.8624105">
• qk = P(w(1)
ik |w(2)
</equation>
<bodyText confidence="0.8667233">
ik = 1, ri = 0). This is the
probability that sentence w(1)
i contains feature
k, given that k appears in w(2)
i and the two sen-
tences are labeled as not paraphrases, ri = 0.
The Kullback-Leibler divergence KL(pk||qk) =
� xpk(x) log pk(x)
qk(x) is then a measure of the discrim-
inability of feature k, and is guaranteed to be non-
</bodyText>
<page confidence="0.995914">
892
</page>
<figureCaption confidence="0.9797866">
Figure 1: Conditional probabilities for a few hand-
selected unigram features, with lines showing contours
with identical KL-divergence. The probabilities are es-
timated based on the MSRPC training set (Dolan et al.,
2004).
</figureCaption>
<bodyText confidence="0.999979434782609">
negative.1 We use this divergence to reweight the
features in W before performing the matrix factor-
ization. This has the effect of increasing the weights
of features whose likelihood of appearing in a pair
of sentences is strongly influenced by the paraphrase
relationship between the two sentences. On the other
hand, if pk = qk, then the KL-divergence will be
zero, and the feature will be ignored in the ma-
trix factorization. We name this weighting scheme
TF-KLD, since it includes the term frequency and
the KL-divergence.
Taking the unigram feature not as an example, we
have pk = [0.66, 0.34] and qk = [0.31, 0.69], for a
KL-divergence of 0.25: the likelihood of this word
being shared between two sentence is strongly de-
pendent on whether the sentences are paraphrases.
In contrast, the feature then has pk = [0.33, 0.67]
and qk = [0.32, 0.68], for a KL-divergence of 3.9 x
10−4. Figure 1 shows the distributions of these and
other unigram features with respect to pk and 1−qk.
The diagonal line running through the middle of the
plot indicates zero KL-divergence, so features on
this line will be ignored.
</bodyText>
<footnote confidence="0.961674333333333">
1We obtain very similar results with the opposite divergence
KL(qk||N). However, the symmetric Jensen-Shannon diver-
gence performs poorly.
</footnote>
<table confidence="0.4744285">
1 unigram recall
2 unigram precision
3 bigram recall
4 bigram precision
5 dependency relation recall
6 dependency relation precision
7 BLEU recall
8 BLEU precision
9 Difference of sentence length
10 Tree-editing distance
</table>
<tableCaption confidence="0.9947485">
Table 1: Fine-grained features for paraphrase classifica-
tion, selected from prior work (Wan et al., 2006).
</tableCaption>
<sectionHeader confidence="0.975248" genericHeader="method">
4 Supervised classification
</sectionHeader>
<bodyText confidence="0.9997355">
While previous work has performed paraphrase clas-
sification using distance or similarity in the latent
space (Guo and Diab, 2012; Socher et al., 2011),
more direct supervision can be applied. Specifically,
we convert the latent representations of a pair of sen-
tences ~v1 and ~v2 into a sample vector,
</bodyText>
<equation confidence="0.999768">
~s(~v1,~v2) = [~v1 + ~v2, |~v1 − ~v2 |(1)
</equation>
<bodyText confidence="0.999958923076923">
concatenating the element-wise sum ~v1 +~v2 and ab-
solute difference |~v1 − ~v2|. Note that ~s(·, ·) is sym-
metric, since ~s(~v1,~v2) = ~s(~v2,~v1). Given this rep-
resentation, we can use any supervised classification
algorithm.
A further advantage of treating paraphrase as a
supervised classification problem is that we can ap-
ply additional features besides the latent represen-
tation. We consider a subset of features identified
by Wan et al. (2006), listed in Table 1. These fea-
tures mainly capture fine-grained similarity between
sentences, for example by counting specific unigram
and bigram overlap.
</bodyText>
<sectionHeader confidence="0.999611" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999561666666667">
Our experiments test the utility of the TF-
KLD weighting towards paraphrase classification,
using the Microsoft Research Paraphrase Corpus
(Dolan et al., 2004). The training set contains 2753
true paraphrase pairs and 1323 false paraphrase
pairs; the test set contains 1147 and 578 pairs, re-
spectively.
The TF-KLD weights are constructed from only
the training set, while matrix factorizations are per-
</bodyText>
<figure confidence="0.999701444444444">
1.0 neither
off
fear
but
0.8
then
not
0.6
nor
shares
0.4
0.2
0.0
0.0 0.2 0.4 0.6 0.8 1.0
pk
study
same
1−qk
</figure>
<page confidence="0.998272">
893
</page>
<bodyText confidence="0.999924526315789">
formed on the entire corpus. Matrix factorization on
both training and (unlabeled) test data can be viewed
as a form of transductive learning (Gammerman et
al., 1998), where we assume access to unlabeled test
set instances.2 We also consider an inductive setting,
where we construct the basis of the latent space from
only the training set, and then project the test set
onto this basis to find the corresponding latent rep-
resentation. The performance differences between
the transductive and inductive settings were gener-
ally between 0.5% and 1%, as noted in detail be-
low. We reiterate that the TF-KLD weights are never
computed from test set data.
Prior work on this dataset is described in sec-
tion 2. To our knowledge, the current state-of-the-
art is a supervised system that combines several ma-
chine translation metrics (Madnani et al., 2012), but
we also compare with state-of-the-art unsupervised
matrix factorization work (Guo and Diab, 2012).
</bodyText>
<subsectionHeader confidence="0.955623">
5.1 Similarity-based classification
</subsectionHeader>
<bodyText confidence="0.999982391304348">
In the first experiment, we predict whether a pair
of sentences is a paraphrase by measuring their co-
sine similarity in latent space, using a threshold for
the classification boundary. As in prior work (Guo
and Diab, 2012), the threshold is tuned on held-out
training data. We consider two distributional feature
sets: FEAT1, which includes unigrams; and FEAT2,
which also includes bigrams and unlabeled depen-
dency pairs obtained from MaltParser (Nivre et al.,
2007). To compare with Guo and Diab (2012), we
set the latent dimensionality to K = 100, which was
the same in their paper. Both SVD and NMF factor-
ization are evaluated; in both cases, we minimize the
Frobenius norm of the reconstruction error.
Table 2 compares the accuracy of a num-
ber of different configurations. The transductive
TF-KLD weighting yields the best overall accu-
racy, achieving 72.75% when combined with non-
negative matrix factorization. While NMF performs
slightly better than SVD in both comparisons, the
major difference is the performance of discrimina-
tive TF-KLD weighting, which outperforms TF-IDF
regardless of the factorization technique. When we
</bodyText>
<footnote confidence="0.449086">
2Another example of transductive learning in NLP is
when Turian et al. (2010) induced word representations from a
corpus that included both training and test data for their down-
stream named entity recognition task.
</footnote>
<figureCaption confidence="0.998482">
Figure 2: Accuracy of feature and weighting combina-
tions in the classification framework.
</figureCaption>
<bodyText confidence="0.940736333333333">
perform the matrix factorization on only the training
data, the accuracy on the test set is 73.58%, with F1
score 80.55%.
</bodyText>
<subsectionHeader confidence="0.999159">
5.2 Supervised classification
</subsectionHeader>
<bodyText confidence="0.999935615384616">
Next, we apply supervised classification, construct-
ing sample vectors from the latent representation as
shown in Equation 1. For classification, we choose
a Support Vector Machine with a linear kernel (Fan
et al., 2008), leaving a thorough comparison of clas-
sifiers for future work. The classifier parameter C is
tuned on a development set comprising 20% of the
original training set.
Figure 2 presents results for a range of latent di-
mensionalities. Supervised learning identifies the
important dimensions in the latent space, yielding
significantly better performance that the similarity-
based classification from the previous experiment.
In Table 3, we compare against prior published
work, using the held-out development set to select
the best value of K (again, K = 400). The best
result is from TF-KLD, with distributional features
FEAT2, achieving 79.76% accuracy and 85.87% F1.
This is well beyond all known prior results on this
task. When we induce the latent basis from only
the training data, we get 78.55% on accuracy and
84.59% F1, also better than the previous state-of-art.
Finally, we augment the distributional represen-
tation, concatenating the ten “fine-grained” fea-
tures in Table 1 to the sample vectors described
in Equation 1. As shown in Table 3, the accu-
</bodyText>
<figure confidence="0.991763">
50 100 150 200 250 300 350 400
K
80
60
Feat,_TF-IDF_SVM
Feats_TF-IDF_SVM
Feat,_TF-KLD_SVM
Feats_TF-KLD_SVM
75
70
Accuracy (%)
</figure>
<page confidence="0.7680875">
65
894
</page>
<table confidence="0.9998815">
Factorization Feature set Weighting K Measure Accuracy (%) F1
SVD unigrams TF-IDF 100 cosine sim. 68.92 80.33
NMF unigrams TF-IDF 100 cosine sim. 68.96 80.14
WTMF unigrams TF-IDF 100 cosine sim. 71.51 not reported
SVD unigrams TF-KLD 100 cosine sim. 72.23 81.19
NMF unigrams TF-KLD 100 cosine sim. 72.75 81.48
</table>
<tableCaption confidence="0.9621775">
Table 2: Similarity-based paraphrase identification accuracy. Results for WTMF are reprinted from the paper by Guo
and Diab (2012).
</tableCaption>
<table confidence="0.986527">
Acc. F1
Most common class 66.5 79.9
(Wan et al., 2006) 75.6 83.0
(Das and Smith, 2009) 73.9 82.3
(Das and Smith, 2009) with 18 features 76.1 82.7
(Bu et al., 2012) 76.3 not reported
(Socher et al., 2011) 76.8 83.6
(Madnani et al., 2012) 77.4 84.1
FEAT2, TF-KLD, SVM 79.76 85.87
FEAT2, TF-KLD, SVM, Fine-grained features 80.41 85.96
</table>
<tableCaption confidence="0.999953">
Table 3: Supervised classification. Results from prior work are reprinted.
</tableCaption>
<bodyText confidence="0.9999845">
racy now improves to 80.41%, with an F1 score of
85.96%. When the latent representation is induced
from only the training data, the corresponding re-
sults are 79.94% on accuracy and 85.36% F1, again
better than the previous state-of-the-art. These re-
sults show that the information captured by the dis-
tributional representation can still be augmented by
more fine-grained traditional features.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999914933333333">
We have presented three ways in which labeled
data can improve distributional measures of seman-
tic similarity at the sentence level. The main innova-
tion is TF-KLD, which discriminatively reweights
the distributional features before factorization, so
that discriminability impacts the induction of the la-
tent representation. We then transform the latent
representation into a sample vector for supervised
learning, obtaining results that strongly outperform
the prior state-of-the-art; adding fine-grained lexi-
cal features further increases performance. These
ideas may have applicability in other semantic sim-
ilarity tasks, and we are also eager to apply them to
new, large-scale automatically-induced paraphrase
corpora (Ganitkevitch et al., 2013).
</bodyText>
<sectionHeader confidence="0.997476" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998698">
We thank the reviewers for their helpful feedback,
and Weiwei Guo for quickly answering questions
about his implementation. This research was sup-
ported by a Google Faculty Research Award to the
second author.
</bodyText>
<sectionHeader confidence="0.998571" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994905277777778">
Sanjeev Arora, Rong Ge, and Ankur Moitra. 2012.
Learning Topic Models - Going beyond SVD. In
FOCS, pages 1–10.
Rahul Bhagat and Eduard Hovy. 2013. What Is a Para-
phrase? Computational Linguistics.
William Blacoe and Mirella Lapata. 2012. A Com-
parison of Vector-based Representations for Seman-
tic Composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 546–556, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Fan Bu, Hang Li, and Xiaoyan Zhu. 2012. String Re-
writing kernel. In Proceedings of ACL, pages 449–
458. Association for Computational Linguistics.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the Joint Conference
</reference>
<page confidence="0.99487">
895
</page>
<reference confidence="0.9995905375">
of the Annual Meeting of the Association for Com-
putational Linguistics and the International Joint
Conference on Natural Language Processing, pages
468–476, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised Construction of Large Paraphrase Corpora:
Exploiting Massively Parallel News Sources. In COL-
ING.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Li-
brary for Large Linear Classification. Journal of Ma-
chine Learning Research, 9:1871–1874.
Alexander Gammerman, Volodya Vovk, and Vladimir
Vapnik. 1998. Learning by transduction. In Proceed-
ings of the Fourteenth conference on Uncertainty in
artificial intelligence, pages 148–155. Morgan Kauf-
mann Publishers Inc.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of NAACL, pages 758–764.
Association for Computational Linguistics.
Weiwei Guo and Mona Diab. 2012. Modeling Sentences
in the Latent Space. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics, pages 864–872, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
of NAACL, pages 455–462. Association for Computa-
tional Linguistics.
Thomas Landauer, Peter W. Foltz, and Darrel Laham.
1998. Introduction to Latent Semantic Analysis. Dis-
cource Processes, 25:259–284.
Daniel D. Lee and H. Sebastian Seung. 2001. Al-
gorithms for Non-Negative Matrix Factorization. In
Advances in Neural Information Processing Systems
(NIPS).
Nitin Madnani, Joel R. Tetreault, and Martin Chodorow.
2012. Re-examining Machine Translation Metrics for
Paraphrase Identification. In HLT-NAACL, pages 182–
190. The Association for Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In AAAI.
Kevin P. Murphy. 2012. Machine Learning: A Proba-
bilistic Perspective. The MIT Press.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95–135.
Saˇsa Petrovi´c, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with application
to twitter. In Proceedings of HLT-NAACL, pages 181–
189. Association for Computational Linguistics.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011.
Dynamic Pooling And Unfolding Recursive Autoen-
coders For Paraphrase Detection. In Advances in Neu-
ral Information Processing Systems (NIPS).
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word Representation: A Simple and General Method
for Semi-Supervised Learning. In ACL, pages 384–
394.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Seman-
tics. JAIR, 37:141–188.
Ssephen Wan, Mark Dras, Robert Dale, and Cecile Paris.
2006. Using Dependency-based Features to Take the
“Para-farce” out of Paraphrase. In Proceedings of the
Australasian Language Technology Workshop.
Dekai Wu. 2005. Recognizing paraphrases and textual
entailment using inversion transduction grammars. In
Proceedings of the ACL Workshop on Empirical Mod-
eling of Semantic Equivalence and Entailment, pages
25–30. Association for Computational Linguistics.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Document
Clustering based on Non-Negative Matrix Factoriza-
tion. In SIGIR, pages 267–273.
</reference>
<page confidence="0.999012">
896
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.904924">
<title confidence="0.996787">Discriminative Improvements to Distributional Sentence Similarity</title>
<author confidence="0.937342">Yangfeng</author>
<affiliation confidence="0.9990665">School of Interactive Georgia Institute of</affiliation>
<email confidence="0.998058">jiyfeng@gatech.edu</email>
<author confidence="0.985238">Jacob</author>
<affiliation confidence="0.999409">School of Interactive Georgia Institute of</affiliation>
<email confidence="0.999018">jacobe@gatech.edu</email>
<abstract confidence="0.999224277777778">Matrix and tensor factorization have been applied to a number of semantic relatedness tasks, including paraphrase identification. The key idea is that similarity in the latent space implies semantic relatedness. We describe three ways in which labeled data can improve the accuracy of these approaches on paraphrase classification. First, we design a new discriminative term-weighting metric which outperforms TF-IDF. Next, we show that using the latent representation from matrix factorization as features in a classification algorithm substantially improves accuracy. Finally, we combine latent features with fine-grained n-gram overlap features, yielding performance that is 3% more accurate than the prior state-of-the-art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sanjeev Arora</author>
<author>Rong Ge</author>
<author>Ankur Moitra</author>
</authors>
<title>Learning Topic Models - Going beyond SVD. In</title>
<date>2012</date>
<booktitle>FOCS,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="5856" citStr="Arora et al., 2012" startWordPosition="877" endWordPosition="880">RM N, where M is the number of instances and N is the number of distributional features. For paraphrase identification, each instance is a sentence; features may be unigrams, or may include higher-order n-grams or dependency pairs. By decomposing the matrix W, we hope to obtain a latent representation in which semantically-related sentences are similar. Singular value decomposition (SVD) is traditionally used to perform this factorization. However, recent work has demonstrated the robustness of nonnegative matrix factorization (NMF; Lee and Seung, 2001) for text mining tasks (Xu et al., 2003; Arora et al., 2012); the difference from SVD is the addition of a non-negativity constraint in the latent representation based on non-orthogonal basis. While W may simply contain counts of distributional features, prior work has demonstrated the utility of reweighting these counts (Turney and Pantel, 2010). TF-IDF is a standard approach, as the inverse document frequency (IDF) term increases the importance of rare words, which may be more discriminative. Guo and Diab (2012) show that applying a special weight to unseen words can further improvement performance on paraphrase identification. We present a new weigh</context>
</contexts>
<marker>Arora, Ge, Moitra, 2012</marker>
<rawString>Sanjeev Arora, Rong Ge, and Ankur Moitra. 2012. Learning Topic Models - Going beyond SVD. In FOCS, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Eduard Hovy</author>
</authors>
<title>What Is a Paraphrase? Computational Linguistics.</title>
<date>2013</date>
<contexts>
<context position="1420" citStr="Bhagat and Hovy, 2013" startWordPosition="195" endWordPosition="198">assification algorithm substantially improves accuracy. Finally, we combine latent features with fine-grained n-gram overlap features, yielding performance that is 3% more accurate than the prior state-of-the-art. 1 Introduction Measuring the semantic similarity of short units of text is fundamental to many natural language processing tasks, from evaluating machine translation (Kauchak and Barzilay, 2006) to grouping redundant event mentions in social media (Petrovi´c et al., 2010). The task is challenging because of the infinitely diverse set of possible linguistic realizations for any idea (Bhagat and Hovy, 2013), and because of the short length of individual sentences, which means that standard bag-of-words representations will be hopelessly sparse. Distributional methods address this problem by transforming the high-dimensional bag-of-words representation into a lower-dimensional latent space. This can be accomplished by factoring a matrix or tensor of term-context counts (Turney and Pantel, 2010); proximity in the induced latent space has been shown to correlate with semantic similarity (Mihalcea et al., 2006). However, factoring the term-context matrix means throwing away a considerable amount of </context>
</contexts>
<marker>Bhagat, Hovy, 2013</marker>
<rawString>Rahul Bhagat and Eduard Hovy. 2013. What Is a Paraphrase? Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A Comparison of Vector-based Representations for Semantic Composition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>546--556</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4199" citStr="Blacoe and Lapata (2012)" startWordPosition="619" endWordPosition="622"> on paraphrase identification, we note three high-level approaches: (1) string similarity metrics such as n-gram overlap and BLEU score (Wan et al., 2006; Madnani et al., 2012), as well as string kernels (Bu et al., 2012); (2) syntactic operations on the parse structure (Wu, 2005; Das and Smith, 2009); and (3) distributional methods, such as latent semantic analysis (LSA; Landauer et al., 1998), which are most relevant to our work. One application of distributional techniques is to replace individual words with distributionally similar alternatives (Kauchak and Barzilay, 2006). Alternatively, Blacoe and Lapata (2012) show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of larger units of text. Socher et al. (2011) propose a syntactically-informed approach to combine word representations, using a recursive auto-encoder to propagate meaning through the parse tree. We take a different approach: rather than representing the meanings of individual words, we directly obtain a distributional representation for the entire sentence. This is inspired by Mihalcea et al. (2006) and Guo and Diab (2012), who treat sentences as pseudo-documents in a</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A Comparison of Vector-based Representations for Semantic Composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546–556, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fan Bu</author>
<author>Hang Li</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>String Rewriting kernel.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>449--458</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3796" citStr="Bu et al., 2012" startWordPosition="558" endWordPosition="561">mprovements in accuracy, demonstrating that these feature sets provide complementary information on semantic similarity. 891 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891–896, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Without attempting to do justice to the entire literature on paraphrase identification, we note three high-level approaches: (1) string similarity metrics such as n-gram overlap and BLEU score (Wan et al., 2006; Madnani et al., 2012), as well as string kernels (Bu et al., 2012); (2) syntactic operations on the parse structure (Wu, 2005; Das and Smith, 2009); and (3) distributional methods, such as latent semantic analysis (LSA; Landauer et al., 1998), which are most relevant to our work. One application of distributional techniques is to replace individual words with distributionally similar alternatives (Kauchak and Barzilay, 2006). Alternatively, Blacoe and Lapata (2012) show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of larger units of text. Socher et al. (2011) propose a syntactically-i</context>
<context position="15708" citStr="Bu et al., 2012" startWordPosition="2482" endWordPosition="2485">(%) 65 894 Factorization Feature set Weighting K Measure Accuracy (%) F1 SVD unigrams TF-IDF 100 cosine sim. 68.92 80.33 NMF unigrams TF-IDF 100 cosine sim. 68.96 80.14 WTMF unigrams TF-IDF 100 cosine sim. 71.51 not reported SVD unigrams TF-KLD 100 cosine sim. 72.23 81.19 NMF unigrams TF-KLD 100 cosine sim. 72.75 81.48 Table 2: Similarity-based paraphrase identification accuracy. Results for WTMF are reprinted from the paper by Guo and Diab (2012). Acc. F1 Most common class 66.5 79.9 (Wan et al., 2006) 75.6 83.0 (Das and Smith, 2009) 73.9 82.3 (Das and Smith, 2009) with 18 features 76.1 82.7 (Bu et al., 2012) 76.3 not reported (Socher et al., 2011) 76.8 83.6 (Madnani et al., 2012) 77.4 84.1 FEAT2, TF-KLD, SVM 79.76 85.87 FEAT2, TF-KLD, SVM, Fine-grained features 80.41 85.96 Table 3: Supervised classification. Results from prior work are reprinted. racy now improves to 80.41%, with an F1 score of 85.96%. When the latent representation is induced from only the training data, the corresponding results are 79.94% on accuracy and 85.36% F1, again better than the previous state-of-the-art. These results show that the information captured by the distributional representation can still be augmented by mor</context>
</contexts>
<marker>Bu, Li, Zhu, 2012</marker>
<rawString>Fan Bu, Hang Li, and Xiaoyan Zhu. 2012. String Rewriting kernel. In Proceedings of ACL, pages 449– 458. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasi-synchronous recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing,</booktitle>
<pages>468--476</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3877" citStr="Das and Smith, 2009" startWordPosition="571" endWordPosition="574">mentary information on semantic similarity. 891 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891–896, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Without attempting to do justice to the entire literature on paraphrase identification, we note three high-level approaches: (1) string similarity metrics such as n-gram overlap and BLEU score (Wan et al., 2006; Madnani et al., 2012), as well as string kernels (Bu et al., 2012); (2) syntactic operations on the parse structure (Wu, 2005; Das and Smith, 2009); and (3) distributional methods, such as latent semantic analysis (LSA; Landauer et al., 1998), which are most relevant to our work. One application of distributional techniques is to replace individual words with distributionally similar alternatives (Kauchak and Barzilay, 2006). Alternatively, Blacoe and Lapata (2012) show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of larger units of text. Socher et al. (2011) propose a syntactically-informed approach to combine word representations, using a recursive auto-encoder </context>
<context position="15631" citStr="Das and Smith, 2009" startWordPosition="2467" endWordPosition="2470">at,_TF-IDF_SVM Feats_TF-IDF_SVM Feat,_TF-KLD_SVM Feats_TF-KLD_SVM 75 70 Accuracy (%) 65 894 Factorization Feature set Weighting K Measure Accuracy (%) F1 SVD unigrams TF-IDF 100 cosine sim. 68.92 80.33 NMF unigrams TF-IDF 100 cosine sim. 68.96 80.14 WTMF unigrams TF-IDF 100 cosine sim. 71.51 not reported SVD unigrams TF-KLD 100 cosine sim. 72.23 81.19 NMF unigrams TF-KLD 100 cosine sim. 72.75 81.48 Table 2: Similarity-based paraphrase identification accuracy. Results for WTMF are reprinted from the paper by Guo and Diab (2012). Acc. F1 Most common class 66.5 79.9 (Wan et al., 2006) 75.6 83.0 (Das and Smith, 2009) 73.9 82.3 (Das and Smith, 2009) with 18 features 76.1 82.7 (Bu et al., 2012) 76.3 not reported (Socher et al., 2011) 76.8 83.6 (Madnani et al., 2012) 77.4 84.1 FEAT2, TF-KLD, SVM 79.76 85.87 FEAT2, TF-KLD, SVM, Fine-grained features 80.41 85.96 Table 3: Supervised classification. Results from prior work are reprinted. racy now improves to 80.41%, with an F1 score of 85.96%. When the latent representation is induced from only the training data, the corresponding results are 79.94% on accuracy and 85.36% F1, again better than the previous state-of-the-art. These results show that the informatio</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>Dipanjan Das and Noah A. Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. In Proceedings of the Joint Conference of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing, pages 468–476, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources.</title>
<date>2004</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="2645" citStr="Dolan et al., 2004" startWordPosition="386" endWordPosition="389">ation, as the original matrix of size M x N (number of instances by number of features) is factored into two smaller matrices of size M x K and N x K, with K « M, N. If the factorization does not take into account labeled data about semantic similarity, important information can be lost. In this paper, we show how labeled data can considerably improve distributional methods for measuring semantic similarity. First, we develop a new discriminative term-weighting metric called TF-KLD, which is applied to the term-context matrix before factorization. On a standard paraphrase identification task (Dolan et al., 2004), this method improves on both traditional TF-IDF and Weighted Textual Matrix Factorization (WTMF; Guo and Diab, 2012). Next, we convert the latent representations of each sentence pair into a feature vector, which is used as input to a linear SVM classifier. This yields further improvements and substantially outperforms the current state-of-the-art on paraphrase classification. We then add “finegrained” features about the lexical similarity of the sentence pair. The combination of latent and finegrained features yields further improvements in accuracy, demonstrating that these feature sets pr</context>
<context position="8036" citStr="Dolan et al., 2004" startWordPosition="1249" endWordPosition="1252">entences are labeled as paraphrases, ri = 1. • qk = P(w(1) ik |w(2) ik = 1, ri = 0). This is the probability that sentence w(1) i contains feature k, given that k appears in w(2) i and the two sentences are labeled as not paraphrases, ri = 0. The Kullback-Leibler divergence KL(pk||qk) = � xpk(x) log pk(x) qk(x) is then a measure of the discriminability of feature k, and is guaranteed to be non892 Figure 1: Conditional probabilities for a few handselected unigram features, with lines showing contours with identical KL-divergence. The probabilities are estimated based on the MSRPC training set (Dolan et al., 2004). negative.1 We use this divergence to reweight the features in W before performing the matrix factorization. This has the effect of increasing the weights of features whose likelihood of appearing in a pair of sentences is strongly influenced by the paraphrase relationship between the two sentences. On the other hand, if pk = qk, then the KL-divergence will be zero, and the feature will be ignored in the matrix factorization. We name this weighting scheme TF-KLD, since it includes the term frequency and the KL-divergence. Taking the unigram feature not as an example, we have pk = [0.66, 0.34]</context>
<context position="10764" citStr="Dolan et al., 2004" startWordPosition="1687" endWordPosition="1690">s representation, we can use any supervised classification algorithm. A further advantage of treating paraphrase as a supervised classification problem is that we can apply additional features besides the latent representation. We consider a subset of features identified by Wan et al. (2006), listed in Table 1. These features mainly capture fine-grained similarity between sentences, for example by counting specific unigram and bigram overlap. 5 Experiments Our experiments test the utility of the TFKLD weighting towards paraphrase classification, using the Microsoft Research Paraphrase Corpus (Dolan et al., 2004). The training set contains 2753 true paraphrase pairs and 1323 false paraphrase pairs; the test set contains 1147 and 578 pairs, respectively. The TF-KLD weights are constructed from only the training set, while matrix factorizations are per1.0 neither off fear but 0.8 then not 0.6 nor shares 0.4 0.2 0.0 0.0 0.2 0.4 0.6 0.8 1.0 pk study same 1−qk 893 formed on the entire corpus. Matrix factorization on both training and (unlabeled) test data can be viewed as a form of transductive learning (Gammerman et al., 1998), where we assume access to unlabeled test set instances.2 We also consider an i</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A Library for Large Linear Classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="13912" citStr="Fan et al., 2008" startWordPosition="2191" endWordPosition="2194">rian et al. (2010) induced word representations from a corpus that included both training and test data for their downstream named entity recognition task. Figure 2: Accuracy of feature and weighting combinations in the classification framework. perform the matrix factorization on only the training data, the accuracy on the test set is 73.58%, with F1 score 80.55%. 5.2 Supervised classification Next, we apply supervised classification, constructing sample vectors from the latent representation as shown in Equation 1. For classification, we choose a Support Vector Machine with a linear kernel (Fan et al., 2008), leaving a thorough comparison of classifiers for future work. The classifier parameter C is tuned on a development set comprising 20% of the original training set. Figure 2 presents results for a range of latent dimensionalities. Supervised learning identifies the important dimensions in the latent space, yielding significantly better performance that the similaritybased classification from the previous experiment. In Table 3, we compare against prior published work, using the held-out development set to select the best value of K (again, K = 400). The best result is from TF-KLD, with distri</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Gammerman</author>
<author>Volodya Vovk</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Learning by transduction.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence,</booktitle>
<pages>148--155</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="11284" citStr="Gammerman et al., 1998" startWordPosition="1777" endWordPosition="1780"> towards paraphrase classification, using the Microsoft Research Paraphrase Corpus (Dolan et al., 2004). The training set contains 2753 true paraphrase pairs and 1323 false paraphrase pairs; the test set contains 1147 and 578 pairs, respectively. The TF-KLD weights are constructed from only the training set, while matrix factorizations are per1.0 neither off fear but 0.8 then not 0.6 nor shares 0.4 0.2 0.0 0.0 0.2 0.4 0.6 0.8 1.0 pk study same 1−qk 893 formed on the entire corpus. Matrix factorization on both training and (unlabeled) test data can be viewed as a form of transductive learning (Gammerman et al., 1998), where we assume access to unlabeled test set instances.2 We also consider an inductive setting, where we construct the basis of the latent space from only the training set, and then project the test set onto this basis to find the corresponding latent representation. The performance differences between the transductive and inductive settings were generally between 0.5% and 1%, as noted in detail below. We reiterate that the TF-KLD weights are never computed from test set data. Prior work on this dataset is described in section 2. To our knowledge, the current state-of-theart is a supervised </context>
</contexts>
<marker>Gammerman, Vovk, Vapnik, 1998</marker>
<rawString>Alexander Gammerman, Volodya Vovk, and Vladimir Vapnik. 1998. Learning by transduction. In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence, pages 148–155. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB: The Paraphrase Database.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>758--764</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The Paraphrase Database. In Proceedings of NAACL, pages 758–764. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Modeling Sentences in the Latent Space.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>864--872</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2763" citStr="Guo and Diab, 2012" startWordPosition="403" endWordPosition="406">atrices of size M x K and N x K, with K « M, N. If the factorization does not take into account labeled data about semantic similarity, important information can be lost. In this paper, we show how labeled data can considerably improve distributional methods for measuring semantic similarity. First, we develop a new discriminative term-weighting metric called TF-KLD, which is applied to the term-context matrix before factorization. On a standard paraphrase identification task (Dolan et al., 2004), this method improves on both traditional TF-IDF and Weighted Textual Matrix Factorization (WTMF; Guo and Diab, 2012). Next, we convert the latent representations of each sentence pair into a feature vector, which is used as input to a linear SVM classifier. This yields further improvements and substantially outperforms the current state-of-the-art on paraphrase classification. We then add “finegrained” features about the lexical similarity of the sentence pair. The combination of latent and finegrained features yields further improvements in accuracy, demonstrating that these feature sets provide complementary information on semantic similarity. 891 Proceedings of the 2013 Conference on Empirical Methods in</context>
<context position="4753" citStr="Guo and Diab (2012)" startWordPosition="705" endWordPosition="708">chak and Barzilay, 2006). Alternatively, Blacoe and Lapata (2012) show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of larger units of text. Socher et al. (2011) propose a syntactically-informed approach to combine word representations, using a recursive auto-encoder to propagate meaning through the parse tree. We take a different approach: rather than representing the meanings of individual words, we directly obtain a distributional representation for the entire sentence. This is inspired by Mihalcea et al. (2006) and Guo and Diab (2012), who treat sentences as pseudo-documents in an LSA framework, and identify paraphrases using similarity in the latent space. We show that the performance of such techniques can be improved dramatically by using supervised information to (1) reweight the individual distributional features and (2) learn the importance of each latent dimension. 3 Discriminative feature weighting Distributional representations (Turney and Pantel, 2010) can be induced from a co-occurrence matrix W E RM N, where M is the number of instances and N is the number of distributional features. For paraphrase identificati</context>
<context position="6315" citStr="Guo and Diab (2012)" startWordPosition="950" endWordPosition="953"> work has demonstrated the robustness of nonnegative matrix factorization (NMF; Lee and Seung, 2001) for text mining tasks (Xu et al., 2003; Arora et al., 2012); the difference from SVD is the addition of a non-negativity constraint in the latent representation based on non-orthogonal basis. While W may simply contain counts of distributional features, prior work has demonstrated the utility of reweighting these counts (Turney and Pantel, 2010). TF-IDF is a standard approach, as the inverse document frequency (IDF) term increases the importance of rare words, which may be more discriminative. Guo and Diab (2012) show that applying a special weight to unseen words can further improvement performance on paraphrase identification. We present a new weighting scheme, TF-KLD, based on supervised information. The key idea is to increase the weights of distributional features that are discriminative, and to decrease the weights of features that are not. Conceptually, this is similar to Linear Discriminant Analysis, a supervised feature weighting scheme for continuous data (Murphy, 2012). More formally, we assume labeled sentence pairs of the form (~w(1) i , ~w(2) i , ri), where ~w(1) i is the binarized vecto</context>
<context position="9774" citStr="Guo and Diab, 2012" startWordPosition="1531" endWordPosition="1534">obtain very similar results with the opposite divergence KL(qk||N). However, the symmetric Jensen-Shannon divergence performs poorly. 1 unigram recall 2 unigram precision 3 bigram recall 4 bigram precision 5 dependency relation recall 6 dependency relation precision 7 BLEU recall 8 BLEU precision 9 Difference of sentence length 10 Tree-editing distance Table 1: Fine-grained features for paraphrase classification, selected from prior work (Wan et al., 2006). 4 Supervised classification While previous work has performed paraphrase classification using distance or similarity in the latent space (Guo and Diab, 2012; Socher et al., 2011), more direct supervision can be applied. Specifically, we convert the latent representations of a pair of sentences ~v1 and ~v2 into a sample vector, ~s(~v1,~v2) = [~v1 + ~v2, |~v1 − ~v2 |(1) concatenating the element-wise sum ~v1 +~v2 and absolute difference |~v1 − ~v2|. Note that ~s(·, ·) is symmetric, since ~s(~v1,~v2) = ~s(~v2,~v1). Given this representation, we can use any supervised classification algorithm. A further advantage of treating paraphrase as a supervised classification problem is that we can apply additional features besides the latent representation. W</context>
<context position="12066" citStr="Guo and Diab, 2012" startWordPosition="1904" endWordPosition="1907">aining set, and then project the test set onto this basis to find the corresponding latent representation. The performance differences between the transductive and inductive settings were generally between 0.5% and 1%, as noted in detail below. We reiterate that the TF-KLD weights are never computed from test set data. Prior work on this dataset is described in section 2. To our knowledge, the current state-of-theart is a supervised system that combines several machine translation metrics (Madnani et al., 2012), but we also compare with state-of-the-art unsupervised matrix factorization work (Guo and Diab, 2012). 5.1 Similarity-based classification In the first experiment, we predict whether a pair of sentences is a paraphrase by measuring their cosine similarity in latent space, using a threshold for the classification boundary. As in prior work (Guo and Diab, 2012), the threshold is tuned on held-out training data. We consider two distributional feature sets: FEAT1, which includes unigrams; and FEAT2, which also includes bigrams and unlabeled dependency pairs obtained from MaltParser (Nivre et al., 2007). To compare with Guo and Diab (2012), we set the latent dimensionality to K = 100, which was th</context>
<context position="15543" citStr="Guo and Diab (2012)" startWordPosition="2450" endWordPosition="2453">d in Equation 1. As shown in Table 3, the accu50 100 150 200 250 300 350 400 K 80 60 Feat,_TF-IDF_SVM Feats_TF-IDF_SVM Feat,_TF-KLD_SVM Feats_TF-KLD_SVM 75 70 Accuracy (%) 65 894 Factorization Feature set Weighting K Measure Accuracy (%) F1 SVD unigrams TF-IDF 100 cosine sim. 68.92 80.33 NMF unigrams TF-IDF 100 cosine sim. 68.96 80.14 WTMF unigrams TF-IDF 100 cosine sim. 71.51 not reported SVD unigrams TF-KLD 100 cosine sim. 72.23 81.19 NMF unigrams TF-KLD 100 cosine sim. 72.75 81.48 Table 2: Similarity-based paraphrase identification accuracy. Results for WTMF are reprinted from the paper by Guo and Diab (2012). Acc. F1 Most common class 66.5 79.9 (Wan et al., 2006) 75.6 83.0 (Das and Smith, 2009) 73.9 82.3 (Das and Smith, 2009) with 18 features 76.1 82.7 (Bu et al., 2012) 76.3 not reported (Socher et al., 2011) 76.8 83.6 (Madnani et al., 2012) 77.4 84.1 FEAT2, TF-KLD, SVM 79.76 85.87 FEAT2, TF-KLD, SVM, Fine-grained features 80.41 85.96 Table 3: Supervised classification. Results from prior work are reprinted. racy now improves to 80.41%, with an F1 score of 85.96%. When the latent representation is induced from only the training data, the corresponding results are 79.94% on accuracy and 85.36% F1,</context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012. Modeling Sentences in the Latent Space. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 864–872, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>455--462</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1206" citStr="Kauchak and Barzilay, 2006" startWordPosition="160" endWordPosition="163">hrase classification. First, we design a new discriminative term-weighting metric called TF-KLD, which outperforms TF-IDF. Next, we show that using the latent representation from matrix factorization as features in a classification algorithm substantially improves accuracy. Finally, we combine latent features with fine-grained n-gram overlap features, yielding performance that is 3% more accurate than the prior state-of-the-art. 1 Introduction Measuring the semantic similarity of short units of text is fundamental to many natural language processing tasks, from evaluating machine translation (Kauchak and Barzilay, 2006) to grouping redundant event mentions in social media (Petrovi´c et al., 2010). The task is challenging because of the infinitely diverse set of possible linguistic realizations for any idea (Bhagat and Hovy, 2013), and because of the short length of individual sentences, which means that standard bag-of-words representations will be hopelessly sparse. Distributional methods address this problem by transforming the high-dimensional bag-of-words representation into a lower-dimensional latent space. This can be accomplished by factoring a matrix or tensor of term-context counts (Turney and Pante</context>
<context position="4158" citStr="Kauchak and Barzilay, 2006" startWordPosition="614" endWordPosition="617">pting to do justice to the entire literature on paraphrase identification, we note three high-level approaches: (1) string similarity metrics such as n-gram overlap and BLEU score (Wan et al., 2006; Madnani et al., 2012), as well as string kernels (Bu et al., 2012); (2) syntactic operations on the parse structure (Wu, 2005; Das and Smith, 2009); and (3) distributional methods, such as latent semantic analysis (LSA; Landauer et al., 1998), which are most relevant to our work. One application of distributional techniques is to replace individual words with distributionally similar alternatives (Kauchak and Barzilay, 2006). Alternatively, Blacoe and Lapata (2012) show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of larger units of text. Socher et al. (2011) propose a syntactically-informed approach to combine word representations, using a recursive auto-encoder to propagate meaning through the parse tree. We take a different approach: rather than representing the meanings of individual words, we directly obtain a distributional representation for the entire sentence. This is inspired by Mihalcea et al. (2006) and Guo and Diab (2012), who</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for automatic evaluation. In Proceedings of NAACL, pages 455–462. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Peter W Foltz</author>
<author>Darrel Laham</author>
</authors>
<title>Introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discource Processes,</booktitle>
<pages>25--259</pages>
<contexts>
<context position="3972" citStr="Landauer et al., 1998" startWordPosition="586" endWordPosition="589">al Methods in Natural Language Processing, pages 891–896, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Without attempting to do justice to the entire literature on paraphrase identification, we note three high-level approaches: (1) string similarity metrics such as n-gram overlap and BLEU score (Wan et al., 2006; Madnani et al., 2012), as well as string kernels (Bu et al., 2012); (2) syntactic operations on the parse structure (Wu, 2005; Das and Smith, 2009); and (3) distributional methods, such as latent semantic analysis (LSA; Landauer et al., 1998), which are most relevant to our work. One application of distributional techniques is to replace individual words with distributionally similar alternatives (Kauchak and Barzilay, 2006). Alternatively, Blacoe and Lapata (2012) show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of larger units of text. Socher et al. (2011) propose a syntactically-informed approach to combine word representations, using a recursive auto-encoder to propagate meaning through the parse tree. We take a different approach: rather than represen</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas Landauer, Peter W. Foltz, and Darrel Laham. 1998. Introduction to Latent Semantic Analysis. Discource Processes, 25:259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Lee</author>
<author>H Sebastian Seung</author>
</authors>
<title>Algorithms for Non-Negative Matrix Factorization.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="5796" citStr="Lee and Seung, 2001" startWordPosition="865" endWordPosition="868">Pantel, 2010) can be induced from a co-occurrence matrix W E RM N, where M is the number of instances and N is the number of distributional features. For paraphrase identification, each instance is a sentence; features may be unigrams, or may include higher-order n-grams or dependency pairs. By decomposing the matrix W, we hope to obtain a latent representation in which semantically-related sentences are similar. Singular value decomposition (SVD) is traditionally used to perform this factorization. However, recent work has demonstrated the robustness of nonnegative matrix factorization (NMF; Lee and Seung, 2001) for text mining tasks (Xu et al., 2003; Arora et al., 2012); the difference from SVD is the addition of a non-negativity constraint in the latent representation based on non-orthogonal basis. While W may simply contain counts of distributional features, prior work has demonstrated the utility of reweighting these counts (Turney and Pantel, 2010). TF-IDF is a standard approach, as the inverse document frequency (IDF) term increases the importance of rare words, which may be more discriminative. Guo and Diab (2012) show that applying a special weight to unseen words can further improvement perf</context>
</contexts>
<marker>Lee, Seung, 2001</marker>
<rawString>Daniel D. Lee and H. Sebastian Seung. 2001. Algorithms for Non-Negative Matrix Factorization. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel R Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Re-examining Machine Translation Metrics for Paraphrase Identification. In</title>
<date>2012</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>182--190</pages>
<contexts>
<context position="3751" citStr="Madnani et al., 2012" startWordPosition="549" endWordPosition="552">f latent and finegrained features yields further improvements in accuracy, demonstrating that these feature sets provide complementary information on semantic similarity. 891 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891–896, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Without attempting to do justice to the entire literature on paraphrase identification, we note three high-level approaches: (1) string similarity metrics such as n-gram overlap and BLEU score (Wan et al., 2006; Madnani et al., 2012), as well as string kernels (Bu et al., 2012); (2) syntactic operations on the parse structure (Wu, 2005; Das and Smith, 2009); and (3) distributional methods, such as latent semantic analysis (LSA; Landauer et al., 1998), which are most relevant to our work. One application of distributional techniques is to replace individual words with distributionally similar alternatives (Kauchak and Barzilay, 2006). Alternatively, Blacoe and Lapata (2012) show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of larger units of text. S</context>
<context position="11963" citStr="Madnani et al., 2012" startWordPosition="1890" endWordPosition="1893"> We also consider an inductive setting, where we construct the basis of the latent space from only the training set, and then project the test set onto this basis to find the corresponding latent representation. The performance differences between the transductive and inductive settings were generally between 0.5% and 1%, as noted in detail below. We reiterate that the TF-KLD weights are never computed from test set data. Prior work on this dataset is described in section 2. To our knowledge, the current state-of-theart is a supervised system that combines several machine translation metrics (Madnani et al., 2012), but we also compare with state-of-the-art unsupervised matrix factorization work (Guo and Diab, 2012). 5.1 Similarity-based classification In the first experiment, we predict whether a pair of sentences is a paraphrase by measuring their cosine similarity in latent space, using a threshold for the classification boundary. As in prior work (Guo and Diab, 2012), the threshold is tuned on held-out training data. We consider two distributional feature sets: FEAT1, which includes unigrams; and FEAT2, which also includes bigrams and unlabeled dependency pairs obtained from MaltParser (Nivre et al.</context>
<context position="15781" citStr="Madnani et al., 2012" startWordPosition="2495" endWordPosition="2498">) F1 SVD unigrams TF-IDF 100 cosine sim. 68.92 80.33 NMF unigrams TF-IDF 100 cosine sim. 68.96 80.14 WTMF unigrams TF-IDF 100 cosine sim. 71.51 not reported SVD unigrams TF-KLD 100 cosine sim. 72.23 81.19 NMF unigrams TF-KLD 100 cosine sim. 72.75 81.48 Table 2: Similarity-based paraphrase identification accuracy. Results for WTMF are reprinted from the paper by Guo and Diab (2012). Acc. F1 Most common class 66.5 79.9 (Wan et al., 2006) 75.6 83.0 (Das and Smith, 2009) 73.9 82.3 (Das and Smith, 2009) with 18 features 76.1 82.7 (Bu et al., 2012) 76.3 not reported (Socher et al., 2011) 76.8 83.6 (Madnani et al., 2012) 77.4 84.1 FEAT2, TF-KLD, SVM 79.76 85.87 FEAT2, TF-KLD, SVM, Fine-grained features 80.41 85.96 Table 3: Supervised classification. Results from prior work are reprinted. racy now improves to 80.41%, with an F1 score of 85.96%. When the latent representation is induced from only the training data, the corresponding results are 79.94% on accuracy and 85.36% F1, again better than the previous state-of-the-art. These results show that the information captured by the distributional representation can still be augmented by more fine-grained traditional features. 6 Conclusion We have presented three</context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Nitin Madnani, Joel R. Tetreault, and Martin Chodorow. 2012. Re-examining Machine Translation Metrics for Paraphrase Identification. In HLT-NAACL, pages 182– 190. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="1930" citStr="Mihalcea et al., 2006" startWordPosition="268" endWordPosition="271">g because of the infinitely diverse set of possible linguistic realizations for any idea (Bhagat and Hovy, 2013), and because of the short length of individual sentences, which means that standard bag-of-words representations will be hopelessly sparse. Distributional methods address this problem by transforming the high-dimensional bag-of-words representation into a lower-dimensional latent space. This can be accomplished by factoring a matrix or tensor of term-context counts (Turney and Pantel, 2010); proximity in the induced latent space has been shown to correlate with semantic similarity (Mihalcea et al., 2006). However, factoring the term-context matrix means throwing away a considerable amount of information, as the original matrix of size M x N (number of instances by number of features) is factored into two smaller matrices of size M x K and N x K, with K « M, N. If the factorization does not take into account labeled data about semantic similarity, important information can be lost. In this paper, we show how labeled data can considerably improve distributional methods for measuring semantic similarity. First, we develop a new discriminative term-weighting metric called TF-KLD, which is applied</context>
<context position="4729" citStr="Mihalcea et al. (2006)" startWordPosition="700" endWordPosition="703">y similar alternatives (Kauchak and Barzilay, 2006). Alternatively, Blacoe and Lapata (2012) show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of larger units of text. Socher et al. (2011) propose a syntactically-informed approach to combine word representations, using a recursive auto-encoder to propagate meaning through the parse tree. We take a different approach: rather than representing the meanings of individual words, we directly obtain a distributional representation for the entire sentence. This is inspired by Mihalcea et al. (2006) and Guo and Diab (2012), who treat sentences as pseudo-documents in an LSA framework, and identify paraphrases using similarity in the latent space. We show that the performance of such techniques can be improved dramatically by using supervised information to (1) reweight the individual distributional features and (2) learn the importance of each latent dimension. 3 Discriminative feature weighting Distributional representations (Turney and Pantel, 2010) can be induced from a co-occurrence matrix W E RM N, where M is the number of instances and N is the number of distributional features. For</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Murphy</author>
</authors>
<title>Machine Learning: A Probabilistic Perspective.</title>
<date>2012</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="6791" citStr="Murphy, 2012" startWordPosition="1023" endWordPosition="1024">s the inverse document frequency (IDF) term increases the importance of rare words, which may be more discriminative. Guo and Diab (2012) show that applying a special weight to unseen words can further improvement performance on paraphrase identification. We present a new weighting scheme, TF-KLD, based on supervised information. The key idea is to increase the weights of distributional features that are discriminative, and to decrease the weights of features that are not. Conceptually, this is similar to Linear Discriminant Analysis, a supervised feature weighting scheme for continuous data (Murphy, 2012). More formally, we assume labeled sentence pairs of the form (~w(1) i , ~w(2) i , ri), where ~w(1) i is the binarized vector of distributional features for the first sentence, ~w(2) i is the binarized vector of distributional features for the second sentence, and ri E {0,1} indicates whether they are labeled as a paraphrase pair. Assuming the order of the sentences within the pair is irrelevant, then for k-th distributional feature, we define two Bernoulli distributions: • pk = P(w(1) ik |w(2) ik = 1, ri = 1). This is the probability that sentence w(1) i contains feature k, given that k appea</context>
</contexts>
<marker>Murphy, 2012</marker>
<rawString>Kevin P. Murphy. 2012. Machine Learning: A Probabilistic Perspective. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="12570" citStr="Nivre et al., 2007" startWordPosition="1981" endWordPosition="1984">t al., 2012), but we also compare with state-of-the-art unsupervised matrix factorization work (Guo and Diab, 2012). 5.1 Similarity-based classification In the first experiment, we predict whether a pair of sentences is a paraphrase by measuring their cosine similarity in latent space, using a threshold for the classification boundary. As in prior work (Guo and Diab, 2012), the threshold is tuned on held-out training data. We consider two distributional feature sets: FEAT1, which includes unigrams; and FEAT2, which also includes bigrams and unlabeled dependency pairs obtained from MaltParser (Nivre et al., 2007). To compare with Guo and Diab (2012), we set the latent dimensionality to K = 100, which was the same in their paper. Both SVD and NMF factorization are evaluated; in both cases, we minimize the Frobenius norm of the reconstruction error. Table 2 compares the accuracy of a number of different configurations. The transductive TF-KLD weighting yields the best overall accuracy, achieving 72.75% when combined with nonnegative matrix factorization. While NMF performs slightly better than SVD in both comparisons, the major difference is the performance of discriminative TF-KLD weighting, which outp</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. MaltParser: A languageindependent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saˇsa Petrovi´c</author>
<author>Miles Osborne</author>
<author>Victor Lavrenko</author>
</authors>
<title>Streaming first story detection with application to twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>181--189</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Petrovi´c, Osborne, Lavrenko, 2010</marker>
<rawString>Saˇsa Petrovi´c, Miles Osborne, and Victor Lavrenko. 2010. Streaming first story detection with application to twitter. In Proceedings of HLT-NAACL, pages 181– 189. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic Pooling And Unfolding Recursive Autoencoders For Paraphrase Detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="4370" citStr="Socher et al. (2011)" startWordPosition="647" endWordPosition="650">), as well as string kernels (Bu et al., 2012); (2) syntactic operations on the parse structure (Wu, 2005; Das and Smith, 2009); and (3) distributional methods, such as latent semantic analysis (LSA; Landauer et al., 1998), which are most relevant to our work. One application of distributional techniques is to replace individual words with distributionally similar alternatives (Kauchak and Barzilay, 2006). Alternatively, Blacoe and Lapata (2012) show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of larger units of text. Socher et al. (2011) propose a syntactically-informed approach to combine word representations, using a recursive auto-encoder to propagate meaning through the parse tree. We take a different approach: rather than representing the meanings of individual words, we directly obtain a distributional representation for the entire sentence. This is inspired by Mihalcea et al. (2006) and Guo and Diab (2012), who treat sentences as pseudo-documents in an LSA framework, and identify paraphrases using similarity in the latent space. We show that the performance of such techniques can be improved dramatically by using super</context>
<context position="9796" citStr="Socher et al., 2011" startWordPosition="1535" endWordPosition="1538">results with the opposite divergence KL(qk||N). However, the symmetric Jensen-Shannon divergence performs poorly. 1 unigram recall 2 unigram precision 3 bigram recall 4 bigram precision 5 dependency relation recall 6 dependency relation precision 7 BLEU recall 8 BLEU precision 9 Difference of sentence length 10 Tree-editing distance Table 1: Fine-grained features for paraphrase classification, selected from prior work (Wan et al., 2006). 4 Supervised classification While previous work has performed paraphrase classification using distance or similarity in the latent space (Guo and Diab, 2012; Socher et al., 2011), more direct supervision can be applied. Specifically, we convert the latent representations of a pair of sentences ~v1 and ~v2 into a sample vector, ~s(~v1,~v2) = [~v1 + ~v2, |~v1 − ~v2 |(1) concatenating the element-wise sum ~v1 +~v2 and absolute difference |~v1 − ~v2|. Note that ~s(·, ·) is symmetric, since ~s(~v1,~v2) = ~s(~v2,~v1). Given this representation, we can use any supervised classification algorithm. A further advantage of treating paraphrase as a supervised classification problem is that we can apply additional features besides the latent representation. We consider a subset of</context>
<context position="15748" citStr="Socher et al., 2011" startWordPosition="2489" endWordPosition="2492"> Weighting K Measure Accuracy (%) F1 SVD unigrams TF-IDF 100 cosine sim. 68.92 80.33 NMF unigrams TF-IDF 100 cosine sim. 68.96 80.14 WTMF unigrams TF-IDF 100 cosine sim. 71.51 not reported SVD unigrams TF-KLD 100 cosine sim. 72.23 81.19 NMF unigrams TF-KLD 100 cosine sim. 72.75 81.48 Table 2: Similarity-based paraphrase identification accuracy. Results for WTMF are reprinted from the paper by Guo and Diab (2012). Acc. F1 Most common class 66.5 79.9 (Wan et al., 2006) 75.6 83.0 (Das and Smith, 2009) 73.9 82.3 (Das and Smith, 2009) with 18 features 76.1 82.7 (Bu et al., 2012) 76.3 not reported (Socher et al., 2011) 76.8 83.6 (Madnani et al., 2012) 77.4 84.1 FEAT2, TF-KLD, SVM 79.76 85.87 FEAT2, TF-KLD, SVM, Fine-grained features 80.41 85.96 Table 3: Supervised classification. Results from prior work are reprinted. racy now improves to 80.41%, with an F1 score of 85.96%. When the latent representation is induced from only the training data, the corresponding results are 79.94% on accuracy and 85.36% F1, again better than the previous state-of-the-art. These results show that the information captured by the distributional representation can still be augmented by more fine-grained traditional features. 6 C</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic Pooling And Unfolding Recursive Autoencoders For Paraphrase Detection. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word Representation: A Simple and General Method for Semi-Supervised Learning. In</title>
<date>2010</date>
<booktitle>ACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="13313" citStr="Turian et al. (2010)" startWordPosition="2099" endWordPosition="2102"> SVD and NMF factorization are evaluated; in both cases, we minimize the Frobenius norm of the reconstruction error. Table 2 compares the accuracy of a number of different configurations. The transductive TF-KLD weighting yields the best overall accuracy, achieving 72.75% when combined with nonnegative matrix factorization. While NMF performs slightly better than SVD in both comparisons, the major difference is the performance of discriminative TF-KLD weighting, which outperforms TF-IDF regardless of the factorization technique. When we 2Another example of transductive learning in NLP is when Turian et al. (2010) induced word representations from a corpus that included both training and test data for their downstream named entity recognition task. Figure 2: Accuracy of feature and weighting combinations in the classification framework. perform the matrix factorization on only the training data, the accuracy on the test set is 73.58%, with F1 score 80.55%. 5.2 Supervised classification Next, we apply supervised classification, constructing sample vectors from the latent representation as shown in Equation 1. For classification, we choose a Support Vector Machine with a linear kernel (Fan et al., 2008),</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word Representation: A Simple and General Method for Semi-Supervised Learning. In ACL, pages 384– 394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From Frequency to Meaning: Vector Space Models of Semantics.</title>
<date>2010</date>
<pages>37--141</pages>
<publisher>JAIR,</publisher>
<contexts>
<context position="1814" citStr="Turney and Pantel, 2010" startWordPosition="248" endWordPosition="252"> Barzilay, 2006) to grouping redundant event mentions in social media (Petrovi´c et al., 2010). The task is challenging because of the infinitely diverse set of possible linguistic realizations for any idea (Bhagat and Hovy, 2013), and because of the short length of individual sentences, which means that standard bag-of-words representations will be hopelessly sparse. Distributional methods address this problem by transforming the high-dimensional bag-of-words representation into a lower-dimensional latent space. This can be accomplished by factoring a matrix or tensor of term-context counts (Turney and Pantel, 2010); proximity in the induced latent space has been shown to correlate with semantic similarity (Mihalcea et al., 2006). However, factoring the term-context matrix means throwing away a considerable amount of information, as the original matrix of size M x N (number of instances by number of features) is factored into two smaller matrices of size M x K and N x K, with K « M, N. If the factorization does not take into account labeled data about semantic similarity, important information can be lost. In this paper, we show how labeled data can considerably improve distributional methods for measuri</context>
<context position="5189" citStr="Turney and Pantel, 2010" startWordPosition="769" endWordPosition="772">epresenting the meanings of individual words, we directly obtain a distributional representation for the entire sentence. This is inspired by Mihalcea et al. (2006) and Guo and Diab (2012), who treat sentences as pseudo-documents in an LSA framework, and identify paraphrases using similarity in the latent space. We show that the performance of such techniques can be improved dramatically by using supervised information to (1) reweight the individual distributional features and (2) learn the importance of each latent dimension. 3 Discriminative feature weighting Distributional representations (Turney and Pantel, 2010) can be induced from a co-occurrence matrix W E RM N, where M is the number of instances and N is the number of distributional features. For paraphrase identification, each instance is a sentence; features may be unigrams, or may include higher-order n-grams or dependency pairs. By decomposing the matrix W, we hope to obtain a latent representation in which semantically-related sentences are similar. Singular value decomposition (SVD) is traditionally used to perform this factorization. However, recent work has demonstrated the robustness of nonnegative matrix factorization (NMF; Lee and Seung</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From Frequency to Meaning: Vector Space Models of Semantics. JAIR, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ssephen Wan</author>
<author>Mark Dras</author>
<author>Robert Dale</author>
<author>Cecile Paris</author>
</authors>
<title>Using Dependency-based Features to Take the “Para-farce” out of Paraphrase.</title>
<date>2006</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop.</booktitle>
<contexts>
<context position="3728" citStr="Wan et al., 2006" startWordPosition="545" endWordPosition="548"> The combination of latent and finegrained features yields further improvements in accuracy, demonstrating that these feature sets provide complementary information on semantic similarity. 891 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891–896, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Without attempting to do justice to the entire literature on paraphrase identification, we note three high-level approaches: (1) string similarity metrics such as n-gram overlap and BLEU score (Wan et al., 2006; Madnani et al., 2012), as well as string kernels (Bu et al., 2012); (2) syntactic operations on the parse structure (Wu, 2005; Das and Smith, 2009); and (3) distributional methods, such as latent semantic analysis (LSA; Landauer et al., 1998), which are most relevant to our work. One application of distributional techniques is to replace individual words with distributionally similar alternatives (Kauchak and Barzilay, 2006). Alternatively, Blacoe and Lapata (2012) show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of </context>
<context position="9616" citStr="Wan et al., 2006" startWordPosition="1508" endWordPosition="1511">respect to pk and 1−qk. The diagonal line running through the middle of the plot indicates zero KL-divergence, so features on this line will be ignored. 1We obtain very similar results with the opposite divergence KL(qk||N). However, the symmetric Jensen-Shannon divergence performs poorly. 1 unigram recall 2 unigram precision 3 bigram recall 4 bigram precision 5 dependency relation recall 6 dependency relation precision 7 BLEU recall 8 BLEU precision 9 Difference of sentence length 10 Tree-editing distance Table 1: Fine-grained features for paraphrase classification, selected from prior work (Wan et al., 2006). 4 Supervised classification While previous work has performed paraphrase classification using distance or similarity in the latent space (Guo and Diab, 2012; Socher et al., 2011), more direct supervision can be applied. Specifically, we convert the latent representations of a pair of sentences ~v1 and ~v2 into a sample vector, ~s(~v1,~v2) = [~v1 + ~v2, |~v1 − ~v2 |(1) concatenating the element-wise sum ~v1 +~v2 and absolute difference |~v1 − ~v2|. Note that ~s(·, ·) is symmetric, since ~s(~v1,~v2) = ~s(~v2,~v1). Given this representation, we can use any supervised classification algorithm. A</context>
<context position="15599" citStr="Wan et al., 2006" startWordPosition="2461" endWordPosition="2464">00 250 300 350 400 K 80 60 Feat,_TF-IDF_SVM Feats_TF-IDF_SVM Feat,_TF-KLD_SVM Feats_TF-KLD_SVM 75 70 Accuracy (%) 65 894 Factorization Feature set Weighting K Measure Accuracy (%) F1 SVD unigrams TF-IDF 100 cosine sim. 68.92 80.33 NMF unigrams TF-IDF 100 cosine sim. 68.96 80.14 WTMF unigrams TF-IDF 100 cosine sim. 71.51 not reported SVD unigrams TF-KLD 100 cosine sim. 72.23 81.19 NMF unigrams TF-KLD 100 cosine sim. 72.75 81.48 Table 2: Similarity-based paraphrase identification accuracy. Results for WTMF are reprinted from the paper by Guo and Diab (2012). Acc. F1 Most common class 66.5 79.9 (Wan et al., 2006) 75.6 83.0 (Das and Smith, 2009) 73.9 82.3 (Das and Smith, 2009) with 18 features 76.1 82.7 (Bu et al., 2012) 76.3 not reported (Socher et al., 2011) 76.8 83.6 (Madnani et al., 2012) 77.4 84.1 FEAT2, TF-KLD, SVM 79.76 85.87 FEAT2, TF-KLD, SVM, Fine-grained features 80.41 85.96 Table 3: Supervised classification. Results from prior work are reprinted. racy now improves to 80.41%, with an F1 score of 85.96%. When the latent representation is induced from only the training data, the corresponding results are 79.94% on accuracy and 85.36% F1, again better than the previous state-of-the-art. These </context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2006</marker>
<rawString>Ssephen Wan, Mark Dras, Robert Dale, and Cecile Paris. 2006. Using Dependency-based Features to Take the “Para-farce” out of Paraphrase. In Proceedings of the Australasian Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Recognizing paraphrases and textual entailment using inversion transduction grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<pages>25--30</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3855" citStr="Wu, 2005" startWordPosition="569" endWordPosition="570">ide complementary information on semantic similarity. 891 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891–896, Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics 2 Related Work Without attempting to do justice to the entire literature on paraphrase identification, we note three high-level approaches: (1) string similarity metrics such as n-gram overlap and BLEU score (Wan et al., 2006; Madnani et al., 2012), as well as string kernels (Bu et al., 2012); (2) syntactic operations on the parse structure (Wu, 2005; Das and Smith, 2009); and (3) distributional methods, such as latent semantic analysis (LSA; Landauer et al., 1998), which are most relevant to our work. One application of distributional techniques is to replace individual words with distributionally similar alternatives (Kauchak and Barzilay, 2006). Alternatively, Blacoe and Lapata (2012) show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of larger units of text. Socher et al. (2011) propose a syntactically-informed approach to combine word representations, using a r</context>
</contexts>
<marker>Wu, 2005</marker>
<rawString>Dekai Wu. 2005. Recognizing paraphrases and textual entailment using inversion transduction grammars. In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 25–30. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Xin Liu</author>
<author>Yihong Gong</author>
</authors>
<title>Document Clustering based on Non-Negative Matrix Factorization. In</title>
<date>2003</date>
<booktitle>SIGIR,</booktitle>
<pages>267--273</pages>
<contexts>
<context position="5835" citStr="Xu et al., 2003" startWordPosition="873" endWordPosition="876">rence matrix W E RM N, where M is the number of instances and N is the number of distributional features. For paraphrase identification, each instance is a sentence; features may be unigrams, or may include higher-order n-grams or dependency pairs. By decomposing the matrix W, we hope to obtain a latent representation in which semantically-related sentences are similar. Singular value decomposition (SVD) is traditionally used to perform this factorization. However, recent work has demonstrated the robustness of nonnegative matrix factorization (NMF; Lee and Seung, 2001) for text mining tasks (Xu et al., 2003; Arora et al., 2012); the difference from SVD is the addition of a non-negativity constraint in the latent representation based on non-orthogonal basis. While W may simply contain counts of distributional features, prior work has demonstrated the utility of reweighting these counts (Turney and Pantel, 2010). TF-IDF is a standard approach, as the inverse document frequency (IDF) term increases the importance of rare words, which may be more discriminative. Guo and Diab (2012) show that applying a special weight to unseen words can further improvement performance on paraphrase identification. W</context>
</contexts>
<marker>Xu, Liu, Gong, 2003</marker>
<rawString>Wei Xu, Xin Liu, and Yihong Gong. 2003. Document Clustering based on Non-Negative Matrix Factorization. In SIGIR, pages 267–273.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000247">
<title confidence="0.9981135">
MCTest: A Challenge Dataset for the Open-Domain
Machine Comprehension of Text
</title>
<author confidence="0.983682">
Matthew Richardson
</author>
<affiliation confidence="0.960791">
Microsoft Research
</affiliation>
<address confidence="0.944421">
One Microsoft Way
Redmond, WA 98052
</address>
<email confidence="0.999083">
mattri@microsoft.com
</email>
<author confidence="0.798753">
Christopher J.C. Burges
</author>
<affiliation confidence="0.803108">
Microsoft Research
</affiliation>
<address confidence="0.9447575">
One Microsoft Way
Redmond, WA 98052
</address>
<email confidence="0.998603">
cburges@microsoft.com
</email>
<author confidence="0.99124">
Erin Renshaw
</author>
<affiliation confidence="0.966701">
Microsoft Research
</affiliation>
<address confidence="0.9450805">
One Microsoft Way
Redmond, WA 98052
</address>
<email confidence="0.999341">
erinren@microsoft.com
</email>
<sectionHeader confidence="0.995649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999892848484848">
We present MCTest, a freely available set of
stories and associated questions intended for
research on the machine comprehension of
text. Previous work on machine comprehen-
sion (e.g., semantic modeling) has made great
strides, but primarily focuses either on lim-
ited-domain datasets, or on solving a more re-
stricted goal (e.g., open-domain relation
extraction). In contrast, MCTest requires ma-
chines to answer multiple-choice reading
comprehension questions about fictional sto-
ries, directly tackling the high-level goal of
open-domain machine comprehension. Read-
ing comprehension can test advanced abilities
such as causal reasoning and understanding
the world, yet, by being multiple-choice, still
provide a clear metric. By being fictional, the
answer typically can be found only in the sto-
ry itself. The stories and questions are also
carefully limited to those a young child would
understand, reducing the world knowledge
that is required for the task. We present the
scalable crowd-sourcing methods that allow
us to cheaply construct a dataset of 500 stories
and 2000 questions. By screening workers
(with grammar tests) and stories (with grad-
ing), we have ensured that the data is the same
quality as another set that we manually edited,
but at one tenth the editing cost. By being
open-domain, yet carefully restricted, we hope
MCTest will serve to encourage research and
provide a clear metric for advancement on the
machine comprehension of text.
</bodyText>
<sectionHeader confidence="0.974534" genericHeader="keywords">
1 Reading Comprehension
</sectionHeader>
<bodyText confidence="0.999825394736842">
A major goal for NLP is for machines to be able to
understand text as well as people. Several research
disciplines are focused on this problem: for exam-
ple, information extraction, relation extraction,
semantic role labeling, and recognizing textual en-
tailment. Yet these techniques are necessarily
evaluated individually, rather than by how much
they advance us towards the end goal. On the other
hand, the goal of semantic parsing is the machine
comprehension of text (MCT), yet its evaluation
requires adherence to a specific knowledge repre-
sentation, and it is currently unclear what the best
representation is, for open-domain text.
We believe that it is useful to directly tackle the
top-level task of MCT. For this, we need a way to
measure progress. One common method for evalu-
ating someone’s understanding of text is by giving
them a multiple-choice reading comprehension
test. This has the advantage that it is objectively
gradable (vs. essays) yet may test a range of abili-
ties such as causal or counterfactual reasoning,
inference among relations, or just basic under-
standing of the world in which the passage is set.
Therefore, we propose a multiple-choice reading
comprehension task as a way to evaluate progress
on MCT. We have built a reading comprehension
dataset containing 500 fictional stories, with 4 mul-
tiple choice questions per story. It was built using
methods which can easily scale to at least 5000
stories, since the stories were created, and the cura-
tion was done, using crowd sourcing almost entire-
ly, at a total of $4.00 per story. We plan to perio-
dically update the dataset to ensure that methods
are not overfitting to the existing data. The dataset
is open-domain, yet restricted to concepts and
words that a 7 year old is expected to understand.
This task is still beyond the capability of today’s
computers and algorithms.
</bodyText>
<page confidence="0.989206">
193
</page>
<note confidence="0.733471">
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 193–203,
Seattle, Washington, USA, 18-21 October 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999957580645161">
By restricting the concept space, we gain the dif-
ficulty of being an open-domain problem, without
the full complexity of the real world (for example,
there will be no need for the machine to understand
politics, technology, or to have any domain specif-
ic expertise). The multiple choice task avoids am-
biguities (such as when the task is to find a
sentence that best matches a question, as in some
early reading comprehension tasks: see Section 2),
and also avoids the need for additional grading,
such as is needed in some TREC tasks. The stories
were chosen to be fictional to focus work on find-
ing the answer in the story itself, rather than in
knowledge repositories such as Wikipedia; the goal
is to build technology that actually understands
stories and paragraphs on a deep level (as opposed
to using information retrieval methods and the re-
dundancy of the web to find the answers).
We chose to use crowd sourcing, as opposed to,
for example, contracting teachers or paying for
existing standardized tests, for three reasons,
namely: (1) scalability, both for the sizes of da-
tasets we can provide, and also for the ease of reg-
ularly refreshing the data; (2) for the variety in
story-telling that having many different authors
brings; and (3) for the free availability that can on-
ly result from providing non-copyrighted data. The
content is freely available at http://research.micro-
soft.com/mct, and we plan to use that site to track
published results and provide other resources, such
as labels of various kinds.
</bodyText>
<sectionHeader confidence="0.993934" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999957073529412">
The research goal of mapping text to meaning rep-
resentations in order to solve particular tasks has a
long history. DARPA introduced the Airline Trav-
el Information System (ATIS) in the early 90’s:
there the task was to slot-fill flight-related infor-
mation by modeling the intent of spoken language
(see Tur et al., 2010, for a review). This data con-
tinues to be a used in the semantic modeling com-
munity (see, for example, Zettlemoyer and Collins,
2009). The Geoquery database contains 880 geo-
graphical facts about the US and has played a simi-
lar role for written (as opposed to spoken) natural
language queries against a database (Zelle and
Mooney, 1996) and it also continues to spur re-
search (see for example Goldwasser et al., 2011),
as does the similar Jobs database, which provides
mappings of 640 sentences to a listing of jobs
(Tang and Mooney, 2001). More recently, Zweig
and Burges (2012) provided a set of 1040 sentenc-
es that comprise an SAT-style multiple choice sen-
tence completion task.
The idea of using story-based reading compre-
hension questions to evaluate methods for machine
reading itself goes back over a decade, when
Hirschmann et al. (1999) showed that a bag of
words approach, together with some heuristic lin-
guistic modeling, could achieve 40% accuracy for
the task of picking the sentence that best matches
the query for “who / what / when / where / why”
questions, on a small reading comprehension da-
taset from Remedia. This dataset spurred several
research efforts, for example using reinforcement
learning (Grois and Wilkins, 2005), named entity
resolution (Harabagiu et al., 2003) and mapping
questions and answers to logical form (Wellner et
al., 2006). Work on story understanding itself goes
back much further, to 1972, when Charniak pro-
posed using a background model to answer ques-
tions about children’s stories. Similarly, the TREC
(and TAC) Question Answering tracks (e.g., Voor-
hees and Tice, 1999) aim to evaluate systems on
their ability to answer factual questions such as
“Where is the Taj Mahal”. The QA4MRE task also
aims to evaluate machine reading systems through
question answering (e.g., Clark et al., 2012). Earli-
er work has also aimed at controlling the scope by
limiting the text to children’s stories: Breck et al.
(2001) collected 75 stories from the Canadian
Broadcasting Corporation’s web site for children,
and generated 650 questions for them manually,
where each question was answered by a sentence
in the text. Leidner et al. (2003) both enriched the
CBC4kids data by adding several layers of annota-
tion (such as semantic and POS tags), and meas-
ured QA performance as a function of question
difficulty. For a further compendium of resources
related to the story comprehension task, see
Mueller (2010).
The task proposed here differs from the above
work in several ways. Most importantly, the data
collection is scalable: if the dataset proves suffi-
ciently useful to others, it would be straightforward
to gather an order of magnitude more. Even the
dataset size presented here is an order of magni-
tude larger than the Remedia or the CBC4kids data
and many times larger than QA4MRE. Second, the
multiple choice task presents less ambiguity (and is
consequently easier to collect data for) than the
</bodyText>
<page confidence="0.998107">
194
</page>
<bodyText confidence="0.995795166666667">
James the Turtle was always getting in trouble.
Sometimes he&apos;d reach into the freezer and empty out
all the food. Other times he&apos;d sled on the deck and get
a splinter. His aunt Jane tried as hard as she could to
keep him out of trouble, but he was sneaky and got
into lots of trouble behind her back.
One day, James thought he would go into town and
see what kind of trouble he could get into. He went to
the grocery store and pulled all the pudding off the
shelves and ate two jars. Then he walked to the fast
food restaurant and ordered 15 bags of fries. He did-
n&apos;t pay, and instead headed home.
His aunt was waiting for him in his room. She told
James that she loved him, but he would have to start
acting like a well-behaved turtle.
After about a month, and after getting into lots of
trouble, James finally made up his mind to be a better
turtle.
</bodyText>
<figure confidence="0.996584454545455">
1) What is the name of the trouble making turtle?
A) Fries
B) Pudding
C) James
D) Jane
2) What did James pull off of the shelves in the gro-
cery store?
A) pudding
B) fries
C) food
D) splinters
3) Where did James go after he went to the grocery
store?
A) his deck
B) his freezer
C) a fast food restaurant
D) his room
4) What did James do after he ordered the fries?
A) went to the grocery store
B) went home without paying
C) ate them
D) made up his mind to be a better turtle
</figure>
<figureCaption confidence="0.998673">
Figure 1. Sample Story and Questions (chosen random-
ly from MC500 train set).
</figureCaption>
<bodyText confidence="0.999978333333333">
task of finding the most appropriate sentence, and
may be automatically evaluated. Further, our sto-
ries are fictional, which means that the information
to answer the question is contained only in the sto-
ry itself (as opposed to being able to directly lever-
age knowledge repositories such as Wikipedia).
This design was chosen to focus the task on the
machine understanding of short passages, rather
than the ability to match against an existing
knowledge base. In addition, while in the
CBC4kids data each answer was a sentence from
the story, here we required that approximately half
of the questions require at least two sentences from
the text to answer; being able to control complexity
in this way is a further benefit of using multiple
choice answers. Finally, as explained in Section 1,
the use of free-form input makes the problem open
domain (as opposed to the ATIS, Geoquery and
Jobs data), leading to the hope that solutions to the
task presented here will be easier to apply to novel,
unrelated tasks.
</bodyText>
<sectionHeader confidence="0.963031" genericHeader="method">
3 Generating the Stories and Questions
</sectionHeader>
<bodyText confidence="0.9999605625">
Our aim was to generate a corpus of fictional story
sets1 that could be scaled with as little expert input
as possible. Thus, we designed the process to be
gated by cost, and keeping the costs low was a
high priority. Crowd-sourcing seemed particularly
appropriate, given the nature of the task, so we
opted to use Amazon Mechanical Turk2 (AMT).
With over 500,000 workers3, it provides the work
force required to both achieve scalability and,
equally importantly, to provide diversity in the sto-
ries and types of questions. We restricted our task
to AMT workers (workers) residing in the United
States. The average worker is 36 years old, more
educated than the United States population in gen-
eral (Paolacci et al., 2010), and the majority of
workers are female.
</bodyText>
<subsectionHeader confidence="0.999929">
3.1 The Story and Questions
</subsectionHeader>
<bodyText confidence="0.904130538461539">
Workers were instructed to write a short (150-300
words) fictional story, and to write as if for a child
in grade school. The choice of 150-300 was made
to keep the task an appropriate size for workers
while still allowing for complex stories and ques-
tions. The workers were free to write about any
topic they desired (as long as it was appropriate for
a young child), and so there is a wide range, in-
cluding vacations, animals, school, cars, eating,
gardening, fairy tales, spaceships, and cowboys.
1 We use the term “story set” to denote the fictional story
together with its multiple choice questions, hypothetical an-
swers, and correct answer labels.
</bodyText>
<footnote confidence="0.999153">
2 http://www.mturk.com
3 https://requester.mturk.com/tour
</footnote>
<page confidence="0.998296">
195
</page>
<bodyText confidence="0.9999387">
Workers were also asked to provide four reading
comprehension questions pertaining to their story
and, for each, four multiple-choice answers. Com-
ing up with incorrect alternatives (distractors) is a
difficult task (see, e.g., Agarwal, 2011) but work-
ers were requested to provide “reasonable” incor-
rect answers that at least include words from the
story so that their solution is not trivial. For exam-
ple, for the question “What is the name of the
dog?”, if only one of the four answers occurs in the
story, then that answer must be the correct one.
Finally, workers were asked to design their
questions and answers such that at least two of the
four questions required multiple sentences from the
story to answer them. That is, for those questions it
should not be possible to find the answer in any
individual sentence. The motivation for this was to
ensure that the task could not be fully solved using
lexical techniques, such as word matching, alone.
Whilst it is still possible that a sophisticated lexical
analysis could completely solve the task, requiring
that answers be constructed from at least two dif-
ferent sentences in the story makes this much less
likely; our hope is that the solution will instead
require some inference and some form of limited
reasoning. This hope rests in part upon the obser-
vation that standardized reading comprehension
tests, whose goal after all is to test comprehension,
generally avoid questions that can be answered by
reading a single sentence.
</bodyText>
<subsectionHeader confidence="0.999301">
3.2 Automatic Validation
</subsectionHeader>
<bodyText confidence="0.99971565">
Besides verifying that the story and all of the ques-
tions and answers were provided, we performed
the following automatic validation before allowing
the worker to complete the task:
Limited vocabulary: The lowercase words in the
story, questions, and answers were stemmed and
checked against a vocabulary list of approximately
8000 words that a 7-year old is likely to know
(Kuperman et al., 2012). Any words not on the list
were highlighted in red as the worker typed, and
the task could not be submitted unless all of the
words satisfied this vocabulary criterion. To allow
the use of arbitrary proper nouns, capitalized words
were not checked against the vocabulary list.
Multiple-sentence questions: As described earli-
er, we required that at least two of the questions
need multiple sentences to answer. Workers were
simply asked to mark whether a question needs one
or multiple sentences and we required that at least
two are marked as multiple.
</bodyText>
<subsectionHeader confidence="0.99922">
3.3 The Workers
</subsectionHeader>
<bodyText confidence="0.999993516129032">
Workers were required to reside in the United
States and to have completed 100 HITs with an
over 95% approval rate4. The median worker took
22 minutes to complete the task. We paid workers
$2.50 per story set and allowed each to do a maxi-
mum of 8 tasks (5 in MC500). We did not experi-
ment with paying less, but this rate amounts to
$6.82/hour, which is approximately the rate paid
by other writing tasks on AMT at the time, though
is also significantly higher than the median wage
of $1.38 found in 2010 (Horton and Chilton,
2010). Workers could optionally leave feedback on
the task, which was overwhelmingly positive – the
most frequent non-stopword in the comments was
“fun” and the most frequent phrase was “thank
you”. The only negative comments (in &lt;1% of
submissions) were when the worker felt that a par-
ticular word should have been on the allowed vo-
cabulary list. Given the positive feedback, it may
be possible to pay less if we collect more data in
the future. We did not enforce story length con-
straints, but some workers interpreted our sugges-
tion that the story be 150-300 words as a hard
constraint, and some asked to be able to write a
longer story.
The MCTest corpus contains two sets of stories,
named MC160 and MC500, and containing 160
and 500 stories respectively. MC160 was gathered
first, then some improvements were made before
gathering MC500. We give details on the differ-
ences between these two sets below.
</bodyText>
<subsectionHeader confidence="0.947411">
3.4 MC160: Manually Curated for Quality
</subsectionHeader>
<bodyText confidence="0.9999552">
In addition to the details described above, MC160
workers were given a target elementary grade
school level (1-4) and a sample story matching that
grade level5. The intent was to produce a set of
stories and questions that varied in difficulty so
that research work can progress grade-by-grade if
needed. However, we found little difference be-
tween grades in the corpus..
After gathering the stories, we manually curated
the MC160 corpus by reading each story set and
</bodyText>
<footnote confidence="0.9345655">
4 The latter two are the default AMT requirements.
5 From http://www.englishforeveryone.org/.
</footnote>
<page confidence="0.99009">
196
</page>
<listItem confidence="0.923014384615385">
1. We went to visit the Smith’s at their house.
2. I altered their suits for them.
3. You&apos;re car is very old.
4. Jim likes to run, hike, and going kayaking.
5. He should of come to work on time.
6. I think its best to wash lots of apples.
7. Are people who write &amp;quot;ping&amp;quot; thinking of subma-
rines?
8. Smoke filled the room, making it hard to breathe.
9. Alert yet aloof - that&apos;s you.
10. They wanted they&apos;re money back.
11. Hawks and eagles like to fly high in the sky.
12. Don&apos;t let her wear them down.
</listItem>
<figure confidence="0.9553486">
13. The cat particularly liked the greasy plate.
14. The company is less successful because we have
less employees.
15. The hamster belongs to Sam and I.
16. No one landed on the air strip today.
17. He was very effected by her tears.
18. You are a tired piece of toast, metaphorically
speaking.
19. Anne plays bass and sings.
20. Him and me met at the park.
</figure>
<figureCaption confidence="0.999898">
Figure 2. Grammar test for qualifying workers.
</figureCaption>
<bodyText confidence="0.995710142857143">
correcting errors. The most common mistakes were
grammatical, though occasionally questions and/or
answers needed to be fixed. 66% of the stories
have at least one correction. We provide both the
curated and original corpuses in order to allow re-
search on reading comprehension in the presence
of grammar, spelling, and other mistakes.
</bodyText>
<subsectionHeader confidence="0.919729">
3.5 MC500: Adding a Grammar Test
</subsectionHeader>
<bodyText confidence="0.999488">
Though the construction of MC160 was successful,
it requires a costly curation process which will not
scale to larger data sets (although the curation was
useful, both for improving the design of MC500,
and for assessing the effectiveness of automated
curation techniques). To more fully automate the
process, we added two more stages: (1) A grammar
test that automatically pre-screens workers for
writing ability, and (2) a second Mechanical Turk
task whereby new workers take the reading com-
prehension tests and rate their quality. We will dis-
cuss stage (2) in the next section.
The grammar test consisted of 20 sentences, half
of which had one grammatical error (see Figure 2).
The incorrect sentences were written using com-
mon errors such as you’re vs. your, using ‘s to in-
dicate plurality, incorrect use of tense, it’s vs. its,
</bodyText>
<table confidence="0.998982">
Quality About
(1-5) animals
No Grammar Test 3.2 73%
Grammar Test 4.3 30%
</table>
<tableCaption confidence="0.988175">
Table 1. Pre-screening workers using a grammar test
improves both quality and diversity of stories. Both
differences are significant using the two-tailed t-test
(p&lt;0.05 for quality and p&lt;0.01 for animals).
</tableCaption>
<bodyText confidence="0.99947">
less vs. fewer, I vs. me, etc. Workers were required
to indicate for each sentence whether it was
grammatically correct or not, and had to pass with
at least 80% accuracy in order to qualify for the
task. The 80% threshold was chosen to trade off
worker quality with the rate at which the tasks
would be completed; initial experiments using a
threshold of 90% indicated that collecting 500 sto-
ries would take many weeks instead of days. Note
that each worker is allowed to write at most 5
stores, so we required at least 100 workers to pass
the qualification test.
To validate the use of the qualification test, we
gathered 30 stories requiring the test (qual) and 30
stories without. We selected a random set of 20
stories (10 from each), hid their origin, and then
graded the overall quality of the story and ques-
tions from 1-5, meaning do not attempt to fix, bad
but rescuable, has non-minor problems, has only
minor problems, and has no problems, respective-
ly. Results are shown in Table 1. The difference is
statistically significant (p&lt;0.05, using the two-
tailed t-test). The qual stories were also more di-
verse, with fewer of them about animals (the most
common topic).
Additional Modifications: Based on our experi-
ence curating MC160, we also made the following
modifications to the task. In order to eliminate triv-
ially-answerable questions, we required that each
answer be unique, and that either the correct an-
swer did not appear in the story or, if it did appear,
that at least two of the incorrect answers also ap-
peared in the story. This is to prevent questions
that are trivially answered by checking which an-
swer appears in the story. The condition on wheth-
er the correct answer appears is to allow questions
such as “How many candies did Susan eat?”,
where the total may never appear in the story, even
though the information needed to derive it does.
An answer is considered to appear in the story if at
least half (rounded down) of its non-stopword
</bodyText>
<page confidence="0.992087">
197
</page>
<bodyText confidence="0.999949315789474">
terms appear in the story (ignoring word endings).
This check is done automatically and must be satis-
fied before the worker is able to complete the task.
Workers could also bypass the check if they felt it
was incorrect, by adding a special term to their
answer.
We were also concerned that the sample story
might bias the workers when writing the story set,
particularly when designing questions that require
multiple sentences to answer. So, we removed the
sample story and grade level from the task.
Finally, in order to encourage more diversity of
stories, we added creativity terms, a set of 15
nouns chosen at random from the allowed vocabu-
lary set. Workers were asked to “please consider”
using one or more of the terms in their story, but
use of the words was strictly optional. On average,
workers used 3.9 of the creativity terms in their
stories.
</bodyText>
<sectionHeader confidence="0.967357" genericHeader="method">
4 Rating the Stories and Questions
</sectionHeader>
<bodyText confidence="0.999976931034483">
In this section we discuss the crowd-sourced rating
of story sets. We wished to ensure story set quality
despite the fact that MC500 was only minimally
manually curated (see below). Pre-qualifying
workers with a grammar test was one step of this
process. The second step was to have additional
workers on Mechanical Turk both evaluate each
story and take its corresponding test. Each story
was evaluated in this way by 10 workers, each of
whom provided scores for each of age-
appropriateness (yes/maybe/no), grammaticality
(few/some/many errors), and story clarity (excel-
lent/reasonable/poor). When answering the four
reading comprehension questions, workers could
also mark a question as “unclear”. Each story set
was rated by 10 workers who were each paid $0.15
per set.
Since we know the purportedly correct answer,
we can estimate worker quality by measuring what
fraction of questions that worker got right. Work-
ers with less than 80% accuracy (ignoring those
questions marked as unclear) were removed from
the set. This constituted just 4.1% of the raters and
4.2% of the judgments (see Figure 3). Only one
rater appeared to be an intentional spammer, an-
swering 1056 questions with only 29% accuracy.
The others primarily judged only one story. Only
one worker fell between, answering 336 questions
with just 75% accuracy.
</bodyText>
<figureCaption confidence="0.9884565">
Figure 3. Just 4.1% of raters had an accuracy below
80% (constituting 4.2% of the judgments).
</figureCaption>
<bodyText confidence="0.999977545454545">
For the remaining workers (those who achieved
at least 80% accuracy), we measured median story
appropriateness, grammar, and clarity. For each
category, stories for which less than half of the
ratings were the best possible (e.g., excellent story
clarity) were inspected and optionally removed
from the data set. This required inspecting 40
(&lt;10%) of the stories, only 2 of which were
deemed poor enough to be removed (both of which
had over half of the ratings all the way at the bot-
tom end of the scale, indicating we could potential-
ly have inspected many fewer stories with the same
results). We also inspected questions for which at
least 5 workers answered incorrectly, or answered
“unclear”. In total, 29 questions (&lt;2%) were in-
spected. 5 were fixed by changing the question, 8
by changing the answers, 2 by changing both, 6 by
changing the story, and 8 were left unmodified.
Note that while not fully automated, this process
of inspecting stories and repairing questions took
one person one day, so is still scalable to at least an
order of magnitude more stories.
</bodyText>
<sectionHeader confidence="0.991655" genericHeader="method">
5 Dataset Analysis
</sectionHeader>
<bodyText confidence="0.999976">
In Table 2, we present results demonstrating the
value of the grammar test and curation process. As
expected, manually curating MC160 resulted in
increased grammar quality and percent of ques-
tions answered correctly by raters. The goal of
MC500 was to find a more scalable method to
achieve the same quality as the curated MC160. As
Table 2 shows, the grammar test improved story
grammar quality from 1.70 to 1.77 (both uncurat-
ed). The rating and one-day curation process in-
</bodyText>
<page confidence="0.99575">
198
</page>
<table confidence="0.999519">
Set AgeAp Clarity Grammar Correct
160 1.88 1.63 1.70 95.3
500 1.92 1.65 1.77 95.3
500 curated 1.94 1.71 1.79 96.9
160 curated 1.91 1.67 1.84ǂ 97.7
</table>
<tableCaption confidence="0.896904">
Table 2. Average age appropriateness, story clarity,
grammar quality (0-2, with 2 being best), and percent of
questions answered correctly by raters, for the original
and curated versions of the data. Bold indicates statisti-
cal significance vs. the original version of the same set,
using the two-sample t-test with unequal variance. The ǂ
indicates the only statistical difference between 500
curated and 160 curated.
</tableCaption>
<table confidence="0.998751333333333">
Corpus Stories Median Average Words Per:
writing
time
Story Question Answer
MC160 160 26 min 204 8.0 3.4
MC500 500 20 min 212 7.7 3.4
</table>
<tableCaption confidence="0.999902">
Table 3. Corpus statistics for MC160 and MC500.
</tableCaption>
<bodyText confidence="0.987620166666667">
creases this to 1.79, whereas a fully manual cura-
tion results in a score of 1.84. Curation also im-
proved the percent of questions answered correctly
for both MC160 and MC500, but, unlike with
grammar, there is no significant difference be-
tween the two curated sets. Indeed, the only statis-
tically significant difference between the two is in
grammar. So, the MC500 grammar test and cura-
tion process is a very scalable method for collect-
ing stories of nearly the quality of the costly
manual curation of MC160.
We also computed correlations between these
measures of quality and various factors such as
story length and time spent writing the story. On
MC500, there is a mild correlation between a
worker’s grammar test score and the judged
grammar quality of that worker’s story (correlation
of 0.24). Interestingly, this relation disappeared
once MC500 was curated, likely due to repairing
the stories with the worst grammar. On MC160,
there is a mild correlation between the clarity and
the number of words in the question and answer
(0.20 and 0.18). All other correlations were below
0.15. These factors could be integrated into an es-
timate for age-appropriateness, clarity, and gram-
mar, potentially reducing the need for raters.
Table 3 provides statistics on each corpus.
MC160 and MC500 are similar in average number
of words per story, question, and answer, as well as
the median writing time. The most commonly used
</bodyText>
<subsectionHeader confidence="0.578076">
Baseline Algorithms
</subsectionHeader>
<table confidence="0.879563769230769">
Require: Passage P, set of passage words PW, ith word in
passage Pi, set of words in question Q, set of words in
hypothesized answers A1..4, and set of stop words U,
Define: C(w) := Ziff (Pi = w);
Define: IC(w) := log (1 + 1 )
C(w)
Algorithm 1 Sliding Window
for i = 1 to 4 do
�(IC(Pi+w)
=1..Isl tl 0
end for
return
Algorithm 2 Distance Based
</table>
<equation confidence="0.887705571428571">
for i = 1 to 4 do
SQ=(QnPW) \U
SA, = ((Ai n PW) \ Q)if � � =0 or � I=0
else
dl =
IPI-1 gESQ min
dP( q, a),E
</equation>
<bodyText confidence="0.880955">
where dP (q, a) is the minimum number of
words between an occurrence of q and an
occurrence of a in P, plus one.
</bodyText>
<figure confidence="0.798131428571428">
end if
end for
return dl A.
Algorithm SW
Return
Algorithm SW+D
Return
</figure>
<figureCaption confidence="0.9896855">
Figure 4. The two lexical-based algorithms used for the
baselines.
</figureCaption>
<bodyText confidence="0.999897090909091">
nouns in MC500 are: day, friend, time, home,
house, mother, dog, mom, school, dad, cat, tree,
and boy. The stories vary widely in theme. The
first 10 stories of the randomly-ordered MC500 set
are about: travelling to Miami to visit friends, wak-
ing up and saying hello to pets, a bully on a
schoolyard, visiting a farm, collecting insects at
Grandpa’s house, planning a friend’s birthday par-
ty, selecting clothes for a school dance, keeping
animals from eating your ice cream, animals order-
ing food, and adventures of a boy and his dog.
</bodyText>
<note confidence="0.352046">
PI
</note>
<page confidence="0.981214">
199
</page>
<table confidence="0.913465666666667">
MC160 Train and Dev: Test:
400 Q’s 240 Q’s
SW SW+D SW SW+D
Single 59.46 68.11 64.29 75.89
Multi 59.53 67.44 48.44 57.81
All 59.50 67.75 55.83 66.25
</table>
<tableCaption confidence="0.990389625">
Table 4. Percent correct for the multiple choice ques-
tions for MC160. SW: sliding window algorithm.
SW+D: combined results with sliding window and
distance based algorithms. Single/Multi: questions
marked by worker as requiring a single/multiple sen-
tence(s) to answer. All differences between SW and
SW+D are significant (p&lt;0.01 using the two-tailed
paired t-test).
</tableCaption>
<table confidence="0.983942833333333">
MC500 Train and Dev: Test: All
1400 Q’s 600 Q’s
SW SW+D SW SW+D SW+D
Single 55.13 61.77 51.10 57.35 60.44
Multi 49.80 55.28 51.83 56.10 55.53
All 52.21 58.21 51.50 56.67 57.75
</table>
<tableCaption confidence="0.9106535">
Table 5. Percent correct for the multiple choice ques-
tions for MC500, notation as above. All differences
between SW and SW+D are significant (p&lt;0.01, test-
ed as above).
</tableCaption>
<bodyText confidence="0.999895">
We randomly divided MC160 and MC500 into
train, development, and test sets of 70, 30, and 60
stories and 300, 50, and 150 stories, respectively.
</bodyText>
<sectionHeader confidence="0.790453" genericHeader="method">
6 Baseline System and Results
</sectionHeader>
<bodyText confidence="0.977523105263158">
We wrote two baseline systems, both using only
simple lexical features. The first system used a
sliding window, matching a bag of words con-
structed from the question and hypothesized an-
swer to the text. Since this ignored long range
dependencies, we added a second, word-distance
based algorithm. The distance-based score was
simply subtracted from the window-based score to
arrive at the final score (we tried scaling the dis-
tance score before subtraction but this did not im-
prove results on the MC160 train set). The
algorithms are summarized in Figure 4. A coin flip
is used to break ties. The use of inverse word
counts was inspired by TF-IDF.
Results for MC160 and MC500 are shown in
Table 4 and Table 5. The MC160 train and devel-
opment sets were used for tuning. The baseline
algorithm was authored without seeing any portion
of MC500, so both the MC160 test set and all of
</bodyText>
<table confidence="0.99229525">
MC160 Test MC500 Test
Baseline (SW+D) 66.25 56.67
RTE 59.79ǂ 53.52
Combined 67.60 60.83ǂ
</table>
<tableCaption confidence="0.97442075">
Table 6. Percent correct for MC160 and MC500 test
sets. The ǂ indicates statistical significance vs. baseline
(p&lt;0.01 using the two-tailed paired t-test). MC160
combined vs. baseline has p-value 0.063.
</tableCaption>
<bodyText confidence="0.999658222222222">
MC500 were used for testing (although we never-
theless report results on the train/test split). Note
that adding the distance based algorithm improved
accuracy by approximately 10% absolute on
MC160 and approximately 6% on MC500. Over-
all, error rates on MC500 are higher than on
MC160, which agrees with human performance
(see Table 2), suggesting that MC500’s questions
are more difficult.
</bodyText>
<sectionHeader confidence="0.940406" genericHeader="method">
7 Recognizing Textual Entailment Results
</sectionHeader>
<bodyText confidence="0.999970777777778">
We also tried using a “recognizing textual entail-
ment” (RTE) system to answer MCTest questions.
The goal of RTE (Dagan et al., 2005) is to deter-
mine whether a given statement can be inferred
from a particular text. We can cast MCTest as an
RTE task by converting each question-answer pair
into a statement, and then selecting the answer
whose statement has the highest likelihood of be-
ing entailed by the story. For example, in the sam-
ple story given in Figure 1, the second question can
be converted into four statements (one for each
answer), and the RTE system should select the
statement “James pulled pudding off of the shelves
in the grocery store” as the most likely one.
For converting question-answer pairs to state-
ments, we used the rules employed in a web-based
question answering system (Cucerzan and
Agichtein, 2005). For RTE, we used BIUTEE
(Stern and Dagan, 2011), which performs better
than the median system in the past four RTE com-
petitions. We ran BIUTEE both in its default con-
figuration, as well as with its optional additional
data sources (FrameNet, ReVerb, DIRT, and others
as found on the BIUTEE home page). The default
configuration performed better so we present its
results here. The results in Table 6 show that the
RTE method performed worse than the baseline.
</bodyText>
<page confidence="0.977331">
200
</page>
<bodyText confidence="0.999985083333333">
We also combined the baseline and RTE system
by training BIUTEE on the train set and using the
development set to optimize a linear combination
of BIUTEE with the baseline; the combined sys-
tem outperforms either component system on
MC500.
It is possible that with some tuning, an RTE sys-
tem will outperform our baseline system. Never-
theless, these RTE results, and the performance of
the baseline system, both suggest that the reading
comprehension task described here will not be triv-
ially solved by off-the-shelf techniques.
</bodyText>
<sectionHeader confidence="0.9889575" genericHeader="method">
8 Making Data and Results an Ongoing
Resource
</sectionHeader>
<bodyText confidence="0.999932346938776">
Our goal in constructing this data is to encourage
research and innovation in the machine compre-
hension of text. Thus, we have made both MC160
and MC500 freely available for download at
http://research.microsoft.com/mct. To our knowl-
edge, these are the largest copyright-free reading
comprehension data sets publicly available. To
further encourage research on these data, we will
be continually updating the webpage with the best-
known published results to date, along with point-
ers to those publications.
One of the difficulties in making progress on a
particular task is implementing previous work in
order to apply improvements to it. To mitigate this
difficulty, we are encouraging researchers who use
the data to (optionally) provide per-answer scores
from their system. Doing so has three benefits: (a)
a new system can be measured in the context of the
errors made by the previous systems, allowing
each research effort to incrementally add useful
functionality without needing to also re-implement
the current state-of-the-art; (b) it allows system
performance to be measured using paired statistical
testing, which will substantially increase the ability
to determine whether small improvements are sig-
nificant; and (c) it enables researchers to perform
error analysis on any of the existing systems, sim-
plifying the process of identifying and tackling
common sources of error. We will also periodically
ensemble the known systems using standard ma-
chine learning techniques and make those results
available as well (unless the existing state-of-the-
art already does such ensembling).
The released data contains the stories and ques-
tions, as well as the results from workers who rated
the stories and took the tests. The latter may be
used, for example, to measure machine perfor-
mance vs. human performance on a per-question
basis (i.e., does your algorithm make similar mis-
takes to humans?), or vs. the judged clarity of each
story. The ratings, as well as whether a question
needs multiple sentences to answer, should typical-
ly only be used in evaluation, since such infor-
mation is not generally available for most text. We
will also provide an anonymized author id for each
story, which could allow additional research such
as using other works by the same author when un-
derstanding a story, or research on authorship at-
tribution (e.g., Stamatatos, 2009).
</bodyText>
<sectionHeader confidence="0.999266" genericHeader="method">
9 Future Work
</sectionHeader>
<bodyText confidence="0.99999476">
We plan to use this dataset to evaluate approaches
for machine comprehension, but are making it
available now so that others may do the same. If
MCTest is used we will collect more story sets and
will continue to refine the collection process. One
interesting research direction is ensuring that the
questions are difficult enough to challenge state-of-
the-art techniques as they develop. One idea for
this is to apply existing techniques automatically
during story set creation to see whether a question
is too easily answered by a machine. By requiring
authors to create difficult questions, each data set
will be made more and more difficult (but still an-
swerable by humans) as the state-of-the-art meth-
ods advance. We will also experiment with timing
the raters as they answer questions to see if we can
find those that are too easy for people to answer.
Removing such questions may increase the diffi-
culty for machines as well. Additionally, any di-
vergence between how easily a person answers a
question vs. how easily a machine does may point
toward new techniques for improving machine
comprehension; we plan to conduct research in this
direction as well as make any such data available
for others.
</bodyText>
<sectionHeader confidence="0.995527" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.9995705">
We present the MCTest dataset in the hope that it
will help spur research into the machine compre-
hension of text. The metric (the accuracy on the
question sets) is clearly defined, and on that metric,
lexical baseline algorithms only attain approxi-
mately 58% correct on test data (the MC500 set) as
</bodyText>
<page confidence="0.993644">
201
</page>
<bodyText confidence="0.999979285714286">
opposed to the 100% correct that the majority of
crowd-sourced judges attain. A key component of
MCTest is the scalable design: we have shown that
data whose quality approaches that of expertly cu-
rated data can be generated using crowd sourcing
coupled with expert correction of worker-identified
errors. Should MCTest prove useful to the com-
munity, we will continue to gather data, both to
increase the corpus size, and to keep the test sets
fresh. The data is available at http://research.micro-
soft.com/mct and any submitted results will be
posted there too. Because submissions will be re-
quested to include the score for each test item, re-
searchers will easily be able to compare their
systems with those of others, and investigation of
ensembles comprised of components from several
different teams will be straightforward. MCTest
also contains supplementary material that re-
searchers may find useful, such as worker accura-
cies on a grammar test and crowd-sourced
measures of the quality of their stories.
</bodyText>
<sectionHeader confidence="0.9982" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999876333333333">
We would like to thank Silviu Cucerzan and Lucy
Vanderwende for their help with converting ques-
tions to statements and other useful discussions.
</bodyText>
<sectionHeader confidence="0.999111" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995524875">
M. Agarwal and P. Mannem. 2011. Automatic Gap-fill
Question Generation from Text Books. In Proceed-
ings of the Sixth Workshop on Innovative Use of NLP
for Building Educational Applications, 56–64.
E. Breck, M. Light, G.S.Mann, E. Riloff, B. Brown, P.
Anand, M. Rooth M. Thelen. 2001. Looking under
the hood: Tools for diagnosing your question answer-
ing engine. In Proceedings of the workshop on Open-
domain question answering, 12, 1-8.
E. Charniak. 1972. Toward a Model of Children’s Story
Comprehension. Technical Report, 266, MIT Artifi-
cial Intelligence Laboratory, Cambridge, MA.
P. Clark, P. Harrison, and X. Yao. An Entailment-Based
Approach to the QA4MRE Challenge. 2012. In Pro-
ceedings of the Conference and Labs of the Evalua-
tion Forum (CLEF) 2012.
S. Cucerzan and E. Agichtein. 2005. Factoid Question
Answering over Unstructured and Structured Content
on the Web. In Proceedings of the Fourteenth Text
Retrieval Conference (TREC).
I. Dagan, O. Glickman, and B. Magnini. 2006. The
PASCAL Recognising Textual Entailment Chal-
lenge. In J. Quiñonero-Candela, I. Dagan, B. Magni-
ni, F. d&apos;Alché-Buc (Eds.), Machine Learning
Challenges. Lecture Notes in Computer Science, Vol.
3944, pp. 177-190, Springer.
D. Goldwasser, R. Reichart, J. Clarke, D. Roth. 2011.
Confidence Driven Unsupervised Semantic Parsing.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, 1486-1495.
E. Grois and D.C. Wilkins. 2005. Learning Strategies
for Story Comprehension: A Reinforcement Learning
Approach. In Proceedings of the Twenty Second In-
ternational Conference on Machine Learning, 257-
264.
S.M. Harabagiu, S.J. Maiorano, and M.A. Pasca. 2003.
Open-Domain Textual Question Answering Tech-
niques. Natural Language Engineering, 9(3):1-38.
Cambridge University Press, Cambridge, UK.
L. Hirschman, M. Light, E. Breck, and J.D. Burger.
1999. Deep Read: A Reading Comprehension Sys-
tem. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics (ACL),
325-332.
J. Horton and L. Chilton. 2010. The labor economics of
paid crowdsourcing. In Proceedings of the 11th ACM
Conference on Electronic Commerce, 209-218.
V. Kuperman, H. Stadthagen-Gonzalez, M. Brysbaert.
2012. Age-of-acquisition ratings for 30,000 English
words. Behavior Research Methods, 44(4):978-990.
J.L. Leidner, T. Dalmas, B. Webber, J. Bos, C. Grover.
2003. Automatic Multi-Layer Corpus Annotation for
Evaluating Question Answering Methods:
CBC4Kids. In Proceedings of the 3rd International
Workshop on Linguistically Interpreted Corpora.
E.T. Mueller. 2010. Story Understanding Resources.
http://xenia.media.mit.edu/~mueller/storyund/storyre
s.html.
G. Paolacci, J. Chandler, and P. Iperirotis. 2010. Run-
ning experiments on Amazon Mechanical Turk.
Judgment and Decision Making. 5(5):411-419.
E. Stamatatos. 2009. A survey of modern authorship
attribution methods. J. Am. Soc. Inf. Sci., 60:538–
556.
A. Stern and I. Dagan. 2011. A Confidence Model for
Syntactically-Motivated Entailment Proofs. In Pro-
ceedings of Recent Advances in Natural Language
Processing (RANLP).
L.R. Tang and R.J. Mooney. 2001. Using Multiple
Clause Constructors in Inductive Logic Programming
for Semantic Parsing. In Proceedings of the 12th Eu-
ropean Conference on Machine Learning (ECML),
466-477.
G. Tur, D. Hakkani-Tur, and L.Heck. 2010. What is left
to be understood in ATIS? Spoken Language Tech-
nology Workshop, 19-24.
E.M. Voorhees and D.M. Tice. 1999. The TREC-8
Question Answering Track Evaluation. In Proceed-
ings of the Eighth Text Retrieval Conference (TREC-
8).
</reference>
<page confidence="0.972302">
202
</page>
<reference confidence="0.999496105263158">
B. Wellner, L. Ferro, W. Greiff, and L. Hirschman.
2005. Reading comprehension tests for computer-
based understand evaluation. Natural Language En-
gineering, 12(4):305-334. Cambridge University
Press, Cambridge, UK.
J.M. Zelle and R.J. Mooney. 1996. Learning to Parse
Database Queries using Inductive Logic Program-
ming. In Proceedings of the Thirteenth National
Conference on Artificial Intelligence (AAAI), 1050-
1055.
L.S. Zettlemoyer and M. Collins. 2009. Learning Con-
text-Dependent Mappings from Sentences to Logical
Form. In Proceedings of the 47th Annual Meeting of
the Association for Computation Linguistics (ACL),
976-984.
G. Zweig and C.J.C. Burges. 2012. A Challenge Set for
Advancing Language Modeling. In Proceedings of
the Workshop on the Future of Language Modeling
for HLT, NAACL-HLT.
</reference>
<page confidence="0.999197">
203
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99738">MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text</title>
<author confidence="0.967292">Matthew</author>
<affiliation confidence="0.943477">Microsoft</affiliation>
<address confidence="0.8787075">One Microsoft Redmond, WA</address>
<email confidence="0.999746">mattri@microsoft.com</email>
<author confidence="0.996041">J C Christopher</author>
<affiliation confidence="0.917267">Microsoft</affiliation>
<address confidence="0.884251">One Microsoft Redmond, WA</address>
<email confidence="0.9996">cburges@microsoft.com</email>
<author confidence="0.982026">Erin</author>
<affiliation confidence="0.908038">Microsoft</affiliation>
<address confidence="0.8817865">One Microsoft Redmond, WA</address>
<email confidence="0.999934">erinren@microsoft.com</email>
<abstract confidence="0.990795301369863">We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text. 1 Reading Comprehension A major goal for NLP is for machines to be able to understand text as well as people. Several research disciplines are focused on this problem: for example, information extraction, relation extraction, semantic role labeling, and recognizing textual entailment. Yet these techniques are necessarily evaluated individually, rather than by how much they advance us towards the end goal. On the other hand, the goal of semantic parsing is the machine comprehension of text (MCT), yet its evaluation requires adherence to a specific knowledge representation, and it is currently unclear what the best representation is, for open-domain text. We believe that it is useful to directly tackle the top-level task of MCT. For this, we need a way to measure progress. One common method for evalusomeone’s of text is by giving them a multiple-choice reading comprehension test. This has the advantage that it is objectively gradable (vs. essays) yet may test a range of abilities such as causal or counterfactual reasoning, inference among relations, or just basic understanding of the world in which the passage is set. Therefore, we propose a multiple-choice reading comprehension task as a way to evaluate progress on MCT. We have built a reading comprehension dataset containing 500 fictional stories, with 4 multiple choice questions per story. It was built using methods which can easily scale to at least 5000 stories, since the stories were created, and the curation was done, using crowd sourcing almost entirely, at a total of $4.00 per story. We plan to periodically update the dataset to ensure that methods are not overfitting to the existing data. The dataset is open-domain, yet restricted to concepts and words that a 7 year old is expected to understand. task is still beyond capability of today’s computers and algorithms.</abstract>
<note confidence="0.538925333333333">193 of the 2013 Conference on Empirical Methods in Natural Language pages 193–203, Washington, USA, 18-21 October 2013. Association for Computational Linguistics</note>
<abstract confidence="0.988791561205274">By restricting the concept space, we gain the difficulty of being an open-domain problem, without the full complexity of the real world (for example, there will be no need for the machine to understand politics, technology, or to have any domain specific expertise). The multiple choice task avoids ambiguities (such as when the task is to find a sentence that best matches a question, as in some early reading comprehension tasks: see Section 2), and also avoids the need for additional grading, such as is needed in some TREC tasks. The stories were chosen to be fictional to focus work on finding the answer in the story itself, rather than in knowledge repositories such as Wikipedia; the goal is to build technology that actually understands stories and paragraphs on a deep level (as opposed to using information retrieval methods and the redundancy of the web to find the answers). We chose to use crowd sourcing, as opposed to, for example, contracting teachers or paying for existing standardized tests, for three reasons, namely: (1) scalability, both for the sizes of datasets we can provide, and also for the ease of regularly refreshing the data; (2) for the variety in story-telling that having many different authors brings; and (3) for the free availability that can only result from providing non-copyrighted data. The content is freely available at http://research.microsoft.com/mct, and we plan to use that site to track published results and provide other resources, such as labels of various kinds. 2 Previous Work The research goal of mapping text to meaning representations in order to solve particular tasks has a long history. DARPA introduced the Airline Trav- Information System (ATIS) the early 90’s: there the task was to slot-fill flight-related information by modeling the intent of spoken language (see Tur et al., 2010, for a review). This data continues to be a used in the semantic modeling community (see, for example, Zettlemoyer and Collins, 2009). The Geoquery database contains 880 geographical facts about the US and has played a similar role for written (as opposed to spoken) natural language queries against a database (Zelle and Mooney, 1996) and it also continues to spur research (see for example Goldwasser et al., 2011), as does the similar Jobs database, which provides mappings of 640 sentences to a listing of jobs (Tang and Mooney, 2001). More recently, Zweig and Burges (2012) provided a set of 1040 sentences that comprise an SAT-style multiple choice sentence completion task. The idea of using story-based reading comprehension questions to evaluate methods for machine reading itself goes back over a decade, when Hirschmann et al. (1999) showed that a bag of words approach, together with some heuristic linguistic modeling, could achieve 40% accuracy for the task of picking the sentence that best matches query for what / when / where / questions, on a small reading comprehension dataset from Remedia. This dataset spurred several research efforts, for example using reinforcement learning (Grois and Wilkins, 2005), named entity resolution (Harabagiu et al., 2003) and mapping questions and answers to logical form (Wellner et al., 2006). Work on story understanding itself goes back much further, to 1972, when Charniak proposed using a background model to answer quesabout children’s stories. the TREC (and TAC) Question Answering tracks (e.g., Voorhees and Tice, 1999) aim to evaluate systems on their ability to answer factual questions such as is the Taj The QA4MRE task also aims to evaluate machine reading systems through question answering (e.g., Clark et al., 2012). Earlier work has also aimed at controlling the scope by the text to stories: Breck et al. (2001) collected 75 stories from the Canadian Broadcasting Corporation’s web site for children, and generated 650 questions for them manually, where each question was answered by a sentence in the text. Leidner et al. (2003) both enriched the CBC4kids data by adding several layers of annotation (such as semantic and POS tags), and measured QA performance as a function of question difficulty. For a further compendium of resources related to the story comprehension task, see Mueller (2010). The task proposed here differs from the above work in several ways. Most importantly, the data collection is scalable: if the dataset proves sufficiently useful to others, it would be straightforward to gather an order of magnitude more. Even the dataset size presented here is an order of magnitude larger than the Remedia or the CBC4kids data and many times larger than QA4MRE. Second, the multiple choice task presents less ambiguity (and is consequently easier to collect data for) than the 194 James the Turtle was always getting in trouble. Sometimes he&apos;d reach into the freezer and empty out all the food. Other times he&apos;d sled on the deck and get a splinter. His aunt Jane tried as hard as she could to keep him out of trouble, but he was sneaky and got into lots of trouble behind her back. One day, James thought he would go into town and see what kind of trouble he could get into. He went to the grocery store and pulled all the pudding off the shelves and ate two jars. Then he walked to the fast food restaurant and ordered 15 bags of fries. He didn&apos;t pay, and instead headed home. His aunt was waiting for him in his room. She told James that she loved him, but he would have to start acting like a well-behaved turtle. After about a month, and after getting into lots of trouble, James finally made up his mind to be a better turtle. 1) What is the name of the trouble making turtle? A) Fries B) Pudding C) James D) Jane 2) What did James pull off of the shelves in the grocery store? A) pudding B) fries C) food D) splinters 3) Where did James go after he went to the grocery store? A) his deck B) his freezer C) a fast food restaurant D) his room 4) What did James do after he ordered the fries? A) went to the grocery store B) went home without paying C) ate them D) made up his mind to be a better turtle Figure 1. Sample Story and Questions (chosen randomly from MC500 train set). task of finding the most appropriate sentence, and may be automatically evaluated. Further, our stories are fictional, which means that the information to answer the question is contained only in the story itself (as opposed to being able to directly leverage knowledge repositories such as Wikipedia). This design was chosen to focus the task on the machine understanding of short passages, rather than the ability to match against an existing knowledge base. In addition, while in the CBC4kids data each answer was a sentence from the story, here we required that approximately half of the questions require at least two sentences from the text to answer; being able to control complexity in this way is a further benefit of using multiple choice answers. Finally, as explained in Section 1, the use of free-form input makes the problem open domain (as opposed to the ATIS, Geoquery and Jobs data), leading to the hope that solutions to the task presented here will be easier to apply to novel, unrelated tasks. 3 Generating the Stories and Questions Our aim was to generate a corpus of fictional story that could be scaled with as little expert input as possible. Thus, we designed the process to be gated by cost, and keeping the costs low was a high priority. Crowd-sourcing seemed particularly appropriate, given the nature of the task, so we to use Amazon Mechanical (AMT). over 500,000 it provides the work force required to both achieve scalability and, equally importantly, to provide diversity in the stories and types of questions. We restricted our task AMT workers residing in the United States. The average worker is 36 years old, more educated than the United States population in general (Paolacci et al., 2010), and the majority of workers are female. 3.1 The Story and Questions Workers were instructed to write a short (150-300 words) fictional story, and to write as if for a child in grade school. The choice of 150-300 was made to keep the task an appropriate size for workers while still allowing for complex stories and questions. The workers were free to write about any topic they desired (as long as it was appropriate for a young child), and so there is a wide range, including vacations, animals, school, cars, eating, gardening, fairy tales, spaceships, and cowboys. 1We use the term “story set” denote the fictional story together with its multiple choice questions, hypothetical answers, and correct answer labels. 195 Workers were also asked to provide four reading comprehension questions pertaining to their story and, for each, four multiple-choice answers. Comup with incorrect alternatives is a difficult task (see, e.g., Agarwal, 2011) but workwere requested to provide “reasonable” rect answers that at least include words from the story so that their solution is not trivial. For examfor the question is the name of the only one of the four answers occurs in the story, then that answer must be the correct one. Finally, workers were asked to design their questions and answers such that at least two of the four questions required multiple sentences from the story to answer them. That is, for those questions it should not be possible to find the answer in any individual sentence. The motivation for this was to ensure that the task could not be fully solved using lexical techniques, such as word matching, alone. Whilst it is still possible that a sophisticated lexical analysis could completely solve the task, requiring that answers be constructed from at least two different sentences in the story makes this much less likely; our hope is that the solution will instead require some inference and some form of limited reasoning. This hope rests in part upon the observation that standardized reading comprehension tests, whose goal after all is to test comprehension, generally avoid questions that can be answered by reading a single sentence. 3.2 Automatic Validation Besides verifying that the story and all of the questions and answers were provided, we performed the following automatic validation before allowing the worker to complete the task: The lowercase words in the story, questions, and answers were stemmed and checked against a vocabulary list of approximately 8000 words that a 7-year old is likely to know (Kuperman et al., 2012). Any words not on the list were highlighted in red as the worker typed, and the task could not be submitted unless all of the words satisfied this vocabulary criterion. To allow the use of arbitrary proper nouns, capitalized words were not checked against the vocabulary list. questions: described earlier, we required that at least two of the questions need multiple sentences to answer. Workers were simply asked to mark whether a question needs one or multiple sentences and we required that at least are marked as 3.3 The Workers Workers were required to reside in the United States and to have completed 100 HITs with an 95% approval The median worker took 22 minutes to complete the task. We paid workers $2.50 per story set and allowed each to do a maximum of 8 tasks (5 in MC500). We did not experiment with paying less, but this rate amounts to $6.82/hour, which is approximately the rate paid by other writing tasks on AMT at the time, though is also significantly higher than the median wage of $1.38 found in 2010 (Horton and Chilton, 2010). Workers could optionally leave feedback on task, which was overwhelmingly positive most frequent non-stopword in the comments was and frequent phrase was only negative comments (in &lt;1% of submissions) were when the worker felt that a particular word should have been on the allowed vocabulary list. Given the positive feedback, it may be possible to pay less if we collect more data in the future. We did not enforce story length constraints, but some workers interpreted our suggestion that the story be 150-300 words as a hard constraint, and some asked to be able to write a longer story. The MCTest corpus contains two sets of stories, named MC160 and MC500, and containing 160 and 500 stories respectively. MC160 was gathered first, then some improvements were made before gathering MC500. We give details on the differences between these two sets below. 3.4 MC160: Manually Curated for Quality In addition to the details described above, MC160 workers were given a target elementary grade school level (1-4) and a sample story matching that The intent was to produce a set of stories and questions that varied in difficulty so that research work can progress grade-by-grade if needed. However, we found little difference between grades in the corpus.. After gathering the stories, we manually curated the MC160 corpus by reading each story set and latter two are the default AMT requirements. http://www.englishforeveryone.org/. 196 1. We went to visit the Smith’s at their house. 2. I altered their suits for them. 3. You&apos;re car is very old. 4. Jim likes to run, hike, and going kayaking. 5. He should of come to work on time. 6. I think its best to wash lots of apples. 7. Are people who write &amp;quot;ping&amp;quot; thinking of submarines? 8. Smoke filled the room, making it hard to breathe. 9. Alert yet aloof that&apos;s you. 10. They wanted they&apos;re money back. 11. Hawks and eagles like to fly high in the sky. 12. Don&apos;t let her wear them down. 13. The cat particularly liked the greasy plate. 14. The company is less successful because we have less employees. 15. The hamster belongs to Sam and I. 16. No one landed on the air strip today. 17. He was very effected by her tears. 18. You are a tired piece of toast, metaphorically speaking. 19. Anne plays bass and sings. 20. Him and me met at the park. Figure 2. Grammar test for qualifying workers. correcting errors. The most common mistakes were grammatical, though occasionally questions and/or answers needed to be fixed. 66% of the stories have at least one correction. We provide both the curated and original corpuses in order to allow research on reading comprehension in the presence of grammar, spelling, and other mistakes. 3.5 MC500: Adding a Grammar Test Though the construction of MC160 was successful, it requires a costly curation process which will not scale to larger data sets (although the curation was useful, both for improving the design of MC500, and for assessing the effectiveness of automated curation techniques). To more fully automate the process, we added two more stages: (1) A grammar test that automatically pre-screens workers for writing ability, and (2) a second Mechanical Turk task whereby new workers take the reading comprehension tests and rate their quality. We will discuss stage (2) in the next section. The grammar test consisted of 20 sentences, half of which had one grammatical error (see Figure 2). The incorrect sentences were written using comerrors such as using inplurality, incorrect use of tense, Quality About (1-5) animals No Grammar Test 3.2 73% Grammar Test 4.3 30% Table 1. Pre-screening workers using a grammar test improves both quality and diversity of stories. Both differences are significant using the two-tailed t-test (p&lt;0.05 for quality and p&lt;0.01 for animals). I etc. Workers were required to indicate for each sentence whether it was grammatically correct or not, and had to pass with at least 80% accuracy in order to qualify for the task. The 80% threshold was chosen to trade off worker quality with the rate at which the tasks would be completed; initial experiments using a threshold of 90% indicated that collecting 500 stories would take many weeks instead of days. Note that each worker is allowed to write at most 5 stores, so we required at least 100 workers to pass the qualification test. To validate the use of the qualification test, we 30 stories requiring the test and 30 stories without. We selected a random set of 20 stories (10 from each), hid their origin, and then graded the overall quality of the story and quesfrom 1-5, meaning not attempt to non-minor only and no respectively. Results are shown in Table 1. The difference is statistically significant (p&lt;0.05, using the twot-test). The were also more diverse, with fewer of them about animals (the most common topic). Based on our experience curating MC160, we also made the following modifications to the task. In order to eliminate trivially-answerable questions, we required that each answer be unique, and that either the correct answer did not appear in the story or, if it did appear, that at least two of the incorrect answers also appeared in the story. This is to prevent questions that are trivially answered by checking which answer appears in the story. The condition on whether the correct answer appears is to allow questions as many candies did Susan where the total may never appear in the story, even though the information needed to derive it does. An answer is considered to appear in the story if at least half (rounded down) of its non-stopword 197 terms appear in the story (ignoring word endings). This check is done automatically and must be satisfied before the worker is able to complete the task. Workers could also bypass the check if they felt it was incorrect, by adding a special term to their answer. We were also concerned that the sample story might bias the workers when writing the story set, particularly when designing questions that require multiple sentences to answer. So, we removed the sample story and grade level from the task. Finally, in order to encourage more diversity of we added a set of 15 nouns chosen at random from the allowed vocabuset. Workers were asked to using one or more of the terms in their story, but use of the words was strictly optional. On average, workers used 3.9 of the creativity terms in their stories. 4 Rating the Stories and Questions In this section we discuss the crowd-sourced rating of story sets. We wished to ensure story set quality despite the fact that MC500 was only minimally manually curated (see below). Pre-qualifying workers with a grammar test was one step of this process. The second step was to have additional workers on Mechanical Turk both evaluate each story and take its corresponding test. Each story was evaluated in this way by 10 workers, each of whom provided scores for each of agegrammaticality and story clarity When answering the four reading comprehension questions, workers could mark a question as Each story set was rated by 10 workers who were each paid $0.15 per set. Since we know the purportedly correct answer, we can estimate worker quality by measuring what fraction of questions that worker got right. Workers with less than 80% accuracy (ignoring those questions marked as unclear) were removed from the set. This constituted just 4.1% of the raters and 4.2% of the judgments (see Figure 3). Only one rater appeared to be an intentional spammer, answering 1056 questions with only 29% accuracy. The others primarily judged only one story. Only one worker fell between, answering 336 questions with just 75% accuracy. Figure 3. Just 4.1% of raters had an accuracy below 80% (constituting 4.2% of the judgments). For the remaining workers (those who achieved at least 80% accuracy), we measured median story appropriateness, grammar, and clarity. For each category, stories for which less than half of the were the best possible (e.g., clarity) were inspected and optionally removed from the data set. This required inspecting 40 (&lt;10%) of the stories, only 2 of which were deemed poor enough to be removed (both of which had over half of the ratings all the way at the bottom end of the scale, indicating we could potentially have inspected many fewer stories with the same results). We also inspected questions for which at least 5 workers answered incorrectly, or answered In total, 29 questions were inspected. 5 were fixed by changing the question, 8 by changing the answers, 2 by changing both, 6 by changing the story, and 8 were left unmodified. Note that while not fully automated, this process of inspecting stories and repairing questions took one person one day, so is still scalable to at least an order of magnitude more stories. 5 Dataset Analysis In Table 2, we present results demonstrating the value of the grammar test and curation process. As expected, manually curating MC160 resulted in increased grammar quality and percent of questions answered correctly by raters. The goal of MC500 was to find a more scalable method to achieve the same quality as the curated MC160. As Table 2 shows, the grammar test improved story grammar quality from 1.70 to 1.77 (both uncurat- The rating and one-day curation process in- 198 Set AgeAp Clarity Grammar Correct 160 1.88 1.63 1.70 95.3 500 1.92 1.65 1.77 95.3 500 curated 1.94 1.71 1.79 96.9 160 curated 1.91 1.67 97.7 Table 2. Average age appropriateness, story clarity, grammar quality (0-2, with 2 being best), and percent of questions answered correctly by raters, for the original and curated versions of the data. Bold indicates statistical significance vs. the original version of the same set, the two-sample t-test with unequal variance. The indicates the only statistical difference between 500 curated and 160 curated. Corpus Stories time Average Words Per: Story Question Answer MC160 160 26 min 204 8.0 3.4 MC500 500 20 min 212 7.7 3.4 Table 3. Corpus statistics for MC160 and MC500. creases this to 1.79, whereas a fully manual curation results in a score of 1.84. Curation also improved the percent of questions answered correctly for both MC160 and MC500, but, unlike with grammar, there is no significant difference between the two curated sets. Indeed, the only statistically significant difference between the two is in grammar. So, the MC500 grammar test and curation process is a very scalable method for collecting stories of nearly the quality of the costly manual curation of MC160. We also computed correlations between these measures of quality and various factors such as story length and time spent writing the story. On MC500, there is a mild correlation between a worker’s grammar test score and the judged quality of worker’s story of 0.24). Interestingly, this relation disappeared once MC500 was curated, likely due to repairing the stories with the worst grammar. On MC160, there is a mild correlation between the clarity and the number of words in the question and answer (0.20 and 0.18). All other correlations were below 0.15. These factors could be integrated into an estimate for age-appropriateness, clarity, and grammar, potentially reducing the need for raters. Table 3 provides statistics on each corpus. MC160 and MC500 are similar in average number of words per story, question, and answer, as well as the median writing time. The most commonly used Baseline Algorithms set of passage words word in set of words in question set of words in answers and set of stop words := w); := log (1 + 1</abstract>
<note confidence="0.6732173">C(w) 1 Window 1 to 4 0 end for return 2 Based 1 to 4 \U PW) \ � =0 I=0</note>
<abstract confidence="0.914791222222222">else = q, (q, a) the minimum number of between an occurrence of an of plus one. end if end for A.</abstract>
<title confidence="0.91769225">Algorithm SW Return Algorithm SW+D Return</title>
<abstract confidence="0.973604254385965">Figure 4. The two lexical-based algorithms used for the baselines. in MC500 are: The stories vary widely in theme. The first 10 stories of the randomly-ordered MC500 set are about: travelling to Miami to visit friends, waking up and saying hello to pets, a bully on a schoolyard, visiting a farm, collecting insects at house, planning a birthday party, selecting clothes for a school dance, keeping animals from eating your ice cream, animals ordering food, and adventures of a boy and his dog. PI 199 MC160 Train and 240 Q’s 400 Q’s SW SW+D SW SW+D Single 59.46 68.11 64.29 75.89 Multi 59.53 67.44 48.44 57.81 All 59.50 67.75 55.83 66.25 Table 4. Percent correct for the multiple choice questions for MC160. SW: sliding window algorithm. SW+D: combined results with sliding window and distance based algorithms. Single/Multi: questions marked by worker as requiring a single/multiple sentence(s) to answer. All differences between SW and SW+D are significant (p&lt;0.01 using the two-tailed paired t-test). MC500 Train and Dev: 600 Q’s All SW SW+D SW SW+D SW+D Single 55.13 61.77 51.10 57.35 60.44 Multi 49.80 55.28 51.83 56.10 55.53 All 52.21 58.21 51.50 56.67 57.75 Table 5. Percent correct for the multiple choice questions for MC500, notation as above. All differences between SW and SW+D are significant (p&lt;0.01, tested as above). We randomly divided MC160 and MC500 into train, development, and test sets of 70, 30, and 60 stories and 300, 50, and 150 stories, respectively. 6 Baseline System and Results We wrote two baseline systems, both using only simple lexical features. The first system used a sliding window, matching a bag of words constructed from the question and hypothesized answer to the text. Since this ignored long range dependencies, we added a second, word-distance based algorithm. The distance-based score was simply subtracted from the window-based score to arrive at the final score (we tried scaling the distance score before subtraction but this did not improve results on the MC160 train set). The algorithms are summarized in Figure 4. A coin flip is used to break ties. The use of inverse word counts was inspired by TF-IDF. Results for MC160 and MC500 are shown in Table 4 and Table 5. The MC160 train and development sets were used for tuning. The baseline algorithm was authored without seeing any portion of MC500, so both the MC160 test set and all of MC160 Test MC500 Test Baseline (SW+D) 66.25 56.67 RTE 53.52 Combined 67.60 Table 6. Percent correct for MC160 and MC500 test The statistical significance vs. baseline (p&lt;0.01 using the two-tailed paired t-test). MC160 combined vs. baseline has p-value 0.063. MC500 were used for testing (although we nevertheless report results on the train/test split). Note that adding the distance based algorithm improved accuracy by approximately 10% absolute on MC160 and approximately 6% on MC500. Overall, error rates on MC500 are higher than on MC160, which agrees with human performance Table 2), suggesting MC500’s questions are more difficult. 7 Recognizing Textual Entailment Results also tried using a textual system to answer MCTest questions. The goal of RTE (Dagan et al., 2005) is to determine whether a given statement can be inferred from a particular text. We can cast MCTest as an RTE task by converting each question-answer pair into a statement, and then selecting the answer whose statement has the highest likelihood of being entailed by the story. For example, in the sample story given in Figure 1, the second question can be converted into four statements (one for each answer), and the RTE system should select the “James pulled pudding of the shelves the grocery the most likely one. For converting question-answer pairs to statements, we used the rules employed in a web-based question answering system (Cucerzan and Agichtein, 2005). For RTE, we used BIUTEE (Stern and Dagan, 2011), which performs better than the median system in the past four RTE competitions. We ran BIUTEE both in its default configuration, as well as with its optional additional data sources (FrameNet, ReVerb, DIRT, and others as found on the BIUTEE home page). The default configuration performed better so we present its results here. The results in Table 6 show that the RTE method performed worse than the baseline. 200 We also combined the baseline and RTE system by training BIUTEE on the train set and using the development set to optimize a linear combination of BIUTEE with the baseline; the combined system outperforms either component system on MC500. It is possible that with some tuning, an RTE system will outperform our baseline system. Nevertheless, these RTE results, and the performance of the baseline system, both suggest that the reading comprehension task described here will not be trivially solved by off-the-shelf techniques. 8 Making Data and Results an Ongoing Resource Our goal in constructing this data is to encourage research and innovation in the machine comprehension of text. Thus, we have made both MC160 and MC500 freely available for download at http://research.microsoft.com/mct. To our knowledge, these are the largest copyright-free reading comprehension data sets publicly available. To further encourage research on these data, we will be continually updating the webpage with the bestknown published results to date, along with pointers to those publications. One of the difficulties in making progress on a particular task is implementing previous work in order to apply improvements to it. To mitigate this difficulty, we are encouraging researchers who use the data to (optionally) provide per-answer scores from their system. Doing so has three benefits: (a) a new system can be measured in the context of the errors made by the previous systems, allowing each research effort to incrementally add useful functionality without needing to also re-implement the current state-of-the-art; (b) it allows system performance to be measured using paired statistical testing, which will substantially increase the ability to determine whether small improvements are significant; and (c) it enables researchers to perform error analysis on any of the existing systems, simplifying the process of identifying and tackling common sources of error. We will also periodically ensemble the known systems using standard machine learning techniques and make those results available as well (unless the existing state-of-theart already does such ensembling). The released data contains the stories and questions, as well as the results from workers who rated the stories and took the tests. The latter may be used, for example, to measure machine performance vs. human performance on a per-question basis (i.e., does your algorithm make similar mistakes to humans?), or vs. the judged clarity of each story. The ratings, as well as whether a question needs multiple sentences to answer, should typically only be used in evaluation, since such information is not generally available for most text. We will also provide an anonymized author id for each story, which could allow additional research such as using other works by the same author when understanding a story, or research on authorship attribution (e.g., Stamatatos, 2009). 9 Future Work We plan to use this dataset to evaluate approaches for machine comprehension, but are making it available now so that others may do the same. If MCTest is used we will collect more story sets and will continue to refine the collection process. One interesting research direction is ensuring that the questions are difficult enough to challenge state-ofthe-art techniques as they develop. One idea for this is to apply existing techniques automatically during story set creation to see whether a question is too easily answered by a machine. By requiring authors to create difficult questions, each data set will be made more and more difficult (but still answerable by humans) as the state-of-the-art methods advance. We will also experiment with timing the raters as they answer questions to see if we can find those that are too easy for people to answer. Removing such questions may increase the difficulty for machines as well. Additionally, any divergence between how easily a person answers a question vs. how easily a machine does may point toward new techniques for improving machine comprehension; we plan to conduct research in this direction as well as make any such data available for others. 10 Conclusion We present the MCTest dataset in the hope that it will help spur research into the machine comprehension of text. The metric (the accuracy on the question sets) is clearly defined, and on that metric, lexical baseline algorithms only attain approximately 58% correct on test data (the MC500 set) as 201 opposed to the 100% correct that the majority of crowd-sourced judges attain. A key component of MCTest is the scalable design: we have shown that data whose quality approaches that of expertly curated data can be generated using crowd sourcing coupled with expert correction of worker-identified errors. Should MCTest prove useful to the community, we will continue to gather data, both to increase the corpus size, and to keep the test sets fresh. The data is available at http://research.microsoft.com/mct and any submitted results will be posted there too. Because submissions will be requested to include the score for each test item, researchers will easily be able to compare their systems with those of others, and investigation of ensembles comprised of components from several different teams will be straightforward. MCTest also contains supplementary material that researchers may find useful, such as worker accuracies on a grammar test and crowd-sourced measures of the quality of their stories. Acknowledgments We would like to thank Silviu Cucerzan and Lucy Vanderwende for their help with converting questions to statements and other useful discussions.</abstract>
<title confidence="0.5785256">References M. Agarwal and P. Mannem. 2011. Automatic Gap-fill Generation from Text Books. In Proceedings of the Sixth Workshop on Innovative Use of NLP Building Educational</title>
<note confidence="0.921912411764706">E. Breck, M. Light, G.S.Mann, E. Riloff, B. Brown, P. Anand, M. Rooth M. Thelen. 2001. Looking under the hood: Tools for diagnosing your question answerengine. In of the workshop on Openquestion answering, 1-8. E. Charniak. 1972. Toward a Model of Children’s Story 266, MIT Artificial Intelligence Laboratory, Cambridge, MA. P. Clark, P. Harrison, and X. Yao. An Entailment-Based to the QA4MRE Challenge. 2012. In Proceedings of the Conference and Labs of the Evalua- Forum (CLEF) S. Cucerzan and E. Agichtein. 2005. Factoid Question Answering over Unstructured and Structured Content the Web. In of the Fourteenth Text Conference I. Dagan, O. Glickman, and B. Magnini. 2006. The</note>
<title confidence="0.441313">PASCAL Recognising Textual Entailment Chal-</title>
<author confidence="0.863602">In J Quiñonero-Candela</author>
<author confidence="0.863602">I Dagan</author>
<author confidence="0.863602">B Magni-</author>
<affiliation confidence="0.303512">ni, F. d&apos;Alché-Buc (Eds.), Machine Learning Challenges. Lecture Notes in Computer Science, Vol.</affiliation>
<address confidence="0.563793">3944, pp. 177-190, Springer.</address>
<note confidence="0.899998">D. Goldwasser, R. Reichart, J. Clarke, D. Roth. 2011. Confidence Driven Unsupervised Semantic Parsing. of the Annual Meeting of the Asfor Computational 1486-1495.</note>
<title confidence="0.753062">E. Grois and D.C. Wilkins. 2005. Learning Strategies for Story Comprehension: A Reinforcement Learning</title>
<note confidence="0.96808435">In of the Twenty Second In- Conference on Machine 257- 264. S.M. Harabagiu, S.J. Maiorano, and M.A. Pasca. 2003. Open-Domain Textual Question Answering Tech- Language 9(3):1-38. Cambridge University Press, Cambridge, UK. L. Hirschman, M. Light, E. Breck, and J.D. Burger. 1999. Deep Read: A Reading Comprehension Sys- In of the Annual Meeting of the for Computational Linguistics 325-332. J. Horton and L. Chilton. 2010. The labor economics of crowdsourcing. In of the ACM on Electronic 209-218. V. Kuperman, H. Stadthagen-Gonzalez, M. Brysbaert. 2012. Age-of-acquisition ratings for 30,000 English Research 44(4):978-990. J.L. Leidner, T. Dalmas, B. Webber, J. Bos, C. Grover. 2003. Automatic Multi-Layer Corpus Annotation for</note>
<title confidence="0.647687">Evaluating Question Answering Methods:</title>
<note confidence="0.991777">In of the 3rd International Workshop on Linguistically Interpreted Corpora. E.T. Mueller. 2010. Story Understanding Resources.</note>
<web confidence="0.729716">http://xenia.media.mit.edu/~mueller/storyund/storyre</web>
<note confidence="0.7719735">G. Paolacci, J. Chandler, and P. Iperirotis. 2010. Running experiments on Amazon Mechanical Turk. and Decision 5(5):411-419. E. Stamatatos. 2009. A survey of modern authorship methods. Am. Soc. Inf. 556.</note>
<title confidence="0.737165285714286">A. Stern and I. Dagan. 2011. A Confidence Model for Entailment Proofs. In Proceedings of Recent Advances in Natural Language L.R. Tang and R.J. Mooney. 2001. Using Multiple Clause Constructors in Inductive Logic Programming Semantic Parsing. In of the Eu- Conference on Machine Learning</title>
<note confidence="0.955833708333333">466-477. G. Tur, D. Hakkani-Tur, and L.Heck. 2010. What is left be understood in ATIS? Language Tech- 19-24. E.M. Voorhees and D.M. Tice. 1999. The TREC-8 Answering Track Evaluation. In Proceedings of the Eighth Text Retrieval Conference (TREC- 202 B. Wellner, L. Ferro, W. Greiff, and L. Hirschman. 2005. Reading comprehension tests for computerunderstand evaluation. Language En- 12(4):305-334. Cambridge University Press, Cambridge, UK. J.M. Zelle and R.J. Mooney. 1996. Learning to Parse Database Queries using Inductive Logic Program- In of the Thirteenth National on Artificial Intelligence 1050- 1055. L.S. Zettlemoyer and M. Collins. 2009. Learning Context-Dependent Mappings from Sentences to Logical In of the Annual Meeting of Association for Computation Linguistics 976-984. G. Zweig and C.J.C. Burges. 2012. A Challenge Set for</note>
<title confidence="0.76483">Language of the Workshop on the Future of Language Modeling</title>
<affiliation confidence="0.642882">HLT,</affiliation>
<address confidence="0.794612">203</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Agarwal</author>
<author>P Mannem</author>
</authors>
<title>Automatic Gap-fill Question Generation from Text Books.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>56--64</pages>
<marker>Agarwal, Mannem, 2011</marker>
<rawString>M. Agarwal and P. Mannem. 2011. Automatic Gap-fill Question Generation from Text Books. In Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, 56–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Breck</author>
<author>M Light</author>
<author>E Riloff G S Mann</author>
<author>B Brown</author>
<author>P Anand</author>
<author>M Rooth M Thelen</author>
</authors>
<title>Looking under the hood: Tools for diagnosing your question answering engine.</title>
<date>2001</date>
<booktitle>In Proceedings of the workshop on Opendomain question answering,</booktitle>
<volume>12</volume>
<pages>1--8</pages>
<contexts>
<context position="7723" citStr="Breck et al. (2001)" startWordPosition="1240" endWordPosition="1243">cal form (Wellner et al., 2006). Work on story understanding itself goes back much further, to 1972, when Charniak proposed using a background model to answer questions about children’s stories. Similarly, the TREC (and TAC) Question Answering tracks (e.g., Voorhees and Tice, 1999) aim to evaluate systems on their ability to answer factual questions such as “Where is the Taj Mahal”. The QA4MRE task also aims to evaluate machine reading systems through question answering (e.g., Clark et al., 2012). Earlier work has also aimed at controlling the scope by limiting the text to children’s stories: Breck et al. (2001) collected 75 stories from the Canadian Broadcasting Corporation’s web site for children, and generated 650 questions for them manually, where each question was answered by a sentence in the text. Leidner et al. (2003) both enriched the CBC4kids data by adding several layers of annotation (such as semantic and POS tags), and measured QA performance as a function of question difficulty. For a further compendium of resources related to the story comprehension task, see Mueller (2010). The task proposed here differs from the above work in several ways. Most importantly, the data collection is sca</context>
</contexts>
<marker>Breck, Light, Mann, Brown, Anand, Thelen, 2001</marker>
<rawString>E. Breck, M. Light, G.S.Mann, E. Riloff, B. Brown, P. Anand, M. Rooth M. Thelen. 2001. Looking under the hood: Tools for diagnosing your question answering engine. In Proceedings of the workshop on Opendomain question answering, 12, 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Toward a Model of Children’s Story Comprehension.</title>
<date>1972</date>
<tech>Technical Report, 266,</tech>
<institution>MIT Artificial Intelligence Laboratory,</institution>
<location>Cambridge, MA.</location>
<marker>Charniak, 1972</marker>
<rawString>E. Charniak. 1972. Toward a Model of Children’s Story Comprehension. Technical Report, 266, MIT Artificial Intelligence Laboratory, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Clark</author>
<author>P Harrison</author>
<author>X Yao</author>
</authors>
<title>An Entailment-Based Approach to the QA4MRE Challenge.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference and Labs of the Evaluation Forum (CLEF)</booktitle>
<contexts>
<context position="7605" citStr="Clark et al., 2012" startWordPosition="1219" endWordPosition="1222"> (Grois and Wilkins, 2005), named entity resolution (Harabagiu et al., 2003) and mapping questions and answers to logical form (Wellner et al., 2006). Work on story understanding itself goes back much further, to 1972, when Charniak proposed using a background model to answer questions about children’s stories. Similarly, the TREC (and TAC) Question Answering tracks (e.g., Voorhees and Tice, 1999) aim to evaluate systems on their ability to answer factual questions such as “Where is the Taj Mahal”. The QA4MRE task also aims to evaluate machine reading systems through question answering (e.g., Clark et al., 2012). Earlier work has also aimed at controlling the scope by limiting the text to children’s stories: Breck et al. (2001) collected 75 stories from the Canadian Broadcasting Corporation’s web site for children, and generated 650 questions for them manually, where each question was answered by a sentence in the text. Leidner et al. (2003) both enriched the CBC4kids data by adding several layers of annotation (such as semantic and POS tags), and measured QA performance as a function of question difficulty. For a further compendium of resources related to the story comprehension task, see Mueller (2</context>
</contexts>
<marker>Clark, Harrison, Yao, 2012</marker>
<rawString>P. Clark, P. Harrison, and X. Yao. An Entailment-Based Approach to the QA4MRE Challenge. 2012. In Proceedings of the Conference and Labs of the Evaluation Forum (CLEF) 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>E Agichtein</author>
</authors>
<title>Factoid Question Answering over Unstructured and Structured Content on the Web.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourteenth Text Retrieval Conference (TREC).</booktitle>
<contexts>
<context position="32338" citStr="Cucerzan and Agichtein, 2005" startWordPosition="5465" endWordPosition="5468">rticular text. We can cast MCTest as an RTE task by converting each question-answer pair into a statement, and then selecting the answer whose statement has the highest likelihood of being entailed by the story. For example, in the sample story given in Figure 1, the second question can be converted into four statements (one for each answer), and the RTE system should select the statement “James pulled pudding off of the shelves in the grocery store” as the most likely one. For converting question-answer pairs to statements, we used the rules employed in a web-based question answering system (Cucerzan and Agichtein, 2005). For RTE, we used BIUTEE (Stern and Dagan, 2011), which performs better than the median system in the past four RTE competitions. We ran BIUTEE both in its default configuration, as well as with its optional additional data sources (FrameNet, ReVerb, DIRT, and others as found on the BIUTEE home page). The default configuration performed better so we present its results here. The results in Table 6 show that the RTE method performed worse than the baseline. 200 We also combined the baseline and RTE system by training BIUTEE on the train set and using the development set to optimize a linear co</context>
</contexts>
<marker>Cucerzan, Agichtein, 2005</marker>
<rawString>S. Cucerzan and E. Agichtein. 2005. Factoid Question Answering over Unstructured and Structured Content on the Web. In Proceedings of the Fourteenth Text Retrieval Conference (TREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge. In</title>
<date>2006</date>
<journal>Lecture Notes in Computer Science,</journal>
<volume>3944</volume>
<pages>177--190</pages>
<publisher>Springer.</publisher>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>I. Dagan, O. Glickman, and B. Magnini. 2006. The PASCAL Recognising Textual Entailment Challenge. In J. Quiñonero-Candela, I. Dagan, B. Magnini, F. d&apos;Alché-Buc (Eds.), Machine Learning Challenges. Lecture Notes in Computer Science, Vol. 3944, pp. 177-190, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Goldwasser</author>
<author>R Reichart</author>
<author>J Clarke</author>
<author>D Roth</author>
</authors>
<title>Confidence Driven Unsupervised Semantic Parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1486--1495</pages>
<contexts>
<context position="6187" citStr="Goldwasser et al., 2011" startWordPosition="989" endWordPosition="992">ry. DARPA introduced the Airline Travel Information System (ATIS) in the early 90’s: there the task was to slot-fill flight-related information by modeling the intent of spoken language (see Tur et al., 2010, for a review). This data continues to be a used in the semantic modeling community (see, for example, Zettlemoyer and Collins, 2009). The Geoquery database contains 880 geographical facts about the US and has played a similar role for written (as opposed to spoken) natural language queries against a database (Zelle and Mooney, 1996) and it also continues to spur research (see for example Goldwasser et al., 2011), as does the similar Jobs database, which provides mappings of 640 sentences to a listing of jobs (Tang and Mooney, 2001). More recently, Zweig and Burges (2012) provided a set of 1040 sentences that comprise an SAT-style multiple choice sentence completion task. The idea of using story-based reading comprehension questions to evaluate methods for machine reading itself goes back over a decade, when Hirschmann et al. (1999) showed that a bag of words approach, together with some heuristic linguistic modeling, could achieve 40% accuracy for the task of picking the sentence that best matches th</context>
</contexts>
<marker>Goldwasser, Reichart, Clarke, Roth, 2011</marker>
<rawString>D. Goldwasser, R. Reichart, J. Clarke, D. Roth. 2011. Confidence Driven Unsupervised Semantic Parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, 1486-1495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Grois</author>
<author>D C Wilkins</author>
</authors>
<title>Learning Strategies for Story Comprehension: A Reinforcement Learning Approach.</title>
<date>2005</date>
<booktitle>In Proceedings of the Twenty Second International Conference on Machine Learning,</booktitle>
<pages>257--264</pages>
<contexts>
<context position="7012" citStr="Grois and Wilkins, 2005" startWordPosition="1124" endWordPosition="1127">comprise an SAT-style multiple choice sentence completion task. The idea of using story-based reading comprehension questions to evaluate methods for machine reading itself goes back over a decade, when Hirschmann et al. (1999) showed that a bag of words approach, together with some heuristic linguistic modeling, could achieve 40% accuracy for the task of picking the sentence that best matches the query for “who / what / when / where / why” questions, on a small reading comprehension dataset from Remedia. This dataset spurred several research efforts, for example using reinforcement learning (Grois and Wilkins, 2005), named entity resolution (Harabagiu et al., 2003) and mapping questions and answers to logical form (Wellner et al., 2006). Work on story understanding itself goes back much further, to 1972, when Charniak proposed using a background model to answer questions about children’s stories. Similarly, the TREC (and TAC) Question Answering tracks (e.g., Voorhees and Tice, 1999) aim to evaluate systems on their ability to answer factual questions such as “Where is the Taj Mahal”. The QA4MRE task also aims to evaluate machine reading systems through question answering (e.g., Clark et al., 2012). Earli</context>
</contexts>
<marker>Grois, Wilkins, 2005</marker>
<rawString>E. Grois and D.C. Wilkins. 2005. Learning Strategies for Story Comprehension: A Reinforcement Learning Approach. In Proceedings of the Twenty Second International Conference on Machine Learning, 257-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Harabagiu</author>
<author>S J Maiorano</author>
<author>M A Pasca</author>
</authors>
<title>Open-Domain Textual Question Answering Techniques.</title>
<date>2003</date>
<journal>Natural Language Engineering,</journal>
<pages>9--3</pages>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="7062" citStr="Harabagiu et al., 2003" startWordPosition="1131" endWordPosition="1134">letion task. The idea of using story-based reading comprehension questions to evaluate methods for machine reading itself goes back over a decade, when Hirschmann et al. (1999) showed that a bag of words approach, together with some heuristic linguistic modeling, could achieve 40% accuracy for the task of picking the sentence that best matches the query for “who / what / when / where / why” questions, on a small reading comprehension dataset from Remedia. This dataset spurred several research efforts, for example using reinforcement learning (Grois and Wilkins, 2005), named entity resolution (Harabagiu et al., 2003) and mapping questions and answers to logical form (Wellner et al., 2006). Work on story understanding itself goes back much further, to 1972, when Charniak proposed using a background model to answer questions about children’s stories. Similarly, the TREC (and TAC) Question Answering tracks (e.g., Voorhees and Tice, 1999) aim to evaluate systems on their ability to answer factual questions such as “Where is the Taj Mahal”. The QA4MRE task also aims to evaluate machine reading systems through question answering (e.g., Clark et al., 2012). Earlier work has also aimed at controlling the scope by</context>
</contexts>
<marker>Harabagiu, Maiorano, Pasca, 2003</marker>
<rawString>S.M. Harabagiu, S.J. Maiorano, and M.A. Pasca. 2003. Open-Domain Textual Question Answering Techniques. Natural Language Engineering, 9(3):1-38. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
<author>M Light</author>
<author>E Breck</author>
<author>J D Burger</author>
</authors>
<title>Deep Read: A Reading Comprehension System.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>325--332</pages>
<marker>Hirschman, Light, Breck, Burger, 1999</marker>
<rawString>L. Hirschman, M. Light, E. Breck, and J.D. Burger. 1999. Deep Read: A Reading Comprehension System. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL), 325-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Horton</author>
<author>L Chilton</author>
</authors>
<title>The labor economics of paid crowdsourcing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th ACM Conference on Electronic Commerce,</booktitle>
<pages>209--218</pages>
<contexts>
<context position="15686" citStr="Horton and Chilton, 2010" startWordPosition="2619" endWordPosition="2622">r multiple sentences and we required that at least two are marked as multiple. 3.3 The Workers Workers were required to reside in the United States and to have completed 100 HITs with an over 95% approval rate4. The median worker took 22 minutes to complete the task. We paid workers $2.50 per story set and allowed each to do a maximum of 8 tasks (5 in MC500). We did not experiment with paying less, but this rate amounts to $6.82/hour, which is approximately the rate paid by other writing tasks on AMT at the time, though is also significantly higher than the median wage of $1.38 found in 2010 (Horton and Chilton, 2010). Workers could optionally leave feedback on the task, which was overwhelmingly positive – the most frequent non-stopword in the comments was “fun” and the most frequent phrase was “thank you”. The only negative comments (in &lt;1% of submissions) were when the worker felt that a particular word should have been on the allowed vocabulary list. Given the positive feedback, it may be possible to pay less if we collect more data in the future. We did not enforce story length constraints, but some workers interpreted our suggestion that the story be 150-300 words as a hard constraint, and some asked </context>
</contexts>
<marker>Horton, Chilton, 2010</marker>
<rawString>J. Horton and L. Chilton. 2010. The labor economics of paid crowdsourcing. In Proceedings of the 11th ACM Conference on Electronic Commerce, 209-218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Kuperman</author>
<author>H Stadthagen-Gonzalez</author>
<author>M Brysbaert</author>
</authors>
<title>Age-of-acquisition ratings for 30,000 English words.</title>
<date>2012</date>
<journal>Behavior Research Methods,</journal>
<pages>44--4</pages>
<contexts>
<context position="14586" citStr="Kuperman et al., 2012" startWordPosition="2425" endWordPosition="2428">in part upon the observation that standardized reading comprehension tests, whose goal after all is to test comprehension, generally avoid questions that can be answered by reading a single sentence. 3.2 Automatic Validation Besides verifying that the story and all of the questions and answers were provided, we performed the following automatic validation before allowing the worker to complete the task: Limited vocabulary: The lowercase words in the story, questions, and answers were stemmed and checked against a vocabulary list of approximately 8000 words that a 7-year old is likely to know (Kuperman et al., 2012). Any words not on the list were highlighted in red as the worker typed, and the task could not be submitted unless all of the words satisfied this vocabulary criterion. To allow the use of arbitrary proper nouns, capitalized words were not checked against the vocabulary list. Multiple-sentence questions: As described earlier, we required that at least two of the questions need multiple sentences to answer. Workers were simply asked to mark whether a question needs one or multiple sentences and we required that at least two are marked as multiple. 3.3 The Workers Workers were required to resid</context>
</contexts>
<marker>Kuperman, Stadthagen-Gonzalez, Brysbaert, 2012</marker>
<rawString>V. Kuperman, H. Stadthagen-Gonzalez, M. Brysbaert. 2012. Age-of-acquisition ratings for 30,000 English words. Behavior Research Methods, 44(4):978-990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Leidner</author>
<author>T Dalmas</author>
<author>B Webber</author>
<author>J Bos</author>
<author>C Grover</author>
</authors>
<title>Automatic Multi-Layer Corpus Annotation for Evaluating Question Answering Methods: CBC4Kids.</title>
<date>2003</date>
<booktitle>In Proceedings of the 3rd International Workshop on Linguistically Interpreted Corpora.</booktitle>
<contexts>
<context position="7941" citStr="Leidner et al. (2003)" startWordPosition="1274" endWordPosition="1277">and TAC) Question Answering tracks (e.g., Voorhees and Tice, 1999) aim to evaluate systems on their ability to answer factual questions such as “Where is the Taj Mahal”. The QA4MRE task also aims to evaluate machine reading systems through question answering (e.g., Clark et al., 2012). Earlier work has also aimed at controlling the scope by limiting the text to children’s stories: Breck et al. (2001) collected 75 stories from the Canadian Broadcasting Corporation’s web site for children, and generated 650 questions for them manually, where each question was answered by a sentence in the text. Leidner et al. (2003) both enriched the CBC4kids data by adding several layers of annotation (such as semantic and POS tags), and measured QA performance as a function of question difficulty. For a further compendium of resources related to the story comprehension task, see Mueller (2010). The task proposed here differs from the above work in several ways. Most importantly, the data collection is scalable: if the dataset proves sufficiently useful to others, it would be straightforward to gather an order of magnitude more. Even the dataset size presented here is an order of magnitude larger than the Remedia or the</context>
</contexts>
<marker>Leidner, Dalmas, Webber, Bos, Grover, 2003</marker>
<rawString>J.L. Leidner, T. Dalmas, B. Webber, J. Bos, C. Grover. 2003. Automatic Multi-Layer Corpus Annotation for Evaluating Question Answering Methods: CBC4Kids. In Proceedings of the 3rd International Workshop on Linguistically Interpreted Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E T Mueller</author>
</authors>
<title>Story Understanding Resources.</title>
<date>2010</date>
<note>http://xenia.media.mit.edu/~mueller/storyund/storyre s.html.</note>
<contexts>
<context position="8209" citStr="Mueller (2010)" startWordPosition="1320" endWordPosition="1321">al., 2012). Earlier work has also aimed at controlling the scope by limiting the text to children’s stories: Breck et al. (2001) collected 75 stories from the Canadian Broadcasting Corporation’s web site for children, and generated 650 questions for them manually, where each question was answered by a sentence in the text. Leidner et al. (2003) both enriched the CBC4kids data by adding several layers of annotation (such as semantic and POS tags), and measured QA performance as a function of question difficulty. For a further compendium of resources related to the story comprehension task, see Mueller (2010). The task proposed here differs from the above work in several ways. Most importantly, the data collection is scalable: if the dataset proves sufficiently useful to others, it would be straightforward to gather an order of magnitude more. Even the dataset size presented here is an order of magnitude larger than the Remedia or the CBC4kids data and many times larger than QA4MRE. Second, the multiple choice task presents less ambiguity (and is consequently easier to collect data for) than the 194 James the Turtle was always getting in trouble. Sometimes he&apos;d reach into the freezer and empty out</context>
</contexts>
<marker>Mueller, 2010</marker>
<rawString>E.T. Mueller. 2010. Story Understanding Resources. http://xenia.media.mit.edu/~mueller/storyund/storyre s.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Paolacci</author>
<author>J Chandler</author>
<author>P Iperirotis</author>
</authors>
<title>Running experiments on Amazon Mechanical Turk. Judgment and Decision Making.</title>
<date>2010</date>
<pages>5--5</pages>
<contexts>
<context position="11887" citStr="Paolacci et al., 2010" startWordPosition="1981" endWordPosition="1984">t input as possible. Thus, we designed the process to be gated by cost, and keeping the costs low was a high priority. Crowd-sourcing seemed particularly appropriate, given the nature of the task, so we opted to use Amazon Mechanical Turk2 (AMT). With over 500,000 workers3, it provides the work force required to both achieve scalability and, equally importantly, to provide diversity in the stories and types of questions. We restricted our task to AMT workers (workers) residing in the United States. The average worker is 36 years old, more educated than the United States population in general (Paolacci et al., 2010), and the majority of workers are female. 3.1 The Story and Questions Workers were instructed to write a short (150-300 words) fictional story, and to write as if for a child in grade school. The choice of 150-300 was made to keep the task an appropriate size for workers while still allowing for complex stories and questions. The workers were free to write about any topic they desired (as long as it was appropriate for a young child), and so there is a wide range, including vacations, animals, school, cars, eating, gardening, fairy tales, spaceships, and cowboys. 1 We use the term “story set” </context>
</contexts>
<marker>Paolacci, Chandler, Iperirotis, 2010</marker>
<rawString>G. Paolacci, J. Chandler, and P. Iperirotis. 2010. Running experiments on Amazon Mechanical Turk. Judgment and Decision Making. 5(5):411-419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Stamatatos</author>
</authors>
<title>A survey of modern authorship attribution methods.</title>
<date>2009</date>
<journal>J. Am. Soc. Inf. Sci.,</journal>
<volume>60</volume>
<pages>556</pages>
<contexts>
<context position="35741" citStr="Stamatatos, 2009" startWordPosition="6015" endWordPosition="6016"> for example, to measure machine performance vs. human performance on a per-question basis (i.e., does your algorithm make similar mistakes to humans?), or vs. the judged clarity of each story. The ratings, as well as whether a question needs multiple sentences to answer, should typically only be used in evaluation, since such information is not generally available for most text. We will also provide an anonymized author id for each story, which could allow additional research such as using other works by the same author when understanding a story, or research on authorship attribution (e.g., Stamatatos, 2009). 9 Future Work We plan to use this dataset to evaluate approaches for machine comprehension, but are making it available now so that others may do the same. If MCTest is used we will collect more story sets and will continue to refine the collection process. One interesting research direction is ensuring that the questions are difficult enough to challenge state-ofthe-art techniques as they develop. One idea for this is to apply existing techniques automatically during story set creation to see whether a question is too easily answered by a machine. By requiring authors to create difficult qu</context>
</contexts>
<marker>Stamatatos, 2009</marker>
<rawString>E. Stamatatos. 2009. A survey of modern authorship attribution methods. J. Am. Soc. Inf. Sci., 60:538– 556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stern</author>
<author>I Dagan</author>
</authors>
<title>A Confidence Model for Syntactically-Motivated Entailment Proofs.</title>
<date>2011</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing (RANLP).</booktitle>
<contexts>
<context position="32387" citStr="Stern and Dagan, 2011" startWordPosition="5474" endWordPosition="5477">erting each question-answer pair into a statement, and then selecting the answer whose statement has the highest likelihood of being entailed by the story. For example, in the sample story given in Figure 1, the second question can be converted into four statements (one for each answer), and the RTE system should select the statement “James pulled pudding off of the shelves in the grocery store” as the most likely one. For converting question-answer pairs to statements, we used the rules employed in a web-based question answering system (Cucerzan and Agichtein, 2005). For RTE, we used BIUTEE (Stern and Dagan, 2011), which performs better than the median system in the past four RTE competitions. We ran BIUTEE both in its default configuration, as well as with its optional additional data sources (FrameNet, ReVerb, DIRT, and others as found on the BIUTEE home page). The default configuration performed better so we present its results here. The results in Table 6 show that the RTE method performed worse than the baseline. 200 We also combined the baseline and RTE system by training BIUTEE on the train set and using the development set to optimize a linear combination of BIUTEE with the baseline; the combin</context>
</contexts>
<marker>Stern, Dagan, 2011</marker>
<rawString>A. Stern and I. Dagan. 2011. A Confidence Model for Syntactically-Motivated Entailment Proofs. In Proceedings of Recent Advances in Natural Language Processing (RANLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Tang</author>
<author>R J Mooney</author>
</authors>
<title>Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of the 12th European Conference on Machine Learning (ECML),</booktitle>
<pages>466--477</pages>
<contexts>
<context position="6309" citStr="Tang and Mooney, 2001" startWordPosition="1010" endWordPosition="1013">related information by modeling the intent of spoken language (see Tur et al., 2010, for a review). This data continues to be a used in the semantic modeling community (see, for example, Zettlemoyer and Collins, 2009). The Geoquery database contains 880 geographical facts about the US and has played a similar role for written (as opposed to spoken) natural language queries against a database (Zelle and Mooney, 1996) and it also continues to spur research (see for example Goldwasser et al., 2011), as does the similar Jobs database, which provides mappings of 640 sentences to a listing of jobs (Tang and Mooney, 2001). More recently, Zweig and Burges (2012) provided a set of 1040 sentences that comprise an SAT-style multiple choice sentence completion task. The idea of using story-based reading comprehension questions to evaluate methods for machine reading itself goes back over a decade, when Hirschmann et al. (1999) showed that a bag of words approach, together with some heuristic linguistic modeling, could achieve 40% accuracy for the task of picking the sentence that best matches the query for “who / what / when / where / why” questions, on a small reading comprehension dataset from Remedia. This datas</context>
</contexts>
<marker>Tang, Mooney, 2001</marker>
<rawString>L.R. Tang and R.J. Mooney. 2001. Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing. In Proceedings of the 12th European Conference on Machine Learning (ECML), 466-477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tur</author>
<author>D Hakkani-Tur</author>
<author>L Heck</author>
</authors>
<title>What is left to be understood in ATIS? Spoken Language Technology Workshop,</title>
<date>2010</date>
<contexts>
<context position="5770" citStr="Tur et al., 2010" startWordPosition="916" endWordPosition="919"> and (3) for the free availability that can only result from providing non-copyrighted data. The content is freely available at http://research.microsoft.com/mct, and we plan to use that site to track published results and provide other resources, such as labels of various kinds. 2 Previous Work The research goal of mapping text to meaning representations in order to solve particular tasks has a long history. DARPA introduced the Airline Travel Information System (ATIS) in the early 90’s: there the task was to slot-fill flight-related information by modeling the intent of spoken language (see Tur et al., 2010, for a review). This data continues to be a used in the semantic modeling community (see, for example, Zettlemoyer and Collins, 2009). The Geoquery database contains 880 geographical facts about the US and has played a similar role for written (as opposed to spoken) natural language queries against a database (Zelle and Mooney, 1996) and it also continues to spur research (see for example Goldwasser et al., 2011), as does the similar Jobs database, which provides mappings of 640 sentences to a listing of jobs (Tang and Mooney, 2001). More recently, Zweig and Burges (2012) provided a set of 10</context>
</contexts>
<marker>Tur, Hakkani-Tur, Heck, 2010</marker>
<rawString>G. Tur, D. Hakkani-Tur, and L.Heck. 2010. What is left to be understood in ATIS? Spoken Language Technology Workshop, 19-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
<author>D M Tice</author>
</authors>
<title>The TREC-8 Question Answering Track Evaluation.</title>
<date>1999</date>
<booktitle>In Proceedings of the Eighth Text Retrieval Conference (TREC8).</booktitle>
<contexts>
<context position="7386" citStr="Voorhees and Tice, 1999" startWordPosition="1182" endWordPosition="1186">ence that best matches the query for “who / what / when / where / why” questions, on a small reading comprehension dataset from Remedia. This dataset spurred several research efforts, for example using reinforcement learning (Grois and Wilkins, 2005), named entity resolution (Harabagiu et al., 2003) and mapping questions and answers to logical form (Wellner et al., 2006). Work on story understanding itself goes back much further, to 1972, when Charniak proposed using a background model to answer questions about children’s stories. Similarly, the TREC (and TAC) Question Answering tracks (e.g., Voorhees and Tice, 1999) aim to evaluate systems on their ability to answer factual questions such as “Where is the Taj Mahal”. The QA4MRE task also aims to evaluate machine reading systems through question answering (e.g., Clark et al., 2012). Earlier work has also aimed at controlling the scope by limiting the text to children’s stories: Breck et al. (2001) collected 75 stories from the Canadian Broadcasting Corporation’s web site for children, and generated 650 questions for them manually, where each question was answered by a sentence in the text. Leidner et al. (2003) both enriched the CBC4kids data by adding se</context>
</contexts>
<marker>Voorhees, Tice, 1999</marker>
<rawString>E.M. Voorhees and D.M. Tice. 1999. The TREC-8 Question Answering Track Evaluation. In Proceedings of the Eighth Text Retrieval Conference (TREC8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wellner</author>
<author>L Ferro</author>
<author>W Greiff</author>
<author>L Hirschman</author>
</authors>
<title>Reading comprehension tests for computerbased understand evaluation.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<pages>12--4</pages>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<marker>Wellner, Ferro, Greiff, Hirschman, 2005</marker>
<rawString>B. Wellner, L. Ferro, W. Greiff, and L. Hirschman. 2005. Reading comprehension tests for computerbased understand evaluation. Natural Language Engineering, 12(4):305-334. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Zelle</author>
<author>R J Mooney</author>
</authors>
<title>Learning to Parse Database Queries using Inductive Logic Programming.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="6106" citStr="Zelle and Mooney, 1996" startWordPosition="974" endWordPosition="977">t to meaning representations in order to solve particular tasks has a long history. DARPA introduced the Airline Travel Information System (ATIS) in the early 90’s: there the task was to slot-fill flight-related information by modeling the intent of spoken language (see Tur et al., 2010, for a review). This data continues to be a used in the semantic modeling community (see, for example, Zettlemoyer and Collins, 2009). The Geoquery database contains 880 geographical facts about the US and has played a similar role for written (as opposed to spoken) natural language queries against a database (Zelle and Mooney, 1996) and it also continues to spur research (see for example Goldwasser et al., 2011), as does the similar Jobs database, which provides mappings of 640 sentences to a listing of jobs (Tang and Mooney, 2001). More recently, Zweig and Burges (2012) provided a set of 1040 sentences that comprise an SAT-style multiple choice sentence completion task. The idea of using story-based reading comprehension questions to evaluate methods for machine reading itself goes back over a decade, when Hirschmann et al. (1999) showed that a bag of words approach, together with some heuristic linguistic modeling, cou</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>J.M. Zelle and R.J. Mooney. 1996. Learning to Parse Database Queries using Inductive Logic Programming. In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI), 1050-1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning Context-Dependent Mappings from Sentences to Logical Form.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computation Linguistics (ACL),</booktitle>
<pages>976--984</pages>
<contexts>
<context position="5904" citStr="Zettlemoyer and Collins, 2009" startWordPosition="940" endWordPosition="943">able at http://research.microsoft.com/mct, and we plan to use that site to track published results and provide other resources, such as labels of various kinds. 2 Previous Work The research goal of mapping text to meaning representations in order to solve particular tasks has a long history. DARPA introduced the Airline Travel Information System (ATIS) in the early 90’s: there the task was to slot-fill flight-related information by modeling the intent of spoken language (see Tur et al., 2010, for a review). This data continues to be a used in the semantic modeling community (see, for example, Zettlemoyer and Collins, 2009). The Geoquery database contains 880 geographical facts about the US and has played a similar role for written (as opposed to spoken) natural language queries against a database (Zelle and Mooney, 1996) and it also continues to spur research (see for example Goldwasser et al., 2011), as does the similar Jobs database, which provides mappings of 640 sentences to a listing of jobs (Tang and Mooney, 2001). More recently, Zweig and Burges (2012) provided a set of 1040 sentences that comprise an SAT-style multiple choice sentence completion task. The idea of using story-based reading comprehension </context>
</contexts>
<marker>Zettlemoyer, Collins, 2009</marker>
<rawString>L.S. Zettlemoyer and M. Collins. 2009. Learning Context-Dependent Mappings from Sentences to Logical Form. In Proceedings of the 47th Annual Meeting of the Association for Computation Linguistics (ACL), 976-984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zweig</author>
<author>C J C Burges</author>
</authors>
<title>A Challenge Set for Advancing Language Modeling.</title>
<date>2012</date>
<booktitle>In Proceedings of the Workshop on the Future of Language Modeling for HLT, NAACL-HLT.</booktitle>
<contexts>
<context position="6349" citStr="Zweig and Burges (2012)" startWordPosition="1016" endWordPosition="1019">ent of spoken language (see Tur et al., 2010, for a review). This data continues to be a used in the semantic modeling community (see, for example, Zettlemoyer and Collins, 2009). The Geoquery database contains 880 geographical facts about the US and has played a similar role for written (as opposed to spoken) natural language queries against a database (Zelle and Mooney, 1996) and it also continues to spur research (see for example Goldwasser et al., 2011), as does the similar Jobs database, which provides mappings of 640 sentences to a listing of jobs (Tang and Mooney, 2001). More recently, Zweig and Burges (2012) provided a set of 1040 sentences that comprise an SAT-style multiple choice sentence completion task. The idea of using story-based reading comprehension questions to evaluate methods for machine reading itself goes back over a decade, when Hirschmann et al. (1999) showed that a bag of words approach, together with some heuristic linguistic modeling, could achieve 40% accuracy for the task of picking the sentence that best matches the query for “who / what / when / where / why” questions, on a small reading comprehension dataset from Remedia. This dataset spurred several research efforts, for</context>
</contexts>
<marker>Zweig, Burges, 2012</marker>
<rawString>G. Zweig and C.J.C. Burges. 2012. A Challenge Set for Advancing Language Modeling. In Proceedings of the Workshop on the Future of Language Modeling for HLT, NAACL-HLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>